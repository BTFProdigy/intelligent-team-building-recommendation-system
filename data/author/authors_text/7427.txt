ASSIST: Automated semantic assistance for translators
Serge Sharoff, Bogdan Babych
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych}@leeds.ac.uk
Paul Rayson, Olga Mudraya, Scott Piao
UCREL, Computing Department
Lancaster University, LA1 4WA, UK
{p.rayson,o.moudraia,s.piao}@lancs.ac.uk
Abstract
The problem we address in this paper is
that of providing contextual examples of
translation equivalents for words from the
general lexicon using comparable corpora
and semantic annotation that is uniform
for the source and target languages. For
a sentence, phrase or a query expression in
the source language the tool detects the se-
mantic type of the situation in question and
gives examples of similar contexts from
the target language corpus.
1 Introduction
It is widely acknowledged that human transla-
tors can benefit from a wide range of applications
in computational linguistics, including Machine
Translation (Carl and Way, 2003), Translation
Memory (Planas and Furuse, 2000), etc. There
have been recent research on tools detecting trans-
lation equivalents for technical vocabulary in a re-
stricted domain, e.g. (Dagan and Church, 1997;
Bennison and Bowker, 2000). The methodology
in this case is based on extraction of terminology
(both single and multiword units) and alignment
of extracted terms using linguistic and/or statisti-
cal techniques (D?jean et al, 2002).
In this project we concentrate on words from the
general lexicon instead of terminology. The ratio-
nale for this focus is related to the fact that trans-
lation of terms is (should be) stable, while gen-
eral words can vary significantly in their transla-
tion. It is important to populate the terminologi-
cal database with terms that are missed in dictio-
naries or specific to a problem domain. However,
once the translation of a term in a domain has been
identified, stored in a dictionary and learned by
the translator, the process of translation can go on
without consulting a dictionary or a corpus.
In contrast, words from the general lexicon ex-
hibit polysemy, which is reflected differently in
the target language, thus causing the dependency
of their translation on corresponding context. It
also happens quite frequently that such variation
is not captured by dictionaries. Novice translators
tend to rely on dictionaries and use direct trans-
lation equivalents whenever they are available. In
the end they produce translations that look awk-
ward and do not deliver the meaning intended by
the original text.
Parallel corpora consisting of original texts
aligned with their translations offer the possibility
to search for examples of translations in their con-
text. In this respect they provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative: millions of pages of orig-
inal texts are produced daily by native speakers
in major languages, such as English, while trans-
lations are produced by a small community of
trained translators from a small subset of source
texts. The imbalance between original texts and
translations is also reflected in the size of parallel
corpora, which are simply too small for variations
in translation of moderately frequent words. For
instance, frustrate occurs 631 times in 100 million
words of the BNC, i.e. this gives in average about
6 uses in a typical parallel corpus of one million
words.
2 System design
2.1 The research hypothesis
Our research hypothesis is that translators can be
assisted by software which suggests contextual ex-
139
amples in the target language that are semantically
and syntactically related to a selected example in
the source language. To enable greater coverage
we will exploit comparable rather than parallel
corpora.
Our research hypothesis leads us to a number of
research questions:
? Which semantic and syntactic contextual fea-
tures of the selected example in the source
language are important?
? How do we find similar contextual examples
in the target language?
? How do we sort the suggested target lan-
guage contextual examples in order to max-
imise their usefulness?
In order to restrict the research to what is
achievable within the scope of this project, we are
focussing on translation from English to Russian
using a comparable corpus of British and Rus-
sian newspaper texts. Newspapers cover a large
set of clearly identifiable topics that are compara-
ble across languages and cultures. In this project,
we have collected a 200-million-word corpus of
four major British newspapers and a 70-million-
word corpus of three major Russian newspapers
for roughly the same time span (2003-2004).1
In our proposed method, contexts of uses of En-
glish expressions defined by keywords are com-
pared to similar Russian expressions, using se-
mantic classes such as persons, places and insti-
tutions. For instance, the word agreement in the
example the parties were frustratingly close to
an agreement = ??????? ???? ?? ???????? ??????
? ?????????? ?????????? belongs to a seman-
tic class that also includes arrangement, contract,
deal, treaty. In the result, the search for collo-
cates of ??????? (close) in the context of agree-
ment words in Russian gives a short list of mod-
ifiers, which also includes the target: ?? ????????
??????.
2.2 Semantic taggers
In this project, we are porting the Lancaster En-
glish Semantic Tagger (EST) to the Russian lan-
guage. We have reused the existing semantic field
taxonomy of the Lancaster UCREL semantic anal-
ysis system (USAS), and applied it to Russian. We
1Russian newspapers are significantly shorter than their
British counterparts.
have also reused the existing software framework
developed during the construction of a Finnish Se-
mantic Tagger (L?fberg et al, 2005); the main ad-
justments and modifications required for Finnish
were to cope with the Unicode character set (UTF-
8) and word compounding.
USAS-EST is a software system for automatic
semantic analysis of text that was designed at
Lancaster University (Rayson et al, 2004). The
semantic tagset used by USAS was originally
loosely based on Tom McArthur?s Longman Lexi-
con of Contemporary English (McArthur, 1981).
It has a multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories.2
In the ASSIST project, we have been working on
both improving the existing EST and developing a
parallel tool for Russian - Russian Semantic Tag-
ger (RST). We have found that the USAS semantic
categories were compatible with the semantic cat-
egorizations of objects and phenomena in Russian,
as in the following example:3
poor JJ I1.1- A5.1- N5- E4.1- X9.1-
?????? A I1.1- A6.3- N5- O4.2- E4.1-
However, we needed a tool for analysing the
complex morpho-syntactic structure of Russian
words. Unlike English, Russian is a highly in-
flected language: generally, what is expressed in
English through phrases or syntactic structures
is expressed in Russian via morphological in-
flections, especially case endings and affixation.
For this purpose, we adopted a Russian morpho-
syntactic analyser Mystem that identifies word
forms, lemmas and morphological characteristics
for each word. Mystem is used as the equivalent
of the CLAWS part-of-speech (POS) tagger in the
USAS framework. Furthermore, we adopted the
Unicode UTF-8 encoding scheme to cope with the
Cyrillic alphabet. Despite these modifications, the
architecture of the RST software mirrors that of
the EST components in general.
The main lexical resources of the RST include
a single-word lexicon and a lexicon of multi-word
expressions (MWEs). We are building the Russian
lexical resources by exploiting both dictionaries
and corpora. We use readily available resources,
e.g. lists of proper names, which are then se-
2For the full tagset, see http://www.comp.lancs.
ac.uk/ucrel/usas/
3I1.1- = Money: lack; A5.1- = Evaluation: bad; N5- =
Quantities: little; E4.1- = Unhappy; X9.1- = Ability, intel-
ligence: poor; A6.3- = Comparing: little variety; O4.2- =
Judgement of appearance: bad
140
mantically classified. To bootstrap the system, we
have hand-tagged the 3,000 most frequent Russian
words based on a large newspaper corpus. Subse-
quently, the lexicons will be further expanded by
feeding texts from various sources into the RST
and classifying words that remain unmatched. In
addition, we will experiment with semi-automatic
lexicon construction using an existing machine-
readable English-Russian bilingual dictionary to
populate the Russian lexicon by mapping words
from each of the semantic fields in the English lex-
icon in turn. We aim at coverage of around 30,000
single lexical items and up to 9,000 MWEs, com-
pared to the EST which currently contains 54,727
single lexical items and 18,814 MWEs.
2.3 The user interface
The interface is powered by IMS Corpus Work-
bench (Christ, 1994) and is designed to be used in
the day-to-day workflow of novice and practising
translators, so the syntax of the CWB query lan-
guage has been simplified to adapt it to the needs
of the target user community.
The interface implements a search model for
finding translation equivalents in monolingual
comparable corpora, which integrates a number of
statistical and rule-based techniques for extending
search space, translating words and multiword ex-
pressions into the target language and restricting
the number of returned candidates in order to max-
imise precision and recall of relevant translation
equivalents. In the proposed search model queries
can be expanded by generating lists of collocations
for a given word or phrase, by generating sim-
ilarity classes4 or by manual selection of words
in concordances. Transfer between the source
language and target language is done via lookup
in a bilingual dictionary or via UCREL seman-
tic codes, which are common for concepts in both
languages. The search space is further restricted
by applying knowledge-based and statistical fil-
ters (such as part-of-speech and semantic class fil-
ters, IDF filter, etc), by testing the co-occurrence
of members of different similarity classes or by
manually selecting the presented variants. These
procedures are elementary building blocks that are
used in designing different search strategies effi-
cient for different types of translation equivalents
4Simclasses consist of words sharing collocates and are
computed using Singular Value Decomposition, as used by
(Rapp, 2004), e.g. Paris and Strasbourg are produced for
Brussels, or bus, tram and driver for passenger.
and contexts.
The core functionality of the system is intended
to be self-explanatory and to have a shallow learn-
ing curve: in many cases default search parame-
ters work well, so it is sufficient to input a word
or an expression in the source language in or-
der to get back a useful list of translation equiv-
alents, which can be manually checked by a trans-
lator to identify the most suitable solution for a
given context. For example, the word combina-
tion frustrated passenger is not found in the ma-
jor English-Russian dictionaries, while none of the
candidate translations of frustrated are suitable in
this context. The default search strategy for this
phrase is to generate the similarity class for En-
glish words frustrate, passenger, produce all pos-
sible translations using a dictionary and to test co-
occurrence of the resulting Russian words in target
language corpora. This returns a list of 32 Rus-
sian phrases, which follow the pattern of ?annoyed
/ impatient / unhappy + commuter / passenger /
driver?. Among other examples the list includes
an appropriate translation ??????????? ????????
(?unsatisfied passenger?).
The following example demonstrates the sys-
tem?s ability to find equivalents when there is
a reliable context to identify terms in the two
languages. Recent political developments in
Russia produced a new expression ?????????????
?????????? (?representative of president?), which
is as yet too novel to be listed in dictionaries.
However, the system can help to identify the peo-
ple that perform this duty, translate their names
to English and extract the set of collocates that
frequently appear around their names in British
newspapers, including Putin?s personal envoy and
Putin?s regional representative, even if no specific
term has been established for this purpose in the
British media.
As words cannot be translated in isolation and
their potential translation equivalents also often
consist of several words, the system detects not
only single-word collocates, but also multiword
expressions. For instance, the set of Russian
collocates of ?????????? (bureaucracy) includes
???????? (Brussels), which offers a straightfor-
ward translation into English and has such mul-
tiword collocates as red tape, which is a suitable
contextual translation for ??????????.
More experienced users can modify default pa-
rameters and try alternative strategies, construct
141
their own search paths from available basic build-
ing blocks and store them for future use. Stored
strategies comprise several elementary stages but
are executed in one go, although intermediate re-
sults can also be accessed via the ?history? frame.
Several search paths can be tried in parallel and
displayed together, so an optimal strategy for a
given class of phrases can be more easily identi-
fied.
Unlike Machine Translation, the system does
not translate texts. The main thrust of the sys-
tem lies in its ability to find several target language
examples that are relevant to the source language
expression. In some cases this results in sugges-
tions that can be directly used for translating the
source example, while in other cases the system
provides hints for the translator about the range of
target language expressions beyond what is avail-
able in bilingual dictionaries. Even if the preci-
sion of the current version is not satisfactory for an
MT system (2-3 suitable translations out of 30-50
suggested examples), human translators are able
to skim through the suggested set to find what is
relevant for the given translation task.
3 Conclusions
The set of tools is now under further development.
This involves an extension of the English seman-
tic tagger, development of the Russian tagger with
the target lexical coverage of 90% of source texts,
designing the procedure for retrieval of semanti-
cally similar situations and completing the user in-
terface. Identification of semantically similar sit-
uations can be improved by the use of segment-
matching algorithms as employed in Example-
Based MT and translation memories (Planas and
Furuse, 2000; Carl and Way, 2003).
There are two main applications of the pro-
posed methodology. One concerns training trans-
lators and advanced foreign language (FL) learn-
ers to make them aware of the variety of transla-
tion equivalents beyond the set offered by the dic-
tionary. The other application pertains to the de-
velopment of tools for practising translators. Al-
though the Russian language is not typologically
close to English and uses another writing system
which does not allow easy identification of cog-
nates, Russian and English belong to the same
Indo-European family and the contents of Rus-
sian and English newspapers reflect the same set
of topics. Nevertheless, the application of this
research need not be restricted to the English-
Russian pair only. The methodology for multilin-
gual processing of monolingual comparable cor-
pora, first tested in this project, will provide a
blueprint for the development of similar tools for
other language combinations.
Acknowledgments
The project is supported by two EPSRC grants:
EP/C004574 for Lancaster, EP/C005902 for Leeds.
References
Peter Bennison and Lynne Bowker. 2000. Designing a
tool for exploiting bilingual comparable corpora. In
Proceedings of LREC 2000, Athens, Greece.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Oliver Christ. 1994. A modular and flexible archi-
tecture for an integrated corpus query system. In
COMPLEX?94, Budapest.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Herv? D?jean, ?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING 2002.
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka
Juntunen, Asko Nyk?nen, and Krista Varantola.
2005. A semantic tagger for the Finnish language.
In Proceedings of the Corpus Linguistics 2005 con-
ference.
Tom McArthur. 1981. Longman Lexicon of Contem-
porary English. Longman.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of LREC 2004, pages 395?398.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proceedings of the workshop on Be-
yond Named Entity Recognition Semantic labelling
for NLP tasks in association with LREC 2004, pages
7?12.
142
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 739?746,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using comparable corpora
to solve problems difficult for human translators
Serge Sharoff, Bogdan Babych, Anthony Hartley
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych,a.hartley}@leeds.ac.uk
Abstract
In this paper we present a tool that uses
comparable corpora to find appropriate
translation equivalents for expressions that
are considered by translators as difficult.
For a phrase in the source language the
tool identifies a range of possible expres-
sions used in similar contexts in target lan-
guage corpora and presents them to the
translator as a list of suggestions. In the
paper we discuss the method and present
results of human evaluation of the perfor-
mance of the tool, which highlight its use-
fulness when dictionary solutions are lack-
ing.
1 Introduction
There is no doubt that both professional and
trainee translators need access to authentic data
provided by corpora. With respect to polyse-
mous lexical items, bilingual dictionaries list sev-
eral translation equivalents for a headword, but
words taken in their contexts can be translated
in many more ways than indicated in dictionar-
ies. For instance, the Oxford Russian Dictionary
(ORD) lacks a translation for the Russian expres-
sion ????????????? ????? (?comprehensive an-
swer?), while the Multitran Russian-English dic-
tionary suggests that it can be translated as ir-
refragable answer. Yet this expression is ex-
tremely rare in English; on the Internet it occurs
mostly in pages produced by Russian speakers.
On the other hand, translations for polysemous
words are too numerous to be listed for all pos-
sible contexts. For example, the entry for strong
in ORD already has 57 subentries and yet it fails
to mention many word combinations frequent in
the British National Corpus (BNC), such as strong
{feeling, field, opposition, sense, voice}. Strong
voice is also not listed in the Oxford French, Ger-
man or Spanish Dictionaries.
There has been surprisingly little research on
computational methods for finding translation
equivalents of words from the general lexicon.
Practically all previous studies have concerned
detection of terminological equivalence. For in-
stance, project Termight at AT&T aimed to de-
velop a tool for semi-automatic acquisition of
termbanks in the computer science domain (Da-
gan and Church, 1997). There was also a study
concerning the use of multilingual webpages to
develop bilingual lexicons and termbanks (Grefen-
stette, 2002). However, neither of them concerned
translations of words from the general lexicon. At
the same time, translators often experience more
difficulty in dealing with such general expressions
because of their polysemy, which is reflected dif-
ferently in the target language, thus causing the
dependency of their translation on the correspond-
ing context. Such variation is often not captured
by dictionaries.
Because of their importance, words from the
general lexicon are studied by translation re-
searchers, and comparable corpora are increas-
ingly used in translation practice and training
(Varantola, 2003). However, such studies are
mostly confined to lexicographic exercises, which
compare the contexts and functions of potential
translation equivalents once they are known, for
instance, absolutely vs. assolutamente in Italian
(Partington, 1998). Such studies do not pro-
vide a computational model for finding appropri-
ate translation equivalents for expressions that are
not listed or are inadequate in dictionaries.
Parallel corpora, conisting of original texts and
739
their exact translations, provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative. Many of them are in the
range of a few million words, which is simply too
small to account for variations in translation of
moderately frequent words. Those that are a bit
larger, such as the Europarl corpus, are restricted
in their domain. For instance, all of the 14 in-
stances of strong voice in the English section of
Europarl are used in the sense of ?the opinion of
a political institution?. At the same time the BNC
contains 46 instances of strong voice covering sev-
eral different meanings.
In this paper we propose a computational
method for using comparable corpora to find trans-
lation equivalents for source language expressions
that are considered as difficult by trainee or pro-
fessional translators. The model is based on de-
tecting frequent multi-word expressions (MWEs)
in the source and target languages and finding a
mapping between them in comparable monolin-
gual corpora, which are designed in a similar way
in the two languages.
The described methodology is implemented in
ASSIST, a tool that helps translators to find solu-
tions for difficult translation problems. The tool
presents the results as lists of translation sugges-
tions (usually 50 to 100 items) ordered alphabeti-
cally or by their frequency in target language cor-
pora. Translators can skim through these lists and
identify an example which is most appropriate in
a given context.
In the following sections we outline our ap-
proach, evaluate the output of the prototype of AS-
SIST and discuss future work.
2 Finding translations in comparable
corpora
The proposed model finds potential translation
equivalents in four steps, which include
1. expansion of words in the original expression
using related words;
2. translation of the resultant set using existing
bilingual dictionaries;
3. further expansion of the set using related
words in the target language;
4. filtering of the set according to expressions
frequent in the target language corpus.
In this study we use several comparable cor-
pora for English and Russian, including large ref-
erence corpora (the BNC and the Russian Refer-
ence Corpus) and corpora of major British and
Russian newspapers. All corpora used in the study
are quite large, i.e. the size of each corpus is in
the range of 100-200 million words (MW), so that
they provide enough evidence to detect such col-
locations as strong voice and clear defiance.
Although the current study is restricted to the
English-Russian pair, the methodology does not
rely on any particular language. It can be ex-
tended to other languages for which large com-
parable corpora, POS-tagging and lemmatisation
tools, and bilingual dictionaries are available. For
example, we conducted a small study for transla-
tion between English and German using the Ox-
ford German Dictionary and a 200 MW German
corpus derived from the Internet (Sharoff, 2006).
2.1 Query expansion
The problem with using comparable corpora to
find translation equivalents is that there is no ob-
vious bridge between the two languages. Unlike
aligned parallel corpora, comparable corpora pro-
vide a model for each individual language, while
dictionaries, which can serve as a bridge, are inad-
equate for the task in question, because the prob-
lem we want to address involves precisely transla-
tion equivalents that are not listed there.
Therefore, a specific query needs first to be
generalised in order to then retrieve a suitable
candidate from a set of candidates. One way
to generalise the query is by using similarity
classes, i.e. groups of words with lexically simi-
lar behaviour. In his work on distributional sim-
ilarity (Lin, 1998) designed a parser to identify
grammatical relationships between words. How-
ever, broad-coverage parsers suitable for process-
ing BNC-like corpora are not available for many
languages. Another, resource-light approach treats
the context as a bag of words (BoW) and detects
the similarity of contexts on the basis of colloca-
tions in a window of a certain size, typically 3-4
words, e.g. (Rapp, 2004). Even if using a parser
can increase precision in identification of contexts
in the case of long-distance dependencies (e.g. to
cook Alice a whole meal), we can find a reason-
able set of relevant terms returned using the BoW
approach, cf. the results of human evaluation for
English and German by (Rapp, 2004).
740
For each source word s0 we produce a list of
similar words: ?(s0) = s1, . . . , sN (in our tool
we use N = 20 as the cutoff). Since lists of dis-
tributionally words can contain words irrelevant to
the source word, we filter them to produce a more
reliable similarity class S(s0) using the assump-
tion that the similarity classes of similar words
have common members:
?w ? S(s0), w ? ?(s0)&w ?
?
?(si)
This yields for experience the following similar-
ity class: knowledge, opportunity, life, encounter,
skill, feeling, reality, sensation, dream, vision,
learning, perception, learn.1 Even if there is no
requirement in the BoW approach that words in
the similarity class are of the same part of speech,
it happens quite frequently that most words have
the same part of speech because of the similarity
of contexts.
2.2 Query translation and further expansion
In the next step we produce a translation class by
translating all words from the similarity class into
the target language using a bilingual dictionary
(T (w) for the translation of w). Then for Step 3
we have two options: a full translation class (TF )
and a reduced one (TR).
TF consists of similarity classes produced for
all translations: S(T (S(s0))). However, this
causes a combinatorial explosion. If a similarity
class contains N words (the average figure is 16)
and a dictionary lists on average M equivalents
for a source word (the average figure is 11), this
procedure outputs on average M ? N2 words in
the full translation class. For instance, the com-
plete translation class for experience contains 998
words. What is worse, some words from the full
translation class do not refer to the domain im-
plied in the original expression because of the am-
biguity of the translation operation. For instance,
the word dream belongs to the similarity class of
experience. Since it can be translated into Rus-
sian as ?????? (?fairy-tale?), the latter Russian word
will be expanded in the full translation class with
words referring to legends and stories. In the later
stages of the project, word sense disambiguation
in corpora could improve precision of translation
classes. However at the present stage we attempt
to trade the recall of the tool for greater precision
by translating words in the source similarity class,
1Ordered according to the score produced by the Singular
Value Decomposition method as implemented by Rapp.
and generating the similarity classes of transla-
tions only for the source word:
TR(s0) = S(T (s0)) ? T (S(s0)).
This reduces the class of experience to 128 words.
This step crucially relies on a wide-coverage
machine readable dictionary. The bilingual dictio-
nary resources we use are derived from the source
file for the Oxford Russian Dictionary, provided
by OUP.
2.3 Filtering equivalence classes
In the final step we check all possible combina-
tions of words from the translation classes for their
frequency in target language corpora.
The number of elements in the set of theoreti-
cally possible combinations is usually very large:
?
Ti, where Ti is the number of words in the trans-
lation class of each word of the original MWE.
This number is much larger than the set of word
combinations which is found in the target lan-
guage corpora. For instance, daunting experience
has 202,594 combinations for the full translation
class of daunting experience and 6,144 for the re-
duced one. However, in the target language cor-
pora we can find only 2,256 collocations with fre-
quency > 2 for the full translation class and 92 for
the reduced one.
Each theoretically possible combination is gen-
erated and looked up in a database of MWEs
(which is much faster than querying corpora for
frequencies of potential collocations). The MWE
database was pre-compiled from corpora using a
method of filtering, similar to part-of-speech fil-
tering suggested in (Justeson and Katz, 1995): in
corpora each N-gram of length 2, 3 and 4 tokens
was checked against a set of filters.
However, instead of pre-defined patterns for en-
tire expressions our filtering method uses sets of
negative constraints, which are usually applied to
the edges of expressions. This change boosts re-
call of retrieved MWEs and allows us to use the
same set of patterns for MWEs of different length.
The filter uses constraints for both lexical and
part-of-speech features, which makes configura-
tion specifications more flexible.
The idea of applying a negative feature filter
rather than a set of positive patterns is based on
the observation that it is easier to describe unde-
sirable features than to enumerate complete lists of
patterns. For example, MWEs of any length end-
ing with a preposition are undesirable (particles in
741
British news Russian news
no of words 217,394,039 77,625,002
REs in filter 25 18
2-grams 6,361,596 5,457,848
3-grams 14,306,653 11,092,908
4-grams 19,668,956 11,514,626
Table 1: MWEs in News Corpora
phrasal verbs, which are desirable, are tagged dif-
ferently by the Tree Tagger, so there is no problem
with ambiguity here). Our filter captures this fact
by having a negative condition for the right edge of
the pattern (regular expression /_IN$/), rather than
enumerating all possible configurations which do
not contain a preposition at the end. In this sense
the filter is permissive: everything that is not ex-
plicitly forbidden is allowed, which makes the de-
scription more economical.
The same MWE database is used for check-
ing frequencies of multiword collocates for cor-
pus queries. For this task, candidate N-grams in
the vicinity of searched patterns are filtered us-
ing the same regular expression grammar of MWE
constraints, and then their corpus frequency is
checked in the database. Thus scores for mul-
tiword collocates can be computed from contin-
gency tables similarly to single-word collocates.
In addition, only MWEs with a frequency
higher than 1 are stored in the database. This fil-
ters out most expressions that co-occur by chance.
Table 1 gives an overview of the number of MWEs
from the news corpus which pass the filter. Other
corpora used in ASSIST (BNC and RRC) yield
similar results. MWE frequencies for each corpus
can be checked individually or joined together.
3 Evaluation
There are several attributes of our system which
can be evaluated, and many of them are crucial
for its efficient use in the workflow of professional
translators, including: usability, quality of final so-
lutions, trade-off between adequacy and fluency
across usable examples, precision and recall of po-
tentially relevant suggestions, as well as real-text
evaluation, i.e. ?What is the coverage of difficult
translation problems typically found in a text that
can be successfully tackled??
In this paper we focus on evaluating the quality
of potentially relevant translation solutions, which
is the central point for developing and calibrat-
ing our methodology. The evaluation experiment
discussed below was specifically designed to as-
sess the usefulness of translation suggestions gen-
erated by our tool ? in cases where translators
have doubts about the usefulness of dictionary so-
lutions. In this paper we do not evaluate other
equally important aspects of the system?s func-
tionality, which will be the matter of future re-
search.
3.1 Set-up of the experiment
For each translation direction we collected ten ex-
amples of possibly recalcitrant translation prob-
lems ? words or phrases whose translation is not
straightforward in a given context. Some of these
examples were sent to us by translators in response
to our request for difficult cases. For each exam-
ple, which we included in the evaluation kit, the
word or phrase either does not have a translation in
ORD (which is a kind of a baseline standard ref-
erence for Russian translators), or its translation
has significantly lower frequency in a target lan-
guage corpus in comparison to the frequency of
the source expression. If an MWE is not listed in
available dictionaries, we produced compositional
(word-for-word) translations using ORD. In order
to remove a possible anti-dictionary bias from our
experiment, we also checked translations in Mul-
titran, an on-line translation dictionary, which was
often quoted as one of the best resources for trans-
lation from and into Russian.
For each translation problem five solutions were
presented to translators for evaluation. One or two
of these solutions were taken from a dictionary
(usually from Multitran, and if available and dif-
ferent, from ORD). The other suggestions were
manually selected from lists of possible solutions
returned by ASSIST. Again, the criteria for se-
lection were intuitive: we included those sugges-
tions which made best sense in the given context.
Dictionary suggestions and the output of ASSIST
were indistinguishable in the questionnaires to the
evaluators. The segments were presented in sen-
tence context and translators had an option of pro-
viding their own solutions and comments. Ta-
ble 2 shows one of the questions sent to evalua-
tors. The problem example is ?????? ?????????
(?precise programme?), which is presented in the
context of a Russian sentence with the following
(non-literal) translation This team should be put
together by responsible politicians, who have a
742
Problem example
?????? ?????????, as in
??????? ??? ??????? ?????? ?????????????
????, ??????? ?????? ????????? ?????? ??
???????.
Translation suggestions Score
clear plan
clear policy
clear programme
clear strategy
concrete plan
Your suggestion ? (optional)
Table 2: Example of an entry in questionnaire
clear strategy for resolving the current crisis. The
third translation equivalent (clear programme) in
the table is found in the Multitran dictionary (ORD
offers no translation for ?????? ?????????). The
example was included because clear programme
is much less frequent in English (2 examples in the
BNC) in comparison to ?????? ????????? in Rus-
sian (70). Other translation equivalents in Table 2
are generated by ASSIST.
We then asked professional translators affiliated
to a translator?s association (identity witheld at this
stage) to rate these five potential equivalents using
a five-point scale:
5 = The suggestion is an appropriate translation
as it is.
4 = The suggestion can be used with some minor
amendment (e.g. by turning a verb into a par-
ticiple).
3 = The suggestion is useful as a hint for an-
other, appropriate translation (e.g. suggestion
elated cannot be used, but its close synonym
exhilarated can).
2 = The suggestion is not useful, even though it is
still in the same domain (e.g. fear is proposed
for a problem referring to hatred).
1 = The suggestion is totally irrelevant.
We received responses from eight translators.
Some translators did not score all solutions, but
there were at least four independent judgements
for each of the 100 translation variants. An exam-
ple of the combined answer sheet for all responses
to the question from Table 2 is given in Table 3 (t1,
Translation t1 t2 t3 t4 t5 ?
clear plan 5 5 3 4 4 0.84
clear policy 5 5 3 4 4 0.84
clear programme 5 5 3 4 4 0.84
clear strategy 5 5 5 5 5 0.00
concrete plan 1 5 3 3 5 1.67
Best Dict 5 5 3 4 4 0.84
Best Syst 5 5 5 5 5 0.00
Table 3: Scores to translation equivalents
t2,. . . denote translators; the dictionary translation
is clear programme).
3.2 Interpretation of the results
The results were surprising in so far as for the ma-
jority of problems translators preferred very differ-
ent translation solutions and did not agree in their
scores for the same solutions. For instance, con-
crete plan in Table 3 received the score 1 from
translator t1 and 5 from t2.
In general, the translators very often picked up
on different opportunities presented by the sug-
gestions from the lists, and most suggestions were
equally legitimate ways of conveying the intended
content, cf. the study of legitimate translation vari-
ation with respect to the BLEU score in (Babych
and Hartley, 2004). In this respect it may be unfair
to compute average scores for each potential solu-
tion, since for most interesting cases the scores do
not fit into the normal distribution model. So aver-
aging scores would mask the potential usability of
really inventive solutions.
In this case it is more reasonable to evaluate
two sets of solutions ? the one generated by AS-
SIST and the other found in dictionaries ? but not
each solution individually. In order to do that for
each translation problem the best scores given by
each translator in each of these two sets were se-
lected. This way of generalising data characterises
the general quality of suggestion sets, and exactly
meets the needs of translators, who collectively get
ideas from the presented sets rather than from in-
dividual examples. This also allows us to mea-
sure inter-evaluator agreement on the dictionary
set and the ASSIST set, for instance, via computing
the standard deviation ? of absolute scores across
evaluators (Table 3). This appeared to be a very
informative measure for dictionary solutions.
In particular, standard deviation scores for the
dictionary set (threshold ? = 0.5) clearly split
743
Agreement: ? for dictionary ? 0.5
Example Dict ASSIST
Ave ? Ave ?
political upheaval 4.83 0.41 4.67 0.82
Disagreement: ? for dictionary >0.5
Example Dict ASSIST
Ave ? Ave ?
clear defiance 4.14 0.90 4.60 0.55
Table 4: Examples for the two groups
Agreement: ? for dictionary ? 0.5
Sub-group Dict ASSIST
Ave ? Ave ?
Agreement E?R 4.73 0.46 4.47 0.80
Agreement R?E 4.90 0.23 4.52 0.60
Agreement?All 4.81 0.34 4.49 0.70
Disagreement: ? for dictionary >0.5
Sub-group Dict ASSIST
Ave ? Ave ?
Disagreement E?R 3.63 1.08 3.98 0.85
Disagreement R?E 3.90 1.02 3.96 0.73
Disagreement?All 3.77 1.05 3.97 0.79
Table 5: Averages for the two groups
our 20 problems into two distinct groups: the first
group below the threshold contains 8 examples,
for which translators typically agree on the qual-
ity of dictionary solutions; and the second group
above the threshold contains 12 examples, for
which there is less agreement. Table 4 shows some
examples from both groups and Table 5 presents
average evaluation scores and standard deviation
figures for both groups.
Overall performance on all 20 examples is the
same for the dictionary responses and for the sys-
tem?s responses: average of the mean top scores
is about 4.2 and average standard deviation of the
scores is 0.8 in both cases (for set-best responses).
This shows that ASSIST can reach the level of
performance of a combination of two authoritative
dictionaries for MWEs, while for its own transla-
tion step it uses just a subset of one-word transla-
tion equivalents from ORD. However, there is an-
other side to the evaluation experiment. In fact, we
are less interested in the system?s performance on
all of these examples than on those examples for
which there is greater disagreement among trans-
lators, i.e. where there is some degree of dissatis-
faction with dictionary suggestions.
012345impin
ge
politic
al uph
eaval
contro
versia
l plan
defus
e tens
ions
?????
?????
??? ?
????
?????
?????
?????
 ????
?????
?????
??
?????
?????????
?????
??
?????
?????
Figure 1: Agreement scores: dictionary
Interestingly, dictionary scores for the agree-
ment group are always higher than 4, which means
that whenever translators agreed on the dictionary
scores they were usually satisfied with the dictio-
nary solution. But they never agreed on the inap-
propriateness of the dictionary: inappropriateness
revealed itself in the form of low scores from some
translators.
This agreement/disagreement threshold can be
said to characterise two types of translation prob-
lems: those for which there exist generally ac-
cepted dictionary solutions, and those for which
translators doubt whether the solution is appropri-
ate. Best-set scores for these two groups of dic-
tionary solutions ? the agreement and disagree-
ment group ? are plotted on the radar charts in
Figures 1 and 2 respectively. The identifiers on
the charts are problematic source language expres-
sions as used in the questionnaire (not translation
solutions to these problems, because a problem
may have several solutions preferred by different
judges). Scores for both translation directions are
presented on the same chart, since both follow the
same pattern and receive the same interpretation.
Figure 1 shows that whenever there is little
doubt about the quality of dictionary solutions, the
radar chart approaches a circle shape near the edge
of the chart. In Figure 2 the picture is different:
the circle is disturbed, and some scores frequently
approach the centre. Therefore the disagreement
group contains those translation problems where
dictionaries provide little help.
The central problem in our evaluation experi-
ment is whether ASSIST is helpful for problems
in the second group, where translators doubt the
quality of dictionary solutions.
Firstly, it can be seen from the charts that judge-
744
012345
?????
?????
?????
?????
?????
???
?????
??? ????
?? ??
?????
??
?????
?????
?
?????
?????
????
?????
?????
??? ??
?????
?
due p
roces
s
negot
iated 
settle
ment
clear 
defian
ce
daunt
ing ex
perien
ce
passio
nately
 seekrecrea
tional
 fear
Figure 2: Disagreement scores: dictionary
012345
?????
?????
?????
?????
?????
???
?????
??? ????
?? ??
?????
??
?????
?????
?
?????
?????
????
?????
?????
??? ??
?????
?
due p
roces
s
negot
iated 
settle
ment
clear 
defian
ce
daunt
ing ex
perien
ce
passio
nately
 seekrecrea
tional
 fear
Figure 3: Disagreement scores: ASSIST
ments on the quality of the system output are more
consistent: score lines for system output are closer
to the circle shape in Figure 1 than those for dic-
tionary solutions in Figure 2 (formally: the stan-
dard deviation of evaluation scores, presented in
Table 4, is lower).
Secondly, as shown in Table 4, in this group av-
erage evaluation scores are slightly higher for AS-
SIST output than for dictionary solutions (3.97 vs
3.77) ? in the eyes of human evaluators ASSIST
outperforms good dictionaries. For good dictio-
nary solutions ASSIST performance is slightly
lower: (4.49 vs 4.81), but the standard deviation
is about the same.
Having said this, solutions from our system are
really not in competition with dictionary solutions:
they provide less literal translations, which often
emerge in later stages of the translation task, when
translators correct and improve an initial draft,
where they have usually put more literal equiva-
lents (Shveitser, 1988). It is a known fact in trans-
lation studies that non-literal solutions are harder
to see and translators often find them only upon
longer reflection. Yet another fact is that non-
literal translations often require re-writing other
segments of the sentence, which may not be ob-
vious at first glance.
4 Conclusions and future work
The results of evaluation show that the tool is
successful in finding translation equivalents for a
range of examples. What is more, in cases where
the problem is genuinely difficult, ASSIST consis-
tently provides scores around 4 ? ?minor adapta-
tions needed?. The precision of the tool is low, it
suggests 50-100 examples with only 2-4 useful for
the current context. However, recall of the output
is more relevant than precision, because transla-
tors typically need just one solution for their prob-
lem, and often have to look through reasonably
large lists of dictionary translations and examples
to find something suitable for a problematic ex-
pression. Even if no immediately suitable trans-
lation can be found in the list of suggestions, it
frequently contains a hint for solving the problem
in the absence of adequate dictionary information.
The current implementation of the model is re-
stricted in several respects. First, the majority of
target language constructions mirror the syntactic
structure of the source language example. Even
if the procedure for producing similarity classes
does not impose restrictions on POS properties,
nevertheless words in the similarity class tend to
follow the POS of the original word, because of
the similarity of their contexts of use. Further-
more, dictionaries also tend to translate words
using the same POS. This means that the ex-
isting method finds mostly NPs for NPs, verb-
object pairs for verb-object pairs, etc, even if the
most natural translation uses a different syntactic
structure, e.g. I like doing X instead of I do X
gladly (when translating from German ich mache
X gerne).
Second, suggestions are generated for the query
expression independently from the context it is
used in. For instance, the words judicial, military
and religious are in the similarity class of politi-
cal, just as reform is in the simclass of upheaval.
So the following example
The plan will protect EC-based investors in Russia
from political upheavals damaging their business.
creates a list of ?possible translations? evoking
various reforms and transformations.
745
These issues can be addressed by introduc-
ing a model of the semantic context of situation,
e.g. ?changes in business practice? as in the ex-
ample above, or ?unpleasant situation? as in the
case of daunting experience. This will allow
less restrictive identification of possible transla-
tion equivalents, as well as reduction of sugges-
tions irrelevant for the context of the current ex-
ample.
Currently we are working on an option to iden-
tify semantic contexts by means of ?semantic sig-
natures? obtained from a broad-coverage seman-
tic parser, such as USAS (Rayson et al, 2004).
The semantic tagset used by USAS is a language-
independent multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories
(such as I1.1- = Money: lack; A5.1- = Eval-
uation: bad), which can be used to detect the
semantic context. Identification of semantically
similar situations can be also improved by the
use of segment-matching algorithms as employed
in Example-Based MT (EBMT) and translation
memories (Planas and Furuse, 2000; Carl and
Way, 2003).
The proposed model looks similar to some im-
plementations of statistical machine translation
(SMT), which typically uses a parallel corpus for
its translation model, and then finds the best possi-
ble recombination that fits into the target language
model (Och and Ney, 2003). Just like an MT sys-
tem, our tool can find translation equivalents for
queries which are not explicitly coded as entries
in system dictionaries. However, from the user
perspective it resembles a dynamic dictionary or
thesaurus: it translates difficult words and phrases,
not entire sentences. The main thrust of our sys-
tem is its ability to find translation equivalents for
difficult contexts where dictionary solutions do not
exist, are questionable or inappropriate.
Acknowledgements
This research is supported by EPSRC grant
EP/C005902.
References
Bogdan Babych and Anthony Hartley. 2004. Ex-
tending the BLEU MT evaluation method with fre-
quency weightings. In Proceedings of the 42d An-
nual Meeting of the Association for Computational
Linguistics, Barcelona.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Gregory Grefenstette. 2002. Multilingual corpus-
based extraction and the very large lexicon. In Lars
Borin, editor, Language and Computers, Parallel
corpora, parallel worlds, pages 137?149. Rodopi.
John S. Justeson and Slava M. Katz. 1995. Techninal
terminology: some linguistic properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1(1):9?27.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Joint COLING-ACL-98, pages
768?774, Montreal.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Alan Partington. 1998. Patterns and meanings: using
corpora for English language research and teach-
ing. John Benjamins, Amsterdam.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of the Forth Language Resources and Evalua-
tion Conference, LREC 2004, pages 395?398, Lis-
bon.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proc. Beyond Named Entity Recognition
Workshop in association with LREC 2004, pages 7?
12, Lisbon.
Serge Sharoff. 2006. Creating general-purpose
corpora using automated search engine queries.
In Marco Baroni and Silvia Bernardini, editors,
WaCky! Working papers on the Web as Corpus.
Gedit, Bologna.
A.D. Shveitser. 1988. ?????? ????????: ??????, ???-
?????, ???????. Nauka, Moskow. (In Russian:
Theory of Translation: Status, Problems, Aspects).
Krista Varantola. 2003. Translators and disposable
corpora. In Federico Zanettin, Silvia Bernardini,
and Dominic Stewart, editors, Corpora in Transla-
tor Education, pages 55?70. St Jerome, Manchester.
746
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 136?143,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Assisting Translators in Indirect Lexical Transfer 
Bogdan Babych, Anthony Hartley, Serge Sharoff
  Centre for Translation Studies 
  University of Leeds, UK 
{b.babych,a.hartley,s.sharoff}@leeds.ac.uk
Olga Mudraya 
  Department of Linguistics 
  Lancaster University, UK 
 o.mudraya@lancs.ac.uk 
Abstract 
We present the design and evaluation of a 
translator?s amenuensis that uses compa-
rable corpora to propose and rank non-
literal solutions to the translation of expres-
sions from the general lexicon. Using dis-
tributional similarity and bilingual diction-
aries, the method outperforms established 
techniques for extracting translation 
equivalents from parallel corpora. The in-
terface to the system is available at: 
http://corpus.leeds.ac.uk/assist/v05/  
1 Introduction 
This paper describes a system designed to assist 
humans in translating expressions that do not nec-
essarily have a literal or compositional equivalent 
in the target language (TL). In the spirit of (Kay, 
1997), it is intended as a translator's amenuensis 
"under the tight control of a human translator ? to 
help increase his productivity and not to supplant him". 
One area where human translators particularly 
appreciate assistance is in the translation of expres-
sions from the general lexicon. Unlike equivalent 
technical terms, which generally share the same 
part-of-speech (POS) across languages and are in 
the ideal case univocal, the contextually appropri-
ate equivalents of general language expressions are 
often indirect and open to variation. While the 
transfer module in RBMT may acceptably under-
generate through a many-to-one mapping between 
source and target expressions, human translators, 
even in non-literary fields, value legitimate varia-
tion. Thus the French expression il faillit ?chouer 
(lit.: he faltered to fail) may be variously rendered 
as he almost/nearly/all but failed; he was on the 
verge/brink of failing/failure; failure loomed. All 
of these translations are indirect in that they in-
volve lexical shifts or POS transformations. 
Finding such translations is a hard task that can 
benefit from automated assistance. 'Mining' such 
indirect equivalents is difficult, precisely because 
of the structural mismatch, but also because of the 
paucity of suitable aligned corpora. The approach 
adopted here includes the use of comparable cor-
pora in source and target languages, which are 
relatively easy to create. The challenge is to gener-
ate a list of usable solutions and to rank them such 
that the best are at the top. 
Thus the present system is unlike SMT (Och and 
Ney, 2003), where lexical selection is effected by a 
translation model based on aligned, parallel cor-
pora, but the novel techniques it has developed are 
exploitable in the SMT paradigm. It also differs 
from now traditional uses of comparable corpora 
for detecting translation equivalents (Rapp, 1999) 
or extracting terminology (Grefenstette, 2002), 
which allows a one-to-one correspondence irre-
spective of the context. Our system addresses diffi-
culties in expressions in the general lexicon, whose 
translation is context-dependent. 
The structure of the paper is as follows. In Sec-
tion 2 we present the method we use for mining 
translation equivalents. In Section 3 we present the 
results of an objective evaluation of the quality of 
suggestions produced by the system by comparing 
our output against a parallel corpus. Finally, in 
Section 4 we present a subjective evaluation focus-
ing on the integration of the system into the work-
flow of human translators. 
2 Methodology 
The software acts as a decision support system for 
translators. It integrates different technologies for 
136
extracting indirect translation equivalents from 
large comparable corpora. In the following subsec-
tions we give the user perspective on the system 
and describe the methodology underlying each of 
its sub-tasks. 
2.1 User perspective 
Unlike traditional dictionaries, the system is a 
dynamic translation resource in that it can success-
fully find translation equivalents for units which 
have not been stored in advance, even for idiosyn-
cratic multiword expressions which almost cer-
tainly will not figure in a dictionary. While our 
system can rectify gaps and omissions in static 
lexicographical resources, its major advantage is 
that it is able to cope with an open set of transla-
tion problems, searching for translation equivalents 
in comparable corpora in runtime. This makes it 
more than just an extended dictionary. 
Contextual descriptors 
From the user perspective the system extracts indi-
rect translation equivalents as sets of contextual 
descriptors ? content words that are lexically cen-
tral in a given sentence, phrase or construction. 
The choice of these descriptors may determine the 
general syntactic perspective of the sentence and 
the use of supporting lexical items. Many transla-
tion problems arise from the fact that the mapping 
between such descriptors is not straightforward. 
The system is designed to find possible indirect 
mappings between sets of descriptors and to verify 
the acceptability of the mapping into the TL. For 
example, in the following Russian sentence, the 
bolded contextual descriptors require indirect 
translation into English. 
???? ???????? ????? ???????????-
?????? ?????, ? ??????? ????????? 
?????? ???????????? 
(Children attend badly repaired schools, in 
which [it] is missing the most necessary) 
Combining direct translation equivalents of 
these words (e.g., translations found in the Oxford 
Russian Dictionary ? ORD) may produce a non-
natural English sentence, like the literal translation 
given above. In such cases human translators usu-
ally apply structural and lexical transformations, 
for instance changing the descriptors? POS and/or 
replacing them with near-synonyms which fit to-
gether in the context of a TL sentence (Munday, 
2001: 57-58). Thus, a structural transformation of 
????? ????????????????? (badly repaired) may 
give in poor repair while a lexical transformation 
of ????????? ?????? ???????????? ([it] is missing 
the most necessary) gives lacking basic essentials. 
Our system models such transformations of the 
descriptors and checks the consistency of the re-
sulting sets in the TL. 
Using the system 
Human translators submit queries in the form of 
one or more SL descriptors which in their opinion 
may require indirect translation. When the transla-
tors use the system for translating into their native 
language, the returned descriptors are usually suf-
ficient for them to produce a correct TL construc-
tion or phrase around them (even though the de-
scriptors do not always form a naturally sounding 
expression). When the translators work into a non-
native language, they often find it useful to gener-
ate concordances for the returned descriptors to 
verify their usage within TL constructions. 
For example, for the sentence above translators 
may submit two queries: ????? ????????-
????????? (badly repaired) and ????????? 
???????????? (missing necessary). For the first 
query the system returns a list of descriptor pairs 
(with information on their frequency in the English 
corpus) ranked by distributional proximity to the 
original query, which we explain in Section 2.2. At 
the top of the list come: 
bad repair = 30  (11.005) 
bad maintenance = 16  (5.301) 
bad restoration = 2  (5.079) 
poor repair = 60  (5.026)? 
Underlined hyperlinks lead translators to actual 
contexts in the English corpus, e.g., poor repair 
generates a concordance containing a desirable TL 
construction which is a structural transformation of 
the SL query: 
in such a poor state of repair 
bridge in as poor a state of repair as the highways 
building in poor repair. 
dwellings are in poor repair; 
Similarly, the result of the second query may 
give the translators an idea about possible lexical 
transformation: 
missing need = 14  (5.035) 
important missing = 8 (2.930) 
missing vital = 8  (2.322) 
lack necessary = 204  (1.982)? 
essential lack = 86  (0.908)? 
137
The concordance for the last pair of descriptors 
contains the phrase they lack the three essentials, 
which illustrates the transformation. The resulting 
translation may be the following: 
Children attend schools that are in poor re-
pair and lacking basic essentials 
Thus our system supports translators in making 
decisions about indirect translation equivalents in a 
number of ways: it suggests possible structural and 
lexical transformations for contextual descriptors; 
it verifies which translation variants co-occur in 
the TL corpus; and it illustrates the use of the 
transformed TL lexical descriptors in actual con-
texts. 
2.2 Generating translation equivalents 
We have generalised the method used in our previ-
ous study (Sharoff et al, 2006) for extracting 
equivalents for continuous multiword expressions 
(MWEs). Essentially, the method expands the 
search space for each word and its dictionary trans-
lations with entries from automatically computed 
thesauri, and then checks which combinations are 
possible in target corpora. These potential transla-
tion equivalents are then ranked by their similarity 
to the original query and presented to the user. The 
range of retrievable equivalents is now extended 
from a relatively limited range of two-word con-
structions which mirror POS categories in SL and 
TL to a much wider set of co-occurring lexical 
content items, which may appear in a different or-
der, at some distance from each other, and belong 
to different POS categories.  
The method works best for expressions from the 
general lexicon, which do not have established 
equivalents, but not yet for terminology. It relies 
on a high-quality bilingual dictionary (en-ru ~30k, 
ru-en ~50K words, combining ORD and the core 
part of Multitran) and large comparable corpora 
(~200M En, ~70M Ru) of news texts. 
For each of the SL query terms q the system 
generates its dictionary translation Tr(q) and its 
similarity class S(q) ? a set of words with a similar 
distribution in a monolingual corpus. Similarity is 
measured as the cosine between collocation vec-
tors, whose dimensionality is reduced by SVD us-
ing the implementation by Rapp (2004). The de-
scriptor and each word in the similarity class are 
then translated into the TL using ORD or the Mul-
titran dictionary, resulting in {Tr(q)? Tr(S(q))}. 
On the TL side we also generate similarity classes, 
but only for dictionary translations of query terms 
Tr(q) (not for Tr(S(q)), which can make output too 
noisy). We refer to the resulting set of TL words as 
a translation class T.  
T = {Tr(q) ? Tr(S(q)) ? S(Tr(q))} 
Translation classes approximate lexical and 
structural transformations which can potentially be 
applied to each of the query terms. Automatically 
computed similarity classes do not require re-
sources like WordNet, and they are much more 
suitable for modelling translation transformations, 
since they often contain a wider range of words of 
different POS which share the same context, e.g., 
the similarity class of the word lack contains words 
such as absence, insufficient, inadequate, lost, 
shortage, failure, paucity, poor, weakness, inabil-
ity, need. This clearly goes beyond the range of 
traditional thesauri. 
For multiword queries, the system performs a 
consistency check on possible combinations of 
words from different translation classes. In particu-
lar, it computes the Cartesian product for pairs of 
translation classes T1 and T2 to generate the set P 
of word pairs, where each word (w1 and w2) comes 
from a different translation class: 
P = T1 ? T2 = {(w1, w2) | w1 ? T1 and w2 ? T2}  
Then the system checks whether each word pair 
from the set P exists in the database D of discon-
tinuous content word bi-grams which actually co-
occur in the TL corpus: 
P? = P ? D 
The database contains the set of all bi-grams that 
occur in the corpus with a frequency ? 4 within a 
window of 5 words (over 9M bigrams for each 
language). The bi-grams in D and in P are sorted 
alphabetically, so their order in the query is not 
important. 
Larger N-grams (N > 2) in queries are split into 
combinations of bi-grams, which we found to be 
an optimal solution to the problem of the scarcity 
of higher order N-grams in the corpus. Thus, for 
the query gain significant importance the system 
generates P?1(significant importance), P?2(gain impor-
tance), P?3(gain significant) and computes P? as:  
P? = {(w1,w2,w3)| (w1,w2) ? P?1 & (w1, w3) ? P?2 
& (w2,w3) ? P?3 }, 
which allows the system to find an indirect equiva-
lent ???????? ??????? ???????? (lit.: receive 
weighty meaning). 
138
Even though P? on average contains about 2% - 
4% of the theoretically possible number of bi-
grams present in P, the returned number of poten-
tial translation equivalents may still be large and 
contain much noise. Typically there are several 
hundred elements in P?, of which only a few are 
really useful for translation. To make the system 
usable in practice, i.e., to get useful solutions to 
appear close to the top (preferably on the first 
screen of the output), we developed methods of 
ranking and filtering the returned TL contextual 
descriptor pairs, which we present in the following 
sections. 
2.3 Hypothesis ranking 
The system ranks the returned list of contextual 
descriptors by their distributional proximity to the 
original query, i.e. it uses scores cos(vq, vw) gener-
ated for words in similarity classes ? the cosine of 
the angle between the collocation vector for a word 
and the collocation vector for the query or diction-
ary translation of the query. Thus, words whose 
equivalents show similar usage in a comparable 
corpus receive the highest scores. These scores are 
computed for each individual word in the output, 
so there are several ways to combine them to 
weight words in translation classes and word com-
binations in the returned list of descriptors.  
We established experimentally that the best way 
to combine similarity scores is to multiply weights 
W(T) computed for each word within its translation 
class T. The weight W(P?(w1,w2)) for each pair of 
contextual descriptors (w1, w2)?P? is computed as: 
W(P?(w1,w2)) = W(T(w1)) ? W(T(w2)); 
Computing W(T(w)), however, is not straightfor-
ward either, since some words in similarity classes 
of different translation equivalents for the query 
term may be the same, or different words from the 
similarity class of the original query may have the 
same translation. Therefore, a word w within a 
translation class may have come by several routes 
simultaneously, and may have done that several 
times. For each word w in T there is a possibility 
that it arrived in T either because it is in Tr(q) or 
occurs   n times in Tr(S(q)) or k times in S(Tr(q)). 
We found that the number of occurrences n and 
k of each word w in each subset gives valuable in-
formation for ranking translation candidates. In our 
experiments we computed the weight W(T) as the 
sum of similarity scores which w receives in each 
of the subsets. We also discovered that ranking 
improves if for each query term we compute in 
addition a larger (and potentially noisy) space of 
candidates that includes TL similarity classes of 
translations of the SL similarity class S(Tr(S(q))). 
These candidates do not appear in the system out-
put, but they play an important role in ranking the 
displayed candidates. The improvement may be 
due to the fact that this space is much larger, and 
may better support relevant candidates since there 
is a greater chance that appropriate indirect equiva-
lents are found several times within SL and TL 
similarity classes. The best ranking results were 
achieved when the original W(T) scores were mul-
tiplied by 2 and added to the scores for the newly 
introduced similarity space S(Tr(S(q))): 
W(T(w))= 2?(1 if w?Tr(q) )+  
2??( cos(vq, vTr(w)) | {w | w? Tr(S(q)) } ) +  
2??( cos(vTr(q), vw) | {w | w? S(Tr(q)) } ) + 
?(cos(vq, vTr(w))?cos (vTr(q), vw) |  
{w | w? S(Tr(S(q))) } ) 
For example, the system gives the following 
ranking for the indirect translation equivalents of 
the Russian phrase ??????? ???????? (lit.: weighty 
meaning) ? figures in brackets represent W(P?) 
scores for each pair of TL descriptors: 
1. significant importance = 7 (3.610)  
2. significant value = 128    (3.211)  
3. measurable value = 6       (2.657)?  
8. dramatic importance = 2    (2.028)  
9. important significant = 70 (2.014)  
10. convincing importance = 6 (1.843) 
The Russian similarity class for ??????? 
(weighty, ponderous) contains: ???????????? 
(convincing) (0.469), ???????? (significant) 
(0.461), ???????? (notable) (0.452) ?????-
?????? (dramatic) (0.371). The equivalent of 
significant is not at the top of the similarity class of 
the Russian query, but it appears at the top of the 
final ranking of pairs in P?, because this hypothesis 
is supported by elements of the set formed by 
S(Tr(S(q))); it appears in similarity classes for no-
table (0.353) and dramatic (0.315), which contrib-
uted these values to the W(T) score of significant: 
W(T(significant)) = 
    2 ? (Tr(????????)=significant (0.461))  
+ (Tr(????????)=notable (0.452)  
  ? S(notable)=significant (0.353)) 
+ (Tr(???????????)=dramatic (0.371)  
  ? S(dramatic)= significant (0.315)) 
The word dramatic itself is not usable as a 
translation equivalent in this case, but its similarity 
139
class contains the support for relevant candidates, 
so it can be viewed as useful noise. On the other 
hand, the word convincing does not receive such 
support from the hypothesis space, even though its 
Russian equivalent is ranked higher in the SL simi-
larity class. 
2.4 Semantic filtering 
Ranking of translation candidates can be further 
improved when translators use an option to filter 
the returned list by certain lexical criteria, e.g., to 
display only those examples that contain a certain 
lexical item, or to require one of the items to be a 
dictionary translation of the query term. However, 
lexical filtering is often too restrictive: in many 
cases translators need to see a number of related 
words from the same semantic field or subject do-
main, without knowing the lexical items in ad-
vance. In this section we present the semantic fil-
ter, which is based on Russian and English seman-
tic taggers which use the same semantic field tax-
onomy for both languages. 
The semantic filter displays only those items 
which have specified semantic field tags or tag 
combinations; it can be applied to one or both 
words in each translation hypothesis in P?. The 
default setting for the semantic filter is the re-
quirement for both words in the resulting TL can-
didates to contain any of the semantic field tags 
from a SL query term. 
In the next section we present evaluation results 
for this default setting (which is applied when the 
user clicks the Semantic Filter button), but human 
translators have further options ? to filter by tags 
of individual words, to use semantic classes from 
SL or TL terms, etc. 
For example, applying the default semantic filter 
for the output of the query ????? ???????-
?????????? (badly repaired) removes the high-
lighted items from the list: 
 1. bad repair = 30       (11.005)  
[2. good repair = 154     (8.884) ] 
 3. bad rebuild = 6       (5.920)  
[4. bad maintenance = 16  (5.301) ] 
 5. bad restoration = 2   (5.079)  
 6. poor repair = 60      (5.026)  
[7. good rebuild = 38     (4.779) ] 
 8. bad construction = 14 (4.779)  
Items 2 and 7 are generated by the system be-
cause good, well and bad are in the same similar-
ity cluster for many words (they often share the 
same collocations). The semantic filter removes 
examples with good and well on the grounds that 
they do not have any of the tags which come from 
the word ????? (badly): in particular, instead of 
tag A5? (Evaluation: Negative) they have tag A5+ 
(Evaluation: Positive). Item 4 is removed on the 
grounds that the words ????????????????? 
(repaired) and maintenance do not have any tags 
in common ? they appear ontologically too far 
apart from the point of view of the semantic tagger. 
The core of the system?s multilingual semantic 
tagging is a knowledge base in which single words 
and MWEs are mapped to their potential semantic 
field categories. Often a lexical item is mapped to 
multiple semantic categories, reflecting its poten-
tial multiple senses. In such cases, the tags are ar-
ranged by the order of likelihood of meanings, 
with the most prominent first. 
3 Objective evaluation 
In the objective evaluation we tested the perform-
ance of our system on a selection of indirect trans-
lation problems, extracted from a parallel corpus 
consisting mostly of articles from English and 
Russian newspapers (118,497 words in the R-E 
direction, 589,055 words in the E-R direction). It 
has been aligned on the sentence level by JAPA 
(Langlais et al, 1998), and further on the word 
level by GIZA++ (Och and Ney, 2003). 
3.1 Comparative performance 
The intuition behind the objective evaluation 
experiment is that the capacity of our tool to find 
indirect translation equivalents in comparable cor-
pora can be compared with the results of automatic 
alignment of parallel texts used in translation mod-
els in SMT: one of the major advantages of the 
SMT paradigm is its ability to reuse indirect 
equivalents found in parallel corpora (equivalents 
that may never come up in hand-crafted dictionar-
ies). Thus, automatically generated GIZA++ dic-
tionaries with word alignment contain many exam-
ples of indirect translation equivalents. 
We use these dictionaries to simulate the genera-
tor of translation classes T, which we recombine to 
construct their Cartesian product P, similarly to the 
procedure we use to generate the output of our sys-
tem. However, the two approaches generate indi-
rect translation equivalence hypotheses on the ba-
sis of radically different material: the GIZA dic-
tionary uses evidence from parallel corpora of ex-
140
isting human translations, while our system re-
combines translation candidates on the basis of 
their distributional similarity in monolingual com-
parable corpora. Therefore we took GIZA as a 
baseline. 
Translation problems for the objective evalua-
tion experiment were manually extracted from two 
parallel corpora: a section of about 10,000 words 
of a corpus of English and Russian newspapers, 
which we also used to train GIZA, and a section of 
the same length from a corpus of interviews pub-
lished on the Euronews.net website. 
We selected expressions which represented 
cases of lexical transformations (as illustrated in 
Section 0), containing at least two content words 
both in the SL and TL. These expressions were 
converted into pairs of contextual descriptors ? 
e.g., recent success, reflect success ? and submit-
ted to the system and to the GIZA dictionary. We 
compared the ability of our system and of GIZA to 
find indirect translation equivalents which matched 
the equivalents used by human translators. The 
output from both systems was checked to see 
whether it contained the contextual descriptors 
used by human translators. We submitted 388 pairs 
of descriptors extracted from the newspaper trans-
lation corpus and 174 pairs extracted from the Eu-
ronews interview corpus. Half of these pairs were 
Russian, and the other half English. 
We computed recall figures for 2-word combi-
nations of contextual descriptors and single de-
scriptors within those combinations. We also show 
the recall of translation variants provided by the 
ORD on this data set. For example, for the query 
????????? ???????????? ([it] is missing neces-
sary [things]) human translators give the solution 
lacking essentials; the lemmatised descriptors are 
lack and essential. ORD returns direct translation 
equivalents missing and necessary. The GIZA dic-
tionary in addition contains several translation 
equivalents for the second term (with alignment 
probabilities) including: necessary ~0.332, need 
~0.226, essential ~0.023. Our system returns both 
descriptors used in human translation as a pair ? 
lack essential (ranked 41 without filtering and 22 
with the default semantic filter). Thus, for a 2-word 
combination of the descriptors only the output of 
our system matched the human solution, which we 
counted as one hit for the system and no hits for 
ORD or GIZA. For 1-word descriptors we counted 
2 hits for our system (both words in the human 
solution are matched), and 1 hit for GIZA ? it 
matches the word essential ~0.023 (which also il-
lustrates its ability to find indirect translation 
equivalents). 
 2w descriptors 1w descriptors 
 news interv news interv 
ORD 6.7% 4.6% 32.9% 29.3% 
GIZA++ 13.9% 3.4% 35.6% 29.0%
Our system 21.9% 19.5% 55.8% 49.4%
Table 1 Conservative estimate of recall 
It can be seen from Table 1 that for the newspa-
per corpus on which it was trained, GIZA covers a 
wider set of indirect translation variants than ORD. 
But our recall is even better both for 2-word and 1-
word descriptors. 
However, note that GIZA?s ability to retrieve 
from the newspaper corpus certain indirect transla-
tion equivalents may be due to the fact that it has 
previously seen them frequently enough to gener-
ate a correct alignment and the corresponding dic-
tionary entry. 
The Euronews interview corpus was not used for 
training GIZA. It represents spoken language and 
is expected to contain more ?radical? transforma-
tions. The small decline in ORD figures here can 
be attributed to the fact that there is a difference in 
genre between written and spoken texts and conse-
quently between transformation types in them. 
However, the performance of GIZA drops radi-
cally on unseen text and becomes approximately 
the same as the ORD. 
This shows that indirect translation equivalents 
in the parallel corpus used for training GIZA are 
too sparse to be learnt one by one and successfully 
applied to unseen data, since solutions which fit 
one context do not necessarily suit others. 
The performance of our system stays at about 
the same level for this new type of text; the decline 
in its performance is comparable to the decline in 
ORD figures, and can again be explained by the 
differences in genre. 
3.2 Evaluation of hypothesis ranking 
As we mentioned, correct ranking of translation 
candidates improves the usability of the system. 
Again, the objective evaluation experiment gives 
only a conservative estimate of ranking, because 
there may be many more useful indirect solutions 
further up the list in the output of the system which 
are legitimate variants of the solutions found in the 
141
parallel corpus. Therefore, evaluation figures 
should be interpreted in a comparative rather then 
an absolute sense. 
We use ranking by frequency as a baseline for 
comparing the ranking described in Section 2.3 ? 
by distributional similarity between a candidate 
and the original query. 
Table 2 shows the average rank of human solu-
tions found in parallel corpora and the recall of 
these solutions for the top 300 examples. Since 
there are no substantial differences between the 
figures for the newspaper texts and for the inter-
views, we report the results jointly for 556 transla-
tion problems in both selections (lower rank fig-
ures are better). 
 Recall Average rank 
2-word descriptors 
frequency (baseline) 16.7% rank=93.7
distributional similarity 19.5% rank=44.4
sim. + semantic filter 14.4% rank=26.7
1-word descriptors 
frequency (baseline) 48.2% rank=42.7
distributional similarity 52.8% rank=21.6
sim. + semantic filter 44.1% rank=11.3
Table 2 Ranking: frequency, similarity and filter 
It can be seen from the table that ranking by 
similarity yields almost a twofold improvement for 
the average rank figures compared to the baseline. 
There is also a small improvement in recall, since 
there are more relevant examples that appear 
within the top 300 entries. 
The semantic filter once again gives an almost 
twofold improvement in ranking, since it removes 
many noisy items. The average is now within the 
top 30 items, which means that there is a high 
chance that a translation solution will be displayed 
on the first screen. The price for improved ranking 
is decline in recall, since it may remove some rele-
vant lexical transformations if they appear to be 
ontologically too far apart. But the decline is 
smaller: about 26.2% for 2-word descriptors and 
16.5% for 1-word descriptors. The semantic filter 
is an optional tool, which can be used to great ef-
fect on noisy output: its improvement of ranking 
outweighs the decline in recall. 
Note that the distribution of ranks is not normal, 
so in Figure 1 we present frequency polygons for 
rank groups of 30 (which is the number of items 
that fit on a single screen, i.e., the number of items 
in the first group (r030) shows solutions that will 
be displayed on the first screen). The majority of 
solutions ranked by similarity appear high in the 
list (in fact, on the first two or three screens). 
0
10
20
30
40
50
60
70
r0
30
r0
60
r0
90
r1
20
r1
50
r1
80
r2
10
r2
40
r2
70
r3
00
similarity
frequency
 
Figure 1 Frequency polygons for ranks 
4 Subjective evaluation 
The objective evaluation reported above uses a 
single reference translation and is correspondingly 
conservative in estimating the coverage of the sys-
tem. However, many expressions studied have 
more than one fluent translation. For instance, in 
poor repair is not the only equivalent for the Rus-
sian expression ????? ?????????????????. It is 
also possible to translate it as unsatisfactory condi-
tion, bad state of repair, badly in need of repair, 
and so on. The objective evaluation shows that the 
system has been able to find the suggestion used 
by a particular translator for the problem studied. It 
does not tell us whether the system has found some 
other translations suitable for the context. Such 
legitimate translation variation implies that the per-
formance of a system should be studied on the ba-
sis of multiple reference translations, though typi-
cally just two reference translations are used (Pap-
ineni, et al 2001). This might be enough for the 
purposes of a fully automatic MT tool, but in the 
context of a translator's amanuensis which deals 
with expressions difficult for human translators, it 
is reasonable to work with a larger range of ac-
ceptable target expressions. 
With this in mind we evaluated the performance 
of the tool with a panel of 12 professional transla-
tors. Problematic expressions were highlighted and 
the translators were asked to find suitable sugges-
tions produced by the tool for these expressions 
and rank their usability on a scale from 1 to 5 (not 
acceptable to fully idiomatic, so 1 means that no 
usable translation was found at all). 
Sentences themselves were selected from prob-
lems discussed on professional translation forums 
proz.com and forum.lingvo.ru. Given the range of 
corpora used in the system (reference and newspa-
142
per corpora), the examples were filtered to address 
expressions used in newspapers. 
The goal of the subjective evaluation experiment 
was to establish the usefulness of the system for 
translators beyond the conservative estimate given 
by the objective evaluation. The intuition behind 
the experiment is that if there are several admissi-
ble translations for the SL contextual descriptors, 
and system output matches any of these solutions, 
then the system has generated something useful. 
Therefore, we computed recall on sets of human 
solutions rather than on individual solutions. We 
matched 210 different human solutions to 36 trans-
lation problems. To compute more realistic recall 
figures, we counted cases when the system output 
matches any of the human solutions in the set. 
Table 3 compares the conservative estimate of the 
objective evaluation and the more realistic estimate 
on a single data set. 
 2w default 2w with sem filt 
Conservative  32.4%; r=53.68 21.9%; r=34.67 
Realistic 75.0%;   r=7.48 61.1%;   r=3.95 
Table 3 Recall and rank for 2-word descriptors 
Since the data set is different, the figures for the 
conservative estimate are higher than those for the 
objective evaluation data set. However, the table 
shows the there is a gap between the conservative 
estimate and the realistic coverage of the transla-
tion problems by the system, and that real coverage 
of indirect translation equivalents is potentially 
much higher. 
Table 4 shows averages (and standard deviation 
?) of the usability scores divided in four groups: (1) 
solutions that are found both by our system and the 
ORD; (2) solutions found only by our system; (3) 
solutions found only by ORD (4) solutions found 
by neither: 
 system (+) system (?) 
ORD (+) 4.03 (0.42) 3.62 (0.89)
ORD (?) 4.25 (0.79) 3.15 (1.15)
Table 4 Human scores and ? for system output 
It can be seen from the table that human users find 
the system most useful for those problems where 
the solution does not match any of the direct dic-
tionary equivalents, but is generated by the system. 
5 Conclusions 
We have presented a method of finding indirect 
translation equivalents in comparable corpora, and 
integrated it into a system which assists translators 
in indirect lexical transfer. The method outper-
forms established methods of extracting indirect 
translation equivalents from parallel corpora. 
We can interpret these results as an indication 
that our method, rather than learning individual 
indirect transformations, models the entire family 
of transformations entailed by indirect lexical 
transfer. In other words it learns a translation strat-
egy which is based on the distributional similarity 
of words in a monolingual corpus, and applies this 
strategy to novel, previously unseen examples. 
The coverage of the tool and additional filtering 
techniques make it useful for professional transla-
tors in automating the search for non-trivial, indi-
rect translation equivalents, especially equivalents 
for multiword expressions. 
References 
Gregory Grefenstette. 2002. Multilingual corpus-based 
extraction and the very large lexicon. In: Lars Borin, 
editor, Language and Computers, Parallel corpora, 
parallel worlds, pages 137-149. Rodopi. 
Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Translation, 
12(1-2):3-23. 
Philippe Langlais, Michel Simard, and Jean V?ronis. 
1998. Methods and practical issues in evaluating 
alignment techniques. In Proc. Joint COLING-ACL-
98, pages 711-717. 
Jeremy Munday. 2001. Introducing translation studies. 
Theories and Applications. Routledge, New York. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51. 
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2001). Bleu: a method for automatic evaluation of 
machine translation, RC22176 W0109-022: IBM. 
Reinhard Rapp. 1999. Automatic identification of word 
translations from unrelated English and German cor-
pora. In Procs. the 37th ACL, pages 395-398. 
Reinhard Rapp. 2004. A freely available automatically 
generated thesaurus of related words. In Procs. LREC 
2004, pages 395-398, Lisbon. 
Serge Sharoff, Bogdan Babych and Anthony Hartley 
2006. Using Comparable Corpora to Solve Problems 
Difficult for Human Translators. In: Proceedings of 
the COLING/ACL 2006 Main Conference Poster 
Sessions, pp. 739-746. 
143
What is at stake:
a case study of Russian expressions starting with a preposition
Serge Sharoff
Centre for Translation Studies
School of Modern Languages and Cultures
University of Leeds, Leeds, LS2 9JT, UK
s.sharoff@leeds.ac.uk
Abstract
The paper describes an experiment in
detecting a specific type of multiword
expressions in Russian, namely expres-
sions starting with a preposition. This
covers not only prepositional phrases
proper, but also fixed syntactic construc-
tions like v techenie (?in the course of?).
First, we collect lists of such construc-
tions in a corpus of 50 mln words using
a simple mechanism that combines sta-
tistical methods with knowledge about
the structure of Russian prepositional
phrases. Then we analyse the results of
this data collection and estimate the ef-
ficiency of the collected list for the reso-
lution of morphosyntactic and semantic
ambiguity in a corpus.
1 Introduction
Computational research on multiword expressions
(MWEs) has mostly addressed the topic for En-
glish (Sag et al, 2001). Some research has dealt
with other languages, such as French (Michiels
and Dufour, 1998) or Chinese (Zhang et al, 2000),
but there has been no computationally tractable re-
search on the topic for Russian. What is more, the
study of MWEs in English has been mostly de-
voted to the description of nominal groups or light
verbs, e.g. (Calzolari et al, 2002), (Sag et al,
2001), while constructions starting with a prepo-
sition, such as in line, at large, have not been the
focus of attention.
Even though the tradition of studying Russian
idiomatic expressions resulted in many descrip-
tions of Russian idioms and phraseological dic-
tionaries, like (Dobrovol?skij, 2000) or (Fedorov,
1995), the studies and dictionaries often concen-
trate on non-decomposable colourful expressions
of the ?kick-the-bucket? type, such as byt? bez
carja v golove (?to have a screw loose?, lit. ?to
be without a tsar in one?s head?) and pay no atten-
tion to the very notion of their frequency. How-
ever, many expressions of this sort are relatively
rare in modern language. For example, there is no
single instance of bez carja v golove in the corpus
we used. At the same time, existing Russian dic-
tionaries of idioms often miss more frequent con-
structions, which are important both for transla-
tion studies and for the development of NLP ap-
plications. The task of the current study is defined
by the ongoing development of the Russian Ref-
erence Corpus (Sharoff, 2004), a general-purpose
corpus of Russian that is comparable to the British
National Corpus (BNC) in its size and coverage.
The goal of the study was to identify the list of
statistically important MWEs in the corpus and to
use them to reduce the ambiguity in corpus analy-
sis.
Existing research on the detection of MWEs
can be positioned between two extremes: linguis-
tic and statistical. The former approaches assume
syntactic parsing of source texts (sometimes shal-
low, sometimes deep to identify the semantic roles
of MWE components) and the ability to get in-
formation from a thesaurus. Detection results can
be further improved by deep semantic analysis of
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 17-23
source texts (Piao et al, 2003). When we apply
such techniques to a Russian corpus of the size of
the BNC, this means that we need accurate and ro-
bust parsing tools, which do not exist for Russian.
Also, no electronic thesaurus, such as WordNet
(Miller, 1990), is available for Russian. Purely sta-
tistical approaches treat multiword expressions as
a bag of words and pay no attention to the possibil-
ity of variation in the inventory and order of MWE
components. Given that the word order in Russian
(and other Slavonic languages) is relatively free
and a typical word (i.e. lemma) has many forms
(typically from 9 for nouns to 50 for verbs), the se-
quences of exact N-grams are much less frequent
than in English, thus rendering purely statistical
approaches useless.
This paper discusses a hybrid approach to the
identification of a specific type of MWEs in Rus-
sian, namely constructions starting with preposi-
tional phrases with the emphasis on those that are
frequent in the corpus. The study is also aimed at
a specific task, namely the disambiguation of their
morphological properties and syntactic functions
in a corpus. The approach assumes the develop-
ment of a list of MWEs supported by computa-
tional tools, including the calculation of standard
statistical measures and shallow parsing of prepo-
sitional phrases. In addition, the scope of the study
is further distinguished by the goal of extracting
MWEs from the core lexicon on the basis of a
general-purpose corpus, while many other MWE
detection studies concerned the extraction of tech-
nical terms specific to a particular domain.
2 The analysis of the structure of
Russian MWEs
First, a few words on the linguistic features of
MWEs in Russian in general and of prepositional
phrases in particular. Russian is an inflecting lan-
guage in which a word inflects for a set of morpho-
logical categories and shows a specific combina-
tion of these categories in its ending. For instance,
a noun in Russian has a fixed gender and inflects
for 6 to 9 cases and for the number (singular or
plural, with relics of the dual, which is relevant
for some words). Similarly, an adjective inflects
for six cases, two numbers and three genders and
agrees with the noun that is the head of the nom-
(1) beloj vorony genitive, singular
(2) beloj vorone dative, singular
(3) belye vorony nominative, plural
Table 1: Examples of the ambiguity of forms
inal group in the values of these three categories.
This means that an approach that treats MWEs as
?words with spaces inside? is not always suitable
for English, and cannot work for Russian. There
is a certain variation in the number of forms in
an MWE like rara avis in English, because rarae
aves and rara avises are both possible according to
(OED, 1989), even though they are extremely rare
(neither is used in the BNC and Internet searches
mostly point to entries in dictionaries), but at least
it is feasible to list the two extra forms separately.
At the same the Russian expression belaja vorona
(corresponding to rara avis, lit. ?white crow?) ex-
ists in 10 different forms (see examples in Table 1,
the endings are underlined) and the variability of
forms applies to any nominal group. The situation
is even more complicated in the case of MWEs
including verbs, given that in addition to several
proper verbal forms, a Russian verb can exist in
the form of up to four participles, each of which is
inflected as an adjective with its own set of forms.
At the same time the large number of forms
does not mean that each form can be mapped to a
lemma and a set of morphological categories with-
out any ambiguity, because the number of endings
is much smaller than the number of possible com-
binations of features. As lines (1) and (2) in Ta-
ble 1 suggest, the genitive and dative forms of sin-
gular feminine adjectives coincide, as well as the
genitive singular and nominative plural forms of
the noun vorona, see lines (1) and (3).1
If we consider prepositional phrases, the
amount of ambiguity is much smaller, because
prepositions govern the case of a nominal group
that follows them and do not themselves inflect.2
However, PPs still exhibit the general problem of
1See (Hajic? and Hladka?, 1998) for a general overview of
problems with the identification of a tagset and the resolution
of the ambiguity in Slavonic languages. Their description is
about Czech, but it can be applied to Russian as well.
2The terminology that distinguishes groups and phrases,
e.g. nominal groups vs. prepositional phrases, follows (Hall-
iday, 1985).
ambiguity in lemma selection. For instance, the
word form tem is ambiguous between the genitive
plural form of the noun tema (topic) and the instru-
mental singular masculine form of the demonstra-
tive pronoun tot (that). What is more, the preposi-
tional phrase s tem from the purely syntactic view-
point can be interpreted in both ways, because the
preposition s can govern either the genitive or the
instrumental case. At the same time the word tem
as the component of s tem chtoby (in order to,
lit. ?with that to?) shows no ambiguity in its part
of speech. More frequently ambiguity concerns
the selection of a lemma or morphological prop-
erties for the collocate. For instance, the second
word in the expression s bol?shim zapasom (with a
huge margin, lit. ?with large storage?) can be anal-
ysed as either of two adjectives bol?shoj (large) or
bo?l?shij (larger). Similarly, the last word in the
expression do six por (until now, lit. ?before this
time?) can be analysed as either of two nouns pora?
(time, season) or po?ra (pore). However, the ex-
pressions as a whole are not ambiguous and have
specific meanings.
The second problem with prepositional phrases
concerns their syntactic function, in particular the
notorious PP attachment problem. Even though
MWEs consisting of a preposition followed by a
nominal group are often identical in their syntac-
tic structure to fully compositional prepositional
phrases, they do not carry the same syntactic func-
tion as the latter. Such MWEs function in the syn-
tactic structure of the clause as a single unit with
a clearly defined meaning that cannot be decom-
posed into the meaning of their components. In
the end, it is better to treat them as adverbs, e.g. v
chastnosti (in particular), pod kljuch (turnkey, lit.
?under key?), or as prepositions in their own right,
e.g. v techenie (?in the course of?). Multiword
expressions starting with a preposition in English
have similar structure, but the difference with Rus-
sian is that there is no change in the structure of the
prepositional group, unlike some English MWEs,
e.g. in line, at large, which do not have a deter-
miner. Thus, we cannot use the difference in the
PP structure as an indicator of an MWE.
The fact that MWEs are not fully compositional
means that the meanings of their constituent words
change resulting a specific idiomatic meaning of
the whole contstruction. In this case we cannot
accept the general assumption of one sense per
discourse (Gale et al, 1992), because words such
as line, large in English or kljuch in Russian can
function in the same discourse in a totally differ-
ent sense. However, the assumption of one sense
per collocation can hold, because an MWE with
a prepositional phrase typically has one and the
same meaning: even though line, large or techenie
are ambiguous, in line, at large and pod kljuch, v
techenie have their specific meanings.
3 Methodology
The study starts with the selection of the list of
the most frequent prepositions to account for a
large number of potential collocations. Informa-
tion on the frequency of prepositions (Table 2) is
taken from the pilot version of the Russian Refer-
ence Corpus, which currently consists of about 55
million words (Table 2 lists the relative frequency
of prepositions in terms of the number of their in-
stances per million words, ipm).
Then for each preposition we extract its most
frequent collocations in the same corpus and
weight them according to the pointwise mutual in-
formation score (MI score) and Student?s t test (T
score). Two types of collocates are extracted: all
lexical items occurring immediately on the right
of a preposition and the longest possible nominal
groups defined as the sequence of adjectives and
nouns with the condition that nouns after the first
one are in the genitive case. This simple pattern
captures the majority of Russian nominal groups,
except those with elaborations of other clauses or
other prepositional phrases embedded inside them.
Anyway, because of their nature they do not be-
long to the class of fixed expressions under study.
The MI score foregrounds collocations in which
the second component rarely (almost never) oc-
curs outside of the expression in question, whereas
the T score foregrounds the most stable colloca-
tions on the basis of their frequency.
For every preposition and the list of its most
significant collocates we select MWEs on the ba-
sis of the lack of compositionality, namely that
there is a specific function performed by the ex-
pression and this function cannot be automatically
derived from the meaning of the words compris-
ing the candidate MWE. The criterion cannot be
defined precisely, but in many cases it is imme-
diately obvious that the candidate MWE is or is
not fully compositional. For instance, the expres-
sion bez vsjakoj svjazi (?for no apparent reason?,
lit. ?without any connection?) is sufficiently fre-
quent (38 instances) and the last element has a
lexical ambiguity svjaz?: connection (either physi-
cal or logical) or communication. When the MWE
is used in texts, it has a specific function, namely
someone?s discourse is evaluated as lacking a con-
tinuity. Thus, bez vsjakoj svjazi is treated as an
MWE. On the other hand, the expression v Rossii
(in Russia) is much more frequent and statistically
significant (14557 instances, its T score is 104.21),
but the set of locations constitutes an open list, in
which other members may be also frequent, e.g. v
SSHA (in the USA, 4739 instances), v Evrope (in
Europe, 2752), v Parizhe (in Paris, 2087), v Ki-
tae (in China, 1055), and the expressions are fully
compositional. None of them are considered to be
MWEs. At the same time, an expression with a
very similar structure: v storone ([to keep] aloof,
lit. ?in side?, 9690 instances, its T-score is 83.95) is
considered to be an MWE, because it is not com-
positional. The vast majority of uses of this ex-
pression do not refer to a physical location, but to
the fact that a person does not take part in a joint
activity.
Also, because of the idiomaticity of the mean-
ing of an MWE, it functions as a whole in the
syntactic structure of the clause, most typically as
an adjunct, and is translated to other languages in
a specific way not necessarily related to preposi-
tional phrases. The possibility of its translation
into English without the use of a prepositional
phrase is another reason for treating the expression
to be a potential MWE.
Finally, an easy test for detecting an MWE con-
cerns the ?penetrability? of the expression, i.e. the
possibility to insert another word, most typically
an adjective or a determiner, into the candidate
MWE. If any insertion is unlikely or the meaning
of components is redefined as the result of inser-
tion, then the expression in question is an MWE.
For instance, even though the MWE v storone can
be modified as v drugoj/levoj/protivopolozhnoj
storone (on the other/left/opposite side), the result-
Word Gloss Frq (ipm) Scored Selected
v/vo in 27966 703 198
na on 16513 198 117
s/so with 11131 734 64
po over 5816 124 56
k to 5468 157 20
u at 4956 203 6
iz out of 4816 194 6
za behind 4711 115 34
ot from 3540 118 13
o about 2956 357 5
dlja for 2302 164 13
do before 1978 477 40
pod under 1467 139 95
pri by 1163 140 2
bez without 1097 459 42
mezhdu between 502 102 9
Total 4384 720
Table 2: The list of prepositions and the number
of their patterns
ing expressions refer to physical locations and not
to the idiomatic meaning of the MWE v storone.
Thus, they are not considered as MWEs but the
possibility of insertion here does not violate the
penetrability of the MWE in question.
4 Results
The automatic procedure detected 4384 candidate
expressions, out of which we selected 720 MWEs.
The summary of prepositions and the number of
their patterns identified in the study is given in Ta-
ble 2. It was expected that more frequent prepo-
sitions participate in a larger number of MWEs.
However, the situation is more complex. Some
prepositions like u or iz occur almost exclusively
in fully compositional patterns, for example, ex-
pressing location: u okna, morja (by the window,
by the sea), or possession: u menja, u Ivana (I
have, Ivan has). Other prepositions that are less
frequent regularly produce non-compositional pat-
terns, e.g. pod rukami (?at hand?, which expresses
the specific meaning of availability, not literally
?under hands?), pod konec (?at the end?).
The results retained in the database include
well-formed prepositional phrases that function as
proper idioms, as well as syntactic constructions
that can take a noun or another nominal group
on their right, such as v techenie (?in the course
of?), which is a PP in its own, or an incom-
plete combination of a preposition and an adjec-
tive such as dlja puschij (?for greater?). The lat-
ter is a part of an open list of well-formed PPs,
as in dlja puschej vazhnosti, (?for greater impor-
tance?), soxrannosti (safety), ostrastki (frighten-
ing), but the word puschij in itself occurs only in
this construction. In other cases, the ?noun? from
the nominal group does not even exist in the con-
temporary language, like in bez umolku ([to talk]
without a pause), so the expression cannot be anal-
ysed correctly without knowing that it is an MWE.
The resulting list also includes multiword ex-
pressions with a slightly different structure, in
cases where an MWE naturally extends to the left
of the prepositionto form a larger pattern. One
example is sudja po vsemu (?to all appearances?,
lit. ?judging over all?), which is an extension of a
prepositional phrase po vsemu, as it gives the only
suitable pattern by far with 1626 instances in the
corpus, with the next most frequent left neighbour
razbrosat? po vsemu (?scatter all over? followed by
a spatial location) having only 34 instances. Also,
the sequence of words po vsemu is ambiguous, e.g.
it can be a part of larger PPs, such as po vsemu
gorodu, domu, zalu (over the whole city, house,
hall), so from the viewpoint of automatic detection
the MWE sudja po vsemu is more reliable.
Another example of an extended pattern is a
complex reflexive expression: drug druga (?each
other?, lit. ?friend friend-acc?), which is a mul-
tiword expression of its own, because no mean-
ing of friendship is explicitly communicated here,
as in nenavidet? drug druga (?to hate each other?,
lit. ?to hate friend friend-acc?). Even though the
original pattern did not cover this structure, the
expression has been detected for almost all prepo-
sitions in the form of PREP+drug-ending, because
the reflexive expression allows the insertion of any
preposition between the two elements, e.g. drug k
drugu (?to each other?, lit. friend to friend). Ex-
pressions of this sort resist the automatic identifi-
cation by means of a simple pattern such as those
used for other MWEs in the study.
It is well-known that ambiguity is abundant in
natural languages. As discussed above, many
word forms in Russian allow several morpholog-
ical analyses and this applies to forms used in
MWEs. Monolingual and bilingual dictionaries
can also give an estimation of the semantic ambi-
guity by counting the number of senses and trans-
lations available for a word, though this will be the
lower bound, because the number of senses and
translations offered in dictionaries does not typ-
ically cover the full variety of types of possible
uses: depending on a context, a word can be trans-
lated in many more ways than is suggested by a
dictionary.
It was relatively straightforward to measure the
reduction of morphological ambiguity. We can
compare the number of morphological analyses
before and after tagging of MWEs. The reduction
of semantic ambiguity can be measured only in-
directly by comparing the difference between the
number of senses detected in a monolingual dic-
tionary and the number of translations in a bilin-
gual dictionary against the same numbers after
tagging of MWEs, because we can assume that
each MWE has only one sense, given the ?one-
sense-per-collocation? hypothesis. Even in cases
when the hypothesis does not hold, as in the case
of the reflexive MWE drug druga, which can be
translated in many different ways depending on
the main predicate in a clause, the combination of
the two words in an MWE saves from the possi-
bility of their separate translation as companion,
friend, mate, pal, comrade, colleague, fellow, etc.
Table 3 shows the level of the ambiguity in the
original texts and the estimates for its reduction
using the list of MWEs. The morphological anal-
ysis was performed using Mystem (Segalovich,
2003), a high-performance analyser which is also
used in Yandex, a major Russian search engine.
The results show that 41% of Russian word forms
are ambiguous with respect to their morphologi-
cal features with an average number of 4.6 anal-
yses per ambiguous word (1.9 on average for all
words).
The estimation of semantic ambiguity is based
on electronic copies of the monolingual Ozhegov
dictionary (Ozhegov, 1988) and the Oxford Rus-
sian bilingual dictionary (ORD, 2000). The for-
mer has 37785 entries with 1.6 senses per entry
Morphology Monolingual Bilingual
Coverage 55022365 38508185 39056759
Average ambiguity 1.90 4.38 11.66
No of ambiguous words 22790728 19254090 19528375
Ambiguity per ambiguous word 4.59 8.76 23.32
Ambiguity after MWEs 4.06 8.39 21.72
Improvement 10.66% 4.27% 6.86%
Table 3: The analysis of the ambiguity resolution
on average, while the Russian-English part of the
latter has 40303 entries with 1.9 translations per
entry. The dictionaries were applied to simple tag-
ging of the running text in the corpus, whereby
every word listed in the dictionaries was tagged
with the respective number of its senses and trans-
lations. The experiment also showed that either of
the two dictionaries covers about 70% of the run-
ning text (noncovered words are typically proper
names). Since more frequent words typically ex-
hibit greater polysemy, the polysemy in the run-
ning text is larger. A word has about 4.4 senses
on average according to (Ozhegov, 1988) and 11.7
translations according to (ORD, 2000). How-
ever, these counts are slightly misleading, because
about half of the words in the corpus are not am-
biguous. But if a word is ambiguous, it exhibits
a much greater set of possible senses and transla-
tions: for instance, (ORD, 2000) lists the word big
as having 35 translations in various contexts, so if
the average ambiguity in the corpus is counted for
ambiguous words only, it reaches 8.8 for senses
and 23.3 for translations.
The results for morphological and semantic am-
biguity are summarised in Table 3. After the appli-
cation of the list of MWEs (they cover only about
2% of the total corpus size), the level of ambigu-
ity for ambiguous lexical items goes down to 4.1
for morphological analysis, 8.4 for senses and 21.7
for translations. This gives a drop of about 11%
for ambiguity in morphological analysis, 4% for
ambiguity of senses and 7% for translations.
5 Conclusions
The paper reports the first attempt to ap-
ply computational methods to the detection
and use of multiword expressions in Rus-
sian. The study resulted in a list of about
700 prepositional phrases which is available
from http://www.comp.leeds.ac.uk/
ssharoff/frqlist/mwes-en.html. The
list offers rough results of MWE selection: it
includes proper idioms, of the type one can find
in a phraseological dictionary, in particular items
missed or underdescribed in such dictionaries,
so that it can be used as a source for improving
them. However, it also includes items on the edge
between idioms and other types of lexicalised
phrases, for instance, grammatical constructions
or institutionalised phrases.
The study shows that a simple method with lit-
tle syntactic knowledge about the structure of PPs
in Russian and no semantic resources can pro-
duce a useful list of MWEs. The combination of
automatic detection of the most significant collo-
cations and manual filtering of the results is not
labour intensive and produces many expressions
that are not covered in existing Russian dictionar-
ies.
The next immediate step would be to use the
lists for the study of translation equivalence be-
tween English and Russian, because MWEs are
also not adequately represented in bilingual dic-
tionaries, whereas their translation causes signif-
icant problems for language learners as well as
for machine translation systems. For instance, the
Oxford Russian Dictionary lists 13 translations of
bez (without), including such idioms as bez uma
(?be crazy about something?, lit. ?without mind?),
but fails to list many other more frequent construc-
tions, such as bez ocheredi (to jostle to the front of
the queue, lit. ?without queue?), bez umolku ([to
talk] nonstop), bez sleda ([to vanish] without any
hint), etc.
The lists can also act as a useful resource for
morphological and semantic disambiguation. The
list covers about 2% of the running text in the cor-
pus, yet it reduces semantic ambiguity in the run-
ning text by 4?7%, and morphological ambiguity
by 11%. We did not experiment with the reduction
of syntactic ambiguity, because there is no Russian
syntactic parser that can give robust parsing of an
unrestricted corpus, such as that used in the study.
Also, there is no easy way to force existing parsers
to treat the identified MWEs as separate syntactic
units on the clause level. However, we expect that
accuracy will increase, because the set of identi-
fied MWEs reduces the number of PP attachment
problems, as each MWE acts as an adjunct unit of
its own within the clause.
The domain of prepositional phrases has been
chosen specifically because it is relatively easy to
guess the structure from the form by means of
shallow parsing. Further experiments may con-
sider detection of other types of MWEs, in par-
ticular, with light verbs, such as brat? primer (to
follow the example of someone, lit. ?take exam-
ple?), which are also very important for transla-
tion, but given the free word order in Russian this
extension requires syntactic parsing to detect the
dependency structure.
Acknowledgements
I?m grateful to Dmitrij Dobrovol?skij, Tony Hart-
ley, Viktor Pekar, and especially to Bogdan
Babych for useful discussions.
References
Nicoletta Calzolari, Charles J. Fillmore, Ralph Gr-
ishman, Nancy Ide, Alessandro Lenci, Catherine
MacLeod, and Antonio Zampolli. 2002. Towards
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the 3rd International
Conference on Language Resources and Evaluation
(LREC 2002), pages 1934?1940.
Dmitrij Dobrovol?skij. 2000. Contrastive idiom analy-
sis: Russian and German idioms in theory and in the
bilingual dictionary. International Journal of Lexi-
cography, 13(3):169?186.
A.I. Fedorov, editor. 1995. Frazeologicheskii slovar
russkogo literaturnogo iazyka. Nauka, Novosibirsk.
William Gale, Kenneth Church, and David Yarowsky.
1992. One sense per discourse. In Proc. of the 4th
DARPA Speech and Natural Language Workshop,
pages 233?237.
Jan Hajic? and Barbora Hladka?. 1998. Tagging inflec-
tive languages: Prediction of morphological cate-
gories for a rich, structured tagset. In Proceedings
of COLING-ACL, pages 483?490.
Michael A. K. Halliday. 1985. An Introduction to
Functional Grammar. Edward Arnold, London.
Archibald Michiels and Nicolas Dufour. 1998.
DEFI, a tool for automatic multi-word unit recog-
nition, meaning assignment and translation selec-
tion. In Proc. of First International Language Re-
sources and Evaluation Conference, pages 1179?
1186. Granada, Spain.
G. Miller. 1990. WordNet: an online lexical database.
International Journal of Lexicography, 3(4).
OED. 1989. Oxford English Dictionary. Clarendon
Press, Oxford.
ORD. 2000. The Oxford Russian Dictionary. Oxford
University Press, Oxford, 3rd edition.
S. I. Ozhegov. 1988. Slovar? russkogo iazyka. Russkii
iazyk, Moskva, 20th edition.
Scott S. L. Piao, Paul Rayson, Dawn Archer, Andrew
Wilson, and Tony McEnery. 2003. Extracting mul-
tiword expressions with a semantic tagger. In Pro-
ceedings of the ACL 2003 Workshop on Multiword
Expressions, pages 49?56.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
expressions: A pain in the neck for NLP. Technical
Report No. 2001-03, LinGO Working Paper.
Ilya Segalovich. 2003. A fast morphological algorithm
with unknown word guessing induced by a dictio-
nary for a web search engine. In Proc. of MLMTA-
2003, Las Vegas.
Serge Sharoff. 2004. Methods and tools for de-
velopment of the Russian Reference Corpus. In
D. Archer, A. Wilson, and P. Rayson, editors, Cor-
pus Linguistics Around the World. Rodopi, Amster-
dam.
J. Zhang, J. Gao, and M. Zhou. 2000. Extraction of
Chinese compound words ? an experimental study
on a very large corpus. In Proc. of the 2nd Chinese
Language Processing Workshop, ACL 2000, pages
132?139.
Mul t i l i ngua l i ty  in a Text  Generat ion  System 
For Three  Slavic Languages  
Geert-Jan Kruijff a, Elke Teich t', John Bateman ~, Ivana Kruijit;Korbayovfi", 
Hana Skoumalovg ~,Serge Sharoff 'l, Lena Sokolova d, Tony Hartley ~, 
Kamenka Staykova/, Ji~'~ Hana" 
?Charles University, Prague; ~University of the Saarland; ~University of Bremen; 
aRRIAI, Moscow; ?University of Brighton; /IIT, BAS, Sofia 
http://www.itri.brighton.ac.uk/projects/agile/ 
Abstract 
This paper describes a lnultilingual text generation 
system in the domain of CAD/CAM software in-- 
structions tbr Bulgarian, Czech and l:\[ussian. Start- 
ing from a language-independent semantic represen- 
tation, the system drafts natural, continuous text 
as typically found in software inammls. The core 
modules for strategic and tactical gene,'ation are im- 
plemented using the KPML platform for linguistic 
resource development and generation. Prominent 
characteristics of the approach implemented a.re a 
treatment of multilinguality that makes maximal use 
of the cominonalities between languages while also 
accounting for their differences and a common repre- 
sentational strategy for both text planning and sen- 
tence generation. 
1 In t roduct ion  
This paper describes the Agile system I tbr the 
multilingual generation of instructional texts as 
found in soft;ware user-manuals in Bulgarian, 
Czech and Russian. The current prototype fo- 
cuses on the automatic drafting of CAD/CAM 
software documentation; routine passages as 
found in the AutoCAD user-manual have been 
taken as target texts. The application sce- 
nario of the Agile system is as follows. First, 
a user constructs, with the help of a GUI, 
language-independent task models that spec- 
ify the contents of the documentation to be 
generated. The user additionally specifies the 
language (currently Bulgarian, Czech or Rus- 
sian) and the register of the text to be gen- 
erated. The Agile system then produces con- 
tinuous instructional texts realizing the speci- 
fied content and conforming to the style of soft- 
ware user-manuals. The texts produced are 
1EU Inco-Copernicus project PL961004: 'Automatic 
Generation of Instructional Texts in the Languages of 
Eastern Europe' 
intended to serve as drafts for final revision; 
this ~drafting' scenario is therefbre analogous to 
that first explored within the Drafter project. 
Within the Agile project, however, we have ex- 
plored a more thoroughly nmltilingual architec- 
ture, making substantial use of existing linguis- 
tic resources and components. 
The design of the Agile system overall re, sts 
on the following three assumI)tions. 
First, the input of the system should be spec- 
ified irrespective of any particular output lan- 
guage. This means that the user must be able to 
express the content that she wants the texts to 
convey, irrespective of what natural language(s) 
she masters and in what language(s) the out- 
put text shouM be realized. Such language- 
independent content specification can take the 
form of some knowledge representation pertain- 
ing to the application domain. 
Second, the texts generated as the outtmt of 
the system should be well-formulated with re- 
spect to the expectations of natiw. ? speakers of 
each particular language covered by the system. 
Since differences among languages may appear 
at any level, language-sensitive d cisions about 
the realization of the specified content must be 
possible throughout he generation process. 
And third, the notion of multilinguality em- 
ployed in the system should be recursive, in 
the sense that the modules responsible tbr the 
generation should themselves be multilingual. 
The text generation tasks which are common 
to the languages under consideration should be 
pertbrmed only once. Ideally, there should be 
one process of generation yielding output in 
multiple languages rather than a sequence of 
monolingual processes. This view of 'intrin- 
sic multilinguality' builds on the approach set 
out in Bateman et al (1999). Each module of 
the system is fnlly multilingual in that it simul- 
474 
taneously enables both integration of linguistic 
resources, defining commonalities bel;ween lan- 
guages, and resource integrity, in |;bat the in- 
dividuality of each of the language-speeitic re- 
sources of a multilingnM ensemble is always pre- 
served. 
We consider these assuml)l;ions an(l the view 
of multilinguality entailed by |;hem to be cru- 
cial for the design of efli;ctive multilingual text 
generation systems. The results so far a(:hicved 
by the Agile system SUl)port this and also ofl'er 
a ~soli(l experiential basis tbr the develot)mcnt of 
fllrther multilingnal generation systems. 
The overall operation of 1;t1(; Agile sysl;em is 
as tbllows. Al/tcr the us(u' has Sl)ecilied some 
inl;en(led text (;OlltenI; (described in Section 2) 
via the Agile GUI, the system i)ro(:eeds to gen- 
eral;e the texts required. To do this, a text 
t)lammr (Section 3) first assigns parts of the, 
task model to text elements and arranges l;h(;m 
in a hierarchical fashion a text t)lan. Then, a 
sentence plammr organizes I;he content of the 
text elements into sentence-sized elml~ks and 
ere~,tes the corresponding input fin' l;he tacti- 
ca,1 generator, expressed in standard sentence 
l)lamfing language (SPI,) lbrmulae. Finally, 1;11(; 
tactical g(meral;or generates t;he linguistic real- 
izations corresponding 1;o these Sl)l~s the text 
(Sect;ion 4). In the stage of the l)rojccI; rt}l)orte(l 
here, we, conceal;rated i)arl;icularly on \])roccdu- 
ral texts. These otlhr sl;el)-by-st;e t) des(:rit)t;ions 
of how to perlbrm domain tasks using the given 
software tools. A simplified version of one such 
procedural text is given (tbr English) in Fig- 
ure 1. This architectm:e mirrors the reference 
architecture for generation diseusse(t in I/,eiter 
8z Dale (1.997). The modules of the system are 
1)ipelined so that a continuous text is generated 
realizing the intended meaning of the inlmt se- 
mantic representation without backtracking or 
revision. 
Several important properties have ('haracter- 
ized the method of development leading to the 
Agile system. These are to a large extent re- 
sponsible for the eflhetiveness of the system. 
These include: 
Re-use  and  adaptat ion  o f  ava i lab le  re-  
sources .  We have re-used snt)stantial bodics 
of e, xisting linguistic resources at all levels rel- 
evant for the system; this t)laye(l a (:rueial role 
in achieving the Sol)histieatcd generation capa- 
7b d~nw a polylinc 
First start the PLINE command using one of these meth- 
ods: 
Windows From the Polylinc tlyout on the, l)raw tool~ 
lmr, choose Polylinc. 
DOS and UNIX  lqom the Draw menu, choose Poly- 
line. 
1.. Spccit~y the start point of the polyline. 
2. S1)ecil~y tim next point of the 1)olylinc. 
3. Press ll,cturn t;o end the polyline. 
Figure l: Example "To draw a polyline" 
bilities now displayed by the system in each of 
its languages of expertise prior to the project 
t\]m'l.'e were 11o substantial ~mtomatic generation 
systenls fi)r any of the languages covered. The 
core modules for strategic and ta(:ti('al gener- 
ation were all imt)lemcnted using the Kernel- 
Penman Multilingual system (KPML: ef Bate- 
man et al, \]999) a Common l,isp base(t gram- 
mar development environment, in addition, 
we adopted the Pemnan Upt)er Model as used 
within Pemnan/KPMl~ as the basis tbr our 
linguistic semantics; a more rcstri(:ted domain 
model (DM) rclewmt o the CAD/CAM-domain 
was &',lined as a st)e('ialization of l;he UM con- 
(:epts. The I)M was iuspired by the domain 
me(tel of the Drafter l)rojet:t, but l)res(ml;s a 
g(m(',ralizati()n ()f the latter in that it allows for 
eml)(;d(ling t:asks and illsLrut'|;ions t:o any arlfi- 
l;rm:y re(:ursive depth (i.e., more complex l;cxt; 
plans). Ah'eady existing lexical resom:ces and 
morphological modules availabh; to the 1)ro.j(',ct 
were re-used tbr Bulgarian, Czech and l~.ussian: 
the Czech and Bulgarian components were mo(t- 
ules written in C (e.g., IIaji(: L; Hla(lk~, 1997, 
tbr Czech) that were interfimed with KPMI, us- 
ing a standard set of API-methods (of. Bate- 
man & Sharoff, 1998). Finally, because no 
grammars uitable for generation in Bulgarian, 
Czech and l/.ussia,n existed, a grammar tbr En- 
glish (NIGEL: Mann & Matthiessen, 1985) was 
re-used to lmild them; tbr the theoretical basis 
of this technique see Teich (1995). 
Combinat ion  o f  two  methods  o f  resources  
deve lopment .  Two methods  were com- 
bined to enable us to develop basic general- 
language grammars and sublanguage grammars 
fin: CAD/CAM instructional texts at; |;11(; same 
time. One nmthod is the system-oriented one 
aimed at lmildiug a computational resource 
475 
with a view of the whole language system: this 
is a method strongly supported by the KPML 
development environment. The other method 
is instance-oriented, and is guided by a detailed 
register analysis. The latter method was partic- 
ularly important given the Agile goal of being 
able to generate texts belonging to rather di- 
verse text types- -  e.g., impersonal vs. personal; 
procedural, flmetional descriptions, overviews 
etc. 
Cross-linguistic resource-sharing. A cross- 
linguistic approach to linguistic specifications 
and implementation was taken by maximizing 
resource sharing, i.e. taking into account sim- 
ilarities and differences among the treated lan- 
guages o that development tasks have been dis- 
tributed across different languages and re-used 
wherever possible. 
2 Language- independent  Content 
Specif icat ions 
The content constructed by a user via the Ag- 
ile GUI is specified in terms of Assertion-bozes 
or A-boxes. These A-boxes are considered to 
be entirely neutral with respect o the language 
that will be used to express the A-box's con- 
tent. Thus individual A-boxes can be used for 
generating multiple languages. A-boxes spec- 
i(y content by instantiating concepts from ~,he 
DM or UM, and placing these concepts in rela- 
tion to one another by means of configurational 
concepts. The configurational concepts define 
adnfissible ways in which content can be struc- 
tured. Figure 2 gives the configurational con- 
cepts distinguished within Agile. 
Procedure A procedure has three slots: 
(i) GOAL (obligatory,filled by a USER-AcTION), 
(ii) METIIODS (optional, filled by a METHOD-LIsT), 
(iii) SIDE-EPFECT (optional, filled by a USER- 
EVENT). 
Method A method has three slots: 
(i) CONSTRAINT (optionM, filled by an OPERATING- 
SYSTEM), 
(ii) PaEeONDITION (optional, filled by a PROCE- 
DURE), 
(iii) SUUSTEPS (obligatory, filled by a PI~OCEDUI/E- 
LIST). 
Method-List A METIIOD-LIST is a list of h/IETIIOD'S. 
Procedure-List A PROCEI)URE-LIST is a list of 
PROCEDURE:S. 
Figure 2: Configurational concepts 
Configurational concepts are devoid of actual 
content. Tile content is provided by inst, antia- 
tions of concepts that represent various user ac- 
tions, interface events, and interface modalities 
and functions. Taken together, these instanti- 
ations provide the basic propositional content 
tbr instructional texts and are taken as input 
tbr the text planning process. 
3 Strategic Generat ion: From 
Content  Specif icat ions to Sentence 
Plans 
To realize an A-box as a text, we go through suc- 
cessive stages of text planning, sentence plan- 
ning, and lexico-grammatical generation (cf 
also Reiter & Dale, 1997). At each stage there 
is an increase in sensitivity to, or dependency 
on, the target language in which output will 
be generated. Although the text planner itself 
is language-independent, the text; plamfing re- 
sources may (lifter fl'om language to language 
as much as is required. This is exactly analo- 
gous to the situation we find within the individ- 
ual language grammars as represented within 
KPML: we therefore represent the text planning 
resources in the same fashion. For the text type 
and languages of concern here, however, w~ria- 
lion across languages at the text planning stage 
turned out to be minimal. 
The organization of an A-box is used to guide 
the text planning process. Itere, we draw a dis- 
tinction between text structure elements (TSEs) 
as the elements from which a (task-oriented) 
text, is built ut), and text templates', which con- 
dition the way TSEs are to be realized linguis- 
tically. We locus on the relation between con- 
cepts on the one hand, and TSEs on the other. 
We are specifically interested in the configura- 
tional concepts that are used to configure the 
content specified in an A-box because we want 
to maintain a close connection between how the 
content can be defined in an A-box and how 
that content is to be spelled out in text. 
3.1 Structuring and Styling 
A text structure element is a predefined com- 
ponent that needs to be filled by one or more 
specific parts of the user's content definition. 
Using the reader-oriented terminology common 
in technical authoring guides, we distinguish 
a small (recursively defined) set of text TSEs; 
these are listed in Figure 3. 
476 
Task-Docmnent  A TASK-\])OCUMFNT has tWO slots: 
(i) TASK-TFI'I,E (ol)ligatory), 
(ii) TASK-INSTI{U(ITIONS (obligatory), being a list 
of" at least one ~\[NSTRUCTION. 
Instruction An INSTRUCTION has three slots: 
(i) TASKS (obligatory), being a list of at least one 
TASK~ 
(ii) CONSTRAINT (optional), 
(iii) Pm,ZCONDITION (optional). 
Task  A TASK has two slots: 
(i) INSTRUCTIONS (ol)tional), 
(ii) SII)I';-EI,'I"I,:C'I' (ol)tional). 
Figure 3: Text Structure Elements (TSEs) 
The TSEs are placed in correspondence with 
the configurational concet)ts of the DM (cf. Fig- 
ure 2); this enat)les us to lmild a text stru('ture 
l;hat folh)ws the structuring of the content in an 
A-1)ox (cf. Figure 4). 
Orthogonal to the notion of text structure l- 
ement is the notion of text temt)late. Whereas 
TSEs capture what needs to be realized, the 
text template (:al)tures how that content is to 
1)e realized. Thus, a feint)late defines a style 
for expressing the content. Am we discuss be- 
low, we define text templates in terms of con- 
straints on the realization of si)e(:iti(" (in(tivid- 
ual) TSEs. D)r examt)le, whereas in Bulgarian 
and Czech headings (to which the '\]'ASK-TITLE 
element corresponds: of. Figure 4) are usually 
realized as nominal groups, in the Russian Au- 
toCAD ulallnal headings are realized as nonii- 
nile purpose clauses as they are ill English. 
3.2 Tex~ P lann ing  g~ Sentence  P lann ing  
The major component of the text pbmner is 
fi)rnmd by a systemic network fi)r text struc- 
turing; this network, called the text structur- 
ing region, defines an additional level of linguis- 
tic resources for the level of genre. This region 
constructs text structures in a way that is very 
similar to the way in which the systemic net- 
works of the grammars of the tactical genera- 
|or build up grammatical structures. In fact, 
by using KPML to implement his means for 
text structuring, the interaction between global 
level text generation (strategic generation) and 
lexico-grammatical expression (tactical genera- 
tion) is greatly facilitated. Moreover, this al)- 
t)roach has the advantage |;tint constraints on 
output realization can 1)e easily accmnulated 
and propagated: for example, the text plan- 
ner can iml)ose constraints on the output lexico- 
grammatical realization of particular text t)lan 
elements, such am the realization of text head- 
ings by a nominalization ill Czech and Bulgar- 
|an or by an infinite purpose clause in Rus- 
sian. This is one contribution to overcoming the 
notorious generation gap prol)leln caused when 
a text planning module lacks control over the 
line-grained istinctions that m'e available in a 
grmmnar. Ill our case, both text plamfing and 
sentence planning are integrated into one and 
the same system and are distinguished by strat- 
ification. 
TASK-TITLE ~-} GOAl, of topmost  PROCEDURE 
TASK-INSTRUCTIONS ~-} METIIODS of PROCEDUI/E 
Sll)E-EIq,'ECT ~ SIDhl-EFFI~CT of PROCEDUII.I\] 
TASK /-~ GOAL of PROCEI)IHtI,; 
(-JONSTRA1NT <-} CONSTRAINT of ~41,VI'IIO1) 
PRECONI)ITION ~ PIH?COND1TION of ~,4ETI1OI) 
1NSTIIUCTI(IN-TAsKS 1--} SUBSTH)S of a METIIOD 
INST1HJCTION +5 MI,TI'IIOD 
Figure 4: Mapping TSEs and configurational 
concepts defined in the DM 
Following on from the orthogomflity of text 
t/;mplates and text structure elements, the text 
structuring region consists of two parts. One 
1)arl; deals wil;h interpreting the A-box in terms 
of TSEs: traversing l;he network of this part of 
the region produces a text structure for the A- 
b/lx contbrufing to the definitions above. The 
second part of the region imposes constraints 
on the realization of the TSEs introduced by 
the first part. Divers(; constraints can be ira- 
posed depending on the user's choice of style, 
e.g., personal (featuring ppredominantly imper- 
atives) vs. impersonal (tbaturing indicatives). 
Tile result of text plmming is a text plan. 
This can be thought of as a hierarchical struc- 
ture (built by TSEs) with lilts of A-box content 
at; its leaves together with additional constraints 
imposed by the text planning process: e.g., that 
the Title segment of the document should not be 
realized as a full (:lause but; rather as a nominal 
phrase or a lmrt)osive det)endent clause. The 
text plan may also include constraints on pre- 
ferred layout of the docmnent elements: this 
ilflbrmation is passed on via HTML annotations. 
The sentence plmmer then takes this text plan 
as intmt, and creates SPL tbrmulae to express 
477 
the content identified by the text plan's leaves. 
The resulting SPLs can also group one or more 
leaves together (aggregation) det)ending on de- 
cisions taken by the text planner concerning dis- 
course relations. Furthennore, constraints on 
realization that were introduced by the text- 
planner are also included into the SPLs at this 
stage. 
Of particular interest multilingually is the 
way concepts may require different kinds of re- 
alizations ill different languages. For example, 
languages need not of course realize concepts 
as single words: in Czech the concept Mcn,t 
gets realized as "menu" but the interface modal- 
ity Dialogboz is realized as a multiword expres- 
sion "dialogovd okno" (whose compofients i.e., 
an adjective and a nominal head may undergo 
various grammatical operations independently). 
The Agile system sentence plammr handles uch 
cases by inserting SPL fbrms corresponding to 
the literal semantics of the complex expressions 
required; these are then expressed via the tac- 
tical generator in the usual way. The result- 
ing SPL formulas thus represent the language- 
specitic semantics of the sentences to be gener- 
ated. Otherwise, if a concept maps to a single 
word, the sentence planner leaves the fnrther 
specification of how the concept should be re- 
alized to the lexico-grammar nd its concept- 
to-word mapI)ings. More extensive diflb.rences 
between languages are handled by conditional- 
izing the text and sentence planner resources 
fltrther according to language. 
4 Tactical Generat ion:  From 
Sentence P lans  to Sentences 
The tactical generation component hat colt- 
structs sentences (and other grammatical units) 
fl'om the SPL tbrmulae specified in the text 
plan relies on linguistic resources tbr Bulgarian, 
Czech and Russian. The necessary grammars 
and lexicons have been constrncted employing 
the methods described in Section 1. As ,toted 
there, the crucial characteristic of this model 
of nmltilingual representation is that it allows 
tbr the representation f both, commonalities and 
differences between languages, as required to 
cover the observable ontrastive-linguistic phe- 
nomena. This can be applied even among typo- 
logically rather distant languages. 
We first illustrate this with respect o some 
of the contrastive-linguistic t)henomena that are 
covered by this model employing exami)les ti'om 
English, Bulgarian, Czech and Russian. We 
then show the organization of the lexicons and 
briefly describe lexical dloice. 
4.1 Semantic and grammatical 
cross-linguistic variation 
One. of the tenets of our model of cross-linguistic 
variation is that languages have a rather high 
degree of similarity semantically attd tend to 
differ syntactically. We can thus expect o have 
identical SPL expressions for Bulgarian, Czech 
and Russian in many cases, although these may 
be realized by diverging syntactic structures. 
However, we also allow for the case in which 
there is no commonality at; this level and even 
the SPL expressions diverge. 2 Example 1 illus- 
trates the latter case (high semantic divergence, 
plus grammatical divergence), and example 2 
the former (semantic ommonality, plus gram- 
matical divergence). 
Example 1: English and Russian spa- 
tial PPs .  The major lexico-grammatical d i f  
ference l)etween English and Russian preposi- 
tional phrases is that the relation expressed by 
the PP is realized by the choice of the prepo- 
sition in English, whereas in Russian, it; is in 
addition realized by case-government. In the 
are.a of spatial PPs, the choice of a particular 
preI)osition in English corresl)onds to a distinc- 
tion in the dimensionality of the object that re- 
alizes the range of the relation expressed by the 
PP. For both PPs expressing a location and PPs 
expressing movement, English distinguishes be- 
tween three-dimensional objects (in, into), one- 
or-two-dimensional objects (on, onto) and zero- 
dimensional objects (at, to). 
In Russian, in contrast, zero-or-three dimen- 
sional objects (preposition: v) are opposed 
to one-or-two-dimensional objects (preposition: 
ha). A fnrther difference between the expres- 
sion of static location vs. movement is expressed 
by case selection: na/v+locative case expresses 
static location, v/na+accusative case expresses 
inovement (entering or reaching an object) and 
the preposition k+dative case expresses move- 
inent towards an object (,lot quite reaching or 
2This distinguishes our approach fl'om interlingua- 
based systems, which typically require a common seman- 
tic (or conceptual) input. 
478 
entering it). In the {-onverse relation, motion 
away from an object, s is sele, eted tbr move- 
ment from within an oh.joel;, and ot fbr move- 
men| away from the vicinity of an ot).jeet. Her(;, 
both prel)ositions govern genitive case. The di- 
mensionality of the object is only relevant for 
the distinction between v/na and s/ot, 1)ut not 
for h. Since the concel)tualizations of spatial re- 
lations are ditf'erent across \]'3nglish and Russian, 
the input SPL expressions diverge, as shown in 
Figure 5); rather than using domain model con- 
cepts, these SPL ext)ressions restrict hemselves 
to Ut)pe, r Model concepts in order to highlight 
the cross-linguistic contrast. This examl)le illus- 
trates well how it is (}ften ne{:e, ssary t{} 'semanti- 
{:ize,' eve, nts differently in (tilt'ere|d; languages in 
order 1;o achieve the most natural results. Not;{; 
that Cze, ch is here very similar to l/nssian. 
a. SPL Russian 
(example 
:name DO-Textl-Ku 
:targetform "Pomestite fragment v bufer." 
:logicalform 
(s / dispositive-material-action 
:lex pomestitj 
:speech-act-id command 
:actee (a / object :lex fragment) 
:destination (d / THREE-D-0BJECT 
:lex bufer))) 
1}. SPL Rn: English 
(example  
:name D0-Textl-En 
:targetform "Put the selection on the clipboard." 
:iogicalform 
(s / dispositive-material-action 
:lex put 
:speech-act-id command 
:actee (a / object :lex selection) 
:destination (d / ONE-0R-TW0-D-0BJECT 
:lex clipboard))) 
Figure 5: SPI, ext}ressions 
Example 2: English, Bulgar ian and Czech 
headers in CAD/CAM texts. Grammatical 
ullits (1) (4) below show all ex~?tIllt, e of ,:r,,ss- 
linguistic commonality at the level of sen|anti{: 
int}ut and divergence at the le, vel of grammar. 
These units all time|ion as selfsutficient Task- 
titles tbr the deseril}tions of particular actions 
that can be t)erformed with the given s{}t'tware. 
(1) En: T{} draw a polyline 
(2) BU: qepTaene na IlOJII4MI4IIFIH 
Drawing- of polylineqNDEF 
NOMINAL  
(3) Cz: Kreslenl kf'ivky 
drawing-NOMINAL \]ine-GEN 
(d) \]/,ll: LIwo6I,I Hal)I4COBaTI, IIO,KHJIIIIIHIO 
in-order draw-INF l)olyline-AcC 
There are two major dit  re,,,ces (:,) (4) 
that need to 1)e accounte, d for: (i) they exhibit 
divergent grammatieal  ranks in that (1) and 
(4) are clauses (uontinite), while (2) and (3) are 
nomil,al groul,s (nominalizations); and ( i i )they 
show divergent syntact ic realizations: (2) 
and (3) ditl'er in that in Bulgarian, wlfich does 
not have (:as(',, the relation 1)etween the syntactic 
head Met)q_'aelte (ch, crtacnc) and the modifier lie- 
:mamma (polilinia) is (;xt)ressed by a t)re, position 
na (ha), whereas in Cze, ch, which has cast, this 
relation is expressed by genitive case, (k?ivky). 
\])espite these (litferen(:es, only the first diver- 
gen(:e has any (;onsequen(:{;s for the S\])L ext)res- 
sions rcquir(;d; I;hc l)asie semantic ommona\]- 
ity among (1)(4)  is 1)reserve, d. This is shown 
in Figm:e 6 t)y me, ans of the standard linguis- 
tic conditionalization 1)rovided 1)y KPML l'or all 
levels of linguistic des(:ription. The COll(tition- 
alization shows that both the English (1.) and 
the Russian (4) ar(' nontinite clauses while, the 
\]hdgarian (2) and the Czech (3) are nominM- 
izations. These S\])l, ext)ressions also show the 
use of (lom~dn ('onc(;1)ts as i)rodu('e(l by the text 
tfl~mner rathe, r than Ut)lmr model concepts as in 
the SPLs in Figure, 5. 
(example 
:name DO-Textl 
:logicalform 
(s / DM::draw 
:en :ru :PROPOSAL-Q & PROPOSAL 
:bu :cz :EXIST-SPEECHACT-Q & NOSPEECHACT 
:actee (d / DM::polyline))) 
Figure 6: Multilingual SPL e, xpression tbr the 
header examlfles 
The second differen('e is handled by the gener- 
ation grmmnars internally. Here, Bulgarian and 
Czech share the basic tractional-grammatical 
description of t)ostmotlifie, rs tbr nomilmlizati(ms 
(Figm:e 7). The ditl'erence in structure only 
479 
shows in syntagmatic realization and is separate 
from the functional description: For Bulgarian, 
the postmodifier marker Ha (ha: %f') is inserted, 
and tbr Czech, the nominal group realizing the 
Postmodifier is attr ibuted genitive ease. a
(gate 
:name MEDIUM-QUALIFIER 
:inputs processual-mediated 
:outputs 
((i.0 medium-qualifier 
(:bu :cz preselect Medium nominal-group) 
(:cz preselect Medium noun-gen-case) 
(:bu insert Mediumqualifiermarker) 
(:bu lexify Mediumqualifiermarker na))) 
:region QUALIFICATION) 
Figure 7: Shared system tbr Bulgarian and 
Czech 
4.2 Lexical choice and lexicons 
The lexical items tbr each language are selected 
from the lexicon via the domain model. A DM 
concept is annotated with one or more lexical 
items from each language. If there is more than 
one item per language, the choice is constrained 
by features imposed by the gralnmar. 
For example the concept DN::draw is anno- 
tated with two lexical items which are the im- 
perfective and perfective forms of the verb draw 
in Czech, Bulgarian and Russian. If the gram- 
mar selects imperfective aspect, tim first is cho- 
sen; if the grammar selects perfective aspect, 
the second is chosen. This mechanism is used 
also fbr the choice between a verb and its nom- 
inalization, among others. With the help of the 
lexicon, the inflectional properties collected tbr 
a particular lexical item during generation are 
translated into a format suitable tbr external 
morphological modules, which are then called. 
The result of the external module, the inflected 
tbrm, is passed back to the KPML system and 
inserted into the grammatical structure. 
5 Eva luat ion  and Conc lus ions  
A first round of evaluation has been carried 
out on the Agile prototype. This directly as- 
sessed the ability of users to control multilin- 
3This description is also valid for Russian, which has a 
nominal group structure similar to Czech. The 13ulgarian 
one is more like English. 
gual generation in tile three languages, as well 
as the design and robustness of the system eom- 
1}onents. Groups of users were given a brief 
training period and then asked to construct 
A-boxes expressing iven content. Texts were 
cross-generated: i.e., the languages were w~ried 
across the A-boxes independently of the native 
languages of the subjects who created them. Er- 
rors were then classified and recommendations 
for the next and final Agile prototype collected. 
The generated texts were then evaluated by ex- 
pert technical authors. They were generally 
judged to be of a broadly similar quality to 
the texts originating from manuals, and both 
kinds of texts received similar criticism. The 
main source of criticism and errors was the de- 
sign of the GUI which is now being improved 
for the final prototype. The overall design of 
the system has theretbre shown itself to offer an 
etfective approach tbr multilingual generation. 
We are now extending the system to cover a 
broader ange of text types as well as the further 
grammatical and semantic variation required by 
the evaluators as well as by the additional text 
types. 
Re ferences  
Bateman, J. A., Matthiessen, C. M. I. M., & Zeng, L. 
(1999). Multilingual natural anguage generation 
for multilingual software: a flmctional inguistic 
approach. Applied Artificial hdelligencc, 13(6), 
607-639. 
Bateman, J. A. & Sharoff, S. (1998). Mult, ilingual 
grammars and multilingual lexicons for nmltilin- 
gual text; generation. In Mnltilinguality in the Icz- 
icon II, ECAI'98 Workshop 13, (pp. 1-8). 
Hajie, J. 8; Hladk?, B. (1997). Probabilistic and 
rule-based tagger of an inflective language a 
comparison. In Proceedings of ANLP'97, (pp. 
111-118). 
Mmm, W. C. & Matthiessen, C. M. I. M. (1985). 
Demonstration of the Nigel text generation com- 
puter progrmn. In J. D. Benson 8: W. S. Greaves 
(Eds.), Systemic Perspectives on Discourse, Vol- 
ume 1 (pp. 50-83). Ablex. 
Reiter, E. & Dale, R. (1997). Building applied natu- 
ral language generation systems. Journal of Nat- 
ural Language Engineering, 3, 57-87. 
Tcich, E. (1995). Towards a methodology for the 
construction of multilingual resources tbr multi- 
lingual text generation. In Proceedings of the I J- 
CAI'95 workshop on multilingual generation, (pp. 
136-148). 
480 
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 749?759,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fine-grained Genre Classification using Structural Learning Algorithms
Zhili Wu
Centre for Translation Studies
University of Leeds, UK
z.wu@leeds.ac.uk
Katja Markert
School of Computing
University of Leeds, UK
scskm@leeds.ac.uk
Serge Sharoff
Centre for Translation Studies
University of Leeds, UK
s.sharoff@leeds.ac.uk
Abstract
Prior use of machine learning in genre
classification used a list of labels as clas-
sification categories. However, genre
classes are often organised into hierar-
chies, e.g., covering the subgenres of fic-
tion. In this paper we present a method
of using the hierarchy of labels to improve
the classification accuracy. As a testbed
for this approach we use the Brown Cor-
pus as well as a range of other corpora, in-
cluding the BNC, HGC and Syracuse. The
results are not encouraging: apart from the
Brown corpus, the improvements of our
structural classifier over the flat one are
not statistically significant. We discuss the
relation between structural learning per-
formance and the visual and distributional
balance of the label hierarchy, suggesting
that only balanced hierarchies might profit
from structural learning.
1 Introduction
Automatic genre identification (AGI) can be
traced to the mid-1990s (Karlgren and Cutting,
1994; Kessler et al, 1997), but this research be-
came much more active in recent years, partly be-
cause of the explosive growth of the Web, and
partly because of the importance of making genre
distinctions in NLP applications. In Information
Retrieval, given the large number of web pages on
any given topic, it is often difficult for the users
to find relevant pages that are in the right genre
(Vidulin et al, 2007). As for other applications,
the accuracy of many tasks, such as machine trans-
lation, POS tagging (Giesbrecht and Evert, 2009)
or identification of discourse relations (Webber,
2009) relies of defining the language model suit-
able for the genre of a given text. For example,
the accuracy of POS tagging reaching 96.9% on
newspaper texts drops down to 85.7% on forums
(Giesbrecht and Evert, 2009), i.e., every seventh
word in forums is tagged incorrectly.
This interest in genres resulted in a prolifer-
ation of studies on corpus development of web
genres and comparison of methods for AGI. The
two corpora commonly used for this task are KI-
04 (Meyer zu Eissen and Stein, 2004) and San-
tinis (Santini, 2007). The best results reported for
these corpora (with 10-fold cross-validation) reach
84.1% on KI-04 and 96.5% accuracy on Santinis
(Kanaris and Stamatatos, 2009). In our research
(Sharoff et al, 2010) we produced even better re-
sults on these two benchmarks (85.8% and 97.1%,
respectively). However, this impressive accuracy
is not realistic in vivo, i.e., in classifying web
pages retrieved as a result of actual queries. One
reason comes from the limited number of genres
present in these two collections (eight genres in
KI-04 and seven in Santinis). As an example, only
front pages of online newspapers are listed in San-
tinis, but not actual newspaper articles, so once an
article is retrieved, it cannot be assigned to any
class at all. Another reason why the high accu-
racy is not useful concerns the limited number of
sources in each collection, e.g., all FAQs in Santi-
nis come from either a website with FAQs on hur-
ricanes or another one with tax advice. In the end,
a classifier built for FAQs on this training data re-
lies on a high topic-genre correlation in this par-
ticular collection and fails to spot any other FAQs.
There are other corpora, which are more diverse
in the range of their genres, such as the fifteen
genres of the Brown Corpus (Kuc?era and Fran-
cis, 1967) or the seventy genres of the BNC (Lee,
2001), but because of the number of genres in
them and the diversity of documents within each
genre, the accuracy of prior work on these collec-
tions is much less impressive. For example, Karl-
gren and Cutting (1994) using linear discriminant
analysis achieve an accuracy of 52% without us-
749
ing cross-validation (the entire Brown Corpus was
used as both the test set and training set), with the
accuracy improving to 65% when the 15 genres
are collapsed into 10, and to 73% with only 4 gen-
res (Figure 1). This result suggests the importance
of the hierarchy of genres. Firstly, making a deci-
sion on higher levels might be easier than on lower
levels (fiction or non-fiction rather than science
fiction or mystery). Secondly, we might be able
to improve the accuracy on lower levels, by taking
into account the relevant position of each node in
the hierarchy (distinguishing between reportage
or editorial becomes easier when we know they
are safely under the category of press).
Figure 1: Hierarchy of Brown corpus.
This paper explores a way of using information on
the hierarchy of labels for improving fine-grained
genre classification. To the best of our knowl-
edge, this is the first work presenting structural
genre classification and distance measures for gen-
res. In Section 2 we present a structural reformula-
tion of Support Vector Machines (SVMs) that can
take similarities between different genres into ac-
count. This formulation necessitates the develop-
ment of distance measures between different gen-
res in a hierarchy, of which we present three dif-
ferent types in Section 3, along with possible esti-
mation procedures for these distances. We present
experiments with these novel structural SVMs and
distance measures on three different corpora in
Section 4. Our experiments show that structural
SVMs can outperform the non-structural standard.
However, the improvement is only statistically sig-
nificant on the Brown corpus. In Section 5 we
investigate potential reasons for this, including
the (im)balance of different genre hierarchies and
problems with our distance measures.
2 Structural SVMs
Discriminative methods are often used for clas-
sification, with SVMs being a well-performing
method in many tasks (Boser et al, 1992;
Joachims, 1999). Linear SVMs on a flat list of
labels achieve high efficiency and accuracy in text
classification when compared to nonlinear SVMs
or other state-of-the-art methods. As for structural
output learning, a few SVM-based objective func-
tions have been proposed, including margin for-
mulation for hierarchical learning (Dekel et al,
2004) or general structural learning (Joachims
et al, 2009; Tsochantaridis et al, 2005). But many
implementations are not publicly available, and
their scalability to real-life text classification tasks
is unknown. Also they have not been applied to
genre classification.
Our formulation can be taken as a special in-
stance of the structural learning framework in
(Tsochantaridis et al, 2005). However, they con-
centrate on more complicated label structures as
for sequence alignment or parsing. They proposed
two formulations, slack-rescaling and margin-
rescaling, claiming that margin-rescaling has two
disadvantages. First, it potentially gives signifi-
cant weight to output values that might not be eas-
ily confused with the target values, because every
increase in the loss increases the required margin.
However, they did not provide empirical evidence
for this claim. Second, margin rescaling is not
necessarily invariant to the scaling of the distance
matrix. We still used margin-rescaling because it
allows us to use the sequential dual method for
large-scale implementation (Keerthi et al, 2008),
which is not applicable to the slack-rescaling for-
mulation. For web page classification we will
need fast processing. In addition, we performed
model calibration to address the second disadvan-
tage (distance matrix invariance).
Let x be a document and wm a weight vector
associated with the genre class m in a corpus with
k genres at the most fine-grained level. The pre-
dicted class is the class achieving the maximum
inner product between x and the weight vector for
the class, denoted as,
argmax
m
wTmx,?m. (1)
750
Accurate prediction requires that when a docu-
ment vector is multiplied with the weight vector
associated with its own class, the resulting inner
product should be larger than its inner products
with a weight vector for any other genre class m.
This helps us to define criteria for weight vectors.
Let xi be the i?th training document, and yi its
genre label. For its weight vector wyi , the inner
productwTyixi should be larger than all other prod-
ucts wTmxi, that is,
wTyixi ?w
T
mxi ? 0,?m. (2)
To strengthen the constraints, the zero value on the
right hand side of the inequality for the flat SVM
can be replaced by a positive value, corresponding
to a distance measure h(yi,m) between two genre
classes, leading to the following constraint:
wTyixi ?w
T
mxi ? h(yi,m),?m. (3)
To allow feasible models, in real scenarios such
constraints can be violated, but the degree of vio-
lation is expected to be small. For each document,
the maximum violation in the k constraints is of
interest, as given by the following loss term:
Lossi = max
m
{h(yi,m)?wTyixi +w
T
mxi}. (4)
Adding up all loss terms over all training docu-
ments, and further introducing a term to penalize
large values in the weight vectors, we have the
following objective function (C is a user-specified
nonnegative parameter).
min
m,i
:
1
2
k?
m=1
wTmwm + C
p?
i=1
Lossi. (5)
Efficient methods can be derived by borrowing the
sequential dual methods in (Keerthi et al, 2008)
or other optimization techniques (Crammer and
Singer, 2002).
3 Genre Distance Measures
The structural SVM (Section 2) requires a dis-
tance measure h between two genres. We can
derive such distance measures from the genre
hierarchy in a way similar to word similarity
measures that were invented for lexical hierar-
chies such as WordNet (see (Pedersen et al,
2007) for an overview). In the following,
we will first shortly summarise path-based and
information-based measures for similarity. How-
ever, information-based measures are based on
the information content of a node in a hierarchy.
Whereas the information content of a word or con-
cept in a lexical hierarchy has been well-defined
(Resnik, 1995), it is less clear how to estimate
the information content of a genre label. We will
therefore discuss several different ways of estimat-
ing information content of nodes in a genre hierar-
chy.
3.1 Distance Measures based on Path Length
If genre labels are organised into a tree (Figure 1),
one of the simplest ways to measure distance be-
tween two genre labels (= tree nodes) is path
length (h(a, b)plen):
f(a, LCS(a, b)) + f(b, LCS(a, b)), (6)
where a and b are two nodes in the tree,
LCS(a, b) is their Least Common Subsumer, and
f(a, LCS(a, b)) is the number of levels passed
through when traversing from a to the ancestral
node LCS(a, b). In other words, the distance
counts the number of edges traversed from nodes a
to b in the tree. For example, the distance between
Learned and Misc in Figure 1 would be 3.
As an alternative, the maximum path length
h(a, b)pmax to their least common subsumer can
be used to reduce the range of possible values:
max{f(a, LCS(a, b)), f(b, LCS(a, b))}. (7)
The Leacock & Chodorow similarity measure
(Leacock and Chodorow, 1998) normalizes the
path length measure (6) by the maximum number
of nodes D when traversing down from the root.
s(a, b)plsk = ?log((h(a, b)plen + 1)/2D). (8)
To convert it into a distance measure, we can
invert it h(a, b)plsk = 1/s(a, b)plsk.
Other path-length based measures include the
Wu & Palmer Similarity (Wu and Palmer, 1994).
s(a, b)pwupal =
2f(R,LCS(a, b))
(f(R, a) + f(R, b))
, (9)
where R describes the hierarchy?s root node. Here
similarity is proportional to the shared path from
the root to the least common subsumer of two
nodes. Since the Wu & Palmer similarity is always
between [0 1), we can convert it into a distance
measure by h(a, b)pwupal = 1? s(a, b)pwupal.
751
3.2 Distance Measures based on Information
Content
Path-based distance measures work relatively well
on balanced hierarchies such as the one in Figure 1
but fail to treat hierarchies with different levels
of granularity well. For lexical hierarchies, as a
result, several distance measures based on infor-
mation content have been suggested where the in-
formation content of a concept c in a hierarchy is
measured by (Resnik, 1995)
IC(c) = ?log(
freq(c)
freq(root)
). (10)
The frequency freq of a concept c is the sum of
the frequency of the node c itself and the frequen-
cies of all its subnodes. Since the root may be a
dummy concept, its frequency is simply the sum
of the frequencies of all its subnodes. The simi-
larity between two nodes can then be defined as
the information content of their least common sub-
sumer:
s(a, b)resk = IC(LCS(a, b)). (11)
If two nodes just share the root as their subsumer,
their similarity will be zero. To convert 11 into a
distance measure, it is possible to add a constant 1
to it before inverting it, as given by
h(a, b)resk = 1/(s(a, b)resk + 1). (12)
Several other similarity measures have been pro-
posed based on the Resnik similarity such as the
one by (Lin, 1998):
s(a, b)lin =
2IC(LCS(a, b))
IC(a) + IC(b)
. (13)
Again to avoid the effect of zero similarity when
defining the Lin?s distance we use:
h(a, b)lin = 1/(s(a, b)lin + 1). (14)
(Jiang and Conrath, 1997) directly define Jiang?s
distance (h(a, b)jng):
IC(a) + IC(b)? 2IC(LCS(a, b)). (15)
3.2.1 Information Content of Genre Labels
The notion of information content of a genre is not
straightforward. We use two ways of measuring
the frequency freq of a genre, depending on its
interpretation.
Genre Frequency based on Document Occur-
rence. We can interpret the ?frequency? of a
genre node simply as the number of all documents
belonging to that genre (including any of its sub-
genres). Unfortunately, there are no estimates for
genre frequencies on, for example, a representa-
tive sample of web documents. Therefore, we ap-
proximate genre frequencies from the document
frequencies (dfs) in the training sets used in clas-
sification. Note that (i) for balanced class distribu-
tions this information will not be helpful and (ii)
that this is a relatively poor substitute for an esti-
mation on an independent, representative corpus.
Genre Frequency based on Genre Labels. We
can also use the labels/names of the genre nodes
as the unit of frequency estimation. Then, the
frequency of a genre node is the occurrence fre-
quency of its label in a corpus plus the occurrence
frequencies of the labels of all its subnodes. Note
that there is no direct correspondence between this
measure and the document frequency of a genre:
measuring the number of times the potential genre
label poem occurs in a corpus is not in any way
equivalent to the number of poems in that corpus.
However, the measure is still structurally aware
as frequencies of labels of subnodes are included,
i.e. a higher level genre label will have higher
frequency (and lower information content) than a
lower level genre label.1
For label frequency estimation, we manually
expand any label abbreviations (such as "newsp"
for BNC genre labels), delete stop words and func-
tion words and then use two search methods. For
the search method word we simply search the fre-
quency of the genre label in a corpus, using three
different corpora (the BNC, Brown and Google
web search). As for the BNC and Brown cor-
pus some labels are very rarely mentioned, we for
these two corpora use also a search method gram
where all character 5-grams within the genre label
are searched for and their frequencies aggregated.
3.3 Terminology
Algorithms are prefixed by the kind of distance
measure they employ ? IC for Information con-
tent and p for path-based). If the measure is infor-
1Obviously when using this measure we rely on genre la-
bels which are meaningful in the sense that lower level labels
were chosen to be more specific and therefore probably rarer
terms in a corpus. The measure could not possibly be use-
ful on a genre hierarchy that would give random names to its
genres such as genre 1.
752
mation content based the specific measure is men-
tioned next, such as lin. The way for measuring
genre frequency is indicated last with df for mea-
suring via document frequency and word/gram
when measured via frequency of genre labels. If
frequencies of genre labels are used, the corpus
for counting the occurrence of genre labels is also
indicated via brown, bnc or the Web as estimated
by Google hit counts gg. Standard non-structural
SVMs are indicated by flat.
4 Experiments
4.1 Datasets
We use four genre-annotated corpora for genre
classification: the Brown Corpus (Kuc?era and
Francis, 1967), BNC (Lee, 2001), HGC (Stubbe
and Ringlstetter, 2007) and Syracuse (Crowston
et al, 2009). They have a wide variety of genre
labels (from 15 in the Brown corpus to 32 genres
in HGC to 70 in the BNC to 292 in Syracuse), and
different types of hierarchies.
4.2 Evaluation Measures
We use standard classification accuracy (Acc) on
the most fine-grained level of target categories in
the genre hierarchy.
In addition, given a structural distance H , mis-
classifications can be weighted based on the dis-
tance measure. This allows us to penalize incor-
rect predictions which are further away in the hi-
erarchy (such as between government documents
and westerns) more than "close" mismatches (such
as between science fiction and westerns). For-
mally, given the classification confusion matrix M
then each Mab for a 6= b contains the number
of class a documents that are misclassified into
class b. To achieve proper normalization in giv-
ing weights to misclassified entries, we can redis-
tribute a total weight k ? 1 to each row of H pro-
portionally to its values, where k is the number
of genres. That is, given g the row summation
of H , we define a weight matrix Q by normal-
izing the rows of H in a way given by Qab =
(k ? 1)hab/ga, a 6= b. We further assign a unit
value to the diagonal of Q. Then it is possible to
construct a structurally-aware measure (S-Acc):
S-Acc =
?
a
Maa/
?
a,b
MabQab. (16)
4.3 Experimental Setup
We compare structural SVMs using all path-based
and information-content based measures (see also
Section 3.3). As a baseline we use the accuracy
achieved by a standard "flat" SVM.
We use 10-fold (randomised) cross validation
throughout. In each fold, for each genre class 10%
of documents are used for testing. For the re-
maining 90%, a portion of 10% are sampled for
parameter tuning, leaving 80% for training. In
each round the validation set is used to help de-
termine the best C associated with Equation (5)
based on the validation accuracy from the candi-
date list 0.0001, 0.0005, 0.001, 0.005, 0.01,
0.05, 0.1, 0.5, 1. Note via this experiment setup,
all methods are tuned to their best performance.
For any algorithm comparison, we use a McNe-
mar test with the significance level of 5% as rec-
ommended by (Dietterich, 1998).
4.4 Features
The features used for genre classification are char-
acter 4-grams for all algorithms, i.e. each docu-
ment is represented by a binary vector indicating
the existence of each character 4-gram. We used
character n-grams because they are very easy to
extract, language-independent (no need to rely on
parsing or even stemming), and they are known
to have the best performance in genre classifica-
tion tasks (Kanaris and Stamatatos, 2009; Sharoff
et al, 2010).
4.5 Brown Corpus Results
The Brown Corpus has 500 documents and is or-
ganized in a hierarchy with a depth of 3. It
contains 15 end-level genres. In one experiment
in (Karlgren and Cutting, 1994) the subgenres un-
der fiction are grouped together, leading to 10 gen-
res to classify.
Results on 10-genre Brown Corpus. A stan-
dard flat SVM achieves an accuracy of 64.4%
whereas the best structural SVM based on Lin?s
information content distance measure (IC-lin-
word-bnc) achieves 68.8% accuracy, significantly
better at the 1% level. The result is also signif-
icantly better than prior work on the Brown cor-
pus in (Karlgren and Cutting, 1994) (who use the
whole corpus as test as well as training data). Ta-
ble 1 summarizes the best performing measures
that all outperform the flat SVM at the 1% level.
753
Table 1: Brown 10-genre Classification Results.
Method Accuracy
Karlgren and Cutting, 1994 65 (Training)
Flat SVM 64.40
SSVM(IC-lin-word-bnc) 68.80
SSVM(IC-lin-word-br) 68.60
SSVM(IC-lin-gram-br) 67.80
Figure 2 provides the box plots of accuracy scores.
The dashed boxes indicate that the distance mea-
sures perform significantly worse than the best
performing IC-lin-word-bnc at the bottom. The
solid boxes indicate the corresponding measures
are statistically comparable to the IC-lin-word-bnc
in terms of the mean accuracy they can achieve.
50 55 60 65 70 75 80
IC?lin?word?bncIC?lin?word?br
IC?jng?dfpwupal
IC?lin?gram?brIC?resk?word?bnc
IC?resk?word?ggplen
IC?resk?dfIC?lin?gram?bnc
IC?resk?gram?brIC?lin?df
IC?resk?gram?bncIC?resk?word?br
IC?lin?word?ggplsk
pmaxIC?jng?word?br
IC?jng?word?bncflat
IC?jng?gram?bncIC?jng?gram?br
IC?jng?word?gg
Accuracy
Figure 2: Accuracy on Brown Corpus (10 genres).
Results on 15-genre Brown Corpus. We per-
form experiments on all 15 genres on the end level
of the Brown corpus. The increase of genre classes
leads to reduced classification performance. In our
experiment, the flat SVM achieves an accuracy of
52.40%, and the structural SVM using path length
measure achieves 55.40%, a difference significant
at the 5% level. The structural SVMs using infor-
mation content measures IC-lin-gram-bnc and IC-
resk-word-br also perform equally well. In addi-
tion, we improve on the training accuracy of 52%
reported in (Karlgren and Cutting, 1994).
We are also interested in structural accuracy (S-
Acc) to see whether the structural SVMs make
fewer "big" mistakes. Table 2 shows a cross com-
parison of structural accuracy. Each row shows
how accurate the corresponding method is un-
der the structural accuracy criteria given in the
column. The ?no-struct? column corresponds to
vanilla accuracy. It is natural to expect each di-
agonal entry of the numeric table to be the high-
est, since the respective method is optimised for
its own structural distance. However, in our case,
Lin?s information content measure and the plen
measure perform well under any structural ac-
curacy evaluation measure and outperform flat
SVMs.
4.6 Other Corpora
In spite of the promising results on the Brown
Corpus, structural SVMs on other corpora (BNC,
HGC, Syracuse) did not show considerable im-
provement.
HGC contains 1330 documents divided into 32
approximately equally frequent classes. Its hierar-
chy has just two levels. Standard accuracy for the
best performing structural methods on HGC is just
the same as for flat SVM (69.1%), with marginally
better structural accuracy (for example, 71.39 vs.
71.04%, using a path-length based structural ac-
curacy). The BNC corpus contains 70 genres and
4053 documents. The number of documents per
class ranges from 2 to 501. The accuracy of SSVM
is also just comparable to flat SVM (73.6%). The
Syracuse corpus is a recently developed large col-
lection of 3027 annotated webpages divided into
292 genres (Crowston et al, 2009). Focusing only
on genres containing 15 or more examples, we ar-
rived at a corpus of 2293 samples and 52 genres.
Accuracy for flat (53.3%) and structural SVMs
(53.7%) are again comparable.
5 Discussion
Given that structural learning can help in topical
classification tasks (Tsochantaridis et al, 2005;
Dekel et al, 2004), the lack of success on genres
is surprising. We now discuss potential reasons for
this lack of success.
5.1 Tree Depth and Balance
Our best results were achieved on the Brown cor-
pus, whose genre tree has at least three attractive
properties. Firstly, it has a depth greater than 2,
i.e. several levels are distinguished. Secondly,
it seems visually balanced: branches from root
to leaves (or terminals) are of pretty much equal
length; branching factors are similar, for exam-
ple ranging between 2 and 6 for the last level of
branching. Thirdly, the number of examples at
754
Table 2: Structural Accuracy on Brown 15-genre Classification.
Method no-struct (=typical accuracy) IC-lin-gram-bnc plen IC-resk-word-br IC-jng-word-gg
flat 52.40 55.34 60.60 58.91 52.19
IC-lin-gram-bnc 55.00 58.15 63.59 61.83 53.85
plen 55.40 58.74 64.51 62.61 54.27
IC-resk-word-br 55.00 58.24 63.96 62.08 54.08
IC-jng-word-gg 46.00 49.00 54.89 53.01 52.58
each leaf node is roughly comparable (distribu-
tional balance).
The other hierarchies violate these properties to
a large extent. Thus, the genres in HGC are al-
most represented by a flat list with just one extra
level over 32 categories. Similarly, the vast ma-
jority of genres in the Syracuse corpus are also
organised in two levels only. Such flat hierar-
chies do not offer much scope to improve over a
completely flat list. There are considerably more
levels in the BNC for some branches, e.g., writ-
ten/national/broadsheet/arts, but many other gen-
res are still only specified to the second level of
its hierarchy, e.g., written/adverts. In addition, the
BNC is also distributionally imbalanced, i.e. the
number of documents per class varies from 2 to
501 documents.
To test our hypothesis, we tried to skew the
Brown genre tree in two ways. First, we kept the
tree relatively balanced visually and distribution-
ally but flattened it by removing the second layer
Press, Misc, Non-Fiction, Fiction from the hierar-
chy, leaving a tree with only two layers. Second,
we skewed the visual and distributional balance of
the tree by collapsing its three leaf-level genres un-
der Press, and the two under non-fiction, leading to
12 genres to classify (cf. Figure 1).
30 35 40 45 50 55 60 65 70
IC?resk?word?bncIC?resk?gram?bnc
IC?resk?word?brIC?lin?gram?bnc
plenpwupal
IC?lin?word?brIC?resk?word?gg
IC?lin?dfIC?lin?word?bnc
IC?lin?gram?brIC?jng?df
flatIC?resk?df
plskIC?resk?gram?br
pmaxIC?lin?word?gg
IC?jng?gram?bncIC?jng?gram?br
IC?jng?word?brIC?jng?word?bnc
IC?jng?word?gg
Accuracy
Figure 3: Accuracy on flattened Brown Corpus (15
genres).
35 40 45 50 55 60 65 70 75
IC?resk?word?brIC?resk?gram?bnc
pmaxIC?resk?gram?br
IC?resk?dfIC?lin?word?bnc
pwupalplen
IC?resk?word?bncplsk
IC?lin?gram?brflat
IC?lin?word?brIC?lin?df
IC?lin?gram?bncIC?jng?gram?br
IC?jng?dfIC?resk?word?gg
IC?lin?word?ggIC?jng?gram?bnc
IC?jng?word?brIC?jng?word?bnc
IC?jng?word?gg
Accuracy
Figure 4: Accuracy on skewed Brown Corpus (12
genres).
As expected, the structural methods on either
skewed or flattened hierarchies are not signifi-
cantly better than the flat SVM. For the flattened
hierarchy of 15 leaf genres the maximal accuracy
is 54.2% vs. 52.4% for the flat SVM (Figure 3), a
non-significant improvement. Similarly, the max-
imal accuracy on the skewed 12-genre hierarchy
is 58.2% vs. 56% (see also Figure 4), again a not
significant improvement.
To measure the degree of balance of a tree,
we introduce two tree balance scores based on
entropy. First, for both measures we extend all
branches to the maximum depth of the tree. Then
level by level we calculate an entropy score, ei-
ther according to how many tree nodes at the next
level belong to a node at this level (denoted as
vb: visual balance), or according to how many
end level documents belong to a node at this level
(denoted as db: distribution balance). To make
trees with different numbers of internal nodes
and leaves more comparable, the entropy score
at each level is normalized by the maximal en-
tropy achieved by a tree with uniform distribution
of nodes/documents, which is simply?log(1/N),
where N denotes the number of nodes at the corre-
755
sponding level. Finally, the entropy scores for all
levels are averaged. It can be shown that any per-
fect N-ary tree will have the largest visual balance
score of 1. If in addition its nodes at each level
contain the same number of documents, the distri-
bution balance score will reach the maximum, too.
Table 3 shows the balance scores for all the cor-
pora we use. The first two rows for the Brown cor-
pus have both large visual balance and distribution
balance scores. As shown earlier, for those two se-
tups the structural SVMs perform better than the
flat approach. In contrast, for the tree hierarchies
of Brown that we deformed or flattened, and also
BNC and Syracuse, either or both of the two bal-
ance scores tend to be lower, and no improvement
has been obtained over the flat approach. This
may indicate that a further exploration of the rela-
tion between tree balance and the performance of
structural SVMs is warranted. However, high vi-
sual balance and distribution scores do not neces-
sarily imply high performance of structural SVMs,
as very flat trees are also visually very balanced.
As an example, HGC has a high visual balance
score due to a shallow hierarchy and a high distri-
butional balance score due to a roughly equal num-
ber of documents contained in each genre. How-
ever, HGC did not benefit from structural learning
as it is also a very shallow hierarchy; therefore we
think that a third variable depth also needs to be
taken into account.
A similar observation on the importance of
well-balanced hierarchies comes from a recent
Pascal challenge on large scale hierarchical text
classification,2 which shows that some flat ap-
proaches perform competitively in topic classifi-
cation with imbalanced hierarchies. However, the
participants do not explore explicitly the relation
between tree balance and performance.
Other methods for measuring tree balance
(some of which are related to ours) are used in
the field of phylogenetic research (Shao and Sokal,
1990) but they are only applicable to visual bal-
ance. In addition, the methods they used often
provide conflicting results on which trees are con-
sidered as balanced (Shao and Sokal, 1990).
5.2 Distance Measures
We also scrutinise our distance measures as these
are crucial for the structural approach. We no-
tice that simple path length based measures per-
2http://lshtc.iit.demokritos.gr/
Table 3: Tree Balance Scores
Corpus depth vb db
Brown (10 genres) 3 0.9115 0.9024
Brown (15 genres) 3 0.9186 0.9083
Brown (15, flattened) 2 0.9855 0.8742
Brown (12, skewed) 3 0.8747 0.8947
HGC (32) 2 0.9562 0.9570
BNC (70) 4 0.9536 0.8039
Syracuse (52) 3 0.9404 0.8634
form well overall; again for the Brown corpus
this is probably due to its balanced hierarchy
which makes path length appropriate. There are
other probable reasons why information content
based measures do not perform better than path-
length based ones. When measured via docu-
ment frequency in a corpus we do not have suffi-
ciently large, representative genre-annotated cor-
pora to hand. When measured via genre label
frequency, we run into at least two problems.
Firstly, as mentioned in Section 3.2.1 genre la-
bel frequency does not have to correspond to class
frequency of documents. Secondly, the labels
used are often abbreviations (e.g. W_institut_doc,
W_newsp_brdsht_nat_social in BNC Corpus),
underspecified (other, misc, unclassified) or a col-
lection of phrases (e.g. belles letters, etc. in
Brown). This made search for frequency very ap-
proximate and also loosens the link between label
and content.
We investigated in more depth how well the dif-
ferent distance measures are aligned. We adapt
the alignment measure between kernels (Cristian-
ini et al, 2002), to investigate how close the dis-
tance matrices are. For two distance matrices H1
and H2, their alignment A(H1, H2) is defined as:
< H1, H2 >F?
< H1, H1 >F , < H2, H2 >F
, (17)
where < H1, H2 >F=
?k
i,j H1(gi, gj)H2(gi, gj)
which is the total sum of the entry-wise products
between the two distance matrices. Figure 5 shows
several distance matrices on the (original) 15 genre
Brown corpus. The plen matrix has clear blocks
for the super genres press, informative, imagina-
tive, etc. The IC-lin-gram-bnc matrix refines dis-
tances in the blocks, due to the introduction of in-
formation content. It keeps an alignment score that
is over 0.99 (the maximum is 1.00) toward the plen
matrix, and still has visible block patterns. How-
ever, the IC-jng-word-bnc significantly adjusts the
756
distance entries, has a much lower alignment score
with the plen matrix, and doesn?t reveal appar-
ent blocks. This partially explains the bad perfor-
mance of the Jiang distance measure on the Brown
corpus (see Section 4). The diagrams also show
the high closeness between the best performing IC
measure and the simple path length based mea-
sure.
plen
Informative Imaginative
Press
Misc
nonfiction
IC?lin?gram?bnc (0.98376)
Informative Imaginative
Press
Misc
nonfiction
plsk (0.96061)
Informative Imaginative
Press
Misc
nonfiction
IC?jng?word?bnc (0.92993)
Informative Imaginative
Press
Misc
nonfiction
Figure 5: Distance Matrices on Brown. Values in
bracket is the alignment with the plen matrix
An alternative to structural distance measures
would be distance measures between the gen-
res based on pairwise cosine similarities between
them. To assess this, we aggregated all character
4-gram training vectors of each genre and calcu-
lated standard cosine similarities. Note that these
similarities are based on the documents only and
do not make use of the Brown hierarchy at all. Af-
ter converting the similarities to distance, we plug
the distance matrix into our structural SVM. How-
ever, accuracy on the Brown corpus (15 genres)
was almost the same as for a flat SVM. Inspecting
the distance matrix visually, we determined that
the cosine similarity could clearly distinguish be-
tween Fiction and Non-Fiction texts but not be-
tween any other genres. This also indicates that
the genre structural hierarchy clearly gives infor-
mation not present in the simple character 4-gram
features we use. For a more detailed discussion
of the problems of the currently prevalently used
character n-grams as features for genre classifica-
tion, we refer the reader to (Sharoff et al, 2010).
6 Conclusions
In this paper, we have evaluated structural learn-
ing approaches to genre classification using sev-
eral different genre distance measures. Although
we were able to improve on non-structural ap-
proaches for the Brown corpus, we found it hard to
improve over flat SVMs on other corpora. As po-
tential reasons for this negative result, we suggest
that current genre hierarchies are either not of suf-
ficient depth or are visually or distributionally im-
balanced. We think further investigation into the
relationship between hierarchy balance and struc-
tural learning is warranted. Further investigation
is also needed into the appropriateness of n-gram
features for genre identification as well as good
measures of genre distance.
In the future, an important task would be the re-
finement or unsupervised generation of new hier-
archies, using information theoretic or data-driven
approaches. For a full assessment of hierarchical
learning for genre classification, the field of genre
studies needs a testbed similar to the Reuters or 20
Newsgroups datasets used in topic-based IR with a
balanced genre hierarchy and a representative cor-
pus of reliably annotated webpages.
With regard to algorithms, we are also inter-
ested in other formulations for structural SVMs
and their large-scale implementation as well as the
combination of different distance measures, for
example in ensemble learning.
Acknowledgements
We would like to thank the authors of each corpus
collection, who invested a lot of effort into produc-
ing them. We are also grateful to Google Inc. for
supporting this research via their Google Research
Awards programme.
References
Boser, B. E., Guyon, I. M., and Vapnik, V. N.
(1992). A training algorithm for optimal mar-
gin classifiers. In COLT ?92: Proceedings of
the fifth annual workshop on Computational
learning theory, pages 144?152, New York,
NY, USA. ACM.
Crammer, K. and Singer, Y. (2002). On the algo-
rithmic implementation of multiclass kernel-
based vector machines. J. Mach. Learn. Res.,
2:265?292.
Cristianini, N., Shawe-Taylor, J., and Kandola, J.
(2002). On kernel target algnment. In Pro-
ceedings of the Neural Information Process-
757
ing Systems, NIPS?01, pages 367?373. MIT
Press.
Crowston, K., Kwasnik, B., and Rubleske, J.
(2009). Problems in the use-centered de-
velopment of a taxonomy of web genres.
In Mehler, A., Sharoff, S., and Santini,
M., editors, Genres on the Web: Com-
putational Models and Empirical Studies.
Springer, Berlin/New York.
Dekel, O., Keshet, J., and Singer, Y. (2004).
Large margin hierarchical classification. In
ICML ?04: Proceedings of the twenty-first in-
ternational conference on Machine learning,
page 27, New York, NY, USA. ACM.
Dietterich, T. G. (1998). Approximate statistical
tests for comparing supervised classification
learning algorithms. Neural Computation,
10:1895?1923.
Giesbrecht, E. and Evert, S. (2009). Part-of-
Speech (POS) Tagging - a solved task? An
evaluation of POS taggers for the Web as
corpus. In Proceedings of the Fifth Web
as Corpus Workshop (WAC5), pages 27?35,
Donostia-San Sebasti?n.
Jiang, J. J. and Conrath, D. W. (1997). Semantic
similarity based on corpus statistics and lexi-
cal taxonomy. CoRR, cmp-lg/9709008.
Joachims, T. (1999). Making large-scale SVM
learning practical. In Sch?lkopf, B., Burges,
C., and Smola, A., editors, Advances in
Kernel Methods ? Support Vector Learning,
pages 41?56. MIT Press.
Joachims, T., Finley, T., and Yu, C.-N. (2009).
Cutting-plane training of structural svms.
Machine Learning, 77(1):27?59.
Kanaris, I. and Stamatatos, E. (2009). Learning to
recognize webpage genres. Information Pro-
cessing and Management, 45:499?512.
Karlgren, J. and Cutting, D. (1994). Recogniz-
ing text genres with simple metrics using dis-
criminant analysis. In Proc. of the 15th. Inter-
national Conference on Computational Lin-
guistics (COLING 94), pages 1071 ? 1075,
Kyoto, Japan.
Keerthi, S. S., Sundararajan, S., Chang, K.-W.,
Hsieh, C.-J., and Lin, C.-J. (2008). A se-
quential dual method for large scale multi-
class linear svms. In KDD ?08: Proceeding of
the 14th ACM SIGKDD international confer-
ence on Knowledge discovery and data min-
ing, pages 408?416, New York, NY, USA.
ACM.
Kessler, B., Nunberg, G., and Sch?tze, H. (1997).
Automatic detection of text genre. In Pro-
ceedings of the 35th ACL/8th EACL, pages
32?38.
Kuc?era, H. and Francis, W. N. (1967). Computa-
tional analysis of present-day American En-
glish. Brown University Press, Providence.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for
word sense identification, pages 305?332. In
C. Fellbaum (Ed.), MIT Press.
Lee, D. (2001). Genres, registers, text types, do-
mains, and styles: clarifying the concepts
and navigating a path through the BNC jun-
gle. Language Learning and Technology,
5(3):37?72.
Lin, D. (1998). An information-theoretic defini-
tion of similarity. In ICML ?98: Proceed-
ings of the Fifteenth International Confer-
ence on Machine Learning, pages 296?304,
San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Meyer zu Eissen, S. and Stein, B. (2004). Genre
classification of web pages. In Proceedings
of the 27th German Conference on Artificial
Intelligence, Ulm, Germany.
Pedersen, T., Pakhomov, S. V. S., Patwardhan, S.,
and Chute, C. G. (2007). Measures of seman-
tic similarity and relatedness in the biomed-
ical domain. J. of Biomedical Informatics,
40(3):288?299.
Resnik, P. (1995). Using information content to
evaluate semantic similarity in a taxonomy.
In IJCAI?95: Proceedings of the 14th inter-
national joint conference on Artificial intel-
ligence, pages 448?453, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
758
Santini, M. (2007). Automatic Identification of
Genre in Web Pages. PhD thesis, University
of Brighton.
Shao, K.-T. and Sokal, R. R. (1990). Tree balance.
Systematic Zoology, 39(3):266?276.
Sharoff, S., Wu, Z., and Markert, K. (2010). The
Web library of Babel: evaluating genre col-
lections. In Proc. of the Seventh Language
Resources and Evaluation Conference, LREC
2010, Malta.
Stubbe, A. and Ringlstetter, C. (2007). Recogniz-
ing genres. In Santini, M. and Sharoff, S.,
editors, Proc. Towards a Reference Corpus of
Web Genres.
Tsochantaridis, I., Joachims, T., Hofmann, T., and
Altun, Y. (2005). Large margin methods
for structured and interdependent output vari-
ables. J. Mach. Learn. Res., 6:1453?1484.
Vidulin, V., Lu?trek, M., and Gams, M. (2007).
Using genres to improve search engines. In
Proc. Towards Genre-Enabled Search En-
gines: The Impact of NLP. RANLP-07.
Webber, B. (2009). Genre distinctions for dis-
course in the Penn TreeBank. In Proc the
47th Annual Meeting of the ACL, pages 674?
682.
Wu, Z. and Palmer, M. (1994). Verbs seman-
tics and lexical selection. In Proceedings of
the 32nd annual meeting on Association for
Computational Linguistics, pages 133?138,
Morristown, NJ, USA. Association for Com-
putational Linguistics.
759
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 262?267,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
English?Russian MT evaluation campaign
Pavel Braslavski
Kontur Labs /
Ural Federal
University,Russia
pbras@yandex.ru
Alexander Beloborodov
Ural Federal University
Russia
xander-beloborodov
@yandex.ru
Maxim Khalilov
TAUS Labs
The Netherlands
maxim
@tauslabs.com
Serge Sharoff
University of Leeds
UK
s.sharoff
@leeds.ac.uk
Abstract
This paper presents the settings and the re-
sults of the ROMIP 2013 MT shared task
for the English?Russian language direc-
tion. The quality of generated translations
was assessed using automatic metrics and
human evaluation. We also discuss ways
to reduce human evaluation efforts using
pairwise sentence comparisons by human
judges to simulate sort operations.
1 Introduction
Machine Translation (MT) between English and
Russian was one of the first translation directions
tested at the dawn of MT research in the 1950s
(Hutchins, 2000). Since then the MT paradigms
changed many times, many systems for this lan-
guage pair appeared (and disappeared), but as far
as we know there was no systematic quantitative
evaluation of a range of systems, analogous to
DARPA?94 (White et al, 1994) and later evalua-
tion campaigns. The Workshop on Statistical MT
(WMT) in 2013 has announced a Russian evalua-
tion track for the first time.1 However, this evalu-
ation is currently ongoing, it should include new
methods for building statistical MT (SMT) sys-
tems for Russian from the data provided in this
track, but it will not cover the performance of ex-
isting systems, especially rule-based (RBMT) or
hybrid ones.
Evaluation campaigns play an important role
in promotion of the progress for MT technolo-
gies. Recently, there have been a number of
MT shared tasks for combinations of several Eu-
ropean, Asian and Semitic languages (Callison-
Burch et al, 2011; Callison-Burch et al, 2012;
Federico et al, 2012), which we took into account
in designing the campaign for the English-Russian
direction. The evaluation has been held in the
1http://www.statmt.org/wmt13/
context of ROMIP,2 which stands for Russian In-
formation Retrieval Evaluation Seminar and is a
TREC-like3 Russian initiative started in 2002.
One of the main challenges in developing MT
systems for Russian and for evaluating them is the
need to deal with its free word order and com-
plex morphology. Long-distance dependencies
are common, and this creates problems for both
RBMT and SMT systems (especially for phrase-
based ones). Complex morphology also leads
to considerable sparseness for word alignment in
SMT.
The language direction was chosen to be
English?Russian, first because of the availabil-
ity of native speakers for evaluation, second be-
cause the systems taking part in this evaluation are
mostly used in translation of English texts for the
Russian readers.
2 Corpus preparation
In designing the set of texts for evaluation, we
had two issues in mind. First, it is known that
the domain and genre can influence MT perfor-
mance (Langlais, 2002; Babych et al, 2007), so
we wanted to control the set of genres. Second,
we were aiming at using sources allowing distri-
bution of texts under a Creative Commons licence.
In the end two genres were used coming from two
sources. The newswire texts were collected from
the English Wikinews website.4 The second genre
was represented by ?regulations? (laws, contracts,
rules, etc), which were collected from the Web
using a genre classification method described in
(Sharoff, 2010). The method provided a sufficient
accuracy (74%) for the initial selection of texts un-
der the category of ?regulations,? which was fol-
lowed by a manual check to reject texts clearly
outside of this genre category.
2http://romip.ru/en/
3http://trec.nist.gov/
4http://en.wikinews.org/
262
The initial corpus consists of 8,356 original
English texts that make up 148,864 sentences.
We chose to retain the entire texts in the cor-
pus rather than individual sentences, since some
MT systems may use information beyond isolated
sentences. 100,889 sentences originated from
Wikinews; 47,975 sentences came from the ?reg-
ulations? corpus. The first 1,002 sentences were
published in advance to allow potential partici-
pants time to adjust their systems to the corpus for-
mat. The remaining 147,862 sentences were the
corpus for testing translation into Russian. Two
examples of texts in the corpus:
90237 Ambassadors from the United States of
America, Australia and Britain have all met
with Fijian military officers to seek insur-
ances that there wasn?t going to be a coup.
102835 If you are given a discount for booking
more than one person onto the same date and
you later wish to transfer some of the dele-
gates to another event, the fees will be recal-
culated and you will be asked to pay addi-
tional fees due as well as any administrative
charge.
For automatic evaluation we randomly selected
947 ?clean? sentences, i.e. those with clear sen-
tence boundaries, no HTML markup remains, etc.
(such flaws sometimes occur in corpora collected
from the Web). 759 sentences originated from
the ?news? part of the corpus, the remaining 188
came from the ?regulations? part. The sentences
came from sources without published translations
into Russian, so that some of the participating sys-
tems do not get unfair advantage by using them for
training. These sentences were translated by pro-
fessional translators. For manual evaluation, we
randomly selected 330 sentences out of 947 used
for automatic evaluation, specifically, 190 from
the ?news? part and 140 from the ?regulations? part.
The organisers also provided participants with
access to the following additional resources:
? 1 million sentences from the English-Russian
parallel corpus released by Yandex (the same
as used in WMT13)5;
? 119 thousand sentences from the English-
Russian parallel corpus from the TAUS Data
Repository.6
These resources are not related to the test corpus
of the evaluation campaign. Their purpose was
5https://translate.yandex.ru/corpus?
lang=en
6https://www.tausdata.org
to make it easier to participate in the shared task
for teams without sufficient data for this language
pair.
3 Evaluation methodology
The main idea of manual evaluation was (1) to
make the assessment as simple as possible for a
human judge and (2) to make the results of evalu-
ation unambiguous. We opted for pairwise com-
parison of MT outputs. This is different from
simultaneous ranking of several MT outputs, as
commonly used in WMT evaluation campaigns.
In case of a large number of participating sys-
tems each assessor ranks only a subset of MT out-
puts. However, a fair overall ranking cannot be al-
ways derived from such partial rankings (Callison-
Burch et al, 2012). The pairwise comparisons
we used can be directly converted into unambigu-
ous overall rankings. This task is also much sim-
pler for human judges to complete. On the other
hand, pairwise comparisons require a larger num-
ber of evaluation decisions, which is feasible only
for few participants (and we indeed had relatively
few submissions in this campaign). Below we also
discuss how to reduce the amount of human efforts
for evaluation.
In our case the assessors were asked to make a
pairwise comparison of two sentences translated
by two different MT systems against a gold stan-
dard translation. The question for them was to
judge translation adequacy, i.e., which MT output
conveys information from the reference translation
better. The source English sentence was not pre-
sented to the assessors, because we think that we
can have more trust in understanding of the source
text by a professional translator. The translator
also had access to the entire text, while the asses-
sors could only see a single sentence.
For human evaluation we employed the multi-
functional TAUS DQF tool7 in the ?Quick Com-
parison? mode.
Assessors? judgements resulted in rankings for
each sentence in the test set. In case of ties the
ranks were averaged, e.g. when the ranks of the
systems in positions 2-4 and 7-8 were tied, their
ranks became: 1 3 3 3 5 6 7.5 7.5. To
produce the final ranking, the sentence-level ranks
were averaged over all sentences.
Pairwise comparisons are time-consuming: n
7https://tauslabs.com/dynamic-quality/
dqf-tools-mt
263
Metric OS1 OS2 OS3 OS4 P1 P2 P3 P4 P5 P6 P7
Automatic metrics ALL (947 sentences)
BLEU 0.150 0.141 0.133 0.124 0.157 0.112 0.105 0.073 0.094 0.071 0.073
NIST 5.12 4.94 4.80 4.67 5.00 4.46 4.11 2.38 4.16 3.362 3.38
Meteor 0.258 0.240 0.231 0.240 0.251 0.207 0.169 0.133 0.178 0.136 0.149
TER 0.755 0.766 0.764 0.758 0.758 0.796 0.901 0.931 0.826 0.934 0.830
GTM 0.351 0.338 0.332 0.336 0.349 0.303 0.246 0.207 0.275 0.208 0.230
Automatic metrics NEWS (759 sentences)
BLEU 0.137 0.131 0.123 0.114 0.153 0.103 0.096 0.070 0.083 0.066 0.067
NIST 4.86 4.72 4.55 4.35 4.79 4.26 3.83 2.47 3.90 3.20 3.19
Meteor 0.241 0.224 0.214 0.222 0.242 0.192 0.156 0.127 0.161 0.126 0.136
TER 0.772 0.776 0.784 0.777 0.768 0.809 0.908 0.936 0.844 0.938 0.839
GTM 0.335 0.324 0.317 0.320 0.339 0.290 0.233 0.201 0.257 0.199 0.217
Table 1: Automatic evaluation results
cases require n(n?1)2 pairwise decisions. In thisstudy we also simulated a ?human-assisted? in-
sertion sort algorithm and its variant with binary
search. The idea is to run a standard sort algo-
rithm and ask a human judge each time a compar-
ison operation is required. This assumes that hu-
man perception of quality is transitive: if we know
that A < B and B < C, we can spare evaluation
of A and C. This approach also implies that sen-
tence pairs to judge are generated and presented to
assessors on the fly; each decision contributes to
selection of the pairs to be judged in the next step.
If the systems are pre-sorted in a reasonable way
(e.g. by an MT metric, under assumption that au-
tomatic pre-ranking is closer to the ?ideal? ranking
than a random one), then we can potentially save
even more pairwise comparison operations. Pre-
sorting makes ranking somewhat biased in favour
of the order established by an MT metric. For ex-
ample, if it favours one system against another,
while in human judgement they are equal, the final
ranking will preserve the initial order. Insertion
sort of n sentences requires n? 1 comparisons in
the best case of already sorted data and n(n?1)2 inthe worst case (reversely ordered data). Insertion
sort with binary search requires? n log n compar-
isons regardless of the initial order. For this study
we ran exhaustive pairwise evaluation and used its
results to simulate human-assisted sorting.
In addition to human evaluation, we also ran
system-level automatic evaluations using BLEU
(Papineni et al, 2001), NIST (Doddington,
2002), METEOR (Banerjee and Lavie, 2005),
TER (Snover et al, 2009), and GTM (Turian et
al., 2003). We also wanted to estimate the correla-
tions of these metrics with human judgements for
the English?Russian pair on the corpus level and
on the level of individual sentences.
4 Results
We received results from five teams, two teams
submitted two runs each, which totals seven par-
ticipants? runs (referred to as P1..P7 in the pa-
per). The participants represent SMT, RBMT,
and hybrid approaches. They included established
groups from academia and industry, as well as new
research teams. The evaluation runs also included
the translations of the 947 test sentences produced
by four free online systems in their default modes
(referred to as OS1..OS4). For 11 runs automatic
evaluation measures were calculated; eight runs
underwent manual evaluation (four online systems
plus four participants? runs; no manual evaluation
was done by agreement with the participants for
the runs P3, P6, and P7 to reduce the workload).
ID Name and information
OS1 Phrase-based SMT
OS2 Phrase-based SMT
OS3 Hybrid (RBMT+statistical PE)
OS4 Dependency-based SMT
P1 Compreno, Hybrid, ABBYY Corp
P2 Pharaon, Moses, Yandex&TAUS data
P3,4 Balagur, Moses, Yandex&news data
P5 ETAP-3, RBMT, (Boguslavsky, 1995)
P6,7 Pereved, Moses, Internet data
OS3 is a hybrid system based on RBMT with
SMT post-editing (PE). P1 is a hybrid system with
analysis and generation driven by statistical evalu-
ation of hypotheses.
264
All (330 sentences)
OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest)
3.159 3.350 3.530 3.961 4.082 5.447 5.998 6.473
News (190 sentences)
OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest)
2.947 3.450 3.482 4.084 4.242 5.474 5,968 6,353
Regulations (140 sentences)
P1 (highest) OS3 OS1 OS2 OS4 P5 P2 P4 (lowest)
3.214 3.446 3.596 3.793 3.864 5.411 6.039 6.636
Simulated dynamic ranking (insertion sort)
P1 (highest) OS1 OS3 OS2 OS4 P5 P4 P2 (lowest)
3.318 3.327 3.588 4.221 4.300 5.227 5.900 6.118
Simulated dynamic ranking (binary insertion sort)
OS1 (highest) P1 OS3 OS2 OS4 P5 P2 P4 (lowest)
2.924 3.045 3.303 3.812 4.267 5.833 5.903 6.882
Table 2: Human evaluation results
Table 1 gives the automatic scores for each
of participating runs and four online systems.
OS1 usually has the highest overall score (except
BLEU), it also has the highest scores for ?regula-
tions? (more formal texts), P1 scores are better for
the news documents.
14 assessors were recruited for evaluation (par-
ticipating team members and volunteers); the to-
tal volume of evaluation is 10,920 pairwise sen-
tence comparisons. Table 2 presents the rankings
of the participating systems using averaged ranks
from the human evaluation. There is no statisti-
cally significant difference (using Welch?s t-test at
p ? 0.05) in the overall ranks within the follow-
ing groups: (OS1, OS3, P1) < (OS2, OS4) < P5
< (P2, P4). OS3 (mostly RBMT) belongs to the
troika of leaders in human evaluation contrary to
the results of its automatic scores (Table 1). Sim-
ilarly, P5 is consistently ranked higher than P2 by
the assessors, while the automatic scores suggest
the opposite. This observation confirms the well-
known fact that the automatic scores underesti-
mate RBMT systems, e.g., (B?char et al, 2012).
To investigate applicability of the automatic
measures to the English-Russian language direc-
tion, we computed Spearman?s ? correlation be-
tween the ranks given by the evaluators and by
the respective measures. Because of the amount
of variation for each measure on the sentence
level, robust estimates, such as the median and
the trimmed mean, are more informative than
the mean, since they discard the outliers (Huber,
1996). The results are listed in Table 3. All mea-
sures exhibit reasonable correlation on the corpus
level (330 sentences), but the sentence-level re-
sults are less impressive. While TER and GTM
are known to provide better correlation with post-
editing efforts for English (O?Brien, 2011), free
word order and greater data sparseness on the sen-
tence level makes TER much less reliable for Rus-
sian. METEOR (with its built-in Russian lemma-
tisation) and GTM offer the best correlation with
human judgements.
The lower part of Table 2 also reports the results
of simulated dynamic ranking (using the NIST
rankings as the initial order for the sort operation).
It resulted in a slightly different final ranking of
the systems since we did not account for ties and
?averaged ranks?. However, the ranking is prac-
tically the same up to the statistically significant
rank differences in reference ranking (see above).
The advantage is that it requires a significantly
lower number of pairwise comparisons. Insertion
sort yielded 5,131 comparisons (15.5 per sentence;
56% of exhaustive comparisons for 330 sentences
and 8 systems); binary insertion sort yielded 4,327
comparisons (13.1 per sentence; 47% of exhaus-
tive comparisons).
Out of the original set of 330 sentences for
human evaluation, 60 sentences were evaluated
by two annotators (which resulted in 60*28=1680
pairwise comparisons), so we were able to calcu-
late the standard Kohen?s ? and Krippendorff?s ?
scores (Artstein and Poesio, 2008). The results of
inter-annotator agreement are: percentage agree-
ment 0.56, ? = 0.34, ? = 0.48, which is simi-
265
Sentence level Corpus
Metric Median Mean Trimmed level
BLEU 0.357 0.298 0.348 0.833
NIST 0.357 0.291 0.347 0.810
Meteor 0.429 0.348 0.393 0.714
TER 0.214 0.186 0.204 0.619
GTM 0.429 0.340 0.392 0.714
Table 3: Correlation to human judgements
lar to sentence ranking reported in other evaluation
campaigns (Callison-Burch et al, 2012; Callison-
Burch et al, 2011). It was interesting to see the
agreement results distinguishing the top three sys-
tems against the rest, i.e. by ignoring the assess-
ments for the pairs within each group, ? = 0.53,
which indicates that the judges agree on the dif-
ference in quality between the top three systems
and the rest. On the other hand, the agreement re-
sults within the top three systems are low: ? =
0.23, ? = 0.33, which is again in line with the re-
sults for similar evaluations between closely per-
forming systems (Callison-Burch et al, 2011).
5 Conclusions and future plans
This was the first attempt at making proper
quantitative and qualitative evaluation of the
English?Russian MT systems. In the future edi-
tions, we will be aiming at developing a new
test corpus with a wider genre palette. We
will probably complement the campaign with
Russian?English translation direction. We hope
to attract more participants, including interna-
tional ones and plan to prepare a ?light version?
for students and young researchers. We will also
address the problem of tailoring automatic evalu-
ation measures to Russian ? accounting for com-
plex morphology and free word order. To this
end we will re-use human evaluation data gath-
ered within the 2013 campaign. While the cam-
paign was based exclusively on data in one lan-
guage direction, the correlation results for auto-
matic MT quality measures should be applicable
to other languages with free word order and com-
plex morphology.
We have made the corpus comprising the source
sentences, their human translations, translations
by participating MT systems and the human eval-
uation data publicly available.8
8http://romip.ru/mteval/
Acknowledgements
We would like to thank the translators, asses-
sors, as well as Anna Tsygankova, Maxim Gubin,
and Marina Nekrestyanova for project coordina-
tion and organisational help. Research on corpus
preparation methods was supported by EU FP7
funding, contract No 251534 (HyghTra). Our spe-
cial gratitude goes to Yandex and ABBYY who
partially covered the expenses incurred on corpus
translation. We?re also grateful to the anonymous
reviewers for their useful comments.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Bogdan Babych, Anthony Hartley, Serge Sharoff, and
Olga Mudraya. 2007. Assisting translators in in-
direct lexical transfer. In Proc. of 45th ACL, pages
739?746, Prague.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June.
Hanna B?char, Rapha?l Rubino, Yifan He, Yanjun Ma,
and Josef van Genabith. 2012. An evaluation
of statistical post-editing systems applied to RBMT
and SMT systems. In Proceedings of COLING?12,
Mumbai.
Igor Boguslavsky. 1995. A bi-directional Russian-to-
English machine translation system (ETAP-3). In
Proceedings of the Machine Translation Summit V,
Luxembourg.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?al, Canada, June.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology, pages 138?145, San Diego, CA.
266
Marcelo Federico, Mauro Cettolo, Luisa Bentivogli,
Michael Paul, and Sebastian Stuker. 2012.
Overview of the IWSLT 2012 evaluation campaign.
In Proceedings of the International Workshop on
Spoken Language Translation (IWSLT), pages 12?
34, Hong Kong, December.
Peter J. Huber. 1996. Robust Statistical Procedures.
Society for Industrial and Applied Mathematics.
John Hutchins, editor. 2000. Early years in ma-
chine translation: Memoirs and biographies of pi-
oneers. John Benjamins, Amsterdam, Philadel-
phia. http://www.hutchinsweb.me.uk/
EarlyYears-2000-TOC.htm.
Philippe Langlais. 2002. Improving a general-purpose
statistical translation engine by terminological lexi-
cons. In Proceedings of Second international work-
shop on computational terminology (COMPUTERM
2002), pages 1?7, Taipei, Taiwan. http://acl.
ldc.upenn.edu/W/W02/W02-1405.pdf.
Sharon O?Brien. 2011. Towards predicting
post-editing productivity. Machine translation,
25(3):197?215.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Thomas J. Wat-
son Research Center.
Serge Sharoff. 2010. In the garden and in the jun-
gle: Comparing genres in the BNC and Internet.
In Alexander Mehler, Serge Sharoff, and Marina
Santini, editors, Genres on the Web: Computa-
tional Models and Empirical Studies, pages 149?
166. Springer, Berlin/New York.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268, Athens, Greece, March.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003.
Evaluation of machine translation and its evaluation.
In Proceedings of Machine Translation Summit IX,
New Orleans, LA, USA, September.
John S. White, Theresa O?Connell, and Francis
O?Mara. 1994. The ARPA MT evaluation method-
ologies: Evolution, lessons, and further approaches.
In Proceedings of AMTA?94, pages 193?205.
267
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101?112,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Design of a hybrid high quality machine translation system 
Kurt Eberle 
Johanna Gei? 
Mireia Ginest?-Rosell 
Bogdan Babych  
Anthony Hartley 
Reinhard Rapp  
Lingenio GmbH Serge Sharoff 
Karlsruher Stra?e 10 
69 126 Heidelberg, Germany 
Martin Thomas 
Centre for Translation Studies 
University of Leeds 
 Leeds, LS2 9JT, UK 
[k.eberle,j.geiss,m.ginesti-rosell] 
@lingenio.de 
[B.Babych,A.Hartley,R.Rapp, 
S.Sharoff,M.Thomas]@leeds.ac.uk  
  
 
 
Abstract 
This paper gives an overview of the 
ongoing FP7 project HyghTra (2010 ? 
2014). The HyghTra project is conducted 
in a partnership between academia and 
industry involving the University of Leeds 
and Lingenio GmbH (company). It adopts a 
hybrid and bootstrapping approach to the 
enhancement of MT quality by applying 
rule-based analysis and statistical 
evaluation techniques to both parallel and 
comparable corpora in order to extract 
linguistic information and enrich the lexical 
and syntactic resources of the underlying 
(rule-based) MT system that is used for 
analysing the corpora. The project places 
special emphasis on the extension of 
systems to new language pairs and 
corresponding rapid, automated creation of 
high quality resources. The techniques are 
fielded and evaluated within an existing 
commercial MT environment. 
1 Motivation 
Statistical Machine Translation (SMT) has been 
around for about 20 years, and for roughly half of 
this time SMT and the 'traditional' Rule-based 
Machine Translation (RBMT) have been seen as 
competing paradigms. During the last decade 
however, there is a trend and growing interest in 
combining the two methodologies. In our approach 
these two approaches are viewed as 
complementary. 
Advantages of SMT are low cost and robustness, 
but definite disadvantages of (pure) SMT are that it 
needs huge amounts of data, which for many 
language pairs are not available and are unlikely to 
become available in the future. Also, SMT tends to 
disregard important classificatory knowledge (such 
as morphosyntactic, categorical and lexical class 
features), which can be provided and used 
relatively easily within non-statistical 
representations.  
On the other hand, advantages of RBMT are that 
its (grammar and lexical) rules and information are 
understandable by humans and can be exploited for 
a lot of applications outside of translation 
(dictionaries, text understanding, dialogue systems, 
etc.).  
The slot grammar approach used in Lingenio 
systems (cf.  McCord 1989, Eberle 2001) is a 
prime example of such linguistically rich 
representations that can be used for a number of 
different applications. Fig.1 shows this by a 
visualization of (an excerpt of) the entry for the 
ambiguous German verb einstellen in the database 
that underlies (a)  the Lingenio translation 
products, where it links up with corresponding set 
of the transfer rules, and (b) Lingenio?s dictionary 
product TranslateDict, which is primarily intended 
for human translators.   
 
101
 
 
Fig 1 a) data base entry einstellen 
('translation' represents links between SL and T entries) 
 
 
 
 
Fig 1 b) product entry einstellen 
 
The obvious disadvantages of RBMT are high cost, 
weaknesses in dealing with incorrect input and in 
making correct choices with respect to ambiguous 
words, structures, and transfer equivalents. 
SMT output is often surprisingly good with respect 
to short distance collocations, but often misses 
correct choices are missed in cases where 
selectional restrictions take effect on distant words. 
RBMT output is generally good if the parser 
assigns the correct analysis to a sentence and  if the 
target words can be correctly chosen from the set 
of alternatives. However, in the presence of 
ambiguous words and structures, and where 
linguistic information is lacking, the decisions may 
be wrong. 
Given the complementarity of SMT and RBMT 
and their very different strengths and weaknesses, 
we take a view that an optimized MT architecture 
must comprise elements of both paradigms. The 
key issue therefore lies in the identification of such 
elements and how to connect them to each other. 
We propose a specific type of hybrid translation ? 
hybrid high quality translation (HyghTra), where 
core RBMT systems are created and enhanced by a 
range of reliable statistical techniques. 
 
2 Development Methodology 
Many hybrid systems described in the literature 
have attempted to put some analytical abstraction 
on top of an SMT kernel.1 In our view this is not 
the best option because, according to the 
underlying philosophy, SMT is linguistically 
ignorant at the beginning and learns all linguistic 
rules automatically from corpora. However, the 
extracted information is typically represented in 
huge data sets which are not readable by humans in 
a natural way. This means that this type of 
architecture does not easily provide interfaces for 
incorporating linguistic knowledge in a canonical 
and simple way. 
Thus we approach the problem from the other end, 
, integrating information derived from corpora 
using statistical methods into RBMT systems. 
Provided the underlying RBMT systems are 
linguistically sound and sufficiently modular in 
structure, we believe this to have greater potential 
for generating high quality output. 
We currently use and carry out the following work 
plan: 
 
(I) Creation of MT systems  
(with rule-based core MT information and 
statistical extension and training): 
(a) We start out with declarative analysis and 
generation components of the considered 
languages, and with basic bilingual dictionaries 
connecting to one another the entries of relatively 
small vocabularies comprising the most frequent 
words of each language in a given translation pair 
(cf. Fig 1 a). 
(b) Having completed this phase, we extend the 
dictionaries and train the analysis-, transfer- and 
generation-components of the rule-based core 
systems using monolingual and bilingual corpora.  
 
                                                           
1 A prominent early example is Frederking and 
colleagues (Frederking & Nirenburg, 1994). For an 
overview of  hybrid MT till the late nineties see Streiter 
et al (1999). More recent  approaches include Groves & 
Way (2006a, 2006b). Commercial implementations 
include AppTek (http://www.apptek.com) and Language 
Weaver (http://www.languageweaver.com). An ongoing 
MT important project investigating hybrid methods is 
EuroMatrixPlus (http://www.euromatrixplus.net/) 
102
(II) Error detection and improvement cycle:  
(a) We automatically discover the most frequent 
problematic grammatical constructions and 
multiword expressions for commercial RBMT and 
SMT systems using automatic construction-based 
evaluation as proposed in (Babych and Hartley, 
2009) and develop a framework for fixing 
corresponding grammar rules and extending 
grammatical coverage of the systems in a semi-
automatic way. This shortens development time for 
commercial MT and contributes to yielding 
significantly higher translation quality. 
 
(III) Extension to other languages: 
Structural similarity and translation by pivot 
languages is used to obtain extension to further 
languages: 
High-quality translation between closely related 
languages (e.g., Russian and Ukrainian or 
Portuguese and Spanish) can be achieved with 
relatively simple resources (using linguistic 
similarity, but also homomorphism assumptions 
with respect to parallel text, if available), while 
greater efforts are put into ensuring better-quality 
translation between more distant languages (e.g. 
German and Russian). According to our prior 
research (Babych et al, 2007b) the pipeline 
between languages of different similarity results in 
improved translation quality for a larger number of 
language pairs (e.g., MT from Portuguese or 
Ukrainian into German is easier if there are high-
quality analysis and transfer modules for Spanish 
and Russian into German (respectively). Of course, 
(III) draws heavily on the detailed analysis and MT 
systems that the industrial partner in HyghTra 
provides for a number of languages. 
 
In the following sections we give more details of 
the work currently done with regard to (I) and with 
regard to parts of (II): the creation of a new MT 
system following the strategy sketched. We cannot 
go further into detail with (II) and (III) here, which 
will become a priority for future research. 
3 Creation of a new system 
Early pilot studies covering some aspects of the 
strategy described here (using information from 
pivot languages and similarity) showed promising 
results (Rapp, 1999; Rapp & Mart?n Vide, 2007; 
see also Koehn & Knight, 2002). 
We expect that the proposed semi-automatic 
creation of a new MT system as sketched above 
will work best if one of the two languages involved 
is already 'known' by modules to which the system 
has access. Against the background of the pipeline 
approach mentioned above in (III), this means that 
we assume an analysis and translation system that 
continuously grows by 'learning' new languages 
where 'learning' is facilitated by information about 
the languages already 'known' and by exploiting 
similarity assumptions ? and, of course, by being 
fed with information prepared and provided by the 
human 'companion' of the system. 
From this perspective, we assume the following 
steps of extending the system (with work done by 
the 'companion' and work done by the system) 
 
1. Acquire parallel and comparable corpora. 
2. Define a core of the morphology of the new 
language and compile a basic dictionary for the 
most frequent words and translations. 
Morphological representations and features for 
new languages are derived both manually and 
automatically, as proposed in (Babych et al, 
2012 (in preparation)). 
3. Using established alignment technology (e.g. 
Giza++) and parallel corpora, generate a first 
extension of this dictionary. 
4. Expand the dictionary of step 3 using 
comparable corpora as proposed in a study by 
Rapp (1999). This is applicable mainly to single 
word units. 
5. Expand coverage of multiword-units using 
novel technology. 
6. Cross-validate the new dictionary with respect 
to available ones by transitivity. 
7. Integrate the new dictionary into the new MT 
system as developing from reusing components 
and adding new components as in 8. 
8. Complete morphology and spell out declarative 
analysis and generation grammar for the new 
language. 
9. Automatically evaluate the translations of the 
most frequent grammatical constructions and 
multiword expressions in a machine-translated 
corpus, prioritising support for these 
constructions with a type of risk-assessment 
framework proposed in Babych and Hartley 
(2008). 
10. Extend support for high-priority constructions 
semi-automatically by mining correct 
103
translations from parallel corpora. 
11. Train and evaluate the new grammar and 
transfer of the new MT system using the new 
dictionary on the basis of available parallel 
corpora. 
 
The following sections give an overview of the 
different steps. 
Step 1: Acquire parallel and comparable 
corpora 
As our parallel corpus, we use the Europarl. The 
size of the current version is up to 40 million 
words per language, and several of the languages 
we are currently considering are covered. Also, we 
make use of other parallel corpora such as the 
Canadian Hansards (Proceedings of the Canadian 
Parliament) for the English?French language pair. 
For non-EU Languages (mainly Russian), we 
intend to conduct a pilot study to establish the 
feasibility of retrieving parallel corpora from the 
web, a problem for which various approaches have 
been proposed (Resnik, 1999; Munteanu & Marcu, 
2005; Wu & Fung, 2005).  
In addition to the parallel corpora, we will need 
large monolingual corpora in the future (at least 
200 million words) for each of the six languages. 
Here, we intend to use newspaper corpora 
supplemented with text collections downloadable 
from the web.  
The corpora are stored in a database that allows 
for assigning analyses of different depth and nature 
to the sentences and for alignment between the 
sentences and their analyses. The architecture of 
this database and the corresponding analysis and 
evaluation frontend is described in (Eberle et al
2010, 2012). Section Results contains examples of 
such representations. 
Step 2: Compile a basic dictionary for the most 
frequent words 
A prerequisite of the suggested hybrid approach 
with rule-based kernel is to define morphological 
classifications for the new language(s). This is 
done exploiting similarities to the classifications as 
available for the existing languages. Currently, this 
has been carried out for Dutch (on the basis of 
German) and for Spanish (on the basis of 
French/other Romance languages). The most 
frequent words (the basic vocabulary of a 
language) are typically also the most ambiguous 
ones. Since the Lingenio systems are lexically 
driven transfer systems (cf. Eberle 2001), we 
define (a) structural conditions,  which inform the 
choice of the possible target words (single words 
or multiword expressions) and (b)restructuring 
conditions, as necessary (cf. Fig 1 a:  attributes 
'transfer conditions' and 'structural change'). In 
order to ensure quality this must be done by human 
lexicographers and therefore costly for a large 
dictionary. However, we manually create only very 
small basic dictionaries and extend these (semi-
automatically) step 3 and those which follow. 
Some important morphosyntactic features of the 
language are derived from a monolingual corpus 
annotated with publicly available part-of-speech 
taggers and lemmatisers. However, these tools 
often do not explicitly represent linguistic features 
needed for the generation stage in RBMT. In 
(Babych et al, 2012) we propose a systematic 
approach to recovering such missing generation-
oriented representations from grammar models and 
statistical combinatorial properties of annotated 
features. 
Step 3: Generating dictionary extensions from 
parallel corpora 
Based on parallel corpora, dictionaries can be 
derived using established techniques of automatic 
sentence alignment and word alignment. For 
sentence alignment, the length-based Gale & 
Church aligner (1993) can be used, or ? 
alternatively ? Dan Melamed?s GSA-algorithm 
(Geometric Sentence Alignment; Melamed, 1999).  
For segmentation of text we use corresponding 
Lingenio-tools (unpublished).2 
For word alignment Giza++ (Och & Ney, 2003) is 
the standard tool. Given a word alignment, the 
extraction of a (SMT) dictionary is relatively 
straightforward. With the exception of sentence 
segmentation, these algorithms are largely 
language independent and can be used for all of the 
languages that we consider. We did this for a 
number of language pairs on the basis of the 
                                                           
2  If these cannot be applied because of  lack of 
information about a language, we intend to use the 
algorithm by Kiss & Strunk (2006). An open-source 
implementation of parts of the Kiss & Strunk algorithm 
is available from Patrick Tschorn at 
http://www.denkselbst.de/sentrick/index.html. 
104
Europarl-texts considered (as stored in our 
database). In order to optimize the results we use 
the dictionaries of step 1 as set of cognates (cf. 
Simard at al 1992, Gough & Way 2004), as well as 
other words easily obtainable from the internet that 
can be used for this purpose (like company names 
and other named entities with cross-language 
identity and terminology translations). Using the 
morphology component of the new language and 
the categorial information from the transfer 
relation, we compute the basic forms of the 
inflected words found. Later, we intend to further 
improve the accuracy of word alignment by 
exploiting chunk type syntactic information of the 
narrow context of the words (cf. Eberle & Rapp 
2008). An early stage variant of this is already used 
in Lingenio products. The corresponding function 
AutoLearn<word> extracts new word relations on 
the basis of existing dictionaries and (partial) 
syntactic analyses. (Fig 2 gives an example). 
 
 
 
 
 
 
 
 
 
 
Fig 2 AutoLearn<word>: new entries using 
transfer links and syntactic analysis 
 
Given the relatively small size of the available 
parallel corpora, we expect that the automatically 
generated dictionaries will comprise about 20,000 
entries each (This corresponds to first results on 
the basis of German?English). This is far too 
small for a serious general purpose MT system. 
Note that, in comparison, the English?German 
dictionary used in the current Lingenio MT 
product comprises more than 480,000 keywords 
and phrases. 
Step 4: Expanding dictionaries using 
comparable corpora (word equations) 
In order to expand the dictionaries using a set of 
monolingual comparable corpora, the basic 
approach pioneered by Fung & McKeown (1997) 
and Rapp (1995, 1999) is to be further developed 
and refined in the second phase of the project as to 
obtain a practical tool that can be used in an 
industrial context. 
The basic assumption underlying the approach 
is that across languages there is a correlation 
between the co-occurrences of words that are 
translations of each other. If ? for example ? in a 
text of one language two words A and B co-occur 
more often than expected by chance, then in a text 
of another language those words that are 
translations of A and B should also co-occur more 
frequently than expected. It is further assumed that 
a small dictionary (as generated in step 2) is 
available at the beginning, and that the aim is to 
expand this basic lexicon. Using a corpus of the 
target language, first a co-occurrence matrix is 
computed whose rows are all word types occurring 
in the corpus and whose columns are all target 
words appearing in the basic lexicon. Next a word 
of the source language is considered whose 
translation is to be determined. Using the source-
language corpus, a co-occurrence vector for this 
word is computed. Then all known words in this 
vector are translated to the target language. As the 
basic lexicon is small, only some of the 
translations are known. All unknown words are 
discarded from the vector and the vector positions 
are sorted in order to match the vectors of the 
target-language matrix. Using standard measures 
for vector similarity, the resulting vector is 
compared to all vectors in the co-occurrence 
matrix of the target language. The vector with the 
highest similarity is considered to be the 
translation of our source-language word. 
From a previous pilot study (Rapp, 1999) it can 
be expected that this methodology achieves an 
accuracy in the order of 70%, which means that 
only a relatively modest amount of manual post-
editing is required.  
The automatically generated results are 
improved and the amount of post-editing is 
reduced by exploiting sense (disambiguation) 
information as available from the analysis 
component for the 'known' language of the new 
language pair.. Also we try to exploit categorial 
and underspecified syntactic information of the 
contexts of the words similar to what has been 
suggested for improving word alignment in the 
previous step (see also Fig.2). Also, as the frequent 
words are already covered by the basic lexicon 
(whose production from parallel corpora on the 
basis of a manually compiled kernel does not show 
 
105
an ambiguity problem of similar significance), and 
as experience shows that most low frequency 
words in a full-size lexicon tend to be 
unambiguous, the ambiguity problem is reduced 
further for the words investigated and extracted by 
this comparison method. 
Step 5: Expanding dictionaries using 
comparable corpora (multiword units) 
In order to account for technical terms, idioms, 
collocations, and typical short phrases, an 
important feature of an MT lexicon is a high 
coverage of multiword units. Very recent work 
conducted at the University of Leeds (Sharoff et 
al., 2006) shows that dictionary entries for such 
multiword units can be derived from comparable 
corpora if a dictionary of single words is available. 
It could even be shown that this methodology can 
be superior to deriving multiword-units from 
parallel corpora (Babych et al, 2007). This is a 
major breakthrough as comparable corpora are far 
easier to acquire than parallel corpora. It even 
opens up the possibility of building domain-
specific dictionaries by using texts from different 
domains. 
The outline of the algorithm is as follows: 
? Extract collocations from a corpus of the 
source language (Smadja, 1993) 
? To translate a collocation, look up all its 
words using any dictionary 
? Generate all possible permutations 
(sequences) of the word translations 
? Count the occurrence frequencies of these 
sequences in a corpus of the target 
language and test for significance 
? Consider the most significant sequence to 
be the translation of the source language 
collocation 
Of course, in later steps of the project, we will 
experiment on filtering these sequences by 
exploiting structural knowledge similarly to what 
was described in the two previous steps. This can 
be obtained on the basis of the declarative analysis 
component of the new language which is 
developed in parallel. 
Step 6: Cross-validate dictionaries 
The combination of the corpus-based methods for 
automatic dictionary generation as described in 
steps 3 to 5 will lead to high coverage dictionaries 
as the availability of very large monolingual 
corpora is no major problem for our languages. 
However, as all steps are error prone, it can be 
expected that a considerable number of dictionary 
entries (e.g. 50%) are not correct. To facilitate (but 
not eliminate) the manual verification of the 
dictionary, we will  perform an automatic cross-
check which utilizes the dictionaries? property of 
transitivity. What we mean by this is that if we 
have two dictionaries, one translating from 
language A to language B, the other from language 
B to language C, then we can also translate from 
language A to C by use of the intermediate 
language (or interlingua) B. That is, the property of 
transitivity, although having some limitations due 
to ambiguity problems, can be exploited to 
automatically generate a raw dictionary for A to C. 
Lingenio  has some experience with this method 
having exploited it for extending and improving its 
English ? French dictionaries using French ? 
German and German ? English. 
As the corpus-based approach (steps 3 to 5) 
allows us to also generate this type of dictionary  
via comparable corpora, we have two different 
ways to generate a dictionary for a particular 
language pair. This means that we can validate one 
with the other. Furthermore, with increasing 
number of language pairs created, there are more 
and more languages that can serve as interlingua or 
'pivot': This, step by step, gives an increasing 
potential for mutual cross-validation.  
Specific attention will be paid to automating as 
far as possible the creation of selectional 
restrictions to be assigned to the transfer relations 
of the new dictionaries in all steps of dictionary 
creation (2?6). We will try to do this on the basis 
of the analysis components as available for the 
languages considered: These are: a completely 
worked out analysis component for the 'old' 
language, a declarative (chunk parsing) component 
for the new one (compare the two following steps 
for this).  
Step 7: Integrate dictionaries in existing 
machine translation systems 
Lingenio has a relatively rich infrastructure for 
automatic importation of various kinds of lexical 
information into the database used by the analyses 
and translation systems. If necessary the 
information on hand (for instance from 
conventional dictionaries of publishing houses) is 
106
completed and normalized during or before 
importation. This may be executed completely 
automatically ? by using the existing analyses 
components and resources respectively as 
databases ? or interactively ? by asking the 
lexicographer for additional information, if needed. 
For example, there may be a list of multiword 
expressions to be imported into the database. In 
order to have available correct syntactic and 
semantic information for these expressions, they 
are analysed by the parser of the corresponding 
language. From the analysis found, the information 
necessary to describe the new lemma in the lexicon 
with respect to semantic type and syntactic 
structure is obtained. The same information is used 
to automatically create correct restructuring 
constraints for translation relations which use the 
new lemma as target. If the parser does not find a 
sound syntactic description, for example because 
some basic information or the expression is 
missing in the lexical database, the lexicographer is 
asked for the missing information or is handed 
over the expression to code it manually.  
Using these tools importation of new lexical 
information, as provided in the previous steps, is 
considerably accelerated.  
Step 8: Compile rule bases for new language 
pairs 
Although experience clearly shows that 
construction and maintenance of the dictionaries is 
by far the most expensive task in (rule-based) 
Machine Translation, the grammars (analysis and 
generation) must of course be developed and 
maintained also. Lingenio has longstanding 
experience with the development of grammars, 
dictionaries and all other components of RBMT.  
The used grammar formalism (slot grammar, 
cf. McCord 1991) is unification based and its 
structuring focuses on dependency, where phrases 
are analysed into heads and grammatical roles ? so 
called (complement and adjunct) slots.  
The grammar formalism and basic rule types 
are designed in a very general way in order to 
allow good portability from one language to 
another such that spelling out the declarative part 
of a grammar does not take very much time (2-4 
person months approx. for relatively similar 
languages like Romance languages according to 
our experience). The portation of linguistic rules to 
new languages is also facilitated by the modular 
design with clearly defined interfaces that make it 
relatively straightforward to integrate information 
from corpora. 
Given a parallel corpus as acquired in step 1, 
the following procedure defines grammar develop-
ment:  
 
1. Define a declarative grammar for the new 
language and train this grammar on the parallel 
-corpus according to the following steps: 
2. Use a chunk parser for the grammar on the 
basis of an efficient part-of-speech tagger for 
the new language.  
3. Combine the chunk analyses of the sentence, 
according to suggestions for packed syntactic 
structures (cf. Schiehlen 2001 and others) and 
underspecified representation structures 
respectively (cf. Eberle, 2004, and others), 
such that the result represents a disjunction of 
the possible analyses of the sentence. 
4. Filter the alternatives of the representation by 
using mapping constraints between source and 
target sentence as can be computed from the 
lexical transfer relations and the structural 
analysis of the sentence. For instance, if we 
know, as in the example of the last section, that 
in the source sentence there is a relative clause 
with lexical elements A, B, . . . modifying a 
head H and that there are translations TH, TA, 
TB, . . . of H, A, B,. . . , in the target sentence 
which, among other possibilities, can be 
supposed to stand in a similar structural 
relation there, then we prefer this relation to 
the competing structural possibilities. (Fig. 3 in 
section results shows the corresponding 
selection for a German-Spanish example in the 
project database). 
5. For each of the remaining structural 
possibilities of the thus revised underspecified 
representation, take its lexical material and 
underspecified structuring as a context for its 
successful firing. For instance, if the 
possibility is left that O is the direct object of 
VP, where VP is an underspecified verbal 
phrase and O an underspecified nominal 
phrase (i.e. where details of the substructuring 
are not spelled out), take the sentence as a 
reference for direct object complementation 
and O and VP as contexts which accept this 
complementation. 
107
6. Develop more abstract conditions from the 
conditions learned according to (5) and 
integrate the different cases. 
7. Tune the results using standard methods of 
corpus-based linguistics. Among other things 
this means: Distinguish between training and 
test corpora, adjust weights according to the 
results of test runs, etc. 
 
The basic idea of the proposed learning procedure 
is similar to that used with respect to learning 
lexical transfer relations: Do not define the 
statistical model for the ?ignorant? state, where the 
surface items of the bilingual corpora are 
considered. Instead, define it for appropriate 
maximally abstract analyses of the sentences 
(which, of course, must be available 
automatically), because, then, much smaller sets of 
data will do. Here, the important question is: What 
is the most abstract level of representation that can 
be reached automatically and which shows reliable 
results? We think that it is the level of 
underspecified syntactic description as used in the 
procedure above. 
The result of training the grammar is a set of 
rules which assign weights and contexts to each 
filler rule of the declarative grammar and thus 
allow to estimate how likely it is that a particular 
rule is applied in a particular context in comparison 
with other rules (Fig. 4 and 5 in section results 
give an overview of the relevance of  grammar 
rules and their triggering conditions w.r.t. 
German).  
We mentioned that the task of translating texts 
into each other does not presuppose that each 
ambiguity in a source sentence is resolved. On the 
contrary, translation should be ambiguity 
preserving (cf. Kay, Gawron & Norvig 1994, 
compare the example above). It is obvious that 
underspecified syntactic representations as 
suggested here are also especially suited for 
preserving ambiguities appropriately.  
Step 9: Automatically evaluate translations of 
the most frequent grammatical constructions 
and multiword expressions in a machine-
translated corpus 
In a later work package of the project, we will run 
a large parallel corpus through available 
(competitive) MT engines, which will be enhanced 
by automatic dictionaries developed during the 
previous stages. On the source-language side of the 
corpus we will automatically generate lists of 
frequent multiword expressions (MWEs) and 
grammatical constructions using the methodology 
proposed in (Sharoff et al, 2006). For each of the 
identified MWEs and constructions we will 
generate a parallel concordance using open-source 
CSAR architecture developed by the Leeds team 
(Sharoff, 2006). The concordance will be 
generated by running queries to the sentence-
aligned parallel corpora and will return lists of 
corresponding sentences from gold-standard 
human translations and corresponding sentences 
generated by MT. Each of these concordances will 
be automatically evaluated using standard MT 
evaluation metrics, such as BLEU. Under these 
settings parallel concordances will be used as 
standard MT evaluation corpora in an automated 
MT evaluation scenario. 
Normally BLEU gives reliable results for MT 
corpora over 7000 words. However, in (Babych 
and Hartley, 2009; Babych and Hartley, 2008) we 
demonstrated that if the corpus is constructed in 
this controlled way, where evaluated fragments of 
sentences are selected as local contexts for specific 
multiword expressions or grammatical 
constructions, then BLEU scores have another 
?island of stability? for much smaller corpora, 
which now may consist of only five or more 
aligned concordance lines. This concordance-based 
evaluation scenario gives correct predictions of 
translation quality for the local context of each of 
the evaluated expressions. 
The scores for the evaluated MWEs and 
constructions will be put in a risk-assessment 
framework, where we will balance the frequency 
of constructions and their translation quality. The 
top priority receive the most frequent expressions 
that are the most problematic ones for a particular 
MT engine, i.e., with queries with lowest BLEU 
scores for their concordances. This framework will 
allow MT developers to work down the priority list 
and correct or extend coverage for those 
constructions which will have the biggest impact 
on MT quality. 
Step 10: Extend support for high-priority 
constructions semi-automatically by mining 
correct translations from parallel corpora 
At this stage we will automate the procedure of 
correcting errors and extending coverage for 
108
problematic MWEs and grammatical 
constructions, identified in Step 9. For this we will 
exploit alignment between source-language 
sentences and gold-standard human translations. In 
the target human translations we will identify 
linguistically-motivated multiword expressions, 
e.g., using part-of-speech patterns or tf-idf 
distribution templates (Babych et al, 2007) and 
run standard alignment tools (e.g., GIZA++) for 
finding the most probable candidate MWEs that 
correspond to the problematic source-language 
expressions. Source and target MWEs paired in 
this way will form the basis for automatically-
generated grammar rules. The rules will normally 
generalise several pairs of MWEs, and may be 
underspecified for certain lexical or morphological 
features. Later such rules will be manually checked 
and corrected by language specialists in MT 
development teams that work on specific 
translation directions. 
This procedure will allow to speed up the grammar 
development procedure for large-scale MT projects 
and will focus on grammatical constructions with 
the highest impact on MT quality, establishing 
them as a top priority for MT developers. In 
HyghTra and with respect to the languages 
considered there, this procedure will be integrated 
into the grammar development and optimization of 
step 8, in particular it will be related to step 4 of 
the procedure sketched there. With regard to 
integration, we aim at an interleaved architecture in 
the long run.  
Step 11: Bootstrap the system 
In Step 11, the new grammar and the transfer of 
the new MT system and the new dictionary may be 
mutually trained further using the steps before and 
applying the system to additional corpora. 
 
4 Results 
Declarative slot grammars for Dutch and Spanish 
have been developed using the patterns of German 
and French ? where declarative  means that there 
has been used no relevant semantic or other 
information in order to spell out weighting or 
filters for rule application -- the only constraint 
being morphosyntactic accessibility. The necessary 
morphological information has been adapted 
similarly from the corresponding model languages. 
The basic dictionaries have been compiled 
manually (Dutch) or extracted from a conventional 
electronic dictionary (translateDict Spanish).  
For a subset of the Spanish corpus (reference 
sentences of the grammar, parts of the open source 
Leeds corpus (Sharoff, 2006), and Europarl), 
syntactic analyses have been computed and stored 
in the database. As the number of analyses grows 
extremely with the length of sentences, only 
relatively short sentences (up to 15 words)  have 
been considered. These analyses are currently 
compared to the analyses of the German 
translations of the corresponding sentences (one 
translation per sentence), which are taken as a kind 
of 'gold' standard as the German analysis 
component (as part of the translation products) has 
proven to be sufficiently reliable. On the basis of 
the comparison a preference on the competitive 
analyses of the Spanish sentence is entailed and 
used for defining a statistical evaluation 
component for the Spanish grammar. Fig.3 shows 
the corresponding representations in the database 
for the sentence Aumenta la demana de energ?a 
el?ctrica por la ola de calor3  and its translation die 
Nachfrage nach Strom steigt wegen der 
Hitzewelle/the demand for electricity increases 
because of the heat-wave. 
 
 
 
 
 
 
 
 
 
 
Fig.3 Selection of analyses via correspondences 
(prefer first Spanish analysis because of subj-congruity) 
 
The analyses are associated with the corresponding 
creation protocols, which are structured lists whose 
items describe, via the identifiers, which rule has 
been applied when and to what structures in the 
process of creating the analysis. From the selection 
of a best analysis for a sentence, we can entail the 
circumstances under which the application of 
particular rules are preferred. This has been carried 
                                                           
3 Sentence taken from the online newspaper El D?a de 
Concepci?n del Uruguay 
 
 
109
out - not yet for the 'new' language Spanish, but for 
the 'known' language German, in order to obtain a 
measure about how correctly the existing grammar 
evaluation component can be replaced by the 
results of the corresponding statistical study.  
 
Fig.4  Frequency of applications of rules 
 
 
 cluster 
applications 
similarity feas  mod feas head 
383, 384,.. 0,86 sent, ... emosentaffv,.. 
557,558,566,.. 0,68 denselb,.. gebv, ... 
 
Fig.5  Preliminary constraints related to grammar 
rule clusters 
 
Fig.4 shows the distribution of rule usages within 
the training set of analyses (of approx.30.000 
sentences). 390 different rules were used with a 
total of 133708 rule applications. The subject rule 
(383) and the noun determiner rule (46) the most 
used rules (35% of all applications). Fig 5. 
illustrates the preliminary results of a clustering 
algorithm where different rule applications are 
grouped into clusters and the key features of the 
head and modifier phrases for each cluster are 
extracted. 
Currently, we try to determine further and tare 
the linguistic features and the weighting which 
models best the evaluation for German. (The gold 
standard that is used in this test is the set of 
analyses mentioned above). The investigations are 
not yet completed, but preliminary results on the 
basis of the morphosyntactic and semantic 
properties of the neighboring elements are 
promising. After consolidation, the findings will be 
transferred to Spanish on the basis of the selection 
procedure illustrated in Fig. 3. The next step of 
grammar training in the immediate future will 
consist of  changing the focus to underspecified 
analyses as described in step 8 
5 Conclusions 
The project tries to make state-of-the-art statistical 
methods available for dictionary development and 
grammar development for a rule-based dominated 
industrial setting and to exploit such methods 
there.  
With regard to SMT dictionary creation, it goes 
beyond the current state of the art as it also aims at 
developing and applying algorithms for the semi-
automatic generation of bilingual dictionaries from 
unrelated monolingual (i.e., comparable) corpora 
of the source and the target language, instead of 
using relatively literally translated (i.e., parallel) 
texts only. Comparable corpora are far easier to 
obtain than parallel corpora. Therefore the 
approach offers a solution to the serious data 
acquisition bottleneck in SMT. This approach is 
also more cognitively plausible than previous 
suggestions on this topic, since human bilinguality 
is normally not based on memorizing parallel texts. 
Our suggestion models human capacity to translate 
texts using linguistic knowledge acquired from 
monolingual data, so it also exemplifies many 
more features of a truly self-learning MT system 
(shared also by a human translator).  
In addition, the proposal suggests a new 
method for spelling out grammars and parsers for 
languages by splitting grammars into declarative 
kernels and trainable decision algorithms and by 
exploiting cross-linguistic knowledge for 
optimizing the results of the corresponding parsers.   
For developing different components and 
dictionaries for the system a bootstrapping 
architecture is suggested that uses the acquired 
lexical information for training the grammar of the 
new language, which in turn uses the 
(underspecified) parser results for optimizing the 
lexical information in the corresponding translation 
dictionaries. We expect that the suggested methods 
significantly improve translation quality and 
reduce the costs of creating new language pairs for 
Machine Translation. The preliminary results 
obtained so far in the project appear promising. 
6 Acknowledgments 
This research is supported by a Marie Curie IAPP 
project taking place within the 7th European 
Community Framework Programme (Grant 
agreement no.: 251534) 
110
7 References 
Armstrong, S.; Kempen, M.; McKelvie, D.; Petitpierre, D.; 
Rapp, R.; Thompson, H. (1998). Multilingual Corpora 
for Cooperation. Proceedings of the 1st International 
Conference on Linguistic Resources and Evaluation 
(LREC), Granada, Vol. 2, 975?980. 
Babych, B., Hartley, A., Sharoff S.; Mudraya, O. (2007). 
Assisting Translators in Indirect Lexical Transfer. 
Proceedings of the 45th Annual Meeting of the ACL.  
Babych, B., Anthony Hartley, & Serge Sharoff (2007b) 
Translating from under-resourced languages: 
comparing direct transfer against pivot translation. 
Proceedings of MT Summit XI, 10-14 September 
2007, Copenhagen, Denmark, 29-35 
Babych, B. & Hartley, A. (2008). Automated MT Evaluation 
for Error Analysis: Automatic Discovery of Potential 
Translation Errors for Multiword Expressions. ELRA 
Workshop on Evaluation ?Looking into the Future of 
Evaluation: When automatic metrics meet task-based  
and performance-based approaches?. Marrakech, 
Morocco 27 May 2008. Proceedings of LREC?08. 
Babych, B. and Hartley, A. (2009). Automated error analysis 
for multiword expressions: using BLEU-type scores 
for automatic discovery of potential translation errors. 
Linguistica Antverpiensia, New Series (8/2009): 
Journal of translation and interpreting studies. Special 
Issue on Evaluation of Translation Technology. 
Babych, B., Babych, S. and Eberle, K. (2012). Deriving 
generation-oriented MT resources from corpora: case 
study and evaluation of de/het classification for Dutch 
Noun (in preparation) 
Baroni, M.; Bernardini, S. (2004). BootCaT: Bootstrapping 
corpora and terms from the web. Proceedings of 
LREC 2004.  
Callison-Burch, C., Miles Osborne, & Philipp Koehn: Re-
evaluating the role of BLEU in machine translation 
research. EACL-2006: 11th Conference of the 
European Chapter of the Association for 
Computational Linguistics, Trento, Italy, April 3-7, 
2006; pp.249-256  
Charniak, E.; Knight, K.; Yamada, K. (2003). Syntax-based 
language models for statistical machine translation". 
Proceedings of MT Summit IX. 
Eberle, Kurt (2001). FUDR-based MT, head switching and the 
lexicon. Proceedings of the the eighth Machine 
Translation Summit, Santiage de Compostela.  
Eberle, Kurt (2004). Flat underspecified representation and its 
meaning for a fragment of German. 
Habilitationsschrift, Universit?t Stuttgart. 
Eberle, K.; Rapp, R. (2008). Rapid Construction of 
Explicative Dictionaries Using Hybrid Machine 
Translation. In: Storrer, A.;  Geyken, A.; Siebert, A.; 
W?rzner, K._M (eds.) Text Resources and Lexical 
Knowledge: Selected Papers from the 9th Conference 
on Natural Language Processing KONVENS 2008. 
Berlin: Mouton de Gruyter..  
Eckart,K., Eberle, K.; Heid, U. (2010) An infrastructure for 
more reliable corpus analysis. Proceedings of the 
Workshop on Web Services and Processing Pipelines 
in HLT of LREC-2010 , Valetta. 
Eberle, K.; Eckart,K., Heid, U.,Haselbach, B. (2012) A 
tool/database interface for multi-level analyses. 
Proceedings of LREC-2012 , Istanbul. 
Frederking, R.; Nirenburg, S.; Farwell, D.;  Helmreich, S.; 
Hovy, E.; Knight, K.; Beale, S.; Domashnev, C.; 
Attardo, D.; Grannes, D.; Brown, R. (1994). Integrated 
Translation from Multiple Sources within the Pangloss 
MARK II Machine Translation System. Proceedings 
of Machine Translation of the Americas, 73?80. 
Frederking, Robert and Sergei Nirenburg (1994). Three heads 
are better than one. In: Proceedings of ANLP-94, 
Stuttgart, Germany.  
Fung, P.; McKeown, K. (1997). Finding terminology 
translations from non-parallel corpora. Proceedings of 
the 5th Annual Workshop on Very Large Corpora, 
Hong Kong: August 1997, 192-202.  
Gale, W.A.; Church, K.W. (1993). A progrm for aligning 
sentences in bilingual corpora. Computational 
Linguistics, 19(1), 75?102. 
Gonz?lez, J.; Antonio L. Lagarda, Jos? R. Navarro, Laura 
Eliodoro, Adri? Gim?nez, Francisco Casacuberta, Joan 
M. de Val and Ferran Fabregat (2004). SisHiTra: A 
Spanish-to-Catalan hybrid machine translation system. 
Berlin: Springer LNCS. 
Gough, N., Way, A. (2004). Example-Based Controlled 
Translation. Proceedings of the Ninth Workshop of the 
European Association for Machine Translation, 
Valetta, Malta.  
Groves, D. & Way, A. (2006b). Hybridity in MT: Experiments 
on the Europarl Corpus. In Proceedings of the 11th 
Conference of the European Association for Machine 
Translation, Oslo, Norway, 115?124. 
Groves, D.; Way, A. (2006a). Hybrid data-driven models of 
machine translation. Machine Translation, 19(3?4). 
Special Issue on Example-Based Machine Translation. 
301?323. 
Habash, N.; Dorr, B. (2002). Handling translation 
divergences: Combining statistical and symbolic 
techniques in generation-heavy machine translation. 
Proceedings of AMTA-2002, Tiburon, California, 
USA. 
Kiss, T.; Strunk, J. (2006): Unsupervised multilingual 
sentence boundary detection. Computational 
Linguistics 32(4), 485?525. 
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical 
Machine Translation. Proceedings of MT Summit X, 
Phuket, Thailand 
Koehn, P.; Knight, K. (2002). Learning a translation lexicon 
from monolingual corpora. In: Proceedings of ACL-02 
Workshop on Unsupervised Lexical Acquisition, 
Philadelphia PA. 
Language Industry Monitor (1992). Statistical methods 
gaining ground. In: Language Industry Monitor, 
September/October 1992 issue. 
111
McCord, M. (1989). A new version of the machine translation 
system LMT.  Journal of Literary and Linguistic 
Computing, 4, 218?299. 
McCord, M. (1991). The slot grammar system.  In: Wedekind, 
J., Rohrer, C.(eds): Unification in Grammar, MIT-
Press. 
Melamed, I. Dan (1999). Bitext maps and aligment via pattern 
recognition. Computational Linguistics, 25(1), 107?
130. 
Munteanu, D.S.; Marcu, D. (2005). Improving machine 
translation performance by exploiting non-parallel 
corpora. Computational Linguistics, 31(4), 477?504. 
Och, F.J.; Ney, H. (2002). Discriminative trainig and 
maximum entropy models for statistical machine 
translation. Proceedings of the  Annual Meeting of the 
Association for Computational Linguistics, 
Philadelphia, PA, 295?302.  
Och, F.J.; Ney, H. (2003). A systematic comparison of various 
statistical alignment models. Computational 
Linguistics, 29(1), 19?51. 
Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. (2002). BLEU: A 
method for automatic evaluation of machine 
translation. In: Proceedings of the 40th Annual 
Meeting of the ACL, Philadelphia, PA, 311?318. 
Rapp, R. (1995). Identifying word translations in non-parallel 
texts. In: Proceedings of the 33rd Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, 1995, 320?322 
Rapp, R. (1999). Automatic identification of word translations 
from unrelated English and German corpora. In: 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics 1999, College 
Park, Maryland. 519?526. 
Rapp, R. (2004). A freely available automatically generated 
thesaurus of related words. In: Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation (LREC), Lisbon, Vol. II, 
395?398. 
Rapp, R.; Martin Vide, C. (2007). Statistical machine 
translation without parallel corpora. In: Georg Rehm, 
Andreas Witt, Lothar Lemnitzer (eds.): Data 
Structures for Linguistic Resources and Applications. 
Proceedings of the Biennial GLDV Conference 2007. 
T?bingen: Gunter Narr. 231?240 
Resnik, R. (1999). Mining the web for bilingual text. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics. 
Sato, S.; Nagao, M. (1990). Toward memory-based 
translation. Proceedings of COLING 1990, 247?252. 
Schiehlen, M. (2001) Syntactic Underspecification. In: Special 
Research Area 340 ? Final report, University of 
Stuttgart.  
Sharoff, S. (2006) Open-source corpora: using the net to fish 
for linguistic data. In International Journal of Corpus 
Linguistics 11(4), 435?462.  
Sharoff, S.; Babych, B.; Hartley, A. (2006). Using comparable 
corpora to solve problems difficult for human 
translators. In: Proceedings of COLING/ACL 2006, 
739?746.  
Sharoff, S. (2006). A uniform interface to large-scale 
linguistic resources. In Proceedings of the Fifth 
Language Resources and Evaluation Conference, 
LREC-2006, Genoa. 
Simard, M., Foster, G., Isabelle, P. (1992). Using Cognates to 
Align Sentences in Bilingual Corpora. Proceeedings of 
the International Conference on Theoretical and 
Methodological Issues, Montr?al. 
Smadja, F. (1993). Retrieving collocations from text: Xtract. 
Computational Linguistics, 19(1), 143?177. 
Streiter, O., Carl, M., Haller, J. (eds)(1999). Hybrid 
Approaches to Machine Translation. IAI working 
papers 36. 
Streiter, O.; Carl, M.; Iomdin, L.L.: 2000, A Virtual 
Translation Machine for Hybrid Machine Translation'. 
In: Proceedings of the Dialogue'2000 International 
Seminar in Computational Linguistics and 
Applications. Tarusa, Russia.  
Streiter, O.; Iomdin, L.L. (2000). Learning Lessons from 
Bilingual Corpora: Benefits for Machine Translation. 
International Journal of Corpus Linguistics, 5(2), 199?
230. 
Thurmair, G. (2005). Hybrid architectures for machine 
translation systems. Language Resources and 
Evaluation, 39 (1), 91?108. 
Thurmair, G. (2006). Using corpus information to improve 
MT quality. Proceedings of the LR4Trans-III 
Workshop, LREC, Genova. 
Thurmair, G. (2007) Automatic evaluation in MT system 
production. MT Summit XI Workshop: Automatic 
procedures in MT evaluation, 11 September 2007, 
Copenhagen, Denmark, 
Veronis, Jean (2006). Technologies du Langue. Actualit?s ? 
Comentaires ? R?flexions. Translation. Systran or 
Reverso? 
http://aixtal.blogspot.com/2006/01/translation-systran-
or-reverso.html  
Wu, D., Fung, P. (2005). Inversion transduction grammar 
constraints for mining parallel sentences from quasi-
comparable corpora. Second International Joint 
Conference on Natural Language Processing 
(IJCNLP-2005). Jeju, Korea. 
 
112
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87?95,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Extracting Multiword Translations  
from Aligned Comparable Documents 
 
Reinhard Rapp Serge Sharoff 
Aix-Marseille Universit?, Laboratoire 
d'Informatique Fondamentale 
F-13288 Marseille, France 
University of Leeds 
Centre for Translation Studies 
Leeds, LS2 9JT, UK 
reinhardrapp@gmx.de S.Sharoff@leeds.ac.uk 
 
Abstract 
Most previous attempts to identify trans-
lations of multiword expressions using 
comparable corpora relied on dictionaries 
of single words. The translation of a mul-
tiword was then constructed from the 
translations of its components. In con-
trast, in this work we try to determine the 
translation of a multiword unit by analyz-
ing its contextual behaviour in aligned 
comparable documents, thereby not pre-
supposing any given dictionary. Whereas 
with this method translation results for 
single words are rather good, the results 
for multiword units are considerably 
worse. This is an indication that the type 
of multiword expressions considered here 
is too infrequent to provide a sufficient 
amount of contextual information. Thus 
indirectly it is confirmed that it should 
make sense to look at the contextual be-
haviour of the components of a multi-
word expression individually, and to 
combine the results. 
1 Introduction 
The task of identifying word translations from 
comparable text has received considerable atten-
tion. Some early papers include Fung (1995) and 
Rapp (1995). Fung (1995) utilized a context het-
erogeneity measure, thereby assuming that words 
with productive context in one language translate 
to words with productive context in another lan-
guage, and words with rigid context translate into 
words with rigid context. In contrast, the under-
lying assumption in Rapp (1995) was that words 
which are translations of each other show similar 
co-occurrence patterns across languages. This 
assumption is effectively an extension of Harris' 
(1954) distributional hypotheses to the multilin-
gual case. 
This work was further elaborated in some by 
now classical papers, such as Fung & Yee (1998) 
and Rapp (1999). Based on these papers, the 
standard approach is to start from a dictionary of 
seed words, and to assume that the words occur-
ring in the context of a source language word 
have similar meanings as the words occurring in 
the context of its target language translation. 
There have been suggestions to eliminate the 
need for the seed dictionary. However, most at-
tempts, such as Rapp (1995), Diab & Finch 
(2000) and Haghighi et al. (2008) did not work to 
an extent that the results would be useful for 
practical purposes. Only recently a more pro-
mising approach has been investigated: Schafer 
& Yarowsky (2002), Hassan & Mihalcea (2009), 
Prochasson & Fung (2011) and Rapp et al. 
(2012) look at aligned comparable documents 
and deal with them in analogy to the treatment of 
aligned parallel sentences, i.e. effectively doing a 
word alignment in a very noisy environment. 
This approach has been rather successful and it 
was possible to improve on previous results. This 
is therefore the approach which we will pursue in 
the current paper.  
However, in contrast to the above mentioned 
papers the focus of our work is on multiword 
expressions, and we will compare the perform-
ance of our algorithm when applied to multiword 
expressions and when applied to single words.  
There has been some previous work on identi-
fying the translations of multiword units using 
comparable corpora, such as Robitaille et al. 
(2006), Babych et al. (2007), Daille & Morin 
(2012); Delpech et al. (2012). However, none of 
this work utilizes aligned comparable documents, 
and the underlying assumption is that the transla-
tion of a multiword unit can be determined by 
looking at its components individually, and by 
merging the results. 
In contrast, we try to explore whether the 
translation of a multiword unit can be determined 
solely by looking at its contextual behavior, i.e. 
whether it is possible to also apply the standard 
approach as successfully used for single words. 
The underlying fundamental question is whether 
the meaning of a multiword unit is determined by 
87
the contextual behavior of the full unit, or by the 
contextual behavior of its components (or by a 
mix of both). But multiword expressions are of 
complex nature, as expressed e.g. by Moon 
(1998): "there is no unified phenomenon to de-
scribe but rather a complex of features that inter-
act in various, often untidy, ways and represent a 
broad continuum between non-compositional (or 
idiomatic) and compositional groups of words." 
The current paper is an attempt to systematically 
approach one aspect of this complexity. 
2 Approach 
Our approach is based on the usual assumption 
that there is a correlation between the patterns of 
word-co-occurrence across languages. However, 
instead of presupposing a bilingual dictionary it 
only requires pre-aligned comparable documents, 
i.e. small or medium sized documents aligned 
across languages which are known to deal with 
similar topics. This could be, for example, news-
paper articles, scientific papers, contributions to 
discussion groups, or encyclopaedic articles. As 
Wikipedia is a large resource and readily avail-
able for many languages, we decided to base our 
study on this encyclopaedia. The Wikipedias 
have the so-called interlanguage links which are 
manually inserted by the authors and connect 
articles referring to the same headword in differ-
ent languages. 
Given that each Wikipedia community con-
tributes in its own language, only occasionally an 
article connected in this way will be an exact 
translation of a foreign language article, and in 
most cases the contents will be rather different. 
On the positive side, the link structure of the in-
terlanguage links tends to be quite dense. For 
example, of the 1,114,696 German Wikipedia 
articles, 603,437 have a link to the corresponding 
English Wikipedia article. 
2.1 Pre-processing and MWE extraction 
We used the same versions of Wikipedia as in 
Rapp et al. (2012) and used the same processing. 
After download, each Wikipedia was minimally 
processed to extract the plain text contents of the 
articles. In this process all templates, e.g. 
'infoboxes', as well as tables were removed, and 
we kept only the webpages with more than 500 
characters of running text (including white 
space). Linguistic processing steps included to-
kenisation, tagging and lemmatisation using the 
default UTF-8 versions of the respective Tree-
Tagger resources (Schmid, 1994). 
From the pre-processed English and German 
Wikipedia, we extracted the multiword expres-
sions using two simple principles, a negative 
POS filter and a containment filter. The negative 
POS filter operates in a rule-based fashion on the 
complete list of n-grams by removing the un-
likely candidates according to a set of con-
straints, such as the presence of determiners or 
prepositions at the edges of expressions, see a 
similar method used by (Justeson & Katz, 1995). 
With some further extensions this was also used 
to produce the multiword lists for the dictionary 
of translation equivalents (Babych et al., 2007). 
We did not use positive shallow filters. These 
would need to capture the relatively complex 
structure of the noun, verb and prepositional 
phrases, while avoiding noise. This can often 
lead to a lack of recall when more complex con-
structions cannot be captured. In contrast, nega-
tive shallow filters simply avoid obvious noise, 
while passing other multiword expressions 
(MWEs) through, which are very often legiti-
mate syntactic constructions in a language in 
question. For example, the following English 
filters1 rejected personal pronouns (PP) and con-
junctions (CC) at the edges of expressions (using 
the Penn Treebank tagset as implemented by 
Treetagger): 
 
^[^ ]+~~PP |~~PP$ 
^[^ ]+~~CC |~~CC$ 
 
Similarly, any MWE candidates including proper 
nouns (NP) and numerals (CD) were discarded: 
 
~~NP 
~~CD 
 
In the end, this helps in improving the recall rate 
while using a relatively small number of pat-
terns: 18 patterns were used for English, 11 for 
German. 
The containment filter further rejects MWEs 
by removing those that regularly occur as a part 
of a longer acceptable MWE. For example, 
graphical user is an acceptable expression pass-
ing through the POS filter, but it is rejected by 
the containment filter since the overwhelming 
majority of its uses are in the containing MWE 
graphical user interface (1507 vs 1304 uses in 
Wikipedia, since MWEs are still possible, e.g., 
graphical user environment).  
                                                 
1
 We use here the standard notation for regular ex-
pressions as implemented in Perl (Friedl, 2002). For 
example, '^' means 'beginning of line' and '$' means 
'end of line'.  
88
English keyterms for 'Airbus 320 family' 
Score f Keyterm 
34.88 4 final_JJ assembly_NN 
31.22 3 firm_NN order_NN 
30.73 3 series_NN aircraft_NN 
29.07 4 flight_NN control_NN 
27.38 3 wing_NN area_NN 
23.26 3 final_JJ approach_NN 
22.19 2 lose_VV life_NN 
20.63 6 passenger_NN and_CC crew_NN 
17.54 2 first_JJ derivative_NN 
17.34 2 fly-by-wire_NN flight_NN control_NN 
16.63 3 flight_NN deck_NN 
16.41 2 crew_NN die_VV 
15.08 2 pilot_NN error_NN 
14.98 2 passenger_NN capacity_NN 
14.38 2 turbofan_NN engine_NN 
14.03 2 development_NN cost_NN 
12.30 2 maiden_JJ flight_NN 
11.54 2 direct_JJ competition_NN 
10.75 2 overall_JJ length_NN 
10.39 2 overrun_VV the_DT runway_NN 
9.54 2 flight_NN control_NN system_NN 
9.31 2 fuel_NN consumption_NN 
8.63 2 roll_VV out_RP 
7.86 3 crew_NN member_NN 
7.54 2 crew_NN on_IN board_NN 
7.33 2 bad_JJ weather_NN 
6.63 2 landing_NN gear_NN 
 
German keyterms for 'Airbus-A320-Familie' 
Score f Keyterm 
155.25 20 Triebwerk 
62.88 4 Fly-by-Wire-System 
59.76 8 Erstflug 
57.67 8 Absturz 
43.79 4 Endmontage 
43.70 4 Hauptfahrwerk 
41.77 4 Tragfl?gel 
36.52 8 Unfall 
35.90 6 Ungl?ck 
33.25 3 Abfluggewicht 
33.10 5 Auslieferung 
30.01 3 Treibstoffverbrauch 
29.00 2 Triebwerkstyp 
28.51 2 Zwillingsreifen 
18.20 2 Absturz_NN verursachen_VV 
16.28 3 Passagier_NN Platz_NN 
16.23 2 Triebwerk_NN antreiben_VV 
13.41 2 Steuerung_NN d_AR Flugzeug_NN 
12.52 2 Absturz_NN f?hren_VV 
11.68 2 Rumpf_NN befinden_VV 
8.59 2 Insasse_NN ums_AP Leben_NN 
8.55 2 Zeitpunkt_NN d_AR Ungl?ck_NN 
Table 1. English and German keyterms for 'Airbus 320 fam-
ily' (lists truncated). Score = log-likelihood score; f = occur-
rence frequency of keyterm; NN = noun; VV = verb; AR = 
article; AP = article+preposition; JJ = adjective; CC = con-
junction; RP = preposition. 
 
2.2 Keyterm extraction 
As the aligned English and German Wikipedia 
documents are typically not translations of each 
other, we cannot apply the usual procedures and 
tools as available for parallel texts (e.g. the Gale 
& Church sentence aligner and the Giza++ word 
alignment tool). Instead we conduct a two step 
procedure:  
1. We first extract salient terms (single word or 
multiword) from each of the documents. 
2. We then align these terms across languages 
using an approach inspired by a connectionist 
(Rumelhart & McClelland, 1987) Winner-
Takes-It-All Network. The respective algo-
rithm is called WINTIAN and is described in 
Rapp et al. (2012) and in Rapp (1996).  
For term extraction, the occurrence frequency of 
a term in a particular document is compared to 
its average occurrence frequency in all Wikipe-
dia documents, whereby a high discrepancy indi-
cates a strong keyness. Following Rayson & 
Garside (2000), we use the log-likelihood score 
to measure keyness, since it has been shown to 
be robust to small numbers of instances. This 
robustness is important as many Wikipedia arti-
cles are rather short.  
This procedure leads to multiword keyterms as 
exemplified in Table 1 for the Wikipedia entry 
Airbus A320 family.  Because of compounding in 
German, many single-word German expressions 
are translated into multiword expressions in Eng-
lish. So we chose to include single-word expres-
sions into the German candidate list for align-
ment with English multiwords.  
One of the problems in obtaining multiword 
keyterms from the Wikipedia articles is relative 
data sparseness. Usually, the frequency of an 
individual multiword expression within a Wiki-
pedia article is between 2 and 4. Therefore we 
had to use a less conservative threshold of 6.63 
(1% significance level) rather than the more 
standard 15.13 (0.01% significance level) for the 
log-likelihood score (see Rayson & Garside, 
2000, and http://ucrel. lancs.ac.uk/llwizard.html). 
2.3 Term alignment 
The WINTIAN algorithm is used for establishing 
term alignments across languages. As a more 
detailed technical description is given in Rapp et 
al. (2012) and in Rapp (1996), we only briefly 
describe this algorithm here, thereby focusing on 
the neural network analogy. The algorithm can 
be considered as an artificial neural network 
where the nodes are all English and German 
89
terms occurring in the keyterm lists. Each Eng-
lish term has connections to all German terms. 
The connections are all initialized with values of 
one when the algorithm is started, but will serve 
as a measure of the translation probabilities after 
the completion of the algorithm. One after the 
other, the network is fed with the pairs of corre-
sponding keyterm lists. Each German term acti-
vates the corresponding German node with an 
activity of one. This activity is then propagated 
to all English terms occurring in the correspond-
ing list of keyterms. The distribution of the activ-
ity is not equal, but in proportion to the connect-
ing weights. This unequal distribution has no 
effect at the beginning when all weights are one, 
but later on leads to rapid activity increases for 
pairs of terms which often occur in correspond-
ing keyterm lists. The assumption is that these 
are translations of each other. Using Hebbian 
learning (Rumelhart & McClelland, 1987) the 
activity changes are stored in the connections. 
We use a heuristic to avoid the effect that fre-
quent keyterms dominate the network: When 
more than 50 of the connections to a particular 
English node have weights higher than one, the 
weakest 20 of them are reset to one. This way 
only translations which are frequently confirmed 
can build up high weights. 
It turned out that the algorithm shows a robust 
behaviour in practice, which is important as the 
corresponding keyterm lists tend to be very noisy 
and, especially for multiword expressions, in 
many cases may contain hardly any terms that 
are actually translations of each other. Reasons 
are that corresponding Wikipedia articles are of-
ten written from different perspectives, that the 
variation in length can be considerable across 
languages, and that multiword expressions tend 
to show more variability with regard to their 
translations than single words. 
3 Results and evaluation 
3.1 Results for single words 
In this subsection we report on our previous re-
sults for single words (Rapp et al., 2012) as these 
serve as a baseline for our new results concern-
ing multiword units. 
The WINTIAN algorithm requires as input 
vocabularies of the source and the target lan-
guage. For both English and German, we con-
structed these as follows: Based on the keyword 
lists for the respective Wikipedia, we counted the 
number of occurrences of each keyword, and 
then applied a threshold of five, i.e. all keywords 
with a lower frequency were eliminated. The rea-
soning behind this is that rare keywords are of 
not much use due to data sparseness. This re-
sulted in a vocabulary size of 133,806 for Eng-
lish, and of 144,251 for German. 
Using the WINTIAN algorithm, the English 
translations for all 144,251 words occurring in 
the German vocabulary were computed. Table 2 
shows the results for the German word Stra?e 
(which means street). 
For a quantitative evaluation we used the 
ML1000 test set comprising 1000 English-
German translations (see Rapp et al., 2012). We 
verified in how many cases our algorithm had 
assigned the expected translation (as provided by 
the gold standard) the top rank among all 
133,806 translation candidates. (Candidates are 
all words occurring in the English vocabulary.) 
This was the case for 381 of the 1000 items, 
which gives us an accuracy of 38.1%. Let us 
mention that this result refers to exact matches 
with the word equations in the gold standard. As 
in reality due to word ambiguity other transla-
tions might also be acceptable (e.g. for Stra?e 
not only street but also road would be accept-
able), these figures are conservative and can be 
seen as a lower bound of the actual performance.  
 
GIVEN GERMAN 
WORD Stra?e 
EXPECTED 
TRANSLATION street 
 
LL-SCORE TRANSLATION 
1 215.3 road 
2 148.2 street 
3 66.0 traffic 
4 46.0 Road 
5 42.6 route 
6 34.6 building 
 
Table 2. Computed translations for Stra?e. 
 
3.2 Results for multiword expressions 
In analogy to the procedure for single words, for 
the WINTIAN algorithm we also needed to de-
fine English and German vocabularies of multi-
word terms. For English, we selected all multi-
word terms which occurred at least three times in 
the lists of English key terms, and for German 
those which occurred at least four times in the 
lists of German key terms. This resulted in simi-
lar sized vocabularies of 114,796 terms for Eng-
lish, and 131,170 for German. Note that the 
threshold for German had to be selected higher 
not because German has more inflectional vari-
ants (which does not matter as we are working 
90
with lemmatized data), but because - other than 
the English - the German vocabulary also in-
cludes unigrams. The reason for this is that Ger-
man is highly compositional, so that English 
multiword units are often translated by German 
unigrams. 
Using the WINTIAN algorithm, the English 
translations for all 131,170 words occurring in 
the German multiword vocabulary were com-
puted, and in another run the German translations 
for all 114,796 English words. Table 3 shows 
some sample results.  
For a quantitative evaluation, we did not have 
a gold standard at hand. As multiword expres-
sions show a high degree of variability with re-
gard to their translations, so that it is hard to 
come up with all possibilities, we first decided 
not to construct a gold standard, but instead did a 
manual evaluation. For this purpose, we ran-
domly selected 100 of the German multiword 
expressions with an occurrence frequency above 
nine, and verified their computed translations 
(i.e. the top ranked item for each) manually. We 
distinguished three categories: 1) Acceptable 
translation; 2) Associatively related to an accept-
able translation; 3) Unrelated to an acceptable 
translation.  
 
 
 English ? German 
 husband_NN and_CC wife_NN 
Rank Aktivity Translation 
1 2.98 Eheleute 
2 1.09 Voraussetzung 
3 1.08 Kirchenrecht 
4 0.76 Trennung 
5 0.35 Mann 
6 0.24 Kirche 
7 0.08 Mischehe 
8 0.08 Diakon 
 
 
 German ? English 
 Eheleute 
Rank Aktivity Translation 
1 3.01 husband_NN_and_CC_wife_NN 
2 1.26 married_JJ_couple_NN 
3 1.02 civil_JJ_law_NN 
4 1.02 equitable_JJ_distribution_NN 
5 1.02 community_NN_property_NN 
6 0.52 law_NN_jurisdiction_NN 
7 0.05 racing_NN_history_NN 
8 0.05 great_JJ_female_JJ 
 
Table 3. Sample results for translation directions EN ? DE 
and DE ? EN. 
 
We also did the same computation for the reverse 
language direction, i.e. for English to German. 
The results are listed in Table 4. These results 
indicate that our procedure, although currently 
state of the art for single words, does not work 
well for multiword units. We investigated the 
data and located the following problems: 
? The problem of data sparseness is, on average, 
considerably more severe for multiword ex-
pressions than it is for single words. 
 
? Although the English and the German vocabu-
lary each contain more than 100,000 items,  
their overlap is still limited. The reason is that 
the number of possible multiword units is very 
high, far higher than the number of words in a 
language. 
 
? We considered only multiword units up to 
length three, but in some cases this may not 
suffice for an acceptable translation. 
 
? In the aligned keyterm lists, only rarely correct 
translations of the source language terms oc-
cur. Apparently the reason is the high variabil-
ity of multiword translations. 
Hereby he last point seems to have a particularly 
severe negative effect on translation quality. 
However, all of these findings are of fundamen-
tal nature and contribute to the insight that at 
least for our set of multiword expressions com-
positionality seems to be more important than 
contextuality. 
 
German ? English 
Judgment Num-ber 
Example taken from actual 
data 
Acceptable 5 Jugendherberge ?  
youth_NN hostel_NN 
Association 38 Maischeg?rung ?  
oak_NN barrel_NN 
Unacceptable 57 Stachelbeere ?  
horror_NN film_NN 
 
English ? German 
Judgment Num-ber 
Example taken from actual 
data 
Acceptable 6 amino_NN acid_NN ? 
Aminos?ure 
Association 52 iron_NN mine_NN ? Ei-
senerz 
Unacceptable 42 kill_VV more_JJ ? Welt-
meistertitel_NN im_AP 
Schwergewicht_NN 
Table 4. Quantitative results involving MWEs. 
 
91
3.3 Large scale evaluation 
As a manual evaluation like the one described 
above is time consuming and subjective, we 
thought about how we could efficiently come up 
with a gold standard for multiword expressions 
with the aim of conducting a large scale auto-
matic evaluation. We had the idea to determine 
the correspondences between our English and 
German MWEs via translation information as 
extracted from a word-aligned parallel corpus. 
Such data we had readily at hand from a pre-
vious project called COMTRANS. During this 
project we had constructed a large bilingual dic-
tionary of bigrams, i.e. of pairs of adjacent words 
in the source language. For constructing the dic-
tionary, we word-aligned the English and Ger-
man parts of the Europarl corpus. For this pur-
pose, using Moses default settings, we combined 
two symmetric runs of Giza++, which considera-
bly improves alignment quality. Then we deter-
mined and extracted for each English bigram the 
German word or word sequence which had been 
used for its translation. Discontinuities of one or 
several word positions were allowed and were 
indicated by the wildcard ?*?. As the above me-
thod for word alignment produces many unjusti-
fied empty assignments (i.e. assignments where a 
source language word pair is erroneously as-
sumed to have no equivalent in the target lan-
guage sentence), so that the majority of these is 
incorrect, all empty assignments were removed 
from the dictionary. 
In the dictionary, for each source language 
word pair its absolute frequency and the absolute 
and relative frequencies of its translation(s) are 
given. To filter out spurious assignments, thresh-
olds of 2 for the absolute and 10% for the rela-
tive frequency of a translation were used. The 
resulting dictionary is available online.2  Table 5 
shows a small extract of the altogether 371,590 
dictionary entries. Alternatively, we could have 
started from a Moses phrase table, but it was eas-
ier for us to use our own data. 
Although the quality of our bigram dictionary 
seems reasonably good, it contains a lot of items 
which are not really interesting multiword ex-
pressions (e.g. arbitrary word sequences such as 
credible if or the discontinuous word sequences 
on the target language side). For this reason we 
filtered the dictionary using the lists of Wikipe-
                                                 
2
 http://www.ftsk.uni-mainz.de/user/rapp/comtrans/ 
There click on "Dictionaries of word pairs" and then 
download "English - German". 
dia-derived multiword expressions as described 
in section 2.1. These contained 418,627 items for 
English and 1,212,341 candidate items for Ger-
man (the latter included unigram compounds). 
That is, in the dictionary those items were re-
moved where either the English side did not 
match any of the English MWEs, or where the 
German side did not match any of the German 
candidates.  
This intersection resulted in a reduction of our 
bigram dictionary from 371,590 items to 137,701 
items. Table 6 shows the results after filtering the 
items listed in Table 5. Note that occasionally 
reasonable MWEs are eliminated if they happen 
not to occur in Wikipedia, or if the algorithm for 
extracting the MWEs does not identify them. 
The reduced dictionary we considered as an 
appropriate gold standard for the automatic eval-
uation of our system. 
 
ENGLISH BIGRAM GERMAN TRANSLATION 
credible if  dann glaubw?rdig * wenn  
credible if  glaubhaft * wenn  
credible if  glaubw?rdig * wenn  
credible in  in * Glaubw?rdigkeit  
credible in  in * glaubw?rdig  
credible investigation  glaubw?rdige Untersuchung  
credible labelling  glaubw?rdige Kennzeichnung  
credible manner  glaubw?rdig  
credible military  glaubw?rdige milit?rische  
credible military  glaubw?rdigen milit?rischen  
credible only  nur dann glaubw?rdig  
credible partner  glaubw?rdiger Partner  
credible policy  Politik * glaubw?rdig  
credible policy  glaubw?rdige Politik  
credible reports  glaubw?rdige Berichte  
credible response  glaubw?rdige Antwort  
credible solution  glaubw?rdige L?sung  
credible system  glaubw?rdiges System  
credible threat  glaubhafte Androhung  
credible to  f?r * glaubw?rdig  
credible to  glaubw?rdig 
Table 5. Extract from the COMTRANS bigram dictionary. 
 
ENGLISH BIGRAM GERMAN TRANSLATION 
credible investigation glaubw?rdige Untersuchung 
credible only nur dann glaubw?rdig 
credible policy glaubw?rdige Politik 
credible response glaubw?rdige Antwort 
credible solution glaubw?rdige L?sung 
credible system glaubw?rdiges System 
credible threat glaubhafte Androhung 
credible to glaubw?rdig 
Table 6. Extract from the bigram dictionary after filtering. 
 
92
As in section 3.2, the next step was to apply 
the keyword extraction algorithm to the English 
and the German Wikipedia documents. Hereby 
only terms occurring in the gold standard dic-
tionary were taken into account. But it turned out 
that, when using the same log-likelihood thresh-
old as in section 3.2, only few keyterms were 
assigned: on average less than one per document. 
This had already been a problem in 3.2, but it 
was now considerably more severe as this time 
the MWE lists had been filtered, and as the filter-
ing had been on the basis of another type of cor-
pus (Europarl rather than Wikipedia). 
This is why, after some preliminary experi-
ments with various thresholds, we finally de-
cided to disable the log-likelihood threshold. In-
stead, on the English side, all keyterms from the 
gold standard were used if they occurred at least 
once in the respective Wikipedia document. On 
the German side, as here we had many unigram 
compounds which tend to be more stable and 
therefore more repetitive than MWEs, we used 
the keyterms if the occurred at least twice. This 
way for most documents we obtained at least a 
few keyterms. 
When running the WINTIAN algorithm on the 
parallel keyword lists, in some cases reasonable 
results were obtained. For example, for the direc-
tion English to German, the system translates 
information society with Informationsgesell-
schaft, and education policy with Bildungs-
politik. As WINTIAN is symmetric and can 
likewise produce a dictionary in the opposite di-
rection, we also generated the results for German 
to English. Here, among the good examples, are 
Telekommunikationsmarkt, which is translated as 
telecommunications market, and Werbekam-
pagne, which is translated as  advertising cam-
paign. However, these are selected examples 
showing that the algorithm works in principle. 
Of more interest is the quantitative evaluation 
which is based on thousands of test words and 
uses the gold standard dictionary. For English to 
German we obtained an accuracy of 0.77% if 
only the top ranked word is taken into account, 
i.e. if this word matches the expected translation. 
This improves to 1.6% if it suffices that the ex-
pected translation is ranked among the top ten 
words. The respective figures for German to 
English are 1.41% and 2.04%. 
The finding that German to English performs 
better can be explained by the fact that other than 
English German is a highly inflectional lan-
guage. That is, when generating translations it is 
more likely for German that an inflectional vari-
ant not matching the gold standard translation is 
ranked first, thus adversely affecting perform-
ance. 
A question more difficult to answer is why the 
results based on the gold standard are considera-
bly worse than the ones reported in section 3.2 
which were based on human judgment. We see 
the following reasons: 
 
? The evaluation in section 3.2 used only a 
small sample so might be not very reliable. 
Also, other than here, it considered only 
source language words with frequencies 
above nine. 
? Unlike the candidate expressions, the gold 
standard data is not lemmatized on the target 
language side. 
? The hard string matching used for the gold-
standard-based evaluation does not allow for 
inflectional variants. 
? The gold-standard-based evaluation used 
terms resulting from the intersection of term 
lists based on Wikipedia and Europarl. It is 
clear that this led to a reduction of average 
term frequency (if measured on the basis of 
Wikipedia), thus increasing the problem of 
data sparseness. 
? As for the same reason the log-likelihood 
threshold had to be abandoned, on average 
less salient terms had to be used. This is 
likely to additionally reduce accuracy. 
? For many terms the gold standard lists sev-
eral possible translations. In the current im-
plementation of the evaluation algorithm 
only one of them is counted as correct. 3 
However, in the human evaluation any rea-
sonable translation was accepted. 
? Some reasonable MWE candidates extracted 
from Wikipedia are not present in the gold 
standard, for example credible evidence, 
credible source, and credible witness are not 
frequent enough in Europarl to be selected 
for alignment. 
 
We should perhaps mention that it would be pos-
sible to come up with better looking accuracies 
by presenting results for selected subsets of the 
source language terms. For example, one could 
concentrate on terms with particularly good cov-
                                                 
3
 This can be justified because an optimal algorithm 
should provide all possible translations of a term. If 
only some translations are provided, only partial 
credit should be given. But this is likely to average 
out over large numbers, so the simple version seems 
acceptable. 
93
erage. Another possibility would be to consider 
MWEs consisting of nouns only. This we actu-
ally did by limiting source and target language 
vocabulary (of MWEs) to compound nouns. The 
results were as follows: 
 
    English to German (top 1):  1.81% 
    English to German (top 10):  3.75% 
    German to English (top 1):  2.03% 
    German to English (top 10):  3.16% 
 
As can be seen, these results look somewhat bet-
ter. But this is only for the reason that translating 
compound nouns appears to be a comparatively 
easier task on average.  
4 Conclusions and future work 
We have presented a method for identifying term 
translations using aligned comparable docu-
ments. Although it is based on a knowledge poor 
approach and does not presuppose a seed lexi-
con, it delivers competitive results for single 
words.  
A disadvantage of our method is that it pre-
supposes that the alignments of the comparable 
documents are known. On the other hand, there 
are methods for finding such alignments auto-
matically not only in special cases such as 
Wikipedia and newspaper texts, but also in the 
case of unstructured texts (although these meth-
ods may require a seed lexicon). 
Concerning the question from the introduc-
tion, namely whether the translation (and conse-
quently also the meaning) of a multiword unit is 
determined compositionally or contextually, our 
answer is as follows: For the type of multiword 
units we were investigating, namely automati-
cally extracted collocations, our results indicate 
that looking at their contextual behavior usually 
does not suffice. The reasons seem to be that 
their contextual behavior shows a high degree of 
variability, that their translations tend to be less 
salient than those of single words, and that the 
problem of data sparseness is considerably more 
severe. 
It must be seen, however, that there are many 
types of multiword expressions, such as idioms, 
metaphorical expressions, named entities, fixed 
phrases, noun compounds, compound verbs, 
compound adjectives, and so on, so that our re-
sults are not automatically applicable to all of 
them. Therefore, in future work we intend to 
compare the behavior of different types of mul-
tiword expressions (e.g. multiword named enti-
ties and short phrases such as those used in 
phrase-based machine translations) and to quan-
tify in how far their behavior is compositional or 
contextual. 
Acknowledgment 
This research was supported by a Marie Curie 
Intra European Fellowship within the 7th Euro-
pean Community Framework Programme. 
References 
Babych, B., Sharoff, S., Hartley, A., and Mudraya, O. 
(2007). Assisting Translators in Indirect Lexical 
Transfer. Proceedings of the 45th Annual Meeting 
of the Association for Computational Linguistics 
ACL 2007, Prague, Czech Republic.  
Daille, B.; Morin, E. (2012). Revising the composi-
tional method for terminology acquisition from 
comparable corpora. Proceedings of Coling 2012, 
Mumbai.  
Delpech, E.; Daille, B.; Morin, E., Lemaire, C. 
(2012). Extraction of domain-specific bilingual 
lexicon from comparable corpora: compositional 
translation and ranking. Proceedings of Coling 
2012, Mumbai.  
Diab, M., Finch, S. (2000): A statistical wordlevel 
translation model for comparable corpora. In: Pro-
ceedings of the Conference on Content-Based Mul-
timedia Information Access (RIAO). 
Friedl, J. (2002). Mastering Regular Expressions. 
O'Reilly.  
Fung, P. (1995). Compiling bilingual lexicon entries 
from a non-parallel English-Chinese corpus. In: 
Proceedings of the  Third Annual Workshop on Ve-
ry Large Corpora, Boston, Massachusetts. 173-
183.  
Fung, P.; Yee, L. Y. (1998). An IR approach for 
translating new words from nonparallel, compara-
ble texts. Proceedings of  COLING/ACL 1998, 
Montreal, Canada. 414-420.  
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., Klein, 
D. (2008): Learning bilingual lexicons from mono-
lingual corpora. In: Proceedings of ACL-HLT 
2008, Columbus, Ohio. 771-779.  
Harris, Z.S. (1954). Distributional structure. Word, 
10(23), 146?162. 
Hassan, S., Mihalcea, R. (2009): Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In: 
Proceedings of EMNLP.  
Justeson, J.S.; Katz, S.M. (1995). Techninal terminol-
ogy: some linguistic properties and an algorithm for 
identification in text. Natural Language  Engineer-
ing, 1(1): 9?27. 
Moon, R.E. 1998. Fixed Expressions and Idioms in 
English: A Corpus-based Approach. Oxford: Clar-
endon Press.  
94
Prochasson, E., Fung, P. (2011). Rare word transla-
tion extraction from aligned comparable docu-
ments. In: Proceedings of ACL-HLT. Portland .  
Rapp, R. (1995). Identifying word translations in non-
parallel texts. In: Proceedings of the 33rd Annual 
Meeting of the ACL. Cambridge, MA, 320-322.  
Rapp, R. (1996). Die Berechnung von Assoziationen. 
Hildesheim: Olms. 
Rapp, R. (1999). Automatic identification of word 
translations from unrelated English and German 
corpora. Proceedings of the 37th Annual Meeting of 
the Association for Computational Linguistics, Col-
lege Park, Maryland. 519?526. 
Rapp, R., Sharoff,  S., Babych, B. (2012). Identifying 
word translations from comparable documents 
without a seed lexicon. In: Proceedings of the 8th 
Language Resources and Evaluation Conference, 
LREC 2012, Istanbul.  
 
Rayson, P.; Garside, R. (2000). Comparing corpora 
using frequency profiling. Proceedings of the 
Workshop on Comparing Corpora (WCC '00 ), Vol-
ume 9, 1?6. 
Robitaille, X., Sasaki, Y., Tonoike, M., Sato, S., Utsu-
ro, T. (2006). Compiling French-Japanese termi-
nologies from the web. In: Proceedings of the 11th 
Conference of EACL, Trento, Italy, 225-232.  
Rumelhart, D.E.; McClelland, J.L. (1987). Parallel 
Distributed Processing. Explorations in the Micro-
structure of Cognition. Volume 1: Foundations. 
MIT Press. 
Schafer, C., Yarowsky, D (2002).: Inducing transla-
tion lexicons via diverse similarity measures and 
bridge languages. In: Proceedings of CoNLL. 
Schmid, H. (1994). Probabilistic part-of-speech tag-
ging using decision trees. International Conference 
on New Methods in Language Processing, 44?49. 
 
95
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39?47,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semi-supervised Graph-based Genre Classification for Web Pages
Noushin Rezapour Asheghi
School of Computing
University of Leeds
scs5nra@leeds.ac.uk
Katja Markert
L3S Research Center
Leibniz Universit?at Hannover
and School of Computing
University of Leeds
markert@l3s.de
Serge Sharoff
School of Modern
Languages and Cultures
University of Leeds
s.sharoff@leeds.ac.uk
Abstract
Until now, it is still unclear which set of
features produces the best result in au-
tomatic genre classification on the web.
Therefore, in the first set of experiments,
we compared a wide range of content-
based features which are extracted from
the data appearing within the web pages.
The results show that lexical features such
as word unigrams and character n-grams
have more discriminative power in genre
classification compared to features such
as part-of-speech n-grams and text statis-
tics. In a second set of experiments,
with the aim of learning from the neigh-
bouring web pages, we investigated the
performance of a semi-supervised graph-
based model, which is a novel technique
in genre classification. The results show
that our semi-supervised min-cut algo-
rithm improves the overall genre classifi-
cation accuracy. However, it seems that
some genre classes benefit more from this
graph-based model than others.
1 Introduction
In Automatic Genre Identification (AGI), docu-
ments are classified based on their genres rather
than their topics or subjects. Genre classes such
as editorial, interview, news and blog which are
recognizable by their distinct purposes, can be on
any topic. The most important application of AGI
could be in Information Retrieval. If a user could
use the search engine to retrieve web pages from
a specific genre such as news articles, reviews
or blogs, search results could be more beneficial.
With the aim of enhancing search engines, AGI
has attracted a lot of attention (see Section 2).
In this paper, we investigate two important open
questions in AGI. The first question is: what set
of features produces the best result in genre clas-
sification on the web? The drawbacks of exist-
ing genre-annotated web corpora (low inter-coder
agreement; false correlations between topic and
genre classes) resulted in researchers? doubt on the
outcomes of classification models based on these
corpora (Sharoff et al., 2010). Therefore, in order
to answer this question, we perform genre classi-
fication with a wide range of features on a reli-
able and source diverse genre-annotated web cor-
pus. The second question that we investigate in
this paper is: could we exploit the graph structure
of the web to increase genre classification accu-
racy? With the aim of learning from the neigh-
bouring web pages, we investigated the perfor-
mance of a semi-supervised graph-based model,
which is a novel technique in genre classification.
The remainder of this paper is structured as fol-
lows. After reviewing related work in Section 2,
we compare different supervised genre classifica-
tion models based on various lexical, POS-based
and text statistics features in Section 3. Section 4
describes our semi-supervised graph-based classi-
fication experiment, where we use the multi-class
min-cut algorithm as a novel technique in genre
classification. Section 5 concludes the findings
and discusses future work.
2 Related Work
There has been a considerable body of research
in AGI. In previous studies on automatic genre
classification of web pages, various types of fea-
tures such as common words (Stamatatos et al.,
2000), function words (Argamon et al., 1998),
word unigrams (Freund et al., 2006), character
n-grams (Kanaris and Stamatatos, 2007), part-of-
speech tags (Karlgren and Cutting, 1994) , part-
of-speech trigrams (Argamon et al., 1998; San-
tini, 2007), document statistics (e.g. average sen-
tence length, average word length and type/token
ratio) (Finn and Kushmerick, 2006; Kessler et
39
al., 1997), HTML tags (e.g. (Santini, 2007))
have been explored. However, researchers con-
ducted genre classification experiments with dif-
ferent features on different corpora with differ-
ent sets of genre labels. As a result, it is dif-
ficult to compare them. This motivated Sharoff
et al. (2010) to examine a wide range of word-
based, character-based and POS-based features on
the existing genre-annotated corpora. They re-
ported that word unigrams and character 4-grams
outperform other features in genre classification.
However, they concluded that the results cannot
be trusted because of two main reasons. First,
some of these collections exhibit low inter-coder
agreement and any results based on unreliable data
could be misleading. Second, the spurious cor-
relation between topic and genre classes in some
of these corpora was one of the reasons for some
of the very impressive results reported by Sharoff
et al. (2010). These good results were achieved
by detecting topics rather than genres of individ-
ual texts. A similar point was made by Petrenz
and Webber (2010) who examined the impact of
topic change on the performance of AGI systems.
They showed that a shift in topic can have a mas-
sive impact on genre classification models which
are based on lexical features such as word uni-
grams or character n-grams. Therefore, the ques-
tion which set of features produces the best result
in automatic genre classification on the web is still
an open question. In order to investigate this ques-
tion, we perform genre classification with a wide
range of features on a reliable and topically diverse
dataset. Section 3.1 describes the dataset and the
experimental setup.
Most of the current works in the field of AGI
concentrated on extracting features from the con-
tent of the documents and classify them by em-
ploying a standard supervised algorithm. How-
ever, on the web there are other sources of
information which can be utilized to improve
genre classification of web pages. For instance,
the web has a graph structure and web pages
are connected via hyper-links. These connec-
tions could be exploited to improve genre clas-
sification. Various graph-based classification al-
gorithms have been proposed to improve topic
classification for web pages, such as the re-
laxation labelling algorithm (Chakrabarti et al.,
1998), iterative classification algorithm (Lu and
Getoor, 2003), Markov logic networks (Crane
and McDowell, 2012), random graph walk (Lin
and Cohen, 2010) and weighted-vote relational
neighbour algorithm (Macskassy and Provost,
2007). These classification algorithms which uti-
lize hyper-link connections between web pages
to construct graphs, outperformed the classifiers
which are solely based on textual content of the
web pages for topic classification. Such connected
data presents opportunities for boosting the perfor-
mance of genre classification too.
Graph-based web page classification presented
in studies such as (Crane and McDowell, 2012;
Lu and Getoor, 2003; Macskassy and Provost,
2007) on the WebKB dataset (CRAVEN, 1998)
could be considered as genre classification as op-
posed to topic classification. The WebKB dataset
contains web pages from four computer science
departments categorised into seven classes: stu-
dent, faculty, staff, department, course, project
and other. However, this dataset is very specific
to the academic domain with low coverage for
the web overall, whereas we examine graph-based
learning for automatic genre classification of web
pages on a much more general dataset with pop-
ular genre classes such as news, blog and edito-
rial. Moreover, the graph-based algorithms used
on the WebKB dataset are all supervised and were
performed on a very clean and noise free dataset
which was achieved by removing the class other.
Class other contains all the web pages which do
not belong to any other predefined classes. How-
ever, our experiment is in a semi-supervised man-
ner which is a much more realistic scenario on the
web, because it is highly unlikely that for each
web page, we have genre labels for all its neigh-
bouring web pages as well. Therefore, we per-
form our experiment on a very noisy dataset where
neighbouring web pages could belong to none of
our predefined genre classes. Section 4 describes
our semi-supervised graph-based classification ex-
periment, where we use a multi-class min-cut al-
gorithm as a novel technique in genre classifica-
tion.
3 Content-based Classification
3.1 Dataset and Experimental Setup
Petrenz and Webber (2010) and Sharoff et
al. (2010) emphasize that the impact of topic on
genre classification should be eliminated or con-
trolled. In order to avoid the influence of topic on
genre classification, some researchers (e.g. (Sta-
40
Number of # of pages from
Genre the same website Fleiss?s ?
web pages websites max min med
Personal Homepage (php) 304 288 9 1 1 0.858
Company/ Business Homepage (com) 264 264 1 1 1 0.713
Educational Organization Homepage (edu) 299 299 1 1 1 0.953
Personal Blog /Diary (blog) 244 215 9 1 1 0.812
Online Shop (shop) 292 209 23 1 1 0.830
Instruction/ How to (instruction) 231 142 15 1 1 0.871
Recipe 332 116 8 1 1 0.971
News 330 127 12 1 1 0.801
Editorial 310 69 11 1 3 0.877
Conversational Forum (forum) 280 106 11 1 1 0.951
Biography (bio) 242 190 15 1 1 0.905
Frequently Asked Questions (faq) 201 140 8 1 1 0.915
Review 266 179 15 1 1 0.880
Story 184 24 38 1 7 0.953
Interview 185 154 11 1 1 0.905
Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi et
al., 2014). To save space, in this paper we use the abbreviation of genre labels which are specified after
the genre names.
matatos et al., 2000) and (Argamon et al., 1998))
use only topic independent features such as com-
mon words or function words in genre classifica-
tion. However, neither of these features are exclu-
sive to genre classification. Function words and
common words are used in authorship classifica-
tion (e.g. (Argamon et al., 2007)) because they can
capture the style of the authors without being in-
fluenced by the topics of the texts. On the other
hand, word unigrams are a popular document rep-
resentation in topic classification. If we want these
models to capture the genre of documents without
being influenced by their topics or the style of their
authors, we must eliminate the influence of these
factors on genre classification by keeping them
constant across the genre classes in the training
data. That means all the documents in the train-
ing set should be about the same topic and written
by the same person. However, constructing such a
dataset is practically impossible for genre classes
on the web. The other more practical solution to
this problem would be to collect data from various
topics and sources in order to minimize the im-
pact of these factors on genre classification. For
that reason, we (Asheghi et al., 2014) created a
web genre annotated corpus which is reliable (with
Fleiss?s kappa (Fleiss, 1971) equal to 0.874) and
source diverse. We tried to reduce the influence
of topic, the writing style of the authors as well as
the design of the websites on genre classification
by collecting data from various sources and top-
ics. The corpus consists of 3964 web pages from
2522 different websites, distributed across 15 gen-
res (Table 1).
Moreover, we prepared two versions of the
corpus: the original text and the main text cor-
pora. First, we converted web pages to plain
text by removing HTML markup using the Krd-
Wrd tool (Steger and Stemle, 2009). This re-
sulted in the original text corpus which contains
individual web pages with all the textual elements
present on them. Moreover, in order to investigate
the influence of boilerplate parts (e.g. advertise-
ments, headers, footers, template materials, navi-
gation menus and lists of links) of the web pages
on genre classification, we removed the boilerplate
parts and extracted the main text of each web page
using the justext tool
1
. This resulted in the cre-
ation of the main text corpus. This is the first time
that the performance of genre classification mod-
els is compared on both the original and the main
text of the web pages.
Since the outputs of the justext tool for 518 of
the web pages were empty files, the main text cor-
pus has fewer pages. However, the main text cor-
pus still has a balanced distribution with a rela-
tively large number of web pages per category. Ta-
ble 2 compares the number of web pages in the two
versions of the corpus. For all the experiments we
use this corpus via 10-fold cross-validation on the
web pages. Also, in order to minimize the effect
of factors such as topic, the writing style of the au-
thors and the design of the websites even further,
we ensured that all the web pages from the same
website are in the same fold. Many, if not all of the
previous studies in automatic genre classification
on the web ignored this essential step when divid-
ing the data into folds. For machine learning, we
1
http://code.google.com/p/justext/
41
Number of web pages in corpora
Genre Original text Main text
php 304 221
com 264 190
edu 299 191
blog 244 242
shop 292 221
instruction 231 229
recipe 332 243
news 330 320
editorial 310 307
forum 280 251
bio 242 242
faq 201 160
review 266 262
story 184 184
interview 185 183
Table 2: Number of web pages in individual genre
classes in both original text and main text corpora.
chose Support Vector Machines (SVM) because
it has been shown by other researchers in AGI
(e.g. (Santini, 2007)) that SVM produces better or
at least similar results compared to other machine
learning algorithms. We used the one-versus-one
multi-class SVM implemented in Weka
2
with the
default setting. All the experiments are carried out
on both the original text and the main text corpora.
3.2 Features
In order to compare the performance of differ-
ent lexical and structural features used in previous
work, we reimplemented the following published
approaches to AGI: function words (Argamon et
al., 1998), part-of-speech n-grams (Santini, 2007),
word unigrams (Freund et al., 2006) and charac-
ter 4-grams binary representation (Sharoff et al.,
2010). We also explored the discriminative power
of other features such as readability features (Pitler
and Nenkova, 2008), HTML tags
3
and named en-
tity tags in genre classification (Table 3). This is
the first time that some of these features such as
average depth of syntax trees and entity coherence
features (Barzilay and Lapata, 2008) are used for
genre classification. To set a base-line, we used
a list of genre names (e.g. news, editorial, in-
terview, review) as features. We used two differ-
ent feature representations: binary and normalized
frequency. In the binary representation of a doc-
ument, the value for each feature is either one or
zero which represents the presence or the absence
of each feature respectively. In the normalized fre-
2
http://www.cs.waikato.ac.nz/ml/weka/
3
http://www.w3schools.com/tags/ref byfunc.asp
quency representation of a document, the value for
each feature is the frequency of that feature which
is normalized by the length of the document.
For extracting lexical features, we tokenized
each document using the Stanford tokenizer (in-
cluded as part of the Stanford part of speech tag-
ger (Toutanova et al., 2003)) and converted all the
tokens to lower case. For extracting POS tags
and named entity tags, we used the Stanford max-
imum entropy tagger
4
and the Stanford Named
Entity Recognizer
5
respectively. For extracting
some of the readability features such as average
parse tree height and average number of noun and
verb phrases per sentences, we used the Stanford
Parser (Klein and Manning, 2003). However, web
pages must be cleaned before they can be fed to
a parser, because parsers cannot handle tables and
list of links. Therefore, we only used the main
text of each web page as an input to the parser.
For web pages for which the justext tool produced
empty files, we treated these features as missing
values. Moreover, we used the Brown Coherence
Toolkit
6
to construct the entity grid for each web
page and computed the probability of each entity
transition type. This tool needs the parsed version
of the text as an input. Therefore, for web pages
for which the justext tool produced empty files, we
also treated these features as missing values.
3.3 Results and Discussion
Table 4 shows the result of the different feature
sets listed in the previous section on both the orig-
inal text and the main text corpora. At first glance,
we see that the results of genre classification on
the original text corpus are higher than the main
text corpus. This shows that boiler plates contain
valuable information which helps genre classifica-
tion.
Moreover, the results show that binary repre-
sentation of word unigrams is the best performing
feature set when we use the whole text of the web
pages. However, on the main text corpus, charac-
ter 4-grams outperform other features. This con-
firms the results reported in (Sharoff et al., 2010).
The results also highlight that the performance of
POS-based features are much less accurate than
that of textual features such as word unigrams and
character n-grams. The results also show that the
combination of word unigrams, text statistics and
4
http://nlp.stanford.edu/software/tagger.shtml
5
http://nlp.stanford.edu/software/CRF-NER.shtml
6
http://www.cs.brown.edu/ melsner/manual.html
42
Category Features
Token features number of tokens and number of types
normalized frequency of punctuation marks and currency characters
Named entity tags normalized frequency of tags: time, location, organization, person, money, date
average parse tree height
average sentence length and word length
Readability features standard deviation of sentence length and of word length
average number of syllables per word
type/token ratio
average number of noun phrases and verb phrases per sentence
entity coherence features (Barzilay and Lapata, 2008)
HTML tags normalized frequency of tags for: sections / style, formatting, programming,
visual features such as forms, images, lists and tables
Table 3: List of text statistics features explored in this paper
part of speech features resulted in improving genre
classification accuracy (compared to the accuracy
achieved by word unigrams alone), for both origi-
nal and main text corpora. However, while the im-
provement for the main text corpus is statistically
significant
7
, there is no significant difference be-
tween these two models for the original corpus.
Surprisingly, adding part of speech 3-grams to the
word unigrams features decreased the genre clas-
sification accuracy in both original and main text
corpora. The reason could be that the model is
over-fitted on the training data and as a result, it
performs poorly on the test data. Therefore, com-
bining various features will not always improve
the performance of the classification task. More-
over, for extracting POS-based features and some
of the text statistics features we rely on tools such
as part-of-speech taggers and parsers whose per-
formance varies for different genres. Even the best
part-of-speech taggers and parsers are error prone
and cannot be trusted on new unseen genres.
4 Graph-based Classification
Until now we extracted features only from the con-
tent of the web pages. However, other sources
of information such as the connections and the
link patterns between the web pages could be ex-
ploited to improve genre classification. The under-
lying assumption of this approach is that a page is
more likely to be connected to pages with the same
genre category. For example, if the neighbouring
web pages of a particular web page are labelled
as shop, it is more likely that this web page is a
shop too, whereas, it is highly unlikely that it is a
news or editorial. This property (i.e. entities with
similar labels are more likely to be connected) is
known as homophily (Sen et al., 2008). We hy-
7
McNemar test at the significance level of 5%
pothesis that homophily exists for genre classes
and it can help us to improve genre classifica-
tion on the web. In this paper, we use a semi-
supervised graph-based algorithm namely, multi-
class min-cut, which is a novel approach in genre
classification. This algorithm, which is a collec-
tive classification method, considers the class la-
bels of all the web pages within a graph.
4.1 Multi-class Min-cut: The Main Idea
The Min-cut classification algorithm originally
proposed by Blum and Chawla (2001) is based
on the idea that linked entities have a tendency
to belong to the same class. In other words, it
is based on the homophily assumption. There-
fore, it should be able to improve genre classifica-
tion on the web if our hypothesis holds. However,
this technique is a binary classification algorithm,
whereas, we have a multi-class problem. Unfor-
tunately, multi-class min-cut is NP-hard and there
is no exact solution for it. Nevertheless, Ganchev
and Pereira (2007) proposed a multi-class exten-
sion to Blum and Chawla (2001)?s min-cut algo-
rithm by encoding a multi-class min-cut problem
as an instance of metric labelling. Kleinberg and
Tardos (1999; 2002) introduced metric labelling
for the first time. The main idea of metric labelling
for web page classification can be described as fol-
lows:
Assume we have a weighted and undirected
graph G = (V,E) where each vertex v ? V is a
web page and the edges represent the hyper-links
between the web pages. The task is to classify
these web pages into a set of labels L. This task can
be denoted as a function f : V ? L. In order to
do this labelling task in an optimal way, we need to
minimize two different types of costs. First, there
is a non-negative cost c(v, l) for assigning label l
43
Feature set Original text Main text
genre names bin 57.39 29.02
genre name nf 38.29 14.16
function words bin 65.71 55.57
function words nf 74.95 66.86
word unigrams bin 89.32 76.61
word unigrams nf 85.21 74.91
character 4-grams bin 87.96 78.88
POS-3grams bin 73.18 61.23
POS-3grams nf 70.28 57.83
POS-2grams bin 64.10 54.91
POS-2grams nf 68.94 60.76
POS nf 60.14 54.64
text statistics 55.47 59.17
word unigrams bin + text statistics 89.48 78.09
word uni-grams bin + text statistics + POS nf 89.63 78.24
word uni-grams bin + POS 3-grams bin 88.14 75.59
Table 4: Classification accuracy of different features in genre classification. bin and nf refer to the use of
binary and normalized frequency representation of the features respectively.
to web page v. Second, if two web pages v
1
and v
2
are connected together with an edge e with weight
w
e
, we need to pay a cost of w
e
? d(f(v
1
), f(v
2
))
where d(., .) denotes distance between the two la-
bels. A big distance value between labels indicates
less similarity between them. Therefore, the total
cost of labelling task f is:
(1)
E(f) =
?
v?V
c(v, f(v)) +
?
e=(v
1
,v
2
)?E
w
e
? d(f(v
1
), f(v
2
))
Kleinberg and Tardos (1999; 2002) developed
an algorithm for minimizing E(f). However,
their algorithm uses linear programming which is
impractical for large data (Boykov et al., 2001).
In a separate study for metric labelling problems,
Boykov et al. (2001) have developed a multi-way
min-cut algorithm to minimize E(f). This algo-
rithm is very fast and can be applied to large-scale
problems with good performance (Boykov et al.,
2001).
4.2 Selection of unlabelled data
A web page w has different kind of neighbours on
the web such as parents, children, siblings, grand
parents and grand children which are mainly dif-
ferentiated based on the distance to the target web
page as well as the direction of the links (Qi and
Davison, 2009). Since the identification of chil-
dren of a web page (i.e. web pages which have
Cosine # of unlabelled Average # of
similarity web pages neighbours
? 0 103,372 40.65
? 0.1 98,824 39.08
? 0.2 87,834 34.23
? 0.3 70,602 26.46
? 0.4 50,232 17.52
? 0.5 28,437 8.62
? 0.6 13,919 3.77
? 0.7 7,241 1.86
? 0.8 3,772 0.98
? 0.9 1,732 0.44
Table 5: Number of unlabelled web pages with
different cosine similarity thresholds. The last col-
umn shows the average number of neighbours per
labelled page.
direct links from the target web page) is a straight-
forward task as their URLs can be extracted from
the HTML version of the target web page, in this
study, we explore the effect of the target web
pages? children on genre classification. Therefore,
in this experiment, by neighbouring web pages we
mean the web pages? children. In order to collect
the neighbouring web pages, for every web page in
the data set, we extracted all its out-going URLs
and downloaded them as unlabelled data. How-
ever, using all these neighbouring pages could
hurt the genre classification accuracy because web
pages are noisy (e.g. links to advertisements) and
some neighbours could have different genres than
the target page. In order to control the negative im-
pact of such neighbours, we could preselect a sub-
set of neighbours whose content are close enough
to the target page. To implement this idea, we
44
computed the cosine similarity between the web
page w and its neighbouring web pages and used
different threshold to select the neighbours. If u is
a neighbour of w and
??
u and
??
w are the represen-
tative feature vectors of these two web pages re-
spectively, we could compute the cosine similarity
between these two web pages using the following
formula:
cos(
??
w ,
??
u ) =
??
w ?
??
u
?
??
w ??
??
u ?
=
?
n
i=1
w
i
? u
i
?
?
n
i=1
(w
i
)
2
?
?
?
n
i=1
(u
i
)
2
(2)
where n is the number of the dimensions of the
vectors and w
i
is the value of the ith dimension
of the vector
??
w . Since the word unigrams bi-
nary representation model yields the best result for
content-based genre classification, we used this
representation of web pages to construct their fea-
ture vectors. Table 5 shows the number of unla-
belled data and the average number of neighbours
per labelled web page for different cosine similar-
ity thresholds.
4.3 Formulation of Semi-supervised
Multi-class Min-cuts
The formulation of semi-supervised multi-class
min-cut for genre classification involves the fol-
lowing steps:
1. We built the weighted and undirected graph
G = (V,E) where vertices are the web pages
(labelled and unlabelled) and the edges rep-
resent the hyper-links between the web pages
and set the weights to 1.
2. For training nodes, set the cost of the correct
label to zero and all other labels to a large
constant.
3. For test nodes and unlabelled nodes, we set
the cost of each label using a supervised clas-
sifier (SVM) using the following formula:
c(w, l) = 1? p
l
(w) (3)
where c(w, l) is the cost of label l for web
page w and p
l
(w) is the probability of w be-
longing to the label l which is computed by a
supervised SVM using word unigrams binary
representation of the web pages.
4. Set d(i, j), which denotes the distance be-
tween two labels i and j, to 1 if i 6= j and
zero otherwise.
5. Employ Boykov et al. (2001) algorithm to
find the minimum total cost using multiway
min-cut algorithm.
4.4 Results and Discussion
We divided the labelled data into 10 folds again
ensuring that all the web pages from the same
websites are in the same fold. We used 8 folds
for training, one fold for validation and one fold
for testing. We learnt the best cosine similar-
ity threshold using validation data and then eval-
uated it on the test data. Tables 6 and 7 illus-
trate the results of the multi-class min-cut algo-
rithm and the content-based algorithm (both using
word unigrams as features) respectively. The re-
sults show that the multi-class min-cut algorithm
significantly outperforms
8
the content-based clas-
sifier for the cosine similarity equal or greater than
0.8 which was chosen on the validation data. It
must be noted that the result of the multi-class
min-cut algorithm when we used all the neigh-
bouring pages was much lower than the content-
based algorithm due to noise. The results also
shows that some genre classes such as news, edito-
rial, blog, interview and instruction benefited more
than other genre classes from the neighbouring
web pages. Genre categories with improved re-
sults are shown in bold in Table 6. The homophily
property of these genre categories was the reason
behind this improvement. For example, the fact
that a news article is more likely to be linked to
other news articles, whereas, an editorial is more
likely to be linked to other editorials, helped us to
differentiate these two categories further. On the
other hand, we observe no improvement or even
decrease in F-measure for some genre categories
such as frequently asked questions, forums and
company home pages. Two reasons could have
contributed to these results. First, the homophily
property might not exist for these categories. Sec-
ond, the homophily property holds for these cate-
gories, however, in order to benefit from this prop-
erty, we need to examine other neighbours of the
target web pages such as parents, siblings, grand
parents, grand children or even more distant neigh-
8
McNemar test at the significance level of 5%
45
class Recall Precision F1-measure
php 0.928 0.850 0.887
forum 0.925 0.977 0.951
review 0.895 0.832 0.862
news 0.897 0.798 0.845
com 0.897 0.891 0.894
shop 0.860 0.965 0.910
instruction 0.870 0.914 0.892
recipe 0.994 0.991 0.993
blog 0.889 0.879 0.884
bio 0.905 0.948 0.926
editorial 0.800 0.932 0.861
faq 0.902 0.841 0.870
edu 0.957 0.963 0.960
story 0.902 0.943 0.922
interview 0.870 0.809 0.839
overall accuracy = 90.11%
Table 6: Recall, Precision and F-measure for multi-
class min-cut genre classification.
class Recall Precision F1-measure
php 0.938 0.798 0.862
forum 0.943 0.974 0.958
review 0.872 0.859 0.866
news 0.894 0.782 0.835
com 0.920 0.874 0.897
shop 0.849 0.950 0.897
instruction 0.866 0.889 0.877
recipe 0.988 0.988 0.988
blog 0.865 0.841 0.853
bio 0.884 0.926 0.905
editorial 0.765 0.926 0.837
faq 0.866 0.879 0.872
edu 0.950 0.969 0.959
story 0.864 0.941 0.901
interview 0.827 0.785 0.805
overall accuracy = 88.98%
9
Table 7: Recall, Precision and F-measure for content-based
genre classification using word unigrams feature set
bours.
5 Conclusions and Future work
In the first set of experiments, we compared
a diverse range of content-based features in
genre classification using a reliable and source
diverse genre-annotated corpus. The evaluation
shows that lexical features outperformed all
other features. Source diversity of the corpus
minimized the influence of topic, authorship and
web page design on genre classification. In the
second experiment, we significantly improved the
genre classification result using a semi-supervised
min-cut algorithm by employing the children of
the target web pages as unlabelled data. The
results of this method which takes advantage of
the graph structure of the web shows that some
genre classes benefit more than others from the
neighbouring web pages. The homophily property
of genre categories such as news, blogs and edi-
torial was the reason behind the improvement of
genre classification in this experiment. In future
work, we would like to examine the effect of other
types of neighbours on genre classification of
web pages and experiment with other graph-based
algorithms.
References
Shlomo Argamon, Moshe Koppel, and Galit Avneri.
1998. Routing documents according to style. In
9
Please note that in this experiment we had less training
data because we used 8 folds for training, one fold for valida-
tion and one fold for testing. As a result, the accuracy of word
unigrams is slightly lower than the result reported in Table 4.
First international workshop on innovative informa-
tion systems, pages 85?92. Citeseer.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional
lexical features. Journal of the American Society
for Information Science and Technology, 58(6):802?
822.
Noushin Rezapour Asheghi, Serge Sharoff, and Katja
Markert. 2014. Designing and evaluating a reliable
corpus of web genres via crowd-sourcing. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC?14).
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Avrim Blum and Shuchi Chawla. 2001. Learning from
labeled and unlabeled data using graph mincuts. In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 19?26. Morgan
Kaufmann Publishers Inc.
Yuri Boykov, Olga Veksler, and Ramin Zabih. 2001.
Fast approximate energy minimization via graph
cuts. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 23(11):1222?1239.
Soumen Chakrabarti, Byron Dom, and Piotr Indyk.
1998. Enhanced hypertext categorization using hy-
perlinks. In ACM SIGMOD Record, volume 27,
pages 307?318. ACM.
Robert Crane and Luke McDowell. 2012. Investigat-
ing markov logic networks for collective classifica-
tion. In ICAART (1), pages 5?15.
M CRAVEN. 1998. Learning to extract symbolic
knowledge from the world wide web. In Proc. of the
15th National Conference on Artificial Intelligence
(AAAI-98).
46
Aidan Finn and Nicholas Kushmerick. 2006. Learning
to classify documents according to genre. Journal
of the American Society for Information Science and
Technology, 57(11):1506?1518.
J.L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378.
L. Freund, C.L.A. Clarke, and E.G. Toms. 2006. To-
wards genre classification for ir in the workplace.
In Proceedings of the 1st international conference
on Information interaction in context, pages 30?36.
ACM.
Kuzman Ganchev and Fernando Pereira. 2007. Trans-
ductive structured classification through constrained
min-cuts. TextGraphs-2: Graph-Based Algorithms
for Natural Language Processing, page 37.
Ioannis Kanaris and Efstathios Stamatatos. 2007.
Webpage genre identification using variable-length
character n-grams. In Tools with Artificial Intelli-
gence, 2007. ICTAI 2007. 19th IEEE International
Conference on, volume 2, pages 3?10. IEEE.
J. Karlgren and D. Cutting. 1994. Recognizing text
genres with simple metrics using discriminant anal-
ysis. In Proceedings of the 15th conference on Com-
putational linguistics-Volume 2, pages 1071?1075.
B. Kessler, G. Numberg, and H. Schutze. 1997. Au-
tomatic detection of text genre. In Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 32?38.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Jon Kleinberg and Eva Tardos. 1999. Approximation
algorithms for classification problems with pairwise
relationships: Metric labeling and markov random
fields. In focs, page 14. Published by the IEEE Com-
puter Society.
Jon Kleinberg and Eva Tardos. 2002. Approximation
algorithms for classification problems with pairwise
relationships: Metric labeling and markov random
fields. Journal of the ACM (JACM), 49(5):616?639.
Frank Lin and William W Cohen. 2010. Semi-
supervised classification of network data using very
few labels. In Advances in Social Networks Analysis
and Mining (ASONAM), 2010 International Confer-
ence on, pages 192?199. IEEE.
Q. Lu and L. Getoor. 2003. Link-based classification
using labeled and unlabeled data. The Continuum
from Labeled to Unlabeled Data in Machine Learn-
ing & Data Mining, page 88.
Sofus A Macskassy and Foster Provost. 2007. Classifi-
cation in networked data: A toolkit and a univariate
case study. The Journal of Machine Learning Re-
search, 8:935?983.
P. Petrenz and B. Webber. 2010. Stable classification
of text genres. Computational Linguistics, (Early
Access):1?9.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 186?195.
Xiaoguang Qi and Brian D Davison. 2009. Web page
classification: Features and algorithms. ACM Com-
puting Surveys (CSUR), 41(2):12.
Marina Santini. 2007. Automatic identification of
genre in web pages. Ph.D. thesis, University of
Brighton.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise
Getoor, Brian Galligher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
magazine, 29(3):93.
S. Sharoff, Z. Wu, and K. Markert. 2010. The web li-
brary of babel: evaluating genre collections. In Pro-
ceedings of the Seventh Conference on International
Language Resources and Evaluation, pages 3063?
3070.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2000. Text genre detection using com-
mon word frequencies. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 808?814.
Johannes M. Steger and Egon W. Stemle. 2009. Krd-
Wrd ? architecture for unified processing of web
content.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180.
47
Proceedings of the 4th International Workshop on Computational Terminology, pages 86?93,
Dublin, Ireland, August 23 2014.
Evaluating Term Extraction Methods for Interpreters
Ran Xu, Serge Sharoff
Centre for Translation Studies
School of Modern Languages and Cultures
University of Leeds, UK,
{sml3rx,s.sharoff}leeds.ac.uk
Abstract
The study investigates term extraction methods using comparable corpora for interpreters. Simul-
taneous interpreting requires efficient use of highly specialised domain-specific terminology in
the working languages of an interpreter with limited time to prepare for new topics. We evaluate
several terminology extraction methods for Chinese and English using settings which replicate
real-life scenarios, concerning the task difficulty, the range of terms and the amount of mate-
rials available, etc. We also investigate interpreters? perception on the usefulness of automatic
termlists. The results show the accuracy of the terminology extraction pipelines is not perfect,
as their precision ranges from 27% on short texts to 83% on longer corpora for English, 24%
to 31% on Chinese. Nevertheless, the use of even small corpora for specialised topics greatly
facilitates interpreters in their preparation.
1 Introduction
The study investigates term extraction methods using comparable corpora for interpreters. Simultane-
ous interpreting requires efficient use of highly specialised domain-specific terminology in the working
languages of the interpreter. By necessity, interpreters often work in a wide range of domains and have
limited time to prepare for new topics. To ensure the best possible simultaneous interpreting of spe-
cialised conferences where a great number of domain-specific terms are used, interpreters need prepa-
ration, usually under considerable time pressure. They need to familiarise themselves with concepts,
technical terms, and proper names in the interpreters? working languages.
However, there is little research into the use of modern terminology extraction tools and pipelines for
the task of simultaneous interpretation. At the start of computer-assisted termbank development, Moser-
Mercer (1992) overviewed the needs and workflow of practicing interpreters with respect to terminology
and offered some guidelines for developing term management tools specifically for the interpreters. That
study did review the functionalities of some termbanks and term management systems, yet there was no
mention of corpus collection (a fairly new idea at the time) or automatic term extraction.
A few previous studies mentioned the application of corpora as potential electronic tools for the in-
terpreters. Fantinuoli (2006) and Gorjanc (2009) discussed the functions of specific online crawling
tools and explored ways to extract specialised terminology from disposable web corpora for interpreters.
Our work is most closely connected to Fantinuoli?s work on evaluation of termlists obtained from Web-
derived corpora. However, that study relied on a single method of corpus collection and term extraction,
and did not include an investigation into integration of corpus research into practice of interpreter train-
ing.
R?tten (2003) suggested a conceptual software model for interpreters? terminology management, in
which termlists are expected to be extracted (semi-)automatically and then to be revised by their users, the
interpreters, who can concentrate on those terms which are relevant and important to remember. However
the study neither tested the functions of the term extraction tools nor further discussed interpreters?
perception on the usefulness of the automatically lists in their preparation for interpreting tasks.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
86
FR0 FR1 FR2 SM1 SM2
En Zh En Zh En Zh En Zh En Zh
Texts 1 1 9 9 81 86 9 12 74 84
Size 774 1,641 42,006 30,174 206,197 129,350 20,533 40,545 166,499 116,235
Table 1: Corpora used in this study (the size is in words for En, in characters for Zh)
Based on R?tten model, this paper will further test the functions of several term extraction tools for
English and Chinese, and will discuss the interpreters? perception on the usefulness of the automatically-
generated lists in their preparation for interpreting tasks.
In the remainder of this paper we will describe the pipelines for corpus collection and terminology
extraction (Section 2), present the results of their numeric evaluation (Section 3), and discuss options for
future research, including the challenges for the term extraction pipelines in this setting (Section 4).
2 Corpus collection and term extraction
In this section we describe pipelines for interpreters? terminology preparation with the use of term ex-
traction tools. We compare several approaches to corpus compilation and processing for specialised texts
as well as several pipelines for terminology extraction.
2.1 Description of the procedure
Two specialized topics In this study MA student interpreters were invited to prepare for simultane-
ous interpreting tasks on two specialised topics: fast reactors (FR) and Seabed minerals (SM). They
were provided with two monolingual specialised corpora in both English and Chinese for their advance
preparation on each of the topics (FR & SM).
Three term extractors The students started with the FR topic, and were asked to manually generate
their own lists from the provided corpora (FR1 En & Zh) before their simultaneous interpreting exercise
on the topic in both directions (English?Chinese and Chinese?English). After their interpreting tasks,
they were then asked to evaluate the relevance of two monolingual lists (En & Zh) which were automati-
cally generated by one of the three tools (TTC TermSuite, Syllabs Tools and TeaBoat). The purpose here
is to see which tool could extract more relevant terms for the needs of trainee interpreters.
We collected and compared the annotation results from the students to select a single tool with com-
paratively better performance. We then invited the students to prepare for the other topic (SM) by using
automatically-generated lists in the simultaneous interpreting preparation.
2.2 Corpus compilation
There are two types of sources where comparable corpora are from:
1. Conference documents and relevant background documents provided by the conference organisers
2. Specialised corpora collected from the internet using WebBootCat (Baroni and Bernardini, 2004)
Table 1 presents all the corpora we use in this study. FR0/SM0 has been created from a single rele-
vant document, representing the speech that the trainee interpreters were asked to interpret from in this
experiment. We also ran term extraction from this ?corpus? since often a text of this length is the only
source of information given to the interpreters in advance. We tried to balance the terminological diffi-
culty for both languages, even if this was not always possible. After manual term selection, we found
that FR0-Zh contains 147 terms per 591 seconds of delivery (15 terms per minute), FR0-En: 86 terms
per 566 seconds (9 t/min), SM0-Zh: 157 terms per 604 seconds (16 t/min), SM0-En: 169 terms per 750
seconds (14/min).
1
1
Counting the term density per unit of text is not straightforward, because of very different tokenisation rules in Chinese
and English.
87
Seeds (En) Seeds (Zh)
fast breeder reactor ????????
fission ??
decay heat ??
uranium ?
plutonium ?
core damage ????
Fukushima accident ????
nuclear waste ???
fuel cycle ????
coolant ???
Table 2: Parallel keyword seeds on Fast Reactors for FR2
FR1 (En & Zh) and SM1 (En & Zh) are comparable corpora, which represent conference documents
and relevant background documents passed from the conference organisers, including speech outlines,
research papers from experts and research institutes, reports from national and international authorities,
as well as popular science articles, Wikipedia articles, specialised journal articles and interviews, etc.
FR2 (En & Zh) and SM2 (En & Zh) are corpora collected by Web crawling using Bootcat(Baroni and
Bernardini, 2004). For instance, to produce FR2 we started with a set of ten relevant keywords in English
and Chinese as shown in Table 2, then used BootCat to retrieve online resources and generate two corpora
(FR2 En & Zh). All the keyword seeds are from the English speech-FR0 that the students were going to
interpret from, and are therefore considered very relevant and important terms. The Chinese keywords
are the translations of the English ones.
Preprocessing included webpage cleaning (Baroni et al., 2008), as well as basic linguistic processing.
Lemmatisation and tagging for English was done using TreeTagger (Schmid, 1994), while for Chinese
we used ?Segmenter?, an automatic tokenisation tool (Liang et al., 2010) followed by TreeTagger for
POS tagging. Lemmatisation is needed because the keywords in a glossary are expected to be in their dic-
tionary form. Lemmatisation also helps in reducing the nearly identical forms, e.g., sulphide deposit(s).
However, lemmatisation also leads to imperfect terms, e.g., recognise type of marine resource, while the
plurals and participles should be expected in a dictionary form (recognised type of marine resources).
2.3 Automatic term extraction
TTC TermSuite (Daille, 2012) is based on lexical patterns defined in terms of Part-of-Speech (POS)
tags with frequency comparison against a reference corpus using specificity index (Ahmad et al., 1994),
which extracts both single (SWT) and multi-word terms (MWT) outputs their lemmas, part of speech,
lexical pattern, term variants (if any), etc. The most important feature of the TTC TermSuite is the fact
that term candidates can be output with their corresponding term variants. Syllabs Tools (Blancafort
et al., 2013) is a knowledge-poor tool, which is based on unsupervised detection of POS tags, following
the procedure of (Clark, 2003), and on the Conditional Random Field framework for term extraction
(Lafferty et al., 2001). Teaboat (Sharoff, 2012) does term extraction by detecting noun phrases using
simple POS patterns in IMS Corpus Workbench (Christ, 1994) and by applying log-likelihood statistics
(Rayson and Garside, 2000) to rank terms by their relevance to the corpus in question against the Internet
reference corpora for English and Chinese (Sharoff, 2006).
3 Term extraction evaluation
Fantinuoli (2006) used five categories to find the level of specialisation and well-formedness of the
automatically-generated candidate termlist:
1. specialised terms that were manually extracted by the terminologist (and are contained in the refer-
ence term list);
88
FR-TTC FR-Teaboat FR-Syllabs SM-Syllabs
EN ZH EN ZH EN ZH EN ZH
0.541 0.500 0.166 0.435 0.181 0.662 0.117 0.221
Table 3: Krippendorff?s ? for different term lists
2. highly specialised terms that were not detected by the terminologist;
3. non-specialised terms that are commonly used in the field of his study (medicine);
4. general terms that are not specific to the medical field;
5. ill-formed, incomplete expressions and fragments.
Our annotation system extends Fantinuoli?s study because the purpose of annotation in this project is
to give the interpreters possibility to extract relevant terms from all the candidate terms regardless of their
levels of specialisation. Our premise is that interpreters may need relevant terms, both highly specialised
and less specialised, in order to prepare themselves for a conference. The annotators are the end users
of the list, i.e. the trainee interpreters who participated in this research. Since the interpreters are tasked
with translating speeches in the domain, they need themselves to decide what is likely to be relevant
instead of relying on the terminologists who describe the overall structure of the domain. The following
is the five-category annotation system that we used in this research:
R relevant terms (terms closely relevant to the topic), eg. breed ratio, uranium-238, decay heat re-
moval system;
P potentially relevant terms (a category between ?I? and ?R?: they are terms; but annotators are
not sure whether they are closely relevant to the topic of their assignment), eg. daughter nuclide,
neutron poison, Western reactor;
I irrelevant terms (terms not relevant to the topic), eg. schematic diagram, milk crate;
G general words (rather than terms), eg. technical option, monthly donation, Google tag, discussion
forum;
IL ill-formed constructions (parts of terms or chunks of words), eg. var, loss of cooling, separate
sample container, first baseline data, control ranging.
It only took several minutes to generate a termlist after uploading the designated corpus onto TTC
TermSuite, Syllabs Tools and TeaBoat. Each of them automatically generated corresponding monolin-
gual termlists sorted by their term specificity scores. For all the tools we set the threshold of obtaining
500 terms (if possible), as a practical limit for all evaluation experiments.
The trainee interpreters were asked to annotate the list by using the above annotation system. Each of
them reported that it took them about 60 minutes to annotate both lists (in EN& ZH) on each of the topics
(FR & SM). All the annotators were briefed about what counts as terms and the annotation system before
they started their evaluation of term lists. We aim for consistency, yet inter-annotator disagreement does
exist and there is a certain degree of subjectivity in annotation. To measure the level of agreement we
used Krippendorff?s ? over the other measures, such as Fleiss? ?, because Krippendorff?s ? offers an
extension of such measures as Fleiss? ? and Scott?s pi by introducing a distance metric for the pairwise
disagreements, thus making it possible to work with interval-scale ratings, e.g., considering disagreement
between R and P as less severe than between R and I (Krippendorff, 2004).
The values of Krippendorff?s ? (see Table 3) are relatively low. The most common cases of disagree-
ment are between R and P (the boundary between them often depends on the amount of knowledge on
the side of the annotator), but also quite surprisingly between R and IL, when some annotators interpret
ill-formed sequences as a contribution to useful terms.
With the disagreement taken into consideration, our evaluation on the number of relevant terms was
judged by the agreement between at least two annotators among four to six annotators for the topic of
FR. This established the gold standard lists reported in Table 4.
The annotation results from Table 4 for English show that Syllabs generated more relevant terms than
the other two tools from both FR0 and FR1. Both Syllabs and Teaboat generated good numbers of
89
English:
Tool FR0 FR1 FR2 SM1
Syllabs 85/104(82%) 309/500 (62%) 400/500 (80%) 441/500 (88%)
Teaboat 44/56(79%) 232/376 (62%) 413/499 (83%)
TTC NA 136/500 (27%) 287/500 (57%)
Chinese:
Tool FR1 SM1
Syllabs 156/500(31%) 130/500 (26%)
Teaboat 141/450(31%)
TTC 119/500(24%)
Table 4: Number of relevant (R) terms against candidate terms
relevant terms from FR2. In addition, Syllabs? and TeaBoat?s English lists contain more specialised
terms in the domain of FR, such as defence-in-depth, once-through fuel cycle, suppression chamber of
the containment, etc. These specialised terms with relatively low frequency are not included in the TTC?s
list. The terms included in TTC?s list are more general terms, such as steam, energy, liquid, heat, leak,
etc., which are likely to be already known by the trainee interpreters.
The English termlists from all the tools contain a number of repetitions in the form of term variants,
following Daille?s definition as ?an utterance which is semantically and conceptually related to an orig-
inal term? (Daille, 2005). The automatically generated termlists contain the following types of term
variations, which are counted as individual term candidates scattered in the termlists:
Morphological variation: bathymetry vs bathymetric (not different when translated into Chinese)
Anaphoric variation: pollymetallic sulphide deposit vs deposit
Pattern switching: meltdown of the core vs core meltdown; level of gamma radiation vs gamma radia-
tion level
Synonymy in variation: deep sea mining vs deep seabed mining, seabed vs seafloor, ferromanganese
crust vs iron-manganese crust
One the one hand, these variations provide useful lexical information about the term, preparing the
interpreters for what is possible in their assignment; on the other hand, the term variants need to be
explicitly linked, which is possible only in the TTC TermSuite tool.
The annotation results from Table 4 for Chinese show, both Syllabs? and Teaboat?s lists offer obvi-
ously less relevant terms from FR1 compared with the English lists. When we further investigate the
distribution of the term classes in annotations in Table 5, Syllabs? Chinese list on FR1 contains a large
number of ill-formed constructions, including incomplete terms, eg. ?? ?water reactor?, ????
? ?Mile Island nuclear plant? and longer chunks, eg. ?????????, ?????????
????????. Teaboat?s list contains a number of general words, eg. ?? ?development?,??
?production? or?? ?project?. Both categories (G and IL) are frequent in the TTC?s Chinese list.
On the basis of these results, we selected a single tool (Syllabs) with comparatively better performance
in both languages to generate termlists on SM1 (En & Zh) and asked 12 annotators to select the relevant
terms and learn the terms during their interpreting preparation. Among the 500 candidate terms for
English, 441 terms were agreed as relevant by at least two annotators, 266 terms were agreed by five
annotators. Precision rates are 88.2% and 53.2% respectively. On the other hand, only 130 terms were
agreed as relevant by two annotators from the 500 Chinese candidate terms. The precision rate for the
Chinese list is 26%. The results basically replicate the previous findings on FR1.
The other pattern we observe from the current data is that the larger the corpus is, the more relevant
terms the tools can generate. If the corpus is of very limited size (eg. FR0-en has only 774 words), the
TTC TermSuite fails to generate any list for a ?corpus? of only 774 words, while the Syllabs and Teaboat
tools produce shorter lists of 104 or 56 terms respectively. The situation is similar to other studies which
used small (single-document) corpora, e.g., (Matsuo and Ishizuka, 2004).
90
FR1-en FR2-en FR1-zh
Syllabs 500 500 500
R 309 400 156
P 90 53 73
I 15 10 5
G 56 16 46
IL 30 21 220
Teaboat 376 499 450
R 232 413 141
P 33 20 61
I 19 5 7
G 73 29 191
IL 19 32 50
TTC 500 500 500
R 136 287 119
P 48 1 32
I 3 1 4
G 310 205 209
IL 3 6 136
Table 5: Distribution of term annotation classes
4 Conclusions and future work
Reliability of the three term extractors The results show the accuracy of the terminology extraction
pipelines is not perfect, as its precision ranges from 27% on short texts to 83% on longer corpora for
English, 24% to 31% for Chinese. Among the three term extractors (TTC TermSuite, Syllabs Tools
and Teaboat), Syllabs is more reliable in generating more relevant terms in English. All the three tools
perform less satisfactory in generating relevant terms in Chinese. We hypothesise that at least three
factors play an important role here:
1. Chinese is written without explicit word boundaries, while term extraction starts with already to-
kenised texts. Errors of the tokenisation process lead to difficulties in obtaining proper terms, e.g.,
??? ?primary loop? becomes?? ?once?? ?road?, also??????? ?and passive security?
becomes?? ?and not??? ?active???? ?security?, which reduces the chances of detecting?
????? ?passive security? as a term.
2. Word ambiguity in Chinese is high. This leads to POS tagging errors, for example, when nouns are
treated as verbs, and this breaks the POS patterns for term extraction, e.g.,??? ?demonstration
reactor? is treated as??/vn?/v.
3. Chinese exhibits more patterns than captured by the three term extraction tools we tested. For
example,???? ?connect to the grid? is potentially a useful term, which is correctly POS-tagged
as??/v??/vn, but not captured by the patterns in all the tools.
Two of the three causes of the results in Chinese concern text pre-processing. . Further investigation
might be helpful in finding out how the pre-processing steps affect the performance of the term extractors
and which terms are affected by each source of errors.
Manual selection Vs Automatic extraction of terms For the interpreters, manually selecting terms
from a single document of limited size (eg. FR0-en=774 words) is possible. However, when conference
documents amount to the size of FR1 (FR1-en=42,006 words), it took the trainee interpreters 9 hours on
average to extract terms manually and to produce initial termlists, since they had to spend the majority
of their time on reading through fairly complex documents, copying the terms from the texts onto their
own termlists and searching for unfamiliar terms.
91
With the use of automatically-generated termlists on the same preparation task, students in the exper-
iment group spent an average of 4 hours producing their initial bilingual termlists. Therefore half of the
time spent on reading could be saved for the interpreters to get familiar with the concepts relevant to the
terms and further activate the terms for their simultaneous interpreting tasks.
Furthermore, if interpreters are given limited time for preparation, they would not be able to read
through larger corpora of the size of FR2 (FR2-en=206,197 words) and to produce termlists from them
manually. That is probably when such tools we discussed in this article may have obvious advantage over
the manual terms extraction by the interpreters. Moreover, in other studies we also demonstrated that in
addition to providing an automatically-extracted termlist, it is also beneficial to link the terms to their
uses in the concordance lines of the corpus they have been extracted from. This is expected to give the
interpreters an easy access to the context of the terms to see how they are used and get more background
knowledge about the domain.
Feedback from students After doing annotation, the students offered their written feedback on the
termlists generated by the three term extractors. They also commented on the usefulness of the Syllabs?
lists for their interpreting preparation.
They generally reported that the termlists provided many relevant terms on the two topics, and the use
of the lists saved their precious preparation time. Some of them found the lists ?unexpectedly accurate
and complete?, and the presence of irrelevant words in the lists and the repetitions in the lists ?tolerable?
(even taking into account the 24% to 31% precision rate for Chinese).
The students told us they used the lists as an important indicator for the content of the conference
documents and relevant background documents. The lists helped them prioritise their preparation on the
most relevant terms and concepts. Most of them expressed the opinion that if they are given very limited
time, they would prefer to use the automatically-generated lists for their preparation. On the other hand,
students reported that the termlists in Chinese offered much less relevant terms and contained quite a
number of ill-formed constructions compared with the lists in English; therefore they felt the lists in
Chinese were less useful and less reliable.
Extraction of proper names Proper names (including names of organisations, names of places, names
and titles of people) are equally if not more important than terms for interpreters, yet many of them are
not included in the automatically-generated lists by the three term extractors (TTC, Syllabs and Teaboat).
Therefore, named entity extraction tools in addition to term extraction are needed to generate more
complete lists for interpreters? use. This would be further explored in our future research.
File formats, plain text, encodings All the tools we tested can only process plain text (including UTF-
8). Nevertheless, all the meeting documents are normally in one of the word processing formats (.pdf,
.doc, .xls or .ppt) other than .txt. Interpreters need to take some time to convert all the files they obtain
from their customers into plain text before they can possibly use any tool mentioned above.
References
Ahmad, K., Davies, A., Fulford, H., and Rogers, M. (1994). What is a term? The semi-automatic extraction of
terms from text. In Hornby, M. S., P?chhacker, F., and Kaindl, K., editors, Translation studies: an interdisci-
pline, pages 267?278. Amsterdam: John Benjamins Publishing Company.
Baroni, M. and Bernardini, S. (2004). Bootcat: Bootstrapping corpora and terms from the web. In Proc. of
LREC2004, Lisbon.
Baroni, M., Chantree, F., Kilgarriff, A., and Sharoff, S. (2008). Cleaneval: a competition for cleaning web pages.
In Proc. of the Sixth Language Resources and Evaluation Conference, LREC 2008, Marrakech.
Blancafort, H., Bouvier, F., Daille, B., Heid, U., Ramm, A., et al. (2013). TTC Web platform: from corpus com-
pilation to bilingual terminologies for MT and CAT tools. In Proceedings, Conference?Futures in technologies
for translation (TRALOGY II)?.
Christ, O. (1994). A modular and flexible architecture for an integrated corpus query system. In COMPLEX?94,
Budapest.
92
Clark, A. (2003). Combining distributional and morphological information for part of speech induction. In Pro-
ceedings of EACL, pages 59?66.
Daille, B. (2005). Variations and application-oriented terminology engineering. Terminology, 11(1):181?197.
Daille, B. (2012). Building bilingual terminologies from comparable corpora: The TTC TermSuite. In 5th Work-
shop on Building and Using Comparable Corpora at LREC 2012.
Fantinuoli, C. (2006). Specialized corpora from the web and term extraction for simultaneous interpreters. In
Baroni, M. and Bernardini, S., editors, WaCky! Working papers on the Web as Corpus, pages 173?190. Gedit,
Bologna. http://wackybook.sslmit.unibo.it.
Gorjanc, V. (2009). Terminology resources and terminological data management for medical interpreters. In
Andres, D. and P?llabauer, S., editors, Sp?rst Du, wie der Bauch rauf-runter? Fachdolmetschen im Gesund-
heitsbereich. http://www.uni-graz.at/06gorjanc.pdf.
Krippendorff, K. (2004). Reliability in content analysis: Some common misconceptions and recommendations.
Human Communication Research, 30(3).
Lafferty, J., McCallum, A., and Pereira, F. C. (2001). Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of ICML-01.
Liang, M., Li, W., and Xu, J. (2010). Using corpora: a practical coursebook. Foreign Langue Teaching and
Research Press, Beijing.
Matsuo, Y. and Ishizuka, M. (2004). Keyword extraction from a single document using word co-occurrence
statistical information. International Journal on Artificial Intelligence Tools, 13(01):157?169.
Moser-Mercer, B. (1992). Banking on terminology conference interpreters in the electronic age. Meta: Transla-
tors? Journal, 37(3):507?522.
Rayson, P. and Garside, R. (2000). Comparing corpora using frequency profiling. In Proc. of the Comparing
Corpora Workshop at ACL 2000, pages 1?6, Hong Kong.
R?tten, A. (2003). Computer-based information management for conference interpreters-or how will i make my
computer act like an infallible information butler? In Proc. Translating and the computer, pages 14?14.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In International Conference on New
Methods in Language Processing, Manchester.
Sharoff, S. (2006). Open-source corpora: using the net to fish for linguistic data. International Journal of Corpus
Linguistics, 11(4):435?462.
Sharoff, S. (2012). Beyond Translation Memories: Finding similar documents in comparable corpora. In Proc.
Translating and the Computer Conference, London.
93
LAW VIII - The 8th Linguistic Annotation Workshop, pages 82?86,
Dublin, Ireland, August 23-24 2014.
Multiple views as aid to linguistic annotation error analysis
Marilena Di Bari
University of Leeds
mlmdb@leeds.ac.uk
Serge Sharoff
University of Leeds
s.sharoff@leeds.ac.uk
Martin Thomas
University of Leeds
m.thomas@leeds.ac.uk
Abstract
This paper describes a methodology for supporting the task of annotating sentiment in natural
language by detecting borderline cases and inconsistencies. Inspired by the co-training strategy,
a number of machine learning models are trained on different views of the same data. The predic-
tions obtained by these models are then automatically compared in order to bring to light highly
uncertain annotations and systematic mistakes. We tested the methodology against an English
corpus annotated according to a fine-grained sentiment analysis annotation schema (SentiML).
We detected that 153 instances (35%) classified differently from the gold standard were accept-
able and further 69 instances (16%) suggested that the gold standard should have been improved.
1 Introduction
This work pertains to the phase of testing the reliability of human annotation. The strength of our
approach relies on the fact that we use multiple supervised machine learning classifiers and analyse their
predictions in parallel to automatically identify disagreements. Those, in fact, ultimately lead to the
discovery of borderline cases in the annotation, an expensive task in terms of time when carried out
manually.
Predictions with a number of different labels are manually analysed, since they may indicate inconsis-
tencies in the annotation and cases difficult to annotate. Conversely, cases with high agreement suggest
that the annotation schema is reliable. On the one hand, the analysis of those disagreements, in conjunc-
tion with the gold annotations, provides fresh insights about the efficacy of the features provided to the
classifiers for the learning phase. On the other hand, when all the classifiers agree on a wrong annotation,
it is a strong signal of ambiguity in the annotation schema and/or guidelines.
In Section 2 we briefly introduce the data to which we apply the methodology described in Section 3.
In Section 4 we report results. In Section 5 we mention studies related to ours and in Section 6 we draw
conclusions and identify steps for future work.
2 Data
We tested our methodology on the SentiML corpus (Di Bari et al., 2013) for which the annotation
guidelines, as well as the original and annotated texts, are publicly available
1
. The corpus consists of
307 English sentences (6987 tokens), taken from political speeches, TED talks (Cettolo et al., 2012), and
news items from the MPQA opinion corpus (Wilson, 2008).
The aim of its annotation is to encapsulate opinions in pairs, by marking the role that each word takes
(modifier or target). For example, in
?More of you have lost your homes and even more are watching your home values plummet?
there would be two pairs: modifier ?lost? and target ?homes?, and modifier ?values? and target ?plum-
met?. Such two pairs are called appraisal groups.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://corpus.leeds.ac.uk/marilena/SentiML
82
Figure 1: Example of dependency tree. Dependency trees provide features for the machine learning step.
For each of these elements several features are annotated that are believed to improve the task of
sentiment analysis. The study presented here relates to the automatic identification of modifiers and
targets.
3 Methodology
To test our methodology we selected a corpus for which various types of linguistic information related
to appraisal groups were annotated. We started with the identification of modifiers and targets, since this
represents the base of all the other levels of annotation.
To test the reliability of annotation we set 10% of our annotated corpus aside, and performed the
machine learning part of the study on the remaining 90% of our corpus.
The first step consists of preparing the features for the machine learning phase. The optimal set to
model the annotation task varies from problem to problem. We used the following:
? Word features, representing the ordinal identifier, word form, lemma and POS tag of each word.
? Contextual features, representing the lemma and POS tags of the preceding and succeeding words.
? Dependency-based features, representing the reference to the word on which the current token de-
pends in the dependency tree (head) along with its lemma, POS tag and relation type (see Fig-
ure 1) (Nivre, 2005).
? Number of linked modifiers, representing the number of adjectives and adverbs linked to the current
word in the dependency tree.
? Role, representing the predicted role (modifier or target) of the current token in conveying sentiment.
The predictions are computed using fixed syntactic rules.
? Gazetteer-based sentiment. We used the NRC Word-Emotion Association Lexicon (Mohammad,
2011) to represent the a-priori sentiment of each word, i.e. regardless of its context.
Once the features are ready, two or more feature partitions (called views in the co-training strategy)
have to be defined in order to be as orthogonal as possible (Abney, 2007). We opted for a linguistically-
grounded dichotomy: lexical features (word features, role and gazetteer-based sentiment) versus syntac-
tic features (contextual and dependency-based features, number of linked modifiers). The training and
test sets are split accordingly.
At this point, machine learning classifiers are chosen. These need to be confidence-rated, i.e. able to
provide a confidence rate for each prediction. In our experiments we selected Na??ve Bayes, Radial Basis
Function Network and Logistic Regression
2
. These models rely on very different strategies, which makes
the analysis more reliable. We discarded Support Vector Machines since in our preliminary experiments
they achieved high precision (a range between 0.60 and 0.77 across modifiers and targets), but very
low recall (a range between 0.05 and 0.06 across modifiers and targets), which resulted in a very low
F-measure (a range between 0.09 and 0.11 across modifiers and targets).
A model for each combination of view and classifier is then produced and tested on the test set. We
performed a 10-fold cross-validation. In the test phase, we opted for a numerical threshold of 0.67 to
consider the predictions reliable. A prediction with a confidence lower than the threshold is considered
uncertain.
For each instance we obtained six predictions, which potentially differ from one another. The agree-
ment score is calculated for each class in order to identify the most frequent prediction.
2
In each case we used the implementation provided by WEKA (http://www.cs.waikato.ac.nz/
?
ml/weka/).
83
Feature set Classifier
Modifier Target
Precision Recall F
?=1
Precision Recall F
?=1
Lexical
Na??ve Bayes 0.71 0.10 0.48 0.82 0.12 0.43
RBF Network 0.52 0.56 0.54 0.51 0.59 0.55
Logistic regression 0.59 0.42 0.49 0.61 0.48 0.54
Syntactic
Na??ve Bayes 0.46 0.48 0.47 0.82 0.12 0.43
RBF Network 0.49 0.35 0.40 0.55 0.50 0.53
Logistic regression 0.58 0.22 0.32 0.60 0.41 0.49
Table 1: Performance of the classifiers trained on two views, lexical and syntactic. Experiments have
been performed using 10-fold cross-validation.
At this point, only the predictions different from the gold annotations are considered: the higher the
agreement score, the more the instance is interesting in the context of our analysis.
The final step consists of manually investigating such cases to shed light on the errors. In this experi-
ment we opted for the use of a simple protocol based on the following classification schema:
? W (wrong), where the classifiers disagree with the gold annotation, which we judge to be correct.
? A (ambiguous), where the classifiers disagree with the gold annotation and we judge both to be
valid. In such cases, the guidelines need to be clearer or the annotation method could have been
simpler.
? M (to modify), where we judge that the gold annotation is incorrect.
This approach has the advantage of yielding a much reduced subset of instances to be examined man-
ually, with respect to the full set.
4 Results
Table 1 shows the performances of the six models obtained from the training of each combination of
view and classifier, mentioned in Section 3. F-measures for modifiers range between 0.32 and 0.54
for modifiers, and 0.43 and 0.55 for targets. Overall, the RBF Network trained on the lexical view
performs best. However, there is no huge difference in general in performances between the lexical and
the syntactic feature sets, which is good in the light of data sparseness.
Performance on the the empty class (no category assigned) was exceptionally good, as 76% was pre-
dicted out of the gold 77%, whereas the performance on the modifiers was 4% out of the gold 12% and
the performance on the targets was 5% out of the gold 11%. Although the annotation allows each token
to be simultaneously annotated as modifier and target, we have not reported the performances for the MT
class as the cases were not significant. Finally, there was a 15% of cases in which the classifiers were not
confident.
In relation to the manual classification of errors (see final paragraph of Section 3) we found that, out
of the total test instances (2066), in 436 cases the most predicted class differed from the gold standard:
the label W was assigned 214 times (49%), the label A was assigned 153 times (35%), the label M
was assigned 69 times (16%). W was mostly assigned when the modifier or the target was correctly
identified, but not its counterpart in the pair (e.g., ?way forward?, ?blame society?, ?wrong side?). It was
also assigned when a word was correctly identified as evoking sentiment (e.g., ?destroy?, ?flourish?),
but only the first of two or more targets was identified (e.g., ?women and children?, ?the city and the
country?).
A was assigned when an adverb was annotated as modifier (e.g., ?through corruption?, ?seize gladly?,
?tragically reminded?): these are cases in which human annotators decide to include the adverb if it is
regarded as important for the sentiment. Other cases in which the label has been used is with compound
modifiers (e.g., ?face to face?, ?in the face of?), phrasal verbs (e.g., ?turn back?, ?carried forth?, ?came
forth?) and difficult couples to link (e.g., ?instruments with which we meet them? [challenges]). Finally,
this label was also used in cases in which the prediction was sensible, but considered less accurate than
the gold one (e.g., in ?enjoy relative plenty?, the gold standard was ?enjoy plenty? and the classifiers
84
predicted ?relative plenty?).
M was assigned when another modifier had been wrongly annotated by the annotator, instead of mod-
ifying the value of the force of the current one (e.g., in ?much more?, only ?more?? should have been
annotated with high force), in the case of couples with no sentiment (e.g., ?future generations?, ?different
form?), of couples not previously identified (e.g., ?stairway filled with smoke?, ?icy river?) or couples
that could have been annotated in an easier way (e.g., ?provoke us to step up and do something?, ?image
resonates with us?).
5 Related work
Evaluating the reliability of human annotation is a challenging and widely studied task (Pustejovsky
and Stubbs, 2012). The standard solution is the measurement of an inter-annotator agreement (IAA)
coefficient according to a variety of formulae that depend on the characteristics of the annotation set-
ting (Artstein and Poesio, 2008).
For example, in the case of Wilson (2008) and Read and Carroll (2007), it was useful to understand
inconsistencies in the selection of the span for attitudes and targets. Since this represents only one of
the commonly recognized challenges, some studies have focused on practically testing a methodological
framework for schema development for fine-grained and quality semantic annotations. (Bayerl et al.,
2003).
Our approach varies from the standard procedure in ways similar to that of Snow et al. (2008). For
each expert annotator (six in total) they trained a system using only the judgements provided by these
annotators, and then created a test set using the average of the responses of the remaining five labellers on
that set. This resulted in six independent expert-trained systems. The difference with our methodology
is that we trained six independent classifiers, but based on judgements of only one human annotator, and
compared the average of the responses of six classifiers with the gold standard.
Jin et al. (2009) also used the strategy of selecting the labelled sentences agreed upon by their classi-
fiers and achieved good performances in the task of identifying opinion sentences.
Finally, our methodology is also similar to one of those mentioned by Yu (2014). The author used the
traditional co-training strategy, i.e. providing a small pool of unlabelled data to two classifiers with con-
fidence rates, in order to obtain automatically labelled examples that would be added to an initial set of
labelled ones. Subsequently, this final large set is used to train the the two classifiers and a combination
of them (constructed by multiplying their predictions) is eventually the one used to label new docu-
ments. Five strategies were applied to obtain the views: (a) using unigrams and bigrams as features, (b)
randomly splitting the feature set in two, (c) using two different supervised learning algorithms because
they would provide useful examples to each other since based on different learning assumptions; (d)
randomly splitting the training set, and (e) applying a character-based language model (CLM) and a bag-
of-words model (BOW). We extended the third strategy by using three classifiers and two different views
for each of them, and by applying this to the task of annotation validation rather than semi-supervised
learning.
6 Conclusions
In this paper we have presented a methodology that makes use of multiple classifiers (based on different
views) in order to detect inconsistent annotations and borderline cases. In our test set, we found that
in 35% of the wrongly classified cases the predictions were different but acceptable, and in the 16% of
them the predictions suggested that the gold standard was wrong. On the other hand, the data resulting
from such procedure related to non-disagreeing predictions can be regarded as expression of either the
efficacy of the annotation schema and guidelines or the features used for the machine learning step.
Our next goal is to improve the performances of the classifiers over the instances that were incorrectly
handled, currently accounting for the 26% in our test set. We will also test the same methodology over
the extraction of the link between targets and modifiers (appraisal groups). The machine learning models,
the datasets and the error analysis are publicly available in order to ensure reproducibility
3
.
3
http://corpus.leeds.ac.uk/marilena/SentiML/LAW2014_error_analysis.zip
85
References
Steven Abney. 2007. Semisupervised Learning for Computational Linguistics. Chapman & Hall/CRC, 1st edition.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist.,
34(4):555?596, December.
Petra S. Bayerl, Harald L?ungen, Ulrike Gut, and Karsten I. Paul. 2003. Methodology for reliable schema de-
velopment and evaluation of manual annotations. In Proceedings of the Workshop on Knowledge Markup and
Semantic Annotation at the Second International Conference on Knowledge Capture (K-CAP 2003, pages 17?
23.
Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit
3
: Web inventory of transcribed and translated
talks. In Proceedings of the 16
th
Conference of the European Association for Machine Translation (EAMT),
pages 261?268, Trento, Italy, May.
Marilena Di Bari, Serge Sharoff, and Martin Thomas. 2013. SentiML: Functional annotation for multilingual sen-
timent analysis. In DH-case 2013: Collaborative Annotations in Shared Environments: metadata, vocabularies
and techniques in the Digital Humanities, ACM International Conference Proceedings.
Wei Jin, Hung H. Ho, and Rohini K. Srihari. 2009. Opinionminer: a novel machine learning system for web opin-
ion mining and extraction. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge
discovery and data mining, KDD ?09, pages 1195?1204, New York, NY, USA. ACM.
Saif Mohammad. 2011. From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences,
and Humanities, pages 105?114, Portland, OR, USA, June.
Joakim Nivre. 2005. Dependency grammar and dependency parsing. Technical report, V?axj?o University.
James Pustejovsky and Amber Stubbs. 2012. Natural Language Annotation for Machine Learning. Oreilly and
Associate Series. O?Reilly Media, Incorporated.
Jonathon Read, David Hope, and John Carroll. 2007. Annotating expressions of appraisal in English. In Proceed-
ings of the Linguistic Annotation Workshop, LAW ?07, pages 93?100, Stroudsburg, PA, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast?but is it good?:
Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP ?08, pages 254?263, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh.
Ning Yu. 2014. Exploring co-training strategies for opinion detection. Journal of the Asssociation for Information
Science and Technology.
86
