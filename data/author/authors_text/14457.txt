Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 655?665, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Do Neighbours Help? An Exploration of Graph-based Algorithms
for Cross-domain Sentiment Classification
Natalia Ponomareva
Statistical Cybermetrics Research group,
University of Wolverhampton, UK
nata.ponomareva@wlv.ac.uk
Mike Thelwall
Statistical Cybermetrics Research group,
University of Wolverhampton, UK
m.thelwall@wlv.ac.uk
Abstract
This paper presents a comparative study
of graph-based approaches for cross-domain
sentiment classification. In particular, the
paper analyses two existing methods: an
optimisation problem and a ranking algorithm.
We compare these graph-based methods with
each other and with the other state-of-
the-art approaches and conclude that graph
domain representations offer a competitive
solution to the domain adaptation problem.
Analysis of the best parameters for graph-
based algorithms reveals that there are no
optimal values valid for all domain pairs
and that these values are dependent on the
characteristics of corresponding domains.
1 Introduction
The sentiment classification (SC) is an active area
of research concerned automatic identification of
sentiment strength or valence of texts. SC of
product reviews is commercially important and
widely researched but it typically needs to be
optimised separately for each type of product (i.e.
domain). When domain-specific data are absent
or insufficient the researchers usually seek solution
in semi-supervised, unsupervised or cross-domain
approaches. In this paper, we focus on cross-domain
methods in order to take advantage of the huge
amount of annotated sentiment data available on the
Internet. Our aim is to find out to what extent it is
possible to learn sentiment phenomena from these
data and transfer them to new domains rather than
induce them from scratch for each new domain.
Previous research has shown that models trained on
one data usually give much worse results on another,
especially when both data sets belong to completely
different domains. This is largely because the
sentiment words and their valences depend a lot
on the domain where they are expressed. The
first problem concerns the words that can convey
opposite sentiments with respect to the context or
domain. For example, a word ?ridiculous? in
book reviews may express a negative meaning when
talking about a book content, however for reviews
on electronics this word can bear a positive meaning
when talking about prices. Another and more
common problem is related to sentiment words that
are specific for each domain. For instance, words
like ?boring?, ?inspiring?, ?engaging? are very
common in book reviews but it is almost impossible
to find them in reviews on electronics. At the same
time, the electronics domain can contain words like
?defective?, ?refund?, ?return?, ?customer service?,
which are very unusual for book reviews.
Several cross-domain approaches have been
suggested recently to solve the problem of accuracy
loss in cross-domain sentiment classification,
namely Structural Correspondence Learning (SCL)
(Blitzer et al2007), the graph-based approach
(Wu et al2009) and Spectral Feature Alignment
(SFA) (Pan et al2010). In this paper, we explore
graph-based algorithms which refer to a group of
techniques that model data as a graph of documents.
This data representation takes into account not only
document contents but also document connectivity
which is modeled as document sentiment similarity
rather than content similarity. Our interest in graph
655
algorithms is two-fold. First, graph-based domain
representations can benefit from two independent
sources of information: scores given by a machine
learning technique which indicate the probability
of a document to belong to a sentiment class and
similarity relations between documents. Second,
unlike other suggested methods, this approach can
be easily adapted to multiple classes, which makes
it possible to classify documents using finer-grained
sentiment scales.
Different graph-based algorithms have been
applied to several SA tasks (Pang and Lee, 2005;
Goldberg and Zhu, 2006; Wu et al2009), but
no comparison has been made to find the most
appropriate one for SC. Moreover, in the framework
of the domain adaption task, we come across the
problem of choosing the best set of parameters,
which, as we further demonstrate, depends on
the characteristics of a corresponding domain
pair. Unfortunately, no study has investigated this
problem. (Pang and Lee, 2005; Goldberg and
Zhu, 2006) exploited the graph-based approach for
a semi-supervised task and experimented with data
belonging to one domain and, therefore did not
come across this issue. The work of (Wu et al
2009) lacks any discussion about the choice of
the parameter values; the authors set some values
equal for all domains without mentioning how they
obtained these numbers.
The present research brings several contributions.
First, we compare two graph-based algorithms
in cross-domain SC settings: the algorithm
exploited in (Goldberg and Zhu, 2006), which
seeks document sentiments as an output of an
optimisation problem (OPTIM) and the algorithm
adopted by (Wu et al2009), that uses ranking
to assign sentiment scores (RANK). Second,
as document similarity is a crucial factor for
satisfactory performance of graph-based algorithms,
we suggest and evaluate various sentiment similarity
measures. Sentiment similarity is different from
topic similarity as it compares documents with
respect to the sentiment they convey rather than
their topic. Finally, we discover the dependency
of algorithm parameter values on domain properties
and, subsequently, the impossibility to find universal
parameter values suitable for all domain pairs.
We discuss a possible strategy for choosing the
best set of parameters based on our previous
study (Ponomareva and Thelwall, 2012), where
we introduced two domain characteristics: domain
similarity and domain complexity and demonstrated
their strong correlation with cross-domain accuracy
loss.
The rest of the paper is structured as follows.
In Section 2 we give a short overview of related
works on cross-domain SC. Section 3 describes and
compares the OPTIM and RANK algorithms. In
Section 4 we discuss an issue of document similarity
and select document representation that correlates
best with document sentiments. Experimental
results are described in Section 5 followed by a
discussion on the strategy for choosing the best
parameter values of the algorithms (Section 6).
Finally, in Section 7 we summarise our contributions
and discuss further research.
2 Related work
Cross-domain sentiment analysis has received
considerable attention during the last five years
and, since then, several approaches to tackle this
problem have emerged. The most straightforward
approach is to use an ensemble of classifiers as
tested in several works (Aue and Gamon, 2005; Li
and Zong, 2008). It is a well-explored technique in
machine learning concerned with training classifiers
on domains where annotated data are available
and then, combining them in ensembles for the
classification of target data. Aue and Gamon (2005)
studied several possibilities to combine data from
domains with known annotations and came up with
the conclusion that an ensemble of classifiers in
a meta-classifier gives higher performance than a
simple merge of all features.
Structural Correspondence Learning (SCL)
(Blitzer et al2007) is another domain transfer
approach, which was also tested on parts of speech
(PoS) tagging (Blitzer et al2006). Its underlying
idea is to find correspondences between features
from source and target domains through modeling
their correlations with pivot features. Pivot features
are features occurring frequently in both domains,
which, at the same time, serve as good predictors
of document classes, like the general sentiment
words ?excellent? and ?awful?. The extraction
656
of pivot features was made on the basis of their
frequency in source and target corpora and their
mutual information with positive and negative
source labels. The correlations between the pivot
features and all other features were modeled using
a supervised learning of linear pivot predictors to
predict occurrences of each pivot in both domains.
The proposed approach was tested on review data
from 4 domains (books, DVDs, kitchen appliances
and electronics) and demonstrated a significant
gain of accuracy for most domain pairs compared
to the baseline. However, for a few domains the
performance degraded due to feature misalignment:
the narrowness of the source domain and diversity
of the target domain created false projections of
features in the target domain. The authors proposed
to correct this misalignment with a small amount of
annotated in-domain data.
Spectral Feature Alignment (SFA), introduced by
Pan et al2010), holds the same idea as SCL,
i.e., an alignment of source and target features
through their co-occurrences with general sentiment
words. But instead of learning representations of
pivots in source and target domains the authors
used spectral clustering to align domain-specific and
domain-independent words into a set of feature-
clusters. The constructed clusters were then used for
the representation of all data examples and training
the sentiment classifier. This new solution yields a
significant improvement on cross-domain accuracy
compared with SCL for almost all domain pairs.
The method suggested by Bollegala et al2011)
also relies on word co-occurrences. In particular,
the authors presented a method for automatic
construction of a sentiment-sensitive thesaurus
where each lexical element (either unigram or
bigram) is connected to a list of related lexical
elements which most frequently appear in the
context expressing the same sentiment. This
thesaurus is then used on the training step to
expand feature vectors with related elements to
overcome the feature mismatch problem. The
method was tested on the same data set as SCL
and SFA but unlike previous works the authors
used a combination of domains to create sentiment-
sensitive thesauri and to train the cross-domain
classifier. They compare the accuracy of their
approach with an average accuracy over the results
with the same target domain given by SCL and
SFA, and concluded that their method surpasses all
existing approaches. However, we think that such
a comparison is not optimal. Indeed, using the
approach described in (Ponomareva and Thelwall,
2012) we can choose the most appropriate data
for training our classifier rather than averaging the
results given by all data sets. Therefore, instead of
average accuracies, the best accuracies with respect
to the same target domain should be compared. This
comparison leads to opposite conclusions, namely
that SCL and SFA significantly outperform the
sentiment-sensitive thesaurus-based method.
Unlike the approaches mentioned above,
graph-based algorithms exploit relations between
documents for finding the correct document scores.
We describe them in more details in the next section.
3 Graph-based algorithms
In this section we present and compare 2 graph-
based algorithms which use similar graph structures
but completely different methods to infer node
scores. The RANK algorithm (Wu et al
2009) is based on node ranking, while OPTIM
(Goldberg and Zhu, 2006) determines solution of
graph optimisation problem. Initially OPTIM was
applied for the rating-inference problem in a semi-
supervised setting. This study, for the first time,
analyses its behaviour for cross-domain SC and
compares its performance with a similar approach.
3.1 OPTIM algorithm
The OPTIM algorithm represents graph-based
learning as described in (Zhu et al2003). Let us
introduce the following notation:
? G = (V,E) is an undirected graph with 2n
nodes V and weighted edges E.
? L stands for labeled data (source domain data)
and U for unlabeled data (target domain data).
? xi is a graph node which refers to a document,
f(xi) is a true label of a document which is
supposed to be unknown even for annotated
documents, allowing for noisy labels. Each
xi ? L is connected to yi which represents
a given rating of a document. The edge
weight between x ? i and yi is a large number
657
Figure 1: Graph models for the OPTIM (A) and RANK (B) algorithms
M introducing the hard constraints between
labeled documents and their ratings. Each xi ?
U is connected to y?i that stands for predicted
rating of a document. The edge weight between
xi and y?i is equal to 1.
? Each unlabeled document xi is connected to its
k nearest labeled documents kNNL(i) (source
domain neighbours). The weight between xi
and xj ? kNNL(i) is measured by a given
similarity w and denoted a ? wij .
? Each unlabeled document xi is connected to
its k? nearest unlabeled documents k?NNU (i)
(target domain neighbours). The weight
between xi and xj ? k?NNU (i) is denoted by
b ? wij .
Figure 1A illustrates the graph structure
described. The algorithm is based on the assumption
that the rating function f(x) is smooth with respect
to the graph, so there are no harsh jumps of
sentiment between nearest neighbours. To satisfy
the smoothness condition sentiment variability
between the closest nodes should be minimised.
Another requirement is to minimise the difference
between each initial node rating and its final value,
although in the case of unlabeled nodes this is
optional. Taking into consideration the conditions
mentioned the sentiment-inference problem can be
formulated as an optimisation problem:
L(f) =
?
i?L
M(f(xi)? yi)
2 +
?
i?U
(f(xi)? y?i)
2+
?
i?U
?
j?kNNL(i)
awij(f(xi)? f(xj))
2+
?
i?U
?
j?k?NNU (i)
bwij(f(xi)? f(xj))
2 ? min (1)
After the substitutions ? = ak + bk? and ? = ba the
final optimisation problem can be written as:
L(f) =
?
i?L
M(f(xi)? yi)
2 +
?
i?U
[(f(xi)? y?i)
2+
?
k + ?k?
(
?
j?kNNL(i)
wij(f(xi)? f(xj))
2+
?
j?k?NNU (i)
?wij(f(xi)? f(xj))
2)]? min (2)
where ? defines the relative weight between
labeled and unlabeled neighbours, while ? controls
the weight of the graph-based solution with respect
to the primarily obtained supervised sentiment
scores.
658
The minimum-loss function which gives the
solution of the optimisation problem can be found
by setting the gradient to zero. For more details on
the problem solution see (Goldberg and Zhu, 2006).
3.2 RANK algorithm
The RANK algorithm has a similar graph structure
(Figure 1B): nodes represent labeled and unlabeled
documents and there is a parameter (in this case
?) that controls the relative importance of labeled
data over unlabeled data and is an analogue
of ? in OPTIM. The weight of edges between
different nodes is also measured by document
similarity. However, there are no edges between
nodes and their initial sentiments because RANK
is an iterative algorithm and each iteration gives
new scores to unlabeled nodes while labeled
nodes remain constant. More precisely, on each
iteration sentiment scores of unlabeled documents
are updated on the basis of the weighted sum of
sentiment scores of the nearest labeled neighbours
and the nearest unlabeled neighbours. The process
stops when convergence is achieved, i.e. the
difference in sentiment scores is less than a
predefined tolerance.
Using the same notation as for OPTIM we can
formulate the iterative procedure in the following
way:
fk(xi) =
?
j?kNNL(i)
?wijf(xj)+
?
j?k?NNU (i)
(1? ?)wijfk?1xj) (3)
where fk(xi) is the node sentiment score on the
k-th iteration. Document scores are normalised
after each iteration to ensure convergence (Wu et
al., 2009). It is worth noting that initially the
authors did not consider having a different number
of neighbours for the source and target domains.
Analysing differences in the graph structures and
assumptions of both models we can say that they
are almost identical. Even the smoothness condition
holds for the RANK algorithm as the score of a
node is an averaged sum of the neighbours. The
only principal difference concerns the requirement
of closeness of initial and final sentiment scores for
OPTIM. This condition gives more control on the
stability of the algorithm performance.
4 Measure of document similarity
A good measure of document similarity is a key
factor for the successful performance of graph-based
algorithms. In this section we propose and evaluate
several measures of document similarity based on
different vector representations and the cosine of
document vectors.
Following (Goldberg and Zhu, 2006) and (Pang
and Lee, 2005) we consider 2 types of document
representations:
- feature-based: this involves weighted
document features. The question here concerns the
features to be selected. When machine learning
is employed the answer is straightforward: the
most discriminative features are the best ones for
our task. However, we assume that we do not
know anything about the domain when measuring
sentiment similarity and, thus, we should establish
the appropriate set of features only relying on our
prior knowledge about sentiment words. According
to previous studies, adjectives, verbs and adverbs are
good indicators of sentiment (Pang and Lee, 2008),
therefore, we keep only unigrams and bigrams that
contain these PoS. We test two feature weights - tfidf
and idf (Ftfidf and Fidf in Table1 respectively). The
evident drawback of such a vector representation
concerns the discarding of nouns, which in many
cases also bear sentiments. To overcome this issue
we introduce a new measure that uses sentiment
dictionaries to add nouns expressing sentiments
(Fidf+SOCAL).
- lexicon-based: uses sentiment dictionaries to
assign scores to lexical elements of two types: words
or sentences. The dimension of the corresponding
document vector representation conforms with the
granularity of the sentiment scale. For example,
in case of binary sentiment scales, a document
vector consists of two dimensions, where first
component corresponds to the percentage of positive
words (sentences) and the second component -
to the percentage of negative words (sentences).
To assign sentiment scores to lexical elements
we exploit different sentiment resources, namely
659
domain Ftfidf Fidf Fidf+SOCAL W2 W10 S2
BO 0.61 0.62 0.64 0.49 0.50 0.44
DV 0.61 0.61 0.64 0.56 0.56 0.51
EL 0.62 0.66 0.68 0.47 0.49 0.46
KI 0.65 0.67 0.68 0.51 0.54 0.53
Table 1: Correlation for various similarity measures with sentiment scores of documents across different domains.
SentiWordNet (Esuli and Sebastiani, 2006), SO-
CAL (Taboada et al2010) and SentiStrength
(Thelwall et al2012). The scores of sentences
are averaged by the number of their positive and
negative words. Preliminary experiments show a big
advantage of SO-CAL-dictionaries comparing with
other resources. SentiWordNet demonstrates quite
an unsatisfactory performance, while SentiStrength,
being very precise, has an insufficient scope and,
therefore, finds no sentiment in a substantial number
of documents.
The best document representation is selected
on the basis of its correlation with the sentiment
scores of documents. To compute correlations
for feature-based measures, we take 1000 features
with highest average tfidf weights. Table 1 gives
the results of a comparison for two document
representations and their different settings. Here
W2 and S2 stand for word-based and sentence-
based representations of dimension 2 and W10 -
for word-based representation of dimension 10.
All use SO-CAL-dictionaries to assign scores to
words or sentences. Feature-based representations
demonstrate significantly better correlations with
document sentiments although for some domains,
like DV, the lexical element-based representation
produces a similar result. Integration of SO-CAL-
dictionaries gives insignificant contribution into the
overall correlation, which maybe due to the limited
number of features participated in the analysis.
In our further experiments we use both Fidf and
Fidf+SOCAL document representations.
5 Experimental results
Our data comprises Amazon product reviews on 4
topics: books (BO), electronics (EL), kitchen (KI)
and DVDs (DV), initially collected and described
by Blitzer et al2007). Reviews are rated using
a binary scale, 1-2 star reviews are considered as
negative and 4-5 star reviews as positive. The data
within each domain are balanced: they contain 1000
positive and 1000 negative reviews.
First, we compute a baseline for each domain
pair by training a Support Vector Machines (SVMs)
classifier using one domain as training data and
another as test data. We choose SVMs as our
main learning technique because they have proved
to be the best supervised algorithm for SC (Pang
and Lee, 2008). In particular, we use the LIBSVM
library (Chang and Lin, 2011) and a linear kernel
function to train the classifier. For the feature
set we experiment with different features and
feature weights and conclude that unigrams and
bigrams weighted with binary values yield the best
performance.
Figure 2: Baseline accuracy for cross-domain SC.
(x-axis - source domains, y-axis - target domains).
Figure 2 presents an isoline image of cross-
domain accuracies for all domain pairs.1 Products
on the x-axis represent source domains and products
1We should point out that in the images the shading between
points is not intended to suggest interpolation but is used to
highlight the overall pattern. Of course the pattern depends on a
domain order on the axes, therefore, similar domains are placed
together to make the regions with high and low accuracies
evident.
660
on the y-axis represent target domains. The isolines
image of the baseline accuracy delivers a good
representation of domain relations. In particular, we
can observe two regions with the highest accuracy
(EL-KI, KI-EL) and (BO-DV, DV-BO) and two
regions with a big performance drop (EL-BO, EL-
DV, KI-BO, KI-DV) and (BO-EL, BO-KI, DV-
EL, DV-KI). As shown in our previous study
(Ponomareva and Thelwall, 2012) the first two
regions conform with the most similar domain pairs
BO, DV and EL, KI.
OPTIM and RANK require the setting of several
parameters: (k, k?, ?, ?) for OPTIM and (k, k?, ?)
for RANK. As it is computationally expensive to
iterate over all possible values of parameters we first
run the algorithms on a small matrix of parameters
and then apply the gradient descent method which
takes the values with highest accuracy as its starting
points. We execute both algorithms with different
similarity measures, Fidf and Fidf+SOCAL. In
Table 2 OPTIM and RANK run with Fidf , while
OPTIM+SOCAL and RANK+SOCAL run with
Fidf+SOCAL. We give the best accuracies achieved
by these algorithms for each domain pair. Unlike
the correlations, the accuracies increase significantly
with the integration of SO-CAL-dictionaries, the
average improvement is about 3% for RANK and
1.5% for OPTIM. In general, RANK consistently
outperforms OPTIM for all domain pairs, OPTIM
shows competitive performance only for the pairs
of similar domains BO-DV, KI-EL and EL-KI.
We should also point out that OPTIM is more
time-consuming as it requires expensive matrix
operations. Due to these advantages of the RANK
algorithm, we mostly focus on its analysis in the rest
of the paper.
It is interesting to examine the performance of
RANK on the basis of the 3D isolines image (Figure
3B). The isolines stretch from left to right indicating
that accuracy is almost independent of the source
domain. Such behaviour for RANK suggests a
positive answer to our question stated in the title:
even if domains are quite different, neighbours from
the same domain will fix these discrepancies. This
property is definitely a big advantage of the RANK
algorithm in the context of the cross-domain task as
it minimises the importance of the source domain.
Obviously more experiments with different data
must be accomplished to prove this conclusion with
a higher level of confidence.
We also compare graph-based algorithms with
other state-of-the-art approaches, such as SCL and
SFA (Table 2, Figure 3). The best results in Table 2
are highlighted and if the difference is statistically
significant with ? = 0.05 the corresponding
accuracy is underlined. Note that we compare
graph-based approaches against the others but not
each other, therefore, if the result given by RANK is
underlined it means that it is statistically significant
only in comparison with SCL and SFA and not with
OPTIM. According to Table 2, RANK surpasses
SCL for almost all domain pairs with an average
difference equal to 2%. Interestingly, without
using SO-CAL-dictionaries RANK loses to both
SCL and SFA for almost all domain pairs. The
advantage of RANK over SFA is disputable as
there is not much consistency about when one
algorithm outperforms another, except that SFA is
better overall for close domains. However Figure
3 suggests an interesting finding: that for domains
with different complexities swapping source and
target alchanges the method that produces the
best performance. A comparison of RANK and SCL
on the Chinese texts given by (Wu et al2009)
shows the same phenomenon. It seems that RANK
works better when the target domain is simpler,
maybe because it can benefit more from in-domain
neighbours of the less rich and ambiguous domain.
In the future, we plan to increase the impact of
lexically different but reliably labeled source data
by implementing the SFA algorithm and measuring
document similarity between feature clusters rather
than separate features.
6 Strategy for choosing optimal
parameters
The results of the RANK and OPTIM algorithms
presented in the previous section represent the
highest accuracies obtained after running gradient
descent method. Table 3 lists the best parameter
values of the RANK algorithm over several domain
pairs. Our attempt to establish some universal values
valid for all domain pairs was not successful as the
choice of the parameters depends upon the domain
properties. Of course, in real life situations we do
661
source-target baseline OPTIM RANK OPTIM+ RANK+ SCL SFA
SOCAL SOCAL
BO-EL 70.0 74.0 77.2 74.4 79.8 77.5 72.5
BO-DV 76.5 78.6 77.4 79.9 79.8 75.8 81.4
BO-KI 69.5 74.6 78.6 77.3 82.8 78.9 78.8
DV-BO 74.4 78.8 78.9 80.5 82.1 79.7 77.5
DV-EL 67.2 73.6 78.8 74.4 80.9 74.1 76.7
DV-KI 70.2 75.6 80.4 77.3 83.2 81.4 80.8
EL-BO 65.5 67.8 69.9 69.5 73.6 75.4 75.7
EL-DV 71.3 74.2 72.6 75.6 77.0 76.2 77.2
EL-KI 81.6 83.6 83.2 85.7 85.3 85.9 86.8
KI-BO 64.7 68.4 70.9 69.7 74.8 68.6 74.8
KI-DV 70.1 72.3 72.4 73.4 78.4 76.9 77.0
KI-EL 79.7 82.6 81.9 83.7 83.7 86.8 85.1
average 71.7 75.3 76.9 76.8 80.1 78.1 78.7
Table 2: Comparison of different cross-domain algorithms
Figure 3: Accuracy obtained with different cross-domain algorithms over various domains: A) OPTIM, B) RANK,
C) SCL, D) SFA. (x-axis - source domains, y-axis - target domains).
662
parameter BO-EL BO-DV BO-KI EL-BO EL-DV EL-KI
? 0.34 0.78 0.30 0.50 0.55 0.9
k 50 100 25 75 50 200
k? 220 50 40 100 150 10
Table 3: Best number of labeled and unlabeled neighbours for the RANK algorithm over various domain pairs
source- similarity complexity ?
target variance
BO-EL 1.23 -1.93 0.34
BO-DV 1.75 0.06 0.76
BO-KI 1.17 -1.26 0.48
DV-BO 1.75 -0.06 0.75
DV-EL 1.22 -1.99 0.52
DV-KI 1.18 -1.32 0.44
EL-BO 1.23 1.93 0.62
EL-DV 1.22 1.99 0.68
EL-KI 1.87 0.67 0.75
KI-BO 1.17 1.26 0.64
KI-DV 1.18 1.32 0.54
KI-EL 1.87 -0.67 0.76
Table 4: Similarity, complexity variance and ? averaged
over the best results (confidence level of 95%) of the
RANK algorithm. The values are given on various
domain pairs
not have a knowledge of the parameter values which
produce the best performance and, therefore, it
would be useful to elaborate a strategy for choosing
the optimal values with respect to a corresponding
domain pair. In our previous work (Ponomareva
and Thelwall, 2012) we introduced two domain
characteristics: domain similarity and domain
complexity variance and proved their impact into
the cross-domain accuracy loss. Domain similarity
and complexity are independent properties of a
domain pair as the former measures similarity of
data distributions for frequent words, while the latter
compares the tails of distributions. In Ponomareva
and Thelwall (2012), we tested various metrics
to estimate these domain characteristics. As a
result, inversed ?2 was proved to be the best
measure of domain similarity as it gave the highest
correlation with the cross-domain accuracy drop.
The percentage of rare words (words that occur
less than 3 times) was found to be the closest
approximation to domain complexity as it showed
the highest correlation with the in-domain accuracy
drop.
It is naturally to assume that if domain similarity
and complexity are responsible for the cross-domain
accuracy loss, they might influence on the parameter
values of domain adaptation algorithms. This is
proved to be true for the ? parameter, whose values
averaged over the top results of the RANK algorithm
are listed in Table 4. We use the confidence
interval of 95% to select the top values of ?.
Table 4 shows that ? is the lowest for dissimilar
domains with a simpler target (negative values of
domain complexity variance), which means that the
RANK algorithm benefits the most from unlabeled
but simpler data. ? grows to values close to 0.6
for dissimilar domains with more complex target
(positive values of domain complexity variance),
which shows that the impact of simpler source data,
though different from target, increases. Finally ?
reaches its maximum for similar domains with the
same level of complexity. Unfortunately, due to
comparable amount of data for each domain, no
cases of similar domains with different complexity
are observed. We plan to study these particular cases
in the future.
High dependency of ? on both domain
characteristics is proved numerically. The
correlation between ? and domain similarity and
complexity reaches 0.91, and decreases drastically
when one of these characteristics is ignored.
Concerning the optimal number of labeled and
unlabeled neighbours, no regularity is evident
(Table 3). In our opinion, that is an effect
of choosing the neighbours on the basis of the
quantitative threshold. Nevertheless, different
domains have distinct pairwise document similarity
distributions. Figure 4 demonstrates similarity
distributions for BO, EL and DV inside and across
domains. Therefore, taking into account only
the quantitative threshold we ignore discrepancies
663
Figure 4: Pairwise document similarity distributions inside domains (A) and across domains (B)
in graph connectivities inside and across domains
and may bring ?bad? neighbours to participate in
decision-making. In our further research we plan
to explore the idea of a qualitative threshold, which
chooses neighbours according to their similarity and
uses the same similarity levels for in-domain and
cross-domain graphs.
7 Conclusions and future work
This paper has studied the performance of two
graph-based algorithms, OPTIM and RANK when
applied to cross-domain sentiment classification.
Comparison on their performance on the same data
has revealed that, in spite of the similar graph
structures, RANK consistently produces better
results than OPTIM. We also have compared the
graph-based algorithms with other cross-domain
methods, including SCL and SFA, and concluded
that RANK considerably outperforms SCL and
obtains better results than SFA for half of the
cases. Given that we consider only the best
accuracies obtained with RANK, such comparison
is not completely fair but it shows the potential of
the RANK algorithm once the strategy for choosing
its optimal parameters is established. In this paper,
we also discuss some ideas about how to infer
optimal parameter values for the algorithms on the
basis of domain characteristics. In particular, the
strong correlation for ? with domain similarity and
complexity has been observed. Unfortunately we
are not able to find any regularity in the number
of source and target domain neighbours, which we
think is the result of the qualitative approach to
selecting the closest neighbours.
As a result of this research we have identified
the following future directions. First, we plan
to improve the RANK performance by choosing
the number of neighbours on the basis of the
document similarity threshold which we set equal
for both in-domain and cross-domain neighbours.
We expect that this modification will diminish the
number of ?bad? neighbours and allow us to reveal a
dependency of similarity threshold on some domain
properties. Another research direction will focus on
the integration of SFA into the similarity measure
to overcome the problem of lexical discrepancy in
the source and target domains. Finally, as all our
conclusions have been drawn on a data set of 12
domain pairs, we plan to increase a number of
domains to verify our findings on larger data sets.
Acknowledgments
This work was supported by a European Union
grant by the 7th Framework Programme, Theme 3:
Science of complex systems for socially intelligent
ICT. It is part of the CyberEmotions project
(contract 231323).
References
Anthony Aue and Michael Gamon. 2005. Customizing
sentiment classifiers to new domains: A case study. In
Proceedings of Recent Advances in Natural Language
Processing (RANLP ?05).
John Blitzer, Ryan McDonald, and Fernando
Pereira. 2006. Domain adaptation with structural
664
correspondence learning. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing (EMNLP ?06), pages 120?128.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL ?07),
pages 440?447.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a sentiment
sensitive thesaurus for cross-domain sentiment
classification. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics (ACL ?11), pages 132?141.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
Andrea Esuli and Fabrizio Sebastiani. 2006.
Sentiwordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Conference
on Language Resources and Evaluation (LREC 06),
pages 417?422.
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing
stars when there aren?t many stars: graph-based
semi-supervised learning for sentiment categorization.
In Proceedings of the First Workshop on Graph
Based Methods for Natural Language Processing
(TextGraphs ?06), pages 45?52.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In Proceedings of ACL-08:
HLT, Short Papers, pages 257?260.
Sinno Jialin Pan, Xiaochuan Niz, Jian-Tao Sunz, Qiang
Yangy, and Zheng Chen. 2010. Cross-domain
sentiment classification via spectral feature alignment.
In Proceedings of International World Wide Web
Conference (WWW ?10).
Bo Pang and Lillian Lee. 2005. Seeing stars: exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics (ACL ?05), pages 115?124.
Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends in
Information Retrieval, 2(1-2):1?135.
Natalia Ponomareva and Mike Thelwall. 2012.
Bibliographies or blenders: Which resource is best for
cross-domain sentiment analysis? In Proceedings of
the 13th Conference on Intelligent Text Processing and
Computational Linguistics (CICLing ?12).
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2010. Lexicon-based methods for
sentiment analysis. Computational Linguistics,
37(2):267?307.
M. Thelwall, K. Buckley, and G. Paltoglou. 2012.
Sentiment strength detection for the social web.
Journal of the American Society for Information
Science and Technology, 63(1):163?173.
Qiong Wu, Songbo Tan, and Xueqi Cheng. 2009. Graph
ranking for sentiment transfer. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
317?320.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In Proceedings of the
20th International Conference on Machine Learning
(ICML ?03), pages 912?919.
665
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1386?1395,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A study of Information Retrieval weighting schemes for sentiment analysis
Georgios Paltoglou
University of Wolverhampton
Wolverhampton, United Kingdom
g.paltoglou@wlv.ac.uk
Mike Thelwall
University of Wolverhampton
Wolverhampton, United Kingdom
m.thelwall@wlv.ac.uk
Abstract
Most sentiment analysis approaches use as
baseline a support vector machines (SVM)
classifier with binary unigram weights.
In this paper, we explore whether more
sophisticated feature weighting schemes
from Information Retrieval can enhance
classification accuracy. We show that vari-
ants of the classic tf.idf scheme adapted
to sentiment analysis provide significant
increases in accuracy, especially when us-
ing a sublinear function for term frequency
weights and document frequency smooth-
ing. The techniques are tested on a wide
selection of data sets and produce the best
accuracy to our knowledge.
1 Introduction
The increase of user-generated content on the web
in the form of reviews, blogs, social networks,
tweets, fora, etc. has resulted in an environ-
ment where everyone can publicly express their
opinion about events, products or people. This
wealth of information is potentially of vital im-
portance to institutions and companies, providing
them with ways to research their consumers, man-
age their reputations and identify new opportuni-
ties. Wright (2009) claims that ?for many busi-
nesses, online opinion has turned into a kind of
virtual currency that can make or break a product
in the marketplace?.
Sentiment analysis, also known as opinion min-
ing, provides mechanisms and techniques through
which this vast amount of information can be pro-
cessed and harnessed. Research in the field has
mainly, but not exclusively, focused in two sub-
problems: detecting whether a segment of text, ei-
ther a whole document or a sentence, is subjective
or objective, i.e. contains an expression of opin-
ion, and detecting the overall polarity of the text,
i.e. positive or negative.
Most of the work in sentiment analysis has fo-
cused on supervised learning techniques (Sebas-
tiani, 2002), although there are some notable ex-
ceptions (Turney, 2002; Lin and He, 2009). Pre-
vious research has shown that in general the per-
formance of the former tend to be superior to that
of the latter (Mullen and Collier, 2004; Lin and
He, 2009). One of the main issues for supervised
approaches has been the representation of docu-
ments. Usually a bag of words representation is
adopted, according to which a document is mod-
eled as an unordered collection of the words that
it contains. Early research by Pang et al (2002) in
sentiment analysis showed that a binary unigram-
based representation of documents, according to
which a document is modeled only by the pres-
ence or absence of words, provides the best base-
line classification accuracy in sentiment analysis
in comparison to other more intricate representa-
tions using bigrams, adjectives, etc.
Later research has focused on extending the
document representation with more complex fea-
tures such as structural or syntactic informa-
tion (Wilson et al, 2005), favorability mea-
sures from diverse sources (Mullen and Collier,
2004), implicit syntactic indicators (Greene and
Resnik, 2009), stylistic and syntactic feature selec-
tion (Abbasi et al, 2008), ?annotator rationales?
(Zaidan et al, 2007) and others, but no systematic
study has been presented exploring the benefits of
employing more sophisticated models for assign-
ing weights to word features.
In this paper, we examine whether term weight-
ing functions adopted from Information Retrieval
(IR) based on the standard tf.idf formula and
adapted to the particular setting of sentiment anal-
ysis can help classification accuracy. We demon-
strate that variants of the original tf.idf weighting
scheme provide significant increases in classifica-
tion performance. The advantages of the approach
are that it is intuitive, computationally efficient
1386
and doesn?t require additional human annotation
or external sources. Experiments conducted on a
number of publicly available data sets improve on
the previous state-of-the art.
The next section provides an overview of rel-
evant work in sentiment analysis. In section 3
we provide a brief overview of the original tf.idf
weighting scheme along with a number of variants
and show how they can be applied to a classifica-
tion scenario. Section 4 describes the corpora that
were used to test the proposed weighting schemes
and section 5 discusses the results. Finally, we
conclude and propose future work in section 6.
2 Prior Work
Sentiment analysis has been a popular research
topic in recent years. Most of the work has fo-
cused on analyzing the content of movie or gen-
eral product reviews, but there are also applica-
tions to other domains such as debates (Thomas et
al., 2006; Lin et al, 2006), news (Devitt and Ah-
mad, 2007) and blogs (Ounis et al, 2008; Mishne,
2005). The book of Pang and Lee (2008) presents
a thorough overview of the research in the field.
This section presents the most relevant work.
Pang et al (2002) conducted early polarity
classification of reviews using supervised ap-
proaches. They employed Support Vector Ma-
chines (SVMs), Naive Bayes and Maximum En-
tropy classifiers using a diverse set of features,
such as unigrams, bigrams, binary and term fre-
quency feature weights and others. They con-
cluded that sentiment classification is more dif-
ficult that standard topic-based classification and
that using a SVM classifier with binary unigram-
based features produces the best results.
A subsequent innovation was the detection and
removal of the objective parts of documents and
the application of a polarity classifier on the rest
(Pang and Lee, 2004). This exploited text coher-
ence with adjacent text spans which were assumed
to belong to the same subjectivity or objectivity
class. Documents were represented as graphs with
sentences as nodes and association scores between
them as edges. Two additional nodes represented
the subjective and objective poles. The weights
between the nodes were calculated using three dif-
ferent, heuristic decaying functions. Finding a par-
tition that minimized a cost function separated the
objective from the subjective sentences. They re-
ported a statistically significant improvement over
a Naive Bayes baseline using the whole text but
only slight increase compared to using a SVM
classifier on the entire document.
Mullen and Collier (2004) used SVMs and ex-
panded the feature set for representing documents
with favorability measures from a variety of di-
verse sources. They introduced features based on
Osgood?s Theory of Semantic Differentiation (Os-
good, 1967) using WordNet to derive the values
of potency, activity and evaluative of adjectives
and Turney?s semantic orientation (Turney, 2002).
Their results showed that using a hybrid SVM
classifier, that uses as features the distance of doc-
uments from the separating hyperplane, with all
the above features produces the best results.
Whitelaw et al (2005) added fine-grained se-
mantic distinctions in the feature set. Their ap-
proach was based on a lexicon created in a semi-
supervised fashion and then manually refined It
consists of 1329 adjectives and their modifiers cat-
egorized under several taxonomies of appraisal at-
tributes based on Martin and White?s Appraisal
Theory (2005). They combined the produced ap-
praisal groups with unigram-based document rep-
resentations as features to a Support Vector Ma-
chine classifier (Witten and Frank, 1999), result-
ing in significant increases in accuracy.
Zaidan et al (2007) introduced ?annotator ra-
tionales?, i.e. words or phrases that explain the
polarity of the document according to human an-
notators. By deleting rationale text spans from the
original documents they created several contrast
documents and constrained the SVM classifier to
classify them less confidently than the originals.
Using the largest training set size, their approach
significantly increased the accuracy on a standard
data set (see section 4).
Prabowo and Thelwall (2009) proposed a hy-
brid classification process by combining in se-
quence several ruled-based classifiers with a SVM
classifier. The former were based on the Gen-
eral Inquirer lexicon (Wilson et al, 2005), the
MontyLingua part-of-speech tagger (Liu, 2004)
and co-occurrence statistics of words with a set
of predefined reference words. Their experiments
showed that combining multiple classifiers can
result in better effectiveness than any individual
classifier, especially when sufficient training data
isn?t available.
In contrast to machine learning approaches
that require labeled corpora for training, Lin and
1387
He (2009) proposed an unsupervised probabilis-
tic modeling framework, based on Latent Dirich-
let Allocation (LDA). The approach assumes that
documents are a mixture of topics, i.e. proba-
bility distribution of words, according to which
each document is generated through an hierarchi-
cal process and adds an extra sentiment layer to
accommodate the opinionated nature (positive or
negative) of the document. Their best attained per-
formance, using a filtered subjectivity lexicon and
removing objective sentences in a manner similar
to Pang and Lee (2004), is only slightly lower than
that of a fully-supervised approach.
3 A study of non-binary weights
We use the terms ?features?, ?words? and ?terms?
interchangeably in this paper, since we mainly fo-
cus on unigrams. The approach nonetheless can
easily be extended to higher order n-grams. Each
document D therefore is represented as a bag-of-
words feature vector: D =
{
w1, w2, ..., w|V |
}
where |V | is the size of the vocabulary (i.e. the
number of unique words) and wi, i = 1, . . . , |V |
is the weight of term i in document D.
Despite the significant attention that sentiment
analysis has received in recent years, the best ac-
curacy without using complex features (Mullen
and Collier, 2004; Whitelaw et al, 2005) or ad-
ditional human annotations (Zaidan et al, 2007) is
achieved by employing a binary weighting scheme
(Pang et al, 2002), where wi = 1, if tfi > 0 and
wi = 0, if tfi = 0, where tfi is the number of
times that term i appears in document D (hence-
forth raw term frequency) and utilizing a SVM
classifier. It is of particular interest that using tfi
in the document representation usually results in
decreased accuracy, a result that appears to be in
contrast with topic classification (Mccallum and
Nigam, 1998; Pang et al, 2002).
In this paper, we also utilize SVMs but our
study is centered on whether more sophisticated
than binary or raw term frequency weighting func-
tions can improve classification accuracy. We
base our approach on the classic tf.idf weighting
scheme from Information Retrieval (IR) and adapt
it to the domain of sentiment classification.
3.1 The classic tf.idf weighting schemes
The classic tf.idf formula assigns weight wi to
term i in document D as:
wi = tfi ? idfi = tfi ? log
N
dfi
(1)
where tfi is the number of times term i occurs in
D, idfi is the inverse document frequency of term
i, N is the total number of documents and dfi is
the number of documents that contain term i.
The utilization of tfi in classification is rather
straightforward and intuitive but, as previously
discussed, usually results in decreased accuracy
in sentiment analysis. On the other hand, using
idf to assign weights to features is less intuitive,
since it only provides information about the gen-
eral distribution of term i amongst documents of
all classes, without providing any additional evi-
dence of class preference. The utilization of idf
in information retrieval is based on its ability to
distinguish between content-bearing words (words
with some semantical meaning) and simple func-
tion words, but this behavior is at least ambiguous
in classification.
Table 1: SMART notation for term frequency vari-
ants. maxt(tf) is the maximum frequency of any
term in the document and avg dl is the average
number of terms in all the documents. For ease of
reference, we also include the BM25 tf scheme.
The k1 and b parameters of BM25 are set to their
default values of 1.2 and 0.95 respectively (Jones
et al, 2000).
Notation Term frequency
n (natural) tf
l (logarithm) 1 + log(tf)
a (augmented) 0.5 + 0.5?tfmaxt(tf)
b (boolean)
{
1, tf > 0
0, otherwise
L (log ave) 1+log(tf)1+log(avg dl)
o (BM25) (k1+1)?tf
k1
(
(1?b)+b? dlavg dl
)
+tf
3.2 Delta tf.idf
Martineau and Finin (2009) provide a solution to
the above issue of idf utilization in a classification
scenario by localizing the estimation of idf to the
documents of one or the other class and subtract-
ing the two values. Therefore, the weight of term
1388
Table 2: SMART notation for inverse document
frequency variants. For ease of reference we also
include the BM25 idf factor and also present the
extensions of the original formulations with their
? variants.
Notation Inverse Document Fre-
quency
n (no) 1
t (idf) logNdf
p (prob idf) logN?dfdf
k (BM25 idf) logN?df+0.5df+0.5
?(t) (Delta idf) logN1?df2N2?df1
?(t?) (Delta smoothed
idf)
logN1?df2+0.5N2?df1+0.5
?(p) (Delta prob idf) log (N1?df1)?df2df1?(N2?df2)
?(p?) (Delta smoothed
prob idf)
log (N1?df1)?df2+0.5(N2?df2)?df1+0.5
?(k) (Delta BM25 idf) log (N1?df1+0.5)?df2+0.5(N2?df2+0.5)?df1+0.5
i in document D is estimated as:
wi = tfi ? log2(
N1
dfi,1
)? tfi ? log2(
N2
dfi,2
)
= tfi ? log2(
N1 ? dfi,2
dfi,1 ?N2
) (2)
where Nj is the total number of training docu-
ments in class cj and dfi,j is the number of train-
ing documents in class cj that contain term i. The
above weighting scheme was appropriately named
Delta tf.idf .
The produced results (Martineau and Finin,
2009) show that the approach produces better
results than the simple tf or binary weighting
scheme. Nonetheless, the approach doesn?t take
into consideration a number of tested notions from
IR, such as the non-linearity of term frequency to
document relevancy (e.g. Robertson et al (2004))
according to which, the probability of a document
being relevant to a query term is typically sub-
linear in relation to the number of times a query
term appears in the document. Additionally, their
approach doesn?t provide any sort of smoothing
for the dfi,j factor and is therefore susceptible to
errors in corpora where a term occurs in docu-
ments of only one or the other class and therefore
dfi,j = 0 .
3.3 SMART and BM25 tf.idf variants
The SMART retrieval system by Salton (1971) is
a retrieval system based on the vector space model
(Salton and McGill, 1986). Salton and Buckley
(1987) provide a number of variants of the tf.idf
weighting approach and present the SMART nota-
tion scheme, according to which each weighting
function is defined by triples of letters; the first
one denotes the term frequency factor, the sec-
ond one corresponds to the inverse document fre-
quency function and the last one declares the nor-
malization that is being applied. The upper rows
of tables 1, 2 and 3 present the three most com-
monly used weighting functions for each factor re-
spectively. For example, a binary document repre-
sentation would be equivalent to SMART.bnn1
or more simply bnn, while a simple raw term fre-
quency based would be notated as nnn or nnc
with cosine normalization.
Table 3: SMART normalization.
Notation Normalization
n (none) 1
c (cosine) 1?
w21+w22+...+w2n
Significant research has been done in IR on di-
verse weighting functions and not all versions of
SMART notations are consistent (Manning et al,
2008). Zobel and Moffat (1998) provide an ex-
haustive study but in this paper, due to space con-
straints, we will follow the concise notation pre-
sented by Singhal et al (1995).
The BM25 weighting scheme (Robertson et al,
1994; Robertson et al, 1996) is a probabilistic
model for information retrieval and is one of the
most popular and effective algorithms used in in-
formation retrieval. For ease of reference, we in-
corporate the BM25 tf and idf factors into the
SMART annotation scheme (last row of table 1
and 4th row of table 2), therefore the weight wi
of term i in document D according to the BM25
scheme is notated as SMART.okn or okn.
Most of the tf weighting functions in SMART
and the BM25 model take into consideration the
non-linearity of document relevance to term fre-
1Typically, a weighting function in the SMART system is
defined as a pair of triples, i.e. ddd.qqq where the first triple
corresponds to the document representation and the second
to the query representation. In the context that the SMART
annotation is used here, we will use the prefix SMART for
the first part and a triple for the document representation in
the second part, i.e. SMART.ddd, or more simply ddd.
1389
quency and thus employ tf factors that scale sub-
linearly in relation to term frequency. Addition-
ally, the BM25 tf variant also incorporates a scal-
ing for the length of the document, taking into con-
sideration that longer documents will by definition
have more term occurences2 . Effective weighting
functions is a very active research area in infor-
mation retrieval and it is outside the scope of this
paper to provide an in-depth analysis but signifi-
cant research can be found in Salton and McGill
(1986), Robertson et al (2004), Manning et al
(2008) or Armstrong et al (2009) for a more re-
cent study.
3.4 Introducing SMART and BM25 Delta
tf.idf variants
We apply the idea of localizing the estimation
of idf values to documents of one class but em-
ploy more sophisticated term weighting functions
adapted from the SMART retrieval system and
the BM25 probabilistic model. The resulting idf
weighting functions are presented in the lower part
of table 2. We extend the original SMART anno-
tation scheme by adding Delta (?) variants of the
original idf functions and additionally introduce
smoothed Delta variants of the idf and the prob
idf factors for completeness and comparative rea-
sons, noted by their accented counterparts. For
example, the weight of term i in document D ac-
cording to the o?(k)n weighting scheme where
we employ the BM25 tf weighting function and
utilize the difference of class-based BM25 idf val-
ues would be calculated as:
wi =
(k1 + 1) ? tfi
K + tfi
? log(N1 ? dfi,1 + 0.5
dfi,1 + 0.5
)
? (k1 + 1) ? tfi
K + tfi
? log(N2 ? dfi,2 + 0.5
dfi,2 + 0.5
)
= (k1 + 1) ? tfi
K + tfi
? log
((N1 ? dfi,1 + 0.5) ? (dfi,2 + 0.5)
(N2 ? dfi,2 + 0.5) ? (dfi,1 + 0.5)
)
where K is defined as k1
(
(1 ? b) + b ? dlavg dl
)
.
However, we used a minor variation of the above
formulation for all the final accented weighting
functions in which the smoothing factor is added
to the product of dfi with Ni (or its variation for
?(p?) and ?(k)), rather than to the dfi alone as the
2We deliberately didn?t extract the normalization compo-
nent from the BM25 tf variant, as that would unnecessarily
complicate the notation.
above formulation would imply (see table 2). The
above variation was made for two reasons: firstly,
when the dfi?s are larger than 1 then the smooth-
ing factor influences the final idf value only in a
minor way in the revised formulation, since it is
added only after the multiplication of the dfi with
Ni (or its variation). Secondly, when dfi = 0, then
the smoothing factor correctly adds only a small
mass, avoiding a potential division by zero, where
otherwise it would add a much greater mass, be-
cause it would be multiplied by Ni.
According to this annotation scheme therefore,
the original approach by Martineau and Finin
(2009) can be represented as n?(t)n.
We hypothesize that the utilization of sophisti-
cated term weighting functions that have proved
effective in information retrieval, thus providing
an indication that they appropriately model the
distinctive power of terms to documents and the
smoothed, localized estimation of idf values will
prove beneficial in sentiment classification.
Table 4: Reported accuracies on the Movie Re-
view data set. Only the best reported accuracy for
each approach is presented, measured by 10-fold
cross validation. The list is not exhaustive and be-
cause of differences in training/testing data splits
the results are not directly comparable. It is pro-
duced here only for reference.
Approach Acc.
SVM with unigrams & binary
weights (Pang et al, 2002), reported
at (Pang and Lee, 2004)
87.15%
Hybrid SVM with Turney/Osgood
Lemmas (Mullen and Collier, 2004)
86%
SVM with min-cuts (Pang and Lee,
2004)
87.2%
SVM with appraisal groups 90.2%
(Whitelaw et al, 2005)
SVM with log likehood ratio feature
selection (Aue and Gamon, 2005)
90.45%
SVM with annotator rationales 92.2%
(Zaidan et al, 2007)
LDA with filtered lexicon, subjectiv-
ity detection (Lin and He, 2009)
84.6%
The approach is straightforward, intuitive, com-
putationally efficient, doesn?t require additional
human effort and takes into consideration stan-
dardized and tested notions from IR. The re-
sults presented in section 5 show that a number
1390
of weighting functions solidly outperform other
state-of-the-art approaches. In the next section, we
present the corpora that were used to study the ef-
fectiveness of different weighting schemes.
4 Experimental setup
We have experimented with a number of publicly
available data sets.
The movie review dataset by Pang et al (2002)
has been used extensively in the past by a number
of researchers (see Table 4), presenting the oppor-
tunity to compare the produced results with pre-
vious approaches. The dataset comprises 2,000
movie reviews, equally divided between positive
and negative, extracted from the Internet Movie
Database3 archive of the rec.arts.movies.reviews
newsgroup. In order to avoid reviewer bias, only
20 reviews per author were kept, resulting in a to-
tal of 312 reviewers4. The best attained accuracies
by previous research on the specific data are pre-
sented in table 4. We do not claim that those re-
sults are directly comparable to ours, because of
potential subtle differences in tokenization, classi-
fier implementations etc, but we present them here
for reference.
The Multi-Domain Sentiment data set (MDSD)
by Blitzer et al (2007) contains Amazon reviews
for four different product types: books, electron-
ics, DVDs and kitchen appliances. Reviews with
ratings of 3 or higher, on a 5-scale system, were
labeled as positive and reviews with a rating less
than 3 as negative. The data set contains 1,000
positive and 1,000 negative reviews for each prod-
uct category for a total of 8,000 reviews. Typically,
the data set is used for domain adaptation applica-
tions but in our setting we only split the reviews
between positive and negative5.
Lastly, we present results from the BLOGS06
(Macdonald and Ounis, 2006) collection that is
comprised of an uncompressed 148GB crawl of
approximately 100,000 blogs and their respective
RSS feeds. The collection has been used for 3 con-
secutive years by the Text REtrieval Conferences
(TREC)6. Participants of the conference are pro-
vided with the task of finding documents (i.e. web
pages) expressing an opinion about specific enti-
3http://www.imdb.com
4The dataset can be found at: http://www.cs.cornell.edu/
People/pabo/movie-review-data/review polarity.tar.gz.
5The data set can be found at http://www.cs.jhu.edu/
mdredze/datasets/sentiment/
6http://www.trec.nist.gov
ties X, which may be people, companies, films
etc. The results are given to human assessors who
then judge the content of the webpages (i.e. blog
post and comments) and assign each webpage a
score: ?1? if the document contains relevant, fac-
tual information about the entity but no expression
of opinion, ?2? if the document contains an ex-
plicit negative opinion towards the entity and ?4?
is the document contains an explicit positive opin-
ion towards the entity. We used the produced as-
sessments from all 3 years of the conference in our
data set, resulting in 150 different entity searches
and, after duplicate removal, 7,930 negative docu-
ments (i.e. having an assessment of ?2?) and 9,968
positive documents (i.e. having an assessment of
?4?), which were used as the ?gold standard? 7.
Documents are annotated at the document-level,
rather than at the post level, making this data set
somewhat noisy. Additionally, the data set is par-
ticularly large compared to the other ones, making
classification especially challenging and interest-
ing. More information about all data sets can be
found at table 5.
We have kept the pre-processing of the docu-
ments to a minimum. Thus, we have lower-cased
all words and removed all punctuation but we have
not removed stop words or applied stemming. We
have also refrained from removing words with
low or high occurrence. Additionally, for the
BLOGS06 data set, we have removed all html for-
matting.
We utilize the implementation of a support vec-
tor classifier from the LIBLINEAR library (Fan et
al., 2008). We use a linear kernel and default
parameters. All results are based on leave-one
out cross validation accuracy. The reason for this
choice of cross-validation setting, instead of the
most standard ten-fold, is that all of the proposed
approaches that use some form of idf utilize the
training documents for extracting document fre-
quency statistics, therefore more information is
available to them in this experimental setting.
Because of the high number of possible combi-
nations between tf and idf variants (6?9?2 = 108)
and due to space constraints we only present re-
sults from a subset of the most representative com-
binations. Generally, we?ll use the cosine nor-
malized variants of unsmoothed delta weighting
schemes, since they perform better than their un-
7More information about the data set, as well as in-
formation on how it can be obtained can be found at:
http://ir.dcs.gla.ac.uk/test collections/blogs06info.html
1391
Table 5: Statistics about the data sets used.
Data set #Documents #Terms #Unique
Terms
Average #Terms
per Document
Movie Reviews 2,000 1,336,883 39,399 668
Multi-Domain Sentiment
Dataset (MDSD)
8,000 1,741,085 455,943 217
BLOGS06 17,898 51,252,850 367,899 2,832
Figure 1: Reported accuracy on the Movie Review data set.
normalized counterparts. We?ll avoid using nor-
malization for the smoothed versions, in order to
focus our attention on the results of smoothing,
rather than normalization.
5 Results
Results for the Movie Reviews, Multi-Domain
Sentiment Dataset and BLOGS06 corpora are re-
ported in figures 1, 2 and 3 respectively.
On the Movie Review data set, the results re-
confirm that using binary features (bnc) is bet-
ter than raw term frequency (nnc) (83.40%) fea-
tures. For reference, in this setting the unnor-
malized vector using the raw tf approach (nnn)
performs similar to the normalized (nnc) (83.40%
vs. 83.60%), the former not present in the graph.
Nonetheless, using any scaled tf weighting func-
tion (anc or onc) performs as well as the binary
approach (87.90% and 87.50% respectively). Of
interest is the fact that although the BM25 tf algo-
rithm has proved much more successful in IR, the
same doesn?t apply in this setting and its accuracy
is similar to the simpler augmented tf approach.
Incorporating un-localized variants of idf (mid-
dle graph section) produces only small increases
in accuracy. Smoothing also doesn?t provide any
particular advantage, e.g. btc (88.20%) vs. bt?c
(88.45%), since no zero idf values are present.
Again, using more sophisticated tf functions pro-
vides an advantage over raw tf , e.g. nt?c at-
tains an accuracy of 86.6% in comparison to at?c?s
88.25%, although the simpler at?c is again as ef-
fective than the BM25 tf (ot?c), which performs at
88%. The actual idf weighting function is of some
importance, e.g. ot?c (88%) vs. okc (87.65%) and
akc (88%) vs. at?c (88.25%), with simpler idf fac-
tors performing similarly, although slightly better
than BM25.
Introducing smoothed, localized variants of idf
and scaled or binary tf weighting schemes pro-
duces significant advantages. In this setting,
smoothing plays a role, e.g. n?(t)c8 (91.60%)
vs. n?(t?)n (95.80%) and a?(p)c (92.80%)
vs. a?(p?)n (96.55%), since we can expect zero
class-based estimations of idf values, supporting
our initial hypothesis on its importance. Addition-
ally, using augmented, BM25 or binary tf weights
is always better than raw term frequency, pro-
viding further support on the advantages of us-
ing sublinear tf weighting functions9 . In this set-
ting, the best accuracy of 96.90% is attained using
BM25 tf weights with the BM25 delta idf variant,
although binary or augmented tf weights using
8The original Delta tf.idf by Martineau and Finin (2009)
has a limitation of utilizing features with df > 2. In our
experiments it performed similarly to n?(t)n (90.60%) but
still lower than the cosine normalized variant n?(t)c in-
cluded in the graph (91.60%).
9Although not present in the graph, for completeness rea-
sons it should be noted that l?(s)n and L?(s)n also per-
form very well, both reaching accuracies of approx. 96%.
1392
Figure 2: Reported accuracy on the Multi-Domain Sentiment data set.
delta idf perform similarly (96.50% and 96.60%
respectively). The results indicate that the tf and
the idf factor themselves aren?t of significant im-
portance, as long as the former are scaled and the
latter smoothed in some manner. For example,
a?(p?)n vs. a?(t?)n perform quite similarly.
The results from the Multi-Domain Sentiment
data set (figure 2) largely agree with the find-
ings on the Movie Review data set, providing a
strong indication that the approach isn?t limited
to a specific domain. Binary weights outperform
raw term frequency weights and perform similarly
with scaled tf ?s. Non-localized variants of idf
weights do provide a small advantage in this data
set alhough the actual idf variant isn?t important,
e.g. btc, bt?c, and okc all perform similarly. The
utilized tf variant also isn?t important, e.g. at?c
(88.39%) vs. bt?c (88.25%).
We focus our attention on the delta idf vari-
ants which provide the more interesting results.
The importance of smoothing becomes apparent
when comparing the accuracy of a?(p)c and its
smoothed variant a?(p?)n (92.56% vs. 95.6%).
Apart from that, all smoothed delta idf variants
perform very well in this data set, including some-
what surprisingly, n?(t?)n which uses raw tf
(94.54%). Considering that the average tf per
document is approx. 1.9 in the Movie Review
data set and 1.1 in the MDSD, the results can be
attributed to the fact that words tend to typically
appear only once per document in the latter, there-
fore minimizing the difference of the weights at-
tributed by different tf functions10 . The best at-
tained accuracy is 96.40% but as the MDSD has
mainly been used for domain adaptation applica-
tions, there is no clear baseline to compare it with.
10For reference, the average tf per document in the
BLOGS06 data set is 2.4.
Lastly, we present results on the BLOGS06
dataset in figure 3. As previously noted, this data
set is particularly noisy, because it has been an-
notated at the document-level rather than the post-
level and as a result, the differences aren?t as pro-
found as in the previous corpora, although they
do follow the same patterns. Focusing on the
delta idf variants, the importance of smoothing
becomes apparent, e.g. a?(p)c vs. a?(p?)n and
n?(t)c vs. n?(t?)n. Additionally, because of the
fact that documents tend to be more verbose in
this data set, the scaled tf variants also perform
better than the simple raw tf ones, n?(t?)n vs.
a?(t?)n. Lastly, as previously, the smoothed lo-
calized idf variants perform better than their un-
smoothed counterparts, e.g. n?(t)n vs. n?(t?)n
and a?(p)c vs. a?(p?)n.
6 Conclusions
In this paper, we presented a study of document
representations for sentiment analysis using term
weighting functions adopted from information re-
trieval and adapted to classification. The pro-
posed weighting schemes were tested on a num-
ber of publicly available datasets and a number
of them repeatedly demonstrated significant in-
creases in accuracy compared to other state-of-the-
art approaches. We demonstrated that for accurate
classification it is important to use term weight-
ing functions that scale sublinearly in relation to
the number of times a term occurs in a document
and that document frequency smoothing is a sig-
nificant factor.
In the future we plan to test the proposed
weighting functions in other domains such as topic
classification and additionally extend the approach
to accommodate multi-class classification.
1393
Figure 3: Reported accuracy on the BLOGS06 data set.
Acknowledgments
This work was supported by a European Union
grant by the 7th Framework Programme, Theme
3: Science of complex systems for socially intelli-
gent ICT. It is part of the CyberEmotions Project
(Contract 231323).
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Trans. Inf. Syst., 26(3):1?34.
Timothy G. Armstrong, Alistair Moffat, William Web-
ber, and Justin Zobel. 2009. Improvements that
don?t add up: ad-hoc retrieval results since 1998.
In David Wai Lok Cheung, Il Y. Song, Wesley W.
Chu, Xiaohua Hu, Jimmy J. Lin, David Wai Lok
Cheung, Il Y. Song, Wesley W. Chu, Xiaohua Hu,
and Jimmy J. Lin, editors, CIKM, pages 601?610,
New York, NY, USA. ACM.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: A case
study. In Proceedings of Recent Advances in Nat-
ural Language Processing (RANLP).
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 440?447, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 984?991, Prague, Czech Republic,
June. Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503?511, Boulder, Colorado, June.
Association for Computational Linguistics.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000.
A probabilistic model of information retrieval: de-
velopment and comparative experiments. Inf. Pro-
cess. Manage., 36(6):779?808.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In CIKM
?09: Proceeding of the 18th ACM conference on In-
formation and knowledge management, pages 375?
384, New York, NY, USA. ACM.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? identifying perspectives at the document and
sentence levels. In Proceedings of the Conference
on Natural Language Learning (CoNLL).
Hugo Liu. 2004. MontyLingua: An end-to-end natural
language processor with common sense. Technical
report, MIT.
C. Macdonald and I. Ounis. 2006. The trec blogs06
collection : Creating and analysing a blog test col-
lection. DCS Technical Report Series.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, 1 edition,
July.
J. R. Martin and P. R. R. White. 2005. The language of
evaluation : appraisal in English / J.R. Martin and
P.R.R. White. Palgrave Macmillan, Basingstoke :.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An Improved Feature Space for Sentiment Analysis.
In Proceedings of the Third AAAI Internatonal Con-
ference on Weblogs and Social Media, San Jose, CA,
May. AAAI Press. (poster paper).
A. Mccallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification.
1394
G. Mishne. 2005. Experiments with mood classifi-
cation in blog posts. In 1st Workshop on Stylistic
Analysis Of Text For Information Access.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 412?
418, Barcelona, Spain, July. Association for Com-
putational Linguistics.
Charles E. Osgood. 1967. The measurement of mean-
ing / [by] [Charles E. Osgood, George J. Suci [and]
Percy H. Tannenbaum]. University of Illinois Press,
Urbana :, 2nd ed. edition.
Iadh Ounis, Craig Macdonald, and Ian Soboroff. 2008.
Overview of the trec-2008 blog trac. In The Seven-
teenth Text REtrieval Conference (TREC 2008) Pro-
ceedings. NIST.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In In Proceedings
of the ACL, pages 271?278.
B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Now Publishers Inc.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Infor-
metrics, 3(2):143?157, April.
Stephen E. Robertson, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
1994. Okapi at trec-3. In TREC, pages 0?.
S E Robertson, S Walker, S Jones, M M Hancock-
Beaulieu, and M Gatford. 1996. Okapi at trec-2.
In In The Second Text REtrieval Conference (TREC-
2), NIST Special Special Publication 500-215, pages
21?34.
Stephen Robertson, Hugo Zaragoza, and Michael Tay-
lor. 2004. Simple bm25 extension to multiple
weighted fields. In CIKM ?04: Proceedings of the
thirteenth ACM international conference on Infor-
mation and knowledge management, pages 42?49,
New York, NY, USA. ACM.
Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical
report, Ithaca, NY, USA.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
G. Salton. 1971. The SMART Retrieval System?
Experiments in Automatic Document Processing.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1n?47.
Amit Singhal, Gerard Salton, and Chris Buckley. 1995.
Length normalization in degraded text collections.
Technical report, Ithaca, NY, USA.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition
from congressional floor-debate transcripts. CoRR,
abs/cs/0607062.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL, pages 417?424.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In CIKM ?05: Proceedings of the 14th
ACM international conference on Information and
knowledge management, pages 625?631, New York,
NY, USA. ACM.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, CA.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations (The Morgan Kaufmann
Series in Data Management Systems). Morgan
Kaufmann, 1st edition, October.
Alex Wright. 2009. Mining the web for feelings, not
facts. August 23, NY Times, last accessed October
2, 2009, http://http://www.nytimes.com/2009/08/24/
technology/internet/ 24emotion.html? r=1.
O.F. Zaidan, J. Eisner, and C.D. Piatko. 2007. Using
Annotator Rationales to Improve Machine Learn-
ing for Text Categorization. Proceedings of NAACL
HLT, pages 260?267.
Justin Zobel and Alistair Moffat. 1998. Exploring the
similarity space. SIGIR Forum, 32(1):18?34.
1395
