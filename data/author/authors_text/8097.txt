Multilingual and cross-lingual news topic tracking 
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, Emilia K?sper & Irina Temnikova
Joint Research Centre, European Commission 
T.P. 267, Via E. Fermi 1 
21020 Ispra (VA), Italy 
http://www.jrc.it/langtech 
Firstname.Lastname@jrc.it 
 
 
Abstract 
We are presenting a working system for automated 
news analysis that ingests an average total of 7600 
news articles per day in five languages. For each 
language, the system detects the major news stories 
of the day using a group-average unsupervised ag-
glomerative clustering process. It also tracks, for 
each cluster, related groups of articles published 
over the previous seven days, using a cosine of 
weighted terms. The system furthermore tracks re-
lated news across languages, in all language pairs 
involved. The cross-lingual news cluster similarity 
is based on a linear combination of three types of 
input: (a) cognates, (b) automatically detected ref-
erences to geographical place names and (c) the re-
sults of a mapping process onto a multilingual clas-
sification system. A manual evaluation showed that 
the system produces good results.  
1 Introduction 
Most large organisations, companies and politi-
cal parties have a department analysing the news 
on a daily basis. Motivations differ, but often these 
organisations want to know how they and their 
leading members are represented in the news, or 
they need to know whether there has been any 
event they ought to know about. Examples of ex-
isting news gathering and analysis systems are In-
formedia1 and the Europe Media Monitor (Best et 
al. 2002). DARPA has taken an interest in the do-
main and launched, in 1996, the Topic Detection 
and Tracking task2 (TDT) under the TIDES pro-
gram. It distinguishes three major tasks: (a) seg-
mentation of a continuous information flow (e.g. 
spoken news) into individual news items, (b) de-
tection of breaking news, i.e. of a new subject that 
has not previously been discussed, and (c) topic 
tracking, i.e. the identification of related news over 
time. Our task is the analysis of a multilingual col-
lection of written news articles, which means that 
segmentation (task a) is of no relevance. Neither 
do we present here work on the detection of new 
                                                     
1 http://www.informedia.cs.cmu.edu/ 
2 http://www.nist.gov/speech/tests/tdt/ 
topics (task b). Instead, we focus on the topic 
tracking task (c), and especially on the novel as-
pect of cross-lingual tracking.  
The aim of our work is to provide an automati-
cally generated overview over the major news of 
each day (midnight to midnight) in the languages 
English, German, French, Spanish and Italian. The 
corpus consists of news items gathered from a 
large number of internet news sites world-wide, 
and of various subscription news wires (Best et al 
2002). The texts are thus from hundreds of differ-
ent sources (feeds) which often discuss the same 
events. Newspapers often publish the news they 
receive from press agencies with no or few 
amendments. The corpus of news articles thus con-
tains not only summaries of the same events writ-
ten by different journalists, but also many dupli-
cates and near duplicates of the same original text 
which need to be eliminated from the collection.  
In order to identify the major news, we identify 
clusters of similar news items, i.e. news items that 
deal with the same subject. All subjects that trigger 
a large number of news articles from various feeds 
are of interest. The related news thus do not neces-
sarily have to discuss events, i.e. things that happen 
at a particular time and place (e.g. the 11/03 Ma-
drid bombing), but they can also be a thread of dis-
cussions on the same subject, such as the campaign 
for the US presidential elections.  
In section 2, we summarise other work on topic 
tracking, on cross-lingual news linking and on fea-
ture extraction methods. Section 3 describes the 
multilingual news corpus and the text feature ex-
traction used for the document representation. In 
section 4, we present the process and evaluation of 
major news identification. Section 5 is dedicated to 
the multi-monolingual topic tracking process and 
its evaluation. Section 6 describes the cross-lingual 
linking of related clusters of major news, plus 
evaluation results. Section 7 points to future work.  
2 Related work 
Allan et al (1998) identify new events and then 
track the topic like in an information filtering task 
by querying new documents against the profile of 
the newly detected topic. Topics are represented as 
a vector of stemmed words and their TF.IDF val-
ues, only considering nouns, verbs, adjectives and 
numbers. In their experiments, using between 10 
and 20 features produced optimal results. Schultz 
(1999) took the alternative approach of clustering 
texts with a single-linkage unsupervised agglom-
erative clustering method, using cosine similarity 
and TF.IDF for term weighting. He concludes that 
?a successful clustering algorithm must incorporate 
a representation for a cluster itself as group aver-
age clustering does?. We followed Schultz? advice. 
Unlike Schultz, however, we use the log-likelihood 
test for term weighting as this measure seems to be 
better when dealing with varying text sizes (Kil-
garriff 1996). We do not consider parts-of-speech, 
lemmatisation or stemming, as we do not have ac-
cess to linguistic resources for all the languages we 
need to work with, but we use an extensive list of 
stop words.  
Approaches to cross-lingual topic tracking are 
rather limited. Possible solutions for this task are to 
either translate documents or words from one lan-
guage into the other, or to map the documents in 
both languages onto some multilingual reference 
system such as a thesaurus. Wactlar (1999) used 
bilingual dictionaries to translate Serbo-Croatian 
words and phrases into English and using the trans-
lations as a query on the English texts to find simi-
lar texts. In TDT-3, only four systems tried to es-
tablish links between documents written in differ-
ent languages. All of them tried to link English and 
Chinese-Mandarin news articles by using Machine 
Translation (e.g. Leek et al 1999). Using a ma-
chine translation tool before carrying out the topic 
tracking resulted in a 50% performance loss, com-
pared to monolingual topic tracking.  
Friburger & Maurel (2002) showed that the iden-
tification and usage of proper names, and espe-
cially of geographical references, significantly im-
proves document similarity calculation and cluster-
ing. Hyland et al (1999) clustered news and de-
tected topics exploiting the unique combinations of 
various named entities to link related documents. 
However, according to Friburger & Maurel (2002), 
the usage of named entities alone is not sufficient.  
Our own approach to cross-lingual topic track-
ing, presented in section 6, is therefore based on 
three kinds of information. Two of them exploit 
the co-occurrence of named entities in related news 
stories: (a) cognates (i.e. words that are the same 
across languages, including names) and (b) geo-
graphical references. The third component, (c) a 
process mapping texts onto a multilingual classifi-
cation scheme, provides an additional, more con-
tent-oriented similarity measure. Pouliquen et al 
(2003) showed that mapping texts onto a multilin-
gual classification system can be very successful 
for the task of identifying document translations. 
This approach should thus also be an appropriate 
measure to identify similar documents in other 
languages, such as news discussing the same topic. 
3 Feature extraction for document represen-
tation 
The similarity measure for monolingual news 
item clustering, discussed in section 4, is a cosine 
of weighted terms (see 3.1) enriched with informa-
tion about references to geographical place names 
(see 3.2).  Related news are tracked over time by 
calculating the cosine of their cluster representa-
tions, while setting certain thresholds (section 5). 
The cross-lingual linking of related clusters, as de-
scribed in section 6, additionally uses the results of 
a mapping process onto a multilingual classifica-
tion scheme (see 3.3).  
The news corpus consists of a daily average of 
3350 English news items, 2100 German, 870 Ital-
ian, 800 French and 530 Spanish articles, coming 
from over three hundred different internet sources.  
3.1 Keyword identification 
For monolingual applications, we represent 
documents by a weighted list of their terms. For 
the weighting, we use the log-likelihood test, 
which is said to perform better than the alternatives 
TF.IDF or chi-square when comparing documents 
of different sizes (Kilgarriff 1996). The reference 
corpus was produced with documents of the same 
type, i.e. news articles. It is planned to update the 
reference word frequency list daily or weekly so as 
to take account of the temporary news bias towards 
specific subjects (e.g. the Iraq war). We set the p-
value to 0.01 in order to limit the size of the vector 
to the most important words. Furthermore, we use 
a large list of stop words that includes not only 
function words, but also many other words that are 
not useful to represent the contents of a document. 
We do not consider part-of-speech information and 
do not carry out stemming or lemmatisation, in 
order to increase the speed of the process and to be 
able to include new languages quickly even if we 
do not have linguistic resources for them. Cluster-
ing results do not seem to suffer from this lack of 
linguistic normalisation, but when we extend the 
system to more highly inflected languages, we will 
have to see whether lemmatisation will be neces-
sary. The result of the keyword identification proc-
ess is thus a representation of each incoming news 
article in a vector space.  
3.2 Geographical Place Name Recognition 
For place name recognition, we use a system that 
has been developed by Pouliquen et al (2004). 
Compared to other named entity recognition sys-
tems, this tool has the advantage that it recognises 
exonyms (foreign language equivalences, e.g. Ven-
ice vs. Venezia) and that it disambiguates between 
places with the same name (e.g. Paris in France vs. 
the other 13 places called Paris in the world). 
However, instead of using the city and region 
names as they are mentioned in the article, each 
place name simply adds to the country score of 
each article. The idea behind this is that the place 
names themselves are already contained in the list 
of keywords. By adding the country score sepa-
rately, we heighten the impact of the geographical 
information on the clustering process.  
The country scores are calculated as follows: for 
each geographical place name identified for a 
given country, we add one to the country counter. 
We then normalise this value using the log-
likelihood value, using the average country counter 
in a large number of other news articles as a refer-
ence base. As with keywords, we plan to update 
the country counter reference frequency list on a 
daily or weekly basis. The resulting normalised 
country score has the same format as the keyword 
list so that it can simply be added to the document 
vector space representation.  
3.3 Mapping documents onto a multilingual 
classification scheme 
For the semantic mapping of news articles, we 
use an existing system developed by Pouliquen et 
al. (2003), which maps documents onto a multilin-
gual thesaurus called Eurovoc. Eurovoc is a wide-
coverage classification scheme with approximately 
6000 hierarchically organised classes. Each of the 
classes has exactly one translation in the currently 
22 languages for which it exists. The system car-
ries out category-ranking classification using Ma-
chine Learning methods. In an inductive process, it 
builds a profile-based classifier by observing the 
manual classification on a training set of docu-
ments with only positive examples. The outcome 
of the mapping process is a ranked list of the 100 
most pertinent Eurovoc classes. Due to the multi-
lingual nature of Eurovoc, this representation is 
independent of the text language so that it is very 
suitable for cross-lingual document similarity cal-
culation, as was shown by Pouliquen et al (2003).  
4 Clustering of news articles 
In this process, larger groups of similar articles 
are grouped into clusters. Unlike in document clas-
sification, clustering is a bottom-up, unsupervised 
process, because the document classes are not 
known beforehand. 
4.1 Building a dendrogram 
In the process, we build a hierarchical clustering 
tree (dendrogram), using an agglomerative algo-
rithm (Jain et al 1999). In a first step, (1) we cal-
culate the similarity between each document pair 
in the collection (i.e. one full day of news in one 
language), applying the cosine formula to the 
document vector pairs. The vector for each single 
document consists of its keywords and their log-
likelihood values, enhanced with the country pro-
file as described in sections 3.1 and 3.2. (2) When 
two or more documents have a cosine similarity of 
90% or more, we eliminate all but one of them as 
we assume that they are duplicates or near-
duplicates, i.e. they are exact copies or slightly 
amended versions of the same news wire. (3) We 
then combine the two most similar documents into 
a cluster, for which we calculate a new representa-
tion by merging the two vectors into one. For the 
node combining the two documents, we also have 
an intra-cluster similarity value showing the degree 
to which the two documents are similar. For the 
rest of the clustering process, this node will be 
treated like a single document, with the exception 
that it will have twice the weight of a single docu-
ment when being merged with another document 
or cluster of documents. We iteratively repeat steps 
(1) and (3) so as to include more and more docu-
ments into the binary dendrogram until all docu-
ments are included. The resulting dendrogram will 
have clusters of articles that are similar, and a list 
of keywords and their weight for each cluster. The 
degree of similarity for each cluster is shown by its 
intra-cluster similarity value.  
4.2 Cluster extraction to identify main events 
In a next step, we search the dendrogram for the 
major news clusters of the day, by identifying all 
sub-clusters of documents that fulfil the following 
conditions: (a) the intra-cluster similarity (cluster 
cohesiveness) is above the threshold of 50%; (b) 
the number X of articles in the cluster is at least 
0.6% of the total number of articles of that lan-
guage per day; (c) the number Y of different feeds 
is at least half the minimum number of articles per 
cluster (Y = X/2).  
The threshold of 50% in (a) was chosen because 
it guarantees that most related articles are included 
in the cluster, while unrelated ones are mostly ex-
cluded (see section 4.3). The minimum number of 
articles per cluster in (b) was chosen to limit the 
number of major news clusters per day. We re-
quested a minimum number of different news 
feeds (c) so as to be sure that the news items are of 
general interest and that we are not dealing with 
some newspaper-specific or local issues.  
With the current settings, the system produces an 
average of 9 English major news clusters per day, 
11 Italian, 16 German, 20 French and 21 Spanish. 
The varying numbers indicate that the settings 
should probably be changed so as to produce a 
similar number of major news clusters per day in 
the various languages. Most likely, the minimum 
number of feeds should have an upper maximum 
value for languages like English with thousands of 
news articles per day.  
For each cluster, we have the following informa-
tion: number of articles, number of sources (feeds), 
intra-cluster similarity measure and keywords. Us-
ing our group-average approach we also have the 
centroid of the cluster (i.e. the vector of features 
that represents the cluster). For each cluster, we 
compute the article that is most similar to the cen-
troid (short: the centroid article). We use the title 
of this centroid article as the title for the cluster 
and we present this article to the users as a first 
document to read about the contents of the whole 
cluster.  
The collection of clusters is mainly presented to 
the users as a flat and independent list of clusters. 
However, as we realised that some of the clusters 
are more related than others (e.g. with the recent 
interest in Iraq, there are often various clusters 
covering different aspects of the political situation 
of the country), we position clusters with an inter-
cluster similarity of over 30% closer to each other 
when presenting them to the users.  
4.3 Evaluation of the monolingual clustering 
The evaluation of clustering results is rather 
tricky. According to Joachims (2003), clustering 
results can be evaluated using a variety of different 
ways: (a) let the market decide (select the winner); 
(b) ask end users; (c) measure the ?tightness? or 
?purity? of clusters; (d) use human-identified clus-
ters to evaluate system-generated ones. The last 
solution (d) is out of our reach because it is very 
resource-consuming; several evaluators would be 
needed for cross-checking the human judgement. 
The ?market? (a) and user groups (b) will use and 
evaluate our system in the near future, but we need 
to evaluate the system prior to showing it to a large 
number of customers. We therefore focus on 
method (c) by letting a person judge how consis-
tently the articles of each cluster treat the same 
story.  
We evaluated the major clusters of English news 
articles (using the 50% intra-cluster similarity 
threshold) produced for the seven-day period start-
ing 9 March 2004. During this period, 71 clusters 
containing 1072 news articles were produced. The 
evaluator was asked to decide, for each cluster and 
on a four-grade scale, to what extent the clustered 
articles were related to the centroid article. Com-
paring the clustered articles to the centroid article 
was chosen over evaluating the homogeneity of the 
cluster because it is both easier and closer to the 
real-life situation of the users: users will enter the 
cluster via the centroid article and will judge the 
other articles according to whether or not they con-
tain the information they expect. The evaluation 
scale distinguishes the following ratings:  
 
(0) wrong link, e.g. Madrid football results vs. 
Madrid elections; this is a hypothetical exam-
ple as no such link was found.  
(1) loosely connected story, e.g. Welsh documen-
tary on drinking vs. alcohol policy in Britain; 
(2) interlinked news stories, e.g. 11/03 Madrid 
bombing vs. elections of the Spanish Prime 
Minister Zapatero vs. Spanish decision to pull 
troops out of Iraq; 
(3) same news story. 
 
In the evaluation, 91.5% of the articles were 
rated as good (3), 7.7% were rated as interlinked 
(2) and 0.8% were rated as loosely connected. No 
wrong links were found. 47 of the 71 clusters only 
contained good articles (3). Loosely connected ar-
ticles (1) were distributed evenly. No more than 
two  articles of this rating were found in a single 
cluster. They never amounted to more than 17% of 
all articles in a cluster (2 out of 12 articles).  
An evaluation of the clusters produced on one 
day?s data with 30% and 40% intra-cluster similar-
ity thresholds showed that the performance de-
creased drastically. In 30%-clusters, we found sev-
eral wrong links (category 0), while no such wrong 
links were found in the 50%-clusters. The total 
number of wrong (0) or loosely connected (1) arti-
cles went up from one (in the 50%-cluster for that 
day) to 37. Furthermore, the worst clusters con-
tained over 50% of such unrelated articles. The 
40%-clusters were of a slightly better quality, but 
they still were clearly less good than the 50%-
clusters: The percentage of wrong (0) and loosely 
connected (1) articles only went up from 0.8% (in 
the 50%-clusters) to 4%, but some of the 40%-
clusters still had more bad (category 0 or 1) than 
good (category 2 or 3) articles. These numbers 
confirm that our choice of the 50% intra-cluster 
similarity threshold is most useful. 
We have not produced a quantitative evaluation 
of the miss rate of the clustering process (i.e. the 
number of related articles not included in the clus-
ter, showing the recall). However, a full-text 
search of the relevant proper names in the rest of 
the news collection showed that the clustering 
process missed very few related articles. In any 
case, from our users? point of view, it is much 
more important to know the major news stories of 
a specific day than being able to access all articles 
on the subject.  
Statistical evaluation showed no correlation be-
tween cluster size and accuracy. However, cate-
gory (2) results were more frequently found in 
clusters pertaining to news stories that go on for a 
long time, such as the US presidential elections. 
These stories get wide coverage without being 
?breaking news?, and many of the articles involved 
are commentaries. Some of the category (2) results 
were also found in stories around the Madrid 
bombing and its consequences: some articles dis-
cussed the bombing itself on 11 March (number of 
dead, investigation, mourning); others discussed 
the fact that, in the 14 March elections, the Spanish 
people elected the socialists as they felt that former 
Prime Minister Aznar?s politics were partially re-
sponsible for this tragedy; yet other articles dis-
cussed the post-election consequences such as the 
decision of the new Socialist government to pull 
out the Spanish troops from Iraq, etc. Many of the 
articles touched upon several of these issues. Arti-
cles were rated as good (3) if they had at least one 
core topic in common with the centroid article.  
5 Monolingual linking of news over time 
Establishing automatic links between the major 
clusters of news published in one language in the 
last 24 hours and the news published in previous 
days can help users in their analysis of events. Es-
tablishing historical links between related news 
stories is the third of the TDT tasks (see the intro-
duction in section 1).  
We track topics by calculating the cosine simi-
larity between all major news clusters of one day 
with all major news clusters of the previous days, 
currently up to a maximum distance of seven days. 
The input for the similarity calculation is the clus-
ter vector produced by the monolingual clustering 
process (see section 4.2). The output for each pair-
wise similarity calculation is a similarity value be-
tween 0 and 1. Whether we decide that two clusters 
are related or not depends on the similarity thresh-
old we set. We found that related clusters over time 
have an extremely high similarity, often around 
90%, which shows that the vocabulary used in 
news stories over time changes very little. For test-
ing purposes, we set the threshold very low, at 
15%, so that we could determine a useful threshold 
during the evaluation process.  
5.1 Evaluation of historical linking 
We evaluated the historical links for the 136 
English clusters of major news produced for the 
two-week period starting on 9 March 2004, look-
ing at the seven-day window preceding the day for 
which each major news cluster was identified. The 
total number of historical links found for this pe-
riod is 228, i.e. on average 1.68 historical links per 
major news cluster. However, for 42 of the 136 
major news clusters, the system did not find any 
related news clusters with a similarity of 15% or 
more.  
We made a binary distinction between ?closely 
related articles? (+) and ?unrelated, or not so re-
lated articles? (?).The evaluation results at varying 
cosine similarity thresholds, displayed in Table 1, 
show that there is no threshold which includes all 
good clusters and excludes all bad ones. Setting the 
threshold at 40% would mean that 173 (135+24+ 
14) of the 203 good clusters (86%) would be found 
while three bad ones would also be shown to the 
user. Setting the threshold at the more inclusive 
level of 20% would mean that 199 of the 203 good 
clusters (98%) would be found, but the number of 
unrelated ones would increase to 17.  
 
Similarity + Related  ? Unrelated 
15 ? 19% 4 8 
20 ? 39% 26 14 
40 ? 59% 14 2 
60 ? 79% 24 0 
80 ? 100% 135 1 
Total 203 25 
Table 1: Evaluation, for varying similarity thresh-
olds, of the automatically detected links between 
major news of the day and the major news pub-
lished in the seven days before. The distinction 
was binary: Related (+) or Not (so) related (?). 
6 Cross-lingual linking of news clusters 
News analysts and employees in press rooms 
and public relations departments often want to see 
how the same news is discussed in different coun-
tries. To allow easy access to related news in other 
languages, we establish cross-lingual links between 
the clusters of major news stories. As major news 
in one country sometimes is only minor news in 
another, we calculate a second, alternative group of 
news clusters for each language and each day, con-
taining a larger number of smaller clusters. To get 
this alternative group of clusters, we set the intra-
cluster similarity to 25% and require that the news 
of the cluster come from at least two different 
news sources. These conditions are much weaker 
than the requirements described in section 4.2. For 
each major news cluster (50% intra-cluster similar-
ity) per day and per language, we thus try to find 
related news in the other languages among any of 
the smaller clusters produced with the 25% intra-
cluster similarity requirement.  
We use three types of input for the calculation of 
cross-lingual cluster similarity: (a) the vector of 
keywords, as described in section 3.1, not en-
hanced with geographical information, (b) the 
country score vector, as described in section 3.2, 
and (c) the vector of Eurovoc descriptors, as de-
scribed in section 3.3. The impact of the three 
components is currently set to 20%, 30% and 50% 
respectively. Using the Eurovoc vector alone 
would give very high similarity values for, say, 
news about elections in France and in the United 
States. By adding the country score, a considerable 
weight in the cross-lingual similarity calculation is 
given to the countries that are mentioned in each 
news cluster. The overlap between the keyword 
vectors of documents in two different languages 
will, of course, be extremely little, but it increases 
with the number of named entities that the docu-
ments have in common. According to Gey (2000), 
30% of content-bearing words in journalistic text 
are proper names.  
The system ignores individual articles, but calcu-
lates the similarity between whole clusters of the 
different languages. The country score and the 
Eurovoc descriptor vector are thus assigned to the 
cluster as a whole, treating all articles of each clus-
ter like one big bag of words. 
6.1 Evaluation of cross-lingual cluster links 
The evaluation for the cross-lingual linking was 
carried out on the same corpus as the evaluation of 
the historical links, i.e. taking the 136 English ma-
jor news clusters as a starting point. Cross-lingual 
cluster links were evaluated for two languages, 
English to French and English to Italian. The 
evaluation was again binary, i.e. clusters were ei-
ther judged as being ?closely related? (+) or ?unre-
lated, or not so related? (?). For 31 English clus-
ters, no French cluster was found. Similarly, for 32 
English clusters, no Italian cluster was found. This 
means that for almost 25% of the English-speaking 
major news stories (31/136), there was no equiva-
lent news cluster in the other languages.  
For the remaining English clusters, a total of 131 
French and 133 Italian clusters were detected by 
the system, i.e. on average more than one for each 
English cluster. However, when several related 
news clusters were found, only the one with the 
highest score was considered in the evaluation.  
Table 2 not only shows that the English-Italian 
links are less reliable than the English-French ones 
(the Italian document representation is inferior to 
the French one because we spent less effort on op-
timising the Italian keyword assignment), but also 
that the quality of cross-lingual links is generally 
lower than the historical links presented in sec-
tion 5.1. If we set the threshold for identifying re-
lated news across languages to 30%, the system 
catches 74 of the 75 good French clusters (99%) 
and 67 of the 69 Italian clusters (97%). However, 
the system then also proposes 13 bad French and 
12 bad Italian clusters to the users. Setting the 
threshold higher would decrease the number of 
wrong hits. However, we decided to use the 
threshold of 30% because we consider it important 
for users to be able to find related news in other 
languages. Furthermore, unrelated clusters are usu-
ally very easy to detect just by looking at the title 
of the cluster.  
 
Similarity FR +  FR ? IT + IT ? 
15 ? 19% 0 7 0 1 
20 ? 29% 1 6 2 11 
30 ? 39% 5 6 7 8 
40 ? 49% 16 4 13 5 
50 ? 59% 19 1 18 6 
60 ? 100% 34 1 29 1 
Total 75 25 69 32 
Table 2: Evaluation, for varying similarity thresh-
olds, of the automatically detected cross-lingual 
links between English major news and French (FR) 
or Italian (IT) news of the same day. The distinc-
tion was binary: Related (+) or Not (so) related (?). 
7 Conclusion and future work 
We have shown that our system can rather accu-
rately identify clusters of major news per day in 
five languages and that it can link these clusters to 
related news over time (topic tracking). The most 
interesting and novel feature of the system is, how-
ever, that it can also identify related news across 
languages, without translating articles or using bi-
lingual dictionaries. This cross-lingual cluster simi-
larity is achieved by a combination of three feature 
sets, which currently have an impact of 50%, 30% 
and 20%, respectively: the main feature set is the 
mapping onto the multilingual classification 
scheme Eurovoc; the others are the countries re-
ferred to in the articles (direct mention of the coun-
try, or of a smaller place name of that country) and 
the cognates (same strings used in the articles 
across languages, i.e. mainly named entities). The 
evaluation has shown that the results are good, but 
that the cross-lingual linking performs less well 
than the monolingual historical linking of related 
news clusters. Users felt that the system performs 
well enough for it to go online soon, for usage by a 
large user community of several thousand people. 
Improvements to the system will nevertheless be 
sought.  
Future work will include testing different set-
tings concerning the relative impact of the three 
components, as well as detecting and using more 
named entities such as absolute and relative date 
expressions, proper names, etc. A further aim is to 
extend the system to another six languages. 
The usage of cognate similarity could be im-
proved. Currently it will not work with Greek, for 
instance, except for a few proper names. We would 
therefore like to experiment with multi-lingual 
stemming methods to exploit the existence of simi-
lar words across languages such as English ele-
phant, French ?l?phant, Spanish and Italian ele-
fante and German Elefant.  
Several customer groups requested an advanced 
news analysis that distinguishes between articles 
about concrete events and articles commenting 
about these events. We will explore this issue, but 
it is very likely that this distinction will require a 
syntactic analysis of the news and cannot be made 
with our bag-of-words approach. 
Finally, we intend to work on breaking news de-
tection, i.e. detecting new events, as opposed to 
detecting major news. This work will require 
working on smaller time windows than the current 
24-hour window.  
8 Acknowledgements 
We would like to thank the Web Technology group 
of the Joint Research Centre for their collaboration 
and for giving us access to their valuable multilin-
gual news collection. Our special thanks goes to 
Clive Best, Erik van der Goot, Ken Blackler and 
Teofilo Garcia. We would also like to thank our 
former colleague Johan Hagman for introducing us 
to the methods and usefulness of cluster analysis.  
References  
Allan James, Ron Papka & Victor Lavrenko 
(1998). On-line New Event Detection and Track-
ing. Proceedings of the 21st Annual International 
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 37-
45. Melbourne, Australia 
Best Clive, Erik van der Goot, Monica de Paola, 
Teofilo Garcia & David Horby (2002). Europe 
Media Monitor ? EMM. JRC Technical Note No. 
I.02.88. Ispra, Italy.  
Friburger N. & D. Maurel (2002). Textual Similar-
ity Based on Proper Names. Proceedings of the 
workshop Mathematical/Formal Methods in In-
formation Retrieval (MFIR?2002) at the 25th 
ACM SIGIR Conference, pp. 155-167. Tampere, 
Finland. 
Gey Frederic (2000). Research to Improve Cross-
Language Retrieval ? Position Paper for CLEF. 
In C. Peters (ed.): Cross-Language Information 
Retrieval and Evaluation, Workshop of Cross-
Language Evaluation Forum (CLEF?2000), Lis-
bon, Portugal. Lecture Notes in Computer Sci-
ence 2069, Springer. 
Hyland R., C. Clifton & R. Holland (1999). Geo-
NODE: Visualizing News in Geospatial Context. 
In Afca99. 
Jain A., M. Murty & P. Flynn (1999). Data cluster-
ing: a review. Pages 264 
Joachims Thorsten (2003). Representing and Ac-
cessing Digital Information. Available at http:// 
www.cs.cornell.edu/Courses/cs630/2003fa/lectur
es/tclust.pdf 
Kilgarriff A. (1996) Which words are particularly 
characteristic of a text? A survey of statistical 
approaches. Proceedings of the AISB Workshop 
on Language Engineering for Document Analy-
sis and Recognition. Sussex, 04/1996, pp. 33-40.  
Leek Tim, Hubert Jin, Sreenivasa Sista & Richard 
Schwartz (1999). The BBN Crosslingual Topic 
Detection and Tracking System. In 1999 TDT 
Evaluation System Summary Papers. 
http://www.nist.gov/speech/tests/tdt/tdt99/papers 
Pouliquen Bruno, Ralf Steinberger & Camelia Ig-
nat (2003). Automatic identification of document 
translations in large multilingual document col-
lections. Proceedings of the International Con-
ference Recent Advances in Natural Language 
Processing (RANLP'2003), pp. 401-408. Borov-
ets, Bulgaria, 10 - 12 September 2003. 
Pouliquen Bruno, Ralf Steinberger, Camelia Ignat 
& Tom de Groeve (2004). Geographical Infor-
mation Recognition and Visualisation in Texts 
Written in Various Languages. Proceedings of 
the 2004 ACM Symposium on Applied Comput-
ing, Session on Information Access and Retrieval 
(Nicosia, Cyprus), Volume 2 of 2, pages 1051-
1058. New York.  
Schultz J. Michael & Mark Liberman (1999). 
Topic detection and Tracking using idf-weighted 
Cosine Coefficient. DARPA Broadcast News 
Workshop Proceedings.  
Wactlar H.D. (1999). New Directions in Video In-
formation Extraction and Summarization. In 
Proceedings of the 10th DELOS Workshop, Sa-
norini, Greece, 24-25 June 1999. 
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 72?79,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing sublanguages in scientific journal articles
through closure properties
Irina P. Temnikova
Linguistic Modelling Laboratory
Bulgarian Academy of Sciences
irina.temnikova@gmail.com
K. Bretonnel Cohen
Computational Bioscience Program
University of Colorado School of Medicine
Department of Linguistics
University of Colorado at Boulder
kevin.cohen@gmail.com
Abstract
It has long been realized that sublanguages
are relevant to natural language process-
ing and text mining. However, practical
methods for recognizing or characterizing
them have been lacking. This paper de-
scribes a publicly available set of tools for
sublanguage recognition. Closure proper-
ties are used to assess the goodness of fit
of two biomedical corpora to the sublan-
guage model. Scientific journal articles
are compared to general English text, and
it is shown that the journal articles fit the
sublanguage model, while the general En-
glish text does not. A number of examples
of implications of the sublanguage char-
acteristics for natural language processing
are pointed out. The software is made pub-
licly available at [edited for anonymiza-
tion].
1 Introduction
1.1 Definitions of ?sublanguage?
The notion of sublanguage has had varied defini-
tions, depending on the aspects of sublanguages
on which the authors focused. (Grishman and Kit-
tredge, 1986) focus on syntactic aspects of sub-
languages: ?. . . the term suggests a subsystem of
language. . . limited in reference to a specific sub-
ject domain. In particular, each sublanguage has
a distinctive grammar, which can profitably be
described and used to solve specific language-
processing problems? (Grishman and Kittredge,
1986).
(Kittredge, 2003) focuses on the spontaneous
appearance of sublanguages in restricted domains,
where the preconditions for a sublanguage to ap-
pear are the sharing of specialized knowledge
about a restricted semantic domain and recurrent
?situations? (e.g. scientific journal articles, or dis-
charge summaries) in which domain experts com-
municate. According to (Kittredge, 2003), charac-
teristics of a sublanguage include a restricted lexi-
con, relatively small number of lexical classes, re-
stricted sentence syntax, deviant sentence syntax,
restricted word co-occurrence patterns, and differ-
ent frequencies of occurrence of words and syn-
tactic patterns from the normal language.
(McDonald, 2000) focuses on the element of re-
striction in sublanguages?the notion that they are
restricted to a specialized semantic domain, a very
?focused? audience, and ?stipulated content,? with
the effect that both word choice and syntactic style
have reduced options as compared to the normal
language.
The notions of restriction that recur in these
definitions of ?sublanguage? lead directly to
(McEnery and Wilson, 2001)?s notion of using
the quantification of closure properties to assess
whether or not a given sample of a genre of lan-
guage use fits the sublanguage model. Closure
refers to the tendency of a genre of language to-
wards finiteness at one or more linguistic levels.
For example, a genre of language might or might
not use a finite set of lexical items, or have a fi-
nite set of sentence structures. Notions of restric-
tion suggest that a sublanguage should tend to-
wards closure on at least some linguistic levels.
To quantify closure, we can examine relationships
between types and tokens in a corpus of the genre.
In particular, we count the number of types that
are observed as an increasing number of tokens
is examined. If a genre does not exhibit closure,
then the number of types will continue to rise con-
tinually as the number of tokens increases. On
the other hand, closure is demonstrated when the
number of types stops growing after some number
of tokens has been examined.
72
1.2 Relevance of sublanguages to natural
language processing
The relevance of sublanguages to natural language
processing has long been recognized in a vari-
ety of fields. (Hirschman and Sager, 1982) and
(Friedman, 1986) show how a sublanguage?based
approach can be used for information extraction
from clinical documents. (Finin, 1986) shows that
sublanguage characterization can be used for the
notoriously difficult problem of interpretation of
nominal compounds. (Sager, 1986) asserts a num-
ber of uses for sublanguage?oriented natural lan-
guage processing, including resolution of syntac-
tic ambiguity, definition of frames for informa-
tion extraction, and discourse analysis. (Sekine,
1994) describes a prototype application of sublan-
guages to speech recognition. (Friedman et al,
1994) uses a sublanguage grammar to extract a va-
riety of types of structured data from clinical re-
ports. (McDonald, 2000) points out that modern
language generation systems are made effective in
large part due to the fact that they are applied to
specific sublanguages. (Somers, 2000) discusses
the relevance of sublanguages to machine trans-
lation, pointing out that many sublanguages can
make machine translation easier and some of them
can make machine translation harder. (Friedman
et al, 2001) uses a sublanguage grammar to ex-
tract structured data from scientific journal arti-
cles.
1.3 Previous work on sublanguage
recognition
Various approaches have been taken to recog-
nizing sublanguages. We posit here two sepa-
rate tasks?recognizing a sublanguage when one
is present, and determining the characteristics of
a sublanguage. Information-theoretic approaches
have a long history. (Sekine, 1994) clustered docu-
ments and then calculated the ratio of the perplex-
ity of the clustered documents to the perplexity
of a random collection of words. (Somers, 1998)
showed that texts drawn from a sublanguage cor-
pus have low weighted cumulative sums. (Stetson
et al, 2002) used relative entropy and squared chi-
square distance to identify a sublanguage of cross-
coverage notes. (Mihaila et al, 2012) looked at
distributions of named entities to identify and dif-
ferentiate between a wide variety of scientific sub-
languages.
Non-information-theoretic, more heuristic
methods have been used to identify sublanguages,
as well. In addition to the information-theoretic
measures described above, (Stetson et al, 2002)
also looked at such measures as length, incidence
of abbreviations, and ambiguity of abbreviations.
(Friedman et al, 2002) use manual analysis to
detect and characterize two biomedical sublan-
guages. (McEnery and Wilson, 2001) examine
closure properties; their approach is so central to
the topic of this paper that we will describe it in
some length separately.
(McEnery and Wilson, 2001) examined the clo-
sure properties of three linguistic aspects of their
material under study. As materials they used two
corpora that were assumed not to meet the sub-
language model?the Canadian Hansard corpus,
containing proceedings from the Canadian Parlia-
ment, and the American Printing House for the
Blind corpus, made up of works of fiction. As
a corpus that was suspected to meet the sublan-
guage model, they used a set of manuals from
IBM. All three corpora differed in size, so they
were sampled to match the size of the smallest
corpus, meaning that all experiments were done
on collections 200,000 words in size. The mate-
rials under study were evaluated for their closure
properties at three linguistic levels. At the most
basic level, they looked at lexical items?simple
word forms. The hypothesis here was that the non-
sublanguage corpora would not tend toward finite-
ness, i.e. would not reach closure. That is, if the
number of word types found was graphed as an
increasing number of tokens was examined, the
resulting line would grow continually and would
show no signs of asymptoting. In contrast, the
sublanguage corpus would eventually reach clo-
sure, i.e. would stop growing appreciably in size
as more tokens were examined.
The next level that they examined was the mor-
phosyntactic level. In particular, they looked at
the number of part-of-speech tags per lexical type.
Here the intuition was that if the lexicon of the
sublanguage is limited, then words might be co-
erced into a greater number of parts of speech.
This would be manifested by a smaller overall
number of unique word/part-of-speech tag combi-
nations. Again, we would expect to see that the
sublanguage corpus would have a smaller number
of word/part-of-speech tag combinations, as com-
pared to the non-sublanguage corpus. Graphing
the count of word type/POS tag sets on the y axis
73
and the cumulative number of tokens examined on
the x axis, we would see slower growth and lower
numbers overall.
The final level that they examined was the syn-
tactic level. In this case, parse tree types were
graphed against the number of sentences exam-
ined. The intuition here is that if the sublanguage
exhibits closure properties on the syntactic level,
then the growth of the line will slow and we will
see lower numbers overall.
(McEnery and Wilson, 2001) found the hy-
potheses regarding closure to be substantiated at
all levels. We will not reproduce their graphs,
but will summarize their findings in terms of ra-
tios. On the lexical level, they found type/token
ratios of 1:140 for the IBM manuals (the assumed
sublanguage), 1:53 for the Hansard corpus (as-
sumed not to represent a sublanguage), and 1:17
for the American Printing House for the Blind cor-
pus (also assumed not to represent a sublanguage).
The IBM manuals consist of a much smaller num-
ber of words which are frequently repeated.
At the morphosyntactic level, they found 7,594
type/POS sets in the IBM manuals, 18,817 in
the Hansard corpus, and 11,638 in the Ameri-
can Printing House for the Blind corpus?a much
smaller number in the apparent sublanguage than
in the non-sublanguage corpora. The word/part-
of-speech tag averages coincided with the ex-
pected findings given these number of types. The
averages were 3.19 for the IBM manuals, 2.45 for
the Hansard corpus, and 2.34 for the American
Printing House for the Blind corpus.
At the syntactic level, they found essentially lin-
ear growth in the number of sentence types as the
number of sentence tokens increased in the two
non-sublanguage corpora?the ratio of sentence
types to sentences in these corpora were 1:1.07 for
the Hansard corpus and 1:1.02 for the American
Printing House for the Blind corpus. In contrast,
the growth of sentence types in the IBM manu-
als was not quite linear. It grew linearly to about
12,000 sentences, asymptoted between 12,000 and
16,000, and then grew essentially linearly but at a
somewhat slower rate from 16,000 to 30,000 sen-
tences. The ratio of sentence types to sentence to-
kens in the IBM manuals was 1:1.66?markedly
higher than in the other two corpora.
1.4 Hypotheses tested in the paper
The null hypothesis is that there will be no differ-
ence in closure properties between the general En-
glish corpus and the two corpora of scientific jour-
nal articles that we examine. If the null hypothesis
is not supported, then it might be deviated from in
three ways. One is that the scientific corpora might
show a greater tendency towards closure than the
general English corpus. A second is that the gen-
eral English corpus might show a greater tendency
towards closure than the scientific corpora. A third
is that there may be no relationship between the
closure properties of the two scientific corpora, re-
gardless of the closure properties of the general
English corpus?one might show a tendency to-
wards closure, and the other not.
2 Materials and Methods
2.1 Materials
The data under examination was drawn from three
sources: the CRAFT corpus (Bada et al, 2012;
Verspoor et al, 2012), the GENIA corpus (Kim
et al, 2003), and a version of the British National
Corpus (Leech et al, 1994) re-tagged with Con-
nexor?s Machinese parser (Ja?rvinen et al, 2004).
The CRAFT and GENIA corpora are composed
of scientific journal articles, while the British Na-
tional Corpus is a representative corpus compris-
ing many different varieties of spoken and written
English.
The CRAFT corpus is a collection of 97 full-
text journal articles from the mouse genomics do-
main. It has been annotated for a variety of lin-
guistic and semantic features; for the purposes of
this study, the relevant ones were sentence bound-
aries, tokenization, and part of speech. We used
the 70-document public release subset of the cor-
pus, which comprises about 453,377 words.
The GENIA corpus is a collection of 1,999 ab-
stracts of journal articles about human blood cell
transcription factors. Like the CRAFT corpus,
it has been annotated for a variety of linguistic
and semantic features, again including sentence
boundaries, tokenization, and part of speech. In
the mid-2000?s, the GENIA corpus was shown to
be the most popular corpus for research in biomed-
ical natural language processing (Cohen et al,
2005). We used version 3.02 of the corpus, con-
taining about 448,843 words.
The experiment requires a corpus of general
English for comparison. For this purpose, we
74
used a subset of the British National Corpus. For
purposes of representativeness, we followed the
Brown corpus strategy of extracting the first 2,000
words from each article until a total of 453,377
words were reached (to match the size of the
CRAFT corpus).
The size of the two data sets is far more than ad-
equate for an experiment of this type?McEnery
and Wilson were able to detect closure properties
using corpora of only 200,000 words in their ex-
periments.
2.2 Methods
2.2.1 Implementation details
To determine the closure properties of arbitrary
corpora, we developed scripts that take a simple
input format into which it should be possible to
convert any annotated corpus. There are two input
file types:
? A file containing one word and its corre-
sponding part-of-speech tag per line. Part of
speech tags can consist of multiple tokens, as
they do in the BNC tag set, or of single to-
kens, as they do in most corpora. This file
format is used as the input for the lexical clo-
sure script and the word type/POS tag script.
? A file containing a sequence of part of speech
tags per line, one line per sentence. This
file format is used as input for the sentence
type closure script. We note that this is an
extremely rough representation of ?syntax,?
and arguably is actually asyntactic in that it
does not represent constituent or dependency
structure at all, but also point out that it has
the advantage of being widely applicable and
agnostic as to any particular theory of syntac-
tic structure. It also increases the sensitivity
of the method to sentence type differences,
providing a stronger test of fit to the sublan-
guage model.
Two separate scripts then process one of these
input files to determine lexical, type/POS, and sen-
tence type closure properties. The output of ev-
ery script is a comma-separated-value file suitable
for importing into Excel or other applications for
producing plots. The two scripts and our scripts
for converting the BNC, CRAFT, and GENIA cor-
pora into the input file formats will be made pub-
licly available at [redacted for anonymization pur-
poses]. To apply the scripts to a new corpus, the
Figure 1: Lexical closure properties. Tick-marks
on x axis indicate increments of 50,000 tokens.
only necessary step is to write a script to convert
from the corpus?s original format to the simple for-
mat of the two input file types described above.
2.2.2 Investigating closure properties
In all three cases, the number of types, whether of
lexical items, lexical type/part-of-speech pair, or
sentence type was counted and graphed on the y
axis, versus the number of tokens that had been
observed up to that point, which was graphed on
the x axis. In the case of the lexical and type/POS
graphs, tokens were words, and in the case of the
sentence graph, ?tokens? were sentences.
We then combined the lines for all three cor-
pora and observed the total size of types, the rate
of growth of the line, and whether or not there was
a tendency towards asymptoting of the growth of
the line, i.e. closure.
Our major deviation from the approach of
(McEnery and Wilson, 2001) was that rather than
parse trees, we used part-of-speech tag sequences
to represent sentence types. This is suboptimal in
that it is essentially asyntactic, and in that it ob-
scures the smoothing factor of abstracting away
from per-token parts of speech to larger syntactic
units. However, as we point out above, it has the
advantages of being widely applicable and agnos-
tic as to any particular theory of syntactic struc-
ture, as well as more sensitive to sentence type dif-
ferences.
3 Results
3.1 Lexical closure properties
Figure 1 shows the growth in number of types of
lexical items as the number of tokens of lexical
items increases. The British National Corpus data
is in blue, the CRAFT data is in red, and the GE-
NIA data is in green.
75
Figure 2: Type-part-of-speech tag closure proper-
ties. Tick-marks on x axis indicate increments of
50,000 tokens.
We note a drastic difference between the curve
for the BNC and the curves for CRAFT and GE-
NIA. The curves for CRAFT and GENIA are quite
similar to each other. Overall, the curve for the
BNC climbs faster and much farther, and is still
climbing at a fast rate after 453,377 tokens have
been examined. In contrast, the curves for CRAFT
and GENIA climb more slowly, climb much less,
and by the time about 50,000 tokens have been ex-
amined the rate of increase is much smaller. The
increase in CRAFT and GENIA does not asymp-
tote, as McEnery and Wilson observed for the IBM
corpus. However, contrasted with the results for
the BNC, there is a clear difference.
The type to token ratios for lexical items for the
corpora as a whole are shown in Table 1. As the
sublanguage model would predict, CRAFT and
GENIA have much higher ratios than BNC.
Corpus name Ratio
BNC 1: 12.650
CRAFT 1: 23.080
GENIA 1: 19.027
Table 1: Lexical type-to-token ratios.
3.2 Type/POS tag closure properties
Figure 2 shows the growth in number of type-
POS tag pairs as the number of tokens of lexical
item/POS tag pairs increases. The data from the
different corpora corresponds to the same colors
as in Figure 1.
Once again, we note a drastic difference be-
tween the curve for the BNC and the curves for
CRAFT and GENIA. If anything, the differences
are more pronounced here than in the case of the
lexical closure graph. Again, we do not see an
asymptote in the increase of the curves for CRAFT
and GENIA, but there is a clear difference when
contrasted with the results for the BNC.
The type-to-token sets ratios for the corpora as a
whole are shown in Table 2. Again, as the sublan-
guage model would predict, we see much higher
ratios in CRAFT and GENIA than in BNC.
Corpus name Ratio
BNC 1: 10.80
CRAFT 1: 19.96
GENIA 1: 18.18
Table 2: Type-to-token ratios for type/POS tags.
Because the Machinese Syntax parser was
used to obtain the part-of-speech tagging for
BNC and the Machinese Syntax parser?s tagset is
much more granular and therefore larger than the
CRAFT and GENIA tag sets, both of which are
adaptations of the Penn treebank tag set, we con-
sidered the hypothesis that the large size differ-
ences of the tag sets were the cause of the differ-
ences observed between BNC and the two corpora
of scientific journal articles. To test this hypothe-
sis, we manually mapped the BNC tag set to the
Penn treebank tag set. The result was a new BNC
list of tags, of the same number and granularity
as the CRAFT/GENIA ones (35-36 tags). Using
this mapping, the BNC part-of-speech tags were
converted to the Penn treebank tag set and the ex-
periment was re-run. The results show that there
is almost no difference between the results from
the first and the second experiments. The resulting
graph is omitted for space, but examining it one
can observe that the differences between the three
corpora in the graph are almost the same in both
graphs. The newly calculated type:tokens ratio for
BNC are also illustrative. They are highly similar
to the type-token ratio for the original tag set?
1:10.82 with the mapped data set vs. 1:10.80 with
the original, much larger tag set. This supports the
original results and demonstrates that differences
in tag set sizes do not interfere with the identifica-
tion of sublanguages.
3.3 Sentence type closure properties
Figure 3 shows the growth in number of sentence
types as the number of sentences increases. The
data from the different corpora corresponds to the
same colors as in Figure 1.
Here we see that all three corpora exhibit sim-
76
Figure 3: Sentence type closure properties. Tick-
marks on x axis indicate increments of 5,000 sen-
tences.
ilar curves?essentially linear, with nearly identi-
cal growth rates. This is a strong contrast with the
results seen in Figures 1 and 2. We suggest some
reasons for this in the Discussion section.
The ratio of sentence types to sentence tokens
for the corpora as a whole are given in Table 3.
As would be expected from the essentially linear
growth observed with token growth for all three
corpora, all three ratios are nearly 1:1.
Corpus name Ratio
BNC 1: 1.03
CRAFT 1: 1.14
GENIA 1: 1.11
Table 3: Sentence type-to-token ratios.
4 Discussion and Conclusions
The most obvious conclusion of this study is that
the null hypothesis can be rejected?the scien-
tific corpora show a greater tendency towards clo-
sure than the general English corpus. Further-
more, we observe that the two scientific corpora
behave quite similarly to each other at all three
levels. This second observation is not necessar-
ily a given. If we can consider for a moment the
notion that there might be degrees of fit to the sub-
language model, it is clear that from a content per-
spective the BNC is unlimited; the CRAFT cor-
pus is limited to mouse genomics, but not to any
particular area of mouse genomics (indeed, it con-
tains articles about development, disease, physiol-
ogy, and other topics); and GENIA is more lim-
ited than CRAFT, being restricted to the topic of
human blood cell transcription factors. If a tech-
nique for sublanguage detection were sufficiently
precise and granular, it might be possible to show a
strict ranking from BNC to CRAFT to GENIA in
terms of fit to the sublanguage model (i.e., BNC
showing no fit, and GENIA showing a greater fit
than CRAFT since its subject matter is even more
restricted). However, this does not occur?in our
data, CRAFT showed a stronger tendency towards
closure at the lexical level, while GENIA shows
a stronger tendency towards closure at the mor-
phosyntactic level. It is possible that the small dif-
ferences at those levels are not significant, and that
the two corpora show the same tendencies towards
closure overall.
One reason that the IBM manuals in the
(McEnery and Wilson, 2001) experiments showed
sentence type closure but the CRAFT and GE-
NIA corpora did not in our experiments is al-
most certainly related to sentence length. The
average length of a sentence in the IBM manu-
als is 11 words, versus 24 in the Hansard corpus
and 21 in the American Printing House for the
Blind corpus. In this respect, the scientific cor-
pora are much more like the Hansard and Ameri-
can Printing House for the Blind corpora than they
are like the IBM manuals?the average length of
a sentence in GENIA is 21.47 words, similar to
the Hansard and American Printing House for the
Blind corpora and about twice the length of sen-
tences in the IBM manuals. Similarly, the aver-
age sentence length of the CRAFT corpus is 22.27
words (twice the average sentence length of the
IBM manuals), and the average sentence length in
the BNC is 20.43 words. Longer sentences imply
greater chances for different sentence types.
Another reason for the tendency towards sen-
tence type closure in the IBM manuals, which was
not observed in CRAFT and GENIA, is the strong
possibility that they were written in a controlled
language that specifies the types of syntactic con-
structions that can be used in writing a manual,
e.g. limiting the use of passives, etc., as well as
lexical choices and limits on other options (Kuhn,
under review). There is no such official controlled
language for writing journal articles.
Finally, one reason that the CRAFT and GENIA
corpora did not show sentence type closure while
the IBM manuals did is that while McEnery and
Wilson represented sentence types as parses, we
represented them as sequences of part-of-speech
tags. Representing sentence types as parse trees
has the effect of smoothing out some variability
at the leaf node level. For this reason, our repre-
77
sentation increases the sensitivity of the method to
sentence type differences, providing a stronger test
of fit to the sublanguage model.
It has been suggested since Harris?s classic
work (Harris et al, 1989) that scientific writing
forms a sublanguage. However, it is also clear
from the work of (Stetson et al, 2002) and (Mi-
haila et al, 2012) that some putative sublanguages
are a better fit to the model than others, and to date
there has been no publicly available, repeatable
method for assessing the fit of a set of documents
to the sublanguage model. This paper presents
the first such package of software and uses it to
evaluate two corpora of scientific journal articles.
Future work will include evaluating the effects of
mapping all numbers to a fixed NUMBER token,
which might affect the tendencies towards lexi-
cal closure; evaluating the effect of the size of
tag sets on type/part-of-speech ratios, which might
affect tendencies towards type/part-of-speech clo-
sure; and seeking a way to introduce more syntac-
tic structure into the sentence type analysis with-
out losing the generality of the current approach.
We will also apply the technique to other biomed-
ical genres, such as clinical documents. There
is also an important next step to take?this work
provides a means for recognizing sublanguages,
but does not tackle the problem of determining
their characteristics. However, despite these limi-
tations, this paper presents a large step towards fa-
cilitating the study of sublanguages by providing
a quantitative means of assessing their presence.
In analyzing the results of the study, some im-
plications for natural language processing are ap-
parent. Some of these are in accord with the is-
sues for sublanguage natural language processing
pointed out in the introduction. Another is that this
work highlights the importance of both classic and
more recent work on concept recognition for sci-
entific journal articles (and other classes of sublan-
guages), such as MetaMap (Aronson, 2001; Aron-
son and Lang, 2010), ConceptMapper (Tanenblatt
et al, 2010), and the many extant gene mention
systems.
Acknowledgments
Irina Temnikova?s work on the research re-
ported in this paper was supported by the project
AComIn ?Advanced Computing for Innovation?,
grant 316087, funded by the FP7 Capacity Pro-
gramme (Research Potential of Convergence Re-
gions). Kevin Bretonnel Cohen?s work was sup-
ported by grants NIH 5R01 LM009254-07 and
NIH 5R01 LM008111-08 to Lawrence E. Hunter,
NIH 1R01MH096906-01A1 to Tal Yarkoni, NIH
R01 LM011124 to John Pestian, and NSF IIS-
1207592 to Lawrence E. Hunter and Barbara
Grimpe. The authors thank Tony McEnery and
Andrew Wilson for advice on dealing with the tag
sets.
References
Alan R. Aronson and Francois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association, 17:229?236.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap
program. In Proc AMIA 2001, pages 17?21.
Michael Bada, Miriam Eckert, Donald Evans, Kristin
Garcia, Krista Shipley, Dmitry Sitnikov, William
A. Baumgartner Jr., Kevin Bretonnel Cohen, Karin
Verspoor, Judith A. Blake, and Lawrence E. Hunter.
2012. Concept annotation in the craft corpus. BMC
Bioinformatics, 13(161).
K. B. Cohen, Lynne Fox, Philip V. Ogren, and
Lawrence Hunter. 2005. Corpus design for biomed-
ical natural language processing. In Proceedings of
the ACL-ISMB workshop on linking biological liter-
ature, ontologies and databases, pages 38?45. As-
sociation for Computational Linguistics.
Timothy W. Finin. 1986. Constraining the interpre-
tation of nominal compounds in a limited context.
In Ralph Grishman and Richard Kittredge, editors,
Analyzing language in restricted domains: sublan-
guage description and processing, pages 85?102.
Lawrence Erlbaum Associates.
Carol Friedman, Philip O. Anderson, John H.M.
Austin, James J. Cimino, and Stephen B. Johnson.
1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical
Informatics Association, 1:161?174.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. GE-
NIES: a natural-language processing system for the
extraction of molecular pathways from journal arti-
cles. Bioinformatics, 17(Suppl. 1):S74?S82.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35:222?235.
Carol Friedman. 1986. Automatic structuring of
sublanguage information. In Ralph Grishman and
Richard Kittredge, editors, Analyzing language in
78
restricted domains: sublanguage description and
processing, pages 85?102. Lawrence Erlbaum As-
sociates.
Ralph Grishman and Richard Kittredge. 1986. Ana-
lyzing language in restricted domains: sublanguage
description and processing. Lawrence Erlbaum As-
sociates.
Zellig Harris, Michael Gottfried, Thomas Ryckman,
Anne Daladier, Paul Mattick, T.N. Harris, and Su-
sanna Harris. 1989. The form of information in
science: analysis of an immunology sublanguage.
Kluwer Academic Publishers.
Lynette Hirschman and Naomi Sager. 1982. Auto-
matic information formatting of a medical sublan-
guage. In Richard Kittredge and John Lehrberger,
editors, Sublanguage: studies of language in re-
stricted semantic domains, pages 27?80. Walter de
Gruyter.
Timo Ja?rvinen, Mikko Laari, Timo Lahtinen, Sirkku
Paajanen, Pirkko Paljakka, Mirkka Soininen, and
Pasi Tapanainen. 2004. Robust language analy-
sis components for practical applications. In Ro-
bust and adaptive information processing for mobile
speech interfaces: DUMAS final workshop, pages
53?56.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. Genia corpus?a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19(Suppl. 1):180?182.
Richard I. Kittredge. 2003. Sublanguages and con-
trolled languages. In Ruslan Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
Tobias Kuhn. under review. Survey and classification
of controlled natural languages. Computational Lin-
guistics.
G. Leech, R. Garside, and M. Bryant. 1994. The large-
scale grammatical tagging of text: experience with
the British National Corpus. In N. Oostdijk and
P. de Haan, editors, Corpus based research into lan-
guage.
David D. McDonald. 2000. Natural language genera-
tion. In Robert Dale, Hermann Moisl, and Harold
Somers, editors, Handbood of Natural Language
Processing, pages 147?179. Marcel Dekker.
Tony McEnery and Andrew Wilson. 2001. Corpus
Linguistics. Edinburgh University Press, 2nd edi-
tion.
Claudiu Mihaila, Riza Theresa Batista-Navarro, and
Sophia Ananiadou. 2012. Analysing entity type
variation across biomedical subdomains. In Third
workshop on building and evaluating resources for
biomedical text mining, pages 1?7.
Naomi Sager. 1986. Sublanguage: linguistic phe-
nomenon, computational tool. In Ralph Grishman
and Richard Kittredge, editors, Analyzing language
in restricted domains: sublanguage description and
processing, pages 1?17. Lawrence Erlbaum Asso-
ciates.
Satoshi Sekine. 1994. A new direction for sublan-
guage nlp. In Proceedings of the international con-
ference on new methods in natural language pro-
cessing, pages 123?129.
Harold Somers. 1998. An attempt to use weighted
cusums to identify sublanguages. In NeM-
LaP3/CoNLL98: New methods in language process-
ing and computational natural language learning,
pages 131?139.
Harold Somers. 2000. Machine translation. In Robert
Dale, Hermann Moisl, and Harold Somers, editors,
Handbook of Natural Language Processing, pages
329?346. Marcel Dekker.
Peter D. Stetson, Stephen B. Johnson, Matthew Scotch,
and George Hripcsak. 2002. The sublanguage of
cross-coverage. In Proc. AMIA 2002 Annual Sym-
posium, pages 742?746.
Michael Tanenblatt, Anni Coden, and Igor Sominsky.
2010. The ConceptMapper approach to named en-
tity recognition. In Language Resources and Evalu-
ation Conference, pages 546?551.
Karin Verspoor, Kevin Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natu-
ral language processing tools. BMC Bioinformatics,
13(207).
79
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 20?29,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The C-Score ? Proposing a Reading Comprehension Metrics as a
Common Evaluation Measure for Text Simplification
Irina Temnikova
Linguistic Modelling Department,
Institute of Information
and Communication Technologies,
Bulgarian Academy of Sciences
irina.temnikova@gmail.com
Galina Maneva
Lab. of Particle and Astroparticle Physics,
Institute of Nuclear Research
and Nuclear Energy,
Bulgarian Academy of Sciences
galina.maneva@gmail.com
Abstract
This article addresses the lack of common
approaches for text simplification evalu-
ation, by presenting the first attempt for
a common evaluation metrics. The arti-
cle proposes reading comprehension eval-
uation as a method for evaluating the re-
sults of Text Simplification (TS). An ex-
periment, as an example application of the
evaluation method, as well as three for-
mulae to quantify reading comprehension,
are presented. The formulae produce an
unique score, the C-score, which gives an
estimation of user?s reading comprehen-
sion of a certain text. The score can be
used to evaluate the performance of a text
simplification engine on pairs of complex
and simplified texts, or to compare the
performances of different TS methods us-
ing the same texts. The approach can be
particularly useful for the modern crowd-
sourcing approaches, such as those em-
ploying the Amazon?s Mechanical Turk1
or CrowdFlower2. The aim of this paper
is thus to propose an evaluation approach
and to motivate the TS community to start
a relevant discussion, in order to come up
with a common evaluation metrics for this
task.
1 Context and Motivation
Currently, the area of Text Simplification (TS)
is getting more and more attention. Starting as
early as in the 1996, Chandrasekar et al pro-
posed an approach for TS as a pre-processing step
before feeding the text to a parser. Next, the
1http://aws.amazon.com/mturk/. Last accessed on May
3rd, 2013.
2http://crowdflower.com/. Last accessed on June 14th,
2013.
PSET project (Devlin, 1999; Canning, 2002), pro-
posed two modules for simplifying text for apha-
sic readers. The text simplification approaches
continued in 2003 with Siddharthan (2003) and
Inui et al (2003), and through the 2005-2006
until the recent explosion of TS approaches in
2010-2012. Recently, several TS-related work-
shops took place: PITR 2012 (Williams et al,
2012), SLPAT 2012 (Alexandersson et al, 2012),
and NLP4ITA 20123 and 2013. As in confirma-
tion with the text simplification definition as the
?process for reducing text complexity at differ-
ent levels? (Temnikova, 2012), the TS approaches
tackle a variety of text complexity aspects, rang-
ing from lexical (Devlin, 1999; Inui et al, 2003;
Elhadad, 2006; Gasperin et al, 2009; Yatskar
et al, 2010; Coster and Kauchak, 2011; Bott et
al., 2012; Specia et al, 2012; Rello et al, 2013;
Drndarevic? et al, 2013), syntactic (Chandrasekar
et al, 1996; Canning, 2002; Siddharthan, 2003;
Inui et al, 2003; Gasperin et al, 2009; Zhu et
al., 2010; Woodsend and Lapata, 2011; Coster
and Kauchak, 2011; Drndarevic? et al, 2013), to
discourse/cohesion (Siddharthan, 2003). The va-
riety of problems tackled by the TS approaches
differ, according to their final aim: (1) being a
pre-processing step of an input to text process-
ing applications, or (2) addressing the reading dif-
ficulties of specific groups of readers. The first
type of final application ranges between parser
input (Chandrasekar et al, 1996), small screens
displays (Daelemans et al, 2004; Grefenstette,
1998), text summarization (Vanderwende et al,
2007), text extraction (Klebanov et al, 2004), se-
mantic role labeling (Vickrey and Koller, 2008)
and Machine Translation (MT) (Ruffino, 1982;
Streiff, 1985).The TS approaches addressing spe-
cific human reading needs, instead, address read-
ers with low levels of literacy (Siddharthan, 2003;
3http://www.taln.upf.edu/nlp4ita/. Last accessed on May
3rd, 2013.
20
Gasperin et al, 2009; Elhadad, 2006; Williams
and Reiter, 2008), language learners (Petersen and
Ostendorf, 2007), and readers with specific cogni-
tive and language disabilities. The TS approaches,
addressing this last type of readers target those
suffering from aphasia (Devlin, 1999; Canning,
2002), deaf readers (Inui et al, 2003), dyslexics
(Rello et al, 2013) and the readers with general
disabilities (Max, 2006; Drndarevic? et al, 2013).
Despite the large number of current work in
TS, there has been almost no attention to defin-
ing common text simplification evaluation ap-
proaches, which would allow the comparison of
different TS systems. Until the present moment,
usually, each approach has applied his/her own
methods and materials, often taken from other
Natural Language Processing (NLP) fields, mak-
ing the comparison difficult or impossible.
The aim of this paper is thus to propose an eval-
uation method and to foster the discussion of this
topic in the text simplification community, as well
as to motivate the TS community to come up with
common evaluation metrics for this task.
Next, Section 2 will describe the existing ap-
proaches to evaluating TS, as well as the few
attempts towards offering a common evaluation
strategy. After that, the next sections will present
our evaluation approach, starting with Section 3
describing its context, Section 4 presenting the for-
mulae, Section 5 offering the results, and finally
Section 6, providing a Discussion and the Conclu-
sions.
2 Evaluation Methods in Text
Simplification
As mentioned in the previous section, until now,
the different authors adopted different combina-
tions of metrics, without reaching to a common
approach, which would allow the comparison of
different systems. As the different TS evalua-
tion methods are applied on a variety of different
text units (words, sentences, texts), this makes the
comparison between approaches even harder. As
the aim of this article is to propose a text simpli-
fication evaluation metrics which would take into
account text comprehensibility and reading com-
prehension, in this discussion we will focus mostly
on the approaches, whose aim is to simplify texts
for target readers and their evaluation strategies.
The existing TS evaluation approaches focus ei-
ther on the quality of the generated text/sentences,
or on the effectiveness of text simplification on
reading comprehension. The first group of ap-
proaches include human judges ratings of simpli-
fication, content preservation, and grammatical-
ity, standard MT evaluation scores (BLEU and
NIST), a variety of other automatic metrics (per-
plexity, precision/recall/F-measure, and edit dis-
tance). The methods, aiming to evaluate the text
simplification impact on reading comprehension,
use, instead, reading speed, reading errors, speech
errors, comprehension questions, answer correct-
ness, and users? feedback. Several approaches
use a variety of readability formulae (the Flesch,
Flesch-Kincaid, Coleman-Liau, and Lorge formu-
lae for English, as well as readability formulae for
other languages, such as for Spanish). Due to the
criticisms of readability formulae (DuBay, 2004),
which often restrict themselves to a very super-
ficial text level, they can be considered to stand
on the borderline between the two previously de-
scribed groups of TS evaluation approaches. As
can be seen from the discussion below, different
TS systems employ a combination of the listed
evaluation approaches.
As one of the first text simplification systems
for target reader populations, PSET, seems to have
applied different evaluation strategies for different
of its components, without running an evaluation
of the system as a whole. The lexical simplifi-
cation component (Devlin, 1999), which replaced
technical terms with more frequent synonyms, was
evaluated via user feedback, comprehension ques-
tions and the use of the Lorge readability formula
(Lorge, 1948). The syntactic simplification system
evaluated its single components and the system as
a whole from different points of view, to a dif-
ferent extent, and used different evaluation strate-
gies. Namely, the text comprehensibility was eval-
uated via reading time and answers? correctness
given by sixteen aphasic readers; the components
replacing passive with active voice and splitting
sentences were evaluated for content preservation
and grammaticality via four human judges? rat-
ings; and finally, the anaphora resolution compo-
nent was evaluated using precision and recall. Sid-
dharthan (2003) did not carry out evaluation with
target readers, while three human judges rated the
grammaticality and the meaning preservation of
ninety-five sentences. Gasperin et al (2009) used
precision, recall and f-measure. Other approaches,
using human judges are those of Elhadad (2006),
21
who also used precision and recall and Yatskar et
al. (2010), who employed three annotators com-
paring pairs of words and indicating them same,
simpler, or more complex. Williams and Reiter
(2008) run two experiments, the larger one in-
volving 230 subjects and measured oral reading
rate, oral reading errors, response correctness to
comprehension questions and finally, speech er-
rors. Drndarevic et al (2013) used 7 readabil-
ity measures for Spanish to evaluate the degree
of simplification, and twenty-five human annota-
tors to evaluate on a Likert scale the grammat-
icality of the output and the preservation of the
original meaning. The recent approaches consid-
ering TS as an MT task, such as Specia (2010),
Zhu et al (2010), Coster and Kauchak (2011)
and Woodsend and Lapata (2011), apply standard
MT evaluation techniques, such as BLEU (Pap-
ineni et al, 2002), NIST (Doddington, 2002), and
TERp (Snover et al, 2009). In addition, Wood-
send and Lapata (2011) apply two readability mea-
sures (Flesch-Kincaid, Coleman-Liau) to evalu-
ate the actual reduction in complexity and human
judges ratings for simplification, meaning preser-
vation, and grammaticality. Zhu et al (2010) ap-
ply the Flesch readability score (Flesch, 1948) and
n-gram language model perplexity, and Coster and
Kauchak (2011) ? two additional automatic tech-
niques (the word-level-F1 and simple string accu-
racy), taken from sentence compression evaluation
(Clarke and Lapata, 2006).
As we consider that the aim of text simplifica-
tion for human readers is to improve text com-
prehensibility, we argue that reading comprehen-
sion must be evaluated, and that evaluating just
the quality of produced sentences is not enough.
Differently from the approaches that employ hu-
man judges, we consider that it is better to test real
human comprehension with target readers popula-
tions, rather than to make conclusions about the
extent of population?s understanding on the basis
of the opinion of a small number of human judges.
In addition, we consider that measuring reading
speed, rate, as well as reading and speed errors,
requires much more complicated and expensive
tools, than having an online system to measure
time to reply and recognize correct answers. Fi-
nally, we consider that cloze tests are an evalu-
ation method that cannot really reflect the com-
plexity of reading comprehension (for example for
measuring manipulations of the syntactic struc-
ture of sentences), and for this reason, we select
multiple-choice questions as the testing method,
which we consider the most reflecting the speci-
ficities of the complexity of a text, more accessi-
ble than eye-tracking technologies, and more ob-
jective than users? feedback. The approach does
not explicitly evaluate the fluency, grammaticality
and content preservation of the simplified text, but
can be coupled with such additional evaluation.
The closest to ours approach is that of Rello
et al (2013) who evaluated reading comprehen-
sion with over ninety readers with and without
dyslexia. Besides using eye-tracking (reading time
and fixations duration), different reading devices,
and users rating the text according to how easy it is
it read, to understand and to remember, they obtain
also a comprehension score based on multiple-
choice questions (MCQ) with 3 answers (1 cor-
rect, 1 partially correct and 1 wrong). The dif-
ference with our approach is that we consider that
having only one correct answer (as suggested by
Gronlund (1982)), is a more objective evaluation,
rather than having one partially correct answer,
which would introduce subjectivity in evaluation.
To support our motivation, some state-of-the-art
approaches state the scarcity of evaluation with
target readers (Williams and Reiter, 2008), note
that there are no commonly accepted evaluation
measures (Coster and Kauchak, 2011), attempt
to address the need of developing reading com-
prehension evaluation methods (Siddharthan and
Katsos, 2012), and propose common evaluation
frameworks (Specia et al, 2012; De Belder and
Moens, 2012). More concretely, Siddhathan and
Katsos (2012) propose the magnitude estimation
of readability judgements and the delayed sen-
tence recall as reading comprehension evaluation
methods. Specia et al (2012) provide a lexical
simplification evaluation framework in the context
of Semeval-2012. The evaluation is performed us-
ing a measure of inter-annotator agreement, based
on Cohen (1960). Similarly, De Belder and Moens
(2012) propose a dataset for evaluating lexical
simplification. No common evaluation framework
has been yet developed for syntactic simplifica-
tion.
As seen in the overview, besides the multitude
of existing approaches, and the few approaches at-
tempting to propose a common evaluation frame-
work, there are no widely accepted evaluation
metrics or methods, which would allow the com-
22
parison of existing approaches. The next section
presents our evaluation approach, which we offer
as a candidate for common evaluation metrics.
3 Proposed Evaluation Metrics
3.1 The Evaluation Experiment
The metrics proposed in this article, was devel-
oped in the context of a previously conducted
large-scale text simplification evaluation experi-
ment (Temnikova, 2012). The experiment aimed
to determine whether a manual, rule-based text
simplification approach (namely a controlled lan-
guage), can re-write existing texts into more un-
derstandable versions. Impact on reading com-
prehension was necessary to evaluate, as the pur-
pose of text simplification was to enhance in first
place the reading comprehension of emergency in-
structions. The controlled language used for sim-
plification was the Controlled Language for Cri-
sis Management (CLCM, more details in (Tem-
nikova, 2012)), which was developed on the ba-
sis of existing psychological and psycholinguis-
tic literature discussing human comprehension un-
der stress, which ensures its psychological valid-
ity. The text units evaluated in this experiments
were whole texts, and more concretely pairs of
original texts and their simplified versions. We ar-
gue that using whole texts for measuring reading
comprehension is better than single sentences, as
the texts provide more context for understanding.
The experiment took place in the format of an on-
line experiment, conducted via a specially devel-
oped web interface, and required users to read sev-
eral texts and answer Multiple-Choice Questions
(MCQ), testing the readers? understanding of each
of the texts. Due to the purpose of the text simpli-
fication (emergency situations simulation), users
were required to read the texts in a limited time,
as to imitate a stressful situation with no time to
think and re-read the text. This aspect will not be
taken into account in the evaluation, as the pur-
pose is to propose a general formula, applicable
to a variety of different text simplification experi-
ments. After reading the text in a limited time, the
text was hidden from the readers, and they were
presented with a screen, asking if they were ready
to proceed with the questions. Next, each question
was displayed one by one, along with its answers,
with the readers not having the option to go back
to the text. In order to ensure the constant atten-
tion of the readers and to reduce readers? tiredness
fact or, the texts were kept short (about 150-170
words each), and the number of texts to be read
by the reader was kept to four. In addition, to en-
sure comparability, all the texts were selected in a
way to be more or less of the same length. The ex-
periment employed a collection of a total of eight
texts, four of which original, non simplified (?com-
plex?) versions, and the other four ? their manu-
ally simplified versions. Each user had to read two
complex and two simplified texts, none of which
was a variant of the other. The interface automati-
cally randomized the order of displaying the texts,
to ensure that different users would get different
combinations of texts in one of the following two
different sequences:
? Complex-Simplified-Complex-Simplified
? Simplified-Complex-Simplified-Complex
This was done in order to minimize the im-
pact of the order of displaying the texts on the
text comprehension results. After reading each
text, the readers were prompted to answer between
four and five questions about each text. The MCQ
method was selected as it is considered being the
most objective and easily measurable way of as-
sessing comprehension (Gronlund, 1982). The
number of questions and answers was selected in
a way to not tire the reader (four to five questions
per text and four to five answers for each ques-
tion), and the questions and answers themselves
were designed following the the best MCQ prac-
tices (Gronlund, 1982). Some of the practices fol-
lowed involved ensuring that there is only one cor-
rect answer per question, making all wrong an-
swers (or ?distractors?) grammatically, and as text
length consistent with the correct answer, in order
to avoid giving hints to the reader, and making all
distractors plausible and equally attractive. Simi-
larly to the texts, the questions and answers were
also displayed in different order to different read-
ers, to avoid that the order influences the compre-
hension results. The correct answer was displayed
in different positions to avoid learning its position
and internally marked in a way to distinguish it
during evaluation from all the distractors in what-
ever position it was displayed. The questions re-
quired understanding of key aspects of the texts, to
avoid relying on pure texts? memorization (such as
under which conditions what was supposed to be
done, explanations, and the order in which actions
needed to be taken). The information, evaluating
23
the users? comprehension, collected during the ex-
periment, was, on one hand the time for answering
each question, and on the other hand, the number
of correct answers given by all participants while
replying to the same question. Besides the fact that
we used a specially developed interface, this eval-
uation approach can be applied to any experiment
employing an interface capable of calculating the
time for answering and to distinguish the correct
answers from the incorrect ones.
The efficiency of the experiment design was
thoroughly tested by running it through several
rounds of pilot experiments and requiring partic-
ipants? feedback.
We claim that the evaluation approach proposed
in this paper can be applied to more simply orga-
nized experiments, as the randomization aspects
are not reflected in the evaluation formulae.
The final experiment involved 103 participants,
collected via a request sent to several mailing lists.
The participants were 55 percent women and 44
percent male, and ranged from undergraduate stu-
dents to retired academicians (i.e. corresponded
to nineteen to fifty-nine years old). As the ex-
periment allowed entering lots of personal data,
it was also known that participants had a vari-
ety of professions (including NLP people, teach-
ers, and lawyers), knew English from the beginner
through intermediate, to native level, and spoke
a large variety of native languages, allowing to
have native speakers from many of the World?s
language families (Non Indo-European and Indo-
European included). Figure 1 shows the coarse-
grained classification made at the time of the ex-
periment, and the distribution of participants per
native languages. A subset of specific native lan-
guage participants will be selected to give an ex-
ample of applying the evaluation metrics to a real
evaluation experiment.
In order to obtain results, we have asked the
participants to enter a rich selection of informa-
tion, and recorded the chosen answer (be it cor-
rect or not), and the time which each participant
employed to give each answer (correct or wrong).
Table 1 shows the data we recorded for each single
answer of every participant.
The data in Table 1 is: Entry id is each given an-
swer, the Domain background (answer y ? yes and
n ? no) indicates whether the participant has any
previous knowledge of the experiment (crisis man-
agement) domain. As each text, question and com-
Type Example
Entry id 1
Age of the participant 24
Gender of the participant f
Profession of the participant Student
Domain background (y/n) n
Native lang. English
Level of English Native
Text number 4
Exper. completed (0/1) 1
User number 1
Question number 30
Answer number 0
Time to reply 18695
Texts pair number 1
Table 1: Participant?s information recorded for
each answer.
plex/simplified texts pair are given reference num-
bers, respectively Text number, Question number,
and Texts pair number record that. As required
by the evaluation method, each entry records also
the Time to reply each question (measured in ?mil-
liseconds?), and the Answer number. As said be-
fore, the correct answers are marked in a special
way, allowing to distinguish them at a later stage,
when counting the number of correct answers.
3.2 Definitions and Evaluation Hypotheses
In order to correctly evaluate the performance of
the text simplification method on the basis of the
above described experiment, the data obtained was
thoughtfully analyzed. The two criteria selected to
best describe the users? performance were time to
reply and number of correct answers. The eval-
uation was done offline, after collecting the data
from the participants. The evaluation analysis
aimed to test the following two hypotheses:
If the text simplification approach has a positive
impact on the reading comprehension:
1. The percentage of correct answers given for
the simplified text will be higher than the per-
centage of correct answers given for the com-
plex text.
2. The time to recognize the correct answer and
reply correctly to the questions about the sim-
plified text will be significantly lower than
the time to recognize the correct answer and
24
Figure 1: Coarse-grained distribution of participants per native languages.
reply correctly to the questions about the
complex text.
The two hypotheses were tested previously by
employing only the key variables (time to reply
and number of correct answers). It has been
proven that comprehension increases with the per-
centage of correct answers and decreases with the
increase of the time to reply. On the basis of
these facts, we define the C-Score (a text Compre-
hension Score) ? an objective evaluation metrics,
which allows to give a reading comprehension es-
timate to a text, or to compare two texts or two or
more text simplification approaches. The C-Score
is calculated text per text. In order to address a va-
riety of situations, we propose three versions of the
C-Score, which cover, gradually, all possible vari-
ables which can affect comprehension in such an
experiment. In the following sections we present
their formulae, the variables involved, and discuss
their results, advantages and shortcomings.
3.3 The C-Score Version One. The C-Score
Simple.
Given a text comprehension experiment featuring
n texts with m questions with r answers each, an
ability to measure time to reply to questions and
to recognize the correct answers, we define the C-
Score Simple as given below:
Csimple =
Pr
tmean
(1)
Where: Pr is the percentage of correct answers,
from all answers given to all the questions about
this text, and t is the average time to reply to all
questions about this text (both with a correct and
a wrong answer). The time is expressed in arbi-
trary seconds-based units, depending on the ex-
periment. The logic behind this formula is simple:
we consider that comprehension increases with the
percentage of correctly answered questions, and
diminishes if the mean time to answer questions
increases.
3.4 The C-Score Version Two. C-Score
Complete.
The C-Score complete takes into consideration a
rich selection of variables reflecting the questions
and answers complexity. In this C-Score version,
we consider that the experiment designers will se-
lect short texts (e.g. 150 words) of a similar length,
with the aim to reduce participants? tiredness fac-
tor, as we did in our experimental settings.
Ccomplete =
Pr
Nq
Nq?
q=1
Qs(q)
tmean(q)
(2)
In this formula, Pr is the percentage of correct
answers by all participants for this text, Nq is the
25
number of questions of this text (4-5 in our experi-
ment), and t is the average time to reply to all ques-
tions about this text (4-5 in our experiment). We
introduce the concept Question Size, (Qs), which
is calculated for each question and takes into ac-
count the number of answers of the question (Na),
the question length in words (Lq), and the total
length in words of its answers (La):
Qs = Na(Lq + La) (3)
We consider that the number of questions nega-
tively influences the comprehension results, as the
reader gets cognitively tired to process more and
more questions about different key aspects of the
text. In addition, Gronlund (1982) suggests to re-
strict the number of questions per text to four-five
to achieve better learning. For this reason, we con-
sider that comprehension decreases, if the num-
ber of questions is higher. We also consider that
answering correctly/faster to a difficult question
shows better text comprehension than giving fast
a correct answer to a simply-worded question. For
this reason we award question difficulty, and we
place it above the fraction.
3.5 The C-Score Version Three. C-Score
Textsize.
Finally, the last version of C-Score takes into ac-
count the case when the texts used for compari-
son can be of a different length, and in this way,
the texts? complexity (for example, when compar-
ing the results of two different TS engines, without
having access to the same texts). For this reason,
the C-Score 3 considers the text length (called text
size, Ts) of the texts used in the experiment. As
a longer text will be more difficult to understand
than a shorter text, the text length is placed near
the percentage of correct answers.
Ctextsize =
PrTs
Nq
Nq?
q=1
Qs(q)
tmean(q)
(4)
4 C-Score Results
We have implemented and applied the above de-
scribed formulae to the experimental data, pre-
sented in Section 3.1. As we have only one text
simplification approach, two user scenarios are
presented:
1. Original (?Complex?) vs. Simplified (?Sim-
ple?) pairs of texts comparison. The subset of
participants are the speakers of Basque, Turk-
ish, Hungarian, Lithuanian, Vietnamese, Chi-
nese, and Indian languages. All three formu-
lae have been applied.
2. Comparison of the comprehension of the
same text of readers from different sub-
groups. The readers have been divided by
age. This scenario can be used to infer
psycho-linguistic findings about the reading
abilities of different participants.
Please note that the texts pairs are: Text 1 and
2; Text 3 and 4; Text 5 and 6; and Text 7 and 8.
In each couple, the first text is complex and the
second is its simplified version. The results for
the first evaluation scenario are respectively dis-
played in Table 2 for C-Score Simple, Table 3 for
C-Score Complete and Table 4 for C-Score Text-
size. The results of C-Score Complete have been
multiplied per 100 for better readability. As a re-
minder, we consider that higher the score is, better
is text comprehension. From this point of view,
if the text simplification approach was successful,
Text 2 (Simplified) should have a higher C-Score
than its original, complex Text 1, Text 4 (Simpli-
fied) should have a higher C-Score than its orig-
inal Text 3, Text 6 (Simplified) ? a higher score
than the complex Text 5, and Text 8 (Simplified) ?
a higher score than its original Text 7.
In the second scenario, the participants data
has been divided into data relevant to participants
under 45 years old (ninety-two participants) and
into participants over 45 years old (eleven partic-
ipants). In this case only the C-Score Simple has
been applied. The results of this evaluation are
shown in Table 5. As our aim is to compare the
reading abilities of different ages of people, and
not the results of text simplification, only the com-
plex texts are taken into account. The results show
that the comprehension score of participants under
45 years old is higher for all texts (despite the un-
even participants? distribution), except in the case
of complex Text 5.
A similar phenomenon can be observed in Ta-
bles 2, 3 and 4, where in all text pairs, except for
pair 3, i.e. Texts 5 and 6 (where can be observed
the opposite), the simplified text has a higher com-
prehension score than its complex original. The
hypothesis about the different behavior of Text 5
and 6 is that it is text-specific. This is confirmed
by Table 5, which shows that besides the big dif-
26
Text number C-Score Simple
Text 1 (Complex) 21.3
Text 2 (Simplified) 35.3
Text 3 (Complex) 24.8
Text 4 (Simplified) 34.9
Text 5 (Complex) 36.8
Text 6 (Simplified) 23.6
Text 7 (Complex) 40.5
Text 8 (Simplified) 51.5
Table 2: Experiment results for C-Score Simple.
ferences in reading comprehension between par-
ticipants under 45 years old and participants over
45 years old, Text 5 has more or less the same
comprehension score for both groups of readers.
From this fact we can assume that this text is prob-
ably fairly easy, so this type of combination of text
simplification rules does not simplify it, and in-
stead, when applied makes it less comprehensible
or more awkward for the human readers.
Text number C-Score Complete
Text 1 (Complex) 66.3
Text 2 (Simplified) 114.3
Text 3 (Complex) 65.3
Text 4 (Simplified) 89.9
Text 5 (Complex) 104.0
Text 6 (Simplified) 66.9
Text 7 (Complex) 106.7
Text 8 (Simplified) 153.0
Table 3: Experiment results for C-Score Com-
plete.
Text number C-Score Textsize
Text 1 (Complex) 109.5
Text 2 (Simplified) 192.0
Text 3 (Complex) 107.7
Text 4 (Simplified) 131.3
Text 5 (Complex) 171.6
Text 6 (Simplified) 102.4
Text 7 (Complex) 176.1
Text 8 (Simplified) 263.3
Table 4: Experiment results for C-ScoreTextsize.
5 Discussion and Conclusions
This article has presented an extended discussion
of the methods employed for evaluation in the text
Text number Under 45 Over 45
Text 1 (Complex) 39.7 22.5
Text 3 (Complex) 37.2 18.4
Text 5 (Complex) 38.4 38.9
Text 7 (Complex) 54.3 35.9
Table 5: C-Score Simple for one text.
simplification domain. In order to address the lack
of common or standard evaluation approaches,
this article proposed three evaluation formulae,
which measure the reading comprehension of pro-
duced texts. The formulae have been developed on
the basis of an extensive reading comprehension
experiment, aiming to evaluate the impact of a text
simplification approach (a controlled language) on
emergency instructions. Two evaluation scenarios
have been presented, the first of which calculated
with all three formulae, while the second used only
the simplest one. In this way, the article aims
to address both the lack of common TS evalua-
tion metrics as suggested in Section 2 (Coster and
Kauchak, 2011) and the scarcity of reading com-
prehension (Siddharthan and Katsos, 2012) evalu-
ation with real users (Williams and Reiter, 2008),
by proposing a tailored approach for this type of
text simplification evaluation. With this article we
aim at inciting the Text Simplification Commu-
nity to open a discussion forum about common
methods for evaluating text simplification, in or-
der to provide objective evaluation metrics allow-
ing the comparison of different approaches, and to
ensure that simplification really achieves its aims.
We also argue that taking in consideration the end-
users and text units used for evaluation is impor-
tant. In our approach, we address only the eval-
uation of text simplification approaches aiming to
improve reading comprehension and experiments
in which time to reply to questions and percent-
age of correct answers can be measured. A plausi-
ble scenario for applying our evaluation approach
would be to use the Amazon Mechanical Turk
for crowd-sourcing and then to evaluate the per-
formance of a text simplification system on com-
plex and simplified texts, to compare the perfor-
mance of two or more approaches, or of two ver-
sions of the same system on the same pairs of
texts. These formulae can be also employed in
psycholinguistically-oriented experiments, which
aim to reach cognitive findings regarding specific
target reader groups, such as dyslexics or autis-
27
tic readers. Future work will involve the com-
parison of the above proposed evaluation metrics
with any of the metrics already employed in the
related work, such as the recent and classic read-
ability formulae, eye-tracking, reading rate, hu-
man judges ratings, and others. We consider that
content preservation and grammaticality are not
necessary to be evaluated for this approach, as the
simplified texts have been produced manually, by
linguists, who were native speakers of English.
Acknowledgments
The authors would like to thank Prof. Dr. Petar
Temnikov for the ideas and advices about the re-
search methodology, Dr. Anke Buttner for the psy-
cholinguistic counseling about the experiment de-
sign, including questions, answers and texts se-
lection and the simplification method psycholog-
ical validity, and Dr. Constantin Orasan and Dr.
Le An Ha for the testing interface implementa-
tion. The research of Irina Temnikova reported in
this paper was partially supported by the project
AComIn ?Advanced Computing for Innovation?,
grant 316087, funded by the FP7 Capacity Pro-
gramme (Research Potential of Convergence Re-
gions).
Finally, the authors would also like to thank the
PITR 2013 reviewers for their useful feedback.
References
Jan Alexandersson, Peter Ljunglf, Kathleen F. Mc-
Coy, Brian Roark, and Annalu Waller, editors.
2012. Proceedings of the Third Workshop on Speech
and Language Processing for Assistive Technolo-
gies. Association for Computational Linguistics,
Montre?al, Canada, June.
Stefan Bott, Luz Rello, Biljana Drndarevic?, and Hora-
cio Saggion. 2012. Can spanish be simpler? lexsis:
Lexical simplification for spanish. In Proceedings of
the 24th International Conference on Computational
Linguistics (Coling 2012), Mumbai, India (Decem-
ber 2012).
Yvonne Canning. 2002. Syntactic Simplification of
Text. Ph.D. thesis, University of Sunderland, UK.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text
simplification. In Proceedings of the 16th confer-
ence on Computational linguistics-Volume 2, pages
1041?1044. Association for Computational Linguis-
tics.
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 377?384. Association for
Computational Linguistics.
Jacob Cohen et al 1960. A coefficient of agreement
for nominal scales. Educational and psychological
measurement, 20(1):37?46.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9. Association for Computational
Linguistics.
Walter Daelemans, Anja Ho?thker, and Erik Tjong Kim
Sang. 2004. Automatic sentence simplification for
subtitling in dutch and english. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 1045?1048.
Jan De Belder and Marie-Francine Moens. 2012. A
dataset for the evaluation of lexical simplification.
In Computational Linguistics and Intelligent Text
Processing, pages 426?437. Springer.
Siobhan Devlin. 1999. Automatic Language Simplifi-
cation for Aphasic Readers. Ph.D. thesis, University
of Sunderland, UK.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145. Morgan Kauf-
mann Publishers Inc.
Biljana Drndarevic?, Sanja S?tajner, Stefan Bott, Susana
Bautista, and Horacio Saggion. 2013. Automatic
text simplification in spanish: A comparative evalu-
ation of complementing modules. In Computational
Linguistics and Intelligent Text Processing, pages
488?500. Springer.
William H. DuBay. 2004. The principles of readabil-
ity. Impact Information, pages 1?76.
Noe?mie Elhadad. 2006. User-sensitive text summa-
rization: Application to the medical domain. Ph.D.
thesis, Columbia University.
Rudolf Flesch. 1948. A new readability yardstick. The
Journal of applied psychology, 32(3).
Caroline Gasperin, Erick Maziero, Lucia Specia, TAS
Pardo, and Sandra M Aluisio. 2009. Natural lan-
guage processing for social inclusion: a text sim-
plification architecture for different literacy levels.
the Proceedings of SEMISH-XXXVI Semina?rio Inte-
grado de Software e Hardware, pages 387?401.
Gregory Grefenstette. 1998. Producing intelligent
telegraphic text reduction to provide an audio scan-
ning service for the blind. In Working notes of the
28
AAAI Spring Symposium on Intelligent Text summa-
rization, pages 111?118.
Norman Edward Gronlund. 1982. Constructing
achievement tests. Prentice Hall.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplifica-
tion for reading assistance: a project note. In Pro-
ceedings of the second international workshop on
Paraphrasing-Volume 16, pages 9?16. Association
for Computational Linguistics.
Beata Beigman Klebanov, Kevin Knight, and Daniel
Marcu. 2004. Text simplification for information-
seeking applications. In On the Move to Mean-
ingful Internet Systems 2004: CoopIS, DOA, and
ODBASE, pages 735?747. Springer.
Irving Lorge. 1948. The lorge and flesch readabil-
ity formulae: A correction. School and Society,
67:141?142.
Aure?lien Max. 2006. Writing for language-impaired
readers. In Computational Linguistics and Intelli-
gent Text Processing, pages 567?570. Springer.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Sarah E. Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis.
In In Proc. of Workshop on Speech and Language
Technology for Education.
Luz Rello, Ricardo Baeza-Yates, Stefan Bott, and Ho-
racio Saggion. 2013. Simplify or help? text sim-
plification strategies for people with dyslexia. Proc.
W4A, 13.
J. Richard Ruffino. 1982. Coping with machine trans-
lation. Practical Experience of Machine Transla-
tion.
Advaith Siddharthan and Napoleon Katsos. 2012. Of-
fline sentence processing measures for testing read-
ability with users. In Proceedings of the First Work-
shop on Predicting and Improving Text Readability
for target reader populations, pages 17?24. Associ-
ation for Computational Linguistics.
Advaith Siddharthan. 2003. Syntactic simplification
and text cohesion. Ph.D. thesis, University of Cam-
bridge, UK.
Matthew Snover, Nitin Madnani, Bonnie J Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
hter?: exploring different human judgments with a
tunable mt metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268. Association for Computational Linguis-
tics.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexi-
cal simplification. In Proceedings of the First Joint
Conference on Lexical and Computational Seman-
tics, pages 347?355. Association for Computational
Linguistics.
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Computational Processing of
the Portuguese Language, pages 30?39. Springer.
A. A. Streiff. 1985. New developments in titus 4.
Lawson (1985), 185:192.
Irina Temnikova. 2012. Text Complexity and Text Sim-
plification in the Crisis Management domain. Ph.D.
thesis, Wolverhampton, UK.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing &
Management, 43(6):1606?1618.
David Vickrey and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-2008: HLT), pages 344?
352.
Sandra Williams and Ehud Reiter. 2008. Generating
basic skills reports for low-skilled readers. Natural
Language Engineering, 14(4):495?525.
Sandra Williams, Advaith Siddharthan, and Ani
Nenkova, editors. 2012. Proceedings of the First
Workshop on Predicting and Improving Text Read-
ability for target reader populations. Association
for Computational Linguistics, Montre?al, Canada,
June.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 409?420. Association
for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365?368. Association for
Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd international conference on computational lin-
guistics, pages 1353?1361. Association for Compu-
tational Linguistics.
29
