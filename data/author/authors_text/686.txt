Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215?223,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Probabilistic Inference for Machine Translation
Phil Blunsom and Miles Osborne
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
{pblunsom,miles}@inf.ed.ac.uk
Abstract
We advance the state-of-the-art for discrimi-
natively trained machine translation systems
by presenting novel probabilistic inference
and search methods for synchronous gram-
mars. By approximating the intractable space
of all candidate translations produced by inter-
secting an ngram language model with a
synchronous grammar, we are able to train
and decode models incorporating millions of
sparse, heterogeneous features. Further, we
demonstrate the power of the discriminative
training paradigm by extracting structured
syntactic features, and achieving increases in
translation performance.
1 Introduction
The goal of creating statistical machine translation
(SMT) systems incorporating rich, sparse, features
over syntax and morphology has consumed much
recent research attention. Discriminative approaches
are widely seen as a promising technique, poten-
tially allowing us to further the state-of-the-art.
Most work on discriminative training for SMT has
focussed on linear models, often with margin based
algorithms (Liang et al, 2006; Watanabe et al,
2006), or rescaling a product of sub-models (Och,
2003; Ittycheriah and Roukos, 2007).
Recent work by Blunsom et al (2008) has shown
how translation can be framed as a probabilistic
log-linear model, where the distribution over trans-
lations is modelled in terms of a latent variable
on derivations. Their approach was globally opti-
mised and discriminative trained. However, a lan-
guage model, an information source known to be
crucial for obtaining good performance in SMT, was
notably omitted. This was because adding a lan-
guage model would mean that the normalising parti-
tion function could no longer be exactly calculated,
thereby preventing efficient parameter estimation.
Here, we show how language models can be
incorporated into large-scale discriminative transla-
tion models, without losing the probabilistic inter-
pretation of the model. The key insight is that we
can use Monte-Carlo methods to approximate the
partition function, thereby allowing us to tackle the
extra computational burden associated with adding
the language model. This approach is theoreti-
cally justified and means that the model contin-
ues to be both probabilistic and globally optimised.
As expected, using a language model dramatically
increases translation performance.
Our second major contribution is an exploita-
tion of syntactic features. By encoding source syn-
tax as features allows the model to use, or ignore,
this information as it sees fit, thereby avoiding the
problems of coverage and sparsity associated with
directly incorporating the syntax into the grammar
(Huang et al, 2006; Mi et al, 2008). We report on
translation gains using this approach.
We begin by introducing the synchronous gram-
mar approach to SMT in Section 2. In Section
3 we define the parametric form of our model
and describe techniques for approximating the
intractable space of all translations for a given
source sentence. In Section 4 we evaluate the abil-
ity of our model to effectively estimate the highly
dependent weights for the sparse features and real-
valued language model. In addition we describe how
215
X?
 ? ??? , Brown?
X
?
 ? ?? X
? 
?? X
? 
, arrived in X
?
 from X
?
?
X
?
 ? ??? , Shanghai?
X
?
 ? ??? , Beijing?
X
?
 ? ?X
? 
? ?? ?? X
? 
, X
? 
X
? 
late last night?
S ? ?X
? 
?
 
? , X
?
 .?
Figure 1. An example SCFG derivation from a Chi-
nese source sentence which yields the English sentence:
?Brown arrived in Shanghai from Beijing late last night.?
our model can easily integrate rich features over
source syntax trees and compare our training meth-
ods to a state-of-the-art benchmark.
2 Synchronous context free grammar
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) describes the gener-
ation of pairs of strings. A string pair is generated
by applying a series of paired context-free rewrite
rules of the form,X ? ??, ?,??, whereX is a non-
terminal, ? and ? are strings of terminals and non-
terminals and ? specifies a one-to-one alignment
between non-terminals in ? and ?. In the context of
SMT, by assigning the source and target languages
to the respective sides of a SCFG it is possible to
describe translation as the process of parsing the
source sentence, while generating the target trans-
lation (Chiang, 2007).
In this paper we only consider grammars
extracted using the heuristics described for the Hiero
SMT system (Chiang, 2007). Note however that our
approach is general and could be used with other
synchronous grammar transducers (e.g., (Galley et
al., 2006)). SCFG productions can specify that the
order of the child non-terminals is the same in both
languages (a monotone production), or is reversed (a
reordering production). Without loss of generality,
here we add the restriction that non-terminals on the
source and target sides of the grammar must have the
same category. Figure 1 shows an example deriva-
tion for Chinese to English translation.
3 Model
We start by defining a log-linear model for the con-
ditional probability distribution over target transla-
tions of a given source sentence. A sequence of
SCFG rule applications which produce a translation
from a source sentence is referred to as a derivation,
and each translation may be produced by many dif-
ferent derivations. As the training data only provides
source and target sentences, the derivations are mod-
elled as a latent variable.
The conditional probability of a derivation, d, for
a target translation, e, conditioned on the source, f ,
is given by:
p?(d, e|f) =
exp
?
k ?kHk(d, e, f)
Z?(f)
(1)
where Hk(d, e, f) =
?
r?d
hk(f , r, q(r,d)) (2)
Using Equation (1), the conditional probability of
a target translation given the source is the sum over
all of its derivations:
p?(e|f) =
?
d??(e,f)
p?(d, e|f)
where ?(e, f) is the set of all derivations of the
target sentence e from the source f.
Here k ranges over the model?s features, and
? = {?k} are the model parameters (weights for
their corresponding features). The function q(r,d)
returns the target ngram context, for a language
model with order m, of rule r in derivation d.
For a rule which spans the target words (i, j) and
target yield(d) = {t0, ? ? ? , tl}:
q(r,d) =
{
ti???ti+m?2?tj?m+2???tj if j ? i > m
ti???tj otherwise
The feature functions hk are real-valued functions
over the source and target sentences, and can include
overlapping and non-independent features of the
data. The features must decompose with the deriva-
tion and the ngram context defined by the function q,
as shown in Equation (2). The features can reference
the entire source sentence coupled with each rule, r,
and its target context, in a derivation.
By directly incorporating the language model
context q into the model formulation, we will not
216
be able to exactly compute the partition function
Z?(f), which sums over all possible derivations.
Even though a dynamic program over this space
would still run in polynomial time, as shown by Chi-
ang (2007), a packed chart representation of the par-
tition function for the binary Hiero grammars used
in this work would require O(n3|T |4(m?1)) space,1
which is far too large to be practical.
Instead we approximate the partition function
using a sum over a large subset of the possible
derivations (?(e, f)):
Z?(f) ?
?
e
?
d?{??(e,f)}
exp
?
k
?kHk(d, e, f)
= Z??(f)
This model formulation raises the questions of
what an appropriate large subset of derivations for
training is, and how to efficiently calculate the sum
over all derivations in decoding. In the following
sections we elucidate and evaluate our solutions to
these problems.
3.1 Sampling Derivations
The training and decoding algorithms presented in
the following sections rely upon Monte-Carlo tech-
niques, which in turn require the ability to draw
derivation samples from the probability distribution
defined by our log-linear model. Here we adapt
previously presented algorithms for sampling from
a PCFG (Goodman, 1998) for use with our syn-
chronous grammar model. Algorithm 1 describes the
algorithm for sampling derivations. The sampling
algorithm assumes the pre-existance of a packed
chart representation of all derivations for a given
source sentence. The inside algorithm is then used
to calculate the scores needed to define a multino-
mial distribution over all partial derivations associ-
ated with expanding a given child rule. These ini-
tial steps are performed once and then an unlim-
ited number of samples can be drawn by calling the
recursive SAMPLE procedure. MULTI draws a sample
from the distribution over rules for a given chart cell,
CHILDREN enumerates the chart cells connected to
a rule as variables, and DERIVATION is a recursive
tree data structure for derivations. The algorithm is
1where |T | is the size of the terminal alphabet, i.e. the num-
ber of unique English words.
Algorithm 1 Top-down recursive derivation sam-
pling algorithm.
1: procedure SAMPLE(X, i, k)
2: rule? MULTI(inside chart(X, i, k))
3: c = ?
4: for (child category, x, y) ? CHILDREN(rule)
do
5: c? c ? SAMPLE(child category, x, y)
6: end for
7: return DERIVATION(X, children)
8: end procedure
first called on a category and chart cell spanning the
entire chart, and then proceeds top down by using
the function MULTI to draw the next rule to expand
from the distribution defined by the inside scores.
3.2 Approximate Inference
Approximating the partition function with Z??(f)
could introduce biases into inference and in the fol-
lowing discussion we describe measures taken to
minimise the effects of the approximation bias.
An obvious approach to approximating the parti-
tion function, and the feature expectations required
for calculating the derivative in training, is to use
the packed chart of derivations produced by running
the cube pruning beam search algorithm of Chiang
(2007) on the source sentence. In this case Z??(f)
includes all the derivations that fall within the cube
pruning beam, hopefully representing the majority
of the probability mass. We denote the partition
function estimated with this cube beam approxima-
tion as Z?cb? (f). This approach has the advantage of
using the same beam search dynamic program dur-
ing training as is used for decoding. As the approxi-
mated partition function does not contain all deriva-
tions, it is possible that some, or all, of the deriva-
tions of the reference translation from the parallel
corpus may be excluded. We must therefore intersect
the packed chart built from the cube beam with that
of the reference derivations to ensure consistency.
Although, as would be done using cube-pruning,
it would seem intuitively sensible to approximate
the partition function using only high probability
derivations, it is possible that doing so will bias
our model in odd ways. The space of derivations
contained within the beam will be tightly clustered
about a maximum, and thus a model trained with
such an approximation will only see a very small
217
Alles / 
Everything
und / 
and
jedes / 
anything
ist / 
is
vorstellbar / 
possible
X
X
X
S
Alles / 
Everything
und / 
and
jedes / 
everyone
ist / 
is
vorstellbar / 
conceivable
X
X
X
S
X
[1,2]
 
Everything
X
[3,4]
 
anything
X
[1,4]
 
Everything 
and
 s
X
[4,5]
 
is
X
[5,6]
 
possible
X
[1,5]
 
Everything * 
is
S
[1,6]
 
Everything * 
possible
X
[5,6]
 
conceivable
X
[2,3]
and
X
[1,3]
 
Everything * 
anything
X
[1,3]
 
Everything * 
everyone
X
[3,4]
 
everyone
S
[1,6]
 
Everything * 
conceivable
Alles / 
Everything
und / 
and
jedes / 
anything
ist / 
is
vorstellbar / 
conceivable
X
X
X
S
(a)
(c)
(b)
2
1
3
4
5
6
2
1
3
4
5
6
2
1
3
4
5
6
Figure 2. A German-English translation example of building Z?sam? (f) from samples. (a) Two sample derivations are
drawn from the model, (b) these samples are then combined into a packed representation, here represented by a
hypergraph with target translations elided for a bigram language model. The derivation in (c) is contained within the
hypergraph even though it was never explicitly inserted.
part of the overall distribution, possibly leading it
astray. Consider the example of a language model
feature: as this is a very strong indicator of transla-
tion quality, we would expect all derivations within
the beam to have a similar (high) language model
score, thereby robbing this feature of its discriminat-
ing power. However if our model could also see the
low probability derivations it would be clear that this
feature is indeed very strongly correlated with good
translations. Thus a good approximation of the space
of derivations is one that includes both good and bad
examples, not just a cluster around the maximum.
A principled solution to the problem of approx-
imating the partition function would be to use a
Markov Chain Monte Carlo (MCMC) sampler to
estimate the sum with a large number of samples.
Most of the sampled derivations would be in the
high probability region of the distribution, however
there would also be a number of samples drawn
from the rest of the space, giving the model a more
global view of the distribution, avoiding the pit-
falls of the narrow view obtained by a beam search.
Although effective, the computational cost of such
an approach is prohibitive as we would need to draw
hundreds of thousands of samples to obtain conver-
gence, for every training iteration.
Here we mediate between the computational
advantages of a beam and the broad view of the dis-
tribution provided by sampling. Using the algorithm
outlined in Section 3.1 we draw samples from the
distribution of derivations and then insert these sam-
ples into a packed chart representation. This process
is illustrated in Figure 2. The packed chart created
by intersecting the sample derivations represents a
space of derivations much greater than the original
samples. In Figure 2 the chart is built from the first
two sampled derivations, while the third derivation
can be extracted from the chart even though it was
never explicitly entered. This approximation of the
partition function (denoted Z?sam? (f)) allows us to
build an efficient packed chart representation of a
large number of derivations, centred on those with
high probability while still including a significant
representation of the low probability space. Deriva-
tions corresponding to the reference can be detected
during sampling and thus we can build the chart
for the reference derivations at the same time as
the one approximating the partition function. This
could lead to some, or none of, the possible ref-
erence derivations being included, as they may not
have been sampled. Although we could intersect all
of the reference derivations with the sampled chart,
this could distort the distribution over derivations,
218
and we believe it to be advantageous to keep the
distributions between the partition function and ref-
erence charts consistent.
Both of the approximations proposed above,
Z?cb? (f) and Z?
sam
? (f), rely on the pre-existence of a
trained translation model in order to either guide the
cube-pruning beam, or define the probability distri-
bution from which we draw samples. We solve this
chicken and egg problem by first training an exact
translation model without a language model, and
then use this model to create the partition function
approximations for training with a language model.
We denote the distribution without a language model
as p?LM? (e|f) and that with as p
+LM
? (e|f).
A final training problem that we need to address
is the appropriate initialisation of the model param-
eters. In theory we could simply randomly initialise
? for p+LM? (e|f), however in practice we found that
this resulted in poor performance on the develop-
ment data. This is due to the complex non-convex
optimisation function, and the fact that many fea-
tures will fall outside the approximated charts result-
ing in random, or zero, weights in testing. We intro-
duce a novel solution in which we use the Gaus-
sian prior over model weights to tie the exact model
trained without a language model, which assigns
sensible values to all rule features, with the approx-
imated model. The prior over model parameters for
p+LM? (e|f) is defined as:
p+LM(?k) ? e
?
??k??
?LM
k ?
2
2?2
Here we have set the mean parameters of the Gaus-
sian distribution for the approximated model to
those learnt for the exact one. This has the effect
that any features that fall outside the approximated
model will simply retain the weight assigned by the
exact model. While for other feature weights the
prior will penalise substantial deviations away from
??LM , essentially encoding the intuition that the
rule rule parameters should not change substantially
with the inclusion of language model features.
This results in the following log-likelihood objec-
tive and corresponding gradient:
L =
?
(ei,fi)?D
log p+LM? (ei|fi) +
?
k
log p+LM0 (?k)
?L
??k
= Ep+LM? (d|ei,fi)
[hk]? Ep+LM? (e|fi)
[hk]
?
?+LMk ? ?
?LM
k
?2
3.3 Decoding
As stated in Equation 3 the probability of a given
translation string is calculated as the sum of the
probabilities of all the derivations that yield that
string. In decoding, where the reference translation
is not known, the exact calculation of this summa-
tion is NP-Hard. This problem also arises in mono-
lingual parsing with probabilistic tree substitution
grammars and has been tackled in the literature
using Monte-Carlo sampling methods (Chappelier
and Rajman, 2000). Their approach is directly appli-
cable to our SCFG decoding problem and we can use
Algorithm 1 to draw sample translation derivations
for the source sentence. The probability of a trans-
lation can be calculated simply from the number of
times a derivation that yields it was sampled, divided
by the total number of samples. For the p?LM? (e|f)
model we can build the full chart of all possible
derivations and thus sample from the true distribu-
tion over derivations. For the p+LM? (e|f) model we
suffer the same problem as in training and cannot
build the full chart. Instead a chart is built using
the cube-pruning algorithm with a wide beam and
we then draw samples from this chart. Although
sampling from a reduced chart will result in biased
samples, in Section 4 we show this approach to be
effective in practice.2 In Section 4 we compare our
sampling approach to the heuristic beam search pro-
posed by Blunsom et al (2008).
It is of interest to compare our proposed decoding
algorithms to minimum Bayes risk (MBR) decoding
(Kumar and Byrne, 2004), a commonly used decod-
ing method. From a theoretical standpoint, the sum-
ming of derivations for a given translation is exactly
2We have experimented with using a Metropolis Hastings
sampler, with p?LM? (e|f) as the proposal distribution, to sam-
ple from the true distribution with the language model. Unfor-
tunately the sample rejection rate was very high such that this
method proved infeasibly slow.
219
equivalent to performing MBR with a 0/1 loss func-
tion over derivations. From a practical perspective,
MBR is normally performed with BLEU as the loss
and approximated using n-best lists. These n-best
lists are produced using algorithms tuned to remove
multiple derivations of the same translation (which
have previously been seen as undesirable). However,
it would be simple to extend our sampling based
decoding algorithm to calculate the MBR estimate
using BLEU , in theory providing a lower variance
estimate than attained with n-best lists.
4 Evaluation
We evaluate our model on the IWSLT 2005 Chinese
to English translation task (Eck and Hori, 2005),
using the 2004 test set as development data for
tuning the hyperparameters and MERT training the
benchmark systems. The statistics for this data are
presented in Table 1.3 The training data made avail-
able for this task consisted of 40k pairs of tran-
scribed utterances, drawn from the travel domain.
The development and test data for this task are some-
what unusual in that each sentence has a single
human translated reference, and fifteen paraphrases
of this reference, provided by monolingual anno-
tators. Model performance is evaluated using the
standard BLEU metric (Papineni et al, 2002) which
measures average n-gram precision, n ? 4, and we
use the NIST definition of the brevity penalty for
multiple reference test sets. We provide evaluation
against both the entire multi-reference sets, and the
single human translation.
Our translation grammar is induced using the
standard alignment and rule extraction heuristics
used in hierarchical translation models (Chiang,
2007).4 As these heuristics aren?t based on a genera-
tive model, and don?t guarantee that the target trans-
lation will be reachable from the source, we discard
those sentence pairs for which we cannot produce a
derivation, leaving 38,405 sentences for training.
Our base model contains a single feature for each
rule which counts the number of times it appeared in
a particular derivation. For models which include a
3Development and test set statistics are for the single human
translation reference.
4With the exception that we allow unaligned words at the
boundary of rules. This improves training set coverage.
language model, we train a standard Kneser-Ney tri-
gram model on the target side of the training corpus.
We also include a word penalty feature to compen-
sate for the shortening effect of the language model.
In total our model contains 2.9M features.
The aims of our evaluation are: (1) to deter-
mine that our proposed training regimes are able to
realise performance increase when training sparse
rule features and a real valued language model fea-
ture together, (2) that the model is able to effectively
use rich features over the source sentence, and (3)
to confirm that our model obtains performance com-
petitive with the current state-of-the-art.
4.1 Inference and Decoding
We have described a number of modelling choices
which aim to compensate for the training biases
introduced by incorporating a language model fea-
ture through approximate inference. Our a priori
knowledge from other SMT systems suggests that
incorporating a language model should lead to large
increases in BLEU score. In this evaluation we aim
to determine whether our training regimes are able
to realises these expected gains.
Table 2 shows a matrix of development BLEU
scores achieved by varying the approximation of the
partition function in training, and varying the decod-
ing algorithm. If we consider the vertical axis we
can see that the sampling method for approximat-
ing the partition function has a small but consistent
advantage over using the cube-pruning beam. The
charts produced by the sampling approach occupy
roughly half the disc space as those produced by
the beam search, so in subsequent experiments we
present results using the Z?sam? (f) approximation.
Comparing the decoding algorithms on the hori-
zontal axis we can reconfirm the findings of Blun-
som et al (2008) that the max-translation decod-
ing outperforms the Viterbi max-derivation approx-
imation. It is also of note that this BLEU increase
is robust to the introduction of the language model
feature, assuaging fears that the max-translation
approach may have been doing the job of the lan-
guage model. We also compare using Monte-Carlo
sampling for decoding with the previously pro-
posed heuristic beam search algorithm. The differ-
ence between the two algorithms is small, however
220
Training Development Test
Chinese English Chinese English Chinese English
Utterances 38405 500 506
Segments/Words 317621 353116 3464 3752 3784 3823
Av. Utterances Length 8 9 6 7 7 7
Longest Utterance 55 68 58 62 61 56
Table 1. IWSLT Chinese to English translation corpus statistics.
Model Max-derivation Max-translation(Beam) Max-translation(Sampling)
p?LM? (e|f) 31.0 32.5 32.6
p+LM? (e|f) (Z?
cb
? (f)) 39.1 39.8 39.8
p+LM? (e|f) (Z?
sam
? (f)) 39.9 40.5 40.6
Table 2. Development set results for varying the approximation of the partition function in training, Z?cb? (f) vs. Z?sam? (f),
and decoding using the Viterbi max-derivation algorithm, or the max-translation algorithm with either a beam approxi-
mation or Monte-Carlo sampling.
we feeling the sampling approach is more theoreti-
cally justified and adopt it for our later experiments.
The most important result from this evaluation
is that both our training regimes realise substantial
gains from the introduction of the language model
feature. Thus we can be confident that our model
is capable of modelling the distribution over trans-
lations even when the space over all derivations is
intractable to dynamically program exactly.
4.2 A Discriminative Syntactic Translation
Model
In the previous sections we?ve described and evalu-
ated a statistical model of translation that is able to
estimate a probability distribution over translations
using millions of sparse features. A prime motiva-
tion for such a model is the ability to define com-
plex features over more than just the surface forms
of the source and target strings. There are limit-
less options for such features, and previous work
has focused on defining token based features such
as part-of-speech and morphology (Ittycheriah and
Roukos, 2007). Although such features are applica-
ble to our model, here we attempt to test the model?s
ability to incorporate complex features over source-
side syntax trees, essentially subsuming and extend-
ing previous work on tree-to-string translation mod-
els (Huang et al, 2006; Mi et al, 2008).
We first parse the source side of our training,
development and test corpora using the Stanford
parser.5 Next, while building the synchronous charts
5http://nlp.stanford.edu/software/lex-parser.shtml
required for training, whenever a rule is used in a
derivation a feature is activated which captures: (1)
the constituent spanning the rule?s source side in the
syntax tree (if any) (2) constituents spanning any
variables in the rule, and (3) the rule?s target side
surface form. Figure 3 illustrates this process.
These syntactic features are equivalent to the
grammar rules extracted for tree-to-string translation
systems. The key difference in our model is that the
source syntax tree is treated as conditioning context
and it?s information encoded as features, thus this
information can be used or ignored as the model sees
fit. This avoids the problems associated with explic-
itly encoding the source syntax in the grammar, such
as sparsity and overly constraining the model. In
addition we could easily incorporate features over
multiple source trees, for example mixing labelled
syntax trees with dependency graphs.
We limit the extraction of syntactic features to
those that appear in at least two training derivations,
giving a total of 4.2M syntactic features, for an over-
all total of 7.1M features.
4.3 Discussion
Table 3 shows the results from applying our
described models to the test set. We benchmark our
results against a model (Hiero) which was directly
trained to optimise BLEUNIST using the standard
MERT algorithm (Och, 2003) and the full set of
translation and lexical weight features described
for the Hiero model (Chiang, 2007). As well as
221
Model BLEUNIST BLEUIBM BLEUHumanRef
p?LM? (e|f) 33.5 35.2 25.2
p+LM? (e|f) 44.6 44.6 31.2
p+LM? (e|f) + syntax 45.3 45.2 31.8
MERT (BLEUNIST ) 46.2 44.5 30.2
Table 3. Test set results.
?? 
???
? ?? ?
V WH ?NN
NP
VP
SQ
Where is the currency exchange office ?
(Step 2) X
2
 -> < [X
1
] ? ?? ?, Where is the [X
1
] ?>
(Step 1) X
1
 -> <?? ???, currency exchange office>
NP
SQ
? ?? ?
Where is the [X
1
] ? Example Syntax feature =
for Step 2
Example Derivation:
X
1
(a) (b)
(c)
Figure 3. Syntax feature example: For the parsed source and candidate translation (a), with the derivation (b), we
extract the syntax feature in (c) by combining the grammar rule with the source syntax of the constituents contained
within that rule.
Source ???????????????????
p?LM? (e|f) don ?t have enough bag on me change please go purchase a new by plane .
p+LM? (e|f) i have enough money to buy a new one by air .
p+LM? (e|f) + syntax i don ?t have enough money to buy a new airline ticket .
MERT (BLEUNIST ) i don ?t have enough money to buy a new ticket .
Reference i do n?t have enough money with me to buy a new airplane ticket .
Table 4. Example test set output produced when: not using a language model, using a language model, also using
syntax, output optimised using MERT and finally the reference
BLEUNIST (brevity penalty uses the shortest ref-
erence), we also include results from the IBM
(BLEUIBM ) metric (brevity penalty uses the closest
reference), and using only the actual human transla-
tion in the test set, not the monolingual paraphrase
multiple references (BLEUHumanRef ).
The first result of interest is that we see an
increase in performance through the incorporation
of the source syntax features. This is an encourag-
ing result as the transcribed speech source sentences
are well out of the domain of the data on which the
parser was trained, suggesting that our model is able
to sift the good information from the noisy in the
unreliable source syntax trees. Table 4 shows illus-
trative system output on the test set.
On the BLEUNIST metric we see that our mod-
els under-perform the MERT trained system. We
hypothesise that this is predominately due to the
interaction of the brevity penalty with the unusual
nature of the multiple paraphrase reference train-
ing and development data. Their performance is
however quite consistent across the different inter-
pretations of the brevity penalty (NIST vs. IBM).
This contrasts with the MERT trained model, which
clearly over-fits to the NIST metric that it was
trained on and underperforms our models when eval-
uated on the single human test translations. If we
directly compare the brevity penalties of the MERT
model (0.868) and our discriminative model incor-
porating source syntax (0.942), on the these single
222
references, we can see that the MERT training has
optimised to the shortest paraphrase reference.
From these results it is difficult to draw any hard
conclusions on the relative performance of the dif-
ferent training regimes. However we feel confident
in claiming that we have achieved our goal of train-
ing a probabilistic model on millions of sparse fea-
tures which obtains performance competitive with
the current state-of-the-art training algorithm.
5 Conclusion
In this paper we have shown that statistical machine
translation can be effectively modelled as a well
posed machine learning task. In doing so we have
described a model capable of estimating a probabil-
ity distribution over translations using sparse com-
plex features, and achieving performance compara-
ble to the state-of-the-art on standard metrics. With
further work on scaling these models to large data
sets, and engineering high performance features, we
believe this research has the potential to provide sig-
nificant increases in translation quality.
Acknowledgements
The authors acknowledge the support of the EPSRC
grant EP/D074959/1.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of the 46th Annual Con-
ference of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08:HLT),
pages 200?208, Columbus, Ohio, June.
Jean-Ce?dric Chappelier and Martin Rajman. 2000.
Monte-carlo sampling for np-hard maximization prob-
lems in the framework of weighted parsing. In NLP
?00: Proceedings of the Second International Confer-
ence on Natural Language Processing, pages 106?
117, London, UK.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proc. of the
International Workshop on Spoken Language Trans-
lation, Pittsburgh, October.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguis-
tics (COLING/ACL-2006), pages 961?968, Sydney,
Australia, July.
Joshua T. Goodman. 1998. Parsing inside-out. Ph.D.
thesis, Cambridge, MA, USA. Adviser-Stuart Shieber.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In In Proceedings of the 7th Bien-
nial Conference of the Association for Machine Trans-
lation in the Americas (AMTA), Boston, MA.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proc. of the 7th International
Conference on Human Language Technology Research
and 8th Annual Meeting of the NAACL (HLT-NAACL
2007), pages 57?64, Rochester, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. of the 4th International Conference on Human
Language Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 169?
176.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465?488.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of the
44th Annual Meeting of the ACL and 21st Inter-
national Conference on Computational Linguistics
(COLING/ACL-2006), pages 761?768, Sydney, Aus-
tralia, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of the 46th Annual Confer-
ence of the Association for Computational Linguistics:
Human Language Technologies (ACL-08:HLT), pages
192?199, Columbus, Ohio, June.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the ACL and 3rd Annual Meeting of
the NAACL (ACL-2002), pages 311?318, Philadelphia,
Pennsylvania.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of the 44th Annual
Meeting of the ACL and 21st International Conference
on Computational Linguistics (COLING/ACL-2006),
pages 777?784, Sydney, Australia.
223
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352?361,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Bayesian Model of Syntax-Directed Tree to String Grammar Induction
Trevor Cohn and Phil Blunsom
School of Informatics
University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
{tcohn,pblunsom}@inf.ed.ac.uk
Abstract
Tree based translation models are a com-
pelling means of integrating linguistic in-
formation into machine translation. Syn-
tax can inform lexical selection and re-
ordering choices and thereby improve
translation quality. Research to date has
focussed primarily on decoding with such
models, but less on the difficult problem of
inducing the bilingual grammar from data.
We propose a generative Bayesian model
of tree-to-string translation which induces
grammars that are both smaller and pro-
duce better translations than the previous
heuristic two-stage approach which em-
ploys a separate word alignment step.
1 Introduction
Many recent advances in statistical machine trans-
lation (SMT) are a result of the incorporation of
syntactic knowledge into the translation process
(Marcu et al, 2006; Zollmann and Venugopal,
2006). This has been facilitated by the use of syn-
chronous grammars to model translation as a gen-
erative process over pairs of strings in two lan-
guages. Such models are particularly attractive
for translating between languages with divergent
word orders, such as Chinese and English, where
syntax-inspired translation rules can succinctly de-
scribe the requisite reordering operations. In con-
trast, standard phrase-based models (Koehn et al,
2003) assume a mostly monotone mapping be-
tween source and target, and therefore cannot
adequately model these phenomena. Currently
the most successful paradigm for the use of syn-
chronous grammars in translation is that of string-
to-tree transduction (Galley et al, 2004; Zollmann
and Venugopal, 2006; Galley et al, 2006; Marcu
et al, 2006). In this case a grammar is extracted
from a parallel corpus, with strings on its source
side and syntax trees on its target side, which is
then used to translate novel sentences by perform-
ing inference over the space of target syntax trees
licensed by the grammar.
To date grammar-based translation models have
relied on heuristics to extract a grammar from a
word-aligned parallel corpus. These heuristics are
extensions of those developed for phrase-based
models (Koehn et al, 2003), and involve sym-
metrising two directional word alignments fol-
lowed by a projection step which uses the align-
ments to find a mapping between source words and
nodes in the target parse trees (Galley et al, 2004).
However, such approaches leave much to be de-
sired. Word-alignments rarely factorise cleanly
with parse trees (i.e., alignment points cross con-
stituent structures), resulting in large and implau-
sible translation rules which generalise poorly to
unseen data (Fossum et al, 2008). The principal
reason for employing a grammar based formal-
ism is to induce rules which capture long-range
reorderings between source and target. However
if the grammar itself is extracted using word-
alignments induced with models that are unable
to capture such reorderings, it is unlikely that the
grammar will live up to expectations.
In this work we draw on recent advances in
Bayesian modelling of grammar induction (John-
son et al, 2007; Cohn et al, 2009) to propose a
non-parametric model of synchronous tree substi-
tution grammar (STSG), continuing a recent trend
in SMT to seek principled probabilistic formula-
tions for heuristic translation models (Zhang et al,
2008; DeNero et al, 2008; Blunsom et al, 2009b;
Blunsom et al, 2009a). This model leverages a
hierarchical Bayesian prior to induce a compact
translation grammar directly from a parsed paral-
lel corpus, unconstrained by word-alignments. We
show that the induced grammars are more plausi-
ble and improve translation output.
This paper is structured as follows: In Section
352
2 we introduce the STSG formalism and describe
current heuristic approaches to grammar induc-
tion. We define a principled Bayesian model of
string-to-tree translation in Section 3, and describe
an inference technique using Gibbs sampling in
Section 4. In Section 5 we analyse an induced
grammar on a corpus of Chinese?English trans-
lation, comparing them with a heuristic grammar
in terms of grammar size and translation quality.
2 Background
Current tree-to-string translation models are a
form of Synchronous Tree Substitution Grammar
(STSG; Eisner (2003)). Formally, a STSG is a
5-tuple, G = (T, T
?
, N, S,R), where T and T
?
are sets of terminal symbols in the target and
source languages respectively, N is a set of non-
terminal symbols, S ? N is the distinguished
root non-terminal and R is a set of productions
(a.k.a. rules). Each production is a tuple compris-
ing an elementary tree and a string, the former
referring to a tree fragment of depth ? 1 where
each internal node is labelled with a non-terminal
and each leaf is labelled with either a terminal or
a non-terminal. The string part of the rule de-
scribes the lexical component of the rule in the
source language and includes a special variable for
each frontier non-terminal in the elementary tree.
These variables describe the reordering and form
the recursion sites in the generative process of cre-
ating tree and string pairs with the grammar. For
example, the rule
?(NP NP 1 (PP (IN of) NP 2 )), 2 ? 1 ? (1)
rewrites a noun-phrase (NP) as a NP and prepo-
sitional phrase (PP) headed by ?of? in the target
language. The rule generates the token ??? in
the source and reverses the order of the two child
noun-phrases, indicated by the numbering of the
variables in the string part.
A derivation creates a (tree, string) pair by start-
ing with the root non-terminal and an empty string,
then choosing a rule to rewrite (substitute) the non-
terminal and expand the string. This process re-
peats by rewriting all frontier non-terminals until
there are none remaining. A Probabilistic STSG
assigns a probability to each rule in the grammar.
The probability of a derivation is the product of
the probabilities of its component rules, and the
probability of a (tree, string) pair is the sum of the
probabilities over all its derivations.
2.1 Heuristic Grammar Induction
Grammar based SMT models almost exclusively
follow the same two-stage approach to gram-
mar induction developed for phrase-based meth-
ods (Koehn et al, 2003). In this approach they
induce a finite-state grammar with phrase-pairs as
rules by taking a sentence aligned parallel cor-
pus and 1) predicting word alignments before 2)
extracting transduction rules that are ?consistent?
with the word aligned data. Although empiri-
cally effective, this two stage approach is less than
ideal due to the disconnect between the word-
based models used for alignment and the phrase-
based translation model. This is problematic as the
word-based model cannot recognise phrase-based
phenomena. Moreover, it raises the problem of
identifying and weighting the rules from the word
alignment.
The same criticisms levied at the phrase-based
models apply equally to the two-stage technique
used for synchronous grammar induction (Galley
et al, 2004; Zollmann and Venugopal, 2006; Gal-
ley et al, 2006; Marcu et al, 2006). Namely that
the word alignment models typically do not use
any syntax and therefore will not be able to model,
e.g., consistent syntactic reordering effects, or the
impact of the syntactic category on phrase transla-
tions. The identification and estimation of gram-
mar rules from word aligned data is also non-
trivial. Galley et al (2004) describe an algorithm
for inducing a string-to-tree grammar using a par-
allel corpus with syntax trees on target side. Their
method projects the source strings onto nodes of
the target tree using the word alignment, and then
extracts the minimal transduction rules as well as
rules composed of adjacent minimal units. The
production weights are estimated either by heuris-
tic counting (Koehn et al, 2003) or using the EM
algorithm. Both estimation techniques are flawed.
The heuristic method is inconsistent in the limit
(Johnson, 2002) while EM is degenerate, placing
disproportionate probability mass on the largest
rules in order to describe the data with as few a
rules as possible (DeNero et al, 2006). With no
limit on rule size this method will learn a single
rule for every training instance, and therefore will
not generalise to unseen sentences. These prob-
lems can be ameliorated by imposing limits on
rule size or early stopping of EM training, how-
ever neither of these techniques addresses the un-
derlying problems.
353
In contrast, our model is trained in a single step,
i.e., the alignment model is the translation model.
This allows syntax to directly inform the align-
ments. We infer a grammar without resorting to
word alignment constraints or limits on rule size.
The model uses a prior to bias towards a compact
grammar with small rules, thus solving the degen-
eracy problem.
3 Model
Our training data comprises parallel target trees
and source strings and our aim is to induce a STSG
that best describes this data. This is achieved
by inferring a distribution over the derivations for
each training instance, where the set of derivations
collectively specify the grammar. In the follow-
ing, we denote the source trees as t, target strings
s, and derivations r which are sequences of gram-
mar rules, r.
As described in section 2.1, previous methods
for estimating a STSG have suffered from degen-
eracy. A principled way to correct for such degen-
erated behaviour is to use a prior over rules which
biases towards small rules. This matches our intu-
ition: we expect good translation rules to be small,
with few internal nodes, frontier non-terminals
and terminal strings. However, we recognise that
on occasion larger rules will be necessary; we al-
low such rules when there is sufficient support in
the data.
We model the grammar as a set of distributions,
G
c
, over the productions for each non-terminal
symbol, c. We adopt a non-parametric Bayesian
approach by treating eachG
c
as a random variable
with a Dirichlet process (DP) prior,
r|c ? G
c
G
c
|?
c
, P
0
? DP(?
c
, P
0
(?|c)) ,
where P
0
(?|c) (the base distribution) is a distribu-
tion over the infinite space of trees rooted with c,
and ?
c
(the concentration parameter) controls the
model?s tendency towards either reusing existing
rules or creating novel ones as each training in-
stance is encountered (and consequently, the ten-
dency to infer larger or smaller grammars). We
discuss the base distribution in more detail below.
Rather than representing the distribution G
c
ex-
plicitly, we integrate over all possible values ofG
c
.
This leads to the following predictive distribution
for the rule r
i
given the previously observed rules
r
?i
= r
1
. . . r
i?1
,
p(r
i
|r
?i
, c, ?
c
, P
0
) =
n
?i
r
i
+ ?
c
P
0
(r
i
|c)
n
?i
c
+ ?
c
, (2)
where n
?i
r
i
is the number number of times
r
i
has been used to rewrite c in r
?i
, and
n
?i
c
=
?
r,R(r)=c
n
?i
r
is the total count of rewrit-
ing c (hereR(r) is the root non-terminal of r). The
distribution is exchangeable, meaning that all per-
mutations of the input sequence are assigned the
same probability. This allows us to treat any item
as being the last, which is fundamental for efficient
Gibbs sampling. Henceforth we adopt the notation
r
?
and n
?
to refer to the rules and counts for the
whole data set excluding the current rules under
consideration, irrespective of their location in the
corpus.
The base distribution, P
0
, assigns a prior prob-
ability to an infinite number of rules, where each
rule is an (elementary tree, source string) pair de-
noted r = (e,w). While there are a myriad of
possible distributions, we developed a very sim-
ple one. We decompose the probability into two
factors,
P
0
(e,w|c) = P (e|c)P (w|e) , (3)
the probability of the target elementary tree
and the probability of the source string, where
c = R(r).
The tree probability, P (e|c) in (3), is modelled
using generative process whereby the root cate-
gory c is expanded into a sequence of child non-
terminals, then each of these are either expanded
or left as-is. This process continues until there
are no unprocessed children. The number of child
nodes for each expansion is drawn from a geo-
metric prior with parameter p
child
, except in the
case of pre-terminals where the number of chil-
dren is always one. The binary expansion deci-
sions are drawn from a Bernoulli prior with pa-
rameter p
expand
, and non-terminals and terminals
are drawn uniformly from N and T respectively.
For example, the source side of rule (1) was gen-
erated as follows: 1) the NP was rewritten as two
children; 2) an NP; and 4) a PP; 5) the NP child
was not expanded; 6) the PP child was expanded;
7) as an IN; and 8) a NP; 9) the IN was expanded to
the terminal ?of?; and 10) the final NP was not ex-
panded. Each of these steps is a draw from the rel-
evant distribution, and the total probability is the
product of the probabilities for each step.
354
Hong/NNP Kong/NNP
?? ? ? ? ? ?? ? ?? ? ?? ?
S
NP
VP ./.
NP PP
Every/DT corner/NN of/IN NP
is/VBZ VP
filled/VBN PP
with/IN NP
fun/NN
0 1 2 3 4 5 6 7 8 109 11
[2,3)
[2,6)
[3,6) [0,1)
[0,2)
[0,11)
[10,11)[6,10)
[6,10)
[7,9) [9,10)
[9,10)
?
?
? ? ? ?
?
Figure 1: Example derivation. Each node is annotated with their span in the target string (aligned nodes are shaded). The dotted
edges show the implied alignments. Preterminals are displayed with their child terminal in the leaf nodes.
The second factor in (3) is P (w|e), the prob-
ability of the source string (a sequence of source
terminals and variables). We assume that the el-
ementary tree is generated first, and condition the
string probability on l = F (e), its number of fron-
tier nodes (i.e., variables). The string is then cre-
ated by choosing a number of terminals from a ge-
ometric prior with parameter p
term
then drawing
each terminal from a uniform distribution over T
?
.
Finally each of the l variables are inserted into the
string one at a time using a uniform distribution
over the possible placements. For the example rule
in (1) the generative process corresponds to 1) de-
ciding to create one terminal; 2) with value ?; 3)
inserting the first variable after the terminal; and
4) inserting the second variable before the termi-
nal. Again, the probability of the string is simply
the product of the probabilities for each step.
Together both the factors in the base distribu-
tion penalise large trees with many nodes and long
strings with many terminals and variables. P
0
de-
creases exponentially with rule size, thus discour-
aging the model from using larger rules; for this
to occur the rules must significantly increase the
likelihood.
4 Training
To train our model we use Gibbs sampling (Geman
and Geman, 1984), a Markov chain Monte Carlo
method (Gilks et al, 1996) in which variables are
repeatedly sampled conditioned on the values of
all other variables in the model.
1
After a period of
burn-in, each sampler state (set of variable assign-
ments) is a sample from the posterior distribution
of the model. In our case, we wish to sample from
the posterior over the grammar, P (r|t, s, ?).
To simplify matters we associate an alignment
variable, a, with every internal node of the trees
in the training set. This variable specifies the span
of source tokens to which node is aligned. Alter-
natively, the node can be unaligned, which is en-
coded as an empty span. I.e. a ? (J ? J) ? ?
where J is the set of target word indices. Spans
are written [i, j): inclusive of i and exclusive of
j. Each aligned node (a 6= ?) forms the root
of a rule as well as being a frontier non-terminal
of an ancestor rule, while unaligned nodes form
part of an ancestor rule.
2
The set of valid align-
ments are constrained by the tree in a number of
ways. Child nodes can be aligned only to sub-
spans of their ancestor nodes? alignments and no
two nodes? alignments can overlap. Finally, the
root node of the tree must be aligned to the full
1
Previous approaches to bilingual grammar induction
have used variational inference to optimise a bound on the
data log-likelihood (Zhang et al, 2008; Blunsom et al,
2009b). Both these approaches truncated the grammar a pri-
ori in order to permit tractable inference. In contrast our
Gibbs sampler can perform inference over the full space of
grammars. See also Blunsom et al (2009a) where we present
a Gibbs sampler for inducing SCFGs without truncation.
2
The Gibbs sampler is an extension of our sampler for
monolingual tree-substitution grammar (Cohn et al, 2009),
which used a binary substitution variable at each node to en-
code the segmentation of a training tree into elementary trees.
355
?(S (NP NP 1 PP 2 ) VP 3 . 4 ), 2 1 3 4 ?
?(NP DT 1 NN 2 ), 1 2 ?
?(DT Every),??
?(NN corner),?????
?(PP (IN of) NP 1 ), 1 ??
?(NP (NNP Hong) (NNP Kong)),???
?(VP (VBZ is) VP 1 ), 1 ?
?(VP VBN 1 PP 2 ),? 1 2 ?
?(VBN filled),???
?(PP (IN with) (NP NN 1 )), 1 ?
?(NN fun),??
?(. .),??
Table 1: Grammar rules specified by the derivation in Fig-
ure 1. Each rule is shown as a tuple comprising a tar-
get elementary tree and a source string. Boxed numbers
show the alignment between string variables and frontier non-
terminals.
span of source words.
Collectively, the training trees and alignment
variables specify the sequence of rules r, which
in turn specify the grammar. Figure 1 shows an
example derivation with alignment variables. The
corresponding STSG rules are shown in Table 1.
4.1 Gibbs operators
The Gibbs sampler works by sampling new val-
ues of the alignment variables, using two differ-
ent Gibbs operators to make the updates. The first
operator, EXPAND, takes a tree node, v, and sam-
ples a new alignment, a
v
, given the alignments of
all other nodes in the same tree and all other trees
in the corpus, denoted a
?
. The set of valid la-
bels is constrained by the other alignments in the
tree, specifically that of the node?s closest aligned
ancestor, a
p
, its closest aligned descendants, a
d
,
and its aligned siblings, a
s
(the aligned descen-
dants of a). The alignment variable may be empty,
a
v
= ?, while non-empty values must obey the
tree constraints. Specifically the span must be a
subspan of its ancestor, a
v
? a
p
, subsume its de-
scendants, a
v
?
?
a
d
, and not overlap its siblings,
j 6?
?
a
s
,?j ? a
v
. Figure 2 shows an exam-
ple with the range of valid values for corner/NN?s
alignment variable and the corresponding align-
ments that these encode.
Each alignment in the set of valid outcomes de-
fines a set of grammar rules. The non-aligned out-
come results in a single rule r
p
rooted at ancestor
node p. While the various aligned outcomes re-
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[3,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[4,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[5,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[4,5)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[3,4)
6 6
6
6
6 6
?
Figure 2: Possible state updates for the (NN corner) node us-
ing the EXPAND operator.
sult in a pair of rules, r
p
?
and r
v
, rooted at p and v
respectively. In the example in Figure 2, the top-
right outcome has a
v
= ? and
r
p
= ?(NP DT 1 (NN corner)), 1 ????? .
The bottom-right outcome, a
v
= [4, 5), describes
the pair of rules:
r
p
?
= ?(NP DT 1 NN 2 ), 1 ? 2 ??? and
r
v
= ?(NN corner),?? .
The set of valid options are then scored according
to the probability of their rules as follows:
P (r
p
|r
?
) =
n
?
r
p
+ ?P
0
(r
p
|c
p
)
n
?
c
p
+ ?
(4)
P (r
p
?
, r
v
|r
?
) = P (r
p
?
|r
?
)P (r
v
|r
?
, r
p
?
)
=
n
?
r
p
?
+ ?P
0
(r
p
?
|c
p
)
n
?
c
p
+ ?
? (5)
n
?
r
v
+ ?(r
p
?
, r
v
) + ?P
0
(r
v
|c
v
)
n
?
c
v
+ ?(c
p
, c
v
) + ?
where c
p
is the non-terminal at node p (simi-
larly for c
v
), n
?
denote counts of trees (e.g., n
?
r
p
)
or the sum over all trees expanding a non-
terminal (e.g., n
?
c
v
) in the conditioning context,
r
?
, and ?(?, ?) is the Kronecker delta function,
which returns 1 when its arguments are identi-
cal and 0 otherwise. For clarity, we have omit-
ted some items from the conditioning context
356
in (4) and (5), namely t, s and hyper-parameters
?, p
child
, p
expand
, p
term
. The ? terms in the sec-
ond factor of (5) account for the changes to n
?
that would occur after observing r
p
?
, which forms
part of the conditioning context for r
v
. If the rules
r
p
?
and r
v
are identical, then the count n
?
r
v
would
increase by one, and if the rules expand the same
root non-terminal, then n
?
c
v
would increase by one.
Equation (4) is evaluated once for the unaligned
outcome, a
v
= ?, and (5) is evaluated for each
valid alignment. The probabilities are normalised
and an outcome sampled.
The EXPAND operator is sufficient to move
from one derivation to any other valid derivation,
however it may take many steps to do so. These
intermediate steps may require the sampler to pass
through highly improbable regions of the state
space, and consequently such moves are unlikely.
The second operator, SWAP, is designed to help
address this problem by increasing the mobility of
the sampler, allowing it to mix more quickly. The
operator considers pairs of nodes, v, w, in one tree
and attempts to swap their alignment values.
3
This
is illustrated in the example in Figure 3. There are
two options being compared: preserving the align-
ments (left) or swapping them (right). This can
change three rules implied by the derivation: that
rooted at the nodes? common aligned ancestor, p,
and those rooted at v and w. For the example, the
left option implies rules
{r
p
= ?(NP DT 1 NN 2 ), 1 2 ?,
r
v
= ?(DT Every),??,
r
w
= ?(NN corner),?????} ,
and the right option implies rules
{r
p
= ?(NP DT 1 NN 2 ), 2 1 ?,
r
v
= ?(DT Every),?????,
r
w
= ?(NN corner),??} .
We simply evaluate the probability of
both triples of rules under our model,
P (r
p
, r
v
, r
w
|r
?
) = P (r
p
|r
?
)P (r
v
|r
?
, r
p
)
P (r
w
|r
?
, r
p
, r
v
), where the additional rules in
the conditioning context signify their inclusion
in the counts r
?
before applying (2) to evaluate
the probability (much the same as in (5) where
3
We rarely need to consider the full quadratic space of
node pairs, as the validity constraints mean that the only
candidates for swapping are siblings (i.e., share the closest
aligned ancestor) which do not have any aligned descendants.
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[3,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[3,6)
[2,6)
6 6
[2,3)
Figure 3: Possible state updates for the pair of nodes
(DT every) and (NN corner) using the SWAP operator.
English? Chinese
Sentences 300k
Words or Segments 11.0M 8.6M
Avg. Sent. Length 36 28
Longest Sent. 80 80
Table 2: NIST Chinese-English corpora statistics
(LDC2003E14, LDC2005E47).
the ? functions encode the changes to the counts).
An outcome is then sampled according to the
normalised probabilities of the preserve vs. swap
rules.
The Gibbs sampler makes use of both operators.
The algorithm visits each (tree, string) pair in the
training set in random order and applies the EX-
PAND operator to every node in the tree. After the
tree has been processed, the SWAP operator is ap-
plied to all candidate pairs of nodes. Visiting all
sentence pairs in this way constitutes a single sam-
ple from the Gibbs sampler.
5 Experiments
We evaluate our non-parametric model of gram-
mar induction on a subset of the NIST Chinese-
English translation evaluation, representing a real-
istic SMT experiment with millions of words and
long sentences. The Chinese-English training data
consists of the FBIS corpus (LDC2003E14) and
the first 100k sentence pairs from the Sinorama
corpus (LDC2005E47). The Chinese text was seg-
mented with a CRF-based Chinese segmenter op-
timized for MT (Chang et al, 2008), and the En-
glish text was parsed using the Stanford parser
(Klein and Manning, 2003).
As a baseline we implemented the heuristic
grammar extraction technique of Galley et al
(2004) (henceforth GHKM). This method finds
the minimum sized translation rules which are
consistent with a word-aligned sentence pair, as
357
described in section 2.1. The rules are then
treated as events in a relative frequency esti-
mate.
4
We used Giza++ Model 4 to obtain
word alignments (Och and Ney, 2003), using
the grow-diag-final-and heuristic to sym-
metrise the two directional predictions (Koehn et
al., 2003).
The model was sampled for 300 iterations to
?burn-in?, where in each iteration we applied both
sampling operators to all nodes (or node pairs)
of all training instances. We initialised the sam-
pler using the GHKM derivation of the training
data (the baseline system). The final state of the
sampler was used to extract the grammar. The
hyperparameters were set by hand to ? = 10
6
,
p
child
= 0.5, p
expand
= 0.5, and p
term
= 0.5.
5
Overall the model took on average 2,218s per full
iteration of Gibbs sampling and 1 week in total
to train, using a single core of a 2.3Ghz AMD
Opteron machine.
5.1 Grammar Analysis
The resulting grammar had 1.62M rules, al-
most identical to the GHKM grammar which had
1.63M. Despite their similarity in size the gram-
mars were quite different, as illustrated in Fig-
ure 4, which shows histograms over various mea-
sures of rule size for the two grammars. Under
each measure the sampled grammar finds many
more simple rules ? shallower with fewer internal
nodes, fewer variables and fewer terminals ? than
the GHKM method. This demonstrates that the
prior is effective in shifting mass away from com-
plex rules. To show how the rules themselves dif-
fer, Table 3 lists rules in the sampled grammar that
are not in the GHKM grammar. Note that many of
these rules are highly plausible, describing regular
tree structures and lexicalisation. These rules have
not been specified to the same extent in the GHKM
grammar. For example the first rule incorporates
4
Our implementation of the GHKM algorithm attaches
unaligned source words to the highest possible node in the
source tree, rather than allowing all attachment points as in
the original presentation (Galley et al, 2004). Allowing all
attachments made no difference to translation performance,
but did make the grammar considerably larger. We imple-
mented only the minimal rule extraction, i.e., with no rule
composition (Galley et al, 2006). Consequently there is no
derivational ambiguity, obviating the need for expectation
maximisation or similar for rule estimation.
5
Note that although ? seems large, it still encourages
sparse distributions as the P
0
values are typically much
smaller than its reciprocal, 10
?6
, especially if the rule is
large. ?P
0
< 1 implies a sparse Dirichlet prior.
1 2 3 4 5 6 7 8 9 10 11
GKHMGibbs
maximum tree depth
num
ber o
f rule
s
0
20k
40k
60k
80k
0 1 2 3 4 5 6 7 8 9 10
variables
num
ber o
f rule
s
0
20k
40k
60k
80k
0 1 2 3 4 5 6 7 8 9 10
source terminals
num
ber o
f rule
s
0
20k
40k
60k
80k
0 1 2 3 4 5 6 7 8 9 10
target terminals
num
ber o
f rule
s
0
25k
50k
75k
100k
Figure 4: Histograms over rule statistics comparing the
heuristic grammar (GHKM) and learnt grammar (Gibbs).
the TOP symbol, while the GHKM grammar in-
stead relies on the rule ?(TOP S 1 ), 1 ? to produce
the same fragment. The model has learnt to dis-
tinguish between sentence-spanning and subsen-
tential S constituents, which typically do not in-
clude final punctuation. The third and ninth (last)
rule are particularly interesting. These rules en-
code reordering effects relating to noun phrases
and subordinate prepositional phrases, in partic-
ular that Chinese prepositional modifiers precede
the nouns they modify. Differences in word or-
der such as these are quite common in Chinese-
English corpora, so it is imperative that they are
modelled accurately.
The rules in the GHKM grammar that do not
appear in the sampled grammar are shown in
Table 4. In contrast to the rules only present in
the sampled grammar, these have much lower
counts, i.e., are less probable. Each of these rules
has been specified further by the Bayesian model.
358
?(TOP (S NP 1 VP 2 . 3 )), 1 2 3 ?
?(S (VP (TO to) VP 1 )), 1 ?
?(NP NP 1 (PP (IN of) NP 2 )), 2 1 ?
?(PP (IN in) NP 1 ), ? 1 ?
?(NP NP 1 (PP (IN of) NP 2 )), 1 2 ?
?(NP (DT the) NN 1 ), ? 1 ?
?(S (VP TO 1 VP 2 )), 1 2 ?
?(VP (VBZ is) NP 1 ), ? 1 ?
?(NP (NP (DT the) NN 1 ) (PP (IN of) NP 2 )), 2 1 ?
Table 3: Top ten rules in the sampled grammar that do not
appear in the GHKM grammar. All the above rules are quite
high probability, with counts between 37,118 and 7,275 from
first to last.
?(PP (IN at) (NP DT 1 (NNS levels))), 1 ??
?(NP NP 1 , 2 NP 3 (, ,) CC 4 NP 5 ), 1 2 3 4 5 ?
?(NP NP 1 , 2 NP 3 , 4 NP 5 (, ,) (CC and) NP 6 ), 1 2 3 4 5 , 6 ?
?(S S 1 (NP (PRP They)) VP 2 . 3 ), 1 2 3 ?
?(S PP 1 , 2 NP 3 VP 4 . 5 ? 6 ), 1 2 3 4 6 5 ?
?(S PP 1 , 2 NP 3 VP 4 . 5 ), 1 ? 2 3 4 5 ?
?(NP (NNP Foreign) (NNP Ministry) NN 1 (NNP Zhu) (NNP Bangzao)),
??? 1 ????
?(S S 1 S 2 ), 1 2 ?
?(S S 1 (NP (PRP We)) VP 2 . 3 ), 1 2 3 ?
?(NP (DT the) (NNS people) POS 1 ), ?? 1 ?
Table 4: Top ten rules in the GHKM grammar that do not ap-
pear in the sampled grammar. These are quite low probability
rules: their counts range from 1,137 to 103.
For example, every instance of the first rule
had the same determiner and target translation,
?(PP (IN at) (NP (DT all) (NNS levels))),???,
and therefore the model specified the determiner,
resulting in a single rule. The model has correctly
learnt that other translations for (DT all) are
not appropriate in this context (e.g., ?, ??
or ??). In a number of the remaining rules
the commas were lexicalised, or S rules were
extended to include the TOP symbol.
To further illustrate the differences between the
grammars, Table 5 shows the rules which include
the possessive particle, ?, and at least one vari-
able. In both grammars there are many fully lex-
icalised rules which translate the token to, e.g., a
determiner or a preposition. The grammars differ
on the complex rules which combine lexicalisa-
tion and frontier non-terminals. The GHKM rules
are all very simple depth-1 SCFG rules, contain-
ing minimal information. In contrast, the sampled
rules are more lexicalised, licensing the insertion
of various English tokens and tree substructure.
Note particularly the second and forth rule which
succinctly describe the reordering of prepositional
Sampled Grammar
?(NP (DT the) NN 1 ),? 1 ?
?(NP (NP (DT the) NN 1 ) (PP (IN of) NP 2 )), 2 ? 1 ?
?(NP (DT the) NN 1 ), 1 ??
?(NP (NP (DT the) JJ 1 NN 2 ) (PP (IN of) NP 3 )), 3 ? 1 2 ?
?(PP (IN of) NP 1 ), 1 ??
GHKM Grammar
?(NP JJ 1 NNS 2 ), 1 ? 2 ?
?(NP JJ 1 NN 2 ), 1 ? 2 ?
?(NP DT 1 JJ 2 NN 3 ), 1 2 ? 3 ?
?(NP PRP$ 1 NN 2 ), 1 ? 2 ?
?(NP NP 1 PP 2 ), 2 ? 1 ?
Table 5: Top five rules which include the possessive particle
? and at least one variable.
phrases with an noun phrase.
5.2 Translation
In order to test the translation performance of
the grammars induced by our model and the
GHKM method
6
we report BLEU (Papineni et
al., 2002) scores on sentences of up to twenty
words in length from the MT03 NIST evaluation.
We built a synchronous beam search decoder to
find the maximum scoring derivation, based on
the CYK+ chart parsing algorithm and the cube-
pruning method of Chiang (2007). Parse edges for
all constituents spanning a given chart cell were
cube-pruned together using a beam of width 1000,
and only edges from the top ten constituents in
each cell were retained. No artificial glue-rules or
rule span limits were employed.
7
The parameters
of the translation system were trained to maximize
BLEU on the MT02 test set (Och, 2003). Decoding
took roughly 10s per sentence for both grammars,
using a 8-core 2.6Ghz Intel Xeon machine.
Table 6 shows the BLEU scores for the baseline
using the GHKM rule induction algorithm, and
our non-parametric Bayesian grammar induction
method. We see a small increase in generalisation
performance from our model. Our previous anal-
6
Our decoder was unable to process unary rules (those
which consume nothing in the source). Monolingual pars-
ing with unary productions is fairly straightforward (Stolcke,
1995), however in the transductive setting these rules can li-
cence infinite insertions in the target string. This is further
complicated by the language model integration. Therefore
we composed each unary rule instance with its descendant
rule(s) to create a non-unary rule.
7
Our decoder lacks certain features shown to be beneficial
to synchronous grammar decoding, in particular rule binari-
sation (Zhang et al, 2006). As such the reported results for
MT03 lag the state-of-the-art: the Moses phrase-based de-
coder (Koehn et al, 2007) achieves 26.8. We believe that im-
provements from a better decoder implementation would be
orthogonal to the improvements presented here (and would
allow us to relax the length restriction on the test set).
359
Model BLEU score
GHKM 26.0
Our model 26.6
Table 6: Translation results on the NIST test set MT03 for
sentences of length ? 20.
ysis (Section 5.1) of the grammars produced by
the two approaches showed our method produced
better lexicalised rules than those induced by the
GHKM algorithm. Galley et al (2006) noted that
the GHKM algorithm often over generalised and
proposed combining minimal rules to form com-
posed rules as a solution. Although composing
rules was effective at improving BLEU scores, the
result was a massive expansion in the size of the
grammar. By learning the appropriate level of lex-
icalisation we believe that our inference algorithm
is having a similar effect as composing rules (Gal-
ley et al, 2006), however the resulting grammar
remains compact, a significant advantage of our
approach.
6 Conclusion
In this paper we have presented a method for in-
ducing a tree-to-string grammar which removes
the need for various heuristics and constraints
from models of word alignment. Instead the model
is capable of directly inferring a grammar in one
step, using the syntactic fragments that it has learnt
to better align the source and target data. Using a
prior which favours sparse distributions and sim-
pler rules, we demonstrate that the model finds
a more parsimonious grammar than the heuristic
technique. Moreover, this grammar results in im-
proved translations on a standard evaluation set.
We expect that various extensions to the model
would improve its performance. One avenue is to
develop a more sophisticated prior over rules, e.g.,
one that recognises common types of rule via the
shape of the tree and ordering pattern in the tar-
get. A second avenue is to develop better means
of inference under the grammar, in order to ensure
faster mixing and a means to escape from local
optima. Finally, we wish to develop a method for
decoding under the full Bayesian model, instead of
the current beam search. With these extensions we
expect that our model of grammar induction has
the potential to greatly improve translation output.
Acknowledgements
The authors acknowledge the support of the EP-
SRC (grants GR/T04557/01 and EP/D074959/1).
This work has made use of the resources pro-
vided by the Edinburgh Compute and Data Facility
(ECDF). The ECDF is partially supported by the
eDIKT initiative.
References
P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009a.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of the Joint conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Pro-
cessing, Singapore. To appear.
P. Blunsom, T. Cohn, M. Osborne. 2009b. Bayesian
synchronous grammar induction. In D. Koller,
D. Schuurmans, Y. Bengio, L. Bottou, eds., Ad-
vances in Neural Information Processing Systems
21, 161?168. MIT Press, Cambridge, MA.
P.-C. Chang, M. Galley, C. D. Manning. 2008. Op-
timizing Chinese word segmentation for machine
translation performance. In Proceedings of the
Third Workshop on Statistical Machine Translation,
224?232, Columbus, Ohio.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
T. Cohn, S. Goldwater, P. Blunsom. 2009. Inducing
compact but accurate tree-substitution grammars.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, 548?556, Boulder, Colorado.
J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006.
Why generative phrase models underperform sur-
face heuristics. In Proceedings on the Workshop on
Statistical Machine Translation, 31?38, New York
City, NY.
J. DeNero, A. Bouchard-C?ot?e, D. Klein. 2008. Sam-
pling alignment structure under a Bayesian transla-
tion model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, 314?323, Honolulu, Hawaii.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion
Volume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, 205?
208, Sapporo, Japan.
V. Fossum, K. Knight, S. Abney. 2008. Using syn-
tax to improve word alignment precision for syntax-
based machine translation. In Proceedings of the
360
Third Workshop on Statistical Machine Translation,
44?52, Columbus, Ohio.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004.
What?s in a translation rule? In Proceedings of the
2004 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, 273?280, Boston, MA.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation mod-
els. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, 961?968, Sydney, Australia.
S. Geman, D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
W. Gilks, S. Richardson, D. J. Spiegelhalter, eds. 1996.
Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
M. Johnson, T. L. Griffiths, S. Goldwater. 2007.
Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In
B. Sch?olkopf, J. Platt, T. Hoffman, eds., Advances
in Neural Information Processing Systems 19, 641?
648. MIT Press, Cambridge, MA.
M. Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Lingusitics,
28(1):71?76.
D. Klein, C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
In In Advances in Neural Information Processing
Systems 15, 3?10. MIT Press.
P. Koehn, F. J. Och, D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of
the 2003 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics, 48?54, Edmonton,
Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
177?180, Prague, Czech Republic.
D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006.
SPMT: Statistical machine translation with syntact-
ified target language phrases. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, 44?52, Sydney, Australia.
F. J. Och, H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, 311?318, Philadelphia, PA.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2).
H. Zhang, L. Huang, D. Gildea, K. Knight. 2006. Syn-
chronous binarization for machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics, 256?
263.
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008.
Bayesian learning of non-compositional phrases
with synchronous parsing. In Proceedings of ACL-
08: HLT, 97?105, Columbus, Ohio.
A. Zollmann, A. Venugopal. 2006. Syntax augmented
machine translation via chart parsing. In Proceed-
ings of the 2006 Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, 138?141,
New York City, NY.
361
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 548?556,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Inducing Compact but Accurate Tree-Substitution Grammars
Trevor Cohn and Sharon Goldwater and Phil Blunsom
School of Informatics
University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
{tcohn,sgwater,pblunsom}@inf.ed.ac.uk
Abstract
Tree substitution grammars (TSGs) are a com-
pelling alternative to context-free grammars
for modelling syntax. However, many popu-
lar techniques for estimating weighted TSGs
(under the moniker of Data Oriented Parsing)
suffer from the problems of inconsistency and
over-fitting. We present a theoretically princi-
pled model which solves these problems us-
ing a Bayesian non-parametric formulation.
Our model learns compact and simple gram-
mars, uncovering latent linguistic structures
(e.g., verb subcategorisation), and in doing so
far out-performs a standard PCFG.
1 Introduction
Many successful models of syntax are based on
Probabilistic Context Free Grammars (PCFGs)
(e.g., Collins (1999)). However, directly learning a
PCFG from a treebank results in poor parsing perfor-
mance, due largely to the unrealistic independence
assumptions imposed by the context-free assump-
tion. Considerable effort is required to coax good
results from a PCFG, in the form of grammar en-
gineering, feature selection and clever smoothing
(Collins, 1999; Charniak, 2000; Charniak and John-
son, 2005; Johnson, 1998). This effort must be re-
peated when moving to different languages, gram-
mar formalisms or treebanks. We propose that much
of this hand-coded knowledge can be obtained auto-
matically as an emergent property of the treebanked
data, thereby reducing the need for human input in
crafting the grammar.
We present a model for automatically learning a
Probabilistic Tree Substitution Grammar (PTSG),
an extension to the PCFG in which non-terminals
can rewrite as entire tree fragments (elementary
trees), not just immediate children. These large frag-
ments can be used to encode non-local context, such
as head-lexicalisation and verb sub-categorisation.
Since no annotated data is available providing TSG
derivations we must induce the PTSG productions
and their probabilities in an unsupervised way from
an ordinary treebank. This is the same problem ad-
dressed by Data Oriented Parsing (DOP, Bod et al
(2003)), a method which uses as productions all sub-
trees of the training corpus. However, many of the
DOP estimation methods have serious shortcomings
(Johnson, 2002), namely inconsistency for DOP1
(Bod, 2003) and overfitting of the maximum like-
lihood estimate (Prescher et al, 2004).
In this paper we develop an alternative means of
learning a PTSG from a treebanked corpus, with the
twin objectives of a) finding a grammar which ac-
curately models the data and b) keeping the gram-
mar as simple as possible, with few, compact, ele-
mentary trees. This is achieved using a prior to en-
courage sparsity and simplicity in a Bayesian non-
parametric formulation. The framework allows us to
perform inference over an infinite space of gram-
mar productions in an elegant and efficient manner.
The net result is a grammar which only uses the in-
creased context afforded by the TSG when necessary
to model the data, and otherwise uses context-free
rules.1 That is, our model learns to use larger rules
when the CFG?s independence assumptions do not
hold. This contrasts with DOP, which seeks to use
all elementary trees from the training set. While our
model is able, in theory, to use all such trees, in prac-
tice the data does not justify such a large grammar.
Grammars that are only about twice the size of a
1While TSGs and CFGs describe the same string lan-
guages, TSGs can describe context-sensitive tree-languages,
which CFGs cannot.
548
treebank PCFG provide large gains in accuracy. We
obtain additional improvements with grammars that
are somewhat larger, but still much smaller than the
DOP all-subtrees grammar. The rules in these gram-
mars are intuitive, potentially offering insights into
grammatical structure which could be used in, e.g.,
the development of syntactic ontologies and guide-
lines for future treebanking projects.
2 Background and related work
A Tree Substitution Grammar2 (TSG) is a 4-tuple,
G = (T,N, S,R), where T is a set of terminal sym-
bols, N is a set of non-terminal symbols, S ? N is
the distinguished root non-terminal and R is a set
of productions (a.k.a. rules). The productions take
the form of elementary trees ? tree fragments of
depth ? 2, where each internal node is labelled with
a non-terminal and each leaf is labelled with either a
terminal or a non-terminal. Non-terminal leaves are
called frontier non-terminals and form the substitu-
tion sites in the generative process of creating trees
with the grammar.
A derivation creates a tree by starting with the
root symbol and rewriting (substituting) it with an
elementary tree, then continuing to rewrite frontier
non-terminals with elementary trees until there are
no remaining frontier non-terminals. Unlike Con-
text Free Grammars (CFGs) a syntax tree may not
uniquely specify the derivation, as illustrated in Fig-
ure 1 which shows two derivations using different
elementary trees to produce the same tree.
A Probabilistic Tree Substitution Grammar
(PTSG), like a PCFG, assigns a probability to each
rule in the grammar. The probability of a derivation
is the product of the probabilities of its component
rules, and the probability of a tree is the sum of the
probabilities of its derivations.
As we mentioned in the introduction, work within
the DOP framework seeks to induce PTSGs from
treebanks by using all possible subtrees as rules, and
one of a variety of methods for estimating rule prob-
abilities.3 Our aim of inducing compact grammars
contrasts with that of DOP; moreover, we develop a
probabilistic estimator which avoids the shortcom-
ings of DOP1 and the maximum likelihood esti-
2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003))
without the adjunction operator.
3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also
tackles a similar learning problem.
mate (Bod, 2000; Bod, 2003; Johnson, 2002). Re-
cent work on DOP estimation also seeks to address
these problems, drawing from estimation theory to
solve the consistency problem (Prescher et al, 2004;
Zollmann and Sima?an, 2005), or incorporating a
grammar brevity term into the learning objective
(Zuidema, 2007). Our work differs from these pre-
vious approaches in that we explicitly model a prior
over grammars within a Bayesian framework.4
Models of grammar refinement (Petrov et al,
2006; Liang et al, 2007; Finkel et al, 2007) also
aim to automatically learn latent structure underly-
ing treebanked data. These models allow each non-
terminal to be split into a number of subcategories.
Theoretically the grammar space of our model is a
sub-space of theirs (projecting the TSG?s elementary
trees into CFG rules). However, the number of non-
terminals required to recreate our TSG grammars
in a PCFG would be exorbitant. Consequently, our
model should be better able to learn specific lexical
patterns, such as full noun-phrases and verbs with
their sub-categorisation frames, while theirs are bet-
ter suited to learning subcategories with larger mem-
bership, such as the terminals for days of the week
and noun-adjective agreement. The approaches are
orthogonal, and we expect that combining a category
refinement model with our TSG model would pro-
vide better performance than either approach alone.
Our model is similar to the Adaptor Grammar
model of Johnson et al (2007b), which is also
a kind of Bayesian nonparametric tree-substitution
grammar. However, Adaptor Grammars require that
each sub-tree expands completely, with only termi-
nal symbols as leaves, while our own model permits
non-terminal frontier nodes. In addition, they disal-
low recursive containment of adapted non-terminals;
we impose no such constraint.
3 Model
Recall the nature of our task: we are given a corpus
of parse trees t and wish to infer a tree-substitution
grammar G that we can use to parse new data.
Rather than inferring a grammar directly, we go
through an intermediate step of inferring a distri-
bution over the derivations used to produce t, i.e.,
4A similar Bayesian model of TSG induction has been de-
veloped independently to this work (O?Donnell et al, 2009b;
O?Donnell et al, 2009a).
549
(a)
S
NP
NP
George
VP
V
hates
NP
NP
broccoli
(b)
S
NP
George
VP
V
V
hates
NP
broccoli
S? NP (VP (V hates) NP)
NP? George
NP? broccoli
S? (NP George) (VP V (NP broccoli))
V? hates
Figure 1: Example derivations for the same tree,
where arrows indicate substitution sites. The ele-
mentary trees used in (a) and (b) are shown below
as grammar productions in bracketed tree notation.
a distribution over sequences of elementary trees e
that compose to form t. We will then essentially read
the grammar off the elementary trees, as described
in Section 5. Our problem therefore becomes one of
identifying the posterior distribution of e given t,
which we can do using Bayes? Rule:
P (e|t) ? P (t|e)P (e) (1)
Since the sequence of elementary trees can be split
into derivations, each of which completely specifies
a tree, P (t|e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the work
in our model is done by the prior distribution over
elementary trees. Note that this is analogous to the
Bayesian model of word segmentation presented by
Goldwater et al (2006); indeed, the problem of in-
ferring e from t can be viewed as a segmentation
problem, where each full tree must be segmented
into one or more elementary trees. As in Goldwater
et al (2006), we wish to favour solutions employing
a relatively small number of elementary units (here,
elementary trees). This can be done using a Dirichlet
process (DP) prior. Specifically, we define the distri-
bution of elementary tree e with root non-terminal
symbol c as
Gc|?c, P0 ? DP(?c, P0(?|c))
e|c ? Gc
whereP0(?|c) (the base distribution) is a distribution
over the infinite space of trees rooted with c, and ?c
(the concentration parameter) controls the model?s
tendency towards either reusing elementary trees or
creating novel ones as each training instance is en-
countered (and consequently, the tendency to infer
larger or smaller sets of elementary trees from the
observed data). We discuss the base distribution in
more detail below.
Rather than representing the distribution Gc ex-
plicitly, we integrate over all possible values of Gc.
The resulting distribution over ei, conditioned on
e<i = e1 . . . ei?1 and the root category c is:
p(ei|e<i, c, ?c, P0) = n
<i
ei,c + ?cP0(ei|c)
n<i?,c + ?c (2)
where n<iei,c is the number number of times ei has
been used to rewrite c in e<i, and n<i?,c =
?
e n<ie,c isthe total count of rewriting c.
As with other DP models, ours can be viewed as a
cache model, where ei can be generated in one of
two ways: by drawing from the base distribution,
where the probability of any particular tree is pro-
portional to ?cP0(ei|c), or by drawing from a cache
of previous expansions of c, where the probability of
any particular expansion is proportional to the num-
ber of times that expansion has been used before.
This view makes it clear that the model embodies
a ?rich-get-richer? dynamic in which a few expan-
sions will occur with high probability, but many will
occur only once or twice, as is typical of natural lan-
guage. Our model is similar in this way to the Adap-
tor Grammar model of Johnson et al (2007a).
We still need to define P0, the base distribution
over tree fragments. We use two such distributions.
The first, PM0 generates each elementary tree by
a series of random decisions: whether to expand a
non-terminal, how many children to produce and
their identities. The probability of expanding a non-
terminal node labelled c is parameterised via a bino-
mial distribution, Bin(?c), while all other decisions
are chosen uniformly at random. The second base
distribution, PC0 , has a similar generative process
but draws non-terminal expansions from a treebank-
trained PCFG instead of a uniform distribution.
Both choices of P0 have the effect of biasing the
model towards simple rules with a small number of
internal nodes. The geometric increase in cost dis-
courages the model from using larger rules; for this
to occur these rules must yield a large increase in the
data likelihood. As PC0 incorporates PCFG probabil-
550
SNP,1
George
VP,0
V,0
hates
NP,1
broccoli
Figure 2: Gibbs state e specifying the derivation in
Figure 1a. Each node is labelled with its substitution
indicator variable.
ities, it assigns higher relative probability to larger
rules, compared to the more draconian PM0 .
4 Training
To train our model we use Gibbs sampling (Geman
and Geman, 1984), a Markov chain Monte Carlo
method in which variables are repeatedly sampled
conditioned on the values of all other variables in
the model. After a period of burn-in, each sam-
pler state (set of variable assignments) is a sample
from the posterior distribution of the model. In our
case, we wish to sample from P (e|t, ?, ?), where
(?, ?) = {?c, ?c} for all categories c. To do so,
we associate a binary variable with each non-root
internal node of each tree in the training set, indi-
cating whether that node is a substitution point or
not. Each substitution point forms the root of some
elementary tree, as well as a frontier non-terminal
of an ancestor node?s elementary tree. Collectively,
the training trees and substitution variables specify
the sequence of elementary trees e that is the current
state of the sampler. Figure 2 shows an example tree
with its substitution variables, corresponding to the
TSG derivation in Figure 1a.
Our Gibbs sampler works by sampling the value
of each substitution variable, one at a time, in ran-
dom order. If d is the node associated with the sub-
stitution variable s under consideration, then the two
possible values of s define two options for e: one
in which d is internal to some elementary tree eM ,
and one in which d is the substitution site con-
necting two smaller trees, eA and eB . In the ex-
ample in Figure 2, when sampling the VP node,
eM = (S NP (VP (V hates) NP)), eA = (S NP VP),
and eB = (VP (V hates) NP). To sample a value for
s, we compute the probabilities of eM and (eA, eB),
conditioned on e?: all other elementary trees in the
training set that share at most a root or frontier non-
terminal with eM , eA, or eB . This is easy to do
because the DP is exchangeable, meaning that the
probability of a set of outcomes does not depend on
their ordering. Therefore, we can treat the elemen-
tary trees under consideration as the last ones to be
sampled, and apply Equation 2, giving us
P (eM |cM )=n
?
eM ,cM + ?cMP0(eM |cM )
n??,cM + ?cM
(3)
P (eA, eB|cA)=n
?
eA,cA + ?cAP0(eA|cA)
n??,cA + ?cA
(4)
?
n?eB ,cB + ?(eA, eB) + ?cBP0(eB|cB)
n??,cB + ?(cA, cB) + ?cB
where cx is the root label of ex, x ? {A,B,M},
the counts n? are with respect to e?, and ?(?, ?) is
the Kronecker delta function, which returns 1 when
its arguments are identical and 0 otherwise. We have
omitted e?, t, ? and ? from the conditioning con-
text. The ? terms in the second factor of (4) account
the changes to n? that would occur after observing
eA, which forms part of the conditioning context for
eB . If the trees eA and eB are identical, then the
count n?eB ,cB would increase by one, and if the treesshare the same root non-terminal, then n??,cB wouldincrease by one.
In the previous discussion, we have assumed
that the model hyperparameters, (?, ?), are known.
However, selecting their values by hand is extremely
difficult and fitting their values on heldout data is of-
ten very time consuming. For this reason we treat
the hyper-parameters as variables in our model and
infer their values during training. We choose vague
priors for each hyper-parameter, encoding our lack
of information about their values. We treat the con-
centration parameters, ?, as being generated by a
vague gamma prior, ?c ? Gamma(0.001, 1000).
We sample a new value ??c using a log-normal dis-
tribution with mean ?c and variance 0.3, which is
then accepted into the distribution p(?c|e, t, ??, ?)
using the Metropolis-Hastings algorithm. We use a
Beta prior for the binomial specification parameters,
?c ? Beta(1, 1). As the Beta distribution is conju-
gate to the binomial, we can directly resample the
? parameters from the posterior, p(?c|e, t, ?, ??).
Both the concentration and substitution parameters
are resampled after every full Gibbs sampling itera-
tion over the training trees.
551
5 Parsing
We now turn to the problem of using the model
to parse novel sentences. This requires finding the
maximiser of
p(t|w, t) =
?
p(t|w, e, ?, ?) p(e, ?, ?|t) de d? d?
(5)
wherew is the sequence of words being parsed and t
the resulting tree, t are the training trees and e their
segmentation into elementary trees.
Unfortunately solving for the maximising parse
tree in (5) is intractable. However, it can approxi-
mated using Monte Carlo techniques. Given a sam-
ple of (e, ?, ?)5 we can reason over the space of
possible trees using a Metropolis-Hastings sampler
(Johnson et al, 2007a) coupled with a Monte Carlo
integral (Bod, 2003). The first step is to sample from
the posterior over derivations, p(d|w, e, ?, ?). This
is achieved by drawing samples from an approxima-
tion grammar, p?(d|w), which are then accepted to
the true distribution using the Metropolis-Hastings
algorithm. The second step records for each sampled
derivation the CFG tree. The counts of trees consti-
tute an approximation to p(t|w, e, ?, ?), from which
we can recover the maximum probability tree.
A natural proposal distribution, p?(d|w), is the
maximum a posterior (MAP) grammar given the el-
ementary tree analysis of our training set (analogous
to the PCFG approximation used in Johnson et al
(2007a)). This is not practical because the approx-
imation grammar is infinite: elementary trees with
zero count in e still have some residual probabil-
ity under P0. In the absence of a better alternative,
we discard (most of) the zero-count rules from MAP
grammar. This results in a tractable grammar repre-
senting the majority of the probability mass, from
which we can sample derivations. We specifically
retain all zero-count PCFG productions observed in
the training set in order to provide greater robustness
on unseen data.
In addition to finding the maximum probability
parse (MPP), we also report results using the maxi-
mum probability derivation (MPD). While this could
be calculated in the manner as described above, we
5Using many samples of (e, ?, ?) in a Monte Carlo inte-
gral is a straight-forward extension to our parsing algorithm. We
did not observe a significant improvement in parsing accuracy
when using a multiple samples compared to a single sample,
and therefore just present results for a single sample.
S ? A | B
A? A A | B B | (A a) (A a) | (B a) (B a)
B ? A A | B B | (A b) (A b) | (B b) (B b)
Figure 3: TSG used to generate synthetic data. All
production probabilities are uniform.
found that using the CYK algorithm (Cocke, 1969)
to find the Viterbi derivation for p? yielded consis-
tently better results. This algorithm maximises an
approximated model, as opposed to approximately
optimising the true model. We also present results
using the tree with the maximum expected count of
CFG rules (MER). This uses counts of the CFG rules
applied at each span (compiled from the derivation
samples) followed by a maximisation step to find the
best tree. This is similar to the MAX-RULE-SUM
algorithm of Petrov and Klein (2007) and maximum
expected recall parsing (Goodman, 2003).
6 Experiments
Synthetic data Before applying the model to
natural language, we first create a synthetic problem
to confirm that the model is capable of recovering
a known tree-substitution grammar. We created 50
random trees from the TSG shown in Figure 3. This
produces binary trees with A and B internal nodes
and ?a? and ?b? as terminals, such that the termi-
nals correspond to their grand-parent non-terminal
(A and a or B and b). These trees cannot be mod-
elled accurately with a CFG because expanding A
and B nodes into terminal strings requires knowing
their parent?s non-terminal.
We train the model for 100 iterations of Gibbs
sampling using annealing to speed convergence.
Annealing amounts to smoothing the distributions
in (3) and (4) by raising them to the power of 1T .Our annealing schedule begins at T = 3 and lin-
early decreases to reach T = 1 in the final iteration.
The sampler converges to the correct grammar, with
the 10 rules from Figure 3.
Penn-treebank parsing We ran our natural lan-
guage experiments on the Penn treebank, using the
standard data splits (sections 2?21 for training, 22
for development and 23 for testing). As our model is
parameter free (the ? and ? parameters are learnt in
training), we do not use the development set for pa-
552
rameter tuning. We expect that fitting these param-
eters to maximise performance on the development
set would lead to a small increase in generalisation
performance, but at a significant cost in runtime. We
replace tokens with count? 1 in the training sample
with one of roughly 50 generic unknown word mark-
ers which convey the token?s lexical features and po-
sition in the sentence, following Petrov et al (2006).
We also right-binarise the trees to reduce the branch-
ing factor in the same manner as Petrov et al (2006).
The predicted trees are evaluated using EVALB6 and
we report the F1 score over labelled constituents and
exact match accuracy over all sentences in the test-
ing sets.
In our experiments, we initialised the sampler by
setting all substitution variables to 0, thus treating
every full tree in the training set as an elementary
tree. Starting with all the variables set to 1 (corre-
sponding to CFG expansions) or a random mix of
0s and 1s considerably increases time until conver-
gence. We hypothesise that this is due to the sampler
getting stuck in modes, from which a series of lo-
cally bad decisions are required to escape. The CFG
solution seems to be a mode and therefore starting
the sampler with maximal trees helps the model to
avoid this mode.
Small data sample For our first treebank exper-
iments, we train on a small data sample by using
only section 2 of the treebank. Bayesian methods
tend to do well with small data samples, while for
larger samples the benefits diminish relative to point
estimates. The models were trained using Gibbs
sampling for 4000 iterations with annealing linearly
decreasing from T = 5 to T = 1, after which
the model performed another 1000 iterations with
T = 1. The final training sample was used in the
parsing algorithm, which used 1000 derivation sam-
ples for each test sentence. All results are the aver-
age of five independent runs.
Table 1 presents the prediction results on the de-
velopment set. The baseline is a maximum likeli-
hood PCFG. The TSG model significantly outper-
forms the baseline with either base distribution PM0
or PC0 . This confirms our hypothesis that CFGs are
not sufficiently powerful to model syntax, but that
the increased context afforded to the TSG can make
a large difference. This result is even more impres-
sive when considering the difference in the sizes of
6See http://nlp.cs.nyu.edu/evalb/.
F1 EX # rules
PCFG 60.20 4.29 3500
TSG PM0 : MPD 72.17 11.92 6609MPP 71.27 12.33 6609
MER 74.25 12.30 6609
TSG PC0 : MPD 75.24 15.18 14923MPP 75.30 15.74 14923
MER 76.89 15.76 14923
SM?=2: MPD 71.93 11.30 16168
MER 74.32 11.77 16168
SM?=5: MPD 75.33 15.64 39758
MER 77.93 16.94 39758
Table 1: Development results for models trained on
section 2 of the Penn tree-bank, showing labelled
constituent F1 and exact match accuracy. Grammar
sizes are the number of rules with count ? 1.
grammar in the PCFG versus TSG models. The TSG
using PM0 achieves its improvements with only dou-
ble as many rules, as a consequence of the prior
which encourages sparse solutions. The TSG results
with the CFG base distribution, PC0 , are more ac-
curate but with larger grammars.7 This base distri-
bution assigns proportionally higher probability to
larger rules than PM0 , and consequently the model
uses these additional rules in a larger grammar.
Surprisingly, the MPP technique is not systemati-
cally better than the MPD approach, with mixed re-
sults under the F1 metric. We conjecture that this is
due to sampling variance for long sentences, where
repeated samples of the same tree are exceedingly
rare. The MER technique results in considerably
better F1 scores than either MPD or MPP, with a
margin of 1.5 to 3 points. This method is less af-
fected by sampling variance due to its use of smaller
tree fragments (PCFG productions at each span).
For comparison, we trained the Berkeley split-
merge (SM) parser (Petrov et al, 2006) on the same
data and decoded using the Viterbi algorithm (MPD)
and expected rule count (MER a.k.a. MAX-RULE-
SUM). We ran two iterations of split-merge training,
after which the development F1 dropped substan-
tially (in contrast, our model is not fit to the devel-
opment data). The result is an accuracy slightly be-
low that of our model (SM?=2). To be fairer to their
model, we adjusted the unknown word threshold to
their default setting, i.e., to apply to word types oc-
7The grammar is nevertheless far smaller than the full DOP
grammar on this data set, which has 700K rules.
553
0 1 2 3 4 5 6 7 8
count
cou
nt of
 cou
nts
0
100
200
300
400
500 depthnodeslexemesvars
Figure 4: Grammar statistics for a TSG PM0 model
trained on section 2 of the Penn treebank, show-
ing a histogram over elementary tree depth, num-
ber of nodes, terminals (lexemes) and frontier non-
terminals (vars).
curring fewer than five times (SM?=5). We expect
that tuning the treatment of unknown words in our
model would also yield further gains. The grammar
sizes are not strictly comparable, as the Berkeley bi-
narised grammars prohibit non-binary rules, and are
therefore forced to decompose each of these rules
into many child rules. But the trend is clear ? our
model produces similar results to a state-of-the-art
parser, and can do so using a small grammar. With
additional rounds of split-merge training, the Berke-
ley grammar grows exponentially larger (200K rules
after six iterations).
Full treebank We now train the model using
PM0 on the full training partition of the Penn tree-
bank, using sections 2?21. We run the Gibbs sampler
for 15,000 iterations while annealing from T = 5 to
T = 1, after which we finish with 5,000 iterations
at T = 1. We repeat this three times, giving an av-
erage F1 of 84.0% on the testing partition using the
maximum expected rule algorithm and 83.0% using
the Viterbi algorithm. This far surpasses the ML-
PCFG (F1 of 70.7%), and is similar to Zuidema?s
(2007) DOP result of 83.8%. However, it still well
below state-of-the art parsers (e.g., the Berkeley
parser trained using the same data representation
scores 87.7%). But we must bear in mind that these
parsers have benefited from years of tuning to the
Penn-treebank, where our model is much simpler
and is largely untuned. We anticipate that careful
data preparation and model tuning could greatly im-
prove our model?s performance.
NP?
(NNP Mr.) NNP
CD (NN %)
(NP CD (NN %)) (PP (IN of) NP)
(NP ($ $) CD) (NP (DT a) (NN share))
(NP (DT the) (N?P (NN company) POS)) N?P
(NP QP (NN %)) (PP (IN of) NP)
(NP CD (NNS cents)) (NP (DT a) (NN share))
(NP (NNP Mr.) (N?P NNP (POS ?s))) NN
QP (NN %)
(NP (NN president)) (PP (IN of) NP)
(NP (NNP Mr.) (N?P NNP (POS ?s))) N?P
NNP (N?P NNP (NNP Corp.))
NNP (N?P NNP (NNP Inc.))
(NP (NN chairman)) (PP (IN of) NP)
VP?
(VBD said) (SBAR (S (NP (PRP it)) VP))
(VBD said) (SBAR (S NP VP))
(VBD rose) (V?P (NP CD (NN %)) V?P)
(VBP want) S
(VBD said) (SBAR (S (NP (PRP he)) VP))
(VBZ plans) S
(VBD said) (SBAR S)
(VBZ says) (SBAR (S NP VP))
(VBP think) (SBAR S)
(VBD agreed) (S (VP (TO to) (VP VB V?P)))
(VBZ includes) NP
(VBZ says) (SBAR (S (NP (PRP he)) VP))
(VBZ wants) S
(VBD closed) (V?P (PP (IN at) NP) (V?P , ADVP))
Table 3: Most frequent lexicalised expansions for
noun and verb phrases, excluding auxiliary verbs.
7 Discussion
So what kinds of non-CFG rules is the model learn-
ing? Figure 4 shows the grammar statistics for a
TSG model trained on the small data sample. This
model has 5611 CFG rules and 1008 TSG rules.
The TSG rules vary in depth from two to nine levels
with the majority between two and four. Most rules
combine a small degree of lexicalisation and a vari-
able or two. This confirms that the model is learn-
ing local structures to encode, e.g., multi-word units,
subcategorisation frames and lexical agreement. The
few very large rules specify full parses for sentences
which were repeated in the training corpus. These
complete trees are also evident in the long tail of
node counts (up to 27; not shown in the figure) and
counts for highly lexicalised rules (up to 8).
To get a better feel for the types of rules being
learnt, it is instructive to examine the rules in the re-
554
NP? PP? ADJP?
DT N?P IN NP JJ
NNS (IN in) NP RB JJ
DT NN (TO to) NP JJ ( ?ADJP CC JJ)
(DT the) N?P TO NP JJ PP
JJ NNS (IN with) NP (RB very) JJ
NP (PP (IN of) NP) (IN of) NP RB ?ADJP
NP PP (IN by) NP (RBR more) JJ
NP (N?P (CC and) NP) (IN at) NP JJ ?ADJP
JJ N?P IN (NP (DT the) N?P) ADJP ( ?ADJP CC ADJP)
NN NNS (IN on) NP RB VBN
(DT the) NNS (IN from) NP RB ( ?ADJP JJ PP)
DT (N?P JJ NN) IN (S (VP VBG NP)) JJ (PP (TO to) NP)
NN IN (NP NP PP) ADJP (PP (IN than) NP)
JJ NN (IN into) NP (RB too) JJ
(NP DT NN) (PP (IN of) NP) (IN for) NP (RB much) JJR
Table 2: Top fifteen expansions sorted by frequency (most frequent at top), taken from the final sample of a
model trained on the full Penn treebank. Non-terminals shown with an over-bar denote a binarised sub span
of the given phrase type.
sultant grammar. Table 2 shows the top fifteen rules
for three phrasal categories for the model trained on
the full Penn treebank. We can see that many of these
rules are larger than CFG rules, showing that the
CFG rules alone are inadequate to model the tree-
bank. Two of the NP rules encode the prevalence
of preposition phrases headed by ?of? within a noun
phrase, as opposed to other prepositions. Also note-
worthy is the lexicalisation of the determiner, which
can affect the type of NP expansion. For instance,
the indefinite article is more likely to have an ad-
jectival modifier, while the definite article appears
more frequently unmodified. Highly specific tokens
are also incorporated into lexicalised rules.
Many of the verb phrase expansions have been
lexicalised, encoding the verb?s subcategorisation,
as shown in Table 3. Notice that each verb here ac-
cepts only one or a small set of argument frames,
indicating that by lexicalising the verb in the VP ex-
pansion the model can find a less ambiguous and
more parsimonious grammar.
The model also learns to use large rules to de-
scribe the majority of root node expansions (we add
a distinguished TOP node to all trees). These rules
mostly describe cases when the S category is used
for a full sentence, which most often include punc-
tuation such as the full stop and quotation marks. In
contrast, the majority of expansions for the S cat-
egory do not include any punctuation. The model
has learnt to differentiate between the two different
classes of S ? full sentence versus internal clause ?
due to their different expansions.
8 Conclusion
In this work we have presented a non-parametric
Bayesian model for inducing tree substitution gram-
mars. By incorporating a structured prior over ele-
mentary rules our model is able to reason over the
infinite space of all such rules, producing compact
and simple grammars. In doing so our model learns
local structures for latent linguistic phenomena, such
as verb subcategorisation and lexical agreement. Our
experimental results show that the induced gram-
mars strongly out-perform standard PCFGs, and are
comparable to a state-of-the-art parser on small data
samples. While our results on the full treebank are
well shy of the best available parsers, we have pro-
posed a number of improvements to the model and
the parsing algorithm that could lead to state-of-the-
art performance in the future.
References
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-oriented parsing. Center for the Study of
Language and Information - Studies in Computational
Linguistics. University of Chicago Press.
Rens Bod. 2000. Combining semantic and syntactic
structure for language modeling. In Proceedings of
the 6th International Conference on Spoken Language
Processing, Beijing, China.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Budapest, Hungary, April.
555
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor, Michigan, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 132?139.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of the
19th International Conference on Computational Lin-
guistics, pages 183?189, Taipei, Taiwan.
John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Courant Institute of
Mathematical Sciences, New York University.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 272?279, Prague, Czech
Republic, June.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of COLING/ACL,
Sydney.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al (Bod et al, 2003),
chapter 8.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 139?146, Rochester,
New York, April.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In Advances in Neural Information Processing Sys-
tems 19.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4),
December.
Mark Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Lingusitics,
28(1):71?76, March.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Computa-
tional Linguistics, pages 483?501. Oxford University
Press, Oxford, England.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697, Prague, Czech
Republic, June.
Timothy J. O?Donnell, Noah D. Goodman, Jesse
Snedeker, and Joshua B. Tenenbaum. 2009a. Com-
putation and reuse in language. In 31st Annual Con-
ference of the Cognitive Science Society, Amsterdam,
The Netherlands, July. To appear.
Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009b. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
Report MIT-CSAIL-TR-2009-013, MIT.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 404?411, Rochester,
New York, April. Association for Computational Lin-
guistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July.
Detlef Prescher, Remko Scha, Khalil Sima?an, and An-
dreas Zollmann. 2004. On the statistical consistency
of dop estimators. In Proceedings of the 14th Meet-
ing of Computational Linguistics in the Netherlands,
Antwerp, Belgium.
Fei Xia. 2002. Automatic grammar generation from
two different perspectives. Ph.D. thesis, University of
Pennsylvania.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics,
10(2):367?388.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 551?560, Prague, Czech Republic, June.
556
Proceedings of ACL-08: HLT, pages 200?208,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Discriminative Latent Variable Model
for Statistical Machine Translation
Phil Blunsom, Trevor Cohn and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
{pblunsom,tcohn,miles}@inf.ed.ac.uk
Abstract
Large-scale discriminative machine transla-
tion promises to further the state-of-the-art,
but has failed to deliver convincing gains over
current heuristic frequency count systems. We
argue that a principle reason for this failure is
not dealing with multiple, equivalent transla-
tions. We present a translation model which
models derivations as a latent variable, in both
training and decoding, and is fully discrimina-
tive and globally optimised. Results show that
accounting for multiple derivations does in-
deed improve performance. Additionally, we
show that regularisation is essential for max-
imum conditional likelihood models in order
to avoid degenerate solutions.
1 Introduction
Statistical machine translation (SMT) has seen
a resurgence in popularity in recent years, with
progress being driven by a move to phrase-based and
syntax-inspired approaches. Progress within these
approaches however has been less dramatic. We be-
lieve this is because these frequency count based1
models cannot easily incorporate non-independent
and overlapping features, which are extremely use-
ful in describing the translation process. Discrimi-
native models of translation can include such fea-
tures without making assumptions of independence
or explicitly modelling their interdependence. How-
ever, while discriminative models promise much,
they have not been shown to deliver significant gains
1We class approaches using minimum error rate training
(Och, 2003) frequency count based as these systems re-scale a
handful of generative features estimated from frequency counts
and do not support large sets of non-independent features.
over their simpler cousins. We argue that this is due
to a number of inherent problems that discrimina-
tive models for SMT must address, in particular the
problems of spurious ambiguity and degenerate so-
lutions. These occur when there are many ways to
translate a source sentence to the same target sen-
tence by applying a sequence of steps (a derivation)
of either phrase translations or synchronous gram-
mar rules, depending on the type of system. Exist-
ing discriminative models require a reference deriva-
tion to optimise against, however no parallel cor-
pora annotated for derivations exist. Ideally, a model
would account for this ambiguity by marginalising
out the derivations, thus predicting the best transla-
tion rather than the best derivation. However, doing
so exactly is NP-complete. For this reason, to our
knowledge, all discriminative models proposed to
date either side-step the problem by choosing simple
model and feature structures, such that spurious am-
biguity is lessened or removed entirely (Ittycheriah
and Roukos, 2007; Watanabe et al, 2007), or else ig-
nore the problem and treat derivations as translations
(Liang et al, 2006; Tillmann and Zhang, 2007).
In this paper we directly address the problem of
spurious ambiguity in discriminative models. We
use a synchronous context free grammar (SCFG)
translation system (Chiang, 2007), a model which
has yielded state-of-the-art results on many transla-
tion tasks. We present two main contributions. First,
we develop a log-linear model of translation which
is globally trained on a significant number of paral-
lel sentences. This model maximises the conditional
likelihood of the data, p(e|f), where e and f are the
English and foreign sentences, respectively. Our es-
timation method is theoretically sound, avoiding the
biases of the heuristic relative frequency estimates
200
l
l
l
l
l l
l
l
l
l
l
sentence length
deriv
ation
s
5 7 9 11 13 15
1e+0
3
1e+0
5
1e+0
8
Figure 1. Exponential relationship between sentence
length and the average number of derivations (on a log
scale) for each reference sentence in our training corpus.
(Koehn et al, 2003). Second, within this frame-
work, we model the derivation, d, as a latent vari-
able, p(e,d|f), which is marginalised out in train-
ing and decoding. We show empirically that this
treatment results in significant improvements over a
maximum-derivation model.
The paper is structured as follows. In Section 2
we list the challenges that discriminative SMT must
face above and beyond the current systems. We sit-
uate our work, and previous work, on discrimina-
tive systems in this context. We present our model
in Section 3, including our means of training and de-
coding. Section 4 reports our experimental setup and
results, and finally we conclude in Section 5.
2 Challenges for Discriminative SMT
Discriminative models allow for the use of expres-
sive features, in the order of thousands or millions,
which can reference arbitrary aspects of the source
sentence. Given most successful SMT models have
a highly lexicalised grammar (or grammar equiva-
lent), these features can be used to smuggle in lin-
guistic information, such as syntax and document
context. With this undoubted advantage come four
major challenges when compared to standard fre-
quency count SMT models:
1. There is no one reference derivation. Often
there are thousands of ways of translating a
source sentence into the reference translation.
Figure 1 illustrates the exponential relationship
between sentence length and the number of
derivations. Training is difficult without a clear
target, and predicting only one derivation at test
time is fraught with danger.
2. Parallel translation data is often very noisy,
with such problems as non-literal translations,
poor sentence- and word-alignments. A model
which exactly translates the training data will
inevitably perform poorly on held-out data.
This problem of over-fitting is exacerbated
in discriminative models with large, expres-
sive, feature sets. Regularisation is essential for
models with more than a handful of features.
3. Learning with a large feature set requires many
training examples and typically many iterations
of a solver during training. While current mod-
els focus solely on efficient decoding, discrim-
inative models must also allow for efficient
training.
Past work on discriminative SMT only address
some of these problems. To our knowledge no sys-
tems directly address Problem 1, instead choosing to
ignore the problem by using one or a small handful
of reference derivations in an n-best list (Liang et al,
2006; Watanabe et al, 2007), or else making local
independence assumptions which side-step the issue
(Ittycheriah and Roukos, 2007; Tillmann and Zhang,
2007; Wellington et al, 2006). These systems all in-
clude regularisation, thereby addressing Problem 2.
An interesting counterpoint is the work of DeNero et
al. (2006), who show that their unregularised model
finds degenerate solutions. Some of these discrim-
inative systems have been trained on large training
sets (Problem 3); these systems are the local models,
for which training is much simpler. Both the global
models (Liang et al, 2006; Watanabe et al, 2007)
use fairly small training sets, and there is no evi-
dence that their techniques will scale to larger data
sets.
Our model addresses all three of the above prob-
lems within a global model, without resorting to n-
best lists or local independence assumptions. Fur-
thermore, our model explicitly accounts for spurious
ambiguity without altering the model structure or ar-
bitrarily selecting one derivation. Instead we model
the translation distribution with a latent variable for
the derivation, which we marginalise out in training
and decoding.
201
the hat
le chapeau
red
the hat
le chapeau
red
Figure 2. The dropping of an adjective in this example
means that there is no one segmentation that we could
choose that would allow a system to learn le ? the and
chapeau? hat.
?S? ? ?S 1 X 2 , S 1 X 2 ?
?S? ? ?X 1 , X 1 ?
?X? ? ?ne X 1 pas, does not X 1 ?
?X? ? ?va, go?
?X? ? ?il, he?
Figure 3. A simple SCFG, with non-terminal symbols S
and X, which performs the transduction: il ne vas pas ?
he does not go
This itself provides robustness to noisy data, in
addition to the explicit regularisation from a prior
over the model parameters. For example, in many
cases there is no one perfect derivation, but rather
many imperfect ones which each include some good
translation fragments. The model can learn from
many of these derivations and thereby learn from
all these translation fragments. This situation is il-
lustrated in Figure 2 where the non-translated ad-
jective red means neither segmentation is ?correct?,
although both together present positive evidence for
the two lexical translations.
We present efficient methods for training and pre-
diction, demonstrating their scaling properties by
training on more than a hundred thousand train-
ing sentences. Finally, we stress that our main find-
ings are general ones. These results could ? and
should ? be applied to other models, discriminative
and generative, phrase- and syntax-based, to further
progress the state-of-the-art in machine translation.
3 Discriminative Synchronous
Transduction
A synchronous context free grammar (SCFG) con-
sists of paired CFG rules with co-indexed non-
terminals (Lewis II and Stearns, 1968). By assign-
ing the source and target languages to the respective
sides of a SCFG it is possible to describe translation
as the process of parsing the source sentence using
a CFG, while generating the target translation from
the other (Chiang, 2007). All the models we present
use the grammar extraction technique described in
Chiang (2007), and are bench-marked against our
own implementation of this hierarchical model (Hi-
ero). Figure 3 shows a simple instance of a hierar-
chical grammar with two non-terminals. Note that
our approach is general and could be used with other
synchronous grammar transducers (e.g., Galley et al
(2006)).
3.1 A global log-linear model
Our log-linear translation model defines a condi-
tional probability distribution over the target trans-
lations of a given source sentence. A particular se-
quence of SCFG rule applications which produces a
translation from a source sentence is referred to as a
derivation, and each translation may be produced by
many different derivations. As the training data only
provides source and target sentences, the derivations
are modelled as a latent variable.
The conditional probability of a derivation, d, for
a target translation, e, conditioned on the source, f ,
is given by:
p?(d, e|f) =
exp
?
k ?kHk(d, e, f)
Z?(f)
(1)
where Hk(d, e, f) =
?
r?d
hk(f , r) (2)
Here k ranges over the model?s features, and
? = {?k} are the model parameters (weights for
their corresponding features). The feature functions
Hk are predefined real-valued functions over the
source and target sentences, and can include over-
lapping and non-independent features of the data.
The features must decompose with the derivation,
as shown in (2). The features can reference the en-
tire source sentence coupled with each rule, r, in a
derivation. The distribution is globally normalised
by the partition function, Z?(f), which sums out the
numerator in (1) for every derivation (and therefore
every translation) of f :
Z?(f) =
?
e
?
d??(e,f)
exp
?
k
?kHk(d, e, f)
Given (1), the conditional probability of a target
translation given the source is the sum over all of
its derivations:
p?(e|f) =
?
d??(e,f)
p?(d, e|f) (3)
202
where ?(e, f) is the set of all derivations of the tar-
get sentence e from the source f.
Most prior work in SMT, both generative and dis-
criminative, has approximated the sum over deriva-
tions by choosing a single ?best? derivation using a
Viterbi or beam search algorithm. In this work we
show that it is both tractable and desirable to directly
account for derivational ambiguity. Our findings
echo those observed for latent variable log-linear
models successfully used in monolingual parsing
(Clark and Curran, 2007; Petrov et al, 2007). These
models marginalise over derivations leading to a de-
pendency structure and splits of non-terminal cate-
gories in a PCFG, respectively.
3.2 Training
The parameters of our model are estimated
from our training sample using a maximum a
posteriori (MAP) estimator. This maximises
the likelihood of the parallel training sen-
tences, D = {(e, f)}, penalised using a prior,
i.e., ?MAP = arg max? p?(D)p(?). We use a
zero-mean Gaussian prior with the probability
density function p0(?k) ? exp
(
??2k/2?
2
)
.2 This
results in the following log-likelihood objective and
corresponding gradient:
L =
?
(e,f)?D
log p?(e|f) +
?
k
log p0(?k) (4)
?L
??k
= Ep?(d|e,f)[hk]? Ep?(e|f)[hk]?
?k
?2
(5)
In order to train the model, we maximise equation
(4) using L-BFGS (Malouf, 2002; Sha and Pereira,
2003). This method has been demonstrated to be ef-
fective for (non-convex) log-linear models with la-
tent variables (Clark and Curran, 2004; Petrov et al,
2007). Each L-BFGS iteration requires the objective
value and its gradient with respect to the model pa-
rameters. These are calculated using inside-outside
inference over the feature forest defined by the
SCFG parse chart of f yielding the partition func-
tion, Z?(f), required for the log-likelihood, and the
marginals, required for its derivatives.
Efficiently calculating the objective and its gradi-
ent requires two separate packed charts, each rep-
resenting a derivation forest. The first one is the full
chart over the space of possible derivations given the
2In general, any conjugate prior could be used instead of a
simple Gaussian.
source sentence. The inside-outside algorithm over
this chart gives the marginal probabilities for each
chart cell, from which we can find the feature ex-
pectations. The second chart contains the space of
derivations which produce the reference translation
from the source. The derivations in this chart are a
subset of those in the full derivation chart. Again,
we use the inside-outside algorithm to find the ?ref-
erence? feature expectations from this chart. These
expectations are analogous to the empirical observa-
tion of maximum entropy classifiers.
Given these two charts we can calculate the log-
likelihood of the reference translation as the inside-
score from the sentence spanning cell of the ref-
erence chart, normalised by the inside-score of the
spanning cell from the full chart. The gradient is cal-
culated as the difference of the feature expectations
of the two charts. Clark and Curran (2004) provides
a more complete discussion of parsing with a log-
linear model and latent variables.
The full derivation chart is produced using a CYK
parser in the same manner as Chiang (2005), and has
complexity O(|e|3). We produce the reference chart
by synchronously parsing the source and reference
sentences using a variant of CYK algorithm over two
dimensions, with a time complexity of O(|e|3|f |3).
This is an instance of the ITG alignment algorithm
(Wu, 1997). This step requires the reference transla-
tion for each training instance to be contained in the
model?s hypothesis space. Achieving full coverage
implies inducing a grammar which generates all ob-
served source-target pairs, which is difficult in prac-
tise. Instead we discard the unreachable portion of
the training sample (24% in our experiments). The
proportion of discarded sentences is a function of
the grammar used. Extraction heuristics other than
the method used herein (Chiang, 2007) could allow
complete coverage (e.g., Galley et al (2004)).
3.3 Decoding
Accounting for all derivations of a given transla-
tion should benefit not only training, but also decod-
ing. Unfortunately marginalising over derivations in
decoding is NP-complete. The standard solution is
to approximate the maximum probability translation
using a single derivation (Koehn et al, 2003).
Here we approximate the sum over derivations di-
rectly using a beam search in which we produce a
beam of high probability translation sub-strings for
each cell in the parse chart. This algorithm is sim-
203
X[1,2]
 
on
X
[2,3]
 
the
X
[3,4]
 
table
X
[1,3]
 
on the
X
[2,4]
 
the table
X
[1,3]
 
on the table
X
[3,4]
 
chart
X
[2,4]
 
the chart
X
[1,3]
 
on the chart
 s
1
  
sur  
2
  
la  
3
  
table 
 
4
Figure 4. Hypergraph representation of max translation
decoding. Each chart cell must store the entire target
string generated.
ilar to the methods for decoding with a SCFG in-
tersected with an n-gram language model, which re-
quire language model contexts to be stored in each
chart cell. However, while Chiang (2005) stores an
abbreviated context composed of the n ? 1 target
words on the left and right edge of the target sub-
string, here we store the entire target string. Addi-
tionally, instead of maximising scores in each beam
cell, we sum the inside scores for each derivation
that produces a given string for that cell. When the
beam search is complete we have a list of trans-
lations in the top beam cell spanning the entire
source sentence along with their approximated in-
side derivation scores. Thus we can assign each
translation string a probability by normalising its in-
side score by the sum of the inside scores of all the
translations spanning the entire sentence.
Figure 4 illustrates the search process for the sim-
ple grammar from Table 2. Each graph node repre-
sents a hypothesis translation substring covering a
sub-span of the source string. The space of trans-
lation sub-strings is exponential in each cell?s span,
and our algorithm can only sum over a small fraction
of the possible strings. Therefore the resulting prob-
abilities are only estimates. However, as demon-
strated in Section 4, this algorithm is considerably
more effective than maximum derivation (Viterbi)
decoding.
4 Evaluation
Our model evaluation was motivated by the follow-
ing questions: (1) the effect of maximising transla-
tions rather than derivations in training and decod-
ing; (2) whether a regularised model performs better
than a maximum likelihood model; (3) how the per-
formance of our model compares with a frequency
count based hierarchical system; and (4) how trans-
lation performance scales with the number of train-
ing examples.
We performed all of our experiments on the
Europarl V2 French-English parallel corpus.3 The
training data was created by filtering the full cor-
pus for all the French sentences between five and
fifteen words in length, resulting in 170K sentence
pairs. These limits were chosen as a compromise
between experiment turnaround time and leaving
a large enough corpus to obtain indicative results.
The development and test data was taken from the
2006 NAACL and 2007 ACL workshops on ma-
chine translation, also filtered for sentence length.4
Tuning of the regularisation parameter and MERT
training of the benchmark models was performed on
dev2006, while the test set was the concatenation
of devtest2006, test2006 and test2007, amounting to
315 development and 1164 test sentences.
Here we focus on evaluating our model?s basic
ability to learn a conditional distribution from sim-
ple binary features, directly comparable to those
currently employed in frequency count models. As
such, our base model includes a single binary iden-
tity feature per-rule, equivalent to the p(e|f) param-
eters defined on each rule in standard models.
As previously noted, our model must be able to
derive the reference sentence from the source for it
to be included in training. For both our discrimina-
tive and benchmark (Hiero) we extracted our gram-
mar on the 170K sentence corpus using the approach
described in Chiang (2007), resulting in 7.8 million
rules. The discriminative model was then trained on
the training partition, however only 130K of the sen-
tences were used as the model could not produce
a derivation of the reference for the remaining sen-
tences. There were many grammar rules that the dis-
criminative model did not observe in a reference
derivation, and thus could not assign their feature a
positive weight. While the benchmark model has a
3http://www.statmt.org/europarl/
4http://www.statmt.org/wmt0{6,7}
204
Decoding
Training derivation translation
All Derivations 28.71 31.23
Single Derivation 26.70 27.32
ML (?2 =?) 25.57 25.97
Table 1. A comparison on the impact of accounting for all
derivations in training and decoding (development set).
positive count for every rule (7.8M), the discrimina-
tive model only observes 1.7M rules in actual refer-
ence derivations. Figure 1 illustrates the massive am-
biguity present in the training data, with fifteen word
sentences averaging over 70M reference derivations.
Performance is evaluated using cased BLEU4
score on the test set. Although there is no direct rela-
tionship between BLEU and likelihood, it provides
a rough measure for comparing performance.
Derivational ambiguity Table 1 shows the im-
pact of accounting for derivational ambiguity in
training and decoding.5 There are two options for
training, we could use our latent variable model and
optimise the probability of all derivations of the
reference translation, or choose a single derivation
that yields the reference and optimise its probability
alone. The second option raises the difficult question
of which one, of the thousands available, we should
choose? We use the derivation which contains the
most rules. The intuition is that small rules are likely
to appear more frequently, and thus generalise bet-
ter to a test set. In decoding we can search for the
maximum probability derivation, which is the stan-
dard practice in SMT, or for the maximum probabil-
ity translation which is what we actually want from
our model, i.e. the best translation.
The results clearly indicate the value in opti-
mising translations, rather than derivations. Max-
translation decoding for the model trained on single
derivations has only a small positive effect, while for
the latent variable model the impact is much larger.6
For example, our max-derivation model trained
on the Europarl data translates carte sur la table as
on the table card. This error in the reordering of card
(which is an acceptable translation of carte) is due
to the rule ?X? ? ?carte X 1 , X 1 card? being the
highest scoring rule for carte. This is reasonable, as
5When not explicitly stated, both here and in subsequent re-
sults, the regularisation parameter was set to one, ?2 = 1.
6We also experimented with using max-translation decoding
for standard MER trained translation models, finding that it had
a small negative impact on BLEU score.
l
l
l
l l l
l
beam width
deve
lopm
ent B
LEU 
(%)
29.0
29.5
30.0
30.5
31.0
31.5
100 1k 10k
Figure 5. The effect of the beam width (log-scale) on max-
translation decoding (development set).
carte is a noun, which in the training data, is often
observed with a trailing adjective which needs to be
reordered when translating into English. In the ex-
ample there is no adjective, but the simple hierarchi-
cal grammar cannot detect this. The max-translation
model finds a good translation card on the table.
This is due to the many rules that enforce monotone
ordering around sur la, ?X? ? ?X 1 sur, X 1 in?
?X? ? ?X 1 sur la X 2 , X 1 in the X 2 ? etc.
The scores of these many monotone rules sum to be
greater than the reordering rule, thus allowing the
model to use the weight of evidence to settle on the
correct ordering.
Having established that the search for the best
translation is effective, the question remains as to
how the beam width over partial translations affects
performance. Figure 5 shows the relationship be-
tween beam width and development BLEU. Even
with a very tight beam of 100, max-translation de-
coding outperforms maximum-derivation decoding,
and performance is increasing even at a width of
10k. In subsequent experiments we use a beam of
5k which provides a good trade-off between perfor-
mance and speed.
Regularisation Table 1 shows that the per-
formance of an unregularised maximum likeli-
hood model lags well behind the regularised max-
translation model. From this we can conclude that
the maximum likelihood model is overfitting the
training set. We suggest that is a result of the degen-
erate solutions of the conditional maximum likeli-
hood estimate, as described in DeNero et al (2006).
Here we assert that our regularised maximum a pos-
205
Grammar Rules ML MAP
(?2 =?) (?2 = 1)
?X???carte, map? 1.0 0.5
?X???carte, notice? 0.0 0.5
?X???sur, on? 1.0 1.0
?X???la, the? 1.0 1.0
?X???table, table? 1.0 0.5
?X???table, chart? 0.0 0.5
?X???carte sur, notice on? 1.0 0.5
?X???carte sur, map on? 0.0 0.5
?X???sur la, on the? 1.0 1.0
?X???la table, the table? 0.0 0.5
?X???la table, the chart? 1.0 0.5
Training data:
carte sur la table? map on the table
carte sur la table? notice on the chart
Table 2. Comparison of the susceptibility to degenerate
solutions for a ML and MAP optimised model, using a sim-
ple grammar with one parameter per rule and a monotone
glue rule: ?X? ? ?X 1 X 2 , X 1X 2 ?
teriori model avoids such solutions.
This is illustrated in Table 2, which shows the
conditional probabilities for rules, obtained by lo-
cally normalising the rule feature weights for a sim-
ple grammar extracted from the ambiguous pair of
sentences presented in DeNero et al (2006). The
first column of conditional probabilities corresponds
to a maximum likelihood estimate, i.e., without reg-
ularisation. As expected, the model finds a degener-
ate solution in which overlapping rules are exploited
in order to minimise the entropy of the rule trans-
lation distributions. The second column shows the
solution found by our model when regularised by a
Gaussian prior with unit variance. Here we see that
the model finds the desired solution in which the true
ambiguity of the translation rules is preserved. The
intuition is that in order to find a degenerate solu-
tion, dispreferred rules must be given large negative
weights. However the prior penalises large weights,
and therefore the best strategy for the regularised
model is to evenly distribute probability mass.
Translation comparison Having demonstrated
that accounting for derivational ambiguity leads to
improvements for our discriminative model, we now
place the performance of our system in the context
of the standard approach to hierarchical translation.
To do this we use our own implementation of Hiero
(Chiang, 2007), with the same grammar but with the
traditional generative feature set trained in a linear
model with minimum BLEU training. The feature
set includes: a trigram language model (lm) trained
System Test (BLEU)
Discriminative max-derivation 25.78
Hiero (pd, gr, rc, wc) 26.48
Discriminative max-translation 27.72
Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc) 28.14
Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc, lm) 32.00
Table 3. Test set performance compared with a standard
Hiero system
on the English side of the unfiltered Europarl corpus;
direct and reverse translation scores estimated as rel-
ative frequencies (pd, pr); lexical translation scores
(plexd , p
lex
r ), a binary flag for the glue rule which al-
lows the model to (dis)favour monotone translation
(gr); and rule and target word counts (rc, wc).
Table 3 shows the results of our system on the
test set. Firstly we show the relative scores of our
model against Hiero without using reverse transla-
tion or lexical features.7 This allows us to directly
study the differences between the two translation
models without the added complication of the other
features. As well as both modelling the same dis-
tribution, when our model is trained with a single
parameter per-rule these systems have the same pa-
rameter space, differing only in the manner of esti-
mation.
Additionally we show the scores achieved by
MERT training the full set of features for Hiero, with
and without a language model.8 We provide these
results for reference. To compare our model directly
with these systems we would need to incorporate ad-
ditional features and a language model, work which
we have left for a later date.
The relative scores confirm that our model, with
its minimalist feature set, achieves comparable per-
formance to the standard feature set without the lan-
guage model. This is encouraging as our model was
trained to optimise likelihood rather than BLEU, yet
it is still competitive on that metric. As expected,
the language model makes a significant difference to
BLEU, however we believe that this effect is orthog-
onal to the choice of base translation model, thus we
would expect a similar gain when integrating a lan-
guage model into the discriminative system.
An informal comparison of the outputs on the de-
velopment set, presented in Table 4, suggests that the
7Although the most direct comparison for the discriminative
model would be with pd model alone, omitting the gr, rc and
wc features and MERT training produces poor translations.
8Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc, lm) represents state-
of-the-art performance on this training/testing set.
206
S: C?est pourquoi nous souhaitons que l?affaire nous soit ren-
voye?e.
R: We therefore want the matter re-referred to ourselves.
D: That is why we want the that matters we to be referred
back.
T: That is why we would like the matter to be referred back.
H: That is why we wish that the matter we be referred back.
S: Par contre, la transposition dans les E?tats membres reste
trop lente.
R: But implementation by the Member States has still been
too slow.
D: However, it is implemented in the Member States is still
too slow.
T: However, the implementation measures in Member States
remains too slow.
H: In against, transposition in the Member States remains too
slow.
S: Aussi, je conside`re qu?il reste e?norme?ment a` faire dans ce
domaine.
R: I therefore consider that there is an incredible amount still
to do in this area.
D: So I think remains a lot to be done in this field.
T: So I think there is still much to be done in this area.
H: Therefore, I think it remains a vast amount to do in this
area.
Table 4. Example output produced by the max-
derivation (D), max-translation (T) decoding algorithms
and Hiero(pd, pr, plexd , p
lex
r , gr, rc, wc) (H) models, relative
to the source (S) and reference (R).
translation optimising discriminative model more
often produces quite fluent translations, yet not in
ways that would lead to an increase in BLEU score.9
This could be considered a side-effect of optimising
likelihood rather than BLEU.
Scaling In Figure 6 we plot the scaling charac-
teristics of our models. The systems shown in the
graph use the full grammar extracted on the 170k
sentence corpus. The number of sentences upon
which the iterative training algorithm is used to esti-
mate the parameters is varied from 10k to the max-
imum 130K for which our model can reproduce the
reference translation. As expected, the more data
used to train the system, the better the performance.
However, as the performance is still increasing sig-
nificantly when all the parseable sentences are used,
it is clear that the system?s performance is suffering
from the large number (40k) of sentences that are
discarded before training.
5 Discussion and Further Work
We have shown that explicitly accounting for com-
peting derivations yields translation improvements.
9Hiero was MERT trained on this set and has a 2% higher
BLEU score compared to the discriminative model.
l
l
l
l
l
l
training sentences
deve
lopm
ent 
BLE
U (%
)
26
27
28
29
30
31
10k 25k 50k 75k 100k 130k
Figure 6. Learning curve showing that the model contin-
ues to improve as we increase the number of training sen-
tences (development set)
Our model avoids the estimation biases associated
with heuristic frequency count approaches and uses
standard regularisation techniques to avoid degener-
ate maximum likelihood solutions.
Having demonstrated the efficacy of our model
with very simple features, the logical next step is
to investigate more expressive features. Promising
features might include those over source side re-
ordering rules (Wang et al, 2007) or source con-
text features (Carpuat and Wu, 2007). Rule fre-
quency features extracted from large training cor-
pora would help the model to overcome the issue of
unreachable reference sentences. Such approaches
have been shown to be effective in log-linear word-
alignment models where only a small supervised
corpus is available (Blunsom and Cohn, 2006).
Finally, while in this paper we have focussed on
the science of discriminative machine translation,
we believe that with suitable engineering this model
will advance the state-of-the-art. To do so would
require integrating a language model feature into
the max-translation decoding algorithm. The use of
richer, more linguistic grammars (e.g., Galley et al
(2004)) may also improve the system.
Acknowledgements
The authors acknowledge the support of the EPSRC
(Blunsom & Osborne, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
207
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguis-
tics (COLING/ACL-2006), pages 65?72, Sydney, Aus-
tralia, July.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of the 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2007), pages 61?72, Prague, Czech Republic.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the 43rd
Annual Meeting of the ACL (ACL-2005), pages 263?
270, Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proc. of the
42nd Annual Meeting of the ACL (ACL-2004), pages
103?110, Barcelona, Spain.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics, 33(4).
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on Statistical Machine Translation, pages
31?38, New York City, June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc. of
the 4th International Conference on Human Language
Technology Research and 5th Annual Meeting of the
NAACL (HLT-NAACL 2004), Boston, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of the 44th Annual Meeting of the ACL and 21st In-
ternational Conference on Computational Linguistics
(COLING/ACL-2006), pages 961?968, Sydney, Aus-
tralia, July.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proc. of the 7th International
Conference on Human Language Technology Research
and 8th Annual Meeting of the NAACL (HLT-NAACL
2007), pages 57?64, Rochester, USA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc. of
the 3rd International Conference on Human Language
Technology Research and 4th Annual Meeting of the
NAACL (HLT-NAACL 2003), pages 81?88, Edmonton,
Canada, May.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465?488.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of the 44th An-
nual Meeting of the ACL and 21st International Con-
ference on Computational Linguistics (COLING/ACL-
2006), pages 761?768, Sydney, Australia, July.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
the 6th Conference on Natural Language Learning
(CoNLL-2002), pages 49?55, Taipei, Taiwan, August.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st An-
nual Meeting of the ACL (ACL-2003), pages 160?167,
Sapporo, Japan.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Discrim-
inative log-linear grammars with latent variables. In
Advances in Neural Information Processing Systems
20 (NIPS), Vancouver, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of the
3rd International Conference on Human Language
Technology Research and 4th Annual Meeting of the
NAACL (HLT-NAACL 2003), pages 134?141, Edmon-
ton, Canada.
Christoph Tillmann and Tong Zhang. 2007. A block bi-
gram prediction model for statistical machine transla-
tion. ACM Transactions Speech Language Processing,
4(3):6.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proc. of the 2007 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2007), pages 737?745, Prague, Czech Re-
public.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 764?773, Prague,
Czech Republic.
Benjamin Wellington, Joseph Turian, Chris Pike, and
I. Dan Melamed. 2006. Scalable purely-
discriminative training for word and tree transducers.
In Proc. of the 7th Biennial Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Boston, USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
208
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782?790,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Gibbs Sampler for Phrasal Synchronous Grammar Induction
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Trevor Cohn?
tcohn@inf.ed.ac.uk
Miles Osborne?
miles@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
We present a phrasal synchronous gram-
mar model of translational equivalence.
Unlike previous approaches, we do not
resort to heuristics or constraints from
a word-alignment model, but instead
directly induce a synchronous grammar
from parallel sentence-aligned corpora.
We use a hierarchical Bayesian prior
to bias towards compact grammars with
small translation units. Inference is per-
formed using a novel Gibbs sampler
over synchronous derivations. This sam-
pler side-steps the intractability issues of
previous models which required inference
over derivation forests. Instead each sam-
pling iteration is highly efficient, allowing
the model to be applied to larger transla-
tion corpora than previous approaches.
1 Introduction
The field of machine translation has seen many
advances in recent years, most notably the shift
from word-based (Brown et al, 1993) to phrase-
based models which use token n-grams as trans-
lation units (Koehn et al, 2003). Although very
few researchers use word-based models for trans-
lation per se, such models are still widely used in
the training of phrase-based models. These word-
based models are used to find the latent word-
alignments between bilingual sentence pairs, from
which a weighted string transducer can be induced
(either finite state (Koehn et al, 2003) or syn-
chronous context free grammar (Chiang, 2007)).
Although wide-spread, the disconnect between the
translation model and the alignment model is arti-
ficial and clearly undesirable. Word-based mod-
els are incapable of learning translational equiv-
alences between non-compositional phrasal units,
while the algorithms used for inducing weighted
transducers from word-alignments are based on
heuristics with little theoretical justification. A
model which can fulfil both roles would address
both the practical and theoretical short-comings of
the machine translation pipeline.
The machine translation literature is littered
with various attempts to learn a phrase-based
string transducer directly from aligned sentence
pairs, doing away with the separate word align-
ment step (Marcu and Wong, 2002; Cherry and
Lin, 2007; Zhang et al, 2008b; Blunsom et al,
2008). Unfortunately none of these approaches
resulted in an unqualified success, due largely
to intractable estimation. Large training sets with
hundreds of thousands of sentence pairs are com-
mon in machine translation, leading to a parameter
space of billions or even trillions of possible bilin-
gual phrase-pairs. Moreover, the inference proce-
dure for each sentence pair is non-trivial, prov-
ing NP-complete for learning phrase based models
(DeNero and Klein, 2008) or a high order poly-
nomial (O(|f |3|e|3))1 for a sub-class of weighted
synchronous context free grammars (Wu, 1997).
Consequently, for such models both the param-
eterisation and approximate inference techniques
are fundamental to their success.
In this paper we present a novel SCFG transla-
tion model using a non-parametric Bayesian for-
mulation. The model includes priors to impose a
bias towards small grammars with few rules, each
of which is as simple as possible (e.g., terminal
productions consisting of short phrase pairs). This
explicitly avoids the degenerate solutions of max-
imum likelihood estimation (DeNero et al, 2006),
without resort to the heuristic estimator of Koehn
et al (2003). We develop a novel Gibbs sampler
to perform inference over the latent synchronous
derivation trees for our training instances. The
sampler reasons over the infinite space of possi-
ble translation units without recourse to arbitrary
restrictions (e.g., constraints drawn from a word-
alignment (Cherry and Lin, 2007; Zhang et al,
2008b) or a grammar fixed a priori (Blunsom et al,
1f and e are the input and output sentences respectively.
782
2008)). The sampler performs local edit operations
to nodes in the synchronous trees, each of which
is very fast, leading to a highly efficient inference
technique. This allows us to train the model on
large corpora without resort to punitive length lim-
its, unlike previous approaches which were only
applied to small data sets with short sentences.
This paper is structured as follows: In Sec-
tion 3 we argue for the use of efficient sam-
pling techniques over SCFGs as an effective solu-
tion to the modelling and scaling problems of
previous approaches. We describe our Bayesian
SCFG model in Section 4 and a Gibbs sampler
to explore its posterior. We apply this sampler
to build phrase-based and hierarchical translation
models and evaluate their performance on small
and large corpora.
2 Synchronous context free grammar
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) generalizes context-
free grammars to generate strings concurrently in
two (or more) languages. A string pair is gener-
ated by applying a series of paired rewrite rules
of the form, X ? ?e, f ,a?, where X is a non-
terminal, e and f are strings of terminals and non-
terminals and a specifies a one-to-one alignment
between non-terminals in e and f . In the context of
SMT, by assigning the source and target languages
to the respective sides of a probabilistic SCFG it
is possible to describe translation as the process
of parsing the source sentence, which induces a
parallel tree structure and translation in the tar-
get language (Chiang, 2007). Figure 1 shows an
example derivation for Japanese to English trans-
lation using an SCFG. For efficiency reasons we
only consider binary or ternary branching rules
and don?t allow rules to mix terminals and non-
terminals. This allows our sampler to more effi-
ciently explore the space of grammars (Section
4.2), however more expressive grammars would be
a straightforward extension of our model.
3 Related work
Most machine translation systems adopt the
approach of Koehn et al (2003) for ?training?
a phrase-based translation model.2 This method
starts with a word-alignment, usually the latent
state of an unsupervised word-based aligner such
2We include grammar based transducers, such as Chiang
(2007) and Marcu et al (2006), in our definition of phrase-
based models.
Grammar fragment:
X ? ?X
1
X
2
X
3
, X
1
X
3
X
2
?
X ? ?John-ga, John?
X ? ?ringo-o, an apple?
X ? ?tabeta, ate?
Sample derivation:
?S
1
,S
1
? ? ?X
2
, X
2
?
? ?X
3
X
4
X
5
, X
3
X
5
X
4
?
? ?John-ga X
4
X
5
, John X
5
X
4
?
? ?John-ga ringo-o X
5
, John X
5
an apple?
? ?John-ga ringo-o tabeta, John ate an apple?
Figure 1: A fragment of an SCFG with a ternary
non-terminal expansion and three terminal rules.
as GIZA++. Various heuristics are used to com-
bine source-to-target and target-to-source align-
ments, after which a further heuristic is used to
read off phrase pairs which are ?consistent? with
the alignment. Although efficient, the sheer num-
ber of somewhat arbitrary heuristics makes this
approach overly complicated.
A number of authors have proposed alterna-
tive techniques for directly inducing phrase-based
translation models from sentence aligned data.
Marcu and Wong (2002) proposed a phrase-based
alignment model which suffered from a massive
parameter space and intractable inference using
expectation maximisation. Taking a different tack,
DeNero et al (2008) presented an interesting new
model with inference courtesy of a Gibbs sampler,
which was better able to explore the full space of
phrase translations. However, the efficacy of this
model is unclear due to the small-scale experi-
ments and the short sampling runs. In this work we
also propose a Gibbs sampler but apply it to the
polynomial space of derivation trees, rather than
the exponential space of the DeNero et al (2008)
model. The restrictions imposed by our tree struc-
ture make sampling considerably more efficient
for long sentences.
Following the broad shift in the field from finite
state transducers to grammar transducers (Chiang,
2007), recent approaches to phrase-based align-
ment have used synchronous grammar formalisms
permitting polynomial time inference (Wu, 1997;
783
Cherry and Lin, 2007; Zhang et al, 2008b; Blun-
som et al, 2008). However this asymptotic time
complexity is of high enough order (O(|f |3|e|3))
that inference is impractical for real translation
data. Proposed solutions to this problem include
imposing sentence length limits, using small train-
ing corpora and constraining the search space
using a word-alignment model or parse tree. None
of these limitations are particularly desirable as
they bias inference. As a result phrase-based align-
ment models are not yet practical for the wider
machine translation community.
4 Model
Our aim is to induce a grammar from a train-
ing set of sentence pairs. We use Bayes? rule
to reason under the posterior over grammars,
P (g|x) ? P (x|g)P (g), where g is a weighted
SCFG grammar and x is our training corpus. The
likelihood term, P (x|g), is the probability of the
training sentence pairs under the grammar, while
the prior term, P (g), describes our initial expec-
tations about what consitutes a plausible gram-
mar. Specifically we incorporate priors encoding
our preference for a briefer and more succinct
grammar, namely that: (a) the grammar should be
small, with few rules rewriting each non-terminal;
and (b) terminal rules which specify phrasal trans-
lation correspondence should be small, with few
symbols on their right hand side.
Further, Bayesian non-parametrics allow the
capacity of the model to grow with the data.
Thereby we avoid imposing hard limits on the
grammar (and the thorny problem of model selec-
tion), but instead allow the model to find a gram-
mar appropriately sized for its training data.
4.1 Non-parametric form
Our Bayesian model of SCFG derivations resem-
bles that of Blunsom et al (2008). Given a gram-
mar, each sentence is generated as follows. Start-
ing with a root non-terminal (z1), rewrite each
frontier non-terminal (zi) using a rule chosen from
our grammar expanding zi. Repeat until there are
no remaining frontier non-terminals. This gives
rise to the following derivation probability:
p(d) = p(z1)
?
ri?d
p(ri|zi)
where the derivation is a sequence of rules d =
(r1, . . . , rn), and zi denotes the root node of ri.
We allow two types of rules: non-terminal and
terminal expansions. The former rewrites a non-
terminal symbol as a string of two or three non-
terminals along with an alignment, specifying
the corresponding ordering of the child trees in
the source and target language. Terminal expan-
sions rewrite a non-terminal as a pair of terminal
n-grams, representing a phrasal translation pair,
where either but not both may be empty.
Each rule in the grammar, ri, is generated from
its root symbol, zi, by first choosing a rule type
ti ? {TERM, NON-TERM} from a Bernoulli distribu-
tion, ri ? Bernoulli(?). We treat ? as a random
variable with its own prior, ? ? Beta(?R, ?R) and
integrate out the parameters, ?. This results in the
following conditional probability for ti:
p(ti|r?i, zi, ?R) =
n?iti,zi + ?
R
n?i?,zi + 2?R
where n?iri,zi is the number of times ri has been
used to rewrite zi in the set of all other rules, r?i,
and n?i?,zi =
?
r n
?i
r,zi is the total count of rewriting
zi. The Dirichlet (and thus Beta) distribution are
exchangeable, meaning that any permutation of its
events are equiprobable. This allows us to reason
about each event given previous and subsequent
events (i.e., treat each item as the ?last?.)
When ti = NON-TERM, we generate a binary
or ternary non-terminal production. The non-
terminal sequence and alignment are drawn from
(z, a) ? ?Nzi and, as before, we define a prior over
the parameters, ?Nzi ? Dirichlet(?
T ), and inte-
grate out ?Nzi . This results in the conditional prob-
ability:
p(ri|ti = NON-TERM, r?i, zi, ?N ) =
nN,?iri,zi + ?
N
nN,?i?,zi + |N |?N
where nN,?iri,zi is the count of rewriting zi with non-
terminal rule ri, n
N,?i
?,zi the total count over all non-
terminal rules and |N | is the number of unique
non-terminal rules.
For terminal productions (ti = TERM) we first
decide whether to generate a phrase in both lan-
guages or in one language only, according to a
fixed probability pnull.3 Contingent on this deci-
sion, the terminal strings are then drawn from
3To discourage null alignments, we used pnull = 10?10
for this value in the experiments we report below.
784
either ?Pzi for phrase pairs or ?
null for single lan-
guage phrases. We choose Dirichlet process (DP)
priors for these parameters:
?Pzi ? DP(?
P , PP1 )
?nullzi ? DP(?
null, Pnull1 )
where the base distributions, PP1 and P
null
1 , range
over phrase pairs or monolingual phrases in either
language, respectively.
The most important choice for our model is
the priors on the parameters of these terminal
distributions. Phrasal SCFG models are subject
to a degenerate maximum likelihood solution in
which all probability mass is placed on long, or
whole sentence, phrase translations (DeNero et al,
2006). Therefore, careful consideration must be
given when specifying the P1 distribution on ter-
minals in order to counter this behavior.
To construct a prior over string pairs, first we
define the probability of a monolingual string (s):
PX0 (s) = PPoisson(|s|; 1)?
1
V |s|X
where the PPoisson(k; 1) is the probability under a
Poisson distribution of length k given an expected
length of 1, while VX is the vocabulary size of
language X . This distribution has a strong bias
towards short strings. In particular note that gener-
ally a string of length k will be less probable than
two of length k2 , a property very useful for finding
?minimal? translation units. This contrasts with a
geometric distribution in which a string of length
k will be more probable than its segmentations.
We define Pnull1 as the string probability of the
non-null part of the rule:
Pnull1 (z ? ?e, f?) =
{ 1
2P
E
0 (e) if |f | = 0
1
2P
F
0 (f) if |e| = 0
The terminal translation phrase pair distribution
is a hierarchical Dirichlet Process in which each
phrase are independently distributed according to
DPs:4
PP1 (z ? ?e, f?) = ?
E
z (e)? ?
F
z (f)
?Ez ? DP(?
PE , PE0 )
4This prior is similar to one used by DeNero et al (2008),
who used the expected table count approximation presented
in Goldwater et al (2006). However, Goldwater et al (2006)
contains two major errors: omitting P0, and using the trun-
cated Taylor series expansion (Antoniak, 1974) which fails
for small ?P0 values common in these models. In this work
we track table counts directly.
and ?Fz is defined analogously. This prior encour-
ages frequent phrases to participate in many differ-
ent translation pairs. Moreover, as longer strings
are likely to be less frequent in the corpus this has
a tendency to discourage long translation units.
4.2 A Gibbs sampler for derivations
Markov chain Monte Carlo sampling allows us to
perform inference for the model described in 4.1
without restricting the infinite space of possible
translation rules. To do this we need a method for
sampling a derivation for a given sentence pair
from p(d|d?). One possible approach would be
to first build a packed chart representation of the
derivation forest, calculate the inside probabilities
of all cells in this chart, and then sample deriva-
tions top-down according to their inside probabil-
ities (analogous to monolingual parse tree sam-
pling described in Johnson et al (2007)). A prob-
lem with this approach is that building the deriva-
tion forest would take O(|f |3|e|3) time, which
would be impractical for long sentences.
Instead we develop a collapsed Gibbs sam-
pler (Teh et al, 2006) which draws new sam-
ples by making local changes to the derivations
used in a previous sample. After a period of burn
in, the derivations produced by the sampler will
be drawn from the posterior distribution, p(d|x).
The advantage of this algorithm is that we only
store the current derivation for each training sen-
tence pair (together these constitute the state of
the sampler), but never need to reason over deriva-
tion forests. By integrating over (collapsing) the
parameters we only store counts of rules used
in the current sampled set of derivations, thereby
avoiding explicitly representing the possibly infi-
nite space of translation pairs.
We define two operators for our Gibbs sam-
pler, each of which re-samples local derivation
structures. Figures 2 and 4 illustrate the permu-
tations these operators make to derivation trees.
The omitted tree structure in these figures denotes
the Markov blanket of the operator: the structure
which is held constant when enumerating the pos-
sible outcomes for an operator.
The Split/Join operator iterates through the
positions between each source word sampling
whether a terminal boundary should exist at
that position (Figure 2). If the source position
785
... ... ...
... ...
... ...
... ...
Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The
dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a
solid line is a null alignment.
...
......
...
...
......
...
Figure 4: Rule insert/delete sampler. A pair of
adjacent nodes in a ternary rule can be re-parented
as a binary rule, or vice-versa.
falls between two existing terminals whose tar-
get phrases are adjacent, then any new target seg-
mentation within those target phrases can be sam-
pled, including null alignments. If the two exist-
ing terminals also share the same parent, then any
possible re-ordering is also a valid outcome, as
is removing the terminal boundary to form a sin-
gle phrase pair. Otherwise, if the visited boundary
point falls within an existing terminal, then all tar-
get split and re-orderings are possible outcomes.
The probability for each of these configurations
is evaluated (see Figure 3) from which the new
configuration is sampled.
While the first operator is theoretically capa-
ble of exploring the entire derivation forest (by
flattening the tree into a single phrase and then
splitting), the series of moves required would be
highly improbable. To allow for faster mixing we
employ the Insert/Delete operator which adds and
deletes the parent non-terminal of a pair of adja-
cent nodes. This is illustrated in Figure 4. The
update equations are analogous to those used for
the Split/Join operator in Figure 3. In order for this
operator to be effective we need to allow greater
than binary branching nodes, otherwise deleting a
nodes would require sampling from a much larger
set of outcomes. Hence our adoption of a ternary
branching grammar. Although such a grammar
would be very inefficient for a dynamic program-
ming algorithm, it allows our sampler to permute
the internal structure of the trees more easily.
4.3 Hyperparameter Inference
Our model is parameterised by a vector of hyper-
parameters, ? = (?R, ?N , ?P , ?PE , ?PF , ?null),
which control the sparsity assumption over var-
ious model parameters. We could optimise each
concentration parameter on the training corpus by
hand, however this would be quite an onerous task.
Instead we perform inference over the hyperpa-
rameters following Goldwater and Griffiths (2007)
by defining a vague gamma prior on each con-
centration parameter, ?x ? Gamma(10?4, 104).
This hyper-prior is relatively benign, allowing the
model to consider a wide range of values for
the hyperparameter. We sample a new value for
each ?x using a log-normal distribution with mean
?x and variance 0.3, which is then accepted into
the distribution p(?x|d, ??) using the Metropolis-
Hastings algorithm. Unlike the Gibbs updates, this
calculation cannot be distributed over a cluster
(see Section 4.4) and thus is very costly. Therefore
for small corpora we re-sample the hyperparame-
ter after every pass through the corpus, for larger
experiments we only re-sample every 20 passes.
4.4 A Distributed approximation
While employing a collapsed Gibbs sampler
allows us to efficiently perform inference over the
786
p(JOIN) ? p(ti = TERM|zi, r?)? p(ri = (zi ? ?e, f?)|zi, r?) (1)
p(SPLIT) ? p(ti = NON-TERM|zi, r?)? p(ri = (zi ? ?zl, zr, ai?)|zi, r
?) (2)
? p(tl = TERM|ti, zi, r
?)? p(rl = (zl ? ?el, fl?)|zl, r
?)
? p(tr = TERM|ti, tl, zi, r
?)? p(rr = (zr ? ?er, fr?)|zl, r
? ? (zl ? ?el, fl?))
Figure 3: Gibbs sampling equations for the competing configurations of the Split/Join sampler, shown in
Figure 2. Eq. (1) corresponds to the top-left configuration, and (2) the remaining configurations where the
choice of el, fl, er, fr and ai specifies the string segmentation and the alignment (monotone or reordered).
massive space of possible grammars, it induces
dependencies between all the sentences in the
training corpus. These dependencies make it diffi-
cult to scale our approach to larger corpora by dis-
tributing it across a number of processors. Recent
work (Newman et al, 2007; Asuncion et al, 2008)
suggests that good practical parallel performance
can be achieved by having multiple processors
independently sample disjoint subsets of the cor-
pus. Each process maintains a set of rule counts for
the entire corpus and communicates the changes
it has made to its section of the corpus only
after sampling every sentence in that section. In
this way each process is sampling according to
a slightly ?out-of-date? distribution. However, as
we confirm in Section 5 the performance of this
approximation closely follows the exact collapsed
Gibbs sampler.
4.5 Extracting a translation model
Although we could use our model directly as a
decoder to perform translation, its simple hier-
archical reordering parameterisation is too weak
to be effective in this mode. Instead we use our
sampler to sample a distribution over translation
models for state-of-the-art phrase based (Moses)
and hierarchical (Hiero) decoders (Koehn et al,
2007; Chiang, 2007). Each sample from our model
defines a hierarchical alignment on which we can
apply the standard extraction heuristics of these
models. By extracting from a sequence of samples
we can directly infer a distribution over phrase
tables or Hiero grammars.
5 Evaluation
Our evaluation aims to determine whether the
phrase/SCFG rule distributions created by sam-
pling from the model described in Section 4
impact upon the performance of state-of-the-
art translation systems. We conduct experiments
translating both Chinese (high reordering) and
Arabic (low reordering) into English. We use the
GIZA++ implementation of IBM Model 4 (Brown
et al, 1993; Och and Ney, 2003) coupled with the
phrase extraction heuristics of Koehn et al (2003)
and the SCFG rule extraction heuristics of Chiang
(2007) as our benchmark. All the SCFG models
employ a single X non-terminal, we leave experi-
ments with multiple non-terminals to future work.
Our hypothesis is that our grammar based
induction of translation units should benefit lan-
guage pairs with significant reordering more than
those with less. While for mostly monotone trans-
lation pairs, such as Arabic-English, the bench-
mark GIZA++-based system is well suited due to
its strong monotone bias (the sequential Markov
model and diagonal growing heuristic).
We conduct experiments on both small and
large corpora to allow a range of alignment quali-
ties and also to verify the effectiveness of our dis-
tributed approximation of the Bayesian inference.
The samplers are initialised with trees created
from GIZA++ Model 4 alignments, altered such
that they are consistent with our ternary grammar.
This is achieved by using the factorisation algo-
rithm of Zhang et al (2008a) to first create ini-
tial trees. Where these factored trees contain nodes
with mixed terminals and non-terminals, or more
than three non-terminals, we discard alignment
points until the node factorises correctly. As the
alignments contain many such non-factorisable
nodes, these trees are of poor quality. However,
all samplers used in these experiments are first
?burnt-in? for 1000 full passes through the data.
This allows the sampler to diverge from its ini-
tialisation condition, and thus gives us confidence
that subsequent samples will be drawn from the
posterior. An expectation over phrase tables and
Hiero grammars is built from every 50th sample
after the burn-in, up until the 1500th sample.
We evaluate the translation models using IBM
BLEU (Papineni et al, 2001). Table 1 lists the
statistics of the corpora used in these experiments.
787
IWSLT NIST
English?Chinese English?Chinese English?Arabic
Sentences 40k 300k 290k
Segs./Words 380k 340k 11.0M 8.6M 9.3M 8.5M
Av. Sent. Len. 9 8 36 28 32 29
Longest Sent. 75 64 80 80 80 80
Table 1: Corpora statistics.
System Test 05
Moses (Heuristic) 47.3
Moses (Bayes SCFG) 49.6
Hiero (Heuristic) 48.3
Hiero (Bayes SCFG) 51.8
Table 2: IWSLT Chinese to English translation.
5.1 Small corpus
Firstly we evaluate models trained on a small
Chinese-English corpus using a Gibbs sampler on
a single CPU. This corpus consists of transcribed
utterances made available for the IWSLT work-
shop (Eck and Hori, 2005). The sparse counts and
high reordering for this corpus means the GIZA++
model produces very poor alignments.
Table 2 shows the results for the benchmark
Moses and Hiero systems on this corpus using
both the heuristic phrase estimation, and our pro-
posed Bayesian SCFG model. We can see that
our model has a slight advantage. When we look
at the grammars extracted by the two models we
note that the SCFG model creates considerably
more translation rules. Normally this would sug-
gest the alignments of the SCFG model are a lot
sparser (more unaligned tokens) than those of the
heuristic, however this is not the case. The pro-
jected SCFG derivations actually produce more
alignment points. However these alignments are
much more locally consistent, containing fewer
spurious off-diagonal alignments, than the heuris-
tic (see Figure 5), and thus produce far more valid
phrases/rules.
5.2 Larger corpora
We now test our model?s performance on a larger
corpus, representing a realistic SMT experiment
with millions of words and long sentences. The
Chinese-English training data consists of the FBIS
corpus (LDC2003E14) and the first 100k sen-
tences from the Sinorama corpus (LDC2005E47).
The Arabic-English training data consists of
the eTIRR corpus (LDC2004E72), the Arabic
l
l
l
l
l
l
l
l
l l
l l
Number of Sampling Passes
Negative 
Log?Post
erior
l l
l
l
l
l
l
l l
l l
476
478
480
482
484
486
488
490
20 40 60 80 100 120 140 160 180 200 220 240
single (exact)distributed
Figure 6: The posterior for the single CPU sampler
and distributed approximation are roughly equiva-
lent over a sampling run.
news corpus (LDC2004T17), the Ummah cor-
pus (LDC2004T18), and the sentences with confi-
dence c > 0.995 in the ISI automatically extracted
web parallel corpus (LDC2006T02). The Chinese
text was segmented with a CRF-based Chinese
segmenter optimized for MT (Chang et al, 2008).
The Arabic text was preprocessed according to the
D2 scheme of Habash and Sadat (2006), which
was identified as optimal for corpora this size. The
parameters of the NIST systems were tuned using
Och?s algorithm to maximize BLEU on the MT02
test set (Och, 2003).
To evaluate whether the approximate distributed
inference algorithm described in Section 4.4 is
effective, we compare the posterior probability of
the training corpus when using a single machine,
and when the inference is distributed on an eight
core machine. Figure 6 plots the mean posterior
and standard error for five independent runs for
each scenario. Both sets of runs performed hyper-
parameter inference every twenty passes through
the data. It is clear from the training curves that the
distributed approximation tracks the corpus prob-
ability of the correct sampler sufficiently closely.
This concurs with the findings of Newman et al
788
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(a) Giza++
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(b) Gibbs
Figure 5: Alignment example. The synchronous tree structure is shown for (b) using brackets to indicate
constituent spans; these are omitted for single token constituents. The right alignment is roughly correct,
except that ?of? and ?an? should be left unaligned (? ?to be? is missing from the English translation).
System MT03 MT04 MT05
Moses (Heuristic) 26.2 30.0 25.3
Moses (Bayes SCFG) 26.4 30.2 25.8
Hiero (Heuristic) 26.4 30.8 25.4
Hiero (Bayes SCFG) 26.7 30.9 26.0
Table 3: NIST Chinese to English translation.
System MT03 MT04 MT05
Moses (Heuristic) 48.5 43.9 49.2
Moses (Bayes SCFG) 48.5 43.5 48.7
Hiero (Heuristic) 48.1 43.5 48.4
Hiero (Bayes SCFG) 48.4 43.4 47.7
Table 4: NIST Arabic to English translation.
(2007) who also observed very little empirical dif-
ference between the sampler and its distributed
approximation.
Tables 3 and 4 show the result on the two NIST
corpora when running the distributed sampler on
a single 8-core machine.5 These scores tally with
our initial hypothesis: that the hierarchical struc-
ture of our model suits languages that exhibit less
monotone reordering.
Figure 5 shows the projected alignment of a
headline from the thousandth sample on the NIST
Chinese data set. The effect of the grammar based
alignment can clearly be seen. Where the combi-
nation of GIZA++ and the heuristics creates out-
lier alignments that impede rule extraction, the
SCFG imposes a more rigid hierarchical struc-
ture on the alignments. We hypothesise that this
property may be particularly useful for syntac-
tic translation models which often have difficulty
5Producing the 1.5K samples for each experiment took
approximately one day.
with inconsistent word alignments not correspond-
ing to syntactic structure.
The combined evidence of the ability of our
Gibbs sampler to improve posterior likelihood
(Figure 6) and our translation experiments demon-
strate that we have developed a scalable and effec-
tive method for performing inference over phrasal
SCFG, without compromising the strong theoreti-
cal underpinnings of our model.
6 Discussion and Conclusion
We have presented a Bayesian model of SCFG
induction capable of capturing phrasal units of
translational equivalence. Our novel Gibbs sam-
pler over synchronous derivation trees can effi-
ciently draw samples from the posterior, overcom-
ing the limitations of previous models when deal-
ing with long sentences. This avoids explicitly
representing the full derivation forest required by
dynamic programming approaches, and thus we
are able to perform inference without resorting to
heuristic restrictions on the model.
Initial experiments suggest that this model per-
forms well on languages for which the monotone
bias of existing alignment and heuristic phrase
extraction approaches fail. These results open the
way for the development of more sophisticated
models employing grammars capable of capturing
a wide range of translation phenomena. In future
we envision it will be possible to use the tech-
niques developed here to directly induce gram-
mars which match state-of-the-art decoders, such
as Hiero grammars or tree substitution grammars
of the form used by Galley et al (2004).
789
Acknowledgements
The authors acknowledge the support of
the EPSRC (Blunsom & Osborne, grant
EP/D074959/1; Cohn, grant GR/T04557/01)
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001 (Dyer).
References
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
A. Asuncion, P. Smyth, M. Welling. 2008. Asynchronous
distributed learning of topic models. In NIPS. MIT Press.
P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian syn-
chronous grammar induction. In Proceedings of NIPS 21,
Vancouver, Canada.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, R. L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics,
19(2):263?311.
P.-C. Chang, D. Jurafsky, C. D. Manning. 2008. Optimizing
Chinese word segmentation for machine translation per-
formance. In Proc. of the Third Workshop on Machine
Translation, Prague, Czech Republic.
C. Cherry, D. Lin. 2007. Inversion transduction grammar for
joint phrasal translation modeling. In Proc. of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST 2007), Rochester, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. DeNero, D. Klein. 2008. The complexity of phrase align-
ment problems. In Proceedings of ACL-08: HLT, Short
Papers, 25?28, Columbus, Ohio. Association for Compu-
tational Linguistics.
J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006. Why gener-
ative phrase models underperform surface heuristics. In
Proc. of the HLT-NAACL 2006 Workshop on Statistical
Machine Translation, 31?38, New York City.
J. DeNero, A. Bouchard-Co?te?, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
M. Eck, C. Hori. 2005. Overview of the IWSLT 2005 eval-
uation campaign. In Proc. of the International Workshop
on Spoken Language Translation, Pittsburgh.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004. What?s
in a translation rule? In Proc. of the 4th International Con-
ference on Human Language Technology Research and
5th Annual Meeting of the NAACL (HLT-NAACL 2004),
Boston, USA.
S. Goldwater, T. Griffiths. 2007. A fully bayesian approach
to unsupervised part-of-speech tagging. In Proc. of the
45th Annual Meeting of the ACL (ACL-2007), 744?751,
Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
N. Habash, F. Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In Proc. of the 6th
International Conference on Human Language Technol-
ogy Research and 7th Annual Meeting of the NAACL
(HLT-NAACL 2006), New York City. Association for
Computational Linguistics.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo. In
Proc. of the 7th International Conference on Human Lan-
guage Technology Research and 8th Annual Meeting of the
NAACL (HLT-NAACL 2007), 139?146, Rochester, New
York.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the 3rd International Con-
ference on Human Language Technology Research and
4th Annual Meeting of the NAACL (HLT-NAACL 2003),
81?88, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,
C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses:
Open source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-2007),
Prague.
P. M. Lewis II, R. E. Stearns. 1968. Syntax-directed trans-
duction. J. ACM, 15(3):465?488.
D. Marcu, W. Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Proc. of the
2002 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), 133?139, Philadelphia.
Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006. SPMT:
Statistical machine translation with syntactified target lan-
guage phrases. In Proc. of the 2006 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP-
2006), 44?52, Sydney, Australia.
D. Newman, A. Asuncion, P. Smyth, M. Welling. 2007.
Distributed inference for latent dirichlet alocation. In
NIPS. MIT Press.
F. J. Och, H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?52.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of the 41st Annual Meeting
of the ACL (ACL-2003), 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a
method for automatic evaluation of machine translation,
2001.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
H. Zhang, D. Gildea, D. Chiang. 2008a. Extracting syn-
chronous grammar rules from word-level alignments in
linear time. In Proc. of the 22th International Con-
ference on Computational Linguistics (COLING-2008),
1081?1088, Manchester, UK.
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008b.
Bayesian learning of non-compositional phrases with syn-
chronous parsing. In Proc. of the 46th Annual Conference
of the Association for Computational Linguistics: Human
Language Technologies (ACL-08:HLT), 97?105, Colum-
bus, Ohio.
790
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337?340,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Note on the Implementation of
Hierarchical Dirichlet Processes
Phil Blunsom
?
pblunsom@inf.ed.ac.uk
Sharon Goldwater
?
sgwater@inf.ed.ac.uk
Trevor Cohn
?
tcohn@inf.ed.ac.uk
Mark Johnson
?
mark johnson@brown.edu
?
Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI, USA
Abstract
The implementation of collapsed Gibbs
samplers for non-parametric Bayesian
models is non-trivial, requiring con-
siderable book-keeping. Goldwater et
al. (2006a) presented an approximation
which significantly reduces the storage
and computation overhead, but we show
here that their formulation was incorrect
and, even after correction, is grossly inac-
curate. We present an alternative formula-
tion which is exact and can be computed
easily. However this approach does not
work for hierarchical models, for which
case we present an efficient data structure
which has a better space complexity than
the naive approach.
1 Introduction
Unsupervised learning of natural language is one
of the most challenging areas in NLP. Recently,
methods from nonparametric Bayesian statistics
have been gaining popularity as a way to approach
unsupervised learning for a variety of tasks,
including language modeling, word and mor-
pheme segmentation, parsing, and machine trans-
lation (Teh et al, 2006; Goldwater et al, 2006a;
Goldwater et al, 2006b; Liang et al, 2007; Finkel
et al, 2007; DeNero et al, 2008). These mod-
els are often based on the Dirichlet process (DP)
(Ferguson, 1973) or hierarchical Dirichlet process
(HDP) (Teh et al, 2006), with Gibbs sampling
as a method of inference. Exact implementation
of such sampling methods requires considerable
bookkeeping of various counts, which motivated
Goldwater et al (2006a) (henceforth, GGJ06) to
develop an approximation using expected counts.
However, we show here that their approximation
is flawed in two respects: 1) It omits an impor-
tant factor in the expectation, and 2) Even after
correction, the approximation is poor for hierar-
chical models, which are commonly used for NLP
applications. We derive an improvedO(1) formula
that gives exact values for the expected counts in
non-hierarchical models. For hierarchical models,
where our formula is not exact, we present an
efficient method for sampling from the HDP (and
related models, such as the hierarchical Pitman-
Yor process) that considerably decreases the mem-
ory footprint of such models as compared to the
naive implementation.
As we have noted, the issues described in this
paper apply to models for various kinds of NLP
tasks; for concreteness, we will focus on n-gram
language modeling for the remainder of the paper,
closely following the presentation in GGJ06.
2 The Chinese Restaurant Process
GGJ06 present two nonparametric Bayesian lan-
guage models: a DP unigram model and an HDP
bigram model. Under the DP model, words in a
corpus w = w
1
. . . w
n
are generated as follows:
G|?
0
, P
0
? DP(?
0
, P
0
)
w
i
|G ? G
where G is a distribution over an infinite set of
possible words, P
0
(the base distribution of the
DP) determines the probability that an item will
be in the support of G, and ?
0
(the concentration
parameter) determines the variance of G.
One way of understanding the predictions that
the DP model makes is through the Chinese restau-
rant process (CRP) (Aldous, 1985). In the CRP,
customers (word tokensw
i
) enter a restaurant with
an infinite number of tables and choose a seat. The
table chosen by the ith customer, z
i
, follows the
distribution:
P (z
i
= k|z
?i
) =
{
n
z
?i
k
i?1+?
0
, 0 ? k < K(z
?i
)
?
0
i?1+?
0
, k = K(z
?i
)
337
The
1
meow
4
cats
2
cats 
3
cats
5
a
b
c
d
e f
g
h
Figure 1. A seating assignment describing the state of
a unigram CRP. Letters and numbers uniquely identify
customers and tables. Note that multiple tables may
share a label.
where z
?i
= z
1
. . . z
i?1
are the table assignments
of the previous customers, n
z
?i
k
is the number of
customers at table k in z
?i
, andK(z
?i
) is the total
number of occupied tables. If we further assume
that table k is labeled with a word type `
k
drawn
from P
0
, then the assignment of tokens to tables
defines a distribution over words, with w
i
= `
z
i
.
See Figure 1 for an example seating arrangement.
Using this model, the predictive probability of
w
i
, conditioned on the previous words, can be
found by summing over possible seating assign-
ments for w
i
, and is given by
P (w
i
= w|w
?i
) =
n
w
?i
w
+ ?
0
P
0
i? 1 + ?
0
(1)
This prediction turns out to be exactly that of the
DP model after integrating out the distribution G.
Note that as long as the base distribution P
0
is
fixed, predictions do not depend on the seating
arrangement z
?i
, only on the count of word w
in the previously observed words (n
w
?i
w
). How-
ever, in many situations, we may wish to estimate
the base distribution itself, creating a hierarchical
model. Since the base distribution generates table
labels, estimates of this distribution are based on
the counts of those labels, i.e., the number of tables
associated with each word type.
An example of such a hierarchical model is the
HDP bigram model of GGJ06, in which each word
typew is associated with its own restaurant, where
customers in that restaurant correspond to words
that follow w in the corpus. All the bigram restau-
rants share a common base distribution P
1
over
unigrams, which must be inferred. Predictions in
this model are as follows:
P
2
(w
i
|h
?i
) =
n
h
?i
(w
i?1
,w
i
)
+ ?
1
P
1
(w
i
|h
?i
)
n
h
?i
(w
i?1
,?)
+ ?
1
P
1
(w
i
|h
?i
) =
t
h
?i
w
i
+ ?
0
P
0
(w
i
)
t
h
?i
?
+ ?
0
(2)
where h
?i
= (w
?i
, z
?i
), t
h
?i
w
i
is the number of
tables labelled with w
i
, and t
h
?i
?
is the total num-
ber of occupied tables. Of particular note for our
discussion is that in order to calculate these condi-
tional distributions we must know the table assign-
ments z
?i
for each of the words in w
?i
. Moreover,
in the Gibbs samplers often used for inference in
1         10         100         1000   
0.1
 
  
  
   
1
 
  
  
   
10
 
  
  
   
100
 
 
Me
an 
num
ber
 of 
lex
ica
l en
trie
s
Word frequency (nw)
 
 
Expectation
Antoniak approx.
Empirical, fixed base
Empirical, inferred base
Figure 2. Comparison of several methods of approx-
imating the number of tables occupied by words of
different frequencies. For each method, results using
? = {100, 1000, 10000, 100000} are shown (from bottom
to top). Solid lines show the expected number of tables,
computed using (3) and assuming P
1
is a fixed uni-
form distribution over a finite vocabulary (values com-
puted using the Digamma formulation (7) are the same).
Dashed lines show the values given by the Antoniak
approximation (4) (the line for ? = 100 falls below the
bottom of the graph). Stars show the mean of empirical
table counts as computed over 1000 samples from an
MCMC sampler in which P
1
is a fixed uniform distri-
bution, as in the unigram LM. Circles show the mean
of empirical table counts when P
1
is inferred, as in the
bigram LM. Standard errors in both cases are no larger
than the marker size. All plots are based on the 30114-
word vocabulary and frequencies found in sections 0-20
of the WSJ corpus.
these kinds of models, the counts are constantly
changing over multiple samples, with tables going
in and out of existence frequently. This can create
significant bookkeeping issues in implementation,
and motivated GGJ06 to present a method of com-
puting approximate table counts based on word
frequencies only.
3 Approximating Table Counts
Rather than explicitly tracking the number of
tables t
w
associated with each word w in their
bigram model, GGJ06 approximate the table
counts using the expectation E[t
w
]. Expected
counts are used in place of t
h
?i
w
i
and t
h
?i
?
in (2).
The exact expectation, due to Antoniak (1974), is
E[t
w
] = ?
1
P
1
(w)
n
w
?
i=1
1
?
1
P
1
(w) + i? 1
(3)
338
Antoniak also gives an approximation to this
expectation:
E[t
w
] ? ?
1
P
1
(w) log
n
w
+ ?
1
P
1
(w)
?
1
P
1
(w)
(4)
but provides no derivation. Due to a misinterpre-
tation of Antoniak (1974), GGJ06 use an approx-
imation that leaves out all the P
1
(w) terms from
(4).
1
Figure 2 compares the approximation to
the exact expectation when the base distribution
is fixed. The approximation is fairly good when
?P
1
(w) > 1 (the scenario assumed by Antoniak);
however, in most NLP applications, ?P
1
(w) <
1 in order to effect a sparse prior. (We return
to the case of non-fixed based distributions in a
moment.) As an extreme case of the paucity of
this approximation consider ?
1
P
1
(w) = 1 and
n
w
= 1 (i.e. only one customer has entered the
restaurant): clearly E[t
w
] should equal 1, but the
approximation gives log(2).
We now provide a derivation for (4), which will
allow us to obtain an O(1) formula for the expec-
tation in (3). First, we rewrite the summation in (3)
as a difference of fractional harmonic numbers:
2
H
(?
1
P
1
(w)+n
w
?1)
?H
(?
1
P
1
(w)?1)
(5)
Using the recurrence for harmonic numbers:
E[t
w
] ? ?
1
P
1
(w)
[
H
(?
1
P
1
(w)+n
w
)
?
1
?
1
P
1
(w) + n
w
?H
(?
1
P
1
(w)+n
w
)
+
1
?
1
P
1
(w)
]
(6)
We then use the asymptotic expansion,
H
F
? logF + ? +
1
2F
, omiting trailing terms
which are O(F
?2
) and smaller powers of F :
3
E[t
w
] ? ?
1
P
1
(w) log
n
w
+?
1
P
1
(w)
?
1
P
1
(w)
+
n
w
2(?
1
P
1
(w)+n
w
)
Omitting the trailing term leads to the
approximation in Antoniak (1974). However, we
can obtain an exact formula for the expecta-
tion by utilising the relationship between the
Digamma function and the harmonic numbers:
?(n) = H
n?1
? ?.
4
Thus we can rewrite (5) as:
5
E[t
w
] = ?
1
P
1
(w)?
[
?(?
1
P
1
(w) + n
w
)? ?(?
1
P
1
(w))
]
(7)
1
The authors of GGJ06 realized this error, and current
implementations of their models no longer use these approx-
imations, instead tracking table counts explicitly.
2
Fractional harmonic numbers between 0 and 1 are given
by H
F
=
R
1
0
1?x
F
1?x
dx. All harmonic numbers follow the
recurrence H
F
= H
F?1
+
1
F
.
3
Here, ? is the Euler-Mascheroni constant.
4
AccurateO(1) approximations of the Digamma function
are readily available.
5
(7) can be derived from (3) using: ?(x+1)??(x) =
1
x
.
Explicit table tracking:
customer(w
i
)? table(z
i
)
n
a : 1, b : 1, c : 2, d : 2, e : 3, f : 4, g : 5, h : 5
o
table(z
i
)? label(`)
n
1 : The, 2 : cats, 3 : cats, 4 : meow, 5 : cats
o
Histogram:
word type?
{
table occupancy? frequency
}
n
The : {2 : 1}, cats : {1 : 1, 2 : 2}, meow : {1 : 1}
o
Figure 3. The explicit table tracking and histogram rep-
resentations for Figure 1.
A significant caveat here is that the expected
table counts given by (3) and (7) are only valid
when the base distribution is a constant. However,
in hierarchical models such as GGJ06?s bigram
model and HDP models, the base distribution is
not constant and instead must be inferred. As can
be seen in Figure 2, table counts can diverge con-
siderably from the expectations based on fixed
P
1
when P
1
is in fact not fixed. Thus, (7) can
be viewed as an approximation in this case, but
not necessarily an accurate one. Since knowing
the table counts is only necessary for inference
in hierarchical models, but the table counts can-
not be approximated well by any of the formu-
las presented here, we must conclude that the best
inference method is still to keep track of the actual
table counts. The naive method of doing so is to
store which table each customer in the restaurant
is seated at, incrementing and decrementing these
counts as needed during the sampling process. In
the following section, we describe an alternative
method that reduces the amount of memory neces-
sary for implementing HDPs. This method is also
appropriate for hierarchical Pitman-Yor processes,
for which no closed-form approximations to the
table counts have been proposed.
4 Efficient Implementation of HDPs
As we do not have an efficient expected table
count approximation for hierarchical models we
could fall back to explicitly tracking which table
each customer that enters the restaurant sits at.
However, here we describe a more compact repre-
sentation for the state of the restaurant that doesn?t
require explicit table tracking.
6
Instead we main-
tain a histogram for each dish w
i
of the frequency
of a table having a particular number of customers.
Figure 3 depicts the histogram and explicit repre-
sentations for the CRP state in Figure 1.
Our alternative method of inference for hierar-
chical Bayesian models takes advantage of their
6
Teh et al (2006) also note that the exact table assign-
ments for customers are not required for prediction.
339
Algorithm 1 A new customer enters the restaurant
1: w: word type
2: P
w
0
: Base probability for w
3: HD
w
: Seating Histogram for w
4: procedure INCREMENT(w,P
w
0
,HD
w
)
5: p
share
?
n
w
?1
w
n
w
?1
w
+?
0
. share an existing table
6: p
new
?
?
0
?P
w
0
n
w
?1
w
+?
0
. open a new table
7: r ? random(0, p
share
+ p
new
)
8: if r < p
new
or n
w
?1
w
= 0 then
9: HD
w
[1] = HD
w
[1] + 1
10: else
. Sample from the histogram of customers at tables
11: r ? random(0, n
w
?1
w
)
12: for c ? HD
w
do . c: customer count
13: r = r ? (c? HD
w
[c])
14: if r ? 0 then
15: HD
w
[c] = HD
w
[c] + 1
16: Break
17: n
w
w
= n
w
?1
w
+ 1 . Update token count
Algorithm 2 A customer leaves the restaurant
1: w: word type
2: HD
w
: Seating histogram for w
3: procedure DECREMENT(w,P
w
0
,HD
w
)
4: r ? random(0, n
w
w
)
5: for c ? HD
w
do . c: customer count
6: r = r ? (c? HD
w
[c])
7: if r ? 0 then
8: HD
w
[c] = HD
w
[c]? 1
9: if c > 1 then
10: HD
w
[c? 1] = HD
w
[c? 1] + 1
11: Break
12: n
w
w
= n
w
w
? 1 . Update token count
exchangeability, which makes it unnecessary to
know exactly which table each customer is seated
at. The only important information is how many
tables exist with different numbers of customers,
and what their labels are. We simply maintain a
histogram for each word type w, which stores, for
each number of customersm, the number of tables
labeled with w that have m customers. Figure 3
depicts the explicit representation and histogram
for the CRP state in Figure 1.
Algorithms 1 and 2 describe the two operations
required to maintain the state of a CRP.
7
When
a customer enters the restaurant (Alogrithm 1)),
we sample whether or not to open a new table.
If not, we sample an old table proportional to the
counts of how many customers are seated there
and update the histogram. When a customer leaves
the restaurant (Algorithm 2), we decrement one
of the tables at random according to the number
of customers seated there. By exchangeability, it
doesn?t actually matter which table the customer
was ?really? sitting at.
7
A C++ template class that implements
the algorithm presented is made available at:
http://homepages.inf.ed.ac.uk/tcohn/
5 Conclusion
We?ve shown that the HDP approximation pre-
sented in GGJ06 contained errors and inappropri-
ate assumptions such that it significantly diverges
from the true expectations for the most common
scenarios encountered in NLP. As such we empha-
sise that that formulation should not be used.
Although (7) allowsE[t
w
] to be calculated exactly
for constant base distributions, for hierarchical
models this is not valid and no accurate calculation
of the expectations has been proposed. As a rem-
edy we?ve presented an algorithm that efficiently
implements the true HDP without the need for
explicitly tracking customer to table assignments,
while remaining simple to implement.
Acknowledgements
The authors would like to thank Tom Grif-
fiths for providing the code used to produce
Figure 2 and acknowledge the support of the
EPSRC (Blunsom, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
References
D. Aldous. 1985. Exchangeability and related topics. In
?
Ecole d?
?
Et?e de Probabiliti?es de Saint-Flour XIII 1983, 1?
198. Springer.
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
J. DeNero, A. Bouchard-C?ot?e, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
S. Ferguson. 1973. A Bayesian analysis of some nonpara-
metric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, C. D. Manning. 2007. The infinite
tree. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006a. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
S. Goldwater, T. Griffiths, M. Johnson. 2006b. Interpolating
between types and tokens by estimating power-law gener-
ators. In Y. Weiss, B. Sch?olkopf, J. Platt, eds., Advances
in Neural Information Processing Systems 18, 459?466.
MIT Press, Cambridge, MA.
P. Liang, S. Petrov, M. Jordan, D. Klein. 2007. The infinite
PCFG using hierarchical Dirichlet processes. In Proc. of
the 2007 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2007), 688?697, Prague,
Czech Republic.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
340
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 197?205,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Quantitative Analysis of Reordering Phenomena
Alexandra Birch Phil Blunsom Miles Osborne
a.c.birch-mayne@sms.ed.ac.uk pblunsom@inf.ed.ac.uk miles@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
Reordering is a serious challenge in sta-
tistical machine translation. We propose
a method for analysing syntactic reorder-
ing in parallel corpora and apply it to un-
derstanding the differences in the perfor-
mance of SMT systems. Results at recent
large-scale evaluation campaigns show
that synchronous grammar-based statisti-
cal machine translation models produce
superior results for language pairs such as
Chinese to English. However, for language
pairs such as Arabic to English, phrase-
based approaches continue to be competi-
tive. Until now, our understanding of these
results has been limited to differences in
BLEU scores. Our analysis shows that cur-
rent state-of-the-art systems fail to capture
the majority of reorderings found in real
data.
1 Introduction
Reordering is a major challenge in statistical ma-
chine translation. Reordering involves permuting
the relative word order from source sentence to
translation in order to account for systematic dif-
ferences between languages. Correct word order is
important not only for the fluency of output, it also
affects word choice and the overall quality of the
translations.
In this paper we present an automatic method
for characterising syntactic reordering found in a
parallel corpus. This approach allows us to analyse
reorderings quantitatively, based on their number
and span, and qualitatively, based on their relation-
ship to the parse tree of one sentence. The methods
we introduce are generally applicable, only requir-
ing an aligned parallel corpus with a parse over the
source or the target side, and can be extended to
allow for more than one reference sentence and
derivations on both source and target sentences.
Using this method, we are able to compare the re-
ordering capabilities of two important translation
systems: a phrase-based model and a hierarchical
model.
Phrase-based models (Och and Ney, 2004;
Koehn et al, 2003) have been a major paradigm
in statistical machine translation in the last few
years, showing state-of-the-art performance for
many language pairs. They search all possible re-
orderings within a restricted window, and their
output is guided by the language model and a
lexicalised reordering model (Och et al, 2004),
both of which are local in scope. However, the
lack of structure in phrase-based models makes it
very difficult to model long distance movement of
words between languages.
Synchronous grammar models can encode
structural mappings between languages which al-
low complex, long distance reordering. Some
grammar-based models such as the hierarchical
model (Chiang, 2005) and the syntactified target
language phrases model (Marcu et al, 2006) have
shown better performance than phrase-based mod-
els on certain language pairs.
To date our understanding of the variation in re-
ordering performance between phrase-based and
synchronous grammar models has been limited to
relative BLEU scores. However, Callison-Burch et
al. (2006) showed that BLEU score alone is insuffi-
cient for comparing reordering as it only measures
a partial ordering on n-grams. There has been little
direct research on empirically evaluating reorder-
ing.
We evaluate the reordering characteristics of
these two paradigms on Chinese-English and
Arabic-English translation. Our main findings are
as follows: (1) Chinese-English parallel sentences
exhibit many medium and long-range reorderings,
but less short range ones than Arabic-English, (2)
phrase-based models account for short-range re-
orderings better than hierarchical models do, (3)
197
by contrast, hierarchical models clearly outper-
form phrase-based models when there is signif-
icant medium-range reordering, and (4) none of
these systems adequately deal with longer range
reordering.
Our analysis provides a deeper understand-
ing of why hierarchical models demonstrate bet-
ter performance for Chinese-English translation,
and also why phrase-based approaches do well at
Arabic-English.
We begin by reviewing related work in Sec-
tion 2. Section 3 describes our method for ex-
tracting and measuring reorderings in aligned and
parsed parallel corpora. We apply our techniques
to human aligned parallel treebank sentences in
Section 4, and to machine translation outputs in
Section 5.We summarise our findings in Section 6.
2 Related Work
There are few empirical studies of reordering be-
haviour in the statistical machine translation lit-
erature. Fox (2002) showed that many common
reorderings fall outside the scope of synchronous
grammars that only allow the reordering of child
nodes. This study was performed manually and
did not compare different language pairs or trans-
lation paradigms. There are some comparative
studies of the reordering restrictions that can be
imposed on the phrase-based or grammar-based
models (Zens and Ney, 2003; Wellington et al,
2006), however these do not look at the reordering
performance of the systems. Chiang et al (2005)
proposed a more fine-grained method of compar-
ing the output of two translation systems by us-
ing the frequency of POS sequences in the output.
This method is a first step towards a better under-
standing of comparative reordering performance,
but neglects the question of what kind of reorder-
ing is occurring in corpora and in translation out-
put.
Zollmann et al (2008) performed an empiri-
cal comparison of the BLEU score performance
of hierarchical models with phrase-based models.
They tried to ascertain which is the stronger model
under different reordering scenarios by varying
distortion limits the strength of language models.
They show that the hierarchical models do slightly
better for Chinese-English systems, but worse for
Arabic-English. However, there was no analysis of
the reorderings existing in their parallel corpora,
or on what kinds of reorderings were produced in
their output. We perform a focused evaluation of
these issues.
Birch et al (2008) proposed a method for ex-
tracting reorderings from aligned parallel sen-
tences.We extend this method in order to constrain
the reorderings to a derivation over the source sen-
tence where possible.
3 Measuring Reordering
Reordering is largely driven by syntactic differ-
ences between languages and can involve complex
rearrangements between nodes in synchronous
trees. Modeling reordering exactly would be
sparse and heterogeneous and thus we make an
important simplifying assumption in order for the
detection and extraction of reordering data to be
tractable and useful. We assume that reordering
is a binary process occurring between two blocks
that are adjacent in the source. We extend the
methods proposed by Birch et al (2008) to iden-
tify and measure reordering. Modeling reordering
as the inversion in order of two adjacent blocks is
similar to the approach taken by the Inverse Trans-
duction Model (ITG) (Wu, 1997), except that here
we are not limited to a binary tree. We also detect
and include non-syntactic reorderings as they con-
stitute a significant proportion of the reorderings.
Birch et al (2008) defined the extraction pro-
cess for a sentence pair that has been word aligned.
This method is simple, efficient and applicable to
all aligned sentence pairs. However, if we have ac-
cess to the syntax tree, we can more accurately
determine the groupings of embedded reorder-
ings, and we can also access interesting informa-
tion about the reordering such as the type of con-
stituents that get reordered. Figure 1 shows the
advantage of using syntax to guide the extraction
process. Embedded reorderings that are extracted
without syntax assume a right branching structure.
Reorderings that are extracted using the syntac-
tic extraction algorithm reflect the correct sentence
structure. We thus extend the algorithm to extract-
ing syntactic reorderings. We require that syntac-
tic reorderings consist of blocks of whole sibling
nodes in a syntactic tree over the source sentence.
In Figure 2 we can see a sentence pair with an
alignment and a parse tree over the source. We per-
form a depth first recursion through the tree, ex-
tracting the reorderings that occur between whole
sibling nodes. Initially a reordering is detected be-
tween the leaf nodes P and NN. The block growing
algorithm described in Birch et al (2008) is then
used to grow block A to include NT and NN, and
block B to include P and NR. The source and tar-
get spans of these nodes do not overlap the spans
198
Figure 1. An aligned sentence pair which shows two
different sets of reorderings for the case without and
with a syntax tree.
of any other nodes, and so the reordering is ac-
cepted. The same happens for the higher level re-
ordering where block A covers NP-TMP and PP-
DIR, and block B covers the VP. In cases where
the spans do overlap spans of nodes that are not
siblings, these reorderings are then extracted us-
ing the algorithm described in Birch et al (2008)
without constraining them to the parse tree. These
non-syntactic reorderings constitute about 10% of
the total reorderings and they are a particular chal-
lenge to models which can only handle isomorphic
structures.
RQuantity
The reordering extraction technique allows us to
analyse reorderings in corpora according to the
distribution of reordering widths and syntactic
types. In order to facilitate the comparison of dif-
ferent corpora, we combine statistics about in-
dividual reorderings into a sentence level metric
which is then averaged over a corpus. This met-
ric is defined using reordering widths over the tar-
get side to allow experiments with multiple lan-
guage pairs to be comparable when the common
language is the target.
We use the average RQuantity (Birch et al,
2008) as our measure of the amount of reordering
in a parallel corpus. It is defined as follows:
RQuantity =
?
r?R |rAt | + |rBt |
I
where R is the set of reorderings for a sentence,
I is the target sentence length, A and B are the
two blocks involved in the reordering, and |rAs |
is the size or span of block A on the target side.
RQuantity is thus the sum of the spans of all the
reordering blocks on the target side, normalised
$ %
$
%
Figure 2. A sentence pair from the test corpus, with its
alignment and parse tree. Two reorderings are shown
with two different dash styles.
by the length of the target sentence. The minimum
RQuantity for a sentence would be 0. The max-
imum RQuantity occurs where the order of the
sentence is completely inverted and the RQuantity
is
?I
i=2 i. See, for example, Figure 1 where the
RQuantity is 94 .
4 Analysis of Reordering in Parallel
Corpora
Characterising the reordering present in different
human generated parallel corpora is crucial to un-
derstanding the kinds of reordering wemust model
in our translations. We first need to extract reorder-
ings for which we need alignments and deriva-
tions. We could use automatically generated an-
notations, however these contain errors and could
be biased towards the models which created them.
The GALE project has provided gold standard
word alignments for Arabic-English (AR-EN) and
Chinese-English (CH-EN) sentences.1 A subset of
these sentences come from the Arabic and Chi-
nese treebanks, which provide gold standard parse
trees. The subsets of parallel data for which we
have both alignments and parse trees consist of
1see LDC corpus LDC2006E93 version GALE-Y1Q4
199
ll
l l
l l
l l l
l
l
0.0
0.2
0.4
0.6
0.8
1.0
Sentence Length Bin
RQu
antit
y
0?9 20?29 40?49 60?69 80?89 >=100
l CH.EN.RQuantityAR.EN.RQuantity
Figure 3. Sentence level measures of RQuantity for the
CH-EN and AR-EN corpora for different English sen-
tence lengths.
l l
l
l
l
l
l
l
l
l
0
500
1000
1500
2000
2500
Reordering Width
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
l CH?ENAR?EN
Figure 4. Comparison of reorderings of different widths
for the CH-EN and AR-EN corpora.
3,380 CH-EN sentences and 4,337 AR-EN sen-
tences.
Figure 3 shows that the different corpora have
very different reordering characteristics. The CH-
EN corpus displays about three times the amount
of reordering (RQuantity) than the AR-EN cor-
pus. For CH-EN, the RQuantity increases with
sentence length and for AR-EN, it remains con-
stant. This seems to indicate that for longer CH-
EN sentences there are larger reorderings, but this
is not the case for AR-EN. RQuantity is low for
very short sentences, which indicates that these
sentences are not representative of the reordering
characteristics of a corpus. The measures seem
to stabilise for sentences with lengths of over 20
words.
The average amount of reordering is interesting,
but it is also important to look at the distribution
of reorderings involved. Figure 4 shows the re-
orderings in the CH-EN and AR-EN corpora bro-
l
l
l
l
l
l
l
l
l
l0
5
10
15
20
25
30
Widths of Reorderings
% N
umb
er o
f Re
orde
rings
 for W
idth
2 3 4 5 6 7?8 9?10 16?20
l NPDNPCPNP.PN
Figure 5. The four most common syntactic types being
reordered forward in target plotted as % of total syntac-
tic reorderings against reordering width (CH-EN).
ken down by the total width of the source span
of the reorderings. The figure clearly shows how
different the two language pairs are in terms of
reordering widths. Compared to the CH-EN lan-
guage pair, the distribution of reorderings in AR-
EN has many more reorderings over short dis-
tances, but many fewer medium or long distance
reorderings. We define short, medium or long dis-
tance reorderings to mean that they have a reorder-
ing of width of between 2 to 4 words, 5 to 8 and
more than 8 words respectively.
Syntactic reorderings can reveal very rich
language-specific reordering behaviour. Figure 5
is an example of the kinds of data that can be used
to improve reordering models. In this graph we se-
lected the four syntactic types that were involved
in the largest number of reorderings. They cov-
ered the block that was moved forward in the tar-
get (block A). We can see that different syntactic
types display quite different behaviour at different
reordering widths and this could be important to
model.
Having now characterised the space of reorder-
ing actually found in parallel data, we now turn
to the question of how well our translation models
account for them. As both the translation models
investigated in this work do not use syntax, in the
following sections we focus on non-syntactic anal-
ysis.
5 Evaluating Reordering in Translation
We are interested in knowing how current trans-
lation models perform specifically with regard to
reordering. To evaluate this, we compare the re-
orderings in the parallel corpora with the reorder-
ings that exist in the translated sentences. We com-
200
None Low Medium High
Average RQuantity
CH-EN 0 0.39 0.82 1.51
AR-EN 0 0.10 0.25 0.57
Number of Sentences
CH-EN 105 367 367 367
AR-EN 293 379 379 379
Table 1. The RQuantity and the number of sentences
for each reordering test set.
pare two state-of-the-art models: the phrase-based
system Moses (Koehn et al, 2007) (with lexi-
calised reordering), and the hierarchical model Hi-
ero (Chiang, 2007). We use default settings for
both models: a distortion limit of seven for Moses,
and a maximum source span limit of 10 words for
Hiero. We trained both models on subsets of the
NIST 2008 data sets, consisting mainly of news
data, totalling 547,420 CH-EN and 1,069,658 AR-
EN sentence pairs. We used a trigram language
model on the entire English side (211M words)
of the NIST 2008 Chinese-English training cor-
pus. Minimum error rate training was performed
on the 2002 NIST test for CH-EN, and the 2004
NIST test set for AR-EN.
5.1 Reordering Test Corpus
In order to determine what effect reordering has
on translation, we extract a test corpus with spe-
cific reordering characteristics from the manually
aligned and parsed sentences described in Sec-
tion 4. To minimise the impact of sentence length,
we select sentences with target lengths from 20 to
39 words inclusive. In this range RQuantity is sta-
ble. From these sentences we first remove those
with no detected reorderings, and we then divide
up the remaining sentences into three sets of equal
sizes based on the RQuantity of each sentence. We
label these test sets: ?none?, ?low?, ?medium? and
?high?.
All test sentences have only one reference En-
glish sentence. MT evaluations using one refer-
ence cannot make strong claims about any partic-
ular test sentence, but are still valid when used to
compare large numbers of hypotheses.
Table 1 and Figure 6 show the reordering char-
acteristics of the test sets. As expected, we see
more reordering for Chinese-English than for Ara-
bic to English.
It is important to note that although we might
name a set ?low? or ?high?, this is only relative
to the other groups for the same language pair.
The ?high? AR-EN set, has a lower RQuantity
than the ?medium? CH-EN set. Figure 6 shows
0
50
100
150
200
250
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
LowMediumHigh
Figure 6. Number of reorderings in the CH-EN test set
plotted against the total width of the reorderings.
none low med high all
MOSESHIERO
14
16
18
20
22
Figure 7. BLEU scores for the different CH-EN reorder-
ing test sets and the combination of all the groups for
the two translation models.The 95% confidence levels
as measured by bootstrap resampling are shown for
each bar.
that the CH-EN reorderings in the higher RQuan-
tity groups have more and longer reorderings. The
AR-EN sets show similar differences in reordering
behaviour.
5.2 Performance on Test Sets
In this section we compare the translation output
for the phrase-based and the hierarchical system
for different reordering scenarios. We use the test
sets created in Section 5.1 to explicitly isolate the
effect reordering has on the performance of two
translation systems.
Figure 7 and Figure 8 show the BLEU score
results of the phrase-based model and the hierar-
chical model on the different reordering test sets.
The 95% confidence intervals as calculated by
bootstrap resampling (Koehn, 2004) are shown for
each of the results. We can see that the models
show quite different behaviour for the different
test sets and for the different language pairs. This
demonstrates that reordering greatly influences the
201
none low med high all
MOSESHIERO
16
18
20
22
24
26
Figure 8. BLEU scores for the different AR-EN reorder-
ing test sets and the combination of all the groups for
the two translation models. The 95% confidence lev-
els as measured by bootstrap resampling are shown for
each bar.
BLEU score performance of the systems.
In Figure 7 we see that the hierarchical model
performs considerably better than Moses on the
?medium? CH-EN set, although the confidence
interval for these results overlap somewhat. This
supports the claim that Hiero is better able to cap-
ture longer distance reorderings than Moses.
Hiero performs significantly worse than Moses
on the ?none? and ?low? sets for CH-EN, and
for all the AR-EN sets, other than ?none?. All
these sets have a relatively low amount of reorder-
ing, and in particular a low number of medium
and long distance reorderings. The phrase-based
model could be performing better because it
searches all possible permutations within a certain
window whereas the hierarchical model will only
permit reorderings for which there is lexical evi-
dence in the training corpus. Within a small win-
dow, this exhaustive search could discover the best
reorderings, but within a bigger window, the more
constrained search of the hierarchical model pro-
duces better results. It is interesting that Hiero is
not always the best choice for translation perfor-
mance, and depending on the amount of reorder-
ing and the distribution of reorderings, the simpler
phrase-based approach is better.
The fact that both models show equally poor
performance on the ?high? RQuantity test set sug-
gests that the hierarchical model has no advantage
over the phrase-based model when the reorder-
ings are long enough and frequent enough. Nei-
ther Moses nor Hiero can perform long distance
reorderings, due to the local constraints placed on
their search which allows performance to be lin-
ear with respect to sentence length. Increasing the
window in which these models are able to perform
reorderings does not necessarily improve perfor-
l
l l
l l
l l l0
20
40
60
80
100
120
140
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7 8 >8
l NoneLowMediumHigh
Figure 9. Reorderings in the CH-EN MOSES transla-
tion of the reordering test set, plotted against the total
width of the reorderings.
mance, due to the number of hypotheses the mod-
els must discriminate amongst.
The performance of both systems on the ?high?
test set could be much worse than the BLEU score
would suggest. A long distance reordering that has
been missed, would only be penalised by BLEU
once at the join of the two blocks, even though it
might have a serious impact on the comprehension
of the translation. This flaw seriously limits the
conclusions that we can draw from BLEU score,
and motivates analysing translations specifically
for reordering as we do in this paper.
Reorderings in Translation
At best, BLEU can only partially reflect the re-
ordering performance of the systems. We therefore
perform an analysis of the distribution of reorder-
ings that are present in the systems? outputs, in or-
der to compare them with each other and with the
source-reference distribution.
For each hypothesis translation, we record
which source words and phrase pairs or rules were
used to produce which target words. From this we
create an alignment matrix from which reorder-
ings are extracted in the same manner as previ-
ously done for the manually aligned corpora.
Figure 9 shows the distribution of reorderings
that occur between the source sentence and the
translations from the phrase-based model. This
graph is interesting when compared with Figure 6,
which shows the reorderings that exist in the orig-
inal reference sentence pair. The two distribu-
tions are quite different. Firstly, as the models use
phrases which are treated as blocks, reorderings
which occur within a phrase are not recorded. This
reduces the number of shorter distance reorder-
ings in the distribution in Figure 6, as mainly short
202
l
l
l l
l
l
l l0
10
20
30
40
50
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7 8 >8
l NoneLowMediumHigh
Figure 10. Reorderings in the CH-EN Hiero translation
of the reordering test set, plotted against the total width
of the reorderings.
phrases pairs are used in the hypothesis. However,
even taking reorderings within phrase pairs into
account, there are many fewer reorderings in the
translations than in the references, and there are
no long distance reorderings.
It is interesting that the phrase-based model is
able to capture the fact that reordering increases
with the RQuantity of the test set. Looking at the
equivalent data for the AR-EN language pair, a
similar pattern emerges: there are many fewer re-
orderings in the translations than in the references.
Figure 10 shows the reorderings from the output
of the hierarchical model. The results are very dif-
ferent to both the phrase-based model output (Fig-
ure 9) and to the original reference reordering dis-
tribution (Figure 6). There are fewer reorderings
here than even in the phrase-based output. How-
ever, the Hiero output has a slightly higher BLEU
score than the Moses output. The number of re-
orderings is clearly not the whole story. Part of the
reason why the output seems to have few reorder-
ings and yet scores well, is that the output of hier-
archical models does not lend itself to the analysis
that we have performed successfully on the ref-
erence or phrase-based translation sentence pairs.
This is because the output has a large number of
non-contiguous phrases which prevent the extrac-
tion of reorderings from within their span. Only
4.6% of phrase-based words were blocked off due
to non-contiguous phrases but 47.5% of the hier-
archical words were. This problem can be amelio-
rated with the detection and unaligning of words
which are obviously dependent on other words in
the non-contiguous phrase.
Even taking blocked off phrases into account,
however, the number of reorderings in the hierar-
l l
l
l
l
l
l
l
l0
100
200
300
400
500
600
Reordering Width
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
l Test.SetPhrase.BasedHierarchical
Figure 11. Number of reorderings in the original CH-
EN test set, compared to the reorderings retained by
the phrase-based and hierarchical models. The data is
shown relative to the length of the total source width of
the reordering.
chical output is still low, especially for the medium
and long distance reorderings, as compared to the
reference sentences. The hierarchical model?s re-
ordering behaviour is very different to human re-
ordering. Even if human translations are freer and
contain more reordering than is strictly necessary,
many important reorderings are surely being lost.
Targeted Automatic Evaluation
Comparing distributions of reorderings is inter-
esting, but it cannot approach the question of how
many reorderings the system performed correctly.
In this section we identify individual reorderings
in the source and reference sentences and detect
whether or not they have been reproduced in the
translation.
Each reordering in the original test set is ex-
tracted. Then the source-translation alignment is
inspected to determine whether the blocks in-
volved in the original reorderings are in the reverse
order in the translation. If so, we say that these re-
orderings have been retained from the reference to
the translation.
If a reordering has been translated by one phrase
pair, we assume that the reordering has been re-
tained, because the reordering could exist inside
the phrase. If the segmentation is slightly differ-
ent, but a reordering of the correct size occurred at
the right place, it is also considered to be retained.
Figure 11 shows that the hierarchical model
retains more reorderings of all widths than the
phrase-based system. Both systems retain few re-
orderings, with the phrase-based model missing
almost all the medium distance reorderings, and
both models failing on all the long distance re-
203
Correct Incorrect NA
Retained 61 4 10
Not Retained 32 31 12
Table 2. Correlation between retaining reordering and it
being correct - for humans and for system
orderings. This is possibly the most direct evi-
dence of reordering performance so far, and again
shows how Hiero has a slight advantage over the
phrase-based systemwith regard to reordering per-
formance.
Targeted Manual Analysis
The relationship between targeted evaluation
and the correct reordering of the translation still
needs to be established. The translation system can
compensate for not retaining a reordering by us-
ing different lexical items. To judge the relevance
of the targeted evaluation we need to perform a
manual evaluation. We present evaluators with the
reference and the translation sentences. We mark
the target ranges of the blocks that are involved
in the particular reordering we are analysing, and
ask the evaluator if the reordering in the translation
is correct, incorrect or not applicable. The not ap-
plicable case is chosen when the translated words
are so different from the reference that their order-
ing is irrelevant. There were three evaluators who
each judged 25 CH-EN reorderings which were re-
tained and 25 CH-EN reorderings which were not
retained by the Moses translation model.
The results in Table 2 show that the retained
reorderings are generally judged to be correct. If
the reordering is not retained, then the evaluators
divided their judgements evenly between the re-
ordering being correct or incorrect. It seems that
the fact that a reordering is not retained does in-
dicate that its ordering is more likely to be incor-
rect. We used Fleiss? Kappa to measure the cor-
relation between annotators. It expresses the ex-
tent to which the amount of agreement between
raters is greater than what would be expected if
all raters made their judgements randomly. In this
case Fleiss? kappa is 0.357 which is considered to
be a fair correlation.
6 Conclusion
In this paper we have introduced a general and
extensible automatic method for the quantitative
analyse of syntactic reordering phenomena in par-
allel corpora.
We have applied our method to a systematic
analysis of reordering both in the training corpus,
and in the output, of two state-of-the-art transla-
tion models. We show that the hierarchical model
performs better than the phrase-based model in sit-
uations where there are many medium distance re-
orderings. In addition, we find that the choice of
translation model must be guided by the type of re-
orderings in the language pair, as the phrase-based
model outperforms the hierarchical model when
there is a predominance of short distance reorder-
ings. However, neither model is able to capture the
reordering behaviour of the reference corpora ad-
equately. These result indicate that there is still
much research to be done if statistical machine
translation systems are to capture the full range of
reordering phenomena present in translation.
References
Alexandra Birch, Miles Osborne, and Philipp Koehn. 2008.
Predicting success in machine translation. In Proceedings
of the Empirical Methods in Natural Language Process-
ing.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine trans-
lation research. In Proceedings of the European Chapter
of the Association for Computational Linguistics, Trento,
Italy.
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz,
Philip Resnik, and Michael Subotin. 2005. The Hiero
machine translation system: Extensions, evaluation, and
analysis. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical Methods
in Natural Language Processing, pages 779?786, Vancou-
ver, Canada.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 263?270,
Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics (to appear), 33(2).
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages 304?
311, Philadelphia, USA.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the
Human Language Technology and North American Asso-
ciation for Computational Linguistics Conference, pages
127?133, Edmonton, Canada. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Association for Computational Linguistics Companion
Demo and Poster Sessions, pages 177?180, Prague, Czech
Republic. Association for Computational Linguistics.
204
Philipp Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?
395, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, pages 44?52, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30(4):417?450.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Li-
bin Shen, David Smith, Katherine Eng, Viren Jain, Zhen
Jin, and Dragomir Radev. 2004. A smorgasbord of fea-
tures for statistical machine translation. In Proceedings of
Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing,
pages 161?168, Boston, USA. Association for Computa-
tional Linguistics.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the complex-
ity of translational equivalence. In Proceedings of the In-
ternational Conference on Computational Linguistics and
of the Association for Computational Linguistics, pages
977?984, Sydney, Australia.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative study
on reordering constraints in statistical machine translation.
In Proceedings of the Association for Computational Lin-
guistics, pages 144?151, Sapporo, Japan.
Andreas Zollmann, Ashish Venugopal, Franz Och, and Jay
Ponte. 2008. A systematic comparison of phrase-based,
hierarchical and syntax-augmented statistical mt. In Pro-
ceedings of International Conference On Computational
Linguistics.
205
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102?110,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Monte Carlo inference and maximization for phrase-based translation
Abhishek Arun?
a.arun@sms.ed.ac.uk
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Adam Lopez?
alopez@inf.ed.ac.uk
Barry Haddow?
bhaddow@inf.ed.ac.uk
Philipp Koehn?
pkoehn@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
Recent advances in statistical machine
translation have used beam search for
approximate NP-complete inference within
probabilistic translation models. We present
an alternative approach of sampling from the
posterior distribution defined by a translation
model. We define a novel Gibbs sampler
for sampling translations given a source
sentence and show that it effectively explores
this posterior distribution. In doing so
we overcome the limitations of heuristic
beam search and obtain theoretically sound
solutions to inference problems such as
finding the maximum probability translation
and minimum expected risk training and
decoding.
1 Introduction
Statistical machine translation (SMT) poses the
problem: given a foreign sentence f , find the
translation e? that maximises the conditional
posterior probability p(e|f). This probabilistic
formulation of translation has driven development
of state-of-the-art systems which are able to learn
from parallel corpora which were generated for
other purposes ? a direct result of employing a
mathematical framework that we can reason about
independently of any particular model.
For example, we can train SMT models using
maximum likelihood estimation (Brown et al, 1993;
Och and Ney, 2000; Marcu and Wong, 2002). Alter-
natively, we can train to minimise probabilistic con-
ceptions of risk (expected loss) with respect to trans-
lation metrics, thereby obtaining better results for
those metrics (Kumar and Byrne, 2004; Smith and
Eisner, 2006; Zens and Ney, 2007). We can also use
Bayesian inference techniques to avoid resorting to
heuristics that damage the probabilistic interpreta-
tion of the models (Zhang et al, 2008; DeNero et
al., 2008; Blunsom et al, 2009).
Most models define multiple derivations for each
translation; the probability of a translation is thus
the sum over all of its derivations. Unfortunately,
finding the maximum probability translation is NP-
hard for all but the most trivial of models in this
setting (Sima?an, 1996). It is thus necessary to resort
to approximations for this sum and the search for its
maximum e?.
The most common of these approximations is
the max-derivation approximation, which for many
models can be computed in polynomial time via
dynamic programming (DP). Though effective for
some problems, it has many serious drawbacks for
probabilistic inference:
1. It typically differs from the true model maxi-
mum.
2. It often requires additional approximations in
search, leading to further error.
3. It introduces restrictions on models, such as
use of only local features.
4. It provides no good solution to compute the
normalization factor Z(f) required by many prob-
abilistic algorithms.
In this work, we solve these problems using a
Monte Carlo technique with none of the above draw-
backs. Our technique is based on a novel Gibbs
sampler that draws samples from the posterior dis-
tribution of a phrase-based translation model (Koehn
et al, 2003) but operates in linear time with respect
to the number of input words (Section 2). We show
102
that it is effective for both decoding (Section 3) and
minimum risk training (Section 4).
2 A Gibbs sampler for phrase-based
translation models
We begin by assuming a phrase-based translation
model in which the input sentence, f , is segmented
into phrases, which are sequences of adjacent
words.1 Each foreign phrase is translated into the
target language, to produce an output sentence e
and an alignment a representing the mapping from
source to target phrases. Phrases are allowed to be
reordered during translation.
The model is defined with a log-linear form,
with feature function vector h and parametrised by
weight vector ?, as described in Koehn et al (2003).
P (e, a|f ;?) = exp [? ? h(e, a, f)]?
?e?,a?? exp [? ? h(e?, a?, f)]
(1)
The features h of the model are usually few and
are themselves typically probabilistic models
indicating e.g, the relative frequency of a target
phrase translation given a source phrase (translation
model), the fluency of the target phrase (language
model) and how phrases reorder with respect
to adjacent phrases (reordering model). There
is a further parameter ? that limits how many
source language words may intervene between
two adjacent target language phrases. For the
experiments in this paper, we use ? = 6.
2.1 Gibbs sampling
We use Markov chain Monte Carlo (MCMC) as an
alternative to DP search (Geman and Geman, 1984;
Metropolis and Ulam, 1949). MCMC probabilis-
tically generates sample derivations from the com-
plete search space. The probability of generating
each sample is conditioned on the previous sam-
ple, forming a Markov chain. After a long enough
interval (referred to as the burn-in) this chain returns
samples from the desired distribution.
Our MCMC sampler uses Gibbs sampling, which
obtains samples from the joint distribution of a set
of random variables X = {X1, . . . , Xn}. It starts
with some initial state (X1 = x10, . . . , Xn = xn0),
and generates a Markov chain of samples, where
1These phrases are not necessarily linguistically motivated.
each sample is the result of applying a set of Gibbs
operators to the previous sample. Each operator is
defined by specifying a subset of the random vari-
ables Y ? X , which the operator updates by sam-
pling from the conditional distribution P (Y |X \Y ).
The set X \ Y is referred to as the Markov blanket
and is unchanged by the operator.
In the case of translation, we require a Gibbs sam-
pler that produces a sequence of samples, SN1 =
(e1, a1) . . . (eN , aN ), that are drawn from the dis-
tribution P (e, a|f). These samples can thus be used
to estimate the expectation of a function h(e, a, f)
under the distribution as follows:
EP (a,e|f)[h] = limN??
1
N
N?
i=1
h(ai, ei, f) (2)
Taking h to be an indicator function
h = ?(a, a?)?(e, e?) provides an estimate of
P (a?, e?|f), and using h = ?(e, e?) marginalises over
all derivations a?, yielding an estimate of P (e?|f).
2.2 Gibbs operators
Our sampler consists of three operators. Examples
of these are depicted in Figure 1.
The RETRANS operator varies the translation of a
single source phrase. Segmentation, alignment, and
all other translations are held constant.
The MERGE-SPLIT operator varies the source
segmentation at a single word boundary. If the
boundary is a split point in the current hypothesis,
the adjoining phrases can be merged, provided
that the corresponding target phrases are adjacent
and the phrase table contains a translation of the
merged phrase. If the boundary is not a split point,
the covering phrase may be split, provided that
the phrase table contains a translation of both new
phrases. Remaining segmentation points, phrase
alignment and phrase translations are held constant.
The REORDER operator varies the target phrase
order for a pair of source phrases, provided that
the new alignment does not violate reordering limit
?. Segmentation, phrase translations, and all other
alignments are held constant.
To illustrate the RETRANS operator, we will
assume a simplified model with two features: a
bigram language model Plm and a translation model
Ptm. Both features are assigned a weight of 1.
103
c?est un re?sultat remarquable
it is some result remarkable
(a)
Initial
c?est un re?sultat remarquable
but some result remarkable
(b)
Retrans
c?est un re?sultat remarquable
it is a result remarkable
(c)
Merge
c?est un re?sultat remarquable
it is a remarkable result
(d)
Reorder
1
Figure 1: Example evolution of an initial hypothesis via
application of several operators, with Markov blanket
indicated by shading.
We denote the start of the sentence with S and the
language model context with C. Assuming the
French phrase c?est can be translated either as it is or
but, the RETRANS operator at step (b) stochastically
chooses an English phrase, e? in proportion to the
phrases? conditional probabilities.
P (but|c?est,C) = Ptm(but|c?est) ? Plm(S but some)Z
and
P (it is|c?est,C) = Ptm(it is|c?est) ? Plm(S it is some)Z
where
Z = Ptm(but|c?est) ? Plm(S but some) +
Ptm(it is|c?est) ? Plm(S it is some)
Conditional distributions for the MERGE-SPLIT and
REORDER operators can be derived in an analogous
fashion.
A complete iteration of the sampler consists of
applying each operator at each possible point in the
sentence, and a sample is collected after each opera-
tor has performed a complete pass.
2.3 Algorithmic complexity
Since both the RETRANS and MERGE-SPLIT oper-
ators are applied by iterating over source side word
positions, their complexity is linear in the size of the
input.
The REORDER operator iterates over the positions
in the input and for the source phrase found at that
position considers swapping its target phrase with
that of every other source phrase, provided that the
reordering limit is not violated. This means that it
can only consider swaps within a fixed-length win-
dow, so complexity is linear in sentence length.
2.4 Experimental verification
To verify that our sampler was behaving as expected,
we computed the KL divergence between its
inferred distribution q?(e|f) and the true distribution
over a single sentence (Figure 2). We computed
the true posterior distribution p(e|f) under an
Arabic-English phrase-based translation model
with parameters trained to maximise expected
BLEU (Section 4), summing out the derivations for
identical translations and computing the partition
term Z(f). As the number of iterations increases,
the KL divergence between the distributions
approaches zero.
3 Decoding
The task of decoding amounts to finding the single
translation e? that maximises or minimises some cri-
terion given a source sentence f . In this section
we consider three common approaches to decod-
ing, maximum translation (MaxTrans), maximum
derivation (MaxDeriv), and minimum risk decoding
(MinRisk):
e? =
?
?
?
argmax(e,a) p(e, a|f) (MaxDeriv)
argmaxe p(e|f) (MaxTrans)
argmine?e? `e?(e)p(e?|f) (MinRisk)
In the minimum risk decoder, `e?(e) is any real-
valued loss (error) function that computes the error
of one hypothesis e with respect to some reference
e?. Our loss is a sentence-level approximation of
(1 ? BLEU).
As noted in section 2, the Gibbs sampler can
be used to provide an estimate of the probability
distribution P (a, e|f) and therefore to determine
the maximum of this distribution, in other words
the most likely derivation. Furthermore, we can
marginalise over the alignments to estimate P (e|f)
104
Iterations
KL d
iverg
ence
l l l
l l
l
l l
l
l
l
l
l l l
l l
10 100 1000 10000 100000 1000000
0.00
1
0.01
0.1
KL Divergence
Figure 2: The KL divergence of the true posterior distri-
bution and the distribution estimated by the Gibbs sam-
pler at different numbers of iterations for the Arabic
source sentence r}ys wzrA? mAlyzyA yzwr Alflbyn (in
English, The prime minister of Malaysia visits the Philip-
pines).
and so obtain the most likely translation. The Gibbs
sampler can therefore be used as a decoder, either
running in max-derivation and max-translation
mode. Using the Gibbs sampler in this way makes
max-translation decoding tractable, and so will
help determine whether max-translation offers any
benefit over the usual max-derivation. Using the
Gibbs sampler as a decoder also allows us to verify
that it is producing valid samples from the desired
distribution.
3.1 Training data and preparation.
The experiments in this section were performed
using the French-English and German-English
parallel corpora from the WMT09 shared translation
task (Callison-Burch et al, 2009), as well as 300k
parallel Arabic-English sentences from the NIST
MT evaluation training data.2 For all language
pairs, we constructed a phrase-based translation
model as described in Koehn et al (2003), limiting
the phrase length to 5. The target side of the parallel
corpus was used to train a 3-gram language model.
2The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
sentences with confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For the German and French systems, the DEV2006
set was used for model tuning and the TEST2007
(in-domain) and NEWS-DEV2009B (out-of-domain)
sets for testing. For the Arabic system, the MT02
set (10 reference translations) was used for tuning
and MT03 (4 reference translations) was used for
evaluation. To reduce the size of the phrase table,
we used the association-score technique suggested
by Johnson et al (2007a). Translation quality is
reported using case-insensitive BLEU (Papineni et
al., 2002).
3.2 Translation performance
For the experiments reported in this section, we
used feature weights trained with minimum error
rate training (MERT; Och, 2003) . Because MERT
ignores the denominator in Equation 1, it is invari-
ant with respect to the scale of the weight vector
? ? the Moses implementation simply normalises
the weight vector it finds by its `1-norm. However,
when we use these weights in a true probabilistic
model, the scaling factor affects the behaviour of
the model since it determines how peaked or flat the
distribution is. If the scaling factor is too small, then
the distribution is too flat and the sampler spends
too much time exploring unimportant probability
regions. If it is too large, then the distribution is too
peaked and the sampler may concentrate on a very
narrow probability region. We optimised the scaling
factor on a 200-sentence portion of the tuning set,
finding that a multiplicative factor of 10 worked best
for fr-en and a multiplicative factor of 6 for de-en. 3
The first experiment shows the effect of different
initialisations and numbers of sampler iterations on
max-derivation decoding performance of the sam-
pler. The Moses decoder (Koehn et al, 2007) was
used to generate the starting hypothesis, either in
full DP max-derivation mode, or alternatively with
restrictions on the features and reordering, or with
zero weights to simulate a random initialisation, and
the number of iterations varied from 100 to 200,000,
with a 100 iteration burn-in in each case. Figure 3
shows the variation of model score with sampler iter-
ation, for the different starting points, and for both
language pairs.
3We experimented with annealing, where the scale factor is
gradually increased to sharpen the distribution while sampling.
However, we found no improvements with annealing.
105
?
20.1
?
20.0
?
19.9
?
19.8
?
19.7
?
19.6
Iterations
Mod
el sc
ore
100 1000 10000
French?English
Initialisationfull
mono
nolm
zero
?
40.6
?
40.4
?
40.2
?
40.0
?
39.8
Iterations
Mod
el sc
ore
100 1000 10000 100000
German?English
Initialisationfull
mono
nolm
zero
Figure 3: Mean maximum model score, as a function of iteration number and starting point. The starting point can
either be the full max-derivation translation (full), the monotone translation (mono), the monotone translation with no
language model (nolm) or the monotone translation with all weights set to zero (zero).
Comparing the best model scores found by the
sampler, with those found by the Moses decoder
with its default settings, we found that around
50,000 sampling iterations were required for
fr-en and 100,000 for de-en, for the sampler to
give equivalent model scores to Moses. From
Figure 3 we can see that the starting point did not
have an appreciable effect on the model score of
the best derivation, except with low numbers of
iterations. This indicates that the sampler is able
to move fairly quickly towards the maximum of
the distribution from any starting point, in other
words it has good mobility. Running the sampler
for 100,000 iterations took on average 1670 seconds
per sentence on the French-English data set and
1552 seconds per sentence on German-English.
A further indication of the dependence of sampler
accuracy on the iteration count is provided by Fig-
ure 4. In this graph, we show the mean Spearman?s
rank correlation between the nbest lists of deriva-
tions when ranked by (i) model score and (ii) the
posterior probability estimated by the sampler. This
measure of sampler accuracy also shows a logarith-
mic dependence on the sample size.
3.3 Minimum risk decoding
The sampler also allows us to perform minimum
Bayes risk (MBR) decoding, a technique introduced
by Kumar and Byrne (2004). In their work, as an
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Corr
elati
on
100 1000 10000 100000
Language Pairsfr?ende?en
Figure 4: Mean Spearman?s rank correlation of 1000-best
list of derivations ranked according to (i) model score and
(ii) posterior probability estimated by sampler. This was
measured on a 200 sentence subset of DEV2006.
approximation of the model probability distribution,
the expected loss of the decoder is calculated by
summing over an n-best list. With the Gibbs sam-
pler, however, we should be able to obtain a much
more accurate view of the model probability distri-
bution. In order to compare max-translation, max-
derivation and MBR decoding with the Gibbs sam-
pler, and the Moses baseline, we ran experiments
106
fr-en de-en
in out in out
Moses 32.7 19.1 27.4 15.9
MaxD 32.6 19.1 27.0 15.5
MaxT 32.6 19.1 27.4 16.0
MBR 32.6 19.2 27.3 16.0
Table 1: Comparison of the BLEU score of the Moses
decoder with the sampler running in max-derivation
(MaxD), max-translation (MaxT) and minumum Bayes
risk (MBR) modes. The test sets are TEST2007 (in) and
NEWS-DEV2009B (out)
on both European language pairs, using both the in-
domain and out-of-domain test sets. The sampler
was initialised with the output of Moses with the
feature weights set to zero and restricted to mono-
tone, and run for 100,000 iterations with a 100 iter-
ation burn-in. The scale factors were set to the same
values as in the previous experiment. The relative
translation quality (measured according to BLEU) is
shown in Table 1.
3.4 Discussion
These results show very little difference between the
decoding methods, indicating that the Gibbs sam-
pling decoder can perform as well as a standard DP
based max-derivation decoder with these models,
and that there is no gain from doing max-translation
or MBR decoding. However it should be noted that
the model used for these experiments was optimised
by MERT, for max-derivation decoding, and so the
experiments do not rule out the possibility that max-
translation and MBR decoding will offer an advan-
tage on an appropriately optimised model.
4 Minimum risk training
In the previous section, we described how our sam-
pler can be used to search for the best translation
under a variety of decoding criteria (max deriva-
tion, translation, and minimum risk). However, there
appeared to be little benefit to marginalizing over
the latent derivations. This is almost certainly a side
effect of the MERT training approach that was used
to construct the models so as to maximise the per-
formance of the model on its single best derivation,
without regard to the shape of the rest of the dis-
tribution (Blunsom et al, 2008). In this section we
describe a further application of the Gibbs sampler:
to do unbiased minimum risk training.
While there have been at least two previous
attempts to do minimum risk training for MT, both
approaches relied on biased k-best approximations
(Smith and Eisner, 2006; Zens and Ney, 2007).
Since we sample from the whole distribution, we
will have a more accurate risk assessment.
The risk, or expected loss, of a probabilistic trans-
lation model on a corpus D, defined with respect to
a particular loss function `e?(e), where e? is the refer-
ence translation and e is a hypothesis translation
L = ?
?e?,f??D
?
e
p(e|f)`e?(e) (3)
This value can be trivially computed using equa-
tion (2). In this section, we are concerned with find-
ing the parameters ? that minimise (3). Fortunately,
with the log-linear parameterization of p(e|f), L is
differentiable with respect to ?:
?L
??k
= ?
?e?,f??D
?
e
p(e|f)`e?(e)
(
hk ? Ep(e|f)[hk]
)
(4)
Equation (4) is slightly more complicated to com-
pute using the sampler since it requires the feature
expectation in order to evaluate the final term. How-
ever, this can be done simply by making two passes
over the samples, computing the feature expecta-
tions on the first pass and the gradient on the second.
We have now shown how to compute our
objective (3), the expected loss, and a gradient
with respect to the model parameters we want
to optimise, (4), so we can use any standard
first-order optimization technique. Since the
sampler introduces stochasticity into the gradient
and objective, we use stochastic gradient descent
methods which are more robust to noise than
more sophisticated quasi-Newtonian methods
like L-BFGS (Schraudolph et al, 2007). For the
experiments below, we updated the learning rate
after each step proportionally to difference in
successive gradients (Schraudolph, 1999).
For the experiments reported in this section, we
used sample sizes of 8000 and estimated the gradi-
ent on sets of 100 sentences drawn randomly (with
replacement) from the development corpus. For a
107
Training Decoder MT03
Moses Max Derivation 44.6
MERT Moses MBR 44.8
Gibbs MBR 44.9
Moses Max Derivation 40.6
MinRisk MaxTrans 41.8
Gibbs MBR 42.9
Table 2: Decoding with minimum risk trained systems,
compared with decoding with MERT-trained systems on
Arabic to English MT03 data
loss function we use 4-gram (1 ? BLEU) computed
individually for each sentence4. By examining per-
formance on held-out data, we find the model con-
verges typically in fewer than 20 iterations.
4.1 Training experiments
During preliminary experiments with training, we
observed on a held-out data set (portions of MT04)
that the magnitude of the weights vector increased
steadily (effectively sharpening the distribution), but
without any obvious change in the objective. Since
this resulted in poor generalization we added a reg-
ularization term of ||~? ? ~?||2/2?2 to L. We initially
set the means to zero, but after further observing that
the translations under all decoding criteria tended to
be shorter than the reference (causing a significant
drop in performance when evaluated using BLEU),
we found that performance could be improved by
setting ?WP = ?0.5, indicating a preference for a
lower weight on this parameter.
Table 2 compares the performance on Arabic to
English translation of systems tuned with MERT
(maximizing corpus BLEU) with systems tuned to
maximise expected sentence-level BLEU. Although
the performance of the minimum risk model under
all decoding criteria is lower than that of the orig-
inal MERT model, we note that the positive effect
of marginalizing over derivations as well as using
minimum risk decoding for obtaining good results
on this model. A full exploration of minimum risk
training is beyond the scope of this paper, but these
initial experiments should help emphasise the versa-
tility of the sampler and its utility in solving a variety
of problems. In the conclusion, we will, however,
4The ngram precision counts are smoothed by adding 0.01
for n > 1
discuss some possible future directions that can be
taken to make this style of training more competitive
with standard baseline systems.
5 Discussion and future work
We have described an algorithmic technique that
solves certain problems, but also verifies the utility
of standard approximation techniques. For exam-
ple, we found that on standard test sets the sampler
performs similarly to the DP max-derivation solu-
tion and equally well regardless of how it is ini-
tialised. From this we conclude that at least for
MERT-trained models, the max-derivation approx-
imation is adequate for finding the best translation.
Although the training approach presented in
Section 4 has a number of theoretical advantages,
its performance in a one-best evaluation falls short
when compared with a system tuned for optimal
one-best performance using MERT. This contradicts
the results of Zens and Ney (2007), who optimise
the same objective and report improvements over a
MERT baseline. We conjecture that the difference
is due to the biased k-best approximation they used.
By considering only the most probable derivations,
they optimise a smoothed error surface (as one
does in minimum risk training), but not one that
is indicative of the true risk. If our hypothesis
is accurate, then the advantage is accidental and
ultimately a liability. Our results are in line with
those reported by Smith and Eisner (2006) who
find degradation in performance when minimizing
risk, but compensate by ?sharpening? the model
distribution for the final training iterations,
effectively maximising one-best performance
rather minimising risk over the full distribution
defined by their model. In future work, we will
explore possibilities for artificially sharpening the
distribution during training so as to better anticipate
the one-best evaluation conditions typical of MT.
However, for applications which truly do require a
distribution over translations, such as re-ranking,
our method for minimising expected risk would be
the objective of choice.
Using sampling for model induction has two fur-
ther advantages that we intend to explore. First,
although MERT performs quite well on models with
108
small numbers of features (such as those we consid-
ered in this paper), in general the algorithm severely
limits the number of features that can be used since
it does not use gradient-based updates during opti-
mization, instead updating one feature at a time. Our
training method (Section 4) does not have this limi-
tation, so it can use many more features.
Finally, for the DP-based max-derivation approx-
imation to be computationally efficient, the features
characterizing the steps in the derivation must be
either computable independently of each other or
with only limited local context (as in the case of the
language model or distortion costs). This has led to
a situation where entire classes of potentially use-
ful features are not considered because they would
be impractical to integrate into a DP based trans-
lation system. With the sampler this restriction is
mitigated: any function of h(e, f, a) may partici-
pate in the translation model subject only to its own
computability. Freed from the rusty manacles of
dynamic programming, we anticipate development
of many useful features.
6 Related work
Our sampler is similar to the decoder of Germann
et al (2001), which starts with an approximate solu-
tion and then incrementally improves it via operators
such as RETRANS and MERGE-SPLIT. It is also
similar to the estimator of Marcu and Wong (2002),
who employ the same operators to search the align-
ment space from a heuristic initialisation. Although
the operators are similar, the use is different. These
previous efforts employed their operators in a greedy
hill-climbing search. In contrast, our operators are
applied probabilistically, making them theoretically
well-founded for a variety of inference problems.
Our use of Gibbs sampling follows from its
increasing use in Bayesian inference problems in
NLP (Finkel et al, 2006; Johnson et al, 2007b).
Most closely related is the work of DeNero
et al (2008), who derive a Gibbs sampler for
phrase-based alignment, using it to infer phrase
translation probabilities. The use of Monte Carlo
techniques to calculate posteriors is similar to that
of Chappelier and Rajman (2000) who use those
techniques to find the best parse under models where
the derivation and the parse are not isomorphic.
To our knowledge, we are the first to apply Monte
Carlo methods to maximum translation and mini-
mum risk translation. Approaches to the former
(Blunsom et al, 2008; May and Knight, 2006) rely
on dynamic programming techniques which do not
scale well without heuristic approximations, while
approaches to the latter (Smith and Eisner, 2006;
Zens et al, 2007) use biased k-best approximations.
7 Conclusion
We have described a Gibbs sampler for approxi-
mating two intractable problems in SMT: maximum
translation decoding (and its variant, minimum risk
decoding) and minimum risk training. By using
Monte Carlo techniques we avoid the biases associ-
ated with the more commonly used DP based max-
derivation (or k-best derivation) approximation. In
doing so we provide a further tool to the translation
community that we envision will allow the devel-
opment and analysis of increasing theoretically well
motivated techniques.
Acknowledgments
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001; and by the
EuroMatrix project funded by the European Commission
(6th Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. In Advances in Neu-
ral Information Processing Systems 21, pages 161?
168.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder,
editors. 2009. Proc. of Workshop on Machine Trans-
lations, Athens.
J.-C. Chappelier and M. Rajman. 2000. Monte-Carlo
sampling for NP-hard maximization problems in the
109
framework of weighted parsing. In Natural Language
Processing ? NLP 2000, number 1835 in Lecture Notes
in Artificial Intelligence, pages 106?117. Springer.
J. DeNero, A. Bouchard, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. of EMNLP.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
U. Germann, M. Jahr, K. Knight, D. Marcu, and
K. Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proceedings of ACL.
Association for Computational Linguistics, July.
J. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007a.
Improving translation quality by discarding most of
the phrasetable. In Proc. of EMNLP-CoNLL, Prague.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL-HLT, pages 139?
146, Rochester, New York, April.
P. Koehn, F. Och, and D.Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?
54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177?180, June.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133?139.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proc. of NAACL-HLT.
N. Metropolis and S. Ulam. 1949. The Monte Carlo
method. Journal of the American Statistical Associa-
tion, 44(247):335?341.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proc. of
COLING, Saarbrucken, Germany, July.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
N. N. Schraudolph, J. Yu, and S. Gu?nter. 2007. A
stochastic quasi-Newton method for online convex
optimization. In Proc. of Artificial Intelligence and
Statistics.
N. N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-
09-99, IDSIA.
K. Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree grammars. In
Proc. of COLING, Copenhagen.
D. A. Smith and J. Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING-ACL, pages 787?794.
R. Zens and H. Ney. 2007. Efficient phrase-table repre-
sentation for machine translation with applications to
online MT and speech translation. In Proc. of NAACL-
HLT, Rochester, New York.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP, pages 524?532, Prague,
Czech Republic.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proc. of ACL: HLT, pages
97?105, Columbus, Ohio.
110
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65?72,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Word Alignment with Conditional Random Fields
Phil Blunsom and Trevor Cohn
Department of Software Engineering and Computer Science
University of Melbourne
{pcbl,tacohn}@csse.unimelb.edu.au
Abstract
In this paper we present a novel approach
for inducing word alignments from sen-
tence aligned data. We use a Condi-
tional Random Field (CRF), a discrimina-
tive model, which is estimated on a small
supervised training set. The CRF is condi-
tioned on both the source and target texts,
and thus allows for the use of arbitrary
and overlapping features over these data.
Moreover, the CRF has efficient training
and decoding processes which both find
globally optimal solutions.
We apply this alignment model to both
French-English and Romanian-English
language pairs. We show how a large
number of highly predictive features can
be easily incorporated into the CRF, and
demonstrate that even with only a few hun-
dred word-aligned training sentences, our
model improves over the current state-of-
the-art with alignment error rates of 5.29
and 25.8 for the two tasks respectively.
1 Introduction
Modern phrase based statistical machine transla-
tion (SMT) systems usually break the translation
task into two phases. The first phase induces word
alignments over a sentence-aligned bilingual cor-
pus, and the second phase uses statistics over these
predicted word alignments to decode (translate)
novel sentences. This paper deals with the first of
these tasks: word alignment.
Most current SMT systems (Och and Ney,
2004; Koehn et al, 2003) use a generative model
for word alignment such as the freely available
GIZA++ (Och and Ney, 2003), an implementa-
tion of the IBM alignment models (Brown et al,
1993). These models treat word alignment as a
hidden process, and maximise the probability of
the observed (e, f) sentence pairs1 using the ex-
pectation maximisation (EM) algorithm. After the
maximisation process is complete, the word align-
ments are set to maximum posterior predictions of
the model.
While GIZA++ gives good results when trained
on large sentence aligned corpora, its generative
models have a number of limitations. Firstly,
they impose strong independence assumptions be-
tween features, making it very difficult to incor-
porate non-independent features over the sentence
pairs. For instance, as well as detecting that a
source word is aligned to a given target word,
we would also like to encode syntactic and lexi-
cal features of the word pair, such as their parts-
of-speech, affixes, lemmas, etc. Features such as
these would allow for more effective use of sparse
data and result in a model which is more robust
in the presence of unseen words. Adding these
non-independent features to a generative model
requires that the features? inter-dependence be
modelled explicitly, which often complicates the
model (eg. Toutanova et al (2002)). Secondly, the
later IBM models, such as Model 4, have to re-
sort to heuristic search techniques to approximate
forward-backward and Viterbi inference, which
sacrifice optimality for tractability.
This paper presents an alternative discrimina-
tive method for word alignment. We use a condi-
tional random field (CRF) sequence model, which
allows for globally optimal training and decod-
ing (Lafferty et al, 2001). The inference algo-
1We adopt the standard notation of e and f to denote the
target (English) and source (foreign) sentences, respectively.
65
rithms are tractable and efficient, thereby avoid-
ing the need for heuristics. The CRF is condi-
tioned on both the source and target sentences,
and therefore supports large sets of diverse and
overlapping features. Furthermore, the model al-
lows regularisation using a prior over the parame-
ters, a very effective and simple method for limit-
ing over-fitting. We use a similar graphical struc-
ture to the directed hidden Markov model (HMM)
from GIZA++ (Och and Ney, 2003). This mod-
els one-to-many alignments, where each target
word is aligned with zero or more source words.
Many-to-many alignments are recoverable using
the standard techniques for superimposing pre-
dicted alignments in both translation directions.
The paper is structured as follows. Section
2 presents CRFs for word alignment, describing
their form and their inference techniques. The
features of our model are presented in Section 3,
and experimental results for word aligning both
French-English and Romanian-English sentences
are given in Section 4. Section 5 presents related
work, and we describe future work in Section 6.
Finally, we conclude in Section 7.
2 Conditional random fields
CRFs are undirected graphical models which de-
fine a conditional distribution over a label se-
quence given an observation sequence. We use
a CRF to model many-to-one word alignments,
where each source word is aligned with zero or
one target words, and therefore each target word
can be aligned with many source words. Each
source word is labelled with the index of its
aligned target, or the special value null, denot-
ing no alignment. An example word alignment
is shown in Figure 1, where the hollow squares
and circles indicate the correct alignments. In this
example the French words une and autre would
both be assigned the index 24 ? for the English
word another ? when French is the source lan-
guage. When the source language is English, an-
other could be assigned either index 25 or 26; in
these ambiguous situations we take the first index.
The joint probability density of the alignment,
a (a vector of target indices), conditioned on the
source and target sentences, e and f , is given by:
p?(a|e, f) =
exp
?
t
?
k ?khk(t, at?1, at, e, f)
Z?(e, f)
(1)
where we make a first order Markov assumption
 
they
are
constrained
by
limits
which
are
imposed
in
order
to
ensure
that
the
freedom
of
one
person
does
not
violate
that
of
another
.
  .
au
treun
edece
llesu
r
pa
s
em
pi?
tene
pe
rso
nn
e
un
ede
libe
rt?laqu
e
ga
ran
tir
po
ur
fix?
es?t?on
t
qu
i
lim
ite
s
ce
rta
ine
spa
r
res
tre
int
s
so
ntils 
Figure 1. A word-aligned example from the Canadian
Hansards test set. Hollow squares represent gold stan-
dard sure alignments, circles are gold possible align-
ments, and filled squares are predicted alignments.
over the alignment sequence. Here t ranges over
the indices of the source sentence (f ), k ranges
over the model?s features, and ? = {?k} are the
model parameters (weights for their correspond-
ing features). The feature functions hk are pre-
defined real-valued functions over the source and
target sentences coupled with the alignment labels
over adjacent times (source sentence locations),
t. These feature functions are unconstrained, and
may represent overlapping and non-independent
features of the data. The distribution is globally
normalised by the partition function, Z?(e, f),
which sums out the numerator in (1) for every pos-
sible alignment:
Z?(e, f) =
?
a
exp
?
t
?
k
?khk(t, at?1, at, e, f)
We use a linear chain CRF, which is encoded in
the feature functions of (1).
The parameters of the CRF are usually esti-
mated from a fully observed training sample (word
aligned), by maximising the likelihood of these
data. I.e. ?ML = argmax? p?(D), where D =
{(a, e, f)} are the training data. Because max-
imum likelihood estimators for log-linear mod-
els have a tendency to overfit the training sam-
ple (Chen and Rosenfeld, 1999), we define a prior
distribution over the model parameters and de-
rive a maximum a posteriori (MAP) estimate,
?MAP = argmax? p?(D)p(?). We use a zero-
mean Gaussian prior, with the probability density
function p0(?k) ? exp
(
?
?2k
2?2k
)
. This yields a
log-likelihood objective function of:
L =
?
(a,e,f)?D
log p?(a|e, f) +
?
k
log p0(?k)
66
=
?
(a,e,f)?D
?
t
?
k
?khk(t, at?1, at, e, f)
? logZ?(e, f)?
?
k
?2k
2?2k
+ const. (2)
In order to train the model, we maximize (2).
While the log-likelihood cannot be maximised for
the parameters, ?, in closed form, it is a con-
vex function, and thus we resort to numerical op-
timisation to find the globally optimal parame-
ters. We use L-BFGS, an iterative quasi-Newton
optimisation method, which performs well for
training log-linear models (Malouf, 2002; Sha
and Pereira, 2003). Each L-BFGS iteration re-
quires the objective value and its gradient with
respect to the model parameters. These are cal-
culated using forward-backward inference, which
yields the partition function, Z?(e, f), required
for the log-likelihood, and the pair-wise marginals,
p?(at?1, at|e, f), required for its derivatives.
The Viterbi algorithm is used to find the maxi-
mum posterior probability alignment for test sen-
tences, a? = argmaxa p?(a|e, f). Both the
forward-backward and Viterbi algorithm are dy-
namic programs which make use of the Markov
assumption to calculate efficiently the exact
marginal distributions.
3 The alignment model
Before we can apply our CRF alignment model,
we must first specify the feature set ? the func-
tions hk in (1). Typically CRFs use binary indica-
tor functions as features; these functions are only
active when the observations meet some criteria
and the label at (or label pair, (at?1, at)) matches
a pre-specified label (pair). However, in our model
the labellings are word indices in the target sen-
tence and cannot be compared readily to labellings
at other sites in the same sentence, or in other sen-
tences with a different length. Such naive features
would only be active for one labelling, therefore
this model would suffer from serious sparse data
problems.
We instead define features which are functions
of the source-target word match implied by a la-
belling, rather than the labelling itself. For exam-
ple, from the sentence in Figure 1 for the labelling
of f24 = de with a24 = 16 (for e16 = of ) we
might detect the following feature:
h(t, at?1, at, f , e) =
{
1, if eat = ?of? ? ft = ?de?
0, otherwise
Note that it is the target word indexed by at, rather
than the index itself, which determines whether
the feature is active, and thus the sparsity of the
index label set is not an issue.
3.1 Features
One of the main advantages of using a conditional
model is the ability to explore a diverse range of
features engineered for a specific task. In our
CRFmodel we employ two main types of features:
those defined on a candidate aligned pair of words;
and Markov features defined on the alignment se-
quence predicted by the model.
Dice and Model 1 As we have access to only a
small amount of word aligned data we wish to be
able to incorporate information about word associ-
ation from any sentence aligned data available. A
common measure of word association is the Dice
coefficient (Dice, 1945):
Dice(e, f) =
2? CEF (e, f)
CE(e) + CF (e)
where CE and CF are counts of the occurrences
of the words e and f in the corpus, while CEF is
their co-occurrence count. We treat these Dice val-
ues as translation scores: a high (low) value inci-
dates that the word pair is a good (poor) candidate
translation.
However, the Dice score often over-estimates
the association between common words. For in-
stance, the words the and of both score highly
when combined with either le or de, simply be-
cause these common words frequently co-occur.
The GIZA++ models can be used to provide better
translation scores, as they enforce competition for
alignment beween the words. For this reason, we
used the translation probability distribution from
Model 1 in addition to the DICE scores. Model 1
is a simple position independent model which can
be trained quickly and is often used to bootstrap
parameters for more complex models. It models
the conditional probability distribution:
p(f ,a|e) =
p(|f |||e|)
(|e|+ 1)|f |
?
|f |?
t=1
p(ft|eat)
where p(f |e) are the word translation probabili-
ties.
We use both the Dice value and the Model 1
translation probability as real-valued features for
each candidate pair, as well as a normalised score
67
over all possible candidate alignments for each tar-
get word. We derive a feature from both the Dice
and Model 1 translation scores to allow compe-
tition between sources words for a particular tar-
get algnment. This feature indicates whether a
given alignment has the highest translation score
of all the candidate alignments for a given tar-
get word. For the example in Figure 1, the words
la, de and une all receive a high translation score
when paired with the. To discourage all of these
French words from aligning with the, the best of
these (la) is flagged as the best candidate. This al-
lows for competition between source words which
would otherwise not occur.
Orthographic features Features based on
string overlap allow our model to recognise
cognates and orthographically similar translation
pairs, which are particularly common between
European languages. Here we employ a number
of string matching features inspired by similar
features in Taskar et al (2005). We use an indica-
tor feature for every possible source-target word
pair in the training data. In addition, we include
indicator features for an exact string match, both
with and without vowels, and the edit-distance
between the source and target words as a real-
valued feature. We also used indicator features to
test for matching prefixes and suffixes of length
three. As stated earlier, the Dice translation
score often erroneously rewards alignments with
common words. In order to address this problem,
we include the absolute difference in word length
as a real-valued feature and an indicator feature
testing whether both words are shorter than 4
characters. Together these features allow the
model to disprefer alignments between words
with very different lengths ? i.e. aligning rare
(long) words with frequent (short) determiners,
verbs etc.
POS tags Part-of-speech tags are an effective
method for addressing the sparsity of the lexi-
cal features. Observe in Figure 2 that the noun-
adjective pair Canadian experts aligns with the
adjective-noun pair spe?cialistes canadiens: the
alignment exactly matches the parts-of-speech.
Access to the words? POS tags will allow simple
modelling of such effects. POS can also be useful
for less closely related language pairs, such as En-
glish and Japanese where English determiners are
never aligned; nor are Japanese case markers.
For our French-English language pair we POS
tagged the source and target sentences with Tree-
Tagger.2 We created indicator features over the
POS tags of each candidate source and target word
pair, as well as over the source word and target
POS (and vice-versa). As we didn?t have access to
a Romanian POS tagger, these features were not
used for the Romanian-English language pair.
Bilingual dictionary Dictionaries are another
source of information for word alignment. We
use a single indicator feature which detects when
the source and target words appear in an entry of
the dictionary. For the English-French dictionary
we used FreeDict,3 which contains 8,799 English
words. For Romanian-English we used a dictio-
nary compiled by Rada Mihalcea,4 which contains
approximately 38,000 entries.
Markov features Features defined over adja-
cent aligment labels allow our model to reflect the
tendency for monotonic alignments between Eu-
ropean languages. We define a real-valued align-
ment index jump width feature:
jump width(t? 1, t) = abs(at ? at?1 ? 1)
this feature has a value of 0 if the alignment labels
follow the downward sloping diagonal, and is pos-
itive otherwise. This differs from the GIZA++ hid-
den Markov model which has individual parame-
ters for each different jump width (Och and Ney,
2003; Vogel et al, 1996): we found a single fea-
ture (and thus parameter) to be more effective.
We also defined three indicator features over
null transitions to allow the modelling of the prob-
ability of transition between, to and from null la-
bels.
Relative sentence postion A feature for the
absolute difference in relative sentence position
(abs( at|e| ?
t
|f |)) allows the model to learn a pref-
erence for aligning words close to the alignment
matrix diagonal. We also included two conjunc-
tion features for the relative sentence position mul-
tiplied by the Dice and Model 1 translation scores.
Null We use a number of variants on the above
features for alignments between a source word and
the null target. The maximum translation score
between the source and one of the target words
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
3http://www.freedict.de
4http://lit.csci.unt.edu/?rada/downloads/RoNLP/R.E.tralex
68
model precision recall f-score AER
Model 4 refined 87.4 95.1 91.1 9.81
Model 4 intersection 97.9 86.0 91.6 7.42
French? English 96.7 85.0 90.5 9.21
English? French 97.3 83.0 89.6 10.01
intersection 98.7 78.6 87.5 12.02
refined 95.7 89.2 92.3 7.37
Table 1. Results on the Hansard data using all features
model precision recall f-score AER
Model 4 refined 80.49 64.10 71,37 28.63
Model 4 intersected 95.94 53.56 68.74 31.26
Romanian? English 82.9 61.3 70.5 29.53
English? Romanian 82.8 60.6 70.0 29.98
intersection 94.4 52.5 67.5 32.45
refined 77.1 68.5 72.6 27.41
Table 2. Results on the Romanian data using all fea-
tures
is used as a feature to represent whether there is
a strong alignment candidate. The sum of these
scores is also used as a feature. Each source word
and POS tag pair are used as indicator features
which allow the model to learn particular words
of tags which tend to commonly (or rarely) align.
3.2 Symmetrisation
In order to produce many-to-many alignments we
combine the outputs of two models, one for each
translation direction. We use the refined method
from Och and Ney (2003) which starts from the
intersection of the two models? predictions and
?grows? the predicted alignments to neighbouring
alignments which only appear in the output of one
of the models.
4 Experiments
We have applied our model to two publicly avail-
able word aligned corpora. The first is the
English-French Hansards corpus, which consists
of 1.1 million aligned sentences and 484 word-
aligned sentences. This data set was used for
the 2003 NAACL shared task (Mihalcea and Ped-
ersen, 2003), where the word-aligned sentences
were split into a 37 sentence trial set and a 447 sen-
tence testing set. Unlike the unsupervised entrants
in the 2003 task, we require word-aligned training
data, and therefore must cannibalise the test set for
this purpose. We follow Taskar et al (2005) by us-
ing the first 100 test sentences for training and the
remaining 347 for testing. This means that our re-
sults should not be directly compared to those en-
trants, other than in an approximate manner. We
used the original 37 sentence trial set for feature
engineering and for fitting a Gaussian prior.
The word aligned data are annotated with both
sure (S) and possible (P ) alignments (S ? P ; Och
and Ney (2003)), where the possible alignments
indicate ambiguous or idiomatic alignments. We
measure the performance of our model using
alignment error rate (AER), which is defined as:
AER(A,S, P ) = 1?
|A ? S|+ |A ? P |
|A|+ |S|
where A is the set of predicted alignments.
The second data set is the Romanian-English
parallel corpus from the 2005 ACL shared task
(Martin et al, 2005). This consists of approxi-
mately 50,000 aligned sentences and 448 word-
aligned sentences, which are split into a 248 sen-
tence trial set and a 200 sentence test set. We
used these as our training and test sets, respec-
tively. For parameter tuning, we used the 17 sen-
tence trial set from the Romanian-English corpus
in the 2003 NAACL task (Mihalcea and Pedersen,
2003). For this task we have used the same test
data as the competition entrants, and therefore can
directly compare our results. The word alignments
in this corpus were only annotated with sure (S)
alignments, and therefore the AER is equivalent
to the F1 score. In the shared task it was found
that models which were trained on only the first
four letters of each word obtained superior results
to those using the full words (Martin et al, 2005).
We observed the same result with our model on
the trial set and thus have only used the first four
letters when training the Dice and Model 1 trans-
lation probabilities.
Tables 1 and 2 show the results when all feature
types are employed on both language pairs. We re-
port the results for both translation directions and
when combined using the refined and intersection
methods. The Model 4 results are from GIZA++
with the default parameters and the training data
lowercased. For Romanian, Model 4 was trained
using the first four letters of each word.
The Romanian results are close to the best re-
ported result of 26.10 from the ACL shared task
(Martin et al, 2005). This result was from a sys-
tem based on Model 4 plus additional parameters
such as a dictionary. The standard Model 4 imple-
mentation in the shared task achieved a result of
31.65, while when only the first 4 letters of each
word were used it achieved 28.80.5
5These results differ slightly our Model 4 results reported
in Table 2.
69
 
(
ii
)
(
a
)
Three
vehicles
will
be
used
by
six
Canadian
experts
related
to
the
provision
of
technical
assistance
.
  .
tec
hn
iqu
e
aid
ede
pre
sta
tio
nlade
ca
dreleda
ns
ca
na
die
ns
sp
?c
iali
ste
s6pa
r
uti
lis?
s
se
ron
t
v?
hic
ule
s3)a)ii( 
(a) With Markov features
 
(
ii
)
(
a
)
Three
vehicles
will
be
used
by
six
Canadian
experts
related
to
the
provision
of
technical
assistance
.
  .
tec
hn
iqu
e
aid
ede
pre
sta
tio
nlade
ca
dreleda
ns
ca
na
die
ns
sp
?c
iali
ste
s6pa
r
uti
lis?
s
se
ron
t
v?
hic
ule
s3)a)ii( 
(b) Without Markov features
Figure 2. An example from the Hansard test set, showing the effect of the Markov features.
Table 3 shows the effect of removing each of the
feature types in turn from the full model. The most
useful features are the Dice and Model 1 values
which allow the model to incorporate translation
probabilities from the large sentence aligned cor-
pora. This is to be expected as the amount of word
aligned data are extremely small, and therefore the
model can only estimate translation probabilities
for only a fraction of the lexicon. We would expect
the dependence on sentence aligned data to de-
crease as more word aligned data becomes avail-
able.
The effect of removing the Markov features can
be seen from comparing Figures 2 (a) and (b). The
model has learnt to prefer alignments that follow
the diagonal, thus alignments such as 3 ? three
and prestation ? provision are found, and miss-
alignments such as de ? of, which lie well off the
diagonal, are avoided.
The differing utility of the alignment word pair
feature between the two tasks is probably a result
of the different proportions of word- to sentence-
aligned data. For the French data, where a very
large lexicon can be estimated from the million
sentence alignments, the sparse word pairs learnt
on the word aligned sentences appear to lead to
overfitting. In contrast, for Romanian, where more
word alignments are used to learn the translation
pair features and much less sentence aligned data
are available, these features have a significant im-
pact on the model. Suprisingly the orthographic
features actually worsen the performance in the
tasks (incidentally, these features help the trial
set). Our explanation is that the other features
(eg. Model 1) already adequately model these cor-
respondences, and therefore the orthographic fea-
feature group Rom? Eng Fre? Eng
ALL 27.41 7.37
?orthographic 27.30 7.25
?Dice 27.68 7.73
?dictionary 27.72 7.21
?sentence position 28.30 8.01
?POS ? 8.19
?Model 1 28.62 8.45
?alignment word pair 32.41 7.20
?Markov 32.75 12.44
?Dice & ?Model 1 35.43 14.10
Table 3. The resulting AERs after removing individual
groups of features from the full model.
tures do not add much additional modelling power.
We expect that with further careful feature engi-
neering, and a larger trial set, these orthographic
features could be much improved.
The Romanian-English language pair appears
to offer a more difficult modelling problem than
the French-English pair. With both the transla-
tion score features (Dice and Model 1) removed
? the sentence aligned data are not used ? the
AER of the Romanian is more than twice that of
the French, despite employing more word aligned
data. This could be caused by the lack of possi-
ble (P) alignment markup in the Romanian data,
which provide a boost in AER on the French data
set, rewarding what would otherwise be consid-
ered errors. Interestingly, without any features
derived from the sentence aligned corpus, our
model achieves performance equivalent to Model
3 trained on the full corpus (Och and Ney, 2003).
This is a particularly strong result, indicating that
this method is ideal for data-impoverished align-
ment tasks.
70
4.1 Training with possible alignments
Up to this point our Hansards model has been
trained using only the sure (S) alignments. As
the data set contains many possible (P) alignments,
we would like to use these to improve our model.
Most of the possible alignments flag blocks of
ambiguous or idiomatic (or just difficult) phrase
level alignments. These many-to-many align-
ments cannot be modelled with our many-to-one
setup. However, a number of possibles flag one-
to-one or many-to-one aligments: for this experi-
ment we used these possibles in training to inves-
tigate their effect on recall. Using these additional
alignments our refined precision decreased from
95.7 to 93.5, while recall increased from 89.2 to
92.4. This resulted in an overall decrease in AER
to 6.99. We found no benefit from using many-to-
many possible alignments as they added a signifi-
cant amount of noise to the data.
4.2 Model 4 as a feature
Previous work (Taskar et al, 2005) has demon-
strated that by including the output of Model 4 as
a feature, it is possible to achieve a significant de-
crease in AER. We trained Model 4 in both direc-
tions on the two language pairs. We added two
indicator features (one for each direction) to our
CRF which were active if a given word pair were
aligned in the Model 4 output. Table 4 displays
the results on both language pairs when these ad-
ditional features are used with the refined model.
This produces a large increase in performance, and
when including the possibles, produces AERs of
5.29 and 25.8, both well below that of Model 4
alone (shown in Tables 1 and 2).
4.3 Cross-validation
Using 10-fold cross-validation we are able to gen-
erate results on the whole of the Hansards test data
which are comparable to previously published re-
sults. As the sentences in the test set were ran-
domly chosen from the training corpus we can ex-
pect cross-validation to give an unbiased estimate
of generalisation performance. These results are
displayed in Table 5, using the possible (P) align-
ments for training. As the training set for each fold
is roughly four times as big previous training set,
we see a small improvement in AER.
The final results of 6.47 and 5.19 with and
without Model 4 features both exceed the perfor-
mance of Model 4 alone. However the unsuper-
model precision recall f-score AER
Rom? Eng 79.0 70.0 74.2 25.8
Fre? Eng 97.9 90.8 94.2 5.49
Fre? Eng (P) 95.5 93.7 94.6 5.29
Table 4. Results using features from Model 4 bi-
directional alignments, training with and without the
possible (P) alignments.
model precision recall f-score AER
Fre? Eng 94.6 92.2 93.4 6.47
Fre? Eng (Model 4) 96.1 93.3 94.7 5.19
Table 5. 10-fold cross-validation results, with and with-
out Model 4 features.
vised Model 4 did not have access to the word-
alignments in our training set. Callison-Burch et
al. (2004) demonstrated that the GIZA++ mod-
els could be trained in a semi-supervised manner,
leading to a slight decrease in error. To our knowl-
edge, our AER of 5.19 is the best reported result,
generative or discriminative, on this data set.
5 Related work
Recently, a number of discriminative word align-
ment models have been proposed, however these
early models are typically very complicated with
many proposing intractable problems which re-
quire heuristics for approximate inference (Liu et
al., 2005; Moore, 2005).
An exception is Taskar et al (2005) who pre-
sented a word matching model for discriminative
alignment which they they were able to solve opti-
mally. However, their model is limited to only pro-
viding one-to-one alignments. Also, no features
were defined on label sequences, which reduced
the model?s ability to capture the strong monotonic
relationships present between European language
pairs. On the French-English Hansards task, using
the same training/testing setup as our work, they
achieve an AER of 5.4 with Model 4 features, and
10.7 without (compared to 5.29 and 6.99 for our
CRF). One of the strengths of the CRF MAP es-
timation is the powerful smoothing offered by the
prior, which allows us to avoid heuristics such as
early stopping and hand weighted loss-functions
that were needed for the maximum-margin model.
Liu et al (2005) used a conditional log-linear
model with similar features to those we have em-
ployed. They formulated a global model, without
making a Markovian assumption, leading to the
need for a sub-optimal heuristic search strategies.
Ittycheriah and Roukos (2005) trained a dis-
71
criminative model on a corpus of ten thousand
word aligned Arabic-English sentence pairs that
outperformed a GIZA++ baseline. As with other
approaches, they proposed a model which didn?t
allow a tractably optimal solution and thus had to
resort to a heuristic beam search. They employed
a log-linear model to learn the observation proba-
bilities, while using a fixed transition distribution.
Our CRF model allows both the observation and
transition components of the model to be jointly
optimised from the corpus.
6 Further work
The results presented in this paper were evaluated
in terms of AER. While a low AER can be ex-
pected to improve end-to-end translation quality,
this is may not necessarily be the case. There-
fore, we plan to assess how the recall and preci-
sion characteristics of our model affect translation
quality. The tradeoff between recall and precision
may affect the quality and number of phrases ex-
tracted for a phrase translation table.
7 Conclusion
We have presented a novel approach for induc-
ing word alignments from sentence aligned data.
We showed how conditional random fields could
be used for word alignment. These models al-
low for the use of arbitrary and overlapping fea-
tures over the source and target sentences, making
the most of small supervised training sets. More-
over, we showed how the CRF?s inference and es-
timation methods allowed for efficient processing
without sacrificing optimality, improving on pre-
vious heuristic based approaches.
On both French-English and Romanian-English
we showed that many highly predictive features
can be easily incorporated into the CRF, and
demonstrated that with only a few hundred word-
aligned training sentences, our model outperforms
the generativeModel 4 baseline. When no features
are extracted from the sentence aligned corpus our
model still achieves a low error rate. Furthermore,
when we employ features derived from Model 4
alignments our CRF model achieves the highest
reported results on both data sets.
Acknowledgements
Special thanks to Miles Osborne, Steven Bird,
Timothy Baldwin and the anonymous reviewers
for their feedback and insightful comments.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
C. Callison-Burch, D. Talbot, and M. Osborne. 2004. Statis-
tical machine translation with word- and sentence-aligned
parallel corpora. In Proceedings of ACL, pages 175?182,
Barcelona, Spain, July.
S. Chen and R. Rosenfeld. 1999. A survey of smoothing
techniques for maximum entropy models. IEEE Transac-
tions on Speech and Audio Processing, 8(1):37?50.
L. R. Dice. 1945. Measures of the amount of ecologic asso-
ciation between species. Journal of Ecology, 26:297?302.
A. Ittycheriah and S. Roukos. 2005. A maximum entropy
word aligner for Arabic-English machine translation. In
Proceedings of HLT-EMNLP, pages 89?96, Vancouver,
British Columbia, Canada, October.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT-NAACL, pages
81?88, Edmonton, Alberta.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labelling sequence data. In Proceedings of ICML, pages
282?289.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In Proceedings of ACL, pages 459?466, Ann
Arbor.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proceedings of CoNLL,
pages 49?55.
J. Martin, R. Mihalcea, and T. Pedersen. 2005. Word align-
ment for languages with scarce resources. In Proceed-
ings of the ACL Workshop on Building and Using Parallel
Texts, pages 65?74, Ann Arbor, Michigan, June.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proceedings of HLT-NAACL
2003 Workshop, Building and Using Parrallel Texts: Data
Driven Machine Translation and Beyond, pages 1?6, Ed-
monton, Alberta.
R. C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of HLT-EMNLP,
pages 81?88, Vancouver, Canada.
F. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguis-
tics, 29(1):19?52.
F. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of HLT-NAACL,
pages 213?220.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrimi-
native matching approach to word alignment. In Proceed-
ings of HLT-EMNLP, pages 73?80, Vancouver, British
Columbia, Canada, October.
K. Toutanova, H. Tolga Ilhan, and C Manning. 2002. Ex-
tentions to HMM-based statistical word alignment mod-
els. In Proceedings of EMNLP, pages 87?94, Philadel-
phia, July.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In Proceedings of 16th
Int. Conf. on Computational Linguistics, pages 836?841.
72
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 169?172, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labelling with
Tree Conditional Random Fields
Trevor Cohn and Philip Blunsom
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au and pcbl@csse.unimelb.ed.au
Abstract
In this paper we apply conditional
random fields (CRFs) to the semantic
role labelling task. We define a random
field over the structure of each sentence?s
syntactic parse tree. For each node
of the tree, the model must predict a
semantic role label, which is interpreted
as the labelling for the corresponding
syntactic constituent. We show how
modelling the task as a tree labelling
problem allows for the use of efficient
CRF inference algorithms, while also
increasing generalisation performance
when compared to the equivalent
maximum entropy classifier. We have
participated in the CoNLL-2005 shared
task closed challenge with full syntactic
information.
1 Introduction
The semantic role labelling task (SRL) involves
identifying which groups of words act as arguments
to a given predicate. These arguments must
be labelled with their role with respect to the
predicate, indicating how the proposition should be
semantically interpreted.
We apply conditional random fields (CRFs) to
the task of SRL proposed by the CoNLL shared
task 2005 (Carreras and Ma`rquez, 2005). CRFs are
undirected graphical models which define a condi-
tional distribution over labellings given an obser-
vation (Lafferty et al, 2001). These models allow
for the use of very large sets of arbitrary, over-
lapping and non-independent features. CRFs have
been applied with impressive empirical results to the
tasks of named entity recognition (McCallum and
Li, 2003; Cohn et al, 2005), part-of-speech (PoS)
tagging (Lafferty et al, 2001), noun phrase chunk-
ing (Sha and Pereira, 2003) and extraction of table
data (Pinto et al, 2003), among other tasks.
While CRFs have not been used to date for SRL,
their close cousin, the maximum entropy model has
been, with strong generalisation performance (Xue
and Palmer, 2004; Lim et al, 2004). Most CRF
implementations have been specialised to work with
chain structures, where the labels and observations
form a linear sequence. Framing SRL as a linear
tagging task is awkward, as there is no easy model
of adjacency between the candidate constituent
phrases.
Our approach simultaneously performs both con-
stituent selection and labelling, by defining an undi-
rected random field over the parse tree. This allows
the modelling of interactions between parent and
child constituents, and the prediction of an optimal
argument labelling for all constituents in one pass.
The parse tree forms an acyclic graph, meaning that
efficient exact inference in a CRF is possible using
belief propagation.
2 Data
The data used for this task was taken from the
Propbank corpus, which supplements the Penn
Treebank with semantic role annotation. Full details
of the data set are provided in Carreras and Ma`rquez
(2005).
2.1 Data Representation
From each training instance we derived a tree, using
the parse structure from the Collins parser. The
169
nodes in the trees were relabelled with a semantic
role label indicating how their corresponding syn-
tactic constituent relates to each predicate, as shown
in Figure 1. The role labels are shown as subscripts
in the figure, and both the syntactic categories and
the words at the leaves are shown for clarity only
? these were not included in the tree. Addition-
ally, the dashed lines show those edges which were
pruned, following Xue and Palmer (2004) ? only
nodes which are siblings to a node on the path from
the verb to the root are included in the tree. Child
nodes of included prepositional phrase nodes are
also included. This reduces the size of the resultant
tree whilst only very occasionally excluding nodes
which should be labelled as an argument.
The tree nodes were labelled such that only argu-
ment constituents received the argument label while
all argument children were labelled as outside, O.
Where there were parse errors, such that no con-
stituent exactly covered the token span of an argu-
ment, the smaller subsumed constituents were all
given the argument label.
We experimented with two alternative labelling
strategies: labelling a constituent?s children with a
new ?inside? label, and labelling the children with
the parent?s argument label. In the figure, the IN and
NP children of the PP would be affected by these
changes, both receiving either the inside I label or
AM-LOC label under the respective strategies. The
inside strategy performed nearly identically to the
standard (outside) strategy, indicating that either the
model cannot reliably predict the inside argument,
or that knowing that the children of a given node are
inside an argument is not particularly useful in pre-
dicting its label. The second (duplication) strategy
performed extremely poorly. While this allowed the
internal argument nodes to influence their ancestor
towards a particular labelling, it also dramatically
increased the number of nodes given an argument
label. This lead to spurious over-prediction of argu-
ments.
The model is used for decoding by predicting the
maximum probability argument label assignment to
each of the unlabelled trees. When these predic-
tions were inconsistent, and one argument subsumed
another, the node closest to the root of the tree was
deemed to take precedence over its descendants.
3 Model
We define a CRF over the labelling y given the
observation tree x as:
p(y|x) =
1
Z(x)
exp
?
c?C
?
k
?kfk(c,yc,x)
where C is the set of cliques in the observation tree,
?k are the model?s parameters and fk(?) is the fea-
ture function which maps a clique labelling to a vec-
tor of scalar values. The function Z(?) is the nor-
malising function, which ensures that p is a valid
probability distribution. This can be restated as:
p(y|x) =
1
Z(x)
exp
?
?
?
?
v?C1
?
k
?kgk(v,yv,x)
+
?
u,v?C2
?
j
?jhj(u, v,yu,yv,x)
?
?
?
where C1 are the vertices in the graph and C2 are
the maximal cliques in the graph, consisting of all
(parent, child) pairs. The feature function has been
split into g and h, each dealing with one and two
node cliques respectively.
Preliminary experimentation without any
pair-wise features (h), was used to mimic a
simple maximum entropy classifier. This model
performed considerably worse than the model
with the pair-wise features, indicating that the
added complexity of modelling the parent-child
interactions provides for more accurate modelling
of the data.
The log-likelihood of the training sample was
optimised using limited memory variable metric
(LMVM), a gradient based technique. This required
the repeated calculation of the log-likelihood and
its derivative, which in turn required the use of
dynamic programming to calculate the marginal
probability of each possible labelling of every clique
using the sum-product algorithm (Pearl, 1988).
4 Features
As the conditional random field is conditioned on
the observation, it allows feature functions to be
defined over any part of the observation. The tree
structure requires that features incorporate either a
node labelling or the labelling of a parent and its
170
SNP NP VP
DT NN NN NN
JJ NN V NP PP
CD NNS NP
DT NNP
IN
The luxury auto maker last year sold 1,214 cars in the US
O
A0
A1 AM-LOCV
AM-TMP O
O O
Figure 1: Syntax tree labelled for semantic roles with respect to the predicate sell. The subscripts show the
role labels, and the dotted and dashed edges are those which are pruned from the tree.
child. We have defined node and pairwise clique fea-
tures using data local to the corresponding syntactic
node(s), as well as some features on the predicate
itself.
Each feature type has been made into binary fea-
ture functions g and h by combining (feature type,
value) pairs with a label, or label pair, where this
combination was seen at least once in the training
data. The following feature types were employed,
most of which were inspired by previous works:
Basic features: {Head word, head PoS, phrase
syntactic category, phrase path, position rel-
ative to the predicate, surface distance to the
predicate, predicate lemma, predicate token,
predicate voice, predicate sub-categorisation,
syntactic frame}. These features are common
to many SRL systems and are described in Xue
and Palmer (2004).
Context features {Head word of first NP in prepo-
sition phrase, left and right sibling head words
and syntactic categories, first and last word
in phrase yield and their PoS, parent syntactic
category and head word}. These features are
described in Pradhan et al (2005).
Common ancestor of the verb The syntactic cate-
gory of the deepest shared ancestor of both the
verb and node.
Feature conjunctions The following features were
conjoined: { predicate lemma + syntactic cate-
gory, predicate lemma + relative position, syn-
tactic category + first word of the phrase}.
Default feature This feature is always on, which
allows the classifier to model the prior prob-
ability distribution over the possible argument
labels.
Joint features These features were only defined
over pair-wise cliques: {whether the parent
and child head words do not match, parent syn-
tactic category + and child syntactic category,
parent relative position + child relative posi-
tion, parent relative position + child relative
position + predicate PoS + predicate lemma}.
5 Experimental Results
The model was trained on the full training set
after removing unparsable sentences, yielding
90,388 predicates and 1,971,985 binary features. A
Gaussian prior was used to regularise the model,
with variance ?2 = 1. Training was performed on
a 20 node PowerPC cluster, consuming a total of
62Gb of RAM and taking approximately 15 hours.
Decoding required only 3Gb of RAM and about 5
minutes for the 3,228 predicates in the development
set. Results are shown in Table 1.
171
Precision Recall F?=1
Development 73.51% 68.98% 71.17
Test WSJ 75.81% 70.58% 73.10
Test Brown 67.63% 60.08% 63.63
Test WSJ+Brown 74.76% 69.17% 71.86
Test WSJ Precision Recall F?=1
Overall 75.81% 70.58% 73.10
A0 82.21% 79.48% 80.82
A1 74.56% 71.26% 72.87
A2 63.93% 56.85% 60.18
A3 63.95% 54.34% 58.75
A4 68.69% 66.67% 67.66
A5 0.00% 0.00% 0.00
AM-ADV 54.73% 48.02% 51.16
AM-CAU 75.61% 42.47% 54.39
AM-DIR 54.17% 30.59% 39.10
AM-DIS 77.74% 73.12% 75.36
AM-EXT 65.00% 40.62% 50.00
AM-LOC 60.67% 54.82% 57.60
AM-MNR 54.66% 49.42% 51.91
AM-MOD 98.34% 96.55% 97.44
AM-NEG 99.10% 96.09% 97.57
AM-PNC 49.47% 40.87% 44.76
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 77.20% 68.54% 72.61
R-A0 87.78% 86.61% 87.19
R-A1 82.39% 75.00% 78.52
R-A2 0.00% 0.00% 0.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 71.05% 51.92% 60.00
V 98.73% 98.63% 98.68
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
6 Conclusion
Conditional random fields proved useful in mod-
elling the semantic structure of text when provided
with a parse tree. Our novel use of a tree structure
derived from the syntactic parse, allowed for parent-
child interactions to be accurately modelled, which
provided an improvement over a standard maximum
entropy classifier. In addition, the parse constituent
structure proved quite appropriate to the task, more
so than modelling the data as a sequence of words or
chunks, as has been done in previous approaches.
Acknowledgements
We would both like to thank our research super-
visor Steven Bird for his comments and feedback
on this work. The research undertaken for this
paper was supported by an Australian Postgraduate
Award scholarship, a Melbourne Research Scholar-
ship and a Melbourne University Postgraduate Over-
seas Research Experience Scholarship.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 Shared Task: Semantic Role Labeling. In
Proceedings of the CoNLL-2005.
Trevor Cohn, Andrew Smith, and Miles Osborne. 2005. Scal-
ing conditional random fields using error correcting codes.
In Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics. To appear.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings of the
18th International Conference on Machine Learning, pages
282?289.
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, and Hae-
Chang Rim. 2004. Semantic role labeling using maximum
entropy model. In Proceedings of the CoNLL-2004 Shared
Task.
Andrew McCallum and Wei Li. 2003. Early results for named
entity recognition with conditional random fields, feature
induction and web-enhanced lexicons. In Proceedings of
the 7th Conference on Natural Language Learning, pages
188?191.
Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference. Morgan Kaufmann.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, pages 235?242.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Daniel Jurafsky. 2005. Sup-
port vector learning for semantic argument classification. In
To appear in Machine Learning journal, Special issue on
Speech and Natural Language Processing.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the Human Lan-
guage Technology Conference and North American Chap-
ter of the Association for Computational Linguistics, pages
213?220.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP.
172
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164?171,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Deep Lexical Acquisition for HPSGs via Supertagging
Phil Blunsom and Timothy Baldwin
Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
{pcbl,tim}@csse.unimelb.edu.au
Abstract
We propose a conditional random field-
based method for supertagging, and ap-
ply it to the task of learning new lexi-
cal items for HPSG-based precision gram-
mars of English and Japanese. Us-
ing a pseudo-likelihood approximation we
are able to scale our model to hun-
dreds of supertags and tens-of-thousands
of training sentences. We show that
it is possible to achieve start-of-the-art
results for both languages using maxi-
mally language-independent lexical fea-
tures. Further, we explore the performance
of the models at the type- and token-level,
demonstrating their superior performance
when compared to a unigram-based base-
line and a transformation-based learning
approach.
1 Introduction
Over recent years, there has been a resurgence of
interest in the use of precision grammars in NLP
tasks, due to advances in parsing algorithm de-
velopment, grammar development tools and raw
computational power (Oepen et al, 2002b). Pre-
cision grammars are defined as implemented
grammars of natural language which capture fine-
grained linguistic distinctions, and are generative
in the sense of distinguishing between grammat-
ical and ungrammatical inputs (or at least have
some in-built notion of linguistic ?markedness?).
Additional characteristics of precision grammars
are that they are frequently bidirectional, and out-
put a rich semantic abstraction for each span-
ning parse of the input string. Examples include
DELPH-IN grammars such as the English Resource
Grammar (Flickinger, 2002; Uszkoreit, 2002), the
various PARGRAM grammars (Butt et al, 1999),
and the Edinburgh CCG parser (Bos et al, 2004).
Due to their linguistic complexity, precision
grammars are generally hand-constructed and thus
restricted in size and coverage. Attempts to
(semi-)automate the process of expanding the cov-
erage of precision grammars have focused on ei-
ther: (a) constructional coverage, e.g. in the form
of error mining for constructional expansion (van
Noord, 2004; Zhang and Kordoni, 2006), or relax-
ation of lexico-grammatical constraints to support
partial and/or robust parsing (Riezler et al, 2002);
or (b) lexical coverage, e.g. in bootstrapping from
a pre-existing grammar and lexicon to learn new
lexical items (Baldwin, 2005a). Our particular in-
terest in this paper is in the latter of these two,
that is the development of methods for automati-
cally expanding the lexical coverage of an existing
precision grammar, or more broadly deep lexical
acquisition (DLA hereafter). In this, we follow
Baldwin (2005a) in assuming a semi-mature pre-
cision grammar with a fixed inventory of lexical
types, based on which we learn new lexical items.
For the purposes of this paper, we focus specif-
ically on supertagging as the mechanism for hy-
pothesising new lexical items.
Supertagging can be defined as the process of
applying a sequential tagger to the task of predict-
ing the lexical type(s) associated with each word
in an input string, relative to a given grammar. It
was first introduced as a means of reducing parser
ambiguity by Bangalore and Joshi (1999) in the
context of the LTAG formalism, and has since been
applied in a similar context within the CCG for-
malism (Clark and Curran, 2004). In both of these
cases, supertagging provides the means to perform
a beam search over the plausible lexical items for
a given string context, and ideally reduces pars-
ing complexity without sacrificing parser accu-
racy. An alternate application of supertagging is
in DLA, in postulating novel lexical items with
which to populate the lexicon of a given gram-
mar to boost parser coverage. This can take place
164
either: (a) off-line for the purposes of rounding
out the coverage of a static lexicon, in which case
we are generally interested in globally maximising
precision over a given corpus and hence predict-
ing the single most plausible lexical type for each
word token (off-line DLA: Baldwin (2005b)); or
(b) on the fly for a given input string to temporar-
ily expand lexical coverage and achieve a spanning
parse, in which case we are interested in maximis-
ing recall by producing a (possibly weighted) list
of lexical item hypotheses to run past the grammar
(on-line DLA: Zhang and Kordoni (2005)). Our
immediate interest in this paper is in the first of
these tasks, although we would ideally like to de-
velop an off-line method which is trivially portable
to the second task of on-line DLA.
In this research, we focus particularly on
the Grammar Matrix-based DELPH-IN family of
grammars (Bender et al, 2002), which includes
grammars of English, Japanese, Norwegian, Mod-
ern Greek, Portuguese and Korean. The Gram-
mar Matrix is a framework for streamlining and
standardising HPSG-based multilingual grammar
development. One property of Grammar Matrix-
based grammars is that they are strongly lexical-
ist and adhere to a highly constrained lexicon-
grammar interface via a unique (terminal) lexi-
cal type for each lexical item. As such, lexical
item creation in any of the Grammar Matrix-based
grammars, irrespective of language, consists pre-
dominantly of predicting the appropriate lexical
type for each lexical item, relative to the lexical
hierarchy for the corresponding grammar. In this
same spirit of standardisation and multilingual-
ity, the aim of this research is to develop max-
imally language-independent supertagging meth-
ods which can be applied to any Grammar Matrix-
based grammar with the minimum of effort. Es-
sentially, we hope to provide the grammar engi-
neer with the means to semi-automatically popu-
late the lexicon of a semi-mature grammar, hence
accelerating the pace of lexicon development and
producing a resource of sufficient coverage to be
practically useful in NLP tasks.
The contributions of this paper are the devel-
opment of a pseudo-likelihood conditional ran-
dom field-based method of supertagging, which
we then apply to the task of off-line DLA for
grammars of both English and Japanese with only
minor language-specific adaptation. We show the
supertagger to outperform previously-proposed
supertagger-based DLA methods.
The remainder of this paper is structured as
follows. Section 2 outlines past work relative
to this research, and Section 3 reviews the re-
sources used in our supertagging experiments.
Section 4 outlines the proposed supertagger model
and reviews previous research on supertagger-
based DLA. Section 5 then outlines the set-up and
results of our evaluation.
2 Past Research
According to Baldwin (2005b), research on DLA
falls into the two categories of in vitro methods,
where we leverage a secondary language resource
to generate an abstraction of the words we hope to
learn lexical items for, and in vivo methods, where
the target resource that we are hoping to perform
DLA relative to is used directly to perform DLA.
Supertagging is an instance of in vivo DLA, as it
operates directly over data tagged with the lexical
type system for the precision grammar of interest.
Research on supertagging which is relevant to
this paper includes the work of Baldwin (2005b) in
training a transformation-based learner over data
tagged with ERG lexical types. We discuss this
method in detail in Section 5.2 and replicate this
method over our English data set for direct com-
parability with this previous research.
As mentioned above, other work on supertag-
ging has tended to view it as a means of driving
a beam search to prune the parser search space
(Bangalore and Joshi, 1999; Clark and Curran,
2004). In supertagging, token-level annotations
(gold-standard, automatically-generated or other-
wise) for a given DLR are used to train a se-
quential tagger, akin to training a POS tagger over
POS-tagged data taken from the Penn Treebank.
One related in vivo approach to DLA targeted
specifically at precision grammars is that of Fou-
vry (2003). Fouvry uses the grammar to guide
the process of learning lexical items for unknown
words, by generating underspecified lexical items
for all unknown words and parsing with them.
Syntactico-semantic interaction between unknown
words and pre-existing lexical items during pars-
ing provides insight into the nature of each un-
known word. By combining such fragments of in-
formation, it is possible to incrementally arrive at
a consolidated lexical entry for that word. That is,
the precision grammar itself drives the incremen-
tal learning process within a parsing context.
165
An alternate approach is to compile out a set of
word templates for each lexical type (with the im-
portant qualification that they do not rely on pre-
processing of any form), and check for corpus oc-
currences of an unknown word in such contexts.
That is, the morphological, syntactic and/or se-
mantic predictions implicit in each lexical type are
made explicit in the form of templates which rep-
resent distinguishing lexical contexts of that lexi-
cal type. This approach has been shown to be par-
ticularly effective over web data, where the sheer
size of the data precludes the possibility of linguis-
tic preprocessing but at the same time ameliorates
the effects of data sparseness inherent in any lexi-
calised DLA approach (Lapata and Keller, 2004).
Other work on DLA (e.g. Korhonen (2002),
Joanis and Stevenson (2003), Baldwin (2005a))
has tended to take an in vitro DLA approach, in
extrapolating away from a DLR to corpus or web
data, and analysing occurrences of words through
the conduit of an external resource (e.g. a sec-
ondary parser or POS tagger). In vitro DLA can
also take the form of resource translation, in map-
ping one DLR onto another to arrive at the lexical
information in the desired format.
3 Task and Resources
In this section, we outline the resources targeted
in this research, namely the English Resource
Grammar (ERG: Flickinger (2002), Copestake
and Flickinger (2000)) and the JACY grammar of
Japanese (Siegel and Bender, 2002). Note that our
choice of the ERG and JACY as testbeds for exper-
imentation in this paper is somewhat arbitrary, and
that we could equally run experiments over any
Grammar Matrix-based grammar for which there
is treebank data.
Both the ERG and JACY are implemented
open-source broad-coverage precision Head-
driven Phrase Structure Grammars (HPSGs:
Pollard and Sag (1994)). A lexical item in each
of the grammars consists of a unique identifier,
a lexical type (a leaf type of a type hierarchy),
an orthography, and a semantic relation. For
example, in the English grammar, the lexical item
for the noun dog is simply:
dog_n1 := n_-_c_le &
[ STEM < "dog" >,
SYNSEM [ LKEYS.KEYREL.PRED "_dog_n_1_rel" ] ].
in which the lexical type of n - c le encodes
the fact that dog is a noun which does not sub-
categorise for any other constituents and which is
countable, "dog" specifies the lexical stem, and
" dog n 1 rel" introduces an ad hoc predicate
name for the lexical item to use in constructing a
semantic representation. In the context of the ERG
and JACY, DLA equates to learning the range of
lexical types a given lexeme occurs with, and gen-
erating a single lexical item for each.
Recent development of the ERG and JACY has
been tightly coupled with treebank annotation, and
all major versions of both grammars are deployed
over a common set of dynamically-updateable
treebank data to help empirically trace the evo-
lution of the grammar and retrain parse selection
models (Oepen et al, 2002a; Bond et al, 2004).
This serves as a source of training and test data for
building our supertaggers, as detailed in Table 1.
In translating our treebank data into a form that
can be understood by a supertagger, multiword ex-
pressions (MWEs) pose a slight problem. Both the
ERG and JACY include multiword lexical items,
which can either be strictly continuous (e.g. hot
line) or optionally discontinuous (e.g. transitive
English verb particle constructions, such as pick
up as in Kim picked the book up).
Strictly continuous lexical items are described
by way of a single whitespace-delimited lexical
stem (e.g. STEM < "hot line" >). When
faced with instances of this lexical item, the su-
pertagger must perform two roles: (1) predict that
the words hot and line combine together to form
a single lexeme, and (2) predict the lexical type
associated with the lexeme. This is performed
in a single step through the introduction of the
ditto lexical type, which indicates that the cur-
rent word combines (possibly recursively) with the
left-adjacent word to form a single lexeme, and
shares the same lexical type. This tagging conven-
tion is based on that used, e.g., in the CLAWS7
part-of-speech tagset.
Optionally discontinuous lexical items are less
of a concern, as selection of each of the discontin-
uous ?components? is done via lexical types. E.g.
in the case of pick up, the lexical entry looks as
follows:
pick_up_v1 := v_p-np_le &
[ STEM < "pick" >,
SYNSEM [ LKEYS [ --COMPKEY _up_p_sel_rel,
KEYREL.PRED "_pick_v_up_rel" ] ] ].
in which "pick" selects for the up p sel rel
predicate, which in turn is associated with the stem
"up" and lexical type p prtcl le. In terms of
lexical tag mark-up, we can treat these as separate
166
ERG JACY
GRAMMAR
Language English Japanese
Lexemes 16,498 41,559
Lexical items 26,297 47,997
Lexical types 915 484
Strictly continuous MWEs 2,581 422
Optionally discontinuous MWEs 699 0
Proportion of lexemes with more than one lexical item 0.29 0.14
Average lexical items per lexeme 1.59 1.16
TREEBANK
Training sentences 20,000 40,000
Training words 215,015 393,668
Test sentences 1,013 1,095
Test words 10,781 10,669
Table 1. Make-up of the English Resource Grammar (ERG) and JACY grammars and treebanks
tags and leave the supertagger to model the mutual
inter-dependence between these lexical types.
For detailed statistics of the composition of the
two grammars, see Table 1.
For morphological processing (including to-
kenisation and lemmatisation), we use the pre-
existing machinery provided with each of the
grammars. In the case of the ERG, this consists
of a finite state machine which feeds into lexical
rules; in the case of JACY, segmentation and lem-
matisation is based on a combination of ChaSen
(Matsumoto et al, 2003) and lexical rules. That
is, we are able to assume that the Japanese data
has been pre-segmented in a form compatible with
JACY, as we are able to replicate the automatic
pre-processing that it uses.
4 Suppertagging
The DLA strategy we adopt in this research is
based on supertagging, which is a simple in-
stance of sequential tagging with a larger, more
linguistically-diverse tag set than is conventionally
the case, e.g., with part-of-speech tagging. Below,
we describe the pseudo-likelihood CRF model we
base our supertagger on and outline the feature
space for the two grammars.
4.1 Pseudo-likelihood CRF-based
Supertagging
CRFs are undirected graphical models which de-
fine a conditional distribution over a label se-
quence given an observation sequence. Here we
use CRFs to model sequences of lexical types,
where each input word in a sentence is assigned
a single tag.
The joint probability density of a sequence la-
belling,   (a vector of lexical types), given the in-
put sentence,  , is given by:

 

	
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204?1213,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Unsupervised Induction of Tree Substitution Grammars
for Dependency Parsing
Phil Blunsom
Computing Laboratory
University of Oxford
Phil.Blunsom@comlab.ox.ac.uk
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Abstract
Inducing a grammar directly from text is
one of the oldest and most challenging tasks
in Computational Linguistics. Significant
progress has been made for inducing depen-
dency grammars, however the models em-
ployed are overly simplistic, particularly in
comparison to supervised parsing models. In
this paper we present an approach to depen-
dency grammar induction using tree substi-
tution grammar which is capable of learn-
ing large dependency fragments and thereby
better modelling the text. We define a hi-
erarchical non-parametric Pitman-Yor Process
prior which biases towards a small grammar
with simple productions. This approach sig-
nificantly improves the state-of-the-art, when
measured by head attachment accuracy.
1 Introduction
Grammar induction is a central problem in Compu-
tational Linguistics, the aim of which is to induce
linguistic structures from an unannotated text cor-
pus. Despite considerable research effort this un-
supervised problem remains largely unsolved, par-
ticularly for traditional phrase-structure parsing ap-
proaches (Clark, 2001; Klein and Manning, 2002).
Phrase-structure parser induction is made difficult
due to two types of ambiguity: the constituent struc-
ture and the constituent labels. In particular the con-
stituent labels are highly ambiguous, firstly we don?t
know a priori how many there are, and secondly la-
bels that appear high in a tree (e.g., an S category
for a clause) rely on the correct inference of all the
latent labels below them. However recent work on
the induction of dependency grammars has proved
more fruitful (Klein and Manning, 2004). Depen-
dency grammars (Mel?c?uk, 1988) should be easier to
induce from text compared to phrase-structure gram-
mars because the set of labels (heads) are directly
observed as the words in the sentence.
Approaches to unsupervised grammar induction,
both for phrase-structure and dependency grammars,
have typically used very simplistic models (Clark,
2001; Klein and Manning, 2004), especially in com-
parison to supervised parsing models (Collins, 2003;
Clark and Curran, 2004; McDonald, 2006). Sim-
ple models are attractive for grammar induction be-
cause they have a limited capacity to overfit, how-
ever they are incapable of modelling many known
linguistic phenomena. We posit that more complex
grammars could be used to better model the unsuper-
vised task, provided that active measures are taken
to prevent overfitting. In this paper we present an
approach to dependency grammar induction using
a tree-substitution grammar (TSG) with a Bayesian
non-parametric prior. This allows the model to learn
large dependency fragments to best describe the text,
with the prior biasing the model towards fewer and
smaller grammar productions.
We adopt the split-head construction (Eisner,
2000; Johnson, 2007) to map dependency parses to
context free grammar (CFG) derivations, over which
we apply a model of TSG induction (Cohn et al,
2009). The model uses a hierarchical Pitman-Yor
process to encode a backoff path from TSG to CFG
rules, and from lexicalised to unlexicalised rules.
Our best lexicalised model achieves a head attach-
ment accuracy of of 55.7% on Section 23 of the WSJ
data set, which significantly improves over state-of-
the-art and far exceeds an EM baseline (Klein and
Manning, 2004) which obtains 35.9%.
1204
CFG Rule DMV Distribution Description
S? LH HR p(root = H) The head of the sentence is H .
LH ? Hl p(STOP |dir = L, head = H, val = 0) H has no left children.
LH ? L1H p(CONT |dir = L, head = H, val = 0) H has at least one left child.
L?H ? Hl p(STOP |dir = L, head = H, val = 1) H has no more left children.
L?H ? L
1
H p(CONT |dir = L, head = H, val = 1) H has another left child.
HR? Hr p(STOP |dir = R, head = H, val = 0) H has no right children.
HR? HR1 p(CONT |dir = R, head = H, val = 0) H has at least one right child.
HR? ? Hr p(STOP |dir = R, head = H, val = 1) H has no more right children.
HR? ? HR1 p(CONT |dir = R, head = H, val = 1) H has another right child.
L1H ? LC CMH? p(C|dir = L, head = H) C is a left child of H .
HR1? H?MC CR p(C|dir = R, head = H) C is a right child of H .
CMH? ? CR L?H p = 1 Unambiguous
H?MC ? HR? LC p = 1 Unambiguous
Table 1: The CFG-DMV grammar schema. Note that the actual CFG is created by instantiating these templates with
part-of-speech tags observed in the data for the variables H and C. Valency (val) can take the value 0 (no attachment
in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent;
superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more
children and X? that it has already attached a child.
2 Background
The most successful framework for unsupervised
dependency induction is the Dependency Model
with Valence (DMV) (Klein and Manning, 2004).
This model has been adapted and extended by a
number of authors and currently represents the state-
of-the-art for dependency induction (Cohen and
Smith, 2009; Headden III et al, 2009). Eisner
(2000) introduced the split-head algorithm which
permits efficient O(|w|3) parsing complexity by
replicating (splitting) each terminal and processing
left and right dependents separately. We employ
the related fold-unfold representation of Johnson
(2007) that defines a CFG equivalent of the split-
head parsing algorithm, allowing us to easily adapt
CFG-based grammar models to dependency gram-
mar. Table 1 shows the equivalent CFG grammar for
the DMV model (CFG-DMV) using the unfold-fold
transformation. The key insight to understanding the
non-terminals in this grammar is that the subscripts
encode the terminals at the boundaries of the span
of that non-terminal. For example the non-terminal
LH encodes that the right most terminal spanned
by this constituent is H (and the reverse for HR),
while AMB encodes that A and B are the left-most
and right-most terminals of the span. The ? and 1
superscripts are used to encode the valency of the
head, both indicate that the head has at least one
attached dependent in the specified direction. This
grammar allows O(|w|3) parsing complexity which
follows from the terminals of the dependency tree
being observed, such that each span of the parse
chart uniquely specifies its possible heads (either the
leftmost, rightmost or both) and therefore the num-
ber of possible non-terminals for each span is con-
stant. The transform is illustrated in figures 1a and
1c which show the CFG tree for an example sentence
and the equivalent dependency tree.
Normally DMV based models have been trained
on part-of-speech tags of the words in a sentence,
rather than the words themselves. Headden III et al
(2009) showed that performance could be improved
by including high frequency words as well as tags
in their model. In this paper we refer to such mod-
els as lexicalised; words which occur more than one
hundred times in the training corpus are represented
by a word/tag pair, while those less frequent are rep-
resented simply by their tags. We are also able to
show that this basic approach to lexicalisation im-
proves the performance of our models.
1205
SLhates[V ]
L1hates[V ]
LN
Nl
NMhates[V]?
NR
Nr
L?hates[V ]
hates[V]l
hates[V ]R
hates[V ]R1
hates[V]?MN
hates[V ]R?
hates[V]r
LN
Nl
NR
Nr
(a) A TSG-DMV derivation for the sentence George hates broc-
coli. George and broccoli occur less than the lexicalisation cutoff
and are thus represented by the part-of-speech N, while hates is
common and therefore is represented by a word/tag pair. Bold
nodes indicate frontier nodes of elementary trees.
S
Lhates[V ]
L1hates[V ]
LN NMhates[V]?
hates[V ]R
hates[V ]R1
hates[V]?MN NR
(b) A TSG-DMV elementary rule from Figure 1a. This rule en-
codes a dependency between the subject and object of hates that
is not present in the CFG-DMV. Note that this rule doesn?t re-
strict hates, or its arguments, to having a single left and right
child. More dependents can be inserted using additional rules
below the M/L/R frontier non-terminals.
George hates broccoli ROOT
(c) A traditional dependency tree representation of the parse tree
in Figure 1a before applying the lexicalisation cutoff.
Figure 1: TSG-DMV representation of dependency trees.
3 Lexicalised TSG-DMV
The models we investigate in this paper build upon
the CFG-DMV by defining a Tree Substitution
Grammar (TSG) over the space of CFG rules. A
TSG is a 4-tuple,G = (T,N, S,R), where T is a set
of terminal symbols, N is a set of non-terminal sym-
bols, S ? N is the distinguished root non-terminal
and R is a set of productions (rules). The produc-
tions take the form of elementary trees ? tree frag-
ments of height ? 1, where each internal node is
labelled with a non-terminal and each leaf is la-
belled with either a terminal or a non-terminal. Non-
terminal leaves are called frontier non-terminals and
form the substitution sites in the generative process
of creating trees with the grammar.
A derivation creates a tree by starting with the
root symbol and rewriting (substituting) it with an
elementary tree, then continuing to rewrite fron-
tier non-terminals with elementary trees until there
are no remaining frontier non-terminals. We can
represent derivations as sequences of elementary
trees, e, by specifying that during the generation of
the tree each elementary tree is substituted for the
left-most frontier non-terminal. Figure 1a shows a
TSG derivation for the dependency tree in Figure 1c
where bold nonterminal labels denote substitution
sites (root/frontier nodes in the elementary trees).
The probability of a derivation, e, is the product
of the probabilities of its component rules,
P (e) =
?
c?e?e
P (e|c) . (1)
where each rewrite is assumed conditionally inde-
pendent of all others given its root nonterminal, c =
root(e). The probability of a tree, t, and string of
words, w, are
P (t) =
?
e:tree(e)=t
P (e) and P (w) =
?
t:yield(t)=w
P (t) ,
respectively, where tree(e) returns the tree for the
derivation e and yield(t) returns the string of termi-
nal symbols at the leaves of t.
A Probabilistic Tree Substitution Grammar
(PTSG), like a PCFG, assigns a probability to each
rule in the grammar, denoted P (e|c). The probabil-
ity of a derivation, e, is the product of the proba-
bilities of its component rules. Estimating a PTSG
requires learning the sufficient statistics for P (e|c)
in (1) based on a training sample. Parsing involves
1206
finding the most probable tree for a given string
(argmaxt P (t|w)). This is typically approximated
by finding the most probable derivation which can
be done efficiently using the CYK algorithm.
3.1 Model
In this work we propose the Tree Substitution Gram-
mar Dependency Model with Valence (TSG-DMV).
We define a hierarchical non-parametric TSG model
on the space of parse trees licensed by the CFG
grammar in Table 1. Our model is a generalisa-
tion of that of Cohn et al (2009) and Cohn et al
(2011). We extend those works by moving from a
single level Dirichlet Process (DP) distribution over
rules to a multi-level Pitman-Yor Process (PYP), and
including lexicalisation. The PYP has been shown
to generate distributions particularly well suited to
modelling language (Teh, 2006; Goldwater et al,
2006). Teh (2006) used a hierarchical PYP to model
backoff in language models, we leverage this same
capability to model backoff in TSG rules. This ef-
fectively allows smoothing from lexicalised to un-
lexicalised grammars, and from TSG to CFG rules.
Here we describe our deepest model which has
a four level hierarchy, depicted graphically in Table
2. In Section 5 we evaluate different subsets of this
hierarchy. The topmost level of our model describes
lexicalised elementary elementary fragments (e) as
produced by a PYP,
e|c ? Gc
Gc|ac, bc,P
lcfg ? PYP(ac, bc,P
lcfg(?|c)) ,
where ac and bc control the strength of the backoff
distribution Plcfg. The space of lexicalised TSG rules
will inevitably be very sparse, so the base distribu-
tion Plcfg backs-off to calculating the probability of
a TSG rules as the product of the CFG rules it con-
tains, multiplied by a geometric distribution over the
size of the rule.
Plcfg(e|c) =
?
f?F(e)
sfc
?
i?I(e)
(1? sic)
?A(lex-cfg-rules(e|c))
?|c ? Ac
Ac|a
lcfg
c , b
lcfg
c ,P
cfg ? PYP(alcfgc , b
lcfg
c ,P
cfg(?|c)),
where I(e) are the set of internal nodes in e exclud-
ing the root, F (e) are the set of frontier non-terminal
nodes, and ci is the non-terminal symbol for node
i and sc is the probability of stopping expanding a
node labelled c. The function lex-cfg-rules(e|c) re-
turns the CFG rules internal to e, each of the form
c? ? ?; each CFG rule is drawn from the back-
off distribution, Ac? . We treat sc as a parameter
which is estimated during training, as described in
Section 4.2.
The next level of backoff (Pcfg) removes the lexi-
calisation from the CFG rules, describing the gener-
ation of a lexicalised rule by first generating an un-
lexicalised rule from a PYP, then generating the lex-
icalisaton from a uniform distribution over words:1
Pcfg(?|c) = B(unlex(?)|unlex(c))
?
1
|w||?|
??|c? ? Bc?
Bc? |a
cfg
c? , b
cfg
c? ,P
sh ? PYP(acfgc? , b
cfg
c? ,P
sh(?|c?)),
where unlex(?) removes the lexicalisation from non-
terminals leaving only the tags.
The final base distribution over CFG-DMV rules
(Psh) is inspired by the skip-head smoothing model
of Headden III et al (2009). This model showed that
smoothing the DMV by removing the heads from the
CFG rules significantly improved performance. We
replicate this behavior through a final level in our hi-
erarchy which generates the CFG rules without their
heads, then generates the heads from a uniform dis-
tribution:
Psh(?|c) = C(drop-head(c? ?))?
1
|P |
?|c ? Cc
Cc|a
sh
c , b
sh
c ? PYP(a
sh
c , b
sh
c ,Uniform(?|c)),
where drop-head(?) removes the symbols that mark
the head on the CFG rules, and P is the set of part-
of-speech tags. Each stage of backoff is illustrated in
Table 2, showing the rules generated from the TSG
elementary tree in Figure 1b.
Note that while the supervised model of Cohn et
al. (2009) used a fixed back-off PCFG distribution,
this model implicitly infers this distribution within
1All unlexicalised words are actually given the generic UNK
symbol as their lexicalisation.
1207
Plcfg Pcfg Psh
S
Lhates[V ] hates[V ]R
Lhates[V ]
L1hates[V ]
S
LV V R
LV
L1V
S
L? ?R
L?
L1?
L1hates[V ]
LN NMhates[V]?
hates[V ]R
hates[V ]R
1
L1V
LN NMV ?
V R
V R1
L1?
LN NM??
?R
?R1
hates[V ]R
1
hates[V]?MN NR
V R1
V ?MN NR
?R1
??MN NR
Table 2: Backoff trees for the elementary tree in Figure 1b.
its hierarchy, essentially learning the DMV model
embedded in the TSG.
In this application to dependency grammar our
model is capable of learning tree fragments which
group CFG parameters. As such the model can learn
to condition dependency links on the valence, e.g. by
combining LH ? L1H and L
1
H ? LC CMH? rules
into a single fragment the model can learn a pa-
rameter that the leftmost child of H is C. By link-
ing together multiple L1H or HR
1 non-terminals the
model can learn groups of dependencies that occur
together, e.g. tree fragments representing the com-
plete preferred argument frame of a verb.
4 Inference
4.1 Training
To train our model we use Markov Chain Monte
Carlo sampling (Geman and Geman, 1984). Where
previous supervised TSG models (Cohn et al, 2009)
permit an efficient local sampler, the lack of an ob-
served parse tree in our unsupervised model makes
this sampler not applicable. Instead we use a re-
cently proposed blocked Metroplis-Hastings (MH)
sampler (Cohn and Blunsom, 2010) which exploits a
factorisation of the derivation probabilities such that
whole trees can be sampled efficiently. See Cohn
and Blunsom (2010) for details. That algorithm is
applied using a dynamic program over an observed
tree, the generalisation to our situation of an inside
pass over the space of all trees is straightforward.
A final consideration is the initialisation of the
sampler. Klein and Manning (2004) emphasised the
importance of the initialiser for achieving good per-
formance with their model. We employ the same
harmonic initialiser as described in that work. The
initial derivations for our sampler are the Viterbi
derivations under the CFG parameterised according
to this initialiser.
4.2 Sampling hyperparameters
We treat the hyper-parameters {(axc , b
x
c , sc) , c ? N}
as random variables in our model and infer their val-
ues during training. We choose quite vague priors
for each hyper-parameter, encoding our lack of in-
formation about their values.
We place prior distributions on the PYP discount
ac and concentration bc hyperparamters and sam-
ple their values using a slice sampler. We use the
range doubling slice sampling technique of (Neal,
2003) to draw a new sample of a?c from its condi-
tional distribution.2 For the discount parameters ac
we employ a uniform Beta distribution, as we have
no strong prior knowledge of what its value should
be (ac ? Beta(1, 1)). Similarly, we treat the concen-
tration parameters, bc, as being generated by a vague
gamma prior, bc ? Gamma(1, 1), and sample a new
value b?c using the same slice-sampling approach as
for ac:
P (bc|z) ? P (z|bc)? Gamma(bc|1, 1).
2We made use of the slice sampler included in
Mark Johnson?s Adaptor Grammar implementation
http://www.cog.brown.edu/?mj/Software.htm.
1208
Corpus Words Sentences
Sections 2-21 (|x| ? 10) 42505 6007
Section 22 (|x| ? 10) 1805 258
Section 23 (|x| ? 10) 2649 398
Section 23 (|x| ? ?) 49368 2416
Table 3: Corpus statistics for the training and testing data
for the TSG-DMV model. All models are trained on the
gold standard part-of-speech tags after removing punctu-
ation.
We use a vague Beta prior for the stopping probabil-
ities in Plcfg, sc ? Beta(1, 1).
All the hyper-parameters are resampled after ev-
ery 10th sample of the corpus derivations.
4.3 Parsing
Unfortunately finding the maximising parse tree for
a string under our TSG-DMV model is intractable
due to the inter-rule dependencies created by the
PYP formulation. Previous work has used Monte
Carlo techniques to sample for one of the maxi-
mum probability parse (MPP), maximum probabil-
ity derivation (MPD) or maximum marginal parse
(MMP) (Cohn et al, 2009; Bod, 2006). We take a
simpler approach and use the Viterbi algorithm to
calculate the MPD under an approximating TSG de-
fined by the last set of derivations sampled for the
corpus during training. Our results indicate that this
is a reasonable approximation, though the experi-
ence of other researchers suggests that calculating
the MMP under the approximating TSG may also
be beneficial for DMV (Cohen et al, 2008).
5 Experiments
We follow the standard evaluation regime for DMV
style models by performing experiments on the text
of the WSJ section of the Penn. Treebank (Marcus et
al., 1993) and reporting head attachment accuracy.
Like previous work we pre-process the training and
test data to remove punctuation, training our unlex-
icalised models on the gold-standard part-of-speech
tags, and including words occurring more than 100
times in our lexicalised models (Headden III et al,
2009). It is very difficult for an unsupervised model
to learn from long training sentences as they contain
a great deal of ambiguity, therefore the majority of
DMV based models have been trained on sentences
restricted in length to ? 10 tokens.3 This has the
added benefit of decreasing the runtime for exper-
iments. We present experiments with this training
scenario. The training data comes from sections 2-
21, while section 23 is used for evaluation. An ad-
vantage of our sampling based approach over pre-
vious work is that we infer all the hyperparameters,
as such we don?t require the use of section 22 for
tuning the model.
The models are evaluated in terms of head attach-
ment accuracy (the percentage of correctly predicted
head indexes for each token in the test data), on two
subsets of the testing data. Although we can argue
that unsupervised models are better learnt from short
sentences, it is much harder to argue that we don?t
then need to be able to parse long sentences with a
trained model. The most commonly employed test
set mirrors the training data by only including sen-
tences ? 10. In this work we focus on the accuracy
of our models on the whole of section 23, without
any pruning for length. The training and testing cor-
pora statistics are presented in Table 3. Subsequent
to the evaluation reported in Table 4 we use section
22 to report the correlation between heldout accu-
racy and the model log-likelihood (LLH) for ana-
lytic purposes.
As we are using a sampler during training, the re-
sult of any single run is non-deterministic and will
exhibit a degree of variance. All our reported results
are the mean and standard deviation (?) from forty
sampling runs.
5.1 Discussion
Table 4 shows the head attachment accuracy results
for our TSG-DMV, plus many other significant pre-
viously proposed models. The subset of hierarchical
priors used by each model is noted in brackets.
The performance of our models is extremely en-
couraging, particularly the fact that it achieves the
highest reported accuracy on the full test set by a
considerable margin. On the |w| ? 10 test set al the
TSG-DMVs are second only to the L-EVG model
of Headden III et al (2009). The L-EVG model
extends DMV by adding additional lexicalisation,
3See Spitkovsky et al (2010a) for an exception to this rule.
1209
Directed Attachment
Accuracy on WSJ23
Model |w| ? 10 |w| ? ?
Attach-Right 38.4 31.7
EM (Klein and Manning, 2004) 46.1 35.9
Dirichlet (Cohen et al, 2008) 46.1 36.9
LN (Cohen et al, 2008) 59.4 40.5
SLN, TIE V&N (Cohen and Smith, 2009) 61.3 41.4
DMV (Headden III et al, 2009) 55.7?=8.0 -
DMV smoothed (Headden III et al, 2009) 61.2?=1.2 -
EVG smoothed (Headden III et al, 2009) 65.0?=5.7 -
L-EVG smoothed (Headden III et al, 2009) 68.8?=4.5 -
Less is More (Spitkovsky et al, 2010a) 56.2 44.1
Leap Frog (Spitkovsky et al, 2010a) 57.1 45.0
Viterbi EM (Spitkovsky et al, 2010b) 65.3 47.9
Hypertext Markup (Spitkovsky et al, 2010c) 69.3 50.4
Adaptor Grammar (Cohen et al, 2010) 50.2 -
TSG-DMV (Pcfg) 65.9?=2.4 53.1?=2.4
TSG-DMV (Pcfg, Psh) 65.1?=2.2 51.5?=2.0
LexTSG-DMV (Plcfg, Pcfg) 67.2?=1.4 55.2?=2.2
LexTSG-DMV (Plcfg, Pcfg, Psh) 67.7?=1.5 55.7?=2.0
Supervised MLE (Cohen and Smith, 2009) 84.5 68.8
Table 4: Mean and variance for the head attachment accu-
racy of our TSG-DMV models (highlighted) with varying
backoff paths, and many other high performing models.
Citations indicate where the model and result were re-
ported. Our models labelled TSG used an unlexicalised
top level Gc PYP, while those labelled LexTSG used the
full lexicalised Gc.
valency conditioning, interpolated back-off smooth-
ing and a random initialiser. In particular Head-
den III et al (2009) shows that the random initialiser
is crucial for good performance, however this ini-
tialiser requires training 1000 models to select a sin-
gle best model for evaluation and results in consider-
able variance in test set performance. Note also that
our model exhibits considerably less variance than
those induced using this random initialiser, suggest-
ing that the combination of the harmonic initialiser
and blocked-MH sampling may be a more practica-
ble training regime.
The recently proposed Adaptor Grammar DMV
model of Cohen et al (2010) is similar in many
way to our TSG model, incorporating a Pitman Yor
prior over units larger than CFG rules. As such it
is surprising that our model is performing signif-
icantly better than this model. We can identify a
number of differences that may impact these results:
the Adaptor Grammar model is trained using vari-
ational inference with the space of tree fragments
truncated, while we employ a sampler which can
nominally explore the full space of tree fragments;
and the adapted tree fragments must be complete
subtrees (i.e. they don?t contain variables), whereas
our model can make use of arbitrary tree fragments.
An interesting avenue for further research would be
to extend the variational algorithm of Cohen et al
(2010) to our TSG model, possibly speeding infer-
ence and allowing easier parallelisation.
In Figure 2a we graph the model LLH on the train-
ing data versus the head attachment accuracy on the
heldout set. The graph was generated by running
160 models for varying numbers of samples and
evaluating their accuracy. This graph indicates that
the improvements in the posterior probability of the
model are correlated with the evaluation, though the
correlation is not as high as we might require in or-
der to use LLH as a model selection criteria similar
to Headden III et al (2009). Further refinements to
the model could improve this correlation.
The scaling perfomance of the model as the num-
ber of samples is increased is shown in Figure 2b.
Performance improves as the training data is sam-
pled for longer, and continues to trend upwards be-
yond 1000 samples (the point for which we?ve re-
ported results in Table 4). This suggests that longer
sampling runs ? and better inference techniques ?
could yield further improvements.
For further analysis Table 5 shows the accuracy
of the model at predicting the head for frequent
types, while Table 6 shows the performance on de-
pendencies of various lengths. We emphasise that
these results are for the single best performing sam-
pler run on the heldout corpus and there is consid-
erable variation in the analyses produced by each
sampler. Unsurprisingly, the model appears to be
more accurate when predicting short dependencies,
a result that is also reflected in the per type accura-
cies. The model is relatively good at identifying the
root verb in each sentence, especially those headed
by past tense verbs (VBD, was), and to a lesser de-
gree VBPs (are). Conjunctions such as and pose
a particular difficulty when evaluating dependency
models as the correct modelling of these remains a
1210
l
ll
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l l
l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
ll l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
ll
l l
l
l
l
l l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
ll
l
ll
l
l
l
l
?5.4 ?5.3 ?5.2 ?5.1 ?5.0 ?4.9 ?4.8
56
58
60
62
64
66
68
Perplexity vs. Accuracy Correlation
PYP.LLH
Dire
cted
.Atta
chm
ent.
Acc
urac
y
(a) Correlation (R2 = 0.2) between the training LLH of the
PYP Model and heldout directed head attachment accuracy
(WSJ Section 22, |w| ? 10) for LexTSG-DMV (Plcfg, Pcfg, Psh).
0 500 1000 1500 2000
59
60
61
62
63
64
65
66
Number of Samples vs. Accuracy
Samples
Dire
cted
 Atta
chm
ent 
Acc
urac
y
l
l
l
l
l
(b) Mean heldout directed head attachment accuracy (WSJ Sec-
tion 22, |w| ? 10) versus the number of samples used during
training for LexTSG-DMV (Plcfg, Pcfg, Psh).
Figure 2
contentious linguistic issue and it?s not clear what
the ?correct? analysis should be. Our model gets a
respectable 75% accuracy for and conjunctions, but
for conjunctions (CC) as a whole, the model per-
forms poorly (39%).
Table 7 list the most frequent TSG rules lexi-
calised with has. The most frequent rule is sim-
ply the single level equivalent of the DMV termi-
nal rule for has. Almost as frequent is rule 3, here
the grammar incorporates the terminal into a larger
elementary fragment, encoding that it is the head
of the past participle occuring immediately to it?s
right. This shows the model?s ability to learn the
verb?s argument position conditioned on both the
head and child type, something lacking in DMV.
Rule 7 further refines this preferred analysis for has
been by lexicalising both the head and child. Rules
(4,5,8,10) employ similar conditioning for proper
and ordinary nouns heading noun phrases to the
left of has. We believe that it is the ability of the
TSG to encode stronger constraints on argument po-
sitions that leads to the model?s higher accuracy
on longer sentences, while other models do well
on shorter sentences but relatively poorly on longer
ones (Spitkovsky et al, 2010c).
6 Conclusion
In this paper we have made two significant contri-
butions to probabilistic modelling and grammar in-
duction. We have shown that it is possible to suc-
cessfully learn hierarchical Pitman-Yor models that
encode deep and complex backoff paths over highly
structured latent spaces. By applying these models
to the induction of dependency grammars we have
also been able to advance the state-of-the-art, in-
creasing the head attachment accuracy on section 23
of the Wall Street Journal Corpus by more than 5%.
Further gains in performance may come from an
exploration of the backoff paths employed within the
model. In particular more extensive experimentation
with alternate priors and larger training data may al-
low the removal of the lexicalisation cutoff which is
currently in place to counter sparsity.
We envisage that in future many grammar for-
malisms that have been shown to be effective in su-
pervised parsing, such as categorial, unification and
tree adjoining grammars, will prove amenable to
unsupervised induction using the hierarchical non-
parametric modelling approaches we have demon-
strated in this paper.
1211
Count LexTSG-DMV Rules
1 94 L?has?V BZ ? (L?has?V BZ has-VBZl)
2 74 L1has?V BZ ? (L
1
has?V BZ (LNN L
1
NN ) NNMhas?V BZ? )
3 71 has?V BZ?MV BN ? (has?V BZ?MV BN (has?V BZR? has-VBZr) LV BN )
4 54 NNMhas?V BZ? ? (NNMhas?V BZ? NNR (L?has?V BZ has-VBZl))
5 36 NNMhas?V BZ? ? (NNMhas?V BZ? NNR L?has?V BZ)
6 36 has?V BZR? ? (has?V BZR? (has?V BZR1 has?V BZ?MV BN (V BNR VBNr)))
7 30 has?V BZ?Mbeen?V BN ? (has?V BZ?Mbeen?V BN (has?V BZR? has-VBZr) Lbeen?V BN )
8 27 NNPMhas?V BZ? ? (NNPMhas?V BZ? NNPR (L?has?V BZ has-VBZl))
9 25 has?V BZR ? (has?V BZR (has?V BZR1 has?V BZ?MNNS (NNSR NNSR1)))
10 18 L1has?V BZ ? (L
1
has?V BZ LNNP NNPMhas?V BZ? )
Table 7: The ten most frequent LexTSG-DMV rules in a final training sample that contain has.
References
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proc. of the 44th Annual Meeting of
the ACL and 21st International Conference on Compu-
tational Linguistics (COLING/ACL-2006), pages 865?
872, Sydney, Australia, July.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proc. of the
42nd Annual Meeting of the ACL (ACL-2004), pages
103?110, Barcelona, Spain.
Alexander Clark. 2001. Unsupervised induction of
stochastic context-free grammars using distributional
clustering. In ConLL ?01: Proceedings of the 2001
workshop on Computational Natural Language Learn-
ing, pages 1?8. Association for Computational Lin-
guistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In NAACL ?09: Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
74?82, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Daphne Koller, Dale
Schuurmans, Yoshua Bengio, and Lon Bottou, editors,
NIPS, pages 321?328. MIT Press.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked inference
in Bayesian tree substitution grammars. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, page To Appear, Uppsala,
Sweden.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In NAACL ?09: Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics on ZZZ, pages 548?556,
Morristown, NJ, USA. Association for Computational
Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2011. Inducing tree-substitution grammars. Journal
of Machine Learning Research. To Appear.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466.
MIT Press, Cambridge, MA.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
101?109, Boulder, Colorado, June.
1212
Child Tag Predicted Accuracy
Head Correct (%)
NN 181 0.64
NNP 130 0.71
DT 127 0.87
NNS 108 0.72
VBD 108 0.81
JJ 106 0.80
IN 81 0.55
RB 65 0.61
PRP 64 0.97
VBZ 47 0.80
VBN 36 0.86
VBP 30 0.77
CD 26 0.23
VB 25 0.68
the 42 0.88
was 29 0.97
The 25 0.83
of 18 0.78
a 18 0.90
to 17 0.50
in 16 0.89
is 15 0.79
n?t 15 0.83
were 12 0.86
are 11 0.92
It 11 1.00
for 9 0.64
and 9 0.75
?s 9 1.00
Table 5: Per tag type predicted count and accuracy,
for the most frequent 15 un/lexicalised tokens on the
WSJ Section 22 |w| ? 10 heldout set (LexTSG-DMV
(Plcfg,Pcfg,Psh)).
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 168?175, Prague, Czech Republic, June.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
128?135, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ?04: Proceedings
Distance Precision Recall F1
1 0.70 0.75 0.72
2 0.70 0.62 0.65
3 0.66 0.62 0.64
4 0.56 0.56 0.56
5 0.53 0.49 0.51
6 0.59 0.66 0.62
7 0.50 0.44 0.47
8 0.57 0.33 0.42
9 0.67 0.40 0.50
10 1.00 0.17 0.29
Table 6: Link distance precision, recall and f-score, on
the WSJ Section 22 |w| ? 10 heldout set.
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 478.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Igor? A. Mel?c?uk. 1988. Dependency Syntax: theory and
practice. State University of New York Press, Albany.
Radford Neal. 2003. Slice sampling. Annals of Statis-
tics, 31:705?767.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From Baby Steps to Leapfrog: How
?Less is More? in unsupervised dependency parsing.
In Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D. Manning. 2010b. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning (CoNLL-2010).
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010c. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2010).
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985?
992.
1213
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 223?232, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Bayesian Model for Learning SCFGs with Discontiguous Rules
Abby Levenberg
Dept. of Computer Science
University of Oxford
ablev@cs.ox.ac.uk
Chris Dyer
School of Computer Science
Carnegie Mellon Univeristy
cdyer@cs.cmu.edu
Phil Blunsom
Dept. of Computer Science
University of Oxford
pblunsom@cs.ox.ac.uk
Abstract
We describe a nonparametric model
and corresponding inference algorithm
for learning Synchronous Context Free
Grammar derivations for parallel text. The
model employs a Pitman-Yor Process prior
which uses a novel base distribution over
synchronous grammar rules. Through both
synthetic grammar induction and statistical
machine translation experiments, we show
that our model learns complex translational
correspondences? including discontiguous,
many-to-many alignments?and produces
competitive translation results. Further,
inference is efficient and we present results on
significantly larger corpora than prior work.
1 Introduction
In the twenty years since Brown et al1992) pio-
neered the first word-based statistical machine trans-
lation (SMT) models substantially more expressive
models of translational equivalence have been devel-
oped. The prevalence of complex phrasal, discon-
tiguous, and non-monotonic translation phenomena
in real-world applications of machine translation has
driven the development of hierarchical and syntac-
tic models based on synchronous context-free gram-
mars (SCFGs). Such models are now widely used in
translation and represent the state-of-the-art in most
language pairs (Galley et al2004; Chiang, 2007).
However, while the models used for translation have
evolved, the way in which they are learnt has not:
na??ve word-based models are still used to infer trans-
lational correspondences from parallel corpora.
In this work we bring the learning of the minimal
units of translation in step with the representational
power of modern translation models. We present a
nonparametric Bayesian model of translation based
on SCFGs, and we use its posterior distribution to
infer synchronous derivations for a parallel corpus
using a novel Gibbs sampler. Our model is able
to: 1) directly model many-to-many alignments,
thereby capturing non-compositional and idiomatic
translations; 2) align discontiguous phrases in both
the source and target languages; 3) have no restric-
tions on the length of a rule, the number of nonter-
minal symbols per rule, or their configuration.
Learning synchronous grammars is hard due to
the high polynomial complexity of dynamic pro-
gramming and the exponential space of possible
rules. As such most prior work for learning SCFGs
has relied on inference algorithms that were heuristi-
cally constrained or biased by word-based alignment
models and small experiments (Wu, 1997; Zhang et
al., 2008; Blunsom et al2009; Neubig et al2011).
In contrast to these previous attempts, our SCFG
model scales to large datasets (over 1.3M sentence
pairs) without imposing restrictions on the form of
the grammar rules or otherwise constraining the set
of learnable rules (e.g., with a word alignment).
We validate our sampler by demonstrating its
ability to recover grammars used to generate
synthetic datasets. We then evaluate our model by
inducing word alignments for SMT experiments
in several typologically diverse language pairs and
across a range of corpora sizes. Our results attest to
our model?s ability to learn synchronous grammars
encoding complex translation phenomena.
223
2 Prior Work
The goal of directly inducing phrasal translation
models from parallel corpora has received a lot of
attention in the NLP and SMT literature. Marcu
and Wong (2002) presented an ambitious maximum
likelihood model and EM inference algorithm for
learning phrasal translation representations. The
first issue this model faced was a massive parameter
space and intractable inference. However a more
subtle issue is that likelihood based models of this
form suffer from a degenerate solution, resulting
in the model learning whole sentences as phrases
rather than minimal units of translation. DeNero
et al2008) recognised this problem and proposed
a nonparametric Bayesian prior for contiguous
phrases. This had the dual benefits of biasing the
model towards learning minimal translation units,
and integrating out the parameters such that a much
smaller set of statistics would suffice for inference
with a Gibbs sampler. However this work fell short
by not evaluating the model independently, instead
only presenting results in which it was combined
with a standard word-alignment initialisation, thus
leaving open the question of its efficacy.
The fact that flat phrasal models lack a structured
approach to reordering has led many researchers to
pursue SCFG induction instead (Wu, 1997; Cherry
and Lin, 2007; Zhang et al2008; Blunsom et
al., 2009). The asymptotic time complexity of
the inside algorithm for even the simplest SCFG
models is O(|s|3|t|3), too high to be practical for
most real translation data. A popular solution to
this problem is to heuristically restrict inference
to derivations which agree with an independent
alignment model (Cherry and Lin, 2007; Zhang et
al., 2008). However this may have the unintended
effect of biasing the model back towards the initial
alignments that they attempt to improve upon.
More recently Neubig et al2011) reported a
novel Bayesian model for phrasal alignment and
extraction that was able to model phrases of multiple
granularities via a synchronous Adaptor Grammar.
However this model suffered from the common
problem of intractable inference and results were
presented for a very small number of samples from
a heuristically pruned beam, making interpreting
the results difficult.
Blunsom et al2009) presented an approach
similar to ours that implemented a Gibbs sampler
for a nonparametric Bayesian model of ITG. While
that work managed to scale to a non-trivially sized
corpus, like other works it relied on a state-of-the-art
word alignment model for initialisation. Our model
goes further by allowing discontiguous phrasal
translation units. Surprisingly, the freedom
that this extra power affords allows the Gibbs
sampler we propose to mix more quickly, allowing
state-of-the-art results from a simple initialiser.
3 Model
We use a nonparametric generative model based on
the 2-parameter Pitman-Yor process (PYP) (Pitman
and Yor, 1997), a generalisation of the Dirichlet Pro-
cess, which has been used for various NLP modeling
tasks with state-of-the-art results such as language
modeling, word segmentation, text compression and
part of speech induction (Teh, 2006; Goldwater et
al., 2006; Wood et al2011; Blunsom and Cohn,
2011). In this section we first provide a brief defi-
nition of the SCFG formalism and then describe our
PYP prior for them.
3.1 Synchronous Context-Free Grammar
An synchronous context-free grammar (SCFG) is a
5-tuple ??,?, V, S,R? that generalises context-free
grammar to generate strings concurrently in two lan-
guages (Lewis and Stearns, 1968). ? is a finite set of
source language terminal symbols, ? is a finite set
of target language terminal symbols, V is a set of
nonterminal symbols, with a designated start sym-
bol S, and R is a set of synchronous rewrite rules.
A string pair is generated by starting with the pair
?S1 | S1? and recursively applying rewrite rules of
the form X ? ?s, t, a? where the left hand side
(LHS) X is a nonterminal in V , s is a string in
(? ? V )?, t is a string in (? ? V )? and a specifies
a one-to-one mapping (bijection) between nontermi-
nal symbols in s and t. The following are examples:1
VP ? ? schlage NP1 NP2 vor | suggest NP2 to NP1 ?
NP ? ? die Kommission | the commission ?
1The nonterminal alignment a is indicated through sub-
scripts on the nonterminals.
224
In a probabilistic SCFG, rules are associated with
probabilities such that the probabilities of all
rewrites of a particular LHS category sum to 1.
Translation with SCFGs is carried out by parsing
the source language with the monolingual source
language projection of the grammar (using standard
monolingual parsing algorithms), which induces
a parallel tree structure and translation in the
target language (Chiang, 2007). Alignment or
synchronous parsing is the process of concurrently
parsing both the source and target sentences,
uncovering the derivation or derivations that give
rise to a string pair (Wu, 1997; Dyer, 2010).
Our goal is to infer the most probable SCFG
derivations that explain a corpus of parallel sen-
tences, given a nonparametric prior over probabilis-
tic SCFGs. In this work we will consider grammars
with a single nonterminal category X.
3.2 Pitman-Yor Process SCFG
Before training we have no way of knowing how
many rules will be needed in our grammar to ade-
quately represent the data. By using the Pitman-
Yor process as a prior on the parameters of a syn-
chronous grammar we can formulate a model which
prefers smaller numbers of rules that are reused
often, thereby avoiding degenerate grammars con-
sisting of large, overly specific rules. However, as
the data being fit grows, the model can become more
complex. The PYP is parameterised by a discount
parameter d, a strength parameter ?, and the base
distribution G0, which gives the prior probability
of an event (in our case, events are rules) before
any observations have occurred. The discount is
subtracted from each positive rule count and damp-
ens the rich get richer effect where frequent rules
are given higher probability compared to infrequent
ones. The strength parameter controls the variance,
or concentration, about the base distribution.
In our model, a draw from a PYP is a distribution
over SCFG rules with a particular LHS (in fact, it is
a distribution over all well-formed rules). From this
distribution we can in turn draw individual rules:
GX ? PY(d, ?,G0),
X ? ?s, t, a? ? GX .
Although the PYP has no known analytical form,
we can marginalise out the GX ?s and reason about
Step 1: Generate source side length.
Step 2: Generate source side configuration of 
terminals (and non-terminal placeholders).
Step 3: Generate target length.
Step 4. Generate target side configuration of 
terminals (and non-terminal placeholders).
Step 5. Generate the words.
X < _ _ _  ||| ? >
X < X1 _ X2 ||| ? >
X < X1 _ X2 ||| _ _ _  >
X < X1 _ X2 ||| _ X1 X2  >
X < X1 ? X2 ||| you X1 X2  >
Figure 1: Example generation of a synchronous
grammar rule in our G0.
individual rules directly using the process described
by Teh (2006). In this process, at time n a rule rn
is generated by stochastically deciding whether to
make another copy of a previously generated rule
or to draw a new one from the base distribution, G0.
Let ? = (?1, ?2, . . .) be the sequence of draws from
G0; thus |?| is the total number of draws from G0. A
rule rn corresponds to a selection of a ?k. Let ck
be a counter indicating the number of times ?k has
been selected. In particular, we set rn to ?k with
probability
ck ? d
? + n ,
and increment ck, or with probability
? + d ? |?|
? + n ,
we draw a new rule from G0, append it to ?, and use
it for rn.
3.3 Base Distribution
The base distribution G0 for the PYP assigns prob-
ability to a rule based our belief about what consti-
tutes a good rule independent of observing any of
225
the data. We describe a novel generative process for
all rules X ? ?s, t, a? that encodes these beliefs.
We describe the generative process generally here
in text, and readers may refer to the example in Fig-
ure 1. The process begins by generating the source
length (total number of terminal and nonterminal
symbols, written |s|) by drawing from a Poisson dis-
tribution with mean 1:
|s| ? Poisson(1) .
This assigns high probability to shorter rules,
but arbitrarily long rules are possible with a low
probability. Then, for every position in s, we decide
whether it will contain a terminal or nonterminal
symbol by repeated, independent draws from a
Bernoulli distribution. Since we believe that shorter
rules should be relatively more likely to contain
terminal symbols than longer rules, we define the
probability of a terminal symbol to be ?|s| where
0 < ? < 1 is a hyperparameter.
si ? Bernoulli(?|s|) ? i ? [1, |s|] .
We next generate the length of the target side of
the rule. Let #NT(s) denote the number of nonter-
minal symbols we generated in s, i.e., the arity of
the rule. Our intuition here is that source and target
lengths should be similar. However, to ensure that
the rule is well-formed, t must contain exactly as
many nonterminal symbols as the source does. We
therefore draw the number of target terminal sym-
bols from a Poisson whose mean is the number of
terminal symbols in the source, plus a small constant
?0 to ensure that it is greater than zero:
|t| ? #NT(s) ? Poisson (|s| ? #NT(s) + ?0) .
We then determine whether each position in t is
a terminal or nonterminal symbol by drawing uni-
formly from the bag of #NT(s) source nontermi-
nals and |t| ? #NT(s) terminal indicators, with-
out replacement. At this point we have created a
rule template which indicates how large the rule is,
whether each position contains a terminal or non-
terminal symbol, and the reordering of the source
nonterminals a. To conclude the process we must
select the terminal types from the source and target
vocabularies. To do so, we use the following distri-
bution:
Pterminals(s, t) =
PM1?(s, t) + PM1?(s, t)
2
where PM1?(s, t) (PM1?(s, t)) first generates the
source (target) terminals from uniform draws from
the vocabulary, then generates the string in the other
language according to IBM MODEL 1, marginaliz-
ing over the alignments (Brown et al1993).
4 Gibbs Sampler
In this section we introduce a Gibbs sampler that
enables us to perform posterior inference given a
corpus of sentence pairs. Our innovation is to repre-
sent the synchronous derivation of a sentence pair in
a hierarchical 4-dimensional binary alignment grid,
with elements z[s,t,u,v] ? {0, 1}.
The settings of the grid variables completely
determine the SCFG rules in the current derivation.
A setting of a binary variable z[s,t,u,v] = 1 represents
a constituent linking the source span [s, t] and the
target span [u, v] in the current derivation; variables
with a value of 0 indicate no link between spans
[s, t] and [u, v].2 This relationship from our grid
representation is illustrated in Figure 2a.
Our Gibbs sampler operates over the space of all
the random variables z[s,t,u,v], resampling one at a
time. Changes to a single variable imply that at most
two additional rules must be generated, as illustrated
in Figure 2b. The probability of choosing a binary
setting of 0 or 1 for a variable is proportional to the
probability of generating the two derivations under
the model described in the previous section. Note
that for a given sentence, most of the bispan vari-
ables must be set to 0 otherwise they would violate
the strict nesting constraint required for valid SCFG
derivations. We discuss below how to exploit this
fact to limit the number of binary variables that must
be resampled for each sentence.
To be valid, a Gibbs sampler must be ergodic and
satisfy detailed balance. Ergodicity requires that
there is non-zero probability that any state in the
sampler be reachable from any other state. Clearly
2Our grid representation is the synchronous generalisation
of the well-known correspondence between CFG derivations
and Boolean matrices; see Lee (2002) for an overview.
226
Amna will
{mna
succeed
kAmyAb
hw
gy
AmnA
awiilaaswu
cl
eduu
(a) An example grid representation of a syn-
chronous derivation. The SCFG rules (annotated
with their bispans) that correspond to this setting
of the grid are:
X[0,4,0,3] ?
? X[0,1,0,1] X[1,4,1,3] | X[0,1,0,1] X[1,4,1,3] ?
X[0,1,0,1] ? ? {mna | Amna ?
X[1,4,1,3] ? ? kAmyAb hw gy | will succeed ?
Amna will
{mna
succeed
kAmyAb
hw
gy
AmnA
awiilaaswu
cl
eduu
(b) The toggle operator resamples a bispan vari-
able (here, z[1,3,2,3], shown in blue) to determine
whether it should be subtracted from the immedi-
ately dominating rule (bispan in red) and made into
a child rule in the derivation. This would require
the addition of the following two rules:
X[1,4,1,3] ? ? X[1,3,2,3] gy | will X[1,3,2,3]?
X[1,3,2,3] ? ? kAmyAb hw | succeed ?
Alternatively, the active bispan variable can be set
so it is not a constituent, which would require the
single rule:
X[1,4,1,3] ? ? kAmyAb hw gy | will succeed ?
Figure 2: A single operation of the Gibbs sampler for a binary alignment grid.
our operator satisfies this since given any configu-
ration of the alignment grid we can use the toggle
operator to flatten the derivation to a single rule and
then break it back down to reach any derivation.
Detailed balance requires that the probability of
transitioning between two possible adjacent sampler
states respects their joint probabilities in the station-
ary distribution. One way to ensure this is to make
the order in which bispan variables are visited deter-
ministic and independent of the variables? current
settings. Then, the probability of the sampler tar-
geting any bispan in the grid is equal regardless of
the current configuration of the alignment grid.
A naive instantiation of this strategy is to visit all
|s|2|t|2 bispans in some order. However, since we
wish to be able to draw many samples, this is not
computationally feasible. A much more efficient
approach avoids resampling variables that would
result in violations without visiting each of them
individually. However, to ensure detailed balanced
is maintained, the order that we resample bispans
has to match the order we would sample them using
any exhaustive approach. We achieve this by always
checking a derivation top-down, from largest to
smallest bispan. Under this ordering, whether or not
a smaller bispan is visited will be independent of
how the larger ones were resampled. Furthermore,
the set of variables that may be resampled is fixed
given this ordering. Therefore, the probability of
sampling any possible bispan in the sentence pair is
still uniform (ensuring detailed balance), while our
sampler remains fast.
5 Evaluation
The preceding sections have introduced a model,
and accompanying inference technique, designed to
induce a posterior distribution over SCFG deriva-
tions containing discontiguous and phrasal transla-
tion rules. The evaluation that follows aims to deter-
mine our models ability to meet these design goals,
and to do so in a range of translation scenarios.
In order to validate both the model and the sam-
pler?s ability to learn an SCFG we first conduct a
synthetic experiment in which the true grammar is
227
known. Subsequently we conduct a series of experi-
ments on real parallel corpora of increasing sizes to
explore the empirical properties of our model.
5.1 Synthetic Data Experiments
Prior work on SCFG induction for SMT has val-
idated modeling claims by reporting BLEU scores
on real translation tasks. However, the combination
of noisy data and the complexity of SMT pipelines
conspire to obscure whether models actually achieve
their design goals, normally stated in terms of an
ability to induce SCFGs with particular properties.
Here we include a small synthetic data experiment
to clearly validate our models ability to learn an
SCFG that includes discontiguous and phrasal trans-
lation rules with non-monotonic word order.
Using the probabilistic SCFG shown in the top
half of Table 1 we stochastically generated three
thousand parallel sentence pairs as training data for
our model. We then ran the Gibbs sampler for fifty
iterations through the data.
The bottom half of Table 1 lists the five rules
with the highest marginal probability estimated by
the sampler. Encouragingly our model was able to
recover a grammar very close to the original. Even
for such a small grammar the space of derivations
is enormous and the task of recovering it from a
data sample is non-trivial. The divergence from the
true probabilities is due to the effect of the prior
assigning shorter rules higher probability. With a
larger data sample we would expect the influence of
the prior in the posterior to diminish.
5.2 Machine Translation Evaluation
Ultimately the efficacy of a model for SCFG induc-
tion will be judged on its ability to underpin a state-
of-the-art SMT system. Here we evaluate our model
by applying it to learning word alignments for par-
allel corpora from which SMT systems are induced.
We train models across a range of corpora sizes and
for language pairs that exhibit the type of complex
alignment phenomena that we are interested in mod-
eling: Chinese ? English (ZH-EN), Urdu ? English
(UR-EN) and German ? English (DE-EN).
Data and Baselines
The UR-EN corpus is the smallest of those used in
our experiments and is taken from the NIST 2009
GRAMMAR RULE TRUE PROBABILITY
X? ? X1 a X2 |X1 X2 1 ? 0.2
X? ? b c d | 3 2 ? 0.2
X? ? b d | 3 ? 0.2
X? ? d | 3 ? 0.2
X? ? c d | 3 1 ? 0.2
SAMPLED RULE SAMPLED PROBABILITY
X? ? d | 3 ? 0.25
X? ? b d | 3 ? 0.24
X? ? c d | 3 1 ? 0.24
X? ? b c d | 3 2 ? 0.211
X? ? X1 a X2 |X1 X2 1 ? 0.012
Table 1: Manually created SCFG used to generate
synthetic data, and the five most probable inferred
rules by our model.
ZH-EN
NIST
UR-EN
NIST
DE-EN
EUROPARL
TRAIN (SRC) 8.6M 1.2M 34M
TRAIN (TRG) 9.5M 1.0M 36M
DEV (SRC) 22K 18K 26K
DEV (TRG) 27K 16K 28K
Table 2: Corpora statistics (in words).
translation evaluation.3 The ZH-EN data is of a
medium scale and comes from the FBIS corpus.
The DE-EN pair constitutes the largest corpus and
is taken from Europarl, the proceedings of the Euro-
pean Parliament (Koehn, 2003). Statistics for the
data are shown in Table 2. We measure translation
quality via the BLEU score (Papineni et al2001).
All translation systems employ a Hiero
translation model during decoding. Baseline
word alignments were obtained by running
GIZA++ in both directions and symmetrizing using
the grow-diag-final-and heuristic (Och and
Ney, 2003; Koehn et al2003). Decoding was
performed with the cdec decoder (Dyer et al
2010) with the synchronous grammar extracted
using the techniques developed by Lopez (2008).
All translation systems include a 5-gram language
model built from a five hundred million token subset
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/
228
LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG
PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT.
UR-EN MT09 23.1 18.5 23.7 24.0
ZH-EN MT03-08 29.4 19.8 28.3 29.8
DE-EN EUROPARL 28.4 25.5 27.8 29.2
Table 3: Results for the SMT experiments in BLEU . The baseline is produced using a full GIZA++ run. The
MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling.
The PYP-SCFG columns show results for the 500th sample for both MODEL 1 and HMM initialisations.
of all the English data made available for the NIST
2009 shared task (Graff, 2003).
Experimental Setup
To obtain the PYP-SCFG word alignments we
ran the sampler for five hundred iterations for each
of the language pairs and experimental conditions
described below. We used the approach of Newman
et al2007) to distribute the sampler across multi-
ple threads. The strength ? and discount d hyper-
parameters of the Pitman-Yor Processes, and the ter-
minal penalty ? (Section 3.3), were inferred using
slice sampling (Neal, 2000).
The Gibbs sampler requires an initial set of
derivations from which to commence sampling. In
our experiments we investigated both weak and
a strong initialisations, the former based on word
alignments from IBM Model 1 and the latter on
alignments from an HMM model (Vogel et al
1996). For decoding we used the word alignments
implied by the derivations in the final sample to
extract a Hiero grammar with the same standard set
of relative frequency, length, and language model
features used for the baseline.
Weak Initialisation
Our first translation experiments ascertain the
degree to which our proposed Gibbs sampling
inference algorithm is able to learn good
synchronous derivations for the PYP-SCFG model.
A number of prior works on alignment with Gibbs
samplers have only evaluated models initialised
with the more complex GIZA++ alignment models
(Blunsom et al2009; DeNero et al2008), as a
result it can be difficult to separate the performance
of the sampler from that of the initialisation.
In order to do this, we initialise the sampler
PYP-SCFG
LANGUAGE PAIR MODEL 1 INIT. HMM INIT.
UR-EN 1.93/2.08 1.45/1.58
ZH-EN 3.47/4.28 1.69/2.37
DE-EN 4.05/4.77 1.50/2.04
Table 4: Average source/target rule lengths in the
PYP-SCFG models after the 500th sample for the
different initialisations.
using just the MODEL 1 distribution used in the
PYP-SCFG model?s base distribution. We denote
this a weak initialisation as no alignment models
outside of those included in the PYP-SCFG model
influence the resulting word alignments. The
BLEU scores for translation systems built from the
five hundredth sample are show in the WEAK M1
INIT. column of Table 3. Additionally we build a
translation system from the MODEL 1 alignment
used to initialise the sampler without using using our
PYP-SCFG model or sampling. BLEU scores are
shown in the MODEL 1 INITIALISATION column
of Table 3. Firstly it is clear MODEL 1 is indeed a
weak initialiser as the resulting translation systems
achieve uniformly low BLEU scores. In contrast, the
models built from the output of the Gibbs sampler
for the PYP-SCFG model achieve BLEU scores
comparable to those of the MODEL 4 BASELINE.
Thus the sampler has moved a good distance from
its initialisation, and done so in a direction that
results in better synchronous derivations.
Strong Initialisation
Given we have established that the sampler can
produce state-of-the-art translation results from a
229
weak initialisation, it is instructive to investigate
whether initialising the model with a strong
alignment system, the GIZA++ HMM (Vogel et
al., 1996), leads to further improvements. Column
HMM INIT. of Table 3 shows the results for
initialising with the HMM word alignments and
sampling for 500 iterations. Starting with a stronger
initial sample results in both quicker mixing and
better translation quality for the same number of
sampling iterations.
Table 4 compares the average lengths of the rules
produced by the sampler with both the strong and
weak initialisers. As the size of the training corpora
increases (UR-EN ? ZH-EN ? DE-EN) we see that
the average size of the rules produced by the weakly
initialised sampler also increases, while that of the
strongly initialised model stays relatively uniform.
Initially both samplers start out with a large num-
ber of long rules and as the sampling progresses
the rules are broken down into smaller, more gen-
eralisable, pieces. As such we conclude from these
metrics that after five hundred samples the strongly
initialised model has converged to sampling from a
mode of the distribution while the weakly initialised
model converges more slowly and on the longer cor-
pora is still travelling towards a mode. This sug-
gests that longer sampling runs, and Gibbs operators
that make simultaneous updates to multiple parts
of a derivation, would enable the weakly initialised
model to obtain better translation results.
Grammar Analysis
The BLEU scores are informative as a measure of
translation quality but we also explored some of the
differences in the grammars obtained from the PYP-
SCFG model compared to the standard approach. In
Figures 3 and 4 we show some basic statistics of
the grammars our model produces. From Figure 3
we see that the number of unique rules in the PYP-
SCFG grammar decreases steadily as the sampler
iterates through the data, so the model is finding an
increasingly sparser distribution with fewer but bet-
ter quality rules as sampling progresses. Note that
the gradient of the curves appears to be a function of
the size of the corpus and suggests that the model
built from the large DE-EN corpus would benefit
from a longer sampling run. Figure 4 shows the dis-
tribution of rules with a given arity as a percentage
 140
 160
 180
 200
 220
 240
 260
 280
 300
 320
 340
 0  20  40  60  80  100
u
n
iq
ue
 g
ra
m
m
ar
 ru
le
s i
n 
PY
P
samples
ur-en (* 1k)
zh-en (* 3k)
de-en (* 10k)
Figure 3: Unique grammar rules for each language
pair as a function of the number of samples. The
number of rule types decreases monotonically as
sampling continues. Rule counts are displayed by
normalised corpus size (see Table 2).
X? ?? | end of ?
X? ??? | ninth ?*
X? ??? X | charter X ?
X? ??? | confidence in ?
X? ????? X | the chinese government X ?
X? ??? | are ?
X? ?????? X | beijing , X ?*
X? ????? | departments concerned ?
X? ??????? X | washington , X ?*
X? ???? X1? X2 , | he X1 X2 , ?*
Table 5: The five highest ZH-EN probability rules in
the Hiero grammar built from the PYP-SCFG that
are not in the baseline Hiero grammar (top), and the
top five rules in the baseline Hiero grammar that
are not in the PYP-SCFG grammar (bottom). An
* indicates a bad translation rule.
of the full grammar after the final sampling iteration.
The model prior biases the results to shorter rules as
the vast majority of the model probability mass is on
rules with zero, one or two nonterminals.
Tables 5 and 6 show the most probable rules in the
Hiero translation system obtained using the PYP-
SCFG alignments that are not present in the TM
from the GIZA++ alignments and visa versa. For
both language pairs, four of the top five rules in
230
X? ? yh | it is ?
X? ? zmyn | the earth ?
X? ? yhy X | the same X ?
X? ? X1 nhyN X2 gy | X2 not be X1 ?
X? ? X1 gY kh X2 | recommend that X2 X1 ?*
X? ? hwN gY | will ?
X? ? Gyr mlky | international ?*
X? ? X1 *rAye kY X2 | X2 to X1 sources ?*
X? ? nY X1 nhyN kyA X2 | did not X1 X2 ?*
X? ? xAtwn X1 ky X2 | woman X2 the X1?
Table 6: Five of the top scoring rules in the UR-EN
Hiero grammar from sampled PYP-SCFG align-
ments (top) versus the baseline UR-EN Hiero gram-
mar rules not in the sampled grammar (bottom). An
* indicates a bad translation rule.
 0
 0.1
 0.2
 0.3
 0.4
 0  1  2 3+
%
 o
f r
ul
es
arity
zh-en
ur-en
de-en
Figure 4: The percentage of rules with a given arity
in the final grammar of the PYP-SCFG model.
the PYP-SCFG grammar that are not in the heuris-
tically extracted grammar are correct and minimal
phrasal units of translation, whereas only two of the
top probability rules in the GIZA++ grammar are of
good translation quality.
6 Conclusion and Further Work
In this paper we have presented a nonparametric
Bayesian model for learning SCFGs directly
from parallel corpora. We have also introduced
a novel Gibbs sampller that allows for efficient
posterior inference. We show state-of-the-art
results and learn complex translation phenomena,
including discontiguous and many-to-many
phrasal alignments, without applying any heuristic
restrictions on the model to make learning tractable.
Our evaluation shows that we can use a principled
approach to induce SCFGs designed specifically
to utilize the full power of grammar based SMT
instead of relying on complex word alignment
heuristics with inherent bias.
Future work includes the obvious extension to
learning SCFGs that contain multiple nonterminals
instead of a single nonterminal grammar. We also
expect that expanding our sampler beyond strict
binary sampling may allow us to explore the space
of hierarchical word alignments more quickly
allowing for faster mixing. We expect with these
extensions our model of grammar induction may
further improve translation output.
Acknowledgements
This work was supported by a grant from Google,
Inc. and EPRSRC grant no. EP/I010858/1 (Leven-
berg and Blunsom), the U. S. Army Research Lab-
oratory and U. S. Army Research Office under con-
tract/grant no. W911NF-10-1-0533 (Dyer).
References
P. Blunsom and T. Cohn. 2011. A hierarchical pitman-
yor process hmm for unsupervised part of speech
induction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 865?874, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. 2009.
A gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 782?790, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
P. F. Brown, V. J. D. Pietra, R. L. Mercer, S. A. D. Pietra,
and J. C. Lai. 1992. An estimate of an upper bound
for the entropy of english. Computational Linguistics,
18(1):31?40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
C. Cherry and D. Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
231
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17?24, Rochester, New York, April.
Association for Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 314?323, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, ACLDemos ?10, pages 7?12.
C. Dyer. 2010. Two monolingual parses are better than
one (synchronous parse). In Proc. of NAACL.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In D. M. Susan Dumais
and S. Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 273?280, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, Syndney, Australia.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium (LDC-2003T05).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceedings
of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
L. Lee. 2002. Fast context-free grammar parsing
requires fast Boolean matrix multiplication. Journal
of the ACM, 49(1):1?15.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15:465?488, July.
A. Lopez. 2008. Machine Translation by Pattern Match-
ing. Ph.D. thesis, University of Maryland.
D. Marcu and D. Wong. 2002. A phrase-based,joint
probability model for statistical machine translation.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 133?
139. Association for Computational Linguistics, July.
R. Neal. 2000. Slice sampling. Annals of Statistics,
31:705?767.
G. Neubig, T. Watanabe, E. Sumita, S. Mori, and
T. Kawahara. 2011. An unsupervised model for joint
phrase alignment and extraction. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 632?641, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2007. Distributed inference for latent dirichlet al
cation. In NIPS. MIT Press.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In ACL ?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 311?318, Morristown, NJ, USA.
Association for Computational Linguistics.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Ann. Probab., 25:855?900.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985?992.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of the 16th conference on Computational linguis-
tics, pages 836?841, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
F. Wood, J. Gasthaus, C. Archambeau, L. James, and
Y. W. Teh. 2011. The sequence memoizer. Commu-
nications of the Association for Computing Machines,
54(2):91?98.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23:377?403, September.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL-08:
HLT, pages 97?105, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
232
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345?356,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Adaptor Grammars for Learning Non-Concatenative Morphology
Jan A. Botha and Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{jan.botha,phil.blunsom}@cs.ox.ac.uk
Abstract
This paper contributes an approach for
expressing non-concatenative morphological
phenomena, such as stem derivation in
Semitic languages, in terms of a mildly
context-sensitive grammar formalism. This
offers a convenient level of modelling ab-
straction while remaining computationally
tractable. The nonparametric Bayesian frame-
work of adaptor grammars is extended to this
richer grammar formalism to propose a prob-
abilistic model that can learn word segmenta-
tion and morpheme lexicons, including ones
with discontiguous strings as elements, from
unannotated data. Our experiments on He-
brew and three variants of Arabic data find
that the additional expressiveness to capture
roots and templates as atomic units improves
the quality of concatenative segmentation and
stem identification. We obtain 74% accuracy
in identifying triliteral Hebrew roots, while
performing morphological segmentation with
an F1-score of 78.1.
1 Introduction
Unsupervised learning of morphology is the task
of acquiring, from unannotated data, the intra-word
building blocks of a language and the rules by which
they combine to form words. This task is of interest
both as a gateway for studying language acquisition
in humans and as a way of producing morphological
analyses that are of practical use in a variety of nat-
ural language processing tasks, including machine
translation, parsing and information retrieval.
A particularly interesting version of the morphol-
ogy learning problem comes from languages that
use templatic morphology, such as Arabic, He-
brew and Amharic. These Semitic languages de-
rive verb and noun stems by interspersing abstract
root morphemes into templatic structures in a non-
concatenative way. For example, the Arabic root
k?t?b can combine with the template (i-a) to derive
the noun stem kitab (book). Established morpho-
logical analysers typically ignore this process and
simply view the derived stems as elementary units
(Buckwalter, 2002), or their account of it coincides
with a requirement for extensive linguistic knowl-
edge and hand-crafting of rules (Finkel and Stump,
2002; Schneider, 2010; Altantawy et al, 2010). The
former approach is bound to suffer from vocabu-
lary coverage issues, while the latter clearly does
not transfer easily across languages. The practical
appeal of unsupervised learning of templatic mor-
phology is that it can overcome these shortcomings.
Unsupervised learning of concatenative morphol-
ogy has received extensive attention, partly driven
by the MorphoChallenge (Kurimo et al, 2010) in re-
cent years, but that is not the case for root-templatic
morphology (Hammarstro?m and Borin, 2011).
In this paper we present a model-based method
that learns concatenative and root-templatic mor-
phology in a unified framework. We build on two
disparate strands of work from the literature: Firstly,
we apply simple Range Concatenating Grammars
(SRCGs) (Boullier, 2000) to parse contiguous and
discontiguous morphemes from an input string.
These grammars are mildly-context sensitive (Joshi,
1985), a superset of context-free grammars that
retains polynomial parsing time-complexity. Sec-
ondly, we generalise the nonparametric Bayesian
learning framework of adaptor grammars (Johnson
et al, 2007) to SRCGs.1 This should also be rel-
1Our formulation is in terms of SRCGs, which are equiv-
alent in power to linear context-free rewrite systems (Vijay-
Shanker et al, 1987) and multiple context-free grammars (Seki
et al, 1991), all of which are weaker than (non-simple) range
concatenating grammars (Boullier, 2000).
345
evant to other applications of probabilistic SRCGs,
e.g. in parsing (Maier, 2010), translation (Kaesham-
mer, 2013) and genetics (Kato et al, 2006).
In addition to unannotated data, our method re-
quires as input a minimal set of high-level grammar
rules that encode basic intuitions of the morphology.
This is where there would be room to become very
language specific. Our aim, however, is not to obtain
a best-published result in a particular language, but
rather to create a method that is applicable across
a variety of morphological processes. The specific
rules used in our empirical evaluation on Arabic and
Hebrew therefore contain hardly any explicit lin-
guistic knowledge about the languages and are ap-
plicable across the family of Semitic languages.
2 A powerful grammar for morphology
Concatenative morphology lends itself well to an
analysis in terms of finite-state transducers (FSTs)
(Koskenniemi, 1984). With some additional effort,
FSTs can also encode non-concatenative morphol-
ogy (Kiraz, 2000; Beesley and Karttunen, 2003;
Cohen-Sygal and Wintner, 2006; Gasser, 2009). De-
spite this seeming adequacy of regular languages to
describe morphology, we see two main shortcom-
ings that motivate moving further up the Chom-
sky hierarchy of formal languages: first is the is-
sue of learning. We are not aware of successful at-
tempts at inducing FST-based morphological analy-
sers in an unsupervised way, and believe the chal-
lenge lies in the fact that FSTs do not offer a conve-
nient way of expressing prior linguistic intuitions to
guide the learning process. Secondly, an FST com-
posed of multiple machines might capture morpho-
logical processes well and excel at analysis, but in-
terpretability of its internal operations are limited.
These shortcomings are overcome for concate-
native morphology by context-free adaptor gram-
mars, which allowed diverse segmentation models
to be formulated and investigated within a single
framework (Johnson et al, 2007; Johnson, 2008;
Sirts and Goldwater, 2013). In principle, that cov-
ers a wide range of phenomena (typical example
language in parentheses): affixal inflection (Czech)
and derivation (English), agglutinative derivation
(Turkish, Finnish), compounding (German). Our
agenda here is to extend that approach to include
non-concatenative processes such as root-templatic
derivation (Arabic), infixation (Tagalog) and cir-
cumfixation (Indonesian). In this pursuit, an ab-
straction that permits discontiguous constituents is
a highly useful modelling tool, but requires looking
beyond context-free grammars.
An idealised generative grammar that would cap-
ture all the aforementioned phenomena could look
like this:
Word? (Pre? Stem Suf?)+ (1)
e.g. English un+accept+able
Stem |Pre |Suf? Morph (2)
Stem? intercal (Root,Template) (3)
e.g. Arabic derivation k?t?b + i?a? kitab (book)
Stem? infix (Stem, Infix) (4)
e.g. Tagalog sulat (write)? sumulat (wrote)
Stem? circfix (Stem,Circumfix) (5)
e.g. Indonesian percaya (to trust)
? kepercayaan (belief)
where the symbols (excluding Word and Stem) im-
plicitly expand to the relevant terminal strings. The
bold-faced ?functions? combine the potentially dis-
contiguous yields of the argument symbols into sin-
gle contiguous strings, e.g. infix(s?ulat, um) pro-
duces stem sumulat.
Taken by themselves, the first two rules are sim-
ply a CFG that describes word formation as the
concatenation of stems and affixes, a formulation
that matches the underlying grammar of Morfessor
(Creutz and Lagus, 2007), a well-studied unsuper-
vised model.
The key aim of our extension is that we want the
grammar to capture a discontiguous string like k?t?b
as a single constituent in a parse tree. This leads to
well-understood problems in probabilistic grammars
(e.g. what is this rule?s probability?), but also corre-
sponds to the linguistic consideration that k?t?b is a
proper morpheme of the language (Prunet, 2006).
3 Simple range concatenating grammars
In this section we define SRCGs formally and
illustrate how they can be used to model non-
concatenative morphology. SRCGs define lan-
guages that are recognisable in polynomial time, yet
can capture discontiguous elements of a string un-
der a single category (Boullier, 2000). An SRCG-
346
rule operates on vectors of ranges in contrast to the
way a CFG-rule operates on single ranges (spans).
In other words, a non-terminal symbol in an SRCG
(CFG) derivation can dominate a subset (substring)
of terminals in an input string.
3.1 Formalism
An SRCG G is a tuple (N,T, V, P, S), with
finite sets of non-terminals (N ), termi-
nals (T ) and variables (V ), with a start sym-
bol S ? N . A rewrite rule p ? P of rank
r = ?(p) ? 0 has the form A(?1, . . . , ??(A)) ?
B1(?1,1, . . . , ?1,?(B1)) . . . Br(?r,1, . . . , ?r,?(Br)),
where each ?, ? ? (T ? V )?, and ?(A) is the
number of arguments a non-terminal A has, called
its arity. By definition, the start symbol has arity 1.
Any variable v ? V appearing in a given rule
must be used exactly once on each side of the
rule. Terminating rules are written with  as the
right-hand side and thus have rank 0.
A range is a pair of integers (i, j) denoting the
substring wi+1 . . . wj of a string w = w1 . . . wn.
A non-terminal becomes instantiated when its vari-
ables are bound to ranges through substitution. Vari-
ables within an argument imply concatenation and
therefore have to bind to adjacent ranges.
An instantiated non-terminal A? is said to de-
rive  if the consecutive application of a sequence
of instantiated rules rewrite it as . A string w is
within the language defined by a particular SRCG
iff the start symbol S, instantiated with the exhaus-
tive range (0, wn), derives .
An important distinction with regard to CFGs is
that, due to the instantiation mechanism, the order-
ing of non-terminals on the right-hand side of an
SRCG rule is irrelevant, i.e. A(ab) ? B(a)C(b)
and A(ab) ? C(b)B(a) are the same rule.2 Con-
sequently, the isomorphisms of any given SRCG
derivation tree all encode the same string, which is
uniquely defined through the instantiation process.
3.2 Application to morphological analysis
A fragment of the idealised grammar schema from
the previous section (?2) can be rephrased as an
SRCG by writing the rules in the newly introduced
2Certain ordering restrictions over the variables within an
argument need to hold for an SRCG to indeed be a simple RCG
(Boullier, 2000).
Word(wakitabi)
Suf(i)
i
Stm(kitab)
Template(i,a)Root(k,t,b)
Pre(wa)
aw k i t a b
Figure 1: Example derivation for wakitabi (and my
book) using the SRCG fragment from ?3.2. CFGs
cannot capture such crossing branches.
notation, and supplying a definition of the intercal
function as simply another rule of the grammar, with
instantiation for w = kitab shown below:
Word(abc)? Pre(a) Stem(b) Suf(c)
Stem(abcde)? Root(a, c, e) Template(b, d),
Stem(?0..1?, ?1..2?, ?2..3?, ?3..4?, ?4..5?)
? Root(?0..1?, ?2..3?, ?4..5?)
Template(?1..2?, ?3..4?)
Given an appropriate set of grammar rules (as we
present in ?5), we can parse an input string to ob-
tain a tree as shown in Figure 1. The overlapping
branches of the tree demonstrate that this grammar
captures something a CFG could not. From the parse
tree one can read off the word?s root morpheme and
the template used.
Although SRCGs specify mildly context-sensitive
grammars, each step in a derivation is context-free ?
a node?s expansion does not depend on other parts
of the tree. This property implies that a recogni-
tion/parsing algorithm can have a worst-case time
complexity that is polynomial in the input length n,
O(n(?+1)?) for arity ? and rank ?, which reduces
to O(n3?) for a binarised grammar. To capture the
maximal case of a root with k ? 1 characters and
k discontiguous templatic characters forming a stem
would require a grammar that has arity ? = k. For
Arabic, which has up to quadriliteral roots (k = 5),
the time complexity would be O(n15).3 This is a
daunting proposition for parsing, but we are careful
3The trade-off between arity and rank with respect to pars-
ing complexity has been characterised (Gildea, 2010), and the
appropriate refactoring may bring down the complexity for our
grammars too.
347
to set up our application of SRCGs in such a way
that this is not too big an obstacle:
Firstly, our grammars are defined over the char-
acters that make up a word, and not over words that
make up a sentence. As such, the input length n
would tend to be shorter than when parsing full sen-
tences from a corpus.
Secondly, we do type-based morphological analy-
sis, a view supported by evidence from Goldwater et
al. (2006), so each unique word in a dataset is only
ever parsed once with a given grammar. The set of
word types attested in the data sources of interest
here is fairly limited, typically in the tens of thou-
sands. For these reasons, our parsing and inference
tasks turn out to be tractable despite the high time
complexity.
4 Learning
4.1 Probabilistic SRCG
The probabilistic extension of SRCGs is similar to
the probabilistic extension of CFGs, and has been
used in other guises (Kato et al, 2006; Maier, 2010).
Each rule r ? P has an associated probability ?r
such that
?
r?PA
?r = 1. A random string in
the language of the grammar can then be obtained
through a generative procedure that begins with the
start symbol S and iteratively expands it until deriv-
ing : At each step for some current symbol A, a
rewrite rule r is sampled randomly from PA in ac-
cordance with the distribution over rules and used
to expand A. This procedure terminates when no
further expansions are possible. Of course, expan-
sions need to respect the range concatenating and or-
dering constraints imposed by the variables in rules.
The expansions imply a chain of variable bindings
going down the tree, and instantiation happens only
when rewriting into s but then propagates back up
the tree.
The probability P (w, t) of the resulting tree t and
terminal string w is the product
?
r ?r over the se-
quence of rewrite rules used. This generative proce-
dure is a conceptual device; in practice, one would
care about parsing some input string under this prob-
abilistic grammar.
4.2 PYSRCAG
A central property of the generative procedure un-
derlying probabilistic SRCGs is the fact that each
expansion happens independently, both of the other
expansions in the tree under construction and of any
other trees. To some extent, this flies in the face of
the reality of estimating a grammar from text, where
one would expect certain sub-trees to be used repeat-
edly across different input strings.
Adaptor grammars weaken this independence as-
sumption by allowing whole subtrees to be reused
during expansion. Informally, they act as a cache of
tree fragments whose tendency to be reused during
expansion is governed by the choice of adaptor func-
tion. Following earlier applications of adaptor gram-
mars (Johnson et al, 2007; Huang et al, 2011), we
employ the Pitman-Yor process (Pitman, 1995; Pit-
man and Yor, 1997) as adaptor function.
A Pitman-Yor Simple Range Concatenat-
ing Adaptor Grammar (PYSRCAG) is a tuple
G = (GS ,M,a, b,?), where GS is a probabilistic
SRCG as defined before and M ? N is a set
of adapted non-terminals. The vectors a and b,
indexed by the elements of M , are the discount
and concentration parameters for each adapted non-
terminal, with a ? [0, 1], b ? 0. ? are parameters to
Dirichlet priors on the rule probabilities ?.
PYSRCAG defines a generative process over a set
of trees T . Unadapted non-terminals A? ? N \M
are expanded as before (?4.1). For each adapted
non-terminal A ? M , a cache CA is maintained
for storing the terminating tree fragments expanded
from A earlier in the process, and we denote the
fragment corresponding to the i-th expansion of A
as zi. In other words, the sequence of indices zi
is the assignment of a sequence of expansions of
A to particular tree fragments. Given a cache CA
that has n previously generated trees comprising
m unique trees each used n1, . . . , nm times (where
n =
?
k nk), the tree fragment for the next expan-
sion of A, zn+1, is sampled conditional on the pre-
vious assignments z< according to
zn+1|z< ?
{
nk?a
n+b if zn+1 = k ? [1,m]
ma+b
n+b if zn+1 = m+ 1,
where a and b are those elements of a and b cor-
responding to A. The first case denotes the situa-
tion where a previously cached tree is reused for this
n + 1-th expansion of A; to be clear, this expands
A with a fully terminating tree fragment, meaning
that none of the nodes descending from A in the
348
tree being generated are subject to further expan-
sion. The second case by-passes the cache and ex-
pandsA according to the rules PA and rule probabil-
ities ?A of the underlying SRCG GS . Other caches
CB(B ? M) may come into play during those
expansions of the descendants of A; thus a PYS-
RCAG can define a hierarchical stochastic process.
Both cases eventually result in a terminating tree-
fragment for A, which is then added to the cache,
updating the counts n, nzn+1 and potentially m.
The adaptation does not affect the string language
of GS , but it maps the distribution over trees to one
that is distributed according to the PYP.
The invariance of SRCGs trees under isomor-
phism would make the probabilistic model deficient,
but we side-step this issue by requiring that grammar
rules are specified in a canonical way that ensures
a one-to-one correspondence between the order of
nodes in a tree and of terminals in the yield.
4.3 Inference under PYSRCAG
The inference procedure under our model is very
similar to that of CFG PY-adaptor grammars, so we
restate the central aspects here but refer the reader
to the original article by Johnson et al (2007) for
further details. First, one may integrate out the
adaptors to obtain a single distribution over the set
of trees generated from a particular non-terminal.
Thus, the joint probability of a particular sequence z
for the adapted non-terminal A with cached counts
(n1, . . . , nm) is
PY (z|a, b) =
?m
k=1 (a(k ? 1) + b)
?nk?1
j=1 (j ? a)
?n?1
i=0 (i+ b)
.
(6)
Taking all the adapted non-terminals into account,
the joint probability of a set of full trees T under the
grammar G is
P (T |a, b,?) =
?
A?M
B(?A + fA)
B(?A)
PY (z(T )|a, b),
(7)
where fA is a vector of the usage counts of rules
r ? PA across T , and B is the Euler beta function.
The posterior distribution over a set of strings
w is obtained by marginalising (7) over all trees
that have w as their yields. This is intractable to
compute directly, so instead we use MCMC tech-
niques to obtain samples from that posterior using a
component-wise Metropolis-Hastings sampler. The
sampler works by visiting each string w in turn and
drawing a new tree for it under a proposal grammar
GQ and randomly accepting that as the new analysis
for w according to the Metropolis-Hastings accept-
reject probability. As proposal grammar, we use the
analogous approximation of our G as Johnson et al
used for PCFGs, namely by taking a static snapshot
GQ of the adaptor grammar where additional rules
rewrite adapted non-terminals as the terminal strings
of their cached trees. Drawing a sample from the
proposal distribution is then a matter of drawing a
random tree from the parse chart of w under GQ.
Lastly, the adaptor hyperparameters a and b
are modelled by placing flat Beta(1, 1) and vague
Gamma(10, 0.1) priors on them, respectively, and
inferring their values using slice sampling (Johnson
and Goldwater, 2009).
5 Modelling root-templatic morphology
We start with a CFG-based adaptor grammar4 that
models words as a stem and any number of prefixes
and suffixes:
Word? Pre? Stem Suf? (8)
Pre | Stem | Suf? Char+ (9)
This fragment can be seen as building on the stem-
and-affix adaptor grammar presented in (Johnson et
al., 2007) for morphological analysis of English, of
which a later version also covers multiple affixes
(Sirts and Goldwater, 2013). In the particular case of
Arabic, multiple affixes are required to handle the at-
tachment of particles and proclitics onto base words.
To extend this to complex stems consisting of a
root with three radicals we have rules like the fol-
lowing:
Stem(abcdefg)? R3(b, d, e) T4(a, c, e, g) (10)
Stem(abcdef)? R3(a, c, e) T3(b, d, f) (11)
Stem(abcde)? R3(a, c, e) T2(b, d) (12)
Stem(abcd)? R3(a, c, d) T1(b) (13)
Stem(abc)? R3(a, b, c) (14)
4Adapted non-terminals are indicated by underlining and
we use the following abbreviations: X ? Y+ means one
or more instances of Y and encodes the rules X ? Ys and
Ys ? Ys Y | Y. Similarly, X ? Y? Z allows zero or more
instances of Y and encodes the rules X ? Z and X ? Y+ Z.
Further relabelling is added as necessary to avoid cycles among
adapted non-terminals.
349
The actual rules include certain permutations of
these, e.g. rule (13) has a variant R3(a, b, d)T1(c).
In unvocalised text, the standard written form of
Modern Standard Arabic (MSA), it may happen that
the stem and the root of a word form are one and the
same. So while rule (14) may look trivial, it ensures
that in such cases the radicals are still captured as de-
scendants of the non-terminal category R3, thereby
making their appearance in the cache.
A discontiguous non-terminal An is rewritten
through recursion on its arity down to 1, i.e.
An(v1, . . . , vn)? Al(v1, . . . , vn?1) Char(vn) with
base case A1(v) ? Char(v), where Char rewrites
all individual terminals as , vi are variables and
l = n?1.5 Note that although we provide the model
with two sets of discontiguous non-terminals R and
T, we do not specify their mapping onto the actual
terminal strings; no subdivision of the alphabet into
vowels and consonants is hard-wired.
6 Experiments
We evaluate our model on standard Arabic, Quranic
Arabic and Hebrew in terms of segmentation quality
and lexicon induction ability. These languages share
various properties, including morphology and lexi-
cal cognates, but are sufficiently different so as to
require manual intervention when transferring rule-
based morphological analysers across languages. A
key question in this evaluation is therefore whether
an appropriate instantiation of our model success-
fully generalises across related languages.
6.1 Data sets
Our models are unsupervised and therefore learn
from raw text, but their evaluation requires anno-
tated data as a gold-standard and these were derived6
as follows:
Arabic (MSA) We created the dataset BW by syn-
thesising 50k morphotactically correct word types
from the morpheme lexicons and consistency rules
supplied with the Buckwalter Arabic Morphological
5Including the arity as part of the non-terminal symbol
names forms part of our convention here to ensure that the
grammar contains no cycles, a situation which would compli-
cate inference under PYSRCAG.
6Our data preprocessing scripts are obtainable from
http://github.com/bothameister/pysrcag-data.
Types Stems Roots m/w c/w
BW 48428 24197 4717 2.3 6.4
BW
?
48428 30891 4707 2.3 10.7
QU
?
18808 12021 1270 1.9 9.9
HEB 5231 3164 492 2.1 6.7
Table 1: Corpus statistics, including average number
of morphemes (m/w) and characters (c/w) per word,
and total surface-realised roots of length 3 or 4.
Analyser (BAMA).7 This allowed control over the
word shapes, which is important to focus the evalu-
ation, while yielding reliable segmentation and root
annotations. BW has no vocalisation; we denote the
corresponding vocalised dataset as BW
?
.
Quranic Arabic We extracted the roughly 18k
word types from a morphologically analysed version
of the Quran (Dukes and Habash, 2010). As an ad-
ditional challenge, we left all given diacritics intact
for this dataset, QU
?
.
Hebrew We leveraged the Hebrew CHILDES
database as an annotated resource (Albert et al,
2013) and were able to extract 5k word types that
feature at least one affix to use as dataset HEB. The
corrected versions of words marked as non-standard
child language were used, diacritics were dropped,
and we conflated stressed and unstressed vowels to
overcome inconsistencies in the source data.
6.2 Models
We consider two classes of models. The first
is the strictly context-free adaptor grammar for
morphemes as sequences of characters using
rules (8)-(9), which we denote as Concat and
MConcat, where the latter allows multiple pre-
fixes/suffixes in a word. These serve as baselines for
the second class in which non-concatenative rules
are added. MTpl and Tpl denote the canonical ver-
7We used version 2.0, LDC2004L02, and sampled word
types having a single stem and at most one prefix, suffix or both,
according to the following random procedure: Sample a shape
(stem: 0.1, pre+stem: 0.25 stem+suf: 0.25, pre+stem+suf: 0.4).
Sample uniformly at random (with replacement) a stem from
the BAMA stem lexicon, and affix(es) from the ones consis-
tent with the chosen stem. The BAMA lexicons contain affixes
and their legitimate concatenations, so some of the generated
words would permit a linguistic segmentation into multiple pre-
fixes/suffixes. Nonetheless, we take as gold-standard segmenta-
tion precisely the items used by our procedure.
350
sions with stems as shown in the set of rules above,
and we experiment with a variant Tpl3Ch that al-
lows the non-terminal T1 to be rewritten as up to
three Char symbols, since the data indicate there are
cases where multiple characters intervene between
the radicals of a root.
These models exclude rule (10), which we include
only in the variant Tpl+T4. Lastly, TplR4 is the ex-
tension of Tpl+T4 to include a stem-forming rule
that uses R4.
As external baseline model we used Morfessor
(Creutz and Lagus, 2007), which performs decently
in morphological segmentation of a variety of lan-
guages, but only handles concatenation.
6.3 Method
The MCMC samplers converged within a few hun-
dred iterations and we collected 100 posterior sam-
ples after 900 iterations of burn-in. Collected sam-
ples, each of which is a set of parse trees of the input
word types, are used in two ways:
First, by averaging over the samples we can es-
timate the joint probability of a word type w and a
parse tree t under the adaptor grammar, conditional
on the data and the model?s hyperparameters. We
take the most probable parse of each word type and
evaluate the implied segmentation against the gold
standard segmentation. Likewise, we evaluate the
implied lexicon of stems, affixes and roots against
the corresponding reference sets. It should be em-
phasised that using this maximally probable analy-
sis is aimed at simplifying the evaluation set-up; one
could also extract multiple analyses of a word since
the model defines a distribution over them.
The second method abstracts away from individ-
ual word-types and instead averages over the union
of all samples to obtain an estimate of the probabil-
ity of a string s being generated by a certain category
(non-terminal) of the grammar. In this way we can
obtain a lexicon of the morphemes in each category,
ranked by their probability under the model.
6.4 Inducing Morpheme Lexicons
The quality of each induced lexicon is measured
with standard set-based precision and recall with re-
spect to the corresponding gold lexicon. The results
are summarised by balanced F-scores in Table 2.
The main result is that all our models capable of
forming complex stems obtain a marked improve-
ment in F-scores over the baseline concatenative
adaptor grammar, and the margin of improvement
grows along with the expressivity of the complex-
stem models tested. This applies across prefix, stem
and suffix categories and across our datasets, with
the exception of QU
?
, which we elaborate on in ?6.5.
Stem lexicons of Arabic were learnt with rel-
atively constant precision (?70%), but modelling
complex stems broadened the coverage by about
3000 stems over the concatenative model (against a
reference set of 24k stems). On vocalised Arabic,
the improvements for stems are along both dimen-
sions. In contrast, affix lexicons for both BW and
BW
?
are noisy and the models all generate greedily
to obtain near perfect recall but low precision.
On our Hebrew data, which comprises only 5k
words, the gains in lexicon quality from modelling
complex stems tend to be larger than on Arabic. This
is consistent with our intuition that an appropriate,
richer Bayesian prior helps overcome data sparsity.
Extracting a lexicon of roots is rendered challeng-
ing by the unsupervised nature of the model as the
labelling of grammar symbols is ultimately arbitrary.
Our simple approach was to regard a character tuple
parsed under category R3 as a root. This had mixed
success, as demonstrated by the outlier scores in Ta-
ble 2. In the one case where it was obvious that T3
had been been co-opted for the role, we report the
F-score obtained on the union of R3 and T3 strings.
Soft decisions The preceding set-based evaluation
imposes hard decisions about category membership.
But adaptor grammars are probabilistic by definition
and should thus also be evaluated in terms of prob-
abilistic ability. One method is to turn the model
predictions into a binary classifier of strings us-
ing Receiver-Operator-Characteristic (ROC) theory.
We plot the true positive rate versus the false pos-
itive rate for each prediction lexicon L? containing
strings that have probability greater than ? under the
model (for a grammar category of interest). A per-
fect classifier would rank all true positives (e.g. stem
strings) above false positives (e.g. non-stem strings),
corresponding to a curve in the upper left corner of
the ROC plot. A random guesser would trace a di-
agonal line. The area under the curves (AUC) is
the probability that the classifier would discriminate
correctly.
351
Vocalised Arabic (BW
?
) Unvocalised Arabic (BW) Hebrew (HEB)
Pre Stem Suf R3 Pre Stem Suf R3 Pre Stem Suf R3
Concat 15.0 20.2 25.4 - 32.8 44.1 40.3 - 18.7 20.9 29.2 -
Tpl 24.7 39.4 35.2 ?42.4 45.9 54.7 47.9 62.7 35.1 59.6 52.9 34.8
Tpl3Ch 28.4 36.0 36.5 5.2 50.3 55.1 48.5 62.4 38.6 61.5 56.6 7.1
Tpl+T4 29.0 44.8 41.0 3.9 46.2 54.2 47.7 62.3 32.5 59.6 53.0 36.4
TplR4 37.8 60.3 47.0 5.2 53.0 57.7 51.9 62.4 38.0 62.4 55.2 34.7
Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse
of each different dataset under each models. ?42.4 was obtained by taking the union of R3 and T3 items to
match the way the model used them (see ?6.4).
BW
?
BW QU
?
HEB
Morfessor 55.57 40.04 44.34 24.20
Concat 47.36 64.22 19.64 60.05
Tpl 60.42 71.91 22.53 77.26
Tpl3Ch 60.52 72.20 25.72 77.41
Tpl+T4 64.49 71.59 24.81 77.14
TplR4 74.54 73.66 - 78.14
Table 3: Segmentation quality in SBF1. The QU
?
results are for the corresponding M* models .
Our models with complex stem formation im-
prove over the baseline on the AUC metric too. We
include the ROC plots for Hebrew stem and root in-
duction in Figure 2, along with the roots the model
was most confident about (Table 4).
6.5 Morphological Analysis per Word Type
In this section we turn to the analyses our models
assign to each word type. Two aspects of interest are
the segmentation into sequential morphemes and the
identification of the root.
Our intercalating adaptor grammars consistently
obtain large gains in segmentation accuracy over the
baseline concatenative model, across all our datasets
(Table 3). We measure segmentation quality as seg-
ment border F1-score (SBF) (Sirts and Goldwater,
2013), which is the F-score over word-internal seg-
mentation points of the predicted analysis with re-
spect to the gold segmentation.
Of the two MSA datasets, the vocalised version
BW
?
presents a more difficult segmentation task as
its words are on average longer and feature 31k
unique contiguous morphemes, compared to the 24k
in BW for the same number of words. It should thus
benefit more from additional model expressivity, as
is reflected in the increase of 10 SBF when adding
the TplR4 rule to the other triliteral ones.
The best triliteral root identification accuracy (on
a per-word basis) was found for HEB (74%) and BW
(67%).8,9 Refer to Figure 3 for example analyses.
An interesting aspect of these results is that tem-
platic rules may aid segmentation quality without
necessarily giving perfect root identification. Mod-
elling stem substructure allows any regularities that
give rise to a higher data likelihood to be picked up.
The low performance on the Quran demands fur-
ther explanation. All our adaptor grammars severely
oversegmented this data, although the mistakes were
not uniformly distributed. Most of the performance
loss is on the 79% of words that have 1-2 mor-
phemes. On the remaining words (having 3-5 mor-
phemes), our models recover and approach the Mor-
fessor baseline (MConcat: 32.7 , MTpl3Ch: 38.6).
Preliminary experiments on BW had indicated
that adaptation of (single) affix categories is crucial
for good performance. Our multi-affixing models
used on QU
?
lacked a further level of adaptation for
composite affixes, which we suspect as a contribut-
ing factor to the lower performance on that dataset.
This remains to be confirmed in future experiments,
but would be consistent with other observations on
the role of hierarchical adaptation in adaptor gram-
mars (Sirts and Goldwater, 2013). The trend that
intercalated rules improve segmentation (compared
to the concatenative grammar) remains consistent
8When excluding cases where root equals stem, root identi-
fication on BW is 55%. Those cases are still not trivial, since
words without roots also exist.
9By way of comparison, Rodrigues and C?avar (2007)
presented an unsupervised statistics-based root identification
method that obtained precision ranging between 50-75%, the
higher requiring vocalised words.
352
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
False Positive Rate
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Tr
ue
 P
os
iti
ve
 R
at
e
TplR4 (0.84)
Tpl+T4 (0.83)
Concat (0.63)
random (0.5)
(a) Stems
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
False Positive Rate
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Tr
ue
 P
os
iti
ve
 R
at
e
TplR4 (0.77)
Tpl+T4 (0.77)
random (0.5)
(b) Triliteral roots
Figure 2: ROC curves for predicting the stem and root lexicons for the HEB dataset. The area under each
curve (AUC), as computed with the trapezium rule, is given in parentheses.
across datasets, despite the lower absolute perfor-
mance on QU
?
.
The performance of the Morfessor baseline was
quite mixed. Contrary to our expectations, it per-
forms best on the ?harder? BW
?
, worst on the ar-
guably simpler HEB and struggled less than the
adaptor grammars on QU
?
.
One factor here is that it learns according to
a grammar with multiple consecutive affixes and
stems, whereas all our experiments (except on QU
?
)
presupposed single affixes. This biases the evalua-
tion slightly in our favour, but works in Morfessor?s
favour on the QU
?
data which is annotated with mul-
tiple affixes.
7 Related work
The distinctive feature of our morphological model
is that it jointly addresses root identification and
morpheme segmentation, and our results demon-
strate the mutual benefit of this.
In contrast, earlier unsupervised approaches tend
to focus on these tasks in isolation.
In unsupervised Arabic segmentation, the para-
metric Bayesian model of (Lee et al, 2011) achieves
F1-scores in the high eighties by incorporating sen-
tential context and inferred syntactic categories,
both of which our model forgoes, although theirs has
no account of discontiguous root morphemes.
Root Example instances
1. spr X G s?ap?ar?.ti te.s?ap?r? ye.s?ap?r?.u
B sipur.im hixs?tap?xar? t
2. lbs X G l?ab?aS?.t li.l?b?oS? ti.l?b?eS?.i
B le hax l?b?iS? ti tx l?ab?S?.i
3. ptx X G p?at?ax?.ti ti.p?t?ex?.i
B li.p?t?oax? nixp?t?ax?.at
5. !al ? B ya.!al.u m?ax!?al?.a !?ac?l?xan it
Table 4: Top Hebrew roots hypothesised by Tpl+T4.
Numbers indicate position when ranked by model
probability. (G)ood and (B)ad instances from
the corpus are given with morpheme boundaries
marked: true positive (.), false negative ( ) and false
positive (x). Hypothesised root characters are bold-
faced, while accent (?) marks gold root characters.
Previous approaches to Arabic root identifica-
tion that sought to use little supervision typically
constrain the search space of candidate characters
within a word, leveraging pre-existing dictionar-
ies (Darwish, 2002; Boudlal et al, 2009) or rule
constraints (Elghamry, 2005; Rodrigues and C?avar,
2007; Daya et al, 2008).
In contrast to these approaches, our model re-
quires no dictionary, and while our grammar rules
effect some constraints on what could be a root, they
are specified in a convenient and flexible manner that
353
Word
Suf
k m
Stem
s t A r
Pre
w l >
wl >stAr km X
Pre(w l) ... Stem ... Suf(k m)
X2 s t ? r
R3 s ? t ? r
R1
r
R2 s ? t
R1
t
R1
s
T2 > ? A
T1
A
T1
>
(a) Concat & Tpl+T4, ?wl>stArkm? (BW)
Word
Stem
n o t i l l A
Pre
l i d a
li danotillA X
Pre(l i) ... Stem(danotill) ... Suf(A)
T4 o ? a ? i ? l
T1
l
T3 o ? a ? i
T1
i
T2 o ? a
T1
a
T1
o
R4 d ? n ? t ? l
R1
l
R3 d ? n ? t
R1
t
R2 d ? n
R1
n
R1
d
(b) Concat & TplR4, ?lidanotillA? (BW
?
)
Figure 3: Parse trees produced for words in the two standard Arabic datasets that were incorrectly segmented
by the baseline grammar. The templatic grammars correctly identified the triliteral and quadriliteral roots,
also fixing the segmentation of (a). In (b), the templatic grammar improved over the baseline by finding
the correct prefix but falsely posited a suffix. Unimportant subtrees are elided for space, while the yields of
discontiguous constituents are indicated next to their symbols, with dots marking gaps. Crossing branches
are not drawn but should be inferrable. Root characters are bold-faced in the reference analysisX. The non-
terminal X2 in (a) is part of a number of implementation-specific helper rules that ensure the appropriate
handling of partly contiguous roots.
makes experimentation with other phenomena easy.
Recent work by Fullwood and O?Donnell (2013)
goes some way toward jointly dealing with non-
concatenative and concatenative morphology in the
unsupervised setting, but their focus is limited to in-
flected stems and does not handle multiple consecu-
tive affixes. They analyse the Arabic verb stem (e.g.
kataba ?he wrote?) into a templatic bit-string denot-
ing root and non-root characters (e.g. r-r-r-) along
with a root morpheme (e.g. ktb) and a so-called
residue morpheme (e.g. aaa). Their nonparamet-
ric Bayesian model induces lexicons of these en-
tities and achieves very high performance on tem-
plates. The explicit formulation of templates allevi-
ates the labelling ambiguity that hampered our eval-
uation (?6.4), but we believe their method of anal-
ysis can be simulated in our framework using the
appropriate SRCG-rules.
Learning root-templatic morphology is loosely re-
lated to morphological paradigm induction (Clark,
2001; Dreyer and Eisner, 2011; Durrett and DeN-
ero, 2013). Our models do not represent templatic
paradigms explicitly, but it is interesting to note that
preliminary experiments with German indicate that
our adaptor grammars pick up on the past participle
forming circumfix in ab+ge+spiel+t (played back).
8 Conclusion and Outlook
We presented a new approach to modelling non-
concatenative phenomena in morphology using sim-
ple range concatenating grammars and extended
adaptor grammars to this formalism. Our experi-
ments show that this richer model improves morpho-
logical segmentation and morpheme lexicon induc-
tion on different languages in the Semitic family.
Various avenues for future work present them-
selves. Firstly, the lightly-supervised, meta-
grammar approach to adaptor grammars (Sirts and
Goldwater, 2013) can be extended to this more
powerful formalism to lessen the burden of defin-
ing the ?right? grammar rules by hand, and possi-
bly boost performance. Secondly, the discontigu-
ous constituents learnt with our framework can be
used as features in other downstream applications.
Especially in low-resource languages, the ability to
model non-concatenative phenomena (e.g. circum-
fixing, ablaut, etc.) can play an important role in re-
ducing data sparsity for tasks like word alignment
and language modelling. Finally, the PYSRCAG
presents another way of learning SRCGs in general,
which can thus be employed in other applications of
SRCGs, including syntactic parsing and translation.
Acknowledgements
We thank the anonymous reviewers for their valu-
able comments. Our PYSRCAG implementation
leveraged the adaptor grammar code released by
Mark Johnson, whom we thank, along with the in-
dividuals who contributed to the public data sources
that enabled the empirical elements of this paper.
354
References
Aviad Albert, Brian MacWhinney, Bracha Nir, and Shuly
Wintner. 2013. The Hebrew CHILDES corpus: tran-
scription and morphological analysis. Language Re-
sources and Evaluation, pages 1?33.
Mohamed Altantawy, Nizar Habash, Owen Rambow, and
Ibrahim Saleh. 2010. Morphological Analysis and
Generation of Arabic Nouns: A Morphemic Func-
tional Approach. In Proceedings of LREC, pages 851?
858.
Kenneth R Beesley and Lauri Karttunen. 2003. Fi-
nite state morphology, volume 18. CSLI publications
Stanford.
Abderrahim Boudlal, Rachid Belahbib, Abdelhak
Lakhouaja, Azzeddine Mazroui, Abdelouafi Meziane,
and Mohamed Bebah. 2009. A Markovian approach
for Arabic Root Extraction. The International Arab
Journal of Information Technology, 8(1):91?98.
Pierre Boullier. 2000. A cubic time extension of context-
free grammars. Grammars, 3(2-3):111?131.
Tim Buckwalter. 2002. Arabic Morphological Ana-
lyzer. Technical report, Linguistic Data Consortium,
Philedelphia.
Alexander Clark. 2001. Learning Morphology with Pair
Hidden Markov Models. In Proceedings of the ACL
Student Workshop, pages 55?60.
Yael Cohen-Sygal and Shuly Wintner. 2006. Finite-
state registered automata for non-concatenative mor-
phology. Computational Linguistics, 32(1):49?82.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1):1?34.
Kareem Darwish. 2002. Building a shallow Arabic
morphological analyzer in one day. In Proceedings
of the ACL Workshop on Computational Approaches
to Semitic Languages, pages 47?54. Association for
Computational Linguistics.
Ezra Daya, Dan Roth, and Shuly Wintner. 2008.
Identifying Semitic Roots: Machine Learning with
Linguistic Constraints. Computational Linguistics,
34(3):429?448.
Markus Dreyer and Jason Eisner. 2011. Discovering
Morphological Paradigms from Plain Text Using a
Dirichlet Process Mixture Model. In Proceedings of
EMNLP, pages 616?627, Edinburgh, Scotland.
Kais Dukes and Nizar Habash. 2010. Morphological An-
notation of Quranic Arabic. In Proceedings of LREC.
Greg Durrett and John DeNero. 2013. Supervised Learn-
ing of Complete Morphological Paradigms. In Pro-
ceedings of NAACL-HLT, pages 1185?1195, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Khaled Elghamry. 2005. A Constraint-based Algorithm
for the Identification of Arabic Roots. In Proceed-
ings of the Midwest Computational Linguistics Collo-
quium. Indiana University. Bloomington, IN.
Raphael Finkel and Gregory Stump. 2002. Generating
Hebrew verb morphology by default inheritance hier-
archies. In Proceedings of the ACL Workshop on Com-
putational Approaches to Semitic Languages. Associ-
ation for Computational Linguistics.
Michelle A. Fullwood and Timothy J. O?Donnell. 2013.
Learning non-concatenative morphology. In Proceed-
ings of the Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 21?27, Sofia, Bulgaria.
Association for Computational Linguistics.
Michael Gasser. 2009. Semitic morphological analysis
and generation using finite state transducers with fea-
ture structures. In Proceedings of EACL, pages 309?
317. Association for Computational Linguistics.
Daniel Gildea. 2010. Optimal Parsing Strategies for Lin-
ear Context-Free Rewriting Systems. In Proceedings
of NAACL, pages 769?776. Association for Computa-
tional Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating Between Types and Tokens
by Estimating Power-Law Generators. In Advances in
Neural Information Processing Systems, Volume 18.
Harald Hammarstro?m and Lars Borin. 2011. Unsuper-
vised Learning of Morphology. Computational Lin-
guistics, 37(2):309?350.
Yun Huang, Min Zhang, and Chew Lim Tan. 2011.
Nonparametric Bayesian Machine Transliteration with
Synchronous Adaptor Grammars. In Proceedings of
ACL (Short papers), pages 534?539.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: Experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL-HLT, pages 317?325.
Association for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A Framework for
Specifying Compositional Nonparametric Bayesian
Models. In Advances in Neural Information Process-
ing Systems, volume 19, page 641. MIT.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using Adaptor Grammars. In Proceedings
of ACL Special Interest Group on Computational Mor-
phology and Phonology (SigMorPhon), pages 20?27.
Association for Computational Linguistics.
Aravind K. Joshi. 1985. Tree adjoining grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In D.R. Dowty, L. Kart-
tunen, and A.M. Zwicky, editors, Natural Language
Parsing, chapter 6, pages 206?250. Cambridge Uni-
versity Press.
355
Miriam Kaeshammer. 2013. Synchronous Linear
Context-Free Rewriting Systems for Machine Trans-
lation. In Proceedings of the Workshop on Syntax, Se-
mantics and Structure in Statistical Translation, pages
68?77, Atlanta, Georgia. Association for Computa-
tional Linguistics.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic Multiple Context-Free Grammar for RNA
Pseudoknot Modeling. In Proceedings of the Inter-
national Workshop on Tree Adjoining Grammar and
Related Formalisms, pages 57?64.
George Anton Kiraz. 2000. Multitiered Nonlinear Mor-
phology Using Multitape Finite Automata: A Case
Study on Syriac and Arabic. Computational Linguis-
tics, 26(1):77?105, March.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
Proceedings of the 10th international conference on
Computational Linguistics, pages 178?181. Associa-
tion for Computational Linguistics.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2010.
Overview and Results of Morpho Challenge 2009. In
Multilingual Information Access Evaluation I. Text Re-
trieval Experiments, volume 6241 of Lecture Notes in
Computer Science, pages 578?597. Springer Berlin /
Heidelberg.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves morpho-
logical segmentation. In Proceedings of CoNLL.
Wolfgang Maier. 2010. Direct Parsing of Discon-
tinuous Constituents in German. In Proceedings of
the NAACL-HLT Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 58?66. Asso-
ciation for Computational Linguistics.
Jim Pitman and Marc Yor. 1997. The Two-Parameter
Poisson-Dirichlet Distribution Derived from a Stable
Subordinator. The Annals of Probability, 25(2):855?
900.
Jim Pitman. 1995. Exchangeable and partially exchange-
able random partitions. Probability Theory and Re-
lated Fields, 102:145?158.
Jean-Franc?ois Prunet. 2006. External Evidence and the
Semitic Root. Morphology, 16(1):41?67.
Paul Rodrigues and Damir C?avar. 2007. Learning Arabic
Morphology Using Statistical Constraint-Satisfaction
Models. In Elabbas Benmamoun, editor, Perspectives
on Arabic Linguistics: Proceedings of the 19th Ara-
bic Linguistics Symposium, pages 63?75, Urbana, IL,
USA. John Benjamins Publishing Company.
Nathan Schneider. 2010. Computational Cognitive
Morphosemantics: Modeling Morphological Compo-
sitionality in Hebrew Verbs with Embodied Construc-
tion Grammar. In Proceedings of the Annual Meeting
of the Berkeley Linguistics Society, Berkeley, CA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2):191?229.
Kairit Sirts and Sharon Goldwater. 2013. Minimally-
Supervised Morphological Segmentation using Adap-
tor Grammars. Transactions of the ACL.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In Proceedings of
ACL, pages 104?111.
356
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700?1709,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recurrent Continuous Translation Models
Nal Kalchbrenner Phil Blunsom
Department of Computer Science
University of Oxford
{nal.kalchbrenner,phil.blunsom}@cs.ox.ac.uk
Abstract
We introduce a class of probabilistic con-
tinuous translation models called Recur-
rent Continuous Translation Models that are
purely based on continuous representations
for words, phrases and sentences and do not
rely on alignments or phrasal translation units.
The models have a generation and a condi-
tioning aspect. The generation of the transla-
tion is modelled with a target Recurrent Lan-
guage Model, whereas the conditioning on the
source sentence is modelled with a Convolu-
tional Sentence Model. Through various ex-
periments, we show first that our models ob-
tain a perplexity with respect to gold transla-
tions that is > 43% lower than that of state-
of-the-art alignment-based translation models.
Secondly, we show that they are remarkably
sensitive to the word order, syntax, and mean-
ing of the source sentence despite lacking
alignments. Finally we show that they match a
state-of-the-art system when rescoring n-best
lists of translations.
1 Introduction
In most statistical approaches to machine transla-
tion the basic units of translation are phrases that are
composed of one or more words. A crucial com-
ponent of translation systems are models that esti-
mate translation probabilities for pairs of phrases,
one phrase being from the source language and the
other from the target language. Such models count
phrase pairs and their occurrences as distinct if the
surface forms of the phrases are distinct. Although
distinct phrase pairs often share significant similari-
ties, linguistic or otherwise, they do not share statis-
tical weight in the models? estimation of their trans-
lation probabilities. Besides ignoring the similar-
ity of phrase pairs, this leads to general sparsity is-
sues. The estimation is sparse or skewed for the
large number of rare or unseen phrase pairs, which
grows exponentially in the length of the phrases, and
the generalisation to other domains is often limited.
Continuous representations have shown promise
at tackling these issues. Continuous representations
for words are able to capture their morphological,
syntactic and semantic similarity (Collobert and We-
ston, 2008). They have been applied in continu-
ous language models demonstrating the ability to
overcome sparsity issues and to achieve state-of-the-
art performance (Bengio et al, 2003; Mikolov et
al., 2010). Word representations have also shown
a marked sensitivity to conditioning information
(Mikolov and Zweig, 2012). Continuous repre-
sentations for characters have been deployed in
character-level language models demonstrating no-
table language generation capabilities (Sutskever et
al., 2011). Continuous representations have also
been constructed for phrases and sentences. The rep-
resentations are able to carry similarity and task de-
pendent information, e.g. sentiment, paraphrase or
dialogue labels, significantly beyond the word level
and to accurately predict labels for a highly diverse
range of unseen phrases and sentences (Grefenstette
et al, 2011; Socher et al, 2011; Socher et al, 2012;
Hermann and Blunsom, 2013; Kalchbrenner and
Blunsom, 2013).
Phrase-based continuous translation models were
first proposed in (Schwenk et al, 2006) and re-
1700
cently further developed in (Schwenk, 2012; Le et
al., 2012). The models incorporate a principled way
of estimating translation probabilities that robustly
extends to rare and unseen phrases. They achieve
significant Bleu score improvements and yield se-
mantically more suggestive translations. Although
wide-reaching in their scope, these models are lim-
ited to fixed-size source and target phrases and sim-
plify the dependencies between the target words tak-
ing into account restricted target language modelling
information.
We describe a class of continuous translation
models called Recurrent Continuous Translation
Models (RCTM) that map without loss of generality
a sentence from the source language to a probabil-
ity distribution over the sentences in the target lan-
guage. We define two specific RCTM architectures.
Both models adopt a recurrent language model for
the generation of the target translation (Mikolov et
al., 2010). In contrast to other n-gram approaches,
the recurrent language model makes no Markov as-
sumptions about the dependencies of the words in
the target sentence.
The two RCTMs differ in the way they condi-
tion the target language model on the source sen-
tence. The first RCTM uses the convolutional sen-
tence model (Kalchbrenner and Blunsom, 2013) to
transform the source word representations into a rep-
resentation for the source sentence. The source sen-
tence representation in turn constraints the genera-
tion of each target word. The second RCTM intro-
duces an intermediate representation. It uses a trun-
cated variant of the convolutional sentence model to
first transform the source word representations into
representations for the target words; the latter then
constrain the generation of the target sentence. In
both cases, the convolutional layers are used to gen-
erate combined representations for the phrases in a
sentence from the representations of the words in the
sentence.
An advantage of RCTMs is the lack of latent
alignment segmentations and the sparsity associated
with them. Connections between source and target
words, phrases and sentences are learnt only implic-
itly as mappings between their continuous represen-
tations. As we see in Sect. 5, these mappings of-
ten carry remarkably precise morphological, syntac-
tic and semantic information. Another advantage is
that the probability of a translation under the models
is efficiently computable requiring a small number
of matrix-vector products that is linear in the length
of the source and the target sentence. Further, trans-
lations can be generated directly from the probabil-
ity distribution of the RCTM without any external
resources.
We evaluate the performance of the models in four
experiments. Since the translation probabilities of
the RCTMs are tractable, we can measure the per-
plexity of the models with respect to the reference
translations. The perplexity of the models is signifi-
cantly lower than that of IBM Model 1 and is> 43%
lower than the perplexity of a state-of-the-art variant
of the IBM Model 2 (Brown et al, 1993; Dyer et
al., 2013). The second and third experiments aim to
show the sensitivity of the output of the RCTM II
to the linguistic information in the source sentence.
The second experiment shows that under a random
permutation of the words in the source sentences,
the perplexity of the model with respect to the refer-
ence translations becomes significantly worse, sug-
gesting that the model is highly sensitive to word
position and order. The third experiment inspects
the translations generated by the RCTM II. The
generated translations demonstrate remarkable mor-
phological, syntactic and semantic agreement with
the source sentence. Finally, we test the RCTMs
on the task of rescoring n-best lists of translations.
The performance of the RCTM probabilities joined
with a single word penalty feature matches the per-
formance of the state-of-the-art translation system
cdec that makes use of twelve features including
five alignment-based translation models (Dyer et al,
2010).
We proceed as follows. We begin in Sect. 2 by
describing the general modelling framework under-
lying the RCTMs. In Sect. 3 we describe the RCTM
I and in Sect. 4 the RCTM II. Section 5 is dedicated
to the four experiments and we conclude in Sect. 6.1
2 Framework
We begin by describing the modelling framework
underlying RCTMs. An RCTM estimates the proba-
bility P (f|e) of a target sentence f = f1, ..., fm being
a translation of a source sentence e = e1, ..., ek. Let
1Code and models available at nal.co
1701
us denote by fi:j the substring of words fi, ..., fj . Us-
ing the following identity,
P (f|e) =
m?
i=1
P (fi|f1:i?1, e) (1)
an RCTM estimates P (f|e) by directly computing
for each target position i the conditional probability
P (fi|f1:i?1, e) of the target word fi occurring in the
translation at position i, given the preceding target
words f1:i?1 and the source sentence e. We see that
an RCTM is sensitive not just to the source sentence
e but also to the preceding words f1:i?1 in the target
sentence; by doing so it incorporates a model of the
target language itself.
To model the conditional probability P (f|e), an
RCTM comprises both a generative architecture for
the target sentence and an architecture for condition-
ing the latter on the source sentence. To fully cap-
ture Eq. 1, we model the generative architecture with
a recurrent language model (RLM) based on a re-
current neural network (Mikolov et al, 2010). The
prediction of the i-th word fi in a RLM depends on
all the preceding words f1:i?1 in the target sentence
ensuring that conditional independence assumptions
are not introduced in Eq. 1. Although the predic-
tion is most strongly influenced by words closely
preceding fi, long-range dependencies from across
the whole sentence can also be exhibited. The con-
ditioning architectures are model specific and are
treated in Sect. 3-4. Both the generative and con-
ditioning aspects of the models deploy continuous
representations for the constituents and are trained
as a single joint architecture. Given the modelling
framework underlying RCTMs, we now proceed to
describe in detail the recurrent language model un-
derlying the generative aspect.
2.1 Recurrent Language Model
A RLM models the probability P (f) that the se-
quence of words f occurs in a given language. Let
f = f1, ..., fm be a sequence of m words, e.g. a sen-
tence in the target language. Analogously to Eq. 1,
using the identity,
P (f) =
m?
i=1
P (fi|f1:i?1) (2)
the model explicitly computes without simpli-
fying assumptions the conditional distributions
R
I O
f
i
P(f   )
i+1
h
R
f
i-1
P(f )
i
f
i+1
P(f   )
i+2
OI
h h h
i-1 i i+1
Figure 1: A RLM (left) and its unravelling to depth 3
(right). The recurrent transformation is applied to the hid-
den layer hi?1 and the result is summed to the represen-
tation for the current word fi. After a non-linear transfor-
mation, a probability distribution over the next word fi+1
is predicted.
P (fi|f1:i?1). The architecture of a RLM comprises
a vocabulary V that contains the words fi of the
language as well as three transformations: an in-
put vocabulary transformation I ? Rq?|V |, a re-
current transformation R ? Rq?q and an output
vocabulary transformation O ? R|V |?q. For each
word fk ? V , we indicate by i(fk) its index in V
and by v(fk) ? R|V |?1 an all zero vector with only
v(fk)i(fk) = 1.
For a word fi, the result of I ? v(fi) ? Rq?1 is
the input continuous representation of fi. The pa-
rameter q governs the size of the word representa-
tion. The prediction proceeds by successively ap-
plying the recurrent transformation R to the word
representations and predicting the next word at each
step. In detail, the computation of each P (fi|f1:i?1)
proceeds recursively. For 1 < i < m,
h1 = ?(I ? v(f1)) (3a)
hi+1 = ?(R ? hi + I ? v(fi+1)) (3b)
oi+1 = O ? hi (3c)
and the conditional distribution is given by,
P (fi = v|f1:i?1) =
exp (oi,v)
?V
v=1 exp(oi,v)
(4)
In Eq. 3, ? is a nonlinear function such as tanh. Bias
values bh and bo are included in the computation. An
illustration of the RLM is given in Fig. 1.
The RLM is trained by backpropagation through
time (Mikolov et al, 2010). The error in the pre-
dicted distribution calculated at the output layer is
1702
backpropagated through the recurrent layers and cu-
mulatively added to the errors of the previous predic-
tions for a given number d of steps. The procedure is
equivalent to standard backpropagation over a RLM
that is unravelled to depth d as in Fig. 1.
RCTMs may be thought of as RLMs, in which
the predicted distributions for each word fi are con-
ditioned on the source sentence e. We next define
two conditioning architectures each giving rise to a
specific RCTM.
3 Recurrent Continuous Translation
Model I
The RCTM I uses a convolutional sentence model
(CSM) in the conditioning architecture. The CSM
creates a representation for a sentence that is pro-
gressively built up from representations of the n-
grams in the sentence. The CSM embodies a hierar-
chical structure. Although it does not make use of an
explicit parse tree, the operations that generate the
representations act locally on small n-grams in the
lower layers of the model and act increasingly more
globally on the whole sentence in the upper layers
of the model. The lack of the need for a parse tree
yields two central advantages over sentence models
that require it (Grefenstette et al, 2011; Socher et
al., 2012). First, it makes the model robustly appli-
cable to a large number of languages for which accu-
rate parsers are not available. Secondly, the transla-
tion probability distribution over the target sentences
does not depend on the chosen parse tree.
The RCTM I conditions the probability of each
target word fi on the continuous representation of the
source sentence e generated through the CSM. This
is accomplished by adding the sentence representa-
tion to each hidden layer hi in the target recurrent
language model. We next describe the procedure in
more detail, starting with the CSM itself.
3.1 Convolutional Sentence Model
The CSM models the continuous representation of
a sentence based on the continuous representations
of the words in the sentence. Let e = e1...ek be
a sentence in a language and let v(ei) ? Rq?1 be
the continuous representation of the word ei. Let
Ee ? Rq?k be the sentence matrix for e defined by,
Ee:,i = v(ei) (5)
(K    M)
*
:,1
M M M
:,1
:,2
:,3
the cat
sat
on the
mat
e
E 
e
K 
2
K 
3
L  
3
K 
i
   :,1
K 
i
   :,2
K 
i
   :,3
i
Figure 2: A CSM for a six word source sentence e and the
computed sentence representation e. K2,K3 are weight
matrices and L3 is a top weight matrix. To the right, an
instance of a one-dimensional convolution between some
weight matrix Ki and a generic matrix M that could for
instance correspond to Ee2. The color coding of weights
indicates weight sharing.
The main component of the architecture of the CSM
is a sequence of weight matrices (Ki)2?i?r that cor-
respond to the kernels or filters of the convolution
and can be thought of as learnt feature detectors.
From the sentence matrix Ee the CSM computes a
continuous vector representation e ? Rq?1 for the
sentence e by applying a sequence of convolutions
to Ee whose weights are given by the weight matri-
ces. The weight matrices and the sequence of con-
volutions are defined next.
We denote by (Ki)2?i?r a sequence of weight
matrices where each Ki ? Rq?i is a matrix of i
columns and r = d
?
2Ne, where N is the length of
the longest source sentence in the training set. Each
row of Ki is a vector of i weights that is treated as
the kernel or filter of a one-dimensional convolution.
Given for instance a matrix M ? Rq?j where the
number of columns j ? i, each row of Ki can be
convolved with the corresponding row in M, result-
ing in a matrix Ki ?M, where ? indicates the con-
volution operation and (Ki ?M) ? Rq?(j?i+1). For
i = 3, the value (Ki ?M):,a is computed by:
Ki:,1M:,a+K
i
:,2M:,a+1 +K
i
:,3M:,a+2 (6)
where  is component-wise vector product. Ap-
plying the convolution kernel Ki yields a matrix
(Ki?M) that has i?1 columns less than the original
matrix M.
Given a source sentence of length k, the CSM
convolves successively with the sentence matrix Ee
1703
the sequence of weight matrices (Ki)2?i?r, one af-
ter the other starting with K2 as follows:
Ee1 = E
e (7a)
Eei+1 = ?(K
i+1 ?Eei ) (7b)
After a few convolution operations, Eei is either a
vector in Rq?1, in which case we obtained the de-
sired representation, or the number of columns in
Eei is smaller than the number i + 1 of columns in
the next weight matrix Ki+1. In the latter case, we
equally obtain a vector in Rq?1 by simply apply-
ing a top weight matrix Lj that has the same num-
ber of columns as Eei . We thus obtain a sentence
representation e ? Rq?1 for the source sentence e.
Note that the convolution operations in Eq. 7b are
interleaved with non-linear functions ?. Note also
that, given the different levels at which the weight
matrices Ki and Li are applied, the top weight
matrix Lj comes from an additional sequence of
weight matrices (Li)2?i?r distinct from (Ki)2?i?r.
Fig. 2 depicts an instance of the CSM and of a one-
dimensional convolution.2
3.2 RCTM I
As defined in Sect. 2, the RCTM I models the condi-
tional probability P (f|e) of a sentence f = f1, ..., fm
in a target language F being the translation of a sen-
tence e = e1, ..., ek in a source language E. Accord-
ing to Eq. 1, the RCTM I explicitly computes the
conditional distributions P (fi|f1:i?1, e). The archi-
tecture of the RCTM I comprises a source vocabu-
lary V E and a target vocabulary V F, two sequences
of weight matrices (Ki)2?i?r and (Li)2?i?r that
are part of the constituent CSM, transformations
I ? Rq?|V
F|, R ? Rq?q and O ? R|V
F|?q that are
part of the constituent RLM and a sentence transfor-
mation S ? Rq?q. We write e = csm(e) for the
output of the CSM with e as the input sentence.
The computation of the RCTM I is a simple mod-
ification to the computation of the RLM described in
Eq. 3. It proceeds recursively as follows:
s = S ? csm(e) (8a)
h1 = ?(I ? v(f1) + s) (8b)
hi+1 = ?(R ? hi + I ? v(fi+1) + s) (8c)
oi+1 = O ? hi (8d)
2For a formal treatment of the construction, see (Kalchbren-
ner and Blunsom, 2013).
and the conditional distributions P (fi+1|f1:i, e) are
obtained from oi as in Eq. 4. ? is a nonlinear func-
tion and bias values are included throughout the
computation. Fig. 3 illustrates an RCTM I.
Two aspects of the RCTM I are to be remarked.
First, the length of the target sentence is predicted
by the target RLM itself that by its architecture has
a bias towards shorter sentences. Secondly, the rep-
resentation of the source sentence e constraints uni-
formly all the target words, contrary to the fact that
the target words depend more strongly on certain
parts of the source sentence and less on other parts.
The next model proposes an alternative formulation
of these aspects.
4 Recurrent Continuous Translation
Model II
The central idea behind the RCTM II is to first es-
timate the length m of the target sentence indepen-
dently of the main architecture. Given m and the
source sentence e, the model constructs a represen-
tation for the n-grams in e, where n is set to 4. Note
that each level of the CSM yields n-gram represen-
tations of e for a specific value of n. The 4-gram
representation of e is thus constructed by truncat-
ing the CSM at the level that corresponds to n = 4.
The procedure is then inverted. From the 4-gram
representation of the source sentence e, the model
builds a representation of a sentence that has the
predicted length m of the target. This is similarly
accomplished by truncating the inverted CSM for a
sentence of length m.
We next describe in detail the Convolutional n-
gram Model (CGM). Then we return to specify the
RCTM II.
4.1 Convolutional n-gram model
The CGM is obtained by truncating the CSM at the
level where n-grams are represented for the chosen
value of n. A column g of a matrix Eei obtained
according to Eq. 7 represents an n-gram from the
source sentence e. The value of n corresponds to
the number of word vectors from which the n-gram
representation g is constructed; equivalently, n is
the span of the weights in the CSM underneath g
(see Fig. 2-3). Note that any column in a matrix
Eei represents an n-gram with the same span value
n. We denote by gram(Eei ) the size of the n-grams
1704
RCTM IIRCTM I
P( f | e ) 
P( f | m, e )
e 
e 
e 
F 
T 
S 
S 
csm 
cgm 
icgm 
E 
F 
g 
g 
Figure 3: A graphical depiction of the two RCTMs. Arrows represent full matrix transformations while lines are
vector transformations corresponding to columns of weight matrices.
represented by Eei . For example, for a sufficiently
long sentence e, gram(Ee2) = 2, gram(E
e
3) = 4,
gram(Ee4) = 7. We denote by cgm(e, n) that matrix
Eei from the CSM that represents the n-grams of the
source sentence e.
The CGM can also be inverted to obtain a repre-
sentation for a sentence from the representation of
its n-grams. We denote by icgm the inverse CGM,
which depends on the size of the n-gram represen-
tation cgm(e, n) and on the target sentence length
m. The transformation icgm unfolds the n-gram
representation onto a representation of a target sen-
tence with m words. The architecture corresponds
to an inverted CGM or, equivalently, to an inverted
truncated CSM (Fig. 3). Given the transformations
cgm and icgm, we now detail the computation of the
RCTM II.
4.2 RCTM II
The RCTM II models the conditional probability
P (f|e) by factoring it as follows:
P (f|e) = P (f|m, e) ? P (m|e) (9a)
=
m?
i=1
P (fi+1|f1:i,m, e) ? P (m|e) (9b)
and computing the distributions P (fi+1|f1:i,m, e)
and P (m|e). The architecture of the RCTM II
comprises all the elements of the RCTM I together
with the following additional elements: a translation
transformation Tq?q and two sequences of weight
matrices (Ji)2?i?s and (Hi)2?i?s that are part of
the icgm3.
The computation of the RCTM II proceeds recur-
sively as follows:
Eg = cgm(e, 4) (10a)
Fg:,j = ?(T ?E
g
:,j) (10b)
F = icgm(Fg,m) (10c)
h1 = ?(I ? v(f1) + S ? F:,1) (10d)
hi+1 = ?(R ? hi + I ? v(fi+1) + S ? F:,i+1) (10e)
oi+1 = O ? hi (10f)
and the conditional distributions P (fi+1|f1:i, e) are
obtained from oi as in Eq. 4. Note how each re-
constructed vector F:,i is added successively to the
corresponding layer hi that predicts the target word
fi. The RCTM II is illustrated in Fig. 3.
3Just like r the value s is small and depends on the length
of the source and target sentences in the training set. See
Sect. 5.1.2.
1705
For the separate estimation of the length of the
translation, we estimate the conditional probability
P (m|e) by letting,
P (m|e) = P (m|k) = Poisson(?k) (11)
where k is the length of the source sentence e and
Poisson(?) is a Poisson distribution with mean ?.
This concludes the description of the RCTM II. We
now turn to the experiments.
5 Experiments
We report on four experiments. The first experiment
considers the perplexities of the models with respect
to reference translations. The second and third ex-
periments test the sensitivity of the RCTM II to the
linguistic aspects of the source sentences. The fi-
nal experiment tests the rescoring performance of
the two models.
5.1 Training
Before turning to the experiments, we describe the
data sets, hyper parameters and optimisation algo-
rithms used for the training of the RCTMs.
5.1.1 Data sets
The training set used for all the experiments com-
prises a bilingual corpus of 144953 pairs of sen-
tences less than 80 words in length from the news
commentary section of the Eighth Workshop on Ma-
chine Translation (WMT) 2013 training data. The
source language is English and the target language
is French. The English sentences contain about
4.1M words and the French ones about 4.5M words.
Words in both the English and French sentences
that occur twice or less are substituted with the
?unknown? token. The resulting vocabularies V E
and V F contain, respectively, 25403 English words
and 34831 French words.
For the experiments we use four different test sets
comprised of the Workshop on Machine Transla-
tion News Test (WMT-NT) sets for the years 2009,
2010, 2011 and 2012. They contain, respectively,
2525, 2489, 3003 and 3003 pairs of English-French
sentences. For the perplexity experiments unknown
words occurring in these data sets are replaced with
the ?unknown? token. The respective 2008 WMT-
NT set containing 2051 pairs of English-French sen-
tences is used as the validation set throughout.
5.1.2 Model hyperparameters
The parameter q that defines the size of the En-
glish vectors v(ei) for ei ? V E, the size of the hid-
den layer hi and the size of the French vectors v(fi)
for v(fi) ? V F is set to q = 256. This yields a
relatively small recurrent matrix and corresponding
models. To speed up training, we factorize the target
vocabulary V F into 256 classes following the proce-
dure in (Mikolov et al, 2011).
The RCTM II uses a convolutional n-gram model
CGM where n is set to 4. For the RCTM I, the num-
ber of weight matrices r for the CSM is 15, whereas
in the RCTM II the number r of weight matrices for
the CGM is 7 and the number s of weight matrices
for the inverse CGM is 9. If a test sentence is longer
than all training sentences and a larger weight matrix
is required by the model, the larger weight matrix is
easily factorized into two smaller weight matrices
whose weights have been trained. For instance, if a
weight matrix of 10 weights is required, but weight
matrices have been trained only up to weight 9, then
one can factorize the matrix of 10 weights with one
of 9 and one of 2. Across all test sets the proportion
of sentence pairs that require larger weight matrices
to be factorized into smaller ones is < 0.1%.
5.1.3 Objective and optimisation
The objective function is the average of the sum
of the cross-entropy errors of the predicted words
and the true words in the French sentences. The En-
glish sentences are taken as input in the prediction
of the French sentences, but they are not themselves
ever predicted. An l2 regularisation term is added to
the objective. The training of the model proceeds by
back-propagation through time. The cross-entropy
error calculated at the output layer at each step is
back-propagated through the recurrent structure for
a number d of steps; for all models we let d = 6.
The error accumulated at the hidden layers is then
further back-propagated through the transformation
S and the CSM/CGM to the input vectors v(ei) of
the English input sentence e. All weights, includ-
ing the English vectors, are randomly initialised and
inferred during training.
The objective is minimised using mini-batch
adaptive gradient descent (Adagrad) (Duchi et al,
2011). The training of an RCTM takes about 15
hours on 3 multicore CPUs. While our experiments
1706
WMT-NT 2009 2010 2011 2012
KN-5 218 213 222 225
RLM 178 169 178 181
IBM 1 207 200 188 197
FA-IBM 2 153 146 135 144
RCTM I 143 134 140 142
RCTM II 86 77 76 77
Table 1: Perplexity results on the WMT-NT sets.
are relatively small, we note that in principle our
models should scale similarly to RLMs which have
been applied to hundreds of millions of words.
5.2 Perplexity of gold translations
Since the computation of the probability of a trans-
lation under one of the RCTMs is efficient, we can
compute the perplexities of the RCTMs with respect
to the reference translations in the test sets. The per-
plexity measure is an indication of the quality that
a model assigns to a translation. We compare the
perplexities of the RCTMs with the perplexity of the
IBM Model 1 (Brown et al, 1993) and of the Fast-
Aligner (FA-IBM 2) model that is a state-of-the-art
variant of IBM Model 2 (Dyer et al, 2013). We add
as baselines the unconditional target RLM and a 5-
gram target language model with modified Kneser-
Nay smoothing (KN-5). The results are reported in
Tab. 1. The RCTM II obtains a perplexity that is
> 43% lower than that of the alignment based mod-
els and that is 40% lower than the perplexity of the
RCTM I. The low perplexity of the RCTMs suggests
that continuous representations and the transforma-
tions between them make up well for the lack of ex-
plicit alignments. Further, the difference in perplex-
ity between the RCTMs themselves demonstrates
the importance of the conditioning architecture and
suggests that the localised 4-gram conditioning in
the RCTM II is superior to the conditioning with the
whole source sentence of the RCTM I.
5.3 Sensitivity to source sentence structure
The second experiment aims at showing the sensi-
tivity of the RCTM II to the order and position of
words in the English source sentence. To this end,
we randomly permute in the training and testing sets
WMT-NT PERM 2009 2010 2011 2012
RCTM II 174 168 175 178
Table 2: Perplexity results of the RCTM II on the WMT-
NT sets where the words in the English source sentences
are randomly permuted.
the words in the English source sentence. The re-
sults on the permuted data are reported in Tab. 2. If
the RCTM II were roughly comparable to a bag-of-
words approach, there would be no difference under
the permutation of the words. By contrast, the dif-
ference of the results reported in Tab. 2 with those
reported in Tab. 1 is very significant, clearly indicat-
ing the sensitivity to word order and position of the
translation model.
5.3.1 Generating from the RCTM II
To show that the RCTM II is sensitive not only to
word order, but also to other syntactic and semantic
traits of the sentence, we generate and inspect can-
didate translations for various English source sen-
tences. The generation proceeds by sampling from
the probability distribution of the RCTM II itself and
does not depend on any other external resources.
Given an English source sentence e, we let m be
the length of the gold translation and we search the
distribution computed by the RCTM II over all sen-
tences of length m. The number of possible target
sentences of length m amounts to |V |m = 34831m
where V = V F is the French vocabulary; directly
considering all possible translations is intractable.
We proceed as follows: we sample with replace-
ment 2000 sentences from the distribution of the
RCTM II, each obtained by predicting one word at
a time. We start by predicting a distribution for the
first target word, restricting that distribution to the
top 5 most probable words and sampling the first
word of a candidate translation from the restricted
distribution of 5 words. We proceed similarly for
the remaining words. Each sampled sentence has a
well-defined probability assigned by the model and
can thus be ranked. Table 3 gives various English
source sentences and some candidate French trans-
lations generated by the RCTM II together with their
ranks.
The results in Tab. 3 show the remarkable syn-
tactic agreements of the candidate translations; the
1707
English source sentence French gold translation RCTM II candidate translation Rank
the patient is sick . le patient est malade . le patient est insuffisante . 1
le patient est mort . 4
la patient est insuffisante . 23
the patient is dead . le patient est mort . le patient est mort . 1
le patient est de?passe? . 4
the patient is ill . le patient est malade . le patient est mal . 3
the patients are sick . les patients sont malades . les patients sont confronte?s . 2
les patients sont corrompus . 5
the patients are dead . les patients sont morts . les patients sont morts . 1
the patients are ill . les patients sont malades . les patients sont confronte?s . 5
the patient was ill . le patient e?tait malade . le patient e?tait mal . 2
the patients are not dead . les patients ne sont pas morts . les patients ne sont pas morts . 1
the patients are not sick . les patients ne sont pas malades . les patients ne sont pas ?unknown? . 1
les patients ne sont pas mal . 6
the patients were saved . les patients ont e?te? sauve?s . les patients ont e?te? sauve?es . 6
Table 3: English source sentences, respective translations in French and candidate translations generated from the
RCTM II and ranked out of 2000 samples according to their decreasing probability. Note that end of sentence dots (.)
are generated as part of the translation.
WMT-NT 2009 2010 2011 2012
RCTM I + WP 19.7 21.1 22.5 21.5
RCTM II + WP 19.8 21.1 22.5 21.7
cdec (12 features) 19.9 21.2 22.6 21.8
Table 4: Bleu scores on the WMT-NT sets of each RCTM
linearly interpolated with a word penalty WP. The cdec
system includes WP as well as five translation models and
two language modelling features, among others.
large majority of the candidate translations are fully
well-formed French sentences. Further, subtle syn-
tactic features such as the singular or plural ending
of nouns and the present and past tense of verbs are
well correlated between the English source and the
French candidate targets. Finally, the meaning of
the English source is well transferred on the French
candidate targets; where a correlation is unlikely or
the target word is not in the French vocabulary, a se-
mantically related word or synonym is selected by
the model. All of these traits suggest that the RCTM
II is able to capture a significant amount of both
syntactic and semantic information from the English
source sentence and successfully transfer it onto the
French translation.
5.4 Rescoring and BLEU Evaluation
The fourth experiment tests the ability of the RCTM
I and the RCTM II to choose the best translation
among a large number of candidate translations pro-
duced by another system. We use the cdec sys-
tem to generate a list of 1000 best candidate trans-
lations for each English sentence in the four WMT-
NT sets. We compare the rescoring performance of
the RCTM I and the RCTM II with that of the cdec
itself. cdec employs 12 engineered features includ-
ing, among others, 5 translation models, 2 language
model features and a word penalty feature (WP). For
the RCTMs we simply interpolate the log probabil-
ity assigned by the models to the candidate transla-
tions with the word penalty feature WP, tuned on the
validation data. The results of the experiment are
reported in Tab. 4.
While there is little variance in the resulting Bleu
scores, the performance of the RCTMs shows that
their probabilities correlate with translation qual-
ity. Combining a monolingual RLM feature with
the RCTMs does not improve the scores, while re-
ducing cdec to just one core translation probability
and language model features drops its score by two
to five tenths. These results indicate that the RCTMs
have been able to learn both translation and language
modelling distributions.
1708
6 Conclusion
We have introduced Recurrent Continuous Transla-
tion Models that comprise a class of purely contin-
uous sentence-level translation models. We have
shown the translation capabilities of these models
and the low perplexities that they obtain with respect
to reference translations. We have shown the ability
of these models at capturing syntactic and semantic
information and at estimating during reranking the
quality of candidate translations.
The RCTMs offer great modelling flexibility due
to the sensitivity of the continuous representations to
conditioning information. The models also suggest
a wide range of potential advantages and extensions,
from being able to include discourse representations
beyond the single sentence and multilingual source
representations, to being able to model morpholog-
ically rich languages through character-level recur-
rences.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning, ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of ibm model 2. In Proc. of NAACL.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. CoRR, abs/1101.0309.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional
Semantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics. Forthcoming.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Convolutional Neural Networks for Discourse Com-
positionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Hai Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In HLT-NAACL, pages 39?48.
Tomas Mikolov and Geoffrey Zweig. 2012. Context de-
pendent recurrent neural network language model. In
SLT, pages 234?239.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Takao
Kobayashi, Keikichi Hirose, and Satoshi Nakamura,
editors, INTERSPEECH, pages 1045?1048. ISCA.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model. In
ICASSP, pages 5528?5531. IEEE.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In ACL.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In J. Shawe-Taylor, R.S.
Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Wein-
berger, editors, Advances in Neural Information Pro-
cessing Systems 24, pages 801?809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositional-
ity Through Recursive Matrix-Vector Spaces. In Pro-
ceedings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Ilya Sutskever, James Martens, and Geoffrey E. Hinton.
2011. Generating text with recurrent neural networks.
In Lise Getoor and Tobias Scheffer, editors, ICML,
pages 1017?1024. Omnipress.
1709
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116?125,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Modelling the Lexicon in
Unsupervised Part of Speech Induction
Greg Dubbin
Department of Computer Science
University of Oxford
United Kingdom
Gregory.Dubbin@wolfson.ox.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford
United Kingdom
Phil.Blunsom@cs.ox.ac.uk
Abstract
Automatically inducing the syntactic part-
of-speech categories for words in text
is a fundamental task in Computational
Linguistics. While the performance of
unsupervised tagging models has been
slowly improving, current state-of-the-art
systems make the obviously incorrect as-
sumption that all tokens of a given word
type must share a single part-of-speech
tag. This one-tag-per-type heuristic coun-
ters the tendency of Hidden Markov
Model based taggers to over generate tags
for a given word type. However, it is
clearly incompatible with basic syntactic
theory. In this paper we extend a state-of-
the-art Pitman-Yor Hidden Markov Model
tagger with an explicit model of the lexi-
con. In doing so we are able to incorpo-
rate a soft bias towards inducing few tags
per type. We develop a particle filter for
drawing samples from the posterior of our
model and present empirical results that
show that our model is competitive with
and faster than the state-of-the-art without
making any unrealistic restrictions.
1 Introduction
Research on the unsupervised induction of part-
of-speech (PoS) tags has the potential to im-
prove both our understanding of the plausibil-
ity of theories of first language acquisition, and
Natural Language Processing applications such
as Speech Recognition and Machine Transla-
tion. While there has been much prior work
on this task (Brown et al., 1992; Clark, 2003;
Christodoulopoulos et al., 2010; Toutanova and
Johnson, 2008; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011), a common thread in
many of these works is that models based on a
Hidden Markov Model (HMM) graphical struc-
ture suffer from a tendency to assign too many
different tags to the tokens of a given word type.
Models which restrict word types to only occur
with a single tag show a significant increase in
performance, even though this restriction is clearly
at odds with the gold standard labeling (Brown et
al., 1992; Clark, 2003; Blunsom and Cohn, 2011).
While the empirically observed expectation for the
number of tags per word type is close to one, there
are many exceptions, e.g. words that occur as both
nouns and verbs (opening, increase, related etc.).
In this paper we extend the Pitman-Yor HMM
tagger (Blunsom and Cohn, 2011) to explicitly in-
clude a model of the lexicon that encodes from
which tags a word type may be generated. For
each word type we draw an ambiguity class which
is the set of tags that it may occur with, captur-
ing the fact that words are often ambiguous be-
tween certain tags (e.g. Noun and Verb), while
rarely between others (e.g. Determiner and Verb).
We extend the type based Sequential Monte Carlo
(SMC) inference algorithm of Dubbin and Blun-
som (2012) to incorporate our model of the lexi-
con, removing the need for the heuristic inference
technique of Blunsom and Cohn (2011).
We start in Section 3 by introducing the origi-
nal PYP-HMM model and our extended model of
the lexicon. Section 4 introduces a Particle Gibbs
sampler for this model, a basic SMC method that
generates samples from the model?s posterior. We
evaluate these algorithms in Section 5, analyzing
their behavior in comparisons to previously pro-
posed state-of-the-art approaches.
116
2 Background
From the early work in the 1990?s, much of the
focus on unsupervised PoS induction has been
on hidden Markov Models (HMM) (Brown et al.,
1992; Kupiec, 1992; Merialdo, 1993). The HMM
has proven to be a powerful model of PoS tag as-
signment. Successful approaches generally build
upon the HMM model by expanding its context
and smoothing the sparse data. Constraints such
as tag dictionaries simplify inference by restricting
the number of tags to explore for each word (Gold-
water and Griffiths, 2007). Ganchev et al. (2010)
used posterior regularization to ensure that word
types have a sparse posterior distribution over tags.
A similar approach constrains inference to only
explore tag assignments such that all tokens of the
same word type are assigned the same tag. These
constraints reduce tag assignment ambiguity while
also providing a bias towards the natural spar-
sity of tag distributions in language (Clark, 2003).
However they do not provide a model based solu-
tion to tag ambiguity.
Recent work encodes similar sparsity infor-
mation with non-parametric priors, relying on
Bayesian inference to achieve strong results with-
out any tag dictionaries or constraints (Goldwater
and Griffiths, 2007; Johnson, 2007; Gao and John-
son, 2008). Liang et al. (2010) propose a type-
based approach to this Bayesian inference similar
to Brown et al. (1992), suggesting that there are
strong dependencies between tokens of the same
word-type. Lee et al. (2010) demonstrate strong
results with a similar model and the introduction
of a one-tag-per-type constraint on inference.
Blunsom and Cohn (2011) extend the Bayesian
inference approach with a hierarchical non-
parametric prior that expands the HMM con-
text to trigrams. However, the hierarchical non-
parametric model adds too many long-range de-
pendencies for the type-based inference proposed
earlier. The model produces state-of-the art re-
sults with a one-tag-per-type constraint, but even
with this constraint the tag assignments must be
roughly inferred from an approximation of the ex-
pectations.
Ambiguity classes representing the set of tags
each word-type can take aid inference by mak-
ing the sparsity between tags and words explicit.
Toutanova and Johnson (2008) showed that mod-
elling ambiguity classes can lead to positive re-
sults with a small tag-dictionary extracted from the
data. By including ambiguity classes in the model,
this approach is able to infer ambiguity classes of
unknown words.
Many improvements in part-of-speech induc-
tion over the last few years have come from the
use of semi-supervised approaches in the form of
projecting PoS constraints across languages with
parallel corpora (Das and Petrov, 2011) or extract-
ing them from the wiktionary (Li et al., 2012).
These semi-supervised methods ultimately rely on
a strong unsupervised model of PoS as their base.
Thus, further improvements in unsupervised mod-
els, especially in modelling tag constrains, should
lead to improvements in semi-supervised part-of-
speech induction.
We find that modelling the lexicon in part-of-
speech inference can lead to more efficient algo-
rithms that match the state-of-the-art unsupervised
performance. We also note that the lexicon model
relies heavily on morphological information, and
suffers without it on languages with flexible word
ordering. These results promise further improve-
ments with more advanced lexicon models.
3 The Pitman-Yor Lexicon Hidden
Markov Model
This article proposes enhancing the standard Hid-
den Markov Model (HMM) by explicitly incorpo-
rating a model of the lexicon that consists of word
types and their associated tag ambiguity classes.
The ambiguity class of a word type is the set of
possible lexical categories to which tokens of that
type can be assigned. In this work we aim to
learn the ambiguity classes unsupervised rather
than have them specified in a tag dictionary.
The Lexicon HMM (Lex-HMM) extends the
Pitman-Yor HMM (PYP-HMM) described by
Blunsom and Cohn (2011). When the ambiguity
class of all of the word types in the lexicon is the
complete tagset, the two models are the same.
3.1 PYP-HMM
The base of the model applies a hierarchical
Pitman-Yor process (PYP) prior to a trigram hid-
den Markov model to jointly model the distribu-
tion of a sequence of latent word tags, t, and
word tokens, w. The joint probability defined
by the transition, P
?
(t
l
|t
n?1
, t
n?2
), and emission,
P
?
(w
n
|t
n
), distributions of a trigram HMM is
P
?
(t,w) =
N+1
?
n=1
P
?
(t
l
|t
n?1
, t
n?2
)P
?
(w
n
|t
n
)
117
where N = |t| = |w| and the special tag $
is added to denote the sentence boundaries. The
model defines a generative process in which the
tags are selected from a transition distribution,
t
l
|t
l?1
, t
l?2
, T , determined by the two previous
tags in their history, and the word tokens are se-
lected from the emission distribution, w
l
|t
l
, E, of
the latest tag.
t
n
|t
n?1
, t
n?2
, T ? T
t
n?1
,t
n?2
w
n
|t
n
, E ? E
t
n
The PYP-HMM draws the above multinomial dis-
tributions from a hierarchical Pitman-Yor Process
prior. The Pitman-Yor prior defines a smooth back
off probability from more complex to less com-
plex transition and emission distributions. In the
PYP-HMM trigram model, the transition distri-
butions form a hierarchy with trigram transition
distributions drawn from a PYP with the bigram
transitions as their base distribution, and the bi-
gram transitions similarly backing off to the uni-
gram transitions. The hierarchical prior can be in-
tuitively understood to smooth the trigram transi-
tion distributions with bigram and unigram distri-
butions in a similar manner to an ngram language
model (Teh, 2006). This back-off structure greatly
reduces sparsity in the trigram distributions and is
achieved by chaining together the PYPs through
their base distributions:
T
ij
|a
T
, b
T
, B
i
? PYP(a
T
, b
T
, B
i
)
B
i
|a
B
, b
B
, U ? PYP(a
B
, b
B
, U)
U |a
U
, b
U
? PYP(a
U
, b
U
,Uniform).
E
i
|a
E
, b
E
, C ? PYP(a
E
, b
E
, C
i
),
where T
ij
, B
i
, and U are trigram, bigram, and un-
igram transition distributions respectively, and C
i
is either a uniform distribution (PYP-HMM) or a
bigram character language model distribution to
model word morphology (PYP-HMM+LM).
Sampling from the posterior of the hierarchi-
cal PYP is calculated with a variant of the Chi-
nese Restaurant Process (CRP) called the Chinese
Restaurant Franchise (CRF) (Teh, 2006; Goldwa-
ter et al., 2006). In the CRP analogy, each latent
variable (tag) in a sequence is represented by a
customer entering a restaurant and sitting at one of
an infinite number of tables. A customer chooses
to sit at a table in a restaurant according to the
probability
P (z
n
= k|z
1:n?1
) =
{
c
?
k
?a
n?1+b
1 ? k ? K
?
K
?
a+b
n?1+b
k = K
?
+ 1
(1)
where z
n
is the index of the table chosen by the
nth customer to the restaurant, z
1:n?1
is the seat-
ing arrangement of the previous n ? 1 customers
to enter, c
?
k
is the count of the customers at table
k, and K
?
is the total number of tables chosen by
the previous n ? 1 customers. All customers at a
table share the same dish, representing the value
assigned to the latent variables. When customers
sit at an empty table, a new dish is assigned to that
table according to the base distribution of the PYP.
To expand the CRP analogy to the CRF for hierar-
chical PYPs, when a customer sits at a new table,
a new customer enters the restaurant of the PYP of
the base distribution.
Blunsom and Cohn (2011) explored two Gibbs
sampling methods for inference with the PYP-
HMM model. The first individually samples tag
assignments for each token. The second employs
a tactic shown to be effective by earlier works by
constraining inference to only one tag per word
type (PYP-1HMM). However marginalizing over
all possible table assignments for more than a sin-
gle tag is intractable. Blunsom and Cohn (2011)
approximates the PYP-1HMM tag posteriors for a
particular sample according to heuristic fractional
table counts. This approximation is shown to be
particularly inaccurate for values of a close to one.
3.2 The Lexicon HMM
We define the lexicon to be the set of all word
types (W ) and a function (L) which maps each
word type (W
i
? W ) to an element in the power
set of possible tags T ,
L : W ? P(T ).
The Lexicon HMM (Lex-HMM) generates the
lexicon with all of the word types and their ambi-
guity classes before generating the standard HMM
parameters. The set of tags associated with each
word type is referred to as its ambiguity class
s
i
? T . The ambiguity classes are generated from
a multinomial distribution with a sparse, Pitman-
Yor Process prior,
s
i
|S ? S
S|a
S
, b
S
? PY P (a
S
, b
S
, G)
118
UB
j
T
ij
E
j
w
1
t
1
w
2
t
2
w
3
t
3
...
W
i
s
i
S
Figure 1: Lex-HMM Structure: The graphical
structure of the Lex-HMM model. In addition to
the trigram transition (T
ij
) and emission (E
j
), the
model includes an ambiguity class (s
i
) for each
word type (W
i
) drawn from a distribution S with
a PYP prior.
where S is the multinomial distribution over all
possible ambiguity classes. The base distribution
of the PYP, G, chooses the size of the ambiguity
class according to a geometric distribution (nor-
malized so that the size of the class is at most the
number of tags |T |). G assigns uniform probabil-
ity to all classes of the same size. A plate diagram
for this model is shown in Figure 1.
This model represents the observation that there
are relatively few distinct ambiguity classes over
all of the word types in a corpus. For example, the
full Penn-Treebank Wall Street Journal (WSJ) cor-
pus with 45 possible tags and 49,206 word types
has only 343 ambiguity classes. Figure 2 shows
that ambiguity classes in the WSJ have a power-
law distribution. Furthermore, these classes are
generally small; the average ambiguity class in the
WSJ corpus has 2.94 tags. The PYP prior favors
power-law distributions and the modified geomet-
ric base distribution favors smaller class sizes.
Once the lexicon is generated, the standard
HMM parameters can be generated as described
in section 3.1. The base emission probabilities C
are constrained to fit the generated lexicon. The
standard Lex-HMM model emission probabilities
for tag t
i
are uniform over all word types with t
i
in their ambiguity class. The character language
model presents a challenge because it is non-trivial
to renormalise over words with t
i
in their ambigu-
ity class. In this case word types without t
i
in their
100 101 102 103Rank
100
101
102
103
104
105
Freq
uen
cy
Gold LexiconPredicted Lexicon
Log-Log Ambiguity Class Frequency vs. Rank
Figure 2: Ambiguity Class Distribution: Log-
log plot of ambiguity class frequency over rank
for the Penn-Treebank WSJ Gold Standard lexicon
highlighting a Zipfian distribution and the ambigu-
ity of classes extracted from the predicted tags.
ambiguity class are assigned an emission probabil-
ity of 0 and the model is left deficient.
Neither of the samplers proposed by Blunsom
and Cohn (2011) and briefly described in section
3.1 are well suited to inference with the lexicon.
Local Gibbs sampling of individual token-tag as-
signments would be very unlikely to explore a
range of confusion classes, while the type based
approximate sample relies on a one-tag-per-type
restriction. Thus in the next section we extend the
Particle Filtering solution presented in Dubbin and
Blunsom (2012) to the problem of simultaneous
resampling the ambiguity class as well as the tags
for all tokens of a given type. This sampler pro-
vides both a more attractive inference algorithm
for the original PYP-HMM and one adaptable to
our Lex-HMM.
4 Inference
To perform inference with both the lexicon and
the tag assignments, we block sample the ambi-
guity class assignment as well as all tag assign-
ments for tokens of the same word type. It would
be intractable to exactly calculate the probabili-
ties to sample these blocks. Particle filters are an
example of a Sequential Monte Carlo technique
which generates unbiased samples from a distribu-
tion without summing over the intractable number
of possibilities.
The particle filter samples multiple independent
sequences of ambiguity classes and tag assign-
ments. Each sequence of samples, called a parti-
119
cle, is generated incrementally. For each particle,
the particle filter first samples an ambiguity class,
and then samples each tag assignment in sequence
based only on the previous samples in the parti-
cle. The value of the next variable in a sequence
is sampled from a proposal distribution based only
on the earlier values in the sequence. Each particle
is assigned an importance weight such that a par-
ticle sampled proportional to its weight represents
an unbiased sample of the true distribution.
Each particle represents a specific sampling of
an ambiguity class, tag sequence, t
W,p
1:n
, and the
count deltas, z
W,p
1:n
. The term t
W,p
1:n
denotes the se-
quence of n tags generated for word-type W and
stored as part of particle p ? [1, P ]. The count
deltas store the differences in the seating arrange-
ment neccessary to calculate the posterior proba-
bilities according to the Chinese restaurant fran-
chise described in section 3.1. The table counts
from each particle are the only data necessary to
calculate the probabilities described in equation
(1).
The ambiguity class for a particle is proposed
by uniformly sampling one tag from the tagset to
add to or remove from the previous iteration?s am-
biguity class with the additional possibility of us-
ing the same ambiguity class. The particle weights
are then set to
P (s
W,p
|S
?W
)
?
t?s
W,p
(e
t
+ 1)
#(E
t
)
?
t?T?s
W,p
(e
t
)
#(E
t
)
where P (s
W,p
|S
?W
) is the probability of the am-
biguity class proposed for particle p for word type
W given the ambiguity classes for the rest of the
vocabulary, e
t
is the number of word types with t
in their ambiguity class, and #(E
t
) is the number
of tables in the CRP for the emission distribution
of tag t. The last two terms of the equation cor-
rect for the difference in the base probabilities of
the words that have already been sampled with a
different lexicon.
At each token occurrence n, the next tag assign-
ment, t
W,p
n
for each particle p ? [1, P ] is deter-
mined by the seating decisions z
W,p
n
, which are
made according the proposal distribution:
q
W,p
n
(z
W,p
n
|z
W,p
1:n?1
, z
?W
) ?
P (z
W,p
n
|c
?2
, c
?1
, z
W,p
1:n?1
, z
?W
)
?P (c
+1
n
|c
?1
n
, z
W,p
n
, z
W,p
1:n?1
, z
?W
)
?P (c
+2
n
|z
W,p
n
, c
+1
n
, z
W,p
1:n?1
, z
?W
)
?P (w
W
n
|z
W,p
n
, z
W,p
1:n?1
, z
?W
).
In this case, c
?k
n
represents a tag in the context of
site t
W
n
offset by k, while z
W,p
1:n?1
and z
?W
rep-
resent the table counts from the seating decisions
previously chosen by particle p and the values at
all of the sites where a word token of type W
does not appear, respectively. This proposal dis-
tribution ignores changes to the seating arrange-
ment between the three transitions involving the
site n. The specific tag assignement, t
W
, p
n
, is
completely determined by the seating decisions
sampled according to this proposal distribution.
Once all of the particles have been sampled, one
of them is sampled with probability proportional
to its weight. This final sample is a sample from
the target distribution.
As the Particle Filter is embedded in a Gibbs
sampler which cycles over all word types this al-
gorithm is an instance of Particle Gibbs. Andrieu
et al. (2010) shows that to ensure the samples gen-
erated by SMC for a Gibbs sampler have the tar-
get distribution as the invariant density, the par-
ticle filter must be modified to perform a condi-
tional SMC update. This means that the particle
filter guarantees that one of the final particles is as-
signed the same values as the previous Gibbs iter-
ation. Therefore, a special 0
th
particle is automati-
cally assigned the value from the prior iteration of
the Gibbs sampler at each site n, though the pro-
posal probability q
W
n
(t
W,0
n
|t
W,p
1:n?1
, z
W,p
1:n?1
) still has
to be calculated to update the weight ?
W,p
n
prop-
erly. This ensures that the sampler has a chance of
reverting to the prior iteration?s sequence.
5 Experiments and Results
We provide an empirical evaluation of our pro-
posed Lex-HMM in terms of the accuracy of
the taggings learned according to the most pop-
ular metric, and the distributions over ambiguity
classes. Our experimental evaluation considers the
impact of our improved Particle Gibbs inference
algorithm both for the original PYP-HMM and
when used for inference in our extended model.
We intend to learn whether the lexicon model
can match or exceed the performance of the other
models despite focusing on only a subset of the
possible tags each iteration. We hypothesize that
an accurate lexicon model and the sparsity it in-
duces over the number of tags per word-type will
improve the performance over the standard PYP-
HMM model while also decreasing training time.
Furthermore, our lexicon model is novel, and its
120
Sampler M-1 Accuracy Time (h)
Meta-Model (CGS10) 76.1 ?
MEMM (BBDK10) 75.5
?
40*
Lex-HMM 71.1 7.9
Type PYP-HMM 70.1 401.2
Local PYP-HMM 70.2 8.6
PYP-1HMM 75.6 20.6
Lex-HMM+LM 77.5 16.9
Type PYP-HMM+LM 73.5 446.0
PYP-1HMM+LM 77.5 34.9
Table 1: M-1 Accuracy on the WSJ Corpus:
Comparison of the accuracy of each of the sam-
plers with and without the language model emis-
sion prior on the English WSJ Corpus. The second
column reports run time in hours where available*.
Note the Lex-HMM+LM model matches the PYP-
1HMM+LM approximation despite finishing in
half the time. The abbreviations in parentheses
indicate that the results were reported in CGS10
(Christodoulopoulos et al., 2010) and BBDK10
(Berg-Kirkpatrick et al., 2010) *CGS10 reports
that the MEMM model takes approximately 40
hours on 16 cores.
accuracy in representing ambiguity classes is an
important aspect of its performance. The model
focuses inference on the most likely tag choices,
represented by ambiguity classes.
5.1 Unsupervised Part-of-Speech Tagging
The most popular evaluation for unsupervised
part-of-speech taggers is to induce a tagging for
a corpus and compare the induced tags to those
annotated by a linguist. As the induced tags are
simply integer labels, we must employ a map-
ping between these and the more meaningful syn-
tactic categories of the gold standard. We re-
port results using the many-to-one (M-1) met-
ric considered most intuitive by the evaluation of
Christodoulopoulos et al. (2010). M-1 measures
the accuracy of the model after mapping each pre-
dicted class to its most frequent corresponding tag.
While Christodoulopoulos et al. (2010) found V-
measure to be more stable over the number of
parts-of-speech, this effect doesn?t appear when
the number of tags is constant, as in our case. For
experiments on English, we report results on the
entire Penn. Treebank (Marcus et al., 1993). For
other languages we use the corpora made avail-
able for the CoNLL-X Shared Task (Buchholz and
Marsi, 2006). All Lex-HMM results are reported
with 10 particles as no significant improvement
was found with 50 particles.
Table 1 compares the M-1 accuracies of both
the PYP-HMM and the Lex-HMM models on the
Penn. Treebank Wall Street Journal corpus. Blun-
som and Cohn (2011) found that the Local PYP-
HMM+LM sampler is unable to mix, achieving
accuracy below 50%, therefore it has been left
out of this analysis. The Lex-HMM+LM model
achieves the same accuracy as the state-of-the-
art PYP-1HMM+LM approximation. The Lex-
HMM+LM?s focus on only the most likely tags for
each word type allows it to finish training in half
the time as the PYP-1HMM+LM approximation
without any artificial restrictions on the number of
tags per type. This contrasts with other approaches
that eliminate the constraint at a much greater cost,
e.g. the Type PYP-HMM, the MEMM, and the
Meta-Model
1
The left side of table 2 compares the M-1 accu-
racies of the Lex-HMM model to the PYP-HMM
model. These models both ignore word morphol-
ogy and rely on word order. The 1HMM approxi-
mation achieves the highest average accuracy. The
Lex-HMM model matches or surpasses the type-
based PYP-HMM approach in six languages while
running much faster due to the particle filter con-
sidering a smaller set of parts-of-speech for each
particle. However, in the absence of morpho-
logical information, the Lex-HMM model has a
similar average accuracy to the local and type-
based PYP-HMM samplers. The especially low
performance on Hungarian, a language with free
word ordering and strong morphology, suggests
that the Lex-HMM model struggles to find ambi-
guity classes without morphology. The Lex-HMM
model has a higher average accuracy than the type-
based or local PYP-HMM samplers when Hungar-
ian is ignored.
The right side of table 2 compares the M-1 ac-
curacies of the Lex-HMM+LM model to the PYP-
HMM+LM. The language model leads to consis-
tently improved performance for each of the sam-
plers excepting the token sampler, which is un-
able to mix properly with the additional complex-
ity. The accuracies achieved by the 1HMM+LM
1
While were unable to get an estimate on the runtime of
the Meta-Model, it uses a system similar to the feature-based
system of the MEMM with an additional feature derived from
the proposed class from the brown model. Therefore, it is
likely that this model has a similar runtime.
121
Language Lex-HMM PYP-HMM Local 1HMM Lex-HMM+LM PYP-HMM+LM 1HMM+LM
WSJ 71.1 70.1 70.2 75.6 77.5 73.5 77.5
Arabic 57.2 57.6 56.2 61.9 62.1 62.7 62.0
Bulgarian 67.2 67.8 67.6 71.4 72.7 72.1 76.2
Czech 61.3 61.6 64.5 65.4 68.2 67.4 67.9
Danish 68.6 70.3 69.1 70.6 74.7 73.1 74.6
Dutch 70.3 71.6 64.1 73.2 71.7 71.8 72.9
Hungarian 57.9 61.8 64.8 69.6 64.4 69.9 73.2
Portuguese 69.5 71.1 68.1 72.0 76.3 73.9 77.1
Spanish 73.2 69.1 68.5 74.7 80.0 75.2 78.8
Swedish 66.3 63.5 67.6 67.2 70.4 67.6 68.6
Average 66.3 (67.2) 66.5 (67.0) 66.1 (66.2) 70.2 (70.3) 71.8 (72.6) 70.7 (70.8) 72.9 (72.9)
Table 2: M-1 Accuracy of Lex-HMM and PYP-HMM models: Comparison of M-1 accuracy for the
lexicon based model (Lex-HMM) and the PYP-HMM model on several languages. The Lex-HMM and
PYP-HMM columns indicate the results of word type based particle filtering with 10 and 100 particles,
respectively, while the Local and 1HMM columns use the token based sampler and the 1HMM approxi-
mation described by Blunsom and Cohn (2011). The token based sampler was run for 500 iterations and
the other samplers for 200. The percentages in brakets represent the average accuracy over all languages
except for Hungarian.
sampler represent the previous state-of-the-art.
These results show that the Lex-HMM+LM model
achieves state-of-the-art M-1 accuracies on sev-
eral datasets, including the English WSJ. The Lex-
HMM+LM model performs nearly as well as, and
often better than, the 1HMM+LM sampler without
any restrictions on tag assignments.
The drastic improvement in the performance
of the Lex-HMM model reinforces our hypothe-
sis that morphology is critical to the inference of
ambiguity classes. Without the language model
representing word morphology, the distinction be-
tween ambiguity classes is too ambiguous. This
leads the sampler to infer an excess of poor am-
biguity classes. For example, the tag assignments
from the Lex-PYP model on the WSJ dataset con-
sist of 660 distinct ambiguity classes, while the
Lex-PYP+LM tag assignments only have 182 dis-
tinct ambiguity classes.
Note that while the Lex-HMM and Lex-
HMM+LM samplers do not have any restrictions
on inference, they do not sacrifice time. The ad-
ditional samples generated by the particle filter
are mitigated by limiting the number of tags each
particle must consider. In practice, this results in
the Lex-HMM samplers with 10 particles running
in half time as the 1HMM samplers. The Lex-
HMM+LM sampler with 10 particles took 16.9
hours, while the 1HMM+LM sampler required
34.9 hours. Furthermore, the run time evaluation
does not take advantage of the inherent distributed
nature of particle filters. Each of the particles can
be sampled completely independentally from the
others, making it trivial to run each on a seperate
core.
5.2 Lexicon Analysis
While section 5.1 demonstrates that the Lex-
HMM+LM sampler performs similarly to the
more restricted 1HMM+LM, we also seek to eval-
uate the accuracy of the lexicon model itself. We
compare the ambiguity classes extracted from the
gold standard and predicted tag assignments of the
WSJ corpus. We also explore the relationship be-
tween the actual and sampled ambiguity classes.
The solid curve in figure 2 shows the distribu-
tion of the number of word types assigned to each
ambiguity set extracted from the gold standard tag
assignments from the Penn Treebank Wall Street
Journal corpus. The straight line strongly indi-
cates that ambiguity classes follow a Zipfian dis-
tribution. Figure 2 also graphs the distribution of
the ambiguity classes extracted from the best tag-
assignment prediction from the model. The pre-
dicted graph has a similar shape to the gold stan-
dard but represents half as many distinct ambigu-
ity classes - 182 versus 343.
For a qualitative analysis of the generated lex-
icon, table 3 lists frequent ambiguity classes and
the most common words assigned to them. The 14
most frequent ambiguity classes contain only one
tag each, the top half of table 3 shows the 5 most
frequent. One third of the word-types in the first
five rows of the table are exactly matched with the
ambiguity classes from the gold standard. Most of
the remaining words in those rows are assigned to
122
Rank Gold Rank Tags Top Word Types
1 1 NNP Mr., Corp. (1), Inc. (.99), Co. (1), Exchange (.99)
2 2 NN % (1), company, stock (.99), -RRB- (0), years (0)
3 3 JJ new, other, first (.9), most (0), major (1)
4 5 NNS companies, prices (1), quarter (0), week (0), investors
5 4 CD $ (0), million (1), billion, 31, # (0)
15 303 NN, CD yen (.47, 0), dollar (1, 0), 150 (0, 1), 29 (0, 1), 33 (0, 1)
16 17 VB, NN plan (.03, .9), offer (.2, .74), issues (0, 0), increase (.34, .66), end (.18, .81)
17 115 DT, NNP As (0, 0), One (0, .01), First (0, .82), Big (0, .91), On (0, .01)
18 11 NN, JJ market (.99, 0), U.S. (0, 0), bank (1, 0), cash (.98, 0), high (.06, .9)
20 22 VBN, JJ estimated (.58, .15), lost (.43, .03), failed (.35, .04), related (.74, .23), re-
duced (.57, .12)
Table 3: Selection of Predicted Ambiguity Classes: Common ambiguity classes from the predicted
part-of-speech assignments from the WSJ data set, and the five most common word types associated
with each ambiguity class. The sets are ranked according to the number of word types associated to
them. Words in bold are matched to exactly the same ambiguity set in the gold standard. The lower
five ambiguity classes are the most common with more than one part-of-speech. Numbers in parentheses
represent the proportion of tokens of that type assigned to each tag in the gold standard for that ambiguity
class.
a class representing almost all of the words? occur-
rences in the gold standard, e.g., ?Corp.? is an NNP
in 1514 out of 1521 occurrences. Some words are
assigned to classes with similar parts of speech,
e.g. {NNS} rather than {NN} for week.
The lower half of table 3 shows the most fre-
quent ambiguity classes with more than a sin-
gle tag. The words assigned to the {NN,CD},
{DT,NNP}, and {NN,JJ} classes are not them-
selves ambiguous. Rather words that are unam-
biguously one of the two tags are often assigned
to an ambiguity class with both. The most com-
mon types in the {NN, CD} set are unambiguously
either NN or CD. In many cases the words are
merged into broader ambiguity classes because the
Lex-HMM+LM uses the language model to model
the morphology of words over individual parts-
of-speech, rather than entire ambiguity classes.
Therefore, a word-type is likely to be assigned
a given ambiguity class as long as at least one
part-of-speech in that ambiguity class is associ-
ated with morphologically similar words. These
results suggest modifying the Lex-HMM+LM to
model word morphology over ambiguity classes
rather than parts-of-speech.
The {VB,NN} and {VBN,JJ} are representative
of true ambiguity classes. Occurrences of words in
these classes are likely to be either of the possible
parts-of-speech. These results show that the Lex-
HMM is modelling ambiguity classes as intended.
6 Conclusion
This paper described an extension to the PYP-
HMM part-of-speech model that incorporates a
sparse prior on the lexicon and an SMC based in-
ference algorithm. These contributions provide a
more plausible model of part-of-speech induction
which models the true ambiguity of tag to type as-
signments without the loss of performance of ear-
lier HMM models. Our empirical evaluation indi-
cates that this model is able to meet or exceed the
performance of the previous state-of-the-art across
a range of language families.
In addition to the promising empirical results,
our analysis indicates that the model learns ambi-
guity classes that are often quite similar to those
in the gold standard. We believe that further im-
provements in both the structure of the lexicon
prior and the inference algorithm will lead to addi-
tional performance gains. For example, the model
could be improved by better modelling the rela-
tionship between a word?s morphology and its am-
biguity class. We intend to apply our model to
recent semi-supervised approaches which induce
partial tag dictionaries from parallel language data
(Das and Petrov, 2011) or the Wiktionary (Li et
al., 2012). We hypothesize that the additional data
should improve the modelled lexicon and conse-
quently improve tag assignments.
The Lex-HMM models ambiguity classes to fo-
cus the sampler on the most likely parts-of-speech
for a given word-type. In doing so, it matches or
improves on the accuracy of other models while
running much faster.
123
References
Christophe Andrieu, Arnaud Doucet, and Roman
Holenstein. 2010. Particle markov chain monte
carlo methods. Journal Of The Royal Statistical So-
ciety Series B, 72(3):269?342.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 582?590, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal Pitman-Yor process hmm for unsupervised part
of speech induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
865?874, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Comput. Linguist., 18:467?479, December.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06,
pages 149?164, Morristown, NJ, USA. Association
for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised POS induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575?584, Cambridge, MA, October. Association for
Computational Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting
of the European Association for Computational Lin-
guistics (EACL), pages 59?66.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gregory Dubbin and Phil Blunsom. 2012. Unsuper-
vised bayesian part of speech inference with particle
gibbs. In Peter A. Flach, Tijl De Bie, and Nello Cris-
tianini, editors, ECML/PKDD (1), volume 7523 of
Lecture Notes in Computer Science, pages 760?773.
Springer.
Kuzman Ganchev, Jo?ao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. Journal of Ma-
chine Learning Research, 99:2001?2049, August.
Jianfeng Gao and Mark Johnson. 2008. A compar-
ison of bayesian estimators for unsupervised hid-
den markov model pos taggers. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?08, pages 344?352,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proc. of the 45th Annual Meeting of
the ACL (ACL-2007), pages 744?751, Prague, Czech
Republic, June.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch?olkopf, and J. Platt, editors, Advances in Neu-
ral Information Processing Systems 18, pages 459?
466. MIT Press, Cambridge, MA.
Mark Johnson. 2007. Why doesnt EM find good
HMM POS-taggers? In Proc. of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 296?305, Prague,
Czech Republic.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech
and Language, 6:225?242.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 853?861, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-
based MCMC. In North American Association for
Computational Linguistics (NAACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Bernard Merialdo. 1993. Tagging english text with
a probabilistic model. Computational Linguistics,
20:155?171.
124
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 985?992, Morristown, NJ,
USA. Association for Computational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In J.C. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neu-
ral Information Processing Systems 20, pages 1521?
1528. MIT Press, Cambridge, MA.
125
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 328?337,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Dynamic Topic Adaptation for Phrase-based MT
Eva Hasler
1
, Phil Blunsom
2
, Philipp Koehn
1
, Barry Haddow
1
1
School of Informatics, University of Edinburgh
2
Dept. of Computer Science, University of Oxford
Abstract
Translating text from diverse sources
poses a challenge to current machine
translation systems which are rarely
adapted to structure beyond corpus level.
We explore topic adaptation on a diverse
data set and present a new bilingual vari-
ant of Latent Dirichlet Allocation to com-
pute topic-adapted, probabilistic phrase
translation features. We dynamically in-
fer document-specific translation proba-
bilities for test sets of unknown origin,
thereby capturing the effects of document
context on phrase translations. We show
gains of up to 1.26 BLEU over the base-
line and 1.04 over a domain adaptation
benchmark. We further provide an anal-
ysis of the domain-specific data and show
additive gains of our model in combination
with other types of topic-adapted features.
1 Introduction
In statistical machine translation (SMT), there has
been a lot of interest in trying to incorporate in-
formation about the provenance of training exam-
ples in order to improve translations for specific
target domains. A popular approach are mixture
models (Foster and Kuhn, 2007) where each com-
ponent contains data from a specific genre or do-
main. Mixture models can be trained for cross-
domain adaption when the target domain is known
or for dynamic adaptation when the target domain
is inferred from the source text under translation.
More recent domain adaptation methods employ
corpus or instance weights to promote relevant
training examples (Matsoukas et al., 2009; Fos-
ter et al., 2010) or do more radical data selection
based on language model perplexity (Axelrod et
al., 2011). In this work, we are interested in the
dynamic adaptation case, which is challenging be-
cause we cannot tune our model towards any spe-
cific domain.
In previous literature, domains have often been
loosely defined in terms of corpora, for exam-
ple, news texts would be defined as belonging to
the news domain, ignoring the specific content of
news documents. It is often assumed that the data
within a domain is homogeneous in terms of style
and vocabulary, though that is not always true in
practice. The term topic on the other hand can
describe the thematic content of a document (e.g.
politics, economy, medicine) or a latent cluster in a
topic model. Topic modelling for machine transla-
tion aims to find a match between thematic context
and topic clusters. We view topic adaptation as
fine-grained domain adaptation with the implicit
assumption that there can be multiple distributions
over translations within the same data set. If these
distributions overlap, then we expect topic adapta-
tion to help separate them and yield better trans-
lations than an unadapted system. Topics can be
of varying granularity and are therefore a flexi-
ble means to structure data that is not uniform
enough to be modelled in its entirety. In recent
years there have been several attempts to integrat-
ing topical information into SMT either by learn-
ing better word alignments (Zhao and Xing, 2006),
by adapting translation features cross-domain (Su
et al., 2012), or by dynamically adapting lexical
weights (Eidelman et al., 2012) or adding sparse
topic features (Hasler et al., 2012).
We take a new approach to topic adaptation by
estimating probabilistic phrase translation features
in a completely Bayesian fashion. The motivation
is that automatically identifying topics in the train-
ing data can help to select the appropriate transla-
tion of a source phrase in the context of a docu-
ment. By adapting a system to automatically in-
duced topics we do not have to trust data from a
given domain to be uniform. We also overcome
the problem of defining the level of granularity for
domain adaptation. With more and more training
data automatically extracted from the web and lit-
tle knowledge about its content, we believe this
is an important area to focus on. Translation of
web sites is already a popular application for MT
systems and could be helped by dynamic model
adaptation. We present results on a mixed data
set of the TED corpus, parts of the Commoncrawl
corpus which contains crawled web data and parts
of the News Commentary corpus which contains
328
Figure 1: Phrasal LDA model for inference on
training data.
documents about politics and economics. We be-
lieve that the broad range of this data set makes it a
suitable testbed for topic adaptation. We focus on
translation model adaptation to learn how words
and phrases translate in a given document-context
without knowing the origin of the document. By
learning translations over latent topics and com-
bining several topic-adapted features we achieve
improvements of more than 1 BLEU point.
2 Bilingual topic models over phrase
pairs
Our model is based on LDA and infers topics
as distributions over phrase pairs instead of over
words. It is specific to machine translation in that
the conditional dependencies between source and
target phrases are modelled explicitly, and there-
fore we refer to it as phrasal LDA. Topic distribu-
tions learned on a training corpus are carried over
to tuning and test sets by running a modified in-
ference algorithm on the source side text of those
sets. Translation probabilities are adapted sepa-
rately to each source text under translation which
makes this a dynamic topic adaptation approach.
In the following we explain our approach to topic
modelling with the objective of estimating better
phrase translation probabilities for data sets that
exhibit a heterogeneous structure in terms of vo-
cabulary and style. The advantage from a mod-
elling point of view is that unlike with mixture
models, we avoid sparsity problems that would
arise if we treated documents or sets of documents
as domains and learned separate models for them.
2.1 Latent Dirichlet Allocation (LDA)
LDA is a generative model that learns latent top-
ics in a document collection. In the original
formulation, topics are multinomial distributions
over words of the vocabulary and each docu-
ment is assigned a multinomial distribution over
topics (Blei et al., 2003). Our goal is to learn
topic-dependent phrase translation probabilities
and hence we modify this formulation by replac-
ing words with phrase pairs. This is straightfor-
ward when both source and target phrases are ob-
served but requires a modified inference approach
when only source phrases are observed in an un-
known test set. Different from standard LDA and
previous uses of LDA for MT, we define a bilin-
gual topic model that learns topic distributions
over phrase pairs. This allows us to model the
units of interest in a more principled way, without
the need to map per-word or per-sentence topics to
phrase pairs. Figure 1 shows a graphical represen-
tation of the following generative process.
For each of N documents in the collection
1. Choose topic distribution ?
d
? Dirichlet(?).
2. Choose the number of phrases pairs P
d
in the
document, P
d
? Poisson(?).
3. For every position d
i
in the document corre-
sponding to a phrase pair p
d,i
of source and
target phrase s
i
and t
i
1
:
(a) Choose a topic z
d,i
?Multinomial(?
d
).
(b) Conditioned on topic z
d,i
, choose a
source phrase s
d,i
?Multinomial(?
z
d,i
).
(c) Conditioned on z
d,i
and s
d,i
, choose tar-
get phrase t
d,i
?Multinomial(?
s
d,i
,z
d,i
).
?, ? and ? are parameters of the Dirichlet dis-
tributions, which are asymmetric for k = 0. Our
inference algorithm is an implementation of col-
lapsed variational Bayes (CVB), with a first-order
Gaussian approximation (Teh et al., 2006). It has
been shown to be more accurate than standard VB
and to converge faster than collapsed Gibbs sam-
pling (Teh et al., 2006; Wang and Blunsom, 2013),
with little loss in accuracy. Because we have to
do inference over a large number of phrase pairs,
CVB is more practical than Gibbs sampling.
2.2 Overview of training strategy
Ultimately, we want to learn translation probabil-
ities for all possible phrase pairs that apply to a
given test document during decoding. Therefore,
topic modelling operates on phrase pairs as they
will be seen during decoding. Given word-aligned
parallel corpora from several domains, we extract
lists of per-document phrase pairs produced by the
extraction algorithm in the Moses toolkit (Koehn
et al., 2007) which contain all phrase pairs consis-
tent with the word alignment. We run CVB on the
set of all training documents to learn latent topics
without providing information about the domains.
1
Parallel documents are modelled as bags of phrase pairs.
329
Using the trained model, CVB with modified in-
ference is run on all test documents with the set of
possible phrase translations that a decoder would
load from a phrase table before decoding. When
test inference has finished, we compute adapted
translation probabilities at the document-level by
marginalising over topics for each phrase pair.
3 Bilingual topic inference
3.1 Inference on training documents
The aim of inference on the training data is to
find latent topics in the distributions over phrase
pairs in each document.This is done by repeatedly
visiting all phrase pair positions in all documents,
computing conditional topic probabilities and up-
dating counts. To bias the model to cluster stop
word phrases in one topic, we place an asymmet-
ric prior over the hyperparameters
2
as described in
(Wallach et al., 2009) to make one of the topics a
priori more probable in every document. We use
a fixed-point update (Minka, 2012) to update the
hyperparameters after every iteration. For CVB
the conditional probability of topic z
d,i
given the
current state of all variables except z
d,i
is
P(z
d,i
= k|z
?(d,i)
,s, t,d,?,?,?) ?
(E
q?
[n
?(d,i)
.,k,s,t
]+?)
(E
q?
[n
?(d,i)
.,k,s,.
]+T
s
??)
(E
q?
[n
?(d,i)
.,k,s,.
]+ ?)
(E
q?
[n
?(d,i)
.,k,.
]+S ? ?)
?(E
q?
[n
?(d,i)
d,k,.
]+?) (1)
where s and t are all source and target phrases in
the collection. n
?(d,i)
.,k,s,t
, n
?(d,i)
.,k,s,.
and n
?(d,i)
d,k,.
are cooc-
currence counts of topics with phrase pairs, source
phrases and documents respectively. E
q?
is the
expectation under the variational posterior and in
comparison to Gibbs sampling where the posterior
would otherwise look very similar, counts are re-
placed by their means. n
?(d,i)
.,k,.
is a topic occurrence
count, T
s
is the number of possible target phrases
for a given source phrase and S is the total num-
ber of source phrases. By modelling phrase trans-
lation probabilities separately as P(t
i
|s
i
,z
i
= k, ..)
and P(s
i
|z
i
= k, ..), we can put different priors on
these distributions. For example, we want a sparse
distribution over target phrases for a given source
phrase and topic to express our translation prefer-
ence under each topic. The algorithm stops when
the variational posterior has converged for all doc-
uments or after a maximum of 100 iterations.
3.2 Inference on tuning and test documents
To compute translation probabilities for tuning
and test documents where target phrases are not
2
Omitted from the following equations for simplicity.
observed, the variational posterior is adapted as
shown in Equation 2
P(z
d,i
= k, t
i, j
|z
?(d,i)
,s, t
?(d,i)
,d,?,?,?) ?
(E
q?
[n
?(d,i)
.,k,s,t
j
]+?)
(E
q?
[n
?(d,i)
.,k,s,.
]+T
s
??)
(E
q?
[n
?(d,i)
.,k,s,.
]+ ?)
(E
q?
[n
?(d,i)
.,k,.
]+S ? ?)
?(E
q?
[n
?(d,i)
d,k,.
]+?) (2)
which now computes the joint conditional prob-
ability of a topic k and a target phrase t
i, j
, given the
source phrase s
i
and the test document d. There-
fore, the size of the support changes from K to
K ?T
s
. While during training inference we compute
a distribution over topics for each source-target
pair, in test inference we can use the posterior to
marginalise out the topics and get a distribution
over target phrases for each source phrase.
We use the Moses decoder to produce lists of
translation options for each document in the tun-
ing and test sets. These lists comprise all phrase
pairs that will enter the search space at decod-
ing time. By default, only 20 target phrases per
source phrase are loaded from the phrase table,
so in order to allow for new phrase pairs to en-
ter the search space and for translation probabil-
ities to be computed more accurately, we allow
for up to 200 target phrases per source. For each
source sentence, we consider all possible phrase
segmentations and applicable target phrases. Un-
like in training, we do not iterate over all phrase
pairs in the list but over blocks of up to 200 target
phrases for a given source phrase. The algorithm
stops when all marginal translation probabilities
have converged though in practice we stopped ear-
lier to avoid overfitting.
3.3 Phrase translation probabilities
After topic inference on the tuning and test data,
the forward translation probabilities P(t|s,d) are
computed. This is done separately for every doc-
ument d because we are interested in the trans-
lation probabilities that depend on the inferred
topic proportions for a given document. For ev-
ery document, we iterate over source positions p
d,i
and use the current variational posterior to com-
pute P(t
i, j
|s
i
,d) for all possible target phrases by
marginalizing over topics:
P(t
i, j
|s
i
,d) =
?
k
P(z
i
= k, t
i, j
|z
?(d,i)
,s, t
?(d,i)
,d)
This is straightforward because during test in-
ference the variational posterior is normalised to
a distribution over topics and target phrases for
a given source phrase. If a source phrase oc-
curs multiple times in the same document, the
probabilities are averaged over all occurrences.
The inverse translation probabilities can be com-
puted analogously except that in cases where we
330
do not have variational posteriors for a given pair
of source and target phrases, an approximation is
needed. We omit the results here since our exper-
iments so far did not indicate improvements with
the inverse features included.
4 More topic-adapted features
Inspired by previous work on topic adaptation for
SMT, we add three additional topic-adapted fea-
tures to our model. All of these features make
use of the topic mixtures learned by our bilingual
topic model. The first feature is an adapted lexi-
cal weight, similar to the features in the work of
Eidelman et al. (2012). Our feature is different in
that we marginalise over topics to produce a single
adapted feature where v[k] is the k
th
element of a
document topic vector for document d and w(t|s,k)
is a topic-dependent word translation probability:
lex(
?
t|s?,d) =
|t|
?
i
1
{ j|(i, j) ? a}
?
?(i, j)?a
?
k
w(t|s,k) ? v[k]
? ?? ?
w(t|s)
(3)
The second feature is a target unigram feature
similar to the lazy MDI adaptation of Ruiz and
Federico (2012). It includes an additional term
that measures the relevance of a target word w
i
by
comparing its document-specific probability P
doc
to its probability under the asymmetric topic 0:
trgUnigrams
t
=
|t|
?
i=1
f (
P
doc
(w
i
)
P
baseline
(w
i
)
)
? ?? ?
lazy MDI
? f (
P
doc
(w
i
)
P
topic0
(w
i
)
)
? ?? ?
relevance
(4)
f (x) =
2
1+
1
x
, x > 0 (5)
The third feature is a document similarity fea-
ture, similar to the semantic feature described by
Banchs and Costa-juss? (2011):
docSim
t
= max
i
(1? JSD(v
train doc
i
,v
test doc
)) (6)
where v
train_doc
i
and v
test_doc
are document topic
vector of training and test documents. Because
topic 0 captures phrase pairs that are common to
many documents, we exclude it from the topic
vectors before computing similarities.
4.1 Feature combination
We tried integrating the four topic-adapted fea-
tures separately and in all possible combinations.
As we will see in the results section, while all fea-
tures improve over the baseline in isolation, the
adapted translation feature P(t|s,d) is the strongest
feature. For the features that have a counterpart in
the baseline model (p(t|s,d) and lex(t|s,d)), we ex-
perimented with either adding or replacing them in
Data Mixed CC NC TED
Train 354K (6450) 110K 103K 140K
Dev 2453 (39) 818 817 818
Test 5664 (112) 1892 1878 1894
Table 1: Number of sentence pairs and documents
(in brackets) in the French-English data sets. The
training data has 2.7M English words per domain.
the log-linear model. We found that while adding
the features worked well and yielded close to zero
weights for their baseline counterparts after tun-
ing, replacing them yielded better results in com-
bination with the other adapted features. We be-
lieve the reason could be that fewer phrase table
features in total are easier to optimise.
5 Experimental setup
5.1 Data and baselines
Our experiments were carried out on a mixed
data set, containing the TED corpus (Cettolo et
al., 2012), parts of the News Commentary cor-
pus (NC) and parts of the Commoncrawl corpus
(CC) from the WMT13 shared task (Bojar et al.,
2013) as described in Table 1. We were guided
by two constraints in chosing our data set. 1) the
data has document boundaries and the content of
each document is assumed to be topically related,
2) there is some degree of topical variation within
each data set. In order to compare to domain adap-
tation approaches, we chose a setup with data from
different corpora. We want to abstract away from
adaptation effects that concern tuning of length
penalties and language models, so we use a mixed
tuning set containing data from all three domains
and train one language model on the concatenation
of (equally sized) target sides of the training data.
Word alignments are trained on the concatenation
of all training data and fixed for all models.
Our baseline (ALL) is a phrase-based French-
English system trained on the concatenation of
all parallel data. It was built with the Moses
toolkit (Koehn et al., 2007) using the 14 standard
core features including a 5gram language model.
Translation quality is evaluated on a large test set,
using the average feature weights of three optimi-
sation runs with PRO (Hopkins and May, 2011).
We use the mteval-v13a.pl script to compute case-
insensitive BLEU. As domain-aware benchmark
systems, we use the phrase table fill-up method
(FILLUP) of Bisazza et al. (2011) which pre-
serves the translation scores of phrases from the
IN model and the linear mixture models (LIN-
TM) of Sennrich (2012b) (both available in the
Moses toolkit). For both systems, we build sepa-
rate phrase tables for each domain and use a wrap-
per to decode tuning and test sets with domain-
specific tables. Both benchmarks have an advan-
331
Model Mixed CC NC TED
IN 26.77 18.76 29.56 32.47
ALL 26.86 19.61 29.42 31.88
Table 2: BLEU of in-domain and baseline models.
Model Avg JSD Rank1-diff
Ted-IN vs ALL 0.15 10.8%
CC-IN vs ALL 0.17 18.4%
NC-IN vs ALL 0.13 13.3%
Table 3: Average JSD of IN vs. ALL models.
Rank1-diff: % PT entries where preferred transla-
tion changes.
tage over our model because they are aware of do-
main boundaries in the test set. Further, LIN-TM
adapts phrase table features in both translation di-
rections while we only adapt the forward features.
Table 2 shows BLEU scores of the baseline sys-
tem as well as the performance of three in-domain
models (IN) tuned under the same conditions. For
the IN models, every portion of the test set is de-
coded with a domain-specific model. Results on
the test set are broken down by domain but also
reported for the entire test set (mixed). For Ted
and NC, the in-domain models perform better than
ALL, while for CC the all-domain model improves
quite significantly over IN.
5.2 General properties of the data sets
In this section we analyse some internal properties
of our three data sets that are relevant for adapta-
tion. All of the scores were computed on the sets
of source side tokens of the test set which were
limited to contain content words (nouns, verbs, ad-
jectives and adverbs). The test set was tagged with
the French TreeTagger (Schmid, 1994). The top of
Table 3 shows the average Jensen-Shannon diver-
gence (using log
2
, JSD ? [0,1]) of each in-domain
model in comparison to the all-domain model,
which is an indicator of how much the distribu-
tions in the IN model change when adding out-of-
domain data. Likewise, Rank1-diff gives the per-
centage of word tokens in the test set where the
preferred translation according to p(e| f ) changes
between IN and ALL. These are the words that
are most affected by adding data to the IN model.
Both numbers show that for Commoncrawl the IN
and ALL models differ more than in the other two
data sets. According to the JS divergence between
NC-IN and ALL, translation distibutions in the NC
phrase table are most similar to the ALL phrase
table. Table 4 shows the average JSD for each IN
model compared to a model trained on half of its
in-domain data. This score gives an idea of how
diverse a data set is, measured by comparing dis-
tributions over translations for source words in the
test set. According to this score, Commoncrawl
is the most diverse data set and Ted the most uni-
Model Avg JSD
Ted-half vs Ted-full 0.07
CC-half vs CC-full 0.17
NC-half vs NC-full 0.09
Table 4: Average JSD of in-domain models
trained on half vs. all of the data.
form. Note however, that these divergence scores
do not provide information about the relative qual-
ity of the systems under comparison. For CC,
the ALL model yields a much higher BLEU score
than the IN model and it is likely that this is due to
noisy data in the CC corpus. In this case, the high
divergence is likely to mean that distributions are
corrected by out-of-domain data rather than being
shifted away from in-domain distributions.
5.3 Topic-dependent decoding
The phrase translation probabilities and additional
features described in the last two sections are used
as features in the log-linear translation model in
addition to the baseline translation features. When
combining all four adapted features, we replace
P(t|s) and lex(t|s) by their adapted counterparts.
We construct separate phrase tables for each doc-
ument in the development and test sets and use a
wrapper around the decoder to ensure that each in-
put document is paired with a configuration file
pointing to its document-specific translation table.
Documents are decoded in sequence so that only
one phrase table needs to be loaded at a time. Us-
ing the wrapped decoder we can run parameter op-
timisation (PRO) in the usual way to get one set of
tuned weights for all test documents.
6 Results
In this section we present experimental results
with phrasal LDA. We show BLEU scores in com-
parison to a baseline system and two domain-
aware benchmark systems. We also evaluate
the adapted translation distributions by looking at
translation probabilities under specific topics and
inspect translations of ambiguous source words.
6.1 Analyis of bilingual topic models
We experimented with different numbers of top-
ics for phrasal LDA. The diagrams in Figure 2
shows blocks of training and test documents in
each of the three domains for a model with 20 top-
ics. Darker shading means that documents have
a higher proportion of a particular topic in their
document-topic distribution. The first topic is the
one that was affected by the asymmetric prior and
inspecting its most probable phrase pairs showed
that it had ?collected? a large number of stop word
phrases. This explains why it is the topic that
is most shared across documents and domains.
332
Figure 2: Document-topic distributions for train-
ing (top) and test (bottom) documents, grouped by
domain and averaged into blocks for visualisation.
Topic 8 Topic 11
europ?enne? european crise? crisis
politiques? political taux? rate
politique? policy financi?re?financial
int?r?ts? interests mon?taire? monetary
Topic 14 Topic 19
h?tel? hotel web? web
plage? beach utiliser? use
situ?? located logiciel? software
chambres? bedrooms donn?es? data
Figure 3: Frequent phrase pairs in learned topics.
There is quite a clear horizontal separation be-
tween documents of different domains, for exam-
ple, topics 6, 8, 19 occur mostly in Ted, NC and
CC documents respectively. The overall structure
is very similar between training (top) and test (bot-
tom) documents, which shows that test inference
was successful in carrying over the information
learned on training documents. There is also some
degree of topic sharing across domains, for exam-
ple topics 4 and 15 occur in documents of all three
domains. Figure 3 shows examples of latent topics
found during inference on the training data. Topic
8 and 11 seem to be about politics and economy
and occur frequently in documents from the NC
corpus. Topic 14 contains phrases related to ho-
tels and topic 19 is about web and software, both
frequent themes in the CC corpus.
6.2 Comparison according to BLEU
In Table 5 we compare our topic-adapted features
when added separately to the baseline phrase ta-
ble. The inclusion of each feature improves over
the concatenation baseline but the combination
of all four features gives the best overall results.
Though the relative performance differs slightly
for each domain portion in the test set, overall the
adapted lexical weight is the weakest feature and
the adapted translation probability is the strongest
feature. We also performed feature ablation tests
and found that no combination of features was su-
perior to combining all four features. This con-
firms that the gains of each feature lead to additive
improvements in the combined model.
In Table 6 we compare topic-adapted models
Model Mixed CC NC TED
lex(e|f,d) 26.99 19.93 29.34 32.19
trgUnigrams 27.15 19.90 29.54 32.50
docSim 27.22 20.11 29.63 32.40
p(e|f,d) 27.31 20.23 29.52 32.58
All features 27.67 20.40 30.04 33.08
Table 5: BLEU scores of pLDA features (50 top-
ics), separately and combined.
Model Mixed CC NC TED
ALL -26.86 19.61 29.42 31.88
3 topics -26.95 19.83 29.46 32.02
5 topics *27.48 19.98 29.94 33.04
10 topics *27.65 20.34 29.99 33.14
20 topics *27.63 20.39 29.93 33.09
50 topics *27.67 20.40 30.04 33.08
100 topics *27.65 20.54 30.00 32.90
>ALL +0.81 +0.93 +0.62 +1.26
Table 6: BLEU scores of baseline and topic-
adapted systems (pLDA) with all 4 features and
largest improvements over baseline.
with varying numbers of topics to the concatena-
tion baseline. We see a consistent gain on all do-
mains when increasing the number of topics from
three to five and ten topics. This is evidence that
the number of domain labels is in fact smaller
than the number of underlying topics. The opti-
mal number of latent topics varies for each domain
and reflects our insights from section 5.2. The CC
domain was shown to be the most diverse and the
best performance on the CC portion of the test set
is achieved with 100 topics. Likewise, the TED
domain was shown to be least diverse and here
the best performance is achieved with only 10 top-
ics. The best performance on the entire test set is
achieved with 50 topics, which is also the optimal
number of topics for the NC domain. The bot-
ton row of the table indicates the relative improve-
ment of the best topic-adapted model per domain
over the ALL model. Using all four topic-adapted
features yields an improvement of 0.81 BLEU on
the mixed test set. The highest improvement on a
given domain is achieved for TED with an increase
of 1.26 BLEU. The smallest improvement is mea-
sured on the NC domain. This is in line with the
observation that distributions in the NC in-domain
table are most similar to the ALL table, therefore
we would expect the smallest improvement for do-
main or topic adaptation. We used bootstrap re-
sampling (Koehn, 2004) to measure significance
on the mixed test set and marked all statistically
significant results compared to the respective base-
lines with asterisk (*: p? 0.01).
To demonstrate the benefit of topic adaptation
over more standard domain adaptation approaches
for a diverse data set, we show the performance
333
Model Mixed CC NC TED
FILLUP -27.12 19.36 29.78 32.71
LIN-TM -27.24 19.61 29.87 32.73
pLDA *27.67 20.40 30.04 33.08
>FILLUP +0.55 +1.04 +0.26 +0.37
>LIN-TM +0.43 +0.79 +0.17 +0.35
Table 7: Comparison of best pLDA system with
two domain-aware benchmark systems.
Model Mixed CC NC TED
LIN-LM
+ ALL -27.16 19.71 29.77 32.46
+ FILLUP -27.20 19.37 29.84 32.90
+ LIN-TM -27.34 19.59 29.92 33.02
+ pLDA *27.84 20.48 30.03 33.57
>ALL +0.68 +0.77 +0.26 +1.11
Table 8: Combination of all models with addi-
tional LM adaptation (pLDA: 50 topics).
of two state-of-the-art domain-adapted systems in
Table 7. Both FILLUP and LIN-TM improve over
the ALL model on the mixed test set, by 0.26 and
0.38 BLEU respectively. The largest improvement
is on TED while on the CC domain, FILLUP de-
creases in performance and LIN-TM yields no im-
provement either. This shows that relying on in-
domain distributions for adaptation to a noisy and
diverse domain like CC is problematic. The pLDA
model yields the largest improvement over the
domain-adapted systems on the CC test set, with
in increase of 1.04 BLEU over FILLUP and 0.79
over LIN-TM. The improvements on the other two
domains are smaller but consistent.
We also compare the best model from Table 6
to all other models in combination with linearly
interpolated language models (LIN-LM), interpo-
lated separately for each domain. Though the
improvements are slightly smaller than without
adapted language models, there is still a gain over
the concatenation baseline of 0.68 BLEU on the
mixed test set and similar improvements to before
over the benchmarks (on TED the improvements
are actually even larger). Thus, we have shown
that topic-adaptation is effective for test sets of
diverse documents and that we can achieve sub-
stantial improvements even in comparison with
domain-adapted translation and language models.
6.3 Properties of adapted distributions and
topic-specific translations
The first column of Table 9 shows the average en-
tropy of phrase table entries in the adapted models
according to p(t|s,d) versus the all-domain model,
computed over source tokens in the test set that
are content words. The entropy decreases in the
adapted tables in all cases which is an indicator
that the distributions over translations of content
Set Model Avg entropy Avg perplexity
CC
pLDA 3.74 9.21
ALL 3.99 10.13
NC
pLDA 3.42 6.96
ALL 3.82 7.51
TED
pLDA 3.33 9.17
ALL 4.00 9.71
Table 9: Average entropy of translation distribu-
tions and test set perplexity of the adapted model.
r?gime
topic 6 diet = 0.79 diet aids = 0.04
topic 8 regime* = 0.82 rule = 0.05
topic 19 restrictions = 0.53 diplomats = 0.10
noyau
topic 9 nucleus* = 0.89 core = 0.01
topic 11 core* = 0.93 inner = 0.03
topic 19 kernel = 0.58 core = 0.11
d?mon
topic 6 devil = 0.89 demon = 0.07
topic 8 demon* = 0.98 devil = 0.01
topic 19 daemon = 0.95 demon = 0.04
Table 10: The two most probable translations of
r?gime, noyau and d?mon and probabilities under
different latent topics (*: preferred by ALL).
words have become more peaked. The second col-
umn shows the average perplexity of target tokens
in the test set which is a measure of how likely a
model is to produce words in the reference trans-
lation. We use the alignment information between
source and reference and therefore limit our anal-
ysis to pairs of aligned words, but nevertheless
this shows that the adapted translation distribu-
tions model the test set distributions better than the
baseline model. Therefore, the adapted distribu-
tions are not just more peaked but also more often
peaked towards the correct translation.
Table 10 shows examples of ambiguous French
words that have different preferred translations de-
pending on the latent topic. The word r?gime can
be translated as diet, regime and restrictions and
the model has learned that the probability over
translations changes when moving from one topic
to another (preferred translations under the ALL
model are marked with *). For example, the trans-
lation to diet is most probable under topic 6 and
the translation to regime which would occur in
a political context is most probable under topic
8. Topic 6 is most prominent among Ted docu-
ments while topic 8 is found most frequently in
News Commentary documents which have a high
percentage of politically related text. The French
word noyau can be translated to nucleus (physics),
core (generic) and kernel (IT) among other trans-
lations and the topics that exhibit these preferred
translations can be attributed to Ted (which con-
tains many talks about physics), NC and CC (with
334
Src: ?il suffit d??jecter le noyau et d?en ins?rer un autre, comme ce qu?on fait pour le cl?nage.?
BL: ?it is the nucleus eject and insert another, like what we do to the cl?nage.?
pLDA: ?he just eject the nucleus and insert another, like what we do to the cl?nage.? (nucleus = 0.77)
Ref: ?you can just pop out the nucleus and pop in another one, and that?s what you?ve all heard about with cloning.?
Src: ?pourtant ceci obligerait les contribuables des pays de ce noyau ? fournir du capital au sud?
BL: ?but this would force western taxpayers to provide the nucleus of capital in the south?
pLDA: ?but this would force western taxpayers to provide the core of capital in the south? (core = 0.78)
Ref: ?but this would unfairly force taxpayers in the core countries to provide capital to the south?
Src: ?le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.?
BL: ?the nucleus contains many drivers, in order to work for most users.?
pLDA: ?the kernel contains many drivers, to work for most users.? (kernel = 0.53)
Ref: ?the precompiled kernel includes a lot of drivers, in order to work for most users.?
Figure 4: pLDA correctly translates noyau in test docs from Ted, NC and CC (adapted probabilities in
brackets). The baseline (nucleus = 0.27, core = 0.27, kernel = 0.23) translates all instances to nucleus.
many IT-related documents). The last example,
d?mon, has three frequent translations in English:
devil, demon and daemon. The last translation
refers to a computer process and would occur in an
IT context. The topic-phrase probabilities reveal
that its mostly likely translation as daemon occurs
under topic 19 which clusters IT-related phrase
pairs and is frequent in the CC corpus. These
examples show that our model can disambiguate
phrase translations using latent topics.
As another motivating example, in Figure 4 we
compare the output of our adapted models to the
output produced by the all-domain baseline for the
word noyau from Table 10. While the ALL base-
line translates each instance of noyau to nucleus,
the adapted model translates each instance differ-
ently depending on the inferred topic mixtures for
each document and always matches the reference
translation. The probabilities in brackets show
that the chosen translations were indeed the most
likely under the respective adapted model. While
the ALL model has a flat distribution over pos-
sible translations, the adapted models are peaked
towards the correct translation. This shows that
topic-specific translation probabilities are neces-
sary when the translation of a word shifts between
topics or domains and that peaked, adapted distri-
butions can lead to more correct translations.
7 Related work
There has been a lot of previous work using topic
information for SMT, most of it using monolin-
gual topic models. For example, Gong and Zhou
(2011) use the topical relevance of a target phrase,
computed using a mapping between source and
target side topics, as an additional feature in de-
coding. Axelrod et al. (2012) build topic-specific
translation models from the TED corpus and se-
lect topic-relevant data from the UN corpus to im-
prove coverage. Su et al. (2012) perform phrase
table adaptation in a setting where only monolin-
gual in-domain data and parallel out-of-domain
data are available. Eidelman et al. (2012) use
topic-dependent lexical weights as features in the
translation model, which is similar to our work
in that topic features are tuned towards useful-
ness of topic information and not towards a tar-
get domain. Hewavitharana et al. (2013) per-
form dynamic adaptation with monolingual top-
ics, encoding topic similarity between a conversa-
tion and training documents in an additional fea-
ture. This is similar to the work of Banchs and
Costa-juss? (2011), both of which inspired our
document similarity feature. Also related is the
work of Sennrich (2012a) who explore mixture-
modelling on unsupervised clusters for domain
adaptation and Chen et al. (2013) who compute
phrase pair features from vector space representa-
tions that capture domain similarity to a develop-
ment set. Both are cross-domain adaptation ap-
proaches, though. Instances of multilingual topic
models outside the field of MT include Boyd-
Graber and Blei (2009; Boyd-Graber and Resnik
(2010) who learn cross-lingual topic correspon-
dences (but do not learn conditional distributions
like our model does). In terms of model structure,
our model is similar to BiTAM (Zhao and Xing,
2006) which is an LDA-style model to learn topic-
based word alignments. The work of Carpuat and
Wu (2007) is similar to ours in spirit, but they pre-
dict the most probable translation in a context at
the token level while our adaptation operates at the
type level of a document.
8 Conclusion
We have presented a novel bilingual topic model
based on LDA and applied it to the task of transla-
tion model adaptation on a diverse French-English
data set. Our model infers topic distributions over
phrase pairs to compute document-specific trans-
lation probabilities and performs dynamic adap-
tation on test documents of unknown origin. We
have shown that our model outperforms a concate-
nation baseline and two domain-adapted bench-
mark systems with BLEU gains of up to 1.26 on
domain-specific test set portions and 0.81 overall.
We have also shown that a combination of topic-
adapted features performs better than each feature
in isolation and that these gains are additive. An
analysis of the data revealed that topic adaptation
compares most favourably to domain adaptation
when the domain in question is rather diverse.
335
Acknowledgements
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance (Eva Hasler) and funding from the Eu-
ropean Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 287658
(EU BRIDGE) and grant agreement 288769 (AC-
CEPT). Thanks to Chris Dyer for an initial discus-
sion about the phrasal LDA model.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP. Association
for Computational Linguistics.
Amittai Axelrod, Xiaodong He, Li Deng, Alex Acero,
and Mei-Yuh Hwang. 2012. New methods and
evaluation experiments on translating TED talks in
the IWSLT benchmark. In Proceedings of ICASSP.
IEEE.
Rafael E. Banchs and Marta R. Costa-juss?. 2011. A
semantic feature for statistical machine translation.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
SSST-5. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of
IWSLT.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
JMLR.
Ond
?
rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of WMT 2013. Asso-
ciation for Computational Linguistics.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual Topic Models for Unaligned Text. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty
in Artificial Intelligence. AUAI Press.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
Sentiment Analysis Across Languages: Multilingual
Supervised Latent Dirichlet Allocation. In Proceed-
ings of EMNLP. Association for Computational Lin-
guistics.
Marine Carpuat and Dekai Wu. 2007. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for SMT. In International Conference
on Theoretical and Methodological Issues in MT.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of EAMT.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in SMT. In
Proceedings of ACL. Association for Computational
Linguistics.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of ACL. Associa-
tion for Computational Linguistics.
G. Foster and R. Kuhn. 2007. Mixture-model adapta-
tion for SMT. In Proceedings of WMT. Association
for Computational Linguistics.
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of EMNLP. Association for
Computational Linguistics.
Zhengxian Gong and Guodong Zhou. 2011. Employ-
ing topic modeling for SMT. In Proceedings of
IEEE (CSAE), volume 4.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse lexicalised features and topic adaptation for
SMT. In Proceedings of IWSLT.
S. Hewavitharana, D. Mehay, S. Ananthakrishnan, and
P. Natarajan. 2013. Incremental topic-based TM
adaptation for conversational SLT. In Proceedings
of ACL. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In ACL 2007: Demo and
poster sessions. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP. Association for Computational Linguis-
tics.
S. Matsoukas, A. Rosti, and B. Zhang. 2009. Discrim-
inative corpus weight estimation for MT. In Pro-
ceedings of EMNLP. Association for Computational
Linguistics.
Thomas P Minka. 2012. Estimating a Dirichlet distri-
bution. Technical report.
Nick Ruiz and Marcello Federico. 2012. MDI Adap-
tation for the Lazy: Avoiding Normalization in LM
Adaptation for Lecture Translation. In Proceedings
of IWSLT.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
336
Rico Sennrich. 2012a. Mixture-modeling with unsu-
pervised clusters for domain adaptation in SMT. In
Proceedings of EAMT.
Rico Sennrich. 2012b. Perplexity Minimization for
Translation Model Domain Adaptation in SMT. In
Proceedings of EACL. Association for Computa-
tional Linguistics.
J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong,
and Q. Liu. 2012. Translation model adaptation
for SMT with monolingual topic information. In
Proceedings of ACL. Association for Computational
Linguistics.
Yee Whye Teh, David Newman, and Max Welling.
2006. A collapsed variational Bayesian inference
algorithm for LDA. In Proceedings of NIPS.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking LDA: Why priors matter.
In Proceedings of NIPS.
Pengyu Wang and Phil Blunsom. 2013. Collapsed
variational Bayesian inference for Hidden Markov
Models. In AISTATS, volume 31 of JMLR Proceed-
ings, pages 599?607.
Bing Zhao and Eric P. Xing. 2006. Bilingual topic ad-
mixture models for word alignment. In Proceedings
of ACL. Association for Computational Linguistics.
337
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 238?241,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Inducing Synchronous Grammars with Slice Sampling
Phil Blunsom
Computing Laboratory
Oxford University
Phil.Blunsom@comlab.ox.ac.uk
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Abstract
This paper describes an efficient sampler for
synchronous grammar induction under a non-
parametric Bayesian prior. Inspired by ideas
from slice sampling, our sampler is able to
draw samples from the posterior distributions
of models for which the standard dynamic pro-
graming based sampler proves intractable on
non-trivial corpora. We compare our sampler
to a previously proposed Gibbs sampler and
demonstrate strong improvements in terms of
both training log-likelihood and performance
on an end-to-end translation evaluation.
1 Introduction
Intractable optimisation algorithms abound in much
of the recent work in Natural Language Process-
ing. In fact, there is an increasing acceptance that
solutions to many of the great challenges of NLP
(e.g. machine translation, summarisation, question
answering) will rest on the quality of approximate
inference. In this work we tackle this problem in
the context of inducing synchronous grammars for
a machine translation system. We concern ourselves
with the lack of a principled, and scalable, algo-
rithm for learning a synchronous context free gram-
mar (SCFG) from sentence-aligned parallel corpora.
The predominant approach for learning phrase-
based translation models (both finite state or syn-
chronous grammar based) uses a cascade of heuris-
tics beginning with predicted word alignments
and producing a weighted set of translation rules
(Koehn et al, 2003). Alternative approaches avoid
such heuristics, instead learning structured align-
ment models directly from sentence aligned data
(e.g., (Marcu and Wong, 2002; Cherry and Lin,
2007; DeNero et al, 2008; Blunsom et al, 2009)).
Although these models are theoretically attractive,
inference is intractable (at least O(|f |3|e|3)). The
efficacy of direct estimation of structured alignment
models therefore rests on the approximations used
to make inference practicable ? typically heuristic
constraints or Gibbs sampling. In this work we show
that naive Gibbs sampling (specifically, Blunsom et
al. (2009)) is ineffectual for inference and reliant on
a high quality initialisation, mixing very slowly and
being easily caught in modes. Instead, blocked sam-
pling over sentence pairs allows much faster mixing,
but done in the obvious way (following Johnson et al
(2007)) would incur a O(|f |3|e|3) time complexity.
Here we draw inspiration from the work of
Van Gael et al (2008) on inference in infinite hid-
den Markov models to develop a novel algorithm
for efficient sampling from a SCFG. We develop an
auxiliary variable ?slice? sampler which can dramati-
cally reduce inference complexity, and thereby make
blocked sampling practicable on real translation cor-
pora. Our evaluation demonstrates that our algorithm
mixes more quickly than the local Gibbs sampler, and
produces translation models which achieve state-of-
the-art BLEU scores without using GIZA++ or sym-
metrisation heuristics for initialisation.
We adopt the generative model of Blunsom et
al. (2009) which creates a parallel sentence pair
by a sequence (derivation) of SCFG productions
d = (r1, r2, ..., rn). The tokens in each language can
be read off the leaves of the derivation tree while
their order is defined hierarchically by the produc-
tions in use. The probability of a derivation is defined
as p(d|?) =
?
r?d ?r where ? are the model param-
eters which are drawn from a Bayesian prior. We
deviate from that models definition of the prior over
phrasal translations, instead adopting the hierarchical
Dirichlet process prior from DeNero et al (2008),
which incorporates IBM Model 1. Blunsom et al
(2009) describe a blocked sampler following John-
son et al (2007) which uses the Metropolis-Hastings
algorithm to correct proposal samples drawn from
an approximating SCFG, however this is discounted
as impractical due to the O(|f |3|e|3) complexity.
Instead a Gibbs sampler is used which samples local
updates to the derivation structure of each training
instance. This avoids the dynamic program of the
238
blocked sampler but at the expense of considerably
slower mixing.
Recently Bouchard-Co?te? et al (2009) proposed
an auxialliary variable sampler, possibly comple-
mentary to ours, which was also evaluated on syn-
chronous parsing. Rather than slice sampling deriva-
tions in a collapsed Bayesian model, this model
employed a secondary proposal model (IBM Mod-
els) and sampled expectations over rule parameters.
2 Slice Sampling a SCFG
It would be advantageous to explore a middle ground
where the scope of the dynamic program is limited to
high probability regions, reducing the running time
to an acceptable level. By employing the technique
of slice sampling (Neal, 2003) we describe an algo-
rithm which stochastically samples from a reduced
space of possible derivations, while ensuring that
these samples are drawn from the correct distribu-
tion. We apply the slice sampler to the approximating
SCFG parameterised by ?, which requires samples
from an inside chart p(d|?) (for brevity, we omit the
dependency on ? in the following).
Slice sampling is an example of auxiliary variable
sampling in which we make use of the fact that if
we can draw samples from a joint distribution, then
we can trivially obtain samples from the marginal
distributions: p(d) =
?
u p(d,u), where d is the
variable of interest and u is an auxiliary variable.
Using a Gibbs sampler we can draw samples from
this joint distribution by alternately sampling from
p(d|u) and p(u|d). The trick is to ensure that u is
defined such that drawing samples from p(d|u) is
more efficient than from p(d).
We define the variable u to contain a slice variable
us for every cell of a synchronous parse chart for
every training instance:1
S = {(i, j, x, y) | 0 ? i < j ? |f |, 0 ? x < y ? |e|}
u = {us ? R | 0 < us < 1, s ? S}
These slice variables act as cutoffs on the probabili-
ties of the rules considered in each cell s: rule appli-
cations rs with ?rs ? us will be pruned from the
dynamic program.2
1The dependence on training instances is omitted here and
subsequently for simplicity. Each instance is independent, and
therefore this formulation can be trivially applied to a set.
2Alternatively, we could naively sample from a pruned chart
using a fixed beam threshold. However, this would not produce
samples from p(d), but some other unknown distribution.
Sampling p(u|d) Unlike Van Gael et al (2008),
there is not a one-to-one correspondence between the
spans of the rules in d and the set S, rather the deriva-
tion?s rule spans form a subset of S . This compli-
cates our definition of p(u|d); we must provide sepa-
rate accounts of how each us is generated depending
on whether there is a corresponding rule for s, i.e.,
rs ? d. We define p(u|d) =
?
s p(us|d), where:
p(us|d) =
{
I(us<?rs )
?rs
, if rs ? d
?(us; a, b) , else
(1)
which mixes a uniform distribution and a Beta dis-
tribution3 depending on the existence of a rule rs
in the derivation d.4 Eq. 1 is constructed such that
only rules with probability greater than the rele-
vant threshold, {rs | ?rs > us}, could have feasibly
been part of a derivation resulting in auxiliary vari-
able u. This is critical in reasoning over the reverse
conditional p(d|u) which only has to consider the
reduced space of rules (formulation below in (4)).
Trivially, the conditioning derivation is recoverable,
?rs ? d, ?rs ? us. We parameterise the ? distribu-
tion in (1) with a heavy skew towards zero in order
to limit the amount of pruning and thereby include
many competing derivations.5
Sampling p(d|u) Recall the probability of a
derivation, p(d) =
?
rs?d ?rs . We draw samples
from the joint distribution, p(d,u), holding u fixed:
p(d|u) ? p(d,u) = p(d)? p(u|d)
=
?
?
?
rs?d
?rs
?
??
( ?
us:rs?d
I(us<?rs )
?rs
?
?
us:rs 6?d ?(us; a, b)
)
=
?
us:rs?d
I (us < ?rs)
?
us:rs 6?d
?(us; a, b) (2)
=
?
us:rs?d
I (us < ?rs)
?(us; a, b)
?
us
?(us; a, b) (3)
?
?
us:rs?d
I (us < ?rs)
?(us; a, b)
(4)
In step (2) we cancel the ?rs terms while in step (3)
we introduce ?(us; a, b) terms to the numerator and
denominator for us : rs ? d to simplify the range
3Any distribution defined over {x ? R | 0 ? x ? 1} may be
used in place of ?, however this may affect the efficiency of the
sampler.
4I(?) returns 1 if the condition is true and 0 otherwise.
5We experiment with a range of a < 1 while fixing b = 1.
239
System BLEU time(s) LLH
Moses (default settings) 47.3 ? ?
LB init. 36.5 ? -257.1
M1 init. 48.8 ? -153.4
M4 init. 49.1 ? -151.4
Gibbs LB init. 45.3 44 -135.4
Gibbs M1 init. 48.2 40 -120.5
Gibbs M4 init. (Blunsom et al, 2009) 49.6 44 -110.3
Slice (a=0.15, b=1) LB init. 47.3 180 -98.9
Slice (a=0.10, b=1) M1 init. 50.4 908 -89.4
Slice (a=0.15, b=1) M1 init. 49.9 144 -90.2
Slice (a=0.25, b=1) M1 init. 49.2 80 -95.6
Table 1: IWSLT Chinese to English translation.
of the second product. The last step (4) discards the
term
?
us ?(us; a, b) which is constant wrt d. The
net result is a formulation which factors with the
derivation structure, thereby eliminating the need to
consider allO(|e|2|f |2) spans in S. Critically p(d|u)
is zero for all spans failing the I (us < ?rs) condition.
To exploit the decomposition of Equation 4 we
require a parsing algorithm that only explores chart
cells whose child cells have not already been pruned
by the slice variables. The standard approach of using
synchronous CYK (Wu, 1997) doesn?t posses this
property: all chart cells would be visited even if they
are to be pruned. Instead we use an agenda based
parsing algorithm, in particular we extend the algo-
rithm of Klein and Manning (2004) to synchronous
parsing.6 Finally, we need a Metropolis-Hastings
acceptance step to account for intra-instance depen-
dencies (the ?rich-get-richer? effect). We omit the
details, save to state that the calculation cancels to
the same test as presented in Johnson et al (2007).7
3 Evaluation
In the following experiments we compare the slice
sampler and the Gibbs sampler (Blunsom et al,
2009), in terms of mixing and translation quality. We
measure mixing in terms of training log-likelihood
(LLH) after a fixed number of sampling iterations.
Translations are produced using Moses (Koehn et al,
2007), initialised with the word alignments from the
final sample, and are evaluated using BLEU(Papineni
et al, 2001). The slice sampled models are restricted
to learning binary branching one-to-one (or null)
alignments,8 while no restriction is placed on the
Gibbs sampler (both use the same model, so have
6Moreover, we only sample values for us as they are visited
by the parser, thus avoiding the quartic complexity.
7Acceptance rates averaged above 99%.
8This restriction is not strictly necessary, however it greatly
simplifies the implementation and increases efficiency.
comparable LLH). Of particular interest is how the
different samplers perform given initialisations of
varying quality. We evaluate three initialisers: M4:
the symmetrised output of GIZA++ factorised into
ITG form (as used in Blunsom et al (2009)); M1:
the output of a heavily pruned ITG parser using the
IBM Model 1 prior for the rule probabilities;9 and
LB: left-branching monotone derivations.10
We experiment with the Chinese?English trans-
lation task from IWSLT, as used in Blunsom et al
(2009).11 Figure 1 shows LLH curves for the sam-
plers initialised with the M1 and LB derivations, plus
the curve for Gibbs sampler with the M4 initialiser.12
Table 1 gives BLEU scores on Test-05 for phrase-
based translation models built from the 1500th sam-
ple for the various models along with the average
time per sample and their final log-likelihood.
4 Discussion
The results are particularly encouraging. The slice
sampler uniformly finds much better solutions than
the Gibbs sampler regardless of initialisation. In
particular, the slice sampled model initialised with
the naive LB structure achieves a higher likelihood
than the M4 initialised model, although this is not
reflected in their relative BLEU scores. In contrast the
Gibbs sampler is more significantly affected by its
initialisation, only deviating slightly before becom-
ing trapped in a mode, as seen in Fig. 1. With suf-
ficient (infinite) time both sampling strategies will
converge on the true posterior regardless of initiali-
sation, however the slice sampler appears to be con-
verging much faster than the Gibbs sampler.
Interestingly, the initialisation heuristics (M1 and
M4) outperform the default heuristics (Koehn et al,
2007) by a considerable margin. This is most likely
because the initialisation heuristics force the align-
ments to factorise with an ITG, resulting in more
aggressive pruning of spurious alignments which in
turn allows for more and larger phrase pairs.
9The following beam heuristics are employed: alignments to
null are only permitted on the longer sentence side; words are
only allowed to align to those whose relative sentence position
is within ?3 words.
10Words of the longer sentence are randomly assigned to null.
11We limit the maximum training sentence length to 40, result-
ing in ? 40k training sentences.
12The GIZA++ M4 alignments don?t readily factorise to
word-based ITG derivations, as such we haven?t produced results
for this initialiser using the slice sampler.
240
0 50 100 150 200 250?
140
?
130
?
120
?
110
?
100
?
90
Samples
Log?
likeli
hood
Slice (a=0.10 b=1) M1Slice (a=0.15 b=1) M1Slice (a=0.20 b=1) M1Slice (a=0.25 b=1) M1Gibbs M1Gibbs M4
0 200 400 600 800 1000
?
200
?
180
?
160
?
140
?
120
?
100
Samples
Log?
likeli
hood
Slice (a=0.15 b=1) M1Slice (a=0.15 b=1) LBGibbs M1Gibbs LBGibbs M4
Figure 1: Training log-likelihood as a function of sampling iteration for Gibbs and slice sampling.
While the LLHs for the slice sampled models and
their BLEU scores appear correlated, this doesn?t
extend to comparisons with the Gibbs sampled mod-
els. We believe that this is because the GIZA++
initialisation alignments also explain the data well,
while not necessarily obtaining a high LLH under
the ITG model. Solutions which score highly in one
model score poorly in the other, despite both produc-
ing good translations.
The slice sampler is slower than the local Gibbs
sampler, its speed depending on the parameterisation
of the Beta distribution (affecting the width of the
beam). In the extreme, exhaustive search using the
full dynamic program is intractable on current hard-
ware,13 and therefore we have achieved our aim of
mediating between local and blocked inference.
This investigation has established the promise
of the SCFG slice sampling technique to provide
a scalable inference algorithm for non-parametric
Bayesian models. With further development, this
work could provide the basis for a family of prin-
cipled inference algorithms for parsing models, both
monolingual and synchronous, and other models that
prove intractable for exact dynamic programming.
References
P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proc. ACL/IJCNLP, 782?790, Suntec,
Singapore. Association for Computational Linguistics.
A. Bouchard-Co?te?, S. Petrov, D. Klein. 2009. Ran-
domized pruning: Efficiently calculating expectations
13Our implementation had not completed a single sample after
a week.
in large dynamic programs. In Advances in Neural
Information Processing Systems 22, 144?152.
C. Cherry, D. Lin. 2007. Inversion transduction grammar
for joint phrasal translation modeling. In Proc. SSST,
Rochester, USA.
J. DeNero, A. Bouchard-Co?te?, D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. EMNLP, 314?323, Honolulu, Hawaii.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo.
In Proc. HLT-NAACL, 139?146, Rochester, New York.
D. Klein, C. D. Manning, 2004. Parsing and hypergraphs,
351?372. Kluwer Academic Publishers, Norwell, MA,
USA, 2004.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, 81?88,
Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst.
2007. Moses: Open source toolkit for statistical
machine translation. In Proc. ACL, Prague.
D. Marcu, W. Wong. 2002. A phrase-based, joint proba-
bility model for statistical machine translation. In Proc.
EMNLP, 133?139, Philadelphia.
R. Neal. 2003. Slice sampling. Annals of Statistics,
31:705?767.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu:
a method for automatic evaluation of machine trans-
lation. Technical Report RC22176 (W0109-022), IBM
Research Division, Thomas J. Watson Research Center,
2001.
J. Van Gael, Y. Saatci, Y. W. Teh, Z. Ghahramani. 2008.
Beam sampling for the infinite hidden markov model.
In ICML, 1088?1095, New York, NY, USA.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
241
Proceedings of NAACL-HLT 2013, pages 969?977,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Systematic Bayesian Treatment of the IBM Alignment Models
Yarin Gal
Department of Engineering
University of Cambridge
Cambridge, CB2 1PZ, United Kingdom
yg279@cam.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Phil.Blunsom@cs.ox.ac.uk
Abstract
The dominant yet ageing IBM and HMM
word alignment models underpin most
popular Statistical Machine Translation
implementations in use today. Though
beset by the limitations of implausible
independence assumptions, intractable
optimisation problems, and an excess of
tunable parameters, these models provide
a scalable and reliable starting point for
inducing translation systems. In this paper we
build upon this venerable base by recasting
these models in the non-parametric Bayesian
framework. By replacing the categorical
distributions at their core with hierarchical
Pitman-Yor processes, and through the use
of collapsed Gibbs sampling, we provide a
more flexible formulation and sidestep the
original heuristic optimisation techniques.
The resulting models are highly extendible,
naturally permitting the introduction of
phrasal dependencies. We present extensive
experimental results showing improvements
in both AER and BLEU when benchmarked
against Giza++, including significant
improvements over IBM model 4.
1 Introduction
The IBM and HMM word alignment models (Brown
et al, 1993; Vogel et al, 1996) have underpinned the
majority of statistical machine translation systems
for almost twenty years. The key attraction of these
models is their principled probabilistic formulation,
and the existence of (mostly) tractable algorithms
for their training.
The dominant Giza++ implementation of the
IBM models (Och and Ney, 2003) employs a
variety of exact and approximate EM algorithms
to optimise categorical alignment distributions.
While effective, this parametric approach results in
a significant number of parameters to be tuned and
intractable summations over the space of alignments
for models 3 and 4. Giza++ hides the hyper-
parameters with defaults and approximates the
intractable expectations using restricted alignment
neighbourhoods. However this approach was shown
to often return alignments with probabilities well
below the true maxima (Ravi and Knight, 2010).
To overcome perceived limitations with the word
based and non-syntactic nature of the IBM models
many alternative approaches to word alignment have
been proposed (e.g. (DeNero et al, 2008; Cohn and
Blunsom, 2009; Levenberg et al, 2012)). While
interesting results have been reported, these alterna-
tives have failed to dislodge the IBM approach.
In this paper we proposed to retain the original
generative stories of the IBM models, while
replacing the inflexible categorical distributions
with hierarchical Pitman-Yor (PY) processes ? a
mathematical tool which has been successfully
applied to a range of language tasks (Teh, 2006b;
Goldwater et al, 2006; Blunsom and Cohn,
2011). In the context of language modelling, the
hierarchical PY process was shown to roughly
correspond to interpolated Kneser-Ney (Kneser and
Ney, 1995; Teh, 2006a). The key contribution of
the hierarchical PY formulation is that it provides
a principle probabilistic framework that easily
extends to latent variable models, such as those used
969
for alignment, for which a Kneser-Ney formulation
is unclear. While Bayesian priors have previously
been applied to IBM model 1 (Riley and Gildea,
2012), in this work we go considerably further by
implementing non-parametric priors for the full
Giza++ training pipeline.
Inference for the proposed models and their
hyper-parameters is done with Gibbs sampling.
This eliminates the intractable summations
over alignments and the need for tuning hyper-
parameters. Further, we exploit the highly
extendible nature of the hierarchical PY process to
implement improvements to the original models
such as the introduction of phrasal dependencies.
We present extensive experimental results
showing improvements in both BLEU scores and
AER when compared to Giza++. The demonstrated
improvements over IBM model 4 suggest that the
heuristics used in the implementation of the EM
algorithm for this model were suboptimal.
We begin with a formal presentation of the hier-
archical PY process used in our Bayesian approach
to replace the original categorical distributions. Sec-
tion 3 introduces our Bayesian formulation of the
word alignment models, while its inference scheme
is presented in the following section. Finally, the
experimental results evaluating our models against
the originals are given in section 5, demonstrating
the superiority of the non-parametric approach.
2 The hierarchical PY process
Before giving the formal definition for the hierar-
chical Pitman-Yor (PY) process, we first give some
intuition into how this distribution works and why
it is commonly used to model problems in natural
language.
The hierarchical PY process is an atomic distri-
bution that can share its atoms between different
levels in a hierarchy. When used for language mod-
elling it captures the probability of observing a word
after any given sequence of n words. It does so by
interpolating the observed frequency of the whole
sequence followed by the word of interest, with the
observed frequency of a shorter sequence followed
by the word of interest. This interpolation is done in
such a way that tokens in a more specific distribution
are interpolated with types in a less specific one.
If there is sufficient evidence for the whole word
sequence, i.e. it is not sparse in the corpus, higher
weight will be given to the frequency of the word
of interest after the more specific sequence than
the shorter one. If the sequence was not observed
frequently and there is not enough information to
model whether the word of interest follows after it
frequently or not, the process will back-off to the
shorter sequence and assign higher weight to its fre-
quency instead. This is done in a recursive fashion,
decreasing the sequence length by one every time
until the probability is interpolated with the uniform
distribution, much like interpolated Kneser-Ney, the
state of the art for language modelling.
Unlike Kneser-Ney, the hierarchical PY approach
naturally extends to model complicated conditional
distributions involving latent variables. Moreover,
almost all instances of priors with categorical distri-
butions can be replaced by the PY process, where in
its most basic representation (with no conditional) it
provides a flexible model of power law frequencies.
The PY process generalises a number of simpler
distributions. The Dirichlet distribution is a distri-
bution over discrete probability mass functions of a
certain given length which is often used to model
prior beliefs on parameter sparsity in machine learn-
ing problems. The Dirichlet process generalises the
Dirichlet distribution to a distribution over infinite
sequences of non-negative reals that sum to one and
is often used for nonparametric Bayesian inference.
The PY process is used in the context of natural lan-
guage processing as it further generalises the Dirich-
let process by adding an additional degree of free-
dom that enables it to produce power-law discrete
probability mass functions that resemble those seen
experimentally in corpora (Goldwater et al, 2006).
Formally, draws from the PY process
G1 ? PY (d, ?,G0) with a discount parameter
0 ? d < 1, a strength parameter ? > ?d, and a base
distribution G0, are constructed using a Chinese
restaurant process analogy as follows:
Xn+1|X1, ..., Xn ?
K?
k=1
mk ? d
? + n
?yk +
? + dK
? + n
G0
Where mk denotes the number of Xis (customers)
assigned to yk (a table) and K is the total number of
yks drawn from G0.
970
Hierarchical PY processes (Teh, 2006b), PY
processes where the base distribution is itself a
PY process, were developed as an extension which
is often used in the context of natural language
processing due to their relationship to back-off
smoothing. Denoting a context of atoms u as
(wi?l, ..., wi?1), the hierarchical PY process is
defined using the above definition of the PY process
by:
wi ?Gu
Gu ?PY (d|u|, ?|u|, Gpi(u))
...
G(wi?1) ?PY (d1, ?1, G?)
G? ?PY (d0, ?0, G0)
where pi(u) = (wi?l+1, ..., wi?1) is the suffix of u,
|u| denotes the length of context u, and G0 is a base
distribution.
3 A Bayesian approach to word alignment
In this work we replace the categorical distributions
at the heart of statistical alignment models with PY
processes. We start by describing the revised models
for IBM model 1 and the HMM alignment model,
before continuing to the more advanced IBM mod-
els 3 and 4. Throughout this section, we assume
that the base distributions in our models (denoted
G0, H0, etc.) are uniform over all atoms, and omit
the strength and concentration parameters of the PY
process for clarity. We use subscripts to denote
the hierarchy, and lower-case superscripts to denote
a fixed condition (for example, Gm0 is the (uni-
form) base distribution that is determined uniquely
for each possible foreign sentence length m).
3.1 Model 1 and the HMM alignment model
The most basic word alignment model, IBM model
1, can be described using the following generative
process (Brown et al, 1993): Given an English sen-
tence E = e1, ..., el, first choose a length m for
the foreign sentence F . Next, choose a vector of
random word positions from the English sentence
A = a1, ..., am to be the alignment, and then for
each foreign word fi choose a translation from the
English word eai aligned to it by A. The existence
of a NULL word at the beginning of the English sen-
tence is assumed, a word to which spurious words in
the foreign sentence can align. From this generative
process the following probability model is derived:
P (F,A|E) = p(m|l)?
m?
i=1
p(ai)p(fi|eai)
Where p(ai) = 1l+1 is uniform over all alignments
and p(fi|eai) ? Categorical.
In our approach we model these distributions
using hierarchical PY processes instead of the
categorical distributions. Thus we place the
following assumptions on IBM model 1:
ai|m ? G
m
0
fi|eai ? Heai
Heai ? PY (H?)
H? ? PY (H0)
In this probability modelling we assume that the
alignment positions are determined using the uni-
form distribution, and that word translations are gen-
erated depending on the source word ? the probabil-
ity of translating to a specific foreign word depends
on the observed frequency of pairs of the foreign
word and the given source word. We back-off to
the frequencies of the foreign words when the source
word wasn?t observed frequently.
The HMM alignment model uses the Hidden
Markov Model to find word alignments. It treats the
translations of the words of the English sentence as
observables and the alignment positions as the latent
variables to be discovered. Its generative process
can be described in an abstract way as follows: we
determine the length of the foreign sentence and
then iterate over the words of the source sentence
emitting translations for each word to fill-in the
words in the foreign sentence from left to right.
The following probability model is derived for the
HMM alignment model (Vogel et al, 1996):
P (F,A|E) =
p(m|l)?
m?
i=1
p(ai|ai?1,m)? p(fi|eai)
For the HMM alignment model we replace
the categorical translation distribution p(fi|eai)
with the same one we used for model 1, and
971
replace the categorical distribution for the transition
p(ai|ai?1,m) with a hierarchical PY process with
a longer sequence of alignment positions in the
conditional:
ai|ai?1,m ? G
m
ai?1
Gmai?1 ? PY (G
m
? )
Gm? ? PY (G
m
0 )
We use a unique distribution for each foreign sen-
tence length, and condition the position on the pre-
vious alignment position, backing-off to the HMM?s
stationary distribution over alignment positions.
3.2 Models 3 and 4
IBM models 3 and 4 introduce the concept of a
word?s fertility, the number of foreign words that are
generated from a specific English word. These mod-
els can be described using the following generative
process. Given an English sentence E, first deter-
mine the length of the foreign sentence: for each
word in the English sentence ei choose a fertility,
denoted ?i. Every time a word is generated, an addi-
tional spurious word from the NULL word in the
English sentence can be generated with a fixed prob-
ability. After the foreign sentence length is deter-
mined translate each English word into its foreign
equivalent (including the NULL word) in the same
way as for model 1. Finally, non-spurious words
are rearranged into the final word positions and the
spurious words inserted into the empty positions. In
model 3 this is done with a zero order HMM, and in
model 4 with two first order HMMs. One controls
the distortion of the head of each English word (the
first foreign word generated from it) relative to the
centre (denoted here ) of the set of foreign words
generated from the previous English word, and the
other controls the distortion within the set itself by
conditioning the word position on the previous word
position.
For the probability model, we follow the notation
of Och and Ney (2003) and define the alignment as
a function from the source sentence positions i to
Bi ? {1, ...,m} where the Bi?s form a partition of
the set {1, ...,m}. The fertility of the English word
i is ?i = |Bi|, and we use Bi,k to refer to the kth
element of Bi in ascending order.
Using the above notation, the following probabil-
ity model is derived (Och and Ney, 2003):
P (F,A|E) =p(B0|B1, ..., Bl)?
l?
i=1
p(Bi|Bi?1, ei)
?
l?
i=0
?
j?Bi
p(fj |ei)
For model 3 the dependence on previous
alignment sets is ignored and the probability
p(Bi|Bi?1, ei) is modelled as
p(Bi|Bi?1, ei) = p(?i|ei)?i!
?
j?Bi
p(j|i,m),
whereas in model 4 it is modelled using two HMMs:
p(Bi|Bi?1, ei) =p(?i|ei)? p=1(Bi,1 ?(Bi?1)|?)
?
?i?
k=2
p>1(Bi,k ?Bi,k?1|?)
For both these models the spurious word genera-
tion is controlled by a binomial distribution:
p(B0|B1, ..., Bl) =
(
m? ?0
?0
)
(1? p0)
m?2?0p?01
1
?0!
for some parameters p0 and p1.
Replacing the categorical priors with hierarchical
PY process ones, we set the translation and fertility
probabilities p(?i|ei)
?
j?Bi
p(fj |ei) using a com-
mon prior that generates translation sequences:
(f1, ..., f?i)|ei ? Hei
Hei ? PY (H
FT
ei )
HFTei ((f
1, ..., f?i)) = HFei (?i)
?
j
HT(fj?1,ei)(f
j)
HFei ? PY (H
F
? )
HF? ? PY (H
F
0 )
HT(fj?1,ei) ? PY (H
T
ei)
HTei ? PY (H
T
? )
HT? ? PY (H
T
0 )
Here we used superscripts for the indexing of words
which do not have to occur sequentially in the sen-
tence. We generate sequences instead of individ-
ual words and fertilities, and fall-back onto these
only in sparse cases. For example, when aligning
the English sentence ?I don?t speak French? to its
972
French translation ?Je ne parle pas franc?ais?, the
word ?not? will generate the phrase (?ne?, ?pas?),
which will later on be distorted into its place around
the verb.
The distortion probability for model 3, p(j|i,m),
is modelled simply as depending on the position of
the source word i and its class:
j|(C(ei), i),m ? G
m
(C(ei),i)
Gm(C(ei),i) ? PY (G
m
i )
Gmi ? PY (G
m
? )
Gm? ? PY (G
m
0 )
where we back-off to the source word position and
then to the frequencies of the alignment positions.
As opposed to this simple mechanism, in the dis-
tortion probability for IBM model 4 there exist two
distinct probability distributions. The first probabil-
ity distribution p=1 controls the head distortion:
Bi,1 ?(Bi?1) | (C(ei), C(fBi,1)),m
? Gm(C(ei),C(fBi,1 ))
Gm(C(ei),C(fBi,1 ))
? PY (GmC(fBi,1 )
)
GmC(fBi,1 )
? PY (Gm? )
Gm? ? PY (G
m
0 )
In this probability modelling we model the jump
size itself, as depending on the word class for the
source word and the word class for the proposed
foreign word, backing-off to the proposed foreign
word class and then to the relative jump frequencies.
The second probability distribution p>1 controls
the distortion within the set of words:
Bi,j ?Bi,j?1|C(fBi,j ),m ? H
m
C(fBi,j )
HmC(fBi,j )
? PY (Hm? )
Hm? ? PY (H
m
0 )
Here we again model the jump size as depending
on the word class for the proposed foreign word,
backing-off to the relative jump frequencies.
Lastly, we add to this probability model a treat-
ment for fertility and translation of NULL words.
The fertility generation follows the idea of the orig-
inal model, where the number of spurious words is
determined by a binomial distribution created from
a set of Bernoulli experiments, each one performed
after the translation of a non-spurious word. We use
an indicator function I to signal whether a spuri-
ous word was generated after a non-spurious word
(I = 1) or not (I = 0).
I = 0, 1|l ? HNFl
HNFl ? PY (H
NF
? )
HNF? ? PY (H
NF
0 )
Then, the translation of spurious words is done in a
straightforward manner:
fi ? H
NT
?
HNT? ? PY (H
NT
0 )
4 Inference
The Gibbs sampling inference scheme together with
the Chinese Restaurant Franchise process (Teh and
Jordan, 2009) are used to induce alignments for a
parallel corpus. A set of restaurants S is constructed
and initialised either randomly or through a pipeline
of alignment results from simpler models, and then
at each iteration each alignment position is removed
from the restaurants and re-sampled, conditioned on
the rest of the alignment positions.
Denoting e, f ,a the sets of all source sen-
tences, their translations, and their corresponding
alignments in our corpus, and denoting E,F,A a
specific source sentence, its translation, and their
corresponding alignment, where ei is the i?th word
of the source sentence and fj , aj are the j?th word
in the foreign sentence and its alignment into the
source sentence, we sample a new value for aj using
the univariate conditional distribution:
P (aj = i|E,F,A?j , e?E , f?F ,a?A,S?aj )
? P (F, (A?j , aj = i)|E, e?E , f?F ,a?A,S?aj )
Where a minus sign in the subscript denotes the
structure without the mentioned element, and S?aj
denotes the configuration of the restaurants after
removing the alignment aj .
This univariate conditional distribution is propor-
tional to the probability assigned by the different
models to an alignment sequence, where the restau-
rants replace the counts of the alignment positions
973
1 1>H 1>H>3 1>H>3>426.0
26.5
27.0
27.5
28.0
28.5
29.0
29.5
BLE
U
Chinese -> English Pipeline
PY-IBMGiza++
Figure 1: BLEU scores of pipelined
Giza++ and pipelined PY-IBM trans-
lating from Chinese into English
1 1>H 1>H>3 1>H>3>413.50
13.75
14.00
14.25
14.50
14.75
BLE
U
English -> Chinese Pipeline
PY-IBMGiza++
Figure 2: BLEU scores of pipelined
Giza++ and pipelined PY-IBM trans-
lating from English into Chinese
1 1>H 1>H>3 1>H>3>432
33
34
35
36
37
38
39
40
AER
AER Pipeline
Giza++PY-IBM
Figure 3: AER of pipelined Giza++
and pipelined PY-IBM aligning
Chinese and English
in the prior. Maximum marginal decoding (Johnson
and Goldwater, 2009) can then be used to get the
MAP estimate of the probability distributions over
the alignment positions for each sentence from the
samples extracted from the Gibbs sampler.
In addition to sampling the alignments, we
also place a uniform Beta prior on the discount
parameters and a vague Gamma prior on the
strength parameters, and sample them using slice
sampling (Neal, 2003). The end result is that the
alignment models have no free parameters to tune.
5 Experimental results
In order to assess our PY process alignment
models (referred to as PY-IBM henceforth) several
experiments were carried out to benchmark them
against the original models (as implemented in
Giza++). We evaluated the BLEU scores (Papineni
et al, 2002) of translations from Chinese into
English and from English into Chinese, as well
as the alignment error rates (AER) of the induced
symmetrised alignments compared to a human
aligned dataset. Moses (Koehn et al, 2007) was
used for the training of the SMT system and
the symmetrisation (using the grow-diag-final
procedure), with MERT (Och, 2003) used for tuning
of the weights, and SRILM (Stolcke, 2002) to build
the language model (5-grams based). The corpus
used for training and evaluation was the Chinese
FBIS corpus. MT02 was used for tuning, and MT03
was used for evaluation. In each case we used
one reference sentence in Chinese and 4 reference
sentences in English.
Most translation systems rely on the Giza++ pack-
age in which the implementation of the original
models is done by combining them in a pipeline.
Model 1 and the HMM alignment model are run
sequentially each for 5 iterations; then models 3 and
4 are run sequentially for 3 iterations each. This
follows the observation of Och and Ney (2003) that
bootstrapping from previous results assists the fer-
tility algorithms find the best alignment neighbour-
hood in order to estimate the expectations.
We assessed the proposed models against the
original models in a pipeline experiment where
both systems were trained on a corpus starting
at model 1, and used the results of the previous
run to initialise the next one ? noting the BLEU
scores and AER at each step. The Gibbs samplers
for the pipelined PY-IBM models were run for 50
iterations for each model and started accumulating
samples after a burn-in period of 10 iterations,
each experiment was repeated three times and
the results averaged. As can be seen in figures
1 to 3, the pipelined PY-IBM models achieved
higher BLEU scores across all steps, with the
highest improvement of 1.6 percentage points in the
pipelined HMM alignment models when translating
974
HMM Model Model 420
21
22
23
24
25
26
27
28
BLE
U
Chinese -> English
PY-IBMGiza++ 10 iter.Giza++
Figure 4: BLEU scores of Giza++?s
and PY-IBM?s HMM model and
model 4 translating from Chinese into
English
HMM Model Model 410.5
11.0
11.5
12.0
12.5
13.0
13.5
14.0
BLE
U
English -> Chinese
PY-IBMGiza++ 10 iter.Giza++
Figure 5: BLEU scores of Giza++?s
and PY-IBM?s HMM model and
model 4 translating from English into
Chinese
HMM Model Model 435
40
45
50
55
60
65
AER
AER
Giza++Giza++ 10 iter.PY-IBM
Figure 6: AER of Giza++?s and PY-
IBM?s HMM model and model 4
aligning Chinese and English
0 20 40 60 80 100 120 140 160iteration (after burn-in)
0.2
0.4
0.6
0.8
1.0
1.2
1.4
# o
f ali
gnm
ent
 pos
ition
 dis
agr
eem
ent
s 1e6
Alignment Disagreement pipeline zh->en
Figure 7: Alignment disagreement of the Chinese to
English pipelined PY-IBM models for the 3 repetitions
from Chinese into English, and an improvement
of 1.2 percentage points in the overall results after
finishing the pipeline.
We also saw an improvement in AER for all
models, where the pipelined PY-IBM model 4
achieved an error rate of 32.9, as opposed to the
result obtained by the Giza++ pipelined model 4
of 34.4. We note an interesting observation that
both Giza++ and PY-IBM model 3 underperformed
compared to the previously run HMM alignment
model, as seen in the English to Chinese pipeline
results and the AER pipeline results.
The alignment disagreement (the number of
changed alignment positions between subsequent
iterations) of the Chinese to English pipelined
PY-IBM models (1 to 4) can be seen in fig. 7. This
graph shows that each model in the pipeline reaches
an alignment disagreement equilibrium after about
20 iterations, and that earlier models have greater
initial deviation from their equilibrium than later
models ? which have an overall lower disagreement.
In order to assess the dependence of the fertil-
ity based models on the initialisation step another
set of experiments was carried out. The models
were trained with a randomly initialised set of align-
ments and assessed after a set number of iterations
for the Giza++ models (5 and 10 for the Giza++
HMM alignment model, and 3 and 10 for the Giza++
IBM model 4), or after 100 iterations with a burn-
in period of 10 iterations for the PY-IBM ones (we
report the average of three runs for both models).
The results, reported in figures 4 to 6, show again
that the PY-IBM model outperformed the Giza++
implementations, and to a large extent in the case
of IBM model 4. This provides further evidence
that the supposition underlying the neighbourhood
975
0 10 20 30 40 50 60 70 80 90iteration (after burn-in)
2.5
3.0
3.5
4.0
4.5
5.0
# o
f ali
gnm
ent
 pos
ition
 dis
agr
eem
ent
s 1e5
Alignment Disagreement Model 4 zh->en
Figure 8: Alignment disagreement of the Chinese to
English PY-IBM model 4 for the 3 repetitions
approximation for training models 3 and 4 ? that
there exists a small set of alignments on which most
of the probability mass concentrates ? is poor. An
interesting observation to note is that the BLEU
score of the non-pipelined PY-IBM model 4 is the
same as the PY-IBM HMM model translating in
both directions, as opposed to an improvement in
the pipelined case. This suggests that the sampler
might not have fully converged after 100 iterations
for model 4 (the number of alignment disagreements
for this experiment can be seen in figure 8). Further
confirmation for this comes from the higher standard
deviation of 0.54 observed for the PY-IBM model 4,
as opposed to a standard deviation for the PY-IBM
HMM model of 0.21 (which is still more significant
than that of the pipelined PY-IBM model 4, whose
standard deviation was 0.13).
Both the PY-IBM and the Giza++ trained mod-
els run in a linear time in the number of sentences,
where due to the nature of MCMC sampling tech-
niques, more iterations are required for its conver-
gence. In our experiments, the running time of
the unoptimised Gibbs sampler was 50 times slower
than the optimised EM.
6 Discussion
The models described in this paper allow one to
use non-parametric approaches to flexibly model
word alignment distributions, overcoming a number
of limitations of the EM algorithm for the fertility
based alignment models. The models achieved a
significant improvement in BLEU scores and AER
on the tested corpus, and are easy to extend without
the need for additional modelling tools.
The alignment models proposed mostly follow the
original generative stories while introducing addi-
tional phrasal conditioning into models 3 and 4.
However there are many other areas in which we
could make use of hierarchical tools to introduce
new dependencies easily without running into spar-
sity problems.
One example is the extension of the transition
history used in the HMM alignment model: IBM
model 1 uses a uniform distribution over transitions,
model 2 conditions on relative sentence positions,
and the HMM model uses a first order dependency.
One extension would be to use longer histories of n
previous positions, handling sparsity with back-off.
Previously proposed approaches to extend the
HMM alignment model include Och and Ney
(2003)?s use of word classes and smoothing, and
the combination of part-of-speech information of
the words surrounding the source word (Brunning
et al, 2009). Using our hierarchical model one
could easily introduce such dependencies on the
context words of the word to be translated and their
part-of-speech information. This could assist in
both translation and reordering disambiguation, and
would incorporate back-off by using smaller and
smaller contexts when such information is sparse.
Further improvements to models 3 and 4 could
be carried out by introducing longer dependencies
in the fertility and distortion distributions. Instead
of conditioning on the previous word, one could
use further information such as PoS tags, previously
translated words, or previous fertilities. Additional
research would involve the use of more effective
variational inference algorithms for hierarchical PY
process based models.
The PY-IBM models described in this paper were
implemented within the Giza++ code base, and
are available as an open source package for further
development and research.1
References
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
1Available at github.com/yaringal/Giza-sharp
976
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Jamie Brunning, Adria` de Gispert, and William Byrne.
2009. Context-dependent alignment models for statis-
tical machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, NAACL ?09, pages 110?
118.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
352?361, Singapore, August. Association for Compu-
tational Linguistics.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314?323, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466.
MIT Press, Cambridge, MA.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 317?325.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Acous-
tics, Speech, and Signal Processing, IEEE Interna-
tional Conference on, 1:181?184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
of the 45th Annual Meeting of the ACL (ACL-2007),
Prague.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 223?232, Jeju Island, Korea,
July. Association for Computational Linguistics.
Radford Neal. 2003. Slice sampling. Annals of Statis-
tics, 31:705?767.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the ACL and 3rd Annual Meeting of
the NAACL (ACL-2002), pages 311?318, Philadelphia,
Pennsylvania.
Sujith Ravi and Kevin Knight. 2010. Does Giza++ make
search errors? Computational Linguistics, 36(3):295?
302, September.
Darcey Riley and Daniel Gildea. 2012. Improving the
ibm alignment models using variational bayes. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Short Papers - Volume
2, ACL ?12, pages 306?310.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. of the International Conference
on Spoken Language Processing.
Y. W. Teh and M. I. Jordan, 2009. Hierarchical Bayesian
Nonparametric Models with Applications. Cambridge
University Press.
Yee Whye Teh. 2006a. A Bayesian interpretation of
interpolated Kneser-Ney. Technical report, National
University of Singapore School of Computing.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL-44, pages
985?992, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Stephen Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. of the 16th International Conference
on Computational Linguistics (COLING ?96), pages
836?841, Copenhagen, Denmark, August.
977
Proceedings of the ACL 2010 Conference Short Papers, pages 225?230,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Blocked Inference in Bayesian Tree Substitution Grammars
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Phil Blunsom
Computing Laboratory
University of Oxford
Phil.Blunsom@comlab.ox.ac.uk
Abstract
Learning a tree substitution grammar is
very challenging due to derivational am-
biguity. Our recent approach used a
Bayesian non-parametric model to induce
good derivations from treebanked input
(Cohn et al, 2009), biasing towards small
grammars composed of small generalis-
able productions. In this paper we present
a novel training method for the model us-
ing a blocked Metropolis-Hastings sam-
pler in place of the previous method?s lo-
cal Gibbs sampler. The blocked sam-
pler makes considerably larger moves than
the local sampler and consequently con-
verges in less time. A core component
of the algorithm is a grammar transforma-
tion which represents an infinite tree sub-
stitution grammar in a finite context free
grammar. This enables efficient blocked
inference for training and also improves
the parsing algorithm. Both algorithms are
shown to improve parsing accuracy.
1 Introduction
Tree Substitution Grammar (TSG) is a compelling
grammar formalism which allows nonterminal
rewrites in the form of trees, thereby enabling
the modelling of complex linguistic phenomena
such as argument frames, lexical agreement and
idiomatic phrases. A fundamental problem with
TSGs is that they are difficult to estimate, even in
the supervised scenario where treebanked data is
available. This is because treebanks are typically
not annotated with their TSG derivations (how to
decompose a tree into elementary tree fragments);
instead the derivation needs to be inferred.
In recent work we proposed a TSG model which
infers an optimal decomposition under a non-
parametric Bayesian prior (Cohn et al, 2009).
This used a Gibbs sampler for training, which re-
peatedly samples for every node in every training
tree a binary value indicating whether the node is
or is not a substitution point in the tree?s deriva-
tion. Aggregated over the whole corpus, these val-
ues and the underlying trees specify the weighted
grammar. Local Gibbs samplers, although con-
ceptually simple, suffer from slow convergence
(a.k.a. poor mixing). The sampler can get easily
stuck because many locally improbable decisions
are required to escape from a locally optimal solu-
tion. This problem manifests itself both locally to
a sentence and globally over the training sample.
The net result is a sampler that is non-convergent,
overly dependent on its initialisation and cannot be
said to be sampling from the posterior.
In this paper we present a blocked Metropolis-
Hasting sampler for learning a TSG, similar to
Johnson et al (2007). The sampler jointly updates
all the substitution variables in a tree, making
much larger moves than the local single-variable
sampler. A critical issue when developing a
Metroplis-Hastings sampler is choosing a suitable
proposal distribution, which must have the same
support as the true distribution. For our model the
natural proposal distribution is a MAP point esti-
mate, however this cannot be represented directly
as it is infinitely large. To solve this problem we
develop a grammar transformation which can suc-
cinctly represent an infinite TSG in an equivalent
finite Context Free Grammar (CFG). The trans-
formed grammar can be used as a proposal dis-
tribution, from which samples can be drawn in
polynomial time. Empirically, the blocked sam-
pler converges in fewer iterations and in less time
than the local Gibbs sampler. In addition, we also
show how the transformed grammar can be used
for parsing, which yields theoretical and empiri-
cal improvements over our previous method which
truncated the grammar.
225
2 Background
A Tree Substitution Grammar (TSG; Bod et
al. (2003)) is a 4-tuple, G = (T,N, S,R), where
T is a set of terminal symbols, N is a set of non-
terminal symbols, S ? N is the distinguished root
nonterminal and R is a set of productions (rules).
The productions take the form of tree fragments,
called elementary trees (ETs), in which each in-
ternal node is labelled with a nonterminal and each
leaf is labelled with either a terminal or a nonter-
minal. The frontier nonterminal nodes in each ET
form the sites into which other ETs can be substi-
tuted. A derivation creates a tree by recursive sub-
stitution starting with the root symbol and finish-
ing when there are no remaining frontier nonter-
minals. Figure 1 (left) shows an example deriva-
tion where the arrows denote substitution. A Prob-
abilistic Tree Substitution Grammar (PTSG) as-
signs a probability to each rule in the grammar,
where each production is assumed to be condi-
tionally independent given its root nonterminal. A
derivation?s probability is the product of the prob-
abilities of the rules therein.
In this work we employ the same non-
parametric TSG model as Cohn et al (2009),
which we now summarise. The inference prob-
lem within this model is to identify the posterior
distribution of the elementary trees e given whole
trees t. The model is characterised by the use of
a Dirichlet Process (DP) prior over the grammar.
We define the distribution over elementary trees e
with root nonterminal symbol c as
Gc|?c, P0 ? DP(?c, P0(?|c))
e|c ? Gc
where P0(?|c) (the base distribution) is a distribu-
tion over the infinite space of trees rooted with c,
and ?c (the concentration parameter) controls the
model?s tendency towards either reusing elemen-
tary trees or creating novel ones as each training
instance is encountered.
Rather than representing the distribution Gc ex-
plicitly, we integrate over all possible values of
Gc. The key result required for inference is that
the conditional distribution of ei, given e?i,=
e1 . . . en\ei and the root category c is:
p(ei|e?i, c, ?c, P0)=
n?iei,c
n?i?,c + ?c
+
?cP0(ei|c)
n?i?,c + ?c
(1)
where n?iei,c is the number number of times ei has
been used to rewrite c in e?i, and n?i?,c =
?
e n
?i
e,c
S
NP
NP
George
VP
V
hates
NP
NP
broccoli
S
NP,1
George
VP,0
V,0
hates
NP,1
broccoli
Figure 1: TSG derivation and its corresponding Gibbs state
for the local sampler, where each node is marked with a bi-
nary variable denoting whether it is a substitution site.
is the total count of rewriting c. Henceforth we
omit the ?i sub-/super-script for brevity.
A primary consideration is the definition of P0.
Each ei can be generated in one of two ways:
by drawing from the base distribution, where the
probability of any particular tree is proportional to
?cP0(ei|c), or by drawing from a cache of previ-
ous expansions of c, where the probability of any
particular expansion is proportional to the number
of times that expansion has been used before. In
Cohn et al (2009) we presented base distributions
that favour small elementary trees which we ex-
pect will generalise well to unseen data. In this
work we show that if P0 is chosen such that it
decomposes with the CFG rules contained within
each elementary tree,1 then we can use a novel dy-
namic programming algorithm to sample deriva-
tions without ever enumerating all the elementary
trees in the grammar.
The model was trained using a local Gibbs sam-
pler (Geman and Geman, 1984), a Markov chain
Monte Carlo (MCMC) method in which random
variables are repeatedly sampled conditioned on
the values of all other random variables in the
model. To formulate the local sampler, we asso-
ciate a binary variable with each non-root inter-
nal node of each tree in the training set, indicat-
ing whether that node is a substitution point or
not (illustrated in Figure 1). The sampler then vis-
its each node in a random schedule and resamples
that node?s substitution variable, where the proba-
bility of the two different configurations are given
by (1). Parsing was performed using a Metropolis-
Hastings sampler to draw derivation samples for
a string, from which the best tree was recovered.
However the sampler used for parsing was biased
1Both choices of base distribution in Cohn et al (2009)
decompose into CFG rules. In this paper we focus on the
better performing one, PC0 , which combines a PCFG applied
recursively with a stopping probability, s, at each node.
226
because it used as its proposal distribution a trun-
cated grammar which excluded all but a handful
of the unseen elementary trees. Consequently the
proposal had smaller support than the true model,
voiding the MCMC convergence proofs.
3 Grammar Transformation
We now present a blocked sampler using the
Metropolis-Hastings (MH) algorithm to perform
sentence-level inference, based on the work of
Johnson et al (2007) who presented a MH sampler
for a Bayesian PCFG. This approach repeats the
following steps for each sentence in the training
set: 1) run the inside algorithm (Lari and Young,
1990) to calculate marginal expansion probabil-
ities under a MAP approximation, 2) sample an
analysis top-down and 3) accept or reject using a
Metropolis-Hastings (MH) test to correct for dif-
ferences between the MAP proposal and the true
model. Though our model is similar to John-
son et al (2007)?s, we have an added complica-
tion: the MAP grammar cannot be estimated di-
rectly. This is a consequence of the base distri-
bution having infinite support (assigning non-zero
probability to infinitely many unseen tree frag-
ments), which means the MAP has an infinite rule
set. For example, if our base distribution licences
the CFG production NP? NP PP then our TSG
grammar will contain the infinite set of elemen-
tary trees NP? NP PP, NP? (NP NP PP) PP,
NP? (NP (NP NP PP) PP) PP, . . . with decreas-
ing but non-zero probability.
However, we can represent the infinite MAP us-
ing a grammar transformation inspired by Good-
man (2003), which represents the MAP TSG in an
equivalent finite PCFG.2 Under the transformed
PCFG inference is efficient, allowing its use as
the proposal distribution in a blocked MH sam-
pler. We represent the MAP using the grammar
transformation in Table 1 which separates the ne,c
and P0 terms in (1) into two separate CFGs, A and
B. Grammar A has productions for every ET with
ne,c ? 1 which are assigned unsmoothed proba-
bilities: omitting the P0 term from (1).3 Grammar
B has productions for every CFG production li-
censed under P0; its productions are denoted using
2Backoff DOP uses a similar packed representation to en-
code the set of smaller subtrees for a given elementary tree
(Sima?an and Buratto, 2003), which are used to smooth its
probability estimate.
3The transform assumes inside inference. For Viterbi re-
place the probability for c? sign(e) with
n?e,c+?cP0(e|c)
n??,c+?c
.
For every ET, e, rewriting c with non-zero count:
c? sign(e)
n?e,c
n??,c+?c
For every internal node ei in e with children ei,1, . . . , ei,n
sign(ei)? sign(ei,1) . . . sign(ei,n) 1
For every nonterminal, c:
c? c? ?c
n??,c+?c
For every pre-terminal CFG production, c? t:
c? ? t PCFG(c? t)
For every unary CFG production, c? a:
c? ? a PCFG(c? a)sa
c? ? a? PCFG(c? a)(1? sa)
For every binary CFG production, c? ab:
c? ? ab PCFG(c? ab)sasb
c? ? ab? PCFG(c? ab)sa(1? sb)
c? ? a?b PCFG(c? ab)(1? sa)sb
c? ? a?b? PCFG(c? ab)(1? sa)(1? sb)
Table 1: Grammar transformation rules to map a MAP TSG
into a CFG. Production probabilities are shown to the right of
each rule. The sign(e) function creates a unique string sig-
nature for an ET e (where the signature of a frontier node is
itself) and sc is the Bernoulli probability of c being a substi-
tution variable (and stopping the P0 recursion).
primed (?) nonterminals. The rule c ? c? bridges
from A to B, weighted by the smoothing term
excluding P0, which is computed recursively via
child productions. The remaining rules in gram-
mar B correspond to every CFG production in the
underlying PCFG base distribution, coupled with
the binary decision whether or not nonterminal
children should be substitution sites (frontier non-
terminals). This choice affects the rule probability
by including a s or 1 ? s factor, and child sub-
stitution sites also function as a bridge back from
grammar B to A. In this way there are often two
equivalent paths to reach the same chart cell using
the same elementary tree ? via grammar A using
observed TSG productions and via grammar B us-
ing P0 backoff; summing these yields the desired
net probability.
Figure 2 shows an example of the transforma-
tion of an elementary tree with non-zero count,
ne,c ? 1, into the two types of CFG rules. Both
parts are capable of parsing the string NP, saw, NP
into a S, as illustrated in Figure 3; summing the
probability of both analyses gives the model prob-
ability from (1). Note that although the probabili-
ties exactly match the true model for a single ele-
mentary tree, the probability of derivations com-
posed of many elementary trees may not match
because the model?s caching behaviour has been
suppressed, i.e., the counts, n, are not incremented
during the course of a derivation.
For training we define the MH sampler as fol-
lows. First we estimate the MAP grammar over
227
S? NP VP{V{saw},NP}
n?e,S
n??,S+?S
VP{V{saw},NP} ? V{saw} NP 1
V{saw} ? saw 1
S? S? ?S
n??,S+?S
S?? NP VP? PCFG(S? NP VP)sNP (1? sV P )
VP?? V? NP PCFG(VP? V NP)(1? sV )sNP
V?? saw PCFG(V? saw)
Figure 2: Example of the transformed grammar for the ET
(S NP (VP (V saw) NP)). Taking the product of the rule
scores above the line yields the left term in (1), and the prod-
uct of the scores below the line yields the right term.
S
S{NP,{VP{V{hates}},NP}}
NP
George
VP{V{hates}},NP
V{hates}
hates
NP
broccoli
S
S?
NP
George
VP?
V?
hates
NP
broccoli
Figure 3: Example trees under the grammar transform, which
both encode the same TSG derivation from Figure 1. The left
tree encodes that the S? NP (VP (V hates) NP elementary
tree was drawn from the cache, while for the right tree this
same elementary tree was drawn from the base distribution
(the left and right terms in (1), respectively).
the derivations of training corpus excluding the
current tree, which we represent using the PCFG
transformation. The next step is to sample deriva-
tions for a given tree, for which we use a con-
strained variant of the inside algorithm (Lari and
Young, 1990). We must ensure that the TSG
derivation produces the given tree, and therefore
during inside inference we only consider spans
that are constituents in the tree and are labelled
with the correct nonterminal. Nonterminals are
said to match their primed and signed counter-
parts, e.g., NP? and NP{DT,NN{car}} both match
NP. Under the tree constraints the time complex-
ity of inside inference is linear in the length of the
sentence. A derivation is then sampled from the
inside chart using a top-down traversal (Johnson
et al, 2007), and converted back into its equiva-
lent TSG derivation. The derivation is scored with
the true model and accepted or rejected using the
MH test; accepted samples then replace the cur-
rent derivation for the tree, and rejected samples
leave the previous derivation unchanged. These
steps are then repeated for another tree in the train-
ing set, and the process is then repeated over the
full training set many times.
Parsing The grammar transform is not only use-
ful for training, but also for parsing. To parse a
sentence we sample a number of TSG derivations
from the MAP which are then accepted or rejected
into the full model using a MH step. The samples
are obtained from the same transformed grammar
but adapting the algorithm for an unsupervised set-
ting where parse trees are not available. For this
we use the standard inside algorithm applied to
the sentence, omitting the tree constraints, which
has time complexity cubic in the length of the sen-
tence. We then sample a derivation from the in-
side chart and perform the MH acceptance test.
This setup is theoretically more appealing than our
previous approach in which we truncated the ap-
proximation grammar to exclude most of the zero
count rules (Cohn et al, 2009). We found that
both the maximum probability derivation and tree
were considerably worse than a tree constructed
to maximise the expected number of correct CFG
rules (MER), based on Goodman?s (2003) algo-
rithm for maximising labelled recall. For this rea-
son we the MER parsing algorithm using sampled
Monte Carlo estimates for the marginals over CFG
rules at each sentence span.
4 Experiments
We tested our model on the Penn treebank using
the same data setup as Cohn et al (2009). Specifi-
cally, we used only section 2 for training and sec-
tion 22 (devel) for reporting results. Our models
were all sampled for 5k iterations with hyperpa-
rameter inference for ?c and sc ? c ? N , but in
contrast to our previous approach we did not use
annealing which we did not find to help general-
isation accuracy. The MH acceptance rates were
in excess of 99% across both training and parsing.
All results are averages over three runs.
For training the blocked MH sampler exhibits
faster convergence than the local Gibbs sam-
pler, as shown in Figure 4. Irrespective of the
initialisation the blocked sampler finds higher
likelihood states in many fewer iterations (the
same trend continues until iteration 5k). To be
fair, the blocked sampler is slower per iteration
(roughly 50% worse) due to the higher overheads
of the grammar transform and performing dy-
namic programming (despite nominal optimisa-
tion).4 Even after accounting for the time differ-
4The speed difference diminishes with corpus size: on
sections 2?22 the blocked sampler is only 19% slower per
228
0 100 200 300 400 500
?
330
000
?
325
000
?
320
000
?
315
000
?
310
000
?
305
000
iteration
log l
ikelih
ood
Block maximal initBlock minimal initLocal minimal initLocal maximal init
Figure 4: Training likelihood vs. iteration. Each sampling
method was initialised with both minimal and maximal ele-
mentary trees.
Training truncated transform
Local minimal init 77.63 77.98
Local maximal init 77.19 77.71
Blocked minimal init 77.98 78.40
Blocked maximal init 77.67 78.24
Table 2: Development F1 scores using the truncated pars-
ing algorithm and the novel grammar transform algorithm for
four different training configurations.
ence the blocked sampler is more effective than the
local Gibbs sampler. Training likelihood is highly
correlated with generalisation F1 (Pearson?s cor-
relation efficient of 0.95), and therefore improving
the sampler convergence will have immediate ef-
fects on performance.
Parsing results are shown in Table 2.5 The
blocked sampler results in better generalisation F1
scores than the local Gibbs sampler, irrespective of
the initialisation condition or parsing method used.
The use of the grammar transform in parsing also
yields better scores irrespective of the underlying
model. Together these results strongly advocate
the use of the grammar transform for inference in
infinite TSGs.
We also trained the model on the standard Penn
treebank training set (sections 2?21). We ini-
tialised the model with the final sample from a
run on the small training set, and used the blocked
sampler for 6500 iterations. Averaged over three
runs, the test F1 (section 23) was 85.3 an improve-
iteration than the local sampler.
5Our baseline ?Local maximal init? slightly exceeds pre-
viously reported score of 76.89% (Cohn et al, 2009).
ment over our earlier 84.0 (Cohn et al, 2009)
although still well below state-of-the-art parsers.
We conjecture that the performance gap is due to
the model using an overly simplistic treatment of
unknown words, and also a further mixing prob-
lems with the sampler. For the full data set the
counts are much larger in magnitude which leads
to stronger modes. The sampler has difficulty es-
caping such modes and therefore is slower to mix.
One way to solve the mixing problem is for the
sampler to make more global moves, e.g., with
table label resampling (Johnson and Goldwater,
2009) or split-merge (Jain and Neal, 2000). An-
other way is to use a variational approximation in-
stead of MCMC sampling (Wainwright and Jor-
dan, 2008).
5 Discussion
We have demonstrated how our grammar trans-
formation can implicitly represent an exponential
space of tree fragments efficiently, allowing us
to build a sampler with considerably better mix-
ing properties than a local Gibbs sampler. The
same technique was also shown to improve the
parsing algorithm. These improvements are in
no way limited to our particular choice of a TSG
parsing model, many hierarchical Bayesian mod-
els have been proposed which would also permit
similar optimised samplers. In particular mod-
els which induce segmentations of complex struc-
tures stand to benefit from this work; Examples
include the word segmentation model of Goldwa-
ter et al (2006) for which it would be trivial to
adapt our technique to develop a blocked sampler.
Hierarchical Bayesian segmentation models have
also become popular in statistical machine transla-
tion where there is a need to learn phrasal transla-
tion structures that can be decomposed at the word
level (DeNero et al, 2008; Blunsom et al, 2009;
Cohn and Blunsom, 2009). We envisage similar
representations being applied to these models to
improve their mixing properties.
A particularly interesting avenue for further re-
search is to employ our blocked sampler for un-
supervised grammar induction. While it is diffi-
cult to extend the local Gibbs sampler to the case
where the tree is not observed, the dynamic pro-
gram for our blocked sampler can be easily used
for unsupervised inference by omitting the tree
matching constraints.
229
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP), pages 782?790, Suntec, Singapore, Au-
gust.
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-oriented parsing. Center for the Study
of Language and Information - Studies in Computa-
tional Linguistics. University of Chicago Press.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
model of syntax-directed tree to string grammar in-
duction. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 352?361, Singapore, August.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 548?556, Boulder, Colorado, June.
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314?323, Honolulu,
Hawaii, October.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721?741.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673?680,
Sydney, Australia, July.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al (Bod et al, 2003),
chapter 8.
Sonia Jain and Radford M. Neal. 2000. A split-merge
Markov chain Monte Carlo procedure for the Dirich-
let process mixture model. Journal of Computa-
tional and Graphical Statistics, 13:158?182.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 139?146,
Rochester, NY, April.
Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4:35?56.
Khalil Sima?an and Luciano Buratto. 2003. Backoff
parameter estimation for the dop model. In Nada
Lavrac, Dragan Gamberger, Ljupco Todorovski, and
Hendrik Blockeel, editors, ECML, volume 2837 of
Lecture Notes in Computer Science, pages 373?384.
Springer.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Vari-
ational Inference. Now Publishers Inc., Hanover,
MA, USA.
230
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 865?874,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Hierarchical Pitman-Yor Process HMM
for Unsupervised Part of Speech Induction
Phil Blunsom
Department of Computer Science
University of Oxford
Phil.Blunsom@cs.ox.ac.uk
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Abstract
In this work we address the problem of
unsupervised part-of-speech induction
by bringing together several strands of
research into a single model. We develop a
novel hidden Markov model incorporating
sophisticated smoothing using a hierarchical
Pitman-Yor processes prior, providing an
elegant and principled means of incorporating
lexical characteristics. Central to our
approach is a new type-based sampling
algorithm for hierarchical Pitman-Yor models
in which we track fractional table counts.
In an empirical evaluation we show that our
model consistently out-performs the current
state-of-the-art across 10 languages.
1 Introduction
Unsupervised part-of-speech (PoS) induction has
long been a central challenge in computational
linguistics, with applications in human language
learning and for developing portable language
processing systems. Despite considerable research
effort, progress in fully unsupervised PoS induction
has been slow and modern systems barely improve
over the early Brown et al (1992) approach
(Christodoulopoulos et al, 2010). One popular
means of improving tagging performance is to
include supervision in the form of a tag dictionary
or similar, however this limits portability and
also comprimises any cognitive conclusions. In
this paper we present a novel approach to fully
unsupervised PoS induction which uniformly
outperforms the existing state-of-the-art across all
our corpora in 10 different languages. Moreover, the
performance of our unsupervised model approaches
that of many existing semi-supervised systems,
despite our method not receiving any human input.
In this paper we present a Bayesian hidden
Markov model (HMM) which uses a non-parametric
prior to infer a latent tagging for a sequence of
words. HMMs have been popular for unsupervised
PoS induction from its very beginnings (Brown
et al, 1992), and justifiably so, as the most
discriminating feature for deciding a word?s PoS is
its local syntactic context.
Our work brings together several strands of
research including Bayesian non-parametric HMMs
(Goldwater and Griffiths, 2007), Pitman-Yor
language models (Teh, 2006b; Goldwater et
al., 2006b), tagging constraints over word types
(Brown et al, 1992) and the incorporation of
morphological features (Clark, 2003). The result
is a non-parametric Bayesian HMM which avoids
overfitting, contains no free parameters, and
exhibits good scaling properties. Our model uses
a hierarchical Pitman-Yor process (PYP) prior to
affect sophisicated smoothing over the transition
and emission distributions. This allows the
modelling of sub-word structure, thereby capturing
tag-specific morphological variation. Unlike many
existing approaches, our model is a principled
generative model and does not include any hand
tuned language specific features.
Inspired by previous successful approaches
(Brown et al, 1992), we develop a new type-
level inference procedure in the form of an
MCMC sampler with an approximate method for
incorporating the complex dependencies that arise
between jointly sampled events. Our experimental
evaluation demonstrates that our model, particularly
when restricted to a single tag per type, produces
865
state-of-the-art results across a range of corpora and
languages.
2 Background
Past research in unsupervised PoS induction has
largely been driven by two different motivations: a
task based perspective which has focussed on induc-
ing word classes to improve various applications,
and a linguistic perspective where the aim is to
induce classes which correspond closely to anno-
tated part-of-speech corpora. Early work was firmly
situtated in the task-based setting of improving gen-
eralisation in language models. Brown et al (1992)
presented a simple first-order HMM which restricted
word types to always be generated from the same
class. Though PoS induction was not their aim, this
restriction is largely validated by empirical analysis
of treebanked data, and moreover conveys the sig-
nificant advantage that all the tags for a given word
type can be updated at the same time, allowing very
efficient inference using the exchange algorithm.
This model has been popular for language mod-
elling and bilingual word alignment, and an imple-
mentation with improved inference called mkcls
(Och, 1999)1 has become a standard part of statis-
tical machine translation systems.
The HMM ignores orthographic information,
which is often highly indicative of a word?s part-
of-speech, particularly so in morphologically rich
languages. For this reason Clark (2003) extended
Brown et al (1992)?s HMM by incorporating a
character language model, allowing the modelling
of limited morphology. Our work draws from these
models, in that we develop a HMM with a one
class per tag restriction and include a character
level language model. In contrast to these previous
works which use the maximum likelihood estimate,
we develop a Bayesian model with a rich prior for
smoothing the parameter estimates, allowing us to
move to a trigram model.
A number of researchers have investigated a semi-
supervised PoS induction task in which a tag dictio-
nary or similar data is supplied a priori (Smith and
Eisner, 2005; Haghighi and Klein, 2006; Goldwater
and Griffiths, 2007; Toutanova and Johnson, 2008;
Ravi and Knight, 2009). These systems achieve
1Available from http://fjoch.com/mkcls.html.
much higher accuracy than fully unsupervised sys-
tems, though it is unclear whether the tag dictionary
assumption has real world application. We focus
solely on the fully unsupervised scenario, which we
believe is more practical for text processing in new
languages and domains.
Recent work on unsupervised PoS induction has
focussed on encouraging sparsity in the emission
distributions in order to match empirical distribu-
tions derived from treebank data (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008). These authors took a Bayesian approach
using a Dirichlet prior to encourage sparse distri-
butions over the word types emitted from each tag.
Conversely, Ganchev et al (2010) developed a tech-
nique to optimize the more desirable reverse prop-
erty of the word types having a sparse posterior dis-
tribution over tags. Recently Lee et al (2010) com-
bined the one class per word type constraint (Brown
et al, 1992) in a HMM with a Dirichlet prior to
achieve both forms of sparsity. However this work
approximated the derivation of the Gibbs sampler
(omitting the interdependence between events when
sampling from a collapsed model), resulting in a
model which underperformed Brown et al (1992)?s
one-class HMM.
Our work also seeks to enforce both forms of
sparsity, by developing an algorithm for type-level
inference under the one class constraint. This work
differs from previous Bayesian models in that we
explicitly model a complex backoff path using a
hierachical prior, such that our model jointly infers
distributions over tag trigrams, bigrams and uni-
grams and whole words and their character level
representation. This smoothing is critical to ensure
adequate generalisation from small data samples.
Research in language modelling (Teh, 2006b;
Goldwater et al, 2006a) and parsing (Cohn et
al., 2010) has shown that models employing
Pitman-Yor priors can significantly outperform the
more frequently used Dirichlet priors, especially
where complex hierarchical relationships exist
between latent variables. In this work we apply
these advances to unsupervised PoS tagging,
developing a HMM smoothed using a Pitman-Yor
process prior.
866
3 The PYP-HMM
We develop a trigram hidden Markov model which
models the joint probability of a sequence of latent
tags, t, and words, w, as
P?(t,w) =
L+1?
l=1
P?(tl|tl?1, tl?2)P?(wl|tl) ,
where L = |w| = |t| and t0 = t?1 = tL+1 = $ are
assigned a sentinel value to denote the start or end of
the sentence. A key decision in formulating such a
model is the smoothing of the tag trigram and emis-
sion distributions, which would otherwise be too dif-
ficult to estimate from small datasets. Prior work
in unsupervised PoS induction has employed simple
smoothing techniques, such as additive smoothing
or Dirichlet priors (Goldwater and Griffiths, 2007;
Johnson, 2007), however this body of work has over-
looked recent advances in smoothing methods used
for language modelling (Teh, 2006b; Goldwater et
al., 2006b). Here we build upon previous work by
developing a PoS induction model smoothed with
a sophisticated non-parametric prior. Our model
uses a hierarchical Pitman-Yor process prior for both
the transition and emission distributions, encoding
a backoff path from complex distributions to suc-
cesssively simpler ones. The use of complex dis-
tributions (e.g., over tag trigrams) allows for rich
expressivity when sufficient evidence is available,
while the hierarchy affords a means of backing off
to simpler and more easily estimated distributions
otherwise. The PYP has been shown to generate
distributions particularly well suited to modelling
language (Teh, 2006a; Goldwater et al, 2006b), and
has been shown to be a generalisation of Kneser-Ney
smoothing, widely recognised as the best smoothing
method for language modelling (Chen and Good-
man, 1996).
The model is depicted in the plate diagram in Fig-
ure 1. At its centre is a standard trigram HMM,
which generates a sequence of tags and words,
tl|tl?1, tl?2, T ? Ttl?1,tl?2
wl|tl, E ? Etl .
U
B
j
T
ij
E
j
C
jk
w
1
t
1
w
2
t
2
w
3
t
3
...
D
j
Figure 1: Plate diagram representation of the trigram
HMM. The indexes i and j range over the set of tags
and k ranges over the set of characters. Hyper-parameters
have been omitted from the figure for clarity.
The trigram transition distribution, Tij , is drawn
from a hierarchical PYP prior which backs off to a
bigram Bj and then a unigram U distribution,
Tij |a
T , bT , Bj ? PYP(a
T , bT , Bj)
Bj |a
B, bB, U ? PYP(aB, bB, U)
U |aU , bU ? PYP(aU , bU ,Uniform) ,
where the prior over U has as its base distribition a
uniform distribution over the set of tags, while the
priors for Bj and Tij back off by discarding an item
of context. This allows the modelling of trigram
tag sequences, while smoothing these estimates with
their corresponding bigram and unigram distribu-
tions. The degree of smoothing is regulated by
the hyper-parameters a and b which are tied across
each length of n-gram; these hyper-parameters are
inferred during training, as described in 3.1.
The tag-specific emission distributions, Ej , are
also drawn from a PYP prior,
Ej |a
E , bE , C ? PYP(aE , bE , Cj) .
We consider two different settings for the base distri-
bution Cj : 1) a simple uniform distribution over the
vocabulary (denoted HMM for the experiments in
section 4); and 2) a character-level language model
(denoted HMM+LM). In many languages morpho-
logical regularities correlate strongly with a word?s
part-of-speech (e.g., suffixes in English), which we
hope to capture using a basic character language
model. This model was inspired by Clark (2003)
867
The big dog
5 23 23 7
b r o w n
Figure 2: The conditioning structure of the hierarchical
PYP with an embedded character language models.
who applied a character level distribution to the sin-
gle class HMM (Brown et al, 1992). We formu-
late the character-level language model as a bigram
model over the character sequence comprising word
wl,
wlk|wlk?1, tl, C ? Ctlwlk?1
Cjk|a
C , bC , Dj ? PYP(a
C , bC , Dj)
Dj |a
D, bD ? PYP(aD, bD,Uniform) ,
where k indexes the characters in the word and,
in a slight abuse of notation, the character itself,
w0 and is set to a special sentinel value denoting
the start of the sentence (ditto for a final end of
sentence marker) and the uniform base distribution
ranges over the set of characters. We expect that
the HMM+LM model will outperform the uniform
HMM as it can capture many consistent morpholog-
ical affixes and thereby better distinguish between
different parts-of-speech. The HMM+LM is shown
in Figure 2, illustrating the decomposition of the tag
sequence into n-grams and a word into its compo-
nent character bigrams.
3.1 Training
In order to induce a tagging under this model we
use Gibbs sampling, a Markov chain Monte Carlo
(MCMC) technique for drawing samples from the
posterior distribution over the tag sequences given
observed word sequences. We present two different
sampling strategies: First, a simple Gibbs sampler
which randomly samples an update to a single tag
given all other tags; and second, a type-level sam-
pler which updates all tags for a given word under a
one-tag-per-word-type constraint. In order to extract
a single tag sequence to test our model against the
gold standard we find the tag at each site with maxi-
mum marginal probability in the sample set.
Following standard practice, we perform
inference using a collapsed sampler whereby
the model parameters U,B, T,E and C are
marginalised out. After marginalisation the
posterior distribution under a PYP prior is described
by a variant of the Chinese Restaurant Process
(CRP). The CRP is based around the analogy of
a restaurant with an infinite number of tables,
with customers entering one at a time and seating
themselves at a table. The choice of table is
governed by
P (zl = k|z?l) =
?
?
?
n?k ?a
l?1+b 1 ? k ? K
?
K?a+b
l?1+b k = K
? + 1
(1)
where zl is the table chosen by the lth customer, z?l
is the seating arrangement of the l? 1 previous cus-
tomers, n?k is the number of customers in z?l who
are seated at table k, K? = K(z?l) is the total num-
ber of tables in z?l, and z1 = 1 by definition. The
arrangement of customers at tables defines a cluster-
ing which exhibits a power-law behavior controlled
by the hyperparameters a and b.
To complete the restaurant analogy, a dish is then
served to each table which is shared by all the cus-
tomers seated there. This corresponds to a draw
from the base distribution, which in our case ranges
over tags for the transition distribution, and words
for the observation distribution. Overall the PYP
leads to a distribution of the form
P T (tl = i|z?l, t?l) =
1
n?h + b
T
? (2)
(
n?hi ?K
?
hia
T +
(
K?h a
T + bT
)
PB(i|z?l, t?l)
)
,
illustrating the trigram transition distribution, where
t?l are all previous tags, h = (tl?2, tl?1) is the con-
ditioning bigram, n?hi is the count of the trigram hi
in t?l, n
?
h the total count over all trigrams beginning
with h, K?hi the number of tables served dish i and
PB(?) is the base distribution, in this case the bigram
distribution.
A hierarchy of PYPs can be formed by making the
base distribution of a PYP another PYP, following a
868
semantics whereby whenever a customer sits at an
empty table in a restaurant, a new customer is also
said to enter the restaurant for its base distribution.
That is, each table at one level is equivalent to a cus-
tomer at the next deeper level, creating the invari-
ants: K?hi = n
?
ui andK
?
ui = n
?
i , where u = tl?1
indicates the unigram backoff context of h. The
recursion terminates at the lowest level where the
base distribution is static. The hierarchical setting
allows for the modelling of elaborate backoff paths
from rich and complex structure to successively sim-
pler structures.
Gibbs samplers Both our Gibbs samplers perform
the same calculation of conditional tag distributions,
and involve first decrementing all trigrams and emis-
sions affected by a sampling action, and then rein-
troducing the trigrams one at a time, conditioning
their probabilities on the updated counts and table
configurations as we progress.
The first local Gibbs sampler (PYP-HMM)
updates a single tag assignment at a time, in a
similar fashion to Goldwater and Griffiths (2007).
Changing one tag affects three trigrams, with
posterior
P (tl|z?l, t?l,w) ? P (tl?2, wl|z?l?2, t?l?2) ,
where l?2 denotes the range l?2, l?1, l, l+1, l+2.
The joint distribution over the three trigrams con-
tained in tl?2 can be calculated using the PYP for-
mulation. This calculation is complicated by the fact
that these events are not independent; the counts of
one trigram can affect the probability of later ones,
and moreover, the table assignment for the trigram
may also affect the bigram and unigram counts, of
particular import when the same tag occurs twice in
a row such as in Figure 2.
Many HMMs used for inducing word classes for
language modelling include the restriction that all
occurrences of a word type always appear with the
same class throughout the corpus (Brown et al,
1992; Och, 1999; Clark, 2003). Our second sampler
(PYP-1HMM) restricts inference to taggings which
adhere to this one tag per type restriction. This
restriction permits efficient inference techniques in
which all tags of all occurrences of a word type are
updated in parallel. Similar techniques have been
used for models with Dirichlet priors (Liang et al,
2010), though one must be careful to manage the
dependencies between multiple draws from the pos-
terior.
The dependency on table counts in the conditional
distributions complicates the process of drawing
samples for both our models. In the non-hierarchical
model (Goldwater and Griffiths, 2007) these
dependencies can easily be accounted for by
incrementing customer counts when such a
dependence occurs. In our model we would need to
sum over all possible table assignments that result
in the same tagging, at all levels in the hierarchy:
tag trigrams, bigrams and unigrams; and also words,
character bigrams and character unigrams. To avoid
this rather onerous marginalisation2 we instead use
expected table counts to calculate the conditional
distributions for sampling. Unfortunately we
know of no efficient algorithm for calculating the
expected table counts, so instead develop a novel
approximation
En+1 [Ki] ? En [Ki] +
(aUEn [K] + bU )P0(i)
(n? En [Ki] bU ) + (aUEn [K] + bU )P0(i)
, (3)
where Ki is the number of tables for the tag uni-
gram i of which there are n + 1 occurrences, En [?]
denotes an expectation after observing n items and
En [K] =
?
j En [Kj ]. This formulation defines
a simple recurrence starting with the first customer
seated at a table, E1 [Ki] = 1, and as each subse-
quent customer arrives we fractionally assign them
to a new table based on their conditional probability
of sitting alone. These fractional counts are then
carried forward for subsequent customers.
This approximation is tight for small n, and there-
fore it should be effective in the case of the local
Gibbs sampler where only three trigrams are being
resampled. For the type based resampling where
large numbers of n are involved (consider resam-
pling the), this approximation can deviate from the
actual value due to errors accumulated in the recur-
sion. Figure 3 illustrates a simulation demonstrating
that the approximation is a close match for small a
and n but underestimates the true value for high a
2Marginalisation is intractable in general, i.e. for the 1HMM
where many sites are sampled jointly.
869
0 20 40 60 80 100
2
4
6
8
10
12
number of customers
expe
cted
 tabl
es
a=0.9a=0.8a=0.5a=0.1
Figure 3: Simulation comparing the expected table
count (solid lines) versus the approximation under Eq. 3
(dashed lines) for various values of a. This data was gen-
erated from a single PYP with b = 1, P0(i) = 14 and
n = 100 customers which all share the same tag.
and n. The approximation was much less sensitive
to the choice of b (not shown).
To resample a sequence of trigrams we start by
removing their counts from the current restaurant
configuration (resulting in z?). For each tag we
simulate adding back the trigrams one at a time,
calculating their probability under the given z? plus
the fractional table counts accumulated by Equation
3. We then calculate the expected table count con-
tribution from this trigram and add it to the accu-
mulated counts. The fractional table count from the
trigram then results in a fractional customer entering
the bigram restaurant, and so on down to unigrams.
At each level we must update the expected counts
before moving on to the next trigram. After per-
forming this process for all trigrams under consider-
ation and for all tags, we then normalise the resulting
tag probabilities and sample an outcome. Once a
tag has been sampled, we then add all the trigrams
to the restaurants sampling their tables assignments
explicitly (which are no longer fractional), recorded
in z. Because we do not marginalise out the table
counts and our expectations are only approximate,
this sampler will be biased. We leave to future work
properly accounting for this bias, e.g., by devising a
Metropolis Hastings acceptance test.
Sampling hyperparameters We treat the
hyper-parameters {(ax, bx) , x ? (U,B, T,E,C)}
as random variables in our model and infer their
values. We place prior distributions on the PYP
discount ax and concentration bx hyperparamters
and sample their values using a slice sampler. For
the discount parameters we employ a uniform
Beta distribution (ax ? Beta(1, 1)), and for
the concentration parameters we use a vague
gamma prior (bx ? Gamma(10, 0.1)). All the
hyper-parameters are resampled after every 5th
sample of the corpus.
The result of this hyperparameter inference is that
there are no user tunable parameters in the model,
an important feature that we believe helps explain its
consistently high performance across test settings.
4 Experiments
We perform experiments with a range of corpora
to both investigate the properties of our proposed
models and inference algorithms, as well as to estab-
lish their robustness across languages and domains.
For our core English experiments we report results
on the entire Penn. Treebank (Marcus et al, 1993),
while for other languages we use the corpora made
available for the CoNLL-X Shared Task (Buchholz
and Marsi, 2006). We report results using the many-
to-one (M-1) and v-measure (VM) metrics consid-
ered best by the evaluation of Christodoulopoulos
et al (2010). M-1 measures the accuracy of the
model after mapping each predicted class to its most
frequent corresponding tag, while VM is a variant
of the F-measure which uses conditional entropy
analogies of precision and recall. The log-posterior
for the HMM sampler levels off after a few hundred
samples, so we report results after five hundred. The
1HMM sampler converges more quickly so we use
two hundred samples for these models. All reported
results are the mean of three sampling runs.
An important detail for any unsupervised
learning algorithm is its initialisation. We used
slightly different initialisation for each of our
inference strategies. For the unrestricted HMM we
randomly assigned each word token to a class. For
the restricted 1HMM we use a similar initialiser to
870
Model M-1 VM
Prototype meta-model (CGS10) 76.1 68.8
MEMM (BBDK10) 75.5 -
mkcls (Och, 1999) 73.7 65.6
MLE 1HMM-LM (Clark, 2003)? 71.2 65.5
BHMM (GG07) 63.2 56.2
PR (Ganchev et al, 2010)? 62.5 54.8
Trigram PYP-HMM 69.8 62.6
Trigram PYP-1HMM 76.0 68.0
Trigram PYP-1HMM-LM 77.5 69.7
Bigram PYP-HMM 66.9 59.2
Bigram PYP-1HMM 72.9 65.9
Trigram DP-HMM 68.1 60.0
Trigram DP-1HMM 76.0 68.0
Trigram DP-1HMM-LM 76.8 69.8
Table 1: WSJ performance comparing previous work
to our own model. The columns display the many-to-1
accuracy and the V measure, both averaged over 5 inde-
pendent runs. Our model was run with the local sampler
(HMM), the type-level sampler (1HMM) and also with
the character LM (1HMM-LM). Also shown are results
using Dirichlet Process (DP) priors by fixing a = 0. The
system abbreviations are CGS10 (Christodoulopoulos et
al., 2010), BBDK10 (Berg-Kirkpatrick et al, 2010) and
GG07 (Goldwater and Griffiths, 2007). Starred entries
denote results reported in CGS10.
Clark (2003), assigning each of the k most frequent
word types to its own class, and then randomly
dividing the rest of the types between the classes.
As a baseline we report the performance of
mkcls (Och, 1999) on all test corpora. This model
seems not to have been evaluated in prior work on
unsupervised PoS tagging, which is surprising given
its consistently good performance.
First we present our results on the most frequently
reported evaluation, the WSJ sections of the Penn.
Treebank, along with a number of state-of-the-art
results previously reported (Table 1). All of these
models are allowed 45 tags, the same number of tags
as in the gold-standard. The performance of our
models is strong, particularly the 1HMM. We also
see that incorporating a character language model
(1HMM-LM) leads to further gains in performance,
improving over the best reported scores under both
M-1 and VM. We have omitted the results for the
HMM-LM as experimentation showed that the local
Gibbs sampler became hopelessly stuck, failing to
0 10 20 30 40 500
2
4
6
8
10
12
14
16
18 x 10
4
Tags sorted by frequency
Fre
qu
en
cy
 
 
Gold tag distribution
1HMM
1HMM?LM
MKCLS
Figure 4: Sorted frequency of tags for WSJ. The gold
standard distribution follows a steep exponential curve
while the induced model distributions are more uniform.
mix due to the model?s deep structure (its peak per-
formance was ? 55%).
To evaluate the effectiveness of the PYP prior we
include results using a Dirichlet Process prior (DP).
We see that for all models the use of the PYP pro-
vides some gain for the HMM, but diminishes for
the 1HMM. This is perhaps a consequence of the
expected table count approximation for the type-
sampled PYP-1HMM: the DP relies less on the table
counts than the PYP.
If we restrict the model to bigrams we see
a considerable drop in performance. Note that
the bigram PYP-HMM outperforms the closely
related BHMM (the main difference being that
we smooth tag bigrams with unigrams). It is also
interesting to compare the bigram PYP-1HMM to
the closely related model of Lee et al (2010). That
model incorrectly assumed independence of the
conditional sampling distributions, resulting in a
accuracy of 66.4%, well below that of our model.
Figures 4 and 5 provide insight into the behavior
of the sampling algorithms. The former shows that
both our models and mkcls induce a more uniform
distribution over tags than specified by the treebank.
It is unclear whether it is desirable for models to
exhibit behavior closer to the treebank, which ded-
icates separate tags to very infrequent phenomena
while lumping the large range of noun types into
a single category. The graph in Figure 5 shows
that the type-based 1HMM sampler finds a good
tagging extremely quickly and then sticks with it,
871
0 50 100 15010
20
30
40
50
60
70
80
Number of samples
M?1
 Acc
urac
y (%
)
 
 
PYP?1HMMPYP?1HMM?LMPYP?HMMPYP?HMM?LM
Figure 5: M-1 accuracy vs. number of samples.
NNIN
NNPDT
JJNNS
,.
CDRB
VBDVB
CCTO
VBZVBN
PRPVBG
VBPMD
POSPRP$
$??
??:
WDTJJR
RPNNPS
WPWRB
JJSRBR
?RRB??LRB?
EXRBS
PDTFW
WP$#
UHSYM
NNIN
NNPDT
JJNNS
,.
CDRB
VBDVB
CCTO
VBZVBN
PRPVBG
VBPMD
POSPRP$
$??
??:
WDTJJR
RPNNPS
WPWRB
JJSRBR
?RRB??LRB?
EXRBS
PDTFW
WP$#
UHSYM
Figure 6: Cooccurence between frequent gold (y-axis)
and predicted (x-axis) tags, comparing mkcls (top) and
PYP-1HMM-LM (bottom). Both axes are sorted in terms
of frequency. Darker shades indicate more frequent cooc-
curence and columns represent the induced tags.
save for the occasional step change demonstrated by
the 1HMM-LM line. The locally sampled model is
far slower to converge, rising slowly and plateauing
well below the other models.
In Figure 6 we compare the distributions over
WSJ tags for mkcls and the PYP-1HMM-LM. On
the macro scale we can see that our model induces a
sparser distribution. With closer inspection we can
identify particular improvements our model makes.
In the first column for mkcls and the third column
for our model we can see similar classes with sig-
nificant counts for DTs and PRPs, indicating a class
that the models may be using to represent the start
of sentences (informed by start transitions or capi-
talisation). This column exemplifies the sparsity of
the PYP model?s posterior.
We continue our evaluation on the CoNLL
multilingual corpora (Table 2). These results show
a highly consistent story of performance for our
models across diverse corpora. In all cases the
PYP-1HMM outperforms the PYP-HMM, which
are both outperformed by the PYP-1HMM-LM.
The character language model provides large
gains in performance on a number of corpora,
in particular those with rich morphology (Arabic
+5%, Portuguese +5%, Spanish +4%). We again
note the strong performance of the mkcls model,
significantly beating recently published state-of-the-
art results for both Dutch and Swedish. Overall our
best model (PYP-1HMM-LM) outperforms both
the state-of-the-art, where previous work exists, as
well as mkcls consistently across all languages.
5 Discussion
The hidden Markov model, originally developed by
Brown et al (1992), continues to be an effective
modelling structure for PoS induction. We have
combined hierarchical Bayesian priors with a tri-
gram HMM and character language model to pro-
duce a model with consistently state-of-the-art per-
formance across corpora in ten languages. How-
ever our analysis indicates that there is still room for
improvement, particularly in model formulation and
developing effective inference algorithms.
Induced tags have already proven their usefulness
in applications such as Machine Translation, thus it
will prove interesting as to whether the improve-
ments seen from our models can lead to gains in
downstream tasks. The continued successes of mod-
els combining hierarchical Pitman-Yor priors with
expressive graphical models attests to this frame-
work?s enduring attraction, we foresee continued
interest in applying this technique to other NLP
tasks.
872
Language mkcls HMM 1HMM 1HMM-LM Best pub. Tokens Tag types
Arabic 58.5 57.1 62.7 67.5 - 54,379 20
Bulgarian 66.8 67.8 69.7 73.2 - 190,217 54
Czech 59.6 62.0 66.3 70.1 - 1,249,408 12c
Danish 62.7 69.9 73.9 76.2 66.7? 94,386 25
Dutch 64.3 66.6 68.7 70.4 67.3? 195,069 13c
Hungarian 54.3 65.9 69.0 73.0 - 131,799 43
Portuguese 68.5 72.1 73.5 78.5 75.3? 206,678 22
Spanish 63.8 71.6 74.7 78.8 73.2? 89,334 47
Swedish 64.3 66.6 67.0 68.6 60.6? 191,467 41
Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published
result (?Berg-Kirkpatrick et al (2010) and ?Lee et al (2010)). This data was taken from the CoNLL-X shared task
training sets, resulting in listed corpus sizes. Fine PoS tags were used for evaluation except for items marked with c,
which used the coarse tags. For each language the systems were trained to produce the same number of tags as the
gold standard.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479, December.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, CoNLL-X ?06, pages 149?
164, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, pages
310?318, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 575?584, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting of the
European Association for Computational Linguistics
(EACL), pages 59?66.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053?3096.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 99:2001?2049, August.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 344?352, Morristown, NJ,
USA. Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), pages 744?751, Prague, Czech Republic,
June.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006a. Contextual dependencies in unsupervised
word segmentation. In Proc. of the 44th Annual Meet-
ing of the ACL and 21st International Conference
on Computational Linguistics (COLING/ACL-2006),
Sydney.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
873
Information Processing Systems 18, pages 459?466.
MIT Press, Cambridge, MA.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 320?
327, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Mark Johnson. 2007. Why doesnt EM find good
HMM POS-taggers? In Proc. of the 2007 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007), pages 296?305, Prague, Czech
Republic.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 853?861, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based
MCMC. In North American Association for Compu-
tational Linguistics (NAACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 71?76,
Morristown, NJ, USA. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conferenceof the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing (ACL-IJCNLP), pages
504?512.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 354?362, Ann Arbor, Michigan, June.
Y. W. Teh. 2006a. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985?992.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL-44, pages
985?992, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 1521?1528. MIT Press,
Cambridge, MA.
874
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894?904,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The Role of Syntax in Vector Space Models of Compositional Semantics
Karl Moritz Hermann and Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
Abstract
Modelling the compositional process by
which the meaning of an utterance arises
from the meaning of its parts is a funda-
mental task of Natural Language Process-
ing. In this paper we draw upon recent
advances in the learning of vector space
representations of sentential semantics and
the transparent interface between syntax
and semantics provided by Combinatory
Categorial Grammar to introduce Com-
binatory Categorial Autoencoders. This
model leverages the CCG combinatory op-
erators to guide a non-linear transforma-
tion of meaning within a sentence. We use
this model to learn high dimensional em-
beddings for sentences and evaluate them
in a range of tasks, demonstrating that
the incorporation of syntax allows a con-
cise model to learn representations that are
both effective and general.
1 Introduction
Since Frege stated his ?Principle of Semantic
Compositionality? in 1892 researchers have pon-
dered both how the meaning of a complex expres-
sion is determined by the meanings of its parts,
and how those parts are combined. (Frege, 1892;
Pelletier, 1994). Over a hundred years on the
choice of representational unit for this process
of compositional semantics, and how these units
combine, remain open questions.
Frege?s principle may be debatable from a lin-
guistic and philosophical standpoint, but it has
provided a basis for a range of formal approaches
to semantics which attempt to capture meaning in
logical models. The Montague grammar (Mon-
tague, 1970) is a prime example for this, build-
ing a model of composition based on lambda-
calculus and formal logic. More recent work
in this field includes the Combinatory Categorial
Grammar (CCG), which also places increased em-
phasis on syntactic coverage (Szabolcsi, 1989).
Recently those searching for the right represen-
tation for compositional semantics have drawn in-
spiration from the success of distributional mod-
els of lexical semantics. This approach represents
single words as distributional vectors, implying
that a word?s meaning is a function of the envi-
ronment it appears in, be that its syntactic role or
co-occurrences with other words (Pereira et al,
1993; Schu?tze, 1998). While distributional se-
mantics is easily applied to single words, spar-
sity implies that attempts to directly extract distri-
butional representations for larger expressions are
doomed to fail. Only in the past few years has
it been attempted to extend these representations
to semantic composition. Most approaches here
use the idea of vector-matrix composition to learn
larger representations from single-word encodings
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al, 2012b). While
these models have proved very promising for com-
positional semantics, they make minimal use of
linguistic information beyond the word level.
In this paper we bridge the gap between recent
advances in machine learning and more traditional
approaches within computational linguistics. We
achieve this goal by employing the CCG formal-
ism to consider compositional structures at any
point in a parse tree. CCG is attractive both for its
transparent interface between syntax and seman-
tics, and a small but powerful set of combinatory
operators with which we can parametrise our non-
linear transformations of compositional meaning.
We present a novel class of recursive mod-
els, the Combinatory Categorial Autoencoders
(CCAE), which marry a semantic process pro-
vided by a recursive autoencoder with the syn-
tactic representations of the CCG formalism.
Through this model we seek to answer two ques-
894
tions: Can recursive vector space models be recon-
ciled with a more formal notion of compositional-
ity; and is there a role for syntax in guiding seman-
tics in these types of models? CCAEs make use of
CCG combinators and types by conditioning each
composition function on its equivalent step in a
CCG proof. In terms of learning complexity and
space requirements, our models strike a balance
between simpler greedy approaches (Socher et
al., 2011b) and the larger recursive vector-matrix
models (Socher et al, 2012b).
We show that this combination of state of the art
machine learning and an advanced linguistic for-
malism translates into concise models with com-
petitive performance on a variety of tasks. In both
sentiment and compound similarity experiments
we show that our CCAE models match or better
comparable recursive autoencoder models.1
2 Background
There exist a number of formal approaches to lan-
guage that provide mechanisms for composition-
ality. Generative Grammars (Jackendoff, 1972)
treat semantics, and thus compositionality, essen-
tially as an extension of syntax, with the generative
(syntactic) process yielding a structure that can be
interpreted semantically. By contrast Montague
grammar achieves greater separation between the
semantic and the syntactic by using lambda calcu-
lus to express meaning. However, this greater sep-
aration between surface form and meaning comes
at a price in the form of reduced computability.
While this is beyond the scope of this paper, see
e.g. Kracht (2008) for a detailed analysis of com-
positionality in these formalisms.
2.1 Combinatory Categorial Grammar
In this paper we focus on CCG, a linguistically
expressive yet computationally efficient grammar
formalism. It uses a constituency-based structure
with complex syntactic types (categories) from
which sentences can be deduced using a small
number of combinators. CCG relies on combi-
natory logic (as opposed to lambda calculus) to
build its expressions. For a detailed introduction
and analysis vis-a`-vis other grammar formalisms
see e.g. Steedman and Baldridge (2011).
CCG has been described as having a transpar-
ent surface between the syntactic and the seman-
1A C++ implementation of our models is available at
http://www.karlmoritz.com/
Tina likes tigers
N (S[dcl]\NP)/NP N
NP NP
>
S[dcl]\NP
<
S[dcl]
Figure 1: CCG derivation for Tina likes tigers with
forward (>) and backward application (<).
tic. It is this property which makes it attractive
for our purposes of providing a conditioning struc-
ture for semantic operators. A second benefit of
the formalism is that it is designed with computa-
tional efficiency in mind. While one could debate
the relative merits of various linguistic formalisms
the existence of mature tools and resources, such
as the CCGBank (Hockenmaier and Steedman,
2007), the Groningen Meaning Bank (Basile et al,
2012) and the C&C Tools (Curran et al, 2007) is
another big advantage for CCG.
CCG?s transparent surface stems from its cate-
gorial property: Each point in a derivation corre-
sponds directly to an interpretable category. These
categories (or types) associated with each term in a
CCG govern how this term can be combined with
other terms in a larger structure, implicitly making
them semantically expressive.
For instance in Figure 1, the word likes has type
(S[dcl]\NP)/NP, which means that it first looks
for a type NP to its right hand side. Subsequently
the expression likes tigers (as type S[dcl]\NP) re-
quires a second NP on its left. The final type of
the phrase S[dcl] indicates a sentence and hence a
complete CCG proof. Thus at each point in a CCG
parse we can deduce the possible next steps in the
derivation by considering the available types and
combinatory rules.
2.2 Vector Space Models of Semantics
Vector-based approaches for semantic tasks have
become increasingly popular in recent years.
Distributional representations encode an ex-
pression by its environment, assuming the context-
dependent nature of meaning according to which
one ?shall know a word by the company it keeps?
(Firth, 1957). Effectively this is usually achieved
by considering the co-occurrence with other words
in large corpora or the syntactic roles a word per-
forms.
Distributional representations are frequently
used to encode single words as vectors. Such rep-
895
resentations have then successfully been applied
to a number of tasks including word sense disam-
biguation (Schu?tze, 1998) and selectional prefer-
ence (Pereira et al, 1993; Lin, 1999).
While it is theoretically possible to apply the
same mechanism to larger expressions, sparsity
prevents learning meaningful distributional repre-
sentations for expressions much larger than single
words.2
Vector space models of compositional seman-
tics aim to fill this gap by providing a methodol-
ogy for deriving the representation of an expres-
sion from those of its parts. While distributional
representations frequently serve to encode single
words in such approaches this is not a strict re-
quirement.
There are a number of ideas on how to de-
fine composition in such vector spaces. A gen-
eral framework for semantic vector composition
was proposed in Mitchell and Lapata (2008), with
Mitchell and Lapata (2010) and more recently Bla-
coe and Lapata (2012) providing good overviews
of this topic. Notable approaches to this issue in-
clude Baroni and Zamparelli (2010), who com-
pose nouns and adjectives by representing them as
vectors and matrices, respectively, with the com-
positional representation achieved by multiplica-
tion. Grefenstette and Sadrzadeh (2011) use a sim-
ilar approach with matrices for relational words
and vectors for arguments. These two approaches
are combined in Grefenstette et al (2013), produc-
ing a tensor-based semantic framework with ten-
sor contraction as composition operation.
Another set of models that have very success-
fully been applied in this area are recursive autoen-
coders (Socher et al, 2011a; Socher et al, 2011b),
which are discussed in the next section.
2.3 Recursive Autoencoders
Autoencoders are a useful tool to compress in-
formation. One can think of an autoencoder
as a funnel through which information has to
pass (see Figure 2). By forcing the autoencoder
to reconstruct an input given only the reduced
amount of information available inside the funnel
it serves as a compression tool, representing high-
dimensional objects in a lower-dimensional space.
Typically a given autoencoder, that is the func-
tions for encoding and reconstructing data, are
2The experimental setup in (Baroni and Zamparelli, 2010)
is one of the few examples where distributional representa-
tions are used for word pairs.
Figure 2: A simple three-layer autoencoder. The
input represented by the vector at the bottom is
being encoded in a smaller vector (middle), from
which it is then reconstructed (top) into the same
dimensionality as the original input vector.
used on multiple inputs. By optimizing the two
functions to minimize the difference between all
inputs and their respective reconstructions, this au-
toencoder will effectively discover some hidden
structures within the data that can be exploited to
represent it more efficiently.
As a simple example, assume input vectors
xi ? Rn, i ? (0..N), weight matrices W enc ?
R(m?n),W rec ? R(n?m) and biases benc ? Rm,
brec ? Rn. The encoding matrix and bias are used
to create an encoding ei from xi:
ei = fenc(xi) =W encxi + benc (1)
Subsequently e ? Rm is used to reconstruct x as
x? using the reconstruction matrix and bias:
x?i = f rec(ei) =W recei + brec (2)
? = (W enc,W rec, benc, brec) can then be learned
by minimizing the error function describing the
difference between x? and x:
E = 12
N?
i
??x?i ? xi
??2 (3)
Now, if m < n, this will intuitively lead to ei
encoding a latent structure contained in xi and
shared across all xj , j ? (0..N), with ? encoding
and decoding to and from that hidden structure.
It is possible to apply multiple autoencoders on
top of each other, creating a deep autoencoder
(Bengio et al, 2007; Hinton and Salakhutdinov,
2006). For such a multi-layered model to learn
anything beyond what a single layer could learn, a
non-linear transformation g needs to be applied at
each layer. Usually, a variant of the sigmoid (?)
896
Figure 3: RAE with three inputs. Vectors with
filled (blue) circles represent input and hidden
units; blanks (white) denote reconstruction layers.
or hyperbolic tangent (tanh) function is used for
g (LeCun et al, 1998).
fenc(xi) = g (W encxi + benc) (4)
f rec(ei) = g (W recei + brec)
Furthermore, autoencoders can easily be used as
a composition function by concatenating two input
vectors, such that:
e = f(x1, x2) = g (W (x1?x2) + b) (5)
(x?1?x?2) = g
(
W ?e+ b?
)
Extending this idea, recursive autoencoders (RAE)
allow the modelling of data of variable size. By
setting the n = 2m, it is possible to recursively
combine a structure into an autoencoder tree. See
Figure 3 for an example, where x1, x2, x3 are re-
cursively encoded into y2.
The recursive application of autoencoders was
first introduced in Pollack (1990), whose recursive
auto-associative memories learn vector represen-
tations over pre-specified recursive data structures.
More recently this idea was extended and applied
to dynamic structures (Socher et al, 2011b).
These types of models have become increas-
ingly prominent since developments within the
field of Deep Learning have made the training
of such hierarchical structures more effective and
tractable (LeCun et al, 1998; Hinton et al, 2006).
Intuitively the top layer of an RAE will encode
aspects of the information stored in all of the input
vectors. Previously, RAE have successfully been
applied to a number of tasks including sentiment
analysis, paraphrase detection, relation extraction
Model CCG Elements
CCAE-A parse
CCAE-B parse + rules
CCAE-C parse + rules + types
CCAE-D parse + rules + child types
Table 1: Aspects of the CCG formalism used by
the different models explored in this paper.
and 3D object identification (Blacoe and Lapata,
2012; Socher et al, 2011b; Socher et al, 2012a).
3 Model
The models in this paper combine the power of
recursive, vector-based models with the linguistic
intuition of the CCG formalism. Their purpose is
to learn semantically meaningful vector represen-
tations for sentences and phrases of variable size,
while the purpose of this paper is to investigate
the use of syntax and linguistic formalisms in such
vector-based compositional models.
We assume a CCG parse to be given. Let C de-
note the set of combinatory rules, and T the set
of categories used, respectively. We use the parse
tree to structure an RAE, so that each combina-
tory step is represented by an autoencoder func-
tion. We refer to these models Categorial Com-
binatory Autoencoders (CCAE). In total this pa-
per describes four models making increasing use
of the CCG formalism (see table 1).
As an internal baseline we use model CCAE-
A, which is an RAE structured along a CCG parse
tree. CCAE-A uses a single weight matrix each for
the encoding and reconstruction step (see Table 2.
This model is similar to Socher et al (2011b), ex-
cept that we use a fixed structure in place of the
greedy tree building approach. As CCAE-A uses
only minimal syntactic guidance, this should al-
low us to better ascertain to what degree the use of
syntax helps our semantic models.
Our second model (CCAE-B) uses the compo-
sition function in equation (6), with c ? C.
fenc(x, y, c) = g (W cenc(x?y) + bcenc) (6)
f rec(e, c) = g (W crece+ bcrec)
This means that for every combinatory rule we de-
fine an equivalent autoencoder composition func-
tion by parametrizing both the weight matrix and
bias on the combinatory rule (e.g. Figure 4).
In this model, as in the following ones, we as-
sume a reconstruction step symmetric with the
897
Model Encoding Function
CCAE-A f(x, y)= g (W (x?y) + b)
CCAE-B f(x, y, c)= g (W c(x?y) + bc)
CCAE-C f(x, y, c, t)= g
(?
p?{c,t} (W p(x?y) + bp)
)
CCAE-D f(x, y, c, tx, ty)= g (W c (W txx+W tyy)+ bc)
Table 2: Encoding functions of the four CCAE models discussed in this paper.
? : X/Y ? : Y
>?? : X
g (W>enc(???) + b>enc)
Figure 4: Forward application as CCG combinator
and autoencoder rule respectively.
Figure 5: CCAE-B applied to Tina likes tigers.
Next to each vector are the CCG category (top)
and the word or function representing it (bottom).
lex describes the unary type-changing operation.
> and < are forward and backward application.
composition step. For the remainder of this paper
we will focus on the composition step and drop the
use of enc and rec in variable names where it isn?t
explicitly required. Figure 5 shows model CCAE-
B applied to our previous example sentence.
While CCAE-B uses only the combinatory
rules, we want to make fuller use of the linguis-
tic information available in CCG. For this pur-
pose, we build another model CCAE-C, which
parametrizes on both the combinatory rule c ? C
and the CCG category t ? T at every step (see
Figure 2). This model provides an additional de-
gree of insight, as the categories T are semanti-
cally and syntactically more expressive than the
CCG combinatory rules by themselves. Summing
over weights parametrised on c and t respectively,
adds an additional degree of freedom and also al-
lows for some model smoothing.
An alternative approach is encoded in model
CCAE-D. Here we consider the categories not of
the element represented, but of the elements it is
generated from together with the combinatory rule
applied to them. The intuition is that in the first
step we transform two expressions based on their
syntax. Subsequently we combine these two con-
ditioned on their joint combinatory rule.
4 Learning
In this section we briefly discuss unsupervised
learning for our models. Subsequently we de-
scribe how these models can be extended to allow
for semi-supervised training and evaluation.
Let ? = (W,B, L) be our model parameters
and ? a vector with regularization parameters for
all model parameters. W represents the set of all
weight matrices, B the set of all biases and L the
set of all word vectors. LetN be the set of training
data consisting of tree-nodes n with inputs xn, yn
and reconstruction rn. The error given n is:
E(n|?) = 12
???rn ? (xn?yn)
???
2 (7)
The gradient of the regularised objective func-
tion then becomes:
?J
?? =
1
N
N?
n
?E(n|?)
?? + ?? (8)
We learn the gradient using backpropagation
through structure (Goller and Ku?chler, 1996), and
minimize the objective function using L-BFGS.
For more details about the partial derivatives
used for backpropagation, see the documentation
accompanying our model implementation.3
3http://www.karlmoritz.com/
898
4.1 Supervised Learning
The unsupervised method described so far learns
a vector representation for each sentence. Such a
representation can be useful for some tasks such as
paraphrase detection, but is not sufficient for other
tasks such as sentiment classification, which we
are considering in this paper.
In order to extract sentiment from our models,
we extend them by adding a supervised classifier
on top, using the learned representations v as input
for a binary classification model:
pred(l=1|v, ?) = sigmoid(Wlabel v + blabel) (9)
Given our corpus of CCG parses with label pairs
(N, l), the new objective function becomes:
J = 1N
?
(N,l)
E(N, l, ?) + ?2 ||?||
2 (10)
Assuming each node n ? N contains children
xn, yn, encoding en and reconstruction rn, so that
n = {x, y, e, r} this breaks down into:
E(N, l, ?) = (11)
?
n?N
?Erec (n, ?) + (1??)Elbl(en, l, ?)
Erec(n, ?) =
1
2
???[xn?yn]? rn
???
2 (12)
Elbl(e, l, ?) =
1
2 ?l ? e?
2 (13)
This method of introducing a supervised aspect
to the autoencoder largely follows the model de-
scribed in Socher et al (2011b).
5 Experiments
We describe a number of standard evaluations
to determine the comparative performance of our
model. The first task of sentiment analysis allows
us to compare our CCG-conditioned RAE with
similar, existing models. In a second experiment,
we apply our model to a compound similarity eval-
uation, which allows us to evaluate our models
against a larger class of vector-based models (Bla-
coe and Lapata, 2012). We conclude with some
qualitative analysis to get a better idea of whether
the combination of CCG and RAE can learn se-
mantically expressive embeddings.
In our experiments we use the hyperbolic tan-
gent as nonlinearity g. Unless stated otherwise we
use word-vectors of size 50, initialized using the
embeddings provided by Turian et al (2010) based
on the model of Collobert and Weston (2008).4
We use the C&C parser (Clark and Curran,
2007) to generate CCG parse trees for the data
used in our experiments. For models CCAE-C and
CCAE-D we use the 25 most frequent CCG cate-
gories (as extracted from the British National Cor-
pus) with an additional general weight matrix in
order to catch all remaining types.
5.1 Sentiment Analysis
We evaluate our model on the MPQA opinion
corpus (Wiebe et al, 2005), which annotates ex-
pressions for sentiment.5 The corpus consists of
10,624 instances with approximately 70 percent
describing a negative sentiment. We apply the
same pre-processing as (Nakagawa et al, 2010)
and (Socher et al, 2011b) by using an additional
sentiment lexicon (Wilson et al, 2005) during the
model training for this experiment.
As a second corpus we make use of the sentence
polarity (SP) dataset v1.0 (Pang and Lee, 2005).6
This dataset consists of 10662 sentences extracted
from movie reviews which are manually labelled
with positive or negative sentiment and equally
distributed across sentiment.
Experiment 1: Semi-Supervised Training In
the first experiment, we use the semi-supervised
training strategy described previously and initial-
ize our models with the embeddings provided by
Turian et al (2010). The results of this evalua-
tion are in Table 3. While we achieve the best per-
formance on the MPQA corpus, the results on the
SP corpus are less convincing. Perhaps surpris-
ingly, the simplest model CCAE-A outperforms
the other models on this dataset.
When considering the two datasets, sparsity
seems a likely explanation for this difference in
results: In the MPQA experiment most instances
are very short with an average length of 3 words,
while the average sentence length in the SP corpus
is 21 words. The MPQA task is further simplified
through the use or an additional sentiment lexicon.
Considering dictionary size, the SP corpus has a
dictionary of 22k words, more than three times the
size of the MPQA dictionary.
4http://www.metaoptimize.com/projects/
wordreprs/
5http://mpqa.cs.pitt.edu/
6http://www.cs.cornell.edu/people/
pabo/movie-review-data/
899
Method MPQA SP
Voting with two lexica 81.7 63.1
MV-RNN (Socher et al, 2012b) - 79.0
RAE (rand) (Socher et al, 2011b) 85.7 76.8
TCRF (Nakagawa et al, 2010) 86.1 77.3
RAE (init) (Socher et al, 2011b) 86.4 77.7
NB (Wang and Manning, 2012) 86.7 79.4
CCAE-A 86.3 77.8
CCAE-B 87.1 77.1
CCAE-C 87.1 77.3
CCAE-D 87.2 76.7
Table 3: Accuracy of sentiment classification on
the sentiment polarity (SP) and MPQA datasets.
For NB we only display the best result among a
larger group of models analysed in that paper.
This issue of sparsity is exacerbated in the more
complex CCAE models, where the training points
are spread across different CCG types and rules.
While the initialization of the word vectors with
previously learned embeddings (as was previously
shown by Socher et al (2011b)) helps the mod-
els, all other model variables such as composition
weights and biases are still initialised randomly
and thus highly dependent on the amount of train-
ing data available.
Experiment 2: Pretraining Due to our analy-
sis of the results of the initial experiment, we ran a
second series of experiments on the SP corpus. We
follow (Scheible and Schu?tze, 2013) for this sec-
ond series of experiments, which are carried out on
a random 90/10 training-testing split, with some
data reserved for development.
Instead of initialising the model with external
word embeddings, we first train it on a large
amount of data with the aim of overcoming the
sparsity issues encountered in the previous exper-
iment. Learning is thus divided into two steps:
The first, unsupervised training phase, uses the
British National Corpus together with the SP cor-
pus. In this phase only the reconstruction signal
is used to learn word embeddings and transforma-
tion matrices. Subsequently, in the second phase,
only the SP corpus is used, this time with both the
reconstruction and the label error.
By learning word embeddings and composition
matrices on more data, the model is likely to gen-
eralise better. Particularly for the more complex
models, where the composition functions are con-
ditioned on various CCG parameters, this should
Training
Model Regular Pretraining
CCAE-A 77.8 79.5
CCAE-B 76.9 79.8
CCAE-C 77.1 81.0
CCAE-D 76.9 79.7
Table 4: Effect of pretraining on model perfor-
mance on the SP dataset. Results are reported on a
random subsection of the SP corpus; thus numbers
for the regular training method differ slightly from
those in Table 3.
help to overcome issues of sparsity.
If we consider the results of the pre-trained ex-
periments in Table 4, this seems to be the case.
In fact, the trend of the previous results has been
reversed, with the more complex models now per-
forming best, whereas in the previous experiments
the simpler models performed better. Using the
Turian embeddings instead of random initialisa-
tion did not improve results in this setup.
5.2 Compound Similarity
In a second experiment we use the dataset from
Mitchell and Lapata (2010) which contains sim-
ilarity judgements for adjective-noun, noun-noun
and verb-object pairs.7 All compound pairs have
been ranked for semantic similarity by a number of
human annotators. The task is thus to rank these
pairs of word pairs by their semantic similarity.
For instance, the two compounds vast amount
and large quantity are given a high similarity score
by the human judges, while northern region and
early age are assigned no similarity at all.
We train our models as fully unsupervised au-
toencoders on the British National Corpus for this
task. We assume fixed parse trees for all of the
compounds (Figure 6), and use these to compute
compound level vectors for all word pairs. We
subsequently use the cosine distance between each
compound pair as our similarity measure. We
use Spearman?s rank correlation coefficient (?) for
evaluation; hence there is no need to rescale our
scores (-1.0 ? 1.0) to the original scale (1.0 ? 7.0).
Blacoe and Lapata (2012) have an extensive
comparison of the performance of various vector-
based models on this data set to which we compare
our model in Table 5. The CCAE models outper-
7http://homepages.inf.ed.ac.uk/mlap/
resources/index.html
900
Verb Object
VB NN
(S\NP)/NP N
NP
>
S\NP
Noun Noun
NN NN
N/N N
>N
Adjective Noun
JJ NN
N/N N
>N
Figure 6: Assumed CCG parse structure for the compound similarity evaluation.
Method Adj-N N-N V-Obj
Human 0.52 0.49 0.55
(Blacoe and Lapata, 2012)
/+ 0.21 - 0.48 0.22 - 0.50 0.18 - 0.35
RAE 0.19 - 0.31 0.24 - 0.30 0.09 - 0.28
CCAE-B 0.38 0.44 0.34
CCAE-C 0.38 0.41 0.23
CCAE-D 0.41 0.44 0.29
Table 5: Correlation coefficients of model predic-
tions for the compound similarity task. Numbers
show Spearman?s rank correlation coefficient (?).
Higher numbers indicate better correlation.
form the RAE models provided by Blacoe and La-
pata (2012), and score towards the upper end of the
range of other models considered in that paper.
5.3 Qualitative Analysis
To get better insight into our models we also per-
form a small qualitative analysis. Using one of the
models trained on the MPQA corpus, we gener-
ate word-level representations of all phrases in this
corpus and subsequently identify the most related
expressions by using the cosine distance measure.
We perform this experiment on all expressions of
length 5, considering all expressions with a word
length between 3 and 7 as potential matches.
As can be seen in Table 6, this works with vary-
ing success. Linking expressions such as convey-
ing the message of peace and safeguard(ing) peace
and security suggests that the model does learn
some form of semantics.
On the other hand, the connection between ex-
pressed their satisfaction and support and ex-
pressed their admiration and surprise suggests
that the pure word level content still has an impact
on the model analysis. Likewise, the expressions
is a story of success and is a staunch supporter
have some lexical but little semantic overlap. Fur-
ther reducing this link between the lexical and the
semantic representation is an issue that should be
addressed in future work in this area.
6 Discussion
Overall, our models compare favourably with the
state of the art. On the MPQA corpus model
CCAE-D achieves the best published results we
are aware of, whereas on the SP corpus we achieve
competitive results. With an additional, unsuper-
vised training step we achieved results beyond the
current state of the art on this task, too.
Semantics The qualitative analysis and the ex-
periment on compounds demonstrate that the
CCAE models are capable of learning semantics.
An advantage of our approach?and of autoen-
coders generally?is their ability to learn in an
unsupervised setting. The pre-training step for
the sentiment task was essentially the same train-
ing step as used in the compound similarity task.
While other models such as the MV-RNN (Socher
et al, 2012b) achieve good results on a particu-
lar task, they do not allow unsupervised training.
This prevents the possiblity of pretraining, which
we showed to have a big impact on results, and fur-
ther prevents the training of general models: The
CCAE models can be used for multiple tasks with-
out the need to re-train the main model.
Complexity Previously in this paper we argued
that our models combined the strengths of other
approaches. By using a grammar formalism we
increase the expressive power of the model while
the complexity remains low. For the complex-
ity analysis see Table 7. We strike a balance be-
tween the greedy approaches (e.g. Socher et al
(2011b)), where learning is quadratic in the length
of each sentence and existing syntax-driven ap-
proaches such as the MV-RNN of Socher et al
(2012b), where the size of the model, that is the
number of variables that needs to be learned, is
quadratic in the size of the word-embeddings.
Sparsity Parametrizing on CCG types and rules
increases the size of the model compared to a
greedy RAE (Socher et al, 2011b). The effect
of this was highlighted by the sentiment analysis
task, with the more complex models performing
901
Expression Most Similar
convey the message of peace safeguard peace and security
keep alight the flame of keep up the hope
has a reason to repent has no right
a significant and successful strike a much better position
it is reassuring to believe it is a positive development
expressed their satisfaction and support expressed their admiration and surprise
is a story of success is a staunch supporter
are lining up to condemn are going to voice their concerns
more sanctions should be imposed charges being leveled
could fray the bilateral goodwill could cause serious damage
Table 6: Phrases from the MPQA corpus and their semantically closest match according to CCAE-D.
Complexity
Model Size Learning
MV-RNN O(nw2) O(l)
RAE O(nw) O(l2)
CCAE-* O(nw) O(l)
Table 7: Comparison of models. n is dictionary
size, w embedding width, l is sentence length. We
can assume l  n  w. Additional factors such
as CCG rules and types are treated as small con-
stants for the purposes of this analysis.
worse in comparison with the simpler ones. We
were able to overcome this issue by using addi-
tional training data. Beyond this, it would also be
interesting to investigate the relationships between
different types and to derive functions to incorpo-
rate this into the learning procedure. For instance
model learning could be adjusted to enforce some
mirroring effects between the weight matrices of
forward and backward application, or to support
similarities between those of forward application
and composition.
CCG-Vector Interface Exactly how the infor-
mation contained in a CCG derivation is best ap-
plied to a vector space model of compositionality
is another issue for future research. Our investi-
gation of this matter by exploring different model
setups has proved somewhat inconclusive. While
CCAE-D incorporated the deepest conditioning on
the CCG structure, it did not decisively outperform
the simpler CCAE-B which just conditioned on
the combinatory operators. Issues of sparsity, as
shown in our experiments on pretraining, have a
significant influence, which requires further study.
7 Conclusion
In this paper we have brought a more formal no-
tion of semantic compositionality to vector space
models based on recursive autoencoders. This was
achieved through the use of the CCG formalism
to provide a conditioning structure for the matrix
vector products that define the RAE.
We have explored a number of models, each of
which conditions the compositional operations on
different aspects of the CCG derivation. Our ex-
perimental findings indicate a clear advantage for
a deeper integration of syntax over models that use
only the bracketing structure of the parse tree.
The most effective way to condition the compo-
sitional operators on the syntax remains unclear.
Once the issue of sparsity had been addressed, the
complex models outperformed the simpler ones.
Among the complex models, however, we could
not establish significant or consistent differences
to convincingly argue for a particular approach.
While the connections between formal linguis-
tics and vector space approaches to NLP may not
be immediately obvious, we believe that there is a
case for the continued investigation of ways to best
combine these two schools of thought. This paper
represents one step towards the reconciliation of
traditional formal approaches to compositional se-
mantics with modern machine learning.
Acknowledgements
We thank the anonymous reviewers for their feed-
back and Richard Socher for providing additional
insight into his models. Karl Moritz would further
like to thank Sebastian Riedel for hosting him at
UCL while this paper was written. This work has
been supported by the EPSRC.
902
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of LREC, pages
3196?3200, Istanbul, Turkey.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems 19, pages 153?160.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP-CoNLL,
pages 546?556.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. CL, 33(4):493?552, December.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of ACL Demo and Poster
Sessions, pages 33?36.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
55. 1952-59:1?32.
Gottfried Frege. 1892. U?ber Sinn und Bedeutung. In
Mark Textor, editor, Funktion - Begriff - Bedeutung,
volume 4 of Sammlung Philosophie. Vandenhoeck
& Ruprecht, Go?ttingen.
Christoph Goller and Andreas Ku?chler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proceedings
of the ICNN-96, pages 347?352. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Geoffrey E. Hinton, Simon Osindero, Max Welling,
and Yee Whye Teh. 2006. Unsupervised discovery
of nonlinear structure using contrastive backpropa-
gation. Cognitive Science, 30(4):725?731.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
CL, 33(3):355?396, September.
Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge, MA.
Marcus Kracht. 2008. Compositionality in Montague
Grammar. In Edouard Machery und Markus Wern-
ing Wolfram Hinzen, editor, Handbook of Composi-
tionality, pages 47 ? 63. Oxford University Press.
Yann LeCun, Leon Bottou, Genevieve Orr, and Klaus-
Robert Muller. 1998. Efficient backprop. In G. Orr
and Muller K., editors, Neural Networks: Tricks of
the trade. Springer.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL,
pages 317?324.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings
of ACL, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
R. Montague. 1970. Universal grammar. Theoria,
36(3):373?398.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In NAACL-
HLT, pages 786?794.
Bo Pang and Lillian Lee. 2005. Seeing stars: exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115?124.
Francis Jeffry Pelletier. 1994. The principle of seman-
tic compositionality. Topoi, 13:11?24.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of ACL, ACL ?93, pages 183?190.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46:77?105.
Christian Scheible and Hinrich Schu?tze. 2013. Cutting
recursive autoencoder trees. In Proceedings of the
International Conference on Learning Representa-
tions.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. CL, 24(1):97?123, March.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011a.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in
Neural Information Processing Systems 24.
903
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
Richard Socher, Brody Huval, Bharath Bhat, Christo-
pher D. Manning, and Andrew Y. Ng. 2012a.
Convolutional-Recursive Deep Learning for 3D Ob-
ject Classification. In Advances in Neural Informa-
tion Processing Systems 25.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012b. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of EMNLP-CoNLL, pages 1201?
1211.
Mark Steedman and Jason Baldridge, 2011. Combina-
tory Categorial Grammar, pages 181?224. Wiley-
Blackwell.
Anna Szabolcsi. 1989. Bound Variables in Syntax:
Are There Any? In Renate Bartsch, Johan van Ben-
them, and Peter van Emde Boas, editors, Semantics
and Contextual Expression, pages 295?318. Foris,
Dordrecht.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384?394.
Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: simple, good sentiment and topic
classification. In Proceedings of ACL, pages 90?94.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of EMNLP-
HLT, HLT ?05, pages 347?354.
904
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 58?68,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Multilingual Models for Compositional Distributed Semantics
Karl Moritz Hermann and Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
Abstract
We present a novel technique for learn-
ing semantic representations, which ex-
tends the distributional hypothesis to mul-
tilingual data and joint-space embeddings.
Our models leverage parallel data and
learn to strongly align the embeddings of
semantically equivalent sentences, while
maintaining sufficient distance between
those of dissimilar sentences. The mod-
els do not rely on word alignments or
any syntactic information and are success-
fully applied to a number of diverse lan-
guages. We extend our approach to learn
semantic representations at the document
level, too. We evaluate these models on
two cross-lingual document classification
tasks, outperforming the prior state of the
art. Through qualitative analysis and the
study of pivoting effects we demonstrate
that our representations are semantically
plausible and can capture semantic rela-
tionships across languages without paral-
lel data.
1 Introduction
Distributed representations of words provide the
basis for many state-of-the-art approaches to var-
ious problems in natural language processing to-
day. Such word embeddings are naturally richer
representations than those of symbolic or discrete
models, and have been shown to be able to capture
both syntactic and semantic information. Success-
ful applications of such models include language
modelling (Bengio et al, 2003), paraphrase detec-
tion (Erk and Pad?o, 2008), and dialogue analysis
(Kalchbrenner and Blunsom, 2013).
Within a monolingual context, the distributional
hypothesis (Firth, 1957) forms the basis of most
approaches for learning word representations. In
Figure 1: Model with parallel input sentences a and b. The
model minimises the distance between the sentence level en-
coding of the bitext. Any composition functions (CVM) can
be used to generate the compositional sentence level repre-
sentations.
this work, we extend this hypothesis to multilin-
gual data and joint-space embeddings. We present
a novel unsupervised technique for learning se-
mantic representations that leverages parallel cor-
pora and employs semantic transfer through com-
positional representations. Unlike most methods
for learning word representations, which are re-
stricted to a single language, our approach learns
to represent meaning across languages in a shared
multilingual semantic space.
We present experiments on two corpora. First,
we show that for cross-lingual document clas-
sification on the Reuters RCV1/RCV2 corpora
(Lewis et al, 2004), we outperform the prior state
of the art (Klementiev et al, 2012). Second,
we also present classification results on a mas-
sively multilingual corpus which we derive from
the TED corpus (Cettolo et al, 2012). The re-
sults on this task, in comparison with a number of
strong baselines, further demonstrate the relevance
of our approach and the success of our method
in learning multilingual semantic representations
over a wide range of languages.
58
2 Overview
Distributed representation learning describes the
task of learning continuous representations for dis-
crete objects. Here, we focus on learning seman-
tic representations and investigate how the use of
multilingual data can improve learning such rep-
resentations at the word and higher level. We
present a model that learns to represent each
word in a lexicon by a continuous vector in R
d
.
Such distributed representations allow a model to
share meaning between similar words, and have
been used to capture semantic, syntactic and mor-
phological content (Collobert and Weston, 2008;
Turian et al, 2010, inter alia).
We describe a multilingual objective function
that uses a noise-contrastive update between se-
mantic representations of different languages to
learn these word embeddings. As part of this, we
use a compositional vector model (CVM, hence-
forth) to compute semantic representations of sen-
tences and documents. A CVM learns seman-
tic representations of larger syntactic units given
the semantic representations of their constituents
(Clark and Pulman, 2007; Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Grefenstette
and Sadrzadeh, 2011; Socher et al, 2012; Her-
mann and Blunsom, 2013, inter alia).
A key difference between our approach and
those listed above is that we only require sentence-
aligned parallel data in our otherwise unsuper-
vised learning function. This removes a number of
constraints that normally come with CVM mod-
els, such as the need for syntactic parse trees, word
alignment or annotated data as a training signal.
At the same time, by using multiple CVMs to
transfer information between languages, we en-
able our models to capture a broader semantic con-
text than would otherwise be possible.
The idea of extracting semantics from multilin-
gual data stems from prior work in the field of
semantic grounding. Language acquisition in hu-
mans is widely seen as grounded in sensory-motor
experience (Bloom, 2001; Roy, 2003). Based
on this idea, there have been some attempts at
using multi-modal data for learning better vec-
tor representations of words (e.g. Srivastava and
Salakhutdinov (2012)). Such methods, however,
are not easily scalable across languages or to large
amounts of data for which no secondary or tertiary
representation might exist.
Parallel data in multiple languages provides an
alternative to such secondary representations, as
parallel texts share their semantics, and thus one
language can be used to ground the other. Some
work has exploited this idea for transferring lin-
guistic knowledge into low-resource languages or
to learn distributed representations at the word
level (Klementiev et al, 2012; Zou et al, 2013;
Lauly et al, 2013, inter alia). So far almost all
of this work has been focused on learning multi-
lingual representations at the word level. As dis-
tributed representations of larger expressions have
been shown to be highly useful for a number of
tasks, it seems to be a natural next step to attempt
to induce these, too, cross-lingually.
3 Approach
Most prior work on learning compositional se-
mantic representations employs parse trees on
their training data to structure their composition
functions (Socher et al, 2012; Hermann and Blun-
som, 2013, inter alia). Further, these approaches
typically depend on specific semantic signals such
as sentiment- or topic-labels for their objective
functions. While these methods have been shown
to work in some cases, the need for parse trees and
annotated data limits such approaches to resource-
fortunate languages. Our novel method for learn-
ing compositional vectors removes these require-
ments, and as such can more easily be applied to
low-resource languages.
Specifically, we attempt to learn semantics from
multilingual data. The idea is that, given enough
parallel data, a shared representation of two paral-
lel sentences would be forced to capture the com-
mon elements between these two sentences. What
parallel sentences share, of course, are their se-
mantics. Naturally, different languages express
meaning in different ways. We utilise this di-
versity to abstract further from mono-lingual sur-
face realisations to deeper semantic representa-
tions. We exploit this semantic similarity across
languages by defining a bilingual (and trivially
multilingual) energy as follows.
Assume two functions f : X ? R
d
and
g : Y ? R
d
, which map sentences from lan-
guages x and y onto distributed semantic
representations in R
d
. Given a parallel corpus C,
we then define the energy of the model given two
sentences (a, b) ? C as:
E
bi
(a, b) = ?f(a)? g(b)?
2
(1)
59
We want to minimize E
bi
for all semantically
equivalent sentences in the corpus. In order to
prevent the model from degenerating, we fur-
ther introduce a noise-constrastive large-margin
update which ensures that the representations of
non-aligned sentences observe a certain margin
from each other. For every pair of parallel sen-
tences (a, b) we sample a number of additional
sentence pairs (?, n) ? C, where n?with high
probability?is not semantically equivalent to a.
We use these noise samples as follows:
E
hl
(a, b, n) = [m+ E
bi
(a, b)? E
bi
(a, n)]
+
where [x]
+
= max(x, 0) denotes the standard
hinge loss and m is the margin. This results in
the following objective function:
J(?) =
?
(a,b)?C
(
k
?
i=1
E
hl
(a, b, n
i
) +
?
2
???
2
)
(2)
where ? is the set of all model variables.
3.1 Two Composition Models
The objective function in Equation 2 could be cou-
pled with any two given vector composition func-
tions f, g from the literature. As we aim to apply
our approach to a wide range of languages, we fo-
cus on composition functions that do not require
any syntactic information. We evaluate the follow-
ing two composition functions.
The first model, ADD, represents a sentence by
the sum of its word vectors. This is a distributed
bag-of-words approach as sentence ordering is not
taken into account by the model.
Second, the BI model is designed to capture bi-
gram information, using a non-linearity over bi-
gram pairs in its composition function:
f(x) =
n
?
i=1
tanh (x
i?1
+ x
i
) (3)
The use of a non-linearity enables the model to
learn interesting interactions between words in a
document, which the bag-of-words approach of
ADD is not capable of learning. We use the hy-
perbolic tangent as activation function.
3.2 Document-level Semantics
For a number of tasks, such as topic modelling,
representations of objects beyond the sentence
level are required. While most approaches to com-
positional distributed semantics end at the word
Figure 2: Description of a parallel document-level composi-
tional vector model (DOC). The model recursively computes
semantic representations for each sentence of a document and
then for the document itself, treating the sentence vectors as
inputs for a second CVM.
level, our model extends to document-level learn-
ing quite naturally, by recursively applying the
composition and objective function (Equation 2)
to compose sentences into documents. This is
achieved by first computing semantic representa-
tions for each sentence in a document. Next, these
representations are used as inputs in a higher-level
CVM, computing a semantic representation of a
document (Figure 2).
This recursive approach integrates document-
level representations into the learning process.
We can thus use corpora of parallel documents?
regardless of whether they are sentence aligned or
not?to propagate a semantic signal back to the
individual words. If sentence alignment is avail-
able, of course, the document-signal can simply
be combined with the sentence-signal, as we did
with the experiments described in ?5.3.
This concept of learning compositional repre-
sentations for documents contrasts with prior work
(Socher et al, 2011; Klementiev et al, 2012, inter
alia) who rely on summing or averaging sentence-
vectors if representations beyond the sentence-
level are required for a particular task.
We evaluate the models presented in this paper
both with and without the document-level signal.
We refer to the individual models used as ADD and
BI if used without, and as DOC/ADD and DOC/BI
is used with the additional document composition
function and error signal.
4 Corpora
We use two corpora for learning semantic rep-
resentations and performing the experiments de-
scribed in this paper.
60
The Europarl corpus v7
1
(Koehn, 2005) was
used during initial development and testing of
our approach, as well as to learn the representa-
tions used for the Cross-Lingual Document Clas-
sification task described in ?5.2. We considered
the English-German and English-French language
pairs from this corpus. From each pair the final
100,000 sentences were reserved for development.
Second, we developed a massively multilin-
gual corpus based on the TED corpus
2
for IWSLT
2013 (Cettolo et al, 2012). This corpus contains
English transcriptions and multilingual, sentence-
aligned translations of talks from the TED confer-
ence. While the corpus is aimed at machine trans-
lation tasks, we use the keywords associated with
each talk to build a subsidiary corpus for multilin-
gual document classification as follows.
3
The development sections provided with the
IWSLT 2013 corpus were again reserved for de-
velopment. We removed approximately 10 per-
cent of the training data in each language to cre-
ate a test corpus (all talks with id ? 1,400). The
new training corpus consists of a total of 12,078
parallel documents distributed across 12 language
pairs
4
. In total, this amounts to 1,678,219 non-
English sentences (the number of unique English
sentences is smaller as many documents are trans-
lated into multiple languages and thus appear re-
peatedly in the corpus). Each document (talk) con-
tains one or several keywords. We used the 15
most frequent keywords for the topic classification
experiments described in section ?5.3.
Both corpora were pre-processed using the set
of tools provided by cdec
5
for tokenizing and low-
ercasing the data. Further, all empty sentences and
their translations were removed from the corpus.
5 Experiments
We report results on two experiments. First, we
replicate the cross-lingual document classification
task of Klementiev et al (2012), learning dis-
tributed representations on the Europarl corpus
and evaluating on documents from the Reuters
RCV1/RCV2 corpora. Subsequently, we design a
1
http://www.statmt.org/europarl/
2
https://wit3.fbk.eu/
3
http://www.clg.ox.ac.uk/tedcldc/
4
English to Arabic, German, French, Spanish, Italian,
Dutch, Polish, Brazilian Portuguese, Romanian, Russian and
Turkish. Chinese, Farsi and Slowenian were removed due to
the small size of those datasets.
5
http://cdec-decoder.org/
multi-label classification task using the TED cor-
pus, both for training and evaluating. The use of
a wider range of languages in the second experi-
ments allows us to better evaluate our models? ca-
pabilities in learning a shared multilingual seman-
tic representation. We also investigate the learned
embeddings from a qualitative perspective in ?5.4.
5.1 Learning
All model weights were randomly initialised us-
ing a Gaussian distribution (?=0, ?
2
=0.1). We
used the available development data to set our
model parameters. For each positive sample we
used a number of noise samples (k ? {1, 10, 50}),
randomly drawn from the corpus at each training
epoch. All our embeddings have dimensionality
d=128, with the margin set to m=d.
6
Further, we
use L2 regularization with ?=1 and step-size in
{0.01, 0.05}. We use 100 iterations for the RCV
task, 500 for the TED single and 5 for the joint
corpora. We use the adaptive gradient method,
AdaGrad (Duchi et al, 2011), for updating the
weights of our models, in a mini-batch setting (b ?
{10, 50}). All settings, our model implementation
and scripts to replicate our experiments are avail-
able at http://www.karlmoritz.com/.
5.2 RCV1/RCV2 Document Classification
We evaluate our models on the cross-lingual doc-
ument classification (CLDC, henceforth) task first
described in Klementiev et al (2012). This task in-
volves learning language independent embeddings
which are then used for document classification
across the English-German language pair. For this,
CLDC employs a particular kind of supervision,
namely using supervised training data in one lan-
guage and evaluating without further supervision
in another. Thus, CLDC can be used to establish
whether our learned representations are semanti-
cally useful across multiple languages.
We follow the experimental setup described in
Klementiev et al (2012), with the exception that
we learn our embeddings using solely the Europarl
data and use the Reuters corpora only during for
classifier training and testing. Each document in
the classification task is represented by the aver-
age of the d-dimensional representations of all its
sentences. We train the multiclass classifier using
an averaged perceptron (Collins, 2002) with the
same settings as in Klementiev et al (2012).
6
On the RCV task we also report results for d=40 which
matches the dimensionality of Klementiev et al (2012).
61
Model en? de de? en
Majority Class 46.8 46.8
Glossed 65.1 68.6
MT 68.1 67.4
I-Matrix 77.6 71.1
dim = 40
ADD 83.7 71.4
ADD+ 86.2 76.9
BI 83.4 69.2
BI+ 86.9 74.3
dim = 128
ADD 86.4 74.7
ADD+ 87.7 77.5
BI 86.1 79.0
BI+ 88.1 79.2
Table 1: Classification accuracy for training on English and
German with 1000 labeled examples on the RCV corpus.
Cross-lingual compositional representations (ADD, BI and
their multilingual extensions), I-Matrix (Klementiev et al,
2012) translated (MT) and glossed (Glossed) word baselines,
and the majority class baseline. The baseline results are from
Klementiev et al (2012).
We present results from four models. The ADD
model is trained on 500k sentence pairs of the
English-German parallel section of the Europarl
corpus. The ADD+ model uses an additional 500k
parallel sentences from the English-French cor-
pus, resulting in one million English sentences,
each paired up with either a German or a French
sentence, with BI and BI+ trained accordingly.
The motivation behind ADD+ and BI+ is to inves-
tigate whether we can learn better embeddings by
introducing additional data from other languages.
A similar idea exists in machine translation where
English is frequently used to pivot between other
languages (Cohn and Lapata, 2007).
The actual CLDC experiments are performed
by training on English and testing on German doc-
uments and vice versa. Following prior work, we
use varying sizes between 100 and 10,000 docu-
ments when training the multiclass classifier. The
results of this task across training sizes are in Fig-
ure 3. Table 1 shows the results for training on
1,000 documents compared with the results pub-
lished in Klementiev et al (2012). Our models
outperform the prior state of the art, with the BI
models performing slightly better than the ADD
models. As the relative results indicate, the addi-
tion of a second language improves model perfor-
mance. It it interesting to note that results improve
in both directions of the task, even though no addi-
tional German data was used for the ?+? models.
5.3 TED Corpus Experiments
Here we describe our experiments on the TED cor-
pus, which enables us to scale up to multilingual
learning. Consisting of a large number of rela-
tively short and parallel documents, this corpus al-
lows us to evaluate the performance of the DOC
model described in ?3.2.
We use the training data of the corpus to learn
distributed representations across 12 languages.
Training is performed in two settings. In the sin-
gle mode, vectors are learnt from a single lan-
guage pair (en-X), while in the joint mode vector-
learning is performed on all parallel sub-corpora
simultaneously. This setting causes words from
all languages to be embedded in a single semantic
space.
First, we evaluate the effect of the document-
level error signal (DOC, described in ?3.2), as well
as whether our multilingual learning method can
extend to a larger variety of languages. We train
DOC models, using both ADD and BI as CVM
(DOC/ADD, DOC/BI), both in the single and joint
mode. For comparison, we also train ADD and
DOC models without the document-level error sig-
nal. The resulting document-level representations
are used to train classifiers (system and settings as
in ?5.2) for each language, which are then evalu-
ated in the paired language. In the English case
we train twelve individual classifiers, each using
the training data of a single language pair only.
As described in ?4, we use 15 keywords for the
classification task. Due to space limitations, we
report cumulative results in the form of F1-scores
throughout this paper.
MT System We develop a machine translation
baseline as follows. We train a machine translation
tool on the parallel training data, using the devel-
opment data of each language pair to optimize the
translation system. We use the cdec decoder (Dyer
et al, 2010) with default settings for this purpose.
With this system we translate the test data, and
then use a Na??ve Bayes classifier
7
for the actual
experiments. To exemplify, this means the de?ar
result is produced by training a translation system
from Arabic to German. The Arabic test set is
translated into German. A classifier is then trained
7
We use the implementation in Mallet (McCallum, 2002)
62
Setting Languages
Arabic German Spanish French Italian Dutch Polish Pt-Br Roman. Russian Turkish
en? L2
MT System 0.429 0.465 0.518 0.526 0.514 0.505 0.445 0.470 0.493 0.432 0.409
ADD single 0.328 0.343 0.401 0.275 0.282 0.317 0.141 0.227 0.282 0.338 0.241
BI single 0.375 0.360 0.379 0.431 0.465 0.421 0.435 0.329 0.426 0.423 0.481
DOC/ADD single 0.410 0.424 0.383 0.476 0.485 0.264 0.402 0.354 0.418 0.448 0.452
DOC/BI single 0.389 0.428 0.416 0.445 0.473 0.219 0.403 0.400 0.467 0.421 0.457
DOC/ADD joint 0.392 0.405 0.443 0.447 0.475 0.453 0.394 0.409 0.446 0.476 0.417
DOC/BI joint 0.372 0.369 0.451 0.429 0.404 0.433 0.417 0.399 0.453 0.439 0.418
L2 ? en
MT System 0.448 0.469 0.486 0.358 0.481 0.463 0.460 0.374 0.486 0.404 0.441
ADD single 0.380 0.337 0.446 0.293 0.357 0.295 0.327 0.235 0.293 0.355 0.375
BI single 0.354 0.411 0.344 0.426 0.439 0.428 0.443 0.357 0.426 0.442 0.403
DOC/ADD single 0.452 0.476 0.422 0.464 0.461 0.251 0.400 0.338 0.407 0.471 0.435
DOC/BI single 0.406 0.442 0.365 0.479 0.460 0.235 0.393 0.380 0.426 0.467 0.477
DOC/ADD joint 0.396 0.388 0.399 0.415 0.461 0.478 0.352 0.399 0.412 0.343 0.343
DOC/BI joint 0.343 0.375 0.369 0.419 0.398 0.438 0.353 0.391 0.430 0.375 0.388
Table 2: F1-scores for the TED document classification task for individual languages. Results are re-
ported for both directions (training on English, evaluating on L2 and vice versa). Bold indicates best
result, underline best result amongst the vector-based systems.
Training
Language
Test Language
Arabic German Spanish French Italian Dutch Polish Pt-Br Rom?n Russian Turkish
Arabic 0.378 0.436 0.432 0.444 0.438 0.389 0.425 0.420 0.446 0.397
German 0.368 0.474 0.460 0.464 0.440 0.375 0.417 0.447 0.458 0.443
Spanish 0.353 0.355 0.420 0.439 0.435 0.415 0.390 0.424 0.427 0.382
French 0.383 0.366 0.487 0.474 0.429 0.403 0.418 0.458 0.415 0.398
Italian 0.398 0.405 0.461 0.466 0.393 0.339 0.347 0.376 0.382 0.352
Dutch 0.377 0.354 0.463 0.464 0.460 0.405 0.386 0.415 0.407 0.395
Polish 0.359 0.386 0.449 0.444 0.430 0.441 0.401 0.434 0.398 0.408
Portuguese 0.391 0.392 0.476 0.447 0.486 0.458 0.403 0.457 0.431 0.431
Romanian 0.416 0.320 0.473 0.476 0.460 0.434 0.416 0.433 0.444 0.402
Russian 0.372 0.352 0.492 0.427 0.438 0.452 0.430 0.419 0.441 0.447
Turkish 0.376 0.352 0.479 0.433 0.427 0.423 0.439 0.367 0.434 0.411
Table 3: F1-scores for TED corpus document classification results when training and testing on two
languages that do not share any parallel data. We train a DOC/ADD model on all en-L2 language pairs
together, and then use the resulting embeddings to train document classifiers in each language. These
classifiers are subsequently used to classify data from all other languages.
Setting Languages
English Arabic German Spanish French Italian Dutch Polish Pt-Br Roman. Russian Turkish
Raw Data NB 0.481 0.469 0.471 0.526 0.532 0.524 0.522 0.415 0.465 0.509 0.465 0.513
Senna 0.400
Polyglot 0.382 0.416 0.270 0.418 0.361 0.332 0.228 0.323 0.194 0.300 0.402 0.295
single Setting
DOC/ADD 0.462 0.422 0.429 0.394 0.481 0.458 0.252 0.385 0.363 0.431 0.471 0.435
DOC/BI 0.474 0.432 0.362 0.336 0.444 0.469 0.197 0.414 0.395 0.445 0.436 0.428
joint Setting
DOC/ADD 0.475 0.371 0.386 0.472 0.451 0.398 0.439 0.304 0.394 0.453 0.402 0.441
DOC/BI 0.378 0.329 0.358 0.472 0.454 0.399 0.409 0.340 0.431 0.379 0.395 0.435
Table 4: F1-scores on the TED corpus document classification task when training and evaluating on the
same language. Baseline embeddings are Senna (Collobert et al, 2011) and Polyglot (Al-Rfou? et al,
2013).
63
100 200 500 1000 5000
10k
60
70
80
Training Documents (de)
C
l
a
s
s
i
fi
c
a
t
i
o
n
A
c
c
u
r
a
c
y
(
%
)
100 200 500 1000 5000
10k
50
60
70
80
90
Training Documents (en)
ADD+ BI+ I-Matrix MT Glossed
Figure 3: Classification accuracy for a number of models (see Table 1 for model descriptions). The left chart shows results for
these models when trained on German data and evaluated on English data, the right chart vice versa.
on the German training data and evaluated on the
translated Arabic. While we developed this system
as a baseline, it must be noted that the classifier of
this system has access to significantly more infor-
mation (all words in the document) as opposed to
our models (one embedding per document), and
we do not expect to necessarily beat this system.
The results of this experiment are in Table 2.
When comparing the results between the ADD
model and the models trained using the document-
level error signal, the benefit of this additional sig-
nal becomes clear. The joint training mode leads
to a relative improvement when training on En-
glish data and evaluating in a second language.
This suggests that the joint mode improves the
quality of the English embeddings more than it
affects the L2-embeddings. More surprising, per-
haps, is the relative performance between the ADD
and BI composition functions, especially when
compared to the results in ?5.2, where the BI mod-
els relatively consistently performed better. We
suspect that the better performance of the additive
composition function on this task is related to the
smaller amount of training data available which
could cause sparsity issues for the bigram model.
As expected, the MT system slightly outper-
forms our models on most language pairs. How-
ever, the overall performance of the models is
comparable to that of the MT system. Consider-
ing the relative amount of information available
during the classifier training phase, this indicates
that our learned representations are semantically
useful, capturing almost the same amount of infor-
mation as available to the Na??ve Bayes classifier.
We next investigate linguistic transfer across
languages. We re-use the embeddings learned
with the DOC/ADD joint model from the previ-
ous experiment for this purpose, and train clas-
sifiers on all non-English languages using those
embeddings. Subsequently, we evaluate their per-
formance in classifying documents in the remain-
ing languages. Results for this task are in Table 3.
While the results across language-pairs might not
be very insightful, the overall good performance
compared with the results in Table 2 implies that
we learnt semantically meaningful vectors and in
fact a joint embedding space across thirteen lan-
guages.
In a third evaluation (Table 4), we apply the em-
beddings learnt with out models to a monolingual
classification task, enabling us to compare with
prior work on distributed representation learning.
In this experiment a classifier is trained in one lan-
guage and then evaluated in the same. We again
use a Na??ve Bayes classifier on the raw data to es-
tablish a reasonable upper bound.
We compare our embeddings with the SENNA
embeddings, which achieve state of the art per-
formance on a number of tasks (Collobert et al,
2011). Additionally, we use the Polyglot embed-
dings of Al-Rfou? et al (2013), who published
word embeddings across 100 languages, including
all languages considered in this paper. We repre-
sent each document by the mean of its word vec-
tors and then apply the same classifier training and
testing regime as with our models. Even though
both of these sets of embeddings were trained on
much larger datasets than ours, our models outper-
form these baselines on all languages?even out-
performing the Na??ve Bayes system on on several
64
Figure 4: t-SNE projections for a number of English, French
and German words as represented by the BI+ model. Even
though the model did not use any parallel French-German
data during training, it learns semantic similarity between
these two languages using English as a pivot, and semanti-
cally clusters words across all languages.
Figure 5: t-SNE projections for a number of short phrases in
three languages as represented by the BI+ model. The pro-
jection demonstrates linguistic transfer through a pivot by. It
separates phrases by gender (red for female, blue for male,
and green for neutral) and aligns matching phrases across lan-
guages.
languages. While this may partly be attributed to
the fact that our vectors were learned on in-domain
data, this is still a very positive outcome.
5.4 Linguistic Analysis
While the classification experiments focused on
establishing the semantic content of the sentence
level representations, we also want to briefly in-
vestigate the induced word embeddings. We use
the BI+ model trained on the Europarl corpus for
this purpose. Figure 4 shows the t-SNE projec-
tions for a number of English, French and German
words. Even though the model did not use any par-
allel French-German data during training, it still
managed to learn semantic word-word similarity
across these two languages.
Going one step further, Figure 5 shows t-SNE
projections for a number of short phrases in these
three languages. We use the English the presi-
dent and gender-specific expressions Mr President
and Madam President as well as gender-specific
equivalents in French and German. The projec-
tion demonstrates a number of interesting results:
First, the model correctly clusters the words into
three groups, corresponding to the three English
forms and their associated translations. Second, a
separation between genders can be observed, with
male forms on the bottom half of the chart and fe-
male forms on the top, with the neutral the presi-
dent in the vertical middle. Finally, if we assume
a horizontal line going through the president, this
line could be interpreted as a ?gender divide?, with
male and female versions of one expression mir-
roring each other on that line. In the case of the
president and its translations, this effect becomes
even clearer, with the neutral English expression
being projected close to the mid-point between
each other language?s gender-specific versions.
These results further support our hypothesis that
the bilingual contrastive error function can learn
semantically plausible embeddings and further-
more, that it can abstract away from mono-lingual
surface realisations into a shared semantic space
across languages.
6 Related Work
Distributed Representations Distributed repre-
sentations can be learned through a number of ap-
proaches. In their simplest form, distributional in-
formation from large corpora can be used to learn
embeddings, where the words appearing within a
certain window of the target word are used to com-
pute that word?s embedding. This is related to
topic-modelling techniques such as LSA (Dumais
et al, 1988), LSI, and LDA (Blei et al, 2003), but
these methods use a document-level context, and
tend to capture the topics a word is used in rather
than its more immediate syntactic context.
Neural language models are another popular ap-
proach for inducing distributed word representa-
tions (Bengio et al, 2003). They have received a
lot of attention in recent years (Collobert and We-
ston, 2008; Mnih and Hinton, 2009; Mikolov et
al., 2010, inter alia) and have achieved state of the
art performance in language modelling. Collobert
et al (2011) further popularised using neural net-
work architectures for learning word embeddings
from large amounts of largely unlabelled data by
showing the embeddings can then be used to im-
prove standard supervised tasks.
65
Unsupervised word representations can easily
be plugged into a variety of NLP related tasks.
Tasks, where the use of distributed representations
has resulted in improvements include topic mod-
elling (Blei et al, 2003) or named entity recogni-
tion (Turian et al, 2010; Collobert et al, 2011).
Compositional Vector Models For a number of
important problems, semantic representations of
individual words do not suffice, but instead a se-
mantic representation of a larger structure?e.g. a
phrase or a sentence?is required. Self-evidently,
sparsity prevents the learning of such representa-
tions using the same collocational methods as ap-
plied to the word level. Most literature instead fo-
cuses on learning composition functions that rep-
resent the semantics of a larger structure as a func-
tion of the representations of its parts.
Very simple composition functions have been
shown to suffice for tasks such as judging bi-
gram semantic similarity (Mitchell and Lapata,
2008). More complex composition functions us-
ing matrix-vector composition, convolutional neu-
ral networks or tensor composition have proved
useful in tasks such as sentiment analysis (Socher
et al, 2011; Hermann and Blunsom, 2013), rela-
tional similarity (Turney, 2012) or dialogue analy-
sis (Kalchbrenner and Blunsom, 2013).
Multilingual Representation Learning Most
research on distributed representation induction
has focused on single languages. English, with its
large number of annotated resources, has enjoyed
most attention. However, there exists a corpus of
prior work on learning multilingual embeddings
or on using parallel data to transfer linguistic in-
formation across languages. One has to differen-
tiate between approaches such as Al-Rfou? et al
(2013), that learn embeddings across a large va-
riety of languages and models such as ours, that
learn joint embeddings, that is a projection into a
shared semantic space across multiple languages.
Related to our work, Yih et al (2011) proposed
S2Nets to learn joint embeddings of tf-idf vectors
for comparable documents. Their architecture op-
timises the cosine similarity of documents, using
relative semantic similarity scores during learn-
ing. More recently, Lauly et al (2013) proposed a
bag-of-words autoencoder model, where the bag-
of-words representation in one language is used to
train the embeddings in another. By placing their
vocabulary in a binary branching tree, the prob-
abilistic setup of this model is similar to that of
Mnih and Hinton (2009). Similarly, Sarath Chan-
dar et al (2013) train a cross-lingual encoder,
where an autoencoder is used to recreate words in
two languages in parallel. This is effectively the
linguistic extension of Ngiam et al (2011), who
used a similar method for audio and video data.
Hermann and Blunsom (2014) propose a large-
margin learner for multilingual word representa-
tions, similar to the basic additive model proposed
here, which, like the approaches above, relies on a
bag-of-words model for sentence representations.
Klementiev et al (2012), our baseline in ?5.2,
use a form of multi-agent learning on word-
aligned parallel data to transfer embeddings from
one language to another. Earlier work, Haghighi
et al (2008), proposed a method for inducing
bilingual lexica using monolingual feature repre-
sentations and a small initial lexicon to bootstrap
with. This approach has recently been extended
by Mikolov et al (2013a), Mikolov et al (2013b),
who developed a method for learning transforma-
tion matrices to convert semantic vectors of one
language into those of another. Is was demon-
strated that this approach can be applied to im-
prove tasks related to machine translation. Their
CBOW model is also worth noting for its sim-
ilarities to the ADD composition function used
here. Using a slightly different approach, Zou et
al. (2013), also learned bilingual embeddings for
machine translation.
7 Conclusion
To summarize, we have presented a novel method
for learning multilingual word embeddings using
parallel data in conjunction with a multilingual ob-
jective function for compositional vector models.
This approach extends the distributional hypoth-
esis to multilingual joint-space representations.
Coupled with very simple composition functions,
vectors learned with this method outperform the
state of the art on the task of cross-lingual docu-
ment classification. Further experiments and anal-
ysis support our hypothesis that bilingual signals
are a useful tool for learning distributed represen-
tations by enabling models to abstract away from
mono-lingual surface realisations into a deeper se-
mantic space.
Acknowledgements
This work was supported by a Xerox Foundation
Award and EPSRC grant number EP/K036580/1.
66
References
R. Al-Rfou?, B. Perozzi, and S. Skiena. 2013. Poly-
glot: Distributed word representations for multilin-
gual nlp. In Proceedings of CoNLL.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155,
March.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
P. Bloom. 2001. Precis of how children learn the
meanings of words. Behavioral and Brain Sciences,
24:1095?1103.
M. Cettolo, C. Girardi, and M. Federico. 2012. Wit
3
:
Web inventory of transcribed and translated talks. In
Proceedings of EAMT.
S. Clark and S. Pulman. 2007. Combining symbolic
and distributional models of meaning. In Proceed-
ings of AAAI Spring Symposium on Quantum Inter-
action. AAAI Press.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: Making effective use of multi-parallel
corpora. In Proceedings of ACL.
M. Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of ACL-
EMNLP.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Re-
search, 12:2121?2159, July.
S. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deer-
wester, and R. Harshman. 1988. Using latent se-
mantic analysis to improve access to textual infor-
mation. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A Decoder, Alignment, and
Learning framework for finite-state and context-free
translation models. In Proceedings of ACL.
K. Erk and S. Pad?o. 2008. A structured vector space
model for word meaning in context. Proceedings of
EMNLP.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
55. 1952-59:1?32.
E. Grefenstette and M. Sadrzadeh. 2011. Experi-
mental support for a categorical compositional dis-
tributional model of meaning. In Proceedings of
EMNLP.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of ACL-HLT.
K. M. Hermann and P. Blunsom. 2013. The Role of
Syntax in Vector Space Models of Compositional
Semantics. In Proceedings of ACL.
K. M. Hermann and P. Blunsom. 2014. Multilingual
Distributed Representations without Word Align-
ment. In Proceedings of ICLR.
N. Kalchbrenner and P. Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. Proceedings of the ACL Workshop on
Continuous Vector Space Models and their Compo-
sitionality.
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of COLING.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Proceedings of the
Machine Translation Summit.
S. Lauly, A. Boulanger, and H. Larochelle. 2013.
Learning multilingual word representations using a
bag-of-words autoencoder. In Deep Learning Work-
shop at NIPS.
D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. 2004.
Rcv1: A new benchmark collection for text catego-
rization research. Journal of Machine Learning Re-
search, 5:361?397, December.
A. K. McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
T. Mikolov, M. Karafi?at, L. Burget, J.
?
Cernock?y, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of INTER-
SPEECH.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient Estimation of Word Representations in
Vector Space. CoRR.
T. Mikolov, Q. V. Le, and I. Sutskever. 2013b. Ex-
ploiting Similarities among Languages for Machine
Translation. CoRR.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In In Proceedings of ACL.
67
A. Mnih and G. Hinton. 2009. A scalable hierarchi-
cal distributed language model. In Proceedings of
NIPS.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and
A. Y. Ng. 2011. Multimodal deep learning. In
ICML.
D. Roy. 2003. Grounded spoken language acquisition:
Experiments in word learning. IEEE Transactions
on Multimedia, 5(2):197?209, June.
A. P. Sarath Chandar, M. K. Mitesh, B. Ravindran,
V. Raykar, and A. Saha. 2013. Multilingual deep
learning. In Deep Learning Workshop at NIPS.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In Proceedings of EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Proceedings of EMNLP-
CoNLL, pages 1201?1211.
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In Pro-
ceedings of NIPS.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL.
P. D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
W.-T. Yih, K. Toutanova, J. C. Platt, and C. Meek.
2011. Learning discriminative projections for text
similarity measures. In Proceedings of CoNLL.
W. Y. Zou, R. Socher, D. Cer, and C. D. Manning.
2013. Bilingual word embeddings for phrase-based
machine translation. In Proceedings of EMNLP.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655?665,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Convolutional Neural Network for Modelling Sentences
Nal Kalchbrenner Edward Grefenstette
{nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.uk
Department of Computer Science
University of Oxford
Phil Blunsom
Abstract
The ability to accurately represent sen-
tences is central to language understand-
ing. We describe a convolutional architec-
ture dubbed the Dynamic Convolutional
Neural Network (DCNN) that we adopt
for the semantic modelling of sentences.
The network uses Dynamic k-Max Pool-
ing, a global pooling operation over lin-
ear sequences. The network handles input
sentences of varying length and induces
a feature graph over the sentence that is
capable of explicitly capturing short and
long-range relations. The network does
not rely on a parse tree and is easily ap-
plicable to any language. We test the
DCNN in four experiments: small scale
binary and multi-class sentiment predic-
tion, six-way question classification and
Twitter sentiment prediction by distant su-
pervision. The network achieves excellent
performance in the first three tasks and a
greater than 25% error reduction in the last
task with respect to the strongest baseline.
1 Introduction
The aim of a sentence model is to analyse and
represent the semantic content of a sentence for
purposes of classification or generation. The sen-
tence modelling problem is at the core of many
tasks involving a degree of natural language com-
prehension. These tasks include sentiment analy-
sis, paraphrase detection, entailment recognition,
summarisation, discourse analysis, machine trans-
lation, grounded language learning and image re-
trieval. Since individual sentences are rarely ob-
served or not observed at all, one must represent
a sentence in terms of features that depend on the
words and short n-grams in the sentence that are
frequently observed. The core of a sentence model
involves a feature function that defines the process
 The  cat  sat  on  the  red  mat  The  cat  sat  on  the  red  mat
Figure 1: Subgraph of a feature graph induced
over an input sentence in a Dynamic Convolu-
tional Neural Network. The full induced graph
has multiple subgraphs of this kind with a distinct
set of edges; subgraphs may merge at different
layers. The left diagram emphasises the pooled
nodes. The width of the convolutional filters is 3
and 2 respectively. With dynamic pooling, a fil-
ter with small width at the higher layers can relate
phrases far apart in the input sentence.
by which the features of the sentence are extracted
from the features of the words or n-grams.
Various types of models of meaning have been
proposed. Composition based methods have been
applied to vector representations of word meaning
obtained from co-occurrence statistics to obtain
vectors for longer phrases. In some cases, com-
position is defined by algebraic operations over
word meaning vectors to produce sentence mean-
ing vectors (Erk and Pad?o, 2008; Mitchell and
Lapata, 2008; Mitchell and Lapata, 2010; Tur-
ney, 2012; Erk, 2012; Clarke, 2012). In other
cases, a composition function is learned and ei-
ther tied to particular syntactic relations (Guevara,
2010; Zanzotto et al, 2010) or to particular word
types (Baroni and Zamparelli, 2010; Coecke et
al., 2010; Grefenstette and Sadrzadeh, 2011; Kart-
saklis and Sadrzadeh, 2013; Grefenstette, 2013).
Another approach represents the meaning of sen-
tences by way of automatically extracted logical
forms (Zettlemoyer and Collins, 2005).
655
A central class of models are those based on
neural networks. These range from basic neu-
ral bag-of-words or bag-of-n-grams models to the
more structured recursive neural networks and
to time-delay neural networks based on convo-
lutional operations (Collobert and Weston, 2008;
Socher et al, 2011; Kalchbrenner and Blunsom,
2013b). Neural sentence models have a num-
ber of advantages. They can be trained to obtain
generic vectors for words and phrases by predict-
ing, for instance, the contexts in which the words
and phrases occur. Through supervised training,
neural sentence models can fine-tune these vec-
tors to information that is specific to a certain
task. Besides comprising powerful classifiers as
part of their architecture, neural sentence models
can be used to condition a neural language model
to generate sentences word by word (Schwenk,
2012; Mikolov and Zweig, 2012; Kalchbrenner
and Blunsom, 2013a).
We define a convolutional neural network archi-
tecture and apply it to the semantic modelling of
sentences. The network handles input sequences
of varying length. The layers in the network in-
terleave one-dimensional convolutional layers and
dynamic k-max pooling layers. Dynamic k-max
pooling is a generalisation of the max pooling op-
erator. The max pooling operator is a non-linear
subsampling function that returns the maximum
of a set of values (LeCun et al, 1998). The op-
erator is generalised in two respects. First, k-
max pooling over a linear sequence of values re-
turns the subsequence of k maximum values in the
sequence, instead of the single maximum value.
Secondly, the pooling parameter k can be dynam-
ically chosen by making k a function of other as-
pects of the network or the input.
The convolutional layers apply one-
dimensional filters across each row of features in
the sentence matrix. Convolving the same filter
with the n-gram at every position in the sentence
allows the features to be extracted independently
of their position in the sentence. A convolutional
layer followed by a dynamic pooling layer and
a non-linearity form a feature map. Like in the
convolutional networks for object recognition
(LeCun et al, 1998), we enrich the representation
in the first layer by computing multiple feature
maps with different filters applied to the input
sentence. Subsequent layers also have multiple
feature maps computed by convolving filters with
all the maps from the layer below. The weights at
these layers form an order-4 tensor. The resulting
architecture is dubbed a Dynamic Convolutional
Neural Network.
Multiple layers of convolutional and dynamic
pooling operations induce a structured feature
graph over the input sentence. Figure 1 illustrates
such a graph. Small filters at higher layers can cap-
ture syntactic or semantic relations between non-
continuous phrases that are far apart in the input
sentence. The feature graph induces a hierarchical
structure somewhat akin to that in a syntactic parse
tree. The structure is not tied to purely syntactic
relations and is internal to the neural network.
We experiment with the network in four set-
tings. The first two experiments involve predict-
ing the sentiment of movie reviews (Socher et
al., 2013b). The network outperforms other ap-
proaches in both the binary and the multi-class ex-
periments. The third experiment involves the cat-
egorisation of questions in six question types in
the TREC dataset (Li and Roth, 2002). The net-
work matches the accuracy of other state-of-the-
art methods that are based on large sets of en-
gineered features and hand-coded knowledge re-
sources. The fourth experiment involves predict-
ing the sentiment of Twitter posts using distant su-
pervision (Go et al, 2009). The network is trained
on 1.6 million tweets labelled automatically ac-
cording to the emoticon that occurs in them. On
the hand-labelled test set, the network achieves a
greater than 25% reduction in the prediction error
with respect to the strongest unigram and bigram
baseline reported in Go et al (2009).
The outline of the paper is as follows. Section 2
describes the background to the DCNN including
central concepts and related neural sentence mod-
els. Section 3 defines the relevant operators and
the layers of the network. Section 4 treats of the
induced feature graph and other properties of the
network. Section 5 discusses the experiments and
inspects the learnt feature detectors.
1
2 Background
The layers of the DCNN are formed by a convo-
lution operation followed by a pooling operation.
We begin with a review of related neural sentence
models. Then we describe the operation of one-
dimensional convolution and the classical Time-
Delay Neural Network (TDNN) (Hinton, 1989;
Waibel et al, 1990). By adding a max pooling
1
Code available at www.nal.co
656
layer to the network, the TDNN can be adopted as
a sentence model (Collobert and Weston, 2008).
2.1 Related Neural Sentence Models
Various neural sentence models have been de-
scribed. A general class of basic sentence models
is that of Neural Bag-of-Words (NBoW) models.
These generally consist of a projection layer that
maps words, sub-word units or n-grams to high
dimensional embeddings; the latter are then com-
bined component-wise with an operation such as
summation. The resulting combined vector is clas-
sified through one or more fully connected layers.
A model that adopts a more general structure
provided by an external parse tree is the Recursive
Neural Network (RecNN) (Pollack, 1990; K?uchler
and Goller, 1996; Socher et al, 2011; Hermann
and Blunsom, 2013). At every node in the tree the
contexts at the left and right children of the node
are combined by a classical layer. The weights of
the layer are shared across all nodes in the tree.
The layer computed at the top node gives a repre-
sentation for the sentence. The Recurrent Neural
Network (RNN) is a special case of the recursive
network where the structure that is followed is a
simple linear chain (Gers and Schmidhuber, 2001;
Mikolov et al, 2011). The RNN is primarily used
as a language model, but may also be viewed as a
sentence model with a linear structure. The layer
computed at the last word represents the sentence.
Finally, a further class of neural sentence mod-
els is based on the convolution operation and the
TDNN architecture (Collobert and Weston, 2008;
Kalchbrenner and Blunsom, 2013b). Certain con-
cepts used in these models are central to the
DCNN and we describe them next.
2.2 Convolution
The one-dimensional convolution is an operation
between a vector of weights m ? R
m
and a vector
of inputs viewed as a sequence s ? R
s
. The vector
m is the filter of the convolution. Concretely, we
think of s as the input sentence and s
i
? R is a sin-
gle feature value associated with the i-th word in
the sentence. The idea behind the one-dimensional
convolution is to take the dot product of the vector
m with each m-gram in the sentence s to obtain
another sequence c:
c
j
= m
?
s
j?m+1:j
(1)
Equation 1 gives rise to two types of convolution
depending on the range of the index j. The narrow
type of convolution requires that s ? m and yields
s1 s1ss ss
c1 c5c5
Figure 2: Narrow and wide types of convolution.
The filter m has size m = 5.
a sequence c ? R
s?m+1
with j ranging from m
to s. The wide type of convolution does not have
requirements on s or m and yields a sequence c ?
R
s+m?1
where the index j ranges from 1 to s +
m ? 1. Out-of-range input values s
i
where i < 1
or i > s are taken to be zero. The result of the
narrow convolution is a subsequence of the result
of the wide convolution. The two types of one-
dimensional convolution are illustrated in Fig. 2.
The trained weights in the filter m correspond
to a linguistic feature detector that learns to recog-
nise a specific class of n-grams. These n-grams
have size n ? m, where m is the width of the
filter. Applying the weights m in a wide convo-
lution has some advantages over applying them in
a narrow one. A wide convolution ensures that all
weights in the filter reach the entire sentence, in-
cluding the words at the margins. This is particu-
larly significant when m is set to a relatively large
value such as 8 or 10. In addition, a wide convo-
lution guarantees that the application of the filter
m to the input sentence s always produces a valid
non-empty result c, independently of the width m
and the sentence length s. We next describe the
classical convolutional layer of a TDNN.
2.3 Time-Delay Neural Networks
A TDNN convolves a sequence of inputs s with a
set of weights m. As in the TDNN for phoneme
recognition (Waibel et al, 1990), the sequence s
is viewed as having a time dimension and the con-
volution is applied over the time dimension. Each
s
j
is often not just a single value, but a vector of
d values so that s ? R
d?s
. Likewise, m is a ma-
trix of weights of size d?m. Each row of m is
convolved with the corresponding row of s and the
convolution is usually of the narrow type. Multi-
ple convolutional layers may be stacked by taking
the resulting sequence c as input to the next layer.
The Max-TDNN sentence model is based on the
architecture of a TDNN (Collobert and Weston,
2008). In the model, a convolutional layer of the
narrow type is applied to the sentence matrix s,
where each column corresponds to the feature vec-
657
tor w
i
? R
d
of a word in the sentence:
s =
?
?
w
1
. . . w
s
?
?
(2)
To address the problem of varying sentence
lengths, the Max-TDNN takes the maximum of
each row in the resulting matrix c yielding a vector
of d values:
c
max
=
?
?
?
max(c
1,:
)
.
.
.
max(c
d,:
)
?
?
?
(3)
The aim is to capture the most relevant feature, i.e.
the one with the highest value, for each of the d
rows of the resulting matrix c. The fixed-sized
vector c
max
is then used as input to a fully con-
nected layer for classification.
The Max-TDNN model has many desirable
properties. It is sensitive to the order of the words
in the sentence and it does not depend on external
language-specific features such as dependency or
constituency parse trees. It also gives largely uni-
form importance to the signal coming from each
of the words in the sentence, with the exception
of words at the margins that are considered fewer
times in the computation of the narrow convolu-
tion. But the model also has some limiting as-
pects. The range of the feature detectors is lim-
ited to the span m of the weights. Increasing m or
stacking multiple convolutional layers of the nar-
row type makes the range of the feature detectors
larger; at the same time it also exacerbates the ne-
glect of the margins of the sentence and increases
the minimum size s of the input sentence required
by the convolution. For this reason higher-order
and long-range feature detectors cannot be easily
incorporated into the model. The max pooling op-
eration has some disadvantages too. It cannot dis-
tinguish whether a relevant feature in one of the
rows occurs just one or multiple times and it for-
gets the order in which the features occur. More
generally, the pooling factor by which the signal
of the matrix is reduced at once corresponds to
s?m+1; even for moderate values of s the pool-
ing factor can be excessive. The aim of the next
section is to address these limitations while pre-
serving the advantages.
3 Convolutional Neural Networks with
Dynamic k-Max Pooling
We model sentences using a convolutional archi-
tecture that alternates wide convolutional layers
K-Max pooling
(k=3)
Fully connected 
layer
Folding
Wide
convolution
(m=2)
Dynamic
k-max pooling
 (k= f(s) =5)
 Projected
sentence 
matrix
(s=7)
Wide
convolution
(m=3)
 The cat sat on the red mat
Figure 3: A DCNN for the seven word input sen-
tence. Word embeddings have size d = 4. The
network has two convolutional layers with two
feature maps each. The widths of the filters at the
two layers are respectively 3 and 2. The (dynamic)
k-max pooling layers have values k of 5 and 3.
with dynamic pooling layers given by dynamic k-
max pooling. In the network the width of a feature
map at an intermediate layer varies depending on
the length of the input sentence; the resulting ar-
chitecture is the Dynamic Convolutional Neural
Network. Figure 3 represents a DCNN. We pro-
ceed to describe the network in detail.
3.1 Wide Convolution
Given an input sentence, to obtain the first layer of
the DCNN we take the embedding w
i
? R
d
for
each word in the sentence and construct the sen-
tence matrix s ? R
d?s
as in Eq. 2. The values
in the embeddings w
i
are parameters that are op-
timised during training. A convolutional layer in
the network is obtained by convolving a matrix of
weights m ? R
d?m
with the matrix of activations
at the layer below. For example, the second layer
is obtained by applying a convolution to the sen-
tence matrix s itself. Dimension d and filter width
m are hyper-parameters of the network. We let the
operations be wide one-dimensional convolutions
as described in Sect. 2.2. The resulting matrix c
has dimensions d? (s+m? 1).
658
3.2 k-Max Pooling
We next describe a pooling operation that is a gen-
eralisation of the max pooling over the time di-
mension used in the Max-TDNN sentence model
and different from the local max pooling opera-
tions applied in a convolutional network for object
recognition (LeCun et al, 1998). Given a value
k and a sequence p ? R
p
of length p ? k, k-
max pooling selects the subsequence p
k
max
of the
k highest values of p. The order of the values in
p
k
max
corresponds to their original order in p.
The k-max pooling operation makes it possible
to pool the k most active features in p that may be
a number of positions apart; it preserves the order
of the features, but is insensitive to their specific
positions. It can also discern more finely the num-
ber of times the feature is highly activated in p
and the progression by which the high activations
of the feature change across p. The k-max pooling
operator is applied in the network after the topmost
convolutional layer. This guarantees that the input
to the fully connected layers is independent of the
length of the input sentence. But, as we see next, at
intermediate convolutional layers the pooling pa-
rameter k is not fixed, but is dynamically selected
in order to allow for a smooth extraction of higher-
order and longer-range features.
3.3 Dynamic k-Max Pooling
A dynamic k-max pooling operation is a k-max
pooling operation where we let k be a function of
the length of the sentence and the depth of the net-
work. Although many functions are possible, we
simply model the pooling parameter as follows:
k
l
= max( k
top
, d
L? l
L
se ) (4)
where l is the number of the current convolutional
layer to which the pooling is applied and L is the
total number of convolutional layers in the net-
work; k
top
is the fixed pooling parameter for the
topmost convolutional layer (Sect. 3.2). For in-
stance, in a network with three convolutional lay-
ers and k
top
= 3, for an input sentence of length
s = 18, the pooling parameter at the first layer
is k
1
= 12 and the pooling parameter at the sec-
ond layer is k
2
= 6; the third layer has the fixed
pooling parameter k
3
= k
top
= 3. Equation 4
is a model of the number of values needed to de-
scribe the relevant parts of the progression of an
l-th order feature over a sentence of length s. For
an example in sentiment prediction, according to
the equation a first order feature such as a posi-
tive word occurs at most k
1
times in a sentence of
length s, whereas a second order feature such as a
negated phrase or clause occurs at most k
2
times.
3.4 Non-linear Feature Function
After (dynamic) k-max pooling is applied to the
result of a convolution, a bias b ? R
d
and a non-
linear function g are applied component-wise to
the pooled matrix. There is a single bias value for
each row of the pooled matrix.
If we temporarily ignore the pooling layer, we
may state how one computes each d-dimensional
column a in the matrix a resulting after the convo-
lutional and non-linear layers. Define M to be the
matrix of diagonals:
M = [diag(m
:,1
), . . . , diag(m
:,m
)] (5)
where m are the weights of the d filters of the wide
convolution. Then after the first pair of a convolu-
tional and a non-linear layer, each column a in the
matrix a is obtained as follows, for some index j:
a = g
?
?
?
M
?
?
?
w
j
.
.
.
w
j+m?1
?
?
?
+ b
?
?
?
(6)
Here a is a column of first order features. Sec-
ond order features are similarly obtained by ap-
plying Eq. 6 to a sequence of first order features
a
j
, ..., a
j+m
?
?1
with another weight matrix M
?
.
Barring pooling, Eq. 6 represents a core aspect
of the feature extraction function and has a rather
general form that we return to below. Together
with pooling, the feature function induces position
invariance and makes the range of higher-order
features variable.
3.5 Multiple Feature Maps
So far we have described how one applies a wide
convolution, a (dynamic) k-max pooling layer and
a non-linear function to the input sentence ma-
trix to obtain a first order feature map. The three
operations can be repeated to yield feature maps
of increasing order and a network of increasing
depth. We denote a feature map of the i-th order
by F
i
. As in convolutional networks for object
recognition, to increase the number of learnt fea-
ture detectors of a certain order, multiple feature
maps F
i
1
, . . . ,F
i
n
may be computed in parallel at
the same layer. Each feature map F
i
j
is computed
by convolving a distinct set of filters arranged in
a matrix m
i
j,k
with each feature map F
i?1
k
of the
lower order i? 1 and summing the results:
659
Fi
j
=
n
?
k=1
m
i
j,k
? F
i?1
k
(7)
where ? indicates the wide convolution. The
weights m
i
j,k
form an order-4 tensor. After the
wide convolution, first dynamic k-max pooling
and then the non-linear function are applied indi-
vidually to each map.
3.6 Folding
In the formulation of the network so far, feature
detectors applied to an individual row of the sen-
tence matrix s can have many orders and create
complex dependencies across the same rows in
multiple feature maps. Feature detectors in differ-
ent rows, however, are independent of each other
until the top fully connected layer. Full depen-
dence between different rows could be achieved
by making M in Eq. 5 a full matrix instead of
a sparse matrix of diagonals. Here we explore a
simpler method called folding that does not intro-
duce any additional parameters. After a convo-
lutional layer and before (dynamic) k-max pool-
ing, one just sums every two rows in a feature map
component-wise. For a map of d rows, folding re-
turns a map of d/2 rows, thus halving the size of
the representation. With a folding layer, a feature
detector of the i-th order depends now on two rows
of feature values in the lower maps of order i? 1.
This ends the description of the DCNN.
4 Properties of the Sentence Model
We describe some of the properties of the sentence
model based on the DCNN. We describe the no-
tion of the feature graph induced over a sentence
by the succession of convolutional and pooling
layers. We briefly relate the properties to those of
other neural sentence models.
4.1 Word and n-Gram Order
One of the basic properties is sensitivity to the or-
der of the words in the input sentence. For most
applications and in order to learn fine-grained fea-
ture detectors, it is beneficial for a model to be able
to discriminate whether a specific n-gram occurs
in the input. Likewise, it is beneficial for a model
to be able to tell the relative position of the most
relevant n-grams. The network is designed to cap-
ture these two aspects. The filters m of the wide
convolution in the first layer can learn to recognise
specific n-grams that have size less or equal to the
filter width m; as we see in the experiments, m in
the first layer is often set to a relatively large value
such as 10. The subsequence of n-grams extracted
by the generalised pooling operation induces in-
variance to absolute positions, but maintains their
order and relative positions.
As regards the other neural sentence models, the
class of NBoW models is by definition insensitive
to word order. A sentence model based on a recur-
rent neural network is sensitive to word order, but
it has a bias towards the latest words that it takes as
input (Mikolov et al, 2011). This gives the RNN
excellent performance at language modelling, but
it is suboptimal for remembering at once the n-
grams further back in the input sentence. Sim-
ilarly, a recursive neural network is sensitive to
word order but has a bias towards the topmost
nodes in the tree; shallower trees mitigate this ef-
fect to some extent (Socher et al, 2013a). As seen
in Sect. 2.3, the Max-TDNN is sensitive to word
order, but max pooling only picks out a single n-
gram feature in each row of the sentence matrix.
4.2 Induced Feature Graph
Some sentence models use internal or external
structure to compute the representation for the in-
put sentence. In a DCNN, the convolution and
pooling layers induce an internal feature graph
over the input. A node from a layer is connected
to a node from the next higher layer if the lower
node is involved in the convolution that computes
the value of the higher node. Nodes that are not
selected by the pooling operation at a layer are
dropped from the graph. After the last pooling
layer, the remaining nodes connect to a single top-
most root. The induced graph is a connected, di-
rected acyclic graph with weighted edges and a
root node; two equivalent representations of an
induced graph are given in Fig. 1. In a DCNN
without folding layers, each of the d rows of the
sentence matrix induces a subgraph that joins the
other subgraphs only at the root node. Each sub-
graph may have a different shape that reflects the
kind of relations that are detected in that subgraph.
The effect of folding layers is to join pairs of sub-
graphs at lower layers before the top root node.
Convolutional networks for object recognition
also induce a feature graph over the input image.
What makes the feature graph of a DCNN pecu-
liar is the global range of the pooling operations.
The (dynamic) k-max pooling operator can draw
together features that correspond to words that are
many positions apart in the sentence. Higher-order
features have highly variable ranges that can be ei-
660
ther short and focused or global and long as the
input sentence. Likewise, the edges of a subgraph
in the induced graph reflect these varying ranges.
The subgraphs can either be localised to one or
more parts of the sentence or spread more widely
across the sentence. This structure is internal to
the network and is defined by the forward propa-
gation of the input through the network.
Of the other sentence models, the NBoW is a
shallow model and the RNN has a linear chain
structure. The subgraphs induced in the Max-
TDNN model have a single fixed-range feature ob-
tained through max pooling. The recursive neural
network follows the structure of an external parse
tree. Features of variable range are computed at
each node of the tree combining one or more of
the children of the tree. Unlike in a DCNN, where
one learns a clear hierarchy of feature orders, in
a RecNN low order features like those of sin-
gle words can be directly combined with higher
order features computed from entire clauses. A
DCNN generalises many of the structural aspects
of a RecNN. The feature extraction function as
stated in Eq. 6 has a more general form than that
in a RecNN, where the value of m is generally 2.
Likewise, the induced graph structure in a DCNN
is more general than a parse tree in that it is not
limited to syntactically dictated phrases; the graph
structure can capture short or long-range seman-
tic relations between words that do not necessar-
ily correspond to the syntactic relations in a parse
tree. The DCNN has internal input-dependent
structure and does not rely on externally provided
parse trees, which makes the DCNN directly ap-
plicable to hard-to-parse sentences such as tweets
and to sentences from any language.
5 Experiments
We test the network on four different experiments.
We begin by specifying aspects of the implemen-
tation and the training of the network. We then re-
late the results of the experiments and we inspect
the learnt feature detectors.
5.1 Training
In each of the experiments, the top layer of the
network has a fully connected layer followed by
a softmax non-linearity that predicts the probabil-
ity distribution over classes given the input sen-
tence. The network is trained to minimise the
cross-entropy of the predicted and true distribu-
tions; the objective includes an L
2
regularisation
Classifier Fine-grained (%) Binary (%)
NB 41.0 81.8
BINB 41.9 83.1
SVM 40.7 79.4
RECNTN 45.7 85.4
MAX-TDNN 37.4 77.1
NBOW 42.4 80.5
DCNN 48.5 86.8
Table 1: Accuracy of sentiment prediction in the
movie reviews dataset. The first four results are
reported from Socher et al (2013b). The baselines
NB and BINB are Naive Bayes classifiers with,
respectively, unigram features and unigram and bi-
gram features. SVM is a support vector machine
with unigram and bigram features. RECNTN is a
recursive neural network with a tensor-based fea-
ture function, which relies on external structural
features given by a parse tree and performs best
among the RecNNs.
term over the parameters. The set of parameters
comprises the word embeddings, the filter weights
and the weights from the fully connected layers.
The network is trained with mini-batches by back-
propagation and the gradient-based optimisation is
performed using the Adagrad update rule (Duchi
et al, 2011). Using the well-known convolution
theorem, we can compute fast one-dimensional
linear convolutions at all rows of an input matrix
by using Fast Fourier Transforms. To exploit the
parallelism of the operations, we train the network
on a GPU. A Matlab implementation processes
multiple millions of input sentences per hour on
one GPU, depending primarily on the number of
layers used in the network.
5.2 Sentiment Prediction in Movie Reviews
The first two experiments concern the prediction
of the sentiment of movie reviews in the Stanford
Sentiment Treebank (Socher et al, 2013b). The
output variable is binary in one experiment and
can have five possible outcomes in the other: neg-
ative, somewhat negative, neutral, somewhat posi-
tive, positive. In the binary case, we use the given
splits of 6920 training, 872 development and 1821
test sentences. Likewise, in the fine-grained case,
we use the standard 8544/1101/2210 splits. La-
belled phrases that occur as subparts of the train-
ing sentences are treated as independent training
instances. The size of the vocabulary is 15448.
Table 1 details the results of the experiments.
661
Classifier Features Acc. (%)
HIER
unigram, POS, head chunks 91.0
NE, semantic relations
MAXENT
unigram, bigram, trigram 92.6
POS, chunks, NE, supertags
CCG parser, WordNet
MAXENT
unigram, bigram, trigram 93.6
POS, wh-word, head word
word shape, parser
hypernyms, WordNet
SVM
unigram, POS, wh-word 95.0
head word, parser
hypernyms, WordNet
60 hand-coded rules
MAX-TDNN unsupervised vectors 84.4
NBOW unsupervised vectors 88.2
DCNN unsupervised vectors 93.0
Table 2: Accuracy of six-way question classifica-
tion on the TREC questions dataset. The second
column details the external features used in the
various approaches. The first four results are re-
spectively from Li and Roth (2002), Blunsom et al
(2006), Huang et al (2008) and Silva et al (2011).
In the three neural sentence models?the Max-
TDNN, the NBoW and the DCNN?the word vec-
tors are parameters of the models that are ran-
domly initialised; their dimension d is set to 48.
The Max-TDNN has a filter of width 6 in its nar-
row convolution at the first layer; shorter phrases
are padded with zero vectors. The convolu-
tional layer is followed by a non-linearity, a max-
pooling layer and a softmax classification layer.
The NBoW sums the word vectors and applies a
non-linearity followed by a softmax classification
layer. The adopted non-linearity is the tanh func-
tion. The hyper parameters of the DCNN are as
follows. The binary result is based on a DCNN
that has a wide convolutional layer followed by a
folding layer, a dynamic k-max pooling layer and
a non-linearity; it has a second wide convolutional
layer followed by a folding layer, a k-max pooling
layer and a non-linearity. The width of the convo-
lutional filters is 7 and 5, respectively. The value
of k for the top k-max pooling is 4. The num-
ber of feature maps at the first convolutional layer
is 6; the number of maps at the second convolu-
tional layer is 14. The network is topped by a soft-
max classification layer. The DCNN for the fine-
grained result has the same architecture, but the
filters have size 10 and 7, the top pooling parame-
ter k is 5 and the number of maps is, respectively,
6 and 12. The networks use the tanh non-linear
Classifier Accuracy (%)
SVM 81.6
BINB 82.7
MAXENT 83.0
MAX-TDNN 78.8
NBOW 80.9
DCNN 87.4
Table 3: Accuracy on the Twitter sentiment
dataset. The three non-neural classifiers are based
on unigram and bigram features; the results are re-
ported from (Go et al, 2009).
function. At training time we apply dropout to the
penultimate layer after the last tanh non-linearity
(Hinton et al, 2012).
We see that the DCNN significantly outper-
forms the other neural and non-neural models.
The NBoW performs similarly to the non-neural
n-gram based classifiers. The Max-TDNN per-
forms worse than the NBoW likely due to the ex-
cessive pooling of the max pooling operation; the
latter discards most of the sentiment features of the
words in the input sentence. Besides the RecNN
that uses an external parser to produce structural
features for the model, the other models use n-
gram based or neural features that do not require
external resources or additional annotations. In the
next experiment we compare the performance of
the DCNN with those of methods that use heavily
engineered resources.
5.3 Question Type Classification
As an aid to question answering, a question may
be classified as belonging to one of many question
types. The TREC questions dataset involves six
different question types, e.g. whether the question
is about a location, about a person or about some
numeric information (Li and Roth, 2002). The
training dataset consists of 5452 labelled questions
whereas the test dataset consists of 500 questions.
The results are reported in Tab. 2. The non-
neural approaches use a classifier over a large
number of manually engineered features and
hand-coded resources. For instance, Blunsom et
al. (2006) present a Maximum Entropy model that
relies on 26 sets of syntactic and semantic fea-
tures including unigrams, bigrams, trigrams, POS
tags, named entity tags, structural relations from
a CCG parse and WordNet synsets. We evaluate
the three neural models on this dataset with mostly
the same hyper-parameters as in the binary senti-
662
POSITIVE
lovely	 	 	 	 	 comedic	 	 	 	 	 moments	 and	 	 	 	 several	 	 	 	 	 fine	 	 	 	 	 	 performances
good	 	 	 	 	 	 	 script	 	 	 	 	 	 ,	 	 	 	 	 	 	 good	 	 	 dialogue	 	 	 	 ,	 	 	 	 	 	 	 	 	 funny	 	 	 	 	 	 	 
sustains	 	 	 throughout	 	 is	 	 	 	 	 	 daring	 ,	 	 	 	 	 	 	 	 	 	 	 inventive	 and	 	 	 	 	 	 	 	 	 
well	 	 	 	 	 	 	 written	 	 	 	 	 ,	 	 	 	 	 	 	 nicely	 acted	 	 	 	 	 	 	 and	 	 	 	 	 	 	 beautifully	 
remarkably	 solid	 	 	 	 	 	 	 and	 	 	 	 	 subtly	 satirical	 	 	 tour	 	 	 	 	 	 de	 	 	 	 	 	 	 	 	 	 
NEGATIVE
,	 	 	 	 	 	 	 	 	 	 nonexistent	 plot	 	 	 	 and	 	 	 	 pretentious	 visual	 	 	 	 style	 	 	 	 	 	 	 
it	 	 	 	 	 	 	 	 	 fails	 	 	 	 	 	 	 the	 	 	 	 	 most	 	 	 basic	 	 	 	 	 	 	 test	 	 	 	 	 	 as	 	 	 	 	 	 	 	 	 	 
so	 	 	 	 	 	 	 	 	 stupid	 	 	 	 	 	 ,	 	 	 	 	 	 	 so	 	 	 	 	 ill	 	 	 	 	 	 	 	 	 conceived	 ,	 	 	 	 	 	 	 	 	 	 	 
,	 	 	 	 	 	 	 	 	 	 too	 	 	 	 	 	 	 	 	 dull	 	 	 	 and	 	 	 	 pretentious	 to	 	 	 	 	 	 	 	 be	 	 	 	 	 	 	 	 	 	 
hood	 	 	 	 	 	 	 rats	 	 	 	 	 	 	 	 butt	 	 	 	 their	 	 ugly	 	 	 	 	 	 	 	 heads	 	 	 	 	 in	 	 	 	 	 	 	 	 	 	 	 	 
'NOT'
n't	 	 	 	 have	 	 	 	 	 any	 	 	 	 	 	 	 	 	 huge	 laughs	 	 	 	 	 	 in	 	 	 	 	 	 	 	 	 	 	 its	 	 	 
no	 	 	 	 	 movement	 ,	 	 	 	 	 	 	 	 	 	 	 no	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 much	 	 
n't	 	 	 	 stop	 	 	 	 	 me	 	 	 	 	 	 	 	 	 	 from	 enjoying	 	 	 	 much	 	 	 	 	 	 	 	 	 of	 	 	 	 
not	 	 	 	 that	 	 	 	 	 kung	 	 	 	 	 	 	 	 pow	 	 is	 	 	 	 	 	 	 	 	 	 n't	 	 	 	 	 	 	 	 	 	 funny	 
not	 	 	 	 a	 	 	 	 	 	 	 	 moment	 	 	 	 	 	 that	 is	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 false	 
'TOO'
,	 	 	 	 	 	 too	 	 	 	 	 	 dull	 	 	 	 	 	 	 	 and	 	 pretentious	 to	 	 	 	 	 	 	 	 	 	 	 be	 	 	 	 	 	 	 	 
either	 too	 	 	 	 	 	 serious	 	 	 	 	 or	 	 	 too	 	 	 	 	 	 	 	 	 lighthearted	 ,	 	 	 	 	 	 	 	 	 
too	 	 	 	 slow	 	 	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 too	 	 long	 	 	 	 	 	 	 	 and	 	 	 	 	 	 	 	 	 	 too	 	 	 	 	 	 	 
feels	 	 too	 	 	 	 	 	 formulaic	 	 	 and	 	 too	 	 	 	 	 	 	 	 	 familiar	 	 	 	 	 to	 	 	 	 	 	 	 	 
is	 	 	 	 	 too	 	 	 	 	 	 predictable	 and	 	 too	 	 	 	 	 	 	 	 	 self	 	 	 	 	 	 	 	 	 conscious	 	 
Figure 4: Top five 7-grams at four feature detectors in the first layer of the network.
ment experiment of Sect. 5.2. As the dataset is
rather small, we use lower-dimensional word vec-
tors with d = 32 that are initialised with embed-
dings trained in an unsupervised way to predict
contexts of occurrence (Turian et al, 2010). The
DCNN uses a single convolutional layer with fil-
ters of size 8 and 5 feature maps. The difference
between the performance of the DCNN and that of
the other high-performing methods in Tab. 2 is not
significant (p < 0.09). Given that the only labelled
information used to train the network is the train-
ing set itself, it is notable that the network matches
the performance of state-of-the-art classifiers that
rely on large amounts of engineered features and
rules and hand-coded resources.
5.4 Twitter Sentiment Prediction with
Distant Supervision
In our final experiment, we train the models on a
large dataset of tweets, where a tweet is automat-
ically labelled as positive or negative depending
on the emoticon that occurs in it. The training set
consists of 1.6 million tweets with emoticon-based
labels and the test set of about 400 hand-annotated
tweets. We preprocess the tweets minimally fol-
lowing the procedure described in Go et al (2009);
in addition, we also lowercase all the tokens. This
results in a vocabulary of 76643 word types. The
architecture of the DCNN and of the other neural
models is the same as the one used in the binary
experiment of Sect. 5.2. The randomly initialised
word embeddings are increased in length to a di-
mension of d = 60. Table 3 reports the results of
the experiments. We see a significant increase in
the performance of the DCNN with respect to the
non-neural n-gram based classifiers; in the pres-
ence of large amounts of training data these clas-
sifiers constitute particularly strong baselines. We
see that the ability to train a sentiment classifier on
automatically extracted emoticon-based labels ex-
tends to the DCNN and results in highly accurate
performance. The difference in performance be-
tween the DCNN and the NBoW further suggests
that the ability of the DCNN to both capture fea-
tures based on long n-grams and to hierarchically
combine these features is highly beneficial.
5.5 Visualising Feature Detectors
A filter in the DCNN is associated with a feature
detector or neuron that learns during training to
be particularly active when presented with a spe-
cific sequence of input words. In the first layer, the
sequence is a continuous n-gram from the input
sentence; in higher layers, sequences can be made
of multiple separate n-grams. We visualise the
feature detectors in the first layer of the network
trained on the binary sentiment task (Sect. 5.2).
Since the filters have width 7, for each of the 288
feature detectors we rank all 7-grams occurring in
the validation and test sets according to their ac-
tivation of the detector. Figure 5.2 presents the
top five 7-grams for four feature detectors. Be-
sides the expected detectors for positive and nega-
tive sentiment, we find detectors for particles such
as ?not? that negate sentiment and such as ?too?
that potentiate sentiment. We find detectors for
multiple other notable constructs including ?all?,
?or?, ?with...that?, ?as...as?. The feature detectors
learn to recognise not just single n-grams, but pat-
terns within n-grams that have syntactic, semantic
or structural significance.
6 Conclusion
We have described a dynamic convolutional neural
network that uses the dynamic k-max pooling op-
erator as a non-linear subsampling function. The
feature graph induced by the network is able to
capture word relations of varying size. The net-
work achieves high performance on question and
sentiment classification without requiring external
features as provided by parsers or other resources.
Acknowledgements
We thank Nando de Freitas and Yee Whye Teh
for great discussions on the paper. This work was
supported by a Xerox Foundation Award, EPSRC
grant number EP/F042728/1, and EPSRC grant
number EP/K036580/1.
663
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP, pages 1183?1193. ACL.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear mod-
els. In SIGIR ?06: Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 615?616, New York, NY, USA. ACM.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical Foundations for a Com-
positional Distributional Model of Meaning. March.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Interna-
tional Conference on Machine Learning, ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context.
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing - EMNLP ?08,
(October):897.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Felix A. Gers and Jrgen Schmidhuber. 2001. Lstm
recurrent networks learn simple context-free and
context-sensitive languages. IEEE Transactions on
Neural Networks, 12(6):1333?1340.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Edward Grefenstette. 2013. Category-theoretic
quantitative compositional distributional models
of natural language semantics. arXiv preprint
arXiv:1311.1539.
Emiliano Guevara. 2010. Modelling Adjective-Noun
Compositionality by Regression. ESSLLI?10 Work-
shop on Compositionality and Distributional Se-
mantic Models.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Sofia, Bulgaria,
August. Association for Computational Linguistics.
Forthcoming.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.
Geoffrey E. Hinton. 1989. Connectionist learning pro-
cedures. Artif. Intell., 40(1-3):185?234.
Zhiheng Huang, Marcus Thint, and Zengchang Qin.
2008. Question classification using head words and
their hypernyms. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?08, pages 927?936, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Nal Kalchbrenner and Phil Blunsom. 2013a. Recur-
rent continuous translation models. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing, Seattle, October. As-
sociation for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013b. Recur-
rent Convolutional Neural Networks for Discourse
Compositionality. In Proceedings of the Workshop
on Continuous Vector Space Models and their Com-
positionality, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Seattle, USA, October.
Andreas K?uchler and Christoph Goller. 1996. Induc-
tive learning in symbolic domains using structure-
driven recurrent neural networks. In G?unther G?orz
and Steffen H?olldobler, editors, KI, volume 1137 of
Lecture Notes in Computer Science, pages 183?197.
Springer.
Yann LeCun, L?eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278?2324, November.
Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7. Association for Computational Linguis-
tics.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT, pages 234?239.
664
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In ICASSP, pages 5528?5531. IEEE.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, volume 8.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46:77?105.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Joo Silva, Lusa Coheur, AnaCristina Mendes, and An-
dreas Wichert. 2011. From symbolic to sub-
symbolic information in question classification. Ar-
tificial Intelligence Review, 35(2):137?154.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Richard Socher, Quoc V. Le, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Grounded Composi-
tional Semantics for Finding and Describing Images
with Sentences. In Transactions of the Association
for Computational Linguistics (TACL).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631?1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
J. Artif. Intell. Res.(JAIR), 44:533?585.
Alexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-
ton, Kiyohiro Shikano, and Kevin J. Lang. 1990.
Readings in speech recognition. chapter Phoneme
Recognition Using Time-delay Neural Networks,
pages 393?404. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666. AUAI Press.
665
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224?229,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Bilingual Word Representations by Marginalizing Alignments
Tom
?
a
?
s Ko
?
cisk?y Karl Moritz Hermann
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{tomas.kocisky,karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
Phil Blunsom
Abstract
We present a probabilistic model that si-
multaneously learns alignments and dis-
tributed representations for bilingual data.
By marginalizing over word alignments
the model captures a larger semantic con-
text than prior work relying on hard align-
ments. The advantage of this approach is
demonstrated in a cross-lingual classifica-
tion task, where we outperform the prior
published state of the art.
1 Introduction
Distributed representations have become an in-
creasingly important tool in machine learning.
Such representations?typically continuous vec-
tors learned in an unsupervised setting?can fre-
quently be used in place of hand-crafted, and thus
expensive, features. By providing a richer rep-
resentation than what can be encoded in discrete
settings, distributed representations have been suc-
cessfully used in many areas. This includes AI and
reinforcement learning (Mnih et al, 2013), image
retrieval (Kiros et al, 2013), language modelling
(Bengio et al, 2003), sentiment analysis (Socher
et al, 2011; Hermann and Blunsom, 2013), frame-
semantic parsing (Hermann et al, 2014), and doc-
ument classification (Klementiev et al, 2012).
In Natural Language Processing (NLP), the use
of distributed representations is motivated by the
idea that they could capture semantics and/or syn-
tax, as well as encoding a continuous notion of
similarity, thereby enabling information sharing
between similar words and other units. The suc-
cess of distributed approaches to a number of
tasks, such as listed above, supports this notion
and its implied benefits (see also Turian et al
(2010) and Collobert and Weston (2008)).
While most work employing distributed repre-
sentations has focused on monolingual tasks, mul-
tilingual representations would also be useful for
several NLP-related tasks. Such problems include
document classification, machine translation, and
cross-lingual information retrieval, where multi-
lingual data is frequently the norm. Furthermore,
learning multilingual representations can also be
useful for cross-lingual information transfer, that
is exploiting resource-fortunate languages to gen-
erate supervised data in resource-poor ones.
We propose a probabilistic model that simulta-
neously learns word alignments and bilingual dis-
tributed word representations. As opposed to pre-
vious work in this field, which has relied on hard
alignments or bilingual lexica (Klementiev et al,
2012; Mikolov et al, 2013), we marginalize out
the alignments, thus capturing more bilingual se-
mantic context. Further, this results in our dis-
tributed word alignment (DWA) model being the
first probabilistic account of bilingual word repre-
sentations. This is desirable as it allows better rea-
soning about the derived representations and fur-
thermore, makes the model suitable for inclusion
in higher-level tasks such as machine translation.
The contributions of this paper are as follows.
We present a new probabilistic similarity measure
which is based on an alignment model and prior
language modeling work which learns and relates
word representations across languages. Subse-
quently, we apply these embeddings to a standard
document classification task and show that they
outperform the current published state of the art
(Hermann and Blunsom, 2014b). As a by-product
we develop a distributed version of FASTALIGN
(Dyer et al, 2013), which performs on par with
the original model, thereby demonstrating the ef-
ficacy of the learned bilingual representations.
2 Background
The IBM alignment models, introduced by Brown
et al (1993), form the basis of most statistical ma-
chine translation systems. In this paper we base
our alignment model on FASTALIGN (FA), a vari-
224
ation of IBM model 2 introduced by Dyer et al
(2013). This model is both fast and produces
alignments on par with the state of the art. Further,
to induce the distributed representations we incor-
porate ideas from the log-bilinear language model
presented by Mnih and Hinton (2007).
2.1 IBM Model 2
Given a parallel corpus with aligned sentences, an
alignment model can be used to discover matching
words and phrases across languages. Such mod-
els are an integral part of most machine translation
pipelines. An alignment model learns p(f ,a|e) (or
p(e,a
?
|f)) for the source and target sentences e
and f (sequences of words). a represents the word
alignment across these two sentences from source
to target. IBM model 2 (Brown et al, 1993) learns
alignment and translation probabilities in a gener-
ative style as follows:
p(f ,a|e) = p(J |I)
J
?
j=1
p(a
j
|j, I, J) p
(
f
j
|e
a
j
)
,
where p(J |I) captures the two sentence lengths;
p(a
j
|j, I, J) the alignment and p
(
f
j
|e
a
j
)
the
translation probability. Sentence likelihood is
given by marginalizing out the alignments, which
results in the following equation:
p(f |e) = p(J |I)
J
?
j=1
I
?
i=0
p(i|j, I, J) p(f
j
|e
i
) .
We use FASTALIGN (FA) (Dyer et al, 2013), a
log-linear reparametrization of IBM model 2. This
model uses an alignment distribution defined by
a single parameter that measures how close the
alignment is to the diagonal. This replaces the
original multinomial alignment distribution which
often suffered from sparse counts. This improved
model was shown to run an order of magnitude
faster than IBM model 4 and yet still outperformed
it in terms of the BLEU score and, on Chinese-
English data, in alignment error rate (AER).
2.2 Log-Bilinear Language Model
Language models assign a probability measure
to sequences of words. We use the log-bilinear
language model proposed by Mnih and Hinton
(2007). It is an n-gram based model defined in
terms of an energy function E(w
n
;w
1:n?1
). The
probability for predicting the next word w
n
given
its preceding context of n ? 1 words is expressed
using the energy function
E(w
n
;w
1:n?1
)=?
(
n?1
?
i=1
r
T
w
i
C
i
)
r
w
n
?b
T
r
r
w
n
?b
w
n
as p(w
n
|w
1:n?1
) =
1
Z
c
exp (?E(w
n
;w
1:n?1
))
where Z
c
=
?
w
n
exp (?E(w
n
;w
1:n?1
)) is the
normalizer, r
w
i
? R
d
are word representations,
C
i
? R
d?d
are context transformation matrices,
and b
r
? R
d
, b
w
n
? R are representation and word
biases respectively. Here, the sum of the trans-
formed context-word vectors endeavors to be close
to the word we want to predict, since the likelihood
in the model is maximized when the energy of the
observed data is minimized.
This model can be considered a variant of a
log-linear language model in which, instead of
defining binary n-gram features, the model learns
the features of the input and output words, and
a transformation between them. This provides a
vastly more compact parameterization of a lan-
guage model as n-gram features are not stored.
2.3 Multilingual Representation Learning
There is some recent prior work on multilin-
gual distributed representation learning. Simi-
lar to the model presented here, Klementiev et
al. (2012) and Zou et al (2013) learn bilingual
embeddings using word alignments. These two
models are non-probabilistic and conditioned on
the output of a separate alignment model, un-
like our model, which defines a probability dis-
tribution over translations and marginalizes over
all alignments. These models are also highly re-
lated to prior work on bilingual lexicon induc-
tion (Haghighi et al, 2008). Other recent ap-
proaches include Sarath Chandar et al (2013),
Lauly et al (2013) and Hermann and Blunsom
(2014a, 2014b). These models avoid word align-
ment by transferring information across languages
using a composed sentence-level representation.
While all of these approaches are related to the
model proposed in this paper, it is important to
note that our approach is novel by providing a
probabilistic account of these word embeddings.
Further, we learn word alignments and simultane-
ously use these alignments to guide the represen-
tation learning, which could be advantageous par-
ticularly for rare tokens, where a sentence based
approach might fail to transfer information.
Related work also includes Mikolov et al
(2013), who learn a transformation matrix to
225
reconcile monolingual embedding spaces, in an
l
2
norm sense, using dictionary entries instead of
alignments, as well as Schwenk et al (2007) and
Schwenk (2012), who also use distributed repre-
sentations for estimating translation probabilities.
Faruqui and Dyer (2014) use a technique based on
CCA and alignments to project monolingual word
representations to a common vector space.
3 Model
Here we describe our distributed word alignment
(DWA) model. The DWA model can be viewed
as a distributed extension of the FA model in that
it uses a similarity measure over distributed word
representations instead of the standard multino-
mial translation probability employed by FA. We
do this using a modified version of the log-bilinear
language model in place of the translation proba-
bilities p(f
j
|e
i
) at the heart of the FA model. This
allows us to learn word representations for both
languages, a translation matrix relating these vec-
tor spaces, as well as alignments at the same time.
Our modifications to the log-bilinear model are
as follows. Where the original log-bilinear lan-
guage model uses context words to predict the next
word?this is simply the distributed extension of
an n-gram language model?we use a word from
the source language in a parallel sentence to pre-
dict a target word. An additional aspect of our
model, which demonstrates its flexibility, is that it
is simple to include further context from the source
sentence, such as words around the aligned word
or syntactic and semantic annotations. In this pa-
per we experiment with a transformed sum over
k context words to each side of the aligned source
word. We evaluate different context sizes and re-
port the results in Section 5. We define the energy
function for the translation probabilities to be
E(f, e
i
) = ?
(
k
?
s=?k
r
T
e
i+s
T
s
)
r
f
?b
T
r
r
f
?b
f
(1)
where r
e
i
, r
f
? R
d
are vector representations for
source and target words e
i+s
? V
E
, f ? V
F
in
their respective vocabularies, T
s
? R
d?d
is the
transformation matrix for each surrounding con-
text position, b
r
? R
d
are the representation bi-
ases, and b
f
? R is a bias for each word f ? V
F
.
The translation probability is given by
p(f |e
i
) =
1
Z
e
i
exp (?E(f, e
i
)) , where
Z
e
i
=
?
f
exp (?E(f, e
i
)) is the normalizer.
In addition to these translation probabilities, we
have parameterized the translation probabilities
for the null word using a softmax over an addi-
tional weight vector.
3.1 Class Factorization
We improve training performance using a class
factorization strategy (Morin and Bengio, 2005)
as follows. We augment the translation probabil-
ity to be p(f |e) = p(c
f
|e) p(f |c
f
, e) where c
f
is a unique predetermined class of f ; the class
probability is modeled using a similar log-bilinear
model as above, but instead of predicting a word
representation r
f
we predict the class representa-
tion r
c
f
(which is learned with the model) and we
add respective new context matrices and biases.
Note that the probability of the word f depends
on both the class and the given context words: it is
normalized only over words in the class c
f
.
In our training we create classes based on word
frequencies in the corpus as follows. Considering
words in the order of their decreasing frequency,
we add word types into a class until the total fre-
quency of the word types in the currently consid-
ered class is less than
total tokens
?
|V
F
|
and the class size is
less than
?
|V
F
|. We have found that the maximal
class size affects the speed the most.
4 Learning
The original FA model optimizes the likelihood
using the expectation maximization (EM) algo-
rithm where, in the M-step, the parameter update
is analytically solvable, except for the ? parameter
(the diagonal tension), which is optimized using
gradient descent (Dyer et al, 2013). We modified
the implementations provided with CDEC (Dyer et
al., 2010), retaining its default parameters.
In our model, DWA, we optimize the likelihood
using the EM as well. However, while training we
fix the counts of the E-step to those computed by
FA, trained for the default 5 iterations, to aid the
convergence rate, and optimize the M-step only.
Let ? be the parameters for our model. Then the
gradient for each sentence is given by
?
??
log p(f |e) =
J
?
k=1
I
?
l=0
[
p(l|k, I, J) p(f
k
|e
l
)
?
I
i=0
p(i|k, I, J) p(f
k
|e
i
)
?
?
??
log(p(l|k, I, J) p(f
k
|e
l
))
]
226
where the first part are the counts from the FA
model and second part comes from our model.
We compute the gradient for the alignment
probabilities in the same way as in the FA model,
and the gradient for the translation probabilities
using back-propagation (Rumelhart et al, 1986).
For parameter update, we use ADAGRAD as the
gradient descent algorithm (Duchi et al, 2011).
5 Experiments
We first evaluate the alignment error rate of our
approach, which establishes the model?s ability to
both learn alignments as well as word representa-
tions that explain these alignments. Next, we use
a cross-lingual document classification task to ver-
ify that the representations are semantically useful.
We also inspect the embedding space qualitatively
to get some insight into the learned structure.
5.1 Alignment Evaluation
We compare the alignments learned here with
those of the FASTALIGN model which produces
very good alignments and translation BLEU
scores. We use the same language pairs and
datasets as in Dyer et al (2013), that is the FBIS
Chinese-English corpus, and the French-English
section of the Europarl corpus (Koehn, 2005). We
used the preprocessing tools from CDEC and fur-
ther replaced all unique tokens with UNK. We
trained our models with 100 dimensional repre-
sentations for up to 40 iterations, and the FA
model for 5 iterations as is the default.
Table 1 shows that our model learns alignments
on part with those of the FA model. This is in line
with expectation as our model was trained using
the FA expectations. However, it confirms that
the learned word representations are able to ex-
plain translation probabilities. Surprisingly, con-
text seems to have little impact on the alignment
error, suggesting that the model receives sufficient
information from the aligned words themselves.
5.2 Document Classification
A standard task for evaluating cross-lingual word
representations is document classification where
training is performed in one and evaluation in an-
other language. This tasks require semantically
plausible embeddings (for classification) which
are valid across two languages (for the semantic
transfer). Hence this task requires more of the
word embeddings than the previous task.
Languages Model
FA DWA DWA
k = 0 k = 3
ZH|EN 49.4 48.4 48.7
EN|ZH 44.9 45.3 45.9
FR|EN 17.1 17.2 17.0
EN|FR 16.6 16.3 16.1
Table 1: Alignment error rate (AER) compar-
ison, in both directions, between the FASTAL-
IGN (FA) alignment model and our model (DWA)
with k context words (see Equation 1). Lower
numbers indicate better performance.
We mainly follow the setup of Klementiev et al
(2012) and use the German-English parallel cor-
pus of the European Parliament proceedings to
train the word representations. We perform the
classification task on the Reuters RCV1/2 corpus.
Unlike Klementiev et al (2012), we do not use that
corpus during the representation learning phase.
We remove all words occurring less than five times
in the data and learn 40 dimensional word embed-
dings in line with prior work.
To train a classifier on English data and test it
on German documents we first project word rep-
resentations from English into German: we select
the most probable German word according to the
learned translation probabilities, and then compute
document representations by averaging the word
representations in each document. We use these
projected representations for training and subse-
quently test using the original German data and
representations. We use an averaged perceptron
classifier as in prior work, with the number of
epochs (3) tuned on a subset of the training set.
Table 2 shows baselines from previous work
and classification accuracies. Our model outper-
forms the model by Klementiev et al (2012), and
it also outperforms the most comparable models
by Hermann and Blunsom (2014b) when training
on German data and performs on par with it when
training on English data.
1
It seems that our model
learns more informative representations towards
document classification, even without additional
monolingual language models or context informa-
tion. Again the impact of context is inconclusive.
1
From Hermann and Blunsom (2014a, 2014b) we only
compare with models equivalent with respect to embedding
dimensionality and training data. They still achieve the state
of the art when using additional training data.
227
Model en? de de? en
Majority class 46.8 46.8
Glossed 65.1 68.6
MT 68.1 67.4
Klementiev et al 77.6 71.1
BiCVM ADD 83.7 71.4
BiCVM BI 83.4 69.2
DWA (k = 0) 82.8 76.0
DWA (k = 3) 83.1 75.4
Table 2: Document classification accuracy when
trained on 1,000 training examples of the RCV1/2
corpus (train?test). Baselines are the majority
class, glossed, and MT (Klementiev et al, 2012).
Further, we are comparing to Klementiev et al
(2012), BiCVM ADD (Hermann and Blunsom,
2014a), and BiCVM BI (Hermann and Blunsom,
2014b). k is the context size, see Equation 1.
5.3 Representation Visualization
Following the document classification task we
want to gain further insight into the types of fea-
tures our embeddings learn. For this we visu-
alize word representations using t-SNE projec-
tions (van der Maaten and Hinton, 2008). Fig-
ure 1 shows an extract from our projection of the
2,000 most frequent German words, together with
an expected representation of a translated English
word given translation probabilities. Here, it is
interesting to see that the model is able to learn
related representations for words chair and rat-
spr?asidentschaft (presidency) even though these
words were not aligned by our model. Figure 2
shows an extract from the visualization of the
10,000 most frequent English words trained on an-
other corpus. Here again, it is evident that the em-
beddings are semantically plausible with similar
words being closely aligned.
6 Conclusion
We presented a new probabilistic model for learn-
ing bilingual word representations. This dis-
tributed word alignment model (DWA) learns both
representations and alignments at the same time.
We have shown that the DWA model is able
to learn alignments on par with the FASTALIGN
alignment model which produces very good align-
ments, thereby determining the efficacy of the
learned representations which are used to calculate
Figure 1: A visualization of the expected represen-
tation of the translated English word chair among
the nearest German words: words never aligned
(green), and those seen aligned (blue) with it.
Figure 2: A cluster of English words from the
10,000 most frequent English words visualized us-
ing t-SNE. Word representations were optimized
for p(zh|en) (k = 0).
word translation probabilities for the alignment
task. Subsequently, we have demonstrated that
our model can effectively be used to project doc-
uments from one language to another. The word
representations our model learns as part of the
alignment process are semantically plausible and
useful. We highlighted this by applying these em-
beddings to a cross-lingual document classifica-
tion task where we outperform prior work, achieve
results on par with the current state of the art and
provide new state-of-the-art results on one of the
tasks. Having provided a probabilistic account of
word representations across multiple languages,
future work will focus on applying this model to
machine translation and related tasks, for which
previous approaches of learning such embeddings
are less suited. Another avenue for further study
is to combine this method with monolingual lan-
guage models, particularly in the context of se-
mantic transfer into resource-poor languages.
Acknowledgements
This work was supported by a Xerox Foundation
Award and EPSRC grant number EP/K036580/1.
We acknowledge the use of the Oxford ARC.
228
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155, February.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311, June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of NAACL-
HLT.
Manaal Faruqui and Chris Dyer. 2014. Improving Vec-
tor Space Word Representations Using Multilingual
Correlation. In Proceedings of EACL.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
HLT.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Compo-
sitional Semantics. In Proceedings of ACL.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of ICLR.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual Models for Compositional Distributional
Semantics. In Proceedings of ACL.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic Frame Iden-
tification with Distributed Word Representations. In
Proceedings of ACL.
Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdi-
nov. 2013. Multimodal neural language models. In
NIPS Deep Learning Workshop.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit.
Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.
2013. Learning multilingual word representations
using a bag-of-words autoencoder. In NIPS Deep
Learning Workshop.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. CoRR, abs/1309.4168.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. 2013. Playing atari with
deep reinforcement learning. In NIPS Deep Learn-
ing Workshop.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics, pages 246?252.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986. Learning representations by back-
propagating errors. Nature, 323:533?536, October.
A P Sarath Chandar, M Khapra Mitesh, B Ravindran,
Vikas Raykar, and Amrita Saha. 2013. Multilingual
deep learning. In Deep Learning Workshop at NIPS.
Holger Schwenk, Marta R. Costa-jussa, and Jose A.
R. Fonollosa. 2007. Smooth bilingual n-gram trans-
lation. In Proceedings of EMNLP-CoNLL.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In Proceedings of COLING: Posters.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visual-
izing high-dimensional data using t-sne. Journal of
Machine Learning Research, 9:2579?2605.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
dings for Phrase-Based Machine Translation. In
Proceedings of EMNLP.
229
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 8,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
New Directions in Vector Space Models of Meaning
Phil Blunsom, Edward Grefenstette
and Karl Moritz Hermann
?
University of Oxford
first.last@cs.ox.ac.uk
Georgiana Dinu
Center for Mind/Brain Sciences
University of Trento
georgiana.dinu@unitn.it
1 Abstract
Symbolic approaches have dominated NLP as a
means to model syntactic and semantic aspects of
natural language. While powerful inferential tools
exist for such models, they suffer from an inabil-
ity to capture correlation between words and to
provide a continuous model for word, phrase, and
document similarity. Distributed representations
are one mechanism to overcome these constraints.
This tutorial will supply NLP researchers with
the mathematical and conceptual background to
make use of vector-based models of meaning in
their own research. We will begin by motivating
the need for a transition from symbolic represen-
tations to distributed ones. We will briefly cover
how collocational (distributional) vectors can be
used and manipulated to model word meaning. We
will discuss the progress from distributional to dis-
tributed representations, and how neural networks
allow us to learn word vectors and condition them
on metadata such as parallel texts, topic labels, or
sentiment labels. Finally, we will present various
forms of semantic vector composition, and discuss
their relative strengths and weaknesses, and their
application to problems such as language mod-
elling, paraphrasing, machine translation and doc-
ument classification.
This tutorial aims to bring researchers up to
speed with recent developments in this fast-
moving field. It aims to strike a balance be-
tween providing a general introduction to vector-
based models of meaning, an analysis of diverg-
ing strands of research in the field, and also being
a hands-on tutorial to equip NLP researchers with
the necessary tools and background knowledge to
start working on such models. Attendees should
be comfortable with basic probability, linear alge-
bra, and continuous mathematics. No substantial
knowledge of machine learning is required.
?
Instructors listed in alphabetical order.
2 Outline
1. Motivation: Meaning in space
2. Learning distributional models for words
3. Neural language modelling and distributed
representations
(a) Neural language model fundamentals
(b) Recurrent neural language models
(c) Conditional neural language models
4. Semantic composition in vector spaces
(a) Algebraic and tensor-based composition
(b) The role of non-linearities
(c) Learning recursive neural models
(d) Convolutional maps and composition
3 Instructors
Phil Blunsom is an Associate Professor at the
University of Oxford?s Department of Computer
Science. His research centres on the probabilistic
modelling of natural languages, with a particular
interest in automating the discovery of structure
and meaning in text.
Georgiana Dinu is a postdoctoral researcher
at the University of Trento. Her research re-
volves around distributional semantics with a fo-
cus on compositionality within the distributional
paradigm.
Edward Grefenstette is a postdoctoral researcher
at Oxford?s Department of Computer Science. He
works on the relation between vector represen-
tations of language meaning and structured logi-
cal reasoning. His work in this area was recently
recognised by a best paper award at *SEM 2013.
Karl Moritz Hermann is a final-year DPhil stu-
dent at the Department of Computer Science in
Oxford. His research studies distributed and com-
positional semantics, with a particular emphasis
on mechanisms to reduce task-specific and mono-
lingual syntactic bias in such representations.
8
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70?74,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Learning Semantics and Selectional Preference of Adjective-Noun Pairs
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
karl.moritz.hermann@cs.ox.ac.uk
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cdyer@cs.cmu.edu
Phil Blunsom
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
phil.blunsom@cs.ox.ac.uk
Stephen Pulman
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
stephen.pulman@cs.ox.ac.uk
Abstract
We investigate the semantic relationship be-
tween a noun and its adjectival modifiers.
We introduce a class of probabilistic mod-
els that enable us to to simultaneously cap-
ture both the semantic similarity of nouns
and modifiers, and adjective-noun selectional
preference. Through a combination of novel
and existing evaluations we test the degree to
which adjective-noun relationships can be cat-
egorised. We analyse the effect of lexical con-
text on these relationships, and the efficacy of
the latent semantic representation for disam-
biguating word meaning.
1 Introduction
Developing models of the meanings of words and
phrases is a key challenge for computational linguis-
tics. Distributed representations are useful in captur-
ing such meaning for individual words (Sato et al,
2008; Maas and Ng, 2010; Curran, 2005). How-
ever, finding a compelling account of semantic com-
positionality that utilises such representations has
proven more difficult and is an active research topic
(Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). It is in
this area that our paper makes its contribution.
The dominant approaches to distributional se-
mantics have relied on relatively simple frequency
counting techniques. However, such approaches fail
to generalise to the much sparser distributions en-
countered when modeling compositional processes
and provide no account of selectional preference.
We propose a probabilistic model of the semantic
representations for nouns and modifiers. The foun-
dation of this model is a latent variable representa-
tion of noun and adjective semantics together with
their compositional probabilities. We employ this
formulation to give a dual view of noun-modifier
semantics: the induced latent variables provide an
explicit account of selectional preference while the
marginal distributions of the latent variables for each
word implicitly produce a distributed representation.
Most related work on selectional preference uses
class-based probabilities to approximate (sparse)
individual probabilities. Relevant papers include
O? Se?aghdha (2010), who evaluates several topic
models adapted to learning selectional preference
using co-occurence and Baroni and Zamparelli
(2010), who represent nouns as vectors and adjec-
tives as matrices, thus treating them as functions
over noun meaning. Again, inference is achieved
using co-occurrence and dimensionality reduction.
2 Adjective-Noun Model
We hypothesize that semantic classes determine the
semantic characteristics of nouns and adjectives, and
that the distribution of either with respect to other
components of the sentences they occur in is also
mediated by these classes (i.e., not by the words
themselves). We assume that in general nouns select
for adjectives,1 and that this selection is dependent
on both their latent semantic classes. In the next sec-
tion, we describe a model encoding our hypotheses.
2.1 Generative Process
We model a corpus D of tuples of the form
(n,m, c1 . . . ck) consisting of a noun n, an adjective
m (modifier), and k words of context. The context
variables (c1 . . . ck) are treated as a bag of words and
1We evaluate this hypothesis as well as its inverse.
70
|N| |M|
|N|
N M
n m
c
k
|D|
?
N
?
N
?
M
?
M
?
c
?
c
|N|
?
n
?
m
?
n
?
m
Figure 1: Plate diagram illustrating our model of noun
and modifier semantic classes (designated N and M , re-
spectively), a modifier-noun pair (m,n), and its context.
include the words to the left and right of the noun,
its siblings and governing verbs. We designate the
vocabulary Vn for nouns, Vm for modifiers and Vc
for context. We use zi to refer to the ith tuple in D
and refer to variables within that tuple by subscript-
ing them with i, e.g., ni and c3,i are the noun and
the third context variable of zi. The latent noun and
adjective class variables are designated Ni and Mi.
The corpus D is generated according to the plate
diagram in figure 1. First, a set of parameters is
drawn. A multinomial ?N representing the distribu-
tion of noun semantic classes in the corpus is drawn
from a Dirichlet distribution with parameter ?N. For
each noun class i we have distributions ?Mi over
adjective classes, ?ni over Vn and ?
c
i over Vc, also
drawn from Dirichlet distributions. Finally, for each
adjective class j, we have distributions ?mj over Vm.
Next, the contents of the corpus are generated by
first drawing the length of the corpus (we do not
parametrise this since we never generate from this
model). Then, for each i, we generate noun class
Ni, adjective class Mi, and the tuple zi as follows:
Ni | ?
N ? Multi(?N)
Mi | ?
M
Ni? Multi(?
M
Ni)
ni | ?
n
Ni? Multi(?
n
Ni)
mi | ?
m
Mi? Multi(?
m
Mi)
?k: ck,i | ?
c
Ni? Multi(?
c
Ni)
2.2 Parameterization and Inference
We use Gibbs sampling to estimate the distributions
ofN andM , integrating out the multinomial param-
eters ?x (Griffiths and Steyvers, 2004). The Dirich-
let parameters ? are drawn independently from a
?(1, 1) distribution, and are resampled using slice
sampling at frequent intervals throughout the sam-
pling process (Johnson and Goldwater, 2009). This
?vague? prior encourages sparse draws from the
Dirichlet distribution. The number of noun and ad-
jective classes N and M was set to 50 each; other
sizes (100,150) did not significantly alter results.
3 Experiments
As our model was developed on the basis of several
hypotheses, we design the experiments and evalu-
ation so that these hypotheses can be examined on
their individual merit. We test the first hypothesis,
that nouns and adjectives can be represented by se-
mantic classes, recoverable using co-occurence, us-
ing a sense clustering evaluation by Ciaramita and
Johnson (2003). The second hypothesis, that the dis-
tribution with respect to context and to each other is
governed by these semantic classes is evaluated us-
ing pseudo-disambiguation (Clark and Weir, 2002;
Pereira et al, 1993; Rooth et al, 1999) and bigram
plausibility (Keller and Lapata, 2003) tests.
To test whether noun classes indeed select for ad-
jective classes, we also evaluate an inverse model
(Modi), where the adjective class is drawn first, in
turn generating both context and the noun class. In
addition, we evaluate copies of both models ignoring
context (Modnc and Modinc).
We use the British National Corpus (BNC), train-
ing on 90 percent and testing on 10 percent of the
corpus. Results are reported after 2,000 iterations
including a burn-in period of 200 iterations. Classes
are marginalised over every 10th iteration.
4 Evaluation
4.1 Supersense Tagging
Supersense tagging (Ciaramita and Johnson, 2003;
Curran, 2005) evaluates a model?s ability to clus-
ter words by their semantics. The task of this eval-
uation is to determine the WORDNET supersenses
of a given list of nouns. We report results on the
WN1.6 test set as defined by Ciaramita and John-
son (2003), who used 755 randomly selected nouns
with a unique supersense from the WORDNET 1.6
71
corpus. As their test set was random, results weren?t
exactly replicable. For a fair comparison, we select
all suitable nouns from the corpus that also appeared
in the training corpus. We report results on type and
token level (52314 tokens with 1119 types). The
baseline2 chooses the most common supersense.
k Token Type
Baseline .241 .210
Ciaramita & Johnson .523 .534
Curran - .680
Mod 10 .592 .517
Modnc 10 .473 .410
Table 1: Supersense evaluation results. Values are the
percentage of correctly assigned supersenses. k indicates
the number of nearest neighbours considered.
We use cosine-similarity on the marginal noun
class vectors to measure distance between nouns.
Each noun in the test set is then assigned a su-
persense by performing a distance-weighted voting
among its k nearest neighbours. Results of this eval-
uation are shown in Table 1, with Figure 2 showing
scores for model Mod across different values for k.
Figure 2: Scores of Mod on the supersense task. The up-
per line denotes token-, the lower type-level scores. The
y-axis is the percentage of correct assignments, the x-axis
denotes the number of neighbours included in the vote.
The results demonstrate that nouns can semanti-
cally be represented as members of latent classes,
while the superiority of Mod over Modnc supports
our hypothesis that context co-occurence is a key
feature for learning these classes.
4.2 Pseudo-Disambiguation
Pseudo-disambiguation was introduced by Clark
and Weir (2002) to evaluate models of selectional
preference. The task is to select the more probable
of two candidate arguments to associate with a given
2The baseline results are from Ciaramita and Johnson
(2003). Using the majority baseline on the full test set, we only
get .176 and .160 for token and type respectively.
predicate. For us, this is to decide which adjective,
a1 or a2, is more likely to modify a noun n.
We follow the approach by Clark and Weir (2002)
to create the test data. To improve the quality of
the data, we filtered using bigram counts from the
Web1T corpus, setting a lower bound on the proba-
ble bigram (a1, n) and chosing a2 from five candi-
dates, picking the lowest count for bigram (a2, n).
We report results for all variants of our model in
Table 2. As baseline we use unigram counts in our
training data, chosing the more frequent adjective.
L-bound 0 100 500 1000
Size 5714 5253 3741 2789
Baseline .543 .543 .539 .550
Mod .783 .792 .810 .816
Modi .781 .787 .800 .810
Modnc .720 .728 .746 .750
Modinc .722 .730 .747 .752
Table 2: Pseudo-disambiguation: Percentage of correct
choices made. L-bound denotes the Web1T lower bound
on the (a1, n) bigram, size the number of decisions made.
While all models decisively beat the baseline, the
models using context strongly outperform those that
do not. This supports our hypothesis regarding the
importance of context in semantic clustering.
The similarity between the normal and inverse
models implies that the direction of the noun-
adjective relationship has negligible impact for this
evaluation.
4.3 Bigram Plausibility
Bigram plausibility (Keller and Lapata, 2003) is a
second evaluation for selectional preference. Unlike
the frequency-based pseudo-disambiguation task, it
evaluates how well a model matches human judge-
ment of the plausibility of adjective-noun pairs.
Keller and Lapata (2003) demonstrated a correlation
between frequencies and plausibility, but this does
not sufficiently explain human judgement. An ex-
ample taken from their unseen data set illustrates the
dissociation between frequency and plausibility:
? Frequent, implausible: ?educational water?
? Infrequent, plausible: ?difficult foreigner?3
The plausibility evaluation has two data sets of 90
adjective-noun pairs each. The first set (seen) con-
tains random bigrams from the BNC. The second set
(unseen) are bigrams not contained in the BNC.
3At the time of writing, Google estimates 56,900 hits for
?educational water? and 575 hits for ?difficult foreigner?. ?Ed-
ucational water? ranks bottom in the gold standard of the unseen
set, ?difficult foreigner? ranks in the top ten.
72
Recent work (O? Se?aghdha, 2010; Erk et al,
2010) approximated plausibility with joint probabil-
ity (JP). We believe that for semantic plausibility
(not probability!) mutual information (MI), which
factors out acutal frequencies, is a better metric.4 We
report results using JP, MI and MI?2.
Seen Unseen
r ? r ?
AltaVista .650 ? .480 ?
BNC (Rasp) .543 .622 .135 .102
Pado? et al .479 .570 .120 .138
LDA .594 .558 .468 .459
ROOTH-LDA .575 .599 .501 .469
DUAL-LDA .460 .400 .334 .278
Mod (JP) .495 .413 .286 .276
Mod (MI) .394 .425 .471 .457
Mod (MI?2) .575 .501 .430 .408
Modnc (JP) .626 .505 .357 .369
Modnc (MI) .628 .574 .427 .385
Modnc (MI?2) .701 .623 .423 .394
Table 3: Results (Pearson r and Spearman ? correlations)
on the Keller and Lapata (2003) plausibility data. Bold
indicates best scores, underlining our best scores. High
values indicate high correlation with the gold standard.
Table 3 shows the performance of our models
compared to results reported in O? Se?aghdha (2010).
As before, results between the normal and the in-
verse model (omitted due to space) are very simi-
lar. Surprisingly, the no-context models consistently
outperform the models using context on the seen
data set. This suggests that the seen data set can
quite precisely be ranked using frequency estimates,
which the no-context models might be better at cap-
turing without the ?noise? introduced by context.
Standard Inverse (i)
r ? r ?
Mod (JP) .286 .276 .243 .245
Mod (MI) .471 .457 .409 .383
Mod (MI?2) .430 .408 .362 .347
Modnc (JP) .357 .369 .181 .161
Modnc (MI) .427 .385 .220 .209
Modnc (MI?2) .423 .394 .218 .185
Table 4: Results on the unseen plausibility dataset.
The results on the unseen data set (Table 4)
prove interesting as well. The inverse no-context
model is performing significantly poorer than any
of the other models. To understand this result we
must investigate the differences between the unseen
data set and the seen data set and to the pseudo-
disambiguation evaluation. The key difference to
pseudo-disambiguation is that we measure a human
4See (Evert, 2005) for a discussion of these metrics.
plausibility judgement, which ? as we have demon-
strated ? only partially correlates with bigram fre-
quencies. Our models were trained on the BNC,
hence they could only learn frequency estimates for
the seen data set, but not for the unseen data.
Based on our hypothesis about the role of con-
text, we expect Mod and Modi to learn semantic
classes based on the distribution of context. Without
the access to that context, we argued thatModnc and
Modinc would instead learn frequency estimates.5
The hypothesis that nouns generally select for ad-
jectives rather than vice versa further suggests that
Mod and Modnc would learn semantic properties
that Modi and Modinc could not learn so well.
In summary, we hence expected Mod to perform
best on the unseen data, learning semantics from
both context and noun-adjective selection. Also, as
supported by the results, we expected Modinc to
performs poorly, as it is the model least capable of
learning semantics according to our hypotheses.
5 Conclusion
We have presented a class of probabilistic mod-
els which successfully learn semantic clusterings of
nouns and a representation of adjective-noun selec-
tional preference. These models encoded our beliefs
about how adjective-noun pairs relate to each other
and to the other words in the sentence. The perfor-
mance of our models on estimating selectional pref-
erence strongly supported these initial hypotheses.
We discussed plausibility judgements from a the-
oretical perspective and argued that frequency esti-
mates and JP are imperfect approximations for plau-
sibility. While models can perform well on some
evaluations by using either frequency estimates or
semantic knowledge, we explained why this does
not apply to the unseen plausibility test. The perfor-
mance on that task demonstrates both the success of
our model and the shortcomings of frequency-based
approaches to human plausibility judgements.
Finally, this paper demonstrated that it is feasi-
ble to learn semantic representations of words while
concurrently learning how they relate to one another.
Future work will explore learning words from
broader classes of semantic relations and the role of
context in greater detail. Also, we will evaluate the
system applied to higher level tasks.
5This could also explain their weaker performance on
pseudo-disambiguation in the previous section, where the neg-
ative examples had zero frequency in the training corpus.
73
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP ?03,
pages 168?175, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Comput.
Linguist., 28:187?206, June.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?05, pages 26?33, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36:723?763.
Stefan Evert. 2005. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. the-
sis, Universita?t Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 1394?1404,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 317?325, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, pages 459?484.
Andrew L. Maas and Andrew Y. Ng. 2010. A probabilis-
tic model for semantic word vectors. In Workshop on
Deep Learning and Unsupervised Feature Learning,
NIPS ?10.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL-HLT?08,
pages 236 ? 244.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 435?444, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st annual meeting on Association for
Computational Linguistics, ACL ?93, pages 183?190,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th annual meeting of the Association
for Computational Linguistics on Computational Lin-
guistics, ACL ?99, pages 104?111, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa.
2008. Knowledge discovery of semantic relationships
between words using nonparametric bayesian graph
model. In Proceeding of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, KDD ?08, pages 587?595, New York,
NY, USA. ACM.
74
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 132?141,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
An Unsupervised Ranking Model for Noun-Noun Compositionality
Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman
Department of Computer Science
University of Oxford
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom,stephen.pulman}@cs.ox.ac.uk
Abstract
We propose an unsupervised system that
learns continuous degrees of lexicality for
noun-noun compounds, beating a strong base-
line on several tasks. We demonstrate that the
distributional representations of compounds
and their parts can be used to learn a fine-
grained representation of semantic contribu-
tion. Finally, we argue such a representation
captures compositionality better than the cur-
rent status-quo which treats compositionality
as a binary classification problem.
1 Introduction
A Multiword Expressions (MWE) can be defined as
a sequence of words whose meaning cannot nec-
essarily be derived from the meaning of the words
making up that sequence, for example:
Rat Race ? self-defeating or pointless pursuit1
MWEs are considered a ?key problem for the de-
velopment of large-scale, linguistically sound nat-
ural language processing technology? (Sag et al,
2002). The challenge posed by MWEs is three-
fold, consisting of MWE identification, classifica-
tion and interpretation. Following the identification
of a MWE, it needs to be established whether the
expression should be treated as lexical (idiomatic)
or as compositional. The final step, learning the se-
mantics of the MWE, strongly depends on this deci-
sion.
1Definition taken from Wikipedia, and clearly not recover-
able if one only knows the meaning of the words ?rat? and ?race?.
The problem posed by MWEs is considered hard,
but at the same time it is highly relevant and inter-
esting. MWEs occur frequently in language and in-
terpreting them correctly would directly improve re-
sults in a number of tasks in NLP such as translation
and parsing (Korkontzelos and Manandhar, 2010).
By extension this makes deciding the lexicality of
MWEs an important challenge for various fields in-
cluding machine translation, question answering and
information retrieval. In this paper we discuss com-
positionality with respect to noun-noun compounds.
Most Computational Linguistics literature treats
compositionality as a binary problem, classifying
compounds as either lexical or compositional. We
show that this approach is too simplistic and argue
for the real-valued treatment of compositionality.
We propose two unsupervised models that learn
compositionality rankings for compounds, placing
them on a scale between lexical and compositional
extremes. We develop a fine-grained representa-
tion of compositionality using a novel generative ap-
proach that models context as generated by com-
pound constituents. This representation differenti-
ates between the semantic contribution of both com-
pound constituents as well as the compound itself.
Comparing it with existing work in the field, we
demonstrate the competitiveness of our approach.
We evaluate on an existing corpus of noun com-
pounds with ranked compositionality data, as well
as on a large corpus with a binary annotation for lex-
ical and compositional compounds. We analyse the
impact of data sparsity and propose an interpolation
approximation which significantly reduces the effect
of sparsity on model performance.
132
2 Related Work
Interpreting MWEs is a difficult task as ?compound
nouns can be freely constructed? (Spa?rck Jones,
1985), and are thus able to proliferate infinitely. At
the same time, semantic composition can take many
different forms, making uniform interpretation of
compounds impossible (Zanzotto et al, 2010).
Most current work on MWEs focuses on inter-
preting compounds and sidesteps the task of deter-
mining whether a compound is compositional in the
first place (Butnariu et al, 2010; Kim and Baldwin,
2008). Such methods, aimed at learning the seman-
tics of compounds, can roughly be divided into two
major strands of research.
One group relies on data intensive methods to ex-
tract semantics vectors from large corpora (Baroni
and Zamparelli, 2010; Zanzotto et al, 2010; Gies-
brecht, 2009). The focus of these approaches is to
develop methods for composing the vectors of un-
igrams into a semantic vector representing a com-
pound. Some of the work in this area touches on the
issue of lexicality, as models learning distributional
representations of MWEs ideally would first estab-
lish whether a given MWE is compositional or not
(Mitchell and Lapata, 2010).
The other group are knowledge intensive ap-
proaches collecting linguistic features (Kim and
Baldwin, 2005; Korkontzelos and Manandhar,
2009). Tratz and Hovy (2010), for instance, train
a classifier for noun compound interpretation on a
large set of WORDNET and Thesaurus features.
Combined approaches include Kim and Baldwin
(2008), who interpret noun compounds by extrapo-
lating their semantics from observations where the
two nouns forming a compound are in an intransi-
tive relationship. For example extracting the phrase
?the family owns a car? from the training data would
help learn that the compound ?family car? describes
a POSSESSOR-OWNED/POSSESSED relationship.
Some of these supervised classifiers include lexi-
cality as a classification option, considering it jointly
with the actual compound interpretation.
Next to the work on MWE interpretation there has
been some work focused on determining lexicality
in its own right (Reddy et al, 2011; Bu et al, 2010;
Kim and Baldwin, 2007).
One possibility is to exploit special properties of
lexical MWEs such as high statistical association
of their constituents (Pedersen, 2011) or syntactic
rigidity (Fazly et al, 2009; McCarthy et al, 2007).
However, these approaches are limited in their ap-
plicability to compound nouns (Reddy et al, 2011).
Another method is to compare the semantics of
a compound and its constituents to decide com-
positionality. The approaches used to determine
those semantics can again be divided into knowl-
edge intensive and data-driven methods. Depending
on the chosen representation of semantics these ap-
proaches can either be used for supervised classifiers
or together with a distance metric comparing vector
space representations of semantics. In a binary set-
ting, a threshold would then be applied to the result
of that distance function (Korkontzelos and Man-
andhar, 2009). In a real-valued setting the distance
metric itself can be used as a measure for compo-
sitionality (Reddy et al, 2011). Related to the vec-
tor space based models, some research focuses on
improving the distance metrics used to compare in-
duced semantics (Bu et al, 2010).
3 Methodology
English noun-noun compounds are majority left-
branching (Lauer, 1995), with a head (the second
element), modified by an attributive noun (first el-
ement). For example:
Ground Floor ? The floor of a building at or near-
est ground level.2
In this paper, we will use the terms attributive noun
(AN) and head noun (HN) to refer to the first and
second noun in a noun compound.
3.1 Real-Valued Representation
Lexicality of MWEs is frequently treated as a bi-
nary property (Tratz and Hovy, 2010; O? Se?aghdha,
2007). We argue that lexicality should instead be
treated as a graded property, as most compound se-
mantics exhibit a mixture of compositional and lexi-
cal influences. For example, ?cocktail dress? derives
a large part of its semantics from ?dress?, but the
compound also contributes an idiosyncratic element
to its meaning.
2Definition from http://www.thefreedictionary.com
133
We define lexicality as the degree to which id-
iosyncrasy contributes to a compound?s semantics.
Inversely phrased, the compositionality of a com-
pound can be defined as the degree to which its sense
is related to the senses of its constituents.3
This graded representation follows Spa?rck Jones
(1985), who argued that ?it is not possible to main-
tain a principled distinction between lexicalised and
non-lexicalised compounds?. Some recent work
also supports this view (Reddy et al, 2011; Bu et
al., 2010; Baldwin, 2006). From a practical per-
spective, a real-valued representation of composi-
tionality should help improve interpretation of com-
pounds. This is especially true when factoring in the
respective semantic contributions of its parts.
3.2 Context Generation
According to the distributional hypothesis, the se-
mantics of a lexical item can be expressed by its
context. We apply this hypothesis to the problem of
noun compound compositionality by using a genera-
tive model on compound context. Our model allows
context to be generated by the compound itself or by
either one of its constituents. By learning which el-
ement of the compound generates which part of its
context we effectively determine the semantic con-
tribution of each element. This in turn gives us a
fine-grained, graded representation of a compound?s
lexicality.
4 Corpora for Evaluation
4.1 Ranked Corpus ? REDDY
As we want to evaluate our models? ability to learn
lexicality as a real-valued property, we require an
annotated data set of noun compounds ranked by
lexicality. To the best of our knowledge the only
such data set was developed by Reddy et al (2011).
This data set contains 90 distinct noun compounds
with real-valued gold standard scores ranking from
0 (lexical) to 5 (compositional). The compounds
are nearly linearly distributed across the [0;5] range,
with inter annotator agreement (Spearman?s ?) of
3For example, the meaning of ?gravy train? has hardly any
relation to either ?gravy? or ?train?. Its semantics are thus highly
dependent on the compound in its own right. On the other end
of the spectrum, ?climate change? is significantly related to both
?climate? and ?change?, contributing little inherent semantics to
its overall meaning.
0.522. We refer to this data set and evaluation as
REDDY throughout this paper.
4.2 Binary Corpora ? TRATZ
We also apply our models to a second, binary classi-
fication task. Tratz and Hovy (2010) compiled a data
set for noun compound interpretation, which classi-
fies noun compounds based on their internal struc-
ture. We use this corpus to extract lexical and com-
positional noun compounds.
After some pre-processing4 the data set contains
18,858 compositional and 118 lexical noun com-
pounds. We believe this to more accurately represent
the real world distribution of lexical and composi-
tional noun compounds: Tratz and Hovy (2010) ex-
tracted noun compounds from several large corpora
including the Wall Street Journal section of the Penn
Treebank, thus obtaining a reasonable approxima-
tion of real world occurrence. Other collections of
noun compounds (O? Se?aghdha, 2007) feature sim-
ilar proportions of lexical and compositional noun
compounds.
The large bias towards compositional noun com-
pounds does not support the status-quo of treating
compositionality as a binary property. As discussed
earlier, we assume that most compounds have a
compositional as well as a lexical element. While
the compositional aspect may be larger for most
compounds this alone does not suffice as a reason
to disregard the lexical element contained in these
compounds.
In order to evaluate our system on the TRATZ
data, we use receiving operator characteristic (ROC)
curves. ROC analysis enables us to evaluate a rank-
ing model without setting an artificial threshold for
the compositionality/lexicality decision.
5 Baseline Approach
We develop a set of advanced baselines related to
the semi-supervised models presented by Reddy et
al. (2011). We define the context K of a noun com-
pound as all words in all sentences the compound
appears in. From this we calculate distributional
representations of a compound (c = ?a, h?) and its
constituent elements a, h. We refer to these repre-
sentations as ~c for the compound and ~a, ~h for the
4We removed trigrams from the data set.
134
Name ? r ?
ADD w.Sac + (1? w).Shc .323 .567
MULT Sac.Shc .379 .551
MIN min(Sac, Shc) .343 .550
MAX max(Sac, Shc) .299 .505
COMB w1.Sac+w2.Shc+w3.Sac.Shc .366 .556
Table 1: Results of COSLEX with different operators on
the REDDY data set, reporting Pearson?s r and Spear-
man?s ? correlations. Weights for operators ADD (w =
0.3) and COMB (w = ?0.3, 0.1, 0.6?) are manually opti-
mised. Values range from -1 (negative correlation) to +1
(perfect correlation) with 0 describing random data.
attributive and head noun, respectively. We can cal-
culate the cosine similarity based lexicality score
(COSLEX) by combining the cosine similarity of the
compound?s distribution with each of its two con-
stituents (Reddy et al, 2011).
Sac = sim(~a,~c)
Shc = sim(~h,~c)
COSLEX(c) = Sac ? Shc
We evaluate a number of alternative operators ? for
combining Sac and Shc. Results for this baseline
on the REDDY corpus are in Table 1,5 with weights
wi on the combination operators manually optimised
for Spearman?s ? on that data set. In effect this
renders this baseline into a supervised approach, so
we would expect it to perform very well. We use
the best performing operators (ADD with w = 0.3,
MULT) as baselines for this paper.
6 Generative Models
We exploit the distributional hypothesis to model
the semantic contribution of the different elements
of a noun compound. For this, we require a sys-
tem that treats a noun compound as a vector of three
semantics-bearing units: the compound itself, its
head and its attributive noun. This system should
then model the relationship between the context of
the compound and these three units, deciding which
of them is responsible for each context element.
5Reddy et al (2011) report higher figures on our baseline
models. The differences are attributed to differences in training
data and parametrization.
6.1 3-way Compound Mixture
We model a corpus D of tuples d = {c, k1, ..., kn}.
Each tuple d contains a noun compound c = ?a, h?
and its context words K = (k1, ..., kn). We use vo-
cabularies Vc for noun compounds, Va for attributive
nouns, Vh for head nouns and Vk for context.
We condition our generative model on the noun
compounds. Given an observation d of a compound
c, we generate each context word in two steps. First,
we choose one of the compounds three elements6 to
generate the next context word. Second, we gener-
ate a new context word conditioned on that element.
Formally, the context is generated as follows.
We draw three multinomial parameters ?c, ?a
and ?h from Dirichlet distributions with parameters
?c, ?a and ?h. ?c represents the distribution over
context words Vk given compound c. ?a and ?h
are distributions over Vk given attributive noun a and
head noun h, respectively. These three distributions
form the mixture components of our model.
A fourth multinomial parameter ?z , drawn from
a Dirichlet distribution with parameter ?z , controls
the distribution over the mixture components. ?z is
specific to each compound c, so multiple observa-
tions of the same compound share this parameter.
For each context word we draw a mixture compo-
nent zc,i ? {c?, a?, h?} from the multinomial distribu-
tion with parameter ?z . zc,i determines which dis-
tribution the context word itself will be drawn from.
Finally, we draw the context word:
?i: ki | ?
{zc,i} ? Multi(?{zc,i})
Thus, for each observation of a compound noun we
have a vector zc = ?z1, ..., zn? detailing how its
context words were created either by the compound
itself or by one of its constituents. To determine lex-
icality, we are interested in learning the multinomial
parameter ?z , which describes to what extent the
compound and its constituents contribute to the gen-
eration of the context (i.e. semantics). We can ap-
proximate ?z from the vector zc.
We define the lexicality score Lex(c) for a com-
pound as the percentage of context words created by
6The compound itself, its attributive noun and its head noun
135
Figure 1: Plate diagram illustrating the MULT-CMPD
model with context words ki drawn from a mixture model
with three components controlled by zi.
the compound and not one of its constituents:
Lex(c) = p(z=c?|?a, h?), (1)
where c = ?a, h?
Figure 1 shows a plate diagram of this model, which
we will refer to as MULT-CMPD.
One hypothesis encoded in model MULT-CMPD
is that deciding which part of a compound (the com-
pound itself, the head or the attributive noun) gen-
erates context is a single decision. An alternative
representation could treat this as a two-step process,
which we encode in a second model BIN-CMPD.
The intuition behind the BIN-CMPD model is that
there are two distinct decisions. First, whether a
compound is compositional or not. Second, whether
(in the compositional case) its semantics stem from
its head or attributive noun
Where MULT-CMPD uses a three component mix-
ture to determine which multinomial distribution to
use, BIN-CMPD uses two cascaded binary mixtures
(see Figure 2). The BIN-CMPD model first chooses
whether to treat a compound as compositional or
lexical. If the compound is determined as composi-
tional, a second binary mixture determines whether
to generate a context word using the attributive (?a)
or head multinomial (?h). For the lexical case, the
model remains unchanged.
Figure 2: Schematic description of compositional-
ity/lexicality decision for models MULT-CMPD and BIN-
CMPD.
Model r ?
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551
MULT-CMPD .141 .435
BIN-CMPD .168 .410
Table 2: Results on the REDDY data set, reporting Pear-
son?s r and Spearman?s ? correlations. Values range from
-1 (negative correlation) to +1 (perfect correlation).
6.1.1 Inference and Sampling
We use Gibbs sampling to learn the vectors z for
each instance d, integrating out the parameters ?x.
We train our models on the British National Corpus
(BNC), extracting all noun-noun compounds from a
parsed version of the corpus.
In order to speed up convergence of the sampler,
we use simulated annealing over the first 20 iter-
ations (Kirkpatrick et al, 1983), helping the ran-
domly initialised model reach a mode faster. We re-
port results using marginal distributions after a fur-
ther 130 iterations, excluding the counts of the an-
nealing stage.
6.1.2 Evaluation
We evaluate our two models on the REDDY data
set by comparing its scores for lexicality (Lex(c))
with the annotated gold standard. The aim of this
evaluation is to determine how accurately the mod-
els can capture gradual distinctions in lexicality. The
ROC analysis on the TRATZ data set furthermore in-
forms us how precise the models are at distinguish-
ing lexical from compositional compounds.
Results of the REDDY evaluation are in Table 2.
We use Spearman?s ? to measure the monotonic cor-
relation of our data to the gold standard. Pearson?s r
additionally captures the linear relationship between
the data, taking into account the relative differences
in Lex(c) scores among noun compounds.
136
Figure 3: ROC analysis of models MULT-CMPD and
BIN-CMPD versus the best COSLEX baseline (ADD) on
the TRATZ data set
While both models, BIN-CMPD and MULT-
CMPD, clearly learn a correlation with lexical-
ity rankings, they underperform the strong, semi-
supervised COSLEX baselines described earlier in
this paper. The second evaluation, on the binary
TRATZ data set shows a different picture (see Fig-
ure 3). The best COSLEX baseline (ADD with
w = 0.2) fails to outperform random choice on this
task. Both generative models clearly beat COSLEX
on this task, with MULT-CMPD in particular per-
forming very well for low sensitivity.
There is no clear distinction in performance be-
tween the two generative approaches. Further anal-
ysis might help us to separate the two more clearly,
and we will continue using both models throughout
this paper.
It is important to note the different performance of
the generative models vs. the cosine similarity ap-
proach on two tasks. The REDDY data set has a
nearly linear distribution of compositionality scores,
while the TRATZ data set is overwhelmingly com-
positional, which more closely represents the real
world distribution of compounds. The poor perfor-
mance of the cosine similarity approach (COSLEX)
on the TRATZ evaluation suggests the limitations
of this approach when applied to more realistic data
such as this data set. An additional explanation for
the semi-supervised baseline?s poorer result is that
the effect of parameter tuning decreases on larger
data.
Investigating the errors made by the models
MULT-CMPD and BIN-CMPD gives rise to a number
of possible explanations for their performance. The
most promising lead is related to data sparsity, with
many of the evaluated noun-noun compounds only
appearing once or twice in the corpus. This makes it
harder for our generative approach to learn sensible
context distributions for these instances.
We will next investigate how to reduce the effects
encountered by sparsity.
6.2 Interpolation
Working on problems related to non-unigram data,
sparsity is a frequently encountered problem. As al-
ready explored in the previous section, this is also
the case for our generative models of lexicality.
It would be possible to use an even larger training
corpus, but there are limitations as to what extent
this is possible. The BNC, containing 100 million
words, is already one of the largest corpora regu-
larly used in Computational Linguistics. However,
adding more data in an unsupervised sense is un-
likely to significantly improve results (Brants et al,
2007).
Alternatively, it would be possible to add spe-
cific training data that included the noun compounds
from the evaluation data sets. This would, how-
ever, compromise the unsupervised nature of our ap-
proach, and it thus not an option either.
In this paper, we will instead focus on extenuat-
ing the effects of data sparsity through other unsu-
pervised means. For this purpose we investigate in-
terpolating on a larger set of noun compounds.
Kim and Baldwin (2007) observed that seman-
tic similarity of verb-particle compounds correlates
with their lexicality. We extend this observation for
noun compounds, hypothesising that the lexicality
of similar words will be similar. We combine this
with the assumption that noun compounds sharing a
constituent are likely to be semantically similar (Ko-
rkontzelos and Manandhar, 2009).
Using this idea, we can approximate the lexical-
ity of a given compound with the lexicality scores of
all compounds sharing either of its constituents. So
far we have calculated the lexicality of a given com-
pound using the formula Lex(c) in Equation 1. The
formula Clex(c) in Equation 2 averages the lexical-
ity scores of a compound with those of its related
137
Function and Model r ?
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551
Lex(c)
MULT-CMPD .141 .435
BIN-CMPD .168 .410
Clex(c)
MULT-CMPD .357 .596
BIN-CMPD .400 .592
Ilex(c)
MULT-CMPD .422 .621
BIN-CMPD .538 .623
Table 3: Results on the REDDY data set, reporting
Pearson?s r and Spearman?s ? correlations, comparing
Ilex(c) and Clec(c) interpolations with Lex(c).
compounds. As p(z=1|?a, h?) directly influences
both p(z=1|?a, ??) and p(z=1|??, h?), we can also
consider dropping it from the approximation such as
in Equation 3. This approach trades some specificity
in favour of reducing sparsity, as we observe more
instances of such related compounds than of a par-
ticular noun compound itself only.
Lex(c) ? Clex(c) (2)
Clex(c) =
p(z=1|?a, ??) + p(z=1|??, h?) + p(z=1|?a, h?)
3
,
where c = ?a, h?
Lex(c) ? Ilex(c) (3)
Ilex(c) =
p(z=1|?a, ??) + p(z=1|??, h?)
2
,
where c = ?a, h?
Both formulations enable us to better deal with
sparse data as decisions are made based on a wider
range of observations. At the same time, we avoid a
loss of specificity as the models and scores are still
highly dependent on the individual noun compound.
We avoid introducing additional degrees of free-
dom by using uniform weights only. However, it
would be simple to turn this approach into a semi-
supervised model by tuning the weights for the dif-
ferent probabilities involved in calculating Clex(c)
and Lex(c). That approach would be comparable to
the operators used on our COSLEX baselines.
Results on the REDDY data set using Clex(c)
and Ilex(c) are in Table 3. Figure 4 shows the im-
pact of these approximations on the Tratz data for
the BIN-CMPD model. These interpolations suggest
strong improvements in performance. It should es-
pecially be noted that Ilex(c) consistently outper-
forms Clex(c), which indicates the strength of the
Figure 4: ROC analysis of model BIN-CMPD on the
TRATZ data set, comparing Ilex(c) and Clec(c) inter-
polations with Lex(c).
related-compound probabilities over the individual
compound probabilities.
These results confirm our suspicion that sparsity
was a major factor affecting our models? perfor-
mance. Furthermore, they strengthen our hypothe-
sis about the relatedness of semantic similarity and
lexicality and demonstrate a sensible approach for
exploiting this relationship.
7 Analysis
We use this section for qualitative evaluation, com-
plementing the quantitative evaluation in the previ-
ous sections. The purpose of the qualitative evalu-
ation is to better understand exactly what it is our
models are learning.
Table 5 lists the compounds that model BIN-
CMPD considers the most lexical and the most com-
positional. The list of compounds with the high lex-
icality scores is dominated by proper nouns such as
countries, companies and persons. This is in line
with expectation as compounds of proper nouns are
fully lexical. Removing proper nouns (also in Table
5), we get a slightly more ambiguous list. For exam-
ple, ?study design? is not considered a lexical com-
pound, but rather a highly institutionalized, com-
positional MWE (Sag et al, 2002). Using Lex(c)
?study design? is ranked as such, so this appears to
be a case where interpolation has a negative impact.
In this paper we argued for a finer grained analysis
of compositionality, taking into account the differ-
138
Context of ?flea market? generated by
flea market flea market
canal, wall, incline,
campsite
stall, Paris, sale,
Saturday, week,
Sunday, quarter,
damage, change
barter, souvenir,
launderette,
Lamine, Canet,
Kouyate, Plage
Context of ?night owl? generated by
night owl night owl
court, fee, guest,
early, day, Baden,
membership, life,
game
waive, player,
Halikarnas, bar,
bird, unbooked,
Vienna
adventurous
Context of ?memory lane? generated by
memory lane memory lane
take, story, about,
tell, real, glimpse,
Britain, reminis-
cence
village, protection,
drive, catwalk,
plant
war, justify, bill,
Campbell, rude-
boys
Context of ?melting pot? generated by
melting pot melting pot
forest, racial,
caribbean, plan,
programme, real-
ity, arrangement
in, into, put, polit-
ical, community,
prepare
ethnic, greatest,
drawing, liaise,
pan-european,
myth
Table 4: Overview over context words generated by model BIN-CMPD. We list a selection of words predominately
generated by each of the mixture components of the given noun-noun compound.
Most Compositional
labour union, tax authority, health council,
market counterparty, employment policy
Most Lexical
study design, family motto, wood shaving,
avoidance behaviour, smash hit
Most Lexical (including Proper Nouns)
Vo Quy, Bonito Oliva, Mamur Zapt, Evander
Holyfield, Saudi Arabia
Table 5: Top lexical and compositional nouns for the
BIN-CMPD model using Ilex(c)
ent impact of both constituents. We tried to achieve
this by modelling a compound?s context as gener-
ated from its various semantic constituents. Table 4
highlights the impact of this method for a number
of noun compounds, showing which context words
were predominately generated by each constituent.
Due to the nature of the context used, some of
the links are semantically not obvious (e.g. the rela-
tionship between owls and Vienna). In some cases
the semantic contribution of the parts is more clearly
separated, such as the contributions of ?memory? and
?lane? to the semantics of ?memory lane?. In sum-
mary, these examples clearly suggest that our mod-
els learn to associate context with compound ele-
ments and that this association is an informed one.
8 Conclusion
We proposed a novel approach for learning lexicality
scores for noun compounds and empirically demon-
strated the feasiblity of this approach. Using a gen-
erative model we were able to beat a strong, semi-
supervised baseline with an unsupervised model.
We discussed the issue of data sparsity in depth
and proposed several approaches for overcoming
this problem. Focusing on unsupervised approaches,
we demonstrated how interpolation can be used to
tackle sparsity. The two interpolation methods that
we implemented helped us to strongly improve over-
all model performance. Our empirical evaluation of
interpolation metricsClex(c) and Ilex(c) also gives
credence to the hypothesis that lexicality is related to
semantic similarity.
On the theoretical side, we offered further support
to the real-valued treatment of lexicality.
Further work will include using larger training
corpora. While the BNC is a popular corpus in Com-
putational Linguistics, it proved to be too small to
learn sensible representations for a number of com-
pounds encountered in the test data. Using larger
corpora will also allow us to further study and re-
duce the sparsity issues encountered.
To study the relationship between constituent and
compound compositionality in greater depth, we
will also investigate alternative approaches for in-
terpolation. Similarity measures that consider the
semantic relevance of individual context elements
should also be considered as a next step.
Another obvious source of future work is to ap-
ply our approach to general collocations beyond the
special case of noun compounds only.
Acknowledgments
The authors would like to acknowledge the use of
the Oxford Supercomputing Centre (OSC) in carry-
ing out this work.
139
References
Timothy Baldwin. 2006. Compositionality and mul-
tiword expressions: Six of one, half a dozen of the
other? In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting Underlying
Properties, page 1, Sydney, Australia. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language Mod-
els in Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867.
Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 116?
124, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O?. Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages 39?
44, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Eugenie Giesbrecht. 2009. In search of semantic com-
positionality in vector spaces. In Proceedings of the
17th International Conference on Conceptual Struc-
tures: Conceptual Structures: Leveraging Semantic
Technologies, ICCS ?09, pages 173?184, Berlin, Hei-
delberg. Springer-Verlag.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using wordnet simi-
larity. In In Proceedings of the 2nd International Joint
Conference on Natural Language Processing, Jeju Is-
land, South Korea, 1113, pages 945?956.
Su Nam Kim and Timothy Baldwin. 2007. Detect-
ing compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of the
7th Meeting of the Pacific Association for Computa-
tional Linguistics, PACLING ?07, pages 40?48.
Su Nam Kim and Timothy Baldwin. 2008. An unsu-
pervised approach to interpreting noun compounds. In
Natural Language Processing and Knowledge Engi-
neering, 2008. NLP-KE ?08. International Conference
on, pages 1?7.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983.
Optimization by simulated annealing. Science,
220(4598):671?680.
Ioannis Korkontzelos and Suresh Manandhar. 2009. De-
tecting compositionality in multi-word expressions.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, ACLShort ?09, pages 65?68, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow
parsing? In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 636?644, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, ACL ?95, pages 47?54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369?379, Prague, Czech Republic. As-
sociation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Diarmuid O? Se?aghdha. 2007. Annotating and learning
compound noun semantics. In Proceedings of the 45th
Annual Meeting of the ACL: Student Research Work-
shop, ACL ?07, pages 73?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ted Pedersen. 2011. Identifying collocations to mea-
sure compositionality: shared task system description.
In Proceedings of the Workshop on Distributional Se-
mantics and Compositionality, DiSCo ?11, pages 33?
37, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
140
pound nouns. In Proceedings of The 5th Interna-
tional Joint Conference on Natural Language Process-
ing 2011 (IJCNLP 2011), Chiang Mai, Thailand.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In In Proc.
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002, pages 1?15.
Karen Spa?rck Jones. 1985. Compound noun interpre-
tation problems. In Frank Fallside and William A.
Woods, editors, Computer speech processing, pages
363?381. Prentice Hall International (UK) Ltd., Hert-
fordshire, UK, UK.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 678?687, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10, pages 1263?1271, Stroudsburg,
PA, USA. Association for Computational Linguistics.
141
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 47?54,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Part of Speech Inference with Particle Filters
Gregory Dubbin and Phil Blunsom
Department of Computer Science
University of Oxford
Wolfson Building, Parks Road
Oxford, OX1 3QD, United Kingdom
Gregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.uk
Abstract
As linguistic models incorporate more subtle
nuances of language and its structure, stan-
dard inference techniques can fall behind. Of-
ten, such models are tightly coupled such that
they defy clever dynamic programming tricks.
However, Sequential Monte Carlo (SMC) ap-
proaches, i.e. particle filters, are well suited
to approximating such models, resolving their
multi-modal nature at the cost of generating
additional samples. We implement two par-
ticle filters, which jointly sample either sen-
tences or word types, and incorporate them
into a Gibbs sampler for part-of-speech (PoS)
inference. We analyze the behavior of the par-
ticle filters, and compare them to a block sen-
tence sampler, a local token sampler, and a
heuristic sampler, which constrains inference
to a single PoS per word type. Our findings
show that particle filters can closely approx-
imate a difficult or even intractable sampler
quickly. However, we found that high poste-
rior likelihood do not necessarily correspond
to better Many-to-One accuracy. The results
suggest that the approach has potential and
more advanced particle filters are likely to lead
to stronger performance.
1 Introduction
Modern research is steadily revealing more of the
subtle structure of natural language to create in-
creasingly intricate models. Many modern problems
in computational linguistics require or benefit from
modeling the long range correlations between latent
variables, e.g. part of speech (PoS) induction (Liang
et al, 2010), dependency parsing (Smith and Eis-
ner, 2008), and coreference resolution (Denis and
Baldridge, 2007). These correlations make infer-
ence difficult because they reflect the complicated
effect variables have on each other in such tightly
coupled models.
Sequential Monte Carlo (SMC) methods, like par-
ticle filters, are particularly well suited to estimating
tightly coupled distributions (Andrieu et al, 2010).
Particle filters sample sequences of latent variable
assignments by concurrently generating several rep-
resentative sequences consistent with a model?s con-
ditional dependencies. The sequential nature of the
sampling simplifies inference by ignoring ambigu-
ous correlations with unsampled variables at the
cost of sampling the sequence multiple times. The
few applications of particle filters in computational
linguistics generally focus on the online nature of
SMC (Canini et al, 2009; Borschinger and John-
son, 2011). However, batch applications still benefit
from the power of SMC to generate samples from
tightly coupled distributions that would otherwise
need to be approximated. Furthermore, the time cost
of the additional samples generated by SMC can be
mitigated by generating them in parallel.
This report presents an initial approach to the inte-
gration of SMC and block sampling, sometimes ref-
fered to as Particle Gibbs (PG) sampling (Andrieu
et al, 2010). Unsupervised PoS induction serves
as a motivating example for future extensions to
other problems. Section 3 reviews the PYP-HMM
model used for PoS inference. Section 4 explains the
Sequential Importance Sampling (SIS) algorithm, a
basic SMC method that generates samples for the
47
block sampler. This approach yields two implemen-
tations: a simple sentence-based block sampler (4.1)
and a more complicated type-based sampler (4.2).
Finally, section 5 evaluates both implementations on
a variety of unsupervised PoS inference tasks, ana-
lyzing the behavior of the SMC inference and com-
paring them to state-of-the-art approaches.
2 Background
SMC was introduced in 1993 as a Bayesian esti-
mator for signal processing problems with strong
non-linear conditional dependencies (Gordon et al,
1993). Since then, SMC methods have been adopted
by many fields, including statistics, biology, eco-
nomics, etc. (Jasra et al, 2008; Beaumont, 2003;
Fernandez-Villaverde and Rubio-Ramirez, 2007).
The SMC approach is the probabilisitic analogue of
the beam search heuristic, where the beam width can
be compared to the number of particles and pruning
is analogous to resampling.
The basic SMC approach serves as the basis for
several variants. Many SMC implementations re-
sample the population of particles to create a new
population that minimizes the effect of increasing
sample variance with increasing sequence length
(Kitagawa, 1996). Particle smoothing variants of
SMC reduce the relative variance of marginals early
in the sequence, as well improving the diversity
of the final sample (Fearnhead et al, 2008). Par-
ticle Markov chain Monte Carlo (PMCMC) for-
mally augments classic Markov chain Monte Carlo
(MCMC) approaches, like Gibbs sampling, with
samples generated by particle filters (Andrieu et al,
2010).
3 The PYP-HMM
The PYP-HMM model of PoS generation demon-
strates the tightly coupled correlations that com-
plicate many standard inference methods (Blunsom
and Cohn, 2011). The model applies a hierarchical
Pitman-Yor process (PYP) prior to a trigram hidden
Markov model (HMM) to jointly model the distri-
bution of a sequence of latent word classes, t, and
word tokens, w. This model performs well on cor-
pora in multiple languages, but the lack of a closed
form solution for the sample probabilities makes it a
strong canditate for PG sampling. The joint proba-
bility defined by a trigram HMM is
P?(t,w) =
N+1?
n=1
P?(tl|tn?1, tn?2)P?(wn|tn)
where N = |t| = |w| and the special tag $ is added
to the boundaries on the sentence. The model de-
fines transition and emission distributions,
tn|tn?1, tn?2, T ? Ttn?1,tn?2
wn|tn, E ? Etn
The PYP-HMM smoothes these distributions by ap-
plying hierarchical PYP priors to them. The hierar-
chical PYP describes a back-off path of simpler PYP
priors,
Tij |a
T , bT , Bi ? PYP(a
T , bT , Bi)
Bi|a
B, bB, U ? PYP(aB, bb, U)
U |aU , bU ? PYP(aU , bU ,Uniform).
Ei|a
E , bE , C ? PYP(aE , bE , Ci),
where Tij , Bi, and U are trigram, bigram, and un-
igram transition distributions respectively and Ci is
either a uniform distribution (PYP-HMM) or a bi-
gram character language model emission distribu-
tion (PYP-HMM+LM, intended to model basic mor-
phology).
Draws from the posterior of the hierarchical
PYP can be calculated with a variant of the Chi-
nese Restaraunt Process (CRP) called the Chinese
Restaurant Franchise (CRF) (Teh, 2006; Goldwater
et al, 2006). In the CRP analogy, each latent vari-
able in a sequence is represented by a customer en-
tering a restaurant and sitting at one of an infinite
number of tables. A customer chooses to sit at a ta-
ble in a restaurant according to the probability
P (zn = k|z1:n?1) =
{
c?k ?a
n?1+b 1 ? k ? K
?
K?a+b
n?1+b k = K
? + 1
(1)
where zn is the index of the table chosen by the nth
customer to the restaurant, z1:n?1 is the seating ar-
rangement of the previous n? 1 customers to enter,
c?k is the count of the customers at table k, and K
?
is the total number of tables chosen by the previ-
ous n? 1 customers. All customers at a table share
the same dish, representing the value assigned to the
48
latent variables. When customers sit at an empty ta-
ble, a new dish is assigned to that table according to
the base distribution of the PYP. To expand the CRP
analogy to the CRF for hierarchical PYPs, when a
customer sits at a new table, a new customer enters
the restaurant representing the PYP of the base dis-
tribution.
4 Sequential Monte Carlo
While MCMC approximates a distribution as the av-
erage of a sequence of samples taken from the poste-
rior of the distribution, SMC approximates a distri-
bution as the importance weighted sum of several se-
quentially generated samples, called particles. This
article describes two SMC samplers that jointly sam-
ple multiple tag assignments: a sentence based block
sampler (sent) and a word type based block sam-
pler (type). The basics of particle filtering are out-
lined below, while the implementation specifics of
the sent and type particle filters are described in
secions 4.1 and 4.2, respectively.
SMC is essentially the probabilistic analogue of
the beam search heuristic. SMC stores P sequences,
analogous to beam width, and extends each incre-
mentally according to a proposal distribution qn,
similar to the heuristic cost function in beam search.
Many particle filtering implementations also include
a resampling step which acts like pruning by reduc-
ing the number of unlikely sequences.
We implemented Sequential Importance
Sampling (SIS), detailed by Doucet and Jo-
hansen (2009), to approximate joint samples
from the sentence and word type distributions.
This approach approximates a target distribution,
pin(x1:n) =
?n(x1:n)
Zn
, of the sequence, x1:n, of n
random variables, that is ?n(x1:n) calculates the
unnormalized density of x1:n.
SIS initilizes each particle p ? [1, P ] by sampling
from the initial proposal distribution q1(x
p
1), where
xpn is the value assigned to the n-th latent variable for
particle p. The algorithm then sequentially extends
each particle according to the conditional proposal
distribution qn(x
p
n|x
p
1:n), where x
p
1:n is the sequence
of values assigned to the first n latent variables in
particle p. After extending a particle p, SIS updates
the importance weight ?pn = ?
p
n?1 ? ?n(x
p
1:n). The
weight update, defined as
?n(x1:n) =
?n(x1:n)
?n?1(x1:n?1)qn(xn|x1:n?1)
, (2)
accounts for the discrepancy between the proposal
distribution, qn, and the target distribution, pin,
without normalizing over x1:n, which becomes in-
tractable for longer sequences even in discrete
domains. The normalizing constant of the tar-
get distribution is approximately Zn ?
?P
p=1 ?
p
n
and the unnormalized density is ?n(x1:n) ??P
p=1 ?
p
nifx
p
1:n = x1:n. The particles can also be
used to generate an unbiased sample from pin by
choosing a particle p proportional to its weight ?pn.
Andrieu et al (2010) shows that to ensure the
samples generated by SMC for a Gibbs sampler has
the target distribution as the invariant density, the
particle filter must be modified to perform a con-
ditional SMC update. This means that the particle
filter guarantees that one of the final particles is as-
signed the same values as the previous Gibbs iter-
ation. Our implementation of the conditional SMC
update reserves one special particle, 0, for which the
proposal distribution always chooses the previous it-
eration?s value at that site.
4.1 Sentence Sampling
The sent particle filter samples blocks of tag as-
signments tS1:n for a sentence, S, composed of to-
kens, wS1:n. Sampling an entire sentence minimizes
the risk of assigning a tag with a high probabil-
ity given its local context but minimal probability
given the entire sentence. Sentences can be sampled
by ignoring table counts while sampling a proposal
sentence, incorporating them after the fact with a
Metropolis-Hastings acceptance test (Gao and John-
son, 2008). The Metropolis-Hastings step simplifies
the sentence block particle filter further by not re-
quiring the conditional SMC update.
While there is already a tractable dynamic pro-
gramming approach to sampling an entire sentence
based on the Forward-Backward algorithm, parti-
cle filtering the sentences PYP-HMM model should
prove beneficial. For the trigram HMM defined
by the model, the forward-backward sampling ap-
proach has time complexity in O(NT 3) for a sen-
tence of length N with T possible tag assignments
at each site. Particle filters with P particles can ap-
proximate these samples in O(NTP ) time, which
49
becomes much faster as the number of tags, T , in-
creases.
Sampling of sentence S begins by removing all
of the transitions and emitions in S from the table
counts, z, resulting in the table counts z?S of tag as-
signments t?S the values assigned to the variables
outside of S. For each site index n ? [1, N ] in the
sentence, the particle filter chooses the new tag as-
signment, tS,pn , for each particle p ? [1, P ] from the
sentence proposal distribution,
qSn (t
S,p
n |t
S,p
1:n?1) ? P (t
S,p
n |t
S,p
n?2, t
S,p
n?1, t
?S , z?S)
? P (wS,pn |t
S,p
n , t
?S , z?S ,w?S).
After each new tag is assigned, the particle?s weight
is updated according to equation (2). The simplic-
ity of the proposal density hints at the advantage of
particle filtering over forward-backward sampling:
it tracks only P histories and their weights rather
than tracking the probability of over all possible his-
tories. Once each particle has assigned a value to
each site in the sentence, one tag sequence is chosen
proportional to its particle weight, ?S,pN .
4.2 Type Sampling
The type sampling case for the PYP-HMM is more
complicated than the sent sampler. The long-range
couplings defined by the hierarchical PYP priors
strongly influence the joint distribution of tags as-
signed to tokens of the same word type (Liang et
al., 2010). Therefore, the affects of the seating de-
cisions of new customers cannot be postponed dur-
ing filtering as in sentence sampling. To account
for this, the type particle filter samples sequences
of seating arrangements and tag assignments jointly,
xW1:n = (t
W
1:n, z
W
1:n), for the word-type, W . The fi-
nal table counts are resampled once a tag assignment
has been chosen from the particles.
Tracking the seating arrangement history for each
particle adds an additional complication to the type
particle filter. The exchangeability of seating deci-
sions means that only counts of customers are nec-
essary to represent the history. Each particle repre-
sents both a tag sequence, tW,p1:n , and the count deltas,
zW,p1:n . The count deltas of each particle are stored in a
hash table that maps a dish in one of the CRF restau-
rants to the number of tables serving that dish and
the total number of customers seated at those tables.
The count delta hash table ensures that it has suffi-
cient data to calculate the correct probabilities (per
equation (1)) by storing any counts that are different
from the base counts, z?W , and defering to the base
counts for any counts it does not have stored.
At each token occurence n, the next tag assign-
ment, tW,pn for each particle p ? [1, P ] is chosen first
according to the word type proposal distribution
qWn (t
W,p
n |t
W,p
1:n?1, z
W,p
1:n?1) ?
P (tW,pn |c
?2
n , c
?1
n , t
?W,p
1:n?1, z
?W,p
1:n?1)
? P (c+1n |c
?1
n , t
W,p
n , t
?W,p
1:n?1, z
?W,p
1:n?1)
? P (c+2n |t
W,p
n , c
+1
n , t
?W,p
1:n?1, z
?W,p
1:n?1)
? P (wWn |t
W,p
n , t
?W,p
1:n?1, z
?W,p
1:n?1,w
?W,p
1:n?1).
In this case, c?kn represents a tag in the context
of site tWn offset by k, while t
?W,p
1:n?1, z
W,p
1:n?1, and
w?W,p1:n?1 represent the tag assignments, table counts,
and word token values chosen by particle p as well
as the values at all of the sites where a word token
of type W does not appear. This proposal distribu-
tion ignores changes to the seating arrangement be-
tween the three transitions involving the site n. The
specific seating arrangement of a particle is chosen
after the tag choice, at which point the weights are
updated by the result of equation (2). As with the
sent sampler, once all of the particles have been
sampled, one of them is sampled with probability
proportional to its weight. This final sample is a
sample from the true target probability.
As mentioned earlier, the sequence of particle ap-
proximations do not have the target distribution as
invariant unless they use the conditional SMC up-
date. Therefore, a the special 0 particle is automat-
ically assigned the value from the prior iteration of
the Gibbs sampler at each site n, though the proposal
probability qWn (t
W,0
n |t
W,p
1:n?1, z
W,p
1:n?1) still has to be
calculated to update the weight ?W,pn properly. This
ensures that the type sampler has a non-zero prob-
ability of reverting to the prior iteration?s sequence.
5 Experiments and Results
We take two approaches to evaluating the SMC
based samplers. The first approach is an analysis of
the samplers as inference algorithms. The samplers
should tend to maximize the posterior likelihood of
the model over iterations, eventually converging to
50
the mode. Section 5.1 analyzes the particle filter
based samplers with various numbers of particles in
an effort to understand how they behave.
Then, section 5.2 evaluates each of the proposed
approaches on PoS inference tasks from several lan-
guages. These results allow a practical comparison
with other PoS inference approaches.
5.1 SMC Analysis
Before comparing the performance of the SMC
block samplers to other inference methods, we wish
to learn more about the approaches themselves. It
is not clear how well the benefits of block sampling
transfer to SMC based approaches. Both the sent
and type samplers are novel approaches to com-
putational linguistics, and many of their properties
are unclear. For example, the samples generated
from the particle filter should have a higher vari-
ance than the target distribution. If the variance is
too high, the sampler will be slower to converge.
While additional particles lower the relative vari-
ance, they also increase the run time linearly. It is
possible that there is a threshold of particles nec-
essary to ensure that some are high likelihood se-
quences, beyond which inference gains are minimal
the additional computational expense is wasted. All
of the experiments in this section were run on the
Arabic corpus from the CoNLL-X shared language
task, which is small enough to quickly experiment
with these issues (Buchholz and Marsi, 2006).
The sentence based sampler, sent, samples from
a distribution that can be exactly computed, facilitat-
ing comparisons between the exact sampler and the
SMC approach. Figure 5.1 compares the posterior
log-likelihoods of the sent sampler and the exact
sentence sampler over 200 iterations. As expected,
the likelihoods of the particle filters approach that of
the exact sentence sampler as the number of particles
increases from 25 to 100, which completely overlaps
the performance of the exact sampler by the 50th it-
eration. This is impressive, because even with 99
additional sequences sampled (one for each particle)
each iteration the SMC approach is still faster than
the exact sampler. Furthermore, the Arabic tagset
has only 20 distinct tags, while other data sets, e.g.
the WSJ and Bulgarian, use tagsets more than twice
as large. The particle filter, which is linear in the
number of tags, should take twice as long per token
0 50 100 150 200Iteration
6.6
6.4
6.2
6.0
5.8
5.6
5.4
5.2
5.0
4.8
Log
-Lik
elih
ood
1e5
Sent: ExactSent: 25 ParticlesSent: 50 ParticlesSent: 100 Particles
Log-Likelihood of Sentence Samplers over 200 Iterations
Figure 1: Posterior Log-Likelihood of PYP-HMM infer-
ence with exact as well as SMC sentence sampler with
various numbers of particles. Error bars represent on
standard deviation over three runs.
sampled on those data, relative to the arabic data.
On the other hand, the forward-backward per token
sample time, which is cubic in tagset size, should
increase at least eightfold. So the time savings im-
prove dramatically as the size of the tagset increases.
Figure 2 compares the table configuration log-
likelihood of the 1HMM approximation imple-
mented by Blunsom and Cohn (2011) with the type
particle filter based sampler as well as the local,
token-based sampler and the exact block sentence
sampler. Unlike the sentence based block sampler,
type sampler cannot be exactly calculated, even
with the 1HMM approach of constraining inference
to only consider sequences that assign the same tag
to every token of the same word type. The 1HMM
sampler approximates these probabilities using ex-
pected table counts. Theoretically, the type sam-
pler should be a better approximation, being guar-
anteed to approach the true distribution as the num-
ber of particles increases. However, the type sam-
pler does not constrain inference as the 1HMM does,
slowing convergence by wasting particles on less
likely tag sequences. As expected, the type sam-
pler converges with the 1HMM sampler with suffi-
ciently many particles in a few iterations. The exact
block sentence sampler surpases approaches by iter-
ation 75 and does not seem to have converged by the
end of 200 iterations.
The local sampler samples a single site at a time
with a standard, token-based Gibbs sampler. The
51
0 50 100 150 200Iteration
6.6
6.4
6.2
6.0
5.8
5.6
5.4
5.2
5.0
4.8
Log
-Lik
elih
ood
1e5
1HMMLocalSent: ExactType: 100 Particles
Log-Likelihood Comparison of Samplers over 200 Iterations
Figure 2: Posterior Log-Likelihood of PYP-HMM infer-
ence with particle filters, 1HMM approximation, and lo-
cal samplers. Error bars represent one standard deviation
over three runs.
local sampler performs surprisingly well, averaging
a slightly higher likelihood than both the 1HMM
and the type samplers. This may be an indication
that the PYP-HMM model is not too tightly coupled
for the local sampler to eventually migrate toward
more likely modes. Note that both the type and
1HMM samplers initially take much larger steps,
before eventually hitting a plateau. This suggests
that some sort of mixed sampler may outperform
its component samplers by only occasionally taking
large steps.
5.2 Unsupervised Part-of-Speech Tagging
The samplers evaluated in section 5.1 induce syn-
tactic categories analogous to PoS tags. However,
to induce PoS tags, each syntactic category must be
assigned to a PoS tag in the tagset. Each site in a
corpus is assigned the most commonly visited syn-
tactic category at that site over all iterations. The
many-to-one (M-1) assignment for a category is the
most common gold standard PoS tag of the tokens
assigned to that category. While this assignment
theoretically allows a perfect accuracy if each token
is assigned to its own category, these experiments
limit the number of induced categories to the size of
the tagset. Table 1 compares the M-1 accuracy of
the sent and type particle filter samplers, from
sections 4.1 and 4.2, with 100 particles each. The
particle filter based samplers rarely score a higher
accuracy than even the local sampler, which com-
pletes 500 iterations before the particle filters com-
plete 200.
While figure 2 shows that the sentence based
block sampler eventually surpasses the 1HMM sam-
pler in likelihood, the accuracies of the 1HMM and
1HMM-LM approximations remain well above the
other approaches. The 1HMM sampler and the 100
particle type sampler have approximately the same
likelihood, yet the M-1 accuracy of the 1HMM sam-
pler is much higher. This suggests that there are
high-likelihood assignments that produce lower ac-
curacy results, presumably related to the fact that the
type sampler is not restricted to assignments with
exactly one tag for each word type. If the model
assigns equal likelihood to these assignments, infer-
ence will not be able to distinguish between them.
Perhaps a model that assigned higher likelihoods to
tag sequences with fewer tags per word type would
have a stronger correlation between likelihood and
accuracy.
6 Future Work
While the results leave much room for improvement,
the approach presented here is the most basic of par-
ticle methods. There has been considerable research
in improvements to particle methods since their in-
troduction in 1993 (Gordon et al, 1993). Two com-
mon approaches to improving particle filters are re-
sampling and particle smoothing (Doucet and Jo-
hansen, 2009; Godsill and Clapp, 2001; Pitt, 2002).
Resampling ensures that particles aren?t wasted on
unlikely sequences. Particle smoothing reduces the
variability of the marginal distributions by combin-
ing the final particles.
7 Conclusion
This paper presented a preliminary approach to in-
corporating particle methods into computational lin-
guistic inference applications. Such approaches
show great potential for inference even in highly de-
pendent distributions, but at a serious computational
cost. However, the type particle filter itself can be
largely run in parallel, only bottlenecking when the
particle weights need to be normalized. Further ex-
pansion of the basic ideas presented will enable scal-
able inference in otherwise intractable models.
52
Language Sent-100 Type-100 Local 1HMM 1HMM-LM Tokens Tag types
WSJ 69.8% 70.1% 70.2% 75.6% 77.5% 1,173,766 45
Arabic 53.5% 57.6% 56.2% 61.9% 62.0% 54,379 20
Bulgarian 64.8% 67.8% 67.6% 71.4% 76.2% 190,217 54
Czech 59.8% 61.6% 64.5% 65.4% 67.9% 1,249,408 12c
Danish 65.0% 70.3% 69.1% 70.6% 74.6% 94,386 25
Dutch 61.6% 71.6% 64.1% 73.2% 72.9% 195,069 13c
Hungarian 61.8% 61.8% 64.8% 69.6% 73.2% 131,799 43
Portuguese 59.4% 71.1% 68.1% 72.0% 77.1% 206,678 22
Table 1: Many-to-1 accuracies on CoNLL and Penn-Treebank Wall Street Journal corpora for sentence- (Sent) and
type- (Type) based filtering. The table lists the average M-1 accuracy measured according to the maximum marginal
tag assignments over 3 seperate runs after 200 iterations for the sent, type, 1HMM and 1HMM-LM samplers,
and 500 iterations for the HMM local sampler. The 1HMM-LM model has been shown to achieve state-of-the-art
unsupervised M-1 accuracies on these datasets. and thus represents the limit of unsupervised M-1 accuracy.
References
Christophe Andrieu, Arnaud Doucet, and Roman Holen-
stein. 2010. Particle markov chain monte carlo meth-
ods. Journal Of The Royal Statistical Society Series B,
72(3):269?342.
Mark A. Beaumont. 2003. Estimation of Population
Growth or Decline in Genetically Monitored Popula-
tions. Genetics, 164(3):1139?1160, July.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Benjamin Borschinger and Mark Johnson. 2011. A parti-
cle filter algorithm for bayesian wordsegmentation. In
Proceedings of the Australasian Language Technology
Association Workshop 2011, pages 10?18, Canberra,
Australia, December.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, CoNLL-X ?06, pages 149?
164, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Kevin R. Canini, Lei Shi, and Thomas L. Griffiths. 2009.
Online inference of topics with latent Dirichlet aloca-
tion. In David van Dyk and Max Welling, editors, Pro-
ceeings of the 12th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS*09), pages
65?72.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Arnaud Doucet and Adam M. Johansen, 2009. A Tuto-
rial on Particle Filtering and Smoothing: Fifteen Years
Later. Oxford University Press.
Paul Fearnhead, David Wyncoll, and Jonathan Tawn.
2008. A sequential smoothing algorithm with linear
computational cost. Technical report, Department of
Mathematics and Statistics, Lancaster University.
Jesus Fernandez-Villaverde and Juan F. Rubio-Ramirez.
2007. Estimating macroeconomic models: A like-
lihood approach. Review of Economic Studies,
74(4):1059?1087, October.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 344?352, Morristown, NJ,
USA. Association for Computational Linguistics.
Simon Godsill and Tim Clapp. 2001. Improvement
strategies for monte carlo particle filters. In A. Doucet,
J. F. G. de Freitas, and N.J. Gordon, editors, SE-
QUENTIAL MONTE CARLO METHODS IN PRAC-
TICE, pages 139?158. Springer-Verlag.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466.
MIT Press, Cambridge, MA.
N. J. Gordon, D. J. Salmond, and A. F. M. Smith. 1993.
Novel approach to nonlinear/non-Gaussian Bayesian
state estimation. Radar and Signal Processing, IEE
Proceedings F, 140(2):107?113, April.
A. Jasra, A. Doucet, D. Stephens, and C. Holmes. 2008.
Interacting sequential Monte Carlo samplers for trans-
dimensional simulation. Computational Statistics &
Data Analysis, 52(4):1765?1791, January.
53
G Kitagawa. 1996. Monte carlo filter and smoother for
non-gaussian nonlinear state space models. Journal Of
Computational And Graphical Statistics, 5(1):1?25.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based
MCMC. In North American Association for Compu-
tational Linguistics (NAACL).
Michael K Pitt. 2002. Smooth particle filters for likeli-
hood evaluation and maximisation. The Warwick Eco-
nomics Research Paper Series (TWERPS) 651, Uni-
versity of Warwick, Department of Economics, July.
David A. Smith and Jason Eisner. 2008. Depen-
dency parsing by belief propagation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?08, pages 145?156,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Yee Whye Teh. 2006. A hierarchical bayesian language
model based on pitman-yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL-44, pages
985?992, Morristown, NJ, USA. Association for Com-
putational Linguistics.
54
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64?80,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The PASCAL Challenge on Grammar Induction
Douwe Gelling and Trevor Cohn
Department of Computer Science
University of Sheffield, UK
{d.gelling,t.cohn}@sheffield.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford, UK
Phil.Blunsom@cs.ox.ac.uk
Joa?o Grac?a
L2F Spoken Language Systems Laboratory
INESC ID Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Abstract
This paper presents the results of the PASCAL
Challenge on Grammar Induction, a compe-
tition in which competitors sought to predict
part-of-speech and dependency syntax from
text. Although many previous competitions
have featured dependency grammars or parts-
of-speech, these were invariably framed as
supervised learning and/or domain adaption.
This is the first challenge to evaluate unsuper-
vised induction systems, a sub-field of syntax
which is rapidly becoming very popular. Our
challenge made use of a 10 different treebanks
annotated in a range of different linguistic for-
malisms and covering 9 languages. We pro-
vide an overview of the approaches taken by
the participants, and evaluate their results on
each dataset using a range of different evalua-
tion metrics.
1 Introduction
Inducing grammatical structure from text has long
been fundamental problem in Computational Lin-
guistics and Natural Language Processing. In re-
cent years interest has grown, spurred by advances
in unsupervised statistical modelling and machine
learning. The task has relevance to cognitive scien-
tists and linguists attempting to gauge the learnabil-
ity of natural language by human children, and also
natural language processing researchers who seek
syntactic representations for languages with few lin-
guistic resources.
Grammar learning has been popular in previous
challenges. For example the CoNLL shared tasks in
2006 and 2007 (Buchholz and Marsi, 2006; Nivre
et al, 2007) involved supervised learning of de-
pendency parsers across a wide range of different
languages. Our challenge has many similarities to
these, in that we focus on dependency grammars,
however we seek to evaluate unsupervised algo-
rithms only using syntactically annotated data for
evaluation and not for training. Additionally we also
consider the related task of part-of-speech (POS) in-
duction, and the next logical challenge: the joint
task of POS and dependency induction. Other re-
lated challenges can be found in the formal gram-
mar community (e.g., the Omphalos1 competition)
in which competitors seek to learn synthetic lan-
guages. In contrast we seek to model natural lan-
guage text, which entails many different challenges.
Research into unsupervised grammar and POS in-
duction holds considerable promise, although cur-
rent approaches are still a long way from solving
the general problem. For example, the majority of
recent research into dependency grammar induction
has adopted the evaluation setting of Klein and Man-
ning (2004) who learn grammars on strings of POS
tags, rather than on words themselves. One aim of
this challenge is to popularise the more difficult and
ambitious task of inducing grammars directly from
text, which can be viewed as integrating the POS and
grammar induction tasks. A second aim is to foster
grammar and POS induction research across a wider
variety of languages, and improving the standard of
evaluation.
We have collated data from existing treebanks in
a variety of different languages, domains and lin-
guistic formalisms. This gives a diverse range of
1See http://www.irisa.fr/Omphalos
64
data upon which to test induction algorithms, yield-
ing a deeper insight into their strengths and short-
comings. One key problem in grammar induction
research is how to evaluate the models? predictions
given that often many different analyses are linguis-
tically plausible, e.g., the choice of whether deter-
miners or nouns should head noun phrases, or how to
represent coordination. Simply comparing against a
single gold standard often results in poor reported
performance because the model has discovered a
different analysis to that used when annotating the
treebank. For this reason it has been popular to use
lenient measures for comparing predicted trees to
the treebank gold standard trees, such as undirected
accuracy and the neutral edge distance (Schwartz et
al., 2011). As well as evaluating using these popular
metrics, we also propose a new method of evaluation
which is also lenient in that it rewards different types
of linguistically plausible output, but requires con-
sistency in the output, something the previous meth-
ods cannot do.
The paper is organised as follows. Section 2 de-
scribes the tasks and our data format and section 3
outlines the different treebanks used for the chal-
lenge. The baselines, our own benchmark systems
and the competitors entries are described in section
5. In section 6 we present and analyse the results
for the three different tracks. Finally we conclude in
section 7.
2 Task Definition
The three tracks of the WILS challenge are de-
scribed below. First we describe the data format for
the submissions common to the three tracks (POS
induction, Dependency induction, and jointly induc-
ing both), and then the three tracks are described
along with the respective evaluation metrics.
2.1 Data format
All datasets were presented in a file format similar to
that used in the CoNLL tasks, but with slight mod-
ifications. In particular the last two columns are re-
moved, as no projective head or projective depen-
dency relations were used, and an extra POS column
was inserted at column 6 to accommodate the Uni-
versal POS tagset (Petrov et al, 2011). Each line in a
file then either consists of 9 columns, separated by a
tab character, or is an empty line. Empty lines sepa-
rate sentences, and all other lines give the annotation
for a single token in the sentence as follows:
1. ID: Token counter, gives the index of current
word in the sentence. Indexing starts at 1.
2. FORM: Surface form of the token in the sen-
tence.
3. LEMMA: Stemmed form of the word form if
available.
4. CPOSTAG: Coarse-grained POS tag.
5. POSTAG: Fine-grained POS tag, or CPOSTAG
again if not available.
6. UPOSTAG: Universal POS tag, based on the
POSTAG and CPOSTAG.
7. FEATS: List of syntactic / morphological fea-
tures, separated by a vertical pipe (|).
8. HEAD: Syntactic head of the token, with 0 in-
dicating the root node.
9. DEPREL: The general type of the dependency
relation, e.g., subject.
In this setup, the LEMMA, FEATS and DEPREL
columns are optional, in which case an underscore
( ) will be used as a placeholder. Each treebank
was split into training, development and testing par-
titions. The HEAD and DEPREL entries were only
supplied for the development and the final testing
sets,2 but not for the training partition. The com-
petitors were encouraged to develop their unsuper-
vised entries on the union of the three partitions, and
make sparse use of the development set, i.e., for san-
ity checking more than model fitting in order to min-
imise the extent of supervision.
2.2 POS induction
In the POS induction track, participants developed
systems to induce the Part-of-Speech (POS) classes
for each word in the testing corpus. In order to train
the systems, the same training and development sets
were used as for the other tracks. These corpora in-
cluded manually supplied POS tags for each token,
2For the initial test set these fields were omitted.
65
which were not to be used for training, only evalua-
tion. Participants submitted predicted tags for each
token, which were scored against the gold-standard.
For evaluation, we used 4 different metrics. The
first is the many-to-one metric (M-1) (also known
as cluster purity), which is widely used for cluster
evaluation as well as evaluation of POS induction.
This metric assigns each word cluster to its most
common tag, and then measures the proportion of
correctly tagged words. The second metric is the
one-to-one mapping (1-1), a constrained version of
Many-to-one mapping in which each predicted tag
is associated with only one gold-standard tag and
vice versa (Haghighi and Klein, 2006). Word clus-
ters are assigned greedily to tags, and in the event
of there being more word classes than tags, some
word classes will be left unassigned. Another met-
ric that was used is Variation of information (VI)
(Meila, 2003), which is based the conditional en-
tropy of between the two different clusterings (John-
son, 2007). Lastly, we use the V-measure (VM) met-
ric (Rosenberg and Hirschberg, 2007), which is an-
other entropy-based measure, but defined in terms of
a F score to balance precision and recall terms (we
use equal weighting of the two factors). Please see
Christodoulopoulos et al (2010) for further details
about these metrics.3 For these metrics, a higher
score is better, with the exception of VI.
For all these metrics, the induced tags are eval-
uated against the universal pos tags, as this means
there are a consistent number of tags across the lan-
guages. Using these metrics, the results will vary as
a result of predicting a different number of tags (in
particular, more tags will mean a higher score for M-
1, and the converse is true for 1-1). However, using
the universal POS tags, we think will make results
less sensitive to large differences in POS inventory
between languages (such as for the Dutch dataset).
2.3 Dependency induction
For the Dependency induction track, the training
data consisted of the original treebank data, but
without dependency annotations. A development set
was also provided, which included the dependency
annotations, but this was meant mainly as a way to
3Thanks to Christos Christodoulopoulos for sharing his im-
plementation of the POS induction metrics, which we have used
in our evaluation.
verify systems, as we mean to minimise the amount
of supervision in the task. The participants were
later supplied with test sets for which the systems
could generate predictions. Only after the predic-
tions were submitted were the fully annotated test
sets released.
The dependency inductions were evaluated on
3 metrics: directed accuracy, undirected accuracy
and Neutral Edge Detection (NED) (Schwartz et
al., 2011). Directed accuracy is the ratio of cor-
rectly predicted dependencies (including direction)
over total amount of predicted dependencies. Undi-
rected accuracy is much the same, but also considers
a predicted dependency correct if the direction of the
dependency is reversed (e.g. if the predicted depen-
dency is not A ? B, but B ? A). Lastly, the NED
metric is a variant of undirected accuracy that also
rewards cases where an edge-flip occurs, meaning
that the predicted parent of a token is actually the
grandparent of that same token in the gold-standard
data. Note that before evaluating with these metrics
punctuation was removed from all sentences, and
any child words under a punctuation node were re-
attached to their nearest ancestor that wasn?t punc-
tuation.
The final ?joint? task consisted of inducing depen-
dency structure from only the tokens in the corpus,
without recourse to the gold POS tags. Where POS
is predicted (e.g., in a pipeline), we included these
in our general POS evaluation. The induced depen-
dency trees were evaluated with the same metrics as
in the dependency induction track, but are consid-
ered separately. We expect these systems to have
lower scores overall due to the lack of gold-standard
POS tags.
3 Treebanks
We selected a number of different treebanks for use
in the challenge, aiming to represent a wide range
of different languages, dialects and genres of text.
In total we used ten different treebanked corpora
in nine different languages. For the practical rea-
sons of simplifying the administration of the chal-
lenge and allowing the data to be reused in future re-
search, we chose corpora with licences allowing ei-
ther free redistribution, or those held by the Linguis-
66
tic Data Consortium (LDC).4 Many of these datasets
have been used before in dependency grammar or
part-of-speech research, particularly the shared tasks
at CoNLL 2006 and 2007. For the purpose of
the competition, we have updated these datasets to
include any annotation updates or additional data,
where available. It is important for unsupervised ap-
proaches to have sufficient amounts of data, espe-
cially given the common sentence length limitations
imposed by most dependency grammar models. As
described in section 2, we have included an extra
field for the universal part-of-speech (UPOS) using
Petrov et al (2011)?s automatic conversion tool.5
Below we describe the different treebanks used,
and the conversion process into our data format for
the purpose of the competition. Please see Table 1
for statistics on each of the treebanks.
Dependency treebanks We used the following
dependency treebanks: Arabic The Prague Ara-
bic Dependency Treebank V1 (Hajic? et al, 2004).6
Basque The Basque 3lb dependency treebank
(Aduriz et al, 2003). Czech The Prague Depen-
dency Treebank 2.0 (Bo?hmova? et al, 2001).7 Dan-
ish The Copenhagen Dependency Treebank ver-
sion 2 (Buch-Kromann et al, 2007). English The
CHILDES US/Brown subcorpus (Sagae et al,
2007). Slovene The jos500k Treebank (Erjavec et
al., 2010). 8 Swedish The Talbanken treebank
(Nivre et al, 2006). The conversion of each of these
treebanks was quite straightforward as they were al-
ready annotated for dependencies. Moreover, many
of these corpora had been used previously in the
CoNLL 2006 and 2007 shared tasks, and therefore
we were able to reuse this data and/or their conver-
sion scripts. In the case of Arabic and Swedish we
used the exact same data, simply converting from
CoNLL dependency format into our own format (re-
moving redundant columns and adding a UPOS col-
umn). While many of the other corpora had also
4In the following corpus descriptions, when not otherwise
specified the corpus is freely available for research purposes.
5http://code.google.com/p/
universal-pos-tags
6LDC catalogue number LDC2004T23.
7LDC catalogue number LDC2006T01.
8For the shared task, the annotation was converted to english
using the tables found at the JOS website: http://nl.ijs.
si/jos/msd/html-en/index.html
been used previously, our data is different, making
use of subsequent corrections to these treebanks and
additional annotated data now available.
First language acquisition provides an important
motivation for grammar induction research, conse-
quently we have included data from the CHILDES
database of child-directed speech. We use the
Brown sub-corpus, a longitudinal study of parent-
child interactions for three children aged between 18
months and 5 years old. The corpus has been man-
ually annotated with syntactic dependencies (Sagae
et al, 2007) and morphology. From this we take all
child-directed utterances, extracting word, morphol-
ogy, part-of-speech and dependency markup, and
developed our own conversion into UPOS. Our test-
ing and development sets were drawn from the first
15 Eve files which were manually annotated for de-
pendency structure. The rest of the corpus, which
had not been manually annotated for syntax, was
merged to form the training set.
Phrase-structure treebanks As well as depen-
dency treebanks, we used three different phrase-
structure treebanks: The Dutch Alpino treebank
(Bouma et al, 2000), the English Penn Treebank
V3 (Marcus et al, 1993),9 and the Portuguese Flo-
resta Sinta?(c)tica treebank (Afonso et al, 2002). As
these treebanks do not explicitly mark dependen-
cies, we automatically extracted these using head
finding heuristics. Thankfully the difficult work
of creating such scripts has already been done as
part of the CoNLL shared tasks. We have reused
their scripts to create dependency representations of
these treebanks, before converting into our file for-
mat and augmenting with UPOS annotation. In the
case of Dutch, we have reused the same CoNLL
2006 data; note that this dataset includes predicted
part-of-speech rather than gold standard annotation
(Buchholz and Marsi, 2006). For the Portuguese,
we used the same Bosque 7.3 sub-corpus10 from
CoNLL 2006, additionally including in our training
set the recently-annotated Selva 1.0 subcorpus.
The Penn Treebank is the most common data set
in parsing and grammar induction. We have patched
9LDC catalogue number LDC99T42.
10An updated version of this corpus is available, however
the file format had changed significantly and we were unable
to adapt the conversion scripts in time for the competition.
67
ar cs da en-childes en-ptb eu nl pt sl sv
annotation d d d d p d p p d d
Training data
Tokens 106.6k 1.2M 68.5k 312.8k 1.1M 124.7k 192.2k 196.4k 193k 184.6k
Sentences 2.8k 68.5k 3.6k 57.4k 45.4k 9.1k 13k 8.7k 9.4k 10.7k
Tokens/sent 38.4 17.1 18.8 5.5 23.9 13.7 14.8 22.6 20.5 17.3
CPOSTAG 15 12 25 31 31 16 13 16 13 41
POSTAG 21 61 141 76 45 50 300 22 31 41
FEATS 22 75 338 29 0 269 310 146 46 0
Development data
Tokens 5.1k 159k 17k 25.3k 32.9k 12.6k 2.9k 10.3k 20.2k 6.9k
Sentences 139 9.3k 1k 5k 1.3k 1k 386 400 1k 389
Tokens/sent 36.8 17.1 17 5.1 24.4 12.5 7.4 25.8 20.2 17.6
% New words 27.5 26 49.8 9.8 11.4 46.1 18.8 27.5 38.7 13.8
Test data
Tokens 5.1k 173.6k 14.7k 28.4k 56.7k 14.3k 5.6k 5.9k 22.6k 5.7k
Sentences 131 10.1k 1k 5.2k 2.4k 1.1k 386 288 1k 389
Tokens/sent 39.1 17.1 14.7 5.4 23.5 12.7 14.5 20.4 22.6 14.5
% New words 24.3 25.3 43.7 9 12.1 51.5 40.5 25.2 37.1 34.6
Table 1: Properties of the treebanks. We report the linguistic annotation method (dependency vs. phrase-structure),
the size of each treebank, the number of types for the different granularities of part-of-speech tags and morphological
features (note that UPOS has a fixed set of 12 tags), and the proportion of word types that were not present in training.
the treebank to include NP-internal structure using
Vadas and Curran?s annotations (Vadas and Cur-
ran, 2007), which was then converted to dependency
structures using the penn-converter11 script
(Johansson and Nugues, 2007). This tool has a num-
ber of options controlling the linguistic decisions
in converting from phrase-structure to dependency
trees, e.g., the treatment of coordination. We ex-
tracted five versions of the treebank, each encoding
each different sets of linguistic assumptions (Tsar-
faty et al, 2011).12 These are denoted default, old-
LTH, CoNLL-2007, functional and lexical; for the
main results we used the standard options, we also
report separately evaluations using each of the five
variants. The treebank was partitioned into training
(sections 0-22), development (sec. 24) and testing
sets (sec. 23).
4 Baselines and Benchmarks
A number of standard baselines and previously pub-
lished benchmark systems were implemented for
each task in order to place the submitted systems in
context.
11http://nlp.cs.lth.se/software/treebank_
converter
12Note that Tsarfaty et al (2011) also propose an evalua-
tion metric for comparing dependency trees, which we have not
used. Note however that it could, in principle, be used for simi-
lar evaluations.
The standard baseline for grammar induction
models is to assume either left branching or right
branching analyses (LB, RB). These capture the ten-
dency for languages to favour one attachment direc-
tion over another. The most frequently cited and
extended model for dependency induction is DMV
(Klein and Manning, 2004). We provide results for
this model trained on each of the coarse (DMVc), fine
(DMVp), and universal (DMVu) POS tag sets, all ini-
tialised with the original harmonic initialiser. As a
further baseline we also evaluated the dependency
trees resulting from directly using the harmonic ini-
tialiser without any training (H).
As a strong benchmark we include the results of
the non-parametric Bayesian model previously pub-
lished in Blunsom and Cohn (2010) (BC). The stated
results are for the unlexicalised model described in
that paper where the final analysis is formed by
choosing the maximum marginal probability depen-
dency links estimated from forty independent Gibbs
sampler runs.
For part-of-speech tagging we include results
from an implementation of the Brown word clus-
tering algorithm (Brown et al, 1992) (Bc,p,u), and
the mkcls tool written by Franz Och (Och, 1999)
(MKc,p,u). Both of these benchmarks were trained
with the number of classes matching the number
in the gold standard of each of the tagsets in turn:
coarse (c), fine (p), and universal (u). A notable
68
property of both of these word class models is that
they enforce a one-tag-per-type restriction that en-
sures there is a one-to-one mapping between word
types and classes.
For POS tagging we also provide benchmark re-
sults from two previously published models. The
first of these is the Pitman-Yor HMM model de-
scribed in (Blunsom and Cohn, 2011), which in-
corporates ta one-tag-per-type restriction (BC). This
model was trained with the same number of tags as
in the gold standard fine tag set for each corpus. The
second benchmark is the HMM with Sparsity Con-
straints trained using Posterior Regularization (PR)
described in (Grac?a et al, 2011). In this model
the HMM emission probabilitiy distribution are esti-
mated using small Maximum Entropy models (fea-
tures set described in the original paper). The mod-
els were trained for 200 iterations of PR using both
the same number of hidden states as the coarse Gc
and universal Gu gold standard. All parameters were
set to the values described in the original paper.
5 Submissions
The shared task received submissions covering a di-
verse range of approaches to the dependency and
part-of-speech induction challenges. Encouragingly
all of these submissions made significant departures
from the benchmark HMM and DMV approaches
which have dominated the published literature on
these tasks in recent years. The submissions were
characterised by varied choices of model structure,
parameterisation, regularisation, and the degree to
which light supervision was provided through con-
straints or the use of labelled tuning data. In the fol-
lowing sections we summarise the approaches taken
by the systems submitted for each task.
5.1 Part-of-Speech Induction
The part-of-speech induction challenge received two
submission, (Chrupa?a, 2012; Christodoulopoulos et
al., 2012). Both of these submissions based their in-
duction systems on LDA inspired models for cluster-
ing word types by the contexts in which they appear.
Notably, the strongest of the provided benchmarks
and the two submissions modelled part-of-speech
tags at the type level, thus restricting all tokens of
a given word type to share the same tag. Though
clearly out of step with the gold standard tagging,
this one-tag-per-type restriction has previously been
shown to be a crude but effective way of regularising
models towards a good solution. Below we sum-
marise the approach of each submission, identified
by the surname of the first author on the submitted
system description.
Chrupa?a (2012) employed a two stage approach
to inducing part-of-speech tags. The first stage used
an LDA style probabilistic model to induce a dis-
tribution over possible tags for a given word type.
These distributions were then hierarchically clus-
tered and the final tags selected using the prefix of
the path from the root node to the word type in the
cluster tree. The length of the prefixes, and thus the
number of tags, was tuned on the labelled develop-
ment data.
The system of Christodoulopoulos et al (2012)
was based upon an LDA type model which included
both contexts and other conditionally independent
features (Christodoulopoulos et al, 2011). This base
system was then iterated with a DMV system and
with the resultant dependencies being repeatedly fed
back into the POS model as features. This submis-
sion is notable for being one of the first to attempt
joint POS and dependency induction rather than tak-
ing a pipeline approach.
5.2 Dependency Induction
The dependency parsing task saw a variety of ap-
proaches with only a couple based on the previously
dominant DMV system. Two forms of light super-
vision were popular, the first being the inclusion of
pre-specified constraints or rules for allowable de-
pendency links, and the second being the tuning of
model parameters or selecting between competing
models on the labelled development data. Obviously
the merits of such supervision would depend on the
desired application for the induced parser. The di-
rect comparison of models which include a form of
universal prior syntactic information with those that
don?t does permit interesting development linguistic
questions to be explored in future.
Bisk and Hockenmaier (2012) chose to induce a
restricted form of Combinatory Categorial Grammar
(CCG), the parses of which were then mapped to
dependency structures. Restrictions on head-child
dependencies were encoded in the allowable cate-
69
gories for each POS tag and the heads of sentences.
Key features of their approach were a maximum
likelihood objective function and an iterative proce-
dure for generating composite categories from sim-
ple ones. Such composite categories allow the pa-
rameterisation of larger units than just head-child
dependencies, improving over the more limited con-
ditioning of DMV.
Marac?ek and Z?abokrtsky? (2012) introduced a
number of novel features in their dependency induc-
tion submission. Wikipedia articles were used to
quantify the reducibility of word types, the degree
to which the word could be removed from a sen-
tence and grammaticality maintained. This metric
was then used, along with a model of child fertil-
ity and dependency distance, within a probabilistic
model. Inference was performed by using a local
Gibbs sampler to approximate the marginal distribu-
tion over head-child links.
S?gaard (2012) presented two model-free heuris-
tic algorithms. The first was based on heuristically
adding dependency edges based on rules such as ad-
jacency, function words, and morphology. The re-
sulting structure is then run through a PageRank al-
gorithm and another heuristic is used to select a tree
from the resulting ranked dependency edges. The
second approach takes the universal rules of Naseem
et al (2010) but rather than estimating a probabilis-
tic model with these rules, a rule based heuristic is
used to select a parse rather. This second model-free
approach in particular provides a strong baseline for
probabilistic models built upon hand-specified de-
pendency rules.
Tu (2012) described a system based on an ex-
tended DMV model. Their work focussed on the
exploration of multiple forms of regularisation, in-
cluding Dirichlet priors and posterior regularisation,
to favour both sparse conditional distributions and
low ambiguity in the induced parse charts. While
many previous works have included sparse priors
on the conditional head-child distributions the ad-
ditional regularisation of the ambiguity over parse
trees is a novel and interesting addition. The la-
belled development sets were employed to both se-
lect between models employing different regularisa-
tion, and to tune model parameters.
5.3 POS and Dependency Induction
There was only a single submission for the task of
inducing dependencies without gold standard part-
of-speech tags supplied. Christodoulopoulos et al
(2012) submitted the same joint tagging and DMV
system used for the POS induction task to the depen-
dency induction task. Results on the development
data indicated that this iterated joint training had a
significant benefit for the induced tags and a smaller
benefit for the dependency structures induced.
6 Results
The main results for the three tasks are shown in Ta-
bles 2, 3, and 4, for the POS induction, dependency
induction and joint tasks, respectively.13 We now
present a detailed analysis of each of the three tasks.
6.1 POS induction
The main evaluation results for the POS induc-
tion task are shown in Table 2, which compares
the induced clusters against the gold universal tags
(UPOS).14 Given the diversity of scenarions used by
each system (e.g. number of hidden states, tuning
on development data) a direct comparison between
the systems can only be illustrative. A first obser-
vation is that depending on the particular evaluation
metric employed the ranking of the systems changes
substantially, for instance the Gu system is the best
using the 1-1 and VI metric but is the worst of the en-
tries (excepting the baselines) when using the other
two metrics. Focusing on the VM metric, which
was shown empirically not to have low bias with re-
spect to the word classes (Christodoulopoulos et al,
2010), the best entry is the BC system which has the
best performance in 9 out of 10 entries followed by
the CGS and the C system. Note that this ranking
holds also for the comparison against fine POS tags,
shown in Table 7.
An interesting aspect is that almost all systems
beat the strong Brown (B) and mkcls (MK) base-
line across the different metrics when we restrict
our attention to the cases where the same number
13Additional tables of results are in the appendix, and fur-
ther results are online at http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure.
14See also Table 7 for the comparison against the fine POS
tags; we base our analysis on UPOS instead as this tag set has a
fixed size irrespective of the treebank.
70
M-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 83.80 N/A 83.33 83.33 65.05 66.61 66.20 69.89 66.22 71.08 72.72 68.03
basque 80.85 79.54 86.67 86.67 77.37 73.88 74.73 77.49 73.32 74.80 78.63 71.40
czech 83.10 66.78 72.27 77.97 N/A N/A 60.85 75.57 60.42 65.43 79.35 57.16
danish 81.44 77.76 84.13 84.92 68.16 53.78 72.12 79.77 47.09 72.26 82.59 53.07
dutch 80.75 70.13 74.04 76.11 63.37 57.64 57.99 84.17 57.31 68.18 84.78 63.04
en-childes 90.36 85.42 91.50 91.50 N/A N/A 82.65 89.70 70.12 86.27 91.44 75.63
en-ptb 86.73 81.93 78.11 84.35 77.14 71.10 77.29 80.88 63.74 79.99 83.88 63.34
portuguese 81.69 77.38 80.38 81.90 75.54 74.35 70.07 74.25 67.60 70.79 72.90 68.08
slovene 70.81 65.31 75.53 75.92 67.94 59.96 61.58 68.93 58.32 58.43 65.69 50.36
swedish 78.61 80.45 79.60 79.60 69.91 58.79 71.69 71.69 57.55 76.45 76.45 57.30
averages 81.82 76.08 80.56 82.23 70.56 64.51 69.52 77.23 62.17 72.37 78.84 62.74
1-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 53.67 N/A 39.44 39.44 39.83 55.52 40.55 33.57 43.31 51.54 40.24 51.58
basque 36.10 36.03 47.15 47.15 47.09 54.70 32.61 20.53 40.62 34.80 27.28 37.65
czech 31.82 49.30 30.49 27.20 N/A N/A 46.19 26.66 45.10 43.70 24.48 39.25
danish 42.54 42.77 31.67 31.04 39.95 45.58 36.04 17.74 39.19 43.89 22.18 44.23
dutch 42.79 56.15 43.10 39.62 56.45 45.37 48.18 21.36 43.12 55.99 21.32 54.09
en-childes 38.79 42.57 43.76 43.76 N/A N/A 40.78 35.54 57.71 43.45 32.00 59.18
en-ptb 41.55 39.57 43.86 31.56 42.07 51.70 39.79 33.90 46.50 40.55 36.22 51.17
portuguese 59.66 47.45 35.90 35.50 46.50 56.08 51.15 42.68 51.58 44.28 35.38 46.31
slovene 39.02 53.04 33.18 32.50 50.90 48.50 46.83 40.16 42.28 40.34 39.32 40.58
swedish 42.38 32.44 26.45 26.45 34.99 54.92 27.56 27.56 51.34 35.82 35.82 43.60
averages 42.83 44.37 37.50 35.42 44.72 51.55 40.97 29.97 46.07 43.44 31.42 46.76
VM
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 61.75 N/A 51.27 51.27 44.81 47.07 39.93 42.43 39.92 47.47 43.91 44.49
basque 42.17 41.52 43.04 43.04 40.86 40.05 34.85 33.33 36.08 36.32 34.35 33.42
czech 52.26 45.31 40.22 39.20 N/A N/A 38.56 42.90 37.46 41.70 46.03 37.34
danish 56.57 54.63 52.46 52.32 47.26 41.96 47.89 44.37 35.13 50.52 48.17 39.96
dutch 56.96 53.35 54.87 52.90 48.57 45.80 43.34 49.33 43.67 51.37 50.11 47.20
en-childes 64.53 62.32 62.76 62.76 N/A N/A 58.87 60.31 57.06 62.76 60.92 60.51
en-ptb 60.73 57.99 53.14 52.09 55.10 52.54 54.76 55.08 48.04 56.81 57.29 48.46
portuguese 64.17 58.41 52.54 52.32 55.96 58.14 52.09 53.18 50.32 52.48 50.87 50.18
slovene 51.15 51.29 46.60 46.50 50.98 45.98 44.49 45.80 38.61 36.79 43.43 36.43
swedish 57.05 54.21 47.08 47.08 48.89 45.73 45.87 45.87 40.84 49.77 49.77 42.83
averages 56.73 53.23 50.40 49.95 49.05 47.16 46.06 47.26 42.71 48.60 48.48 44.08
VI
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 2.48 N/A 3.70 3.70 3.39 2.98 3.78 3.94 3.53 3.31 3.82 3.30
basque 3.82 3.44 3.98 3.98 3.25 2.82 3.92 4.98 3.45 3.79 4.76 3.58
czech 3.83 3.41 4.92 5.77 N/A N/A 3.70 4.76 3.69 3.63 4.53 3.83
danish 3.36 3.34 4.31 4.38 3.78 3.46 3.86 5.43 3.79 3.64 4.90 3.59
dutch 3.56 3.13 3.28 3.71 3.30 3.44 3.66 5.22 3.60 3.26 5.15 3.39
en-childes 2.81 2.86 3.06 3.06 N/A N/A 3.13 3.34 2.59 2.84 3.33 2.50
en-ptb 3.18 3.28 3.67 4.36 3.34 3.03 3.46 3.62 3.36 3.36 3.52 3.28
portuguese 2.47 2.83 3.96 4.09 2.96 2.62 3.19 3.36 3.10 3.21 3.52 3.15
slovene 3.62 3.14 4.80 4.86 3.16 3.30 3.61 4.09 3.73 4.15 4.33 3.99
swedish 3.31 3.68 4.98 4.98 3.90 3.32 4.46 4.46 3.70 4.07 4.07 3.62
averages 3.24 3.23 4.07 4.29 3.39 3.12 3.68 4.32 3.45 3.53 4.19 3.42
Table 2: Results for the POS induction task, showing one-to-one, many-to-one, VM and VI scores, measured against
the gold UPOS tags. Each system is shown in a column, where the title is an acronym of the authors? last names, or
else the name of a benchmark system (B is the Brown clusterer and MK is mkcls). The superscripts c, p and u denote
different applications of the same method with a number of word classes set to equal the true number of coarse tags,
full tags or universal tags, respectively, for each treebank.
71
of hidden states are used (the exception being the G
system which occasionally under-performed against
MK). Interestingly the assumption of one-tag-per-
word, made by all but the G system, works very
well in practice leading to consistently strong re-
sults. This suggests that dealing with word ambigu-
ity is still an unresolved issue in unsupervised POS
induction.
Comparing the performance of the systems for
different languages, as expected the languages for
which we have a larger corpora (English CHILDES
and PTB and Czech) tend to result in systems with
better accuracies. An interesting future question is
how do the propose methods scale when training on
really large corpora (e.g., wikipedia) both in terms
of performance (accuracy) but also in the resources
they required.
Finally, the wild divergences in the system rank-
ings when considering the different evaluation met-
rics calls for some sort of external evaluation using
the induced clusters as features to other end sys-
tems, for instance semi-supervised tagging. The
main question is if there will be a definitive ranking
between systems for a diverse set of tasks, or if on
the contrary the effectiveness of the output of each
system will vary according to the task at hand.
6.2 Dependency induction
The main evaluation results for the dependency task
are shown in Table 3. From this we make several
observations.15 Firstly, for almost all the corpora
the participants systems have outperformed the sim-
ple baselines, and by a significant margin. There
are three exceptions to this: for Arabic, Basque and
Danish the left or right-branching baselines outper-
forms most or all of the competitors. This may in-
dicate that these languages are inherently difficult,
or may simply be a consequence of these three lan-
guages having the least data of all of our corpora.
Basque and Dutch proved to be the hardest of the
treebanks, with the lowest overall scores, and the
CHILDES (English) and Portuguese were the eas-
iest. The reasons for this are not immediately clear,
15Table 3 evaluates against the full test sets, however it is
traditional to present results for short sentences mirroring the
common training setup. See Tables 8 and 9 for results over
sentences with 10 words or fewer, excluding punctuation. Note
that our analysis is based on the results for the full test set.
although we speculate that Basque is difficult due to
its dissimilarity from other European languages, and
therefore may not match the assumptions underly-
ing models developed primarily on English. Dutch
is difficult as its annotation was non-projective, and
it has a very large set of POS tags, while CHILDES
is made easier due to its extremely short and simple
sentences.
In terms of declaring a ?winner?, it is clear that
Tu?s system ranks best under directed accuracy and
NED, and a very narrow second (to the organisers?
submission, BC) for undirected accuracy. Moreover
Tu?s system was a consistent performed across all
corpora, with no single result well below the results
of the other participants. Note that the three different
metrics often predict the same winner across the dif-
ferent treebanks, however there are some large dis-
crepancies, such as Portuguese and Dutch where the
directed and undirected accuracy metrics concur, but
NED produces a very different ranking. It is unclear
which metric should be trusted more than another;
this could only be assessed by correlating these met-
rics with some form of secondary evaluation, such
as in a task based setting or obtaining human gram-
maticality judgements.16
The benchmark systems include DMV (Klein and
Manning, 2004), which has historical importance
in terms of being the first research systems to out-
perform simple baselines for dependency induction,
and also the model upon which most recent depen-
dency induction research is based, including many
of the competitors in the competition. We ob-
serve that in most cases the competitors have out-
performed the DMV models, in many cases by a
large margin. In all cases DMV improved over
its initialisation condition (the harmonic initialiser),
although often this improvement was only slight,
underscoring the importance of good initialisation.
The effect of inducing DMV grammars from var-
ious different granularity of POS tags made little
difference in most cases, although for Dutch17 and
the English PTB there change was more dramatic.
16It was our intention to include a task-based evaluation for
machine translation, but this proved impractical for the compe-
tition due to the volumes of data that we would require each
participant to process.
17Note that for Dutch the full POS tags were not gold stan-
dard, but were system predictions.
72
Directed
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8
basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4
czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3
danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2
dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5
en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3
en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9
portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7
slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7
swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3
averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9
Undirected
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 57.3 29.7 57.6 62.0 58.7 48.0 58.4 59.3 41.8 42.0 43.7 41.2 61.7 63.9
basque 58.0 47.2 43.3 45.0 43.2 47.5 24.3 53.3 48.1 47.7 40.3 37.6 53.9 53.1
czech 59.0 45.0 57.8 54.3 55.5 49.3 55.8 61.4 46.2 46.7 45.3 38.5 51.5 52.3
danish 60.8 50.7 60.7 56.1 60.3 56.6 60.5 61.6 55.1 54.1 51.6 46.0 58.7 59.9
dutch 61.0 45.0 47.5 51.5 48.9 46.8 51.4 54.6 52.2 45.0 52.2 37.2 50.1 50.8
en-childes 63.5 68.4 67.2 59.9 61.4 62.0 62.4 66.9 63.8 64.0 57.5 49.0 50.0 49.9
en-ptb 66.2 58.1 49.7 57.6 48.2 49.5 58.8 62.1 43.1 53.1 43.0 36.2 51.7 51.5
portuguese 56.6 72.4 61.4 61.9 49.8 52.6 66.9 61.4 44.3 48.1 43.6 41.2 55.7 56.8
slovene 58.1 47.9 45.2 49.1 44.5 42.4 53.5 61.8 42.1 40.6 42.1 32.5 40.8 41.1
swedish 70.0 58.5 58.8 59.3 60.4 53.5 65.2 66.9 51.1 51.1 53.3 44.5 53.0 53.2
averages 61.0 52.3 54.9 55.7 53.1 50.8 55.7 60.9 48.8 49.2 47.3 40.4 52.7 53.2
NED
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 63.6 37.3 59.2 67.1 63.2 56.5 65.8 64.1 48.9 48.0 48.8 47.5 62.7 69.0
basque 69.6 55.8 51.5 55.6 53.4 58.8 38.0 65.8 57.6 57.1 51.5 49.3 67.2 59.1
czech 71.0 55.7 70.2 65.2 67.3 63.2 69.7 71.6 53.2 52.9 54.1 47.6 56.3 68.6
danish 72.0 63.1 72.9 69.5 73.5 65.9 71.8 76.4 64.8 63.5 58.9 53.5 61.6 71.5
dutch 71.6 58.6 68.6 72.0 69.7 60.6 63.8 66.9 63.5 54.5 63.5 46.9 55.1 67.0
en-childes 80.9 79.6 82.8 74.1 72.7 77.1 83.2 80.4 78.1 78.3 77.5 67.2 61.0 75.2
en-ptb 75.2 69.8 69.4 73.8 67.2 64.1 71.6 69.8 49.8 67.0 49.6 44.8 53.9 68.1
portuguese 67.5 79.8 75.6 75.7 71.7 66.9 78.2 80.4 62.1 66.6 61.3 51.8 57.3 75.4
slovene 64.4 60.7 56.9 58.9 57.1 55.9 66.7 68.7 49.2 47.3 49.2 38.9 43.8 56.6
swedish 80.1 70.9 73.2 73.8 72.7 66.7 77.0 77.1 64.0 64.0 62.0 56.0 56.5 71.0
averages 71.6 63.1 68.0 68.6 66.9 63.6 68.6 72.1 59.1 59.9 57.6 50.3 57.6 68.1
Table 3: Directed accuracy, undirected accuracy and NED results for the dependency task (using supplied POS). The
first column (BC) is our benchmark system, the next seven are participants systems, and the remaining columns consist
of the DMV benchmark and various simple baselines. The superscripts c, p and u denote which type of POS was used,
and S1 and S2 denote two different submissions for S?gaard (2012).
Overall the full POS tagset lead to the best perfor-
mance over the coarse and universal tags (consider-
ing undirected accuracy or NED), which is to be ex-
pected as there is considerably more syntactic infor-
mation contained in the full POS. This must be bal-
anced against the additional model complexity from
expanding its parameter space, which may explain
why the difference in performance differences are so
small. The same pattern can also be seen in Marac?ek
and Z?abokrtsky? (2012)?s submission, whose system
using full POS (Mp) outperformed their other vari-
ants.
6.3 Joint task
As we had only one submission for the joint prob-
lem of POS and dependency induction, there are
few conclusions we can draw for this joint task (see
Table 4 for the results, and Table 9 for the short
sentence evaluation). Compared to the dependency
induction task using gold standard POS, as shown
in Table 3, the accuracy for the joint models are
lower. Interestingly, the DMV model performs best
when using the same number of word clusters as
there are POS tags, mirroring the findings reported
73
directed
testset CGS DMVc DMVp DMVu
arabic N/A 35.3 44.4 34.2
basque 24.5 27.5 25.1 28.7
czech 24.7 19.9 33.2 20.0
danish 21.4 23.3 31.9 10.0
dutch 15.1 20.6 33.7 20.5
en-childes 29.9 38.6 42.2 40.3
en-ptb 21.5 22.5 23.3 17.2
portuguese 19.7 28.5 28.0 17.1
slovene 19.2 13.9 11.5 14.4
swedish 23.6 26.4 26.4 20.5
averages 22.2 25.7 30.0 22.3
undirected
testset CGS DMVc DMVp DMVu
arabic N/A 45.5 52.5 45.0
basque 43.5 46.4 47.3 47.0
czech 38.9 37.5 50.9 38.5
danish 51.4 52.2 48.8 37.3
dutch 40.3 41.9 48.6 40.8
en-childes 54.9 59.2 60.8 58.1
en-ptb 43.4 45.4 48.8 39.4
portuguese 45.5 51.8 52.7 39.8
slovene 32.8 33.3 36.7 32.8
swedish 45.6 48.9 48.9 40.3
averages 44.0 46.2 49.6 41.9
NED
testset CGS DMVc DMVp DMVu
arabic N/A 53.4 57.6 53.3
basque 55.9 55.6 54.4 54.7
czech 51.2 49.3 63.4 51.5
danish 61.7 60.3 60.4 46.3
dutch 47.2 57.5 56.8 55.2
en-childes 78.2 77.7 78.1 76.5
en-ptb 53.9 60.2 63.5 47.5
portuguese 50.0 69.4 70.8 57.9
slovene 40.7 38.7 47.5 40.3
swedish 54.5 65.4 65.4 54.3
averages 54.8 58.8 61.8 53.8
Table 4: Directed, undirected and NED accuracy results
for evaluating the predicted dependency structures in the
joint task (i.e., not using supplied POS tags). The first
column is the participant?s system and the next three are
DMV models trained on the Brown word clusters (see
section 6.1).
above with gold standard tags. The best joint sys-
tem was the DMVp model, which only marginally
under-performed the equivalent DMV model trained
on gold POS. This is an encouraging finding, sug-
gesting that word clusters are able to represent im-
portant POS distinctions to inform deeper syntactic
processing.
6.4 Analysis
Until now we have adopted the standard metrics in
dependency evaluation: namely directed head at-
tachment accuracy, and its more lenient counter-
parts, undirected accuracy and NED. The latter met-
rics reward structures that almost match the gold
standard tree, by way of rewarding child-parent
edges that are predicted in the reverse direction, i.e.,
attaching the child as the parent (NED takes this fur-
ther, by also rewarding the grandparent-child edge
when this occurs). This allows some degree of flexi-
bility when considering various contentious linguis-
tic decisions such as whether a preposition should
head a preposition phrase, or the head of the child
noun-phrase. This added leniency comes at a price,
as shown in Table 3 where the undirected accuracy
and NED results are considerably higher than di-
rected accuracy, and display less spread of values
(look in particular at the random trees, Ra). Is is
unclear that the predicted trees are truly predicting
linguistically plausible structures, but instead that
the differences are due largely to chance. Moreover,
systems that predict linguistic phenomena inconsis-
tently between sentences or across types of related
phenomena are rewarded under these lenient met-
rics.
For these reasons we also consider a different,
less permissive, evaluation method, using multiple
references of the treebank where each is annotated
with different styles of dependency. As described
in section 2, we processed the Penn treebank five
times with different options to the LTH conversion
tool. This affected the treatment of coordination,
preposition phrases, subordinate clauses, infinitival
clauses etc. Next we compare the directed accu-
racy of the systems against these five different ?gold
standard? references, which are displayed in Table 5,
alongside the maximum score for each system. Note
that most systems performed well against the stan-
dard, conll2007 and functional references but poorly
against the lexical and oldLTH references.18 Con-
sidering the latter two references, a different system
would be selected as the highest performing, namely
Bisk and Hockenmaier (2012) (BH) over Blunsom
and Cohn (2010) (BC) which wins in the other cases.
18The common difference here is that the latter two refer-
ences do not treat prepositions as heads of PPs.
74
This evaluation method rewards many different lin-
guistically plausible structures, but in such a way
that the predictions must be consistent between dif-
ferent sentences in the testing set, and in their treat-
ment of related linguistic phenomena. One caveat
is that this method can only be used when there
are many references, although in many cases differ-
ent outputs can be generated automatically, e.g., by
adjusting head-finding heuristics in converting be-
tween phrase-structure to dependency trees.
The previous analysis has rated each system in
terms of overall performance against treebank trees,
however this doesn?t necessarily mean that the pre-
dictions of the best ranked system will be the most
useful ones in a task-based setting. Take the ex-
ample of information extraction, in which a central
problem is to identify the arguments (subject, object
etc) of a given verb. This setting gives rise to some
types of dependency edges being more valuable than
others. We present comparative results for the Penn
treebank in Table 6 showing the directed accuracy
for different types of dependency relations. Observe
that there is a wide spread of accuracies for predict-
ing the head word of the sentence (ROOT), and simi-
larly for verbs? subject and object arguments. These
scores are similar to the scores for the local modi-
fiers shown, such as NMOD which describe the ar-
guments of a noun. This is surprising as noun edges
tend to be much shorter than for the arguments to a
verb, and thus should be easier to predict. Also in-
teresting are the spread of results for the CC edges
(these link a coordinating conjunction to its head),
suggesting that the systems learn to represent coor-
dination in very different ways to the method used
in the reference.
Figure 1 illustrates the directed accuracy over dif-
ferent lengths of dependency edge. For all systems
the accuracy diminishes with edge length, however
some fall at a much faster rate. The two best systems
(Tu, BC) have similar overall accuracy results, but
it is clear that Tu does better on short edges while
BC does better on longer ones. The same pattern
was also observed when considering the average ac-
curacy over all treebanks (not shown), although the
systems? results were closer together.
system ROOT SBJ OBJ PRD NMOD COORD CC
Tu 71.0 64.8 53.7 49.4 56.9 36.8 11.4
LB 17.8 40.1 15.3 18.0 41.9 27.7 9.7
BC 74.9 65.7 53.0 50.2 56.8 36.3 71.4
DMVc 17.0 11.7 16.0 31.3 27.8 25.7 9.2
DMVu 17.6 9.3 16.4 25.0 27.8 25.7 8.6
BH 67.5 55.3 44.9 45.6 58.6 27.6 62.7
Mu 29.3 42.4 38.8 51.8 34.5 30.5 33.0
R 12.9 9.4 16.1 21.1 12.1 15.7 2.7
Mc 60.7 47.4 39.9 45.8 36.5 33.9 44.3
RB 17.9 12.4 26.2 36.5 15.3 25.4 1.1
H 19.4 29.3 12.2 22.2 17.3 20.9 10.3
DMVp 54.7 42.0 30.7 30.1 28.9 25.4 24.3
S2 45.2 41.9 44.2 49.8 39.7 25.4 63.8
Mp 67.8 54.3 49.6 59.4 47.7 37.7 49.7
S1 43.1 47.9 36.3 46.7 27.9 23.5 7.6
Table 6: Directed accuracy results on the Penn treebank,
stratified by dependency relation. For clarity, only 9 im-
portant relation types are shown. The vertical bars sepa-
rate different groups of relations, from left to right, relat-
ing to the main verb, general modifiers and coordination.
7 Conclusion
This challenge set out to evaluate the state-of-the-
art in part-of-speech and dependency grammar in-
duction, promoting research in this field and, im-
portantly, providing a fair means of evaluation. The
participants submissions used a wide variety of dif-
ferent approaches, many of which we shown to
have improved over competitive benchmark sys-
tems. While the results were overall very positive,
it is fair to say that the tasks of part-of-speech and
grammar induction are still very much open chal-
lenges, and that there is still considerable room for
improvement. The data submitted to this evaluation
campaign will provide a great resource for devising
new methods of evaluation, and we plan to pursue
this avenue in future work, in particular task-based
evaluation such as in an information extraction or
machine translation setting.
8 Acknowledgements
This challenge was funded by the PASCAL 2 (Pat-
tern Analysis, Statistical Modelling and Compu-
tational Intelligence) European Network of Excel-
lence. We would also like to thank the treebank
providers for allowing us to use their resources, as-
sisting us in converting these into our desired for-
mat, and helping to resolve various questions. In
particular, special thanks to Zdenek Zabokrtsky and
Jan (Czech and Arabic), Tomaz Erjavec (Slovene),
and Eckhard Bick and Diana Santos (Portuguese).
We are also indebted to the organisers of the previ-
75
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
conll2007 54.9 51.7 40.4 49.2 36.8 32.2 41.7 54.2 20.9 33.2 20.4 18.0 30.1 20.3
functional 59.6 52.4 41.5 47.4 36.2 30.6 40.0 58.5 20.9 37.2 20.6 19.3 29.2 23.7
lexical 40.6 41.9 28.5 37.3 24.8 27.7 35.5 39.5 23.5 23.1 23.0 14.4 33.1 10.1
oldLTH 41.4 43.6 28.8 37.8 24.6 28.6 36.1 39.5 22.3 23.7 21.8 14.3 32.0 10.7
standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4
best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7
Table 5: Directed accuracy results measured against different conversions of the Penn Treebank into dependency trees.
l l
l
l
l
l l
l l
2 4 6 8
10
20
30
40
50
60
70
edge length
direc
ted a
ccura
cy (%)
l TuBCBHMZ?pS?2DMV?p
Figure 1: Directed accuracy on the Penn treebank strat-
ified by dependency length. For clarity only a subset of
the systems are shown, and edges of length 10 or more
were omitted.
ous CoNLL 2006 and 2007 competitions, who con-
tributed significant efforts into collating so many
treebanks and developing treebank conversion tools,
making our job much easier than it would other-
wise have been. Thanks to Sebastian Reidel, Joakim
Nivre and Sabine Buchholz for promptly answer-
ing our questions. We would like to thank the
LDC, who allowed their licenced data to be used
free of charge by the competitors, and Ilya Ahtaridis
who administered the licencing and corpus distribu-
tion. Thanks also to Valentin Spitkovski and Chris-
tos Christodoulopoulos who kindly provided us with
their evaluation scripts, and finally, the participants
themselves for taking part.
References
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT).
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. Floresta sinta?(c)tica: a treebank
for Portuguese. In Proceedings of the Third Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2002), pages 1698?1703, May.
Yonatan Bisk and Julia Hockenmaier. 2012. Induction of
linguistic structure with combinatory categorial gram-
mars. In Proceedings of the NAACL-HLT 2012 Work-
shop on Inducing Linguistic Structure Shared Task,
June.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised in-
duction of tree substitution grammars for dependency
parsing. In Proceedings of the 2010 Conference on
Empirical Methods on Natural Language Processing
(EMNLP), pages 1204?1213, Cambridge, MA, USA.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2001. The Prague Dependency Treebank: A
Three-Level Annotation Scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Syntactically
Annotated Corpora, pages 103?127. Kluwer Aca-
demic Publishers.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2000. Alpino: Wide coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands (CLIN 2000), pages 45?59.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479, December.
Matthias Buch-Kromann, Ju?rgen Wedekind,
and Jakob Elming. 2007. The Copen-
hagen Danish-English dependency tree-
bank. http://code.google.com/p/
copenhagen-dependency-treebank.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
76
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning (CoNLL-X), pages
149?164.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: how far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, EMNLP ?10, pages
575?584.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for PoS induction using multiple features. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 638?647, Edin-
burgh, Scotland, UK., July.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2012. Turning the pipeline into
a loop: Iterated unsupervised dependency parsing
and pos induction. In Proceedings of the NAACL-
HLT 2012 Workshop on Inducing Linguistic Structure
Shared Task, June.
Grzegorz Chrupa?a. 2012. Hierarchical clustering
of word class distributions. In Proceedings of the
NAACL-HLT 2012 Workshop on Inducing Linguistic
Structure Shared Task, June.
Tomaz? Erjavec, Darja Fis?er, Simon Krek, and Nina
Ledinek. 2010. The JOS linguistically tagged corpus
of Slovene. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10).
Joa?o Grac?a, Kuzman Ganchev, Lu??sa Coheur, Fernando
Pereira, and Benjamin Taskar. 2011. Controlling
complexity in part-of-speech induction. J. Artif. Intell.
Res. (JAIR), 41:527?551.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (HLT-NAACL
?06), pages 320?327.
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf, and
Emanuel Bes?ka. 2004. Prague Arabic dependency
treebank: Development in data and tools. In Proceed-
ings of the NEMLAR International Conference on Ara-
bic Language Resources and Tools, pages 110?117.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of the 16th Nordic Conference of Compu-
tational Linguistics (NODALIDA 2007).
Mark Johnson. 2007. Why doesn?t EM find good hmm
pos-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?07, pages 296?305.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ?04: Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 478.
David Marac?ek and Zdene?k Z?abokrtsky?. 2012. Unsuper-
vised dependency parsing using reducibility and fertil-
ity features. In Proceedings of the NAACL-HLT 2012
Workshop on Inducing Linguistic Structure Shared
Task, June.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Marina Meila. 2003. Comparing Clusterings by the Vari-
ation of Information. Learning Theory and Kernel
Machines, pages 173?187.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1234?1244, October.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of the fifth
international conference on Language Resources and
Evaluation (LREC2006).
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
June.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan T. McDonald.
2011. A universal part-of-speech tagset. CoRR,
abs/1104.2086.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2007. High-accuracy annotation and pars-
ing of CHILDES transcripts. In Proceedings of the
ACL-2007 Workshop on Cognitive Aspects of Compu-
tational Language Acquisition., June.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
77
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 663?672.
Anders S?gaard. 2012. Two baselines for unsupervised
dependency parsing. In Proceedings of the NAACL-
HLT 2012 Workshop on Inducing Linguistic Structure
Shared Task, June.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-annotation evaluation. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 385?396,
Edinburgh, UK, July.
Kewei Tu. 2012. Combining the sparsity and unambi-
guity biases for grammar induction. In Proceedings of
the NAACL-HLT 2012 Workshop on Inducing Linguis-
tic Structure Shared Task, June.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL-07), pages 240?247,
June.
Appendix
Directed
testset CGS DMVc DMVp DMVu
arabic 36.1 42.6 51.9 49.1
basque 28.4 28.9 27.1 30.2
czech 33.1 28.6 38.2 28.3
danish 27.9 36.4 38.4 18.2
dutch 31.0 39.0 41.1 40.3
en-childes 31.2 40.8 44.3 42.1
en-ptb 22.7 25.1 23.1 23.1
portuguese 26.7 38.4 34.5 31.1
slovene 26.3 20.6 19.2 22.6
swedish 29.0 30.9 30.9 26.5
averages 29.3 33.1 34.9 31.1
Undirected
testset CGS DMVc DMVp DMVu
arabic 58.3 52.8 58.3 61.1
basque 49.3 49.2 50.5 50.0
czech 48.7 45.9 57.2 47.9
danish 56.3 60.9 57.0 43.7
dutch 47.0 53.6 57.2 53.8
en-childes 56.3 61.0 62.7 59.9
en-ptb 50.7 52.9 54.1 46.9
portuguese 51.8 61.1 59.4 51.6
slovene 40.5 41.9 45.3 41.2
swedish 52.5 57.1 57.1 48.6
averages 51.1 53.6 55.9 50.5
NED
testset CGS DMVc DMVp DMVu
arabic 62.0 63.0 66.7 67.6
basque 67.4 62.8 62.5 62.3
czech 65.1 60.7 72.0 64.0
danish 72.0 72.4 73.2 60.3
dutch 57.7 64.9 65.0 64.7
en-childes 79.9 79.6 79.9 78.4
en-ptb 67.9 73.4 74.3 63.7
portuguese 58.2 80.7 79.5 72.6
slovene 55.7 52.3 59.4 51.9
swedish 64.8 78.4 78.4 65.7
averages 65.1 68.8 71.1 65.1
Table 9: Evaluation of the joint task on the dependency
output using a maximum sentence length of 10.
78
M-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 75.06 N/A 79.00 79.00 62.20 62.24 61.81 64.60 61.55 65.69 67.82 63.23
basque 71.58 69.20 75.23 75.23 65.97 56.37 64.37 68.58 62.17 63.59 68.22 60.49
czech 74.84 61.53 66.97 76.00 N/A N/A 55.60 72.51 55.01 60.38 73.38 51.96
danish 56.48 55.41 70.28 71.62 49.24 35.32 50.21 66.50 33.57 49.40 61.60 35.02
dutch 80.72 70.13 74.04 76.08 63.37 57.64 57.99 83.76 57.31 68.18 84.64 63.04
en-childes 84.23 77.57 85.35 85.35 N/A N/A 76.34 85.11 59.75 77.55 86.39 59.55
en-ptb 78.26 72.26 62.86 73.46 63.15 56.32 65.10 68.10 48.76 70.31 73.96 47.91
portuguese 76.00 72.05 75.47 77.13 68.40 65.86 65.52 69.61 61.84 64.63 66.81 62.95
slovene 67.29 59.78 72.18 72.71 63.95 55.23 54.85 63.68 52.15 54.08 59.31 45.40
swedish 66.20 67.86 73.55 73.55 60.10 48.43 61.21 61.21 47.51 64.39 64.39 46.04
averages 73.07 67.31 73.49 76.01 62.05 54.68 61.30 70.37 53.96 63.82 70.65 53.56
1-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 50.90 N/A 39.15 39.15 41.49 53.92 39.89 34.23 40.63 50.53 41.98 50.27
basque 42.45 46.25 52.38 52.38 48.91 45.55 41.29 33.49 50.80 43.27 35.73 43.43
czech 31.45 48.24 32.18 31.74 N/A N/A 43.12 33.55 41.94 38.93 28.33 35.31
danish 43.08 43.64 32.17 31.77 40.56 34.83 33.48 30.87 26.33 38.92 30.59 32.95
dutch 43.22 55.85 43.26 39.98 56.45 45.37 48.13 21.88 43.10 55.86 22.42 54.04
en-childes 64.10 63.62 64.50 64.50 N/A N/A 59.96 56.87 59.75 63.43 53.40 57.68
en-ptb 57.63 56.02 45.52 41.35 48.95 53.15 55.43 49.60 47.57 54.10 51.80 45.43
portuguese 59.71 50.18 36.13 35.38 54.42 60.08 49.57 45.00 48.25 46.57 38.37 45.10
slovene 42.62 50.66 33.23 32.59 56.55 50.30 44.97 44.34 40.62 41.24 40.01 38.93
swedish 48.76 40.54 34.07 34.07 38.21 46.32 36.12 36.12 44.57 41.90 41.90 38.05
averages 48.39 50.55 41.26 40.29 48.19 48.69 45.20 38.59 44.36 47.48 38.45 44.12
VM
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 61.59 N/A 52.95 52.95 46.99 47.18 40.75 43.16 40.40 47.95 45.09 45.14
basque 53.83 51.34 54.45 54.45 49.34 44.26 44.18 45.46 45.37 45.02 44.90 42.76
czech 56.80 50.22 45.06 46.76 N/A N/A 42.50 49.93 41.51 45.15 51.38 39.62
danish 61.57 59.00 63.39 63.62 53.35 43.20 51.83 58.38 33.46 52.52 58.44 39.46
dutch 57.82 53.94 55.01 53.40 48.99 46.26 44.08 50.49 44.37 52.02 51.33 47.99
en-childes 80.17 76.59 78.18 78.18 N/A N/A 73.67 76.47 65.44 76.14 76.87 68.25
en-ptb 71.44 68.12 59.90 61.31 63.90 60.04 63.79 63.64 52.96 66.40 66.50 54.33
portuguese 67.49 60.37 54.61 54.74 58.91 59.58 53.30 54.99 50.26 53.15 52.67 50.76
slovene 54.80 52.13 51.85 51.88 52.99 48.55 45.33 48.33 40.13 39.25 45.73 38.68
swedish 61.52 58.23 56.09 56.09 55.02 48.69 51.76 51.76 43.39 54.28 54.28 44.51
averages 62.70 58.88 57.15 57.34 53.69 49.72 51.12 54.26 45.73 53.19 54.72 47.15
VI
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 2.65 N/A 3.76 3.76 3.47 3.19 3.96 4.12 3.73 3.49 3.96 3.48
basque 3.65 3.50 3.78 3.78 3.45 3.36 4.09 4.79 3.67 4.00 4.72 3.83
czech 3.80 3.49 4.96 5.48 N/A N/A 3.92 4.57 3.91 3.85 4.46 4.17
danish 3.76 3.85 4.07 4.08 4.29 4.53 4.54 4.91 5.24 4.45 4.78 4.85
dutch 3.53 3.14 3.31 3.72 3.33 3.47 3.68 5.16 3.61 3.26 5.07 3.39
en-childes 1.86 2.12 2.11 2.11 N/A N/A 2.39 2.32 2.59 2.17 2.31 2.47
en-ptb 2.69 2.90 3.67 4.03 3.16 3.08 3.24 3.41 3.66 3.05 3.19 3.50
portuguese 2.40 2.90 4.01 4.11 2.97 2.74 3.35 3.46 3.35 3.40 3.63 3.37
slovene 3.65 3.40 4.65 4.68 3.34 3.48 3.92 4.23 4.03 4.38 4.51 4.25
swedish 3.36 3.78 4.57 4.57 3.89 3.65 4.45 4.45 4.11 4.17 4.17 4.07
averages 3.13 3.23 3.89 4.03 3.49 3.44 3.75 4.14 3.79 3.62 4.08 3.74
Table 7: One to one, Many to one, VM and VI scores of POS induction results evaluated against fine POS tags (c.f.,
Table 2 which used UPOS).
79
Directed
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8
basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4
czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3
danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2
dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5
en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3
en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9
portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7
slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7
swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3
averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9
Undirected
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 69.4 59.3 59.3 69.4 69.4 59.3 65.7 67.6 52.8 53.7 55.6 54.6 61.1 69.4
basque 65.6 59.5 49.8 50.1 48.4 54.3 32.8 66.4 60.1 58.1 48.5 42.2 56.4 53.9
czech 65.9 59.9 69.2 66.6 67.6 62.3 63.5 70.1 51.6 51.2 50.5 47.2 54.2 56.9
danish 67.9 63.5 70.6 64.4 71.1 67.9 70.7 70.2 65.0 64.3 60.0 56.4 59.7 63.9
dutch 63.2 59.9 57.5 63.3 58.0 58.5 58.5 60.5 62.7 56.7 62.9 51.1 56.3 60.7
en-childes 65.3 70.6 69.4 62.4 63.7 64.3 63.6 69.1 65.7 66.1 59.5 51.2 51.2 51.0
en-ptb 79.4 79.2 65.9 72.5 62.4 62.5 75.1 78.8 53.8 65.3 53.2 52.0 58.8 54.9
portuguese 66.3 81.9 71.6 70.2 62.3 65.8 78.5 72.1 54.0 60.9 54.3 53.3 56.7 63.8
slovene 70.6 63.7 56.3 59.1 55.1 54.8 63.7 72.0 46.4 53.1 46.3 44.9 45.5 46.0
swedish 82.3 73.5 70.1 71.1 75.4 66.5 77.3 83.7 64.5 64.5 66.1 59.2 59.2 59.5
averages 69.6 67.1 64.0 64.9 63.3 61.6 64.9 71.0 57.7 59.4 55.7 51.2 55.9 58.0
NED
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 78.7 68.5 66.7 75.9 75.9 68.5 72.2 71.3 61.1 63.0 63.0 63.0 64.8 75.9
basque 77.9 68.6 62.9 65.2 64.2 70.1 55.0 79.5 69.3 69.0 63.8 58.2 73.2 64.0
czech 79.9 72.6 81.8 78.6 79.9 76.0 78.1 81.2 62.9 61.7 63.6 60.3 63.2 73.8
danish 81.7 76.3 83.2 78.4 84.3 77.3 84.4 85.0 77.9 75.8 69.5 67.8 66.2 77.5
dutch 71.0 71.8 77.1 78.4 76.8 72.6 68.5 71.0 73.2 64.6 73.0 60.5 64.4 71.0
en-childes 82.4 81.6 84.5 76.7 74.8 79.0 84.9 82.5 79.9 80.4 79.9 70.1 63.2 76.9
en-ptb 86.7 89.4 87.5 88.4 84.0 78.7 87.1 84.3 64.0 80.7 64.2 64.3 65.2 75.0
portuguese 78.2 90.7 87.8 87.8 87.5 82.9 91.9 90.2 75.6 81.7 76.5 67.7 60.9 82.4
slovene 79.3 76.8 70.8 72.8 70.4 68.1 78.9 79.8 59.4 62.2 59.4 55.8 54.3 62.2
swedish 91.7 85.8 83.1 85.6 87.1 80.8 87.6 92.1 76.1 76.1 76.4 75.3 67.2 79.1
averages 80.8 78.2 78.5 78.8 78.5 75.4 78.9 81.7 69.9 71.5 68.9 64.3 64.3 73.8
Table 8: Evaluation of the dependency task using a maximum sentence length of 10. See also Table 3 which presents
the same results with no length restriction.
80
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74?82,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
?Not not bad? is not ?bad?: A distributional account of negation
Karl Moritz Hermann Edward Grefenstette
University of Oxford Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, United Kingdom
firstname.lastname@cs.ox.ac.uk
Phil Blunsom
Abstract
With the increasing empirical success of
distributional models of compositional se-
mantics, it is timely to consider the types
of textual logic that such models are ca-
pable of capturing. In this paper, we ad-
dress shortcomings in the ability of cur-
rent models to capture logical operations
such as negation. As a solution we pro-
pose a tripartite formulation for a continu-
ous vector space representation of seman-
tics and subsequently use this representa-
tion to develop a formal compositional no-
tion of negation within such models.
1 Introduction
Distributional models of semantics characterize
the meanings of words as a function of the words
they co-occur with (Firth, 1957). These models,
mathematically instantiated as sets of vectors in
high dimensional vector spaces, have been applied
to tasks such as thesaurus extraction (Grefenstette,
1994; Curran, 2004), word-sense discrimination
(Schu?tze, 1998), automated essay marking (Lan-
dauer and Dumais, 1997), and so on.
During the past few years, research has shifted
from using distributional methods for modelling
the semantics of words to using them for mod-
elling the semantics of larger linguistic units such
as phrases or entire sentences. This move from
word to sentence has yielded models applied to
tasks such as paraphrase detection (Mitchell and
Lapata, 2008; Mitchell and Lapata, 2010; Grefen-
stette and Sadrzadeh, 2011; Blacoe and Lapata,
2012), sentiment analysis (Socher et al, 2012;
Hermann and Blunsom, 2013), and semantic re-
lation classification (ibid.). Most efforts approach
the problem of modelling phrase meaning through
vector composition using linear algebraic vector
operations (Mitchell and Lapata, 2008; Mitchell
and Lapata, 2010; Zanzotto et al, 2010), matrix
or tensor-based approaches (Baroni and Zampar-
elli, 2010; Coecke et al, 2010; Grefenstette et al,
2013; Kartsaklis et al, 2012), or through the use
of recursive auto-encoding (Socher et al, 2011;
Hermann and Blunsom, 2013) or neural-networks
(Socher et al, 2012). On the non-compositional
front, Erk and Pado? (2008) keep word vectors sep-
arate, using syntactic information from sentences
to disambiguate words in context; likewise Turney
(2012) treats the compositional aspect of phrases
and sentences as a matter of similarity measure
composition rather than vector composition.
These compositional distributional approaches
often portray themselves as attempts to recon-
cile the empirical aspects of distributional seman-
tics with the structured aspects of formal seman-
tics. However, they in fact only principally co-opt
the syntax-sensitivity of formal semantics, while
mostly eschewing the logical aspects.
Expressing the effect of logical operations in
high dimensional distributional semantic models
is a very different task than in boolean logic. For
example, whereas predicates such as ?red? are seen
in predicate calculi as functions mapping elements
of some set Mred to > (and all other domain ele-
ments to ?), in compositional distributional mod-
els we give the meaning of ?red? a vector-like
representation, and devise some combination op-
eration with noun representations to obtain the
representation for an adjective-noun pair. Under
the logical view, negation of a predicate therefore
yields a new truth-function mapping elements of
the complement of Mred to > (and all other do-
main elements to?), but the effect of negation and
other logical operations in distributional models is
not so sharp: we expect the representation for ?not
red? to remain close to other objects of the same
domain of discourse (i.e. other colours) while be-
ing sufficiently different from the representation of
?red? in some manner. Exactly how textual logic
74
would best be represented in a continuous vector
space model remains an open problem.
In this paper we propose one possible formu-
lation for a continuous vector space based repre-
sentation of semantics. We use this formulation
as the basis for providing an account of logical
operations for distributional models. In particu-
lar, we focus on the case of negation and how it
might work in higher dimensional distributional
models. Our formulation separates domain, value
and functional representation in such a way as to
allow negation to be handled naturally. We ex-
plain the linguistic and model-related impacts of
this mode of representation and discuss how this
approach could be generalised to other semantic
functions.
In Section 2, we provide an overview of work
relating to that presented in this paper, covering
the integration of logical elements in distributional
models, and the integration of distributional el-
ements in logical models. In Section 3, we in-
troduce and argue for a tripartite representation
in distributional semantics, and discuss the issues
relating to providing a linguistically sensible no-
tion of negation for such representations. In Sec-
tion 4, we present matrix-vector models similar to
that of Socher et al (2012) as a good candidate
for expressing this tripartite representation. We
argue for the elimination of non-linearities from
such models, and thus show that negation cannot
adequately be captured. In Section 5, we present
a short analysis of the limitation of these matrix-
vector models with regard to the task of modelling
non-boolean logical operations, and present an im-
proved model bypassing these limitations in Sec-
tion 6. Finally, in Section 7, we conclude by sug-
gesting future work which will extend and build
upon the theoretical foundations presented in this
paper.
2 Motivation and Related Work
The various approaches to combining logic with
distributional semantics can broadly be put into
three categories: those approaches which use
distributional models to enhance existing logical
tools; those which seek to replicate logic with the
mathematical constructs of distributional models;
and those which provide new mathematical defini-
tions of logical operations within distributional se-
mantics. The work presented in this paper is in the
third category, but in this section we will also pro-
vide a brief overview of related work in the other
two in order to better situate the work this paper
will describe in the literature.
Vector-assisted logic The first class of ap-
proaches seeks to use distributional models of
word semantics to enhance logic-based models of
textual inference. The work which best exempli-
fies this strand of research is found in the efforts of
Garrette et al (2011) and, more recently, Beltagy
et al (2013). This line of research converts logi-
cal representations obtained from syntactic parses
using Bos? Boxer (Bos, 2008) into Markov Logic
Networks (Richardson and Domingos, 2006), and
uses distributional semantics-based models such
as that of Erk and Pado? (2008) to deal with issues
polysemy and ambiguity.
As this class of approaches deals with improv-
ing logic-based models rather than giving a dis-
tributional account of logical function words, we
view such models as orthogonal to the effort pre-
sented in this paper.
Logic with vectors The second class of ap-
proaches seeks to integrate boolean-like logical
operations into distributional semantic models us-
ing existing mechanisms for representing and
composing semantic vectors. Coecke et al (2010)
postulate a mathematical framework generalising
the syntax-semantic passage of Montague Gram-
mar (Montague, 1974) to other forms of syntac-
tic and semantic representation. They show that
the parses yielded by syntactic calculi satisfying
certain structural constraints can be canonically
mapped to vector combination operations in dis-
tributional semantic models. They illustrate their
framework by demonstrating how the truth-value
of sentences can be obtained from the combina-
tion of vector representations of words and multi-
linear maps standing for logical predicates and re-
lations. They furthermore give a matrix interpre-
tation of negation as a ?swap? matrix which in-
verts the truth-value of vectorial sentence repre-
sentations, and show how it can be embedded in
sentence structure.
Recently, Grefenstette (2013) showed that the
examples from this framework could be extended
to model a full quantifier-free predicate logic using
tensors of rank 3 or lower. In parallel, Socher et
al. (2012) showed that propositional logic can be
learned using tensors of rank 2 or lower (i.e. only
matrices and vectors) through the use of non-linear
75
activation functions in recursive neural networks.
The work of Coecke et al (2010) and Grefen-
stette (2013) limits itself to defining, rather than
learning, distributional representations of logical
operators for distributional models that simulate
logic, and makes no pretense to the provision of
operations which generalise to higher-dimensional
distributional semantic representations. As for
the non-linear approach of Socher et al (2012),
we will discuss, in Section 4 below, the limita-
tions with this model with regard to the task of
modelling logic for higher dimensional represen-
tations.
Logic for vectors The third and final class of
approaches is the one the work presented here
belongs to. This class includes attempts to de-
fine representations for logical operators in high
dimensional semantic vector spaces. Such ap-
proaches do not seek to retrieve boolean logic and
truth values, but to define what logical operators
mean when applied to distributional representa-
tions. The seminal work in this area is found in the
work of Widdows and Peters (2003), who define
negation and other logical operators algebraically
for high dimensional semantic vectors. Negation,
under this approach, is effectively a binary rela-
tion rather than a unary relation: it expresses the
semantics of statements such as ?A NOT B? rather
than merely ?NOT B?, and does so by projecting
the vector for A into the orthogonal subspace of
the vector for B. This approach to negation is use-
ful for vector-based information retrieval models,
but does not necessarily capture all the aspects of
negation we wish to take into consideration, as
will be discussed in Section 3.
3 Logic in text
In order to model logical operations over semantic
vectors, we propose a tripartite meaning represen-
tation, which combines the separate and distinct
treatment of domain-related and value-related as-
pects of semantic vectors with a domain-driven
syntactic functional representation. This is a unifi-
cation of various recent approaches to the problem
of semantic representation in continuous distribu-
tional semantic modelling (Socher et al, 2012;
Turney, 2012; Hermann and Blunsom, 2013).
We borrow from Socher et al (2012) and oth-
ers (Baroni and Zamparelli, 2010; Coecke et al,
2010) the idea that the information words refer to
is of two sorts: first the semantic content of the
word, which can be seen as the sense or reference
to the concept the word stands for, and is typi-
cally modelled as a semantic vector; and second,
the function the word has, which models the effect
the word has on other words it combines with in
phrases and sentences, and is typically modelled
as a matrix or higher-order tensor. We borrow
from Turney (2012) the idea that the semantic as-
pect of a word should not be modelled as a single
vector where everything is equally important, but
ideally as two or more vectors (or, as we do here,
two or more regions of a vector) which stand for
the aspects of a word relating to its domain, and
those relating to its value.
We therefore effectively suggest a tripartite rep-
resentation of the semantics of words: a word?s
meaning is modelled by elements representing its
value, domain, and function, respectively.
The tripartite representation We argue that the
tripartite representation suggested above allows us
to explicitly capture several aspects of semantics.
Further, while there may be additional distinct as-
pects of semantics, we argue that this is a minimal
viable representation.
First of all, the differentiation between do-
main and value is useful for establishing similar-
ity within subspaces of meaning. For instance,
the words blue and red share a common domain
(colours) while having very different values. We
hypothesise that making this distinction explicit
will allow for the definition of more sophisticated
and fine-grained semantics of logical operations,
as discussed below. Although we will represent
domain and value as two regions of a vector, there
is no reason for these not to be treated as separate
vectors at the time of comparison, as done by Tur-
ney (2012).
Through the third part, the functional repre-
sentation, we capture the compositional aspect of
semantics: the functional representation governs
how a term interacts with its environment. In-
spired by the distributional interpretation (Baroni
and Zamparelli, 2010; Coecke et al, 2010) of
syntactically-paramatrized semantic composition
functions from Montogovian semantics (Mon-
tague, 1974), we will also assume the function part
of our representation to be parametrized princi-
pally by syntax and domain rather than value. The
intuition behind taking domain into account in ad-
dition to syntactic class being that all members of
a domain largely interact with their environment
76
in the same fashion.
Modeling negation The tripartite representation
proposed above allows us to define logical opera-
tions in more detail than competing approaches.
To exemplify this, we focus on the case of nega-
tion.
We define negation for semantic vectors to be
the absolute complement of a term in its domain.
This implies that negation will not affect the do-
main of a term but only its value. Thus, blue and
not blue are assumed to share a common domain.
We call this naive form of negation the inversion
of a term A, which we idealise as the partial inver-
sion Ainv of the region associated with the value
of the word in its vector representation A.
?
?
d
v
v
?
?
?
?
d
v
?v
?
?
?
?
d
v
??v
?
?
[
f
] [
f
] [
f
]
W Winv ?W
Figure 1: The semantic representations of a word
W , its inverse W inv and its negation ?W . The
domain part of the representation remains un-
changed, while the value part will partially be in-
verted (inverse), or inverted and scaled (negation)
with 0 < ? < 1. The (separate) functional repre-
sentation also remains unchanged.
Additionally, we expect negation to have a
diminutive effect. This diminutive effect is best
exemplified in the case of sentiment: good is more
positive than not bad, even though good and bad
are antonyms of each other. By extension not not
good and not not not bad end up somewhere in the
middle?qualitative statements still, but void of
any significant polarity. To reflect this diminutive
effect of negation and double negation commonly
found in language, we define the idealised diminu-
tive negation ?A of a semantic vectorA as a scalar
inversion over a segment of the value region of its
representation with the scalar ? : 0 < ? < 1, as
shown in Figure 1.
As we defined the functional part of our rep-
resentation to be predominately parametrized by
syntax and domain, it will remain constant under
negation and inversion.
4 A general matrix-vector model
Having discussed, above, how the vector compo-
nent of a word can be partitioned into domain and
value, we now turn to the partition between se-
mantic content and function. A good candidate for
modelling this partition would be a dual-space rep-
resentation similar to that of Socher et al (2012).
In this section, we show that this sort of represen-
tation is not well adapted to the modelling of nega-
tion.
Models using dual-space representations have
been proposed in several recent publications, no-
tably in Turney (2012) and Socher et al (2012).
We use the class of recursive matrix-vector mod-
els as the basis for our investigation; for a detailed
introduction see the MV-RNN model described in
Socher et al (2012).
We begin by describing composition for a gen-
eral dual-space model, and apply this model to the
notion of compositional logic in a tripartite repre-
sentation discussed earlier. We identify the short-
comings of the general model and subsequently
discuss alternative composition models and mod-
ifications that allow us to better capture logic in
vector space models of meaning.
Assume a basic model of compositionality for
such a tripartite representation as follows. Each
term is encoded by a semantic vector v captur-
ing its domain and value, as well as a matrix M
capturing its function. Thus, composition consists
of two separate operations to learn semantics and
function of the composed term:
vp = fv(va,vb,Ma,Mb) (1)
Mp = fM (Ma,Mb)
As we defined the functional representation to be
parametrized by syntax and domain, its compo-
sition function does not require va and vb as in-
puts, with all relevant information already being
contained in Ma,Mb. In the case of Socher et al
(2012) these functions are as follows:
Mp =WM
[
Ma
Mb
]
(2)
vp = g
(
Wv
[
Mavb
Mbva
])
(3)
where g is a non-linearity.
4.1 The question of non-linearities
While the non-linearity g could be equipped with
greater expressive power, such as in the boolean
77
logic experiment in Socher et al (2012)), the aim
of this paper is to place the burden of composition-
ality on the atomic representations instead. For
this reason we treat g as an identity function, and
WM , Wv as simple additive matrices in this inves-
tigation, by setting
g = I Wv =WM = [I I]
where I is an identity matrix. This simplification
is justified for several reasons.
A simple non-linearity such as the commonly
used hyperbolic tangent or sigmoid function will
not add sufficient power to overcome the issues
outlined in this paper. Only a highly complex non-
linear function would be able to satisfy the require-
ments for vector space based logic as discussed
above. Such a function would defeat the point
however, by pushing the ?heavy-lifting? from the
model structure into a separate function.
Furthermore, a non-linearity effectively en-
codes a scattergun approach: While it may have
the power to learn a desired behaviour, it similarly
has a lot of power to learn undesired behaviours
and side effects. From a formal perspective it
would therefore seem more prudent to explicitly
encode desired behaviour in a model?s structure
rather than relying on a non-linearity.
4.2 Negation
We have outlined our formal requirements for
negation in the previous section. From these re-
quirements we can deduce four equalities, con-
cerning the effect of negation and double nega-
tion on the semantic representation and function
of a term. The matrices J? and J? (illustrated in
?
?
?
?
?
?
?
?
?
?
1
. . . 0
1
??
0
. . .
??
?
?
?
?
?
?
?
?
?
?
Figure 2: A partially scaled and inverted identity
matrix J?. Such a matrix can be used to trans-
form a vector storing a domain and value repre-
sentation into one containing the same domain but
a partially inverted value, such as W and ?W de-
scribed in Figure 1.
Figure 2) describe a partially scaled and inverted
identity matrix, where 0 < ?, ? < 1.
fv(not, a) = J?va (4)
fM (not, a) ?Ma (5)
fv(not, fv(not, a)) = J?J?va (6)
fM (not, fM (not, a)) ?Ma (7)
Based on our assumption about the constant do-
main and interaction across negation, we can re-
place the approximate equality with a strict equal-
ity in Equations 5 and 7. Further, we assume that
both Ma 6= I and Ma 6= 0, i.e. that A has a spe-
cific and non-zero functional representation. We
make a similar assumption for the semantic repre-
sentation va 6= 0.
Thus, to satisfy the equalities in Equations 4
through 7, we can deduce the values of vnot and
Mnot as discussed below.
Value and Domain in Negation Under the sim-
plifications of the model explained earlier, we
know that the following is true:
fv(a, b) = g
(
Wv
[
Mavb
Mbva
])
= I
(
[
I I
]
[
Mavb
Mbva
])
=Mavb +Mbva
I.e. the domain and value representation of a par-
ent is the sum of the two Mv multiplications of
its children. The matrix Wv could re-weight this
addition, but that would not affect the rest of this
analysis.
Given the idea that the domain stays constant
under negation and that a part of the value is in-
verted and scaled, we further know that these two
equations hold:
?a ? A : fv(not, a) = J?va
?a ? A : fv(not, fv(not, a)) = J?J?va
Assuming that both semantic and functional
representation across all A varies and is non-zero,
these equalities imply the following conditions for
the representation of not:
Mnot = J? = J?
vnot = 0
These two equations suggest that the term not has
no inherent value (vnot = 0), but merely acts as a
function, inverting part of another terms semantic
representation (Mnot = J?).
78
Functional Representation in Negation We
can apply the same method to the functional rep-
resentation. Here, we know that:
fM (a, b) =WM
[
Ma
Mb
]
=
[
I I
]
[
Ma
Mb
]
=Ma +Mb
Further, as defined in our discussion of nega-
tion, we require the functional representation to
remain unchanged under negation:
?a ? A : fM (not, a) =Ma
?a ? A : fM (not, fM (not, a)) =Ma
These requirements combined leave us to con-
clude that Mnot = 0. Combined with the result
from the first part of the analysis, this causes a
contradiction:
Mnot = 0
Mnot = J?
=? J? = 0 
This demonstrates that the MV-RNN as de-
scribed in this paper is not capable of modelling
semantic logic according to the principles we out-
lined. The fact that we would require Mnot = 0
further supports the points made earlier about the
non-linearities and setting WM to
[
I I
]
. Even a
specific WM and non-linearity would not be able
to ensure that the functional representation stays
constant under negation given a non-zero Mnot.
Clearly, any other complex semantic represen-
tation would suffer from the same issue here?the
failure of double-negation to revert a representa-
tion to its (diminutive) original.
5 Analysis
The issue identified with the MV-RNN style mod-
els described above extends to a number of other
models of vector spaced compositionality. It can
be viewed as a problem of uninformed composi-
tion caused by a composition function that fails to
account for syntax and thus for scope.
Of course, identifying the scope of negation is a
hard problem in its own right?see e.g. the *SEM
2012 shared task (Morante and Blanco, 2012).
However, at least for simple cases, we can deduce
scope by considering the parse tree of a sentence:
S
VP
ADJP
JJ
blue
RB
not
VBZ
is
NP
N
car
Det
This
Figure 3: The parse tree for This car is not blue,
highlighting the limited scope of the negation.
If we consider the parse tree for this car is not blue,
it is clear that the scope of the negation expressed
includes the colour but not the car (Figure 3).
While the MV-RNN model in Socher et al
(2012) incorporates parse trees to guide the order
of its composition steps, it uses a single composi-
tion function across all steps. Thus, the functional
representation of not will to some extent propagate
outside of its scope, leading to a vector capturing
something that is not blue, but also not quite a car.
There are several possibilities for addressing
this issue. One possibility is to give greater weight
to syntax, for instance by parametrizing the com-
position functions fv and fM on the parse struc-
ture. This could be achieved by using specific
weight matrices Wv and WM for each possible
tag. While the power of this approach is limited
by the complexity of the parse structure, it would
be better able to capture effects such as the scoping
and propagation of functional representations.
Another approach, which we describe in greater
detail in the next section, pushes the issue of
propagation onto the word level. While both ap-
proaches could easily be combined, this second
option is more consistent with our aim of avoid-
ing the implicit encoding of logic into fixed model
parameters in favour of the explicit encoding in
model structure.
6 An improved model
As we outlined in this paper, a key requirement
for a compositional model motivated by formal se-
mantics is the ability to propagate functional rep-
resentations, but also to not propagate these repre-
sentations when doing so is not semantically ap-
propriate. Here, we propose a modification of the
MV-RNN class of models that can capture this dis-
79
tinction without the need to move the composition
logic into the non-linearity.
We add a parameter ? to the representation of
each word, controlling the degree to which its
functional representation propagates after having
been applied in its own composition step.
Thus, the composition step of the new model
requires three equations:
Mp =WM
[
?a
?a+?b
Ma
?b
?a+?b
Mb
]
(8)
vp = g
(
Wv
[
Mavb
Mbva
])
(9)
?p = max(?a, ?b) (10)
Going back to the discussion of negation, this
model has the clear advantage of being able to cap-
ture negation in the way we defined it. As fv(a, b)
is unchanged, these two equations still hold:
Mnot = J? = J?
vnot = 0
However, as fM (a, b) is changed, the second
set of equations changes. We use Z as the ?-
denominator (Z = ?a + ?B) for simplification:
fM (a, b) =WM
[?a
Z Ma
?b
Z Mb
]
=
[
I
I
] [?a
Z Ma
?b
Z Mb
]
=
?a
Z
Ma +
?b
Z
Mb
Further, we still require the functional representa-
tion to remain constant under negation:
?a ? A : fM (not, a) =Ma
?a ? A : fM (not, fM (not, a)) =Ma
Thus, we can infer the following two conditions
on the new model:
?not
Z
Mnot ? 0
?a
Z
Ma ?Ma
From our previous investigation we already know
that Mnot = J? 6= 0, i.e. that not has a non-
zero functional representation. While this caused
a contradiction for the original MV-RNN model,
the design of the improved model can resolve this
issue through the ?-parameter:
?not = 0
Thus, we can use this modified MV-RNN model
to represent negation according to the principles
outlined in this paper. The result ?not = 0 is in
accordance with our intuition about the propaga-
tion of functional aspects of a term: We commonly
expect negation to directly affect the things un-
der its scope (not blue) by choosing their semantic
complement. However, this behaviour should not
propagate outside of the scope of the negation. A
not blue car is still very much a car, and when a
film is not good, it is still very much a film.
7 Discussion and Further Work
In this paper, we investigated the capability of con-
tinuous vector space models to capture the seman-
tics of logical operations in non-boolean cases.
Recursive and recurrent vector models of meaning
have enjoyed a considerable amount of success in
recent years, and have been shown to work well on
a number of tasks. However, the complexity and
subsequent power of these models comes at the
price that it can be difficult to establish which as-
pect of a model is responsible for what behaviour.
This issue was recently highlighted by an inves-
tigation into recursive autoencoders for sentiment
analysis (Scheible and Schu?tze, 2013). Thus, one
of the key challenges in this area of research is the
question of how to control the power of these mod-
els. This challenge motivated the work in this pa-
per. By removing non-linearities and other param-
eters that could disguise model weaknesses, we fo-
cused our work on the basic model design. While
such features enhance model power, they should
not be used to compensate for inherently flawed
model designs.
As a prerequisite for our investigation we estab-
lished a suitable encoding of textual logic. Distri-
butional representations have been well explained
on the word level, but less clarity exists as to the
semantic content of compositional vectors. With
the tripartite meaning representation we proposed
one possible approach in that direction, which we
subsequently expanded by discussing how nega-
tion should be captured in this representation.
Having established a suitable and rigorous sys-
tem for encoding meaning in compositional vec-
tors, we were thus able to investigate the repre-
80
sentative power of the MV-RNN model. We fo-
cused this paper on the case of negation, which
has the advantage that it does not require many
additional assumptions about the underlying se-
mantics. Our investigation showed that the basic
MV-RNN model is incompatible with our notion
of negation and thus with any textual logic build-
ing on this proposal.
Subsequently, we analysed the reasons for this
failure. We explained how the issue of nega-
tion affects the general class of MV-RNN models.
Through the issue of double-negation we further
showed how this issue is largely independent on
the particular semantic encoding used. Based on
this analysis we proposed an improved model that
is able to capture such textual logic.
In summary, this paper has two key contribu-
tions. First, we developed a tripartite represen-
tation for vector space based models of seman-
tics, incorporating multiple previous approaches
to this topic. Based on this representation, the
second contribution of this paper was a modified
MV-RNN model that can capture effects such as
negation in its inherent structure.
In future work, we would like to build on the
proposals in this paper, both by extending our
work on textual logic to include formulations for
e.g. function words, quantifiers, or locative words.
Similarly, we plan to experimentally validate these
ideas. Possible tasks for this include sentiment
analysis and relation extraction tasks such as in
Socher et al (2012) but also more specific tasks
such as the *SEM shared task on negation scope
and reversal (Morante and Blanco, 2012).
Acknowledgements
The first author is supported by the UK Engineer-
ing and Physical Sciences Research Council (EP-
SRC). The second author is supported by EPSRC
Grant EP/I03808X/1.
References
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
I. Beltagy, C. Chau, G. Boleda, D. Garrette, E. Erk, and
R. Mooney. 2013. Montague meets markov: Deep
semantics with probabilistic logical form. June.
W. Blacoe and M. Lapata. 2012. A comparison of
vector-based representations for semantic composi-
tion. Proceedings of the 2012 Conference on Empir-
ical Methods in Natural Language Processing.
J. Bos. 2008. Wide-coverage semantic analysis with
boxer. In Proceedings of the 2008 Conference on
Semantics in Text Processing, pages 277?286. Asso-
ciation for Computational Linguistics.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical Foundations for a Compositional Distribu-
tional Model of Meaning. March.
J. R. Curran. 2004. From distributional to semantic
similarity. Ph.D. thesis.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. Proceedings
of the Conference on Empirical Methods in Natural
Language Processing - EMNLP ?08, (October):897.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis.
D. Garrette, K. Erk, and R. Mooney. 2011. Integrating
logical representations with probabilistic informa-
tion using markov logic. In Proceedings of the Ninth
International Conference on Computational Seman-
tics, pages 105?114. Association for Computational
Linguistics.
E. Grefenstette and M. Sadrzadeh. 2011. Experi-
mental support for a categorical compositional dis-
tributional model of meaning. In Proceedings of
EMNLP, pages 1394?1404.
E. Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning
for compositional distributional semantics. In Pro-
ceedings of the Tenth International Conference on
Computational Semantics. Association for Compu-
tational Linguistics.
E. Grefenstette. 2013. Towards a formal distributional
semantics: Simulating logical calculi with tensors.
Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery.
K. M. Hermann and P. Blunsom. 2013. The role of
syntax in vector space models of compositional se-
mantics. In Proceedings of ACL, Sofia, Bulgaria,
August. Association for Computational Linguistics.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2012. A
unified sentence space for categorical distributional-
compositional semantics: Theory and experiments.
In Proceedings of 24th International Conference
on Computational Linguistics (COLING 2012):
Posters, pages 549?558, Mumbai, India, December.
81
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
volume 8.
J. Mitchell and M. Lapata. 2010. Composition in Dis-
tributional Models of Semantics. Cognitive Science.
R. Montague. 1974. English as a Formal Language.
Formal Semantics: The Essential Readings.
R. Morante and E. Blanco. 2012. *SEM 2012 shared
task: resolving the scope and focus of negation. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, SemEval
?12, pages 265?274, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine learning, 62(1-2):107?136.
C. Scheible and H. Schu?tze. 2013. Cutting recursive
autoencoder trees. In Proceedings of the Interna-
tional Conference on Learning Representations.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational linguistics, 24(1):97?123.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011. Dynamic pooling and un-
folding recursive autoencoders for paraphrase detec-
tion. Advances in Neural Information Processing
Systems, 24:801?809.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1201?1211.
P. D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
D. Widdows and S. Peters. 2003. Word vectors and
quantum logic: Experiments with negation and dis-
junction. Mathematics of language, 8(141-154).
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and
S. Manandhar. 2010. Estimating linear models for
compositional distributional semantics. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 1263?1271. Associa-
tion for Computational Linguistics.
82
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 119?126,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of sen-
tences, so do sentences combine to form
the meaning of paragraphs, dialogues and
general discourse. We introduce both a
sentence model and a discourse model cor-
responding to the two levels of composi-
tionality. The sentence model adopts con-
volution as the central operation for com-
posing semantic vectors and is based on
a novel hierarchical convolutional neural
network. The discourse model extends the
sentence model and is based on a recur-
rent neural network that is conditioned in
a novel way both on the current sentence
and on the current speaker. The discourse
model is able to capture both the sequen-
tiality of sentences and the interaction be-
tween different speakers. Without feature
engineering or pretraining and with simple
greedy decoding, the discourse model cou-
pled to the sentence model obtains state of
the art performance on a dialogue act clas-
sification experiment.
1 Introduction
There are at least two levels at which the mean-
ing of smaller linguistic units is composed to form
the meaning of larger linguistic units. The first
level is that of sentential compositionality, where
the meaning of words composes to form the mean-
ing of the sentence or utterance that contains them
(Frege, 1892). The second level extends beyond
the first and involves general discourse composi-
tionality, where the meaning of multiple sentences
or utterances composes to form the meaning of
the paragraph, document or dialogue that com-
prises them (Korta and Perry, 2012; Potts, 2011).
The problem of discourse compositionality is the
problem of modelling how the meaning of general
discourse composes from the meaning of the sen-
tences involved and, since the latter in turn stems
from the meaning of the words, how the meaning
of discourse composes from the words themselves.
Tackling the problem of discourse composition-
ality promises to be central to a number of differ-
ent applications. These include sentiment or topic
classification of single sentences within the con-
text of a longer discourse, the recognition of di-
alogue acts within a conversation, the classifica-
tion of a discourse as a whole and the attainment
of general unsupervised or semi-supervised repre-
sentations of a discourse for potential use in di-
alogue tracking and question answering systems
and machine translation, among others.
To this end much work has been done on mod-
elling the meaning of single words by way of se-
mantic vectors (Turney and Pantel, 2010; Col-
lobert and Weston, 2008) and the latter have found
applicability in areas such as information retrieval
(Jones et al, 2006). With regard to modelling
the meaning of sentences and sentential compo-
sitionality, recent proposals have included sim-
ple additive and multiplicative models that do
not take into account sentential features such as
word order or syntactic structure (Mitchell and
Lapata, 2010), matrix-vector based models that
do take into account such features but are lim-
ited to phrases of a specific syntactic type (Ba-
roni and Zamparelli, 2010) and structured mod-
els that fully capture such features (Grefenstette et
al., 2011) and are embedded within a deep neu-
ral architecture (Socher et al, 2012; Hermann and
Blunsom, 2013). It is notable that the additive
and multiplicative models as well as simple, non-
compositional bag of n-grams and word vector av-
eraging models have equalled or outperformed the
structured models at certain phrase similarity (Bla-
coe and Lapata, 2012) and sentiment classifica-
119
tion tasks (Scheible and Schu?tze, 2013; Wang and
Manning, 2012).
With regard to discourse compositionality, most
of the proposals aimed at capturing semantic as-
pects of paragraphs or longer texts have focused
on bag of n-grams or sentence vector averaging
approaches (Wang and Manning, 2012; Socher et
al., 2012). In addition, the recognition of dialogue
acts within dialogues has largely been treated in
non-compositional ways by way of language mod-
els coupled to hidden Markov sequence models
(Stolcke et al, 2000). Principled approaches to
discourse compositionality have largely been un-
explored.
We introduce a novel model for sentential com-
positionality. The composition operation is based
on a hierarchy of one dimensional convolutions.
The convolutions are applied feature-wise, that is
they are applied across each feature of the word
vectors in the sentence. The weights adopted in
each convolution are different for each feature, but
do not depend on the different words being com-
posed. The hierarchy of convolution operations
involves a sequence of convolution kernels of in-
creasing sizes (Fig. 1). This allows for the com-
position operation to be applied to sentences of
any length, while keeping the model at a depth
of roughly ?2l where l is the length of the sen-
tence. The hierarchy of feature-wise convolution
operations followed by sigmoid non-linear acti-
vation functions results in a hierarchical convo-
lutional neural network (HCNN) based on a con-
volutional architecture (LeCun et al, 2001). The
HCNN shares with the structured models the as-
pect that it is sensitive to word order and adopts a
hierarchical architecture, although it is not based
on explicit syntactic structure.
We also introduce a novel model for discourse
compositionality. The discourse model is based
on a recurrent neural network (RNN) architecture
that is a powerful model for sequences (Sutskever
et al, 2011; Mikolov et al, 2010). The model
aims at capturing two central aspects of discourse
and its meaning: the sequentiality of the sentences
or utterances in the discourse and, where applica-
ble, the interactions between the different speak-
ers. The underlying RNN has its recurrent and out-
put weights conditioned on the respective speaker,
while simultaneously taking as input at every turn
the sentence vector for the current sentence gener-
ated through the sentence model (Fig. 2).
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential co positionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.
We here introduce
1 Introduction
2 Compositionality Models
2.1 Sentenc Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were tak n from th formatting instructions
of the International Joint Conference on Artificial
Intelligence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 General Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.
6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.
We here introduce
1 Introduction
2 Compositionality Models
2.1 Sentence Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference n Artificial
Intell gence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 General Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.
6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to f rmat
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox. .uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just s words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.
We here i troduce
1 Introduction
2 Compositionality Models
2.1 Sentence Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and arlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference on Artificial
Intelligence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 General Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Typ sing e-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.
6.1 Electronically-available sources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to f rmat
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so d sentences in turn combin se-
quentially to form the meaning of general
discourse. Discourse ay take the form
of paragraph , soliloqui or conversations
between multiple speakers. The problem
of ross-sentential compositionality is the
problem of modelling how the me ning
of th various forms of discourse arises
from the meaning of the utterances and t
words inv lved.
We here introduce
1 Introduction
2 Compositionality Models
2.1 Sentence Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceeding , includ ng those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Cha g and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou N
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and e rlier ACL and
EACL formats. Those versions re written by
sever l people, including John Chen, Henry S.
T ompson and Donald Walker. Additional le-
ments were t ken from the formatting instructions
of the International Joint Conferenc on Artificial
Intelligence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 G neral Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-widt figures or tables (see the guidelines
in Subsectio 6.5). Type single-spaced. Start
all pag s directly under the top margin. S e the
guidelines later regarding formatting the first pag .
The manuscript should be printed si gle-sided and
its len th should not xceed the maximum pag
limit described in Section 8. Do not number the
pages.
6.1 Electronically-availabl resources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format
Figur 1: A hierarchical convolutional neural net-
work f r sentential compositionality. The bottom
layer represents a single feature across all the word
vectors in the sentence. The top layer is the value
for that feature in the r ulting sent nc vector.
Lines represent single weights and color coded
lines indicate sharing of weights. The parameter
k indicates the size of the convolution kernel at
the corresponding layer.
We experiment with the discourse model cou-
pled to the sentence model on the task of recog-
nizing dialogue acts of utterances within a conver-
sation. The dataset is given by 1134 transcribed
and annotated telephone conversations amounting
to about 200K utterances from the Switchboard
Dialogue Act Corpus (Calhoun et al, 2010).1 The
model is trained in a supervised setting without
previous pretraining; word vectors are also ran-
domly initialised. The model learns a probability
distribution over the dialogue acts at step i given
the sequence of utterances up to step i, the se-
quence of acts up to the previous step i?1 and the
binary sequence of agents up to the current step
i. Predicting the sequence of dialogue acts is per-
formed in a greedy fashion.2
We proceed as follows. In Sect. 2 we give the
motivation and the definition for the HCNN sen-
tence model. In Sect. 3 we do the same for the
RCNN discourse model. In Sect. 4 we describe
the dialogue act classification experiment and the
training procedure. We also inspect the discourse
vector representations produced by the model. We
conclude in Sect. 5.
1The dataset is available at compprag.
christopherpotts.net/swda.html
2Code and trained model available at nal.co
120
SI
O
i
x
i-1
P(x )
i
i-1
H
Hello
HAL 
do you read me
s
i
Figure 2: Recurrent convolutional neural network
(RCNN) discourse model based on a RNN archi-
tecture. At each step the RCNN takes as input
the current sentence vector si generated through
the HCNN sentence model and the previous label
xi?1 to predict a probability distribution over the
current label P (xi). The recurrent weights Hi?1
are conditioned on the previous agent ai?1 and
the output weights are conditioned on the current
agent ai. Note also the sentence matrix Ms of the
sentence model and the hierarchy of convolutions
applied to each feature that is a row in Ms to pro-
duce the corresponding feature in si.
2 Sentence Model
The general aim of the sentence model is to com-
pute a vector for a sentence s given the sequence
of words in s and a vector for each of the words.
The computation captures certain general consid-
erations regarding sentential compositionality. We
first relate such considerations and we then pro-
ceed to give a definition of the model.
2.1 Sentential compositionality
There are three main aspects of sentential compo-
sitionality that the model aims at capturing. To
relate these, it is useful to note the following basic
property of the model: a sentence s is paired to the
matrix Ms whose columns are given sequentially
by the vectors of the words in s. A row in Ms cor-
responds to the values of the corresponding feature
across all the word vectors. The first layer of the
network in Fig. 1 represents one such row of Ms,
whereas the whole matrix Ms is depicted in Fig.
2. The three considerations are as follows.
First, at the initial stage of the composition,
the value of a feature in the sentence vector is
a function of the values of the same feature in
the word vectors. That is, the m-th value in the
sentence vector of s is a function of the m-th
row of Ms. This aspect is preserved in the ad-
ditive and multiplicative models where the com-
position operations are, respectively, addition +
and component-wise multiplication . The cur-
rent model preserves the aspect up to the compu-
tation of the sentence vector s by adopting one-
dimensional, feature-wise convolution operations.
Subsequently, the discourse model that uses the
sentence vector s includes transformations across
the features of s (the transformation S in Fig. 2).
The second consideration concerns the hierar-
chical aspect of the composition operation. We
take the compositionality of meaning to initially
yield local effects across neighbouring words and
then yield increasingly more global effects across
all the words in the sentence. Composition oper-
ations like those in the structured models that are
guided by the syntactic parse tree of the sentence
capture this trait. The sentence model preserves
this aspect not by way of syntactic structure, but
by adopting convolution kernels of gradually in-
creasing sizes that span an increasing number of
words and ultimately the entire sentence.
The third aspect concerns the dependence of the
composition operation. The operation is taken to
depend on the different features, but not on the dif-
ferent words. Word specific parameters are intro-
duced only by way of the learnt word vectors, but
no word specific operations are learnt. We achieve
this by using a single convolution kernel across a
feature, and by utilizing different convolution ker-
nels for different features. Given these three as-
pects of sentential compositionality, we now pro-
ceed to describe the sentence model in detail.
2.2 Hierarchical Convolutional Neural
Network
The sentence model is taken to be a CNN where
the convolution operation is applied one dimen-
sionally across a single feature and in a hierarchi-
cal manner. To describe it in more detail, we first
recall the convolution operation that is central to
the model. Then we describe how we compute the
sequence of kernel sizes and how we determine the
hierarchy of layers in the network.
121
kk
k
k
m
m
m
m
1
1
2
2
3
3
4
4
(k   m)
1
*
Figure 3: Convolution of a vector m with a kernel
k of size 4.
2.2.1 Kernel and One-dimensional
Convolution
Given a sentence s and its paired matrix Ms, let
m be a feature that is a row in Ms. Before
defining kernels and the convolution operation,
let us consider the underlying operation of local
weighted addition. Let w1, ..., wk be a sequence
of k weights; given the feature m, local weighted
addition over the first k values of m gives:
y = w1m1 + ...+ wkmk (1)
Then, a kernel simply defines the value of k
by specifying the sequence of weights w1, ..., wk
and the one-dimensional convolution applies local
weighted addition with the k weights to each sub-
sequence of values of m.
More precisely, let a one-dimensional kernel k
be a vector of weights and assume |k| ? |m|,
where | ? | is the number of elements in a vec-
tor. Then we define the discrete, valid, one-
dimensional convolution (k ?m) of kernel k and
feature m by:
(k ?m)i :=
k?
j=1
kj ?mk+i?j (2)
where k = |k| and |k ?m| = |m| ? k + 1. Each
value in k ?m is a sum of k values of m weighted
by values in k (Fig. 3). To define the hierarchical
architecture of the model, we need to define a se-
quence of kernel sizes and associated weights. To
this we turn next.
2.2.2 Sequence of Kernel Sizes
Let l be the number of words in the sentence
s. The sequence of kernel sizes ?kli?i?t depends
only on the length of s and itself has length t =
d
?
2le ? 1. It is given recursively by:
kl1 = 2, k
l
i+1 = k
l
i + 1, k
l
t = l ?
t?1?
j=1
(klj ? 1)
(3)
That is, kernel sizes increase by one until the re-
sulting convolved vector is smaller or equal to the
last kernel size; see for example the kernel sizes in
Fig. 1. Note that, for a sentence of length l, the
number of layers in the HCNN including the input
layer will be t + 1 as convolution with the cor-
responding kernel is applied at every layer of the
model. Let us now proceed to define the hierarchy
of layers in the HCNN.
2.2.3 Composition Operation in a HCNN
Given a sentence s, its length l and a sequence
of kernel sizes ?kli?i?t, we may now give the
recursive definition that yields the hierarchy of
one-dimensional convolution operations applied
to each feature f that is a row in Ms. Specifi-
cally, for each feature f , let Kfi be a sequence of
t kernels, where the size of the kernel |Kfi | = kli.
Then we have the hierarchy of matrices and corre-
sponding features as follows:
M1f,: = M
s
f,: (4)
Mi+1f,: = ?( K
f
i ?Mif,: + bif ) (5)
for some non-linear sigmoid function ? and bias
bif , where i ranges over 1, ..., t. In sum, one-
dimensional convolution is applied feature-wise to
each feature of a matrix at a certain layer, where
the kernel weights depend both on the layer and
the feature at hand (Fig. 1). A hierarchy of matri-
ces is thus generated with the top matrix being a
single vector for the sentence.
2.2.4 Multiple merged HCNNs
Optionally one may consider multiple parallel
HCNNs that are merged according to different
strategies either at the top sentence vector layer or
at intermediate layers. The weights in the word
vectors may be tied across different HCNNs. Al-
though potentially useful, multiple merged HC-
NNs are not used in the experiment below.
This concludes the description of the sentence
model. Let us now proceed to the discourse model.
122
Open
the 
pod bay doors HAL
Dave
I'm afraid I can't do thats
S S
i
s
i+1
I
H
O
O
i
i
i+1
P(x )
P(x     )  
i+1
i
x
i-1
I
x
i
Figure 4: Unravelling of a RCNN discourse model to depth d = 2. The recurrent Hi and output Oi
weights are conditioned on the respective agents ai.
3 Discourse Model
The discourse model adapts a RNN architecture
in order to capture central properties of discourse.
We here first describe such properties and then de-
fine the model itself.
3.1 Discourse Compositionality
The meaning of discourse - and of words and utter-
ances within it - is often a result of a rich ensemble
of context, of speakers? intentions and actions and
of other relevant surrounding circumstances (Ko-
rta and Perry, 2012; Potts, 2011). Far from cap-
turing all aspects of discourse meaning, we aim
at capturing in the model at least two of the most
prominent ones: the sequentiality of the utterances
and the interactions between the speakers.
Concerning sequentiality, just the way the
meaning of a sentence generally changes if words
in it are permuted, so does the meaning of a para-
graph or dialogue change if one permutes the sen-
tences or utterances within. The change of mean-
ing is more marked the larger the shift in the order
of the sentences. Especially in tasks where one is
concerned with a specific sentence within the con-
text of the previous discourse, capturing the order
of the sentences preceding the one at hand may be
particularly crucial.
Concerning the speakers? interactions, the
meaning of a speaker?s utterance within a dis-
course is differentially affected by the speaker?s
previous utterances as opposed to other speakers?
previous utterances. Where applicable we aim at
making the computed meaning vectors reflect the
current speaker and the sequence of interactions
with the previous speakers. With these two aims
in mind, let us now proceed to define the model.
3.2 Recurrent Convolutional Neural Network
The discourse model coupled to the sentence
model is based on a RNN architecture with inputs
from a HCNN and with the recurrent and output
weights conditioned on the respective speakers.
We take as given a sequence of sentences or ut-
terances s1, ..., sT , each in turn being a sequence
of words si = yi1...yil , a sequence of labels
x1, ..., xT and a sequence of speakers or agents
a1, ..., aT , in such way that the i-th utterance is
performed by the i-th agent and has label xi. We
denote by si the sentence vector computed by way
of the sentence model for the sentence si. The
RCNN computes probability distributions pi for
the label at step i by iterating the following equa-
tions:
hi = ?( Ixi?1 +Hi?1hi?1 + Ssi + bh) (6)
pi = softmax(Oihi + bo) (7)
where I,Hi,Oi are corresponding weight matri-
ces for each agent ai and softmax(y)k = e
yk
?
j e
yj
returns a probability distribution. Thus pi is taken
to model the following predictive distribution:
pi = P (xi|x<i, s?i, a?i) (8)
123
Dialogue Act Label Example Train (%) Test (%)
Statement And, uh, it?s a legal firm office. 36.9 31.5
Backchannel/Acknowledge Yeah, anything could happen. 18.8 18.2
Opinion I think that would be great. 12.7 17.1
Abandoned/Uninterpretable So, - 7.6 8.6
Agreement/Accept Yes, exactly. 5.5 5.0
Appreciation Wow. 2.3 2.2
Yes?No?Question Is that what you do? 2.3 2.0
Non?Verbal [Laughter], [Throat-clearing] 1.7 1.9
Other labels (34) 12.2 13.5
Total number of utterances 196258 4186
Total number of dialogues 1115 19
Table 1: Most frequent dialogue act labels with examples and frequencies in train and test data.
An RCNN and the unravelling to depth d = 2 are
depicted respectively in Fig. 2 and Fig. 4. With
regards to vector representations of discourse, we
take the hidden layer hi as the vector represent-
ing the discourse up to step i. This concludes the
description of the discourse model. Let us now
consider the experiment.
4 Predicting Dialogue Acts
We experiment with the prediction of dialogue
acts within a conversation. A dialogue act spec-
ifies the pragmatic role of an utterance and helps
identifying the speaker?s intentions (Austin, 1962;
Korta and Perry, 2012). The automated recog-
nition of dialogue acts is crucial for dialogue
state tracking within spoken dialogue systems
(Williams, 2012). We first describe the Switch-
board Dialogue Act (SwDA) corpus (Calhoun et
al., 2010) that serves as the dataset in the experi-
ment. We report on the training procedure and the
results and we make some qualitative observations
regarding the discourse representations produced
by the model.
4.1 SwDA Corpus
The SwDA corpus contains audio recordings and
transcripts of telephone conversations between
multiple speakers that do not know each other and
are given a topic for discussion. For a given utter-
ance we use the transcript of the utterance, the dia-
logue act label and the speaker?s label; no other an-
notations are used in the model. Overall there are
42 distinct dialogue act labels such as Statement
and Opinion (Tab.1). We adopt the same data split
of 1115 train dialogues and 19 test dialogues as
used in (Stolcke et al, 2000).
4.2 Objective Function and Training
We minimise the cross-entropy error of the pre-
dicted and the true distributions and include an
l2 regularisation parameter. The RCNN is trun-
cated to a depth d = 2 so that the prediction of
a dialogue act depends on the previous two utter-
ances, speakers and dialogue acts; adopting depths
> 2 has not yielded improvements in the experi-
ment. The derivatives are efficiently computed by
back-propagation (Rumelhart et al, 1986). The
word vectors are initialised to random vectors of
length 25 and no pretraining procedure is per-
formed. We minimise the objective using L-BFGS
in mini-batch mode; the minimisation converges
smoothly.
4.3 Prediction Method and Results
The prediction of a dialogue act is performed in
a greedy fashion. Given the two previously pre-
dicted acts x?i?1, x?i?2, one chooses the act x?i that
has the maximal probability in the predicted dis-
tribution P (xi). The LM-HMM model of (Stol-
cke et al, 2000) learns a language model for each
dialogue act and a Hidden Markov Model for the
sequence of dialogue acts and it requires all the
utterances in a dialogue in order to predict the dia-
logue act of any one of the utterances. The RCNN
makes the weaker assumption that only the utter-
ances up to utterance i are available to predict the
dialogue act x?i. The accuracy results of the mod-
els are compared in Tab. 3.
4.4 Discourse Vector Representations
We inspect the discourse vector representations
that the model generates. After a dialogue is pro-
cessed, the hidden layer h of the RCNN is taken
124
Center A: Do you repair your own car? A: ? I guess we can start. A: Did you use to live around here?
Dialogue B: I try to, whenever I can. B: Okay. B: Uh, Redwood City.
First NN A: Do you do it every day? A: I think for serial murder ? A: Can you stand up in it?
B: I try to every day. B: Uh-huh. B: Uh, in parts.
Second NN A: Well, do you have any children? A: The USSR ? wouldn?t do it A: [Laughter] Do you have any kids
that you take fishing?
B: I?ve got one. B: Uh-huh. B: Uh, got a stepdaughter.
Third NN A: Do you manage the money? A: It seems to me there needs A: Is our five minutes up?
to be some ground, you know,
some rules ?
B: Well, I, we talk about it. B: Uh-huh. B: Uh, pretty close to it.
Fourth NN A: Um, do you watch it every A: It sounds to me like, uh, A: Do you usually go out, uh,
Sunday? you are doing well. with the children or without them?
B: [Breathing] Uh, when I can. B: My husband?s retired. B: Well, a variety.
Table 2: Short dialogues and nearest neighbours (NN).
Accuracy (%)
RCNN 73.9
LM-HMM trigram 71.0
LM-HMM bigram 70.6
LM-HMM unigram 68.2
Majority baseline 31.5
Random baseline 2.4
Table 3: SwDA dialogue act tagging accuracies.
The LM-HMM results are from (Stolcke et al,
2000). Inter-annotator agreement and theoretical
maximum is 84%.
to be the vector representation for the dialogue
(Sect. 3.2). Table 2 includes three randomly cho-
sen dialogues composed of two utterances each;
for each dialogue the table reports the four near-
est neighbours. As the word vectors and weights
are initialised randomly without pretraining, the
word vectors and the weights are induced during
training only through the dialogue act labels at-
tached to the utterances. The distance between
two word, sentence or discourse vectors reflects
a notion of pragmatic similarity: two words, sen-
tences or discourses are similar if they contribute
in a similar way to the pragmatic role of the utter-
ance signalled by the associated dialogue act. This
is suggested by the examples in Tab. 2, where a
centre dialogue and a nearest neighbour may have
some semantically different components (e.g. ?re-
pair your own car? and ?manage the money?), but
be pragmatically similar and the latter similarity is
captured by the representations. In the examples,
the meaning of the relevant words in the utter-
ances, the speakers? interactions and the sequence
of pragmatic roles are well preserved across the
nearest neighbours.
5 Conclusion
Motivated by the compositionality of meaning
both in sentences and in general discourse, we
have introduced a sentence model based on a novel
convolutional architecture and a discourse model
based on a novel use of recurrent networks. We
have shown that the discourse model together with
the sentence model achieves state of the art results
in a dialogue act classification experiment with-
out feature engineering or pretraining and with
simple greedy decoding of the output sequence.
We have also seen that the discourse model pro-
duces compelling discourse vector representations
that are sensitive to the structure of the discourse
and promise to capture subtle aspects of discourse
comprehension, especially when coupled to fur-
ther semantic data and unsupervised pretraining.
Acknowledgments
We thank Ed Grefenstette and Karl Moritz Her-
mann for great conversations on the matter. The
authors gratefully acknowledge the support of the
Clarendon Fund and the EPSRC.
References
[Austin1962] John L. Austin. 1962. How to do things
with words. Oxford: Clarendon.
[Baroni and Zamparelli2010] Marco Baroni and
Roberto Zamparelli. 2010. Nouns are vectors, ad-
jectives are matrices: Representing adjective-noun
constructions in semantic space. In EMNLP, pages
1183?1193.
[Blacoe and Lapata2012] William Blacoe and Mirella
Lapata. 2012. A comparison of vector-based rep-
resentations for semantic composition. In EMNLP-
CoNLL, pages 546?556.
125
[Calhoun et al2010] Sasha Calhoun, Jean Carletta, Ja-
son M. Brenier, Neil Mayo, Dan Jurafsky, Mark
Steedman, and David Beaver. 2010. The nxt-format
switchboard corpus: a rich resource for investigat-
ing the syntax, semantics, pragmatics and prosody
of dialogue. Language Resources and Evaluation,
44(4):387?419.
[Collobert and Weston2008] R. Collobert and J. We-
ston. 2008. A unified architecture for natural lan-
guage processing: Deep neural networks with mul-
titask learning. In International Conference on Ma-
chine Learning, ICML.
[Frege1892] Gottlob Frege. 1892. U?ber Sinn
und Bedeutung. Zeitschrift fu?r Philosophie und
philosophische Kritik, 100.
[Grefenstette et al2011] Edward Grefenstette,
Mehrnoosh Sadrzadeh, Stephen Clark, Bob
Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional
models of meaning. CoRR, abs/1101.0309.
[Hermann and Blunsom2013] Karl Moritz Hermann
and Phil Blunsom. 2013. The Role of Syntax in
Vector Space Models of Compositional Semantics.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Sofia, Bulgaria, August. Association
for Computational Linguistics. Forthcoming.
[Jones et al2006] Rosie Jones, Benjamin Rey, Omid
Madani, and Wiley Greiner. 2006. Generating
query substitutions. In WWW, pages 387?396.
[Korta and Perry2012] Kepa Korta and John Perry.
2012. Pragmatics. In Edward N. Zalta, editor, The
Stanford Encyclopedia of Philosophy. Winter 2012
edition.
[LeCun et al2001] Y. LeCun, L. Bottou, Y. Bengio, and
P. Haffner. 2001. Gradient-based learning applied
to document recognition. In Intelligent Signal Pro-
cessing, pages 306?351. IEEE Press.
[Mikolov et al2010] Tomas Mikolov, Martin Karafia?t,
Lukas Burget, Jan Cernocky?, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based lan-
guage model. In INTERSPEECH, pages 1045?
1048.
[Mitchell and Lapata2010] Jeff Mitchell and Mirella
Lapata. 2010. Composition in distributional models
of semantics. Cognitive Science, 34(8):1388?1429.
[Potts2011] Christopher Potts. 2011. Pragmatics.
In Ruslan Mitkov, editor, The Oxford Handbook
of Computational Linguistics. Oxford University
Press, 2 edition.
[Rumelhart et al1986] D. E. Rumelhart, G. E. Hinton,
and R. J. Williams. 1986. Learning internal repre-
sentations by error propagation. MIT Press Compu-
tational Models Of Cognition And Perception Series,
page 318362.
[Scheible and Schu?tze2013] Christian Scheible and
Hinrich Schu?tze. 2013. Cutting recursive autoen-
coder trees. CoRR, abs/1301.2811.
[Socher et al2012] Richard Socher, Brody Huval,
Christopher D. Manning, and Andrew Y. Ng. 2012.
Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In Proceedings of the 2012
Conference on Empirical Methods in Natural
Language Processing (EMNLP).
[Stolcke et al2000] Andreas Stolcke, Klaus Ries, Noah
Coccaro, Elizabeth Shriberg, Rebecca A. Bates,
Daniel Jurafsky, Paul Taylor, Rachel Martin,
Carol Van Ess-Dykema, and Marie Meteer. 2000.
Dialog act modeling for automatic tagging and
recognition of conversational speech. Computa-
tional Linguistics, 26(3):339?373.
[Sutskever et al2011] Ilya Sutskever, James Martens,
and Geoffrey E. Hinton. 2011. Generating text with
recurrent neural networks. In ICML, pages 1017?
1024.
[Turney and Pantel2010] Peter D. Turney and Patrick
Pantel. 2010. From frequency to meaning: Vec-
tor space models of semantics. J. Artif. Intell. Res.
(JAIR), 37:141?188.
[Wang and Manning2012] Sida Wang and Christo-
pher D. Manning. 2012. Baselines and bigrams:
Simple, good sentiment and topic classification. In
ACL (2), pages 90?94.
[Williams2012] Jason D. Williams. 2012. A belief
tracking challenge task for spoken dialog systems.
In NAACL-HLT Workshop on Future Directions and
Needs in the Spoken Dialog Community: Tools and
Data, SDCTD ?12, pages 23?24, Stroudsburg, PA,
USA. Association for Computational Linguistics.
126
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 173?182,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Collapsed Variational Bayesian Inference for PCFGs
Pengyu Wang
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Pengyu.Wang@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Phil.Blunsom@cs.ox.ac.uk
Abstract
This paper presents a collapsed variational
Bayesian inference algorithm for PCFGs
that has the advantages of two dominant
Bayesian training algorithms for PCFGs,
namely variational Bayesian inference and
Markov chain Monte Carlo. In three kinds
of experiments, we illustrate that our al-
gorithm achieves close performance to the
Hastings sampling algorithm while using
an order of magnitude less training time;
and outperforms the standard variational
Bayesian inference and the EM algorithms
with similar training time.
1 Introduction
Probabilistic context-free grammars (PCFGs) are
commonly used in parsing and grammar induction
systems (Johnson, 1998; Collins, 1999; Klein and
Manning, 2003; Matsuzaki et al, 2005). The tra-
ditional method for estimating the parameters of
PCFGs from terminal strings is the inside-outside
(IO) algorithm (Baker, 1979). As a special in-
stance of the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977), based on the prin-
ciple of maximum-likelihood estimation (MLE),
the standard IO algorithm learns relatively uni-
form probability distributions for grammars, while
the true distributions can be highly skewed (John-
son et al, 2007). In order to encourage sparse
grammars and avoid overfitting, recent research
for training PCFGs has drifted away from MLE in
favor of Bayesian inference algorithms that make
either deterministic or stochastic approximations
(Kurihara and Sato, 2006; Johnson et al, 2006;
Johnson et al, 2007).
Variational Bayesian inference (VB) (Kurihara
and Sato, 2006) for PCFGs extends EM and places
no constraints when updating parameters in the M
step. By minimising the divergence between the
true posterior and an approximate one in which
the strong dependencies between the parameters
and latent variables are broken, this determinis-
tic algorithm efficiently converges to an inaccu-
rate and only locally optimal solution like EM.
Alternatively, Johnson et al (2007) proposed two
Markov Chain Monte Carlo algorithms for PCFGs
that can reach the true posterior after convergence.
However, it is often difficult to diagnose a sam-
pler?s convergence, and mixing is notoriously slow
for distributions with tightly coupled hidden vari-
ables such as PCFGs, especially when the data sets
are large. Therefore, there remains a challenge for
more efficient, but also accurate and deterministic
inference algorithms for PCFGs.
In this paper, we present a collapsed variational
Bayesian inference (CVB) algorithm for PCFGs.
It has the same computational complexity as the
standard variational Bayesian inference, but offers
almost the same performance as the stochastic al-
gorithms due to its weak assumptions. The idea of
operating VB in the collapsed space was proposed
by Teh et al (2007) and Sung et al (2008), and it
was successfully applied to ?bag-of-words? mod-
els such as latent Dirichlet alocation (LDA) (Teh
et al, 2007) and mixture of Gaussian (Sung et al,
2008), where the latent variables are conditionally
independent given the parameters. By combining
the CVB idea and the dynamic programming tech-
niques used in structurally dependent models, we
deliver a both efficient and accurate algorithm for
training PCFGs and other structured natural lan-
guage models.
The rest of the paper is structured as follows.
We begin with the Bayesian models of PCFGs,
and relate the existing training algorithms. Sec-
tion 3 introduces collapsed variational Bayesian
inference for ?bag-of-words? models (defined in
Section 3.1). We discuss the difficulty in apply-
ing such inference to structured models, followed
by an approximate CVB algorithm for PCFGs.
173
An alternative approach is also included in brief.
In Section 4, we validate our CVB algorithm in
three simple experiments. They are inferring a
sparse grammar that describes the morphology of
the Sotho language (Johnson et al, 2007), unsu-
pervised dependency parsing (Klein and Manning,
2004) and supervised parsing with latent annota-
tions (Matsuzaki et al, 2005). Section 5 concludes
with future work.
2 Approximate inference for PCFGs
2.1 Definitions
A PCFG is a tuple (T,N, S,R, ?), where T , N ,
R and ? are the finite sets of terminals, non-
terminals, rules and parameters respectively, and
S ? N is the start symbol. We adopt a similar
notation to Johnson et al (2007), and assume that
the context free grammar G = (T,N, S,R) is in
Chomsky normal form and the empty string  /? T .
Hence, each rule r ? R takes either the form
A ? BC or A ? w, where A,B,C ? N and
w ? T . Let ?A?? be the probability of derivation
rule A ? ?, where ? ranges over (N ? N) ? T .
In the Bayesian setting, we place Dirichlet priors
with hyperparameters ?A = {?A??} on each ?A
= {?A??}.
Given a corpus of sentences w = (w1, ..., wn)
and the corresponding hidden parse trees t =
(t1, ..., tn), the joint probability distribution of pa-
rameters and variables is1:
P (w, t, ?|?) =P (?|?)
n?
i=1
PG(wi, ti|?)
=
( ?
A?N
PD(?A|?A)
)?
r?R
?fr(t)r
(1)
PD(?A|?A) =
1
B(?A)
?
r?RA
??r?1r
B(?A) =
?
r?RA ?(?r)
?(
?
r?RA ?r)
where fr(t) is the frequency of product rule r in
all the parse trees t, and RA is the set of rules
with left-hand side A. For a Dirichlet distribution
PD(?A|?A), B(?A) is the normalization constant
that can be written in terms of the gamma function
? (i.e. the generalised factorial function).
1Strictly speaking, for each (w, t) pair, if a hidden tree t
is arbitrary, we need to include two delta functions, namely
?(w = yield(t)) and ?(G ?? t). We assume that both delta
functions are true, otherwise the probability of such pair is 0.
2.2 Variational Bayesian inference
The standard inside-outside algorithm for PCFGs
belongs to the general EM class, which is further
a subclass of VB (Beal, 2003). VB maximises the
negative free energy ?F(Q(t, ?)), a lower bound
of the log marginal likelihood of the observation
logP (w|?). This is equivalent to minimising the
Kullback-Leibler divergence.
logP (w|?) ? ?F(Q(t, ?))
=EQ(t,?)[logP (w, t, ?|?)]? EQ(t,?)[logQ(t, ?)]
Q(t, ?) is an approximate posterior, where the pa-
rameters and hidden variables are assumed to be
independent. Thus, it is factorised:
Q(t, ?) ? Q(t)Q(?) (2)
This strong independence assumption allows for
the separate updates of Q(t) and Q(?) iteratively,
optimising the negative free energy ?F(Q(z, ?)).
For the traditional IO algorithm using maximum
likelihood estimation, Q(?) is further assumed to
be degenerate, i.e. Q(?) = ?(? = ??).
E step: Q(t) ? exp(EQ(?)[logP (w, t, ?)])
M step: ?? = argmax
?
P (w, t, ?)
In the E step, we update Q(t). For each tree t,
Q(t) ? PG(w, t|??)
=
?
r?R
(??r)fr(t) (3)
The distribution over parse tree Q(t) is intractable
to compute as its normalization requires summing
over all possible parse trees producing w. We use
dynamic programming to compute inside and out-
side probabilities recursively with the aim of accu-
mulating the expected counts.
E[fA?BC(t)|w] ?
?
0?i<j<k?|w|
POUT(A, i, k)?
?A?BCPIN(B, i, j)PIN(C, j, k)
E[fA?w(t)|w] ?
?
0?i?|w|
POUT(A, i)?
?A?wi?(wi = w)
where PIN(A, i, k) is the inside probability of ob-
servation wi,k = wi, ..., wk given A is the root of
the subtree, and POUT(A, i, k) is the probability of
A spanning (i, k), together with the rest of w.
174
In the M step, we find the optimal ?? based on
the MLE principle:
??A?? =
E[fA??(t)|w]?
A????RA E[fA???(t)|w]
E[fA??(t)|w] =
n?
i=1
E[fA??(ti)|wi]
VB inference is the generalisation of EM in the
sense that it allows arbitrary parametric forms of
Q(?). Thus, the update equation in the M step is:
Q(?) ? exp(EQ(t)[logP (w, t, ?|?)])
By the conjugacy property, the new Q(?) is still
in Dirichlet distribution form except with updated
hyperparameters as shown by Kurihara and Sato
(2006). Instead, Beal (2003) suggested an equiva-
lent mean parameters ??. Based on implementation
of the EM algorithm, we only need a minor modi-
fication in the M step.
??A?? =
m(E[fA??(t)|w]+?A??)
m(
?
A????RA(E[fA???(t)|w]+?A???))
m(x) = exp(?(x))
where ?(x) = ??(x)?x is the digamma function.
From the joint distribution in (1) proportional
to the true posterior, we notice that the parame-
ters and hidden variables are intimately coupled.
Fluctuations in the parameters can induce changes
in the hidden variables and vice-versa. Hence, the
independence assumption in (2) and Figure 1(d)
seems too strong, leading to inaccurate local max-
imums, although it allows for efficient and deter-
ministic updates in EM and VB. The dependencies
between parameters and hidden variables are kept
intact for the remaining algorithms in this paper.
2.3 Markov Chain Monte Carlo
The standard Gibbs sampler for PCFGs iteratively
samples the parameters ? and all the parse trees
t. Its mixing can be slowed by again the strong
dependencies between the parameters and hidden
variables. Instead of reparsing all the hidden trees
t for each sample of ?, collapsed Gibbs sampling
(CGS) improves upon Gibbs sampling in terms of
convergence speed by integrating out the param-
eters, and sampling directly from P (t|w, ?) in a
component-wise manner. Thus, it also deals with
the dependencies exactly.
By using the conjugacy property, we can easily
compute the marginal distribution of w and t:
P (w, t|?) =
?
?
PG(w, t|?)PD(?|?)d?
=
?
A?N
B(fA(t) + ?A)
B(?A)
(4)
where we define fA(t) to be a vector of rule fre-
quencies in t indexed by A ? ? ? RA. Hence,
the conditional distribution for a parse tree ti given
all others is:
P (ti|wi,w?i, t?i, ?) ? P (wi, ti|w?i, t?i, ?)
=
?
A?N
B(fA(t) + ?A)
B(fA(t?i) + ?A)
(5)
where w?i and t?i denote all other sentences and
trees. It is noticeable that sampling a parse tree
from the above conditional distribution is difficult.
The frequencies fA(t) effectively mean that the
production probabilities are dependent on the cur-
rent parse tree ti. That is rule parameters can be
updated on the fly inside a parse tree, which pro-
hibits efficient dynamic programming tricks.
In order to solve this problem, Johnson et al
(2007) proposed a Hastings sampler that specified
an alternative rule probabilities ?H of a proposal
distribution P (ti|wi, ?H), where
?HA?? =
fA??(t?i) + ?A???
A????RA(fA???(t?i) + ?A???)
The rule probabilities ?H are based on the statistics
collected from all other parse trees, and they are
fixed for the conditional distribution of the current
parse tree. Therefore, by using a variant of inside
algorithm (Goodman, 1998), one can efficiently
sample a parse tree, which will be either accepted
or rejected based on the Metropolis choice.
The MCMC based algorithms do not make any
assumptions at all, and they can converge to the
true posterior, either in joint or collapsed space as
shown in Figure 1(b), 1(c). However, one needs to
have experience about the number of samples to
be collected and the burn-in period. For compu-
tationally intensive tasks such as learning PCFGs
from a large corpus, a sufficiently large number
of samples are required to decrease the sampling
variance. Therefore, MCMC algorithms improves
the performance over EM and VB at the cost of
much more training time.
175
Figure 1: Graphical representations of the PCFG with n = 3 trees (a), and the (approximate) posteriors
for Gibbs sampling (b), collapsed Gibbs sampling (c), variational Bayesian inference (d), and collapsed
variational Bayesian inference (e). We use dashed lines to depict the weak dependencies.
3 Collapsed variational Bayesian
inference
3.1 For bag-of-words models
Leveraging the insight that a sampling algorithm
in collapsed space mixes faster than the standard
one, Teh et al (2007) proposed a similar argument
that a VB inference algorithm in collapsed space
is more effective than the standard one. Following
the success in LDA (Teh et al, 2007), a number of
research results have been accumulated around ap-
plying CVB to a variety of ?bag-of-words? mod-
els (Sung et al, 2008; Sato et al, 2012; Wang and
Blei, 2012).
Formally, we define a model to be independent
and identically distributed (i.i.d.) (or informally
?bag-of-words?) if its hidden variables are condi-
tionally independent given the parameters. LDA,
IBM word alignment model 1 and 2, and various
finite mixture models are typical examples.
For an i.i.d. model, integrating out parameters
induces dependencies that spread over many hid-
den variables, and thus the dependency between
any two variables is very weak. This provides an
ideal setting to apply the mean field method (i.e.
fully factorized VB), as its underlying assumption
is that any variable depends on only the summary
statistics collected from other variables called the
field, and any particular variable?s impact on the
field is very small. Hence, the mean field assump-
tion is better satisfied in collapsed space with very
weak dependencies than in joint space with strong
dependencies. As a result, we expect that VB in
collapsed space can achieve more accurate results
than the standard VB, and the results would be
very close to the true posterior.
Even in collapsed space, CVB remains a deter-
ministic algorithm that updates the posterior dis-
tributions over the hidden variables just like VB
and EM. Therefore, we expect CVB to be compu-
tationally efficient as well.
3.2 For structured NLP models
We notice that the basic condition for applying the
CVB algorithm to a specific model is for the model
to be i.i.d., such that the hidden variables are only
weakly dependent in collapsed space, providing an
ideal condition to operate VB. However, the i.i.d.
condition is certainly not true for structured NLP
models such as hidden Markov models (HMMs)
and PCFGs. Given the shape of a parse tree, a
hidden variable is strongly dependent on its par-
ent, siblings and children, and weakly dependent
on the rest. Even worse, to infer a grammar from
terminal strings, we don?t even have access to the
shape of parse trees, let alne analyzing the depen-
dencies of hidden variables inside trees.
Although the PCFG model is not i.i.d. at the
variable level, we can lift the idea of CVB up to
the tree level. As our research domain is those
large scale applications in language processing, a
common feature of those problems is that there
are usually many sentences, each of which has a
hidden parse tree behind it. Hence, we may con-
sider each sentence together with its parse tree to
be drawn i.i.d. from the same set of parameters.
Therefore, at the tree level, a PCFG can be con-
sidered as an i.i.d. model as shown in Figure 1(a)
and thus, it can be fitted in the CVB framework
as described in Section 3.1. We summarise the as-
176
Q(ti) ?
?
A?N
?
A???RA exp(EQ(t?i)[log(
?fA??(ti)?1
j=0 (fA??(t?i) + ?A?? + j))])
?(PA??? fA??? (ti))?1
j=0 exp(EQ(t?i)[log(
?
A????RA(fA??(t?i) + ?A??? + j))])
Figure 2: The exact mean field update in collapsed space for the parse tree ti.
Q(ti) ?
?
r=A???R
( EQ(t?i)[fA??(t?i)] + ?A???
A???(EQ(t?i)[fA???(t?i)] + ?A???)
)fr(ti)
Figure 3: The approximate mean field update in collapsed space for the parse tree ti.
sumptions made by each algorithm in Figure 1(b-
e) before presenting the CVB algorithm formally.
The CVB algorithm for the PCFG model keeps
the dependencies between the parameters and the
hidden parse trees in an exact fashion:
Q(t, ?) = Q(t)Q(?|t)
We factorise Q(t) by breaking only the weak de-
pendencies between parse trees, while keeping the
inside dependencies intact, as we don?t make fur-
ther assumptions about Q(t) for each t.
Q(t) ?
n?
i=1
Q(ti)
By the above factorisations, we compute the neg-
ative variational free energy ?F(Q(t)Q(?|t)) as
follows:
?F(Q(t)Q(?|t))
=EQ(t)Q(?|t)[logP (w, t, ?|?)? logQ(t)Q(?|t)]
=EQ(t)[EQ(?|t)[log P (w, t, ?|?)Q(?|t) ]? logQ(t)]
Maximizing ?F(Q(t)Q(?|t)) requires to update
Q(?|t) and Q(t) in turn. In particular, Q(?|t) is
set equal to the true posterior P (?|w, t, ?):
?F(Q(t)P (?|w, t))
=EQ(t)[EP (?|w,t,?)[log P (w, t, ?|?)P (?|w, t, ?) ]? logQ(t)]
=EQ(t)[logP (w, t|?)? logQ(t)]
Finally, we update the approximate posterior for
each parse tree t by using the mean field method
in the collapsed space:
Q(ti) ? exp(EQ(t?i)[logP (wi, ti|w?i, t?i, ?)])
(6)
The inner term P (wi, ti|w?i, t?i, ?) in the above
equation is just the unnormalized collapsed Gibbs
sampling in (5). Plugging in (5), and expanding
terms such asB(?A) and ?(x), we obtain an exact
computation of Q(ti) in Figure 2.
The exact computation is both intractable and
expensive. The intractability comes from the sim-
ilar problem as in the collapsed Gibbs sampling
that we are unable to calculate the normalisation
term ?ti Q(ti). Hence, we follow Johnson et al(2007) to approximate it by using only the statis-
tics from other sentences, namely ?H and ignoring
the local contribution.
P (wi, ti|w?i, t?i, ?) ?
?
A???R
(
?HA??
)fA??(ti)
(7)
We discuss the accuracy of (7) in Section 3.3. For
those expensive computations of the expected log
counts in Figure 2, Teh et al (2007) and Sung et
al. (2008) suggested the use of a linear Gaussian
approximation based on the law of large numbers.
EQ(t?i)[log(fA??(t?i) + ?A??)]
? log(EQ(t?i)[fA??(t?i)] + ?A??) (8)
Substituting (7) into (6), and employing the linear
approximation, we derive an approximate CVB al-
gorithm as shown in Figure 3. In addition, its form
is much more simplified and interpretable com-
pared with the exact computation in Figure 2.
The surprising similarity between the approxi-
mate CVB update in Figure 3 and E step update in
(3) indicates that the dynamic programming used
in both EM and VB can take over from now. To
run inside-outside recursion, the EM algorithm
employs the parameters ?? based on maximum
likelihood estimation; the VB algorithm employs
177
the mean parameters ??; and our CVB algorithm
employs the parameters ?CVB computed from the
expected counts of all other sentences.
The implementation can be easily achieved by
modifying code of the EM algorithm. We keep
track of the expected counts at global level, sub-
tract the local mean counts for ti before update,
run the inside-outside recursion using ?CVB, and
finally add the updated distribution back into the
global counts. Therefore, we only need to replace
the parameters with the expected counts, and make
update after each sentence; the core of the inside-
outside implementation remains the same.
Our CVB algorithm bears some similarities to
the online EM algorithm with maximum a pos-
terior (MAP) updates (Neal and Hinton, 1998;
Liang and Klein, 2009), but they differ in several
ways. The online EM algorithm updates each tree
ti based on the statistics of all the trees, optimising
the same objective function p(w|?) as the batch
EM algorithm. MAP estimation searches for the
optimal posterior p(w|?)p(?). On the other hand,
our CVB algorithm optimises the data likelihood
p(w). The smoothing effects for the MAP estima-
tion (?A?? ? 1) prevent the use of sparse priors,
whereas the CVB algorithm (?A??) overcomes
such difficulty by parameter integration.
3.3 Discussion
Breaking the weak dependencies between hidden
variables and employing the linear approximation
have been argued to be accurate (Teh et al, 2007;
Sung et al, 2008; Sato and Nakagawa, 2012), and
they are the standard procedures in applying the
CVB algorithms to i.i.d. models.
In our CVB algorithm for PCFGs, we introduce
an extra approximation in (7), which we argue is
accurate. Theoretically, the inaccuracy only oc-
curs when there are repeated rules in a parse tree as
shown in Figure 2, so the same rule seen later uses
a slightly different probability. Even if the inac-
curacy indeed occurs, in our described scenario of
many sentences, the local contribution from a sin-
gle sentence is small compared with the statistics
from all other sentences. Empirically, we replicate
the experiment of Setho language by Johnson et al
(2007) in Section 4.1, and we find that the sampled
trees based on ?H never get rejected, illustrating an
acceptance rate close to 100%, and meaning that
?H is a very accurate Metropolis proposal. Since
all the assumptions made by the CVB algorithm
Figure 4: A fragment of a tree structure
are reasonable and weak, we expect its results to
be close to true posteriors.
3.4 An alternative approach
We briefly sketch an alternative CVB algorithm at
the variable level for completeness.
For a structured NLP model with its shape to
be fixed such as the PCFG with latent annotations
(PCFG-LA) (Matsuzaki et al, 2005) (See defini-
tion in Section 4.3), we can simply ignore all the
dependencies between the hidden variables in the
collapsed space, despite whether they are strong
(for adjacent nodes) or weak (for others). Al-
though it seems that we have made unreasonable
assumptions, it is not transparent which is worse
comparing with the assumptions in the standard
VB. Following this assumption, we can derive a
CVB algorithm similar to the corresponding local
sampling algorithm that samples one hidden vari-
able at a time. For example, the approximate pos-
terior over the subtype of the node A in the above
tree fragment in Figure 4 is updated follows:
q(A = a)
? E[fB?aC(t
?A)] + ?
E[fB(t?A)] + |RB|? ?
E[fa?DE(t?A)] + ?
E[fa(t?A)] + |Ra|?
where we use A to denote the node position, and
a to denote its hidden subtype. q(A = a) means
the probability of node A being in subtype a. In
addition, we need to take into account the distribu-
tions over its adjacent variables. In our case, A is
strongly dependent on nodesB,C,D,E, and only
weakly dependent on other variables (not shown in
the above tree fragment) via global counts, e.g.:
E[fB?aC(t?A)]
=
?
b
?
c
q(B = b)q(C = c)E[fb?ac(t?A)]
However, it is not obvious how to use this alter-
native approach in general, and the performances
of resulting algorithms remain unclear. Therefore,
we implement only the CVB algorithm at the tree
level in Section 3.2 for our experiments.
178
4 Experiments
We conduct three simple experiments to validate
our CVB algorithm for PCFGs. In Section 4.1, we
illustrate the significantly reduced training time
of our CVB algorithm compared to the related
Hastings algorithm; whereas in later two sections,
we demonstrate the increased performance of our
CVB algorithm compared to the corresponding
VB and EM algorithms.
4.1 Inferring sparse grammars
Firstly, we conduct the same experiment of in-
ferring sparse grammars describing the morphol-
ogy of the Sotho language as in Johnson et al
(2007). We use the same corpus of unsegmented
Sotho verb types from CHILDES (MacWhinney
and Snow, 1985), and define the same initial CFG
productions by allowing each non-terminal to emit
any substrings in the corpus as terminals plus five
predefined morphological rules at the top level.
We randomly withhold 10% of the verb types
from the corpus for testing, and use the rest 90%
for training. Both algorithms are evaluated by
their per word perplexity on the test data set with
prior set to 10?5 as suggested by Johnson et al
(2007). We run 5 times with random starts, and
report the averaged results in Figure 5. The Hast-
ings algorithm2 takes roughly 1,000 iterations to
converge, while our CVB algorithm reaches the
convergence even before 10 iterations, consuming
only a fraction of training time (CVB: 1.5 minutes;
Hastings: 20 minutes). As well as little difference
margin in final perplexities shown in Figure 5, we
also evaluated segmentation quality measured by
the F1 scores, and again the difference is trivial
(CVB: 29.8%, Hastings: 31.3%).
4.2 Dependency model with valence
As a second empirical validation of our CVB in-
ference algorithm, we apply it to unsupervised
grammar induction with the popular Dependency
Model with Valence (DMV) (Klein and Manning,
2004). Although the original maximum likelihood
formulation of this model has long since been sur-
passed by more advanced models, all of the state-
of-the-art approaches to unsupervised dependency
parsing still have DMV at their core (Headden III
et al, 2009; Blunsom and Cohn, 2010; Spitkovsky
et al, 2012). As such we believe demonstrating
2Annealing is not used in order to facilitate the perplexity
calculation in the test set.
0 5 10 15 20
67
89
1011
12
Number of Iterations (CVB)
Test P
erplexi
ty
 
 CVBHastings
0 500 1000 1500 2000Number of Iterations (Hastings)
Figure 5: Perplexities averaged over 5 runs on the
extracted corpus of Sotho verbs.
improved inference on this core model will enable
future improvements to more complex models.
We evaluate a Dirichlet-Multinomial formula-
tion of DMV in the standard fashion by train-
ing on sections 2-21 and testing on section 23 of
the Penn. Wall Street Journal treebank (Marcus
et al, 1993). We initialise our models using the
original harmonic initialiser (Klein and Manning,
2004). Figure 6 displays the directed accuracy re-
sults for DMV model trained with CVB and VB
with Dirichlet ? parameters of either 1 or 0.1, as
well as the previously reported MLE result. In
both cases we see superior results for CVB infer-
ence, providing evidence that CVB may be a bet-
ter choice of inference algorithm for Bayesian for-
mulations of generative grammar induction mod-
els such as DMV.
4.3 PCFG with latent annotations
The vanilla PCFGs estimated by simply taking the
empirical rule frequencies off treebanks are not ac-
curate models to capture the syntactic structures in
most natural languages as demonstrated by Char-
niak (1997) and Klein and Manning (2003). Our
third experiment is to apply the CVB algorithm
to the PCFGs with latent annotations (PCFGs-
LA) (Matsuzaki et al, 2005), where each non-
terminal symbol is augmented with hidden vari-
ables (or subtypes). Given a parsed corpus, train-
ing a PCFG-LA yields a finer grammar with the
automatically induced features represented by the
subtypes. For example, an augmented binary rule
takes the form A[a] ? B[b]C[c], where a, b, c ?
[1, H] are the hidden subtypes, and H denotes the
number of subtypes for each non-terminal.
179
1.0 0.10.45
0.46
0.47
0.48
0.49
Bayesian Priors
F1 Sco
res
 
 EMVBCVB
Figure 6: DMV trained by EM, VB and CVB. F1
scores on section 23, WSJ.
Objective Precision Recall F1 Exact
EM 75.84 72.92 74.35 11.13
VB 76.98 73.32 75.11 11.49
CVB 78.85 76.98 77.90 12.56
Table 1: PCFG-LA (2 subtypes) trained by EM,
VB and CVB. Precision, Recall, F1 scores, Exact
match scores on section 23, WSJ.
We follow the same experiment set-up as DMV,
and report the results on the section 23, using the
best grammar tested on the development set (sec-
tion 22) from 5 random runs for each algorithm.
We adopt Petrov et al (2006)?s methods to process
the data: right binarising and replacing infrequent
words with the generic unknown word marker for
English, and to initialise: adding 1% randomness
to the parameters ?0 to start the EM training. We
calculate the expected counts from (G, ?0) to ini-
tialise our VB and CVB algorithms.
In Table 1, when each non-terminal is split into
2 hidden subtypes, we show that our CVB algo-
rithm outperforms the EM and VB algorithms in
terms of all the evaluation objectives. We also
investigate the hidden state space with higher di-
mensions (4,8,16 subtypes), and find our CVB al-
gorithm retains the advantages over the other two,
whereas the VB algorithm fails to surpass the EM
algorithm as reported in Figure 7.
5 Conclusion and future work
In this paper we have presented a collapsed vari-
ational Bayesian inference algorithm for PCFGs.
We make use of the common scenario where the
data consists of multiple short sentences, such that
1 2 4 8 16
0.65
0.7
0.75
0.8
0.85
0.9
Number of Hidden States
F1 Sco
res
 
 
EMVBCVB
Figure 7: PCFG-LA (2,4,8,16 subtypes) trained by
EM, VB and CVB. F1 scores on section 23, WSJ.
we can ignore the local dependencies induced by
collapsing the parameters. The assumptions in our
CVB algorithm are reasonable for a range of pars-
ing applications and justified in three tasks by the
empirical observations: it produces more accurate
results than standard VB, and close results to sam-
pling with significantly less training time.
While not state-of-the-art, the models we have
demonstrated our CVB algorithm on underlie a
number of high performance grammar induction
and parsing systems (Cohen and Smith, 2009;
Blunsom and Cohn, 2010; Petrov and Klein, 2007;
Liang et al, 2007). Therefore, our work naturally
extends to employing our CVB algorithm in more
advanced models such as hierarchical splitting and
merging system used in Berkeley parser (Petrov
and Klein, 2007), and generalising our CVB al-
gorithm to the non-parametric models such as tree
substitution grammars (Blunsom and Cohn, 2010)
and infinite PCFGs (Liang et al, 2007).
We have also sketched an alternative CVB al-
gorithm which makes a harsher independence as-
sumption for the latent variables but then requires
no approximation of the variational posterior by
performing inference individually for each parse
node. This model breaks some strong dependen-
cies within parse trees, but if we expect the pos-
terior to be highly skewed by using a sparse prior,
the product of constituent marginals may well be a
good approximation. We leave further exploration
of this algorithm for future work.
Acknowledgments
We would like to thank Mark Johnson for the data
used in Section 4.1 and valuable advice.
180
References
James K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65(S1):S132.
Matthew Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, The
Gatsby Computational Neuroscience Unit, Univer-
sity College London.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1204?1213, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the fourteenth national conference
on artificial intelligence and ninth conference on
Innovative applications of artificial intelligence,
AAAI?97/IAAI?97, pages 598?603. AAAI Press.
Shay B. Cohen and Noah A. Smith. 2009. Shared
logistic normal distributions for soft parameter ty-
ing in unsupervised grammar induction. In NAACL
?09: Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 74?82, Morristown, NJ,
USA. Association for Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistics Soci-
ety, Series B, 39(1):1?38.
Joshua T. Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Cambridge, MA, USA. Adviser-Stuart
Shieber.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 101?109, Boulder, Colorado, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2006. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. In NIPS.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of the 7th Inter-
national Conference on Human Language Technol-
ogy Research and 8th Annual Meeting of the NAACL
(HLT-NAACL 2007), pages 139?146, Rochester,
New York, April.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24:613?632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics, page 478.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language.
In Proceedings of the 8th international conference
on Grammatical Inference: algorithms and appli-
cations, ICGI?06, pages 84?96, Berlin, Heidelberg.
Springer-Verlag.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In Proceedings HLT/NAACL.
Percy Liang, Slav Petrov, Michael Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchi-
cal Dirichlet processes. In Proc. of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 688?697, Prague,
Czech Republic.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Child Lan-
guage.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic cfg with latent annotations.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ?05,
pages 75?82, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Radford Neal and Geoffrey E. Hinton. 1998. A view of
the em algorithm that justifies incremental, sparse,
and other variants. In Learning in Graphical Mod-
els, pages 355?368. Kluwer Academic Publishers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
181
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Issei Sato and Hiroshi Nakagawa. 2012. Rethinking
collapsed variational bayes inference for LDA. In
Proceedings of the 29th International Conference on
Machine Learning.
Issei Sato, Kenichi Kurihara, and Hiroshi Nakagawa.
2012. Practical collapsed variational bayes infer-
ence for hierarchical dirichlet process. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ?12, pages 105?113, New York, NY, USA.
ACM.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2012. Three dependency-and-boundary
models for grammar induction. In Proceedings of
the 2012 Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL 2012).
Jaemo Sung, Zoubin Ghahramani, and Sung-Yang
Bang. 2008. Latent-space variational Bayes. IEEE
Trans. Pattern Anal. Mach. Intell., 30(12), Decem-
ber.
Yee Whye Teh, David Newman, and Max Welling.
2007. A collapsed variational Bayesian inference
algorithm for latent Dirichlet alocation. In In Ad-
vances in Neural Information Processing Systems,
volume 19.
Chong Wang and David Blei. 2012. Truncation-free
stochastic variational inference for bayesian non-
parametric models. In Neural Information Process-
ing Systems.
182
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 22?27,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
A Deep Architecture for Semantic Parsing
Edward Grefenstette, Phil Blunsom, Nando de Freitas and Karl Moritz Hermann
Department of Computer Science
University of Oxford, UK
{edwgre, pblunsom, nando, karher}@cs.ox.ac.uk
Abstract
Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.
1 Introduction
The ubiquity of always-online computers in the
form of smartphones, tablets, and notebooks has
boosted the demand for effective question answer-
ing systems. This is exemplified by the grow-
ing popularity of products like Apple?s Siri or
Google?s Google Now services. In turn, this cre-
ates the need for increasingly sophisticated meth-
ods for semantic parsing. Recent work (Artzi and
Zettlemoyer, 2013; Kwiatkowski et al., 2013; Ma-
tuszek et al., 2012; Liang et al., 2011, inter alia)
has answered this call by progressively moving
away from strictly rule-based semantic parsing, to-
wards the use of distributed representations in con-
junction with traditional grammatically-motivated
re-write rules. This paper seeks to extend this line
of thinking to its logical conclusion, by provid-
ing the first (to our knowledge) entirely distributed
neural semantic generative parsing model. It does
so by adapting deep learning methods from related
work in sentiment analysis (Socher et al., 2012;
Hermann and Blunsom, 2013), document classifi-
cation (Yih et al., 2011; Lauly et al., 2014; Her-
mann and Blunsom, 2014a), frame-semantic pars-
ing (Hermann et al., 2014), and machine trans-
lation (Mikolov et al., 2010; Kalchbrenner and
Blunsom, 2013a), inter alia, combining two em-
pirically successful deep learning models to form
a new architecture for semantic parsing.
The structure of this short paper is as follows.
We first provide a brief overview of the back-
ground literature this model builds on in ?2. In ?3,
we begin by introducing two deep learning models
with different aims, namely the joint learning of
embeddings in parallel corpora, and the generation
of strings of a language conditioned on a latent
variable, respectively. We then discuss how both
models can be combined and jointly trained to
form a deep learning model supporting the gener-
ation of knowledgebase queries from natural lan-
guage questions. Finally, in ?4 we conclude by
discussing planned experiments and the data re-
quirements to effectively train this model.
2 Background
Semantic parsing describes a task within the larger
field of natural language understanding. Within
computational linguistics, semantic parsing is typ-
ically understood to be the task of mapping nat-
ural language sentences to formal representations
of their underlying meaning. This semantic rep-
resentation varies significantly depending on the
task context. For instance, semantic parsing has
been applied to interpreting movement instruc-
tions (Artzi and Zettlemoyer, 2013) or robot con-
trol (Matuszek et al., 2012), where the underlying
representation would consist of actions.
Within the context of question answering?the
focus of this paper?semantic parsing typically
aims to map natural language to database queries
that would answer a given question. Kwiatkowski
22
et al. (2013) approach this problem using a multi-
step model. First, they use a CCG-like parser
to convert natural language into an underspecified
logical form (ULF). Second, the ULF is converted
into a specified form (here a FreeBase query),
which can be used to lookup the answer to the
given natural language question.
3 Model Description
We describe a semantic-parsing model that learns
to derive quasi-logical database queries from nat-
ural language. The model follows the structure of
Kwiatkowski et al. (2013), but relies on a series of
neural networks and distributed representations in
lieu of the CCG and ?-Calculus based representa-
tions used in that paper.
The model described here borrows heavily from
two approaches in the deep learning literature.
First, a noise-contrastive neural network similar to
that of Hermann and Blunsom (2014a, 2014b) is
used to learn a joint latent representation for nat-
ural language and database queries (?3.1). Sec-
ond, we employ a structured conditional neural
language model in ?3.2 to generate queries given
such latent representations. Below we provide the
necessary background on these two components,
before introducing the combined model and de-
scribing its learning setup.
3.1 Bilingual Compositional Sentence Models
The bilingual compositional sentence model
(BiCVM) of Hermann and Blunsom (2014a) pro-
vides a state-of-the-art method for learning se-
mantically informative distributed representations
for sentences of language pairs from parallel cor-
pora. Through the joint production of a shared la-
tent representation for semantically aligned sen-
tence pairs, it optimises sentence embeddings
so that the respective representations of dissim-
ilar cross-lingual sentence pairs will be weakly
aligned, while those of similar sentence pairs will
be strongly aligned. Both the ability to jointly
learn sentence embeddings, and to produce latent
shared representations, will be relevant to our se-
mantic parsing pipeline.
The BiCVM model shown in Fig. 1 assumes
vector composition functions g and h, which map
an ordered set of vectors (here, word embed-
dings from D
A
,D
B
) onto a single vector in R
n
.
As stated above, for semantically equivalent sen-
tences a, b across languages L
A
,L
B
, the model
aims to minimise the distance between these com-
posed representations:
E
bi
(a, b) = ?g(a)? h(b)?
2
In order to avoid strong alignment between dis-
similar cross-lingual sentence pairs, this error
is combined with a noise-contrastive hinge loss,
where n ? L
B
is a randomly sampled sentence,
dissimilar to the parallel pair {a, b}, and m de-
notes some margin:
E
hl
(a, b, n) = [m+ E
bi
(a, b)? E
bi
(a, n)]
+
,
where [x]
+
= max(0, x). The resulting objective
function is as follows
J(?) =
?
(a,b)?C
(
k
?
i=1
E
hl
(a, b, n
i
) +
?
2
???
2
)
,
with
?
2
???
2
as the L
2
regularization term and
?={g, h,D
A
,D
B
} as the set of model variables.
...
L1 sentence embedding
L1 word embeddings
L2 sentence embedding
L2 word embeddings
contrastive estimation
g
h
Figure 1: Diagrammatic representation of a
BiCVM.
While Hermann and Blunsom (2014a) applied
this model only to parallel corpora of sentences,
it is important to note that the model is agnostic
concerning the inputs of functions g and h. In this
paper we will discuss how this model can be ap-
plied to non-sentential inputs.
23
3.2 Conditional Neural Language Models
Neural language models (Bengio et al., 2006) pro-
vide a distributed alternative to n-gram language
models, permitting the joint learning of a pre-
diction function for the next word in a sequence
given the distributed representations of a subset
of the last n?1 words alongside the representa-
tions themselves. Recent work in dialogue act la-
belling (Kalchbrenner and Blunsom, 2013b) and
in machine translation (Kalchbrenner and Blun-
som, 2013a) has demonstrated that a particular
kind of neural language model based on recurrent
neural networks (Mikolov et al., 2010; Sutskever
et al., 2011) could be extended so that the next
word in a sequence is jointly generated by the
word history and the distributed representation for
a conditioning element, such as the dialogue class
of a previous sentence, or the vector representation
of a source sentence. In this section, we briefly de-
scribe a general formulation of conditional neural
language models, based on the log-bilinear mod-
els of Mnih and Hinton (2007) due to their relative
simplicity.
A log-bilinear language model is a neural net-
work modelling a probability distribution over the
next word in a sequence given the previous n?1,
i.e. p(w
n
|w
1:n?1
). Let |V | be the size of our vo-
cabulary, and R be a |V | ? d vocabulary matrix
where the R
w
i
demnotes the row containing the
word embedding in R
d
of a word w
i
, with d be-
ing a hyper-parameter indicating embedding size.
Let C
i
be the context transform matrix in R
d?d
which modifies the representation of the ith word
in the word history. Let b
w
i
be a scalar bias as-
sociated with a word w
i
, and b
R
be a bias vector
in R
d
associated with the model. A log-bilinear
model expressed the probability of w
n
given a his-
tory of n?1 words as a function of the energy of
the network:
E(w
n
;w
1:n?1
) =
?
(
n?1
?
i=1
R
T
w
i
C
i
)
R
w
n
? b
T
R
R
w
n
? b
w
n
From this, the probability distribution over the
next word is obtained:
p(w
n
|w
1:n?1
) =
e
?E(w
n
;w
1:n?1
)
?
w
n
e
?E(w
n
;w
1:n?1
)
To reframe a log-bilinear language model as a
conditional language model (CNLM), illustrated
?
w
n
w
n-1
w
n-2
w
n-3
Figure 2: Diagrammatic representation of a Con-
ditional Neural Language Model.
in Fig. 2, let us suppose that we wish to jointly
condition the next word on its history and some
variable ?, for which an embedding r
?
has been
obtained through a previous step, in order to com-
pute p(w
n
|w
1:n?1
, ?). The simplest way to do this
additively, which allows us to treat the contribu-
tion of the embedding for ? as similar to that of an
extra word in the history. We define a new energy
function:
E(w
n
;w
1:n?1
, ?) =
?
((
n?1
?
i=1
R
T
w
i
C
i
)
+ r
T
?
C
?
)
R
w
n
? b
T
R
R
w
n
? b
w
n
to obtain the probability
p(w
n
|w
1:n?1
, ?) =
e
?E(w
n
;w
1:n?1
,?)
?
w
n
e
?E(w
n
;w
1:n?1
,?)
Log-bilinear language models and their condi-
tional variants alike are typically trained by max-
imising the log-probability of observed sequences.
3.3 A Combined Semantic Parsing Model
The models in ??3.1?3.2 can be combined to form
a model capable of jointly learning a shared la-
tent representation for question/query pairs using
a BiCVM, and using this latent representation to
learn a conditional log-bilinear CNLM. The full
model is shown in Fig. 3. Here, we explain the
final model architecture both for training and for
subsequent use as a generative model. The details
of the training procedure will be discussed in ?3.4.
The combination is fairly straightforward, and
happens in two steps at training time. For the
24
...
Knowledgebase query
Question
Latent
representation
Query embedding
Question embedding
Relation/object
embeddings
Word embeddings
Conditional
Log-bilinear
Language Model
g
h
Figure 3: Diagrammatic representation of the full
model. First the mappings for obtaining latent
forms of questions and queries are jointly learned
through a BiCVM. The latent form for questions
then serves as conditioning element in a log-
bilinear CNLM.
first step, shown in the left hand side of Fig. 3,
a BiCVM is trained against a parallel corpora
of natural language question and knowledgebase
query pairs. Optionally, the embeddings for the
query symbol representations and question words
are initialised and/or fine-tuned during training,
as discussed in ?3.4. For the natural language
side of the model, the composition function g can
be a simple additive model as in Hermann and
Blunsom (2014a), although the semantic informa-
tion required for the task proposed here would
probably benefit from a more complex composi-
tion function such as a convolution neural net-
work. Function h, which maps the knowledgebase
queries into the shared space could also rely on
convolution, although the structure of the database
queries might favour a setup relying primarily on
bi-gram composition.
Using function g and the original training data,
the training data for the second stage is created
by obtaining the latent representation for the ques-
tions of the original dataset. We thereby obtain
pairs of aligned latent question representations and
knowledgebase queries. This data allows us to
train a log-bilinear CNLM as shown on the right
side of Fig. 3.
Once trained, the models can be fully joined to
produce a generative neural network as shown in
Fig. 4. The network modelling g from the BiCVM
...
Question
Generated Query
g
Figure 4: Diagrammatic representation of the final
network. The question-compositional segment of
the BiCVM produces a latent representation, con-
ditioning a CNLM generating a query.
takes the distributed representations of question
words from unseen questions, and produces a la-
tent representation. The latent representation is
then passed to the log-bilinear CNLM, which con-
ditionally generates a knowledgebase query corre-
sponding to the question.
3.4 Learning Model Parameters
We propose training the model of ?3.3 in a two
stage process, in line with the symbolic model of
Kwiatkowski et al. (2013).
First, a BiCVM is trained on a parallel corpus
C of question-query pairs ?Q,R? ? C, using com-
position functions g for natural language questions
and h for database queries. While functions g and
hmay differ from those discussed in Hermann and
Blunsom (2014a), the basic noise-contrastive op-
timisation function remains the same. It is possi-
ble to initialise the model fully randomly, in which
25
case the model parameters ? learned at this stage
include the two distributed representation lexica
for questions and queries, D
Q
and D
R
respec-
tively, as well as all parameters for g and h.
Alternatively, word embeddings inD
Q
could be
initialised with representations learned separately,
for instance with a neural language model or a
similar system (Mikolov et al., 2010; Turian et al.,
2010; Collobert et al., 2011, inter alia). Likewise,
the relation and object embeddings inD
R
could be
initialised with representations learned from dis-
tributed relation extraction schemas such as that
of Riedel et al. (2013).
Having learned representations for queries in
D
R
as well as function g, the second training phase
of the model uses a new parallel corpus consisting
of pairs ?g(Q), R? ? C
?
to train the CNLM as pre-
sented in ?3.3.
The two training steps can be applied iteratively,
and further, it is trivial to modify the learning
procedure to use composition function h as an-
other input for the CNLM training phrase in an
autoencoder-like setup.
4 Experimental Requirements and
Further Work
The particular training procedure for the model
described in this paper requires aligned ques-
tion/knowledgebase query pairs. There exist some
small corpora that could be used for this task
(Zelle and Mooney, 1996; Cai and Yates, 2013). In
order to scale training beyond these small corpora,
we hypothesise that larger amounts of (potentially
noisy) training data could be obtained using a
boot-strapping technique similar to Kwiatkowski
et al. (2013).
To evaluate this model, we will follow the ex-
perimental setup of Kwiatkowski et al. (2013).
With the provisio that the model can generate
freebase queries correctly, further work will seek
to determine whether this architecture can gener-
ate other structured formal language expressions,
such as lambda expressions for use in textual en-
tailement tasks.
Acknowledgements
This work was supported by a Xerox Foundation
Award, EPSRC grants number EP/I03808X/1 and
EP/K036580/1, and the Canadian Institute for Ad-
vanced Research (CIFAR) Program on Adaptive
Perception and Neural Computation.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexi-
con Extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Sofia, Bulgaria,
August. Association for Computational Linguistics.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of the 2nd International
Conference on Learning Representations, Banff,
Canada, April.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual Models for Compositional Distributional
Semantics. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Baltimore, USA,
June. Association for Computational Linguistics.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic Frame Iden-
tification with Distributed Word Representations. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Baltimore, USA, June. Association
for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013a. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA. Association for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current convolutional neural networks for discourse
compositionality. arXiv preprint arXiv:1306.3584.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
26
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.
2014. Learning multilingual word representa-
tions using a bag-of-words autoencoder. CoRR,
abs/1401.1803.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 590?599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke S.
Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In Proceedings of the 29th Inter-
national Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1,
2012.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
?13), June.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of EMNLP-CoNLL, pages 1201?1211.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017?1024.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL, Stroudsburg, PA, USA.
Wen-Tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning Discrimina-
tive Projections for Text Similarity Measures. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?11,
pages 247?256, Stroudsburg, PA, USA. Association
for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 1050?1055.
27
