Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1368?1378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning to Map into a Universal POS Tagset
Yuan Zhang, Roi Reichart, Regina Barzilay
Massachusetts Institute of Technology
{yuanzh, roiri, regina}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
We present an automatic method for mapping
language-specific part-of-speech tags to a set
of universal tags. This unified representation
plays a crucial role in cross-lingual syntactic
transfer of multilingual dependency parsers.
Until now, however, such conversion schemes
have been created manually. Our central hy-
pothesis is that a valid mapping yields POS
annotations with coherent linguistic proper-
ties which are consistent across source and
target languages. We encode this intuition
in an objective function that captures a range
of distributional and typological characteris-
tics of the derived mapping. Given the ex-
ponential size of the mapping space, we pro-
pose a novel method for optimizing over soft
mappings, and use entropy regularization to
drive those towards hard mappings. Our re-
sults demonstrate that automatically induced
mappings rival the quality of their manually
designed counterparts when evaluated in the
context of multilingual parsing.1
1 Introduction
In this paper, we explore an automatic method for
mapping language-specific part-of-speech tags to a
universal tagset. In multilingual parsing, this uni-
fied input representation is required for cross-lingual
syntactic transfer. Specifically, the universal tagset
annotations enable an unlexicalized parser to capi-
talize on annotations from one language when learn-
ing a model for another.
1The source code and data for the work presented in this
paper is available at http://groups.csail.mit.edu/
rbg/code/unitag/emnlp2012
While the notion of a universal POS tagset is
widely accepted, in practice it is hardly ever used
for annotation of monolingual resources. In fact,
available POS annotations are designed to capture
language-specific idiosyncrasies and therefore are
substantially more detailed than a coarse universal
tagset. To reconcile these cross-lingual annotation
differences, a number of mapping schemes have
been proposed in the parsing community (Zeman
and Resnik, 2008; Petrov et al 2011; Naseem et
al., 2010). In all of these cases, the conversion is
performed manually and has to be repeated for each
language and annotation scheme anew.
Despite the apparent simplicity, deriving a map-
ping is by no means easy, even for humans. In fact,
the universal tagsets manually induced by Petrov
et al(2011) and by Naseem et al(2010) disagree
on 10% of the tags. An example of such discrep-
ancy is the mapping of the Japanese tag ?PVfin? to
the universal tag ?particle? according to one scheme,
and to ?verb? according to another. Moreover, the
quality of this conversion has a direct implication on
the parsing performance. In the Japanese example
above, this difference in mapping yields a 6.7% dif-
ference in parsing accuracy.
The goal of our work is to induce the mapping
for a new language, utilizing existing manually-
constructed mappings as training data. The exist-
ing mappings developed in the parsing community
rely on gold POS tags for the target language. A
more realistic scenario is to employ the mapping
technique to resource-poor languages where gold
POS annotations are lacking. In such cases, a map-
ping algorithm has to operate over automatically in-
1368
duced clusters on the target language (e.g., using
the Brown algorithm) and convert them to universal
tags. We are interested in a mapping approach that
can effectively handle both gold tags and induced
clusters.
Our central hypothesis is that a valid mapping
yields POS annotations with coherent linguistic
properties which are consistent across languages.
Since universal tags play the same linguistic role
in source and target languages, we expect similar-
ity in their global distributional statistics. Figure 1a
shows statistics for two close languages, English and
German. We can see that their unigram frequencies
on the five most common tags are very close. Other
properties concern POS tag per sentence statistics ?
e.g., every sentence has to have at least one verb. Fi-
nally, the mappings can be further constrained by ty-
pological properties of the target language that spec-
ify likely tag sequences. This information is readily
available even for resource poor language (Haspel-
math et al 2005). For instance, since English and
German are prepositional languages, we expect to
observe adposition-noun sequences but not the re-
verse (see Figure 1b for sample sentences). We en-
code these heterogeneous properties into an objec-
tive function that guides the search for the optimal
mapping.
Having defined a quality measure for mappings,
our goal is to find the optimal mapping. However,
such partition optimization problems2 are NP hard
(Garey and Johnson, 1979). A naive approach to
the problem is to greedily improve the map, but it
turns out that this approach yields poor quality map-
pings. We therefore develop a method for optimiz-
ing over soft mappings, and use entropy regulariza-
tion to drive those towards hard mappings. We con-
struct the objective in a way that facilitates simple
monotonically improving updates corresponding to
solving convex optimization problems.
We evaluate our mapping approach on 19
languages that include representatives of Indo-
European, Semitic, Basque, Japonic and Turkic fam-
ilies. We measure mapping quality based on the
target language parsing accuracy. In addition to
considering gold POS tags for the target language,
2Instances of related hard problems are 3-partition and
subset-sum.
we also evaluate the mapping algorithm on auto-
matically induced POS tags. In all evaluation sce-
narios, our model consistently rivals the quality
of manually induced mappings. We also demon-
strate that the proposed inference procedure outper-
forms greedy methods by a large margin, highlight-
ing the importance of good optimization techniques.
We further show that while all characteristics of
the mapping contribute to the objective, our largest
gain comes from distributional features that capture
global statistics. Finally, we establish that the map-
ping quality has a significant impact on the accuracy
of syntactic transfer, which motivates further study
of this topic.
2 Related Work
Multilingual Parsing Early approaches for multi-
lingual parsing used parallel data to bridge the gap
between languages when modeling syntactic trans-
fer. In this setup, finding the mapping between var-
ious POS annotation schemes was not essential; in-
stead, the transfer algorithm could induce it directly
from the parallel data (Hwa et al 2005; Xi and
Hwa, 2005; Burkett and Klein, 2008). However,
more recent transfer approaches relinquish this data
requirement, learning to transfer from non-parallel
data (Zeman and Resnik, 2008; McDonald et al
2011; Cohen et al 2011; Naseem et al 2010).
These approaches assume access to a common input
representation in the form of universal tags, which
enables the model to connect patterns observed in
the source language to their counterparts in the tar-
get language.
Despite ongoing efforts to standardize POS tags
across languages (e.g., EAGLES initiative (Eynde,
2004)), many corpora are still annotated with
language-specific tags. In previous work, their map-
ping to universal tags was performed manually. Yet,
even though some of these mappings have been de-
veloped for the same CoNLL dataset (Buchholz and
Marsi, 2006; Nivre et al 2007), they are not identi-
cal and yield different parsing performance (Zeman
and Resnik, 2008; Petrov et al 2011; Naseem et al
2010). The goal of our work is to automate this pro-
cess and construct mappings that are optimized for
performance on downstream tasks (here we focus on
parsing). As our results show, we achieve this goal
1369
Noun Verb Det. Prep. Adj.0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Unigr
amFr
equen
cy
EnglishGerman -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases]and sales [by corporate insiders]
-Einer der sich [f?r den Milliard?r] [ausspricht] [ist] Steve Jobs dem Perot [f?r den aufbau]der Computerfirma Next 20 Millionen Dollar [bereitstellte]
(a) (b)
Figure 1: Illustration of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five
tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in
blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions.
on a broad range of languages and evaluation sce-
narios.
Syntactic Category Refinement Our work also
relates to work in syntactic category refinement in
which POS categories and parse tree non-terminals
are refined in order to improve parsing perfor-
mance (Finkel et al 2007; Klein and Manning,
2003; Matsuzaki et al 2005; Petrov et al 2006;
Petrov and Klein, 2007; Liang et al 2007). Our
work differs from these approaches in two ways.
First, these methods have been developed in the
monolingual setting, while our mapping algorithm is
designed for multilingual parsing. Second, these ap-
proaches are trained on the syntactic trees of the tar-
get language, which enables them to directly link the
quality of newly induced categories with the quality
of syntactic parsing. In contrast, we are not given
trees in the target language. Instead, our model is
informed by mappings derived for other languages.
3 Task Formulation
The input to our task consists of a target corpus writ-
ten in a language T , and a set of non-parallel source
corpora written in languages {S1, . . . , Sn}. In the
source corpora, each word is annotated with both
a language-specific POS tag and a universal POS
tag (Petrov et al 2011). In the target corpus each
word is annotated only with a language-specific POS
tag, either gold or automatically induced.
Our goal is to find a map from the set of LT target
language tags to the set of K universal tags. We as-
sume that each language-specific tag is only mapped
to one universal tag, which means we never split a
language-specific tag and LT ? K holds for every
language. We represent the map by a matrix A of
size K ? LT where A(c|f) = 1 if the target lan-
guage tag f is mapped to the universal tag c, and
A(c|f) = 0 otherwise.3 Note that each column of
A should contain a single value of 1. We will later
relax the requirement thatA(c|f) ? {0, 1}. A candi-
date mappingA can be applied to the target language
to produce sentences labeled with universal tags.
4 Model
In this section we describe an objective that reflects
the quality of an automatic mapping.
Our key insight is that for a good mapping, the
statistics over the universal tags should be similar for
source and target languages because these tags play
the same role cross-linguistically. For example, we
should expect the frequency of a particular universal
tag to be similar in the source and target languages.
One choice to make when constructing an objec-
tive is the source languages to which we want to be
similar. It is clear that choosing all languages is not a
good idea, since they are not all expected to have dis-
tributional properties similar to the target language.
There is strong evidence that projecting from sin-
gle languages can lead to good parsing performance
3We use c and f to reflect the fact that universal tags are
a coarse version (hence c) of the language specific fine tags
(hence f ).
1370
(McDonald et al 2011). Therefore, our strategy is
to choose a single source language for comparison.
The choice of the source language is based on sim-
ilarity between typological properties; we describe
this in detail in Section 5.
We must also determine which statistical proper-
ties we expect to be preserved across languages. Our
model utilizes three linguistic phenomena which are
consistent across languages: POS tag global distri-
butional statistics, POS tag per sentence statistics,
and typology-based ordering statistics. We define
each of these below.
4.1 Mapping Characterization
We focus on three categories of mapping properties.
For each of the relevant statistics we define a func-
tion Fi(A) that has low values if the source and tar-
get statistics are similar.
Global distributional statistics: The unigram and
bigram statistics of the universal tags are expected
to be similar across languages with close typological
profiles. We use pS(c1, c2) to denote the bigram dis-
tribution over universal tags in the source language,
and pT (f1, f2) to denote the bigram distribution over
language specific tags in the target language. The
bigram distribution over universal tags in the target
language depends on A and pT (f1, f2) and is given
by:
pT (c1, c2;A) =
?
f1,f2
A(c1|f1)A(c2|f2)pT (f1, f2)
(1)
To enforce similarity between source and target dis-
tributions, we wish to minimize the KL divergence
between the two: 4
Fbi(A) = DKL[pS(c1, c2)|pT (c1, c2;A)] (2)
We similarly define Funi(A) as the distance be-
tween unigram distributions.
Per sentence statistics: Another defining property
of POS tags is their average count per sentence.
Specifically, we focus on the verb count per sen-
tence, which we expect be similar across languages.
4We use the KL divergence because it assigns low weights
to infrequent universal tags. Furthermore, this choice results in
a simple, EM-like parameter estimation algorithm as discussed
in Section 5.
To express this constraint, we use nv(s,A) to
denote the number of verbs (i.e., the universal
tags corresponding to verbs according to A) in
sentence s. This is a linear function of A. We also
use E[nv(s,A)] to denote the average number of
verbs per sentence, and V [nv(s,A)] to denote the
variance. We estimate these two statistics from
the source language and denote them by ESv, VSv.
Good mappings are expected to follow these
patterns by having a variance upper bounded by
VSv and an average lower bounded by ESv.5 This
corresponds to minimizing the following objectives:
FEv(A) = max [0, ESv ? E[nv(s,A)]]
FV v(A) = max [0, V [nv(s,A)]? VSv]
Note that the above objectives are convex in A,
which will make optimization simpler. We refer to
the two terms jointly as Fverb(A).
Typology-based ordering statistics: Typolog-
ical features can be useful for determining the
relative order of different tags. If we know that
the target language has a particular typological
feature, we expect its universal tags to obey the
given relative ordering. Specifically, we expect it to
agree with ordering statistics for source languages
with a similar typology. We consider two such
features here. First, in pre-position languages the
preposition is followed by the noun phrase. Thus, if
T is such a language, we expect the probability of
a noun phrase following the adposition to be high,
i.e., cross some threshold. Formally, we define C1 =
{noun, adj, num, pron, det} and consider the set of
bigram distributions Spre that satisfy the following
constraint:
?
c?C1
pT (adp,c) ? apre (3)
where apre =
?
c?C1 pS(adp,c) is calculated from
the source language. This constraint set is non-
convex in A due to the bilinearity of the bi-
gram term. To simplify optimization6 we take an
5The rationale is that we want to put a lower bound on the
number of verbs per sentence, and induce it from the source
language. Furthermore, we expect the number of verbs to be
well concentrated, and we induce its maximal variance from
the source language.
6In Section 5 we shall see that this makes optimization eas-
ier.
1371
approach inspired by the posterior regularization
method (Ganchev et al 2010) and use the objective:
Fc(A) = min
r(c1,c2)?Spre
DKL[r(c1, c2)|pT (c1, c2;A)]
(4)
The above objective will attain lower values for A
such that pT (c1, c2;A) is close to the constraint set.
Specifically, it will have a value of zero when the
bigram distribution induced by A has the property
specified in Spre. We similarly define a set Spost
for post-positional languages.
As a second typological feature, we consider the
Demonstrative-Noun ordering. In DN languages we
want the probability of a determiner to come be-
fore C2 = {noun, adj, num}, (i.e., frequent universal
noun-phrase tags), to cross a threshold. This con-
straint translates to:
?
c?C2
pT (det, c) ? adet (5)
where adet =
?
c?C2 pS(det, c) is a threshold de-
termined from the source language. We denote the
set of distributions that have this property by SDN,
and add them to the constraint in (4). The overall
constraint set is denoted by S.
4.2 The Overall Objective
We have defined a set of functions Fi(A) that are
expected to have low values for good mappings. To
combine those, we use a weighted sum: F?(A) =?
i ?i ? Fi(A). (The weights in this equation are
learned; we discussed the procedure in Section 5)
Optimizing over the set of mappings is difficult
since each mapping is a discrete set whose size is
exponential size in LT . Technically, the difficulty
comes from the requirement that elements of A are
integral and its columns sum to one. To relax this
restriction, we will allow A(c|f) ? [0, 1] and en-
courage A to correspond to a mapping by adding an
entropy regularization term:
H[A] = ?
?
f
?
c
A(c|f) logA(c|f) (6)
This term receives its minimal value when the con-
ditional probability of the universal tags given a
language-specific tag is 1 for one universal tag and
zero for the others.
The overall objective is then: F (A) = F?(A) +
? ?H[A], where ? is the weight of the entropy term.7
The resulting optimization problem is:
min
A??
F (A) (7)
where ? is the set of non-negative matrices whose
columns sum to one:
? =
{
A :
A(c|f) ? 0 ?c, f
?K
c=1A(c|f) = 1 ?f
}
(8)
5 Parameter Estimation
In this section we describe the parameter estimation
process for our model. We start by describing how
to optimize A. Next, we discuss the weight selec-
tion algorithm, and finally the method for choosing
source languages.
5.1 Optimizing the Mapping A
Recall that our goal is to solve the optimization
problem in Eq. (7). This objective is non convex
since the function H[A] is concave, and the objec-
tive F (A) involves bilinear terms in A and loga-
rithms of their sums (see Equations (1) and (2)).
While we do not attempt to solve the problem
globally, we do have a simple update scheme that
monotonically decreases the objective. The update
can be derived in a similar manner to expectation
maximization (EM) (Neal and Hinton, 1999) and
convex concave procedures (Yuille and Rangarajan,
2003). Figure 2 describes our optimization algo-
rithm. The key ideas in deriving it are using pos-
terior distributions as in EM, and using a variational
formulation of entropy. The term Fc(A) is handled
in a similar way to the posterior regularization algo-
rithm derivation. A detailed derivation is provided
in the supplementary file. 8
The kth iteration of the algorithm involves several
steps:
? In step 1, we calculate the current esti-
mate of the bigram distribution over tags,
pT (c1, c2;Ak).
7Note that as ? ? ?, only valid maps will be selected by
the objective.
8The supplementary file is available at http://groups.
csail.mit.edu/rbg/code/unitag/emnlp2012.
1372
? In step 2, we find the bigram distribution in
the constraint set S that is closest in KL di-
vergence to pT (c1, c2;Ak), and denote it by
rk(c1, c2). This optimization problem is con-
vex in r(c1, c2).
? In step 3, we calculate the bigram posterior
over language specific tags given a pair of uni-
versal tags. This is analogous to the standard
E-step in EM.
? In step 4, we use the posterior in step 3 and the
bigram distributions pS(c1, c2) and rk(c1, c2)
to obtain joint counts over language specific
and universal bigrams.
? In step 5, we use the joint counts from step 4
to obtain counts over pairs of language specific
and universal tags.
? In step 6, analogous to the M-step in EM, we
optimize over the mapping matrix A. The ob-
jective is similar to the Q function in EM, and
also includes the Fverb(A) term, and a linear
upper bound on the entropy term. The objec-
tive can be seen to be convex in A.
As mentioned above, each of the optimization prob-
lems in steps 2 and 6 is convex, and can therefore be
solved using standard convex optimization solvers.
Here, we use the CVX package (Grant and Boyd,
2008; Grant and Boyd, 2011). It can be shown that
the algorithm improves F (A) at every iteration and
converges to a local optimum.
The above algorithm generates a mapping A that
may contain fractional entries. To turn it into a hard
mapping we round A by mapping each f to the c
that maximizes A(c|f) and then perform greedy im-
provement steps (one f at a time) to further improve
the objective. The regularization constant ? is tuned
to minimize the F?(A) value of the rounded A.
5.2 Learning the Objective Weights
Our F?(A) objective is a weighted sum of the in-
dividual Fi(A) functions. In the following, we de-
scribe how to learn the ?i weights for every target
language. We would like F?(A) to have low values
when A is a good map. Since our performance goal
is parsing accuracy, we consider a map to be good
Initialize A0.
Repeat
Step 1 (calculate current bigram estimate):
pT (c1, c2;A
k) =
?
f1,f2
Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
Step 2 (incorporate constraints):
rk(c1, c2) = arg min
r?S
DKL[r(c1, c2)|pT (c1, c2;A
k)]
Step 3 (calculate model posterior):
p(f1, f2|c1, c2;Ak) ? Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
Step 4: (complete joint counts):
Nk(c1, c2, f1, f2) = p(f1, f2|c1, c2;A
k)
(
rk(c1, c2) + pS(c1, c2)
)
Step 5 (obtain pairwise):
Mk(c, f) = Nk1 (c, f) +N
k
2 (c, f)
where Nk1 (c, f) =
?
c2,f2
Nk(c, c2, f, f2) and similarly for
Nk2 (c, f).
Step 6 (M step with entropy linearization): Set Ak+1 to be the
solution of
min
A??
?
?
c,f
[
Mk(c, f) logA(c|f) + A(c|f) logAk(c|f)
]
+ Fverb(A)
Until Convergence of Ak
Figure 2: An iterative algorithm for minimizing our ob-
jective in Eq. (7). For simplicity we assume that all the
weights ?i and ? are equal to one. It can be shown that
the objective monotonically decreases in every iteration.
if it results in high parsing accuracy, as measured
when projecting a parser from to S to T .
Since we do not have annotated parses in T , we
use the other source languages S = {S1, . . . , Sn}
to learn the weight. For each Si as the target, we
first train a parser for each language in S \ {Si} as
if it was the source, using the map of Petrov et al
(2011), and choose S?i ? S \ {Si} which gives the
highest parsing accuracy on Si. Next we generate
7000 candidate mappings for Si by randomly per-
turbing the map of (Petrov et al 2011). We evalu-
ate the quality of each candidate A by projecting the
parser of S?i to Si, and recording the parsing accu-
racy. Among all the candidates we choose the high-
est accuracy one and denote it by A?(Si). We now
want the score F (A?(Si)) to be lower than that of all
other candidates. To achieve this, we train a ranking
SVM whose inputs are pairs of mapsA?(Si) and an-
1373
other worse A(Si). These map pairs are taken from
many different traget languages, i.e. many different
Si. The features given to the SVM are the terms of
the score Fi(A). The goal of the SVM is to weight
these terms such that the better map A?(Si) has a
lower score. The weights assigned by the SVM are
taken as ?i.
5.3 Source Language Selection
As noted in Section 4 we construct F (A) by choos-
ing a single source language S. Here we describe the
method for choosing S. Our goal is to choose S that
is closest to T in terms of typology. Assume that
languages are described by binary typological vec-
tors vL. We would like to learn a diagonal matrix
D such that d(S, T ;D) = (vS ? vT )TD(vS ? vT )
reflects the similarity between the languages. In our
context, a good measure of similarity is the perfor-
mance of a parser trained on S and projected on T
(using the optimal map A). We thus seek a matrix
D such that d(S, T ;D) is ranked according to the
parsing accuracy. The matrix D is trained using an
SVM ranking algorithm that tries to follow the rank-
ing of parsing accuracy. Similar to the technique for
learning the objective weights, we train across many
pairs of source languages.9
The typological features we use are a subset
of the features described in ?The World Atlas of
Languages Structure? (WALS, (Haspelmath et al
2005)), and are shown in Table 1.
6 Evaluation Set-Up
Datasets We test our model on 19 languages: Ara-
bic, Basque, Bulgarian, Catalan, Chinese, Czech,
Danish, Dutch, English, German, Greek, Hungar-
ian, Italian, Japanese, Portuguese, Slovene, Span-
ish, Swedish, and Turkish. Our data is taken from
the CoNLL 2006 and 2007 shared tasks (Buch-
holz and Marsi, 2006; Nivre et al 2007). The
CoNLL datasets consist of manually created depen-
dency trees and language-specific POS tags. Fol-
lowing Petrov et al(2011), our model maps these
language-specific tags to a set of 12 universal tags:
noun, verb, adjective, adverb, pronoun, determiner,
adposition, numeral, conjunction, particle, punctua-
tion mark and X (a general tag).
9Ties are broken using the F (A) objective.
Evaluation Procedure We perform a separate ex-
periment for each of the 19 languages as the tar-
get and a source language chosen from the rest (us-
ing the method from Section 5.3). For the selected
source language, we assume access to the mapping
of Petrov et al(2011).
Evaluation Measures We evaluate the quality of
the derived mapping in the context of the target lan-
guage parsing accuracy. In both the training and
test data, the language-specific tags are replaced
with universal tags: Petrov?s tags for the source lan-
guages and learned tags for the target language. We
train two non-lexicalized parsers using source anno-
tations and apply them to the target language. The
first parser is a non-lexicalized version of the MST
parser (McDonald et al 2005) successfully used in
the multilingual context (McDonald et al 2011). In
the second parser, parameters of the target language
are estimated as a weighted mixture of parameters
learned from supervised source languages (Cohen et
al., 2011). For the parser of Cohen et al(2011), we
trained the model on the four languages used in the
original paper ? English, German, Czech and Ital-
ian. When measuring the performance on each of
these four languages, we selected another set of four
languages with a similar level of diversity.10
Following the standard evaluation practice in
parsing, we use directed dependency accuracy as our
measure of performance.
Baselines We compare mappings induced by our
model against three baselines: the manually con-
structed mapping of Petrov et al(2011), a randomly
constructed mapping and a greedy mapping. The
greedy mapping uses the same objective as our full
model, but optimizes it using a greedy method. In
each iteration, this method makes |LT | passes over
the language-specific tags, selecting a substitution
that contributes the most to the objective.
Initialization To reduce the dimension of our al-
gorithm?s search space and speed up our method, we
start by clustering the language-specific POS tags of
the target into |K| = 12 clusters using an unsuper-
10We also experimented with a version of the Cohen et al
(2011) model trained on all the source languages. This set-up
resulted in decreased performance. For this reason, we chose to
train the model on the four languages.
1374
ID Feature Description Values
81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV
85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions
86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive
87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective
88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative, before and after
Table 1: The set of typological features that we use for source language selection. The first column gives the ID of
the feature as listed in WALS. The second column describes the feature and the last column enumerates the allowable
values for each feature; besides these values each feature can also have a value of ?No dominant order?.
vised POS induction algorithm (Lee et al 2010).11
Our mapping algorithm then learns the connection
between these clusters and universal tags.
For initialization, we perform multiple random
restarts and select the one with the lowest final ob-
jective score.
7 Results
We first present the results of our model using the
gold POS tags for the target language. Table 2 sum-
marizes the performance of our model and the base-
lines.
Comparison against Baselines On average, the
mapping produced by our model yields parsers with
higher accuracy than all of the baselines. These re-
sults are consistent for both parsers (McDonald et
al., 2011; Cohen et al 2011). As expected, random
mappings yield abysmal results ? 20.2% and 12.7%
for the two parsers. The low accuracy of parsers that
rely on the Greedy mapping ? 29.9% and 25.4% ?
show that a greedy approach is a poor strategy for
mapping optimization.
Surprisingly, our model slightly outperforms the
mapping of (Petrov et al 2011), yielding an aver-
age accuracy of 56.7% as compared to the 55.4%
achieved by its manually constructed counterpart for
the direct transfer method (McDonald et al 2011).
Similar results are observed for the mixture weights
parser (Cohen et al 2011). The main reason for
these differences comes from mistakes introduced in
the manual mapping. For example, in Czech tag ?R?
is labeled as ?pronoun?, while actually it should be
mapped to ?adposition?. By correcting this mistake,
we gain 5% in parsing accuracy for the direct trans-
fer parser.
11This pre-clustering results in about 3% improvement, pre-
sumably since it uses contextual information beyond what our
algorithm does.
Overall, the manually constructed mapping and
our model?s output disagree on 21% of the assign-
ments (measured on the token level). However,
the extent of disagreement is not necessarily predic-
tive of the difference in parsing performance. For
instance, the manual and automatic mappings for
Catalan disagree on 8% of the tags and their pars-
ing accuracy differs by 5%. For Greek on the other
hand, the disagreement between mappings is much
higher ? 17%, yet the parsing accuracy is very
close. This phenomenon shows that not all mistakes
have equal weight. For instance, a confusion be-
tween ?pronoun? and ?noun? is less severe in the
parsing context than a confusion between ?pronoun?
and ?adverb?.
Impact of Language Selection To assess the
quality of our language selection method, we com-
pare the model against an oracle that selects the best
source for a given target language. As Table 2 shows
our method is very close to the oracle performance,
with only 0.7% gap between the two. In fact, for
10 languages our method correctly predicts the best
pairing. This result is encouraging in other contexts
as well. Specifically, McDonald et al(2011) have
demonstrated that projecting from a single oracle-
chosen language can lead to good parsing perfor-
mance, and our technique may allow such projection
without an oracle.
Relations between Objective Values and Opti-
mization Performance The suboptimal perfor-
mance of the Greedy method shows that choosing
a good optimization strategy plays a critical role in
finding the desired mapping. A natural question to
ask is whether the objective value is predictive of the
end goal parsing performance. Figure 3 shows the
objective values for the mappings computed by our
method and the baselines for four languages. Over-
1375
Direct Transfer Parser (Accuracy) Mixture Weight Parser (Accuracy)
Tag Diff.
Random Greedy Petrov Model Best Pair Random Greedy Petrov Model.
Catalan 15.9 32.5 74.8 79.3 79.3 12.6 24.6 65.6 73.9 8.8
Italian 16.4 41.0 68.7 68.3 71.4 11.7 33.5 64.2 61.9 6.7
Portuguese 15.8 24.6 72.0 75.1 75.1 10.7 14.1 70.4 72.6 12.2
Spanish 11.5 27.4 72.1 68.9 68.9 6.4 26.5 58.8 62.8 7.5
Danish 35.5 23.7 46.6 46.5 49.2 4.2 23.7 51.4 51.7 5.0
Dutch 18.0 22.1 58.2 56.8 57.3 7.1 15.3 54.9 53.2 4.9
English 14.7 19.0 51.6 49.0 49.0 13.3 15.1 47.5 41.8 17.7
German 15.8 24.3 55.7 50.4 51.6 20.9 18.7 52.4 51.8 15.0
Swedish 15.1 26.3 63.1 63.1 63.1 9.1 36.5 55.7 55.9 8.2
Bulgarian 17.4 28.0 51.6 63.4 63.4 22.6 39.9 64.6 60.4 35.7
Czech 19.0 34.4 47.7 57.3 57.3 12.7 26.2 48.3 55.7 28.5
Slovene 15.6 21.8 43.5 51.4 52.8 11.3 20.7 42.2 53.0 38.8
Greek 17.3 19.5 62.3 59.7 59.8 22.0 15.2 56.2 57.0 17.0
Hungarian 28.4 44.1 53.8 52.3 52.3 4.0 43.8 46.4 51.7 18.1
Arabic 22.1 45.4 51.5 51.2 52.9 3.9 40.9 48.3 51.1 15.7
Basque 18.0 19.2 27.9 33.1 35.1 6.3 8.3 32.3 30.6 43.8
Chinese 22.4 34.1 46.0 47.6 49.5 17.7 34.9 44.0 40.4 38.1
Japanese 36.5 46.2 51.4 53.6 53.6 15.4 18.0 25.7 28.7 73.8
Turkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9
Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3
Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). The second section
is for the weighted mixture parsing model (Cohen et al 2011). The first two columns (Random and Greedy) of each
section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the
results when the mapping of Petrov et al(2011) is used. The fourth column (Model) shows the results when our
mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best
source language is selected for every target language. The last column (Tag Diff.) presents the difference between our
mapping and the mapping of Petrov et al(2011) by showing the percentage of target language tokens for which the
two mappings select a different universal tag.
all, our method and the manual mappings reach sim-
ilar values, both considerably better than other base-
lines. While the parsing performance correlates with
the objective, the correlation is not perfect. For in-
stance, on Greek our mapping has a better objective
value, but lower parsing performance.
Ablation Analysis We next analyze the contribu-
tion of each component of our objective to the result-
ing performance.12 The strongest factor in our ob-
jective is the distributional features capturing global
statistics. Using these features alone achieves an
average accuracy of 51.1%, only 5.6% less than
the full model score. Adding just the verb-related
constraints to the distributional similarity objectives
improves the average model performance by 2.1%.
12The results are consistent for both parsers, here we report
the accuracy for the direct transfer method (McDonald et al
2011).
Adding just the typological constraints yields a very
modest performance gain of 0.5%. This is not sur-
prising ? the source language is selected to be typo-
logically similar to the target language, and thus its
distributional properties are consistent with typolog-
ical features. However, adding both the verb-related
constraints and the typological constraints results in
a synergistic performance gain of 5.6% over the dis-
tributional similarity objective, a gain which is much
better than the sum of the two individual gains.
Application to Automatically Induced POS Tags
A potential benefit of the proposed method is to re-
late automatically induced clusters in the target lan-
guage to universal tags. In our experiments, we in-
duce such clusters using Brown clustering,13 which
13In our experiments, we employ Liang?s implementation
http://cs.stanford.edu/?pliang/software/. The number of clus-
ters is set to 30.
1376
Catalan German Greek Arabic0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
 
 ModelPetrovGreedyRandom
Figure 3: Objective values for the different mappings
used in our experiments for four languages. Note that
the goal of the optimization procedure is to minimize the
objective value.
has been successfully used for similar purposes in
parsing research (Koo et al 2008). We then map
these clusters to the universal tags using our algo-
rithm.
The average parsing accuracy on the 19 languages
is 45.5%. Not surprisingly, automatically induced
tags negatively impact parsing performance, yield-
ing a decrease of 11% when compared to mappings
obtained using manual POS annotations (see Ta-
ble 2). To further investigate the impact of inaccu-
rate tags on the mapping performance, we compare
our model against the oracle mapping model that
maps each cluster to the most common universal tag
of its members. Parsing accuracy obtained using this
method is 45.1%, closely matching the performance
of our mapping algorithm.
An alternative approach to mapping words into
universal tags is to directly partition words into K
clusters (without passing through language specific
tags). In order for these clusters to be meaningful
as universal tags, we can provide several prototypes
for each cluster (e.g., ?walk? is a verb etc.). To test
this approach we used the prototype driven tagger of
Haghighi and Klein (2006) with 15 prototypes per
universal tag.14 The resulting universal tags yield
an average parsing accuracy of 40.5%. Our method
(using Brown clustering as above) outperforms this
14Oracle prototypes were obtained by taking the 15 most
frequent words for each universal tag. This yields almost the
same total number of prototypes as those in the experiment of
(Haghighi and Klein, 2006).
baseline by about 5%.
8 Conclusions
We present an automatic method for mapping
language-specific part-of-speech tags to a set of uni-
versal tags. Our work capitalizes on manually de-
signed conversion schemes to automatically create
mappings for new languages. Our experimental re-
sults demonstrate that automatically induced map-
pings rival the quality of their hand-crafted coun-
terparts. We also establish that the mapping quality
has a significant impact on the accuracy of syntactic
transfer, which motivates further study of this topic.
Finally, our experiments show that the choice of
mapping optimization scheme plays a crucial role in
the quality of the derived mapping, highlighting the
importance of optimization for the mapping task.
Acknowledgments
The authors acknowledge the support of the NSF
(IIS-0835445), the MURI program (W911NF-10-1-
0533) and the DARPA BOLT program. We thank
Tommi Jaakkola, the members of the MIT NLP
group and the ACL reviewers for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP,
pages 50?61.
Frank Van Eynde. 2004. Part of speech tagging en lem-
matisering van het corpus gesproken nederlands. In
Technical report.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL, pages 272?279.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. JMLR, 11:2001?2049.
1377
Michael Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman & Co.
Michael C. Grant and Stephen P. Boyd. 2008. Graph im-
plementations for nonsmooth convex programs. In Re-
cent Advances in Learning and Control, Lecture Notes
in Control and Information Sciences, pages 95?110.
Springer-Verlag Limited.
Michael C. Grant and Stephen P. Boyd. 2011. CVX:
Matlab software for disciplined convex programming,
version 1.21.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL, pages 320?327.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Journal of Natural Language Engineering, 11:311?
325.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL, pages 595?603.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging. In
Proceedings of EMNLP, pages 853?861.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchi-
cal dirichlet processes. In Proceedings of EMNLP-
CoNLL, pages 688?697.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of ACL, pages 75?82.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of EMNLP, pages 523?530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP, pages 62?72.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP, pages 1234?1244.
Radford M. Neal and Geoffrey E. Hinton. 1999. A view
of the em algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models, pages 355?368. MIT Press.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL-
COLING, pages 433?440.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851?858.
Alan Yuille and Anand Rangarajan. 2003. The concave-
convex procedure (cccp). In Proceedings of Neural
Computation, volume 15, pages 915?936.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP-08 Workshop on NLP for Less
Privileged Languages, pages 35?42.
1378
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1013?1024,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Greed is Good if Randomized: New Inference for Dependency Parsing
Yuan Zhang
?
, Tao Lei
?
, Regina Barzilay, and Tommi Jaakkola
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{yuanzh, taolei, regina, tommi}@csail.mit.edu
Abstract
Dependency parsing with high-order fea-
tures results in a provably hard decoding
problem. A lot of work has gone into
developing powerful optimization meth-
ods for solving these combinatorial prob-
lems. In contrast, we explore, analyze, and
demonstrate that a substantially simpler
randomized greedy inference algorithm al-
ready suffices for near optimal parsing: a)
we analytically quantify the number of lo-
cal optima that the greedy method has to
overcome in the context of first-order pars-
ing; b) we show that, as a decoding algo-
rithm, the greedy method surpasses dual
decomposition in second-order parsing; c)
we empirically demonstrate that our ap-
proach with up to third-order and global
features outperforms the state-of-the-art
dual decomposition and MCMC sampling
methods when evaluated on 14 languages
of non-projective CoNLL datasets.
1
1 Introduction
Dependency parsing is typically guided by param-
eterized scoring functions that involve rich fea-
tures exerting refined control over the choice of
parse trees. As a consequence, finding the high-
est scoring parse tree is a provably hard combina-
torial inference problem (McDonald and Pereira,
2006). Much of the recent work on parsing has
focused on solving these problems using powerful
optimization techniques. In this paper, we follow a
different strategy, arguing that a much simpler in-
ference strategy suffices. In fact, we demonstrate
that a randomized greedy method of inference sur-
passes the state-of-the-art performance in depen-
dency parsing.
?
Both authors contributed equally.
1
Our code is available at https://github.com/
taolei87/RBGParser.
Our choice of a randomized greedy algorithm
for parsing follows from a successful track record
of such methods in other hard combinatorial prob-
lems. These conceptually simple and intuitive
algorithms have delivered competitive approxi-
mations across a broad class of NP-hard prob-
lems ranging from set cover (Hochbaum, 1982) to
MAX-SAT (Resende et al., 1997). Their success
is predicated on the observation that most realiza-
tions of problems are much easier to solve than the
worst-cases. A simpler algorithm will therefore
suffice in typical cases. Evidence is accumulating
that parsing problems may exhibit similar proper-
ties. For instance, methods such as dual decom-
position offer certificates of optimality when the
highest scoring tree is found. Across languages,
dual decomposition has shown to lead to a cer-
tificate of optimality for the vast majority of the
sentences (Koo et al., 2010; Martins et al., 2011).
These remarkable results suggest that, as a com-
binatorial problem, parsing appears simpler than
its broader complexity class would suggest. In-
deed, we show that a simpler inference algorithm
already suffices for superior results.
In this paper, we introduce a randomized greedy
algorithm that can be easily used with any rich
scoring function. Starting with an initial tree
drawn uniformly at random, the algorithm makes
only local myopic changes to the parse tree in an
attempt to climb the objective function. While a
single run of the hill-climbing algorithm may in-
deed get stuck in a locally optimal solution, mul-
tiple random restarts can help to overcome this
problem. The same algorithm is used both for
learning the parameters of the scoring function as
well as for parsing test sentences.
The success of a randomized greedy algorithm
is tied to the number of local maxima in the search
space. When the number is small, only a few
restarts will suffice for the greedy algorithm to
find the highest scoring parse. We provide an al-
1013
gorithm for explicitly counting the number of lo-
cal optima in the context of first-order parsing,
and demonstrate that the number is typically quite
small. Indeed, we find that a first-order parser
trained with exact inference or using our random-
ized greedy algorithm delivers basically the same
performance.
We hypothesize that parsing with high-order
scoring functions exhibits similar properties. The
main rationale is that, even in the presence of high-
order features, the resulting scoring function re-
mains first-order dominant. The performance of
a simple arc-factored first-order parser is only a
few percentage points behind higher-order parsers.
The higher-order features in the scoring function
offer additional refinement but only a few changes
above and beyond the first-order result. As a
consequence, most of the arc choices are already
determined by a much simpler, polynomial time
parser.
We use dual decomposition to show that the
greedy method indeed succeeds as an inference al-
gorithm even with higher-order scoring functions.
In fact, with second-order features, regardless of
which method was used for training, the random-
ized greedy method outperforms dual decomposi-
tion by finding higher scoring trees. For the sen-
tences that dual decomposition is optimal (obtains
a certificate), the greedy method finds the same
solution in over 99% of the cases. Our simple
inference algorithm is therefore likely to scale to
higher-order parsing and we demonstrate empiri-
cally that this is indeed so.
We validate our claim by evaluating the method
on the CoNLL dependency benchmark that com-
prises treebanks from 14 languages. Aver-
aged across all languages, our method out-
performs state-of-the-art parsers, including Tur-
boParser (Martins et al., 2013) and our earlier
sampling-based parser (Zhang et al., 2014). On
seven languages, we report the best published re-
sults. The method is not sensitive to initialization.
In fact, drawing the initial tree uniformly at ran-
dom results in the same performance as when ini-
tialized from a trained first-order distribution. In
contrast, sufficient randomization of the starting
point is critical. Only a small number of restarts
suffices for finding (near) optimal parse trees.
2 Related Work
Finding Optimal Structure in Parsing The use
of rich-scoring functions in dependency parsing
inevitably leads to the challenging combinatorial
problem of finding the maximizing parse. In fact,
McDonald and Pereira (2006) demonstrated that
the task is provably NP-hard for non-projective
second-order parsing. Not surprisingly, approx-
imate inference has been at the center of pars-
ing research. Examples of these approaches in-
clude easy-first parsing (Goldberg and Elhadad,
2010), inexact search (Johansson and Nugues,
2007; Zhang and Clark, 2008; Huang et al., 2012;
Zhang et al., 2013), partial dynamic program-
ming (Huang and Sagae, 2010) and dual decom-
position (Koo et al., 2010; Martins et al., 2011).
Our work is most closely related to the MCMC
sampling-based approaches (Nakagawa, 2007;
Zhang et al., 2014). In our earlier work, we devel-
oped a method that learns to take guided stochas-
tic steps towards a high-scoring parse (Zhang et
al., 2014). In the heart of that technique are so-
phisticated samplers for traversing the space of
trees. In this paper, we demonstrate that a sub-
stantially simpler approach that starts from a tree
drawn from the uniform distribution and uses hill-
climbing for parameter updates achieves similar or
higher performance.
Another related greedy inference method has
been used for non-projective dependency pars-
ing (McDonald and Pereira, 2006). This method
relies on hill-climbing to convert the highest scor-
ing projective tree into its non-projective approxi-
mation. Our experiments demonstrate that when
hill-climbing is employed as a primary learning
mechanism for high-order parsing, it exhibits dif-
ferent properties: the distribution for initialization
does not play a major role in the final outcome,
while the use of restarts contributes significantly
to the quality of the resulting tree.
Greedy Approximations for NP-hard Problems
There is an expansive body of research on greedy
approximations for NP-hard problems. Examples
of NP-hard problems with successful greedy ap-
proximations include the traveling saleman prob-
lem problem (Held and Karp, 1970; Rego et
al., 2011), the MAX-SAT problem (Mitchell et
al., 1992; Resende et al., 1997) and vertex
cover (Hochbaum, 1982). While some greedy
methods have poor worst-case complexity, many
1014
of them work remarkably well in practice. Despite
the apparent simplicity of these algorithms, un-
derstanding their properties is challenging: often
their ?theoretical analyses are negative and incon-
clusive? (Amenta and Ziegler, 1999; Spielman and
Teng, 2001). Identifying conditions under which
approximations are provably optimal is an active
area of research in computer science theory (Du-
mitrescu and T?oth, 2013; Jonsson et al., 2013).
In NLP, randomized and greedy approximations
have been successfully used across multiple ap-
plications, including machine translation and lan-
guage modeling (Brown et al., 1993; Ravi and
Knight, 2010; Daum?e III et al., 2009; Moore and
Quirk, 2008; Deoras et al., 2011). In this paper,
we study the properties of these approximations in
the context of dependency parsing.
3 Method
3.1 Preliminaries
Let x be a sentence and T (x) be the set of possi-
ble dependency trees over the words in x. We use
y ? T (x) to denote a dependency tree for x, and
y(m) to specify the head (parent) of the modifier
word indexed by m in tree y. We also use m to
denote the indexed word when there is no ambi-
guity. In addition, we define T (y,m) as the set
of ?neighboring trees? of y obtained by changing
only the head of the modifier, i.e. y(m).
The dependency trees are scored according to
S(x, y) = ? ? ?(x, y), where ? is a vector of pa-
rameters and ?(x, y) is a sparse feature vector rep-
resentation of tree y for sentence x. In this work,
?(x, y) will include up to third-order features as
well as a range of global features commonly used
in re-ranking methods (Collins, 2000; Charniak
and Johnson, 2005; Huang, 2008).
The parameters ? in the scoring function are
estimated on the basis of a training set D =
{(x?
i
, y?
i
)}
N
i=1
of sentences x?
i
and the correspond-
ing gold (target) trees y?
i
. We adopt a max-margin
framework for this learning problem. Specifically,
we aim to find parameter values that score the gold
target trees higher than others:
?i ? {1, ? ? ? , N}, y ? T (x?
i
),
S(x?
i
, y?
i
) ? S(x?
i
, y) + ?y?
i
? y?
1
? ?
i
where ?
i
? 0 is the slack variable (non-zero values
are penalized against) and ?y?
i
? y?
1
is the ham-
ming distance between the gold tree y?
i
and a can-
didate parse y.
In an online learning setup, parameters are up-
dated successively after each sentence. Each up-
date still requires us to find the ?strongest viola-
tion?, i.e., a candidate tree y? that scores higher
than the gold tree y?
i
:
y? = argmax
y?T (x?
i
)
{S(x?
i
, y) + ?y ? y?
i
?
1
}
The parameters are then revised so as to select
against the offending y?. Instead of a standard
parameter update based on y? as in perceptron,
stochastic gradient descent, or passive-aggressive
updates, our implementation follows Lei et al.
(2014) where the first-order parameters are broken
up into a tensor. Each tensor component is updated
successively in combination with the parameters
corresponding to MST features (McDonald et al.,
2005) and higher-order features (when included).
2
3.2 Algorithm
During training and testing, the key combinatorial
problem we must solve is that of decoding, i.e.,
finding the highest scoring tree y? ? T (x) for each
sentence x (or x?
i
). In our notation,
y? = argmax
y?T (x?
i
)
{? ? ?(x?
i
, y) + ?y ? y?
i
?
1
} (train)
y? = argmax
y?T (x)
{? ? ?(x, y)} (test)
While the decoding problem with feature sets sim-
ilar to ours has been shown to be NP-hard, many
approximation algorithms work remarkably well.
We commence with a motivating example.
Locality and Parsing One possible reason for
why greedy or other approximation algorithms
work well for dependency parsing is that typical
sentences and therefore the learned scoring func-
tions S(x, y) = ? ? ?(x, y) are primarily ?lo-
cal?. By this we mean that head-modifier deci-
sions could be made largely without considering
the surrounding structure (the context). For exam-
ple, in English an adjective and a determiner are
typically attached to the following noun.
We demonstrate the degree of locality in de-
pendency parsing by comparing a first-order tree-
based parser to the parser that predicts each head
word independently of others. Note that the in-
dependent prediction of dependency arcs does not
necessarily give rise to a tree. The parameters of
2
We refer the readers to Lei et al. (2014) for more details
about the tensor scoring function and the online update.
1015
Dataset Indp. Pred Tree Pred
Slovene 83.7 84.2
Arabic 79.0 79.2
Japanese 93.4 93.7
English 91.6 91.9
Average 86.9 87.3
Table 1: Head attachment accuracy of a first-order
local classifier (left) and a first-order structural
prediction model (right). The two types of mod-
els are trained using the same set of features.
Input: parameter ?, sentence x
Output: dependency tree y?
1: Randomly initialize tree y
(0)
;
2: t = 0;
3: repeat
4: list = bottom-up node list of y
(t)
;
5: for each word m in list do
6: y
(t+1)
= argmax
y?T (y
(t)
,m)
S(x, y);
7: t = t+ 1;
8: end for
9: until no change in this iteration
10: return y? = y
(t)
;
Figure 1: A randomized hill-climbing algorithm
for dependency parsing.
the two parsers, the independent prediction and
a tree-based parser, are trained separately with
the corresponding decoding algorithm but with the
same feature set.
Table 1 shows that the accuracy of the inde-
pendent prediction ranges from 79% to 93% on
four CoNLL datasets. The results are on par with
the first-order structured prediction model. This
experiment reinforces the conclusion in Liang et
al. (2008), where a local classifier was shown
to achieve comparable accuracy to a sequential
model (e.g. CRF) in POS tagging and named-
entity recognition.
Hill-Climbing with Random Restarts We
build here on the motivating example and explore
greedy algorithms as generalizations of purely lo-
cal decoding. Greedy algorithms break the decod-
ing problem into a sequence of simple local steps,
each required to improve the solution. In our case,
simple local steps correspond to choosing the head
for each modifier word.
We begin with a tree y
(0)
, which can be a sam-
ple drawn uniformly from T (x) (Wilson, 1996).
Our greedy algorithm then updates y
(t)
to a bet-
ter tree y
(t+1)
by revising the head of one modifier
word while maintaining the constraint that the re-
sulting structure is a tree. The modifiers are con-
sidered in the bottom-up order relative to the cur-
rent tree (the word furthest from the root is consid-
ered first). We provide an analysis to motivate this
bottom-up update strategy in Section 4.1. The al-
gorithm continues until the score can no longer be
improved by changing the head of a single word.
The resulting tree represents a locally optimal pre-
diction relative to a single-arc greedy algorithm.
Figure 1 gives the algorithm in pseudo-code.
There are many possible variations of the sim-
ple randomized greedy hill-climbing algorithm.
First, the Wilson sampling algorithm (Wilson,
1996) can be naturally extended to obtain i.i.d.
samples from any first-order distributions. There-
fore, we could initialize the tree y
(0)
with a tree
from a first-order parser, or draw the initial tree
from a first-order distribution other than uniform.
However, perhaps surprisingly, as we demon-
strate later, little is lost with uniform initializa-
tion. Second, since a single run of randomized
hill-climbing is relatively cheap and runs are in-
dependent to each other, it is easy to execute mul-
tiple runs independently in parallel. The final pre-
dicted tree is then simply the highest scoring tree
across the multiple runs. We demonstrate that only
a small number of parallel runs are necessary for
near optimal prediction.
4 Analysis
4.1 First-Order Parsing
We provide here a firmer basis for why the ran-
domized greedy algorithm can be expected to
work. While the focus of the rest of the paper
is on higher-order parsing, we limit ourselves in
this subsection to first-order parsing. The reasons
for this are threefold. First, a simple greedy algo-
rithm is already not guaranteed a priori to work in
the context of a first-order scoring function. The
conclusions from this analysis are therefore likely
to carry over to higher-order parsing scenarios as
well. Second, a first-order arc-factored scoring
provides us an easy way to ascertain when the ran-
domized greedy algorithm indeed found the high-
est scoring tree. Finally, we are able to count the
1016
Dataset Average Len.
# of local optima at percentile fraction of finding global optima (%)
50% 70% 90% 0 <Len.? 15 Len.> 15
Turkish 12.1 1 1 2 100 100
Slovene 15.9 2 20 3647 100 98.1
English 24.0 21 121 2443 100 99.3
Arabic 36.8 2 35 >10000 100 99.1
Table 2: The left part of the table shows the local optimum statistics of the first-order model. The
sentences are sorted by the number of local optima. Columns 3 to 5 show the number of local optima of
a sentence at different percentile of the sorted list. For example, on English 50% of the sentences have
no more than 21 local optimum trees. The right part shows the fraction of finding global optima using
300 uniform restarts for each sentence.
number of locally optimal solutions for a greedy
algorithm in the context of first-order parsing and
can therefore relate this property to the success
rates of the algorithm.
Reachability We begin by highlighting a basic
property of trees, namely that single arc changes
suffice for transforming any tree to any other tree
in a small number of steps while maintaining that
each intermediate structure is also a tree. In this
sense, a target tree is reachable from any start-
ing point using only single arc changes. More
formally, let y be any starting tree and y
?
the de-
sired target. Let m
1
,m
2
, ? ? ? ,m
n
be the bottom-
up list of words (modifiers) corresponding to tree
y, where m
1
is the word furthest from the root.
We can simply change each head y(m
i
) to that of
y
?
(m
i
) in this order i = 1, . . . , n. The bottom-up
order guarantees that no cycle is introduced with
respect to the remaining (yet unmodified) nodes of
y. The fact that y
?
is a valid tree implies no cycle
will appear with respect to the already modified
nodes.
Note that, according to this property, any tree
is reachable from any starting point using only k
modifications, where k is the number of head dif-
ferences, i.e. k = |{m : y(m) 6= y
?
(m)}|. The
result also suggests that it may be helpful to per-
form the greedy steps in the bottom-up order, a
suggestion that we follow in our implementation.
Broadly speaking, we have established that
the greedy algorithm is not inherently limited by
virtue of its basic steps. Of course, it is a differ-
ent question whether the scoring function supports
such local changes towards the correct target tree.
Locally Optimal Trees While greedy algo-
rithms are notoriously prone to getting stuck in
locally optimal solutions, we establish here that
Function CountOptima(G = ?V,E?)
V = {w
0
, w
1
, ? ? ? , w
n
} where w
0
is the
root
E = {e
ij
? R} are the arc scores
Return: the number of local optima
1: Let y(0) = ? and y(i) = argmax
j
e
ji
;
2: if y is a tree (no cycle) then return 1;
3: Find a cycle C ? V in y;
4: count = 0;
// contract the cycle
5: create a vertex w
?
;
6: ?j /? C : e
?j
= max
k?C
e
kj
;
7: for each vertex w
i
? C do
8: ?j /? C : e
j?
= e
ji
;
9: V
?
= V ? {w
?
} \ C;
10: E
?
= E ? {e
?j
, e
j?
| ?j /? C}
11: count += CountOptima(G
?
= ?V
?
, E
?
?);
12: end for
13: return count;
Figure 2: A recursive algorithm for counting lo-
cal optima for a sentence with words w
1
, ? ? ? , w
n
(first-order parsing). The algorithm resembles the
Chu-Liu-Edmonds algorithm for finding the max-
imum directed spanning tree (Chu and Liu, 1965).
decoding with learned scoring functions involves
only a small number of local optima. In our case,
a local optimum corresponds to a tree y where no
single change of head y(m) results in a higher
scoring tree. Clearly, the highest scoring tree is
also a local optimum in this sense. If there were
many such local optima, finding the one with the
highest score would be challenging for a greedy
algorithm, even with randomization.
We begin with a worst case analysis and estab-
1017
Dataset
Trained with Hill-Climbing (HC) Trained with Dual Decomposition (DD)
%Cert (DD) s
DD
>s
HC
s
DD
=s
HC
s
DD
<s
HC
%Cert (DD) s
DD
>s
HC
s
DD
=s
HC
s
DD
<s
HC
Turkish 98.7 0.0 99.8 0.2 98.7 0.0 100.0 0.0
Slovene 94.5 0.0 98.7 1.3 92.3 0.2 99.0 0.8
English 94.5 0.3 98.7 1.0 94.6 0.5 98.7 0.8
Arabic 78.8 3.4 93.9 2.7 75.3 4.7 88.4 6.9
Table 3: Decoding quality comparison between hill-climbing (HC) and dual decomposition (DD). Mod-
els are trained either with HC (left) or DD (right). s
HC
denotes the score of the tree retrieved by HC
and s
DD
gives the analogous score for DD. The columns show the percentage of all test sentences for
which one method succeeds in finding a higher or the same score. ?Cert? column gives the percentage
of sentences for which DD finds a certificate.
lish a tight upper bound on the number of local
optima for a first-order scoring function.
Theorem 1 For any first-order scoring function
that factorizes into the sum of arc scores S(x, y) =
?
S
arc
(y(m),m): (a) the number of locally op-
timal trees is at most 2
n?1
for n words; (b) this
upper bound is tight.
3
While the number of possible dependency trees
is (n + 1)
n?1
(Cayley?s formula), the number of
local optima is at most 2
n?1
. This is still too many
for longer sentences, suggesting that, in the worst
case, a randomized greedy algorithm is unlikely to
find the highest scoring tree. However, the scor-
ing functions we learn for dependency parsing are
considerably easier.
Average Case Analysis In contrast to the worst-
case analysis above, we will count here the actual
number of local optima per sentence for a first-
order scoring function learned from data with the
randomized greedy algorithm. Figure 2 provides
pseudo-code for our counting algorithm. The al-
gorithm is derived by tailoring the proof of Theo-
rem 1 to each sentence.
Table 2 shows the empirical number of locally
optimal trees estimated by our algorithm across 4
different languages. Decoding with trained scor-
ing functions in the average case is clearly sub-
stantially easier than the worst case. For exam-
ple, on the English test set more than 70% of the
sentences have at most 121 locally optimal trees.
Since the average sentence length is 24, the dis-
crepancy between the typical number (e.g., 121)
and the worst case (2
24?1
) is substantial. As a re-
sult, only a small number of restarts is likely to
suffice for finding optimal trees in practice.
Optimal Decoding We can easily verify
whether the randomized greedy algorithm indeed
3
A proof sketch is given in Appendix.
succeeds in finding the highest scoring trees with
a learned first-order scoring function. We have
established above that there are typically only a
small number of locally optimal trees. We would
therefore expect the algorithm to work. We show
the results in the second part of Table 2. For short
sentences of length up to 15, our method finds the
global optimum for all the test sentences. Success
rates remain high even for longer test sentences.
4.2 Higher-Order Parsing
Exact decoding with high-order features is known
to be provably hard (McDonald et al., 2005). We
begin our analysis here with a second-order (sib-
ling/grandparent) model, and compare our ran-
domized hill-climbing (HC) method to dual de-
composition (DD), re-implementing Koo et al.
(2010). Table 3 compares decoding quality for the
two methods across four languages. Overall, in
97.8% of the sentences, HC obtains the same score
as DD, in 1.3% of the cases HC finds a higher
scoring tree, and in 0.9% of cases DD results in
a better tree. The results follow the same pattern
regardless of which method was used to train the
scoring function. The average rate of certificates
for DD was 92%. In over 99% of these sentences,
HC reaches the same optimum.
We expect that these observations about the suc-
cess of HC carry over to other high-order parsing
models for several reasons. First, a large num-
ber of arcs are pruned in the initial stage, con-
siderably reducing the search space and minimiz-
ing the number of possible locally optimal trees.
Second, many dependencies can be determined
already with independent arc prediction (see our
motivating example above), predictions that are
readily achieved with a greedy algorithm. Finally,
high-order features represent smaller refinements,
i.e., suggest only a few changes above and be-
yond the dominant first-order scores. Greedy al-
1018
gorithms are therefore likely to be able to leverage
at least some of this potential. We demonstrate be-
low that this is indeed so.
Our methods are trained within the max-margin
framework. As a result, we are expected to find
the highest scoring competing tree for each train-
ing sentence (the ?strongest violation?). One may
question therefore whether possible sub-optimal
decoding for some training sentences (finding ?a
violation? rather than the ?strongest violation?)
impacts the learned parser. To this end, Huang et
al. (2012) have established that weaker violations
do suffice for separable training sets.
5 Experimental Setup
Dataset and Evaluation Measures We evalu-
ate our model on CoNLL dependency treebanks
for 14 different languages (Buchholz and Marsi,
2006; Surdeanu et al., 2008), using standard train-
ing and testing splits. We use part-of-speech tags
and the morphological information provided in the
corpus. Following standard practice, we use Unla-
beled Attachment Score (UAS) excluding punctu-
ation (Koo et al., 2010; Martins et al., 2013) as the
evaluation metric in all our experiments.
Baselines We compare our model with the Tur-
boParser (Martins et al., 2013) and our earlier
sampling-based parser (Zhang et al., 2014). For
both parsers, we directly compare with the re-
cent published results on the CoNLL datasets.
We also compare our parser against the best pub-
lished results for the individual languages in our
datasets. This comparison set includes four ad-
ditional parsers: Martins et al. (2011), Koo et al.
(2010), Zhang et al. (2013) and our tensor-based
parser (Lei et al., 2014).
Features We use the same feature templates as
in our prior work (Zhang et al., 2014; Lei et al.,
2014)
4
. Figure 3 shows the first- to third-order
feature templates that we use in our model. For
the global features we use right-branching, coor-
dination, PP attachment, span length, neighbors,
valency and non-projective arcs features.
Implementation Details Following standard
practices, we train our model using the passive-
aggressive online learning algorithm (MIRA)
and parameter averaging (Crammer et al., 2006;
4
We refer the readers to Zhang et al. (2014) and Lei et al.
(2014) for the detailed definition of each feature template.
arc!
head bigram!!h h m m+1h m consecutive sibling!h m s grandparent!g h mgrand-sibling!g h m s
tri-siblings!h m s t grand-grandparent!g h mgg
outer-sibling-grandchild!h m sgc h s gcminner-sibling-grandchild!
Figure 3: First- to third-order features.
Arabic Slovene English Chinese German
?2
?1
0
1
2
3
4
5 Len ? 15Len > 15
Figure 4: Absolute UAS improvement of our full
model over the first-order model. Sentences in the
test set are divided into 2 groups based on their
lengths.
Collins, 2002). By default we use an adaptive
strategy for running the hill-climbing algorithm
? for a given sentence we repeatedly run the al-
gorithm in parallel
5
until the best tree does not
change for K = 300 consecutive restarts. For
each restart, by default we initialize the tree y
(0)
by sampling from the first-order distribution us-
ing the current learned parameter values (and first-
order scores). We train our first-order and third-
order model for 10 epochs and our full model for
20 epochs for all languages, and report the average
performance across three independent runs.
6 Results
Comparison with the Baselines Table 4 sum-
marizes the results of our model, along with the
state-of-the-art baselines. On average across 14
languages, our full model with the tensor com-
ponent outperforms both TurboParser and the
sampling-based parser. The direct comparison
5
We use 8 threads in all the experiments.
1019
Our Model
Exact 1st
Turbo Sampling
Best Published
1st 3rd Full
w/o tensor
Full (MA13) (ZL14)
Arabic 78.98 79.95 79.38 80.24 79.22 79.64 80.12 81.12 (MS11)
Bulgarian 92.15 93.38 93.69 93.72 92.24 93.10 93.30 94.02 (ZH13)
Chinese 91.20 93.00 92.76 93.04 91.17 89.98 92.63 92.68 (LX14)
Czech 87.65 90.11 90.34 90.77 87.82 90.32 91.04 91.04 (ZL14)
Danish 90.50 91.43 91.66 91.86 90.56 91.48 91.80 92.00 (ZH13)
Dutch 84.49 86.43 87.04 87.39 84.79 86.19 86.47 86.47 (ZL14)
English 91.85 93.01 93.20 93.25 91.94 93.22 92.94 93.22 (MA13)
German 90.52 91.91 92.64 92.67 90.54 92.41 92.07 92.41 (MA13)
Japanese 93.78 93.80 93.35 93.56 93.74 93.52 93.42 93.74 (LX14)
Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10)
Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11)
Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14)
Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13)
Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10)
Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58
Table 4: Results of our model and several state-of-the-art systems. ?Best Published UAS? includes the
most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et
al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set
of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser
(Zhang et al., 2014) and tensor features (Lei et al., 2014).
Dataset
MAP-1st Uniform Rnd-1st
UAS Init. UAS Init. UAS Init.
Slovene 85.2 80.1 86.7 13.7 86.7 34.2
Arabic 78.8 75.1 79.7 12.4 80.2 32.8
English 91.1 82.0 93.3 39.6 93.3 55.6
Chinese 87.2 75.3 93.2 36.8 93.0 54.5
Dutch 84.8 79.5 87.0 26.9 87.4 45.6
Average 85.4 78.4 88.0 25.9 88.1 44.5
Table 5: Comparison between different initializa-
tion strategies: (a) MAP-1st: only the MAP tree
of the first-order score; (b) Uniform: random trees
are sampled from the uniform distribution; and
(c) Rnd-1st: random trees are sampled from the
first-order distribution. For each method, the table
shows the average accuracy of the initial tree and
the final parsing accuracy.
with TurboParser is achieved by restricting our
model to third order features which still outper-
forms TurboParser (89.10% vs 88.72%). To com-
pare against the sampling-based parser, we em-
ploy our model without the tensor component. The
two models achieve a similar average performance
(89.24% and 89.23% respectively). Since relative
parsing performance depends on a target language,
we also include comparison with the best pub-
lished results. The model achieves the best pub-
lished results for seven languages.
Another noteworthy comparison concerns first-
order parsers. As Table 4 shows, the exact and ap-
proximate versions of the first-order parser deliver
almost identical performance.
Impact of High-Order Features Table 4 shows
that the model can effectively utilize high-order
features. Comparing the average performance of
the model variants, we see that the accuracy on
the benchmark languages consistently improves
when higher-order features are added. This char-
acteristic of the randomized greedy parser is in
line with findings about other state-of-the-art high-
order parsers (Martins et al., 2013; Zhang et al.,
2014). Figure 4 breaks down these gains based
on the sentence length. As expected, on most lan-
guages high-order features are particularly helpful
when parsing longer sentences.
Impact of Initialization and Restarts Table 5
shows the impact of initialization on the model
performance for several languages. We consider
three strategies: the MAP estimate of the first-
order score from the model, uniform sampling and
sampling from the first-order distribution. The ac-
curacy of initial trees varies greatly, ranging from
78.4% for the MAP estimate to 25.9% and 44.5%
for the latter randomized strategies. However, the
resulting parsing accuracy is not determined by
the initial accuracy. In fact, the two sampling
strategies result in almost identical parsing perfor-
mance. While the first-order MAP estimate gives
the best initial guess, the overall parsing accuracy
of this method lags behind. This result demon-
strates the importance of restarts ? in contrast to
the randomized strategies, the MAP initialization
performs only a single run of hill-climbing.
1020
Length ? 15 Length > 15
Slovene 100 98.11
English 100 99.12
Table 6: Fractions (%) of the sentences that find
the best solution among 3,000 restarts within the
first 300 restarts.
0 200 400 600 800 1000
0.994
0.996
0.998
1
# Restarts
Scor
e
 
 
len?15len>15
(a) Slovene
0 200 400 600 800 1000
0.994
0.996
0.998
1
# Restarts
Scor
e
 
 
len?15len>15
(b) English
Figure 5: Convergence analysis on Slovene and
English datasets. The graph shows the normalized
score of the output tree as a function of the number
of restarts. The score of each sentence is normal-
ized by the highest score obtained for this sentence
after 3,000 restarts. We only show the curves up to
1,000 restarts because they all reach convergence
after around 500 restarts.
Convergence Properties Figure 5 shows the
score of the trees retrieved by our full model with
respect to the number of restarts, for short and long
sentences in English and Slovene. To facilitate the
comparison, we normalize the score of each sen-
tence by the maximal score obtained for this sen-
tence after 3,000 restarts. Overall, most sentences
converge quickly. This view is also supported by
Table 6 which shows the fraction of the sentences
that converge within the first 300 restarts. We can
see that all the short sentences (length up to 15)
reach convergence within the allocated restarts.
Perhaps surprisingly, more than 98% of the long
sentences also converge within 300 restarts.
Decoding Speed As the number of restarts im-
pacts the parsing accuracy, we can trade perfor-
mance for speed. Figure 6 shows that the model
2 4 6 8 10 12 14x 10?3
82
84
86
88
Sec/Tok
UAS
 
 
3rd?order ModelFull Model
(a) Slovene
2 4 6 8 10x 10?3
88
90
92
94
Sec/Tok
UAS
 
 
3rd?order ModelFull Model
(b) English
Figure 6: Trade-off between performance and
speed on Slovene and English datasets. The graph
shows the accuracy as a function of decoding
speed measured in second per token. Variations in
decoding speed is achieved by changing the num-
ber of restarts.
achieves high performance with acceptable pars-
ing speed. While various system implementation
issues such as programming language and com-
putational platform complicate a direct compari-
son with other parsing systems, our model deliv-
ers parsing time roughly comparable to other state-
of-the-art graph-based systems (for example, Tur-
boParser and MST parser) and the sampling-based
parser.
7 Conclusions
We have shown that a simple, generally appli-
cable randomized greedy algorithm for inference
suffices to deliver state-of-the-art parsing perfor-
mance. We argued that the effectiveness of such
greedy algorithms is contingent on having a small
number of local optima in the scoring function. By
algorithmically counting the number of locally op-
timal solutions in the context of first-order parsing,
we show that this number is indeed quite small.
Moreover, we show that, as a decoding algorithm,
the greedy method surpasses dual decomposition
in second-order parsing. Finally, we empirically
demonstrate that our approach with up to third-
order and global features outperforms the state-of-
the-art parsers when evaluated on 14 languages of
1021
non-projective CoNLL datasets.
Appendix
We provide here a more detailed justification for
the counting algorithm in Figure 2 and, by exten-
sion, a proof sketch of Theorem 1. The bullets
below follow the operation of the algorithm.
? Whenever independent selection of the heads
results in a valid tree, there is only 1 opti-
mum (Lines 1&2 of the algorithm). Other-
wise there must be a cycle C in y (Line 3 of
the algorithm)
? We claim that any locally optimal tree y
?
of
the graph G = (V,E) must contain |C| ? 1
arcs of the cycle C ? V . This can be shown
by contradiction. If y
?
contains less than
|C| ? 1 arcs of C, then (a) we can construct
a tree y
??
that contains |C| ? 1 arcs; (b) the
heads in y
??
are strictly better than those in
y
?
over the unused part of the cycle; (c) by
reachability, there is a path y
?
? y
??
so y
?
cannot be a local optimum.
? Any locally optimal tree in G must select an
arc inC and reassign it. The rest of the |C|?1
arcs will then result in a chain.
? By contracting cycle C we obtain a new
graph G
?
of size |G| ? |C| + 1 (Lines 5-11
of the algorithm). Easy to verify that (not
shown): any local optimum in G
?
is a local
optimum in G and vice versa.
The theorem follows as a corollary of these
steps. To see this, let F (G
m
) be the number of
local optima in the graph of size m:
F (G
m
) ? max
C?V (G)
?
i
F (G
(i)
m?c+1
)
where G
(i)
m?c+1
is the graph (of size m ? c + 1)
created by selecting the i
th
arc in cycleC and con-
tracting G
m
accordingly, and c = |C| is the size
of the cycle. Define
?
F (m) as the upper bound of
F (G
m
) for any graph of size m. By the above
formula, we know that
?
F (m) ? max
2?c<m
?
F (m? c+ 1)? c
By solving for
?
F (m) we get
?
F (m) ? 2
m?2
. Since
m = n+1 for a sentence with n words, the upper-
bound of local optima is 2
n?1
.
To show the tightness, for any n > 0, create
the graph G
n+1
with arc scores e
ij
= e
ji
= i for
any 0 ? i < j ? n. Note that w
n
? w
n?1
?
w
n
forms the circle C of size 2, it can be shown
by induction on n and F (G
n+1
) that F (G
n+1
) =
F (G
n
)? 2 = 2
n?1
.
Acknowledgments
This research is developed in collaboration with
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the IYAS project. The authors acknowl-
edge the support of the U.S. Army Research Of-
fice under grant number W911NF-10-1-0533, and
of the DARPA BOLT program. We thank the MIT
NLP group and the ACL reviewers for their com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
References
Nina Amenta and G?unter Ziegler, 1999. Deformed
Products and Maximal Shadows of Polytopes. Con-
temporary Mathematics. American Mathematics So-
ciety.
Peter F. Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Scientia
Sinica, 14(10):1396.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, ICML ?00, pages 175?182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
1022
Language Processing - Volume 10, EMNLP ?02. As-
sociation for Computational Linguistics.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
learning, 75(3):297?325.
Anoop Deoras, Tom?a?s Mikolov, and Kenneth Church.
2011. A fast re-scoring strategy to capture long dis-
tance dependencies. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 1116?1127. Associa-
tion for Computational Linguistics.
Adrian Dumitrescu and Csaba D T?oth. 2013. The trav-
eling salesman problem for lines, balls and planes.
In SODA, pages 828?843. SIAM.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750. Association for Computa-
tional Linguistics.
Michael Held and Richard M. Karp. 1970. The
traveling-salesman problem and minimum spanning
trees. Operations Research, 18(6):1138?1162.
Dorit S. Hochbaum. 1982. Approximation algo-
rithms for the set covering and vertex cover prob-
lems. SIAM Journal on Computing, 11(3):555?556.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077?
1086. Association for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151. Association for Computational Linguis-
tics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning. In
EMNLP-CoNLL, pages 1134?1138.
Peter Jonsson, Victor Lagerkvist, Gustav Nordh, and
Bruno Zanuttini. 2013. Complexity of sat problems,
clone theory and the exponential time hypothesis. In
SODA, pages 1264?1277. SIAM.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Percy Liang, Hal Daum?e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592?599. ACM.
Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M?ario A. T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11.
Association for Computational Linguistics.
Andr?e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
David Mitchell, Bart Selman, and Hector Levesque.
1992. Hard and easy distributions of sat problems.
In AAAI, volume 92, pages 459?465. Citeseer.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statis-
tical machine translation. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 585?592. Association
for Computational Linguistics.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In EMNLP-CoNLL,
pages 952?956.
Sujith Ravi and Kevin Knight. 2010. Does giza++
make search errors? Computational Linguistics,
36(3):295?302.
C?esar Rego, Dorabela Gamboa, Fred Glover, and Colin
Osterman. 2011. Traveling salesman problem
heuristics: leading methods, implementations and
latest advances. European Journal of Operational
Research, 211(3):427?441.
1023
Mauricio G. C. Resende, L. S. Pitsoulis, and P. M.
Pardalos. 1997. Approximate solution of weighted
max-sat problems using grasp. Satisfiability prob-
lems, 35:393?405.
Daniel Spielman and Shang-Hua Teng. 2001.
Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. In Pro-
ceedings of the thirty-third annual ACM symposium
on Theory of computing, pages 296?305. ACM.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, CoNLL ?08. Association for
Computational Linguistics.
David B. Wilson. 1996. Generating random spanning
trees more quickly than the cover time. In Proceed-
ings of the twenty-eighth annual ACM symposium on
Theory of computing, pages 296?303. ACM.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562?571. Association for Computa-
tional Linguistics.
Hao Zhang, Liang Zhao, Kai Huang, and Ryan Mc-
Donald. 2013. Online learning for inexact hyper-
graph search. In Proceedings of EMNLP.
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Jaakkola, and Amir Globerson. 2014. Steps to ex-
cellence: Simple inference with refined scoring of
dependency trees. In Proceedings of the 52th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.
1024
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 291?301,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Transfer Learning for Constituency-Based Grammars
Yuan Zhang, Regina Barzilay
Massachusetts Institute of Technology
{yuanzh, regina}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
In this paper, we consider the problem
of cross-formalism transfer in parsing.
We are interested in parsing constituency-
based grammars such as HPSG and CCG
using a small amount of data specific for
the target formalism, and a large quan-
tity of coarse CFG annotations from the
Penn Treebank. While all of the target
formalisms share a similar basic syntactic
structure with Penn Treebank CFG, they
also encode additional constraints and se-
mantic features. To handle this appar-
ent discrepancy, we design a probabilistic
model that jointly generates CFG and tar-
get formalism parses. The model includes
features of both parses, allowing trans-
fer between the formalisms, while pre-
serving parsing efficiency. We evaluate
our approach on three constituency-based
grammars ? CCG, HPSG, and LFG, aug-
mented with the Penn Treebank-1. Our ex-
periments show that across all three for-
malisms, the target parsers significantly
benefit from the coarse annotations.1
1 Introduction
Over the last several decades, linguists have in-
troduced many different grammars for describing
the syntax of natural languages. Moreover, the
ongoing process of developing new formalisms is
intrinsic to linguistic research. However, before
these grammars can be used for statistical pars-
ing, they require annotated sentences for training.
The difficulty of obtaining such annotations is a
key limiting factor that inhibits the effective use of
these grammars.
1The source code for the work is available at
http://groups.csail.mit.edu/rbg/code/
grammar/acl2013.
The standard solution to this bottleneck has re-
lied on manually crafted transformation rules that
map readily available syntactic annotations (e.g,
the Penn Treebank) to the desired formalism. De-
signing these transformation rules is a major un-
dertaking which requires multiple correction cy-
cles and a deep understanding of the underlying
grammar formalisms. In addition, designing these
rules frequently requires external resources such
as Wordnet, and even involves correction of the
existing treebank. This effort has to be repeated
for each new grammar formalism, each new anno-
tation scheme and each new language.
In this paper, we propose an alternative ap-
proach for parsing constituency-based grammars.
Instead of using manually-crafted transformation
rules, this approach relies on a small amount of
annotations in the target formalism. Frequently,
such annotations are available in linguistic texts
that introduce the formalism. For instance, a
textbook on HPSG (Pollard and Sag, 1994) il-
lustrates grammatical constructions using about
600 examples. While these examples are infor-
mative, they are not sufficient for training. To
compensate for the annotation sparsity, our ap-
proach utilizes coarsely annotated data readily
available in large quantities. A natural candidate
for such coarse annotations is context-free gram-
mar (CFG) from the Penn Treebank, while the
target formalism can be any constituency-based
grammars, such as Combinatory Categorial Gram-
mar (CCG) (Steedman, 2001), Lexical Functional
Grammar (LFG) (Bresnan, 1982) or Head-Driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994). All of these formalisms share a sim-
ilar basic syntactic structure with Penn Treebank
CFG. However, the target formalisms also encode
additional constraints and semantic features. For
instance, Penn Treebank annotations do not make
an explicit distinction between complement and
adjunct, while all the above grammars mark these
291
roles explicitly. Moreover, even the identical syn-
tactic information is encoded differently in these
formalisms. An example of this phenomenon is
the marking of subject. In LFG, this informa-
tion is captured in the mapping equation, namely
? SBJ =?, while Penn Treebank represents it as
a functional tag, such as NP-SBJ. Figure 1 shows
derivations in the three target formalisms we con-
sider, as well as a CFG derivation. We can see that
the derivations of these formalisms share the same
basic structure, while the formalism-specific infor-
mation is mainly encoded in the lexical entries and
node labels.
To enable effective transfer the model has to
identify shared structural components between
the formalisms despite the apparent differences.
Moreover, we do not assume parallel annotations.
To this end, our model jointly parses the two cor-
pora according to the corresponding annotations,
enabling transfer via parameter sharing. In partic-
ular, we augment each target tree node with hidden
variables that capture the connection to the coarse
annotations. Specifically, each node in the target
tree has two labels: an entry which is specific to
the target formalism, and a latent label containing
a value from the Penn Treebank tagset, such as NP
(see Figure 2). This design enables us to repre-
sent three types of features: the target formalism-
specific features, the coarse formalism features,
and features that connect the two. This model-
ing approach makes it possible to perform transfer
to a range of target formalisms, without manually
drafting formalism-specific rules.
We evaluate our approach on three
constituency-based grammars ? CCG, HPSG,
and LFG. As a source of coarse annotations,
we use the Penn Treebank.2 Our results clearly
demonstrate that for all three formalisms, pars-
ing accuracy can be improved by training with
additional coarse annotations. For instance, the
model trained on 500 HPSG sentences achieves
labeled dependency F-score of 72.3%. Adding
15,000 Penn Treebank sentences during training
leads to 78.5% labeled dependency F-score, an
absolute improvement of 6.2%. To achieve similar
performance in the absence of coarse annotations,
the parser has to be trained on about 1,500
sentences, namely three times what is needed
when using coarse annotations. Similar results are
2While the Penn Treebank-2 contains richer annotations,
we decided to use the Penn Treebank-1 to demonstrate the
feasibility of transfer from coarse annotations.
CFG CCG 
LFG 
I                eat               apples NP                VB                   NP 
VP 
S 
I                eat                apples NP        (S[dcl]\NP)/NP         NP 
S[dcl]\NP 
S[dcl] 
 I               eat                 apples [Pron.I]     [   SBJ,   OBJ]       [N.3pl] 
ROOT ?=?? ?=?SBJ!? =?OBJ!??=? HPSG   I              eat                 apples [N.no3sg]   [N<V.bse>N]        [N.3pl] head_comp subj_head 
Figure 1: Derivation trees for CFG as well as
CCG, HPSG and LFG formalisms.
also observed on CCG and LFG formalisms.
2 Related Work
Our work belongs to a broader class of research
on transfer learning in parsing. This area has gar-
nered significant attention due to the expense asso-
ciated with obtaining syntactic annotations. Trans-
fer learning in parsing has been applied in differ-
ent contexts, such as multilingual learning (Sny-
der et al, 2009; Hwa et al, 2005; McDonald et
al., 2006; McDonald et al, 2011; Jiang and Liu,
2009), domain adaptation (McClosky et al, 2010;
Dredze et al, 2007; Blitzer et al, 2006), and cross-
formalism transfer (Hockenmaier and Steedman,
2002; Miyao et al, 2005; Cahill et al, 2002; Rie-
zler et al, 2002; Chen and Shanker, 2005; Candito
et al, 2010).
There have been several attempts to map anno-
tations in coarse grammars like CFG to annota-
tions in richer grammar, like HPSG, LFG, or CCG.
Traditional approaches in this area typically rely
on manually specified rules that encode the rela-
tion between the two formalisms. For instance,
mappings may specify how to convert traces and
functional tags in Penn Treebank to the f-structure
in LFG (Cahill, 2004). These conversion rules
are typically utilized in two ways: (1) to create a
new treebank which is consequently used to train a
parser for the target formalism (Hockenmaier and
Steedman, 2002; Clark and Curran, 2003; Miyao
et al, 2005; Miyao and Tsujii, 2008), (2) to trans-
late the output of a CFG parser into the target for-
malism (Cahill et al, 2002).
The design of these rules is a major linguis-
tic and computational undertaking, which requires
multiple iterations over the data to increase cov-
erage (Miyao et al, 2005; Oepen et al, 2004).
By nature, the mapping rules are formalism spe-
292
cific and therefore not transferable. Moreover, fre-
quently designing such mappings involves modifi-
cation to the original annotations. For instance,
Hockenmaier and Steedman (2002) made thou-
sands of POS and constituent modifications to the
Penn Treebank to facilitate transfer to CCG. More
importantly, in some transfer scenarios, determin-
istic rules are not sufficient, due to the high am-
biguity inherent in the mapping. Therefore, our
work considers an alternative set-up for cross-
formalism transfer where a small amount of an-
notations in the target formalism is used as an al-
ternative to using deterministic rules.
The limitation of deterministic transfer rules
has been recognized in prior work (Riezler et al,
2002). Their method uses a hand-crafted LFG
parser to create a set of multiple parsing candi-
dates for a given sentence. Using the partial map-
ping from CFG to LFG as the guidance, the re-
sulting trees are ranked based on their consistency
with the labeled LFG bracketing imported from
CFG. In contrast to this method, we neither require
a parser for the target formalism, nor manual rules
for partial mapping. Consequently, our method
can be applied to many different target grammar
formalisms without significant engineering effort
for each one. The utility of coarse-grained tree-
banks is determined by the degree of structural
overlap with the target formalism.
3 The Learning Problem
Recall that our goal is to learn how to parse the tar-
get formalisms while using two annotated sources:
a small set of sentences annotated in the target for-
malism (e.g., CCG), and a large set of sentences
with coarse annotations. For the latter, we use the
CFG parses from the Penn Treebank. For sim-
plicity we focus on the CCG formalism in what
follows. We also generalize our model to other
formalisms, as explained in Section 5.4.
Our notations are as follows: an input sentence
is denoted by S. A CFG parse is denoted by yCFG
and a CCG parse is denoted by yCCG. Clearly the
set of possible values for yCFG and yCCG is deter-
mined by S and the grammar. The training set is a
set of N sentences S1, . . . , SN with CFG parses
y1CFG, . . . , yNCFG, and M sentences S?1, . . . , S?M
with CCG parses y1CCG, . . . , yMCCG. It is impor-
tant to note that we do not assume we have parallel
data for CCG and CFG.
Our goal is to use such a corpus for learning
eat apples 
coarse feature on yCFG VP VP,NP 
VP    (S[dcl]\NP)/NP 
VP    S[dcl]\NP 
NP    NP 
formalism feature on yCCG S[dcl]\NP (S[dcl]\NP)/NP,NP 
joint feature on yCFG, yCCG VP, S[dcl]\NP (VP, (S[dcl]\NP)/NP), (NP, NP) 
Figure 2: Illustration of the joint CCG-CFG representa-
tion. The shadowed labels correspond to the CFG deriva-
tion yCFG, whereas the other labels correspond to the CCG
derivation yCCG. Note that the two derivations share the
same (binarized) tree structure. Also shown are features that
are turned on for this joint derivation (see Section 6).
how to generate CCG parses to unseen sentences.
4 A Joint Model for Two Formalisms
The key idea behind our work is to learn a joint
distribution over CCG and CFG parses. Such a
distribution can be marginalized to obtain a distri-
bution over CCG or CFG and is thus appropriate
when the training data is not parallel, as it is in our
setting.
It is not immediately clear how to jointly model
the CCG and CFG parses, which are structurally
quite different. Furthermore, a joint distribution
over these will become difficult to handle com-
putationally if not constructed carefully. To ad-
dress this difficulty, we make several simplifying
assumptions. First, we assume that both parses are
given in normal form, i.e., they correspond to bi-
nary derivation trees. CCG parses are already pro-
vided in this form in CCGBank. CFG parses in the
Penn Treebank are not binary, and we therefore bi-
narize them, as explained in Section 5.3.
Second, we assume that any yCFG and yCCG
jointly generated must share the same derivation
tree structure. This makes sense. Since both for-
malisms are constituency-based, their trees are ex-
pected to describe the same constituents. We de-
note the set of valid CFG and CCG joint parses for
sentence S by Y(S).
The above two simplifying assumptions make
it easy to define joint features on the two parses,
as explained in Section 6. The representation and
features are illustrated in Figure 2.
We shall work within the discriminative frame-
work, where given a sentence we model a dis-
tribution over parses. As is standard in such
settings, the distribution will be log-linear in a
set of features of these parses. Denoting y =
(yCFG, yCCG), we seek to model the distribution
293
p(y|S) corresponding to the probability of gen-
erating a pair of parses (CFG and CCG) given a
sentence. The distribution thus has the following
form:
pjoint(y|S; ?) =
1
Z(S; ?)e
f(y,S)?? . (1)
where ? is a vector of parameters to be learned
from data, and f(y, S) is a feature vector. Z(S; ?)
is a normalization (partition) function normalized
over y ? Y(S) the set of valid joint parses.
The feature vector contains three types of fea-
tures: CFG specific, CCG specific and joint CFG-
CCG. We denote these by fCFG, fCCG, fjoint.
These depend on yCCG, yCFG and y respectively.
Accordingly, the parameter vector ? is a concate-
nation of ?CCG, ?CFG and ?joint.
As mentioned above, we can use Equation 1
to obtain distributions over yCCG and yCFG via
marginalization. For the distribution over yCCG
we do precisely this, namely use:
pCCG(yCCG|S; ?) =
?
yCFG
pjoint(y|S; ?) (2)
For the distribution over yCFG we could have
marginalized pjoint over yCCG. However, this
computation is costly for each sentence, and has
to be repeated for all the sentences. Instead, we
assume that the distribution over yCFG is a log-
linear model with parameters ?CFG (i.e., a sub-
vector of ?) , namely:
pCFG(yCFG|S; ?CFG) =
efCFG(yCFG,S)??CFG
Z(S; ?CFG)
.
(3)
Thus, we assume that both pjoint and pCFG have
the same dependence on the fCFG features.
The Likelihood Objective: Given the models
above, it is natural to use maximum likelihood to
find the optimal parameters. To do this, we define
the following regularized likelihood function:
L(?) =
N?
i=1
log
(
pCFG(yiCFG|Si, ?CFG)
)
+
M?
i=1
log
(
pCCG(yiCCG|S?i, ?)
)
? ?2 ???
2
2
where pCCG and pCFG are defined in Equations
2 and 3 respectively. The last term is the l2-norm
regularization. Our goal is then to find a ? that
maximizes L(?).
Training Algorithm: For maximizing L(?)
w.r.t. ? we use the limited-memory BFGS algo-
rithm (Nocedal and Wright, 1999). Calculating
the gradient of L(?) requires evaluating the ex-
pected values of f(y, S) and fCFG under the dis-
tributions pjoint and pCFG respectively. This can
be done via the inside-outside algorithm.3
Parsing Using the Model: To parse a sentence
S, we calculate the maximum probability assign-
ment for pjoint(y|S; ?).4 The result is both a CFG
and a CCG parse. Here we will mostly be inter-
ested in the CCG parse. The joint parse with max-
imum probability is found using a standard CYK
chart parsing algorithm. The chart construction
will be explained in Section 5.
5 Implementation
This section introduces important implementa-
tion details, including supertagging, feature for-
est pruning and binarization methods. Finally,
we explain how to generalize our model to other
constituency-based formalisms.
5.1 Supertagging
When parsing a target formalism tree, one needs
to associate each word with a lexical entry. How-
ever, since the number of candidates is typically
more than one thousand, the size of the chart ex-
plodes. One effective way of reducing the number
of candidates is via supertagging (Clark and Cur-
ran, 2007). A supertagger is used for selecting a
small set of lexical entry candidates for each word
in the sentence. We use the tagger in (Clark and
Curran, 2007) as a general suppertagger for all the
grammars considered. The only difference is that
we use different lexical entries in different gram-
mars.
5.2 Feature Forest Pruning
In the BFGS algorithm (see Section 4), feature ex-
pectation is computed using the inside-outside al-
gorithm. To perform this dynamic programming
efficiently, we first need to build the packed chart,
namely the feature forest (Miyao, 2006) to rep-
resent the exponential number of all possible tree
3To speed up the implementation, gradient computation
is parallelized, using the Message Passing Interface pack-
age (Gropp et al, 1999).
4An alternative approach would be to marginalize over
yCFG and maximize over yCCG. However, this is a harder
computational problem.
294
structures. However, a common problem for lex-
icalized grammars is that the forest size is too
large. In CFG, the forest is pruned according to
the inside probability of a simple generative PCFG
model and a prior (Collins, 2003). The basic idea
is to prune the trees with lower probability. For the
target formalism, a common practice is to prune
the forest using the supertagger (Clark and Cur-
ran, 2007; Miyao, 2006). In our implementation,
we applied all pruning techniques, because the for-
est is a combination of CFG and target grammar
formalisms (e.g., CCG or HPSG).
5.3 Binarization
We assume that the derivation tree in the target for-
malism is in a normal form, which is indeed the
case for the treebanks we consider. As mentioned
in Section 4, we would also like to work with bi-
narized CFG derivations, such that all trees are in
normal form and it is easy to construct features
that link the two (see Section 6).
Since Penn Treebank trees are not binarized, we
construct a simple procedure for binarizing them.
The procedure is based on the available target for-
malism parses in the training corpus, which are bi-
narized. We illustrate it with an example. In what
follows, we describe derivations using the POS of
the head words of the corresponding node in the
tree. This makes it possible to transfer binariza-
tion rules between formalisms.
Suppose we want to learn the binarization rule
of the following derivation in CFG:
NN? (DT JJ NN) (4)
We now look for binary derivations with these
POS in the target formalism corpus, and take the
most common binarization form. For example, we
may find that the most common binarization to bi-
narize the CFG derivation in Equation 4 is:
NN? (DT (JJ NN))
If no (DT JJ NN) structure is observed in the
CCG corpus, we first apply the binary branching
on the children to the left of the head, and then on
the children to the right of the head.
We also experiment with using fixed binariza-
tion rules such as left/right branching, instead of
learning them. This results in a drop on the depen-
dency F-score by about 5%.
5.4 Implementation in Other Formalisms
We introduce our model in the context of CCG,
but the model can easily be generalized to other
constituency-based grammars, such as HPSG and
LFG. In a derivation tree, the formalism-specific
information is mainly encoded in the lexical en-
tries and the applied grammar rules, rather than the
tree structures. Therefore we only need to change
the node labels and lexical entries to the language-
specific ones, while the framework of the model
remains the same.
6 Features
Feature functions in log-linear models are de-
signed to capture the characteristics of each
derivation in the tree. In our model, as mentioned
in Section 1, the features are also defined to en-
able information transfer between coarse and rich
formalisms. In this section, we first introduce how
different types of feature templates are designed,
and then show an example of how the features help
transfer the syntactic structure information. Note
that the same feature templates are used for all the
target grammar formalisms.
Recall that our y contains both the CFG and
CCG parses, and that these use the same derivation
tree structure. Each feature will consider either the
CFG derivation, the CCG derivation or these two
derivations jointly.
The feature construction is similar to construc-
tions used in previous work (Miyao, 2006). The
features are based on the atomic features listed in
Table 1. These will be used to construct f(y, S) as
explained next.
hl lexical entries/CCG categories of the head word
r grammar rules, i.e. HPSG schema, resulting CCG
categories, LFG mapping equations
sy CFG syntactic label of the node (e.g. NP, VP)
d distance between the head words of the children
c whether a comma exists between the head words
of the children
sp the span of the subtree rooted at the node
hw surface form of the head word of the node
hp part-of-speech of the head word
pi part-of-speech of the i-th word in the sentence
Table 1: Templates of atomic features.
We define the following feature templates:
fbinary for binary derivations, funary for unary
derivations, and froot for the root nodes. These
use the atomic features in Table 1, resulting in the
295
following templates:
fbinary =
? r, syp, d, c
syl, spl, hwl, hpl, hll,
syr, spr, hwr, hpr, hlr,
pst?1, pst?2, pen+1, pen+2
?
funary = ?r, syp, hw, hp, hl?
froot = ?sy, hw, hp, hl?
In the above we used the following notation: p, l, r
denote the parent node and left/right child node,
and st, en denote the starting and ending index of
the constituent.
We also consider templates with subsets of the
above features. The final list of binary feature tem-
plates is shown in Table 2. It can be seen that some
features depend only on the CFG derivations (i.e.,
those without r,hl), and are thus in fCFG(y, S).
Others depend only on CCG derivations (i.e.,
those without sy), and are in fCCG(y, S). The
rest depend on both CCG and CFG and are thus
in fjoint(y, S).
Note that after binarization, grandparent and
sibling information becomes very important in en-
coding the structure. However, we limit the fea-
tures to be designed locally in a derivation in order
to run inside-outside efficiently. Therefore we use
the preceding and succeeding POS tag information
to approximate the grandparent and sibling infor-
mation. Empirically, these features yield a signifi-
cant improvement on the constituent accuracy.
fCFG
?d,wl,r, hpl,r, syp,l,r?, ?d,wl,r, syp,l,r?,
?c, wl,r, hpl,r, syp,l,r?, ?c, wl,r, syp,l,r?,
?d, c, hpl,r, syp,l,r?, ?d, c, syp,l,r?,
?c, spl,r, hpl,r, syp,l,r?, ?c, spl,r, syp,l,r?,
?pst?1, syp,l,r?, ?pen+1, syp,l,r?,
?pst?1, pen+1, syp,l,r?,
?pst?1, pst?2, syp,l,r?, ?pen+1, pen+2, syp,l,r?,
?pst?1, pst?2, pen+1, pen+2, syp,l,r?,
fCCG
?r, d, c, hwl,r, hpl,r, hll,r?, ?r, d, c, hwl,r, hpl,r?
?r, d, c, hwl,r, hll,r?,
?r, c, spl,r, hwl,r, hpl,r, hll,r?
?r, c, spl,r, hwl,r, hpl,r, ?, ?r, c, spl,r, hwl,r, hll,r?
?r, d, c, hpl,r, hll,r?, ?r, d, c, hpl,r?, ?r, d, c, hll,r?
?r, c, hpl,r, hll,r?, ?r, c, hpl,r?, ?r, c, hll,r?
fjoint ?r, d, c, syl,r, hll,r?, ?r, d, c, syl,r??r, c, spl,r, syl,r, hll,r?, ?r, c, spl,r, syl,r?
Table 2: Binary feature templates used in f(y, S).
Unary and root features follow a similar pattern.
In order to apply the same feature templates to
other target formalisms, we only need to assign
the atomic features r and hl with the formalism-
specific values. We do not need extra engineering
work on redesigning the feature templates.
eat apples VP    (S[dcl]\NP)/NP 
VP    S[dcl]\NP 
NP    NP 
VP VP,NP 
S[dcl]\NP (S[dcl]\NP)/NP,NP 
VP, S[dcl]\NP (VP, (S[dcl]\NP)/NP), (NP, NP) 
CCGbank 
VP 
Penn Treebank 
VP NP write letters 
VP VP,NP fCFG (y,S) : fCFG (y,S) :fCCG (y,S) :f joint (y, S) :
Figure 3: Example of transfer between CFG and
CCG formalisms.
Figure 3 gives an example in CCG of how
features help transfer the syntactic information
from Penn Treebank and learn the correspondence
to the formalism-specific information. From the
Penn Treebank CFG annotations, we can learn
that the derivation VP?(VP, NP) is common, as
shown on the left of Figure 3. In a CCG tree, this
tendency will encourage the yCFG (latent) vari-
ables to take this CFG parse. Then weights on the
fjoint features will be learned to model the con-
nection between the CFG and CCG labels. More-
over, the formalism-specific features fCCG can
also encode the formalism-specific syntactic and
semantic information. These three types of fea-
tures work together to generate a tree skeleton and
fill in the CFG and CCG labels.
7 Evaluation Setup
Grammar Train Dev. Test
CCG
Sec. 02-21
Sec. 00 Sec. 23HPSG
LFG 140 sents. in 560 sents. inPARC700 PARC700
Table 3: Training/Dev./Test split on WSJ sections
and PARC700 for different grammar formalisms.
Datasets: As a source of coarse annotations, we
use the Penn Treebank-1 (Marcus et al, 1993). In
addition, for CCG, HPSG and LFG, we rely on
formalism-specific corpora developed in prior re-
search (Hockenmaier and Steedman, 2002; Miyao
et al, 2005; Cahill et al, 2002; King et al, 2003).
All of these corpora were derived via conversion
of Penn Treebank to the target formalisms. In par-
ticular, our CCG and HPSG datasets were con-
verted from the Penn Treebank based on hand-
296
0 1000 3000 7000 11000 1500074
7678
8082
8486
88
 
 
Labeled DepUnlabeled DepUnlabeled Parseval
(a) CCG
0 1000 3000 7000 11000 1500072
7476
7880
8284
86
 
 
Labeled DepUnlabeled DepUnlabeled Parseval
(b) HPSG
0 1000 3000 7000 11000 1500065
70
75
80
 
 
Labeled DepUnlabeled DepUnlabeled Parseval
(c) LFG
Figure 4: Model performance with 500 target formalism trees and different numbers of CFG trees,
evaluated using labeled/unlabeled dependency F-score and unlabeled PARSEVAL.
crafted rules (Hockenmaier and Steedman, 2002;
Miyao et al, 2005). Table 3 shows which sec-
tions of the treebanks were used in training, test-
ing and development for both formalisms. Our
LFG training dataset was constructed in a sim-
ilar fashion (Cahill et al, 2002). However, we
choose to use PARC700 as our LFG tesing and de-
velopment datasets, following the previous work
by (Kaplan et al, 2004). It contains 700 man-
ually annotated sentences that are randomly se-
lected from Penn Treebank Section 23. The split
of PARC700 follows the setting in (Kaplan et al,
2004). Since our model does not assume parallel
data, we use distinct sentences in the source and
target treebanks. Following previous work (Hock-
enmaier, 2003; Miyao and Tsujii, 2008), we only
consider sentences not exceeding 40 words, except
on PARC700 where all sentences are used.
Evaluation Metrics: We use two evaluation
metrics. First, following previous work, we eval-
uate our method using the labeled and unlabeled
predicate-argument dependency F-score. This
metric is commonly used to measure parsing qual-
ity for the formalisms considered in this paper.
The detailed definition of this measure as applied
for each formalism is provided in (Clark and Cur-
ran, 2003; Miyao and Tsujii, 2008; Cahill et al,
2004). For CCG, we use the evaluation script
from the C&C tools.5 For HPSG, we evaluate
all types of dependencies, including punctuations.
For LFG, we consider the preds-only dependen-
cies, which are the dependencies between pairs
of words. Secondly, we also evaluate using unla-
beled PARSEVAL, a standard measure for PCFG
parsing (Petrov and Klein, 2007; Charniak and
Johnson, 2005; Charniak, 2000; Collins, 1997).
The dependency F-score captures both the target-
5Available at http://svn.ask.it.usyd.edu.au/trac/candc/wiki
grammar labels and tree-structural relations. The
unlabeled PARSEVAL is used as an auxiliary mea-
sure that enables us to separate these two aspects
by focusing on the structural relations exclusively.
Training without CFG Data: To assess the
impact of coarse data in the experiments be-
low, we also consider the model trained only on
formalism-specific annotations. When no CFG
sentences are available, we assign all the CFG la-
bels to a special value shared by all the nodes. In
this set-up, the model reduces to a normal log-
linear model for the target formalism.
Parameter Settings: During training, all the
feature parameters ? are initialized to zero. The
hyperparameters used in the model are tuned on
the development sets. We noticed, however, that
the resulting values are consistent across differ-
ent formalisms. In particular, we set the l2-norm
weight to ? = 1.0, the supertagger threshold to
? = 0.01, and the PCFG pruning threshold to
? = 0.002.
8 Experiment and Analysis
Impact of Coarse Annotations on Target For-
malism: To analyze the effectiveness of annota-
tion transfer, we fix the number of annotated trees
in the target formalism and vary the amount of
coarse annotations available to the algorithm dur-
ing training. In particular, we use 500 sentences
with formalism-specific annotations, and vary the
number of CFG trees from zero to 15,000.
As Figure 4 shows, CFG data boosts parsing ac-
curacy for all the target formalisms. For instance,
there is a gain of 6.2% in labeled dependency
F-score for HPSG formalism when 15,000 CFG
trees are used. Moreover, increasing the number
of coarse annotations used in training leads to fur-
ther improvement on different evaluation metrics.
297
0 1000 2000 3000 4000 5000 60007475
7677
7879
8081
8283
84
 
 
w/o CFG15000 CFG
(a) CCG
0 1000 2000 3000 4000 5000 600072
7476
7880
8284
 
 
w/o CFG15000 CFG
(b) HPSG
0 1000 2000 3000 4000 5000 600066
6870
7274
7678
 
 
w/o CFG15000 CFG
(c) LFG
0 1000 2000 3000 4000 5000 60007778
7980
8182
8384
8586
8788
 
 
w/o CFG15000 CFG
(d) CCG
0 1000 2000 3000 4000 5000 60007072
7476
7880
8284
86
 
 
w/o CFG15000 CFG
(e) HPSG
0 1000 2000 3000 4000 5000 600068
7072
7476
7880
8284
 
 
w/o CFG15000 CFG
(f) LFG
Figure 5: Model performance with different target formalism trees and zero or 15,000 CFG trees. The
first row shows the results of labeled dependency F-score and the second row shows the results of unla-
beled PARSEVAL.
Tradeoff between Target and Coarse Annota-
tions: We also assess the relative contribution
of coarse annotations when the size of annotated
training corpus in the target formalism varies. In
this set of experiments, we fix the number of CFG
trees to 15,000 and vary the number of target an-
notations from 500 to 4,000. Figure 5 shows
the relative contribution of formalism-specific an-
notations compared to that of the coarse annota-
tions. For instance, Figure 5a shows that the pars-
ing performance achieved using 2,000 CCG sen-
tences can be achieved using approximately 500
CCG sentences when coarse annotations are avail-
able for training. More generally, the result con-
vincingly demonstrates that coarse annotations are
helpful for all the sizes of formalism-specific train-
ing data. As expected, the improvement margin
decreases when more formalism-specific data is
used.
Figure 5 also illustrates a slightly different char-
acteristics of transfer performance between two
evaluation metrics. Across all three grammars,
we can observe that adding CFG data has a
more pronounced effect on the PARSEVAL mea-
sure than the dependency F-score. This phe-
nomenon can be explained as follows. The un-
labeled PARSEVAL score (Figure 5d-f) mainly re-
lies on the coarse structural information. On
the other hand, predicate-argument dependency F-
score (Figure 5a-c) also relies on the target gram-
mar information. Because that our model only
transfers structural information from the source
treebank, the gains of PARSEVAL are expected to
be larger than that of dependency F-score.
Grammar Parser # Grammar trees1,000 15,000
CCG C&C 74.1 / 83.4 82.6 / 90.1Model 76.8 / 85.5 84.7 / 90.9
HPSG Enju 75.8 / 80.6 84.2 / 87.3Model 76.9 / 82.0 84.9 / 88.3
LFG
Pipeline
Annotator 68.5 / 74.0 82.6 / 85.9
Model 69.8 / 76.6 81.1 / 84.7
Table 4: The labeled/unlabeled dependency F-
score comparisons between our model and state-
of-the-art parsers.
Comparison to State-of-the-art Parsers: We
would also like to demonstrate that the above
gains of our transfer model are achieved using
an adequate formalism-specific parser. Since our
model can be trained exclusively on formalism-
specific data, we can compare it to state-of-the-
art formalism-specific parsers. For this experi-
ment, we choose the C&C parser (Clark and Cur-
ran, 2003) for CCG, Enju parser (Miyao and Tsu-
jii, 2008) for HPSG and pipeline automatic an-
notator (Cahill et al, 2004) with Charniak parser
for LFG. For all three parsers, we use the imple-
mentation provided by the authors with the default
parameter values. All the models are trained on
either 1,000 or 15,000 sentences annotated with
formalism-specific trees, thus evaluating their per-
formances on small scale or large scale of data.
As Table 4 shows, our model is competitive with
298
all the baselines described above. It?s not sur-
prising that Cahill?s model outperforms our log-
linear model because it relies heavily on hand-
crafted rules optimized for the dataset.
Correspondence between CFG and Target For-
malisms: Finally, we analyze highly weighted
features. Table 5 shows such features for HPSG;
similar patterns are also found for the other
grammar formalisms. The first two features are
formalism-specific ones, the first for HPSG and
the second for CFG. They show that we correctly
learn a frequent derivation in the target formalism
and CFG. The third one shows an example of a
connection between CFG and the target formal-
ism. Our model correctly learns that a syntactic
derivation with children VP and NP is very likely
to be mapped to the derivation (head comp)?
([N?V?N],[N.3sg]) in HPSG.
Feature type Features with high weight
Target
formalism
Template
(r) ? (hll, hpl)(hlr, pr)
Examples
(head comp)?
([N?V?N],VB)([N.3sg],NN)
Coarse
formalism
Template
(syp) ? (syl, hpl)(syr, hpr)
Examples
(VP)?(VP,VB)(NP,NN)
Joint
features
Template
(r) ? (hll, syl)(ler, syr)
Examples
(head comp)?
([N?V?N],VP)([N.3sg],NP)
Table 5: Example features with high weight.
9 Conclusions
We present a method for cross-formalism trans-
fer in parsing. Our model utilizes coarse syn-
tactic annotations to supplement a small num-
ber of formalism-specific trees for training on
constituency-based grammars. Our experimen-
tal results show that across a range of such for-
malisms, the model significantly benefits from the
coarse annotations.
Acknowledgments
The authors acknowledge the support of the Army
Research Office (grant 1130128-258552). We
thank Yusuke Miyao, Ozlem Cetinoglu, Stephen
Clark, Michael Auli and Yue Zhang for answering
questions and sharing the codes of their work. We
also thank the members of the MIT NLP group
and the ACL reviewers for their suggestions and
comments. Any opinions, findings, conclusions,
or recommendations expressed in this paper are
those of the authors, and do not necessarily reflect
the views of the funding organizations.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128. Association for Com-
putational Linguistics.
Joan Bresnan. 1982. The mental representation of
grammatical relations, volume 1. The MIT Press.
Aoife Cahill, Mairad McCarthy, Josef van Genabith,
and Andy Way. 2002. Parsing with pcfgs and au-
tomatic f-structure annotation. In Proceedings of
the Seventh International Conference on LFG, pages
76?95. CSLI Publications.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics, page 319. As-
sociation for Computational Linguistics.
Aoife Cahill. 2004. Parsing with Automatically Ac-
quired, Wide-Coverage, Robust, Probabilistic LFG
Approximation. Ph.D. thesis.
Marie Candito, Beno??t Crabbe?, Pascal Denis, et al
2010. Statistical french dependency parsing: tree-
bank conversion and first results. In Proceed-
ings of the Seventh International Conference on
Language Resources and Evaluation (LREC 2010),
pages 1840?1847.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139.
John Chen and Vijay K Shanker. 2005. Automated
extraction of tags from the penn treebank. New de-
velopments in parsing technology, pages 73?89.
Stephen Clark and James R Curran. 2003. Log-linear
models for wide-coverage ccg parsing. In Proceed-
ings of the 2003 conference on Empirical methods
in natural language processing, pages 97?104. As-
sociation for Computational Linguistics.
299
Stephen Clark and James R Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical pprsing. In Proceedings of the
eighth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 16?23.
Association for Computational Linguistics.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joao V Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adap-
tation for dependency parsing. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL,
volume 2007.
William Gropp, Ewing Lusk, and Anthony Skjellum.
1999. Using MPI: portable parallel programming
with the message passing interface, volume 1. MIT
press.
Julia Hockenmaier and Mark Steedman. 2002.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proceedings of the Third LREC
Conference, pages 1974?1981.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with combinatory categorial grammar.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2005. Breaking the resource bottleneck for multi-
lingual parsing. Technical report, DTIC Document.
Wenbin Jiang and Qun Liu. 2009. Automatic adap-
tation of annotation standards for dependency pars-
ing: using projected treebank as source corpus. In
Proceedings of the 11th International Conference on
Parsing Technologies, pages 25?28. Association for
Computational Linguistics.
Ronald M. Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow and
deep stochastic parsing. In Proceedings of NAACL.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald M Kaplan. 2003. The
parc 700 dependency bank. In Proceedings of the
EACL03: 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), pages 1?8.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 28?36. Association for Computational Lin-
guistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, pages 216?220. Association for
Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62?72. Association for Computational Lin-
guistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34(1):35?80.
Yusuke Miyao, Takashi Ninomiya, and Junichi Tsu-
jii. 2005. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. Natural Language
Processing?IJCNLP 2004, pages 684?693.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis.
Jorge Nocedal and Stephen J Wright. 1999. Numerical
optimization. Springer verlag.
Stephan Oepen, Dan Flickinger, and Francis Bond.
2004. Towards holistic grammar engineering
and testing?grafting treebank maintenance into the
grammar revision cycle. In Proceedings of the IJC-
NLP workshop beyond shallow analysis. Citeseer.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 404?411.
Carl Pollard and Ivan A Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.
Stefan Riezler, Tracy H King, Ronald M Kaplan,
Richard Crouch, John T Maxwell III, and Mark
Johnson. 2002. Parsing the wall street journal us-
ing a lexical-functional grammar and discriminative
estimation techniques. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 271?278. Association for Com-
putational Linguistics.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
300
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 73?81. Association for Computational Lin-
guistics.
Mark Steedman. 2001. The syntactic process. MIT
press.
Yue Zhang, Stephen Clark, et al 2011. Shift-reduce
ccg parsing. In Proceedings of the 49th Meeting
of the Association for Computational Linguistics,
pages 683?692.
301
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 197?207,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Steps to Excellence: Simple Inference with Refined Scoring of
Dependency Trees
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola
Massachusetts Institute of Technology
{yuanzh, taolei, regina, tommi}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
Much of the recent work on depen-
dency parsing has been focused on solv-
ing inherent combinatorial problems as-
sociated with rich scoring functions. In
contrast, we demonstrate that highly ex-
pressive scoring functions can be used
with substantially simpler inference pro-
cedures. Specifically, we introduce a
sampling-based parser that can easily han-
dle arbitrary global features. Inspired
by SampleRank, we learn to take guided
stochastic steps towards a high scoring
parse. We introduce two samplers for
traversing the space of trees, Gibbs and
Metropolis-Hastings with Random Walk.
The model outperforms state-of-the-art re-
sults when evaluated on 14 languages
of non-projective CoNLL datasets. Our
sampling-based approach naturally ex-
tends to joint prediction scenarios, such
as joint parsing and POS correction. The
resulting method outperforms the best re-
ported results on the CATiB dataset, ap-
proaching performance of parsing with
gold tags.
1
1 Introduction
Dependency parsing is commonly cast as a max-
imization problem over a parameterized scoring
function. In this view, the use of more expres-
sive scoring functions leads to more challenging
combinatorial problems of finding the maximiz-
ing parse. Much of the recent work on parsing has
been focused on improving methods for solving
the combinatorial maximization inference prob-
lems. Indeed, state-of-the-art results have been ob-
1
The source code for the work is available at
http://groups.csail.mit.edu/rbg/code/
global/acl2014.
tained by adapting powerful tools from optimiza-
tion (Martins et al, 2013; Martins et al, 2011;
Rush and Petrov, 2012). We depart from this view
and instead focus on using highly expressive scor-
ing functions with substantially simpler inference
procedures. The key ingredient in our approach is
how learning is coupled with inference. Our com-
bination outperforms the state-of-the-art parsers
and remains comparable even if we adopt their
scoring functions.
Rich scoring functions have been used for some
time. They first appeared in the context of rerank-
ing (Collins, 2000), where a simple parser is used
to generate a candidate list which is then reranked
according to the scoring function. Because the
number of alternatives is small, the scoring func-
tion could in principle involve arbitrary (global)
features of parse trees. The power of this method-
ology is nevertheless limited by the initial set of
alternatives from the simpler parser. Indeed, the
set may already omit the gold parse. We dispense
with the notion of a candidate set and seek to ex-
ploit the scoring function more directly.
In this paper, we introduce a sampling-based
parser that places few or no constraints on the
scoring function. Starting with an initial candi-
date tree, our inference procedure climbs the scor-
ing function in small (cheap) stochastic steps to-
wards a high scoring parse. The proposal distri-
bution over the moves is derived from the scoring
function itself. Because the steps are small, the
complexity of the scoring function has limited im-
pact on the computational cost of the procedure.
We explore two alternative proposal distributions.
Our first strategy is akin to Gibbs sampling and
samples a new head for each word in the sentence,
modifying one arc at a time. The second strat-
egy relies on a provably correct sampler for first-
order scores (Wilson, 1996), and uses it within a
Metropolis-Hastings algorithm for general scoring
functions. It turns out that the latter optimizes the
197
score more efficiently than the former.
Because the inference procedure is so simple,
it is important that the parameters of the scoring
function are chosen in a manner that facilitates
how we climb the scoring function in small steps.
One way to achieve this is to make sure that im-
provements in the scoring functions are correlated
with improvements in the quality of the parse.
This approach was suggested in the SampleRank
framework (Wick et al, 2011) for training struc-
tured prediction models. This method was origi-
nally developed for a sequence labeling task with
local features, and was shown to be more effec-
tive than state-of-the-art alternatives. Here we ap-
ply SampleRank to parsing, applying several mod-
ifications such as the proposal distributions men-
tioned earlier.
The benefits of sampling-based learning go be-
yond stand-alone parsing. For instance, we can
use the framework to correct preprocessing mis-
takes in features such as part-of-speech (POS)
tags. In this case, we combine the scoring func-
tion for trees with a stand-alone tagging model.
When proposing a small move, i.e., sampling a
head of the word, we can also jointly sample its
POS tag from a set of alternatives provided by
the tagger. As a result, the selected tag is influ-
enced by a broad syntactic context above and be-
yond the initial tagging model and is directly opti-
mized to improve parsing performance. Our joint
parsing-tagging model provides an alternative to
the widely-adopted pipeline setup.
We evaluate our method on benchmark multi-
lingual dependency corpora. Our method outper-
forms the Turbo parser across 14 languages on av-
erage by 0.5%. On four languages, we top the best
published results. Our method provides a more
effective mechanism for handling global features
than reranking, outperforming it by 1.3%. In terms
of joint parsing and tagging on the CATiB dataset,
we nearly bridge (88.38%) the gap between in-
dependently predicted (86.95%) and gold tags
(88.45%). This is better than the best published
results in the 2013 SPMRL shared task (Seddah et
al., 2013), including parser ensembles.
2 Related Work
Earlier works on dependency parsing focused on
inference with tractable scoring functions. For in-
stance, a scoring function that operates over each
single dependency can be optimized using the
maximum spanning tree algorithm (McDonald et
al., 2005). It was soon realized that using higher
order features could be beneficial, even at the cost
of using approximate inference and sacrificing op-
timality. The first successful approach in this arena
was reranking (Collins, 2000; Charniak and John-
son, 2005) on constituency parsing. Reranking
can be combined with an arbitrary scoring func-
tion, and thus can easily incorporate global fea-
tures over the entire parse tree. Its main disadvan-
tage is that the output parse can only be one of the
few parses passed to the reranker.
Recent work has focused on more powerful in-
ference mechanisms that consider the full search
space (Zhang and McDonald, 2012; Rush and
Petrov, 2012; Koo et al, 2010; Huang, 2008). For
instance, Nakagawa (2007) deals with tractabil-
ity issues by using sampling to approximate
marginals. Another example is the dual decompo-
sition (DD) framework (Koo et al, 2010; Martins
et al, 2011). The idea in DD is to decompose the
hard maximization problem into smaller parts that
can be efficiently maximized and enforce agree-
ment among these via Lagrange multipliers. The
method is essentially equivalent to linear program-
ming relaxation approaches (Martins et al, 2009;
Sontag et al, 2011), and also similar in spirit to
ILP approaches (Punyakanok et al, 2004).
A natural approach to approximate global in-
ference is via search. For instance, a transition-
based parsing system (Zhang and Nivre, 2011)
incrementally constructs a parsing structure us-
ing greedy beam-search. Other approaches op-
erate over full trees and generate a sequence
of candidates that successively increase the
score (Daum?e III et al, 2009; Li et al, 2013;
Wick et al, 2011). Our work builds on one such
approach ? SampleRank (Wick et al, 2011), a
sampling-based learning algorithm. In SampleR-
ank, the parameters are adjusted so as to guide the
sequence of candidates closer to the target struc-
ture along the search path. The method has been
successfully used in sequence labeling and ma-
chine translation (Haddow et al, 2011). In this
paper, we demonstrate how to adapt the method
for parsing with rich scoring functions.
3 Sampling-Based Dependency Parsing
with Global Features
In this section, we introduce our novel sampling-
based dependency parser which can incorporate
198
arbitrary global features. We begin with the no-
tation before addressing the decoding and learning
algorithms. Finally, we extend our model to a joint
parsing and POS correction task.
3.1 Notations
We denote sentences by x and the corresponding
dependency trees by y ? Y(x). Here Y(x) is the
set of valid (projective or non-projective) depen-
dency trees for sentence x. We use x
j
to refer
to the jth word of sentence x, and h
j
to the head
word of x
j
. A training set of size N is given as a
set of pairs D = {(x
(i)
, y
(i)
)}
N
i=1
where y
(i)
is the
ground truth parse for sentence x
(i)
.
We parameterize the scoring function s(x, y) as
s(x, y) = ? ? f(x, y) (1)
where f(x, y) is the feature vector associated with
tree y for sentence x. We do not make any assump-
tions about how the feature function decomposes.
In contrast, most state-of-the-art parsers operate
under the assumption that the feature function de-
composes into a sum of simpler terms. For exam-
ple, in the second-order MST parser (McDonald
and Pereira, 2006), all the feature terms involve
arcs or consecutive siblings. Similarly, parsers
based on dual decomposition (Martins et al, 2011;
Koo et al, 2010) assume that s(x, y) decomposes
into a sum of terms where each term can be maxi-
mized over y efficiently.
3.2 Decoding
The decoding problem consists of finding a valid
dependency tree y ? Y(x) that maximizes the
score s(x, y) = ? ? f(x, y) with parameters ?.
For scoring functions that extend beyond first-
order arc preferences, finding the maximizing non-
projective tree is known to be NP-hard (McDonald
and Pereira, 2006). We find a high scoring tree
through sampling, and (later) learn the parameters
? so as to further guide this process.
Our sampler generates a sequence of depen-
dency structures so as to approximate independent
samples from
p(y|x, T, ?) ? exp (s(x, y)/T ) (2)
The temperature parameter T controls how con-
centrated the samples are around the maximum
of s(x, y) (e.g., see Geman and Geman (1984)).
Sampling from target distribution p is typically as
hard as (or harder than) that maximizing s(x, y).
Inputs: ?, x, T
0
(initial temperature), c (temperature
update rate), proposal distribution q.
Outputs: y
?
T ? T
0
Set y
0
to some random tree
y
?
? y
0
repeat
y
?
? q(?|x, y
t
, T, ?)
if s(x, y
?
) > s(x, y
?
) then
y
?
? y
?
? = min
[
1,
p(y
?
)q(y
t
|y
?
)
p(y
t
)q(y
?
|y
t
)
]
Sample Bernouli variable Z with P [Z = 1] = ?.
if Z = 0 then
y
t+1
? y
t
else
y
t+1
? y
?
t? t+ 1
T ? c ? T
until convergence
return y
?
Figure 1: Sampling-based algorithm for decoding
(i.e., approximately maximizing s(x, y)).
We follow here a Metropolis-Hastings sampling
algorithm (e.g., see Andrieu et al (2003)) and
explore different alternative proposal distributions
q(y
?
|x, y, ?, T ). The distribution q governs the
small steps that are taken in generating a sequence
of structures. The target distribution p folds into
the procedure by defining the probability that we
will accept the proposed move. The general struc-
ture of our sampling algorithm is given in Figure 1.
3.2.1 Gibbs Sampling
Perhaps the most natural choice of the proposal
distribution q is a conditional distribution from p.
This is feasible if we restrict the proposed moves
to only small changes in the current tree. In our
case, we choose a word j randomly, and then sam-
ple its head h
j
according to p with the constraint
that we obtain a valid tree (when projective trees
are sought, this constraint is also incorporated).
For this choice of q, the probability of accepting
the new tree (? in Figure 1) is identically one.
Thus new moves are always accepted.
3.2.2 Exact First-Order Sampling
One shortcoming of the Gibbs sampler is that it
only changes one variable (arc) at a time. This
usually leads to slow mixing, requiring more sam-
ples to get close to the parse with maximum
score. Ideally, we would change multiple heads
in the parse tree simultaneously, and sample those
choices from the corresponding conditional distri-
bution of p. While in general this is increasingly
difficult with more heads, it is indeed tractable if
199
Inputs: x, y
t
, ?, K (number of heads to change).
Outputs: y
?
for i = 1 to |x| do
inTree[i]? false
ChangeNode[i]? false
Set ChangeNode to true for K random nodes.
head[0]? ?1
for i = 1 to |x| do
u? i
while not inTree[u] do
if ChangeNode[u] then
head[u]? randomHead(u, ?)
else
head[u]? y
t
(u)
u? head[u]
if LoopExist(head) then
EraseLoop(head)
u? i
while not inTree[u] do
inTree[u]? true
u? head[u]
return Construct tree y
?
from the head array.
Figure 2: A proposal distribution q(y
?
|y
t
) based
on the random walk sampler of Wilson (1996).
The function randomHead samples a new head for
node u according to the first-order weights given
by ?.
the model corresponds to a first-order parser. One
such sampling algorithm is the random walk sam-
pler of Wilson (1996). It can be used to obtain
i.i.d. samples from distributions of the form:
p(y) ?
?
i?j?y
w
ij
, (3)
where y corresponds to a tree with a spcified root
and w
ij
is the exponential of the first-order score.
y is always a valid parse tree if we allow multiple
children of the root and do not impose projective
constraint. The algorithm in Wilson (1996) iter-
ates over all the nodes, and for each node performs
a random walk according to the weights w
ij
until
the walk creates a loop or hits a tree. In the first
case the algorithm erases the loop and continues
the walk. If the walk hits the current tree, the walk
path is added to form a new tree with more nodes.
This is repeated until all the nodes are included in
the tree. It can be shown that this procedure gen-
erates i.i.d. trees from p(y).
Since our features do not by design correspond
to a first-order parser, we cannot use the Wilson
algorithm as it is. Instead we use it as the proposal
function and sample a subset of the dependen-
cies from the first-order distribution of our model,
while fixing the others. In each step we uniformly
sample K nodes to update and sample their new
1!
2!
not?Monday? not ssssssssssss" ?""" wasloop erased!Black?Monday?was
ROOT! It! was! not! Black! Monday!
2!
1!
3!
ROOT! It! was! not! Black! Monday!
(b) walk path:!
(c) walk path:!
(a) original tree!
ROOT! It! was! not! Black! Monday!
Figure 3: An illustration of random walk sam-
pler. The index on each edge indicates its order on
each walk path. The heads of the red words are
sampled while others are fixed. The blue edges
represent the current walk path and the black ones
are already in the tree. Note that the walk direc-
tion is opposite to the dependency direction. (a)
shows the original tree before sampling; (b) and
(c) show the walk path and how the tree is gener-
ated in two steps. The loop not? Monday? not
in (b) is erased.
heads using the Wilson algorithm (in the experi-
ments we use K = 4). Note that blocked Gibbs
sampling would be exponential in K, and is thus
very slow already at K = 4. The procedure is de-
scribed in Figure 2 with a graphic illustration in
Figure 3.
3.3 Training
In this section, we describe how to learn the
adjustable parameters ? in the scoring function.
The parameters are learned in an on-line fash-
ion by successively imposing soft constraints be-
tween pairs of dependency structures. We intro-
duce both margin constraints and constraints per-
taining to successive samples generated along the
search path. We demonstrate later that both types
of constraints are essential.
We begin with the standard margin constraints.
An ideal scoring function would always rank the
gold parse higher than any alternative. Moreover,
alternatives that are far from the gold parse should
score even lower. As a result, we require that
s(x
(i)
, y
(i)
)? s(x
(i)
, y) ? ?(y
(i)
, y) ?y (4)
where ?(y
(i)
, y) is the number of head mistakes
in y relative to the gold parse y
(i)
. We adopt here
a shorthand Err(y) = ?(y
(i)
, y), where the de-
200
pendence on y
(i)
is implied from context. Note
that Equation 4 contains exponentially many con-
straints and cannot be enforced jointly for general
scoring functions. However, our sampling proce-
dure generates a small number of structures along
the search path. We enforce only constraints cor-
responding to those samples.
The second type of constraints are enforced be-
tween successive samples along the search path.
To illustrate the idea, consider a parse y that dif-
fers from y
(i)
in only one arc, and a parse y
?
that
differs from y
(i)
in ten arcs. We cannot necessarily
assume that s(x, y) is greater than s(x, y
?
) without
additional encouragement. Thus, we can comple-
ment the constraints in Equation 4 with additional
pairwise constraints (Wick et al, 2011):
s(x
(i)
, y)? s(x
(i)
, y
?
) ? Err(y
?
)? Err(y) (5)
where similarly to Equation 4, the difference in
scores scales with the differences in errors with re-
spect to the target y
(i)
. We only enforce the above
constraints for y, y
?
that are consecutive samples
in the course of the sampling process. These con-
straints serve to guide the sampling process de-
rived from the scoring function towards the gold
parse.
We learn the parameters ? in an on-line fashion
to satisfy the above constraints. This is done via
the MIRA algorithm (Crammer and Singer, 2003).
Specifically, if the current parameters are ?
t
, and
we enforce constraint Equation 5 for a particular
pair y, y
?
, then we will find ?
t+1
that minimizes
min ||? ? ?
t
||
2
+ C?
s.t. ? ? (f(x, y)? f(x, y
?
)) ? Err(y
?
)? Err(y)? ?
(6)
The updates can be calculated in closed form. Fig-
ure 4 summarizes the learning algorithm. We re-
peatedly generate parses based on the current pa-
rameters ?
t
for each sentence x
(i)
, and use succes-
sive samples to enforce constraints in Equation 4
and Equation 5 one at a time.
3.4 Joint Parsing and POS Correction
It is easy to extend our sampling-based parsing
framework to joint prediction of parsing and other
labels. Specifically, when sampling the new heads,
we can also sample the values of other variables at
the same time. For instance, we can sample the
POS tag, the dependency relation or morphology
information. In this work, we investigate a joint
Inputs: D = {(x
(i)
, y
(i)
)}
N
i=1
.
Outputs: Learned parameters ?.
?
0
? 0
for e = 1 to #epochs do
for i = 1 toN do
y
?
? q(?|x
(i)
, y
t
i
i
, ?
t
)
y
+
= arg min
y?
{
y
t
i
i
,y
?
}
Err(y)
y
?
= arg max
y?
{
y
t
i
i
,y
?
}
Err(y)
y
t
i
+1
i
? acceptOrReject(y
?
, y
t
i
i
, ?
t
)
t
i
? t
i
+ 1
?f = f(x
(i)
, y
+
)? f(x
(i)
, y
?
)
?Err = Err(y
+
)? Err(y
?
)
if ?Err 6= 0 and ?
t
? ?f < ?Err then
?
t+1
? updateMIRA(?f,?Err, ?
t
)
t? t+ 1
?f
g
= f(x
(i)
, y
(i)
)? f(x
(i)
, y
t
i
i
)
if ?
t
? ?f
g
< Err(y
t
i
i
) then
?
t+1
? updateMIRA(?f
g
, Err(y
t
i
i
), ?
t
)
t? t+ 1
return Average of ?
0
, . . . , ?
t
parameters.
Figure 4: SampleRank algorithm for learning. The
rejection strategy is as in Figure 1. y
t
i
i
is the t
i
th
tree sample of x
(i)
. The first MIRA update (see
Equation 6) enforces a ranking constraint between
two sampled parses. The second MIRA update en-
forces constraints between a sampled parse and the
gold parse. In practice several samples are drawn
for each sentence in each epoch.
POS correction scenario in which only the pre-
dicted POS tags are provided in the testing phase,
while both gold and predicted tags are available
for the training set.
We extend our model such that it jointly learns
how to predict a parse tree and also correct the pre-
dicted POS tags for a better parsing performance.
We generate the POS candidate list for each word
based on the confusion matrix on the training set.
Let c(t
g
, t
p
) be the count when the gold tag is t
g
and the predicted one is t
p
. For each word w, we
first prune out its POS candidates by using the vo-
cabulary from the training set. We don?t prune
anything if w is unseen. Assuming that the pre-
dicted tag forw is t
p
, we further remove those tags
t if their counts are smaller than some threshold
c(t, t
p
) < ? ? c(t
p
, t
p
)
2
.
After generating the candidate lists for each
word, the rest of the extension is rather straight-
forward. For each sampling, let H be the set of
candidate heads and T be the set of candidate POS
tags. The Gibbs sampler will generate a new sam-
ple from the space H ? T . The other parts of the
algorithm remain the same.
2
In our work we choose ? = 0.003, which gives a 98.9%
oracle POS tagging accuracy on the CATiB development set.
201
arc!
head bigram!!h h m m+1arbitrary sibling!?!h m sh m consecutive sibling!h m s grandparent!g h m
grand-sibling!g h m s tri-siblings!h m s t grand-grandparent!g h mgg
outer-sibling-grandchild!h m sgc h s gcminner-sibling-grandchild!
Figure 5: First- to third-order features.
4 Features
First- to Third-Order Features The feature
templates of first- to third-order features are
mainly drawn from previous work on graph-
based parsing (McDonald and Pereira, 2006),
transition-based parsing (Nivre et al, 2006) and
dual decomposition-based parsing (Martins et al,
2011). As shown in Figure 5, the arc is the basic
structure for first-order features. We also define
features based on consecutive sibling, grandpar-
ent, arbitrary sibling, head bigram, grand-sibling
and tri-siblings, which are also used in the Turbo
parser (Martins et al, 2013). In addition to these
first- to third-order structures, we also consider
grand-grandparent and sibling-grandchild struc-
tures. There are two types of sibling-grandchild
structures: (1) inner-sibling when the sibling is
between the head and the modifier and (2) outer-
sibling for the other cases.
Global Features We used feature shown promis-
ing in prior reranking work Charniak and Johnson
(2005), Collins (2000) and Huang (2008).
? Right Branch This feature enables the model
to prefer right or left-branching trees. It counts
the number of words on the path from the root
node to the right-most non-punctuation word,
normalized by the length of the sentence.
? Coordination In a coordinate structure, the two
adjacent conjuncts usually agree with each other
on POS tags and their span lengths. For in-
stance, in cats and dogs, the conjuncts are both
short noun phrases. Therefore, we add differ-
ent features to capture POS tag and span length
consistency in a coordinate structure.
? PP Attachment We add features of lexical tu-
eat! with! knife! and! fork!
Figure 6: An example of PP attachment with coor-
dination. The arguments should be knife and fork,
not and.
ples involving the head, the argument and the
preposition of prepositional phrases. Generally,
this feature can be defined based on an instance
of grandparent structure. However, we also han-
dle the case of coordination. In this case, the ar-
guments should be the conjuncts rather than the
coordinator. Figure 6 shows an example.
? Span Length This feature captures the distribu-
tion of the binned span length of each POS tag.
It also includes flags of whether the span reaches
the end of the sentence and whether the span is
followed by the punctuation.
? Neighbors The POS tags of the neighboring
words to the left and right of each span, together
with the binned span length and the POS tag at
the span root.
? Valency We consider valency features for each
POS tag. Specifically, we add two types of va-
lency information: (1) the binned number of
non-punctuation modifiers and (2) the concate-
nated POS string of all those modifiers.
? Non-projective Arcs A flag indicating if a de-
pendency is projective or not (i.e. if it spans a
word that does not descend from its head) (Mar-
tins et al, 2011). This flag is also combined with
the POS tags or the lexical words of the head and
the modifier.
POS Tag Features In the joint POS correction
scenario, we also add additional features specifi-
cally for POS prediction. The feature templates
are inspired by previous feature-rich POS tagging
work (Toutanova et al, 2003). However, we are
free to add higher order features because we do
not rely on dynamic programming decoding. In
our work we use feature templates up to 5-gram.
Table 1 summarizes all POS tag feature templates.
5 Experimental Setup
Datasets We evaluate our model on standard
benchmark corpora ? CoNLL 2006 and CoNLL
2008 (Buchholz and Marsi, 2006; Surdeanu et al,
2008) ? which include dependency treebanks for
14 different languages. Most of these data sets
202
1-gram
?t
i
?, ?t
i
, w
i?2
?, ?t
i
, w
i?1
?, ?t
i
, w
i
?, ?t
i
, w
i+1
?,
?t
i
, w
i+2
?
2-gram
?t
i?1
, t
i
?, ?t
i?2
, t
i
?, ?t
i?1
, t
i
, w
i?1
?,
?t
i?1
, t
i
, w
i
?
3-gram
?t
i?1
, t
i
, t
i+1
?, ?t
i?2
, t
i
, t
i+1
, ?, ?t
i?1
, t
i
, t
i+2
?,
?t
i?2
, t
i
, t
i+2
?
4-gram
?t
i?2
, t
i?1
, t
i
, t
i+1
?, ?t
i?2
, t
i?1
, t
i
, t
i+2
?,
?t
i?2
, t
i
, t
i+1
, t
i+2
?
5-gram ?t
i?2
, t
i?1
, t
i
, t
i+1
, t
i+2
?
Table 1: POS tag feature templates. t
i
and w
i
de-
notes the POS tag and the word at the current posi-
tion. t
i?x
and t
i+x
denote the left and right context
tags, and similarly for words.
contain non-projective dependency trees. We use
all sentences in CoNLL datasets during training
and testing. We also use the Columbia Arabic
Treebank (CATiB) (Marton et al, 2013). CATiB
mostly includes projective trees. The trees are an-
notated with both gold and predicted versions of
POS tags and morphology information. Follow-
ing Marton et al (2013), for this dataset we use
12 core POS tags, word lemmas, determiner fea-
tures, rationality features and functional genders
and numbers.
Some CATiB sentences exceed 200 tokens. For
efficiency, we limit the sentence length to 70 to-
kens in training and development sets. However,
we do not impose this constraint during testing.
We handle long sentences during testing by apply-
ing a simple split-merge strategy. We split the sen-
tence based on the ending punctuation, predict the
parse tree for each segment and group the roots of
resulting trees into a single node.
Evaluation Measures Following standard prac-
tice, we use Unlabeled Attachment Score (UAS)
as the evaluation metric in all our experiments.
We report UAS excluding punctuation on CoNLL
datasets, following Martins et al (2013). For the
CATiB dataset, we report UAS including punctu-
ation in order to be consistent with the published
results in the 2013 SPMRL shared task (Seddah et
al., 2013).
Baselines We compare our model with the Turbo
parser and the MST parser. For the Turbo parser,
we directly compare with the recent published re-
sults in (Martins et al, 2013). For the MST parser,
we train a second-order non-projective model us-
ing the most recent version of the code
3
.
We also compare our model against a discrim-
inative reranker. The reranker operates over the
3
http://sourceforge.net/projects/mstparser/
top-50 list obtained from the MST parser
4
. We
use a 10-fold cross-validation to generate candi-
date lists for training. We then train the reranker
by running 10 epochs of cost-augmented MIRA.
The reranker uses the same features as our model,
along with the tree scores obtained from the MST
parser (which is a standard practice in reranking).
Experimental Details Following Koo and Collins
(2010), we always first train a first-order pruner.
For each word x
i
, we prune away the incoming
dependencies ?h
i
, x
i
? with probability less than
0.005 times the probability of the most likely head,
and limit the number of candidate heads up to 30.
This gives a 99% pruning recall on the CATiB
development set. The first-order model is also
trained using the algorithm in Figure 4. Af-
ter pruning, we tune the regularization parameter
C = {0.1, 0.01, 0.001} on development sets for
different languages. Because the CoNLL datasets
do not have a standard development set, we ran-
domly select a held out of 200 sentences from the
training set. We also pick the training epochs from
{50, 100, 150} which gives the best performance
on the development set for each language. After
tuning, the model is trained on the full training set
with the selected parameters.
We apply the Random Walk-based sampling
method (see Section 3.2.2) for the standard de-
pendency parsing task. However, for the joint
parsing and POS correction on the CATiB dataset
we do not use the Random Walk method because
the first-order features in normal parsing are no
longer first-order when POS tags are also vari-
ables. Therefore, the first-order distribution is not
well-defined and we only employ Gibbs sampling
for simplicity. On the CATiB dataset, we restrict
the sample trees to always be projective as de-
scribed in Section 3.2.1. However, we do not im-
pose this constraint for the CoNLL datasets.
6 Results
Comparison with State-of-the-art Parsers Ta-
ble 2 summarizes the performance of our model
and of the baselines. We first compare our model
to the Turbo parser using the Turbo parser fea-
ture set. This is meant to test how our learning
and inference methods compare to a dual decom-
position approach. The first column in Table 2
4
The MST parser is trained in projective mode for rerank-
ing because generating top-k list from second-order non-
projective model is intractable.
203
Our Model (UAS)
Turbo (UAS)
MST 2nd-Ord.
(UAS)
Best Published UAS
Top-50
Reranker
Top-500
RerankerTurbo Feat. Full Feat.
Arabic 79.86 80.21 79.64 78.75 81.12 (Ma11) 79.03 78.91
Bulgarian 92.97 93.30 93.10 91.56 94.02 (Zh13) 92.81 -
Chinese 92.06 92.63 89.98 91.77 91.89 (Ma10) 92.25 -
Czech 90.62 91.04 90.32 87.30 90.32 (Ma13) 88.14 -
Danish 91.45 91.80 91.48 90.50 92.00 (Zh13) 90.88 90.91
Dutch 85.83 86.47 86.19 84.11 86.19 (Ma13) 81.01 -
English 92.79 92.94 93.22 91.54 93.22 (Ma13) 92.41 -
German 91.79 92.07 92.41 90.14 92.41 (Ma13) 91.19 -
Japanese 93.23 93.42 93.52 92.92 93.72 (Ma11) 93.40 -
Portuguese 91.82 92.41 92.69 91.08 93.03 (Ko10) 91.47 -
Slovene 86.19 86.82 86.01 83.25 86.95 (Ma11) 84.81 85.37
Spanish 88.24 88.21 85.59 84.33 87.96 (Zh13) 86.85 87.21
Swedish 90.48 90.71 91.14 89.05 91.62 (Zh13) 90.53 -
Turkish 76.82 77.21 76.90 74.39 77.55 (Ko10) 76.35 76.23
Average 88.87 89.23 88.72 86.86 89.33 87.92 -
Table 2: Results of our model, the Turbo parser, and the MST parser. ?Best Published UAS? includes the
most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins
et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald
(2012) and Zhang et al (2013). Martins et al (2013) is the current Turbo parser. The last two columns
shows UAS of the discriminative reranker.
shows the result for our model with an average of
88.87%, and the third column shows the results
for the Turbo parser with an average of 88.72%.
This suggests that our learning and inference pro-
cedures are as effective as the dual decomposition
method in the Turbo parser.
Next, we add global features that are not used by
the Turbo parser. The performance of our model
is shown in the second column with an average of
89.23%. It outperforms the Turbo parser by 0.5%
and achieves the best reported performance on
four languages. Moreover, our model also outper-
forms the 88.80% average UAS reported in Mar-
tins et al (2011), which is the top performing sin-
gle parsing system (to the best of our knowledge).
Comparison with Reranking As column 6 of Ta-
ble 2 shows, our model outperforms the reranker
by 1.3%
5
. One possible explanation of this perfor-
mance gap between the reranker and our model is
the small number of candidates considered by the
reranker. To test this hypothesis, we performed
experiments with top-500 list for a subset of lan-
guages.
6
As column 7 shows, this increase in the
list size does not change the relative performance
of the reranker and our model.
Joint Parsing and POS Correction Table 3
shows the results of joint parsing and POS cor-
rection on the CATiB dataset, for our model and
5
Note that the comparison is conservative because we
can also add MST scores as features in our model as in
reranker. With these features our model achieves an average
UAS 89.28%.
6
We ran this experiment on 5 languages with small
datasets due to the scalability issues associated with rerank-
ing top-500 list.
state-of-the-art systems. As the upper part of the
table shows, the parser with corrected tags reaches
88.38% compared to the accuracy of 88.46% on
the gold tags. This is a substantial increase from
the parser that uses predicted tags (86.95%).
To put these numbers into perspective, the bot-
tom part of Table 3 shows the accuracy of the best
systems from the 2013 SPMRL shared task on
Arabic parsing using predicted information (Sed-
dah et al, 2013). Our system not only out-
performs the best single system (Bj?orkelund et
al., 2013) by 1.4%, but it also tops the ensem-
ble system that combines three powerful parsers:
the Mate parser (Bohnet, 2010), the Easy-First
parser (Goldberg and Elhadad, 2010) and the
Turbo parser (Martins et al, 2013)
Impact of Sampling Methods We compare two
sampling methods introduced in Section 3.2 with
respect to their decoding efficiency. Specifically,
we measure the score of the retrieved trees in test-
ing as a function of the decoding speed, measured
by the number of tokens per second. We change
the temperature update rate c in order to decode
with different speed. In Figure 7 we show the cor-
responding curves for two languages: Arabic and
Chinese. We select these two languages as they
correspond to two extremes in sentence length:
Arabic has the longest sentences on average, while
Chinese has the shortest ones. For both languages,
the tree score improves over time. Given sufficient
time, both sampling methods achieve the same
score. However, the Random Walk-based sam-
pler performs better when the quality is traded for
speed. This result is to be expected given that each
204
Dev. Set (? 70) Testing Set
POS Acc. UAS POS Acc. UAS
Gold - 90.27 - 88.46
Predicted 96.87 88.81 96.82 86.95
POS Correction 97.72 90.08 97.49 88.38
CADIM 96.87 87.4- 96.82 85.78
IMS-Single - - - 86.96
IMS-Ensemble - - - 88.32
Table 3: Results for parsing and corrective tagging
on the CATiB dataset. The upper part shows UAS
of our model with gold/predicted information or
POS correction. Bottom part shows UAS of the
best systems in the SPMRL shared task. IMS-
Single (Bj?orkelund et al, 2013) is the best single
parsing system, while IMS-Ensemble (Bj?orkelund
et al, 2013) is the best ensemble parsing system.
We also show results for CADIM (Marton et al,
2013), the second best system, because we use
their predicted features.
0 20 40 60 80 1002.648
2.65
2.652
2.654
2.656
2.658 x 104
Toks/sec
Score
 
 
GibbsRandom Walk
(a) Arabic
0 100 200 300 400 500 600 700 8001.897
1.898
1.899
1.9 x 10
4
Toks/sec
Score
 
 
GibbsRandom Walk
(b) Chinese
Figure 7: Total score of the predicted test trees as
a function of the decoding speed, measured in the
number of tokens per second.
iteration of this sampler makes multiple changes
to the tree, in contrast to a single-edge change of
Gibbs sampler.
The Effect of Constraints in Learning Our train-
ing method updates parameters to satisfy the pair-
wise constraints between (1) subsequent samples
on the sampling path and (2) selected samples and
the ground truth. Figure 8 shows that applying
both types of constraints is consistently better than
using either of them alone. Moreover, these re-
sults demonstrate that comparison between subse-
quent samples is more important than comparison
against the gold tree.
Decoding Speed Our sampling-based parser is an
Danish Japanese Portuguese Swedish89
90
91
92
93
94
UAS
(%)
 
 BothNeighborGold
Figure 8: UAS on four languages when train-
ing with different constraints. ?Neighbor? corre-
sponds to pairwise constraints between subsequent
samples, ?Gold? represents constraints between a
single sample and the ground truth, ?Both? means
applying both types of constraints.
anytime algorithm, and therefore its running time
can be traded for performance. Figure 7 illustrates
this trade-off. In the experiments reported above,
we chose a conservative cooling rate and contin-
ued to sample until the score no longer changed.
The parser still managed to process all the datasets
in a reasonable time. For example, the time that it
took to decode all the test sentences in Chinese and
Arabic were 3min and 15min, respectively. Our
current implementation is in Java and can be fur-
ther optimized for speed.
7 Conclusions
This paper demonstrates the power of combining a
simple inference procedure with a highly expres-
sive scoring function. Our model achieves the best
results on the standard dependency parsing bench-
mark, outperforming parsing methods with elabo-
rate inference procedures. In addition, this frame-
work provides simple and effective means for joint
parsing and corrective tagging.
Acknowledgments
This research is developed in collaboration with
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the IYAS project. The authors acknowledge
the support of the MURI program (W911NF-10-
1-0533, the DARPA BOLT program and the US-
Israel Binational Science Foundation (BSF, Grant
No 2012330). We thank the MIT NLP group and
the ACL reviewers for their comments.
205
References
Christophe Andrieu, Nando De Freitas, Arnaud
Doucet, and Michael I Jordan. 2003. An introduc-
tion to mcmc for machine learning. Machine learn-
ing, 50(1-2):5?43.
Anders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the SPMRL 2013 shared task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 135?
145, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In COLING,
pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, ICML ?00, pages 175?182.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
The Journal of Machine Learning Research, 3:951?
991.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
learning, 75(3):297?325.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. Pattern Analysis andMachine
Intelligence, IEEE Transactions on, (6):721?741.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750. Association for Computa-
tional Linguistics.
Barry Haddow, Abhishek Arun, and Philipp Koehn.
2011. Samplerank training for phrase-based ma-
chine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 261?
271. Association for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1?11. Association for
Computational Linguistics.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1288?1298. Association for Compu-
tational Linguistics.
Quannan Li, Jingdong Wang, Zhuowen Tu, and
David P Wipf. 2013. Fixed-point model for struc-
tured labeling. In Proceedings of the 30th Interna-
tional Conference on Machine Learning (ICML-13),
pages 214?221.
Andr?e FT Martins, Noah A Smith, and Eric P Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 342?350. Association for
Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M?ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approxi-
mate variational inference. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 34?44. Association for
Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Pedro MQ Aguiar,
and M?ario AT Figueiredo. 2011. Dual decompo-
sition with many overlapping components. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 238?249. As-
sociation for Computational Linguistics.
Andr?e FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Yuval Marton, Nizar Habash, Owen Rambow, and
Sarah Alkhulani. 2013. Spmrl13 shared task sys-
tem: The cadim arabic dependency parser. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 76?
80.
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL.
206
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 523?530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, pages 216?220. Association for
Computational Linguistics.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In EMNLP-CoNLL,
pages 952?956.
Joakim Nivre, Johan Hall, Jens Nilsson, G?uls?en Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 221?225. Association for Computational Lin-
guistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1346. Association for Computa-
tional Linguistics.
Alexander M Rush and Slav Petrov. 2012. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 498?507. Association for Computational Lin-
guistics.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D Choi, Rich?ard Farkas, Jennifer
Foster, Iakes Goenaga, Koldo Gojenola Gallete-
beitia, Yoav Goldberg, et al 2013. Overview of the
spmrl 2013 shared task: A cross-framework evalua-
tion of parsing morphologically rich languages. In
Proceedings of the Fourth Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
146?182.
D. Sontag, A. Globerson, and T. Jaakkola. 2011. In-
troduction to dual decomposition for inference. In
Optimization for Machine Learning, pages 219?254.
MIT Press.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 159?177. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Michael L. Wick, Khashayar Rohanimanesh, Kedar
Bellare, Aron Culotta, and Andrew McCallum.
2011. Samplerank: Training factor graphs with
atomic gradients. In Lise Getoor and Tobias Schef-
fer, editors, Proceedings of the 28th International
Conference on Machine Learning, ICML 2011,
pages 777?784.
David Bruce Wilson. 1996. Generating random span-
ning trees more quickly than the cover time. In
Proceedings of the twenty-eighth annual ACM sym-
posium on Theory of computing, pages 296?303.
ACM.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 320?331. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188?193. Association for Computational Linguis-
tics.
Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-
ald. 2013. Online learning for inexact hypergraph
search. In Proceedings of EMNLP.
207
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1381?1391,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Low-Rank Tensors for Scoring Dependency Structures
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{taolei, yuxin, yuanzh, regina, tommi}@csail.mit.edu
Abstract
Accurate scoring of syntactic structures
such as head-modifier arcs in dependency
parsing typically requires rich, high-
dimensional feature representations. A
small subset of such features is often se-
lected manually. This is problematic when
features lack clear linguistic meaning as
in embeddings or when the information is
blended across features. In this paper, we
use tensors to map high-dimensional fea-
ture vectors into low dimensional repre-
sentations. We explicitly maintain the pa-
rameters as a low-rank tensor to obtain low
dimensional representations of words in
their syntactic roles, and to leverage mod-
ularity in the tensor for easy training with
online algorithms. Our parser consistently
outperforms the Turbo and MST parsers
across 14 different languages. We also ob-
tain the best published UAS results on 5
languages.
1
1 Introduction
Finding an expressive representation of input sen-
tences is crucial for accurate parsing. Syntac-
tic relations manifest themselves in a broad range
of surface indicators, ranging from morphological
to lexical, including positional and part-of-speech
(POS) tagging features. Traditionally, parsing re-
search has focused on modeling the direct connec-
tion between the features and the predicted syntac-
tic relations such as head-modifier (arc) relations
in dependency parsing. Even in the case of first-
order parsers, this results in a high-dimensional
vector representation of each arc. Discrete fea-
tures, and their cross products, can be further com-
plemented with auxiliary information about words
1
Our code is available at https://github.com/
taolei87/RBGParser.
participating in an arc, such as continuous vector
representations of words. The exploding dimen-
sionality of rich feature vectors must then be bal-
anced with the difficulty of effectively learning the
associated parameters from limited training data.
A predominant way to counter the high dimen-
sionality of features is to manually design or select
a meaningful set of feature templates, which are
used to generate different types of features (Mc-
Donald et al, 2005a; Koo and Collins, 2010; Mar-
tins et al, 2013). Direct manual selection may be
problematic for two reasons. First, features may
lack clear linguistic interpretation as in distribu-
tional features or continuous vector embeddings of
words. Second, designing a small subset of tem-
plates (and features) is challenging when the rel-
evant linguistic information is distributed across
the features. For instance, morphological proper-
ties are closely tied to part-of-speech tags, which
in turn relate to positional features. These features
are not redundant. Therefore, we may suffer a per-
formance loss if we select only a small subset of
the features. On the other hand, by including all
the rich features, we face over-fitting problems.
We depart from this view and leverage high-
dimensional feature vectors by mapping them into
low dimensional representations. We begin by
representing high-dimensional feature vectors as
multi-way cross-products of smaller feature vec-
tors that represent words and their syntactic rela-
tions (arcs). The associated parameters are viewed
as a tensor (multi-way array) of low rank, and opti-
mized for parsing performance. By explicitly rep-
resenting the tensor in a low-rank form, we have
direct control over the effective dimensionality of
the set of parameters. We obtain role-dependent
low-dimensional representations for words (head,
modifier) that are specifically tailored for parsing
accuracy, and use standard online algorithms for
optimizing the low-rank tensor components.
The overall approach has clear linguistic and
1381
computational advantages:
? Our low dimensional embeddings are tailored
to the syntactic context of words (head, modi-
fier). This low dimensional syntactic abstrac-
tion can be thought of as a proxy to manually
constructed POS tags.
? By automatically selecting a small number of
dimensions useful for parsing, we can lever-
age a wide array of (correlated) features. Un-
like parsers such as MST, we can easily bene-
fit from auxiliary information (e.g., word vec-
tors) appended as features.
We implement the low-rank factorization model
in the context of first- and third-order depen-
dency parsing. The model was evaluated on 14
languages, using dependency data from CoNLL
2008 and CoNLL 2006. We compare our results
against the MST (McDonald et al, 2005a) and
Turbo (Martins et al, 2013) parsers. The low-rank
parser achieves average performance of 89.08%
across 14 languages, compared to 88.73% for the
Turbo parser, and 87.19% for MST. The power of
the low-rank model becomes evident in the ab-
sence of any part-of-speech tags. For instance,
on the English dataset, the low-rank model trained
without POS tags achieves 90.49% on first-order
parsing, while the baseline gets 86.70% if trained
under the same conditions, and 90.58% if trained
with 12 core POS tags. Finally, we demonstrate
that the model can successfully leverage word vec-
tor representations, in contrast to the baselines.
2 Related Work
Selecting Features for Dependency Parsing A
great deal of parsing research has been dedicated
to feature engineering (Lazaridou et al, 2013;
Marton et al, 2010; Marton et al, 2011). While
in most state-of-the-art parsers, features are se-
lected manually (McDonald et al, 2005a; McDon-
ald et al, 2005b; Koo and Collins, 2010; Mar-
tins et al, 2013; Zhang and McDonald, 2012a;
Rush and Petrov, 2012a), automatic feature selec-
tion methods are gaining popularity (Martins et al,
2011b; Ballesteros and Nivre, 2012; Nilsson and
Nugues, 2010; Ballesteros, 2013). Following stan-
dard machine learning practices, these algorithms
iteratively select a subset of features by optimizing
parsing performance on a development set. These
feature selection methods are particularly promis-
ing in parsing scenarios where the optimal feature
set is likely to be a small subset of the original set
of candidate features. Our technique, in contrast,
is suitable for cases where the relevant information
is distributed across a larger set of related features.
Embedding for Dependency Parsing A lot of
recent work has been done on mapping words into
vector spaces (Collobert and Weston, 2008; Turian
et al, 2010; Dhillon et al, 2011; Mikolov et al,
2013). Traditionally, these vector representations
have been derived primarily from co-occurrences
of words within sentences, ignoring syntactic roles
of the co-occurring words. Nevertheless, any such
word-level representation can be used to offset in-
herent sparsity problems associated with full lexi-
calization (Cirik and S?ensoy, 2013). In this sense
they perform a role similar to POS tags.
Word-level vector space embeddings have so
far had limited impact on parsing performance.
From a computational perspective, adding non-
sparse vectors directly as features, including their
combinations, can significantly increase the num-
ber of active features for scoring syntactic struc-
tures (e.g., dependency arc). Because of this is-
sue, Cirik and S?ensoy (2013) used word vectors
only as unigram features (without combinations)
as part of a shift reduce parser (Nivre et al, 2007).
The improvement on the overall parsing perfor-
mance was marginal. Another application of word
vectors is compositional vector grammar (Socher
et al, 2013). While this method learns to map
word combinations into vectors, it builds on ex-
isting word-level vector representations. In con-
trast, we represent words as vectors in a manner
that is directly optimized for parsing. This frame-
work enables us to learn new syntactically guided
embeddings while also leveraging separately esti-
mated word vectors as starting features, leading to
improved parsing performance.
Dimensionality Reduction Many machine
learning problems can be cast as matrix problems
where the matrix represents a set of co-varying
parameters. Such problems include, for example,
multi-task learning and collaborative filtering.
Rather than assuming that each parameter can be
set independently of others, it is helpful to assume
that the parameters vary in a low dimensional
subspace that has to be estimated together with the
parameters. In terms of the parameter matrix, this
corresponds to a low-rank assumption. Low-rank
constraints are commonly used for improving
1382
generalization (Lee and Seung, 1999; Srebro
et al, 2003; Srebro et al, 2004; Evgeniou and
Pontil, 2007)
A strict low-rank assumption can be restrictive.
Indeed, recent approaches to matrix problems de-
compose the parameter matrix as a sum of low-
rank and sparse matrices (Tao and Yuan, 2011;
Zhou and Tao, 2011). The sparse matrix is used to
highlight a small number of parameters that should
vary independently even if most of them lie on
a low-dimensional subspace (Waters et al, 2011;
Chandrasekaran et al, 2011). We follow this de-
composition while extending the parameter matrix
into a tensor.
Tensors are multi-way generalizations of ma-
trices and possess an analogous notion of rank.
Tensors are increasingly used as tools in spec-
tral estimation (Hsu and Kakade, 2013), includ-
ing in parsing (Cohen et al, 2012) and other NLP
problems (de Cruys et al, 2013), where the goal
is to avoid local optima in maximum likelihood
estimation. In contrast, we expand features for
parsing into a multi-way tensor, and operate with
an explicit low-rank representation of the associ-
ated parameter tensor. The explicit representa-
tion sidesteps inherent complexity problems asso-
ciated with the tensor rank (Hillar and Lim, 2009).
Our parameters are divided into a sparse set corre-
sponding to manually chosen MST or Turbo parser
features and a larger set governed by a low-rank
tensor.
3 Problem Formulation
We will commence here by casting first-order de-
pendency parsing as a tensor estimation problem.
We will start by introducing the notation used in
the paper, followed by a more formal description
of our dependency parsing task.
3.1 Basic Notations
Let A ? R
n?n?d
be a 3-dimensional tensor (a 3-
way array). We denote each element of the tensor
as A
i,j,k
where i ? [n], j ? [n], k ? [d] and [n]
is a shorthand for the set of integers {1, 2, ? ? ? , n}.
Similarly, we use M
i,j
and u
i
to represent the ele-
ments of matrix M and vector u, respectively.
We define the inner product of two tensors (or
matrices) as ?A,B? = vec(A)
T
vec(B), where
vec(?) concatenates the tensor (or matrix) ele-
ments into a column vector. The squared norm
of a tensor/matrix is denoted by ?A?
2
= ?A,A?.
The Kronecker product of three vectors is de-
noted by u?v?w and forms a rank-1 tensor such
that
(u? v ? w)
i,j,k
= u
i
v
j
w
k
.
Note that the vectors u, v, and w may be column
or row vectors. Their orientation is defined based
on usage. For example, u ? v is a rank-1 matrix
uv
T
when u and v are column vectors (u
T
v if they
are row vectors).
We say that tensor A is in Kruskal form if
A =
r
?
i=1
U(i, :)? V (i, :)?W (i, :) (1)
where U, V ? R
r?n
, W ? R
r?d
and U(i, :) is the
i
th
row of matrix U . We will directly learn a low-
rank tensor A (because r is small) in this form as
one of our model parameters.
3.2 Dependency Parsing
Let x be a sentence and Y(x) the set of possible
dependency trees over the words in x. We assume
that the score S(x, y) of each candidate depen-
dency tree y ? Y(x) decomposes into a sum of
?local? scores for arcs. Specifically:
S(x, y) =
?
h?m ? y
s(h? m) ?y ? Y(x)
where h ? m is the head-modifier dependency
arc in the tree y. Each y is understood as a col-
lection of arcs h ? m where h and m index
words in x.
2
For example, x(h) is the word cor-
responding to h. We suppress the dependence on
x whenever it is clear from context. For exam-
ple, s(h ? m) can depend on x in complicated
ways as discussed below. The predicted parse is
obtained as y? = arg max
y?Y(x)
S(x, y).
A key problem is how we parameterize the
arc scores s(h ? m). Following the MST
parser (McDonald et al, 2005a) we can define
rich features characterizing each head-modifier
arc, compiled into a sparse binary vector ?
h?m
?
R
L
that depends on the sentence x as well as the
chosen arc h? m (again, we suppress the depen-
dence on x). Based on this feature representation,
we define the score of each arc as s
?
(h ? m) =
2
Note that in the case of high-order parsing, the sum
S(x, y) may also include local scores for other syntactic
structures, such as grandhead-head-modifier score s(g ?
h ? m). See (Martins et al, 2013) for a complete list of
these structures.
1383
Unigram features:
form form-p form-n
lemma lemma-p lemma-n
pos pos-p pos-n
morph bias
Bigram features:
pos-p, pos
pos, pos-n
pos, lemma
morph, lemma
Trigram features:
pos-p, pos, pos-n
Table 1: Word feature templates used by our
model. pos, form, lemma and morph stand for
the fine POS tag, word form, word lemma and the
morphology feature (provided in CoNLL format
file) of the current word. There is a bias term that
is always active for any word. The suffixes -p and
-n refer to the left and right of the current word re-
spectively. For example, pos-p means the POS tag
to the left of the current word in the sentence.
??, ?
h?m
? where ? ? R
L
represent adjustable pa-
rameters to be learned, and L is the number of pa-
rameters (and possible features in ?
h?m
).
We can alternatively specify arc features in
terms of rank-1 tensors by taking the Kronecker
product of simpler feature vectors associated with
the head (vector ?
h
? R
n
), and modifier (vector
?
m
? R
n
), as well as the arc itself (vector ?
h,m
?
R
d
). Here ?
h,m
is much lower dimensional than
the MST arc feature vector ?
h?m
discussed ear-
lier. For example, ?
h,m
may be composed of only
indicators for binned arc lengths
3
. ?
h
and ?
m
, on
the other hand, are built from features shown in
Table 1. By taking the cross-product of all these
component feature vectors, we obtain the full fea-
ture representation for arc h? m as a rank-1 ten-
sor
?
h
? ?
m
? ?
h,m
? R
n?n?d
Note that elements of this rank-1 tensor include
feature combinations that are not part of the fea-
ture crossings in ?
h?m
. In this sense, the rank-1
tensor represents a substantial feature expansion.
The arc score s
tensor
(h? m) associated with the
3
In our current version, ?
h,m
only contains the binned
arc length. Other possible features include, for example, the
label of the arc h ? m, the POS tags between the head and
the modifier, boolean flags which indicate the occurence of
in-between punctutations or conjunctions, etc.
tensor representation is defined analogously as
s
tensor
(h? m) = ?A, ?
h
? ?
m
? ?
h,m
?
where the adjustable parametersA also form a ten-
sor. Given the typical dimensions of the compo-
nent feature vectors, ?
h
, ?
m
, ?
h,m
, it is not even
possible to store all the parameters in A. Indeed,
in the full English training set of CoNLL-2008, the
tensor involves around 8 ? 10
11
entries while the
MST feature vector has approximately 1.5 ? 10
7
features. To counter this feature explosion, we re-
strict the parameters A to have low rank.
Low-Rank Dependency Scoring We can repre-
sent a rank-r tensor A explicitly in terms of pa-
rameter matrices U , V , and W as shown in Eq. 1.
As a result, the arc score for the tensor reduces to
evaluating U?
h
, V ?
m
, and W?
h,m
which are all
r dimensional vectors and can be computed effi-
ciently based on any sparse vectors ?
h
, ?
m
, and
?
h,m
. The resulting arc score s
tensor
(h ? m) is
then
r
?
i=1
[U?
h
]
i
[V ?
m
]
i
[W?
h,m
]
i
(2)
By learning parameters U , V , andW that function
well in dependency parsing, we also learn context-
dependent embeddings for words and arcs. Specif-
ically,U?
h
(for a given sentence, suppressed) is an
r dimensional vector representation of the word
corresponding to h as a head word. Similarly,
V ?
m
provides an analogous representation for a
modifier m. Finally, W?
h,m
is a vector embed-
ding of the supplemental arc-dependent informa-
tion. The resulting embedding is therefore tied
to the syntactic roles of the words (and arcs), and
learned in order to perform well in parsing.
We expect a dependency parsing model to ben-
efit from several aspects of the low-rank tensor
scoring. For example, we can easily incorpo-
rate additional useful features in the feature vec-
tors ?
h
, ?
m
and ?
h,m
, since the low-rank assump-
tion (for small enough r) effectively counters the
otherwise uncontrolled feature expansion. More-
over, by controlling the amount of information
we can extract from each of the component fea-
ture vectors (via rank r), the statistical estimation
problem does not scale dramatically with the di-
mensions of ?
h
, ?
m
and ?
h,m
. In particular, the
low-rank constraint can help generalize to unseen
arcs. Consider a feature ?(x(h) = a) ? ?(x(m) =
1384
b) ? ?(dis(x, h,m) = c) which is non-zero only
for an arc a ? b with distance c in sentence x.
If the arc has not been seen in the available train-
ing data, it does not contribute to the traditional
arc score s
?
(?). In contrast, with the low-rank con-
straint, the arc score in Eq. 2 would typically be
non-zero.
Combined Scoring Our parsing model aims to
combine the strengths of both traditional features
from the MST/Turbo parser as well as the new
low-rank tensor features. In this way, our model
is able to capture a wide range of information in-
cluding the auxiliary features without having un-
controlled feature explosion, while still having the
full accessibility to the manually engineered fea-
tures that are proven useful. Specifically, we de-
fine the arc score s
?
(h? m) as the combination
(1? ?)s
tensor
(h? m) + ?s
?
(h? m)
= (1? ?)
r
?
i=1
[U?
h
]
i
[V ?
m
]
i
[W?
h,m
]
i
+ ? ??, ?
h?m
? (3)
where ? ? R
L
, U ? R
r?n
, V ? R
r?n
, and W ?
R
r?d
are the model parameters to be learned. The
rank r and ? ? [0, 1] (balancing the two scores)
represent hyper-parameters in our model.
4 Learning
The training set D = {(x?
i
, y?
i
)}
N
i=1
consists of N
pairs, where each pair consists of a sentence x
i
and the corresponding gold (target) parse y
i
. The
goal is to learn values for the parameters ?, U , V
and W that optimize the combined scoring func-
tion S
?
(x, y) =
?
h?m?y
s
?
(h ? m), defined
in Eq. 3, for parsing performance. We adopt a
maximum soft-margin framework for this learning
problem. Specifically, we find parameters ?, U , V ,
W , and {?
i
} that minimize
C
?
i
?
i
+ ???
2
+ ?U?
2
+ ?V ?
2
+ ?W?
2
s.t. S
?
(x?
i
, y?
i
) ? S
?
(x?
i
, y
i
) + ?y?
i
? y
i
?
1
? ?
i
?y
i
? Y(x?
i
), ?i. (4)
where ?y?
i
?y
i
?
1
is the number of mismatched arcs
between the two trees, and ?
i
is a non-negative
slack variable. The constraints serve to separate
the gold tree from other alternatives in Y(x?
i
) with
a margin that increases with distance.
The objective as stated is not jointly convex
with respect to U , V and W due to our explicit
representation of the low-rank tensor. However, if
we fix any two sets of parameters, for example, if
we fix V andW , then the combined score S
?
(x, y)
will be a linear function of both ? and U . As a re-
sult, the objective will be jointly convex with re-
spect to ? and U and could be optimized using
standard tools. However, to accelerate learning,
we adopt an online learning setup. Specifically,
we use the passive-aggressive learning algorithm
(Crammer et al, 2006) tailored to our setting, up-
dating pairs of parameter sets, (?, U), (?, V ) and
(?,W ) in an alternating manner. This method is
described below.
Online Learning In an online learning setup,
we update parameters successively based on each
sentence. In order to apply the passive-aggressive
algorithm, we fix two of U , V and W (say, for ex-
ample, V and W ) in an alternating manner, and
apply a closed-form update to the remaining pa-
rameters (here U and ?). This is possible since
the objective function with respect to (?, U) has a
similar form as in the original passive-aggressive
algorithm. To illustrate this, consider a training
sentence x
i
. The update involves finding first the
best competing tree,
y?
i
= arg max
y
i
?Y(x?
i
)
S
?
(x?
i
, y
i
) + ?y?
i
? y
i
?
1
(5)
which is the tree that violates the constraint in
Eq. 4 most (i.e. maximizes the loss ?
i
). We then
obtain parameter increments ?? and ?U by solv-
ing
min
??, ?U, ??0
1
2
????
2
+
1
2
??U?
2
+ C?
s.t. S
?
(x?
i
, y?
i
) ? S
?
(x?
i
, y?
i
) + ?y?
i
? y?
i
?
1
? ?
In this way, the optimization problem attempts to
keep the parameter change as small as possible,
while forcing it to achieve mostly zero loss on this
single instance. This problem has a closed form
solution
?? = min
{
C,
loss
?
2
?d??
2
+ (1? ?)
2
?du?
2
}
?d?
?U = min
{
C,
loss
?
2
?d??
2
+ (1? ?)
2
?du?
2
}
(1? ?)du
1385
where
loss = S
?
(x?
i
, y?
i
) + ?y?
i
? y?
i
?
1
? S
?
(x?
i
, y?
i
)
d? =
?
h?m ? y?
i
?
h?m
?
?
h?m ? y?
i
?
h?m
du =
?
h?m ? y?
i
[(V ?
m
) (W?
h,m
)]? ?
h
?
?
h?m ? y?
i
[(V ?
m
) (W?
h,m
)]? ?
h
where (u v)
i
= u
i
v
i
is the Hadamard (element-
wise) product. The magnitude of change of ? and
U is controlled by the parameterC. By varyingC,
we can determine an appropriate step size for the
online updates. The updates also illustrate how ?
balances the effect of the MST component of the
score relative to the low-rank tensor score. When
? = 0, the arc scores are entirely based on the low-
rank tensor and ?? = 0. Note that ?
h
, ?
m
, ?
h,m
,
and ?
h?m
are typically very sparse for each word
or arc. Therefore du and d? are also sparse and
can be computed efficiently.
Initialization The alternating online algorithm
relies on how we initializeU , V , andW since each
update is carried out in the context of the other
two. A random initialization of these parameters is
unlikely to work well, both due to the dimensions
involved, and the nature of the alternating updates.
We consider here instead a reasonable determinis-
tic ?guess? as the initialization method.
We begin by training our model without any
low-rank parameters, and obtain parameters ?.
The majority of features in this MST component
can be expressed as elements of the feature ten-
sor, i.e., as [?
h
? ?
m
? ?
h,m
]
i,j,k
. We can there-
fore create a tensor representation of ? such that
B
i,j,k
equals the corresponding parameter value
in ?. We use a low-rank version of B as the ini-
tialization. Specifically, we unfold the tensor B
into a matrix B
(h)
of dimensions n and nd, where
n = dim(?
h
) = dim(?
m
) and d = dim(?
h,m
).
For instance, a rank-1 tensor can be unfolded as
u ? v ? w = u ? vec(v ? w). We compute the
top-r SVD of the resulting unfolded matrix such
that B
(h)
= P
T
SQ. U is initialized as P . Each
right singular vector S
i
Q(i, :) is also a matrix in
R
n?d
. The leading left and right singular vectors
of this matrix are assigned to V (i, :) and W (i, :)
respectively. In our implementation, we run one
epoch of our model without low-rank parameters
and initialize the tensor A.
Parameter Averaging The passive-aggressive
algorithm regularizes the increments (e.g. ?? and
?U ) during each update but does not include any
overall regularization. In other words, keeping up-
dating the model may lead to large parameter val-
ues and over-fitting. To counter this effect, we use
parameter averaging as used in the MST and Turbo
parsers. The final parameters are those averaged
across all the iterations (cf. (Collins, 2002)). For
simplicity, in our algorithm we average U , V , W
and ? separately, which works well empirically.
5 Experimental Setup
Datasets We test our dependency model on 14
languages, including the English dataset from
CoNLL 2008 shared tasks and all 13 datasets from
CoNLL 2006 shared tasks (Buchholz and Marsi,
2006; Surdeanu et al, 2008). These datasets in-
clude manually annotated dependency trees, POS
tags and morphological information. Following
standard practices, we encode this information as
features.
Methods We compare our model to MST and
Turbo parsers on non-projective dependency pars-
ing. For our parser, we train both a first-order
parsing model (as described in Section 3 and 4)
as well as a third-order model. The third order
parser simply adds high-order features, those typ-
ically used in MST and Turbo parsers, into our
s
?
(x, y) = ??, ?(x, y)? scoring component. The
decoding algorithm for the third-order parsing is
based on (Zhang et al, 2014). For the Turbo
parser, we directly compare with the recent pub-
lished results in (Martins et al, 2013). For the
MST parser, we train and test using the most re-
cent version of the code.
4
In addition, we im-
plemented two additional baselines, NT-1st (first
order) and NT-3rd (third order), corresponding to
our model without the tensor component.
Features For the arc feature vector ?
h?m
, we
use the same set of feature templates as MST
v0.5.1. For head/modifier vector ?
h
and ?
m
, we
show the complete set of feature templates used
by our model in Table 1. Finally, we use a similar
set of feature templates as Turbo v2.1 for 3rd order
parsing.
To add auxiliary word vector representations,
we use the publicly available word vectors (Cirik
4
http://sourceforge.net/projects/mstparser/
1386
First-order only High-order
Ours NT-1st MST Turbo Ours-3rd NT-3rd MST-2nd Turbo-3rd Best Published
Arabic 79.60 78.71 78.3 77.23 79.95 79.53 78.75 79.64 81.12 (Ma11)
Bulgarian 92.30 91.14 90.98 91.76 93.50 92.79 91.56 93.1 94.02 (Zh13)
Chinese 91.43 90.85 90.40 88.49 92.68 92.39 91.77 89.98 91.89 (Ma10)
Czech 87.90 86.62 86.18 87.66 90.50 89.43 87.3 90.32 90.32 (Ma13)
Danish 90.64 89.80 89.84 89.42 91.39 90.82 90.5 91.48 92.00 (Zh13)
Dutch 84.81 83.77 82.89 83.61 86.41 86.08 84.11 86.19 86.19 (Ma13)
English 91.84 91.40 90.59 91.21 93.02 92.82 91.54 93.22 93.22 (Ma13)
German 90.24 89.70 89.54 90.52 91.97 92.26 90.14 92.41 92.41 (Ma13)
Japanese 93.74 93.36 93.38 92.78 93.71 93.23 92.92 93.52 93.72 (Ma11)
Portuguese 90.94 90.67 89.92 91.14 91.92 91.63 91.08 92.69 93.03 (Ko10)
Slovene 84.25 83.15 82.09 82.81 86.24 86.07 83.25 86.01 86.95 (Ma11)
Spanish 85.27 84.95 83.79 83.61 88.00 87.47 84.33 85.59 87.96 (Zh13)
Swedish 89.86 89.66 88.27 89.36 91.00 90.83 89.05 91.14 91.62 (Zh13)
Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10)
Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43
Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the
English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyper-
parameter ? = 0.3. To remove the tensor in our model, we ran experiments with ? = 1, corresponding
to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et
al. (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011a), Martins et al (2013),
Koo et al (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al (2013).
and S?ensoy, 2013), learned from raw data (Glober-
son et al, 2007; Maron et al, 2010). Three
languages in our dataset ? English, German and
Swedish ? have corresponding word vectors in this
collection.
5
The dimensionality of this representa-
tion varies by language: English has 50 dimen-
sional word vectors, while German and Swedish
have 25 dimensional word vectors. Each entry of
the word vector is added as a feature value into
feature vectors ?
h
and ?
m
. For each word in the
sentence, we add its own word vector as well as
the vectors of its left and right words.
We should note that since our model parameter
A is represented and learned in the low-rank form,
we only have to store and maintain the low-rank
projections U?
h
, V ?
m
andW?
h,m
rather than ex-
plicitly calculate the feature tensor ?
h
??
m
??
h,m
.
Therefore updating parameters and decoding a
sentence is still efficient, i.e., linear in the num-
ber of values of the feature vector. In contrast,
assume we take the cross-product of the auxiliary
word vector values, POS tags and lexical items of
a word and its context, and add the crossed val-
ues into a normal model (in ?
h?m
). The number
of features for each arc would be at least quadratic,
growing into thousands, and would be a significant
impediment to parsing efficiency.
Evaluation Following standard practices, we
train our full model and the baselines for 10
5
https://github.com/wolet/sprml13-word-embeddings
epochs. As the evaluation measure, we use un-
labeled attachment scores (UAS) excluding punc-
tuation. In all the reported experiments, the hyper-
parameters are set as follows: r = 50 (rank of the
tensor), C = 1 for first-order model and C = 0.01
for third-order model.
6 Results
Overall Performance Table 2 shows the per-
formance of our model and the baselines on 14
CoNLL datasets. Our model outperforms Turbo
parser, MST parser, as well as its own variants
without the tensor component. The improvements
of our low-rank model are consistent across lan-
guages: results for the first order parser are better
on 11 out of 14 languages. By comparing NT-1st
and NT-3rd (models without low-rank) with our
full model (with low-rank), we obtain 0.7% abso-
lute improvement on first-order parsing, and 0.3%
improvement on third-order parsing. Our model
also achieves the best UAS on 5 languages.
We next focus on the first-order model and
gauge the impact of the tensor component. First,
we test our model by varying the hyper-parameter
? which balances the tensor score and the tradi-
tional MST/Turbo score components. Figure 1
shows the average UAS on CoNLL test datasets
after each training epoch. We can see that the im-
provement of adding the low-rank tensor is con-
sistent across various choices of hyper parame-
1387
2 4 6 8 1084.0%
84.5%
85.0%
85.5%
86.0%
86.5%
87.0%
87.5%
88.0%
# Epochs 
 
?=0.0?=0.2?=0.3?=0.4NT?1st
Figure 1: Average UAS on CoNLL testsets af-
ter different epochs. Our full model consistently
performs better than NT-1st (its variation without
tensor component) under different choices of the
hyper-parameter ?.
no word vector with word vector
English 91.84 92.07
German 90.24 90.48
Swedish 89.86 90.38
Table 3: Results of adding unsupervised word vec-
tors to the tensor. Adding this information yields
consistent improvement for all languages.
ter ?. When training with the tensor component
alone (? = 0), the model converges more slowly.
Learning of the tensor is harder because the scor-
ing function is not linear (nor convex) with respect
to parameters U , V and W . However, the tensor
scoring component achieves better generalization
on the test data, resulting in better UAS than NT-
1st after 8 training epochs.
To assess the ability of our model to incorpo-
rate a range of features, we add unsupervised word
vectors to our model. As described in previous
section, we do so by appending the values of dif-
ferent coordinates in the word vector into ?
h
and
?
m
. As Table 3 shows, adding this information in-
creases the parsing performance for all the three
languages. For instance, we obtain more than
0.5% absolute improvement on Swedish.
Syntactic Abstraction without POS Since our
model learns a compressed representation of fea-
ture vectors, we are interested to measure its per-
formance when part-of-speech tags are not pro-
vided (See Table 4). The rationale is that given all
other features, the model would induce representa-
tions that play a similar role to POS tags. Note that
Our model NT-1st
-POS +wv. -POS +POS
English 88.89 90.49 86.70 90.58
German 82.63 85.80 78.71 88.50
Swedish 81.84 85.90 79.65 88.75
Table 4: The first three columns show parsing re-
sults when models are trained without POS tags.
The last column gives the upper-bound, i.e. the
performance of a parser trained with 12 Core POS
tags. The low-rank model outperforms NT-1st by
a large margin. Adding word vector features fur-
ther improves performance.
the performance of traditional parsers drops when
tags are not provided. For example, the perfor-
mance gap is 10% on German. Our experiments
show that low-rank parser operates effectively in
the absence of tags. In fact, it nearly reaches the
performance of the original parser that used the
tags on English.
Examples of Derived Projections We manu-
ally analyze low-dimensional projections to assess
whether they capture syntactic abstraction. For
this purpose, we train a model with only a ten-
sor component (such that it has to learn an accu-
rate tensor) on the English dataset and obtain low
dimensional embeddings U?
w
and V ?
w
for each
word. The two r-dimension vectors are concate-
nated as an ?averaged? vector. We use this vector
to calculate the cosine similarity between words.
Table 5 shows examples of five closest neighbors
of queried words. While these lists include some
noise, we can clearly see that the neighbors ex-
hibit similar syntactic behavior. For example, ?on?
is close to other prepositions. More interestingly,
we can consider the impact of syntactic context
on the derived projections. The bottom part of
Table 5 shows that the neighbors change substan-
tially depending on the syntactic role of the word.
For example, the closest words to the word ?in-
crease? are verbs in the context phrase ?will in-
crease again?, while the closest words become
nouns given a different phrase ?an increase of?.
Running Time Table 6 illustrates the impact of
estimating low-rank tensor parameters on the run-
ning time of the algorithm. For comparison, we
also show the NT-1st times across three typical
languages. The Arabic dataset has the longest av-
erage sentence length, while the Chinese dataset
1388
greatly profit says on when
actively earnings adds with where
openly franchisees predicts into what
significantly shares noted at why
outright revenue wrote during which
substantially members contends over who
increase will increase again an increase of
rise arguing gain
advance be prices
contest charging payment
halt gone members
Exchequer making subsidiary
hit attacks hit the hardest hit is
shed distributes monopolies
rallied stayed pills
triggered sang sophistication
appeared removed ventures
understate eased factors
Table 5: Five closest neighbors of the queried
words (shown in bold). The upper part shows our
learned embeddings group words with similar syn-
tactic behavior. The two bottom parts of the table
demonstrate that how the projections change de-
pending on the syntactic context of the word.
#Tok. Len.
Train. Time (hour)
NT-1st Ours
Arabic 42K 32 0.13 0.22
Chinese 337K 6 0.37 0.65
English 958K 24 1.88 2.83
Table 6: Comparison of training times across three
typical datasets. The second column is the number
of tokens in each data set. The third column shows
the average sentence length. Both first-order mod-
els are implemented in Java and run as a single
process.
has the shortest sentence length in CoNLL 2006.
Based on these results, estimating a rank-50 tensor
together with MST parameters only increases the
running time by a factor of 1.7.
7 Conclusions
Accurate scoring of syntactic structures such as
head-modifier arcs in dependency parsing typi-
cally requires rich, high-dimensional feature rep-
resentations. We introduce a low-rank factoriza-
tion method that enables to map high dimensional
feature vectors into low dimensional representa-
tions. Our method maintains the parameters as a
low-rank tensor to obtain low dimensional repre-
sentations of words in their syntactic roles, and to
leverage modularity in the tensor for easy train-
ing with online algorithms. We implement the
approach on first-order to third-order dependency
parsing. Our parser outperforms the Turbo and
MST parsers across 14 languages.
Future work involves extending the tensor com-
ponent to capture higher-order structures. In par-
ticular, we would consider second-order structures
such as grandparent-head-modifier by increasing
the dimensionality of the tensor. This tensor will
accordingly be a four or five-way array. The online
update algorithm remains applicable since each di-
mension is optimized in an alternating fashion.
8 Acknowledgements
The authors acknowledge the support of the MURI
program (W911NF-10-1-0533) and the DARPA
BOLT program. This research is developed in col-
laboration with the Arabic Language Technoligies
(ALT) group at Qatar Computing Research Insti-
tute (QCRI) within the LYAS project. We thank
Volkan Cirik for sharing the unsupervised word
vector data. Thanks to Amir Globerson, Andreea
Gane, the members of the MIT NLP group and
the ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
References
Miguel Ballesteros and Joakim Nivre. 2012. Mal-
tOptimizer: An optimization tool for MaltParser. In
EACL. The Association for Computer Linguistics.
Miguel Ballesteros. 2013. Effective morpholog-
ical feature selection with MaltOptimizer at the
SPMRL 2013 shared task. In Proceedings of
the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages. Association for
Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06.
Association for Computational Linguistics.
Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Par-
rilo, and Alan S Willsky. 2011. Rank-sparsity in-
coherence for matrix decomposition. SIAM Journal
on Optimization.
Volkan Cirik and H?usn?u S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task : Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages. Association for
Computational Linguistics.
1389
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1. Associ-
ation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing - Volume 10, EMNLP ?02. As-
sociation for Computational Linguistics.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of
semantic compositionality. In HLT-NAACL. The As-
sociation for Computational Linguistics.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multiview learning of word embeddings via
CCA. In Advances in Neural Information Process-
ing Systems.
A Evgeniou and Massimiliano Pontil. 2007. Multi-
task feature learning. In Advances in neural infor-
mation processing systems: Proceedings of the 2006
conference. The MIT Press.
Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftali Tishby. 2007. Euclidean embedding of co-
occurrence data. Journal of Machine Learning Re-
search.
Christopher Hillar and Lek-Heng Lim. 2009. Most
tensor problems are NP-hard. arXiv preprint
arXiv:0911.1393.
Daniel Hsu and Sham M Kakade. 2013. Learning mix-
tures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th
Conference on Innovations in Theoretical Computer
Science. ACM.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?10. Association for Com-
putational Linguistics.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Angeliki Lazaridou, Eva Maria Vecchi, and Marco
Baroni. 2013. Fish transporters and miracle
homes: How compositional distributional semantics
can help NP parsing. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-
of-speech induction. In Advances in Neural Infor-
mation Processing Systems.
Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M?ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M?ario A. T. Figueiredo. 2011a. Dual
decomposition with many overlapping components.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11. Association for Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Pedro MQ Aguiar,
and M?ario AT Figueiredo. 2011b. Structured spar-
sity in structured prediction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.
Andr?e FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow.
2010. Improving arabic dependency parsing with
lexical and inflectional morphological features. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, SPMRL ?10. Association for Computa-
tional Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow.
2011. Improving arabic dependency parsing with
form-based and functional morphological features.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
1390
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR.
Peter Nilsson and Pierre Nugues. 2010. Automatic
discovery of feature sets for dependency parsing. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010). Coling
2010 Organizing Committee.
Joakim Nivre, Johan Hall, Jens Nilsson, G?uls?en Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering.
Alexander Rush and Slav Petrov. 2012a. Vine pruning
for efficient multi-pass dependency parsing. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL ?12).
Alexander M Rush and Slav Petrov. 2012b. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compo-
sitional vector grammars. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics.
Nathan Srebro, Tommi Jaakkola, et al 2003. Weighted
low-rank approximations. In ICML.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola.
2004. Maximum-margin matrix factorization. In
Advances in neural information processing systems.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, CoNLL ?08. Association for
Computational Linguistics.
Min Tao and Xiaoming Yuan. 2011. Recovering low-
rank and sparse components of matrices from in-
complete and noisy observations. SIAM Journal on
Optimization.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?10. Association for Com-
putational Linguistics.
Andrew E Waters, Aswin C Sankaranarayanan, and
Richard Baraniuk. 2011. SpaRCS: Recovering low-
rank and sparse matrices from compressive mea-
surements. In Advances in Neural Information Pro-
cessing Systems.
Hao Zhang and Ryan McDonald. 2012a. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12. Association for Compu-
tational Linguistics.
Hao Zhang and Ryan McDonald. 2012b. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning.
Association for Computational Linguistics.
Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-
ald. 2013. Online learning for inexact hypergraph
search. In Proceedings of EMNLP.
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Jaakkola, and Amir Globerson. 2014. Steps to ex-
cellence: Simple inference with refined scoring of
dependency trees. In Proceedings of the 52th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.
Tianyi Zhou and Dacheng Tao. 2011. Godec: Ran-
domized low-rank & sparse matrix decomposition in
noisy case. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11).
1391
