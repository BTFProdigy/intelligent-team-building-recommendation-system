Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 877?886, Prague, June 2007. c?2007 Association for Computational Linguistics
Translating Unknown Words by Analogical Learning
Philippe Langlais and Alexandre Patry
Dept. I.R.O.
Universite? de Montre?al
{felipe,patryale}@iro.umontreal.ca
Abstract
Unknown words are a well-known hindrance
to natural language applications. In particu-
lar, they drastically impact machine transla-
tion quality. An easy way out commercial
translation systems usually offer their users
is the possibility to add unknown words
and their translations into a dedicated lex-
icon. Recently, Stroppa and Yvon (2005)
have shown how analogical learning alone
deals nicely with morphology in different
languages. In this study we show that ana-
logical learning offers as well an elegant and
effective solution to the problem of identify-
ing potential translations of unknown words.
1 Introduction
Analogical reasoning has received some attention in
cognitive science and artificial intelligence (Gentner
et al, 2001). It has been for a long time a faculty as-
sessed in the so-called SAT Reasoning tests used in
the application process to colleges and universities
in the United States. Turney (2006) has shown that
it is possible to compute relational similarities in a
corpus in order to solve 56% of typical analogical
tests quizzed in SAT exams. The interested reader
can find in (Lepage, 2003) a particularly dense treat-
ment of analogy, including a fascinating chapter on
the history of the notion of analogy.
The concept of proportional analogy, denoted
[A : B = C : D ], is a relation between four
entities which reads: ?A is to B as C is to D?.
Among proportional analogies, we distinguish for-
mal analogies, that is, ones that arise at the graph-
ical level, such as [fournit : fleurit = fournie :
fleurie] in French or [believer : unbelievable =
doer : undoable] in English. Formal analogies are
often good indices for deeper analogies (Stroppa and
Yvon, 2005).
Lepage and Denoual (2005) presented the sys-
tem ALEPH, an intriguing example-based system
entirely built on top of an automatic formal anal-
ogy solver. This system has achieved state-of-the-
art performance on the IWSLT task (Eck and Hori,
2005), despite its striking purity. As a matter of
fact, ALEPH requires no distances between exam-
ples, nor any threshold.1 It does not even rely on
a tokenization device. One reason for its success
probably lies in the specificity of the BTEC corpus:
short and simple sentences of a narrow domain. It is
doubtful that ALEPH would still behave adequately
on broader tasks, such as translating news articles.
Stroppa and Yvon (2005) propose a very help-
ful algebraic description of a formal analogy and
describe the theoretical foundations of analogical
learning which we will recap shortly. They show
both its elegance and efficiency on two morphologi-
cal analysis tasks for three different languages.
Recently, Moreau et al (2007) showed that for-
mal analogies of a simple kind (those involving suf-
fixation and/or prefixation) offer an effective way to
extend queries for improved information retrieval.
In this study, we show that analogical learning
can be used as an effictive method for translating
unknown words or phrases. We found that our ap-
proach has the potential to propose a valid transla-
tion for 80% of ordinary unknown words, that is,
words that are not proper names, compound words,
or numerical expressions. Specific solutions have
been proposed for those token types (Chen et al,
1998; Al-Onaizan and Knight, 2002; Koehn and
Knight, 2003).
The paper is organized as follows. We first recall
1Some heuristics are applied for speeding up the system.
877
in Section 2 the principle of analogical learning and
describe how it can be applied to the task of enrich-
ing a bilingual lexicon. In Section 3, we present the
corpora we used in our experiments. We evaluate
our approach over two translation tasks in Section 4.
We discuss related work in Section 5 and give per-
spectives of our work in Section 6.
2 Analogical Learning
2.1 Principle
Our approach to bilingual lexical enrichment is an
instance of analogical learning described in (Stroppa
and Yvon, 2005). A learning set L = {L1, . . . , LN}
gathers N observations. A set of features computed
on an incomplete observation X defines an input
space. The inference task consists in predicting the
missing features which belong to an output space.
We denote I(X) (resp. O(X)) the projection of X
into the input (resp. output) space. The inference
procedure involves three steps:
1. Building EI(X) = {(A,B,C) ? L3 | [I(A) :
I(B) = I(C) : I(X)]}, the set of input stems2
of X , that is the set of triplets (A,B,C) which
form with X an analogical equation.
2. Building EO(X) = {Y | [O(A) : O(B) =
O(C) : Y ] ,?(A,B,C) ? EI(X)} the set of
solutions to the analogical equations obtained
by projecting the stems of EI(X) into the out-
put space.
3. Selecting O(X) among the elements of
EO(X).
This inference procedure shares similarities with
the K-nearest-neighbor (k-NN) approach. In partic-
ular, since no model of the training material is be-
ing learned, the training corpus needs to be stored
in order to be queried. On the contrary to k-NN,
however, the search for closest neighbors does not
require any distance, but instead relies on relational
similarities. This purity has a cost: while in k-NN
inference, neighbors can be found in time linear to
the training size, in analogical learning, this oper-
ation requires a computation time cubic in N , the
2In Turney?s work (Turney, 2006), a stem designates the first
two words of a proportional analogy.
number of observations. In many applications of in-
terest, including the one we tackle here, this is sim-
ply impractical and heuristics must be applied.
The first and second steps of the inference proce-
dure rely on the existence of an analogical solver,
which we sketch in the next section. One impor-
tant thing to note at this stage, is that an analogical
equation may have several solutions, some being le-
gitimate word-forms in a given language, others be-
ing not. Thus, it is important to select wisely the
generated solutions, therefore Step 3. In practice,
the inference procedure involves the computation of
many analogical equations, and a statistic as simple
as the frequency of a solution often suffices to sepa-
rate good from spurious solutions.
2.2 Analogical Solver
Lepage (1998) proposed an algorithm for comput-
ing the solutions of a formal analogical equation
[A : B = C : ? ]. We implemented a variant of
this algorithm which requires to compute two edit-
distance tables, one between A and B and one be-
tween A and C. Since we are looking for subse-
quences of B and C not present in A, insertion cost
is null. Once this is done, the algorithm synchro-
nizes the alignments defined by the paths of min-
imum cost in each table. Intuitively, the synchro-
nization of two alignments (one between A and B,
and one between A and C) consists in composing in
the correct order subsequences of the strings B and
C that are not in A. We refer the reader to (Lep-
age, 1998) for the intricacies of this process which
is illustrated in Figure 1 for the analogical equation
[even : usual = unevenly : ? ]. In this exam-
ple, there are 681 different paths that align even and
usual (with a cost of 4), and 1 path which aligns even
with unevenly (with a cost of 0). This results in 681
synchronizations which generate 15 different solu-
tions, among which only unusually is a legitimate
word-form.
In practice, since the number of minimum-cost
paths may be exponential in the size of the strings
being aligned, we consider the synchronization of
a maximum of M best paths in each edit-distance
table. The worst-case complexity of our analogical
solver is O([|A| ? (|B| + |C|)] + [M2 ? (|A| +
ins(B,C))]), where the first term corresponds to
the computation of the two edit-distance tables,
878
4 4 4 4 4 4 n 4 4 3 3 2 1 0 0 0
3 3 3 3 3 3 e 3 3 3 2 1 0 0 0 0
2 2 2 2 2 2 v 2 2 2 1 0 0 0 0 0
1 1 1 1 1 1 e 1 1 1 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
l a u s u / . u n e v e n l y
e v e n e v e n
u s u a l u n e v e n l y
?usua-un-l-ly
e v e n e v e n
u s u a l u n e v e n l y
?un-usu-a-l-ly
Figure 1: The top table reports the edit-distance ta-
bles computed between even and usual (left part),
and even and unevenly (right part). The bottom
part of the figure shows 2 of the 681 synchroniza-
tions computed while solving the equation [even :
usual = unevenly : ? ]. The first one corresponds to
the path marked in bold italics and leads to a spuri-
ous solution; the second leads to a legitimate solu-
tion and corresponds to the path shown as squares.
and the second one corresponds to the maximum
time needed to synchronize them. |X| denotes the
length, counted in characters of the string X , whilst
ins(B,C) stands for the number of characters of B
and C not belonging to A. Given the typical length
of the strings we consider in this study, our solver is
quite efficient.3
Stroppa and Yvon (2005) described a generaliza-
tion of this algorithm which can solve a formal ana-
logical equation by composing two finite-state trans-
ducers.
2.3 Application to Lexical Enrichment
Analogical inference can be applied to the task of
extending an existing bilingual lexicon (or transfer
table) with new entries. In this study, we focus on
a particular enrichment task: the one of translating
valid words or phrases that were not encountered at
training time.
A simple example of how our approach translates
unknown words is illustrated in Figure 2 for the (un-
3Several thousands of equations solved within one second.
Step 1 source (French) stems
[activite?s : activite? = futilite?s : futilite?]
[hostilite?s : hostilite? = futilite?s : futilite?] . . .
Step 2a projection by lexicon look-up
activite?s?actions hostilite??hostility
hostilite?s?hostilities activite??action
futilite?s?trivialities,gimmicks . . .
Step 2b target (English) resolution
[actions : action = gimmicks : ? ] ? gimmick
[hostilities : hostility = trivialities : ? ] ? triviality
[hobbies : hobby = trivialities : ? ] ? triviality
Step 3 selection of target candidates
?triviality, 2?, ?gimmick , 1?, . . .
Figure 2: Illustration of the analogical inference pro-
cedure applied to the translation of the unknown
French word futilite?.
known) French word futilite?. In this example, trans-
lations is inferred by commuting plural and singular
words. The inference process lazily captures the fact
that English plural nouns ending in -ies usually cor-
respond to singular nouns ending in -y.
Formally, we are given a training corpus L =
{?S1, T1?, . . . , ?SN , TN ?} which consists of a col-
lection of N bilingual lexicon entries ?Si, Ti?. The
input space is in our case the space of possible
source words, while the output space is the set of
possible target words. We define:
?X ? ?S ,T?, I(X) = S and O(X) = T
Given an unknown source word-form S, Step 1 of
the inference process consists in identifying source
stems which have S as a solution:4
EI(S) = {?i, j, k? ? [1, N ]
3 | [Si : Sj = Sk : S]}.
During Step 2a, each source stem belonging to
EI(S) is projected form by form into (potentially
several) stems in the output space, thanks to an op-
erator proj that will be defined shortly:
E?i ,j ,k ?(S) = {T | [U : V = W : T ]} where
(U, V,W ) ? (projL(Si)? projL(Sj)? projL(Sk)).
4All strings in a stem must be different, otherwise, it can be
shown that all source words would be considered.
879
During Step 2b, each solution to those output
stems is collected in EO(S) along with its associated
frequency:
EO(S) =
?
?i ,j ,k ??EI(S)
E?i ,j ,k ?(S).
Step 3 selects from EO(S) one or several solu-
tions. We use frequency as criteria to sort the gener-
ated solutions. The projection mechanism we resort
to in this study simply is a lexicon look-up:
projL(S) = {T | ?S, T ? ? L}.
There are several situations where this inference
procedure will introduce noise. First, both source
and target analogical equations can lead to spuri-
ous solutions. For instance, [show : showing =
eating : ? ] will erroneously produce eatinging. Sec-
ond, an error in the original lexicon may introduce
as well erroneous target word-forms. For instance,
when translating the German word proklamierung,
by making use of the analogy [formalisiert :
formalisierung = proklamiert : proklamierung],
the English equation [formalised : formalized =
sets : ? ] will be considered if it happens that
proklamiert?sets belongs to L; in which case, zets
will be erroneously produced.
We control noise in several ways. The source
word-forms we generate are filtered by imposing
that they belong to the input space. We also use a
(large) target vocabulary to eliminate spurious tar-
get word-forms (see Section 3). More importantly,
since we consider many analogical equations when
translating a word-form, spurious analogical solu-
tions tend to appear less frequently than ones arising
from paradigmatic commutations.
2.4 Practical Considerations
Searching for EI(S) is an operation which requires
solving a number of (source) analogical equations
cubic in the size of the input space. In many settings
of interest, including ours, this is simply not practi-
cal. We therefore resort to two strategies to reduce
computation time. The first one consists in using the
analogical equations in a generative mode. Instead
of searching through the set of stems ?Si, Sj , Sk?
that have for solution the unknown source word-
form S, we search for all pairs (Si, Sj) to the so-
lutions of [Si : Sj = S :?] that are valid word-forms
of the input space. Note that this is an exact method
which follows from the property (Lepage, 1998):
[A : B = C : D ] ? [B : A = D : C ]
This leaves us with a quadratic computation time
which is still intractable in our case. Therefore,
we apply a second strategy which consists in com-
puting the analogical equations [Si : Sj = S :?]
for the only words Si and Sj close enough to S.
More precisely, we enforce that Si ? v?(S) and that
Sj ? v?(Si) for a neighborhood function v?(A) of
the form:
v?(A) = {B | f(B,A) ? ?}
where f is a distance; we used the edit-distance in
this study (Levenshtein, 1966). Note that the second
strategy we apply is only a heuristic.
3 Resources
In this work, we are concerned with one concrete
problem a machine translation system must face:
the one of translating unknown words. We are fur-
ther focusing on the shared task of the workshop
on Statistical Machine Translation, which took place
last year (Koehn and Monz, 2006) and consisted in
translating Spanish, German, and French texts from
and to English. For some reasons, we restricted our-
selves to translating only into English. The training
material available is coming from the Europarl cor-
pus. The test material was divided into two parts.5
The first one (hereafter called test-in) is com-
posed of 2 000 sentences from European parliament
debates. The second part (called test-out) gath-
ers 1 064 sentences6 collected from editorials of the
Project Syndicate website.7 The main statistics per-
tinent to our study are summarized in Table 1.
A rough analysis of the 441 different unknown
words encountered in the French test sets reveals
that 54 (12%) of them contain at least one digit
(years, page numbers, law numbers, etc.), 83 (20%)
are proper names, 37 (8%) are compound words,
18 (4%) are foreign words (often Latin or Greek
5The participants were not aware of this.
6We removed 30 sentences which had encoding problems.
7http://www.project-syndicate.com
880
French Spanish German
test- in out in out in out
|unknown| 180 265 233 292 469 599
oov% 0.26 1.22 0.38 1.37 0.84 2.87
Table 1: Number of different (source) test words not
seen at training time, and out-of-vocabulary rate ex-
pressed as a percentage (oov%).
words), 7 words are acronyms, and 4 are tokeniza-
tion problems. The 238 other words (54%) are ordi-
nary words.
We considered different lexicons for testing our
approach. These lexicons were derived from the
training material of the shared task by training with
GIZA++ (Och and Ney, 2000) ?default settings?
two transfer tables (source-to-target and the reverse)
that we intersected to remove some noise.
In order to investigate how sensitive our approach
is to the amount of training material available, we
varied the size of our lexicon LT by considering dif-
ferent portions of the training corpus (T = 5 000,
10 000, 100 000, 200 000, and 500 000 pairs of sen-
tences). The lexicon trained on the full training ma-
terial (688 000 pairs of sentences), called Lref here-
after, is used for validation purposes. We kept (at
most) the 20 best associations of each source word
in these lexicons. In practice, because we intersect
two models, the average number of translations kept
for each source word is lower (see Table 2).
Last, we collected from various target texts (En-
glish here) we had at our disposal, a vocabulary set
V gathering 466 439 words, that we used to filter out
spurious word-forms generated by our approach.
4 Experiments
4.1 Translating Unknown Words
For the three translation directions (from Span-
ish, German, and French into English), we ap-
plied the analogical reasoning to translate the (non-
numerical) source words of the test material, absent
from LT . Examples of translations produced by ana-
logical inference are reported in Figure 3, sorted by
decreasing order of times they have been generated.
anti-agricole  (anti-farm,5) (anti-agricultural,3)
(anti-rural,3) (anti-farming,3) (anti-farmer,3)
fleurie  (flourishing,5) (flourished,4) (flourish,1)
futilite?  (trivialities,27) (triviality,14) (futile,9)
(meaningless,9) (futility,4) (meaninglessness,4)
(superfluous,2) (unwieldy,2) (unnecessary,2)
(uselessness,2) (trivially,1) (tie,1) (trivial,1)
butoir  (deadline,42) (deadlines,33) (blows,1)
court-circuitant  (bypassing,13) (bypass,12)
(bypassed,5) (bypasses,1)
xviie  (xvii,18) (sixteenth,3) (eighteenth,1)
Figure 3: Candidate translations inferred from
L200 000 and their frequency. The candidates re-
ported are those that have been intersected with V .
Translations in bold are clearly erroneous.
4.1.1 Baselines
We devised two baselines against which we com-
pared our approach (hereafter ANALOG). The first
one, BASE1, simply proposes as translations the tar-
get words in the lexicon LT which are the most simi-
lar (in the sense of the edit-distance) to the unknown
source word. Naturally, this approach is only appro-
priate for pairs of languages that share many cog-
nates (i.e., docteur ? doctor). The second base-
line, BASE2, is more sensible and more closely cor-
responds to our approach. We first collect a set of
source words that are close-enough (according to the
edit-distance) to the unknown word. Those source
words are then projected into the output space by
simple bilingual lexicon look-up. So for instance,
the French word demanda will be translated into the
English word request if the French word demande is
in LT and that request is one of its sanctioned trans-
lations.
Each of these baselines is tested in two variants.
The first one (id), which allows a direct comparison,
proposes as many translations as ANALOG does. The
second one (10) proposes the first 10 translations of
each unknown word.
4.1.2 Automatic Evaluation
Evaluating the quality of translations requires to
inspect lists of words each time we want to test a
variant of our approach. This cumbersome process
not only requires to understand the source language,
881
LT 5 000 10 000 50 000 100 000 200 000 500 000
p% r% p% r% p% r% p% r% p% r% p% r%
test-in
ANALOG 51.4 30.7 55.3 44.4 58.8 64.3 58.2 65.1 59.4 65.2 30.4 67.6
BASE1id 31.6 30.7 32.3 44.4 24.7 64.3 20.3 65.1 20.9 65.2 8.7 67.6
BASE2id 34.5 30.7 37.1 44.4 39.0 64.3 37.8 65.1 34.4 65.2 56.5 67.6
BASE110 26.7 100.0 28.3 100.0 23.9 100.0 20.0 100.0 16.6 100.0 11.8 100.0
BASE210 26.3 100.0 30.8 100.0 29.3 100.0 27.6 100.0 24.9 100.0 55.9 100.0
unk [3 171 , 9.1] [2 245 , 7.7] [754 , 4.0] [456 , 2.9] [253 , 2.0] [34 , 1.2]
test-out
ANALOG 52.8 28.9 55.3 42.5 52.9 68.8 54.7 74.6 55.7 81.0 43.3 88.2
BASE1id 28.0 28.9 29.0 42.5 27.3 68.8 23.1 74.6 26.8 81.0 22.7 88.2
BASE2id 32.9 28.9 35.0 42.5 32.5 68.8 35.9 74.6 40.8 81.0 59.1 88.2
BASE110 24.7 100.0 25.9 100.0 25.1 100.0 20.9 100.0 25.2 100.0 25.0 100.0
BASE210 21.7 100.0 26.4 100.0 27.2 100.0 29.4 100.0 33.6 100.0 57.9 100.0
unk [2 270 , 8.2] [1 701 , 6.9] [621 , 3.4] [402 , 2.4] [226 , 1.8] [76 , 1.4]
Table 2: Performance of the different approaches on the French-to-English direction as a function of the
number T of pairs of sentences used for training LT . A pair [n , t] in lines labeled by unk stands for the
number of words to translate, and the average number of their translations in Lref .
but happens to be in practice a delicate task. We
therefore decided to resort to an automatic evalua-
tion procedure which relies on Lref , a bilingual lex-
icon which entries are considered correct.
We translated all the words of Lref absent from
LT . We evaluated the different approaches by com-
puting response and precision rates. The response
rate is measured as the percentage of words for
which we do have at least one translation produced
(correct or not). The precision is computed in our
case as the percentage of words for which at least
one translation is sanctioned by Lref . Note that this
way of measuring response and precision is clearly
biased toward translation systems that can hypoth-
esize several candidate translations for each word,
as statistical systems usually do. The reason of this
choice was however guided by a lack of precision of
the reference we anticipated, a point we discuss in
Section 4.1.3.
The figures for the French-to-English direction
are reported in Table 2. We observe that the ratio
of unknown words that get a translation by ANA-
LOG is clearly impacted by the size of the lexicon
LT we use for computing analogies: the larger the
better. This was expected since the larger a lexicon
is, the higher the number of source analogies that
can be made and consequently, the higher the num-
ber of analogies that can be projected onto the out-
put space. The precision of ANALOG is rather stable
across variants and ranges between 50% to 60%.
The second observation we make is that the base-
lines perform worse than ANALOG in all but the
L500 000 cases. Since our baselines propose trans-
lations to each source word, their response rate is
maximum. Their precision, however, is an issue.
Expectedly, BASE1 is the worst of the two baselines.
If we arbitrarily fix the response rate of BASE2 to the
one of ANALOG, the former approach shows a far
lower precision (e.g., 34.4 against 59.4 for L200 000).
This not only indicates that analogical learning is
handling unknown words better than BASE2, but as
well, that a combination of both approaches could
potentially yield further improvements.
A last observation concerns the fact that ANALOG
performs equally well on the out-domain material.
This is very important from a practical point of view
and contrasts with some related work we discuss in
Section 5.
At first glance, the fact that BASE2 outperforms
ANALOG on the larger training size is disappoint-
ing. After investigations, we came to the conclusion
that this is mainly due to two facts. First, the num-
882
ber of unknown words on which both systems were
tested is rather low in this particular case (e.g., 34
for the in-domain corpus). Second, we noticed a de-
ficiency of the reference lexicon Lref for many of
those words. After all, this is not surprising since
the words unseen in the 500 000 pairs of training
sentences, but encountered in the full training cor-
pus (688 000 pairs) are likely to be observed only a
few times, therefore weakening the associations au-
tomatically acquired for these entries. We evaluate
that a third of the reference translations were wrong
in this setting, which clearly raises some doubts on
our automatic evaluation procedure in this case.
The performance of ANALOG across the three lan-
guage pairs are reported in Table 3. We observe a
drop of performance of roughly 10% (both in preci-
sion and response) for the German-to-English trans-
lation direction. This is likely due to the heuris-
tic procedure we apply during the search for stems,
which is not especially well suited for handling com-
pound words that are frequent in German.
We observe that for Spanish- and German-to-
English translation directions, the precision rate
tends to decrease for larger values of T . One ex-
planation for that is that we consider all analogies
equally likely in this work, while we clearly noted
that some are spurious ones. With larger training
material, spurious analogies become more likely.
French Spanish German
T p% r% p% r% p% r%
5 51.4 30.7 52.8 30.3 49.3 23.1
10 55.3 44.4 52.0 45.2 47.6 33.3
50 58.8 64.3 54.0 66.5 44.6 53.2
100 58.2 65.1 53.9 69.1 45.8 55.6
200 59.4 65.2 46.4 71.8 43.0 59.2
Table 3: Performance across language pairs mea-
sured on test-in. The number T of pairs of sen-
tences used for training LT is reported in thousands.
We measured the impact the translations produced
by ANALOG have on a state-of-the-art phrase-based
translation engine, which is described in (Patry et
al., 2006). For that purpose, we extended a phrase-
table with the first translation proposed by ANALOG
or BASE2 for each unknown word of the test ma-
terial. Results in terms of word-error-rate (WER)
and BLEU score (Papineni et al, 2002) are reported
in Table 4 for those sentences that contain at least
one unknown word. Small but consistent improve-
ments are observed for both metrics with ANALOG.
This was expected, since the original system sim-
ply leaves the unknown words untranslated. What
is more surprising is that the BASE2 version slightly
underperforms the baseline. The reason is that some
unknown words that should appear unmodified in
a translation, often get an erroneous translation by
BASE2. Forcing BASE2 to propose a translation
for the same words for which ANALOG found one,
slightly improves the figures (BASE2id).
French Spanish German
WER BLEU WER BLEU WER BLEU
base 61.8 22.74 54.0 27.00 69.9 18.15
+BASE2 61.8 22.72 54.2 26.89 70.3 18.05
+BASE2id 61.7 22.81 54.1 27.01 70.1 18.14
+ANALOG 61.6 22.90 53.7 27.27 69.7 18.30
sentences 387 452 814
Table 4: Translation quality produced by our phrase-
based SMT engine (base) with and without the
first translation produced by ANALOG, BASE2, or
BASE2id for each unknown word.
4.1.3 Manual Evaluation
As we already mentioned, the lexicon used as a
reference in our automatic evaluation procedure is
not perfect, especially for low frequency words. We
further noted that several words receive valid trans-
lations that are not sanctioned by Lref . This is for
instance the case of the examples in Figure 4, where
circumventing and fellow are arguably legitimate
translations of the French words contournant and
concitoyen, respectively. Note that in the second ex-
ample, the reference translation is in the plural form
while the French word is not.
Therefore, we conducted a manual evaluation of
the translations produced from L100 000 by ANA-
LOG and BASE2 on the 127 French words of the
corpus test-in8 unknown of Lref . Those are
the non-numerical unknown words the participat-
ing systems in the shared task had to face in the
8We did not notice important differences between test-in
and test-out.
883
contournant (49 candidates)
ANALOG  (circumventing,55) (undermining,20)
(evading,19) (circumvented,17) (overturning,16)
(circumvent,15) (circumvention,15) (bypass,13)
(evade,13) (skirt,12)
Lref  skirting, bypassing, by-pass, overcoming
concitoyen (24 candidates)
ANALOG  (citizens,26) (fellow,26) (fellow-
citizens,26) (people,26) (citizen,23) (fellow-
citizen,21) (fellows,5) (peoples,3) (civils,3) (fel-
lowship,2)
Lref  fellow-citizens
Figure 4: 10 best ranked candidate translations pro-
duced by ANALOG from L200 000 for two unknown
words and their sanctioned translations in Lref .
Words in bold are present in both the candidate and
the reference lists.
in-domain part of the test material. 75 (60%) of
those words received at least one valid translation
by ANALOG while only 63 (50%) did by BASE2.
Among those words that received (at least) one valid
translation, 61 (81%) were ranked first by ANA-
LOG against only 22 (35%) by BASE2. We fur-
ther observed that among the 52 words that did not
receive a valid translation by ANALOG, 38 (73%)
did not receive a translation at all. Those untrans-
lated words are mainly proper names (bush), foreign
words (munere), and compound words (rhe?nanie-
du-nord-westphalie), for which our approach is not
especially well suited.
We conclude from this informal evaluation that
80% of ordinary unknown words received a valid
translation in our French-to-English experiment, and
that roughly the same percentage had a valid trans-
lation proposed in the first place by ANALOG.
4.2 Translating Unknown Phrases
Our approach is not limited to translate solely un-
known words, but might serve as well to enrich
existing entries in a lexicon. For instance, low-
frequency words, often poorly handled by current
statistical methods, could receive useful translations.
This is illustrated in Figure 5 where we report the
best candidates produced by ANALOG for the French
word invite?es, which appears 7 times in the 200 000
invite?e (61 candidates)
ANALOG  (invited,135) (requested,92) (cal-
led,77) (urged,75) (guest,72) (asked,47) (re-
quest,43) (invites,27) (invite,26) (urge,26)
L200 000  asked, generate, urged
Figure 5: 10 best candidates produced by ANALOG
for the low-frequency French word invite?es and its
translations in L200 000.
first pairs of the training corpus. Interestingly, ANA-
LOG produced the candidate guest which corre-
sponds to a legitimate meaning of the French word
that was absent in the training data.
Because it can treat separators as any other char-
acter, ANALOG is not bounded to translate only
words. As a proof of concept, we applied analogical
reasoning to translate those source sequences of at
most 5 words in the test material that contain an un-
known word. Since there are many more sequences
than there are words, the input space in this exper-
iment is far larger, and we had to resort to a much
more aggressive pruning technique to find the stems
of the sequences to be translated.
expulsent  (expelling,36) (expel,31) (are ex-
pelling,23) (are expel,10)
focaliserai  (focus,10) (focus solely,9) (concen-
trate all,9) (will focus,9) (will placing,9)
de?passeront  (will exceed,4) (exceed,3) (will be
exceed,3) (we go beyond,2) (will be exceeding,2)
non-re?ussite de  (lack of success for,4) (lack of
success of,4) (lack of success,4)
que vous subissez  (you are experiencing,2)
Figure 6: Examples of translations produced by
ANALOG where the input (resp. output) space is
defined by the set of source (resp. target) word se-
quences. Words in bold are unknown.
We applied the automatic evaluation procedure
described in Section 4.1.2 for the French-to-English
translation direction, with a reference lexicon being
this time the phrase table acquired on the full train-
ing material.9 The response rate in this experiment is
particularly low since only a tenth of the sequences
9This model contains 1.5 millions pairs of phrases.
884
received (at least) a translation by ANALOG. Those
are short sequences that contain at most three words,
which clearly indicates the limitation of our prun-
ing strategy. Among those sequences that received
at least one translation, the precision rate is 55%,
which is consistent with the rate we measured while
translating words.
Examples of translations are reported in Figure 6.
We observe that single words are not contrived any-
more to be translated by a single word. This allows
to capture 1:n relations such as de?passeront?will
exceed, where the future tense of the French word is
adequately rendered by the modal will in English.
5 Related Work
We are not the first to consider the translation of un-
known words or phrases. Several authors have for
instance proposed approaches for translating proper
names and named entities (Chen et al, 1998; Al-
Onaizan and Knight, 2002). Our approach is com-
plementary to those ones.
Recently and more closely related to the approach
we described, Callison-Burch et al (2006) proposed
to replace an unknown phrase in a source sentence
by a paraphrase. Paraphrases in their work are ac-
quired thanks to a word alignment computed over
a large external set of bitexts. One important dif-
ference between their work and ours is that our ap-
proach does not require additional material.10 In-
deed, they used a rather idealistic set of large, ho-
mogeneous bitexts (European parliament debates) to
acquire paraphrases from. Therefore we feel our ap-
proach is more suited for translating ?low density?
languages and languages with a rich morphology.
Several authors considered as well the translation
of new words by relying on distributional colloca-
tional properties computed from a huge non-parallel
corpus (Rapp, 1999; Fung and Yee, 1998; Takaaki
and Matsuo, 1999; Koehn and Knight, 2002). Even
if admittedly non-parallel corpora are easier to ac-
quire than bitexts, this line of work is still heavily
dependent on huge external resources.
Most of the analogies made at the word level in
our study are capturing morphological information.
10We do use a target vocabulary list to filter out spurious
analogies, but we believe we could do without. The frequency
with which we generate a string could serve to decide upon its
legitimacy.
The use of morphological analysis in (statistical)
machine translation has been the focus of several
studies, (Nie?en, 2002) among the first. Depend-
ing on the pairs of languages considered, gains have
been reported when the training material is of mod-
est size (Lee, 2004; Popovic and Ney, 2004; Gold-
water and McClosky, 2005). Our approach does not
require any morphological knowledge of the source,
the target, or both languages. Admittedly, several
unsupervised morphological induction methodolo-
gies have been proposed, e.g., the recent approach
in Freitag (2005). In any case, as we have shown,
ANALOG is not bounded to treat only words, which
we believe to be at our advantage.
6 Discussion and Future Work
In this paper, we have investigated the appropri-
ateness of analogical learning to handle unknown
words in machine translation. On the contrary to
several lines of work, our approach does not rely on
massive additional resources but capitalizes instead
on an information which is inherently pertaining to
the language. We measured that roughly 80% of or-
dinary unknown French words can receive a valid
translation into English with our approach.
This work is currently being developed in several
directions. First, we are investigating why our ap-
proach remains silent for some words or phrases.
This will allow us to better characterize the limita-
tions of ANALOG and will hopefully lead us to de-
sign a better strategy for identifying the stems of a
given word or phrase. Second, we are investigat-
ing how a systematic enrichment of a phrase-transfer
table will impact a phrase-based statistical machine
translation engine. Last, we want to investigate the
training of a model that can learn regularities from
the analogies we are making. This would relieve us
from requiring the training material while translat-
ing, and would allow us to compare our approach
with other methods proposed for unsupervised mor-
phology acquisition.
Acknowledgement We are grateful to the anony-
mous reviewers for their useful suggestions and to
Pierre Poulin for his fruitful comments. This study
has been partially funded by NSERC.
885
References
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proc. of the 40th ACL, pages 400?408,
Philadelphia, Pennsylvania, USA.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proc. of HLT-NAACL, pages 17?
24, New York City, USA.
Hsin-Hsi Chen, Sheng-Jie Hueng, Yung-Wei Ding, and
Shih-Chung Tsai. 1998. Proper name translation
in cross-language information retrieval. In Proc. of
the 17th COLING, pages 232?236, Montreal, Que?bec,
Canada.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In International
Workshop on Spoken Language Translation (IWSLT),
Pittsburgh, Pennsylvania, USA.
Dayne Freitag. 2005. Morphology induction from term
clusters. In Proc. of the 9th CoNLL, pages 128?135,
Ann Arbor, Michigan, USA.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proc. of the 36th ACL, pages 414?420,
San Francisco, California, USA.
Dedre Gentner, Keith J. Holyoak, and Boicho N.
Konikov. 2001. The Analogical Mind. The MIT
Press, Cambridge, Massachusetts, USA.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological analy-
sis. In Proc. of HLT-EMNLP, pages 676?683, Van-
couver, British Columbia, Canada.
Philipp Koehn and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora. In
Proc. of the ACL Workshop on Unsupervised Lexical
Acquisition, pages 9?16, Philadelphia, Pennsylvania,
USA.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proc. of the 10th EACL,
pages 187?193, Budapest, Hungary.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proc. of the HLT-NAACL Work-
shop on Statistical Machine Translation, pages 102?
121, New York City, USA.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proc. of HLT-NAACL,
Boston, Massachusetts, USA.
Yves Lepage and Etienne Denoual. 2005. ALEPH: an
EBMT system based on the preservation of propor-
tionnal analogies between sentences across languages.
In Proc. of IWSLT, Pittsburgh, Pennsylvania, USA.
Yves Lepage. 1998. Solving analogies on words: an
algorithm. In Proc. of COLING-ACL, pages 728?734,
Montreal, Que?bec, Canada.
Yves Lepage. 2003. De l?analogie rendant compte de la
commutation en linguistique. Ph.D. thesis, Universite?
Joseph Fourier, Grenoble, France.
Vladimir. I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. Sov.
Phys. Dokl., 6:707?710.
Fabienne Moreau, Vincent Claveau, and Pascale Se?billot.
2007. Automatic morphological query expansion us-
ing analogy-based machine learning. In Proc. of the
29th ECIR, Roma, Italy.
Sonja Nie?en. 2002. Improving Statistical Ma-
chine Translation using Morpho-syntactic Informa-
tion. Ph.D. thesis, RWTH, Aachen, Germany.
Franz-Joseph Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. of the 38th ACL,
pages 440?447, Hong Kong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of the 40th ACL,
pages 311?318, Philadelphia, Pennsylvania, USA.
Alexandre Patry, Fabrizo Gotti, and Philippe Langlais.
2006. Mood at work: Ramses versus Pharaoh. In
Proc. of the HLT-NAACL Workshop on Statistical Ma-
chine Translation, pages 126?129, New York City,
USA.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical machine
translation. In Proc. of the 4th LREC, pages 1585?
1588, Lisbon, Portugal.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proc. of the 37th ACL, pages 519?526, Col-
lege Park, Maryland, USA.
Nicolas Stroppa and Franc?ois Yvon. 2005. An analog-
ical learner for morphological analysis. In Proc. of
the 9th CoNLL, pages 120?127, Ann Arbor, Michigan,
USA.
Tanaka Takaaki and Yoshihiro Matsuo. 1999. Extrac-
tion of translation equivalents from non-parallel cor-
pora. In Proc. of the 8th TMI, pages 109?119, Chester,
England.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416, Sept.
886
Proceedings of the Workshop on Statistical Machine Translation, pages 126?129,
New York City, June 2006. c?2006 Association for Computational Linguistics
Mood at work: Ramses versus Pharaoh
Alexandre Patry, Fabrizio Gotti and Philippe Langlais
RALI ? DIRO
Universite? de Montre?al
{patryale,gottif,felipe}@iro.umontreal.ca
Abstract
We present here the translation system we
used in this year?s WMT shared task. The
main objective of our participation was
to test RAMSES, an open source phrase-
based decoder. For that purpose, we used
the baseline system made available by the
organizers of the shared task1 to build the
necessary models. We then carried out a
pair-to-pair comparison of RAMSES with
PHARAOH on the six different translation
directions that we were asked to perform.
We present this comparison in this paper.
1 Introduction
Phrase-based (PB) machine translation (MT) is now
a popular paradigm, partly because of the relative
ease with which we can automatically create an ac-
ceptable translation engine from a bitext. As a mat-
ter of fact, deriving such an engine from a bitext con-
sists in (more or less) gluing together dedicated soft-
ware modules, often freely available. Word-based
models, or the so-called IBM models, can be trained
using the GIZA or GIZA++ toolkits (Och and Ney,
2000). One can then train phrase-based models us-
ing the THOT toolkit (Ortiz-Mart??nez et al, 2005).
For their part, language models currently in use in
SMT systems can be trained using packages such as
SRILM (Stolcke, 2002) and the CMU-SLM toolkit
(Clarkson and Rosenfeld, 1997).
1www.statmt.org/wmt06/shared-task/
baseline.html
Once all the models are built, one can choose
to use PHARAOH (Koehn, 2004), an efficient full-
fledged phrase-based decoder. We only know of
one major drawback when using PHARAOH: its
licensing policy. Indeed, it is available for non-
commercial use in its binary form only. This
severely limits its use, both commercially and sci-
entifically (Walker, 2005).
For this reason, we undertook the design of a
generic architecture called MOOD (Modular Object-
Oriented Decoder), especially suited for instantiat-
ing SMT decoders. Two major goals directed our
design of this package: offering open source, state-
of-the-art decoders and providing an architecture to
easily build these decoders. This effort is described
in (Patry et al, 2006).
As a proof of concept that our framework (MOOD)
is viable, we attempted to use its functionalities to
implement a clone of PHARAOH, based on the com-
prehensive user manual of the latter. This clone,
called RAMSES, is now part of the MOOD distribu-
tion, which can be downloaded freely from the page
http://smtmood.sourceforge.net.
We conducted a pair-to-pair comparison between
the two engines that we describe in this paper. We
provide an overview of the MOOD architecture in
Section 2. Then we describe briefly RAMSES in Sec-
tion 3. The comparison between the two decoders in
terms of automatic metrics is analyzed in Section 4.
We confirm this comparison by presenting a man-
ual evaluation we conducted on an random sample
of the translations produced by both decoders. This
is reported in Section 5. We conclude in Section 6.
126
2 The MOOD Framework
A decoder must implement a specific combination of
two elements: a model representation and a search
space exploration strategy. MOOD is a framework
designed precisely to allow such a combination, by
clearly separating its two elements. The design of
the framework is described in (Patry et al, 2006).
MOOD is implemented with the C++ program-
ming language and is licensed under the Gnu Gen-
eral Public License (GPL)2. This license grants the
right to anybody to use, modify and distribute the
program and its source code, provided that any mod-
ified version be licensed under the GPL as well.
As explained in (Walker, 2005), this kind of license
stimulates new ideas and research.
3 MOOD at work: RAMSES
As we said above, in order to test our design, we
reproduced the most popular phrase-based decoder,
PHARAOH (Koehn, 2004), by following as faithfully
as possible its detailed user manual. The command-
line syntax RAMSES recognizes is compatible with
that of PHARAOH. The output produced by both
decoders are compatible as well and RAMSES can
also output its n-best lists in the same format as
PHARAOH does, i.e. in a format that the CARMEL
toolkit can parse (Knight and Al-Onaizan, 1999).
Switching decoders is therefore straightforward.
4 RAMSES versus PHARAOH
To compare the translation performances of both
decoders in a meaningful manner, RAMSES and
PHARAOH were given the exact same language
model and translation table for each translation ex-
periment. Both models were produced with the
scripts provided by the organizers. This means in
practice that the language model was trained using
the SRILM toolkit (Stolcke, 2002). The word align-
ment required to build the phrase table was pro-
duced with the GIZA++ package. A Viterbi align-
ment computed from an IBM model 4 (Brown et al,
1993) was computed for each translation direction.
Both alignments were then combined in a heuristic
way (Koehn et al, ). Each pair of phrases in the
2http://www.gnu.org/copyleft/gpl.html
model is given 5 scores, described in the PHARAOH
training manual.3
To tune the coefficients of the log-linear
combination that both PHARAOH and RAMSES
use when decoding, we used the organizers?
minimum-error-rate-training.perl
script. This tuning step was performed on the
first 500 sentences of the dedicated development
corpora. Inevitably, RAMSES differs slightly
from PHARAOH, because of some undocumented
embedded heuristics. Thus, we found appropriate
to tune each decoder separately (although with
the same material). In effect, each decoder does
slightly better (with BLEU) when it uses its own best
parameters obtained from tuning, than when it uses
the parameters of its counterpart.
Eight coefficents were adjusted this way: five for
the translation table (one for each score associated
to each pair of phrases), and one for each of the fol-
lowing models: the language model, the so-called
word penalty model and the distortion model (word
reordering model). Each parameter is given a start-
ing value and a range within which it is allowed to
vary. For instance, the language model coefficient?s
starting value is 1.0 and the coefficient is in the range
[0.5?1.5]. Eventually, we obtained two optimal con-
figurations (one for each decoder) with which we
translated the TEST material.
We evaluated the translations produced by both
decoders with the organizers? multi-bleu.perl
script, which computes a BLEU score (and displays
the n-gram precisions and brevity penalty used). We
report the scores we gathered on the test corpus of
2000 pairs of sentences in Table 1. Overall, both
decoders offer similar performances, down to the
n-gram precisions. To assess the statistical signifi-
cance of the observed differences in BLEU, we used
the bootstrapping technique described in (Zhang
and Vogel, 2004), randomly selecting 500 sentences
from each test set, 1000 times. Using a 95% con-
fidence interval, we determined that the small dif-
ferences between the two decoders are not statis-
tically significant, except for two tests. For the
direction English to French, RAMSES outperforms
PHARAOH, while in the German to English direc-
3http://www.statmt.org/wmt06/
shared-task/training-release-1.3.tgz
127
tion, PHARAOH is better. Whenever a decoder is
better than the other, Table 1 shows that it is at-
tributable to higher n-gram precisions; not to the
brevity penalty.
We further investigated these two cases by calcu-
lating BLEU for subsets of the test corpus sharing
similar sentence lengths (Table 2). We see that both
decoders have similar performances on short sen-
tences, but can differ by as much as 1% in BLEU on
longer ones. In contrast, on the Spanish-to-English
translation direction, where the two decoders offer
similar performances, the difference between BLEU
scores never exceeds 0.23%.
Expectedly, Spanish and French are much easier
to translate than German. This is because, in this
study, we did not apply any pre-processing strat-
egy that we know can improve performances, such
as clause reordering or compound-word splitting
(Collins et al, 2005; Langlais et al, 2005).
Table 2 shows that it does not seem much more
difficult to translate into English than from English.
This is surprising: translating into a morphologically
richer language should be more challenging. The
opposite is true for German here: without doing any-
thing specific for this language, it is much easier to
translate from German to English than the other way
around. This may be attributed in part to the lan-
guage model: for the test corpus, the perplexity of
the language models provided is 105.5 for German,
compared to 59.7 for English.
5 Human Evaluation
In an effort to correlate the objective metrics with
human reviews, we undertook the blind evaluation
of a sample of 100 pairwise translations for the three
Foreign language-to-English translation tasks. The
pairs were randomly selected from the 3064 trans-
lations produced by each engine. They had to be
different for each decoder and be no more than 25
words long.
Each evaluator was presented with a source sen-
tence, its reference translation and the translation
produced by each decoder. The last two were in ran-
dom order, so the evaluator did not know which en-
gine produced the translation. The evaluator?s task
was two-fold. (1) He decided whether one transla-
tion was better than the other. (2) If he replied ?yes?
D BLEU p1 p2 p3 p4 BP
es ? en
P 30.65 64.10 36.52 23.70 15.91 1.00
R 30.48 64.08 36.30 23.52 15.76 1.00
fr ? en
P 30.42 64.28 36.45 23.39 15.64 1.00
R 30.43 64.58 36.59 23.54 15.73 0.99
de ? en
P 25.15 61.19 31.32 18.53 11.61 0.99
R 24.49 61.06 30.75 17.73 10.81 1.00
en ? es
P 29.40 61.86 35.32 22.77 15.02 1.00
R 28.75 62.23 35.03 22.32 14.58 0.99
en ? fr
P 30.96 61.10 36.56 24.49 16.80 1.00
R 31.79 61.57 37.38 25.30 17.53 1.00
en ? de
P 18.03 52.77 22.70 12.45 7.25 0.99
R 18.14 53.38 23.15 12.75 7.47 0.98
Table 1: Performance of RAMSES and PHARAOH
on the provided test set of 2000 pairs of sentences
per language pair. P stands for PHARAOH, R for
RAMSES. All scores are percentages. pn is the n-
gram precision and BP is the brevity penalty used
when computing BLEU.
in test (1), he stated whether the best translation was
satisfactory while the other was not. Two evalua-
tors went through the 3 ? 100 sentence pairs. None
of them understands German; subject B understands
Spanish, and both understand French and English.
The results of this informal, yet informative exercise
are reported in Table 3.
Overall, in many cases (64% and 48% for subject
A and B respectively), the evaluators did not pre-
fer one translation over the other. On the Spanish-
and French-to-English tasks, both subjects slightly
preferred the translations produced by RAMSES. In
about one fourth of the cases where one translation
was preferred did the evaluators actually flag the se-
lected translation as significantly better.
6 Discussion
We presented a pairwise comparison of two de-
coders, RAMSES and PHARAOH. Although RAM-
SES is roughly twice as slow as PHARAOH, both de-
128
Test set [0,15] [16,25] [26,?[
en ? fr (P) 33.52 30.65 30.39
en ? fr (R) 33.78 31.19 31.35
de ? en (P) 29.74 24.30 24.76
de ? en (R) 29.85 23.92 23.78
es ? en (P) 34.23 28.32 30.60
es ? en (R) 34.46 28.39 30.40
Table 2: BLEU scores on subsets of the test corpus
filtered by sentence length ([min words, max words]
intervals), for Pharaoh and Ramses.
Preferred Improved
P R No P R
es ? en
subject A 13 16 71 6 1
subject B 23 31 46 3 8
fr ? en
subject A 18 19 63 5 3
subject B 20 21 59 8 8
de ? en
subject A 24 18 58 5 9
subject B 30 31 39 3 3
Total 128 136 336 30 32
Table 3: Human evaluation figures. The column
Preferred indicates the preference of the subject
(Pharaoh, Ramses or No preference). The column
Improved shows when a subject did prefer a trans-
lation and also said that the preferred translation was
correct while the other one was not.
coders offer comparable performances, according to
automatic and informal human evaluations.
Moreover, RAMSES is the product of clean frame-
work: MOOD, a solid tool for research projects. Its
code is open source and the architecture is modular,
making it easier for researchers to experiment with
SMT. We hope that the availability of the source
code and the clean design of MOOD will make it a
useful platform to implement new decoders.
Acknowledgments
We warmly thanks Elliott Macklovitch for his par-
ticipation in the manual annotation task. This work
has been partially funded by an NSERC grant.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L.
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, 19(2):263?311.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the CMU-cambridge toolkit. In Proc.
of Eurospeech, pages 2707?2710, Rhodes, Greece.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of the 43rd ACL, pages 531?540, Ann Arbor, MI.
K. Knight and Y. Al-Onaizan, 1999. A Primer on
Finite-State Software for Natural Language Process-
ing. www.isi.edu/licensed-sw/carmel.
P. Koehn, F. Joseph Och, and D. Marcu. Statistical
Phrase-Based Translation. In Proc. of HLT, Edmon-
ton, Canada.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder for
Phrase-Based SMT. In Proc. of the 6th AMTA, pages
115?124, Washington, DC.
P. Langlais, G. Cao, and F. Gotti. 2005. RALI: SMT
shared task system description. In 2nd ACL workshop
on Building and Using Parallel Texts, pages 137?140,
Ann Arbor, MI.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proc. of ACL, pages 440?447,
Hongkong, China.
D. Ortiz-Mart??nez, I. Garcia?-Varea, and F. Casacuberta.
2005. Thot: a toolkit to train phrase-based statistical
translation models. In Proc. of MT Summit X, pages
141?148, Phuket, Thailand.
A. Patry, F. Gotti, and P. Langlais. 2006. MOOD
a modular object-oriented decoder for statistical ma-
chine translation. In Proc. of LREC, Genoa, Italy.
A. Stolcke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP, Denver, USA.
D.J. Walker. 2005. The open ?a.i.? kitTM: General ma-
chine learning modules from statistical machine trans-
lation. In Workshop of MT Summit X, ?Open-Source
Machine Translation?, Phuket, Thailand.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proc. of the 10th TMI, Baltimore, MD.
129
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 103?109,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RALI Machine Translation System for WMT 2010
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry and Philippe Langlais
RALI - Universite? de Montre?al
C.P. 6128, succursale Centre-ville
H3C 3J7, Montre?al, Que?bec, Canada
{huetstep,bourdaij,patryale,felipe}@iro.umontreal.ca
Abstract
We describe our system for the translation
task of WMT 2010. This system, devel-
oped for the English-French and French-
English directions, is based on Moses and
was trained using only the resources sup-
plied for the workshop. We report exper-
iments to enhance it with out-of-domain
parallel corpora sub-sampling, N-best list
post-processing and a French grammatical
checker.
1 Introduction
This paper presents the phrase-based machine
translation system developed at RALI in order
to participate in both the French-English and
English-French translation tasks. In these two
tasks, we used all the corpora supplied for the con-
straint data condition apart from the LDC Giga-
word corpora.
We describe its different components in Sec-
tion 2. Section 3 reports our experiments to sub-
sample the available out-of-domain corpora in or-
der to adapt the translation models to the news
domain. Section 4, dedicated to post-processing,
presents how N-best lists are reranked and how the
French 1-best output is corrected by a grammatical
checker. Section 5 studies how the original source
language of news acts upon translation quality. We
conclude in Section 6.
2 System Architecture
2.1 Pre-processing
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated
French words starting with a capital letter. We
significantly cleaned up the parallel Giga word
corpus (noted as gw hereafter), keeping 18.1 M
of the original 22.5 M sentence pairs. For exam-
ple, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with
capital letters were removed.
Moreover, training material was tokenized with
the tool provided for the workshop and truecased,
meaning that the words occuring after a strong
punctuation mark were lowercased when they be-
longed to a dictionary of common all-lowercased
forms; the others were left unchanged. In order
to reduce the number of words unknown to the
translation models, all numbers were serialized,
i.e. mapped to a special unique token. The origi-
nal numbers are then placed back in the translation
in the same order as they appear in the source sen-
tence. Since translations are mostly monotonic be-
tween French and English, this simple algorithm
works well most of the time.
2.2 Language Models
We trained Kneser-Ney discounted 5-gram lan-
guage models (LMs) on each available corpus us-
ing the SRILM toolkit (Stolcke, 2002). These
LMs were combined through linear interpola-
tion: first, an out-of-domain LM was built from
Europarl, UN and gw; then, this model was
combined with the two in-domain LMs trained
on news-commentary and news.shuffled, which
will be referred to as nc and ns in the remainder
of the article. Weights were fixed by optimizing
the perplexity of a development corpus made of
news-test2008 and news-syscomb2009 texts.
In order to reduce the size of the LMs, we
limited the vocabulary of our models to 1 M
words for English and French. The words of
these vocabularies were selected from the com-
putation of the number of their occurences us-
ing the method proposed by Venkataraman and
Wang (2003). The out-of-vocabulary rate mea-
sured on news-test2009 and news-test2010
with a so-built vocabulary varies between 0.6 %
103
and 0.8 % for both English and French, while it
was between 0.4 % and 0.7 % before the vocabu-
lary was pruned.
To train the LM on the 48 M-sentence English
ns corpus, 32 Gb RAM were required and up to
16 Gb RAM, for the other corpora. To reduce the
memory needs during decoding, LMs were pruned
using the SRILM prune option.
2.3 Alignment and Translation Models
All parallel corpora were aligned with
Giza++ (Och and Ney, 2003). Our transla-
tion models are phrase-based models (PBMs)
built with Moses (Koehn et al, 2007) with the
following non-default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized
reordering model scores were optimized on the
development corpus thanks to the MERT algo-
rithm (Och, 2003).
2.4 Experiments
This section reports experiments done on the
news-test2009 corpus for testing various config-
urations. In these first experiments, we trained
LMs and translation models on the Europarl cor-
pus.
Case We tested two methods to handle case. The
first one lowercases all training data and docu-
ments to translate, while the second one normal-
izes all training data and documents into their nat-
ural case. These two methods require a post-
processing recapitalization but this last step is
more basic for the truecase method. Training mod-
els on lowercased material led to a 23.15 % case-
insensitive BLEU and a 21.61 % case-sensitive
BLEU; from truecased corpora, we obtained a
23.24 % case-insensitive BLEU and a 22.13 %
case-sensitive BLEU. As truecasing induces an in-
crease of the two metrics, we built all our mod-
els in truecase. The results shown in the remain-
der of this paper are reported in terms of case-
insensitive BLEU which showed last year a bet-
ter correlation with human judgments than case-
sensitive BLEU for the two languages we con-
sider (Callison-Burch et al, 2009).
Tokenization Two tokenizers were tested: one
provided for the workshop and another we devel-
oped. They differ mainly in the processing of com-
pound words: our in-house tokenizer splits these
words (e.g. percentage-wise is turned into percent-
age - wise), which improves the lexical coverage of
the models trained on the corpus. This feature
does not exist in the WMT tool. However, us-
ing the WMT tokenizer, we measured a 23.24 %
BLEU, while our in-house tokenizer yielded a
lower BLEU of 22.85 %. Follow these results
prompted us to use the WMT tokenizer.
Serialization In order to test the effect of se-
rialization, i.e. the mapping of all numbers to
a special unique token, we measured the BLEU
score obtained by a PBM trained on Europarl for
English-French, when numbers are left unchanged
(Table 1, line 1) or serialized (line 2). These
results exhibit a slight decrease of BLEU when
serialization is performed. Moreover, if BLEU
is computed using a serialized reference (line 3),
which is equivalent to ignoring deserialization er-
rors, a minor gain of BLEU is observed, which
validates our recovering method. Since resorting
to serialization/deserialization yields comparable
performance to a system not using it, while reduc-
ing the model?s size, we chose to use it.
BLEU
no serialization 23.24
corpus serialization 23.13
corpus and reference serialization 23.27
Table 1: BLEU measured for English-French on
news-test2009 when training on Europarl.
LM Table 2 reports the perplexity measured on
news-test2009 for French (column 1) and En-
glish (column 3) LMs learned on different cor-
pora and interpolated using the development cor-
pus. We also provide the BLEU score (column 2)
for English-French obtained from translation mod-
els trained on Europarl and nc. As expected, us-
ing in-domain corpora (line 2) for English-French
led to better results than using out-of-domain data
(line 3). The best perplexities and BLEU score
are obtained when LMs trained on all the available
corpora are combined (line 4). The last three lines
exhibit how LMs perform when they are trained on
in-domain corpora without pruning them. While
the gzipped 5-gram LM (last line) obtained in
104
such a manner occupies 1.4 Gb on hard disk, the
gzipped pruned 5-gram LM (line 4) trained using
all corpora occupies 0.9 Gb and yields the same
BLEU score. This last LM was used in all the ex-
periments reported in the subsequent sections.
corpora
Fr En
ppl BLEU ppl
nc 327 22.44 454
nc + ns 125 25.69 166
Europarl + UN + Gw 156 24.91 225
all corpora 113 26.01 151
nc + ns (3g, unpruned) 138 25.32 -
nc + ns (4g, unpruned) 124 25.86 -
nc + ns (5g, unpruned) 120 26.04 -
Table 2: LMs perplexities and BLEU scores mea-
sured on news-test2009. Translation models
used here were trained on nc and Europarl.
3 Domain adaptation
As the only news parallel corpus provided for
the workshop contains 85k sentence pairs, we
must resort to other parallel out-of-domain cor-
pora in order to build reliable translation models.
If in-domain and out-of-domain LMs are usually
mixed with the well-studied interpolation tech-
niques, training translation models from data of
different domains has received less attention (Fos-
ter and Kuhn, 2007; Bertoldi and Federico, 2009).
Therefore, there is still no widely accepted tech-
nique for this last purpose.
3.1 Effects of the training data size
We investigated how increasing training data acts
upon BLEU score. Table 3 shows a high increase
of 2.7 points w.r.t. the use of nc alone (line 1)
when building the phrase table and the reordering
model from nc and either the 1.7 M-sentence-pair
Europarl (line 2) or a 1.7 M-sentence-pair cor-
pus extracted from the 3 out-of-domain corpora:
Europarl, UN and Gw (line 3). Training a PBM on
merged parallel corpora is not necessarily the best
way to combine data from different domains. We
repeated 20 times nc before adding it to Europarl
so as to have the same amount of out-of-domain
and in-domain material. This method turned out
to be less successful since it led to a minor 0.15
BLEU decrease (line 4) w.r.t. our previous system.
Following the motto ?no data is better than more
corpora En?Fr Fr?En
nc 23.29 23.23
nc + Europarl 26.01 -
nc + 1.7 M random pairs 26.02 26.68
20?nc + Europarl 25.86 -
nc + 8.7 M pairs (part 0) 26.44 27.65
nc + 8.7 M pairs (part 1) 26.68 27.46
nc + 8.7 M pairs (part 2) 26.54 27.50
3 models merged 26.86 27.56
Table 3: BLEU (in %) measured on news-
test2009 for English-French and French-English
when translations models and lexicalized reorder-
ing models are built using various amount of data
in addition to nc.
data?, a PBM was built using all the parallel cor-
pora at our disposal. Since the overall parallel sen-
tences were too numerous for our computational
resources to be simultaneously used, we randomly
split out-of-domain corpora into 3 parts of 8.7 M
sentence pairs each and then combined them with
nc. PBMs were trained on each of these parts
(lines 5 to 7), which yields respectively 0.5 and
0.8 BLEU gain for English-French and French-
English w.r.t. the use of 1.7 M out-of-domain sen-
tence pairs. The more significant improvement no-
ticed for the French-English direction is probably
explained by the fact that the French language is
morphologically richer than English. The 3 PBMs
were then combined by merging the 3 phrase ta-
bles. To do so, the 5 phrase table scores computed
by Moses were mixed using the geometric average
and a 6th score was added, which counts the num-
ber of phrase tables where the given phrase pair
occurs. We ended up with a phrase table contain-
ing 623 M entries, only 9 % and 4 % of them being
in 2 and 3 tables respectively. The resulting phrase
table led to a slight improvement of BLEU scores
(last line) w.r.t. the previous models, except for the
model trained on part 0 for French-English.
3.2 Corpus sub-sampling
Whereas using all corpora improves translation
quality, it requires a huge amount of memory and
disk space. We investigate in this section ways to
select sentence pairs among large out-of-domain
corpora.
Unknown words The main interest of adding
new training material relies on the finding of
words missing in the phrase table. According to
105
this principle, nc was extended with new sentence
pairs containing an unknown word (Table 4, line 2)
or a word that belongs to our LM vocabulary and
that occurs less than 3 times in the current cor-
pus (line 3). This resulted in adding 400 k pairs
in the first case and 950 k in the second one, with
BLEU scores close or even better than those ob-
tained with 1.7 M.
corpora En?Fr Fr?En
nc + 1.7 M random pairs 26.02 26.68
nc + 400k pairs (occ = 1) 25.67 -
nc + 950k pairs (occ = 3) 26.13 -
nc + Joshua sub-sampling 26.98 27.68
nc + IR (1-g q, w/ repet) 25.81 -
nc + IR (1-g q, no repet) 26.56 27.54
nc + IR (1,2-g q, w/ repet) 26.26 -
nc + IR (1,2-g q, no repet) 26.53 -
nc + 8.7 M pairs 26.68 27.65
+ IR score (1g q, no repet) 26.93 27.65
3 large models merged 26.86 27.56
+ IR score (1g q, no repet) 26.98 27.74
Table 4: BLEU measured on news-test2009 for
English-French and French-English using transla-
tion models trained on nc and a subset of out-of-
domain corpora.
Unknown n-grams We applied the sub-
sampling method available in the Joshua
toolkit (Li et al, 2009). This method adds a
new sentence pair when it contains new n-grams
(with 1 ? n ? 12) occurring less than 20 times in
the current corpus, which led us to add 1.5 M pairs
for English-French and 1.4 M for French-English.
A significant improvement of BLEU is observed
using this method (0.8 for English-French and
1.0 for French-English) w.r.t. the use of 1.7 M
randomly selected pairs. However, this method
has the major drawback of needing to build a new
phrase table for each document to translate.
Information retrieval Information retrieval
(IR) methods have been used in the past to sub-
sample parallel corpora (Hildebrand et al, 2005;
Lu? et al, 2007). These studies use sentences
belonging to the development and test corpora as
queries to select the k most similar source sen-
tences in an indexed parallel corpus. The retrieved
sentence pairs constitute a training corpus for
the translation models. In order to alleviate the
fact that a new PBM has to be learned for each
new test corpus, we built queries using sentences
contained in the monolingual ns corpus, leading
to the selection of sentence pairs stylistically
close to those in the news domain. The source
sentences of the three out-of-domain corpora
were indexed using Lemur.1 Two types of queries
were built from ns sentences after removing stop
words: the first one is limited to unigrams, the
second one contains both unigrams and bigrams,
with a weight for bigrams twice as high as for
unigrams. The interest of the latter query type is
based on the hypothesis that bigrams are more
domain-dependent than unigrams. Another choice
that needs to be made when using IR methods is
concerning the retention of redundant sentences
in the final corpus.
Lines 5 to 8 of Table 4 show the results obtained
when sentence pairs were gathered up to the size
of Europarl, i.e. 1.7 M pairs. 10 sentences were
retrieved per query in various configurations: with
or without bigrams inside queries, with or without
duplicate sentence pairs in the training corpus. Re-
sults demonstrate the interest of the approach since
the BLEU scores are close to those obtained us-
ing the previous tested method based on n-grams
of the test data. Taking bigrams into account does
not improve results and adding only once new sen-
tences is more relevant than duplicating them.
Since using all data led to even better perfor-
mances (see last line of Table 3), we used infor-
mation provided by the IR method in the PBMs
trained on nc + 8.7 M out-of-domain sentence
pairs or taking into account all the training ma-
terial. To this end, we included a new score in
the phrase tables which is fixed to 1 for entries
that are in the phrase table trained on sentences
retrieved with unigram queries without repetition
(see line 6 of Table 4), and 0 otherwise. Therefore,
this score aims at boosting the weight of phrases
that were found in sentences close to the news do-
main. The results reported in the 4 last lines of Ta-
ble 4 show minor but consistent gains when adding
this score. The outputs of the PBMs trained on
all the training corpus and which obtained the best
BLEU scores on news-test2009 were submitted
as contrastive runs. The two first lines of Table 5
report the results on this years?s test data, when
the score related to the retrieved corpus is incor-
porated or not. These results still exhibit a minor
improvement when adding this score.
1www.lemurproject.org
106
En?Fr Fr?En
BLEU BLEU-cased TER BLEU BLEU-cased TER
PBM 27.5 26.5 62.2 27.8 26.9 61.2
+IR score 27.7 26.6 62.1 28.0 27.0 61.0
+N-best list reranking 27.9 26.8 62.1 28.0 27.0 61.2
+grammatical checker 28.0 26.9 62.0 - - -
Table 5: Official results of our system on news-test2010.
4 Post-processing
4.1 N-best List Reranking
Our best PBM enhanced by IR methods was em-
ployed to generate 500-best lists. These lists were
reranked combining the global decoder score with
the length ratio between source and target sen-
tences, and the proportions of source sentence n-
grams that are in the news monolingual corpora
(with 1 ? n ? 5). Weights of these 7 scores are
optimized via MERT on news-test2009. Lines 2
and 3 of Table 5 provide the results obtained be-
fore and after N-best list reranking. They show a
tiny gain for all metrics for English-French, while
the results remain constant for French-English.
Nevertheless, we decided to use those translations
for the French-English task as our primary run.
4.2 Grammatical Checker
PBM outputs contain a significant number of
grammatical errors, even when LMs are trained
on large data sets. We tested the use of a gram-
matical checker for the French language: Antidote
RX distributed by Druide informatique inc.2 This
software was applied in a systematic way on the
first translation generated after N-best reranking.
Thus, as soon as the software suggests one or sev-
eral choices that it considers as more correct than
the original translation, the first proposal is kept.
The checked translation is our first run for English-
French.
Antidote RX changed at least one word in
26 % of the news-test2010 sentences. The most
frequent type of corrections are agreement errors,
like in the following example where the agreement
between the subject nombre (number) is correctly
made with the adjective coupe? (cut), thanks to the
full syntactic parsing of the French sentence.
Source: [...] the number of revaccinations could then be
cut [...]
Reranking: [...] le nombre de revaccinations pourrait
2www.druide.com
alors e?tre coupe?es [...]
+Grammatical checker: [...] le nombre de revacci-
nations pourrait alors e?tre coupe? [...]
The example below exhibits a good decision
made by the grammatical checker on the mood of
the French verb e?tre (to be).
Source: It will be a long time before anything else will be
on offer in Iraq.
Reranking: Il faudra beaucoup de temps avant que tout
le reste sera offert en Irak.
+Grammatical checker: Il faudra beaucoup de temps
avant que tout le reste soit offert en Irak.
A last interesting type of corrected errors con-
cerns negation. Antidote has indeed the capacity
to add the French particle ne when it is missing in
the expressions ne ... pas, ne ... plus, aucun ne, per-
sonne ne or rien ne. The results obtained using the
grammatical checker are reported in the last line
of Table 5. The automatic evaluation shows only a
minor improvement but we expect the changes in-
duced by this tool to be more significant for human
annotators.
5 Effects of the Original Source
Language of Articles on Translation
During our experiments, we found that translation
quality is highly variable depending on the origi-
nal source language of the news sentences. This
phenomenon is correlated to the previous work of
Kurokawa et al (2009) that showed that whether
or not a piece of text is an original or a trans-
lation has an impact on translation performance.
The main reason that explains our observations
is probably that the topics and the vocabulary of
news originally expressed in languages other than
French and English tend to differ more from those
of the training materials used to train PBM mod-
els for these two languages. In order to take into
account this phenomenon, MERT tuning was re-
peated for each original source language, using the
107
same PBM models trained on all parallel corpora
and incorporating an IR score.
Columns 1 and 3 of Table 5 display the BLEU
measured using our previous global MERT op-
timization made on 2553 sentence pairs, while
columns 2 and 4 show the results obtained when
running MERT on subsets of the development ma-
terial, made of around 700 sentence pairs each.
The BLEU measured on the whole 2010 test set
is reported in the last line. As expected, language-
dependent MERT tends to increase the LM weight
for English and French. However, an absolute
0.35 % BLEU decrease is globally observed for
English-French using this approach and a 0.21 %
improvement for French-English.
En?Fr Fr?En
MERT global lang dep global lang dep
Cz 21.95 21.45 21.84 21.85
En 30.80 29.84 33.73 35.00
Fr 37.59 36.96 31.59 32.62
De 16.60 16.73 17.41 17.76
Es 24.52 24.45 29.25 28.31
total 27.64 27.39 27.99 28.20
Table 6: BLEU scores measured on parts of
news-test2010 according to the original source
language.
6 Conclusion
This paper presented our statistical machine trans-
lation system developed for the translation task us-
ing Moses. Our submitted runs were generated
from models trained on all the corpora made avail-
able for the workshop, as this method had pro-
vided the best results in our experiments. This
system was enhanced using IR methods which
exploits news monolingual copora, N-best list
reranking and a French grammatical checker.
This was our first participation where such a
huge amount data was involved. Training models
on so many sentences is challenging from an engi-
neering point of view and requires important com-
putational resources and storage capacities. The
time spent in handling voluminous data prevented
us from testing more approaches. We suggest that
the next edition of the workshop could integrate
a task restraining the number of parameters in the
models trained.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In 4th EACL Workshop
on Statistical Machine Translation (WMT), Athens,
Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In 2nd ACL Workshop
on Statistical Machine Translation (WMT), Prague,
Czech Republic.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In 10th conference
of the European Association for Machine Transla-
tion (EAMT), Budapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics (ACL), Companion Vol-
ume, Prague, Czech Republic.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and
its impact on machine translation. In 12th Machine
Translation Summit, Ottawa, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Join Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague, Czech Repub-
lic.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Sapporo, Japan.
108
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing (ICSLP),
Denver, CO, USA.
Arnand Venkataraman and Wen Wang. 2003. Tech-
niques for effective vocabulary selection. In 8th Eu-
ropean Conference on Speech Communication and
Technology (Eurospeech), Geneva, Switzerland.
109
Identifying Parallel Documents from a Large Bilingual Collection of Texts:
Application to Parallel Article Extraction in Wikipedia.
Alexandre Patry
KeaText
845, Boulevard Dcarie, bureau 202
Saint-Laurent, Canada H4L 3L7
alexandre.patry@keatext.com
Philippe Langlais
DIRO/RALI
Universite? de Montre?al
Montre?al, Canada H3C3J7
felipe@iro.umontreal.ca
Abstract
While several recent works on dealing with
large bilingual collections of texts, e.g. (Smith
et al, 2010), seek for extracting parallel sen-
tences from comparable corpora, we present
PARADOCS, a system designed to recognize
pairs of parallel documents in a (large) bilin-
gual collection of texts. We show that this
system outperforms a fair baseline (Enright
and Kondrak, 2007) in a number of con-
trolled tasks. We applied it on the French-
English cross-language linked article pairs of
Wikipedia in order see whether parallel ar-
ticles in this resource are available, and if
our system is able to locate them. Accord-
ing to some manual evaluation we conducted,
a fourth of the article pairs in Wikipedia are
indeed in translation relation, and PARADOCS
identifies parallel or noisy parallel article pairs
with a precision of 80%.
1 Introduction
There is a growing interest within the Machine
Translation (MT) community to investigate compa-
rable corpora. The idea that they are available in
a much larger quantity certainly contributes to fos-
ter this interest. Still, parallel corpora are playing
a crucial role in MT. This is therefore not surprising
that the number of bitexts available to the commu-
nity is increasing.
Callison-Burch et al (2009) mined from institu-
tional websites the 109 word parallel corpus1 which
gathers 22 million pairs of (likely parallel) French-
English sentences. Tiedemann (2009) created the
1http://www.statmt.org/wmt10
Opus corpus,2 an open source parallel corpus gath-
ering texts of various sources, in several languages
pairs. This is an ongoing effort currently gathering
more than 13 Gigabytes of compressed files. The
Europarl corpus3 (Koehn, 2005) gathers no less than
2 Gigabytes of compressed documents in 20 lan-
guage pairs. Some other bitexts are more marginal in
nature. For instance, the novel 1984 of George Or-
wel has been organized into an English-Norvegian
bitext (Erjavec, 2004) and Beyaz Kale of Orhan Pa-
muk as well as Sofies Verden of Jostein Gaardner
are available for the Swedish-Turk language pair
(Megyesi et al, 2006).
A growing number of studies investigate the ex-
traction of near parallel material (mostly sentences)
from comparable data. Among them, Munteanu et
al. (2004) demonstrate that a classifier can be trained
to recognize parallel sentences in comparable cor-
pora mined from news collections. A number of
related studies (see section 5) have also been pro-
posed; some of them seeking to extract parallel sen-
tences from cross-language linked article pairs in
Wikipedia4 (Adafre and de Rijke, 2006; Smith
et al, 2010). None of these studies addresses specif-
ically the issue of discovering parallel pairs of arti-
cles in Wikipedia.
In this paper, we describe PARADOCS, a system
capable of mining parallel documents in a collec-
tion, based on lightweight content-based features ex-
tracted from the documents. On the contrary to other
systems designed to target parallel corpora (Chen
2http://opus.lingfil.uu.se/
3http://www.statmt.org/europarl/
4http://fr.wikipedia.org/
87
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 87?95,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
and Nie, 2000; Resnik and Smith, 2003), we do
not assume any specific naming conventions on file-
names or URLs.
The reminder of this article is organized as fol-
lows. In the next section, we describe our ap-
proach to mining parallel documents in a bilingual
collection of texts. We test our approach on the
Europarl corpus in section 3. We present in sec-
tion 4 the application of our system to a subpart
of the French-English articles of Wikipedia. We
describe related work in section 5, summarize our
work in section 6 and present future works in sec-
tion 7.
2 PARADOCS
In order to identify pairs of parallel documents in a
bilingual collection of texts, we designed a system,
named PARADOCS, which is making as few assump-
tions as possible on the language pair being consid-
ered, while still making use of the content of the doc-
uments in the collection. Our system is built on three
lightweight components. The first one searches for
target documents that are more likely parallel to a
given source document (section 2.1). The second
component classifies (candidate) pairs of documents
as parallel or not (section 2.2). The third component
is designed to filter out some (wrongly) recognized
parallel pairs, making use of collection-level infor-
mation (section 2.3).
2.1 Searching Candidate Pairs
In a collection containing n documents in a given
language, and m in another one, scoring each of the
n?m potential pairs of source-target documents be-
comes rapidly intractable. In our approach, we re-
sort to an information retrieval system in order to
select the target documents that are most likely par-
allel to a given source one. In order to do so, we
index target documents t in the collection thanks to
an indexing strategy ? that will be described shortly.
Then, for a source document s, we first index it, that
is, we compute ?(s), and query the retrieval engine
with ?(s), which in turn returns the N most simi-
lar target documents found in the collection. In our
experiments, we used the Lucene5 retrieval library.
5http://lucene.apache.org
We tested two indexing strategies: one reduces a
document to the sequence of hapax words it contains
(? ?hap), the other one reduces it to its sequence
of numerical entities (? ?num). Hapax words have
been found very useful in identifying parallel pairs
of documents (Enright and Kondrak, 2007) as well
as for word-aligning bitexts (Lardilleux and Lep-
age, 2007). Following Enright and Kondrak (2007),
we define hapax words as blank separated strings of
more than 4 characters that appear only once in the
document being indexed. Also, we define a numer-
ical entity as a blank separated form containing at
least one digit. It is clear from this description that
our indexing strategies can easily be applied to many
different languages.
2.2 Identifying candidate pairs
Each candidate pair delivered by Lucene, is classi-
fied as parallel or not by a classifier trained in a su-
pervised way to recognize parallel documents. Here
again, we want our classifier to be as agnostic as
possible to the pair of languages considered. This
is why we adopted very light feature extractors ?
which are built on three types of entities in docu-
ments: numerical entities (? ?num), hapax words
(? ?hap) and punctuation marks6 (? ?punc). For
each sequence of entities ?(s) and ?(t) of a source
document s and a target document t respectively, we
compute the three following features:
? the normalized edit-distance between the two
representations:
? = ed(?(s), ?(t))/max(|?(s)|, |?(t)|)
where |?(d)| stands for the size of the sequence
of entities contained in d. Intuitively, ? gives
the proportion of entities shared across docu-
ments,
? the total number of entities in the representation
of both documents:
|?(s)|+ |?(t)|
We thought this information might complement
the one of ? which is relative to the document?s
sequence length.
6We only considered the 6 following punctuation marks that
are often preserved in translation: .!?():
88
? A binary feature which fires whenever the pair
of documents considered receives the smaller
edit-distance among all the pairs of documents
involving this source document:
?(s, t) ={ 1 if ed (?(s), ?(t)) ? ed (?(s), ?(t?)) ? t?
0 otherwise
Intuitively, the target document considered is
more likely the good one if it has with the
source document the smallest edit distance.
Since we do compute edit-distance for all the
candidate documents pairs, this feature comes
at no extra computational cost.
We compute these three features for each se-
quence of entities considered. For instance, if we
represent a document according to its sequence of
numerical entities and its hapax words, we do com-
pute a total of 6 features.7
It is fair to say that our feature extraction strat-
egy is very light. In particular, it does not capitalize
on an existing bilingual lexicon. Preliminary exper-
iments with features making use of such a lexicon
turned out to be less successful, due to issues in the
coverage of the lexicon (Patry and Langlais, 2005).
To create and put to the test our classifier, we used
the free software package Weka (Hall et al, 2009),
written in Java.8 This package allows the easy ex-
perimentation of numerous families of classifiers.
We investigated logistic regression (logit), naive
bayes models (bayes), adaboost (ada), as well as
decision tree learning (j48).
2.3 Post-treatments
The classifiers we trained label each pair of docu-
ments independently of other candidate pairs. This
independence assumption is obviously odd and leads
to situations where several target documents are
paired to a given source document and vice-versa.
Several solutions can be applied; we considered two
simple ones in this work. The first one, hereafter
named nop, consists in doing nothing; therefore
leaving potential duplicates source or target docu-
ments. The second solution, called dup, filters out
7We tried with less success to compute a single set of fea-
tures from a representation considering all entities.
8www.cs.waikato.ac.nz/ml/weka/
pairs sharing documents. Another solution we did
not implement would require to keep from the set of
pairs concerning a given source document the one
with the best score as computed by our classifier. We
leave this as future work.
3 Controlled Experiments
We checked the good behavior of PARADOCS
in a controlled experimental setting, using the
Europarl corpus. This corpus is organized into
bitexts, which means that we have a ground truth
against which we can evaluate our system.
3.1 Corpus
We downloaded version 5 of the Europarl cor-
pus.9 Approximatively 6 000 documents are avail-
able in 11 languages (including English), that is, we
have 6 000 bitexts in 10 language pairs where En-
glish is one of the languages. The average number
of sentences per document is 273. Some documents
contain problems (encoding problems, files ending
unexpectedly, etc.). We did not try to cope with this.
In order to measure how sensible our approach is
to the size of the documents, we considered several
slices of them (from 10 to 1000 sentences). 10
3.2 Protocol
We tested several experimental conditions, varying
the language pairs considered (en-da, -de, -el, -es,
-fi, -fr, -it, -nl, -pt and -sv) as well as the doc-
ument length (10, 20, 30, 50, 70, 100 and 1 000
sentences). We also tested several system configu-
rations, varying the indexing strategy (num, hap),
the entities used for representing documents (hap,
num, num+hap, num+punc), the classifier used
(logit, ada, bayes, and j48), as well as the
post-filtering strategy (nop, dup). This means that
we conducted no less than 4 480 experiments.
Because we know which documents are paral-
lel, we can compute precision (percentage of iden-
tified parallel pairs that are truly parallel) and recall
(percentage of true parallel pairs identified) for each
configuration.
9http://www.statmt.org/europarl
10We removed the first sentences of each document, since
they may contain titles or other information that may artificially
ease pairing.
89
Since our approach requires to train a classifier,
we resorted in this experiment to a 5-fold cross-
validation procedure where we trained our classifiers
on 4/5 of the corpus and tested on the remaining part.
The figures reported in the reminder of this section
are averaged over the 5 folds. Also, all configura-
tions tested in this section considered the N = 20
most similar target documents returned by the re-
trieval engine for each source document.
3.3 Results
3.3.1 Search errors
We first measured search errors observed during
step 1 of our system. There are actually two types
of errors: one when no document is returned by
Lucene (nodoc) and one when none of the target
documents returned by the retrieval engine are sanc-
tioned ones (nogood). Figure 1 shows both error
types for the Dutch-English language pair, as a func-
tion of the document length.11 Clearly, search errors
are more important when documents are short. Ap-
proximatively a tenth of the source documents of (at
most) 100 sentences do not receive by Lucene any
target document. For smaller documents, this hap-
pens for as much as a third of the documents. Also,
it is interesting to note that in approximatively 6% of
the cases where Lucene returns target documents,
the good one is not present. Obviously we pay the
prize of our lightweight indexation scheme. In or-
der to increase the recall of our system, nodoc er-
rors could be treated by employing an indexing strat-
egy which would use more complex features, such
as sufficiently rare words (possibly involving a key-
word test, e.g. tf.idf). This is left as future work.
3.3.2 Best System configuration
In order to determine the factors which influence
the most our system, we varied the language pairs
(10 values) and the length of the documents (7 val-
ues) and counted the number of times a given sys-
tem configuration obtained the best f-measure over
the 70 tests we conducted. We observed that most
of the time, the configurations recording the best
f-measure are those that exploit numerical entities
(both at indexing time and feature extraction time).
Actually, we observed that computing features on
11Similar figures have been observed for other language
pairs.
nb. of sent.
errors %
 10
 15
 20
 25
 30
 35
 40
 45
 8  16  32  64  128  256  512  1024
nodoc + nogoodnodoc
Figure 1: Percentage of Dutch documents for which
Lucene returns no English document (nodoc), or no
correct document (nodoc+nogood) as a function of the
document size counted in sentences.
hapax words or punctuation marks on top of nu-
merical entities do not help much. One possible
explanation is that often, and especially within the
Europarl corpus, hapax words correspond to nu-
merical entities. Also, we noted that frequently, the
wining configuration is the one embedding a logistic
regression classifier, tightly followed by the decision
tree learner.
3.3.3 Sensitivity to the language pair
We also tested the sensibility of our approach to
the language pair being considered. Apart from the
fact that the French-English pair was the easiest to
deal with, we did not notice strong differences in
performance among language pairs. For documents
of at most 100 sentences, the worst f-measure (0.93)
is observed for the Dutch/English language pair,
while the best one (0.95) is observed for the French-
English pair. Slightly larger differences were mea-
sured for short documents.
nb. of sent.
gain %
 0
 5
 10
 15
 20
 25
 30
 35
 40
 8  16  32  64  128  256  512  1024
dadeelesfifritnlptsv
Figure 2: Absolute gains of the best variant of our sys-
tem over the approach described by Enright and Kon-
drak (2007).
90
3.3.4 Sanity check
We conducted a last sanity check by comparing
our approach to the one of (Enright and Kondrak,
2007). This approach simply ranks the candidate
pairs in decreasing order of the number of hapax
words they share. The absolute gains of our ap-
proach over theirs are reported in Figure 2, as a
function of the document length and the language
pair considered. Our system systematically outper-
forms the hapax approach of (Enright and Kondrak,
2007) regardless of the length of the documents and
the language pairs considered. An average absolute
gain of 13.6% in f-measure is observed for long doc-
uments, while much larger gains are observed for
shorter ones. It has to be noted, that our approach
requires to train a classifier, which makes it poten-
tially less useful in some situations. Also, we used
the best of our system in this comparison.
4 Experiments with Wikipedia
Many articles in Wikipedia are available in
several languages. Often, they are explicitly
marked as linked across languages. For instance,
the English article [Text corpus] is linked to the
French one [Corpus], but they are not transla-
tion of each other, while the English article [De-
cline of the Roman Empire] and the French one
[De?clin de l?empire romain d?Occident] are paral-
lel.12
4.1 Resource
During summer 2009, we collected all French-
English cross-language linked articles from
Wikipedia. A very straightforward pre-
processing stage involving simple regular expres-
sions removed part of the markup specific to this
resource. We ended up with 537 067 articles in
each language. The average length of the English
pages is 711 words, while the average for French is
445 words. The difference in length among linked
articles has been studied by Filatova (2009) on a
small excerpt of bibliographical articles describing
48 persons listed in the biography generation task
(Task 5) of DUC 2004.13
12At least they were at the time of redaction.
13http://duc.nist.gov/duc2004/tasks.html/
4.2 Parallelness of cross-language linked
article pairs in FR-EN Wikipedia.
In this experiment, we wanted to measure the pro-
portion of cross-language linked article pairs in
Wikipedia that are in translation relation. In or-
der to do so, we manually evaluated 200 pairs of arti-
cles in our French-English Wikipedia repository.
A web interface was developed in order to anno-
tate each pair, following the distinction introduced
by Fung and Cheung (2004): parallel indicates
sentence-aligned texts that are in translation relation;
noisy characterizes two documents that are never-
theless mostly bilingual translations of each other;
topic corresponds to documents which share sim-
ilar topics, but that are not translation of each oth-
ers and very-non that stands for rather unrelated
texts.
The results of the manual evaluation are reported
in the left column of table 1. We observe that a
fourth of the pairs of articles are indeed parallel or
noisy parallel. This figure quantifies the observa-
tion made by Adafre and de Rijke (2006) that while
some articles in Wikipedia tend to be translations
of each other, the majority of the articles tend to be
written independently of each other. To the best of
our knowledge, this is the first time someone is mea-
suring the degree of parallelness of Wikipedia at
the article level.
If our sample is representative (something which
deserves further investigations), it means that more
than 134 000 pairs of documents in the French-
English Wikipedia are parallel or noisy parallel.
We would like to stress that, while conducting
the manual annotation, we frequently found diffi-
cult to label pairs of articles with the classes pro-
posed by Fung and Cheung (2004). Often, we could
spot a few sentences translated in pairs that we rated
very-non or topic. Also, it was hard to be con-
sistent over the annotation session with the distinc-
tion made between those two classes. Many arti-
cles are divided into sub-topics, some of which be-
ing covered in the other article, some being not.
4.3 Parallelness of the article pairs identified
by PARADOCS
We applied PARADOCS to our Wikipedia collec-
tion. We indexed the French pages with the Lucene
91
Wikipedia PARADOCS
Type Count Ratio Count Ratio
very-non 92 46% 5 2.5%
topic 58 29% 34 17%
noisy 22 11% 39 19.5%
parallel 28 14% 122 61%
Total 200 200
Table 1: Manual analysis of 200 pairs cross-language
linked in Wikipedia (left) and 200 pairs of articles
judged parallel by our system (right).
toolkit using the num indexing scheme. Each En-
glish article was consequently transformed with the
same strategy before querying Lucene, which was
asked to return the N = 5 most similar French arti-
cles. We limited the retrieval to 5 documents in this
experiment in order to reduce computation time. As
a matter of fact, running our system on Wikipedia
took 1.5 days of computation on 8 nodes of a pen-
tium cluster. Most of this time was devoted to com-
pute edit-distance features.
Each candidate pair of articles was then labeled
as parallel or not by a classifier we trained to rec-
ognize parallel documents in an in-house collection
of French-English documents we gathered in 2009
from a website dedicated to Olympic games.14 Us-
ing a classifier trained on a different task gives us the
opportunity to see how our system would do if used
out-of-the-box. A set of 1844 pairs of documents
have been automatically aligned (at the document
level) thanks to heuristics on URL names; then man-
ually checked for parallelness. The best classifier
we developed on this collection (thanks to a 5-fold
cross-validation procedure) was a decision tree clas-
sifier (j48) which achieves an average f-measure of
90% (92.7% precision, and 87.4% recall). This is
the classifier we used in this experiment.
From the 537 067 English documents of our col-
lection, 106 896 (20%) did not receive any answer
from Lucene (nodoc). A total of 117 032 pairs of
documents were judged by the classifier as parallel.
The post-filtering stage (dup) eliminated slightly
less than half of them, leaving us with a total of
14http://www.olympic.org
61 897 pairs. We finally eliminated those pairs that
were not cross-language linked in Wikipedia. We
ended up with a set of 44 447 pairs of articles iden-
tified as parallel by our system.
Since there is no reference telling us which cross-
language linked articles in Wikipedia are indeed
parallel, we resorted to a manual inspection of a ran-
dom excerpt of 200 pairs of articles identified as par-
allel by our system. The sampling was done in a way
that reflects the distribution of the scores of the clas-
sifier over the pairs of articles identified as parallel
by our system.
The results of this evaluation are reported in the
right column of table 1. First, we observe that
20% (2.5+17) of the pairs identified as parallel by
our system are at best topic aligned. One explana-
tion for this is that topic aligned articles often share
numbers (such as dates), sometimes in the same or-
der, especially in bibliographies that are frequent in
Wikipedia. Clearly, we are paying the prize of
a lightweight content-oriented system. Second, we
observe that 61% of the annotated pairs were indeed
parallel, and that roughly 80% of them were parallel
or noisy parallel. Although PARADOCS is not as ac-
curate as it was on the Europarl corpus, it is still
performing much better than random.
4.4 Further analysis
We scored the manually annotated cross-language
linked pairs described in section 4.2 with our clas-
sifier. The cumulative distribution of the scores is
reported in table 2. We observe that 64% (100-
35.7%) of the parallel pairs are indeed rated as par-
allel (p ? 0.5) by our classifier. This percentage is
much lower for the other types of article pairs. On
the contrary, for very non-parallel pairs, the classi-
p ? 0.1 p ? 0.2 p < 0.5 avr.
very-non 1.1% 91.4% 92.5% 0.25
topic 1.7% 74.6% 78.0% 0.37
noisy 13.6% 77.3% 90.9% 0.26
parallel 7.1% 25.0% 35.7% 0.71
Table 2: Cumulative distribution and average score given
by our classifier to the 200 manually annotated pairs of
articles cross-language linked in Wikipedia.
92
fier assigns a score lower than 0.2 in more than 91%
of the cases. This shows that the score given by the
classifier correlates to some extent with the degree
of parallelness of the article pairs.
Among the 28 pairs of cross-language linked arti-
cle pairs manually labelled as parallel (see table 1),
only 2 pairs were found parallel by PARADOCS,
even if 18 of them received a score of 1 by the classi-
fier. This discrepancy is explained in part by the fil-
ter (dup) which is too drastic since it removes all the
pairs sharing one document. We already discussed
alternative strategies. The retrieval stage of our sys-
tem is as well responsible of some failures, espe-
cially since we considered the 5 first French docu-
ments returned by Lucene. We further inspected
the 10 (28-18) pairs judged parallel but scored by
our classifier as non parallel. We observed sev-
eral problems; the most frequent one being a fail-
ure of our pre-processing step which leaves unde-
sired blocs of text in one of the article, but not in
the other (recall we kept the preprocessing very ag-
nostic to the specificities of Wikipedia). These
blocs might be infoboxes or lists recapitulating im-
portant dates, or even sometimes HTML markup.
The presence of numerical entities in those blocs is
confounding the classifier.
5 Related Work
Pairing parallel documents in a bilingual collection
of texts has been investigated by several authors.
Most of the previous approaches for tackling this
problem capitalize on naming conventions (on file
URL names) for pairing documents. This is for in-
stance the case of PTMINER (Chen and Nie, 2000)
and STRAND (Resnik and Smith, 2003), two sys-
tems that are intended to mine parallel documents
over the Web. Since heuristics on URL names does
not ensure parallelness, other cues, such as the ratio
of the length of the documents paired or their HTML
structure, are further being used. Others have pro-
posed to use features computed after sentence align-
ing a candidate pair of documents (Shi et al, 2006),
a very time consuming strategy (that we tried with-
out success). Others have tried to use bilingual lex-
icons in order to compare document pairs; this is
for instance the case of the BITS system (Ma and
Liberman, 1999). Also, Enright and Kondrak (2007)
propose a very lightweight content-based approach
to pairing documents, capitalizing on the number of
hapax words they share. We show in this study, that
this approach can easily be outperformed.
Zhao and Vogel (2002) were among the first to
report experiments on harvesting comparable news
collections in order to extract parallel sentences.
With a similar goal, Munteanu et al (2004) pro-
posed to train in a supervised way (using some par-
allel data) a classifier designed to recognize paral-
lel sentences. They applied their classifier on two
monolingual news corpora in Arabic and English,
covering similar periods, and showed that the paral-
lel material extracted, when added to an in-domain
parallel training corpus of United Nation texts, im-
proved significantly an Arabic-to-English SMT sys-
tem tested on news data. Still, they noted that the
extracted material does not come close to the qual-
ity obtained by adding a small out-domain parallel
corpus to the in-domain training material. Different
variants of this approach have been tried afterwards,
e.g. (Abdul-Rauf and Schwenk, 2009).
To the best of our knowledge, Adafre and de Rijke
(2006) where the first to look at the problem of ex-
tracting parallel sentences from Wikipedia. They
compared two approaches for doing so that both
search for parallel sentence pairs in cross-language
linked articles. The first one uses an MT engine in
order to translate sentences of one document into the
language of the other article; then parallel sentences
are selected based on a monolingual similarity mea-
sure. The second approach represents each sentence
of a pair of documents in a space of hyperlink an-
chored texts. An initial lexicon is collected from the
title of the articles that are linked across languages
(they also used the Wikipedia?s redirect feature
to extend the lexicon with synonyms). This lexicon
is used for representing sentences in both languages.
Whenever the anchor text of two hyperlinks, one in
a source sentence, and one in a target sentence is
sanctioned by the lexicon, the ID of the lexicon en-
try is used to represent each hyperlink, thus making
sentences across languages sharing some representa-
tion. They concluded that the latter approach returns
fewer incorrect pairs than the MT based approach.
Smith et al (2010) extended these previous lines
of work in several directions. First, by training a
global classifier which is able to capture the ten-
93
dency of parallel sentences to appear in chunks. Sec-
ond, by applying it at large on Wikipedia. In
their work, they extracted a large number of sen-
tences identified as parallel from linked pairs of arti-
cles. They show that this extra materiel, when added
to the training set, improves a state-of-the-art SMT
system on out-domain test sets, especially when the
in-domain training set is not very large.
The four aforementioned studies implement some
heuristics in order to limit the extraction of paral-
lel sentences to some fruitful document pairs. For
news collections, the publication time can for in-
stance be used for narrowing down the search; while
for Wikipedia articles, the authors concentrate
on document pairs that are linked across languages.
PARADOCS could be used for narrowing the search
space down to a set of parallel or closely parallel
document pairs. We see several ways this could
help the process of extracting parallel fragments.
For one thing, we know that extracting parallel
sentences from a parallel corpus is something we
do well, while extracting parallel sentences from a
comparable corpus is a much riskier enterprise (not
even mentioning time issues). As a matter of fact,
Munteanu et al (2004) mentioned the inherent noise
present in pairs of sentences extracted from com-
parable corpora as a reason why a large set of ex-
tracted sentence pairs does not contribute to improve
an SMT system more that a small but highly specific
parallel dataset. Therefore, a system like ours could
be used to decide which sort of alignment technique
should be used, given a pair of documents. For an-
other thing, one could use our system to delimit a
set of fruitful documents to harvest in the first place.
The material acquired this way could then be used
to train models that could be employed for extract-
ing noisiest document pairs, hopefully for the sake
of the quality of the material extracted.
6 Conclusion
We have described a system for identifying paral-
lel documents in a bilingual collection. This system
does not presume specific information, such as file
(or URL) naming conventions, which can sometime
be useful for mining parallel documents. Also, our
system relies on a very lightweight set of content-
based features (basically numerical entities and pos-
sibly hapax words), therefore our claim of a lan-
guage neutral system.
We conducted a number of experiments on the
Europarl corpus in order to control the impact
of some of its hyper-parameters. We show that
our approach outperforms the fair baseline described
in (Enright and Kondrak, 2007). We also con-
ducted experiments in extracting parallel documents
in Wikipedia. We were satisfied by the fact that
we used a classifier trained on another task in this
experiment, but still got good results (a precision of
80% if we consider noisy parallel document pairs
as acceptable). We conducted a manual evalua-
tion of some cross-language linked article pairs and
found that 25% of those pairs were indeed paral-
lel or noisy parallel. This manually annotated data
that can be downloaded at http://www.iro.
umontreal.ca/?felipe/bucc11/.
7 Future Work
In their study on infobox arbitrage, Adar et al
(2009) noted that currently, cross-language links in
Wikipedia are essentially made by volunteers,
which explains why many such links are missing.
Our approach lends itself to locate missing links
in Wikipedia. Another extension of this line of
work, admittedly more prospective, would be to de-
tect recent vandalizations (modifications or exten-
sions) operated on one language only of a parallel
pair of documents.
Also, we think that there are other kinds of data
on which our system could be invaluable. This is
the reason why we refrained in this work to engi-
neer features tailored for a specific data collection,
such as Wikipedia. One application of our sys-
tem we can think of, is the organization of (pro-
prietary) translation memories. As a matter if fact,
many companies do not organize the flow of the doc-
uments they handle in a systematic way and there is
a need for tools able to spot texts that are in transla-
tion relation.
Acknowledgments
We are grateful to Fabienne Venant who participated
in the manual annotation we conducted in this study.
94
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On
the use of comparable corpora to improve smt per-
formance. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, EACL ?09, pages 16?23.
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-
ing Similar Sentences across Multiple Languages in
Wikipedia. In 11th EACL, pages 62?69, Trento, Italy.
Eytan Adar, Michael Skinner, and Daniel S. Weld. 2009.
Information arbitrage across multi-lingual wikipedia.
In Proceedings of the Second ACM International Con-
ference on Web Search and Data Mining, WSDM ?09,
pages 94?103.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Jiang Chen and Jian-Yun Nie. 2000. Parallel Web text
mining for cross-language IR. In RIAO, pages 62?67,
Paris, France.
Jessica Enright and Gregorz Kondrak. 2007. A Fast
Method for Parallel Document Identification. In
NAACL HLT 2007, Companion Volume, pages 29?32,
Rochester, NY.
Tomaz Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In LREC, Lisbon, Portugal.
Elena Filatova. 2009. Directions for exploiting asymme-
tries in multilingual wikipedia. In Third International
Cross Lingual Information Access Workshop, pages
30?37, Boulder, Colorado.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 57?63, Barcelona, Spain, July. Association for
Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11, Issue 1(10?18).
Philipp Koehn. 2005. Europarl: A multilingual corpus
for evaluation of machine translation. In 10th Machine
Translation Summit, Phuket, Thailand, sep.
Adrien Lardilleux and Yves Lepage. 2007. The con-
tribution of the notion of hapax legomena to word
alignment. In 3rd Language & Technology Conference
(LTC?07), pages 458?462, Poznan? Poland.
Xiaoyi Ma and Mark Liberman. 1999. Bits: A method
for bilingual text search over the web. In Machine
Translation Summit VII, Singapore, sep.
Beata Bandmann Megyesi, Eva Csato Johansson, and
Anna Sgvall Hein. 2006. Building a Swedish-Turkish
Parallel Corpus. In LREC, Genoa, Italy.
Dragos Stefan Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation perfor-
mance via parallel sentence extraction from compara-
ble corpora. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 265?272, Boston, Massachusetts, USA,
May. Association for Computational Linguistics.
Alexandre Patry and Philippe Langlais. 2005. Automatic
identification of parallel documents with light or with-
out linguistic resources. In 18th Annual Conference on
Artificial Intelligence (Canadian AI), pages 354?365,
Victoria, British-Columbia, Canada.
Philip Resnik and Noah A. Smith. 2003. The web as a
parallel corpus. Computational Linguistics, 29:349?
380. Special Issue on the Web as a Corpus.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A dom tree alignment model for mining par-
allel data from the web. In Proceedings of the 21st
International Conference on Computational Linguis-
tics (COLING) and the 44th annual meeting of the As-
sociation for Computational Linguistics (ACL), pages
489?496, Sydney, Australia.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Human
Language Technologies: The 2010 Annual Conference
of the NAACL, HLT ?10, pages 403?411.
Jo?rg Tiedemann. 2009. News from OPUS ? A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing, pages 237?248. John Benjamins,
Amsterdam/Philadelphia.
Bing Zhao and Stephan Vogel. 2002. Adaptive parallel
sentences mining from web bilingual news collection.
In Proceedings of the 2002 IEEE International Con-
ference on Data Mining, ICDM ?02, pages 745?748,
Maebashi City, Japan.
95
