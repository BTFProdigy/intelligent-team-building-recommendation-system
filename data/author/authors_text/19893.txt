Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1962?1973, Dublin, Ireland, August 23-29 2014.
Exploring Syntactic Features for Native Language Identification:
A Variationist Perspective on Feature Encoding and
Ensemble Optimization
Serhiy Bykh
Seminar f?ur Sprachwissenschaft
Universit?at T?ubingen
sbykh@sfs.uni-tuebingen.de
Detmar Meurers
Seminar f?ur Sprachwissenschaft
Universit?at T?ubingen
dm@sfs.uni-tuebingen.de
Abstract
In this paper, we systematically explore lexicalized and non-lexicalized local syntactic features
for the task of Native Language Identification (NLI). We investigate different types of feature
representations in single- and cross-corpus settings, including two representations inspired by a
variationist perspective on the choices made in the linguistic system. To combine the different
models, we use a probabilities-based ensemble classifier and propose a technique to optimize and
tune it. Combining the best performing syntactic features with four types of n-grams outperforms
the best approach of the NLI Shared Task 2013.
1 Introduction and related work
Native Language Identification (NLI) is the task of identifying the native language of a writer by analyz-
ing texts written by this writer in a non-native language. NLI started to attract attention in computational
linguistics with the work of Koppel et al. (2005). Since then, the interest has increased steadily, leading
to the First NLI Shared Task in 2013, with 29 participating teams (Tetreault et al., 2013).
The task of NLI is usually treated as a text classification problem with the L1s as classes. A wide range
of features, reaching from character or word-based n-grams to different types of syntactic models have
been employed in NLI. For example, Wong and Dras (2011) utilized character and part-of-speech (POS)
n-grams as well as cross-sections of parse trees and Context-Free Grammar (CFG) features, i.e., local
trees. Their approach with a binary representation of non-lexicalized rules (except for those rules lexi-
calized with function words and punctuation) outperformed a setup using only lexical features, such as
n-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002). Swanson
and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG)
rules replacing terminals (except for function words) by a special symbol. TSG outperformed CFG fea-
tures in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized
CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al.,
2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features gen-
erally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al.
(2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information.
Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and
Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013).
Before extending syntactic modeling further, in this paper we want to systematically explore the range
of options involving CFG rule features for NLI. We consider non-lexicalized and lexicalized CFG fea-
tures, and different feature representations, from binary encodings to a normalized frequency encoding
inspired by a variationist sociolinguistic perspective.
Previous research in this domain often limited the use of lexicalized rules given that the lexicalization
may lead to an unintended topic or domain dependence. Yet, NLI research has since established that
lexical features, such as word-based n-grams, are among the best performing features both in single-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1962
and in cross-corpus settings (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Jarvis and Crossley,
2012; Brooke and Hirst, 2013b; Bykh et al., 2013; Gebre et al., 2013; Jarvis et al., 2013; Lynum, 2013),
making them an essential component of any approach with state-of-the-art performance. At the same
time, the question whether an NLI approach and its results capture general characteristics of language
and language learning instead of only encoding the characteristics of a specific data set remains an
essential concern. In the experiments in this paper, we thus include experiments on both a topic-balanced
single-corpus and on a highly heterogeneous cross-corpus data set.
The range of feature types used in NLI research raises a further question, namely how the different
sources of information are best combined. The most simple solution is to put all features into a single
vector. However, Tetreault et al. (2012) pointed out that the performance can be increased by using a
probability-estimate based ensemble (meta-classifier), which was confirmed in Bykh et al. (2013) and
Cimino et al. (2013). But which models are worth integrating into such a meta-classifier? Some of
the models may be redundant despite performing well individually; on the other hand, some models
may improve the ensemble despite performing relatively poorly by itself. We explore this issue by
implementing a basic ensemble optimization algorithm performing model selection.
In terms of the structure of the paper, in section 2 we first introduce the corpora used in the single-
corpus and cross-corpus settings. Section 3 then presents the first set of experiments, systematically
exploring lexicalized and unlexicalized Context-Free Grammar Rules (CFGR) as features. Given the
significant complexity of the overall feature space, we then explore model selection for optimizing the
ensemble classifier in section 4. In section 5, we combine the CFGR features with n-grams, resulting in
the best accuracy reported for the standard TOEFL11 test set. Section 6 sums up the paper and sketches
some directions for future research.
2 Data
The research in this paper makes use of two sets of data:
First, there is the TOEFL11 (T11) data set (Blanchard et al., 2013), which was introduced for the NLI
Shared Task 2013 and has become a standard frame of reference for NLI research. We use this standard
setup for single-corpus evaluation, where each L1 is represented by 1100 essays, of which 100 essays
are singled out in the standard test set. The remaining 1000 essays per L1 (= T11 train ? dev) constitute
our training data in the single-corpus settings.
Second, we make use of a range of other learner corpora to study how well the results generalize.
Concretely, for our cross-corpus settings we employ the NT11 corpus of Bykh et al. (2013), which
consists of the ICLE (Granger et al., 2009), FCE (Yannakoudakis et al., 2011), BALC (Randall and
Groom, 2009), ICNALE (Ishikawa, 2011), and T
?
UTEL-NLI (Bykh et al., 2013) corpora. In total NT11
includes 5843 texts, with the following division into languages: Arabic (846), Chinese (1048), French
(456), German (500), Hindu (400), Italian (467), Japanese (447), Korean (684), Spanish (446), Telugu
(200), Turkish (349). In the cross-corpus settings, we train on NT11 and test on the standard T11 test set.
3 Systematically exploring Context-Free Grammar Rules (CFGR)
3.1 Features
In this paper, we focus on the CFG production rules (CFGR) as syntactic features for the task of NLI.
CFG rules are the most basic and widely used local syntactic units modularizing the overall syntactic
analysis of a sentence. We parsed the T11 and NT11 corpora using the Stanford Parser (Klein and
Manning, 2002) and extracted all CFG rules from the T11 and NT11 training sets. On this basis we
defined the following tree feature types:
1. CFGR
ph
: Only phrasal CFG production rules excluding all terminals
? S? NP VP, NP? D NN, . . .
2. CFGR
lex
: Only lexicalized CFG production rules of the type preterminal? terminal
? JJ? nice, JJ? quick, NN? vacation, . . .
3. CFGR
ph?lex
= CFGR
ph
? CFGR
lex
(i.e., the union of the above two)
1963
A variationist perspective on feature representation We explore four different feature representa-
tions: The two standard ones are a frequency-based (freq) representation, where the values are the raw
counts of the occurrences of the rule in the given parsed document, and a binary (bin) representation,
which only indicates whether a rule is present or absent in that document.
Complementing these standard feature representations, we explored two options that take as starting
point the observation that CFG rules with the same left-hand side category represent different ways to
rewrite that category. So in a sense, under a top-down perspective, there is a choice between different
ways of realizing a given category.
This is reminiscent of variationist sociolinguistic analysis, where one studies the linguistic choices
made by a given speaker and connects the choices with extra-linguistic variables such as the age or
gender of a speaker. For example, in William Labov?s field-defining study ?The Social Stratification
of (r) in New York City Department Stores? from his book ?Sociolinguistic Patterns? (Labov, 1972),
he found that the presence or absence of the consonant [r] in postvocalic position (e.g., car, fourth)
correlates with the ranking of people in status or prestige, i.e., social stratification. Speakers thus make
choices in how to realize a given variable by producing one of the variants (see also Tagliamonte, 2011).
Inspired by this perspective, in Meurers et al. (2013) we discussed how a variationist perspective on
syntactic alternations can provide interpretable features for NLI classification.
Under a variationist perspective, producing one of the variants of a given variable also means not
choosing the other variants of that variable. So it is this grouping of observations that we want to take
into account in terms of encoding local trees as features when we interpret the mother category as the
variable to be realized and the different CFG rules with that left-hand side as variants of that variable.
This results in two feature representations, a simple one (var
s
) and a weighted one (var
w
).
The var
s
and var
w
frequency normalizations for each variant v from the set of variants V realizing a
particular variable out of the set of variables V is defined as follows:
var
s
(v ? V ) =
f(v)
F (V )
var
w
(v ? V ) = var
s
(v) ? w(V )
Here, f(v) yields the frequency x of a particular variant v, F (V ) is the sum over the frequencies of
all variants v realizing the variable V , and w(V ) is the weight for the variable V :
f(v) = x
F (V ) =
?
v?V
f(v)
w(V ? V ) =
F (V )
n
?
i=1
F (V
i
)
The weighting applied in var
w
takes into account the frequency proportion of each variable V in the
overall variables set V , assigning higher weights for more frequent variables. Mathematically it reduces
to normalizing each variant by the sum of the frequencies over all variants across all variables, i.e., to
the relative frequency of each variant v with respect to the set of all variables V . At the same time,
we will see in the next section that the individual variables keep an independent status in terms of the
classification setup, where we train a separate classifier for each variable.
1964
3.2 Results
Classifier We use the L2-regularized Logistic Regression from the LIBLINEAR package (Fan et al.,
2008), which we accessed through WEKA (Hall et al., 2009). To obtain results for all feature repre-
sentations which are comparable across the different settings we uniformly scale all values employing
the -Z option of WEKA. This means that the freq feature representation based on the raw frequencies
in essence also becomes normalized. This is particularly relevant in the context of the cross-corpus
evaluation, where raw frequencies are particularly questionable given highly variable text sizes.
Single- vs. cross-corpus results The results for the three feature types using the four different feature
representations are presented in Table 1. The chance baseline for the given data setup is 9.1%. There
are big accuracy differences between the single- and cross-corpus settings despite very similar feature
counts. The drop for the cross-corpus settings is roughly around
1
2
compared to the single-corpus settings.
This is in line with previous results on the same data sets using a wide range of features (Bykh et al.,
2013), confirming the fact that obtaining high cross-corpus results remains challenging in NLI.
features single-corpus (sc): T11 training
freq bin var
s
var
w
feat. #
CFGR
ph
50.00% 44.27% 48.45% 49.82% 14,713
CFGR
lex
75.73% 72.45% 71.00% 76.91% 83,402
CFGR
ph?lex
78.18% 73.55% 75.36% 78.82% 98,115
features cross-corpus (cc): NT11 training
freq bin var
s
var
w
feat. #
CFGR
ph
21.27% 22.91% 26.27% 27.73% 15,253
CFGR
lex
26.73% 32.00% 28.82% 36.82% 78,923
CFGR
ph?lex
28.27% 34.27% 32.55% 38.82% 94,176
Table 1: Results for the CFGR feature variants obtained on the standard T11 test set
Best feature type The CFGR
lex
feature type clearly outperforms the more abstract CFGR
ph
feature
type, yielding up to 28% difference in accuracy for the single-corpus and up to 9% for the cross-corpus
settings. In contrast to previous research assuming that lexicalized trees are too topic-specific, the results
show that CFGR
lex
is a valuable feature type in both the single-corpus and the cross-corpus settings.
The CFGR
lex
features combine syntactic and lexical information, such as the fact that a given token with
a particular POS is used, e.g., the token can being used as a noun in There is a can of beer in the fridge
instead of as the more frequent modal verb use in He can dance. Note that this is different from using
word and POS unigrams as features, where the relevant connection is lost. In both the T11 data, which
is topic balanced, for single-corpus evaluation and the very heterogeneous NT11 data containing a wide
range of topics for cross-corpus evaluation, we obtained consistently better results for CFGR
lex
than for
CFGR
ph
. Some syntactic rules including lexical information thus seem to generalize well across topics.
Combining CFGR
ph
and CFGR
lex
into CFGR
ph?lex
gives an additional boost in performance.
Best feature representation There are clear differences in Table 1 between the results for the four
feature representations. var
w
yields the best accuracies in five out of six settings, across different feature
types and corpora.
The results show that WEKA-normalized raw frequencies such as freq yield the worst results in a
cross-corpus setting but perform very well single-corpus, which is in line with the assumption that raw
frequency features do not generalize well. In our experiments, the performance of freq in a cross-corpus
setting is up to 10.55% worse than what is yielded by var
w
, despite comparable single-corpus perfor-
mance. freq also consistently performs worse than var
s
in the cross-corpus setting, despite outperforming
var
s
single-corpus.
1965
Using binary features (bin) yields better results cross-corpus than freq, whereas in the single-corpus
setting it is the other way round. The abstraction introduced by the binary feature representation thus
shows a positive effect in terms of the capability of the features to generalize to other data sets.
For the abstract CFGR
ph
features, var
s
performs better than freq or bin in the cross-corpus setting.
The fact that the var
w
is performing consistently better than var
s
shows that weighting is important.
Hence, incorporating the insight from variationist sociolinguistics is not only conceptually interesting as
a theoretical perspective, but also provides a quantitative advantage in terms of performance.
CFGR categories as variables As mentioned above, the best performance is achieved by combining
CFGR
ph
and CFGR
lex
into the CFGR
ph?lex
feature type using the weighted variationist feature rep-
resentation var
w
. Thus, we focused on that feature type and explored it more in depth. We did so by
splitting the overall var
w
normalized CFGR
ph?lex
feature set by the variable, i.e., the different mother
nodes. We trained separate models, where each of those models consists of features encoding the differ-
ent variants, i.e., the different realizations in which a given mother node can be rewritten. Our aim was
to investigate the accuracy of the individual variable-based models and their contribution to the overall
performance. Figures 1 and 2 depict the single-corpus (sc) and cross-corpus (cc) accuracies yielded by
each individual variable-based model, for presentation reasons shown separately for the CFGR
ph
and
the CFGR
lex
subsets.
ADJ
P
ADV
P
CON
JP
FRA
G INT
J LST NAC NP NX PP PRN PRT QP RRC S SBA
R
SBA
RQ SIN
V SQ UCP VP
WH
ADJ
P
WH
ADV
P
WH
NP
WH
PP X
0,00%
5,00%
10,00%
15,00%
20,00%
25,00%
30,00%
35,00%
40,00%
45,00%
50,00%
sccc
models
accu
racy
Figure 1: Accuracy for the individual CFGR
ph
variable based models, var
w
normalized
The CFGR
ph
results in Figure 1 show that a small subset of variables performs relatively well. Most
of the models perform poorly, yielding accuracies close to the chance baseline. The best performing
variables are essentially the main phrasal categories, such as S, NP, VP, PP, ADJP, ADVP or SBAR.
The results for the CFGR
lex
in Figure 2 show a similar pattern. There is a subset of variables which
perform relatively well, usually models based on the main POS categories, such as the nominal (NN) and
verbal (VB) categories as well as adjectives (JJ), prepositions (IN) and adverbs (RB). Some punctuation
marks also seem to play a role. The rest of the models yields accuracies around the chance baseline.
This might be due to data sparsity given that the main POS categories also are the most frequent. But
those main categories also have the highest number of variants through which they can be realized. The
good performance of the models for the variables with the highest number of variants thus confirms the
assumption that the choice of one of the realization options of a given category is influenced by the L1.
Should we focus only on those high-performing models ? or do the other models also contain relevant,
independent information which is worth preserving? We address that question in the next section.
1966
AA AD DJ PV CO NF RR RRG RRI TI LD FF FFS FFS
I FFI SDJ SXI SGS SGS
Q GB GBG GBI GS IUL JX WH 0B 0BD 0B, 0BF 0BS 0B% OD
J OS OS
Q
OG
B
5TG
B5
5GG
B5 1 Q 22 3 4 s ccm3mmo
d3mmo
em3mmo
ed3mmo
lm3mmo
ld3mmo
am3mmo
ad3mmo
um3mmo
ud3mmo
dm3mmo
ryyy
?????r
?yy?
??y?
Figure 2: Accuracy for the individual CFGR
lex
variable based models, var
w
normalized
4 Ensemble optimization and tuning
Ensemble generation To combine the individual models, we employ a probability-estimate-based en-
semble approach, following Tetreault et al. (2012) and Bykh et al. (2013). This meta-classifier combines
the probability distributions provided by the individual classifier for each of the incorporated models as
features. To obtain the ensemble training files, we performed 10-fold cross-validation for each model on
the corresponding training set and took the probability estimate distributions. For testing, we took the
probability estimate distribution yielded by each individual model trained on the corresponding training
set and tested on the T11 test set. To obtain the probability estimates for the individual models we used
LIBLINEAR as described in section 3.2. The ensembles were trained and tested using LIBSVM with an
RBF kernel (Chang and Lin, 2011), which outperformed LIBLINEAR for this purpose.
Ensemble optimization (+opt) The growing range of features used for NLI raises the question of how
to perform model selection. Even when analyzing a single feature type in depth, as we do in section 3.2,
we already must determine which of the low-performing models to keep in an ensemble. We approach
the question with a simple incremental ensemble optimization algorithm performing model selection.
Algorithm 1 Ensemble Optimization / Ensemble Model Selection
M
a
? {m
1
, ...,m
n
} . overall ensemble, i.e., all ensemble models
M
b
? ? . current best performing ensemble
while M
a
6= ? do . iterate until M
a
is empty
m
b
? MAX(M
a
) . get the model with the highest accuracy m
b
out of M
a
M
t
?M
b
? {m
b
} . join the previous best performing ensemble M
b
and {m
b
}
if ACC(M
t
) > ACC(M
b
) then . check if the new ensemble is performing better than M
b
M
b
?M
t
. if the accuracy improves, store the new ensemble in M
b
end if
REMOVE(m
b
,M
a
) . remove m
b
from M
a
end while
1967
In each iteration step the optimization algorithm shown in Algorithm 1 retrieves the current best single
model m
b
out of the model set M
a
(which is initialized with the overall model set for a particular setting),
joins it with the previous best performing ensemble M
b
(which is initialized to ?), compares the accuracy
of that new ensemble with the accuracy of the previous best ensemble. It retains the new ensemble as the
best ensemble if the accuracy improves, or keeps the previous best ensemble as best ensemble otherwise.
In Algorithm 1, we describe only the gist of the optimization, omitting some details to keep it transparent.
Some ambiguities have to be resolved. If there are several models in M
a
yielding the same accuracy, one
has to decide, which of them to pick as the next m
b
. We resolve that issue by always picking the model
with the least number of features. When several models yield the same accuracy and have the same
number of features, we resort to alphabetical order. The optimization is always carried out using 10-fold
cross-validation results on the training data (to obtain the accuracy ranking on M
a
and to perform each
optimization step). The test set is not part of the optimization at any point. Only after optimization is the
resulting ensemble applied to the test set and we report the corresponding accuracies.
Ensemble tuning (+all) In order to further tune the ensemble, we explore the following idea: We
generate a single ensemble model m
n+1
based on all of the features used in a particular setting, i.e., all the
features incorporated by the models m
1
. . .m
n
. Then we include that m
n+1
model in the M
a
ensemble
as just another model, and use that new M
+1
a
ensemble either directly or as basis for the optimization.
Since m
n+1
incorporates all of the features of interest for a particular setting, it is expected to yield more
reliable probability estimates than the other individual ensemble models in M
+1
a
, each covering only
a subset of that feature set. Incorporating such an m
n+1
into the ensemble may stabilize the resulting
system, i.e., the machine learning algorithms may learn to rely on m
n+1
in settings, where the rest of
the included models m
1
. . .m
n
show a rather poor individual performance and are of limited use. In the
tables and explanations below, we refer to the model m
n+1
as [all] and to the M
+1
a
ensemble as +all.
For building the m
n+1
model included in the M
+1
a
ensemble there are two options. We can build
it on the basis of the probabilities of the models or on the union of the original feature values of those
models. In the former case, the final ensemble model essentially is a meta-meta-classifier. For the settings
integrating the same type of feature representations (cf. results in Tables 2 and 4), we use the original
feature values merged into a single vector to build m
n+1
. For the settings integrating different feature
types (cf. results in Table 6), we use the probability estimates from the models m
1
. . .m
n
to build m
n+1
.
Ensemble results for the CFGR variables The ensemble results for the separate variable-based mod-
els for the CFGR
ph?lex
feature type are presented in Table 2. We provide single-corpus (sc) and cross-
corpus (cc) results for different ensemble settings, where +/- opt states whether ensemble optimization
was performed, and +/- all whether tuning was employed. Concretely, (-opt, -all) means that the ensem-
ble M
a
was used without any optimization or tuning, and correspondingly (+opt, +all) means that the
optimized and tuned version of M
a
(i.e., the optimized version of the ensemble M
+1
a
) was employed. In
the remaining two cases (+opt, -all) and (-opt, +all) either optimization or tuning was used, respectively.
The column baseline lists the corresponding results from Table 1, which were obtained by putting all the
features in a single vector. The number in parentheses specifies the number of models combined in the
ensemble: in the features column, it shows the overall number of separate variable-based models, and in
the +opt columns, it is the number of models selected by the optimization algorithm.
features data baseline ensemble
-opt +opt
-all +all -all +all
CFGR
ph?lex
(71) sc 78.82% 66.00% 79.18% 71.27% (14) 79.64% (8)
cc 38.82% 18.09% 34.18% 32.55% (10) 39.00% (1)
Table 2: Results for the CFGR
ph?lex
ensembles with different optimization settings
The results show that generating an ensemble using all of the individual variable-based models without
optimization and tuning (-opt, -all) leads to a big accuracy drop compared to the baseline. The fact that
1968
the drop in the cross-corpus setting is more than 20% is particularly striking. We assume that this is due
to the poor performance of most of the individual models, yielding probabilities of little use overall. The
few relatively well-performing models we discussed in section 3.2 apparently are flooded by the noise
introduced by the others. Thus, for a set of rather low-performing models without any optimization, it
seems preferable to provide the classifier with access to the individual features instead of to the noisy
probability estimates. The optimization (+opt, -all) leads to a clear improvement over the non-optimized
settings. In the single-corpus setting only 14 of the 71 models were kept and in cross-corpus only 10.
Table 3 shows the selected models in the order in which they are selected by the ensemble optimization
algorithm. For (+opt, -all), the table basically consists of the best performing variables (i.e., the models
containing as features the different ways to rewrite the given mother category) as discussed in section 3.2,
suggesting that the algorithm makes meaningful choices.
data CFGR
ph?lex
: selected models
+opt, -all +opt, +all
sc [NN]+[JJ]+[RB]+[NNS]+[VB]+[NP]+[S]+[VP] [all]+[NN]+[JJ]+[RB]+[PRP]+[VBN]+[NNP]+[WDT] (8)
+[IN]+[VBP]+[VBG]+[VBN]+[NNP]+[,] (14)
cc [NN]+[JJ]+[NNS]+[NP]+[RB]+[VB]+[VP]+[NNP] [all] (1)
+[S]+[IN] (10)
Table 3: The CFGR
ph?lex
model sets selected by optimization
The flipside of the coin is that low-performing models generally were not found to have a positive
effect and thus were not included. Yet, optimization by itself is not successful overall given that the
(+opt, -all) accuracy remains below the single feature set baseline.
Applying tuning without optimization (-opt, +all) outperforms the optimization result. Thus, includ-
ing the overall model [all] in the ensemble improves the meta-classifier. In the single-corpus setting, the
accuracy is slightly higher than the baseline, in cross-corpus it remains below the baseline.
Turning on both optimization and tuning (+opt, +all) yields the overall best results of Table 2, 79.64%
for single-corpus and 39% for the cross-corpus setting. The corresponding entry in Table 3 shows that
tuning significantly reduces the number of selected models. This is not unexpected given that the overall
model [all] essentially includes all the information. In the cross-corpus setting, [all] indeed is the only
model selected. Interestingly, in the single-corpus setting, the optimization algorithm identifies some
additional models to improve the accuracy, mainly ones that also perform well individually. While this
amounts to adding information that in principle is already available to the [all] model, the improvement
may stem from the abstract nature of the probability estimates used as features of the meta-classifier.
When both optimization and tuning are applied, the tuning apparently stabilizes the ensemble leading to
higher performance, and the optimization algorithm further improves the result by reducing the noise.
5 Combining CFGR with four types of n-grams
Based on the systematic exploration of the CFGR domain, we turn to combining our new feature type
CFGR
ph?lex
with n-gram features as the best performing features for NLI (Tetreault et al., 2013; Jarvis
et al., 2013). Adapting the n-gram approach we presented in Bykh and Meurers (2012), we use all
recurring n-grams with 1 ? n ? 10 at different levels of representation, including the word-based (W),
open-class POS-based (OP) and POS-based (P) n-grams from our previous work as well as lemma-based
(L) n-grams (Jarvis et al., 2013). We employ binary feature encoding for all n-gram types.
For POS-tagging we use the OpenNLP
1
toolkit, for lemmatizing we employ the MATE
2
tools
(Bj?orkelund et al., 2010). To obtain a fine grained, flexible n-gram setting, we generate an ensemble
model for each n-gram type and each n, which results in 40 n-gram models.
1
http://opennlp.apache.org
2
https://code.google.com/p/mate-tools
1969
Table 4 provides the results for the n-gram ensembles built on the basis of the recurring word-, lemma-,
POS-, OCPOS-based n-grams with 1 ? n ? 10 in the same format as Table 2 for CFGR
ph?lex
.
3
Different from the CFGR
ph?lex
case, the results for the n-gram ensemble model without optimization
or tuning (-opt, -all) already are 4?5% higher than the single vector baseline.
features data baseline ensemble
-opt +opt
-all +all -all +all
N-GRAMS (40) sc 77.09% 82.27% 82.55% 83.00% (13) 82.27% (8)
cc 31.00% 34.91% 34.55% 36.45% (6) 35.45% (6)
Table 4: Results for the n-gram ensembles with different optimization settings
The best results, 83% for single-corpus and 36.45% for the cross-corpus setting, are obtained by ap-
plying the optimization. The n-gram ensembles seem to benefit more from optimization than from tuning
in general. The feature counts for the n-grams (single-corpus: 4,822,874; cross-corpus: 3,687,375) are
far higher than for CFGR
ph?lex
(single-corpus: 98,115; cross-corpus: 94,176), so there may be more
noise in the [all] model, making it less useful for the tuning step.
Table 5 lists the models selected by the optimization algorithm in order in which they are selected.
The n-gram types and the n of the model is indicated, e.g., ?[OP-3]? means ?OCPOS-based trigrams?.
data N-GRAMS: selected models
+opt, -all +opt, +all
sc [W-2]+[L-2]+[W-1]+[L-1]+[L-3]+[W-3]+[OP-3] [all]+[W-2]+[L-2]+[W-1]+[L-1]+[L-3]+[OP-4]+[L-4] (8)
+[OP-1]+[OP-5]+[P-3]+[P-5]+[P-2]+[OP-8] (13)
cc [W-2]+[W-1]+[L-1]+[L-3]+[W-3]+[OP-2] (6) [W-2]+[W-1]+[all]+[L-1]+[L-3]+[P-4] (6)
Table 5: The n-gram model sets selected by optimization
For the more surface-based n-gram (word- and lemma-based), the optimizer selected only up to n = 3,
whereas for the more abstract ones (POS- and OCPOS-based), models up to n = 8 were included. Thus,
when abstracting from the surface, one can get some useful information out of longer n-grams that
apparently is not contained in the short surface-based ones. Different from the CFGR
ph?lex
variables-
based ensemble, we here find that relatively low-performing models such as those considering longer n
n-grams are kept when optimizing the ensemble.
Having established the performance of the n-gram ensembles, we can turn to combining the
CFGR
ph?lex
and n-gram models. The results are presented in Table 6.
features data ensemble
-opt +opt
-all +all -all +all
(a) CFGR
ph?lex
(71) + N-GRAMS (40) sc 82.09% 82.91% 82.91% (20) 83.55% (6)
cc 34.09% 36.00% 36.73% (8) 38.45% (3)
(b) CFGR
ph?lex
(71) + N-GRAMS [+opt, -all] (ME) sc 83.09% 83.73% 82.64% (4) 84.18% (5)
cc 37.36% 39.55% 38.00% (3) 40.27% (3)
(c) CFGR
ph?lex
[+opt, +all] (ME) + N-GRAMS (40) sc 83.73% 84.82% 84.73% (13) 83.82% (13)
cc 36.82% 38.91% 42.00% (5) 43.00% (4)
(d) CFGR
ph?lex
[+opt, +all] (ME) + N-GRAMS [+opt, -all] (ME) sc 83.45% 83.45% 83.45% (2) 83.36% (2)
cc 41.27% 42.00% 41.27% (2) 40.55% (2)
Table 6: Optimization results combining n-grams and CFGR
ph?lex
3
For space reasons, we cannot present the individual results for the separate n-gram models here, but interested readers
can consult Bykh and Meurers (2012), where word-, POS- and OCPOS-based n-gram results are discussed in detail. The
lemma-based n-grams we are adding here perform very much like the word-based n-grams.
1970
We explore four different ways to combine the two model sets, and the table shows the best results for
each of the setups in bold, once for the single-corpus and once for the cross-corpus setting.
For the results of setup (a), we use the ensemble consisting of all individual models separately.
In (b), the CFGR
ph?lex
models are included as in (a), but we replace the n-gram models by a single
meta-ensemble model (ME) generated using the best n-grams setting (+opt, -all), which consists of 13
models for single-corpus and six models for the cross-corpus setting (see Table 4). ME thus is a meta-
meta-classifier, generated by applying the ensemble model generation routine to an ensemble.
In (c), we invert the (b) setting: The CFGR
ph?lex
features are replaced by a meta-ensemble generated
using the best performing CFGR
ph?lex
setting (+opt, +all), which consists of eight models for the
single-corpus, and one model for the cross-corpus setting (see Table 2).
Finally, in (d) we combine the meta-ensemble for CFGR
ph?lex
with the meta-ensemble for the n-
grams obtaining an ensemble consisting of two models
The best results of 84.82% in the single-corpus setting and 43% cross-corpus, underlined in the table,
are obtained in setup (c). These are the overall best results across all experiments described in this paper.
The best result in the single-corpus setting involves tuning only, whereas in the cross-corpus setting it
involves tuning and optimization selecting the models [all]+[CFGR +all +opt]+[W-2]+[W-1].
The single-corpus accuracy of 84.82% is the best result reported so far for the NLI Shared Task 2013
data with the T11 train ? dev set for training and the T11 test set for testing. The best previous result
was 83.6% (Jarvis et al., 2013).
In the cross-corpus setting, the 43% accuracy also outperforms the previous best result on the
NT11 data (Bykh et al., 2013) by 4.5%.
In sum, the overall best results in the single-corpus and cross-corpus settings are obtained starting with
the whole n-gram model set plus an optimized CFGR
ph?lex
meta-ensemble. This confirms the useful-
ness of the optimized ensemble setup and underlines that combining a range of linguistic properties, from
n-grams at different levels of abstraction to local syntactic trees characteristics, is a particularly fruitful
approach for native language identification as a good example of an experimental task putting linguistic
modeling to the test with real-life data.
6 Conclusions
In the research presented, we systematically explored non-lexicalized and lexicalized CFG production
rules (CFGR) as features for the task of NLI using both single-corpus and cross-corpus settings. Includ-
ing lexicalized CFG rule features clearly improved the results in both setting so that it seems worthwhile
not to discard them a priori, which was the standard in previous research.
Pursuing a variationist perspective to CFGR feature representation resulted in improved performance
and it supported an in-depth exploration of the contribution of the different variables and variants as
well as of the value of local syntactic features for NLI in general. Training a separate classifier for each
variable provides quantitative advantages by facilitating high-performing ensemble setups and supports
a qualitative discussion of the categories reflecting the choices made by the learners with a given L1.
Investigating different meta-classifier setups, we explored ensemble optimization and tuning tech-
niques that improved the accuracy over putting all features in a single vector or a basic ensemble setup.
Combining the syntactic CFGR with four types of n-grams yielded a single-corpus accuracy of 84.82%
on the TOEFL11 test set. To the best of our knowledge this is the highest accuracy reported so far on
this standard data set of the NLI Shared Task 2013. The combined model also outperformed our best
previous cross-corpus result on the NT11 corpus.
In terms of future work, we intend to explore a broader range of linguistic features from a variationist
perspective, for example on the morphological level. To investigate the generalizability of the types of
features used, we also plan to apply our approach to NLI targeting second langauges other than English.
1971
References
Anders Bj?orkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A high-performance syntactic and
semantic dependency parser. In Demonstration Volume of the 23rd International Conference on Computational
Linguistics (COLING 2010), Beijing, pages 23?27. https://code.google.com/p/mate-tools/.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native english. Technical report, Educational Testing Service.
Julian Brooke and Graeme Hirst. 2012. Robust, lexicalized native language identification. In Proceedings of the
24th International Conference on Computational Linguistics (COLING), pages 391?408, Mumbai, India.
Julian Brooke and Graeme Hirst. 2013a. Native language detection with ?cheap? learner corpora. In Sylviane
Granger, Ga?etanelle Gilquin, and Fanny Meunier, editors, Twenty Years of Learner Corpus Research. Looking
Back, Moving Ahead. Proceedings of the First Learner Corpus Research Conference (LCR 2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2013b. Using other learner corpora in the 2013 nli shared task. In Proceedings
of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT
2013, Atlanta, GA.
Serhiy Bykh and Detmar Meurers. 2012. Native language identification using recurring n-grams ? investigating
abstraction and domain dependence. In Proceedings of the 24th International Conference on Computational
Linguistics (COLING), pages 425?440, Mumbay, India.
Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, and Detmar Meurers. 2013. Combining shallow and linguistically
motivated features in native language identification. In Proceedings of the 8th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2:27:1?27:27. Software available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Andrea Cimino, Felice Dell?Orletta, Giulia Venturi, and Simonetta Montemagni. 2013. Linguistic profiling based
on general?purpose features and native language identification. In Proceedings of the 8th Workshop on Innova-
tive Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.
R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J. Lin. 2008. Liblinear: A library for large linear
classification. The Journal of Machine Learning Research, 9:1871?1874. Software available at http:
//www.csie.ntu.edu.tw/
?
cjlin/liblinear.
Binyam Gebrekidan Gebre, Marcos Zampieri, Peter Wittenburg, and Tom Heskes. 2013. Improving native lan-
guage identification with tf-idf weighting. In Proceedings of the 8th Workshop on Innovative Use of NLP for
Building Educational Applications (BEA-8) at NAACL-HLT 2013, Atlanta, GA.
S. Granger, E. Dagneaux, and F. Meunier. 2002. International Corpus of Learner English. Presses Universitaires
de Louvain, Louvain-la-Neuve.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot, 2009. International Corpus of Learner
English, Version 2. Presses Universitaires de Louvain, Louvain-la-Neuve.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The
weka data mining software: An update. In The SIGKDD Explorations, volume 11, pages 10?18.
Shin?ichiro Ishikawa. 2011. A new horizon in learner corpus studies: The aim of the ICNALE projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Corpora and language technologies in teaching, learning and
research, pages 3?11. University of Strathclyde Publishing, Glasgow, UK. http://language.sakura.
ne.jp/icnale/index.html.
Scott Jarvis and Scott A. Crossley, editors. 2012. Approaching Language Transfer through Text Classification:
Explorations in the Detection-based Approach. Second Language Acquisition. Multilingual Matters.
Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013. Maximizing classification accuracy in native language iden-
tification. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications
(BEA-8) at NAACL-HLT 2013, Atlanta, GA.
Dan Klein and Christopher D. Manning. 2002. Fast exact inference with a factored model for natural language
parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002).
1972
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Determining an author?s native language by mining a text
for errors. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in
data mining (KDD ?05), pages 624?628, New York.
William Labov. 1972. Sociolinguistic Patterns. University of Pennsylvania Press.
Andr?e Lynum. 2013. Native language identification using large scale lexical features. In Proceedings of the
8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013,
Atlanta, GA.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras. 2013. Nli shared task 2013: Mq submission. In
Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at
NAACL-HLT 2013, Atlanta, GA.
Detmar Meurers, Julia Krivanek, and Serhiy Bykh. 2013. On the automatic analysis of learner corpora: Native
language identification as experimental testbed of language modeling between surface features and linguistic
abstraction. In Diachrony and Synchrony in English Corpus Studies, Frankfurt am Main. Peter Lang.
Mick Randall and Nicholas Groom. 2009. The BUiD Arab learner corpus: a resource for studying the acquisition
of L2 english spelling. In Proceedings of the Corpus Linguistics Conference (CL), Liverpool, UK.
Benjamin Swanson and Eugene Charniak. 2012. Native language detection with tree substitution grammars. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 193?197, Jeju Island, Korea, July. Association for Computational Linguistics.
Ben Swanson and Eugene Charniak. 2013. Extracting the native language signal for second language acquisition.
In Proceedings of NAACL-HLT. Association for Computational Linguistics.
Ben Swanson. 2013. Exploring syntactic representations for native language identification. In Proceedings of the
8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA-8) at NAACL-HLT 2013,
Atlanta, GA.
Sali A. Tagliamonte. 2011. Variationist Sociolinguistics: Change, Observation, Interpretation. John Wiley &
Sons.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Martin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native language identification. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING), pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared
task. In Proceedings of the Eighth Workshop on Building Educational Applications Using NLP, Atlanta, GA,
USA, June. Association for Computational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting parse structures for native language identification. In
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600?1610,
Edinburgh, Scotland, UK., July.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume 1, HLT ?11, pages 180?189, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics. Corpus available: http://ilexir.co.uk/applications/
clc-fce-dataset.
1973
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 197?206,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Combining Shallow and Linguistically Motivated Features in
Native Language Identification
Serhiy Bykh Sowmya Vajjala Julia Krivanek Detmar Meurers
Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen
{sbykh, sowmya, krivanek, dm}@sfs.uni-tuebingen.de
Abstract
We explore a range of features and ensembles
for the task of Native Language Identification
as part of the NLI Shared Task (Tetreault et al,
2013). Starting with recurring word-based n-
grams (Bykh and Meurers, 2012), we tested
different linguistic abstractions such as part-
of-speech, dependencies, and syntactic trees
as features for NLI. We also experimented
with features encoding morphological proper-
ties, the nature of the realizations of particu-
lar lemmas, and several measures of complex-
ity developed for proficiency and readabil-
ity classification (Vajjala and Meurers, 2012).
Employing an ensemble classifier incorporat-
ing all of our features we achieved an ac-
curacy of 82.2% (rank 5) in the closed task
and 83.5% (rank 1) in the open-2 task. In
the open-1 task, the word-based recurring n-
grams outperformed the ensemble, yielding
38.5% (rank 2). Overall, across all three tasks,
our best accuracy of 83.5% for the standard
TOEFL11 test set came in second place.
1 Introduction
Native Language Identification (NLI) tackles the
problem of determining the native language of an
author based on a text the author has written in a
second language. With Tomokiyo and Jones (2001),
Jarvis et al (2004), and Koppel et al (2005) as first
publications on NLI, the research focus in computa-
tional linguistics is relatively young. But with over
a dozen new publications in the last two years, it is
gaining significant momentum.
In Bykh and Meurers (2012), we explored a data-
driven approach using recurring n-grams with three
levels of abstraction using parts-of-speech (POS). In
the present work, we continue exploring the contri-
bution and usefulness of more linguistically moti-
vated features in the context of the NLI Shared Task
(Tetreault et al, 2013), where our approach is in-
cluded under the team name ?Tu?bingen?.
2 Corpora used
T11: TOEFL11 (Blanchard et al, 2013) This is the
main corpus of the NLI Shared Task 2013. It con-
sists of essays written by English learners with 11
native language (L1) backgrounds (Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, Turkish), and from three different
proficiency levels (low, medium, high). Each L1 is
represented by a set of 1100 essays (train: 900, dev:
100, test: 100). The labels for the train and dev sets
were given from the start, the labels for the test set
were provided after the results were submitted.
ICLE: International Corpus of Learner English
(Granger et al, 2009) The ICLEv2 corpus consists
of 6085 essays written by English learners of 16 dif-
ferent L1 backgrounds. They are at a similar level of
English proficiency, namely higher intermediate to
advanced and of about the same age. For the cross-
corpus tasks we used the essays for the seven L1s in
the intersection with T11, i.e., Chinese (982 essays),
French (311), German (431), Italian (391), Japanese
(366), Spanish (248), and Turkish (276).
FCE: First Certificate in English Corpus (Yan-
nakoudakis et al, 2011) The FCE dataset consists
of 1238 scripts produced by learners taking the First
Certificate in English exam, assessing English at an
197
upper-intermediate level. For the cross-corpus tasks,
we used the essays by learners of the eight L1s in
the intersection with T11, i.e., Chinese (66 essays),
French (145), German (69), Italian (76), Japanese
(81), Korean (84), Spanish (198), and Turkish (73).
BALC: BUiD (British University in Dubai) Arab
Learner Corpus (Randall and Groom, 2009) The
BALC corpus consists of 1865 English learner texts
written by students with an Arabic L1 background
from the last year of secondary school and the first
year of university. The texts were scored and as-
signed to six proficiency levels. For the cross-corpus
NLI tasks, we used the data from the levels 3?5
amounting to overall 846 texts. We excluded the two
lowest and the highest, sixth level based on pretests
with the full BALC data.
ICNALE: International Corpus Network of
Asian Learners of English (Ishikawa, 2011) The
version of the ICNALE corpus we used consists of
5600 essays written by college students in ten coun-
tries and areas in Asia as well as by English na-
tive speakers. The learner essays are assigned to
four proficiency levels following the CEFR guide-
lines (A2, B1, B2, B2+). For the cross-corpus tasks,
we used the essays written by learners from Korea
(600 essays) and from Pakistan (400).1 Without ac-
cess to a corpus with Hindi as L1, we decided to la-
bel the essays written by Pakistani students as Hindi.
Most of the languages spoken in Pakistan, including
the official language Urdu, belong to the same Indo-
Aryan/-Iranian language family as Hindi. Our main
focus here was on avoiding overlap with Telugu, the
other Indian language in this shared task, which be-
longs to the Dravidian language family.
TU?TEL-NLI: Tu?bingen Telugu NLI Corpus We
collected 200 English texts written by Telugu native
speakers from bilingual (English-Telugu) blogs, lit-
erary articles, news and movie review websites.
NT11: NON-TOEFL11 We combined the ICLE,
FCE, ICNALE, BALC and TU?TEL-NLI sources
discussed above in the NT11 corpus consisting of
overall 5843 essays for 11 L1s, as shown in Table 1.
1We did not include ICNALE data for more L1s to avoid
overrepresentation of already well-represented Asian L1s.
Corpora
L1 ICLE FCE BALC ICNALE TU?TEL #
ARA - - 846 - - 846
CHI 982 66 - - - 1048
FRE 311 145 - - - 456
GER 431 69 - - - 500
HIN - - - 400 - 400
ITA 391 76 - - - 467
JPN 366 81 - - - 447
KOR - 84 - 600 - 684
SPA 248 198 - - - 446
TEL - - - - 200 200
TUR 276 73 - - - 349
# 3005 792 846 1000 200 5843
Table 1: Distribution of essays for the 11 L1s in NT11
3 Features
Recurring word-based n-grams (rc. word ng.)
Following, Bykh and Meurers (2012), we used all
word-based n-grams occurring in at least two texts
of the training set. We focused on recurring uni-
grams and bigrams, which in our previous work and
in T11 testing with the dev set worked best. For the
larger T11 train ? NT11 set, recurring n-grams up
to length five were best, but for uniformity we only
used word-based unigrams and bigrams for all tasks.
As in our previous work, we used a binary feature
representation encoding the presence or absence of
the n-gram in a given essay.
Recurring OCPOS-based n-grams (rc. OCPOS
ng.) All OCPOS n-grams occurring in at least two
texts of the training set were obtained as described
in Bykh and Meurers (2012). OCPOS means that
the open class words (nouns, verbs, adjectives and
cardinal numbers) are replaced by the corresponding
POS tags. For POS tagging we used the OpenNLP
toolkit (http://opennlp.apache.org).
In Bykh and Meurers (2012), recurring OCPOS
n-grams up to length three performed best. How-
ever, for T11 we found that including four- and five-
grams was beneficial. This confirms our assumption
that longer n-grams can be sufficiently common to
be useful (Bykh and Meurers, 2012, p. 433). Thus
we used the recurring OCPOS n-grams up to length
five for the experiments in this paper. We again used
a binary feature representation.
198
Recurring word-based dependencies (rc. word
dep.) Extending the perspective on recurring pieces
of data to other data types, we explored a new fea-
ture: recurring word-based dependencies. A feature
of this type consists of a head and all its immediate
dependents. The dependencies were obtained using
the MATE parser (Bohnet, 2010). The words in each
n-tuple are recorded in lowercase and listed in the or-
der in which they occur in the text; heads thus are not
singled out in this encoding. For example, the sen-
tence John gave Mary an interesting book yields the
following two potential features (john, gave, mary,
book) and (an, interesting, book). As with recur-
ring n-grams we utilized only features occurring in
at least two texts of the training set, and we used a
binary feature representation.
Recurring function-based dependencies (rc.
func. dep.) The recurring function-based depen-
dencies are a variant of the recurring word-based
dependencies described above, where each depen-
dent is represented by its grammatical function. The
above example sentence thus yields the two features
(sbj, gave, obj, obj) and (nmod, nmod, book).
Complexity Given that the proficiency level of a
learner was shown to play a role in NLI (Tetreault
et al, 2012), we implemented all the text com-
plexity features from Vajjala and Meurers (2012),
who used measures of learner language complex-
ity from SLA research for readability classification.
These features consist of lexical richness and syn-
tactic complexity measures from SLA research (Lu,
2010; 2012) as well as other syntactic parse tree
properties and traditionally used readability formu-
lae. The parse trees were built using the Berke-
ley parser (Petrov and Klein, 2007) and the syntac-
tic complexity measures were estimated using the
Tregex package (Levy and Andrew, 2006).
In addition, we included morphological and POS
features from the CELEX Lexical Database (Baayen
et al, 1995). The morphological properties of words
in CELEX include information about the deriva-
tional, inflectional and compositional features of
the words along with information about their mor-
phological origins and complexity. POS properties
of the words in CELEX describe the various at-
tributes of a word depending on its parts of speech.
We included all the non-frequency based and non-
word-string attributes from the English Morphology
Lemma (EML) and English Syntax Lemma (ESL)
files of the CELEX database. We also defined Age
of Acquisition features based on the psycholinguis-
tic database compiled by Kuperman et al (2012). Fi-
nally, we included the ratios of various POS tags to
the total number of words as POS density features,
using the POS tags from the Berkeley parser output.
Suffix features The use of different derivational
and inflectional suffixes may contain information
regarding the L1 ? either through L1 transfer, or
in terms of what suffixes are taught, e.g., for
nominalization. In a very basic approximation of
morphological analysis, we used the porter stem-
mer implementation of MorphAdorner (http://
morphadorner.northwestern.edu). For each
word in a learner text, we removed the stem
it identified from the word, and if a suffix re-
mained, we matched it against the Wiktionary list of
English suffixes (http://en.wiktionary.org/
wiki/Appendix:Suffixes:English). For each
valid suffix thus identified, we defined a binary fea-
ture (suffix, bin.) recording the presence/absence
and a feature counting the number of occurrences
(suffix, cnt.) in a given learner text.
Stem-suffix features We also wondered whether
the subset of morphologically complex unigrams
may be more indicative than considering all uni-
grams as features. As a simple approximation of this
idea, we used the stemmer plus suffix-list approach
mentioned above and used all words for which a suf-
fix was identified as features, both binary (stemsuf-
fix, bin.) and count-based (stemsuffix, cnt.).
Local trees Based on the syntactic trees assigned
by the Berkeley Parser (Petrov and Klein, 2007), we
extracted all local trees, i.e., trees of depth one. For
example, for the sentence I have a tree, the parser
output is: (ROOT (S (NP (PRP I)) (VP (VBP have)
(NP (DT a) (NN tree))) (. .))) for which the local
trees are (S NP VP .), (NP PRP), (NP DT NN), (VP
VBP NP), (ROOT S). Count-based features are used.
Stanford dependencies Tetreault et al (2012) ex-
plored the utility of basic dependencies as features
for NLI. In our approach, we extracted all Stanford
199
dependencies (de Marneffe et al, 2006) using the
trees assigned by the Berkeley Parser. We consid-
ered lemmatized typed dependencies (type dep. lm.)
such as nsubj(work,human) and POS tagged ones
(type dep. POS) such as nsubj(VB,NN) for our fea-
tures. We used count-based features for those typed
dependencies.
Dependency number (dep. num.) We encoded the
number of dependents realized by a verb lemma,
normalized by this lemma?s count. For example, if
the lemma take occurred ten times in a document,
three times with two dependents and seven times
with three dependents, we get the features take:2-
dependents = 3/10 and take:3-dependents = 7/10.
Dependency variability (dep. var.) These fea-
tures count possible dependent-POS combinations
for a verb lemma, normalized by this verb lemma?s
count. If in the example above, the lemma take
occurred three times with two dependents JJ-NN,
two times with three dependents JJ-NN-VB, and five
times with three dependents NN-NN-VB, we ob-
tain take:JJ-NN = 3/10, take:JJ-NN-VB = 2/10, and
take:NN-NN-VB = 5/10.
Dependency POS (dep. POS) These features are
derived from the dep. var. features and encode how
frequent which kind of category was a dependent for
a given verb lemma. Continuing the example above,
take takes dependents of three different categories:
JJ, NN and VB. For each category, we create a fea-
ture, the value of which is the category count divided
by the number of dependents of the given lemma,
normalized by the lemma?s count in the document.
In the example, we obtain take:JJ = (1/2 + 1/3)/10,
take:NN = (1/2 + 1/3 + 2/3)/10, and take:VB = (1/3
+ 1/3)/10.
Lemma realization matrix (lm. realiz.) We spec-
ified a set of features that is calculated for each dis-
tinct lemma and three feature sets generalizing over
all lemmas of the same category:
1. Distinct lemma counts of a specific category
normalized by the total count of this category
in a document. For example, if the lemma can
is found in a document two times as a verb and
five times as a noun, and the document contains
30 verbs and 50 nouns, we obtain the two fea-
tures can:VB = 2/30 and can:NN = 5/50.
2. Type-Lemma ratio: lemmas of same category
normalized by total lemma count
3. Type-Token ratio: tokens of same category nor-
malized by total token count
4. Lemma-Token Ratio: lemmas of same category
normalized by tokens of same category
Proficiency and prompt features Finally, for some
settings in the closed task we also included two nom-
inal features to encode the proficiency (low, medium,
high) and the prompt (P1?P8) features provided as
meta-data along with the T11 corpus.
4 Results
4.1 Evaluation Setup
We developed our approach with a focus on the
closed task, training the models on the T11 train set
and testing them on the T11 dev set. For the
closed task, we report the accuracies on the dev set
for all models (single feature type models and en-
sembles as introduced in sections 4.2 and 4.3),
before presenting the accuracies on the submitted
test set models, which were trained on the T11 train
? dev set. In addition, for the submitted models
we report the accuracies obtained via 10-fold cross-
validation on the T11 train ? dev set using the folds
specification provided by the organizers of the NLI
Shared Task 2013.
The results for the open-1 task are obtained by
training the models on the NT11 set, and the results
for the open-2 task are obtained by training the mod-
els on the T11 train ? dev set ? NT11 set. For the
open-1 and open-2 tasks, we report the basic single
feature type results on the T11 dev set and two sets
of results on the T11 test set: the results for the ac-
tual submitted systems and the results for the com-
plete systems, i.e., including the features used in the
closed task submissions that for the open tasks were
only computed after the submission deadline (given
our focus on the closed task and finite computational
infrastructure). We include the figures for the com-
plete systems to allow a proper comparison of the
performance of our models across the tasks.
Below we provide a description of the various ac-
curacies (%) we report for the different tasks:
200
? Acctest: Accuracy on the T11 test set after
training the model on:
? closed: T11 train ? dev set
? open-1: NT11 set
? open-2: T11 train ? dev set ? NT11 set
? Accdev: Accuracy on the T11 dev set after
training the model on:
? closed: T11 train set
? open-1: NT11 set
? open-2: T11 train set ? NT11 set
? Acc10train?dev: Accuracy on the T11 train ? dev
set obtained via 10-fold cross-validation using
the data split information provided by the orga-
nizers, applicable only for the closed task.
In terms of the tools used for classification, we
employed LIBLINEAR (Fan et al, 2008) using
L2-regularized logistic regression, LIBSVM (Chang
and Lin, 2011) using C-SVC with the RBF kernel
and WEKA SMO (Platt, 1998; Hall et al, 2009) fit-
ting logistic models to SVM outputs (the -M option).
Which classifier was used where is discussed below.
4.2 Single Feature Type Classifier Results
First we evaluated the performance of each fea-
ture separately for the closed task by computing the
Accdev values. These results constituted the basis
for the ensembles discussed in section 4.3. We also
report the corresponding results for the open-1 and
open-2 tasks, which were partly obtained after the
system submission and thus were not used for de-
veloping the approach. As classifier, we generally
used LIBLINEAR, except for complexity and lm.
realiz., where SMO performed consistently better.
The summary of the single feature type performance
is shown in Table 2.
The results reveal some first interesting insights
into the employed feature sets. The figures show
that the recurring word-based n-grams (rc. word ng.)
taken from Bykh and Meurers (2012) are the best
performing single feature type in our set yielding an
Accdev value of 81.3%. This finding is in line with
the previous research on different data sets showing
that lexical information seems to be highly relevant
for the task of NLI (Brooke and Hirst, 2011; Bykh
and Meurers, 2012; Jarvis et al, 2012; Jarvis and
Paquot, 2012; Tetreault et al, 2012). But also the
more abstract linguistic features, such as complexity
Accdev
Feature type closed open-1 open-2
1. rc. word ng. 81.3 42.0 80.3
2. rc. OCPOS ng. 67.6 26.6 64.8
3. rc. word dep. 67.7 30.9 69.4
4. rc. func. dep. 62.4 28.2 61.3
5. complexity 37.6 19.7 36.5
6. stemsuffix, bin. 50.3 21.4 48.8
7. stemsuffix, cnt. 48.2 19.3 47.1
8. suffix, bin. 20.4 9.1 17.5
9. suffix, cnt. 19.0 13.0 17.7
10. type dep. lm. 67.3 25.7 67.5
11. type dep. POS 46.6 27.8 27.6
12. local trees 49.1 26.2 25.7
13. dep. num. 39.7 19.6 41.8
14. dep. var. 41.5 18.6 40.1
15. dep. POS 47.8 21.5 47.4
16. lm. realiz. 70.3 30.3 66.9
Table 2: Single feature type results on T11 dev set
measures, local trees, or dependency variation mea-
sures seem to contribute relevant information, con-
sidering the random baseline of 9% for this task.
Having explored the performance of the single
feature type models, the interesting question was,
whether it is possible to obtain a higher accuracy
than yielded by the recurring word-based n-grams
by combining multiple feature types into a single
model. We thus investigated different combinations,
with a primary focus on the closed task.
4.3 Combining Feature Types
We followed Tetreault et al (2012) in exploring two
options: On the one hand, we combined the differ-
ent feature types directly in a single vector. On the
other hand, we used an ensemble classifier. The en-
semble setup used combines the probability distribu-
tions provided by the individual classifier for each
of the incorporated feature type models. The indi-
vidual classifiers were trained as discussed above,
and ensembles were trained and tested using LIB-
SVM, which in our tests performed better for this
purpose than LIBLINEAR. To obtain the ensemble
training files, we performed 10-fold cross-validation
for each feature model on the T11 train set (for in-
ternal evaluation) and on the T11 train ? dev set (for
201
submission) and took the corresponding probability
estimate distributions. For the ensemble test files,
we took the probability estimate distribution yielded
by each feature model trained on the T11 train set
and tested on the T11 dev set (for internal evalua-
tion), as well as by each feature model trained on
the T11 train ? dev set and tested on the T11 test set
(for submission).
In our tests, the ensemble classifier always outper-
formed the single vector combination, which is in
line with the findings of Tetreault et al (2012). We
thus focused on ensemble classification for combin-
ing the different feature types.
4.4 Closed Task (Main) Results
We submitted the predictions for the systems listed
in Table 3, which we chose in order to test all fea-
ture types together, the best performing single fea-
ture type, everything except for the best single fea-
ture type, and two subsets, with the latter primarily
including more abstract linguistic features.
id system description system type
1 overall system ensemble
2 rc. word ng. single model
3 #1 minus rc. word ng. ensemble
4 well performing subset ensemble
5 ?linguistic subset? ensemble
Table 3: Submitted systems for all three tasks
The results for the submitted systems are shown in
Table 4. Here and in the following result tables, the
system ids in the table headers correspond to the ids
in Table 3, the best result on the test set is shown in
bold, and the symbols have the following meaning:
? x = feature type used
? - = feature type not used
? -* = feature type ready after submission
We report theAcctest,Accdev andAcc10train?dev ac-
curacies introduced in section 4.1. The Accdev re-
sults are consistently better than the Acctest results,
highlighting that relying on a single development
set can be problematic. The cross-validation results
are more closely aligned with the ultimate test set
performance.
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. x - x x -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. x - x - x
11. type dep. POS x - x - x
12. local trees x - x - x
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
proficiency x - x x -
prompt x - x x -
Acctest 82.2 79.6 81.0 81.5 74.7
Accdev 85.4 81.3 83.5 84.9 76.3
Acc10train?dev 82.4 78.9 80.7 81.7 74.1
Table 4: Results for the closed task
Overall, comparing the results for the different
systems shows the following main points (with the
system ids in the discussion shown in parentheses):
? The overall system performed better than any
single feature type alone (cf. Tables 2 and 4).
The ensemble thus is successful in combining
the strengths of the different feature types.
? The rc. word ng. feature type alone (2) per-
formed very well, but the overall system with-
out that feature type (3) still outperformed it.
Thus apparently the different properties ac-
cessed by more elaborate linguistic modelling
contribute some information not provided by
the surface-based n-gram feature.
? A system incorporating a subset of the differ-
ent feature types (4) performed still reasonably
well. Hence, it is conceivable that a subsys-
tem consisting of some selected feature types
would perform equally well (eliminating only
information present in multiple feature types)
or even outperform the overall system (by re-
moving some noise). This point will be inves-
tigated in detail in our future work.
202
? System 5, combining a subset of feature types,
where each one incorporates some degree
of linguistic abstraction (in contrast to pure
surface-based feature types such as word-based
n-grams), performed at a reasonably high level,
supporting the assumption that incorporating
more linguistic knowledge into the system de-
sign has something to contribute.
Putting our results into the context of the NLI
Shared Task 2013, with our best Acctest value of
82.2% for closed as the main task, we ranked fifth
out of 29 participating teams. The best result in
the competition, obtained by the team ?Jarvis?, is
83.6%. According to the significance test results
provided by the shared task organizers, the differ-
ence of 1.4% is not statistically significant (0.124
for pairwise comparison using McNemar?s test).
4.5 Open-1 Task Results
The Accdev values for the single feature type models
for the open-1 task were included in Table 2. The
results for the test set are presented in Table 5. We
report two different Acctest values: the accuracy for
the actual submitted systems (Acctest) and for the
corresponding complete systems (Acctest with ?) as
discussed in section 4.1.
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. x - x x -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. -? - -? - -?
11. type dep. POS -? - -? - -?
12. local trees -? - -? - -?
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
Acctest 36.4 38.5 33.2 37.8 21.2
Acctest with ? 37.0 n/a 35.4 n/a 29.9
Table 5: Results for the open-1 task
Conceptually, the open-1 task is a cross-corpus
task, where we used the NT11 data for training and
T11 data for testing. It is more challenging for sev-
eral reasons. First, the models are trained on data
that is likely to be different from the one of the
test set in a number of respects, including possible
differences in genre, task and topic, or proficiency
level. Second, the amount of data we were able to
obtain to train our model is far below what was pro-
vided for the closed task. Thus a drop in accuracy is
to be expected.
Particularly interesting is the fact that our best re-
sult for the open-1 task (38.5%) was obtained using
the rc. word ng. feature type alone. Thus adding
the more abstract features did not improve the accu-
racy. The reason for that may be the smaller train-
ing corpus size, the uneven distribution of the texts
among the different L1s in the NT11 corpus, or the
mentioned potential differences between NT11 and
T11 in genre, task and topic, and learner proficiency.
Also interesting is the fact that the system combining
a subset of feature types outperformed the overall
system. This finding supports the assumption men-
tioned in section 4.4 that the ensemble classifier can
be optimized by informed, selective model combina-
tion instead of combining all available information.
To put our results into the context of the NLI
Shared Task 2013, our best Acctest value of 38.5%
for the open-1 task achieved rank two out of three
participating teams. The best accuracy of 56.5% was
obtained by the team ?Toronto?. While the open-
1 task results in general are much lower than the
closed task results, highlighting an important chal-
lenge for future NLI work, they nevertheless are
meaningful steps forward considering the random
baseline of 9%.
4.6 Open-2 Task Results
For the open-2 task we provide the same information
as for open-1. The Accdev values for the single fea-
ture type models are shown in Table 2, and the two
Acctest values, i.e., the accuracy for the actual sub-
mitted systems (Acctest) and for the complete sys-
tems (Acctest with ?) can be found in Table 6.
For the open-2 task, we put the T11 train ?
dev and NT11 sets together to train our models. The
interesting question behind this task is, whether it is
possible to improve the accuracy of NLI by adding
203
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. -? - -? -? -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. -? - -? - -?
11. type dep. POS x - x - x
12. local trees x - x - x
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
Acctest 83.5 81.0 79.3 82.5 64.8
Acctest with ? 84.5 n/a 83.3 82.9 79.8
Table 6: Results for the open-2 task
data from corpora other than the one used for test-
ing. This is far from obvious, especially considering
the low results obtained for the open-1 task pointing
to significant differences between the T11 and the
NT11 corpora.
Overall, when using all feature types, our results
for the open-2 task (84.5%) are better than those we
obtained for the closed task (82.2%). So adding data
from a different domain improves the results, which
is encouraging since it indicates that something gen-
eral about the language used is being learned, not
(just) something specific to the T11 corpus. Essen-
tially, the open-2 task also is closest to the real-world
scenario of using whatever resources are available to
obtain the best result possible.
Putting the results into the context of the NLI
Shared Task 2013, our best Acctest value of 83.5%
(84.5%) is the highest accuracy for the open-2 task,
i.e, first rank out of four participating teams.
5 Conclusions
We explored the task of Native Language Identifi-
cation using a range of different feature types in the
context of the NLI Shared Task 2013. We consid-
ered surface features such as recurring word-based
n-grams system as our basis. We then explored
the contribution and usefulness of some more elab-
orate, linguistically motivated feature types for the
given task. Using an ensemble model combining
features based on POS, dependency, parse trees as
well as lemma realization, complexity and suffix in-
formation features, we were able to outperform the
high accuracy achieved by the surface-based recur-
ring n-grams features alone. The exploration of
linguistically-informed features thus is not just of
analytic interest but can also make a quantitative dif-
ference for obtaining state-of-the-art performance.
In terms of future work, we have started exploring
the various feature types in depth to better under-
stand the causalities and correlations behind the re-
sults obtained. We also intend to explore more com-
plex linguistically motivated features further, such
as features based on syntactic alternations as used in
Krivanek (2012). Studying such variation of linguis-
tic properties, instead of recording their presence as
we mostly did in this exploration, also stands to pro-
vide a more directly interpretable perspective on the
feature space identified as effective for NLI.
Acknowledgments
We thank Dr. Shin?ichiro Ishikawa and Dr. Mick
Randall for providing access to the ICNALE corpus
and the BALC corpus respectively. We also thank
the shared task organizers for organizing this inter-
esting competition and sharing the TOEFL11 cor-
pus. Our research is partially funded through the Eu-
ropean Commission?s 7th Framework Program un-
der grant agreement number 238405 (CLARA).
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX lexical database (cd-rom). CDROM,
http://www.ldc.upenn.edu/Catalog/
readme_files/celex.readme.html.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native english. Technical report, Edu-
cational Testing Service.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
24th International Conference on Computational Lin-
guistics (COLING), pages 89?97.
Julian Brooke and Graeme Hirst. 2011. Native lan-
guage detection with ?cheap? learner corpora. In
204
Learner Corpus Research 2011 (LCR 2011), Louvain-
la-Neuve.
Serhiy Bykh and Detmar Meurers. 2012. Native lan-
guage identification using recurring n-grams ? in-
vestigating abstraction and domain dependence. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 425?
440, Mumbay, India.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of the 5th International Conference on Language
Resources and Evaluation (LREC-2006), Genoa, Italy,
May 24-26.
R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J.
Lin. 2008. Liblinear: A library for large linear classi-
fication. The Journal of Machine Learning Research,
9:1871?1874. Software available at http://www.
csie.ntu.edu.tw/?cjlin/liblinear.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and
Magali Paquot, 2009. International Corpus of Learner
English, Version 2. Presses Universitaires de Louvain,
Louvain-la-Neuve.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update. In
The SIGKDD Explorations, volume 11, pages 10?18.
Shin?ichiro Ishikawa. 2011. A new horizon in learner
corpus studies: The aim of the ICNALE projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
pora and language technologies in teaching, learning
and research, pages 3?11. University of Strathclyde
Publishing, Glasgow, UK. http://language.
sakura.ne.jp/icnale/index.html.
Scott Jarvis and Magali Paquot. 2012. Exploring the
role of n-grams in L1-identification. In Scott Jarvis
and Scott A. Crossley, editors, Approaching Language
Transfer through Text Classification: Explorations in
the Detection-based Approach, pages 71?105. Multi-
lingual Matters.
Scott Jarvis, Gabriela Castan?eda-Jime?nez, and Rasmus
Nielsen. 2004. Investigating L1 lexical transfer
through learners? wordprints. Presented at the 2004
Second Language Research Forum. State College,
Pennsylvania, USA.
Scott Jarvis, Gabriela Castan?eda-Jime?nez, and Rasmus
Nielsen. 2012. Detecting L2 writers? L1s on the
basis of their lexical styles. In Scott Jarvis and
Scott A. Crossley, editors, Approaching Language
Transfer through Text Classification: Explorations in
the Detection-based Approach, pages 34?70. Multilin-
gual Matters.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining (KDD ?05), pages 624?628,
New York.
Julia Krivanek. 2012. Investigating syntactic alternations
as characteristic features of learner language. Master?s
thesis, University of Tu?bingen, April.
Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc
Brysbaert. 2012. Age-of-acquisition ratings for
30,000 english words. Behavior Research Methods,
44(4):978?990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474?496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Languages Journal.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York, April.
John C. Platt. 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector ma-
chines. Technical Report MSR-TR-98-14, Microsoft
Research.
Mick Randall and Nicholas Groom. 2009. The BUiD
Arab learner corpus: a resource for studying the ac-
quisition of L2 english spelling. In Proceedings of the
Corpus Linguistics Conference (CL), Liverpool, UK.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native lan-
guage identification. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING), pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
205
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from round here, are you? naive bayes de-
tection of non-native utterance text. In Proceedings of
the 2nd Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 239?246.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In Joel
Tetreault, Jill Burstein, and Claudial Leacock, editors,
Proceedings of the 7th Workshop on Innovative Use
of NLP for Building Educational Applications (BEA7)
at NAACL-HLT, pages 163?-173, Montre?al, Canada,
June. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automati-
cally grading ESOL texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 180?189, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Corpus available from http://ilexir.co.uk/
applications/clc-fce-dataset.
206
