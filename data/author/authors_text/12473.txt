Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 97?102,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Parsing Syntactic and Semantic Dependencies for Multiple Languages 
with A Pipeline Approach 
 
Han Ren, Donghong Ji Jing Wan, Mingyao Zhang 
School of Computer Science Center for Study of Language & Information 
Wuhan University Wuhan University 
Wuhan 430079, China Wuhan 430079, China 
cslotus@mail.whu.edu.cn 
donghong_ji@yahoo.com 
{jennifer.wanj, my.zhang}@gmail.com
 
 
 
Abstract 
This paper describes a pipelined approach for 
CoNLL-09 shared task on joint learning of 
syntactic and semantic dependencies. In the 
system, we handle syntactic dependency pars-
ing with a transition-based approach and util-
ize MaltParser as the base model. For SRL, 
we utilize a Maximum Entropy model to iden-
tify predicate senses and classify arguments. 
Experimental results show that the average 
performance of our system for all languages 
achieves 67.81% of macro F1 Score, 78.01% 
of syntactic accuracy, 56.69% of semantic la-
beled F1, 71.66% of macro precision and 
64.66% of micro recall. 
1 Introduction 
Given a sentence with corresponding part-of-
speech for each word, the task of syntactic and se-
mantic dependency parsing contains two folds: (1) 
identifying the syntactic head of each word and 
assigning the dependency relationship between the 
word and its head; (2) identifying predicates with 
proper senses and labeling semantic dependencies 
for them. 
For data-driven syntactic dependency parsing, 
many approaches are based on supervised learning 
using treebank or annotated datasets. Currently, 
graph-based and transition-based algorithms are 
two dominating approaches that are employed by 
many researchers, especially in previous CoNLL 
shared tasks. Graph-based algorithms (Eisner, 
1996; McDonald et al, 2005) assume a series of 
dependency tree candidates for a sentence and the 
goal is to find the dependency tree with highest 
score. Transition-based algorithms (Yamada and 
Matsumoto, 2003; Nivre et al, 2004) utilize transi-
tion histories learned from dependencies within 
sentences to predict next state transition and build 
the optimal transition sequence. Although different 
strategies were considered, two approaches yielded 
comparable results at previous tasks. 
Semantic role labeling contains two problems: 
identification and labeling. Identification is a bi-
nary classification problem, and the goal is to iden-
tify annotated units in a sentence; while labeling is 
a multi-class classification problem, which is to 
assign arguments with appropriate semantic roles. 
Hacioglu (2004) utilized predicate-argument struc-
ture and map dependency relations to semantic 
roles. Liu et al (2005) combined two problems 
into a classification one, avoiding some annotated 
units being excluded due to some incorrect identi-
fication results. In addition, various features are 
also selected to improve accuracy of SRL. 
In this paper, we propose a pipelined approach 
for CoNLL-09 shared task on joint learning of syn-
tactic and semantic dependencies, and describe our 
system that can handle multiple languages. In the 
system, we handle syntactic dependency parsing 
with a transition-based approach. For SRL, we util-
ize Maximum Entropy model to identify predicate 
senses and classify arguments. 
The remain of the paper is organized as follows. 
In Section 2, we discuss the processing mechanism 
containing syntactic and semantic dependency 
parsing of our system in detail. In Section 3, we 
give the evaluation results and analysis. Finally, 
the conclusion and future work are given in Sec-
tion 4. 
97
2 System Description  
The system, which is a two-stage pipeline, proc-
esses syntactic and semantic dependencies respec-
tively. To reduce the difficulties in SRL, predicates 
of each sentence in all training and evaluation data 
are labeled, thus predicate identification can be 
ignored. 
 
Figure 1. System Architectures 
 
For syntactic dependencies, we employ a state-
of-the-art dependency parser and basic plus ex-
tended features for parsing. For semantic depend-
encies, a Maximum Entropy Model is used both in 
predicate sense identification and semantic role 
labeling. Following subsections will show compo-
nents of our system in detail. 
2.1 Syntactic Dependency Parsing 
In the system, MaltParser1 is employed for syntac-
tic dependency parsing. MaltParser is a data-driven 
deterministic dependency parser, based on a Sup-
port Vector Machine classifier. An extensive re-
search (Nivre, 2007) parsing with 9 different 
languages shows that the parser is language-
independent and yields good results. 
MaltParser supports two kinds of parsing algo-
rithms: Nivre?s algorithms and Covington?s incre-
mental algorithms. Nivre?s algorithms, which are 
deterministic algorithms consisting of a series of 
shift-reduce procedures, defines four operations: 
?Right. For a given triple <t|S, n|I, A>, S 
represents STACK and I represents INPUT. If 
dependency relation t ? n exists, it will be 
                                                          
1 http://w3.msi.vxu.se/~jha/maltparser/ 
pendency relation t?n exists, it will be appended 
into A and t will be removed from S. 
?Left. For a given triple <t|S, n|I, A>, if de-
pendency relation n?t exists, it will be appended 
into A and n will be pushed into S. 
?Reduce. If dependency relation n?t does not 
exist, and the parent node of t exists left to it, t will 
be removed from S. 
?Shift. If none of the above satisfies, n will be 
pushed into S. 
The deterministic algorithm simplifies determi-
nation for Reduce operation. As a matter of fact, 
some languages, such as Chinese, have more flexi-
ble word order, and some words have a long dis-
tance with their children. In this case, t should not 
be removed from S, but be handled with Shift op-
eration. Otherwise, dependency relations between t 
and its children will never be identified, thus se-
quential errors of dependency relations may occur 
after the Reduce operation. 
For syntactic dependencies with long distance, 
an improved Reduce strategy is: if the dependency 
relation between n and t does not exist, and the 
parent node of t exists left to it and the dependency 
relation between the parent node and n, t will be 
removed from S. The Reduce operation is projec-
tive, since it doesn?t influence the following pars-
ing procedures. The Improved algorithm is 
described as follows: 
(1) one of the four operations is performed ac-
cording to the dependency relation between t and n 
until EOS; if only one token remains in S, go to (3). 
(2) continue to select operations for remaining 
tokens in S; when Shift procedure is performed, 
push t to S; if only one token remains in S and I 
contains more tokens than only EOS, goto (1). 
(3) label all odd tokens in S as ROOT, pointing 
to EOS. 
We also utilize history-based feature models 
implemented in the parser to predict the next action 
in the deterministic derivation of a dependency 
structure. The parser provides some default fea-
tures that is general for most languages: (1) part-
of-speech features of TOP and NEXT and follow-
ing 3 tokens; (2) dependency features of TOP con-
taining leftmost and rightmost dependents, and of 
NEXT containing leftmost dependents; (3) Lexical 
98
features of TOP, head of TOP, NEXT and follow-
ing one token. We also extend features for multiple 
languages: (1) count of part-of-speech features of 
following tokens extend to 5; (2) part-of-speech 
and dependent features of head of TOP. 
2.2 Semantic Dependency Parsing 
Each defacto predicate in training and evaluation 
data of CoNLL09 is labeled with a sign ?Y?, which 
simplifies the work of semantic dependency pars-
ing. In our system, semantic dependency parsing is 
a pipeline that contains two parts: predicate sense 
identification and semantic role labeling. For 
predicate sense identification, each predicate is 
assigned a certain sense number. For semantic role 
labeling, local and global features are selected. 
Features of each part are trained by a classification 
algorithm.  Both parts employ a Maximum Entropy 
Tool MaxEnt in a free package OpenNLP 2 as a 
classifier. 
2.2.1  Predicate Sense Identification 
The goal of predicate sense identification is to de-
cide the correct frame for a predicate. According to 
PropBank (Palmer, et al, 2005), predicates contain 
one or more rolesets corresponding to different 
senses. In our system, a classifier is employed to 
identify each predicate?s sense.  
Suppose { }01, 02, , LC = ? N
s t
                                                          
 is the sense set 
(NL is the count of categories corresponding to the 
language L, eg., in Chinese training set NL = 10 
since predicates have at most 10 senses in the set), 
and ti is the ith sense of word w in sentence s. The 
model is implemented to assign each predicate to 
the most probatilistic sense. 
( | , )i C it P w?=argmax                (1) 
Features for predicate sense identification are 
listed as follows: 
?WORD, LEMMA, DEPREL: The lexical 
form and lemma of the predicate; the dependency 
relation between the predicate and its head; for 
Chinese and Japanese, WORD is ignored. 
? HEAD_WORD, HEAD_POS: The lexical 
form and part-of-speech of the head of the predi-
cate. 
2 http://maxent.sourceforge.net/ 
? CHILD_WORD_SET, CHILD_POS_SET, 
CHILD_DEP_SET: The lexical form, part-of-
speech and dependency relation of dependents of 
the predicate. 
?LSIB_WORD, LSIB_POS, LSIB_DEPREL, 
RSIB_WORD, RSIB_POS, RSIB_DEPREL: The 
lexical form, part-of-speech and dependency rela-
tion of the left and right sibling token of the predi-
cate. Features of sibling tokens are adopted, 
because senses of some predicates can be inferred 
from its left or right sibling. 
For English data set, we handle verbal and 
nominal predicates respectively; for other lan-
guages, we handle all predicates with one classifier. 
If a predicate in the evaluation data does not exist 
in the training data, it is assigned the most frequent 
sense label in the training data. 
2.2.2  Semantic Role Labeling 
Semantic role labeling task contains two parts: ar-
gument identification and argument classification. 
In our system the two parts are combined as one 
classification task. Our reason is that those argu-
ment candidates that potentially become semantic 
roles of corresponding predicates should not be 
pruned by incorrect argument identification. In our 
system, a predicate-argument pair consists of any 
token (except predicates) and any predicate in a 
sentence. However, we find that argument classifi-
cation is a time-consuming procedure in the ex-
periment because the classifier spends much time 
on a great many of invalid predicate-argument 
pairs. To reduce useless computing, we add a sim-
ple pruning method based on heuristic rules to re-
move invalid pairs, such as punctuations and some 
functional words.  
Features used in our system are based on (Ha-
cioglu, 2004) and (Pradhan et al 2005), and de-
scribed as follows:  
?WORD, LEMMA, DEPREL: The same with 
those mentioned in section 2.2.1. 
?VOICE: For verbs, the feature is Active or 
Passive; for nouns, it is null. 
?POSITION: The word?s position correspond-
ing to its predicate: Left, Right or Self. 
?PRED: The lemma plus sense of the word. 
?PRED_POS: The part-of-speech of the predi-
cate. 
99
?LEFTM_WORD, LEFTM_POS, RIGHTM_ 
WORD, RIGHTM_POS: Leftmost and rightmost 
word and their part-of-speech of the word. 
? POS_PATH: All part-of-speech from the 
word to its predicate, including Up, Down, Left 
and Right, eg. ?NN?VV?CC?VV?. 
?DEPREL_PATH: Dependency relations from 
the word to its predicate, eg. ?COMP?RELC?
COMP??. 
?ANC_POS_PATH, ANC_DEPREL_PATH: 
Similar to POS_PATH and DEPREL_PATH, part-
of-speech and dependency relations from the word 
to the common ancestor with its predicate. 
?PATH_LEN: Count of passing words from 
the word to its predicate. 
? FAMILY: Relationship between the word 
and its predicate, including Child, Parent, Descen-
dant, Ancestor, Sibling, Self and Null. 
? PRED_CHD_POS, PRED_CHD_DEPREL: 
Part-of-speech and dependency relations of all 
children of the word?s predicate. 
For different languages, some features men-
tioned above are invalid and should be removed, 
and some extended features could improve the per-
formance of the classifier. In our system we mainly 
focus on Chinese, therefore, WORD and VOICE 
should be removed when processing Chinese data 
set. We also adopt some features proposed by (Xue, 
2008): 
? POS_PATH_BA, POS_PATH_SB, POS_ 
PATH_LB: BA and BEI are functional words that 
impact the order of arguments. In PropBank, BA 
words have the POS tag BA, and BEI words have 
two POS tags: SB (short BEI) and LB (long BEI). 
3 Experimental Results  
Our experiments are based on a PC with a Intel 
Core 2 Duo 2.1G CPU and 2G memory. Training 
and evaluation data (Taul? et al, 2008; Xue et al, 
2008; Haji? et al, 2006; Palmer et al, 2002; Bur-
chardt et al, 2006; Kawahara et al, 2002) have 
been converted to a uniform CoNLL Shared Task 
format. In all experiments, SVM and ME model 
are trained using training data, and tested with 
development data of all languages.  
The system for closed challenge is designed as 
two parts. For syntactic dependency training and 
parsing, we utilize the projective model in Malt-
Parser for data sets. We also follow default settings 
in MaltParser, such as assigned parameters for 
LIBSVM and combined prediction strategy, and 
utilize improved approaches mentioned in section 
2. For semantic dependency training and parsing, 
we choose the count of iteration as 100 and cutoff 
value as 10 for the ME model. Table 1 shows the 
training time for syntactic and semantic depend-
ency of all languages. Parsing time for syntactic is 
not more than 30 minutes, and for semantic is not 
more than 5 minutes of each language. 
 
 syn prd sem 
English 7h 12min 47min 
Chinese 8h 18min 61min 
Japanese 7h 14min 46min 
Czech 13h 46min 77min 
German 6h 16min 54min 
Spanish 6h 15min 55min 
Catalan 6h 15min 50min 
Table 1. Training cost for all languages. syn, prd and 
sem mean training time for syntactic dependency, predi-
cate identification and semantic dependency. 
3.1 Syntactic Dependency Parsing 
We utilize MaltParser with improved algorithms 
mentioned in section 2.1 for syntactic dependency 
parsing, and the results are shown in Table 2. 
 
 LAS UAS label-acc. 
English 87.57 89.98 92.19 
Chinese 79.17 81.22 85.94 
Japanese 91.47 92.57 97.28 
Czech 57.30 75.66 65.39 
German 76.63 80.31 85.97 
Spanish 76.11 84.40 84.69 
Catalan 77.84 86.41 85.78 
Table 2. Performance of syntactic dependency parsing 
 
Table 2 indicates that parsing for Japanese and 
English data sets has a better performance than 
other languages, partly because determinative algo-
rithm and history-based grammar are more suited 
for these two languages. To compare the perform-
ance of our approach of improved deterministic 
algorithm and extended features, we make another 
experiment that utilize original arc-standard algo-
rithm and base features for syntactic experiments. 
Due to time limitation, the experiments are only 
based on Chinese training and evaluation data. The 
results show that LAS and UAS drops about 2.7% 
and 2.2% for arc-standard algorithm, 1.6% and 
1.2% for base features. They indicate that our de-
100
terministic algorithm and the extend features can 
help to improve syntactic dependency parsing. We 
also notice that the results of Czech achieve a 
lower performance than other languages. It mainly 
because the language has more rich morphology, 
usually accompanied by more flexible word order. 
Although using a large training set, linguistic prop-
erties greatly influence the parsing result. In addi-
tion, extended features are not suited for this 
language and the feature model should be opti-
mized individually. 
For all of the experiments we mainly focus on 
the language of Chinese. When parsing Chinese 
data sets we find that the focus words where most 
of the errors occur are almost punctuations, such as 
commas and full stops. Apart from errors of punc-
tuations, most errors occur on prepositions such as 
the Chinese word ?at?. Most of these problems 
come from assigning the incorrect dependencies, 
and the reason is that the parsing algorithm con-
cerns the form rather than the function of these 
words. In addition, the prediction of dependency 
relation ROOT achieves lower precision and recall 
than others, indicating that MaltParser overpredicts 
dependencies to the root. 
3.2 Semantic Dependency Parsing 
MaxEnt is employed as our classifier to train and 
parse semantic dependencies, and the results are 
shown in Table 3, in which all criterions are la-
beled. 
 
 P R F1 
English 76.57 60.45 67.56 
Chinese 75.45 69.92 72.58 
Japanese 91.93 43.15 58.73 
Czech 68.83 57.78 62.82 
German 62.96 47.75 54.31 
Spanish 40.11 39.50 39.80 
Catalan 41.34 40.66 41.00 
Table 3. Performance of semantic dependency parsing 
 
As shown in Table 3, the scores of the latter 
five languages are quite lower than those of the 
former two languages, and the main reason could 
be inferred from the scores of Table 2 that the drop 
of the performance of semantic dependency pars-
ing comes from the low performance of syntactic 
dependency parsing. Another reason is that, mor-
phological features are not be utilized in the classi-
fier. Our post experiments after submission show 
that average performance could improve the per-
formance after adding morphological and some 
combined features. In addition, difference between 
precision and recall indicates that the classification 
procedure works better than the identification 
procedure in semantic role labeling.  
For Chinese, semantic role of some words with 
part-of-speech VE have been mislabeled. It?s 
mainly because that these words in Chinese have 
multiple part-of-speech. The errors of POS and 
PRED greatly influence the system to perform 
these words. Another main problem occurs on the 
pairs NN + A0/A1. Identification of the two pairs 
are much lower than VA/VC/VE/VV + A0/A1 
pairs. The reason is that the identification of nomi-
nal predicates have more errors than that of verbal 
predicates due to the combination of SRL for these 
two kinds of predicates. For further study, verbal 
predicates and nominal predicates should be han-
dled respectively so that the overall performance 
can be improved.  
3.3 Overall Performance 
The average performance of our system for all lan-
guages achieves 67.81% of macro F1 Score, 
78.01% of syntactic accuracy, 56.69% of semantic 
labeled F1, 71.66% of macro precision and 64.66% 
of micro recall. 
4 Conclusion 
In this paper, we propose a pipelined approach for 
CoNLL-09 shared task on joint learning of syntac-
tic and semantic dependencies, and describe our 
system that can handle multiple languages. Our 
system focuses on improving the performance of 
syntactic and semantic dependency respectively. 
Experimental results show that the overall per-
formance can be improved for multiple languages 
by long distance dependency algorithm and ex-
tended history-based features. Besides, the system 
fits for verbal predicates than nominal predicates 
and the classification procedure works better than 
identification procedure in semantic role labeling. 
For further study, respective process should be 
handled between these two kinds of predicates, and 
argument identification should be improved by 
using more discriminative features for a better 
overall performance. 
101
Acknowledgments 
This work is supported by the Natural Science 
Foundation of China under Grant Nos.60773011, 
90820005, and Independent Research Foundation 
of Wuhan University. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2006). Genoa, Italy. 
Jason M. Eisner. 1996. Three new probabilistic models 
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pp.340?345. 
Kadri Hacioglu. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics 
(COLING). 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie Mikulo-
v? and Zden?k ?abokrtsk?. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data 
Consortium, Philadelphia, Pennsylvania, USA. ISBN 
1-58563-370-4. LDC Cat. No. LDC2006T01. URL: 
http://ldc.upenn.edu. 
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL 2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th 
Conference on Computational Natural Language 
Learning (CoNLL-2009). Boulder, Colorado, USA. 
June 4-5. pp.3-22. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp.2008-2013. 
Ryan McDonald, Koby Crammer, and Fernando Pereira.  
2005. Online large-margin training of dependency 
parsers. In Proceedings of the 43rd Annual Meeting of 
the Association for Computational Linguistics (ACL), 
pp.91?98. 
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. 
Memory-based dependency parsing. In Proceedings 
of the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL), pp.49?56. 
Joakim Nivre. 2004. Incrementality in Deterministic 
Dependency Parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop 
at ACL-2004, Barcelona, Spain, pp.50-57. 
Joakim Nivre and Johan Hall. 2005. MaltParser: A lan-
guage-independent system for data-driven depend-
ency parsing. In Proceedings of the Fourth Workshop 
on Treebanks and Linguistic Theories (TLT). 
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, 
Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov 
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural language Engineering, Volume 13, Is-
sue 02, pp.95-135. 
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin and Daniel Jurafsky. 
2005. Support Vector Learning for Semantic Argu-
ment classification. Machine Learning Journal, 2005, 
60(3): 11?39. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of 
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008). 
Mariona Taul?, Maria Ant?nia Mart? and Marta Reca-
sens. 2008. AnCora: Multilevel Annotated Corpora 
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC-2008). Marrakech, Morocco. 
Liu Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and 
Huaijun Liu. 2005. Semantic role labeling system us-
ing maximum entropy classifier. In Proceedings of 
the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL). 
Nianwen Xue. 2008. Labeling Chinese Predicates with 
Semantic roles. Computational Linguistics, 34(2): 
225-255. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank.  Natural Language 
Engineering, 15(1):143-172. 
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector ma-
chines. In Proceedings of the 8th International 
Workshop on Parsing Technologies (IWPT), pp.195?
206. 
 
102
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1601?1611, Dublin, Ireland, August 23-29 2014.
 
 Word Sense Induction Using Lexical Chain based Hypergraph Model 
 
Tao Qian1,3, Donghong Ji*1, Mingyao Zhang2, Chong Teng1, and Congling Xia1 
(1) Computer School, Wuhan University, Wuhan, China 
?2?College of Foreign Languages and Literature, Wuhan University, Wuhan, China 
 (3) College of Computer Science and Technology, Hubei University of  
Science and Technology,XianNing, China 
{taoqian, dhji, myzhang, tengchong, clxia}@whu.edu.cn 
 
 
Abstract 
Word Sense Induction is a task of automatically finding word senses from large scale texts. It is general-
ly considered as an unsupervised clustering problem. This paper introduces a hypergraph model in 
which nodes represent instances of contexts where a target word occurs and hyperedges represent high-
er-order semantic relatedness among instances. A lexical chain based method is used for discovering the 
hyperedges, and hypergraph clustering methods are used for finding word senses among the context in-
stances. Experiments show that this model outperforms other methods in supervised evaluation and 
achieves comparable performance with other methods in unsupervised evaluation.  
1 Introduction 
Word sense induction (WSI) aims to automatically find senses of a given target word (Yarowsky, 
1995) from large scale texts. Compared with existing manual word sense resources, WSI techniques 
use clustering algorithms to determine the possible senses for a word. 
Word sense induction is generally considered as an unsupervised clustering problem. The input for 
the clustering algorithm is context instances of a target word, represented by word bags or co-
occurrence vectors, and the output is a grouping of these instances into classes, each corresponding to 
an induced sense.  
Traditional methods in WSI tend to adopt the vector space model, in which the context of each in-
stance of a target word is represented as a vector of features based on frequency statistics and proba-
bility distributions, e.g., first-order or second-order vector (Sch?tze, 1998; Purandare and Pedersen, 
2004; Cruys et al., 2011). These vectors are clustered and the resulting clusters represent the induced 
senses. Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; 
V?ronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope 
and Keller, 2013), which have been recently explored successfully to some extent. Graph-based me-
thods are considering the notion of a co-occurrence graph, assuming a binary relatedness between co-
occurring words. 
One of the key challenges in WSI is learning the higher-order semantic relatedness among multiple 
context instances. Previous approaches (Klapaftis and Manandhar. 2007; Bordag, 2006) for WSI are 
used to construct higher-order relatedness by counting co-occurrence frequency or collocation of muti-
words, regardless of global semantic similarity. 
Lexical chain (Morris and Hirst, 1991) is defined as a sequence of semantically related words in text 
and provides important clues about the text structure and topic. It can be viewed as a global counter-
part of the measures of semantic similarity (Navigli, 2009). For example, Figure 1 gives three context 
instances containing Apple.  
                                                 
* Corresponding author E-mail: dhji@whu.edu.cn. 
 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings foo-
ter are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
1601
  Figure 1. Context instances of Apple 
 
Obviously, four Apples in Figure 1 all refer to the Apple Company. We can directly group three in-
stances by the lexical chain: iPod-iTunes-hardware and software product-Inc. This lexical chain 
represents a higher-order semantic relatedness among the three instances. 
In this paper, we propose a hypergraph model from a global perspective, in which nodes represent 
instances of contexts where a target word occurs and hyperedges denote higher-order semantic rela-
tedness among instances. A lexical chain based method is used for identifying the hyperedges. This 
method for lexical chain extraction is a knowledge-free method based on LDA topic model (Remus 
and Biemann, 2013).  
The remainder of this paper is structured as follows. Section 2 presents an overview of the related 
work. Section 3 describes our model in details. Section 4 provides a quantitative evaluation and com-
parison with other algorithms in the SemEval-2013 word sense induction task. Finally, section 5 draws 
conclusions and lays out some future research directions.  
2 Related Work 
2.1 Word sense induction 
A number of diverse approaches to WSI have been proposed so far. Context features are often 
represented in a variety of forms such as co-occurrence of words within phrases (Pantel and Lin, 2002; 
Dorow and Widdows, 2003), parts of speeches (Purandare and Pedersen, 2004), and grammatical rela-
tions (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, 
such as two words before and after the target word, the sentence or even larger paragraph within which 
contains the target word. 
Most of the work in WSI is the vector space model, such as context-based vector algorithm 
(Sch?tze, 1998; Ide et al., 2001; Van de Cruys et al., 2011), substitute-based vector algorithm (Yatbaz 
et al., 2012; Baskaya et al., 2013). In this model, the context of each instance of a target word is 
represented as a vector of features based on frequency statistics or probability distributions (e.g., first-
order or second-order vector). These vectors are clustered by various algorithms and the resulting clus-
ters represent the induced senses.  
Another family of employed approach is graph-based methods, which have been successfully ap-
plied in the sense induction task with some better results achieved. In this framework words are 
represented as nodes in the graph and vertices are drawn between the target word and its co-
occurrences. The co-occurrences between words can be obtained on the basis of grammatical (Wid-
dows and Dorow, 2002) or collocational relations (V?ronis, 2004). Senses are induced by identifying 
highly dense sub-graphs (hubs) in the co-occurrence graph.  
Klapaftis (2007) uses hypergraph model for WSI, in which co-occurrences of two or more words 
are represented by using weighted hyperedges. This model fully exploits the existence of collocations 
or terms consisting of more than two words. In fact, the method converts the sense induction problem 
to the clustering of the contextual words, and the result relies on local word co-occurrence frequency.  
Our hypergraph model is constructed from a global perspective, where the whole context instance is 
regarded as a node.  
WSI evaluation also is an important issue in WSI tasks. Previous WSI evaluations in SemEval 
(Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding 
the single most salient sense of a target word given its context. However, as shown in Erk and McCar-
thy (2009), multiple senses of the target word may be perceived by readers from different angles and a 
graded notion of sense labeling may be considered as the most appropriate. The SemEval-2013 WSI 
evaluation is designed to explore the possibility of finding all perceived senses of a target word in a 
single context instance. Our model is evaluated and verified on the SemEval-2013 WSI task. 
1602
  
 
2.2 Lexical chain extraction 
The Lexical chain method is an important technique in natural language processing. A lexical chain is 
a sequence of semantic related words in text and provides important clues about the text structure and 
topic. It has formed a theoretically well-founded building block in a lot of applications, such as word 
sense disambiguation (Manabu and Takeo, 1994), malapropism detection and correction (Hirst and St-
Onge, 1998), summarization (Barzilay et al., 1997), topic tracking (Carthy, 2004), text segmentation 
(Stokes et al., 2004), and others.  
There are mainly two approaches for lexical chain extraction. One focuses on the use of knowledge 
resources like WordNet (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background 
information in order to quantify semantic relations between words. A major disadvantage of this strat-
egy is that it relies on the resource, which has a direct impact on the quality of lexical chains. Another 
approach is based on statistical methods (Remus and Biemann, 2013).  In this paper, we follow Remus 
and Biemann (2013) to automatically extract lexical chain by using LDA topic model.  
3  Hypergraph model 
In general, lexical chain based hypergraph model contains the following steps:  
i) Automatically extracting lexical chains based on topic model;  
ii) Constructing hypergraph with lexical chains;  
iii) Inducing word sense by hypergraph clustering. 
3.1 Lexical chain extraction 
The extraction technique of lexical chains is based on LDA topic model. LDA topic model (Blei et al., 
2003) is a probabilistic model of text generation designed for revealing some hidden structure in large 
data collections. The key idea is that each document can be represented as a probability distribution 
over a fixed set of topics where each topic can be represented as a probability distribution over words. 
We use LDA topic model for estimating the semantic closeness of lexical terms, and explore a way of 
utilizing LDA?s topic information in constructing lexical chains automatically. In our model, docu-
ment is replaced with context instance of a target word.  
We adopt the idea of interpreting lexical chains as topics and placing all word tokens that share the 
same topic into the same chain. Lexical chains are usually extracted from the same paragraph or text, 
whose topic distributions are identical. However, in our experiment the context instances of a target 
word for WSI are derived from different articles, whose topic distributions are varied. Therefore both 
lexical and contextual topics are modeled. After training the LDA model, we use the information of 
the per-document topic distribution ?d= p(z|d), the per-topic word distribution?w=p(w|z) and the 
sampling topic of a word zw.  
The key work lies in how to assign a word to a topic in training LDA model. Since single samples 
of topics per word may exhibit a large variance (Riedl and Biemann, 2012), we sample several times 
and use the mode (most frequently assigned) topic ID per word as the topic assignment. 
Algorithm 1. lexical chains extraction algorithm 
Input: training set D of target word, hyper-parameters of LDA model;  semantic threshold ?. 
Output: lexical chain set S 
1  ?,?,Z          LDA (D) 
2  for each topic z 
3     lc =""                 //  lc denotes a lexical chain 
4     for each doc d 
5         for each word w in doc d 
6             if ( zw = z  and p(w,d|z) > ? )  
7                lc.add (w) 
8   S.add (lc)  
9   return S 
1603
   
The extraction algorithm is shown in algorithm 1. In order to improve the quality of identified lexi-
cal chains, a threshold ? is set to filter those invalid words whose generating probability of sampling 
topics in the document is lower than ?.  
( , | ) ( | ) ( | )p w d z p z d p w z ?? >                 (1) The threshold ? is essential for the quality of lexical chains, which directly impacts on the performance 
of the model. Detailed analysis for the threshold ? will be given in section 4.5. 
3.2 Hypergraph creation 
A hypergraph H = (V, E) is a generalization of a graph whose edge can connect more than two vertices. 
Just as graphs represent many kinds of information in mathematical and computer science problems, 
hypergraphs also arise in important practical problems, including circuit layout, boolean satisfiability, 
numerical linear algebra, complex network and article co-citation, etc.  
Figure 2 shows an example of hypergraph creation in our model. We represent each context in-
stance as a vertex and connect those context instances with a lexical chain across them by a hyperedge. 
A hyperedge weight equals to the weight of the corresponding lexical chain, defined as follows: 
( | d ) (w | )
( ) (2)| |
i
i i
w C
p z p z
w e C
?
=                       
?
   
where lexical chain C corresponds to hyperedge e, |C| is the number of words in C, and z is the sam-
pling topic of C. 
3.3 Hypergraph clustering 
For hypergraph clustering, the hypergraph is usually transformed into induced graph. There are two 
transformation strategies: one is vertex expansions (2006; Zhou et al., 2006), i.e., clique expansion or 
star expansion, in which a hyperedge is transformed into a clique; the other is called hyperedge expan-
sion (Pu and Faltings, 2012) based on a network flow technique, in which hyperedges are projected 
back to vertices through the adjacency information between hyperedges and vertices. 
Hypergraph clustering algorithm can be divided into two classes: one is based on minimal norma-
lized cut, and the other is based on maximal density. We use three general hypergraph clustering algo-
rithms to identify the context instance clusters. The three algorithms are simply shown in figure 3 and 
described as the follows. 
1) Normalized Hypergraph Cut (NHC) 
 
1604
  The NHC algorithm (Zhou et al., 2006) is a typical approach based on vertex expansion. The objec-
tive is to obtain a partition in which the connection among the vertices in the same cluster is dense 
while the connection between two clusters is sparse. The main steps include transforming the hyper-
graph into an induced graph first, and then adopting the normalized Laplacian to spectral partitioning. 
2) Hyperedge Expansion Clustering (HEC) 
Some works (e.g., Shashua et al., 2006; Bulo and Pelillo, 2012) have shown that the pairwise affini-
ty relations after the projection to the induced graph would introduce information loss, and working 
directly on the hypergraph could produce better performance.  
The hyperedge expansion works as follows. It constructs a directed graph G = (V, E) that includes 
two vertices e+ and e- for each hyperedge e in the original hypergraph. Note that the vertices in G cor-
respond to the hyperedges, but not the vertices in the original hypergraph. A directed edge is placed 
from e+ to e- with weight w(e) where w is the weighting function in the hypergraph. For every pair of 
overlapping hyperedges e1 and e2, two directed edges 1 2(e ,e )? +  and 2 1(e ,e )? +  are added to G with weights 
w(e2) and w(e1). After hypergraph expansion, it adopts spectral method for clustering.  
3) Schype  
The Schype (Michoel and Nachtergaele, 2012) is a maximal density cluster algorithm. According to 
the generalization of the Perron-Frobenius theorem, there exists a unique, positive vector, called the 
dominant eigenvector, over the set of vertices of the hypergraph, which produces a maximal density 
sub-graph with linear time. The procedure is as follow:  
i) Finding maximal density sub-graph by computing the dominant eigenvector. 
ii) Removing all vertices and hyperedges of the sub-graph from hypergraph. 
iii) Repeating above steps until no vertex in hypergraph occurs.  
This algorithm tends to generate many fine-grained clusters. We follow Tan and Kumar (2006) to 
merge clusters using two measures: cohesion and separation. The cohesion of a cluster Ci is defined as: 
,
#( | , )
(C ) | C |
i ix C y C
i
i
e x y e
cohesion ? ?
?
=
?
                 (3) 
where #( | , )e x y e?  is the number of hyperedges containing nodes x and y in C and |Ci| is the number of 
vertices in Ci. Separation between two clusters Ci, Cj is defined as: 
,
#( | , )
(C ,C ) 1 ( )| C | | C |
i jx C y C
i j
i j
e x y e
separation ? ?
?
= ?
?
?
       (4) 
1605
 We merge cluster pairs with high cohesion and low separation. The intuition is that context in-
stances in such pairs will maintain a relatively high degree of semantic similarity. High cohesion is 
defined as greater than average cohesion of all clusters. Low separation is defined as a reciprocal rela-
tionship between two clusters: if a cluster Ci has the lowest separations to a cluster Cj and Cj has the 
lowest separation to Ci, then the two (high cohesion) clusters are merged. This merging process is ite-
rated until it converges. 
4 Experiment and Evaluation 
4.1 Dataset 
Our WSI evaluation is based on the dataset provided by the SemEval-2013 shared 13th task. Test data 
was drawn from the Open American National Corpus (OANC) (Ide and Suderman, 2004) across a va-
riety of genres and from both the spoken and written portions of the corpus. It consists of 4,806 in-
stances of 50 target words: 20 verbs, 20 nouns and 10 adjectives. Due to the unsupervised nature of the 
task, participants were not provided with sense-labeled training data.  However, WSI systems were 
provided with the ukWac corpus (Baroni et al., 2009) to use in inducing senses. Additionally, we used 
the SemEval-2013 lexical trial data sets as development sets to tune parameters. 
4.2 Implementation details 
The training data is extracted from uKWac corpus. For each target word, we extracted 10K context 
instances, and each instance is a sentence window containing the target word. Additionally, we ran-
domly selected 10K sentences as common auxiliary corpus, including none of the target word. The 
training data are tagged with POS tags and lemmatized with TreeTagger (Schmid, 1994).  Removing 
stop words, nouns are taken as features. Meanwhile, we also removed words that co-occur with the 
target of word less than 50 times over the whole ukWac data.  
The training data in the model contains 20K instances: 10K instances of target word, 10K auxiliary 
instances. Specifically, we used the JGibbLDA1 framework for topic model estimation and inference, 
and examined the following LDA parameters: number of topics K, dirichlet hyperparameters for doc-
ument-topic distribution ? and topic-term distribution ?. We tested combinations in the ranges 
K=1000?1500?2000,  ?=5/K..50/K and ?=0.001..0.1. The highest performance of the WSI system 
was found for K = 2000, ? =0.025, ? = 0.001. Similar to tuning the dirichlet hyperparameters of LDA, 
the best parameter ? in lexical chain extraction is 0.0001 in the ranges ? =0.01.. 0.000001. 
We adopt the three clustering algorithms to cluster hypergraph2. The number of clusters is set as 10 
for NHC and HEC, while Schype algorithm generates the number of the clusters (but requiring the 
edge-vertex ratio to be pre-defined), whose average number of senses is 31.8 after clusters are merged. 
Additionally, For Schype algorithm, we used the default values of parameters, except that the ?min-
clustscore? parameter, a minimal score to output a cluster, being tuned to 0.1. 
The sense inventory acquired from the induction step can be used for disambiguation of individual 
instances. Each sense is represented as a vector, whose element is a word and the value of element is 
co-occurrence frequency with target word in the training set. Each test instance is also represented as a 
vector. The similarity between the instance and the induced sense is computed by using cosine func-
tion. For each test instance, it is compared with each sense separately, and finally the sense is selected 
if the cosine value is greater than a certain threshold ?. In experiment, ? is 0.04 for NHC and HEC, 
and is 0.1 for schype. 
4.3 Evaluation measures 
Evaluation in the SemEval-2013 WSI task can be divided into two categories:  
1. A traditional WSD task for unsupervised WSD and WSI systems, 
2. A clustering comparison setting that evaluates the similarity of the sense inventories for WSI sys-
tems.  
                                                 
1 http://sourceforge.net/projects/jgibblda/ 
2  The Hypergraph Analysis Toolbox (HAT) for NHC and HEC: http://lia.epfl.ch/index.php/research/relational-learning 
and the Schype?s code: http://www.roslin.ed.ac.uk/tom-michoel/software/ 
1606
 In the first evaluation, we adopt a WSD task with three objectives: (1) detecting which senses are 
applicable, (2) ranking senses by their applicability, and (3) measuring agreement in applicability rat-
ings with human annotators. Each objective uses a specific measurement:  
i):  Jaccard Index: given two sets of sense labels for an instance, X and Y, the Jaccard Index is used 
to measure the agreement: X Y
X Y
?
?
. The Jaccard Index is maximized when X and Y use identical labels, 
and is minimized when the sets of sense labels are disjoint. 
ii):  Positionally-weighted Kendall?s ? similarity: for graded sense evaluation, we consider an rank-
ing scoring based on Kumar and Vassilvitskii(2010), which weights the penalty of reordering the low-
er positions less than the penalty of reordering the first ranks. 
iii):  Weighted Normalized Discounted Cumulative Gain (WNDCG): NDCG (Moffat and Zobel, 
2008) normally compares the rankings of two lists. It is extended to weighting the DCG by consider-
ing the relative difference in the two weights.  
Because the induced senses will likely vary in number and nature between systems, the WSD evalu-
ation has to incorporate a sense alignment step, in which it performs by splitting the test in-stances into 
two sets: a mapping set and an evaluation set. The optimal mapping from induced senses to gold-
standard senses is learned from the mapping set, and the sense alignment is used to map the predic-
tions of the WSI system to pre-defined senses for the evaluation set. The particular split we use to cal-
culate WSD effectiveness in this paper is 80%/20% (mapping/test), averaged across 5 random splits. 
In the clustering evaluation, similarity between participant?s clusters and the gold standard clusters 
is measured by way of two metrics. 
i): Fuzzy Normalised Mutual information (NMI): it extends the method of (Lancichinetti et al., 2009) 
to compute NMI between overlapping clusters. Fuzzy NMI captures the alignment of the two clusters 
independent of the cluster sizes and therefore serves as an effective measure of the ability of an ap-
proach to accurately model rare senses. 
ii): Fuzzy B-Cubed: it adapts the overlapping B-Cubed measured defined in Amigo et al. (2009) to 
the fuzzy clustering setting, and provides an item-based evaluation that is sensitive to the cluster size 
skew and effectively captures the expected performance of the system on a dataset where the cluster 
distribution would be equivalent.  
4.4 Results 
We compared our models with four baselines and three benchmark systems from the SemEval-2013 
task. Four baselines are described as follows. 
? Baseline MFS ? most frequent sense baseline, assigning all test instances to the MFS in the 
test data (regardless of what applicability rating it was given).  
? One sense ? labels each instance with the same induced sense. 
? One sense per instance (1clinst) ? labels each instance with its own induced. 
? Baseline Random-n ? randomly assigns each test instance to one of n randomly selected in-
duced senses, where n is the number of senses for the target word in WordNet 3.1.  
Three benchmark systems as the following are those which achieved better results in the original 
SemEval-2013 task. 
? AI-KU is based on a lexical substitution method. 
? UoS uses dependency-parsed features from the corpus, which are then clustered into senses us-
ing the MaxMax algorithm (Hope and Keller, 2013). 
? Unimelb is a non-parameter topic model which uses a Hierarchical Dirichlet Process (Teh et al., 
2006) to automatically infer the number of senses from contextual and positional features. 
4.4.1 Supervised evaluation 
In the supervised evaluation, the automatically induced clusters are mapped to gold standard senses, 
using one part of the test set. The obtained mapping is used to label the other part of test set with gold 
standard tags, which means that the methods are evaluated in a standard WSD task. In experiment, we 
follow the 80/20 setup: 80% for mapping and 20% for test. 
Table 1 shows the results of our systems on test data using all instances (including verbs, nouns and 
adjectives) for the WSD evaluation. As in previous WSD tasks, the MFS baseline on Jaccard Index 
measures is quite competitive, outperforming all systems in detecting which senses are applicable.  
1607
  
System JI-F1 WKT-F1 
 WNDCG-
F1 
NHC 0.325 0.692 0.375 
HEC 0.327 0.693 0.376 
Schype 0.376 0.753 0.345 
AI-KU  0.244 0.642 0.332 
UNIMELB 0.213 0.62 0.371 
UoS 0.232 0.625 0.374 
MFS 0.455 0.465 0.339 
One sense 0.192 0.609 0.288 
1c1inst 0.0 0.095 0.0 
Random-n 0.29 0.638 0.286 
 
Table 1. The supervised results over the SemEv-
al-2013 dataset. 
System FNMI FBC 
NHC 0.046  0.406  
HEC 0.037  0.400  
Schype 0.042  0.377  
AI-KU 0.039 0.451 
UNIMELB 0.056 0.459 
UoS 0.045 0.448 
MFS - - 
One sense 0 0.632 
1c1inst 0.071 0 
Random-n 0.016 0.245 
 
Table 2.  The unsupervised results over the Se-
mEval-2013 dataset. 
 
However, most systems in this task were able to outperform the MFS baseline in ranking senses and 
quantifying their applicability. 
On the other hand, it also indicates that our three systems achieve better or comparable scores And 
the Schype gets the highest scores in detecting senses and ranking senses over all systems.  
4.4.2 Unsupervised evaluation 
Unsupervised evaluation aims to measure the similarity of the induced sense inventories for WSI sys-
tems. Unlike supervised metrics, it avoids potential loss of sense information since this setting does 
not require any sense mapping procedure to convert induced senses to WordNet senses.  
Table 2 shows the performance of our systems, benchmarks and baselines. It shows that the NMI-
measure is biased towards the one sense per instance baseline and the FBC-measure one sense base-
line. However, systems are capable of performing well in both the Fuzzy NMI and Fuzzy B-Cubed 
measures, thereby avoiding the extreme performance of either baseline. Generally, the performance of 
our model gets balanced scores. 
4.5 Discussion 
Topic models, such as LDA and HDP (Brody and Lapata, 2009; Lau et al., 2012), have been success-
fully adopted for WSI, in which one topic is viewed as one sense. Our work is motivated by lexical 
chain that represents the intrinsic semantic relatedness among context instances on the viewpoint of 
linguistics. Topic model is used to find lexical chains which are interpreted as topics. We have com-
pared the Unimelb (Lau et al., 2013), a HDP topic model, with our model in the experiments. Addi-
tionally, we also follow Lau et al. (2012) to train a LDA model with a fixed number of topics based on 
our training data for WSI3. Table 3 shows the supervised result compared to the Schype. 
These experiments show promising performance for our model, which captures richer semantic re-
latedness by using lexical chains. Lexical chains play a key role for the performance of our model. 
Intuitively, when lexical chains are too long, the higher-order relatedness would be mixed with some 
noises, while when lexical chains are too short, some higher-order relatedness will be missed. In order 
to verify the effectiveness of lexical chains, we tune the parameter ? in lexical chain extraction proce-
dure and the results are shown in Figure 4.  
Another issue is the impact of POS labels of word for WSI task. The test data in SemeVal-2013 
WSI task contain nouns, verbs and adjectives. We also test the performance based on different POS 
labels. Table 4 gives the supervised evaluation performance of our three systems on adjectives, verbs 
and nouns respectively. We found that the performance for adjectives in sense detection is the best, 
verbs followed and nouns worst, whereas it?s reversed in sense ranking. The probable interpretation is  
                                                 
3 The LDA model parameters are set as follows: K=10, ? =0.025, ? = 0.001. 
1608
   
Figure 4.  Performance analysis on Jaccard index 
measure for different threshold parameter ? in 
lexical chains extraction procedure. 
 
Type LDAK=10 Schype 
JI-F1 0.318 0.376 
WKT-F1 0.692 0.753 
 WNDCG-F1 0.334 0.345 
 
Table 3. The supervised results over the SemEv-
al-2013 dataset between LDAk=10 and Schype for 
WSI. 
 
  NHC HEC Schype 
POS JI WKT WNDCG JI WKT WNDCG JI WKT WNDCG
nouns 0.306  0.697  0.370  0.313 0.702 0.367  0.363 0.767  0.246  
verbs 0.336  0.686  0.374  0.336 0.690 0.380  0.384 0.749  0.245  
adjs 0.347  0.697  0.396  0.342 0.678 0.390  0.394 0.733  0.248  
 
Table 4. The supervised performance of three algorithms respectively on nouns, verbs and adjectives. 
 
that adjective?s average sense number is the lowest, and the sense granularity is greater than verbs and 
nouns over the test data4. 
5 Conclusions and future work 
In this paper, we present a hypergraph model in which a node represents an instance and a hyperedge 
represents higher-order semantic relatedness among instances. Compared with other strategies based 
on binary local comparison, the model captures complex semantic relatedness among the instances 
from a global perspective.  
The evaluation results indicate that our model outperforms or reaches competitive performance 
comparable to other systems for the SemEval-2013 word sense induction task. Additionally, the expe-
riments also show that both sense number and sense granularity of a target word affect the perfor-
mance of WSI. 
For future work, we would like to explore better ways to extract and evaluate lexical chain for WSI 
task. In addition, for the three clustering algorithms, they generally require the number of clusters or 
edge-vertex ratio to be pre-defined, so we will seek more effective hypergraph clustering algorithms to 
automatically determine the parameters. Finally, the hypergraph model proposed in this work is not 
specific to the sense induction task, and can be adapted for other applications, such as document clas-
sification and clustering, information retrieval, etc. 
Acknowledgements 
This work is supported by the National Natural Science Foundation of China (No. 61173062, 61373108, 
61133012), the major program of the National Social Science Foundation of China (No. 11&ZD189), and the 
High Performance Computing Center of Computer School, Wuhan University. 
Reference 
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task 02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7?12.  
Eneko Agirre, David Martnez, Oier L?pez de Lacalle, and Aitor Soroa. 2006. Two graph-based algorithms for 
state-of-the-art WSD. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language 
                                                 
4 In the test data, the average number of senses of nouns, verbs, adjectives respectively is 7.15, 6.85, 5.9 respectively. 
1609
 Processing, pages 585?593.  
Enrique Amig?, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering 
evaluation metrics based on formal constraints. Information retrieval, 12(4):461?486. 
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collec-
tion of very large linguistically processed web-crawled corpora. Language resources and evaluation, 
43(3):209?226. 
Regina Barzilay, Michael Elhadad, et al. 1997. Using lexical chains for text summarization. In Proceedings of 
the ACL workshop on Intelligent scalable text summarization, volume 17, pages 10?17.  
Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz Yuret. 2013. AI-kU: Using substitute vectors and co-
occurrence modeling for word sense induction and disambiguation. In Proceedings of SemEval-2013.  
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of Machine 
learning research, 3:993?1022. 
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference 
of the European Chapter of the Association for Computational Linguistics.  
Stefan Bordag. 2006. Word sense induction: Triplet-based clustering and automatic evaluation. In Proceedings 
of the 11th Conference of the European Chapter of the Association for Computational Linguistics 
Joe Carthy. 2004. Lexical chains versus keywords for topic tracking. In Computational Linguistics and Intelli-
gent Text Processing, pages 507?510.  
Antonio Di Marco and Roberto Navigli. 2011. Clustering web search results with maximum spanning trees. In 
AI* IA 2011: Artificial Intelligence Around Man and Beyond, pages 201?212. 
Beate Dorow and Dominic Widdows. 2003. Discovering corpus-specific word senses. In Proceedings of the 
tenth Conference on European chapter of the Association for Computational Linguistics-Volume 2, pages 79?
82.  
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In 
Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint 
Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 10?18.  
Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and cor-
rection of malapropisms. WordNet: An electronic lexical database, 305:305?332. 
David Hope and Bill Keller. 2013. Maxmax: a graph-based soft clustering algorithm applied to word sense in-
duction. In Computational Linguistics and Intelligent Text Processing, pages 368?381.  
Nancy Ide and Keith Suderman. 2004. The american national corpus first release. In LREC.  
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2001. Automatic sense tagging using parallel corpora. In NLPRS, 
pages 83?90. 
David Jurgens and Ioannis Klapaftis. 2013. SemEval-2013 Task 13: Word Sense Induction for Graded and Non-
Graded Senses. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 
2013). 
Ioannis P Klapaftis and Suresh Manandhar. 2007. Uoy: a hypergraph model for word sense induction & disam-
biguation. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 414?417.  
Andrea Lancichinetti, Santo Fortunato, and J?nos Kert?sz. 2009. Detecting the overlapping and hierarchical 
community structure in complex networks. New Journal of Physics, 11(3):033015. 
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013. unimelb: Topic Modelling-based Word Sense Induction. 
In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). 
Jey Han Lau, Paul Cook and Diana McCarthy, David Newman and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Associa-
tion for Computational Linguistics. 
Okumura Manabu and Honda Takeo. 1994. Word sense disambiguation and text segmentation based on lexical 
cohesion. In Proceedings of the 15th conference on Computational linguistics-Volume 2, pages 755?761.  
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, and Sameer S Pradhan. 2010. SemEval-2010 task 14: 
Word sense induction & disambiguation. In Proceedings of the 5th International Workshop on Semantic 
1610
 Evaluation, pages 63?68.  
Tom Michoel and Bruno Nachtergaele. 2012. Alignment and integration of complex networks by hypergraph-
based spectral clustering. Physical Review E, 86(5):056111. 
Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM 
Transactions on Information Systems (TOIS), 27(1):2. 
Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the 
structure of text. Computational linguistics, 17(1):21?48. 
Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR),41(2):10. 
Michael Steinbach Pang-Ning Tan and Vipin Kumar. 2006. Introduction to Data Mining. Pearson Addison Wes-
ley. 
Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the eighth ACM 
SIGKDD, pages 613?619.  
Li Pu and Boi Faltings. 2012. Hypergraph learning with hyperedge expansion. In Machine Learning and Know-
ledge Discovery in Databases, pages 410?425. 
Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and simi-
larity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41?48. 
Boston. 
Kumar, Ravi and Vassilvitskii, Sergei. 2010. Generalized distances between rankings. In Proceedings of the 19th 
international conference on World wide web, pages 571-580.  
Steffen Remus and Chris Biemann. 2013. Three knowledge-free methods for automatic lexical chain extraction. 
In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational 
Linguistics: Human Language Technologies, pages 989?999, Atlanta, Georgia, June.  
Martin Riedl and Chris Biemann. 2012. Sweeping through the topic space: bad luck? roll again! In Proceedings 
of the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP, pages 19?27.  
Samuel Rota Bulo and Marcello Pelillo. 2012. A game-theoretic approach to hypergraph clustering. 
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international 
conference on new methods in language processing, volume 12, pages 44?49.  
Hinrich Sch?tze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123. 
Amnon Shashua, Ron Zass, and Tamir Hazan. 2006. Multi-way clustering using super-symmetric non-negative 
tensor factorization. In Computer Vision?ECCV 2006, pages 595?608.  
Nicola Stokes, Joe Carthy, and Alan F Smeaton. 2004. Select: a lexical cohesion based news story segmentation 
system. AI Communications, 17(1):3?12. 
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes. 
Journal of the american statistical association, 101(476). 
Tim Van de Cruys, Marianna Apidianaki, et al. 2011. Latent semantic word sense induction and disambiguation. 
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-
guage Technologies (ACL/HLT), pages 1476?1485. 
Jean V?ronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech & Language, 
18(3):223?252. 
Dominic Widdows and Beate Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proceedings 
of the 19th International Conference on Computational linguistics-Volume 1, pages 1?7.  
David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings 
of the 33rd annual meeting on Association for Computational Linguistics, pages 189?196.  
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012. Learning syntactic categories using paradigmatic repre-
sentations of word context. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural Language Learning, pages 940?951.  
Dengyong Zhou, Jiayuan Huang, and Bernhard Sch?lkopf. 2006. Learning with hypergraphs: Clustering, classi-
fication, and embedding. In Advances in Neural Information Processing Systems, pages 1601?1608. 
1611
