Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1241?1249,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Real-Word Spelling Correction using Google Web 1T 3-grams
Aminul Islam
Department of Computer Science
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
mdislam@site.uottawa.ca
Diana Inkpen
Department of Computer Science
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
diana@site.uottawa.ca
Abstract
We present a method for detecting and
correcting multiple real-word spelling er-
rors using the Google Web 1T 3-gram data
set and a normalized and modified ver-
sion of the Longest Common Subsequence
(LCS) string matching algorithm. Our
method is focused mainly on how to im-
prove the detection recall (the fraction of
errors correctly detected) and the correc-
tion recall (the fraction of errors correctly
amended), while keeping the respective
precisions (the fraction of detections or
amendments that are correct) as high as
possible. Evaluation results on a standard
data set show that our method outperforms
two other methods on the same task.
1 Introduction
Real-word spelling errors are words in a text that
occur when a user mistakenly types a correctly
spelled word when another was intended. Errors
of this type may be caused by the writer?s igno-
rance of the correct spelling of the intended word
or by typing mistakes. Such errors generally go
unnoticed by most spellcheckers as they deal with
words in isolation, accepting them as correct if
they are found in the dictionary, and flagging them
as errors if they are not. This approach would
be sufficient to detect the non-word error myss in
?It doesn?t know what the myss is all about.? but
not the real-word error muss in ?It doesn?t know
what the muss is all about.? To detect the latter,
the spell-checker needs to make use of the sur-
rounding context such as, in this case, to recog-
nise that fuss is more likely to occur than muss in
the context of all about. Ironically, errors of this
type may even be caused by spelling checkers in
the correction of non-word spelling errors when
the auto-correct feature in some word-processing
software sometimes silently change a non-word to
the wrong real word (Hirst and Budanitsky, 2005),
and sometimes when correcting a flagged error,
the user accidentally make a wrong selection from
the choices offered (Wilcox-O?Hearn et al, 2008).
An extensive review of real-word spelling cor-
rection is given in (Pedler, 2007; Hirst and Budan-
itsky, 2005) and the problem of spelling correction
more generally is reviewed in (Kukich, 1992).
The Google Web 1T data set (Brants and Franz,
2006), contributed by Google Inc., contains En-
glish word n-grams (from unigrams to 5-grams)
and their observed frequency counts calculated
over 1 trillion words from web page text col-
lected by Google in January 2006. The text was
tokenised following the Penn Treebank tokenisa-
tion, except that hyphenated words, dates, email
addresses and URLs are kept as single tokens.
The sentence boundaries are marked with two spe-
cial tokens <S> and </S>. Words that occurred
fewer than 200 times were replaced with the spe-
cial token <UNK>. Table 1 shows the data sizes
of the Web 1T corpus. The n-grams themselves
Table 1: Google Web 1T Data Sizes
Number of Number Size on disk
(in KB)
Tokens 1,024,908,267,229 N/A
Sentences 95,119,665,584 N/A
Unigrams 13,588,391 185,569
Bigrams 314,843,401 5,213,440
Trigrams 977,069,902 19,978,540
4-grams 1,313,818,354 32,040,884
5-grams 1,176,470,663 33,678,504
must appear at least 40 times to be included in the
Web 1T corpus
1
. It is expected that this data will
be useful for statistical language modeling, e.g.,
1
Details of the Google Web 1T data set can be found at
www.ldc.upenn.edu/Catalog/docs/LDC2006T13/readme.txt
1241
for machine translation or speech recognition, as
well as for other uses.
In this paper, we present a method for detecting
and correcting multiple real-word spelling errors
using the Google Web 1T 3-gram data set, and a
normalized and modified version of the Longest
Common Subsequence (LCS) string matching al-
gorithm (details are in section 3.1). By multiple er-
rors, we mean that if we have n words in the input
sentence, then we try to detect and correct at most
n-1 errors. We do not try to detect and correct an
error, if any, in the first word as it is not compu-
tationally feasible to search in the Google Web 1T
3-grams while keeping the first word in the 3-gram
as a variable. Our intention is to focus on how to
improve the detection recall (the fraction of errors
correctly detected) or correction recall (the frac-
tion of errors correctly amended) while maintain-
ing the respective precisions (the fraction of de-
tections or amendments that are correct) as high as
possible. The reason behind this intention is that if
the recall for any method is around 0.5, this means
that the method fails to detect or correct around 50
percent of the errors. As a result, we can not com-
pletely rely on these type of methods, for that we
need some type of human interventions or sugges-
tions to detect or correct the rest of the undetected
or uncorrected errors. Thus, if we have a method
that can detect or correct almost 80 percent of the
errors, even generating some extra candidates that
are incorrect is more helpful to the human.
This paper is organized as follow: Section 2
presents a brief overview of the related work. Our
proposed method is described in Section 3. Eval-
uation and experimental results are discussed in
Section 4. We conclude in Section 5.
2 Related Work
Work on real-word spelling correction can roughly
be classified into two basic categories: methods
based on semantic information or human-made
lexical resources, and methods based on machine
learning or probability information. Our proposed
method falls into the latter category.
2.1 Methods Based on Semantic Information
The ?semantic information? approach first pro-
posed by Hirst and St-Onge (1998) and later devel-
oped by Hirst and Budanitsky (2005) detected se-
mantic anomalies, but was not restricted to check-
ing words from predefined confusion sets. This
approach was based on the observation that the
words that a writer intends are generally seman-
tically related to their surrounding words, whereas
some types of real-word spelling errors are not,
such as (using Hirst and Budanitsky?s example),
?It is my sincere hole (hope) that you will recover
swiftly.? Such ?malapropisms? cause ?a pertur-
bation of the cohesion (and coherence) of a text.?
Hirst and Budanitsky (2005) use semantic distance
measures in WordNet (Miller et al, 1993) to de-
tect words that are potentially anomalous in con-
text - that is, semantically distant from nearby
words; if a variation in spelling results in a word
that was semantically closer to the context, it is
hypothesized that the original word is an error (a
?malapropism?) and the closer word is its correc-
tion.
2.2 Methods Based on Machine Learning
Machine learning methods are regarded as lexical
disambiguation tasks and confusion sets are used
to model the ambiguity between words. Normally,
the machine learning and statistical approaches
rely on pre-defined confusion sets, which are sets
(usually pairs) of commonly confounded words,
such as {their, there, they?re} and {principle, prin-
cipal}. The methods learn the characteristics of
typical context for each member of the set and de-
tect situations in which one member occurs in con-
text that is more typical of another. Such meth-
ods, therefore, are inherently limited to a set of
common, predefined errors, but such errors can in-
clude both content and function words. Given an
occurrence of one of its confusion set members,
the spellchecker?s job is to predict which mem-
ber of that confusion set is the most appropriate in
the context. Golding and Roth (1999), an exam-
ple of a machine-learning method, combined the
Winnow algorithm with weighted-majority voting,
using nearby and adjacent words as features. An-
other example of a machine-learning method is
that of Carlson et al (2001).
2.3 Methods Based on Probability
Information
Mays et al (1991) proposed a statistical method
using word-trigram probabilities for detecting and
correcting real-word errors without requiring pre-
defined confusion sets. In this method, if the
trigram-derived probability of an observed sen-
tence is lower than that of a sentence obtained by
replacing one of the words with a spelling varia-
1242
tion, then we hypothesize that the original is an
error and the variation is what the user intended.
Wilcox-O?Hearn et al (2008) analyze the ad-
vantages and limitations of Mays et al (1991)?s
method, and present a new evaluation of the al-
gorithm, designed so that the results can be com-
pared with those of other methods, and then con-
struct and evaluate some variations of the algo-
rithm that use fixed-length windows. They con-
sider a variation of the method that optimizes over
relatively short, fixed-length windows instead of
over a whole sentence (except in the special case
when the sentence is smaller than the window),
while respecting sentence boundaries as natural
breakpoints. To check the spelling of a span of
d words requires a window of length d+4 to ac-
commodate all the trigrams that overlap with the
words in the span. The smallest possible window
is therefore 5 words long, which uses 3 trigrams
to optimize only its middle word. They assume
that the sentence is bracketed by twoBoS and two
EoS markers (to accommodate trigrams involving
the first two and last two words of the sentence).
The window starts with its left-hand edge at the
first BoS marker, and the Mays et al (1991)?s
method is run on the words covered by the tri-
grams that it contains; the window then moves d
words to the right and the process repeats until all
the words in the sentence have been checked. As
Mays et al (1991)?s algorithm is run separately in
each window, potentially changing a word in each,
Wilcox-O?Hearn et al (2008)?s method as a side-
effect also permits multiple corrections in a single
sentence.
Wilcox-O?Hearn et al (2008) show that
the trigram-based real-word spelling-correction
method of Mays et al (1991) is superior in per-
formance to the WordNet-based method of Hirst
and Budanitsky (2005), even on content words
(?malapropisms?), especially when supplied with
a realistically large trigram model. Wilcox-
O?Hearn et al (2008) state that their attempts to
improve the method with smaller windows and
with multiple corrections per sentence were not
successful, because of excessive false positives.
Verberne (2002) proposed a trigram-based
method for real-word errors without explicitly us-
ing probabilities or even localizing the possible er-
ror to a specific word. This method simply as-
sumes that any word trigram in the text that is
attested in the British National Corpus (Burnard,
2000) is correct, and any unattested trigram is a
likely error. When an unattested trigram is ob-
served, the method then tries the spelling varia-
tions of all words in the trigram to find attested
trigrams to present to the user as possible correc-
tions. The evaluation of this method was carried
out on only 7100 words of the Wall Street Journal
corpus, with 31 errors introduced (i.e., one error
in every approximately 200 words) obtaining a re-
call of 0.33 for correction, a precision of 0.05 and
a F-measure of 0.086.
3 Proposed Method
The proposed method first tries to determine some
probable candidates and then finds the best one
among the candidates or sorts them based on some
weights. We consider a string similarity function
and a normalized frequency value function in our
method. The following sections present a detailed
description of each of these functions followed by
the procedure to determine some probable candi-
dates along with the procedure to sort the candi-
dates.
3.1 Similarity between Two Strings
We use the longest common subsequence (LCS)
(Allison and Dix, 1986) measure with some nor-
malization and small modifications for our string
similarity measure. We use the same three differ-
ent modified versions of LCS that we (Islam and
Inkpen, 2008) used, along with another modified
version of LCS, and then take a weighted sum of
these
2
. Kondrak (2005) showed that edit distance
and the length of the longest common subsequence
are special cases of n-gram distance and similarity,
respectively. Melamed (1999) normalized LCS by
dividing the length of the longest common subse-
quence by the length of the longer string and called
it longest common subsequence ratio (LCSR). But
LCSR does not take into account the length of the
shorter string which sometimes has a significant
impact on the similarity score.
Islam and Inkpen (2008) normalized the longest
common subsequence so that it takes into account
the length of both the shorter and the longer string
and called it normalized longest common subse-
2
We (Islam and Inkpen, 2008) use modified versions be-
cause in our experiments we obtained better results (precision
and recall) for schema matching on a sample of data than
when using the original LCS, or other string similarity mea-
sures.
1243
quence (NLCS) which is:
v
1
= NLCS(s
i
, s
j
) =
len(LCS(s
i
, s
j
))
2
len(s
i
)? len(s
j
)
(1)
While in classical LCS, the common subse-
quence needs not be consecutive, in spelling cor-
rection, a consecutive common subsequence is im-
portant for a high degree of matching. We (Is-
lam and Inkpen, 2008) used maximal consecutive
longest common subsequence starting at charac-
ter 1, MCLCS
1
and maximal consecutive longest
common subsequence starting at any character n,
MCLCS
n
. MCLCS
1
takes two strings as input
and returns the shorter string or maximal consec-
utive portions of the shorter string that consecu-
tively match with the longer string, where match-
ing must be from first character (character 1) for
both strings. MCLCS
n
takes two strings as in-
put and returns the shorter string or maximal con-
secutive portions of the shorter string that con-
secutively match with the longer string, where
matching may start from any character (char-
acter n) for both of the strings. We normal-
ized MCLCS
1
and MCLCS
n
and called it nor-
malized MCLCS
1
(NMCLCS
1
) and normalized
MCLCS
n
(NMCLCS
n
), respectively.
v
2
=NMCLCS
1
(s
i
, s
j
) =
len(MCLCS
1
(s
i
, s
j
))
2
len(s
i
)? len(s
j
)
(2)
v
3
=NMCLCS
n
(s
i
, s
j
) =
len(MCLCS
n
(s
i
, s
j
))
2
len(s
i
)? len(s
j
)
(3)
Islam and Inkpen (2008) did not consider consecu-
tive common subsequences ending at the last char-
acter, though MCLCS
n
sometimes covers this,
but not always. We argue that the consecutive
common subsequence ending at the last character
is as significant as the consecutive common sub-
sequence starting at the first character. So, we
introduce the maximal consecutive longest com-
mon subsequence ending at the last character,
MCLCS
z
(Algorithm 1). Algorithm 1, takes two
strings as input and returns the shorter string or the
maximal consecutive portions of the shorter string
that consecutively matches with the longer string,
where matching must end at the last character for
both strings. We normalize MCLCS
z
and call it
normalized MCLCS
z
(NMCLCS
z
).
v
4
=NMCLCS
z
(s
i
, s
j
) =
len(MCLCS
z
(s
i
, s
j
))
2
len(s
i
)? len(s
j
)
(4)
We take the weighted sum of these individual
values v
1
, v
2
, v
3
, and v
4
to determine string simi-
larity score, where ?
1
, ?
2
, ?
3
, ?
4
are weights and
?
1
+?
2
+?
3
+?
4
= 1. Therefore, the similarity
of the two strings, S ? [0, 1] is:
S(s
i
, s
j
) = ?
1
v
1
+ ?
2
v
2
+ ?
3
v
3
+ ?
4
v
4
(5)
We heuristically set equal weights for our ex-
periments
3
. Theoretically, v
3
? v
2
and v
3
? v
4
.
To give an example, consider s
i
= albastru and
s
j
= alabasteru, then
LCS(s
i
, s
j
) = albastru
MCLCS
1
(s
i
, s
j
) = al
MCLCS
n
(s
i
, s
j
) = bast
MCLCS
z
(s
i
, s
j
) = ru
NLCS(s
i
, s
j
) = 8
2
/(8? 10) = 0.8
NMCLCS
1
(s
i
, s
j
) = 2
2
/(8? 10) = 0.05
NMCLCS
n
(s
i
, s
j
) = 4
2
/(8? 10) = 0.2
NMCLCS
z
(s
i
, s
j
) = 2
2
/(8? 10) = 0.05
The string similarity, S = ?
1
v
1
+?
2
v
2
+?
3
v
3
+
?
4
v
4
= 0.25? 0.8 + 0.25? 0.05 + 0.25? 0.2 +
0.25? 0.05 = 0.275
3.2 Normalized Frequency Value
We determine the normalized frequency value of
each candidate word for a single position with re-
spect to all other candidates for the same position.
If we find n replacements of a word w
i
which are
{w
i1
, w
i2
, ? ? ? , w
ij
, ? ? ? , w
in
}, and their frequen-
cies {f
i1
, f
i2
, ? ? ? , f
ij
, ? ? ? , f
in
}, where f
ij
is the
frequency of a 3-gram (where any candidate word
w
ij
is a member of the 3-gram), then we determine
the normalized frequency value of any candidate
word w
ij
, represented as F (w
ij
) ? (0, 1], as the
frequency of the 3-gram havingw
ij
over the maxi-
mum frequency among all the candidate words for
that position:
F (w
ij
) =
f
ij
max(f
i1
, f
i2
, ? ? ? , f
ij
, ? ? ? , f
in
)
(6)
3.3 Determining Candidate Words
Our task is to correct real-word spelling error
from an input text using Google Web 1T 3-gram
data set. Let us consider an input text W which
3
We use equal weights in several places in this paper in
order to keep the system unsupervised. If development data
would be available, we could adjust the weights.
1244
Algorithm 1: MCLCS
z
( Maximal Consec-
utive LCS ending at the last character)
input : s
i
, s
j
/
*
s
i
and s
j
are input
strings where |s
i
| ? |s
j
|
*
/
output: str /
*
str is the Maximal
Consecutive LCS ending at
the last character
*
/
str ?NULL1
c? 12
while |s
i
| ? c do3
x? SubStr(s
i
,?c, 1) /
*
returns4
cth character of s
i
from the
end
*
/
y ? SubStr(s
j
,?c, 1) /
*
returns5
cth character of s
j
from the
end
*
/
if x = y then6
str ? SubStr(s
i
,?c, c)7
else8
return str9
end10
increment c11
end12
after tokenization
4
has m words, i.e., W =
{w
1
, w
2
, . . . , w
m
}. Our method aims to correct
m-1 spelling errors, for all m-1 word positions,
except for the first word position, as we do not try
to correct the first word. We use a slight differ-
ent way to correct the first word (i.e., w
2
) and the
last word (i.e., w
m
) among those m-1 words, than
for the rest of the words. First, we discuss how
we find the candidates for a word (say w
i
, where
2<i<m) which is not either w
2
or w
m
. Then, we
discuss the procedure to find the candidates for ei-
ther w
2
or w
m
. Our method could have worked
for the first word too. We did not do it here due
4
We need to tokenize the input sentence to make the 3-
grams formed using the tokens returned after the tokeniza-
tion consistent with the Google 3-grams. The input sentence
is tokenized in a manner similar to the tokenization of the
Wall Street Journal portion of the Penn Treebank. Notable
exceptions include the following:
- Hyphenated word are usually separated, and hyphen-
ated numbers usually form one token.
- Sequences of numbers separated by slashes (e.g., in
dates) form one token.
- Sequences that look like urls or email addresses form
one token.
to efficiency reasons. Google 3-grams are sorted
based on the first word, then the second word, and
so on. Based on this sorting, all Google 3-grams
are stored in 97 different files. All the 97 Google
3-gram files could have been needed to access a
single word, instead of accessing just one 3-gram
file as we do for any other words. This is because
when the first word needs to be corrected, it might
be in any file among those 97 Google 3-gram files.
No error appears in the first position among 1402
inserted malapropisms. The errors start appearing
from the second position till the last position.
3.3.1 Determining Candidate Words for w
i
(2 < i < m)
We use the following steps:
1. We define the term cut off frequency for word
w
i
or word w
i+1
as the frequency of the 3-
gram w
i?1
w
i
w
i+1
in the Google Web 1T 3-
grams, if the said 3-gram exists. Otherwise,
we set the cut off frequency of w
i
as 0. The
intuition behind using the cut off frequency
is the fact that, if the word is misspelled,
then the correct one should have a higher fre-
quency than the misspelled one. Thus, using
the cut off frequency, we isolate a large num-
ber of candidates that we do not need to pro-
cess.
2. We find all the 3-grams (where only w
i
is changed while w
i?1
and w
i+1
are un-
changed) having frequency greater than the
cut off frequency of w
i
(determined in
step 1). Let us consider that we find
n replacements of w
i
which are R
1
=
{w
i1
, w
i2
, ? ? ? , w
in
} and their frequencies
F
1
= {f
i1
, f
i2
, ? ? ? , f
in
} where f
ij
is the fre-
quency of the 3-gram w
i?1
w
ij
w
i+1
.
3. We determine the cut off frequency for word
w
i?1
or word w
i
as the frequency of the 3-
gram w
i?2
w
i?1
w
i
in the Google Web 1T 3-
grams, if the said 3-gram exists. Otherwise,
we set the cut off frequency of w
i
as 0.
4. We find all the 3-grams (where only w
i
is changed while w
i?2
and w
i?1
are un-
changed) having frequency greater than the
cut off frequency of w
i
(determined in
step 3). Let us consider that we find
n replacements of w
i
which are R
2
=
{w
i1
, w
i2
, ? ? ? , w
in
} and their frequencies
1245
F2
= {f
i1
, f
i2
, ? ? ? , f
in
} where f
ij
is the fre-
quency of the 3-gram w
i?2
w
i?1
w
ij
.
5. For each w
ij
? R
1
, we calculate the string
similarity between w
ij
and w
i
using equation
(5) and then assign a weight using the follow-
ing equation (7) only to the words that return
the string similarity value greater than 0.5.
weight = ?S(w
i
, w
ij
)+(1??)F (w
ij
) (7)
Equation (7) is used to ensure a balanced
weight between the string similarity function
and the normalized frequency value function
where ? refers to how much importance we
give to the string similarity function with re-
spect to the normalized frequency value func-
tion
5
.
6. For each w
ij
? R
2
, we calculate the string
similarity between w
ij
and w
i
using equa-
tion (5), and then assign a weight using the
equation (7) only to the words that return the
string similarity value greater than 0.5.
7. We sort the words found in step 5 and in step
6 that were given weights, if any, in descend-
ing order by the assigned weights and keep
only one word as candidate word
6
.
3.3.2 Determining Candidate Words for w
2
We use the following steps:
1. We determine the cut off frequency for word
w
2
as the frequency of the 3-gram w
1
w
2
w
3
in the Google Web 1T 3-grams, if the said
3-gram exists. Otherwise, we set the cut off
frequency of w
2
as 0.
2. We find all the 3-grams (where only w
2
is
changed while w
1
and w
3
are unchanged)
having frequency greater than the cut off fre-
quency of w
2
(determined in step 1). Let us
consider that we find n replacements of w
2
which are R
1
= {w
21
, w
22
, ? ? ? , w
2n
}, and
their frequencies F
1
= {f
21
, f
22
, ? ? ? , f
2n
},
5
We give more importance to string similarity function
with respect to frequency value function throughout the sec-
tion of ?determining candidate words? to have more candidate
words so that the chance of including the target word into the
set of candidate words gets higher. For this reason, we heuris-
tically set ?=0.85 in equation (7) instead of setting ?=0.5.
6
Sometimes the top candidate word might be either a plu-
ral form or a past participle form of the original word. Or
even it might be a high frequency function word (e.g., the).
We omit these type of words from the candidacy.
where f
2j
is the frequency of the 3-gram w
1
w
2j
w
3
.
3. For each w
2j
? R
1
, we calculate the string
similarity between w
2j
and w
2
using equa-
tion (5), and then assign a weight using the
following equation only to the words that re-
turn the string similarity value greater than
0.5.
weight = ?S(w
2
, w
2j
) + (1? ?)F (w
2j
)
4. We sort the words found in step 3 that were
given weights, if any, in descending order by
the assigned weights and keep only one word
as candidate word.
3.3.3 Determining Candidate Words for w
m
We use the following steps:
1. We determine the cut off frequency for word
w
m
as the frequency of the 3-gram w
m?2
w
m?1
w
m
in the Google Web 1T 3-grams,
if the said 3-gram exists. Otherwise, we set
the cut off frequency of w
m
as 0.
2. We find all the 3-grams (where only w
m
is changed while w
m?2
and w
m?1
are un-
changed) having frequency greater than the
cut off frequency of w
m
(determined in
step 1). Let us consider that we find
n replacements of w
m
which are R
2
=
{w
m1
, w
m2
, ? ? ? , w
mn
} and their frequencies
F
2
= {f
m1
, f
m2
, ? ? ? , f
mn
}, where f
mj
is
the frequency of the 3-gram w
m?2
w
m?1
w
mj
.
3. For each w
mj
? R
2
, we calculate the string
similarity between w
mj
and w
m
using equa-
tion (5) and then assign a weight using the
following equation only to the words that re-
turn the string similarity value greater than
0.5.
weight = ?S(w
m
, w
mj
) + (1? ?)F (w
mj
)
4. We sort the words found in step 3 that were
given weights, if any, in descending order by
the assigned weights and keep only one word
as the candidate word.
1246
4 Evaluation and Experimental Results
We used as test data the same data that Wilcox-
O?Hearn et al (2008) used in their evaluation of
Mays et al (1991) method, which in turn was a
replication of the data used by Hirst and St-Onge
(1998) and Hirst and Budanitsky (2005) to evalu-
ate their methods.
The data consisted of 500 articles (approxi-
mately 300,000 words) from the 1987?89 Wall
Street Journal corpus, with all headings, identi-
fiers, and so on removed; that is, just a long stream
of text. It is assumed that this data contains no er-
rors; that is, the Wall Street Journal contains no
malapropisms or other typos. In fact, a few typos
(both non-word and real-word) were noticed dur-
ing the evaluation, but they were small in number
compared to the size of the text.
Malapropisms were randomly induced into this
text at a frequency of approximately one word in
200. Specifically, any word whose base form was
listed as a noun in WordNet (but regardless of
whether it was used as a noun in the text; there was
no syntactic analysis) was potentially replaced by
any spelling variation found in the lexicon of the
ispell spelling checker
7
. A spelling variation was
defined as any word with an edit distance of 1 from
the original word; that is, any single-character in-
sertion, deletion, or substitution, or the transposi-
tion of two characters, that results in another real
word. Thus, none of the induced malapropisms
were derived from closed-class words, and none
were formed by the insertion or deletion of an
apostrophe or by splitting a word. The data con-
tained 1402 inserted malapropisms.
Because it had earlier been used for evaluat-
ing Mays et al (1991)?s trigram method, which
operates at the sentence level, the data set had
been divided into three parts, without regard
for article boundaries or text coherence: sen-
tences into which no malapropism had been in-
duced; the original versions of the sentences
that received malapropisms; and the malapropized
sentences. In addition, all instances of num-
bers of various kinds had been replaced by tags
such as <INTEGER>, <DOLLAR VALUE>,
7
Ispell is a fast screen-oriented spelling checker that
shows you your errors in the context of the original file, and
suggests possible corrections when it can figure them out.
The original was written in PDP-10 assembly in 1971, by
R. E. Gorin. The C version was written by Pace Willisson
of MIT. Geoff Kuenning added the international support and
created the current release.
and <PERCENTAGE VALUE>. Actual (ran-
dom) numbers or values were restored for these
tags. Some spacing anomalies around punctuation
marks were corrected. A detailed description of
this data can be found in (Hirst, 2008; Wilcox-
O?Hearn et al, 2008).
SUCCESSFUL CORRECTION:
The Iran revelations were particularly disturbing
to the Europeans because they came on the heels
of the Reykjavik summit between President Rea-
gan and Soviet reader ? leader [leader] Mikhail
Gorbachev.
Even the now sainted Abraham Lincoln was of-
ten reviled while in officer? office [office], some-
times painted by cartoonists and editorial writers
as that baboon in the White House.
FALSE POSITIVE:
? ? ? by such public displays of interest in Latinos
? Latin [Latinos], many undocumented ? ? ?
The southeast Asian nation was one reported
contributor ? contribution [contributor] to the
Nicaraguans.
FALSE NEGATIVE:
Kevin Mack, Geldermann president and chief
executive officer, didn?t return calls for comment
on the Clayton purchaser [purchase].
U.S. manufactures [manufacturers], in short,
again are confronting a ball game in which they
will be able to play.
TRUE POSITIVE DETECTION, FALSE POSI-
TIVE CORRECTION:
Hawkeye also is known to rear ? reader [fear]
that a bankruptcy-law filing by the parent com-
pany, which theoretically shouldn?t affect the op-
erations of its member banks, would spark runs on
the banks that could drag down the whole entity.
The London Daily News has quoted sources
saying as many as 23 British mercenaries were en-
listed by KMS to lid? slide [aid] the Contras.
Table 2: Examples of successful and unsuccessful
corrections. Italics indicate observed word, arrow
indicates correction, square brackets indicate in-
tended word.
Some examples of successful and unsuccessful
corrections using our proposed method are shown
in Table 2.
Table 3 shows our method?s results on the de-
scribed data set compared with the results for the
trigram method of Wilcox-O?Hearn et al (2008)
1247
Detection correction
R P F
1
R P F
1
Lexical cohesion
(Hirst and Budanitsky, 2005)
0.306 0.225 0.260 0.281 0.207 0.238
Trigrams
(Wilcox-O?Hearn et al, 2008)
0.544 0.528 0.536 0.491 0.503 0.497
Multiple 3-grams
0.890 0.445 0.593 0.763 0.381 0.508
Table 3: A comparison of recall, precision, and F
1
score for three methods of malapropism detection
and correction on the same data set.
and the lexical cohesion method of Hirst and Bu-
danitsky (2005). The data shown here for tri-
gram method are not from (Wilcox-O?Hearn et al,
2008), but rather are later results following some
corrections reported in (Hirst, 2008). We have not
tried optimizing our adjustable parameters: ? and
?s, because the whole data set was used as test-
ing set by the other methods we compare with. To
keep the comparison consistent, we did not use
any portion of the data set for training purpose.
Having optimized parameters could lead to a bet-
ter result. The performance is measured using Re-
call (R), Precision (P ) and F
1
:
R =
true positives
true positives + false negatives
P =
true positives
true positives + false positives
F
1
=
2PR
P +R
The fraction of errors correctly detected is the de-
tection recall and the fraction of detections that
are correct is the detection precision. Again, the
fraction of errors correctly amended is the correc-
tion recall and the fraction of amendments that
are correct is the correction precision. To give
an example, consider a sentence from the data set:
?The Philippine president, in her commencement
address at the academy, complained that the U.S.
was living? giving [giving] advice instead of the
aid ? said [aid] it pledged.?, where italics indi-
cate the observed word, arrow indicates the correc-
tion and the square brackets indicate the intended
word. The detection recall of this sentence is 1.0
and the precision is 0.5. The correction recall of
this sentence is 1.0 and the precision is 0.5. For
both cases, the F
1
score is 0.667.
We loose some precision because our method
tries to detect and correct errors for all the words
(except the first word) in the input sentence, and,
as a result, it generates more false positives than
the other methods. Even so, we get better F
1
scores than the other competing methods. Ac-
cepting 8.3 percents extra incorrect detections, we
get 34.6 percents extra correct detections of errors,
and similarly, accepting 12.2 percents extra incor-
rect amendments, we get 27.2 percents extra cor-
rect amendments of errors compared with the tri-
grams method (Wilcox-O?Hearn et al, 2008)
8
.
5 Conclusion
The Google 3-grams proved to be very useful in
detecting real-word errors, and finding the correc-
tions. We did not use the 4-grams and 5-grams
because of data sparsity. When we tried with 5-
grams the results were lower than the ones pre-
sented in Section 4. Having sacrificed a bit the
precision score, our proposed method achieves a
very good detection recall (0.89) and correction
recall (0.76). Our attempts to improve the detec-
tion recall or correction recall, while maintaining
the respective precisions as high as possible are
helpful to the human correctors who post-edit the
output of the real-word spell checker. If there is
no postediting, at least more errors get corrected
automatically. Our method could also detect and
correct misspelled words, not only malapropisms,
without any modification. In future work, we plan
to extend our method to allow for deleted or in-
serted words, and to find the corrected strings in
the Google Web 1T n-grams. In this way we
will be able to correct grammar errors too. We
also plan more experiments using the 5-grams, but
backing off to 4-grams and 3-grams when needed.
Acknowledgments
This work is funded by the Natural Sciences and
Engineering Research Council of Canada. We
want to thank Professor Graeme Hirst from the
Department of Computer Science, University of
Toronto, for providing the evaluation data set.
8
We can run our algorithm on subsets of data to check for
variance in the results. We cannot test statistical significance
compared to the related work (t-test), because we do not have
the system from related work to run it on subsets of the data.
1248
References
L. Allison and T.I. Dix. 1986. A bit-string longest-
common-subsequence algorithm. Information Pro-
cessing Letters, 23:305?310.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1.1. Technical report, Google
Research.
Lou Burnard, 2000. Reference Guide for the
British National Corpus (World Edition), October.
www.natcorp.ox.ac.uk/docs/userManual/urg.pdf.
Andrew J. Carlson, Jeffrey Rosen, and Dan Roth.
2001. Scaling up context-sensitive text correction.
In Proceedings of the Thirteenth Conference on In-
novative Applications of Artificial Intelligence Con-
ference, pages 45?50. AAAI Press.
Andrew R. Golding and Dan Roth. 1999. A winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lex-
ical cohesion. Natural Language Engineering,
11(1):87?111, March.
Graeme Hirst and David St-Onge, 1998. WordNet: An
electronic lexical database, chapter Lexical chains
as representations of context for the detection and
correction of malapropisms, pages 305?332. The
MIT Press, Cambridge, MA.
Graeme Hirst. 2008. An evaluation of the contextual
spelling checker of microsoft office word 2007, Jan-
uary. http://ftp.cs.toronto.edu/pub/gh/Hirst-2008-
Word.pdf.
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2(2):1?25.
G. Kondrak. 2005. N-gram similarity and distance. In
Proceedings of the 12h International Conference on
String Processing and Information Retrieval, pages
115?126, Buenos Aires, Argentina.
Karen Kukich. 1992. Technique for automatically
correcting words in text. ACM Comput. Surv.,
24(4):377?439.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Informa-
tion Processing and Management, 27(5):517?522.
I. D. Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K.J. Miller. 1993. Introduction to wordnet:
An on-line lexical database. Technical Report 43,
Cognitive Science Laboratory, Princeton University,
Princeton, NJ.
Jennifer Pedler. 2007. Computer Correction of Real-
word Spelling Errors in Dyslexic Text. Ph.D. thesis,
Birkbeck, London University.
Suzan Verberne. 2002. Context-sensitive spell check-
ing based on word trigram probabilities. Master?s
thesis, University of Nijmegen, February-August.
L. Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-
der Budanitsky. 2008. Real-word spelling correc-
tion with trigrams: A reconsideration of the mays,
damerau, and mercer model. In Alexander Gel-
bukh, editor, Proceedings, 9th International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-2008) (Lecture Notes
in Computer Science 4919, Springer-Verlag), pages
605?616, Haifa, February.
1249
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 49?56, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Semantic Similarity for Detecting Recognition Errors in Automatic 
Speech Transcripts 
 
 
Diana Inkpen Alain D?silets 
School of Information Technology and Engineering Institute for Information Technology 
University of Ottawa National Research Council of Canada 
Ottawa, ON, K1N 6H5, Canada Ottawa, ON, K1AOR6, Canada 
diana@site.uottawa.ca alain.desilets@nrc-cnrc.gc.ca 
 
  
Abstract 
Browsing through large volumes of spoken 
audio is known to be a challenging task for 
end users. One way to alleviate this prob-
lem is to allow users to gist a spoken audio 
document by glancing over a transcript 
generated through Automatic Speech Rec-
ognition. Unfortunately, such transcripts 
typically contain many recognition errors 
which are highly distracting and make gist-
ing more difficult. In this paper we present 
an approach that detects recognition errors 
by identifying words which are semantic 
outliers with respect to other words in the 
transcript. We describe several variants of 
this approach. We investigate a wide range 
of evaluation measures and we show that 
we can significantly reduce the number of 
errors in content words, with the trade-off 
of losing some good content words.  
1 Introduction 
Spoken audio documents are becoming more and 
more common place due to the rising popularity of 
technologies such as: video and audio conferenc-
ing, video web-casting and digital cameras for the 
consumer market. Unfortunately, speech docu-
ments are inherently hard to browse because of 
their transient nature.  For example, imagine trying 
to locate the audio segment in the recording of a 
60-minute meeting, where John talked about pro-
ject X. Typically, this would require fast forward-
ing through the audio by some amount, then 
listening and trying to remember if the current seg-
ment was spoken before or after the desired seg-
ment, then fast-forwarding or backtracking by a 
small amount, and so on.  
One way to make audio browsing of audio docu-
ments more efficient is to allow the user to navi-
gate through a textual transcript that is cross-
referenced with corresponding time points into the 
original audio (Nakatani et al 1998; Hirschberg et 
al. 1999). Such transcripts can easily be produced 
with Automatic Speech Recognition (ASR) sys-
tems today. Unfortunately, such transcripts typi-
cally contain recognition errors that make them 
hard to browse and understand. Although Word 
Error Rates (WER) of the order of 20% can be 
achieved for broadcast quality audio, the WER for 
more common situations (ex: less-than-broadcast 
quality recordings of meetings) is typically in the 
order of 50% or more.  
The work we present in this paper aims at auto-
matically identifying recognition errors and remov-
ing them from the transcript, in order to make 
gisting and browsing of the corresponding audio 
more efficient. For example, consider the follow-
ing portion of a transcript that was produced with 
the Dragon NaturallySpeaking speech recognition 
system from the audio of a meeting: 
?Weenie to decide quickly whether local for large 
expensive plasma screen aura for a bunch of 
smaller and cheaper ones and Holland together? 
Now consider the following filtered transcript 
where recognition errors were automatically blot-
ted out using our proposed algorithm:  
? ... to decide quickly whether ... large expensive 
plasma screen ... for a bunch of smaller and 
cheaper ones and ... together? 
We believe that transcripts like this second one 
may be more efficient for gisting and browsing the 
49
content of the original audio whose correct tran-
script is: 
?We need to decide quickly whether we will go for 
a large expensive plasma screen or for a bunch of 
smaller and cheaper ones and tile them together.? 
Our approach to filtering recognition errors is to 
identify semantic outliers. By this, we mean 
words that do not cohere well semantically with 
other words in the transcript. More often than not, 
such outliers turn out to be mistranscribed words. 
We present several variants of an algorithm for 
identifying semantic outliers, and evaluate them in 
terms of how well they are able to filter out recog-
nition errors. 
2 Related Work 
Hirschberg et al (1999), and Nakatani et al (1998) 
proposed the idea of using automatic transcripts for 
gisting and navigating audio documents. Text-
based summarization techniques on automatic 
speech transcription have also been used. For ex-
ample, the method of D?silets et al (2001) was 
found to produce accurate keyphrases for transcrip-
tions with Word Error Rates (WER) in the order of  
25%, but performance was less than ideal for tran-
scripts with WER in the order of 60%. With such 
transcripts, a large proportion of the extracted key-
phrases included serious transcription errors. Ink-
pen and D?silets (2004) presented an experiment 
that filters out errors in keywords extracted from 
speech, by identifying the keywords that are not 
semantically close to the rest of the keywords.  
Semantic similarity measures were used for 
many tasks. Two examples are: real-word error 
correction (Budanitsky and Hirst, 2000) and an-
swering synonym questions (Turney, 2001), 
(Jarmasz and Szpakowicz, 2003).  
There is a lot of research on confidence meas-
ures for identifying errors in speech recognition 
output. Most papers on this topic use information 
that is internal to the ASR system, generated by the 
decoder during the recognition process. Examples 
are likelihood ratios derived by a Viterbi decoder 
(Gillick et al, 1997), measures of competing 
words at a word boundary (Cox and Rose, 1996), 
word score densities in N-best lists, and various 
acoustic and phonetic features. Machine learning 
techniques were used to identify the best combina-
tions of features for classification (Chase, 1997) 
(Schaaf and Kemp, 1997) (Ma et al, 2001) 
(Skantze and Edlund, 2004) (Zhou and Meng, 
2004) (Zhou et al, 2005). Some of these methods 
achieve good performance, although they use dif-
ferent test sets and report different evaluation 
measures from the set we enumerate in Section 6.  
In our work, we use information that is external 
to the ASR system, because new knowledge seems 
likely to help in the detection of semantic outliers.  
In this respect, the work of Cox and Dasmahapatra 
(2000) is closest to ours. They compared the accu-
racy of a measure based on Latent Semantic 
Analysis (LSA) (Landauer and Dumais, 1997) to 
an ASR-based confidence measure, and found that 
the ASR-based measure (using N-best lists) outper-
formed the LSA approach. While the N-best lists 
approach was better at the high-Recall end of the 
spectrum, the LSA was better at the high-Precision 
end. They also showed that a hybrid combination 
of the two approaches worked best. Our work is 
similar to the LSA-based part of Cox and Dasma-
hapatra, except that we use Point-wise Mutual 
Information (PMI) instead of LSA. Because PMI 
scales up to very large corpora, it has been shown 
to work better than LSA for assessing the semantic 
similarity of words (Turney, 2001). Another dis-
tinguishing feature is that Cox and Dasmahapatra 
only looked at transcripts with moderate WER, 
whereas we additionally evaluate the technique for 
the purpose of doing error filtering on transcripts 
with high WER, which are more typical of non-
broadcast conversational audio.   
3 The Data 
We evaluated our algorithms on a randomly se-
lected subset of 100 stories from the TDT2 English 
Audio corpus. We conducted experiments with two 
types of automatically-generated speech tran-
scripts. The first ones were generated by the 
NIST/BBN time-adaptive speech recognizer and 
have a moderate WER (27.6%), which is represen-
tative of what can be obtained with a speaker-
independent ASR system tuned for the Broadcast 
News domain. In the rest of this paper, we refer to 
these moderate accuracy transcripts as the BBN 
dataset. The second set of transcripts was obtained 
using the Dragon NaturallySpeaking speaker-
dependent recognizer. Their WER (62.3%) was 
much higher because the voice model was not 
trained for speaker-independent broadcast quality 
audio. These transcripts approximate the type of 
50
high WER seen in more casual less-than-broadcast 
quality audio. We refer to these transcripts as the 
Dragon dataset. 
4 The method 
Our algorithm tries to detect recognition errors by 
identifying and filtering semantic outliers in the 
transcripts. In other words, it declares as recogni-
tion errors all the words with low semantic similar-
ity to other words in the transcript. The algorithm 
focuses on content words, i.e., words that do not 
appear in a list of 779 stopwords (including closed-
class words, such as prepositions, articles, etc.). 
The reason to ignore stopwords is that they tend to 
co-occur with most words, and are therefore se-
mantically coherent with most words. The basic 
algorithm for determining if a word w is a recogni-
tion error is as follows.  
 
1. Compute the neighborhood N(w) of w as the 
set of content words that occur before and after w 
in a context window (including w itself).  
 
2. Compute pair-wise semantic similarity scores 
S(wi, wj) between all pairs of words wi ? wj (in-
cluding w) in the neighborhood N(w), using a se-
mantic similarity measure. Scale up those S(wi, wj) 
by a constant so that they are all non-negative, and 
the smallest one is 0. 
 
3. For each wi in the neighborhood N(w) (includ-
ing w), compute its semantic coherence SC(wi). 
by ?aggregating? the pair-wise semantic similari-
ties S(wi, wj) of wi with all its neighbors (wi ? wj) 
into a single number. 
 
4. Let SCavg be the average of SC(wi) over all wi in 
the neighborhood N(w). 
 
5. Label w as a recognition error if SC(w) < 
K?SCavg, where K is a parameter that allows us to 
control the amount of error filtering (K% of the 
average semantic coherence score). Low values of 
K mean little error filtering and high values of K 
mean a lot of error filtering.  
 
We tested a number of variants of Steps 1-3. For 
Step 1, we experimented with two ways of com-
puting the neighborhood N(w). The first approach 
was to set N(w) to be all the words in the transcript 
(the All variant). The second neighborhood ap-
proach was to set N(w) to be the set of 10 content 
words before and after w in the transcript (the 
Window variant).  
For Step 2 we experimented with two different 
measures for evaluating the pair-wise semantic 
similarities S(wi, wj). The first measure used a 
hand-crafted dictionary (the Roget variant) 
whereas the second one used a statistical measure 
based on a large corpus (the PMI variant).  
For Step 3 we experimented with different 
schemes for ?aggregating? the pair-wise semantic 
similarities S(wi, wj) into a single semantic coher-
ence number SC(wi) for a given word wi. The first 
aggregation scheme was simply to average the 
SC(wi) values (the AVG variant). Note that with 
this scheme, we filter words that do not cohere 
well with all the words in the neighborhood N(w). 
This might be too aggressive in the case of the All 
variant, especially for longer or multi-topic audio 
documents. Therefore, we investigated other ag-
gregation schemes that only required words to co-
here well with a subset of the words in N(w). The 
second aggregation scheme was to set SC(wi) to 
the value of the most similar neighbor in N(w) (the 
MAX variant). The third aggregation scheme was 
to set SC(wi) to the average of the 3 most similar 
neighbors in N(w) (the 3MAX variant).  
Thus, there are altogether 2x2x3 = 12 possible 
configurations of the algorithm. In the rest of this 
paper, we will refer to specific configurations us-
ing the following naming scheme: Step1Variant-
Step2Variant-Step3Variant. For example, All-
PMI-AVG means the configuration that uses the 
All variant of Step 1, the PMI variant of Step 2, 
and the AVG variant of step 3. 
It is worth noting that all configurations of this 
algorithm are computationally intensive, mainly 
because of Step 2. However, since our aim is to 
provide transcripts for browsing audio recordings, 
we do not have to correct errors in real time.  
5 Choosing a semantic similarity measure 
Semantic similarity refers to the degree with which 
two words (two concepts) are related. For example, 
most human judges would agree that paper and 
pencil are more closely related than car and 
toothbrush. We use the term semantic similarity in 
this paper in a more general sense of semantic re-
latedness (two concepts can be related by their 
context of use without necessarily being similar).  
51
There are three types of semantic similarity 
measures: dictionary-based (lexical taxonomy 
structure), corpus-based, and hybrid. Most of the 
dictionary-based measures use path length in 
WordNet ? for example (Leacock and Chodorow, 
1998), (Hirst and St-Onge, 1998).  The corpus-
based measures use some form of vector similarity. 
The cosine measure uses frequency counts in its 
vectors and cosine to compute similarity; the sim-
pler methods use binary vectors and compute coef-
ficients such as: Matching, Dice, Jaccard, and 
Overlap. Examples of hybrid measures, based on 
WordNet and small corpora, are: Resnik (1995), 
Jiang and Conrath (1997), Lin (1998). All diction-
ary-based measures have the disadvantage of lim-
ited coverage: they cannot deal with many proper 
names and new words that are not in the diction-
ary. For WordNet-based approaches, there is the 
additional issue that they tend to work well only 
for nouns because the noun hierarchy in WordNet 
is the most developed. Also, most of the WordNet-
based measures do not work for words with differ-
ent part-of-speech, with small exceptions such as 
the extended Lesk measure (Banerjee and Peder-
sen, 2003).  
We did a pre-screening of the various semantic 
similarity measures in order to choose the one 
measure of each type (dictionary-based and cor-
pus-based) that seemed most promising for our 
task of detecting semantic outliers in automatic 
speech transcripts. The dictionary-based ap-
proaches that we evaluated were: the WordNet-
based measure by Leacock and Chodorow (1987), 
and one other dictionary-based measure that uses 
the Roget thesaurus. The Roget measure (Jarmasz 
and Szpakowicz, 2003) has the advantage that it 
works across part-of-speech. The corpus-based 
measures we evaluated were: (a) the cosine meas-
ure based on word co-occurrence vectors (Lesk, 
1969), (b) a new method that computes the Pearson 
correlation coefficient of the co-occurrence vectors 
instead of the cosine, and (c) a measure based on 
point-wise mutual information. We computed the 
first two measures on the 100-million-words Brit-
ish National Corpus (BNC)1, and the third one on a 
much larger-corpus of Web data (one terabyte) 
accessed through the Waterloo Multitext system 
(Clarke and Terra, 2003). The reason for using 
corpora of different sizes is that PMI is the only 
                                                          
1 http://www.natcorp.ox.ac.uk/index.html 
one of the three corpus-based approaches that 
scales up to a terabyte corpus. 
We describe here in detail the PMI corpus-based 
measure, because it is the most important for this 
paper. The semantic similarity score between two 
words w1 and w2 is defined as the probability of 
seeing the two words together divided by the prob-
ability of each word separately: PMI(w1,w2) = log 
[P(w1,w2) / (P(w1)?P(w2))] =  log [C(w1,w2)?N / 
(C(w1)?C(w2))], where C(w1,w2), C(w1), C(w2) are 
frequency counts, and N is the total number of 
words in the corpus. Such counts can easily and 
efficiently be retrieved for a terabyte corpus using 
the Waterloo Multitext system. 
In order to assess how well the semantic similar-
ity measures correlate with human perception, we 
use the set of 30 word pairs of Miller and Charles 
(1991), and the 65 pairs of Rubenstein and Goode-
nough (1965). Both used humans to judge the simi-
larity. The Miller and Charles pairs were a subset 
of the Rubenstein and Goodenough pairs. Note that 
both of those sets were limited to nouns that ap-
peared in the Roget thesaurus, and they are there-
fore favorably biased towards dictionary-based 
approaches. Table 1 shows the correlation of 5 
similarity measures for the Rubenstein and Goode-
nough (R&G) and Miller and Charles (M&C) data-
set. Note that although there are many WordNet-
based semantic similarity measures, we only show 
correlations for Leacock and Chodorow (L&C) 
because it was previously shown to be better corre-
lated (Jarmasz and Szpakowicz, 2003). We do not 
show figures for hybrid measures either because 
the same study showed L&C to be better. 
 
Table 1: Correlation between human assigned and various 
machine assigned semantic similarity scores. 
 Dictionary-based Corpus-based 
 L&C Roget Cos. Corr. PMI 
M&C 0.821 0.878 0.406 0.438 0.759 
R&G 0.852 0.818 0.472 0.517 0.746 
 
We see that the WordNet-based L&C measure 
based (Leacock and Chodorow, 1998 and the Ro-
get measure (Jarmasz and Szpakowicz, 2003) both 
achieve high correlations but the two vector cor-
pus-based measures (Cosine and Pearson Correla-
tion) achieve much lower correlation. The only 
corpus-based measure that does well is PMI, 
probably because of the much larger corpus.  
52
We decided to experiment with two of the meas-
ures (one corpus-based and one thesaurus based) 
for computing the semantic similarity of word 
pairs in Step 2 of the algorithm described in Sec-
tion 3. The two measures are: PMI computed on 
the Waterloo terabyte corpus and the Roget-based 
measure. These two seem the most promising 
given the nature of our task and the correlation fig-
ures reported above. 
6 Evaluation Measures 
We use several evaluation measures to determine 
how well our algorithm works for identifying se-
mantic outliers. As summarized in Table 2, the task 
of detecting recognition errors can be viewed as a 
classification task. For each word, the algorithm 
must predict whether or not that word was tran-
scribed correctly.  
 
Table 2: Recognition error detection can be seen as a classifi-
cation task. 
 
 
 Correctly 
transcribed 
(actual) 
NOT Correctly 
transcribed 
(actual) 
Correctly 
 transcribed 
 (predicted) 
True Positive 
(TP) 
False Positive 
(FP) 
NOT Correctly 
transcribed 
 (predicted) 
False Negative 
(FN) 
True Negative 
(TN) 
 
Note that we decide if a word is actually cor-
rectly transcribed or not by using the alignment of 
an automatic transcript with the manual transcript. 
A standard evaluation tool (sclite2) computes WER 
by counting the number of substitutions, deletions, 
and insertions needed to align a reference tran-
script with a hypothesis file. It also marks the 
words that are correct in automatic transcript (the 
hypothesis file). The rest of the words are the ac-
tual recognition errors (the insertions or substitu-
tions). The deletions ? words that are absent from 
the automatic transcript ? cannot be tagged by the 
confidence measure. 
We define the following performance measures 
in order to evaluate the improvement of the filtered 
transcripts compared to the initial transcripts:  
 
1. Word error rate in the initial transcript and in 
the filtered transcript. These measures can be com-
puted with and without stopwords (for which our 
                                                          
2 http://www.nist.gov/speech/tools/ 
algorithm does not apply). Note that WER without 
stopwords could be slightly lower than traditional 
WER mostly because content words tend to be rec-
ognized more accurately than stopwords (D?silets 
et al 2001). When filtering out semantic outliers, 
there will be gaps in the filtered transcript, there-
fore the general WER might not improve because 
it penalizes heavily the deletions.  
 
2. Content word error rate (cWER). This is the 
error rate in an automatic transcript (initial or fil-
tered) from the point of view of the confidence 
measure, for the content words only. It penalizes 
the words in the automatic transcripts that should 
not be there, but not any missing words (no dele-
tions are penalized). In the case of a transcript fil-
tered by our algorithm, it excludes not only the 
stopwords, but also the filtered words. We com-
puted cWER with sclite without penalizing for the 
gaps created by the filtered words.  
 
3. The percentage of lost good content words 
(%Lost). This is the percentage of correctly rec-
ognized content words which are lost in the proc-
ess of filtering out recognition errors, defined as:  
%Lost = 100 * FN / (TP + FN). We could also 
compute the percent of discarded words, without 
regard if they should have been filtered out or not. 
D = (TN + FN) / (TP + FP + TN + FN). 
 
4. Precision (P), Recall (R) and F-measure. Pre-
cision is the proportion of truly correct words con-
tained in the list of content words which the 
algorithm labeled as correct. Recall is the propor-
tion of truly correct content words that the algo-
rithm was able to retain. F-measure is the 
geometric mean of P and R and expresses a trade-
off between those two measures.  P = TP / (TP + 
FP); R = TP / (TP + FN); F = 2PR / (P+R). 
7 Results 
We ran various configurations of the algorithm 
described in Section 4 on the 100 story sample 
from the TDT2 corpus. This section discusses the 
results of those experiments. We studied the Preci-
sion-Recall (P-R) curves for various configurations 
of our algorithm over the 100 stories, for the two 
types of transcripts: the BBN and Dragon datasets. 
Figures 1 and 2 show an example for each dataset. 
Each point on a P-R curve shows the Precision and 
Recall for one value of K in {0, 20, 40, 60, 80, 
53
100, 120, 140, 160, 180, 200}. Points on the left 
correspond to aggressive filtering (high values of 
K), whereas points on the right correspond to leni-
ent filtering (low values of K).  
First, we looked at the relative merits of the two 
semantic similarity measures (PMI and Roget) for 
Step 2. Figures 1 and 2 plot the P-R curves for the 
All-PMI-AVG and All-Roget-AVG configurations. 
The graphs clearly indicate that PMI performs bet-
ter, especially for the high WER Dragon dataset. 
So PMI was used in the rest of the experiments.  
Next, we looked at the variants for setting up the 
neighborhood N(w) in Step 1 (All vs. Window). 
The three P-R curves for All-PMI-X and Window-
PMI-X for all aggregation approaches X in {AVG, 
MAX, 3MAX} are not shown here because they 
were similar to the P-PMI curves from Figures 1 
and 2, for the BBN dataset and for the Dragon 
dataset, respectively. The Window variant was 
marginally better for X=MAX on both datasets, as 
well as for X=3MAX on the BBN dataset. In all 
other cases, the Window and All variants per-
formed approximately the same.  
Next, we looked at the different schemes for ag-
gregating the pair-wise similarity scores in Step 3 
(AVG, MAX, 3MAX). By plotting the P-R curves 
for All-PMI-AVG, All-PMI-MAX, and All-PMI-
3MAX for both datasets we obtained again curves 
similar to the P-PMI curves from Figures 1 and 2. 
It seemed that AVG performs slightly better for 
high Recall, the difference being more marked 
when there is no windowing or when we are work-
ing on the Dragon dataset. The 3MAX and MAX 
variants seemed to be slightly better at high Preci-
sion with acceptable Recall values, with 3MAX 
being always equal or very slightly better than 
MAX. In an audio gisting and browsing context 
Precision is more important than Recall, therefore 
we can choose 3MAX. 
Having established Window-PMI-3MAX as one 
of the better configurations, we now look more 
closely at its performance.  
Figures 3 and 4 show how the content word er-
ror rate (cWER), the percentage of lost good words 
(%Lost), and the F-measure vary as we apply more 
and more aggressive error filtering (by increasing 
K) to both datasets. We see that our semantic out-
lier filtering approach is able to significantly re-
duce the number of transcription errors, while 
losing some correct words. For example, with the  
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Recall
P
re
ci
si
on
P-PMI
P-Roget
 
Fig 1: P-R curves of PMI vs. Roget (with All and AVG) on 
the BBN dataset. Each P-R point corresponds to a different 
value of the threshold K (high Recall for low values of K, high 
Precision for high values of K). 
 
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Recall
P
re
ci
si
on
P-PMI
P-Roget
 
Fig 2: P-R curves of PMI vs. Roget (with All and AVG) on 
the Dragon dataset 
 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
K (threshold)
cW
E
R
 / 
%
 lo
st
 / 
F
 
cWER-BBN
%Lost-BBN
F-measure
 
Fig.3. Content Words Error Rate (cWER), %Lost good key-
words (%Lost) and F-measure as a function of the filtering 
level K for the Window-PMI-3MAXconfiguration on the BBN 
dataset. 
 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
K (threshold)
cW
E
R
 / 
%
 lo
st
 / 
F
cWER-Dragon
%Lost-Dragon
F-measure
 
Fig.4. Content Words Error Rate (cWER), %Lost good key-
words (%Lost) and F-measure as a function of the filtering 
level K for the Window-PMI-3MAX configuration on the 
Dragon dataset. 
54
 
moderately accurate BBN dataset, we can reduce 
cWER by 50%, while losing 45% of the good con-
tent words (K=100).  For the low accuracy Dragon 
dataset, we can reduce cWER by 50%, while los-
ing 50% of the good content words (K=120). We 
can choose lower thresholds, for smaller reduction 
in cWER but smaller percent of lost good content 
words. Even small reductions in cWER are impor-
tant, especially for less-than-broadcast conditions 
where WER is initially very high.  
In general, we were not able to show an im-
provement in WER computed in a standard way 
(item 1 in Section 6), because of the high penalty 
due to deletions for both filtered semantic outliers 
and lost good content words. The percent of lost 
good words is admittedly too high, but this seems 
to be the case for speech error confidence measures 
(which do not remove the words tagged as incor-
rect). Also, for the purpose of audio browsing and 
gisting, we believe that fewer errors even with loss 
of content are preferable for intelligibility.  
Comparing our results to those reported by Cox 
and Dasmahapatra (2000) our PMI-based measure 
seems to performs better than their LSA-based 
measure, judging by the shape of the Precision-
Recall curves. (For example, at Precision=90%, 
they obtained Recall=12%, whereas we obtain 
20%. At Precision=80%, they obtain Recall=50%, 
whereas we get Recall=100%.) Note however that 
their results and ours are not completely compara-
ble since the experiments used different audio cor-
pora (WSJCAM0 vs. TDT2), but those two 
corpora seem to exhibit similar initial WERs (the 
WER appears to be around 30% for WSJCAM0; 
the WER is 27.6% for our BBN dataset). Also, it is 
worth noting the LSA measure was computed 
based on a corpus that was very similar to the au-
dio corpus used to evaluate the performance of the 
measure (both were Wall Street Journal corpora). 
If one was to evaluate this measure on audio from 
a completely different domain (ex: news in the sci-
entific or technical domain), one would expect the 
performance to drop significantly. In contrast, our 
PMI measure was computed based on a general 
sample of the World Wide Web, which was not 
tailored to the audio corpus used to evaluate its 
performance. Therefore, our numbers are probably 
more representative of what would be experienced 
with audio corpora outside of the Wall Street Jour-
nal domain.  
8 Conclusion and Future Work 
We presented a basic method for filtering recogni-
tion errors of content words from automatic speech 
transcripts, by identifying semantic outliers. We 
described and evaluated several variants of the ba-
sic algorithm.  
In future work, we plan to run our experiments 
on other datasets when they become available to 
us. In particular, we want to experiment with 
multi-topic audio documents where we expect 
more marked advantages for windowing and alter-
native aggregation schemes like MAX and 3MAX. 
We plan to explore ways to scale up other corpus-
based semantic similarity measures to large tera-
byte corpora. We plan to explore more approaches 
to detecting semantic outliers, for example cluster-
ing or lexical chains (Hirst and St-Onge, 1997).  
The most promising direction is to combine our 
method with confidence measures that use internal 
information from the ASR system (although the 
internal information is hard to obtain when using 
an ASR as a black box, and it could be recognizer-
specific). A combination is likely to improve the 
performance, with the PMI-based measure contrib-
uting at the high-Precision end and the internal 
ASR measure contributing to the high-Recall end 
of the spectrum. To increase Recall we can also 
identify named entities and not filter them out. 
Some named entities could have high semantic 
similarity with the text if they are frequently men-
tioned in the same contexts in the Web corpus, but 
some names could be common to many contexts. 
Another future direction will be to actually cor-
rect the errors instead of just filtering them out. For 
example, we might look at the top N speech recog-
nizer hypotheses (for a fairly large N like 1000) 
and choose the one that maximizes semantic cohe-
sion. A final direction for research is to conduct 
experiments with human subjects, to evaluate the 
degree to which filtered transcripts are better than 
unfiltered ones for tasks like browsing, gisting and 
searching audio clips. 
Acknowledgments 
We thank the following people: Peter Turney and his col-
leagues for useful feedback; Gerald Penn for feedback on 
earlier versions of this paper; Egidio Terra and Charlie Clarke 
for giving us permission to use the Multitext System, the NRC 
copy; Mario Jarmasz and Stan Szpakowicz for sharing their 
code for the Roget similarity measure; Aminul Islam for the 
55
correlation figures and the correlative measure. Our research is 
supported by the Natural Sciences and Engineering Research 
Council of Canada, University of Ottawa, IBM Toronto Cen-
tre for Advanced Studies, and the National Research Council.  
References 
Alexander Budanitsky and Graeme Hirst. 2001. Semantic 
distance in WordNet: An experimental, application-
oriented evaluation of five measures. Workshop on Word-
Net and Other Lexical Resources, NAACL 2001, Pitts-
burgh, PA, USA, 29-34. 
Satanjeev Banerjee, and Ted Pedersen. 2003. Gloss overlaps 
as a measure of semantic relatedness. In Proceedings of the 
Eighteenth International Joint Conference on Artificial In-
telligence (IJCAI?03), Acapulco, Mexico. 
Charlie Clarke and Egidio Terra. 2003. Passage retrieval vs. 
document retrieval for factoid question answering. ACM 
SIGIR?03, 327-328. 
Stephen Cox and Srinandan Dasmahapatra. 2000. A Semanti-
cally-Based Confidence Measure for Speech Recognition, 
Int. Conf. on Spoken Language Processing, Beijing, China, 
vol. 4, 206-209. 
Stephen Cox and R.C. Rose. 1996. Confidence Measures for 
the SWITCHBOARD Database. IEEE Conf. on Acoustics, 
Speech, and Signal Processing, 511-515. 
Lin Chase. 1997. Word and Acoustic Confidence Annotation 
for Large Vocabulary Speech Recognition, Proceedings of 
Eurospeech'97, Rhodes, Greece, 815-818. 
Alain D?silets, Berry de Brujin, and Joel Martin. 2001. Ex-
tracting keyphrases from spoken audio documents. 
SIGIR?01 Workshop on Information Retrieval Techniques 
for Speech Applications, 36-50. 
Diana Inkpen and Alain D?silets. 2004. Extracting semanti-
cally-coherent keyphrases from speech. Canadian Acous-
tics, 32(3):130-131. 
L.Gillick, Y.Ito, and J.Young. 1997. A Probabilistic Approach 
to Confidence Estimation and Evaluation. IEEE Conf. on 
Acoustics, Speech, and Signal Processing, 266-277. 
Julia Hirschberg, Steve Whittaker, Donald Hindle, Fernando 
Pereira, Amit Singhal. 1999. Finding information in audio: 
a new paradigm for audio browsing and retrieval. Proceed-
ings of the ESCA ETRW Workshop, 26-33. 
Graeme Hirst and David St-Onge. 1998. Lexical chains as 
representations of context for the detection and correction 
of malapropisms. In: C. Fellbaum (editor), WordNet: An 
electronic lexical database and some of its applications, 
The MIT Press, Cambridge, MA, 305-332. 
Mario Jarmasz and Stan Szpakowicz. 2003. Roget's thesaurus 
and semantic similarity, Proceedings of the International 
Conference RANLP-2003 (Recent Advances in Natural 
Language Processing), Borovets, Bulgaria, 212-219. 
Jay J. Jiang and David W. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. In Pro-
ceedings of the International Conference on Research in 
Computational Linguistics (ROCLING X), Taiwan. 
Thomas Landauer and Susan Dumais. 1997. A solution to 
Plato?s problem: representation of knowledge.  Psychologi-
cal Review 104: 211-240. 
Claudia Leacock and Martin Chodorow. 1998. Combining 
local context and WordNet similarity for word sense identi-
fication. In C. Felbaum (editor), WordNet: An Electronic 
Lexical Database, MIT Press, Cambridge, MA, 264-283. 
M.E. Lesk. 1969. Word-word associations in document re-
trieval systems. American Documentation 20(1): 27-38.  
Dekang Lin. 1998. An information-theoretic definition of 
similarity. In Proceedings of the 15th International Confer-
ence of Machine Learning. 
Changxue Ma, Mark A. Randolph, and Joe Drish. 2001. A 
support vector machines-based rejection technique for 
speech recognition. Proceedings of ICASSP'01, Salt Lake 
City, USA, vol. 1, 381-384.  
Lidia Mangu and M. Padmanabhan. 2001. Error corrective 
mechanisms for speech recognition. Proceedings of 
ICASSP'01, Salt Lake City, USA, vol. 1, 29-32.  
George A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity, Language and Cognitive Proc-
esses, 6(1):1-28. 
Christine Nakatani, Steve Whittaker, Julia Hirshberg. 1998. 
Now you hear it, now you don?t: Empirical Studies of Au-
dio Browsing Behavior. Proceedings of the Fifth Interna-
tional Conference on Spoken Language Processing, 
(SLP?98), Sydney, Australia. 
Philip Resnik. 1995. Using information content to evaluate 
semantic similarity. In Proceedings of the 14th Joint Inter-
national Conference of Artificial Intelligence, Montreal, 
Canada, 448-453. 
Herbert Rubenstein and John B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of ACM, 
8(10): 627-633.  
Thomas Schaaf and Thomas Kemp. 1997. Confidence meas-
ures for spontaneous speech recognition, in Proceedings of 
ICASSP?97, Munich, Germany, vol. II, 875-878. 
Gabriel Skantze and J. Edlund. 2004. Error detection on word 
level. In Proceedings of Robust 2004, Norwich. 
Peter D. Turney. 2001. Mining the Web for synonyms: PMI-
IR versus LSA on TOEFL, Proceedings of the Twelfth 
European Conference on Machine Learning (ECML-2001), 
Freiburg, Germany, 491-502.  
Lina Zhou, Jinjuan Feng, Andrew Sears, Yongmei Shi. 2005. 
Applying the Na?ve Bayes Classifier to Assist Users in De-
tecting Speech Recognition Errors. Procs. of the 38th An-
nual Hawaii International Conference on System Sciences). 
Z.Y. Zhou and Helen M. Meng, 2004. A Two-Level Schema 
for Detecting Recognition Errors, Proceedings of the 8th 
International Conference on Spoken Language Processing 
(ICSLP), Korea. 
56
Building and Using
a Lexical Knowledge Base
of Near-Synonym Differences
Diana Inkpen?
University of Ottawa
Graeme Hirst?
University of Toronto
Choosing the wrong word in a machine translation or natural language generation system can
convey unwanted connotations, implications, or attitudes. The choice between near-synonyms
such as error, mistake, slip, and blunder?words that share the same core meaning, but differ
in their nuances?can be made only if knowledge about their differences is available.
We present a method to automatically acquire a new type of lexical resource: a knowledge
base of near-synonym differences. We develop an unsupervised decision-list algorithm that learns
extraction patterns from a special dictionary of synonym differences. The patterns are then used
to extract knowledge from the text of the dictionary.
The initial knowledge base is later enriched with information from other machine-readable
dictionaries. Information about the collocational behavior of the near-synonyms is acquired from
free text. The knowledge base is used by Xenon, a natural language generation system that shows
how the new lexical resource can be used to choose the best near-synonym in specific situations.
1. Near-Synonyms
Near-synonyms are words that are almost synonyms, but not quite. They are not fully
intersubstitutable, but vary in their shades of denotation or connotation, or in the com-
ponents of meaning they emphasize; they may also vary in grammatical or collocational
constraints. For example, the word foe emphasizes active warfare more than enemy does
(Gove 1984); the distinction between forest and woods is a complex combination of size,
proximity to civilization, and wildness (as determined by the type of animals and plants
therein) (Room 1981); among the differences between task and job is their collocational
behavior with the word daunting: daunting task is a better collocation than daunting job.
More examples are given in Table 1 (Hirst 1995).
There are very few absolute synonyms, if they exist at all. So-called dictionaries
of synonyms actually contain near-synonyms. This is made clear by dictionaries such
as Webster?s New Dictionary of Synonyms (Gove 1984) and Choose the Right Word (here-
after CTRW) (Hayakawa 1994), which list clusters of similar words and explicate the
differences between the words in each cluster. An excerpt from CTRW is presented in
Figure 1. These dictionaries are in effect dictionaries of near-synonym discrimination.
? School of Information Technology and Engineering, Ottawa, ON, Canada, K1N 6N5;
diana@site.uottawa.ca.
? Department of Computer Science, Toronto, ON, Canada, M5S 3G4; gh@cs.toronto.edu.
Submission received: 5 October 2004; revised submission received: 15 June 2005; accepted for publication:
4 November 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 2
Table 1
Examples of near-synonym variations.
Type of variation Example
Stylistic, formality pissed : drunk : inebriated
Stylistic, force ruin : annihilate
Expressed attitude skinny : thin : slim
Emotive daddy : dad : father
Continuousness seep : drip
Emphasis on different aspects of meaning enemy : foe
Fuzzy boundary woods : forest
Collocational task : job (in the context of daunting)
Writers often turn to such resources when confronted with a choice between near-
synonyms, because choosing the wrong word can be imprecise or awkward or convey
unwanted implications. These dictionaries are made for human use, and they are avail-
able only on paper, not in electronic format.
Understanding the differences between near-synonyms is important for fine-
grained distinctions in machine translation. For example, when translating the French
word erreur to English, one of the near-synonyms mistake, blooper, blunder, boner, con-
tretemps, error, faux pas, goof, slip, solecism could be chosen, depending on the context and
on the nuances that need to be conveyed. More generally, knowledge of near-synonyms
is vital in natural language generation systems that take a nonlinguistic input (semantic
representation) and generate text. When more than one word can be used, the choice
should be based on some explicit preferences. Another application is an intelligent
thesaurus, which would assist writers not only with lists of possible synonyms but also
with the nuances they carry (Edmonds 1999).
1.1 Distinctions among Near-Synonyms
Near-synonyms can vary in many ways. DiMarco, Hirst, and Stede (1993) analyzed
the types of differences adduced in dictionaries of near-synonym discrimination. They
found that there was no principled limitation on the types, but a small number of types
occurred frequently. A detailed analysis of the types of variation is given by Edmonds
(1999). Some of the most relevant types of distinctions, with examples from CTRW, are
presented below.
Denotational distinctions Near-synonyms can differ in the frequency with which
they express a component of their meaning (e.g., Occasionally, invasion suggests a large-
scale but unplanned incursion), in the latency (or indirectness) of the expression of the
component (e.g., Test strongly implies an actual application of these means), and in fine-
grained variations of the idea itself (e.g., Paternalistic may suggest either benevolent
rule or a style of government determined to keep the governed helpless and dependent). The
frequency is signaled in the explanations in CTRW by words such as always, usually,
sometimes, seldom, never. The latency is signaled by many words, including the obvious
words suggests, denotes, implies, and connotes. The strength of a distinction is signaled by
words such as strongly and weakly.
Attitudinal distinctions Near-synonyms can convey different attitudes of the
speaker toward an entity in the situation. Attitudes can be pejorative, neutral, or favor-
able. Examples of sentences in CTRW expressing attitudes, in addition to denotational
distinctions, are these: Blurb is also used pejoratively to denote the extravagant and insincere
224
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 1
An excerpt from Choose the Right Word (CTRW) by S. I. Hayakawa. Copyright ?1987. Reprinted
by arrangement with HarperCollins Publishers, Inc.
praise common in such writing. Placid may have an unfavorable connotation in suggesting an
unimaginative, bovine dullness of personality.
Stylistic distinctions Stylistic variations of near-synonyms concern their level of
formality, concreteness, force, floridity, and familiarity (Hovy 1990). Only the first
three of these occur in CTRW. A sentence in CTRW expressing stylistic distinctions
is this: Assistant and helper are nearly identical except for the latter?s greater informality.
Words that signal the degree of formality include formal, informal, formality, and slang.
The degree of concreteness is signaled by words such as abstract, concrete, and concretely.
Force can be signaled by words such as emphatic and intensification.
1.1.1 The Class Hierarchy of Distinctions. Following the analysis of the distinctions
among near-synonyms of Edmonds and Hirst (2002), we derived the class hierarchy of
225
Computational Linguistics Volume 32, Number 2
distinctions presented in Figure 2. The top-level class DISTINCTIONS consists of DENO-
TATIONAL DISTINCTIONS, ATTITUDE, and STYLE. The last two are grouped together in
a class ATTITUDE-STYLE DISTINCTIONS because they are expressed by similar syntactic
constructions in the text of CTRW. Therefore the algorithm to be described in Section 2.2
will treat them together.
The leaf classes of DENOTATIONAL DISTINCTIONS are SUGGESTION, IMPLICATION,
and DENOTATION; those of ATTITUDE are FAVORABLE, NEUTRAL, and PEJORATIVE;
those of STYLE are FORMALITY, CONCRETENESS, and FORCE. All these leaf nodes have
the attribute STRENGTH, which takes the values low, medium, and high. All the leaf nodes
except those in the class STYLE have the attribute FREQUENCY, which takes the values
always, usually, sometimes, seldom, and never. The DENOTATIONAL DISTINCTIONS
have an additional attribute: the peripheral concept that is suggested, implied, or
denoted.
1.2 The Clustered Model of Lexical Knowledge
Hirst (1995) and Edmonds and Hirst (2002) show that current models of lexical
knowledge used in computational systems cannot account well for the properties of
near-synonyms.
The conventional view is that the denotation of a lexical item is represented as
a concept or a structure of concepts (i.e., a word sense is linked to the concept it
lexicalizes), which are themselves organized into an ontology. The ontology is often
language independent, or at least language neutral, so that it can be used in multilin-
gual applications. Words that are nearly synonymous have to be linked to their own
slightly different concepts. Hirst (1995) showed that such a model entails an awkward
taxonomic proliferation of language-specific concepts at the fringes, thereby defeating
the purpose of a language-independent ontology. Because this model defines words
Figure 2
The class hierarchy of distinctions: rectangles represent classes, ovals represent attributes that a
class and its descendants have.
226
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
in terms of necessary and sufficient truth conditions, it cannot account for indirect
expressions of meaning or for fuzzy differences between near-synonyms.
Edmonds and Hirst (2002) modified this model to account for near-synonymy. The
meaning of each word arises out of a context-dependent combination of a context-
independent denotation and a set of explicit differences from its near-synonyms, much
as in dictionaries of near-synonyms. Thus the meaning of a word consists both of
necessary and sufficient conditions that allow the word to be selected by a lexical choice
process and a set of nuances of indirect meaning that may be conveyed with different
strengths. In this model, a conventional ontology is cut off at a coarse grain and the
near-synonyms are clustered under a shared concept, rather than linking each word to
a separate concept. The result is a clustered model of lexical knowledge. Thus, each cluster
has a core denotation that represents the essential shared denotational meaning of its
near-synonyms. The internal structure of a cluster is complex, representing semantic
(or denotational), stylistic, and expressive (or attitudinal) differences between the near-
synonyms. The differences or lexical nuances are expressed by means of peripheral
concepts (for denotational nuances) or attributes (for nuances of style and attitude).
The clustered model has the advantage that it keeps the ontology language neutral
by representing language-specific distinctions inside the cluster of near-synonyms. The
near-synonyms of a core denotation in each language do not need to be in separate
clusters; they can be part of one larger cross-linguistic cluster.
However, building such representations by hand is difficult and time-consuming,
and Edmonds and Hirst (2002) completed only nine of them. Our goal in the present
work is to build a knowledge base of these representations automatically by extracting
the content of all the entries in a dictionary of near-synonym discrimination. Un-
like lexical resources such as WordNet (Miller 1995), in which the words in synsets
are considered ?absolute? synonyms, ignoring any differences between them, and
thesauri such as Roget?s (Roget 1852) and Macquarie (Bernard 1987), which contain
hierarchical groups of similar words, the knowledge base will include, in addition
to the words that are near-synonyms, explicit explanations of differences between
these words.
2. Building a Lexical Knowledge Base of Near-Synonym Differences
As we saw in Section 1, each entry in a dictionary of near-synonym discrimination
lists a set of near-synonyms and describes the differences among them. We will use
the term cluster in a broad sense to denote both the near-synonyms from an entry and
their differences. Our aim is not only to automatically extract knowledge from one such
dictionary in order to create a lexical knowledge base of near-synonyms (LKB of NS),
but also to develop a general method that could be applied to any such dictionary
with minimal adaptation. We rely on the hypothesis that the language of the entries
contains enough regularity to allow automatic extraction of knowledge from them.
Earlier versions of our method were described by Inkpen and Hirst (2001).
The task can be divided into two phases, treated by two consecutive modules, as
shown in Figure 3. The first module, the extraction module, will be described in this
section. The generic clusters produced by this module contain the concepts that near-
synonyms may involve (the peripheral concepts) as simple strings. This generic LKB
of NS can be adapted for use in any Natural Language Processing (NLP) application.
The second module customizes the LKB of NS so that it satisfies the requirements of
the particular system that is to employ it. This customization module transforms the
227
Computational Linguistics Volume 32, Number 2
Figure 3
The two modules of the task.
strings from the generic clusters into concepts in the particular ontology. An example of
a customization module will be described in Section 6.
The dictionary that we use is Choose the Right Word (Hayakawa 1994) (CTRW),1
which was introduced in Section 1. CTRW contains 909 clusters, which group 5,452
near-synonyms (more precisely, near-synonym senses, because a word can be in more
than one cluster) with a total of 14,138 sentences (excluding examples of usage), from
which we derive the lexical knowledge base. An example of the results of this phase,
corresponding to the second, third, and fourth sentence for the absorb cluster in Figure 1,
is presented in Figure 4.
This section describes the extraction module, whose architecture is presented in
Figure 5. It has two main parts. First, it learns extraction patterns; then it applies the
patterns to extract differences between near-synonyms.
2.1 Preprocessing the Dictionary
After OCR scanning of CTRW and error correction, we used XML markup to segment
the text of the dictionary into cluster name, cluster identifier, members (the near-
synonyms in the cluster), entry (the textual description of the meaning of the near-
synonyms and of the differences among them), cluster?s part of speech, cross-references
to other clusters, and antonyms list. Sentence boundaries were detected by using
general heuristics, plus heuristics specific for this particular dictionary; for example,
examples appear in square brackets and after a colon.
2.2 The Decision-List Learning Algorithm
Before the system can extract differences between near-synonyms, it needs to learn
extraction patterns. For each leaf class in the hierarchy (Figure 2) the goal is to learn
a set of words and expressions from CTRW?that is, extraction patterns?that charac-
terizes descriptions of the class. Then, during the extraction phase, for each sentence (or
fragment of a sentence) in CTRW the program will decide which leaf class is expressed,
with what strength, and what frequency. We use a decision-list algorithm to learn sets
of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and
ATTITUDE-STYLE DISTINCTIONS. These are split further for each leaf class, as explained
in Section 2.3.
The algorithm we implemented is inspired by the work of Yarowsky (1995) on word
sense disambiguation. He classified the senses of a word on the basis of other words
that the given word co-occurs with. Collins and Singer (1999) classified proper names
1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project.
228
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 4
Example of distinctions extracted from CTRW.
as PERSON, ORGANIZATION, or LOCATION using contextual rules (that rely on other
words appearing in the context of the proper names) and spelling rules (that rely on
words in the proper names). Starting with a few spelling rules (using some proper-name
features) in the decision list, their algorithm learns new contextual rules; using these
rules, it then learns more spelling rules, and so on, in a process of mutual bootstrapping.
Riloff and Jones (1999) learned domain-specific lexicons and extraction patterns (such
as shot in ?x? for the terrorism domain). They used a mutual bootstrapping technique to
alternately select the best extraction pattern for a category and add its extractions to the
semantic lexicon; the newly added entries in the lexicon help in the selection of the next
best extraction pattern.
Our decision-list (DL) algorithm (Figure 6) is tailored for extraction from CTRW.
Like the algorithm of Collins and Singer (1999), it learns two different types of rules:
Main rules are for words that are significant for distinction classes; auxiliary rules are
for frequency words, strength words, and comparison words. Mutual bootstrapping in
the algorithm alternates between the two types. The idea behind the algorithm is that
starting with a few main rules (seed words), the program selects examples containing
them and learns a few auxiliary rules. Using these, it selects more examples and learns
new main rules. It keeps iterating until no more new rules are learned.
The rules that the program learns are of the form x ? h(x), meaning that word
x is significant for the given class with confidence h(x). All the rules for that class
form a decision list that allows us to compute the confidence with which new patterns
Figure 5
The architecture of the extraction module.
229
Computational Linguistics Volume 32, Number 2
Figure 6
The decision-list learning algorithm.
are significant for the class. The confidence h(x) for a word x is computed with the
formula:
h(x) =
count(x, E?) + ?
count(x, E) + k?
(1)
where E? is the set of patterns selected for the class, and E is the set of all input data.
So, we count how many times x is in the patterns selected for the class versus the total
number of occurrences in the training data. Following Collins and Singer (1999), k =
2, because there are two partitions (relevant and irrelevant for the class). ? = 0.1 is a
smoothing parameter.
In order to obtain input data, we replace all the near-synonyms in the text of the dic-
tionary with the term near syn; then we chunk the text with Abney?s chunker (Abney
1996). The training set E is composed of all the verb phrases, noun phrases, adjectival
phrases, and adverbial phrases (denoted vx, nx, ax, rx, respectively) that occur more
230
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
than t times in the text of the dictionary (where t = 3 in our experiments). Phrases
that occur very few times are not likely to be significant patterns and eliminating them
makes the process faster (fewer iterations are needed).
We apply the DL algorithm for each of the classes DENOTATIONAL DISTINCTIONS
and ATTITUDE-STYLE DISTINCTIONS. The input to the algorithm is as follows: the set
E of all chunks, the main seed words, and the restrictions on the part of speech of
the words in main and auxiliary rules. For the class DENOTATIONAL DISTINCTIONS
the main seed words are suggest, imply, denote, mean, designate, connote; the words in
main rules are verbs and nouns, and the words in auxiliary rules are adverbs and
modals. For the class ATTITUDE-STYLE DISTINCTIONS the main seed words are formal,
informal, pejorative, disapproval, favorable, abstract, concrete; the words in main rules are
adjectives and nouns, and the words in auxiliary rules are adverbs. For example, for the
class DENOTATIONAL DISTINCTIONS, starting with the rule suggest ? 0.99, the program
selects examples such as these (where the numbers give the frequency in the training
data):
[vx [md can] [vb suggest]]--150
[vx [rb sometimes] [vb suggest]]--12
Auxiliary rules are learned for the words sometimes and can, and using these rules, the
program selects more examples such as these:
[vx [md can] [vb refer]]--268
[vx [md may] [rb sometimes] [vb imply]]--3
From these, new main rules are learned for the words refer and imply. With these rules,
more auxiliary rules are selected?for the word may and so on.
The ATTITUDE and STYLE classes had to be considered together because both of
them use adjectival comparisons. Examples of ATTITUDE-STYLE DISTINCTIONS class are
these:
[ax [rbs most] [jj formal]]--54
[ax [rb much] [more more] [jj formal]]--9
[ax [rbs most] [jj concrete]]--5
2.3 Classification and Extraction
After we run the DL algorithm for the class DENOTATIONAL DISTINCTIONS, we split
the words in the list of main rules into its three sub-classes, as shown in Figure 2. This
sub-classification is manual for lack of a better procedure. Furthermore, some words can
be insignificant for any class (e.g., the word also) or for the given class; during the sub-
classification we mark them as OTHER. We repeat the same procedure for frequencies
and strengths with the words in the auxiliary rules. The words marked as OTHER and
the patterns that do not contain any word from the main rules are ignored in the next
processing steps. Similarly, after we run the algorithm for the class ATTITUDE-STYLE
DISTINCTIONS, we split the words in the list of main rules into its sub-classes and
sub-sub-classes (Figure 2). Frequencies are computed from the auxiliary rule list, and
strengths are computed by a module that resolves comparisons.
Once we had obtained the words and patterns for all the classes, we implemented
an automatic knowledge-extraction program that takes each sentence in CTRW and
231
Computational Linguistics Volume 32, Number 2
tries to extract one or more pieces of knowledge from it. Examples of results after this
stage are presented in Figure 4. The information extracted for denotational distinctions
is the near-synonym itself, the class, frequency, and strength of the distinction, and
the peripheral concept. At this stage, the peripheral concept is a string extracted from
the sentence. Strength takes the value low, medium, or high; frequency takes the value
always, usually, sometimes, seldom, or never. Default values (usually and medium)
are used when the strength or the frequency are not specified in the sentence. The
information extracted for attitudinal and stylistic distinctions is analogous.
The extraction program considers what near-synonyms each sentence fragment is
about (most often expressed as the subject of the sentence), what the expressed distinc-
tion is, and with what frequency and relative strength. If it is a denotational distinction,
then the peripheral concept involved has to be extracted too (from the object position in
the sentence). Therefore, our program looks at the subject of the sentence (the first noun
phrase before the main verb) and the object of the sentence (the first noun phrase after
the main verb). This heuristic works for sentences that present one piece of information.
There are many sentences that present two or more pieces of information. In such cases,
the program splits a sentence into coordinated clauses (or coordinated verb phrases) by
using a parser (Collins 1996) to distinguish when a coordinating conjunction (and, but,
whereas) is conjoining two main clauses or two parts of a complex verb phrase. From 60
randomly selected sentences, 52 were correctly dealt with (41 needed no split, 11 were
correctly split). Therefore, the accuracy was 86.6%. The eight mistakes included three
sentences that were split but should not have been, and five that needed splitting but
were not. The mistakes were mainly due to wrong parse trees.
When no information is extracted in this way, a few general patterns are matched
with the sentence in order to extract the near-synonyms; an example of such pattern
is: To NS1 is to NS2 .... There are also heuristics to retrieve compound subjects of
the form near-syn and near-syn and near-syn, near-syn, and near-syn. Once the class is
determined to be either DENOTATIONAL DISTINCTIONS or ATTITUDE-STYLE DISTINC-
TIONS, the target class (one of the leaves in the class hierarchy in Figure 2) is deter-
mined by using the manual partitions of the rules in the main decision list of the two
classes.
Sometimes the subject of a sentence refers to a group of near-synonyms. For ex-
ample, if the subject is the remaining words, our program needs to assert information
about all the near-synonyms from the same cluster that were not yet mentioned in the
text. In order to implement coreference resolution, we applied the same DL algorithm
to retrieve expressions used in CTRW to refer to near-synonyms or groups of near-
synonyms.
Sometimes CTRW describes stylistic and attitudinal distinctions relative to other
near-synonyms in the cluster. Such comparisons are resolved in a simple way by consid-
ering only three absolute values: low, medium, high. We explicitly tell the system which
words represent what absolute values of the corresponding distinction (e.g., abstract is
at the low end of Concreteness) and how the comparison terms increase or decrease
the absolute value (e.g., less abstract could mean a medium value of Concreteness).
2.4 Evaluation
Our program was able to extract 12,365 distinctions from 7,450 of the 14,138 sentences of
CTRW. (The rest of the sentences usually do not contain directly expressed distinctions;
for example: A terror-stricken person who is drowning may in panic resist the efforts of
someone who is trying to save him.)
232
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
In order to evaluate the final results, we randomly selected 25 clusters as a de-
velopment set, and another 25 clusters as a test set. The development set was used
to tune the program by adding new patterns if they helped improve the results. The
test set was used exclusively for testing. We built by hand a standard solution for
each set. The baseline algorithm is to choose the default values whenever possible.
There are no defaults for the near-synonyms the sentence is about or for peripheral
concepts; therefore, for these, the baseline algorithm assigns the sentence subject and
object, respectively, using only tuples extracted by the chunker.
The measures that we used for evaluating each piece of information extracted from
a sentence fragment were precision and recall. The results to be evaluated have four com-
ponents for ATTITUDE-STYLE DISTINCTIONS and five components for DENOTATIONAL
DISTINCTIONS. There could be missing components (except strength and frequency,
which take default values). Precision is the total number of correct components found
(summed over all the sentences in the test set) divided by the total number of com-
ponents found. Recall is the total number of correct components found divided by the
number of components in the standard solution.
For example, for the sentence Sometimes, however, profit can refer to gains outside the
context of moneymaking, the program obtains ?profit, usually, medium, Denotation,
gains outside the context of moneymaking?, whereas the solution is ?profit, some-
times, medium, Denotation, gains outside the context of money-making?. The pre-
cision is .80 (four correct out of five found), and the recall is also .80 (four correct out of
five in the standard solution).
Table 2 presents the results of the evaluation.2 The first row of the table presents
the results as a whole (all the components of the extracted lexical knowledge base). Our
system increases precision by 36 percentage points and recall by 46 percentage points
over baseline on the development set.3 Recall and precision are both slightly higher still
on the test set; this shows that the patterns added during the development stage were
general.
The second row of the table gives the evaluation results for extracting only the
class of the distinction expressed, ignoring the strengths, frequencies, and peripheral
concepts. This allows for a more direct evaluation of the acquired extraction patterns.
The baseline algorithm attained higher precision than in the case when all the com-
ponents are considered because the default class Denotation is the most frequent in
CTRW. Our algorithm attained slightly higher precision and recall on the development
set than it did in the complete evaluation, probably due to a few cases in which the
frequency and strength were incorrectly extracted, and slightly lower on the test set,
probably due to some cases in which the frequency and strength were easy to extract
correctly.
2.5 Conclusion
The result of this stage is a generic lexical knowledge base of near-synonym differences.
In subsequent sections, it will be enriched with knowledge from other sources; informa-
tion about the collocational behavior of the near-synonyms is added in Section 3, and
2 The improvement over the baseline is statistically significant (at p = 0.005 level or better) for all the results
presented in this article, except in one place to be noted later. Statistical significance tests were done with
the paired t test, as described by Manning and Schu?tze (1999, pages 208?209).
3 These values are improved over those of earlier systems presented by Inkpen and Hirst (2001).
233
Computational Linguistics Volume 32, Number 2
Table 2
Precision and recall of the baseline and of our algorithm (for all the components and for the
distinction class only; boldface indicates best results).
Baseline algorithm Our system (dev. set) Our system (test set)
Precision Recall Precision Recall Precision Recall
All .40 .23 .76 .69 .83 .73
Class only .49 .28 .79 .70 .82 .71
more distinctions acquired from machine-readable dictionaries are added in Section 4.
To be used in a particular NLP system, the generic LKB of NS needs to be customized
(Section 6). Section 7 shows how the customized LKB of NS can actually be used in
Natural Language Generation (NLG).
The method for acquiring extraction patterns can be applied to other dictionaries
of synonym differences. The extraction patterns that we used to build our LKB of NS
are general enough to work on other dictionaries of English synonyms. To verify this,
we applied the extraction programs presented in Section 2.3, without modification, to
the usage notes in the Merriam-Webster Online Dictionary.4 The distinctions expressed in
these usage notes are similar to the explanations from CTRW, but the text of these notes
is shorter and simpler. In a sample of 100 usage notes, we achieved a precision of 90%
and a recall of 82%.
3. Adding Collocational Knowledge from Free Text
In this section, the lexical knowledge base of near-synonym differences will be enriched
with knowledge about the collocational behavior of the near-synonyms. We take col-
locations here to be pairs of words that appear together, consecutively or separated by
only a few non-content words, much more often than by chance. Our definition is purely
statistical, and we make no claim that the collocations that we find have any element of
idiomaticity; rather, we are simply determining the preferences of our near-synonyms
for combining, or avoiding combining, with other words. For example daunting task is a
preferred combination (a collocation, in our terms), whereas daunting job is less preferred
(it should not be used in lexical choice unless there is no better alternative), and daunting
duty is an anti-collocation (Pearce 2001) that sounds quite odd and must not be used in
lexical choice.
There are three main steps in acquiring this knowledge, which are shown in Fig-
ure 7. The first two look in free text?first the British National Corpus, then the World
Wide Web?for collocates of all near-synonyms in CTRW, removing any closed-class
words (function words). For example, the phrase defeat the enemy will be treated as defeat
enemy; we will refer to such pairs as bigrams, even if there were intervening words. The
third step uses the t-test (Church et al 1991) to classify less frequent or unobserved
bigrams as less preferred collocations or anti-collocations. We outline the three steps
below; a more-detailed discussion is presented by Inkpen and Hirst (2002).
4 http://www.m-w.com/cgi-bin/dictionary
234
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 7
The three steps in acquiring collocational knowledge for near-synonyms.
3.1 Extracting Collocations from the British National Corpus
In step 1 of our procedure, our data was the 100-million-word part-of-speech-tagged
British National Corpus (BNC).5 Only 2.61% of the near-synonyms are absent from the
BNC; and only 2.63% occur between one and five times. We first preprocessed the BNC
by removing all words tagged as closed-class and all the proper names, and then used
the Ngram Statistics Package6 (Pedersen and Banerjee 2003), which counts bigram (or
n-gram) frequencies in a corpus and computes various statistics to measure the degree
of association between two words: pointwise mutual information (MI), Dice, chi-square
(?2), log-likelihood (LL), and Fisher?s exact test. (See Manning and Schu?tze [1999] for a
review of statistical methods that can be used to identify collocations.)
Because these five measures rank potential collocations in different ways and have
different advantages and drawbacks, we decided to combine them by choosing as a
collocation any bigram that was ranked by at least two of the measures as one of
that measure?s T most-highly ranked bigrams; the threshold T may differ for each
measure. Lower values for T increase the precision (reduce the chance of accepting
noncollocations) but may not get many collocations for some of the near-synonyms;
higher values increase the recall at the expense of lower precision. Because there is
no principled way of choosing these values, we opted for higher recall, with step 2
of the process (Section 3.2) filtering out many noncollocations in order to increase
the precision. We took the first 200,000 bigrams selected by each measure, except for
Fisher?s measure for which we took all 435,000 that were ranked equal top. From these
lists, we retained only those bigrams in which one of the words is a near-synonym
in CTRW.7
5 http://www.hcu.ox.ac.uk/BNC/
6 http://www.d.umn.edu/?tpederse/nsp.html. We used version 0.4, known at the time as the Bigram
Statistics Package (BSP).
7 Collocations of a near-synonym with the wrong part of speech were not considered (the collocations are
tagged), but when a near-synonym has more than one major sense, collocations for senses other than the
one required in the cluster could be retrieved. For example, for the cluster job, task, duty, and so on, the
collocation import duty is likely to be for a different sense of duty (the tariff sense). Therefore
disambiguation is required (assuming one sense per collocation). We experimented with a simple
Lesk-style method (Lesk 1986). For each collocation, instances from the corpus were retrieved, and the
content words surrounding the collocations were collected. This set of words was then intersected with
the entry for the near-synonym in CTRW. A non-empty intersection suggests that the collocation and the
entry use the near-synonym in the same sense. If the intersection was empty, the collocation was not
retained. However, we ran the disambiguation algorithm only for a subset of CTRW, and then abandoned
it, because hand-annotated data are needed to evaluate how well it works and because it was very
time-consuming (due to the need to retrieve corpus instances for each collocation). Moreover, skipping
disambiguation is relatively harmless because including these wrong senses in the final lexical
knowledge base of near-synonym collocations will not hurt. For example, if the collocation import duty is
associated with the cluster job, duty, etc., it simply will not be used in practice because the concepts of
import and this sense of duty are unlikely to occur together in coherent interlingual input.
235
Computational Linguistics Volume 32, Number 2
3.2 Filtering with Mutual Information from Web Data
In the previous step we emphasized recall at the expense of precision: Because of
the relatively small size of the BNC, it is possible that the classification of a bigram
as a collocation in the BNC was due to chance. However, the World Wide Web (the
portion indexed by search engines) is big enough that a result is more reliable. So we
can use frequency on the Web to filter out the more dubious collocations found in the
previous step.8 We did this for each putative collocation by counting its occurrence on
the Web, the occurrence of each component word, and computing the pointwise mutual
information (PMI) between the words. Only those whose pointwise mutual information
exceeded a threshold Tpmi were retained.
More specifically, if w is a word that collocates with one of the near-synonyms x in
a cluster, a proxy PMIprox for the pointwise mutual information between the words can
be given by the ratio
P(w, x)
P(x)
=
nwx
nx
where nwx and nx are the number of occurrences of wx and x, respectively. The for-
mula does not include P(w) because it is the same for various x. We used an inter-
face to the AltaVista search engine to do the counts, using the number of hits (i.e.,
matching documents) as a proxy for the actual number of bigrams.9 The threshold
Tpmi for PMIprox was determined empirically by finding the value that optimized re-
sults on a standard solution, constructed as follows. We selected three clusters from
CTRW, with a total of 24 near-synonyms. For these, we obtained 916 candidate collo-
cations from the BNC. Two human judges (computational linguistics students, native
speakers of English) were asked to mark the true collocations (what they considered
to be good usage in English). The candidate pairs were presented to the judges in
random order, and each was presented twice.10 A bigram was considered to be a
true collocation only if both judges considered it so. We used this standard solution
to choose the value of Tpmi that maximizes the accuracy of the filtering program.
Accuracy on the test set was 68.3% (compared to approximately 50% for random
choice).
3.3 Finding Less Preferred Collocations and Anti-Collocations
In seeking knowledge of less preferred collocations and anti-collocations, we are look-
ing for bigrams that occur infrequently or not at all. The low frequency or absence of a
8 Why not just use the Web and skip the BNC completely? Because we would then have to count Web
occurrences of every near-synonym in CTRW combined with every content word in English. Using the
BNC as a first-pass filter vastly reduces the search space.
9 The collocations were initially acquired from the BNC with the right part of speech for the near-synonym
because the BNC is part-of-speech-tagged, but on the Web there are no part-of-speech tags; therefore a
few inappropriate instances may be included in the counts.
10 One judge was consistent (judged a collocation in the same way both times it appeared) in 90.4% of the
cases and the other in 88% of the cases. The agreement between the two judges was 78% (computed in a
strict way; i.e., we considered agreement only when the two judges had the same opinion including the
cases when they were not consistent), yielding ? = .51 with a 95% confidence interval of 0.47 to 0.55. (The
notion of confidence intervals for ? is defined, e.g., by Sim and Wright [2005]. The computations were
done with the PEPI statistical package [http://sagebrushpress.com/pepibook.html].) These figures show
that the task is not easy for humans.
236
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
bigram in the BNC may be due to chance. However, the World Wide Web is big enough
that a negative result is more reliable. So we can again use frequency on the Web?this
time to determine whether a bigram that was infrequent or unseen in the BNC is truly
a less preferred collocation or anti-collocation.
The bigrams of interest now are those in which collocates for a near-synonym that
were found in step 1 and filtered in step 2 are combined with another member of the
same near-synonym cluster. For example, if the collocation daunting task was found,
we now look on the Web for the apparent noncollocations daunting job, daunting duty,
and other combinations of daunting with near-synonyms of task. A low number of
co-occurrences indicates a less preferred collocation or anti-collocation. We employ the
t-test, following Manning and Schu?tze (1999, pages 166?168), to look for differences.
The collocations of each near-synonym with a given collocate are grouped in three
classes, depending on the t values of pairwise collocations. A t value comparing each
collocation and the collocation with maximum frequency is computed, and so is the
t value between each collocation and the collocation with minimum frequency. Table 3
presents an example.
After the t-test scores were computed, a set of thresholds was determined to classify
the collocations in the three groups: preferred collocations, less preferred collocations,
and anti-collocations. Again, we used a standard solution in the procedure. Two judges
manually classified a sample of 2,838 potential collocations obtained for the same three
clusters of near-synonyms from 401 collocations that remained after filtering. They
were instructed to mark as preferred collocations all the potential collocations that
they considered good idiomatic use of language, as anti-collocations the ones that they
would not normally use, and as less preferred collocations the ones that they were not
comfortable classifying in either of the other two classes. When the judges agreed, the
class was clear. When they did not agree, we used simple rules, such as these: When
one judge chose the class-preferred collocation, and the other chose the class anti-
collocation, the class in the solution was less preferred collocation (because such cases
seemed to be difficult and controversial); when one chose preferred collocation, and the
other chose less preferred collocation, the class in the solution was preferred collocation;
when one chose anti-collocation, and the other chose less preferred collocation, the class
in the solution was anti-collocation. The agreement between judges was 84%, ? = 0.66
(with a 95% confidence interval of 0.63 to 0.68).
Table 3
Example of counts, mutual information scores, and t-test scores for the collocate daunting with
near-synonyms of task. The second column shows the number of hits for the collocation daunting
x, where x is the near-synonym in the first column. The third column shows PMIprox (scaled by
105 for readability), the fourth column, the t values between the collocation with maximum
frequency (daunting task) and daunting x, and the last column, the t-test between daunting x and
the collocations with minimum frequency (daunting stint and daunting hitch).
x Hits PMIprox t max t min
task 63,573 0.011662 ? 252.07
job 485 0.000022 249.19 22.02
assignment 297 0.000120 250.30 17.23
chore 96 0.151899 251.50 9.80
duty 23 0.000022 251.93 4.80
stint 0 0 252.07 ?
hitch 0 0 252.07 ?
237
Computational Linguistics Volume 32, Number 2
Table 4
Example of results of our program for collocations of near-synonyms in the task cluster.
?
marks
preferred collocations, ? marks less preferred collocations, and ? marks anti-collocations. The
combined opinion of the judges about the same pairs of words is shown in parentheses.
Collocates
Near-synonyms daunting particular tough
task
?
(
?
)
?
(
?
)
?
(
?
)
job ? (
?
)
?
(
?
)
?
(
?
)
assignment ? (?) ? (?) ? (?)
chore ? ( ?) ? (?) ? ( ?)
duty ? ( ?) ? (?) ? ( ?)
stint ? ( ?) ? ( ?) ? ( ?)
hitch ? ( ?) ? ( ?) ? ( ?)
We used this standard solution as training data to C4.511 to learn a decision tree
for the three-way classifier. The features in the decision tree are the t-test between
each collocation and the collocation from the same group that has maximum frequency
on the Web, and the t-test between the current collocation and the collocation that
has minimum frequency (as presented in Table 3). We did 10-fold cross-validation to
estimate the accuracy on unseen data. The average accuracy was 84.1%, with a standard
error of 0.5%; the baseline of always choosing the most frequent class, anti-collocations,
yields 71.4%. We also experimented with including PMIprox as a feature in the decision
tree, and with manually choosing thresholds (without a decision tree) for the three-
way classification, but the results were poorer. The three-way classifier can fix some
of the mistakes of the PMI filter: If a wrong collocation remains after the PMI filter,
the classifier can classify it in the anti-collocations class. We conclude that the acquired
collocational knowledge has acceptable quality.
3.4 Results
We obtained 1,350,398 distinct bigrams that occurred at least four times. We selected col-
locations for all 909 clusters in CTRW (5,452 near-synonyms in total). Table 4 presents an
example of results for collocational classification of bigrams, where
?
marks preferred
collocations, ? marks less preferred collocations, and ? marks anti-collocations. This
gave us a lexical knowledge base of near-synonym collocational behavior. An example
of collocations extracted for the near-synonym task is presented in Table 5, where the
columns are, in order, the name of the measure, the rank given by the measure, and the
value of the measure.
4. Adding Knowledge from Machine-Readable Dictionaries
Information about near-synonym differences can be found in other types of dictionaries
besides those explicitly on near-synonyms. Although conventional dictionaries, unlike
CTRW, treat each word in isolation, they may nonetheless contain useful information
11 http://www.cse.unsw.edu.au/?quinlan
238
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Table 5
Example of collocations extracted for the near-synonym task. The first collocation was selected
(ranked in the set of first T collocations) by four measures; the second collocation was selected
by two measures.
Collocation Measure Rank Score
daunting/A task/N MI 24,887 10.85
LL 5,998 907.96
?2 16,341 122,196.82
Dice 2,766 0.02
repetitive/A task/N MI 64,110 6.77
?2 330,563 430.40
about near-synonyms because some definitions express a distinction relative to another
near-synonym. From the SGML-marked-up text of the Macquarie Dictionary12 (Delbridge
et al 1987), we extracted the definitions of the near-synonyms in CTRW for the expected
part of speech that contained another near-synonym from the same cluster. For example,
for the CTRW cluster burlesque, caricature, mimicry, parody, takeoff, travesty, one definition
extracted for the near-synonym burlesque was any ludicrous take-off or debasing caricature
because it contains caricature from the same cluster. A series of patterns was used
to extract the difference between the two near-synonyms wherever possible. For the
burlesque example, the extracted information was
?burlesque, usually, medium, Denotation, ludicrous?,
?burlesque, usually, medium, Denotation, debasing?.
The number of new denotational distinctions acquired by this method was 5,731.
We also obtained additional information from the General Inquirer13 (Stone et al
1966), a computational lexicon that classifies each word in it according to an extendable
number of categories, such as pleasure, pain, virtue, and vice; overstatement and un-
derstatement; and places and locations. The category of interest here is Positiv/Negativ.
There are 1,915 words marked as Positiv (not including words for yes, which is a separate
category of 20 entries), and 2,291 words marked as Negativ (not including the separate
category of no in the sense of refusal). For each near-synonym in CTRW, we used
this information to add a favorable or unfavorable attitudinal distinction accordingly.
If there was more than one entry (several senses) for the same word, the attitude
was asserted only if the majority of the senses had the same marker. The number
of attitudinal distinctions acquired by this method was 5,358. (An attempt to use the
occasional markers for formality in WordNet in a similar manner resulted in only 11
new distinctions.)
As the knowledge from each source is merged with the LKB, it must be checked for
consistency in order to detect conflicts and resolve them. The algorithm for resolving
conflicts is a voting scheme based on the intuition that neutral votes should have less
weight than votes for the two extremes. The algorithm outputs a list of the conflicts
and a proposed solution. This list can be easily inspected by a human, who can change
12 http://www.macquariedictionary.com.au/
13 http://www.wjh.harvard.edu/?inquirer/
239
Computational Linguistics Volume 32, Number 2
Figure 8
Fragment of the representation of the error cluster (prior to customization).
the solution of the conflict in the final LKB of NS, if desired. The consistency-checking
program found 302 conflicts for the merged LKB of 23,469 distinctions. After conflict
resolution, 22,932 distinctions remained. Figure 8 shows a fragment of the knowledge
extracted for the near-synonyms of error after merging and conflict resolution.
5. Related Work
5.1 Building Lexical Resources
Lexical resources for natural language processing have also been derived from other
dictionaries and knowledge sources. The ACQUILEX14 Project explored the utility of
constructing a multilingual lexical knowledge base (LKB) from machine-readable ver-
sions of conventional dictionaries. Ide and Ve?ronis (1994) argue that it is not possible to
build a lexical knowledge base from a machine-readable dictionary (MRD) because the
information it contains may be incomplete, or it may contain circularities. It is possible
to combine information from multiple MRDs or to enhance an existing LKB, they say,
although human supervision may be needed.
Automatically extracting world knowledge from MRDs was attempted by projects
such as MindNet at Microsoft Research (Richardson, Dolan, and Vanderwende 1998),
and Barrie`rre and Popowich?s (1996) project, which learns from children?s dictionaries.
IS-A hierarchies have been learned automatically from MRDs (Hearst 1992) and from
corpora (Caraballo [1999] among others).
14 http://www.cl.cam.ac.uk/Research/NL/acquilex/acqhome.html
240
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Research on merging information from various lexical resources is related to the
present work in the sense that the consistency issues to be resolved are similar. One
example is the construction of Unified Medical Language System (UMLS)15 (Lindberg,
Humphreys, and McCray 1993), in the medical domain. UMLS takes a wide range of
lexical and ontological resources and brings them together as a single resource. Most of
this work is done manually at the moment.
5.2 Acquiring Collocational Knowledge
There has been much research on extracting collocations for different applications. Like
Church et al (1991), we use the t-test and mutual information (MI), but unlike them
we use the Web as a corpus for this task (and a modified form of mutual information),
and we distinguish three types of collocations. Pearce (2001) improved the quality of
retrieved collocations by using synonyms from WordNet (Pearce 2001); a pair of words
was considered a collocation if one of the words significantly prefers only one (or
several) of the synonyms of the other word. For example, emotional baggage is a good
collocation because baggage and luggage are in the same synset and ?emotional luggage
is not a collocation. Unlike Pearce, we use a combination of t-test and MI, not just
frequency counts, to classify collocations.
There are two typical approaches to collocations in previous NLG systems: the
use of phrasal templates in the form of canned phrases, and the use of automatically
extracted collocations for unification-based generation (McKeown and Radev 2000).
Statistical NLG systems (such as Nitrogen [Langkilde and Knight 1998]) make good
use of the most frequent words and their collocations, but such systems cannot choose
a less-frequent synonym that may be more appropriate for conveying desired nuances
of meaning if the synonym is not a frequent word.
Turney (2001) used mutual information to choose the best answer to questions about
near-synonyms in the Test of English as a Foreign Language (TOEFL) and English as
a Second Language (ESL). Given a problem word (with or without context) and four
alternative words, the question is to choose the alternative most similar in meaning
to the problem word (the problem here is to detect similarities, whereas in our work
differences are detected). His work is based on the assumption that two synonyms are
likely to occur in the same document (on the Web). This can be true if the author needs
to avoid repeating the same word, but not true when the synonym is of secondary
importance in a text. The alternative that has the highest pointwise mutual information
for information retrieval (PMI-IR) with the problem word is selected as the answer. We
used the same measure in Section 3.3?the mutual information between a collocation
and a collocate that has the potential to discriminate between near-synonyms. Both
works use the Web as a corpus, and a search engine to estimate the mutual information
scores.
5.3 Near-Synonyms
As noted in the introduction, our work is based on that of Edmonds and Hirst (2002) and
Hirst (1995), in particular the model for representing the meaning of the near-synonyms
presented in Section 1.2 and the preference satisfaction mechanism used in Section 7.
15 http://www.nml.nih.gov/research/umls/
241
Computational Linguistics Volume 32, Number 2
Other related research involving differences between near-synonyms has a lin-
guistic or lexicographic, rather than computational, flavor. Apresjan built a bilin-
gual dictionary of synonyms, more specifically a dictionary of English synonyms
explained in Russian (Apresjan et al 1980). It contains 400 entries selected from
the approximately 2,500 entries from Webster?s New Dictionary of Synonyms, but re-
organized by splitting or merging clusters of synonyms, guided by lexicographic
principles described by Apresjan (2000). An entry includes the following types of
differences: semantic, evaluative, associative and connotational, and differences in
emphasis or logical stress. These differences are similar to the ones used in our
work.
Gao (2001) studied the distinctions between near-synonym verbs, more specifically
Chinese physical action verbs such as verbs of cutting, putting, throwing, touching,
and lying. Her dissertation presents an analysis of the types of semantic distinctions
relevant to these verbs, and how they can be arranged into hierarchies on the basis of
their semantic closeness.
Ploux and Ji (2003) investigated the question of which words should be considered
near-synonyms, without interest in their nuances of meaning. They merged clusters of
near-synonyms from several dictionaries in English and French and represented them
in a geometric space. In our work, the words that are considered near-synonyms are
taken from CTRW; a different dictionary of synonyms may present slightly different
views. For example, a cluster may contain some extra words, some missing words, or
sometimes the clustering could be done in a different way. A different approach is to
automatically acquire near-synonyms from free text. Lin et al (2003) acquire words that
are related by contextual similarity and then filter out the antonyms by using a small
set of manually determined patterns (such as ?either X or Y?) to construct Web queries
for pairs of candidate words. The problem of this approach is that it still includes words
that are in relations other than near-synonymy.
6. Customizing the Lexical Knowledge Base of Near-Synonym Differences
The initial LKB of NS built in Sections 2 to 4 is a general one, and it could, in principle, be
used in any (English) NLP system. For example, it could be used in the lexical-analysis
or lexical-choice phase of machine translation. Figure 9 shows that during the analysis
phase, a lexical knowledge base of near-synonym differences in the source language
is used, together with the context, to determine the set of nuances that are expressed
in the source-language text (in the figure, the source language is French and the target
language is English). In the generation phase, these nuances become preferences for the
lexical-choice process. Not only must the target-language text express the same meaning
as the source-language text (necessary condition), but the choice of words should satisfy
the preferences as much as possible.
In order to be integrated with the other components of the NLP system, the LKB will
probably need some adaptation?in particular, the core denotations and the peripheral
concepts will need to be mapped to the ontology that the system employs. This might
be a general-purpose ontology, such as Cyc (Lenat 1995) and WordNet, or an ontology
built specially for the system (such as Mikrokosmos (Mahesh and Nirenburg 1995) or
domain-specific ontologies). In this section, we focus on the generation phase of an in-
terlingual machine translation system, specifically the lexical-choice process, and show
how the LKB was adapted for Xenon, a natural language generation system. Xenon
is a general-purpose NLG system that exploits our LKB of NS. To implement Xenon,
242
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 9
Lexical analysis and choice in machine translation; adapted from Edmonds and Hirst (2002). The
solid lines show the flow of data: input, intermediate representations, and output; the dashed
lines show the flow of knowledge from the knowledge sources to the analysis and the generation
module. The rectangles denote the main processing modules; the rest of the boxes denote data or
knowledge sources.
we modified the lexical-choice component of a preexisting NLG system, HALogen
(Langkilde 2000; Langkilde and Knight 1998), to handle knowledge about the near-
synonym differences. (Xenon will be described in detail in Section 7.) This required
customization of the LKB to the Sensus ontology (Knight and Luk 1994) that HALogen
uses as its representation.
Customization of the core denotations for Xenon was straightforward. The core
denotation of a cluster is a metaconcept representing the disjunction of all the Sensus
concepts that could correspond to the near-synonyms in a cluster. The names of meta-
concepts, which must be distinct, are formed by the prefix generic, followed by the
name of the first near-synonym in the cluster and the part of speech. For example, if the
cluster is lie, falsehood, fib, prevarication, rationalization, untruth, the name of the cluster is
generic lie n.
Customizing the peripheral concepts, which are initially expressed as strings, could
include parsing the strings and mapping the resulting syntactic representation into a
semantic representation. For Xenon, however, we implemented a set of 22 simple rules
that extract the actual peripheral concepts from the initial peripheral strings. A trans-
formation rule takes a string of words part-of-speech tagged and extracts a main word,
several roles, and fillers for the roles. The fillers can be words or recursive structures. In
Xenon, the words used in these representations are not sense-disambiguated. Here are
two examples of input strings and extracted peripheral concepts:
"an embarrassing breach of etiquette"
=> (C / breach :GPI etiquette :MOD embarrassing)
"to an embarrassing or awkward occurrence"
=> (C / occurrence :MOD (OR embarrassing awkward))
The roles used in these examples are MOD (modifier) and GPI (generalized possession
inverse). The rules that were used for these two examples are these:
Adj Noun1 of Noun2 => (C / Noun1 :GPI Noun2 :MOD Adj)
Adj1 or Adj2 Noun => (C / Noun :MOD (OR Adj1 Adj2))
243
Computational Linguistics Volume 32, Number 2
We evaluated our customization of the LKB on a hand-built standard solution
for a set of peripheral strings: 139 strings to be used as a test set and 97 strings to
be used as a development set. The rules achieved a coverage of 75% on the test set
with an accuracy of 55%.16 In contrast, a baseline algorithm of taking the first word
in each string as the peripheral concept covers 100% of the strings, but with only 16%
accuracy.
Figure 10 shows the full customized representation for the near-synonyms of error,
derived from the initial representation that was shown earlier in Figure 8. (See Inkpen
[2003] for more examples of customized clusters.) The peripheral concepts are factored
out, and the list of distinctions contains pointers to them. This allows peripheral con-
cepts to be shared by two or more near-synonyms.
7. Xenon: An NLG System that Uses Knowledge of Near-Synonym Differences
This section presents Xenon, a large-scale NLG system that uses the lexical knowledge-
base of near-synonyms customized in Section 6. Xenon integrates a new near-synonym
choice module with the sentence realization system HALogen17 (Langkilde 2000;
Langkilde and Knight 1998). HALogen is a broad-coverage general-purpose natural
language sentence generation system that combines symbolic rules with linguistic
information gathered statistically from large text corpora. The internal architecture
of HALogen is presented in Figure 11. A forest of all possible sentences (combi-
nations of words) for the input is built, and the sentences are then ranked ac-
cording to an n-gram language model in order to choose the most likely one as
output.
Figure 12 presents the architecture of Xenon. The input is a semantic representation
and a set of preferences to be satisfied. The final output is a set of sentences and
their scores. A concrete example of input and output is shown in Figure 13. Note
that HALogen may generate some ungrammatical constructs, but they are (usually)
assigned lower scores. The first sentence (the highest ranked) is considered to be the
solution.
7.1 Metaconcepts
The semantic representation input to Xenon is represented, like the input to HAL-
ogen, in an interlingua developed at University of Southern California/Information
Sciences Institute (USC/ISI).18 As described by Langkilde-Geary (2002b), this lan-
guage contains a specified set of 40 roles, whose fillers can be either words, con-
cepts from Sensus (Knight and Luk 1994), or complex interlingual representations.
The interlingual representations may be underspecified: If some information needed
by HALogen is not present, it will use its corpus-derived statistical information to
16 We found that sometimes a rule would extract only a fragment of the expected configuration of concepts
but still provided useful knowledge; however, such cases were not considered to be correct in this
evaluation, which did not allow credit for partial correctness. For example, if the near-synonym command
denotes the/TD stated/VB demand/NN of/IN a/TD superior/JJ, the expected peripheral concept is
(C1 / demand :GPI superior :MOD stated). If the program extracted only (C1 / demand :GPI
superior), the result was not considered correct, but the information might still help in an NLP system.
17 http://www.isi.edu/licensed-sw/halogen/
18 http://www.isi.edu/licensed-sw/halogen/interlingua.html
244
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 10
The final representation of the error cluster.
make choices. Xenon extends this representation language by adding metaconcepts
that correspond to the core denotation of the clusters of near-synonyms. For example,
in Figure 13, the metaconcept is generic lie n. As explained in Section 6, metacon-
cepts may be seen as a disjunction of all the senses of the near-synonyms of the
cluster.
7.2 Near-Synonym Choice
The near-synonym choice module has to choose the most appropriate near-synonym
from the cluster specified in the input. It computes a satisfaction score that becomes
a weight (to be explained in section 7.3) for each near-synonym in the cluster. HAL-
ogen makes the final choice by adding these weights to n-gram probabilities from its
language model (more precisely, the negative logarithms of these values) and choosing
245
Computational Linguistics Volume 32, Number 2
Figure 11
The architecture of the sentence realizer HALogen.
the highest-ranked sentence. For example, the expanded representation of the input
in Figure 13 is presented in Figure 14. The near-synonym choice module gives higher
weight to fib because it satisfies the preferences better than the other near-synonyms in
the cluster, lie, falsehood, fib, prevarication, rationalization, and untruth.
7.3 Preferences and Similarity of Distinctions
The preferences that are input to Xenon could be given by the user, or they could come
from an analysis module if Xenon is used in a machine translation system (correspond-
ing to nuances of near-synonyms in a different language, see Figure 9). The preferences,
like the distinctions expressed in the LKB of NS, are of three types: attitudinal, stylistic,
and denotational. Examples of each:
(low formality)
(disfavour :agent)
(imply (C / assessment :MOD ( M / (*OR* ignorant uninformed)).
The formalism for expressing preferences is from I-Saurus (Edmonds 1999). The
preferences are transformed internally into pseudodistinctions that have the same form
as the corresponding type of distinctions so that they can be directly compared with the
distinctions. The pseudodistinctions corresponding to the previous examples are these:
(-? low Formality)
(- always high Pejorative :agent)
(- always medium Implication
(C/assessment :MOD (M/(OR ignorant uninformed)).
Figure 12
The architecture of the natural language generation system Xenon.
246
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 13
Example of input and output of Xenon.
Figure 14
The interlingual representation of the input in Figure 13 after expansion by the near-synonym
choice module.
For each near-synonym NS in a cluster, a weight is computed by summing the
degree to which the near-synonym satisfies each preference from the set P of input
preferences:
Weight(NS, P) =
?
p?P
Sat(p, NS). (2)
The weights are transformed through an exponential function so that numbers are
comparable with the differences of probabilities from HALogen?s language model:
f (x) = e
xk
e ? 1 . (3)
We set k = 15 as a result of experiments with a development set.
For a given preference p ? P, the degree to which it is satisfied by NS is reduced to
computing the similarity between each of NS?s distinctions and a pseudodistinction d(p)
generated from p. The maximum value over i is taken (where di(w) is the ith distinction
of NS):
Sat(p, NS) = max
i
Sim(d(p), di(NS)). (4)
247
Computational Linguistics Volume 32, Number 2
where the similarity of two distinctions, or of a distinction and a preference (trans-
formed into a distinction), is computed with the three types of similarity measures that
were used by Edmonds and Hirst (2002) in I-Saurus:
Sim(d1, d2) =
?
?
?
?
?
?
?
Simden(d1, d2) if d1 and d2 are denotational distinctions
Simatt(d1, d2) if d1 and d2 are attitudinal distinctions
Simsty(d1, d2) if d1 and d2 are stylistic distinctions
0 otherwise
(5)
Distinctions are formed out of several components, represented as symbolic values
on certain dimensions, such as frequency (seldom, sometimes, etc.) and strength (low,
medium, high). In order to compute a numeric score, each symbolic value is mapped
into a numeric one. The numeric values are not as important as their relative difference.
If the two distinctions are not of the same type, they are incommensurate and their
similarity is zero. The formulas for Simatt and Simsty involve relatively straightforward
matching. However, Simden requires the matching of complex interlingual structures.
This boils down to computing the similarity between the main concepts of the two
interlingual representations, and then recursively mapping the shared semantic roles
(and compensating for the roles that appear in only one). When atomic concepts or
words are reached, we use a simple measure of word/concept similarity based on
the hierarchy of Sensus. All the details of these formulas, along with examples, are
presented by Inkpen and Hirst (2003).
7.4 Integrating the Knowledge of Collocational Behavior
Knowledge of collocational behavior is not usually present in NLG systems. Adding it
will increase the quality of the generated text, making it more idiomatic: The system
will give priority to a near-synonym that produces a preferred collocation and will not
choose one that causes an anti-collocation to appear in the generated sentence.
Unlike most other NLG systems, HALogen already incorporates some collocational
knowledge implicitly encoded in its language model (bigrams or trigrams), but this is
mainly knowledge of collocations between content words and function words. There-
fore, in its integration into Xenon, the collocational knowledge acquired in Section 3
will be useful, as it includes collocations between near-synonyms and other nearby
content words. Also, it is important whether the near-synonym occurs before or after
the collocate; if both positions are possible, both collocations are in the knowledge
base.
The architecture of Xenon extended with the near-synonym collocation module
is presented in Figure 15. The near-synonym collocation module intercepts the forest
structure, modifies its weights as necessary, and then forwards it to the statistical
ranking module. If a potential anti-collocation is seen in the forest structure, the weight
of the near-synonym is discounted by Wanti colloc; if a less preferred collocation is seen,
the weight of the near-synonym is discounted by Wless pref colloc. For preferred collo-
cations, the weight is unchanged. If the collocate is not the only alternative, the other
alternatives should be discounted, unless they also form a preferred collocation. Sec-
tion 7.5.2 explains how the discount weights were chosen.
248
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 15
The architecture of Xenon extended with the near-synonym collocation module. In this figure,
the knowledge sources are not shown.
7.5 Evaluation of Xenon
The components of Xenon to be evaluated here are the near-synonym choice module
and the near-synonym collocation module. We evaluate each module in interaction with
the sentence-realization module HALogen,19 first individually and then both working
together.20
An evaluation of HALogen itself was presented by Langkilde-Geary (2002a) using
a section of the Penn Treebank as test set. HALogen was able to produce output for
80% of a set of 2,400 inputs (automatically derived from the test sentences by an input
construction tool). The output was 94% correct when the input representation was
fully specified, and between 94% and 55% for various other experimental settings. The
accuracy was measured using the BLEU score (Papineni et al 2001) and the string
edit distance by comparing the generated sentences with the original sentences. This
evaluation method can be considered as English-to-English translation via meaning
representation.
7.5.1 Evaluation of the Near-Synonym Choice Module. For the evaluation of the near-
synonym choice module, we conducted two experiments. (The collocation module was
disabled for these experiments.) Experiment 1 involved simple monolingual generation.
Xenon was given a suite of inputs: Each was an interlingual representation of a sentence
and the set of nuances that correspond to a near-synonym in the sentence (see Figure 16).
The sentence generated by Xenon was considered correct if the expected near-synonym,
whose nuances were used as input preferences, is chosen. The sentences used in this
first experiment were very simple; therefore, the interlingual representations were easily
built by hand. In the interlingual representation, the near-synonym was replaced with
the corresponding metaconcept. There was only one near-synonym in each sentence.
Two data sets were used in Experiment 1: a development set of 32 near-synonyms of
the five clusters presented in Figure 17 in order to set the exponent k of the scaling
function in equation (3), and a test set of 43 near-synonyms selected from six clusters,
namely, the set of English near-synonyms shown in Figure 18.
19 All the evaluation experiments presented in this section used HALogen?s trigram language model. The
experiments were repeated with the bigram model, and the results were almost identical.
20 Preliminary evaluation experiments of only the near-synonym choice module were presented by Inkpen
and Hirst (2003).
249
Computational Linguistics Volume 32, Number 2
Figure 16
The architecture of Experiment 1.
Figure 17
Development data set used in Experiment 1.
Some of Xenon?s choices could be correct solely because the expected near-synonym
happens to be the default one (the one with the highest probability in the language
model). So as a baseline (the performance that can be achieved without using the LKB
of NS), we ran Xenon on all the test cases, but without input preferences.
The results of Experiment 1 are presented in Table 6. For each data set, the second
column shows the number of test cases. The column labeled ?Total correct? shows the
number of answers considered correct (when the expected near-synonym was chosen).
The column labeled ?Ties? shows the number of cases when the expected near-synonym
had weight 1.0, but there were other near-synonyms that also had weight 1.0 because
they happen to have identical nuances in the LKB of NS. The same column shows in
parentheses how many of these ties caused an incorrect near-synonym choice. In such
cases, Xenon cannot be expected to make the correct choice, or, more precisely, the other
choices are equally correct, at least as far as Xenon?s LKB is concerned. Therefore, the
Figure 18
Test data set used in Experiment 1 (English only) and Experiment 2 (English and French).
250
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Table 6
Results of Experiment 1 (boldface indicates best results).
No. Correct Base- Accuracy
of Total by line (no ties) Accuracy
Data set cases correct default Ties % % %
Development 32 27 5 5 (4) 15.6 84.3 96.4
Test 43 35 6 10 (5) 13.9 81.3 92.1
accuracies computed without considering these cases (the seventh column) are under-
estimates of the real accuracy of Xenon. The last column presents accuracies taking the
ties into account, defined as the number of correct answers divided by the difference
between the total number of cases and the number of incorrectly resolved ties.
Experiment 2 is based on machine translation. These experiments measure how
successful the translation of near-synonyms is, both from French into English and
from English into English. The experiments used pairs of French and English sen-
tences that are translations of one another (and that contain near-synonyms of interest),
extracted from sentence-aligned parallel text, the bilingual Canadian Hansard. Ex-
amples are shown in Figure 19.21 For each French sentence, Xenon should generate
an English sentence that contains an English near-synonym that best matches the nu-
ances of the French original. If Xenon chooses exactly the English near-synonym used
in the parallel text, then Xenon?s behavior is correct. This is a conservative evaluation
measure because there are cases in which more than one of the possibilities would be
acceptable.
As illustrated earlier in Figure 9, an analysis module is needed. For the evalua-
tion experiments, a simplified analysis module is sufficient. Because the French and
English sentences are translations of each other, we can assume that their interlingual
representation is essentially the same. So for the purpose of these experiments, we
can use the interlingual representation of the English sentence to approximate the
interlingual representation of the French sentence and simply add the nuances of the
French near-synonym to the representation. This is a simplification because there may
be some sentences for which the interlingual representation of the French sentence is
different because of translation divergences between languages (Dorr 1993). For the
sentences in our test data, a quick manual inspection shows that this happens very
rarely or not at all. This simplification eliminates the need to parse the French sen-
tence and the need to build a tool to extract its semantics. As depicted in Figure 20,
the interlingual representation is produced with a preexisting input construction tool
that was previously used by Langkilde-Geary (2002a) in her HALogen evaluation
experiments. In order to use this tool, we parsed the English sentences with Charniak?s
parser (Charniak 2000).22 The tool was designed to work on parse trees from the Penn
TreeBank, which have some extra annotations; it worked on parse trees produced
by Charniak?s parser, but it failed on some parse trees probably more often than it
21 The sentences were obtained from USC/ISI (http://www.isi.edu/natural-language/download/hansard/)
(approximately one million pairs of sentences). Other sources of parallel text, such as parallel translations
of the Bible (http://benjamin.umd.edu/parallel/) (Resnik 1999) and a collection of Web pages (Resnik,
Olsen, and Diab 1999), happened to contain very few occurrences of the near-synonyms of interest.
22 ftp://ftp.cs.brown.edu/pub/nlparser/
251
Computational Linguistics Volume 32, Number 2
Figure 19
Examples of parallel sentences used in Experiment 2.
Figure 20
The architecture of Experiment 2 (French to English).
did in HALogen?s evaluation experiments. We replaced each near-synonym with the
metaconcept that is the core meaning of its cluster. The interlingual representation for
the English sentence is semantically shallow; it does not reflect the meaning of the
French sentence perfectly, but in these experiments we are interested only in the near-
synonyms from the test set; therefore, the other words in the French sentence are not
important.
The analyzer of French nuances of Figure 20 needs to extract nuances from an
LKB of French synonyms. We created by hand an LKB for six clusters of French near-
synonyms (those from Figure 18) from two paper dictionaries of French synonyms,
Be?nac (1956) and Bailly (1973). For each peripheral string, in French, an equivalent
concept is found in Sensus by looking for English translations of the words and then
finding Sensus concepts for the appropriate senses of the English words. Figure 21
presents a fragment of a cluster of French near-synonyms. For example, if we are
told that erreur denotes fausse opinion, the equivalent peripheral concept is (P8 (c8 /
|view<belief| :mod |false>untrue|)). If we are told that gaffe denotes be?vue
grossiere, then the equivalent peripheral concept is (P7 (c7 / |glaring,gross|)).
A similar experiment, translating not from French into English but from English
into English, is useful for evaluation purposes. An English sentence containing a near-
synonym is processed to obtain its semantic representation (where the near-synonym is
replaced with a metaconcept), and the lexical nuances of the near-synonym are input as
preferences to Xenon. Ideally, the same near-synonym as in the original sentence would
be chosen by Xenon (we consider it to be the correct choice). The percentage of times
this happens is an evaluation measure. The architecture of this experiment is presented
in Figure 22.
It happens that not all the near-synonyms in the test data set were found in Han-
sard?in fact, only 13 distinct pairs occur as translations of each other. Some of these
pairs are very frequent, and some are rare. In order to evaluate the system for all these
near-synonyms, both with and without regard to their frequency, we prepared two
252
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 21
Fragment of a cluster of French near-synonyms.
Figure 22
The architecture of Experiment 2 (English to English).
data sets by sampling Hansard in two different ways. Sentence data set 1 contains,
for each French and English near-synonym pair in the test data set, two pairs of
sentences in which the English word appears as a translation of the French. The
sentences selected for each pair were the first two for which the input construction
tool produced a valid interlingual representation. Sentence data set 2 is similar to
set 1, but instead of having two sentences for a near-synonym pair, it contains all the
sentence pairs in a large fragment of Hansard in which the near-synonyms of interest
occurred. Therefore, this data set has the advantage of a natural frequency distribution.
It has the disadvantage that the results for the less-frequent near-synonyms, which
tend to be the ?harder? and more-interesting cases (see below), may be swamped by
the more-frequent, relatively ?easy? cases. Initially there were 564 pairs of sentences,
but the input construction tool worked successfully only on 298 English sentences. The
interlingual representations that it produced are quite complex, typically several pages
long.
The results of Experiment 2 are presented in Table 7;23 the interpretation of the
columns is the same as for Table 6. For each set of sentences, the baseline is the same
23 The improvement over baseline is statistically significant for all the results in Table 7, except the third line.
253
Computational Linguistics Volume 32, Number 2
Table 7
Results of Experiment 2 (boldface indicates best results).
Data set No. Correct Base- Accuracy
and of Total by line (no ties) Accuracy
condition cases correct default Ties % % %
Sentence set 1 26 13 10 5 (3) 38.4 50.0 56.5
French to English
Sentence set 1 26 26 10 2 (0) 38.4 100 100
English to English
Sentence set 2 298 217 214 7 (1) 71.8 72.8 73.0
French to English
Sentence set 2 298 296 214 2 (0) 71.8 99.3 99.3
English to English
for the French-to-English and English-to-English experiments because no nuances were
used as input for the baseline experiments. The baseline for data set 2 is quite high
(71.8%), because it contains sentences with frequent near-synonyms, which happen
to be the ones that Xenon chooses by default in the absence of input preferences.
Xenon?s performance is well above baseline, with the exception of the French-to-English
condition on sentence data set 2.
In the English-to-English experiments, there are two reasons to expect Xenon?s
accuracy to be less than 100% even if the input nuances are the nuances of a particular
English near-synonym. The first reason is that there are cases in which two or more
near-synonyms get an equal, maximal score because they do not have nuances that
differentiate them (either they are perfectly interchangeable, or the LKB of NS does not
contain enough knowledge) and the one chosen is not the expected one. The second
reason is that sometimes Xenon does not choose the expected near-synonym even if it is
the only one with maximal weight. This may happen because HALogen makes the final
choice by combining the weight received from the near-synonym choice module with
the probabilities from the language model that is part of HALogen. Frequent words may
have high probabilities in the language model. If the expected near-synonym is very
rare, or maybe was not seen at all by the language model, its probability is very low;
yet it is exactly those cases where a writer chooses a rare near-synonym over a more-
frequent alternative that the choice is the most telling. When combining the weights
with the probabilities, a frequent near-synonym may win even if it has a lower weight
assigned by the near-synonym choice module. In such cases, the default near-synonym
(the most frequent of the cluster) wins. Sometimes such behavior is justified because
there may be other constraints that influence HALogen?s choice.
In the French-to-English experiments, the performance of Xenon is lower than in the
English-to-English experiments. There are two explanations. First, there is some overlap
between the nuances of the French and the English near-synonyms, but less than one
would expect. For example, the English adjective alcoholic is close in nuances to the
French adjective alcoolique, but they have no nuance in common in Xenon?s knowledge
bases simply because of the incompleteness of the explanations given by lexicographers
in the dictionary entries.
The second explanation is related to what is considered the ?correct? choice of near-
synonym. Sometimes more than one translation of a French near-synonym could be
254
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
correct, but in this conservative evaluation, the solution is the near-synonym that was
used in the equivalent English sentence. Therefore, test cases that would be considered
correct by a human judge are harshly penalized. Moreover, the near-synonym choice
module always chooses the same translation for a near-synonym, even if the near-
synonym is translated in Hansard in more than one way, because Xenon does not
consider the context of the near-synonym in the sentence. (The context is taken into
account only when the collocation module is enabled and a preferred collocation is
detected in the sentences.) For example, the French noun erreur is translated in Hansard
sometimes as error, sometimes as mistake. Both have nuances in common with erreur,
but mistake happened to have higher overlap with erreur than error; as a result, the near-
synonym choice module always chooses mistake (except when the collocation module
is enabled and finds a preferred collocation such as administrative error). All the cases
in which error was used as translation of erreur in Hansard are penalized as incorrect
in the evaluation of the near-synonym choice module. A few of these cases could be
indeed incorrect, but probably many of them would be considered correct by a human
judge.
Another way to look at the performance of Xenon is to measure how many times it
makes appropriate choices that cannot be made by HALogen?that is, cases that make
good use of the nuances from the LKB of NS. This excludes the test cases with default
near-synonyms?those in which Xenon makes the right choice simply because of its
language model?and cases of ties in which Xenon cannot make the expected choice.
Accuracies for nondefault cases vary from 84.3% to 100%.
7.5.2 Evaluation of the Near-Synonym Collocation Module. For the evaluation of the
near-synonym collocation module, we collected sentences from the BNC that contain
preferred collocations from the knowledge base of near-synonym collocational behav-
ior. The BNC was preferred over Hansard for these evaluation experiments because it
is a balanced corpus and contains the collocations of interest, whereas Hansard does
not contain some of the collocations and near-synonyms of interest. The sentences were
collected from the first half of the BNC (50 million words). Sentence data sets 3 and 4
contain collocations for the development set of near-synonyms in Figure 17; sentence
data sets 5 and 6 contain collocations for the English near-synonyms in Figure 18.
Sets 3 and 5 include at most two sentences per collocation (the first two sentences from
the corpus, except in cases when the input construction tool failed to produce valid
interlingual representations); sets 4 and 6 include all the sentences with collocations
as they occurred in the fragment of the corpus (except the sentences for which the
input construction tool failed). For example, for set 4 there were initially 527 sentences,
and the input construction tool succeeded on 297 of them. Set 3 was used for develop-
ment?to choose the discount weights (see below)?and the others only for testing. The
architecture of this experiment is the same as that of the English-to-English experiments
(Figure 22), except that in this case it was the near-synonym choice module that was
disabled.
We observe that the sentence data sets may contain collocations for the wrong
senses of some near-synonyms because, as explained in Section 3.4, the near-synonym
collocations knowledge base may contain, for a cluster, collocations for a different sense.
For example, the collocation trains run appears in the cluster flow, gush, pour, run, spout,
spurt, squirt, stream, when it should appear only in another cluster. In this case the near-
synonym run should not be replaced with the metaconcept generic flow v because
it corresponds to a different metaconcept. These sentences should be eliminated from
255
Computational Linguistics Volume 32, Number 2
Table 8
The results of the evaluation of Xenon?s collocations module (boldface indicates best results).
HALogen only (baseline) HALogen + collocations
Sentence No. of Correct Pref. Anti- Correct Pref. Anti-
data set cases NS choice collocs collocs NS choice collocs collocs
Set 3 105 62% 88% 12% 70% 100% 0%
Set 4 297 83% 91% 9% 87% 99% 1%
Set 5 44 59% 80% 20% 75% 100% 0%
Set 6 185 58% 68% 32% 86% 99% 1%
the data sets, but this would involve disambiguation or manual elimination. However,
they do not affect the evaluation results because they are unlikely to produce anti-
collocations. This is because trains run is a frequent bigram, whereas trains flow is not;
Xenon will make the correct choice by default.
Sentence data set 3 was used to choose the best values of the discount weights
Wanti colloc and Wless pref colloc. In fact, the latter could be approximated by the former,
treating less preferred collocations as anti-collocations, because the number of less
preferred collocations is very small in the knowledge base. As the value of the dis-
count weight Wanti colloc increased (from 0.0 and 1.0), the number of anti-collocations
generated decreased; there were no anti-collocations left for Wanti colloc = 0.995.
Table 8 presents the results of the evaluation experiments. These results refer to
the evaluation of Xenon with the near-synonym collocations module enabled and the
near-synonym choice module disabled (lexical nuances are ignored in this experiment).
The baseline used for comparison is obtained by running HALogen only, without any
extension modules (no knowledge of collocations). For each test, the first four columns
contain the number of test cases, the number of near-synonyms correctly chosen by
the baseline system, the number of preferred collocations, and the number of anti-
collocations produced by the baseline system. The remainder of the columns present
results obtained by running Xenon with only the near-synonym collocations module
enabled (i.e., HALogen and the collocations module): the number of near-synonyms
correctly chosen, the number of preferred collocations produced, and the number of
anti-collocations produced. The number of anti-collocations was successfully reduced
to zero, except for sentence sets 4 and 6 where 1% of the anti-collocations remained. The
sixth column (correct choices or accuracy) differs from the seventh column (preferred
collocations) in the following way: The correct choice is the near-synonym used in the
original BNC sentence; sometimes the generated sentence can choose a different near-
synonym that is not the expected one but which participates in a preferred collocation
(this happens when more than one near-synonym from the same cluster collocates
well with the collocate word). For example, both serious mistake and serious blunder are
preferred collocations, while only one of mistake and blunder is the correct choice in any
particular context. The number of correct choices is relevant in this experiment only to
show that the collocations module does not have a negative effect on correctness; it even
increases the accuracy.24
24 The accuracy without ties was used here; therefore the numbers are conservative.
256
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
7.5.3 Evaluation of the Two Modules in Interaction. The interaction between the near-
synonym choice module and the collocations module increases Xenon?s performance.
To prove this, we repeated the experiments of the previous section, but this time with
input preferences (the nuances of the near-synonym from the original sentence). The
architecture of this test is the same as that of the English-to-English experiments in
Section 7.5.1, depicted in Figure 22. Table 9 shows the number of correct near-synonym
choices (and the percent accuracy) for the baseline case (no nuances, no collocation
module; i.e., HALogen by itself), for the collocations module alone (i.e., HALogen and
the collocations module only; this column is also part of Table 8), for the near-synonym
choice module alone (i.e., HALogen and the nuances module only), and for Xenon with
both modules enabled. When both modules are enabled there is a slight increase in
accuracy on sentence data sets 4, 5, and 6; the accuracy on set 3 is the same as using the
near-synonyms module only.
7.6 Summary
This section presented Xenon, an NLG system capable of choosing the near-synonym
that best satisfies a set of input preferences (lexical nuances). The input preferences
could come from an analysis module for a different language; in this case the translation
into English would preserve not only the meaning but also nuances of meaning. The
evaluation of Xenon?s two new modules shows that they behave well, both indepen-
dently and in interaction.
The evaluation showed that we were successful in dealing with lexical nuances in
general. One weak point of the evaluation was the relatively small overlap in coverage
of the French and English knowledge bases. Another bottle-neck was the need for a
language-neutral ontology.
8. Conclusion
We have presented a method for extracting knowledge from dictionaries of near-
synonym discrimination. The method can potentially be applied to any dictionary
Table 9
Correct near-synonym choices for the baseline system (HALogen only), for HALogen with each
module of Xenon separately, and for HALogen with both modules of Xenon (boldface indicates
best results).
HALogen
(baseline) Xenon
Correct NS Correct NS Correct NS
Sentence Number collocations nuances nuances +
data set of cases Correct NS module module collocations
Set 3 105 62% 70% 93% 93%
Set 4 297 83% 87% 95% 95%
Set 5 44 59% 75% 93% 95%
Set 6 185 58% 86% 91% 95%
257
Computational Linguistics Volume 32, Number 2
of near-synonym discrimination for any language for which preprocessing tools,
such as part-of-speech taggers and parsers, are available. We built a new lexical re-
source, a lexical knowledge base of differences among English near-synonyms, by
applying the extraction method to Choose the Right Word. The precision and recall
of the extracted knowledge was estimated to be in the range of 70?80%. If higher
precision and recall are needed for particular applications, a human could vali-
date each extraction step. We enriched the initial lexical knowledge base of near-
synonyms with distinctions extracted from machine-readable dictionaries.
We have presented Xenon, a natural language generation system that uses the
LKB of NS to choose the near-synonym that best matches a set of input preferences.
Xenon extends a previous NLG system with two new modules: a module that chooses
near-synonyms on the basis of their lexical nuances, and a module that chooses near-
synonyms on the basis of their collocations. To evaluate Xenon, we manually built
a small LKB of French near-synonyms. The test set consisted of English and French
sentences that are mutual translations. An interlingual representation (with the near-
synonym replaced by the core denotation of its cluster) was input to Xenon, together
with the nuances of the near-synonym from the French sentence. The generated sen-
tence was considered correct if the chosen English near-synonym was the one from the
original English sentence. We also evaluated the near-synonym collocation module and
the interaction of the two modules.
Short-term future work includes overcoming some limitations and extending the
current work, such as extending the near-synonym representation with other types
of distinctions such as information about more general and more specific words, and
information about special meanings that some words have in particular contexts or
domains (e.g., in a legal context).
Longer-term future work directions include the following:
Analysis of lexical nuances A full-fledged analysis module could be developed.
Sense disambiguation would be required when a near-synonym is a member of more
than one cluster. It is more difficult to model the influence of the context and the
complex interaction of the lexical nuances. Such an analysis module could be used
in an Machine Translation (MT) system that preserves lexical nuances. It could also
be used to determine nuances of text for different purposes. For example, a system
could decide if a text is positive, neutral, or negative in its semantic orientation. Then
Xenon could be used to generate a new text that has the same meaning as the origi-
nal text but a different semantic orientation. This could be useful, for example, in an
application that sends letters to customers: If the initial draft of the text is found to be
too negative, it could be transformed into a more positive text before it is sent to the
customer.
Lexical and conceptual associations The method presented in Section 3 could
be extended to acquire lexical associations (i.e., longer-distance collocations) of near-
synonyms. Words that strongly associate with the near-synonyms can be useful,
especially those that associate with only one of the near-synonyms in the cluster.
These strong associations could provide additional knowledge about nuances of near-
synonyms.
An experiment similar to that presented in Section 3 could look for words that co-
occur in a window of size K > 2 to acquire lexical associations, which would include
the collocations extracted in Section 3.2. Church et al (1991) presented associations for
the near-synonyms ship and boat; they suggest that a lexicographer looking at these
associations can infer that boats are generally smaller than ships because they are found
in rivers and lakes and are used for small jobs (e.g., fishing, police, pleasure), whereas
258
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
ships are found in seas and are used for serious business (e.g., cargo, war). It could be
possible to automatically infer this kind of knowledge or to validate already acquired
knowledge.
Words that do not associate with a near-synonym but associate with all the other
near-synonyms in a cluster can tell us something about its nuances of meaning. For
example, terrible slip is an anti-association, whereas terrible associates with mistake,
blunder, error. This is an indication that slip is a minor error. By further generalization,
the associations could become conceptual associations. This may allow the automatic
learning of denotational distinctions between near-synonyms from free text. The con-
cepts that are common to all the near-synonyms in a cluster could be part of the core
denotation, whereas those that associate only with one near-synonym could be part of
a distinction.
Cross-lingual lexical nuances The method presented in Section 2 could be used to
automatically build a lexical knowledge base of near-synonym differences for other lan-
guages, such as French, for which dictionaries of synonym discriminations are available
(in paper form) along with other resources, such as part-of-speech taggers and parsers.
In order to use the French and the English knowledge bases in the same system, a study
of the cross-lingual lexical nuances will be needed.
Analysis of types of peripheral nuances Linguists and lexicographers have looked
at differences between particular types of near-synonyms. For example, Gao (2001)
studied the semantic distinctions between Chinese physical action verbs; one type of
distinctive peripheral nuance is the manner in which the movement is made for each
verb. This kind of study could help to develop a list of the main types of peripheral
nuances (peripheral concepts). In our work, the form that the peripheral nuances can
take is not restricted, because the list of peripheral nuances is open-ended. However,
it may be possible to keep the form unrestricted but add restrictions for the most
important types of peripheral nuances.
Intelligent thesaurus The acquired lexical knowledge base of near-synonym differ-
ences could be used to develop an intelligent thesaurus that assists a writer not only
with a list of words that are similar to a given word but also with explanations about
the differences in nuances of meaning between the possible choices. The intelligent
thesaurus could order the choices to suit a particular writing context. The knowledge
about the collocational behavior of near-synonyms can be used in determining the
order: Near-synonyms that produce anti-collocations would be ranked lower than near-
synonyms that produce preferred collocations.
Automatic acquisition of near-synonyms This work considered only the near-
synonyms and distinctions that were listed by the lexicographers of CTRW. Other
dictionaries of synonym discrimination may have slightly different views. Merging
clusters from different dictionaries is possible. Also, near-synonym clusters could be
acquired from free text. This would distinguish near-synonyms from the pool of re-
lated words. As mentioned in Section 5.3, Lin et al (2003) acquired words that are
related by contextual similarity, and then filtered out the antonyms. Words that are
related by relations other than near-synonymy could also be filtered out. One way to
do this could be to collect signatures for each potential near-synonym?words that as-
sociate with it in many contexts. For two candidate words, if one signature is contained
in the other, the words are probably in an IS-A relation; if the signatures overlap totally,
it is a true near-synonymy relation; if the signatures overlap partially, it is a different
kind of relation. The acquisition of more near-synonyms, followed by the acquisition of
more distinctions, is needed to increase the coverage of our lexical knowledge base of
near-synonym differences.
259
Computational Linguistics Volume 32, Number 2
Acknowledgments
We thank Irene Langkilde-Geary for making
the input construction tool available and for
her advice on how to connect our preference
satisfaction mechanism to HALogen. We
thank Phil Edmonds for making available
the source code of I-Saurus. We thank
Suzanne Stevenson, Gerald Penn, Ted
Pedersen, and anonymous reviewers for
their feedback on various stages of this work.
We thank Ol?ga Feiguina for being an
enthusiastic research assistant and for
implementing the programs from Section 4.
Our work is financially supported by the
Natural Sciences and Engineering Research
Council of Canada, the University of
Toronto, and the University of Ottawa.
References
Abney, Steven. 1996. Partial parsing via
finite-state cascades. In Proceedings of the
8th European Summer School in Logic,
Language and Information (ESS-LLI?96),
Robust Parsing Workshop, pages 124?131,
Prague, Czech Republic.
Apresjan, J. D., V. V. Botiakova, T. E.
Latiskeva, M. A. Mosiagina, I. V. Polik, V. I.
Rakitina, A. A. Rozenman, and E. E.
Sretenskaia. 1980. Anglo-Russkii
Sinonimicheskii Slovar. Izdatelstvo Russkii
Jazik.
Apresjan, Juri. 2000. Systematic Lexicography.
Translation, Oxford University Press.
Bailly, Rene?, editor. 1973. Dictionnaire des
Synonymes de la Langue Franc?aise. Larousse,
Paris.
Barrie`re, Caroline and Fred Popowich. 1996.
Concept clustering and knowledge
integration from a children?s dictionary. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING?96), pages 65?70, Copenhagen,
Denmark.
Be?nac, Henri, editor. 1956. Dictionnaire des
Synonymes. Librarie Hachette, Paris.
Bernard, J. R. L., editor. 1986. The Macquarie
Thesaurus?Companion to The Macquarie
Dictionary. Macquarie Library, Sydney,
Australia.
Caraballo, Sharon. 1999. Automatic
acquisition of a hypernym-labeled noun
hierarchy from text. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 120?126,
College Park, MD.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st Conference of the North American
Chapter of the Association for Computational
Linguistics and the 6th Conference on Applied
Natural Language Processing (NAACL-ANLP
2000), pages 132?139, Seattle, WA.
Church, Kenneth, William Gale, Patrick
Hanks, and Donald Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik,
editor, Lexical Acquisition: Using On-line
Resources to Build a Lexicon, pages 115?164.
Lawrence Erlbaum.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 184?191,
Santa Cruz.
Collins, Michael and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora
(EMNLP/VLC-99), pages 100?110,
College Park, MD.
Delbridge, A., J. R. L. Bernard, D. Blair, W. S.
Ramson, and Susan Butler, editors. 1987.
The Macquarie Dictionary. Macquarie
Library, Sydney, Australia.
DiMarco, Chrysanne, Graeme Hirst, and
Manfred Stede. 1993. The semantic and
stylistic differentiation of synonyms and
near-synonyms. In Proceedings of AAAI
Spring Symposium on Building Lexicons for
Machine Translation, pages 114?121,
Stanford, CA.
Dorr, Bonnie J. 1993. The Use of Lexical
Semantics in Interlingual Machine
Translation. The MIT Press, Cambridge,
MA.
Edmonds, Philip. 1999. Semantic
representations of near-synonyms for
automatic lexical choice. Ph.D. thesis,
University of Toronto.
Edmonds, Philip and Graeme Hirst. 2002.
Near-synonymy and lexical choice.
Computational Linguistics, 28(2):105?145.
Gao, Hong. 2001. The Physical Foundation of
the Patterning of Physical Action Verbs. Ph.D.
thesis, Lund University.
Gove, Philip B., editor. 1984. Webster?s New
Dictionary of Synonyms. G.&C. Merriam Co.
Hayakawa, S. I., editor. 1994. Choose the
Right Word. Second Edition, revised
by Eugene Ehrlich. HarperCollins
Publishers.
Hearst, Marti. 1992. Automatic acquisition
of hyponyms from large corpora. In
Proceedings of the 14th International
Conference on Computational Linguistics
(COLING?92), pages 538?545, Nantes,
France.
260
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Hirst, Graeme. 1995. Near-synonymy and the
structure of lexical knowledge. In Working
Notes, AAAI Symposium on Representation
and Acquisition of Lexical Knowledge:
Polysemy, Ambiguity, and Generativity,
pages 51?56, Stanford University.
Hovy, Eduard. 1990. Pragmatics and
language generation. Artificial Intelligence,
43:153?197.
Ide, Nancy and Jean Ve?ronis. 1994.
Knowledge extraction from
machine-readable dictionaries: An
evaluation. In P. Steffens, editor, Machine
Translation and the Lexicon, pages 19?34,
Springer-Verlag.
Inkpen, Diana. 2003. Building a Lexical
Knowledge Base of Near-Synonym Differences.
Ph.D. thesis, University of Toronto.
Inkpen, Diana Zaiu and Graeme Hirst. 2001.
Building a lexical knowledge base of
near-synonym differences. In Proceedings of
the Workshop on WordNet and Other Lexical
Resources, Second Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2001),
pages 47?52, Pittsburgh, PA.
Inkpen, Diana Zaiu and Graeme Hirst. 2002.
Acquiring collocations for lexical choice
between near-synonyms. In Proceedings of
the Workshop on Unsupervised Lexical
Acquisition, 40th Annual Meeting of the
Association for Computational Linguistics
(ACL 2002), pages 67?76, Philadelphia, PA.
Inkpen, Diana Zaiu and Graeme Hirst. 2003.
Near-synonym choice in natural language
generation. In Proceedings of the
International Conference RANLP-2003
(Recent Advances in Natural Language
Processing), pages 204?211, Borovets,
Bulgaria.
Knight, Kevin and Steve Luk. 1994. Building
a large knowledge base for machine
translation. In Proceedings of the 12th
National Conference on Artificial Intelligence
(AAAI-94), pages 773?778, Seattle, WA.
Langkilde, Irene. 2000. Forest-based
statistical sentence generation. In
Proceedings of the 1st Conference of the North
American Chapter of the Association for
Computational Linguistics and the 6th
Conference on Applied Natural Language
Processing (NAACL-ANLP 2000),
pages 170?177, Seattle, WA.
Langkilde, Irene and Kevin Knight. 1998. The
practical value of N-grams in generation.
In Proceedings of the 9th International
Natural Language Generation Workshop,
pages 248?255, Niagara-on-the-Lake,
Canada.
Langkilde-Geary, Irene. 2002a. An empirical
verification of coverage and correctness for
a general-purpose sentence generator. In
Proceedings of the 12th International Natural
Language Generation Workshop, pages 17?24,
New York, NY.
Langkilde-Geary, Irene. 2002b. A Foundation
for a General-Purpose Natural Language
Generation: Sentence Realization Using
Probabilistic Models of Language. Ph.D.
thesis, University of Southern California.
Lenat, Doug. 1995. Cyc: A large-scale
investment in knowledge infrastructure.
Communications of the ACM, 38(11):33?38.
Lesk, Michael. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of
SIGDOC Conference, pages 24?26, Toronto,
Canada.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and
Ming Zhou. 2003. Identifying synonyms
among distributionally similar words. In
Proceedings of the Eighteenth Joint
International Conference on Artificial
Intelligence (IJCAI-03), pages 1492?1493,
Acapulco, Mexico.
Lindberg, Donald A. B., Betsy L. Humphreys,
and Alexa T. McCray. 1993. The unified
medical language system. Methods of
Information in Medicine, 32(4):281?289.
Mahesh, Kavi and Sergei Nirenburg. 1995. A
situated ontology for practical NLP. In
Proceedings of Workshop on Basic Ontological
Issues in Knowledge Sharing, International
Joint Conference on Artificial Intelligence
(IJCAI-95), Montreal, Canada.
Manning, Christopher and Hinrich Schu?tze.
1999. Foundations of Statistical Natural
Language Processing. The MIT Press,
Cambridge, MA.
McKeown, Kathleen and Dragomir Radev.
2000. Collocations. In Robert Dale,
Hermann Moisl, and Harold Somers,
editors, Handbook of Natural Language
Processing, pages 507?524. Marcel Dekker.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the
ACM, 38(11):39?41.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. BLEU: A
method for automatic evaluation of
machine translation. Technical report
RC22176, IBM Research Division, Thomas
J. Watson Research Center.
Pearce, Darren. 2001. Synonymy in
collocation extraction. In Proceedings of the
Workshop on WordNet and Other Lexical
Resources, Second meeting of the North
261
Computational Linguistics Volume 32, Number 2
American Chapter of the Association for
Computational Linguistics, pages 41?46,
Pittsburgh, PA.
Pedersen, Ted and Satanjeev Banerjee. 2003.
The design, implementation, and use of
the ngram statistical package. In
Proceedings of the 4th International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing 2003),
pages 370?381, Mexico City, Mexico.
Ploux, Sabine and Hyungsuk Ji. 2003. A
model for matching semantic maps
between languages (French/English,
English/French). Computational Linguistics,
29(2):155?178.
Resnik, Philip. 1999. Mining the web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 527?534,
College Park, MD.
Resnik, Philip, Mari Broman Olsen, and
Mona Diab. 1999. The bible as a parallel
corpus: Annotating the ?Book of 2000
Tongues?. Computers and the Humanities,
33(1?2):129?153.
Richardson, Stephen, William Dolan, and
Lucy Vanderwende. 1998. MindNet:
Acquiring and structuring semantic
information from text. In Proceedings of the
36th Annual Meeting of the Association for
Computational Linguistics joint with 17th
International Conference on Computational
Linguistics (ACL-COLING?98),
pages 1098?1102, Montreal, Quebec,
Canada.
Riloff, Ellen and Rosie Jones. 1999. Learning
dictionaries for information extraction by
multi-level bootstrapping. In Proceedings of
the 16th National Conference on Artificial
Intelligence (AAAI-99), pages 474?479,
Orlando, FL.
Roget, Peter Mark, editor. 1852. Roget?s
Thesaurus of English Words and Phrases.
Longman Group Ltd., Harlow, Essex,
England.
Room, Adrian, editor. 1981. Room?s Dictionary
of Distinguishables. Routhledge & Kegan
Paul, Boston.
Sim, Julius and Chris C. Wright. 2005. The
kappa statistic in reliability studies: Use,
interpretation, and sample size
requirements. Physical Therapy,
85(3):257?268.
Stone, Philip J., Dexter C. Dunphy,
Marshall S. Smith, Daniel M. Ogilvie, and
associates. 1966. The General Inquirer: A
Computer Approach to Content Analysis. The
MIT Press, Cambridge, MA.
Turney, Peter. 2001. Mining the web for
synonyms: PMI-IR versus LSA on
TOEFL. In Proceedings of the Twelfth
European Conference on Machine Learning
(ECML 2001), pages 491?502, Freiburg,
Germany.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 189?196, Cambridge,
MA.
262
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 61?64,
New York, June 2006. c?2006 Association for Computational Linguistics
Investigating Cross-Language Speech Retrieval for a  
Spontaneous Conversational Speech Collection  
 
Diana Inkpen, Muath Alzghool Gareth J.F. Jones Douglas W. Oard 
School of Info. Technology and Eng. School of Computing College of Info. Studies/UMIACS  
University of Ottawa Dublin City University University of Maryland 
Ottawa, Ontario, Canada, K1N 6N5 Dublin 9, Ireland College Park, MD 20742, USA 
{diana,alzghool}@site.uottawa.ca Gareth.Jones@computing.dcu.ie oard@umd.edu 
 
  
Abstract 
Cross-language retrieval of spontaneous 
speech combines the challenges of working 
with noisy automated transcription and lan-
guage translation. The CLEF 2005 Cross-
Language Speech Retrieval (CL-SR) task 
provides a standard test collection to inves-
tigate these challenges. We show that we 
can improve retrieval performance: by care-
ful selection of the term weighting scheme; 
by decomposing automated transcripts into 
phonetic substrings to help ameliorate tran-
scription errors; and by combining auto-
matic transcriptions with manually-assigned 
metadata. We further show that topic trans-
lation with online machine translation re-
sources yields effective CL-SR. 
1 Introduction 
The emergence of large collections of digitized 
spoken data has encouraged research in speech re-
trieval. Previous studies, notably those at TREC 
(Garafolo et al 2000), have focused mainly on 
well-structured news documents. In this paper we 
report on work carried out for the Cross-Language 
Evaluation Forum (CLEF) 2005 Cross-Language 
Speech Retrieval (CL-SR) track (White et al 2005). 
The document collection for the CL-SR task is a 
part of the oral testimonies collected by the USC 
Shoah Foundation Institute for Visual History and 
Education (VHI) for which some Automatic Speech 
Recognition (ASR) transcriptions are available 
(Oard et al, 2004). The data is conversional spon-
taneous speech lacking clear topic boundaries; it is 
thus a more challenging speech retrieval task than 
those explored previously. The CLEF data is also 
annotated with a range of automatic and manually 
generated sets of metadata. While the complete VHI 
dataset contains interviews in many languages, the 
CLEF 2005 CL-SR task focuses on English speech. 
Cross-language searching is evaluated by making 
the topic statements (from which queries are auto-
matically formed) available in several languages. 
This task raises many interesting research ques-
tions; in this paper we explore alternative term 
weighting methods and content indexing strategies.  
The remainder of this paper is structured as fol-
lows: Section 2 briefly reviews details of the CLEF 
2005 CL-SR task; Section 3 describes the system 
we used to investigate this task; Section 4 reports 
our experimental results; and Section 5 gives con-
clusions and details for our ongoing work.   
2 Task description 
The CLEF-2005 CL-SR collection includes 8,104 
manually-determined topically-coherent segments 
from 272 interviews with Holocaust survivors, wit-
nesses and rescuers, totaling 589 hours of speech. 
Two ASR transcripts are available for this data, in 
this work we use transcripts provided by IBM Re-
search in 2004 for which a mean word error rate of 
38% was computed on held out data. Additional, 
metadata fields for each segment include: two sets 
of 20 automatically assigned thesaurus terms from 
different kNN classifiers (AK1 and AK2), an aver-
age of 5 manually-assigned thesaurus terms (MK), 
and a 3-sentence summary written by a subject mat-
ter expert. A set of 38 training topics and 25 test 
topics were generated in English from actual user 
requests. Topics were structured as Title, Descrip-
tion and Narrative fields, which correspond roughly 
to a 2-3 word Web query, what someone might first 
say to a librarian, and what that librarian might ul-
timately understand after a brief reference inter-
view. To support CL-SR experiments the topics 
were re-expressed in Czech, German, French, and 
Spanish by native speakers in a manner reflecting 
61
the way questions would be posed in those lan-
guages. Relevance judgments were manually gener-
ated using by augmenting an interactive search-
guided procedure and purposive sampling designed 
to identify additional relevant segments. See (Oard 
et al 2004) and (White et al 2005) for details.  
3 System Overview 
Our Information Retrieval (IR) system was built 
with off-the-shelf components.  Topics were trans-
lated from French, Spanish, and German into Eng-
lish using seven free online machine translation 
(MT) tools. Their output was merged in order to 
allow for variety in lexical choices. All the transla-
tions of a topic Title field were combined in a 
merged Title field of the translated topics; the same 
procedure was adopted for the Description and Nar-
rative fields. Czech language topics were translated 
using InterTrans, the only web-based MT system 
available to us for this language pair. Retrieval was 
carried out using the SMART IR system (Buckley 
et al 1993) applying its standard stop word list and 
stemming algorithm.  
In system development using the training topics we 
tested SMART with many different term weighting 
schemes combining collection frequency, document 
frequency and length normalization for the indexed 
collection and topics (Salton and Buckley, 1988). In 
this paper we employ the notation used in SMART 
to describe the combined schemes: xxx.xxx. The 
first three characters refer to the weighting scheme 
used to index the document collection and the last 
three characters refer to the weighting scheme used 
to index the topic fields. For example, lpc.atc means 
that lpc was used for documents and atc for queries. 
lpc would apply log term frequency weighting (l) 
and probabilistic collection frequency weighting (p) 
with cosine normalization to the document collec-
tion (c). atc would apply augmented normalized 
term frequency (a), inverse document frequency 
weight (t) with cosine normalization (c). 
One scheme in particular (mpc.ntn) proved to 
have much better performance than other combina-
tions. For weighting document terms we used term 
frequency normalized by the maximum value (m) 
and probabilistic collection frequency weighting (p) 
with cosine normalization (c). For topics we used 
non-normalized term frequency (n) and inverse 
document frequency weighting (t) without vector 
normalization (n). This combination worked very 
well when all the fields of the query were used; it 
also worked well with Title plus Description, but 
slightly less well with the Title field alone. 
4 Experimental Investigation 
In this section we report results from our experi-
mental investigation of the CLEF 2005 CL-SR task. 
For each set of experiments we report Mean unin-
terpolated Average Precision (MAP) computed us-
ing the trec_eval script. The topic fields used are 
indicated as: T for title only, TD for title + descrip-
tion, TDN for title + description + narrative. The 
first experiment shows results for different term 
weighting schemes; we then give cross-language 
retrieval results. For both sets of experiments, 
?documents? are represented by combining the 
ASR transcription with the AK1 and AK2 fields. 
Thus each document representation is generated 
completely automatically. Later experiments ex-
plore two alternative indexing strategies. 
4.1 Comparison of Term Weighting Schemes 
The CLEF 2005 CL-SR collection is quite small by 
IR standards, and it is well known that collection 
size matters when selecting term weighting schemes 
(Salton and Buckley, 1988).  Moreover, the docu-
ments in this case are relatively short, averaging 
about 500 words (about 4 minutes of speech), and 
that factor may affect the optimal choice of weight-
ing schemes as well.  We therefore used the training 
topics to explore the space of available SMART 
term weighting schemes.  Table 1 presents results 
for various weighting schemes with  English topics. 
There are 3,600 possible combinations of weighting 
schemes available: 60 schemes (5 x 4 x 3) for 
documents and 60 for queries. We tested a total of 
240 combinations. In Table 1 we present the results 
for 15 combinations (the best ones, plus some oth-
ers to illustate  the diversity of the results). mpc.ntn 
is still the best for the test topic set; but, as shown, a 
few other weighting schemes achieve similar per-
formance. Some of the weighting schemes perform 
better when indexing all the topic fields (TDN), 
some on TD, and some on title only (T). npn.ntn 
was best for TD and lsn.ntn and lsn.atn are best for 
T. The mpc.ntn weighting scheme is used for all 
other experiments in this section.  We are investi-
gating the reasons for the effectiveness of this 
weighting scheme in our experiments. 
62
TDN TD T  Weighting 
scheme Map Map Map 
1 Mpc.mts 0.2175 0.1651 0.1175 
2 Mpc.nts 0.2175 0.1651 0.1175 
3 Mpc.ntn  0.2176 0.1653 0.1174 
4 npc.ntn 0.2176 0.1653 0.1174 
5 Mpc.mtc 0.2176 0.1653 0.1174 
6 Mpc.ntc 0.2176 0.1653 0.1174 
7 Mpc.mtn 0.2176 0.1653 0.1174 
8 Npn.ntn 0.2116 0.1681 0.1181 
9 lsn.ntn 0.1195 0.1233 0.1227 
10 lsn.atn 0.0919 0.1115 0.1227 
11 asn.ntn 0.0912 0.0923 0.1062 
12 snn.ntn 0.0693 0.0592 0.0729 
13 sps.ntn 0.0349 0.0377 0.0383 
14 nps.ntn 0.0517 0.0416 0.0474 
15 Mtc.atc 0.1138 0.1151 0.1108 
Table 1. MAP, 25 English test topics. Bold=best scores. 
4.2 Cross-Language Experiments 
Table 2 shows our results for the merged ASR, 
AK1 and AK2 documents with multi-system topic 
translations for French, German and Spanish, and 
single-system Czech translation. We can see that 
Spanish topics perform well compared to monolin-
gual English. However, results for German and 
Czech are much poorer. This is perhaps not surpris-
ing for the Czech topics where only a single transla-
tion is available. For German, the quality of 
translation was sometimes low and some German 
words were retained untranslated. For French, only 
TD topic fields were available.  In this case we can 
see that cross-language retrieval effectiveness is 
almost identical to monolingual English. Every re-
search team participating in the CLEF 2005 CL-SR 
task submitted at least one TD English run, and 
among those our mpc.ntn system yielded the best 
MAP (Wilcoxon signed rank test for paired sam-
ples, p<0.05). However, as we show in Table 4, 
manual metadata can yield better retrieval effec-
tiveness than automatic description.  
 
Topic 
Language 
System Map Fields 
English Our system 0.1653 TD 
English Our system 0.2176 TDN 
Spanish Our system 0.1863 TDN 
French Our system 0.1685 TD 
German Our system 0.1281 TDN 
Czech Our system 0.1166 TDN 
Table 2. MAP, cross-language, 25 test topics 
Language Map Fields Description 
English 0.1276 T Phonetic 
English 0.2550 TD Phonetic 
English 0.1245 T Phonetic+Text 
English 0.2590 TD Phonetic+Text 
Spanish 0.1395 T Phonetic 
Spanish 0.2653 TD Phonetic 
Spanish 0.1443 T Phonetic+Text 
Spanish 0.2669 TD Phonetic+Text 
French 0.1251 T Phonetic 
French 0.2726 TD Phonetic 
French 0.1254 T Phonetic+Text 
French 0.2833 TD Phonetic+Text 
German 0.1163 T Phonetic 
German 0.2356 TD Phonetic 
German 0.1187 T Phonetic+Text 
German 0.2324 TD Phonetic+Text 
Czech 0.0776 T Phonetic 
Czech 0.1647 TD Phonetic 
Czech 0.0805 T Phonetic+Text 
Czech 0.1695 TD Phonetic+Text 
Table 3. MAP, phonetic 4-grams, 25 test topics. 
4.3 Results on Phonetic Transcriptions 
In Table 3 we present results for an experiment 
where the text of the collection and topics, without 
stemming, is transformed into a phonetic transcrip-
tion. Consecutive phones are then grouped into 
overlapping n-gram sequences (groups of n sounds, 
n=4 in our case) that we used for indexing. The 
phonetic n-grams were provided by Clarke (2005), 
using NIST?s text-to-phone tool1. For example, the 
phonetic form for the query fragment child survi-
vors is: ch_ay_l_d s_ax_r_v ax_r_v_ay r_v_ay_v 
v_ay_v_ax ay_v_ax_r v_ax_r_z. 
The phonetic form helps compensate for the 
speech recognition errors. With TD queries, the re-
sults improve substantially compared with the text 
form of the documents and queries (9% relative). 
Combining phonetic and text forms (by simply in-
dexing both phonetic n-grams and text) yields little 
additional improvement. 
4.4 Manual summaries and keywords 
Manually prepared transcripts are not available 
for this test collection, so we chose to use manually 
assigned metadata as a reference condition.  To ex-
plore the effect of merging automatic and manual 
fields, Table 4 presents the results combining man-
                                                          
1 http://www.nist.gov/speech/tools/ 
63
ual keywords and manual summaries with ASR 
transcripts, AK1, and AK2. Retrieval effectiveness 
increased substantially for all topic languages. The 
MAP score improved with 25% relative when add-
ing the manual metadata for English TDN.  
Table 4 also shows comparative results between 
and our results and results reported by the Univer-
sity of Maryland at CLEF 2005 using a widely used 
IR system (InQuery) that has a standard term 
weighting algorithm optimized for large collections. 
For English TD, our system is 6% (relative) better 
and for French TD 10% (relative) better.  The Uni-
versity of Maryland results with only automated 
fields are also lower than the results we report in 
Table 2 for the same fields. 
 
Table 4. MAP, indexing all fields (MK, summaries, 
ASR transcripts, AK1 and AK2), 25 test topics. 
Language System Map Fields 
English Our system 0.4647 TDN 
English Our system 0.3689 TD 
English InQuery 0.3129 TD 
English Our system 0.2861 T 
Spanish Our system 0.3811 TDN 
French Our system 0.3496 TD 
French InQuery 0.2480 TD 
French Our system 0.3496 TD 
German Our system 0.2513 TDN 
Czech Our system 0.2338 TDN 
5 Conclusions and Further Investigation 
The system described in this paper obtained the best 
results among the seven teams that participated in 
the CLEF 2005 CL-SR track. We believe that this 
results from our use of the 38 training topics to find 
a term weighting scheme that is particularly suitable 
for this collection. Relevance judgments are typi-
cally not available for training until the second year 
of an IR evaluation; using a search-guided process 
that does not require system results to be available 
before judgments can be performed made it possi-
ble to accelerate that timetable in this case.  Table 2 
shows that performance varies markedly with the 
choice of weighting scheme.  Indeed, some of the 
classic weighting schemes yielded much poorer 
results than the one  we ultimately selected. In this 
paper we presented results on the test queries, but 
we observed similar effects on the training queries. 
On combined manual and automatic data, the 
best MAP score we obtained for English topics is 
0.4647. On automatic data, the best MAP is 0.2176. 
This difference could result from ASR errors or 
from terms added by human indexers that were not 
available to the ASR system to be recognized. In 
future work we plan to investigate methods of re-
moving or correcting some of the speech recogni-
tion errors in the ASR transcripts using semantic 
coherence measures. 
In ongoing further work we are exploring the re-
lationship between properties of the collection and 
the weighting schemes in order to better understand 
the underlying reasons for the demonstrated effec-
tiveness of the mpc.ntn weighting scheme.  
The challenges of CLEF CL-SR task will con-
tinue to expand in subsequent years as new collec-
tions are introduced (e.g., Czech interviews in 
2006). Because manually assigned segment bounda-
ries are available only for English interviews, this 
will yield an unknown topic boundary condition 
that is similar to previous experiments with auto-
matically transcribed broadcast news the Text Re-
trieval Conference (Garafolo et al 2000), but with 
the additional caveat that topic boundaries are not 
known for the ground truth relevance judgments.    
References 
Chris Buckley, Gerard Salton, and James Allan. 1993. 
Automatic retrieval with locality information using 
SMART. In Proceedings of the First Text REtrieval 
Conference (TREC-1), pages 59?72. 
Charles L. A. Clarke. 2005. Waterloo Experiments for 
the CLEF05 SDR Track, in Working Notes for the 
CLEF 2005 Workshop, Vienna, Austria 
John S. Garofolo, Cedric G.P. Auzanne and Ellen M. 
Voorhees. 2000. The TREC Spoken Document Re-
trieval Track: A Success Story. In Proceedings of the 
RIAO Conference: Content-Based Multimedia Infor-
mation Access, Paris, France, pages 1-20. 
Douglas W. Oard, Dagobert Soergel, David Doermann, 
Xiaoli Huang, G. Craig Murray, Jianqiang Wang, 
Bhuvana Ramabhadran, Martin Franz and Samuel 
Gustman. 2004. Building an Information Retrieval 
Test Collection for Spontaneous Conversational 
Speech, in  Proceedings of SIGIR, pages 41-48. 
Gerard Salton and Chris Buckley. 1988. Term-weighting 
approaches in automatic retrieval. Information Proc-
essing and Management, 24(5):513-523. 
Ryen W. White, Douglas W. Oard, Gareth J. F. Jones, 
Dagobert Soergel and Xiaoli Huang. 2005. Overview 
of the CLEF-2005 Cross-Language Speech Retrieval 
Track, in Working Notes for the CLEF 2005 Work-
shop, Vienna, Austria 
64
Proceedings of NAACL HLT 2007, pages 356?363,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Near-Synonym Choice in an Intelligent Thesaurus
Diana Inkpen
School of Information Technology and Engineering,
University of Ottawa
800 King Edward, Ottawa, ON, Canada, K1N 6N5
diana@site.uottawa.ca
Abstract
An intelligent thesaurus assists a writer
with alternative choices of words and or-
ders them by their suitability in the writ-
ing context. In this paper we focus on
methods for automatically choosing near-
synonyms by their semantic coherence
with the context. Our statistical method
uses the Web as a corpus to compute
mutual information scores. Evaluation
experiments show that this method per-
forms better than a previous method on
the same task. We also propose and evalu-
ate two more methods, one that uses anti-
collocations, and one that uses supervised
learning. To asses the difficulty of the
task, we present results obtained by hu-
man judges.
1 Introduction
When composing a text, a writer can access a the-
saurus to retrieve words that are similar to a given
target word, when there is a need to avoid repeating
the same word, or when the word does not seem to
be the best choice in the context.
Our intelligent thesaurus is an interactive appli-
cation that presents the user with a list of alterna-
tive words (near-synonyms), and, unlike standard
thesauri, it orders the choices by their suitability to
the writing context. We investigate how the collo-
cational properties of near-synonyms can help with
choosing the best words. This problem is difficult
because the near-synonyms have senses that are very
close to each other, and therefore they occur in sim-
ilar contexts; we need to capture the subtle differ-
ences specific to each near-synonym.
Our thesaurus brings up only alternatives that
have the same part-of-speech with the target word.
The choices could come from various inventories
of near-synonyms or similar words, for example the
Roget thesaurus (Roget, 1852), dictionaries of syn-
onyms (Hayakawa, 1994), or clusters acquired from
corpora (Lin, 1998).
In this paper we focus on the task of automat-
ically selecting the best near-synonym that should
be used in a particular context. The natural way to
validate an algorithm for this task would be to ask
human readers to evaluate the quality of the algo-
rithm?s output, but this kind of evaluation would be
very laborious. Instead, we validate the algorithms
by deleting selected words from sample sentences,
to see whether the algorithms can restore the miss-
ing words. That is, we create a lexical gap and eval-
uate the ability of the algorithms to fill the gap. Two
examples are presented in Figure 1. All the near-
synonyms of the original word, including the word
itself, become the choices in the solution set (see the
figure for two examples of solution sets). The task is
to automatically fill the gap with the best choice in
the particular context. We present a method of scor-
ing the choices. The highest scoring near-synonym
will be chosen. In order to evaluate how well our
method works we consider that the only correct so-
lution is the original word. This will cause our eval-
uation scores to underestimate the performance, as
more than one choice will sometimes be a perfect
356
Sentence: This could be improved by more detailed considera-
tion of the processes of ......... propagation inherent in digitizing
procedures.
Original near-synonym: error
Solution set: mistake, blooper, blunder, boner, contretemps,
error, faux pas, goof, slip, solecism
Sentence: The day after this raid was the official start of oper-
ation strangle, an attempt to completely destroy the ......... lines
of communication.
Original near-synonym: enemy
Solution set: opponent, adversary, antagonist, competitor, en-
emy, foe, rival
Figure 1: Examples of sentences with a lexical gap,
and candidate near-synonyms to fill the gap.
solution. Moreover, what we consider to be the best
choice is the typical usage in the corpus, but it may
vary from writer to writer. Nonetheless, it is a con-
venient way of producing test data.
The statistical method that we propose here is
based on semantic coherence scores (based on mu-
tual information) of each candidate with the words
in the context. We explore how far such a method
can go when using the Web as a corpus. We estimate
the counts by using the Waterloo MultiText1 system
(Clarke and Terra, 2003b) with a corpus of about
one terabyte of text collected by a Web crawler. We
also propose a method that uses collocations and
anti-collocations, and a supervised method that uses
words and mutual information scores as featured for
machine learning.
2 Related work
The idea of using the Web as a corpus of texts
has been exploited by many researchers. Grefen-
stette (1999) used the Web for example-based ma-
chine translation; Kilgarriff (2001) investigated the
type of noise in Web data; Mihalcea and Moldovan
(1999) and Agirre and Martinez (2000) used it as an
additional resource for word sense disambiguation;
Resnik (1999) mined the Web for bilingual texts;
Turney (2001) used Web frequency counts to com-
pute information retrieval-based mutual-information
scores. In a Computational Linguistics special issue
on the Web as a corpus (Kilgarriff and Grefenstette,
1We thank Egidio Terra, Charlie Clarke, and Univ. of Wa-
terloo for allowing us to use MultiText, and to Peter Turney and
IIT/NRC for giving us access to their local copy of the corpus.
2003), Keller and Lapata (2003) show that Web
counts correlate well with counts collected from a
balanced corpus: the size of the Web compensates
for the noise in the data. In this paper we are using a
very large corpus of Web pages to address a problem
that has not been successfully solved before.
In fact, the only work that addresses exactly the
same task is that of Edmonds (1997), as far as we
are aware. Edmonds gives a solution based on a
lexical co-occurrence network that included second-
order co-occurrences. We use a much larger corpus
and a simpler method, and we obtain much better
results.
Our task has similarities to the word sense disam-
biguation task. Our near-synonyms have senses that
are very close to each other. In Senseval, some of
the fine-grained senses are also close to each other,
so they might occur in similar contexts, while the
coarse-grained senses are expected to occur in dis-
tinct contexts. In our case, the near-synonyms are
different words to choose from, not the same word
with different senses.
3 A statistical method for near-synonym
choice
Our method computes a score for each candidate
near-synonym that could fill in the gap. The near-
synonym with the highest score is the proposed so-
lution. The score for each candidate reflects how
well a near-synonym fits in with the context. It is
based on the mutual information scores between a
near-synonym and the content words in the context
(we filter out the stopwords).
The pointwise mutual information (PMI) be-
tween two words x and y compares the probabil-
ity of observing the two words together (their joint
probability) to the probabilities of observing x and y
independently (the probability of occurring together
by chance) (Church and Hanks, 1991): PMI(x, y) =
log2
P (x,y)
P (x)P (y)
The probabilities can be approximated by:
P (x) = C(x)/N , P (y) = C(y)/N , P (x, y) =
C(x, y)/N , where C denotes frequency counts and
N is the total number of words in the corpus. There-
fore: PMI(x, y) = log2
C(x,y)?N
C(x)?C(y) , where N can be
ignored in comparisons.
We model the context as a window of size 2k
357
around the gap (the missing word): k words to the
left and k words to the right of the gap. If the sen-
tence is s = ? ? ?w1 ? ? ?wk Gap wk+1 ? ? ?w2k ? ? ?,
for each near-synonym NSi from the group of can-
didates, the semantic coherence score is computed
by the following formula:
Score(NSi, s) = ?kj=1PMI(NSi, wj) +
?2kj=k+1PMI(NSi, wj).
We also experimented with the same formula
when the sum is replaced with maximum to see
whether a particular word in the context has higher
influence than the sum of all contributions (though
the sum worked better).
Because we are using the Waterloo terabyte cor-
pus and we issue queries to its search engine,
we have several possibilities of computing the fre-
quency counts. C(x, y) can be the number of co-
occurrences of x and y when y immediately follows
x, or the distance between x and y can be up to q.
We call q the query frame size. The tool for access-
ing the corpus allows us to use various values for q.
The search engine also allows us to approxi-
mate words counts with document counts. If the
counts C(x), C(y), and C(x, y) are approximated
as the number of document in which they appear,
we obtain the PMI-IR formula (Turney, 2001). The
queries we need to send to the search engine are
the same but they are restricted to document counts:
C(x) is the number of document in which x occurs;
C(x, y) is the number of documents in which x is
followed by y in a frame of size q.
Other statistical association measures, such as
log-likelihood, could be used. We tried only PMI
because it is easy to compute on a Web corpus and
because PMI performed better than other measures
in (Clarke and Terra, 2003a).
We present the results in Section 6.1, where we
compare our method to a baseline algorithm that al-
ways chooses the most frequent near-synonyms and
to Edmonds?s method for the same task, on the same
data set. First, however, we present two other meth-
ods to which we compare our results.
4 The anti-collocations method
For the task of near-synonym choice, another
method that we implemented is the anti-collocations
method. By anti-collocation we mean a combina-
ghastly mistake spelling mistake
?ghastly error spelling error
ghastly blunder ?spelling blunder
?ghastly faux pas ?spelling faux pas
?ghastly blooper ?spelling blooper
?ghastly solecism ?spelling solecism
?ghastly goof ?spelling goof
?ghastly contretemps ?spelling contretemps
?ghastly boner ?spelling boner
?ghastly slip ?spelling slip
Table 1: Examples of collocations and anti-
collocations. The ? indicates the anti-collocations.
tion of words that a native speaker would not use
and therefore should not be used when automatically
generating text. This method uses a knowledge-
base of collocational behavior of near-synonyms ac-
quired in previous work (Inkpen and Hirst, 2006). A
fragment of the knowledge-base is presented in Ta-
ble 1, for the near-synonyms of the word error and
two collocate words ghastly and spelling. The lines
marked by ? represent anti-collocations and the rest
represent strong collocations.
The anti-collocations method simply ranks the
strong collocations higher than the anti-collocations.
In case of ties it chooses the most frequent near-
synonym. In Section 6.2 we present the results of
comparing this method to the method from the pre-
vious section.
5 A supervised learning method
We can also apply supervised learning techniques to
our task. It is easy to obtain labeled training data,
the same way we collected test data for the two un-
supervised methods presented above. We train clas-
sifiers for each group of near-synonyms. The classes
are the near-synonyms in the solution set. The word
that produced the gap is the expected solution, the
class label; this is a convenient way of producing
training data, no need for manual annotation. Each
sentence is converted into a vector of features to be
used for training the supervised classifiers. We used
two types of features. The features of the first type
are the PMI scores of the left and right context with
each class (each near-synonym from the group). The
number of features of this type is twice the number
of classes, one score for the part of the sentence at
the left of the gap, and one for the part at the right
of the gap. The features of the second type are the
358
1. mistake, error, fault
2. job, task, chore
3. duty, responsibility, obligation
4. difficult, hard
5. material, stuff
6. put up, provide, offer
7. decide, settle, resolve, adjudicate.
Table 2: The near-synonym groups used in Exp1.
words in the context window. For each group of
near-synonyms, we used as features the 500 most-
frequent words situated close to the gaps in a devel-
opment set. The value of a word feature for each
training example is 1 if the word is present in the
sentence (at the left or at the right of the gap), and 0
otherwise. We trained classifiers using several ma-
chine learning algorithms, to see which one is best
at discriminating among the near-synonyms. In Sec-
tion 6.3, we present the results of several classifiers.
A disadvantage of the supervised method is that it
requires training for each group of near-synonyms.
Additional training would be required whenever we
add more near-synonyms to our knowledge-base.
6 Evaluation
6.1 Comparison to Edmonds?s method
In this section we present results of the statistical
method explained in Section 3. We compare our
results with those of Edmonds?s (1997), whose so-
lution used the texts from the year 1989 of the
Wall Street Journal (WSJ) to build a lexical co-
occurrence network for each of the seven groups
of near-synonyms from Table 2. The network in-
cluded second-order co-occurrences. Edmonds used
the WSJ 1987 texts for testing, and reported accura-
cies only a little higher than the baseline. The near-
synonyms in the seven groups were chosen to have
low polysemy. This means that some sentences with
wrong senses of near-synonyms might be in the
For comparison purposes, in this section we use
the same test data (WSJ 1987) and the same groups
of near-synonyms (we call these sentences the Exp1
data set). Our method is based on mutual informa-
tion, not on co-occurrence counts. Our counts are
collected from a much larger corpus.
Table 3 presents the comparative results for the
seven groups of near-synonyms (we did not repeat
Accuracy
Set No. of Base- Edmonds Stat. Stat.
cases line method method method
(Docs) (Words)
1. 6,630 41.7% 47.9% 61.0% 59.1%
2. 1,052 30.9% 48.9% 66.4% 61.5%
3. 5,506 70.2% 68.9% 69.7% 73.3%
4. 3,115 38.0% 45.3% 64.1% 66.0%
5. 1,715 59.5% 64.6% 68.6% 72.2%
6. 11,504 36.7% 48.6% 52.0% 52.7%
7. 1,594 37.0% 65.9% 74.5% 76.9%
AVG 31,116 44.8% 55.7% 65.1% 66.0%
Table 3: Comparison between the statistical method
from Section 3, baseline algorithm, and Edmonds?s
method (Exp1 data set).
them in the first column of the table, only the num-
ber of the group.). The last row averages the ac-
curacies for all the test sentences. The second col-
umn shows how many test sentences we collected
for each near-synonym group. The third column is
for the baseline algorithm that always chooses the
most frequent near-synonym. The fourth column
presents the results reported in (Edmonds, 1997).
column show the results of the supervised learning
classifier described in Section 5. The fifth column
presents the result of our method when using doc-
ument counts in PMI-IR, and the last column is for
the same method when using word counts in PMI.
We show in bold the best accuracy for each data set.
We notice that the automatic choice is more difficult
for some near-synonym groups than for the others.
In this paper, by accuracy we mean the number of
correct choices made by each method (the number of
gaps that were correctly filled). The correct choice is
the near-synonym that was initially replaced by the
gap in the test sentence.
To fine-tune our statistical method, we used the
data set for the near-synonyms of the word difficult
collected from the WSJ 1989 corpus as a develop-
ment set. We varied the context window size k and
the query frame q, and determined good values for
the parameter k and q. The best results were ob-
tained for small window sizes, k = 1 and k = 2
(meaning k words to the left and k words to the right
of the gap). For each k, we varied the query frame
size q. The results are best for a relatively small
query frame, q = 3, 4, 5, when the query frame is
the same or slightly larger then the context window.
359
The results are worse for a very small query frame,
q = 1, 2 and for larger query frames q = 6, 7, ..., 20
or unlimited. The results presented in the rest of the
paper are for k = 2 and q = 5. For all the other data
sets used in this paper (from WSJ 1987 and BNC)
we use the parameter values as determined on the
development set.
Table 3 shows that the performance is generally
better for word counts than for document counts.
Therefore, we prefer the method that uses word
counts (which is also faster in our particular set-
ting). The difference between them is not statis-
tically significant. Our statistical method performs
significantly better than both Edmond?s method and
the baseline algorithm. For all the results presented
in this paper, statistical significance tests were done
using the paired t-test, as described in (Manning and
Schu?tze, 1999), page 209.
On average, our method performs 22 percentage
points better than the baseline algorithm, and 10
percentage points better than Edmonds?s method.
Its performance is similar to that of the supervised
method (see Section 6.3). An important advan-
tage of our method is that it works on any group
of near-synonyms without training, whereas Ed-
monds?s method required a lexical co-occurrence
network to be built in advance for each group of
near-synonyms and the supervised method required
training for each near-synonym group.
We note that the occasional presence of near-
synonyms with other senses than the ones we need
might make the task somewhat easier. Nonetheless,
the task is still difficult, even for human judges, as
we will see in Section 6.4. On the other hand, be-
cause the solution allows only one correct answer
the accuracies are underestimated.
6.2 Comparison to the anti-collocations
method
In a second experiment we compare the results of
our methods with the anti-collocation method de-
scribed in Section 4. We use the data set from our
previous work, which contain sentences from the
first half of the British National Corpus, with near-
synonyms from the eleven groups from Table 4.
The number of near-synonyms in each group is
higher compared with WordNet synonyms, because
they are taken from (Hayakawa, 1994), a dictionary
1. benefit, advantage, favor, gain, profit
2. low, gush, pour, run, spout, spurt, squirt, stream
3. deficient, inadequate, poor, unsatisfactory
4. afraid, aghast, alarmed, anxious, apprehensive, fearful,
frightened, scared, terror-stricken
5. disapproval, animadversion, aspersion, blame, criticism, rep-
rehension
6. mistake, blooper, blunder, boner, contretemps, error, faux
pas, goof, slip, solecism
7. alcoholic, boozer, drunk, drunkard, lush, sot
8. leave, abandon, desert, forsake
9. opponent, adversary, antagonist, competitor, enemy, foe, ri-
val
10. thin, lean, scrawny, skinny, slender, slim, spare, svelte, wil-
lowy, wiry
11. lie, falsehood, fib, prevarication, rationalization, untruth
Table 4: The near-synonym groups used in Exp2.
that explains differences between near-synonyms.
Moreover we retain only the sentences in which at
least one of the context words is in our previously
acquired knowledge-base of near-synonym colloca-
tions. That is, the anti-collocations method works
only if we know how a word in the context collo-
cates with the near-synonyms from a group. For the
sentences that do not contain collocations or anti-
collocations, it will perform no better than the base-
line, because the information needed by the method
is not available in the knowledge-base. Even if we
increase the coverage of the knowledge-base, the
anti-collocation method might still fail too often due
to words that were not included.
Table 5 presents the results of the comparison. We
used two data sets: TestSample, which includes at
most two sentences per collocation (the first two sen-
tences from the corpus); and TestAll, which includes
all the sentences with collocations as they occurred
in the corpus. The reason we chose these two tests is
not to bias the results due to frequent collocations.
The last two columns are the accuracies achieved
by our method. The second last column shows the
results of the method when the word counts are ap-
proximated with document counts. The improve-
ment over the baseline is 16 to 27 percentage points.
The improvement over the anti-collocations method
is 10 to 17 percentage points.
6.3 Comparison to supervised learning
We present the results of the supervised method
from Section 5 on the data sets used in Section 6.1.
360
Accuracy
Test set No. Base- Anti- Stat. Stat.
of line collocs method method
cases method (Docs) (Words)
Test 171 57.0% 63.3% 75.6% 73.3%
Sample
TestAll 332 48.5% 58.6% 70.0% 75.6%
Table 5: Comparison between the statistical method
from Section 3 and the anti-collocations method
from Section 4. (Exp2 data set from Section 6.2).
ML method (Weka) Features Accuracy
Decision Trees PMI scores 65.4%
Decision Rules PMI scores 65.5%
Na??ve Bayes PMI scores 52.5%
K-Nearest Neighbor PMI scores 64.5%
Kernel Density PMI scores 60.5%
Boosting (Dec. Stumps) PMI scores 67.7%
Na??ve Bayes 500 words 68.0%
Decision Trees 500 words 67.0%
Na??ve Bayes PMI + 500 words 66.5%
Boosting (Dec. Stumps) PMI + 500 words 69.2%
Table 6: Comparative results for the supervised
learning method using various ML learning algo-
rithms (Weka), averaged over the seven groups of
near-synonyms from the Exp1 data set.
As explained before, the data sets contain sentences
with a lexical gap. For each of the seven groups
of near-synonyms, the class to choose from, in or-
der to fill in the gaps is one of the near-synonyms in
each cluster. We implemented classifiers that use as
features either the PMI scores of the left and right
context with each class, or the words in the con-
text windows, or both types of features combined.
We used as features the 500 most-frequent words for
each group of near-synonyms. We report accuracies
for 10-fold cross-validation.
Table 6 presents the results, averaged for the seven
groups of near-synonyms, of several classifiers from
the Weka package (Witten and Frank, 2000). The
classifiers that use PMI features are Decision Trees,
Decision Rules, Na??ve Bayes, K-Nearest Neighbor,
Kernel Density, and Boosting a weak classifier (De-
cision Stumps ? which are shallow decision trees).
Then, a Na??ve Bayes classifier that uses only the
word features is presented, and the same type of
classifiers with both types of features. The other
classifiers from the Weka package were also tried,
but the results did not improve and these algorithms
Accuracy
Test Base- Supervised Supervised Unsuper-
set line Boosting Boosting vised
(PMI) (PMI+words) method
1. 41.7% 55.8% 57.3% 59.1%
2. 30.9% 68.1% 70.8% 61.5%
3. 70.2% 86.5% 86.7% 73.3%
4. 38.0% 66.5% 66.7% 66.0%
5. 59.5% 70.4% 71.0% 72.2%
6. 36.7% 53.0% 56.1% 52.7%
7. 37.0% 74.0% 75.8% 76.9%
AVG 44.8% 67.7% 69.2% 66.0%
Table 7: Comparison between the unsupervised sta-
tistical method from Section 3 and the supervised
method described in Section 5, on the Exp1 data set.
had difficulties in scaling up. In particular, when
using the 500 word features for each training exam-
ple, only the Na??ve Bayes algorithm was able to run
in reasonable time. We noticed that the Na??ve Bayes
classifier performs very poorly on PMI features only
(55% average accuracy), but performs very well on
word features (68% average accuracy). In contrast,
the Decision Tree classifier performs well on PMI
features, especially when using boosting with Deci-
sion Stumps. When using both the PMI scores and
the word features, the results are slightly higher. It
seems that both types of features are sufficient for
training a good classifier, but combining them adds
value.
Table 7 presents the detailed results of two of the
supervised classifiers, and repeats, for easier com-
parison, the results of the unsupervised statistical
method from Section 6.1. The supervised classifier
that uses only PMI scores performs similar to the un-
supervised method. The best supervised classifier,
that uses both types of features, performs slightly
better than the unsupervised statistical method, but
the difference is not statistically significant. We con-
clude that the results of the supervised methods and
the unsupervised statistical method are similar. An
important advantage of the unsupervised method is
that it works on any group of near-synonyms without
training.
6.4 Results obtained by human judges
We asked two human judges, native speakers of En-
glish, to guess the missing word in a random sample
of the Exp1 data set (50 sentences for each of the
361
Test set J1-J2 J1 J2 System
Agreement Acc. Acc. Accuracy
1. 72% 70% 76% 53%
2. 82% 84% 84% 68%
3. 86% 92% 92% 78%
4. 76% 82% 76% 66%
5. 76% 82% 74% 64%
6. 78% 68% 70% 52%
7. 80% 80% 90% 77%
AVG 78.5% 79.7% 80.2% 65.4%
Table 8: Results obtained by two human judges on a
random subset of the Exp1 data set.
7 groups of near-synonyms, 350 sentences in total).
The judges were instructed to choose words from the
list of near-synonyms. The choice of a word not in
the list was allowed, but not used by the two judges.
The results in Table 8 show that the agreement be-
tween the two judges is high (78.5%), but not per-
fect. This means the task is difficult, even if some
wrong senses in the test data might have made the
task easier in a few cases.
The human judges were allowed to choose more
than one correct answer when they were convinced
that more than one near-synonym fits well in the
context. They used this option sparingly, only in 5%
of the 350 sentences. Taking the accuracy achieved
of the human judges as an upper limit, the automatic
method has room for improvement (10-15 percent-
age points). In future work, we plan to allow the
system to make more than one choice when appro-
priate (for example when the second choice has a
very close score to the first choice).
7 The intelligent thesaurus
Our experiments show that the accuracy of the first
choice being the best choice is 66 to 75%; therefore
there will be cases when the writer will not choose
the first alternative. But the accuracy for the first
two choices is quite high, around 90%, as presented
in Table 9.
If the writer is in the process of writing and selects
a word to be replaced with a near-synonym proposed
by the thesaurus, then only the context on the left of
the word can be used for ordering the alternatives.
Our method can be easily adapted to consider only
the context on the left of the gap. The results of
this case are presented in Table 10, for the data sets
Test set Accuracy Accuracy
first choice first 2 choices
Exp1, AVG 66.0% 88.5%
Exp2, TestSample 73.3% 94.1%
Exp2, TestAll 75.6% 87.5%
Table 9: Accuracies for the first two choices as or-
dered by an interactive intelligent thesaurus.
Test set Accuracy Accuracy
first choice first 2 choices
Exp1, AVG 58.0% 84.8%
Exp2, TestSample 57.4% 75.1%
Exp2, TestAll 56.1% 77.4%
Table 10: Results of the statistical method when
only the left context is considered.
used in the previous sections. The accuracy values
are lower than in the case when both the left and the
right context are considered (Table 9). This is due
in part to the fact that some sentences in the test sets
have very little left context, or no left context at all.
On the other hand, many times the writer composes
a sentence or paragraph and then she/he goes back
to change a word that does not sound right. In this
case, both the left and right context will be available.
In the intelligent thesaurus, we could combine
the supervised and unsupervised method, by using
a supervised classifier when the confidence in the
classification is high, and by using the unsupervised
method otherwise. Also the unsupervised statisti-
cal method would be used for the groups of near-
synonyms for which a supervised classifier was not
previously trained.
8 Conclusion
We presented a statistical method of choosing the
best near-synonym in a context. We compared this
method to a previous method (Edmonds?s method)
and to the anti-collocation method and showed that
the performance improved considerably. We also
show that the unsupervised statistical method per-
forms comparably to a supervised learning method.
Our method based on PMI scores performs well,
despite the well-known limitations of PMI in cor-
pora. PMI tends to have problems mostly on very
small counts, but it works reasonably with larger
counts. Our web corpus is quite large, therefore the
problem of small counts does not appear.
362
In the intelligent thesaurus, we do not make the
near-synonym choice automatically, but we let the
user choose. The first choice offered by the the-
saurus is the best one quite often; the first two
choices are correct 90% of the time.
Future work includes a word sense disambigua-
tion module. In case the target word selected by the
writer has multiple senses, they could trigger sev-
eral groups of near-synonyms. The system will de-
cide which group represents the most likely senses
by computing the semantic coherence scores aver-
aged over the near-synonyms from each group.
We plan to explore the question of which inven-
tory of near-synonyms or similar words is the most
suitable for use in the intelligent thesaurus.
Choosing the right near-synonym in context is
also useful in other applications, such as natural lan-
guage generation (NLG) and machine translation.
In fact we already used the near-synonym choice
module in an NLG system, for complementing the
choices made by using the symbolic knowledge in-
corporated into the system.
References
Eneko Agirre and David Martinez. 2000. Exploring au-
tomatic word sense disambiguation with decision lists
and the Web. In Proceedings of the Workshop on Se-
mantic Annotation And Intelligent Content, COLING
2000, Saarbru?cken/Luxembourg/Nancy.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16 (1):22?29.
Charles L. A. Clarke and Egidio Terra. 2003a. Fre-
quency estimates for statistical word similarity mea-
sures. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL 2003), pages 165?172, Edmonton, Canada.
Charles L. A. Clarke and Egidio Terra. 2003b. Passage
retrieval vs. document retrieval for factoid question an-
swering. In Proceedings of the 26th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 427?428,
Toronto, Canada.
Philip Edmonds. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
Proceedings of the 35th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 507?509,
Madrid, Spain.
Gregory Grefenstette. 1999. The World Wide Web as a
resource for example-based machine translation tasks.
In Proceedings of the ASLIB Conference on Translat-
ing and Computers, London, UK.
S. I. Hayakawa, editor. 1994. Choose the Right Word.
Second Edition, revised by Eugene Ehrlich. Harper-
Collins Publishers.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge-base of near-synonym dif-
ferences. Computational Linguistics, 32 (2):223?262.
Frank Keller andMirella Lapata. 2003. Using theWeb to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29 (3):459?484.
Adam Kilgarriff and Gregory Grefenstette. 2003. Intro-
duction to the special issue on the Web as a corpus.
Computational Linguistics, 29 (3):333?347.
Adam Kilgarriff. 2001. Web as corpus. In Proceedings
of the 2001 Corpus Linguistics conference, pages 342?
345, Lancaster, UK.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics joint with 17th International Conference on Com-
putational Linguistics (ACL-COLING?98), pages 768?
774, Montreal, Quebec, Canada.
Christopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
The MIT Press, Cambridge, MA.
Rada Mihalcea and Dan Moldovan. 1999. A method for
word sense disambiguation from unrestricted text. In
Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 152?158,
Maryland, MD.
Philip Resnik. 1999. Mining the Web for bilingual text.
In Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 527?534,
Maryland, MD.
Peter Mark Roget, editor. 1852. Roget?s Thesaurus of
English Words and Phrases. Longman Group Ltd.,
Harlow, Essex, UK.
Peter Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML 2001), pages 491?502, Freiburg, Germany.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical machine learning tools with Java implemen-
tations. Morgan Kaufmann, San Francisco, CA.
363
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 441?448,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semi-Supervised Learning of Partial Cognates using  
Bilingual Bootstrapping 
 
Oana Frunza and Diana Inkpen 
 
School of Information Technology and Engineering 
University of Ottawa 
Ottawa, ON, Canada, K1N 6N5 
{ofrunza,diana}@site.uottawa.ca 
 
  
Abstract 
Partial cognates are pairs of words in two 
languages that have the same meaning in 
some, but not all contexts. Detecting the 
actual meaning of a partial cognate in 
context can be useful for Machine Trans-
lation tools and for Computer-Assisted 
Language Learning tools. In this paper 
we propose a supervised and a semi-
supervised method to disambiguate par-
tial cognates between two languages: 
French and English. The methods use 
only automatically-labeled data; therefore 
they can be applied for other pairs of lan-
guages as well. We also show that our 
methods perform well when using cor-
pora from different domains. 
1 Introduction 
When learning a second language, a student 
can benefit from knowledge in his / her first lan-
guage (Gass, 1987), (Ringbom, 1987), (LeBlanc 
et al 1989). Cognates ? words that have similar 
spelling and meaning ? can accelerate vocabu-
lary acquisition and facilitate the reading com-
prehension task. On the other  hand, a student has 
to pay attention to the pairs of words that look 
and sound similar but have different meanings ? 
false friends pairs, and especially to pairs of 
words that share meaning in some but not all 
contexts ? the partial cognates.  
Carroll (1992) claims that false friends can be 
a hindrance in second language learning. She 
suggests that a cognate pairing process between 
two words that look alike happens faster in the 
learner?s mind than a false-friend pairing. Ex-
periments with second language learners of dif-
ferent stages conducted by Van et al (1998) 
suggest that missing false-friend recognition can 
be corrected when cross-language activation is 
used ? sounds, pictures, additional explanation, 
feedback. 
   Machine Translation (MT) systems can benefit 
from extra information when translating a certain 
word in context. Knowing if a word in the source 
language is a cognate or a false friend with a 
word in the target language can improve the 
translation results. Cross-Language Information 
Retrieval systems can use the knowledge of the 
sense of certain words in a query in order to re-
trieve desired documents in the target language.  
Our task, disambiguating partial cognates, is in 
a way equivalent to coarse grain cross-language 
Word-Sense Discrimination. Our focus is disam-
biguating French partial cognates in context: de-
ciding if they are used as cognates with an 
English word, or if they are used as false friends. 
There is a lot of work done on monolingual 
Word Sense Disambiguation (WSD) systems that 
use supervised and unsupervised methods and 
report good results on Senseval data, but there is 
less work done to disambiguate cross-language 
words. The results of this process can be useful 
in many NLP tasks. 
   Although French and English belong to differ-
ent branches of the Indo-European family of lan-
guages, their vocabulary share a great number of 
similarities. Some are words of Latin and Greek 
origin: e.g., education and theory. A small num-
ber of very old, ?genetic" cognates go back all 
the way to Proto-Indo-European, e.g., m?re - 
mother and pied - foot. The majority of these 
pairs of words penetrated the French and English 
language due to the geographical, historical, and 
cultural contact between the two countries over 
441
many centuries (borrowings). Most of the bor-
rowings have changed their orthography, follow-
ing different orthographic rules (LeBlanc and 
Seguin, 1996) and most likely their meaning as 
well. Some of the adopted words replaced the 
original word in the language, while others were 
used together but with slightly or completely dif-
ferent meanings. 
   In this paper we describe a supervised and also 
a semi-supervised method to discriminate the 
senses of partial cognates between French and 
English. In the following sections we present 
some definitions, the way we collected the data, 
the methods that we used, and evaluation ex-
periments with results for both methods.   
2 Definitions  
We adopt the following definitions. The defini-
tions are language-independent, but the examples 
are pairs of French and English words, respec-
tively. 
Cognates, or True Friends (Vrais Amis), are 
pairs of words that are perceived as similar and 
are mutual translations. The spelling can be iden-
tical or not, e.g., nature - nature, reconnaissance 
- recognition. 
False Friends (Faux Amis) are pairs of words in 
two languages that are perceived as similar but 
have different meanings, e.g., main (= hand) - 
main (= principal or essential), blesser (= to in-
jure) - bless (= b?nir).  
Partial Cognates are pairs of words that have 
the same meaning in both languages in some but 
not all contexts. They behave as cognates or as 
false friends, depending on the sense that is used 
in each context. For example, in French, facteur 
means not only factor, but also mailman, while 
?tiquette can also mean label or sticker, in addi-
tion to the cognate sense. 
Genetic Cognates are word pairs in related lan-
guages that derive directly from the same word 
in the ancestor (proto-)language. Because of 
gradual phonetic and semantic changes over long 
periods of time, genetic cognates often differ in 
form and/or meaning, e.g., p?re - father, chef - 
head. This category excludes lexical borrowings, 
i.e., words transferred from one language to an-
other at some point of time, such as concierge. 
3 Related Work 
As far as we know there is no work done to dis-
ambiguate partial cognates between two lan-
guages.  
   Ide (2000) has shown on a small scale that 
cross-lingual lexicalization can be used to define 
and structure sense distinctions. Tufis et al 
(2004) used cross-lingual lexicalization, word-
nets alignment for several languages, and a clus-
tering algorithm to perform WSD on a set of 
polysemous English words. They report an accu-
racy of 74%. 
   One of the most active researchers in identify-
ing cognates between pairs of languages is 
Kondrak (2001; 2004).  His work is more related 
to the phonetic aspect of cognate identification. 
He used in his work algorithms that combine dif-
ferent orthographic and phonetic measures, re-
current sound correspondences, and some 
semantic similarity based on glosses overlap. 
Guy (1994) identified letter correspondence be-
tween words and estimates the likelihood of re-
latedness. No semantic component is present in 
the system, the words are assumed to be already 
matched by their meanings. Hewson (1993), 
Lowe and Mazadon (1994) used systematic 
sound correspondences to determine proto-
projections for identifying cognate sets.  
   WSD is a task that has attracted researchers 
since 1950 and it is still a topic of high interest. 
Determining the sense of an ambiguous word, 
using bootstrapping and texts from a different 
language was done by Yarowsky (1995),  Hearst 
(1991), Diab (2002), and Li and Li (2004).   
   Yarowsky (1995) has used a few seeds and 
untagged sentences in a bootstrapping algorithm 
based on decision lists. He added two constrains 
? words tend to have one sense per discourse and 
one sense per collocation. He reported high accu-
racy scores for a set of 10 words. The monolin-
gual bootstrapping approach was also used by 
Hearst (1991), who used a small set of hand-
labeled data to bootstrap from a larger corpus for 
training a noun disambiguation system for Eng-
lish. Unlike Yarowsky (1995), we use automatic 
collection of seeds. Besides our monolingual 
bootstrapping technique, we also use bilingual 
bootstrapping. 
   Diab (2002) has shown that unsupervised WSD 
systems that use parallel corpora can achieve 
results that are close to the results of a supervised 
approach. She used parallel corpora in French, 
English, and Spanish, automatically-produced 
with MT tools to determine cross-language lexi-
calization sets of target words. The major goal of 
her work was to perform monolingual English 
WSD. Evaluation was performed on the nouns 
from the English all words data in Senseval2. 
Additional knowledge was added to the system 
442
from WordNet in order to improve the results. In 
our experiments we use the parallel data in a dif-
ferent way: we use words from parallel sentences 
as features for Machine Learning (ML). Li and 
Li (2004) have shown that word translation and 
bilingual bootstrapping is a good combination for 
disambiguation. They were using a set of 7 pairs 
of Chinese and English words. The two senses of 
the words were highly distinctive: e.g. bass as 
fish or music; palm as tree or hand. 
Our work described in this paper shows that 
monolingual and bilingual bootstrapping can be 
successfully used to disambiguate partial cog-
nates between two languages. Our approach dif-
fers from the ones we mentioned before not only 
from the point of human effort needed to anno-
tate data ? we require almost none, and from the 
way we use the parallel data to automatically 
collect training examples for machine learning, 
but also by the fact that we use only off-the-shelf 
tools and resources: free MT and ML tools, and 
parallel corpora. We show that a combination of 
these resources can be used with success in a task 
that would otherwise require a lot of time and 
human effort.  
4 Data for Partial Cognates 
We performed experiments with ten pairs of par-
tial cognates. We list them in Table 1. For a 
French partial cognate we list its English cognate 
and several false friends in English. Often the 
French partial cognate has two senses (one for 
cognate, one for false friend), but sometimes it 
has more than two senses: one for cognate and 
several for false friends (nonetheless, we treat 
them together). For example, the false friend 
words for note have one sense for grades and one 
for bills. 
The partial cognate (PC), the cognate (COG) 
and false-friend (FF) words were collected from 
a web resource1. The resource contained a list of 
400 false-friends with 64 partial cognates. All 
partial cognates are words frequently used in the 
language. We selected ten partial cognates pre-
sented in Table 1 according to the number of ex-
tracted sentences (a balance between the two 
meanings), to evaluate and experiment our pro-
posed methods. 
The human effort that we required for our 
methods was to add more false-friend English 
words, than the ones we found in the web re-
source. We wanted to be able to distinguish the 
                                                          
1 http://french.about.com/library/fauxamis/blfauxam_a.htm 
senses of cognate and false-friends for a wider 
variety of senses. This task was done using a bi-
lingual dictionary2.  
 
Table 1. The ten pairs of partial cognates. 
French par-
tial cognate 
English  
cognate 
English false friends 
blanc blank white, livid 
circulation circulation traffic 
client client customer, patron, patient, 
spectator, user, shopper 
corps corps body, corpse 
d?tail detail retail 
mode mode fashion, trend, style, 
vogue 
note note mark, grade, bill, check,  
account 
police police policy, insurance, font, 
face 
responsable responsi-
ble 
in charge, responsible 
party, official, representa-
tive, person in charge, 
executive, officer  
route route road, roadside 
 
4.1 Seed Set Collection 
Both the supervised and the semi-supervised 
method that we will describe in Section 5 are 
using a set of seeds. The seeds are parallel sen-
tences, French and English, which contain the 
partial cognate. For each partial-cognate word, a 
part of the set contains the cognate sense and 
another part the false-friend sense.  
As we mentioned in Section 3, the seed sen-
tences that we use are not hand-tagged with the 
sense (the cognate sense or the false-friend 
sense); they are automatically annotated by the 
way we collect them. To collect the set of seed 
sentences we use parallel corpora from Hansard3, 
and EuroParl4, and the, manually aligned BAF 
corpus.5  
The cognate sense sentences were created by 
extracting parallel sentences that had on the 
French side the French cognate and on the Eng-
lish side the English cognate. See the upper part 
of Table 2 for an example. 
     The same approach was used to extract sen-
tences with the false-friend sense of the partial 
cognate, only this time we used the false-friend 
English words. See lower the part of Table 2. 
                                                          
2 http://www.wordreference.com 
3 http://www.isi.edu/natural-language/download/hansard/   
   and  http://www.tsrali.com/ 
4 http://people.csail.mit.edu/koehn/publications/europarl/ 
5 http://rali.iro.umontreal.ca/Ressources/BAF/  
443
Table 2. Example sentences from parallel corpus. 
Fr 
(PC:COG) 
Je note, par exemple, que l'accus? a fait 
une autre d?claration tr?s incriminante ? 
Hall environ deux mois plus tard. 
En 
(COG) 
I note, for instance, that he made another 
highly incriminating statement to Hall 
two months later. 
Fr 
(PC:FF) 
S'il g?le les gens ne sont pas capables de 
r?gler leur note de chauffage 
En 
(FF) 
If there is a hard frost, people are unable 
to pay their bills. 
 
   To keep the methods simple and language-
independent, no lemmatization was used. We 
took only sentences that had the exact form of 
the French and English word as described in Ta-
ble 1. Some improvement might be achieved 
when using lemmatization. We wanted to see 
how well we can do by using sentences as they 
are extracted from the parallel corpus, with no 
additional pre-processing and without removing 
any noise that might be introduced during the 
collection process. 
From the extracted sentences, we used 2/3 of 
the sentences for training (seeds) and 1/3 for test-
ing when applying both the supervised and semi-
supervised approach. In Table 3 we present the 
number of seeds used for training and testing.  
We will show in Section 6, that even though 
we started with a small amount of seeds from a 
certain domain ? the nature of the parallel corpus 
that we had, an improvement can be obtained in  
discriminating the senses of partial cognates us-
ing free text from other domains.  
 
Table 3. Number of parallel sentences used as seeds. 
Partial 
Cognates 
Train 
CG 
Train 
FF 
Test 
CG 
Test 
FF 
Blanc 54 78 28 39 
Circulation 213 75 107 38 
Client 105 88 53 45 
Corps 88 82 44 42 
D?tail 120 80 60 41 
Mode 76 104 126 53 
Note 250 138 126 68 
Police 154 94 78 48 
Responsable 200 162 100 81 
Route 69 90 35 46 
AVERAGE 132.9 99.1 66.9 50.1 
 
5 Methods 
In this section we describe the supervised and the 
semi-supervised methods that we use in our ex-
periments. We will also describe the data sets 
that we used for the monolingual and bilingual 
bootstrapping technique.  
   For both methods we have the same goal: to 
determine which of the two senses (the cognate 
or the false-friend sense) of a partial-cognate 
word is present in a test sentence. The classes in 
which we classify a sentence that contains a par-
tial cognate are: COG (cognate) and FF (false-
friend). 
5.1 Supervised Method 
For both the supervised and semi-supervised 
method we used the bag-of-words (BOW) ap-
proach of modeling context, with binary values 
for the features. The features were words from 
the training corpus that appeared at least 3 times 
in the training sentences. We removed the stop-
words from the features. A list of stopwords for 
English and one for French was used. We ran 
experiments when we kept the stopwords as fea-
tures but the results did not improve.  
Since we wanted to learn the contexts in which 
a partial cognate has a cognate sense and the con-
texts in which it has a false-friend sense, the cog-
nate and false friend words were not taken into 
account as features. Leaving them in would mean 
to indicate the classes, when applying the 
methods for the English sentences since all the 
sentences with the cognate sense contain the cog-
nate word and all the false-friend sentences do 
not contain it. For the French side all collected 
sentences contain the partial cognate word, the 
same for both senses.  
As a baseline for the experiments that we pre-
sent we used the ZeroR classifier from WEKA6, 
which predicts the class that is the most frequent 
in the training corpus. The classifiers for which 
we report results are: Na?ve Bayes with a kernel 
estimator, Decision Trees - J48, and a Support 
Vector Machine implementation - SMO. All the 
classifiers can be found in the WEKA package. 
We used these classifiers because we wanted to 
have a probabilistic, a decision-based and a func-
tional classifier. The decision tree classifier al-
lows us to see which features are most 
discriminative. 
Experiments were performed with other classi-
fiers and with different levels of tuning, on a 10-
fold cross validation approach as well; the classi-
fiers we mentioned above were consistently the 
ones that obtained the best accuracy results.   
The supervised method used in our experi-
ments consists in training the classifiers on the 
                                                          
6 http://www.cs.waikato.ac.nz/ml/weka/ 
444
automatically-collected training seed sentences, 
for each partial cognate, and then test their per-
formance on the testing set. Results for this 
method are presented later, in Table 5. 
5.2 Semi-Supervised Method 
For the semi-supervised method we add unla-
belled examples from monolingual corpora: the 
French newspaper LeMonde7 1994, 1995 (LM), 
and the BNC8 corpus, different domain corpora 
than the seeds. The procedure of adding and us-
ing this unlabeled data is described in the Mono-
lingual Bootstrapping (MB) and Bilingual 
Bootstrapping (BB) sections.  
5.2.1  Monolingual Bootstrapping 
The monolingual bootstrapping algorithm that 
we used for experiments on French sentences 
(MB-F) and on English sentences (MB-E) is:  
 
For each pair of partial cognates (PC)  
1. Train a classifier on the training seeds ? us-
ing the BOW approach and a NB-K classifier 
with attribute selection on the features. 
2. Apply the classifier on unlabeled data ? 
sentences that contain the PC word, extracted 
from LeMonde (MB-F) or from BNC (MB-E)  
3. Take the first k newly classified sentences, 
both from the COG and FF class and add 
them to the  training seeds  (the most confident 
ones ? the  prediction  accuracy greater or 
equal than a threshold =0.85) 
4. Rerun the experiments training on the new 
training set 
5. Repeat steps 2 and 3 for t times  
   endFor 
 
For the first step of the algorithm we used NB-K 
classifier because it was the classifier that consis-
tently performed better. We chose to perform 
attribute selection on the features after we tried 
the method without attribute selection. We ob-
tained better results when using attribute selec-
tion. This sub-step was performed with the 
WEKA tool, the Chi-Square attribute selection 
was chosen. 
In the second step of the MB algorithm the 
classifier that was trained on the training seeds 
was then used to classify the unlabeled data that 
was collected from the two additional resources. 
For the MB algorithm on the French side we 
trained the classifier on the French side of the 
                                                          
7 http://www.lemonde.fr/ 
8 http://www.natcorp.ox.ac.uk/ 
training seeds and then we applied the classifier 
to classify the sentences that were extracted from 
LeMonde and contained the partial cognate. The 
same approach was used for the MB on the Eng-
lish side only this time we were using the English 
side of the training seeds for training the classi-
fier and the BNC corpus to extract new exam-
ples. In fact, the MB-E step is needed only for 
the BB method. 
Only the sentences that were classified with a 
probability greater than 0.85 were selected for 
later use in the bootstrapping algorithm.  
   The number of sentences that were chosen 
from the new corpora and used in the first step of 
the MB and BB are presented in Table 4. 
 
Table 4. Number of sentences selected from the 
LeMonde and BNC corpus. 
PC LM 
COG 
LM 
FF 
BNC 
COG 
BNC 
FF 
Blanc 45 250 0 241 
Circulation 250 250 70 180 
Client 250 250 77 250 
Corps 250 250 131 188 
D?tail 250 163 158 136 
Mode 151 250 176 262 
Note 250 250 178 281 
Police 250 250 186 200 
Responsable 250 250 177 225 
Route 250 250 217 118 
 
For the partial-cognate Blanc with the cognate 
sense, the number of sentences that had a prob-
ability distribution greater or equal with the 
threshold was low. For the rest of partial cog-
nates the number of selected sentences was lim-
ited by the value of parameter k in the algorithm.  
5.2.2   Bilingual Bootstrapping 
The algorithm for bilingual bootstrapping that we 
propose and tried in our experiments is: 
 
1. Translate the English sentences that were col-
lected in the MB-E step into French using an 
online MT9 tool and add them to the French seed 
training data.  
2.  Repeat the MB-F and MB-E steps for T times. 
 
For the both monolingual and bilingual boot-
strapping techniques the value of the parameters 
t and T is 1 in our experiments. 
                                                          
9 http://www.freetranslation.com/free/web.asp 
445
6 Evaluation and Results 
In this section we present the results that we 
obtained with the supervised and semi-
supervised methods that we applied to disam-
biguate partial cognates. 
Due to space issue we show results only for 
testing on the testing sets and not for the 10-fold 
cross validation experiments on the training data. 
For the same reason, we present the results that 
we obtained only with the French side of the par-
allel corpus, even though we trained classifiers 
on the English sentences as well. The results for 
the 10-fold cross validation and for the English 
sentences are not much different than the ones 
from Table 5 that describe the supervised method 
results on French sentences. 
 
   Table 5. Results for the Supervised Method.    
PC ZeroR NB-K Trees SMO 
Blanc 58% 95.52% 98.5% 98.5% 
Circulation 74% 91.03% 80% 89.65% 
Client 54.08% 67.34% 66.32% 61.22% 
Corps 51.16% 62% 61.62% 69.76% 
D?tail 59.4% 85.14% 85.14% 87.12% 
Mode 58.24% 89.01% 89.01% 90% 
Note 64.94% 89.17% 77.83% 85.05% 
Police 61.41% 79.52% 93.7% 94.48% 
Responsable 55.24% 85.08% 70.71% 75.69% 
Route 56.79% 54.32% 56.79% 56.79% 
AVERAGE 59.33% 80.17% 77.96% 80.59% 
 
Table 6 and Table 7 present results for the MB 
and BB. More experiments that combined MB 
and BB techniques were also performed. The 
results are presented in Table 9. 
   Our goal is to disambiguate partial cognates 
in general, not only in the particular domain of 
Hansard and EuroParl. For this reason we used 
another set of automatically determined sen-
tences from a multi-domain parallel corpus. 
The set of new sentences (multi-domain) was 
extracted in the same manner as the seeds from 
Hansard and EuroParl. The new parallel corpus 
is a small one, approximately 1.5 million words, 
but contains texts from different domains: maga-
zine articles, modern fiction, texts from interna-
tional organizations and academic textbooks. We 
are using this set of sentences in our experiments 
to show that our methods perform well on multi-
domain corpora and also because our aim is to be 
able to disambiguate PC in different domains. 
From this parallel corpus we were able to extract 
the number of sentences shown in Table 8. 
With this new set of sentences we performed 
different experiments both for MB and BB. All 
results are described in Table 9. Due to space 
issue we report the results only on the average 
that we obtained for all the 10 pairs of partial 
cognates.  
The symbols that we use in Table 9 represent:  
S ? the seed training corpus, TS ? the seed test 
set,  BNC and LM ? sentences extracted from 
LeMonde and BNC (Table 4), and NC ? the sen-
tences that were extracted from the multi-domain 
new corpus. When we use the + symbol we put 
together all the sentences extracted from the re-
spective corpora. 
 
Table 6. Monolingual Bootstrapping on the French side. 
PC ZeroR NB-K Dec.Tree SMO 
Blanc 58.20% 97.01% 97.01% 98.5% 
Circulation 73.79% 90.34% 70.34% 84.13% 
Client 54.08% 71.42% 54.08% 64.28% 
Corps 51.16% 78% 56.97% 69.76% 
D?tail 59.4% 88.11% 85.14% 82.17% 
Mode 58.24% 89.01% 90.10% 85% 
Note 64.94% 85.05% 71.64% 80.41% 
Police 61.41% 71.65% 92.91% 71.65% 
Responsable 55.24% 87.29% 77.34% 81.76% 
Route 56.79% 51.85% 56.79% 56.79% 
AVERAGE 59.33% 80.96% 75.23% 77.41% 
 
Table 7. Bilingual Bootstrapping. 
PC ZeroR NB-K Dec.Tree SMO 
Blanc 58.2% 95.52% 97.01% 98.50% 
Circulation 73.79% 92.41% 63.44% 87.58% 
Client 45.91% 70.4% 45.91% 63.26% 
Corps 48.83% 83% 67.44% 82.55% 
D?tail 59% 91.08% 85.14% 86.13% 
Mode 58.24% 87.91% 90.1% 87% 
Note 64.94% 85.56% 77.31% 79.38% 
Police 61.41% 80.31% 96.06% 96.06% 
Responsable 44.75% 87.84% 74.03% 79.55% 
Route 43.2% 60.49% 45.67% 64.19% 
AVERAGE 55.87% 83.41% 74.21% 82.4% 
 
 
446
Table 8. New Corpus (NC) sentences. 
PC COG FF 
Blanc 18 222 
Circulation 26 10 
Client 70 44 
Corps 4 288 
D?tail 50 0 
Mode 166 12 
Note 214 20 
Police 216 6 
Responsable 104 66 
Route 6 100 
 
6.1  Discussion of the Results
The results of the experiments and the methods 
that we propose show that we can use with suc-
cess unlabeled data to learn from, and that the 
noise that is introduced due to the seed set collec-
tion is tolerable by the ML techniques that we 
use.  
Some results of the experiments we present in 
Table 9 are not as good as others. What is impor-
tant to notice is that every time we used MB or 
BB or both, there was an improvement. For some 
experiments MB did better, for others BB was 
the method that improved the performance; 
nonetheless for some combinations MB together 
with BB was the method that worked best.  
In Tables 5 and 7 we show that BB improved 
the results on the NB-K classifier with 3.24%, 
compared with the supervised method (no boot-
strapping), when we tested only on the test set 
(TS), the one that represents 1/3 of the initially-
collected parallel sentences. This improvement is 
not statistically significant, according to a t-test.  
In Table 9 we show that our proposed methods 
bring improvements for different combinations 
of training and testing sets. Table 9, lines 1 and 2 
show that BB with NB-K brought an improve-
ment of 1.95% from no bootstrapping, when we 
tested on the multi-domain corpus NC. For the 
same setting, there was an improvement of 
1.55% when we tested on TS (Table 9, lines 6 
and 8). When we tested on the combination 
TS+NC, again BB brought an improvement of 
2.63% from no bootstrapping (Table 9, lines 10 
and 12). The difference between MB and BB 
with this setting is 6.86% (Table 9, lines 11 and 
12). According to a t-test the 1.95% and 6.86% 
improvements are statistically significant. 
 Table 9. Results for different experiments with 
monolingual and bilingual bootstrapping (MB and 
BB).  
Train Test ZeroR NB-K Trees SMO 
S (no 
bootstrapping) 
NC 67% 71.97% 73.75% 76.75%
S+BNC 
(BB) 
NC 64% 73.92% 60.49% 74.80%
S+LM 
(MB) 
NC 67.85% 67.03% 64.65% 65.57%
S +LM+BNC 
(MB+BB) 
NC 64.19% 70.57% 57.03% 66.84%
S+LM+BNC 
(MB+BB) 
TS 55.87% 81.98% 74.37% 78.76%
S+NC 
(no bootstr.) 
TS 57.44% 82.03% 76.91% 80.71%
S+NC+LM 
(MB) 
TS 57.44% 82.02% 73.78% 77.03%
S+NC+BNC 
(BB) 
TS 56.63% 83.58% 68.36% 82.34%
S+NC+LM+ 
BNC(MB+BB)
TS 58% 83.10% 75.61% 79.05%
S (no bootstrap-
ping) 
TS+NC 62.70% 77.20% 77.23% 79.26%
S+LM 
(MB) 
TS+NC 62.70% 72.97% 70.33% 71.97%
S+BNC 
(BB) 
TS+NC 61.27% 79.83% 67.06% 78.80%
S+LM+BNC 
(MB+BB) 
TS+NC 61.27% 77.28% 65.75% 73.87%
 
    The number of features that were extracted 
from the seeds was more than double at each MB 
and BB experiment, showing that even though 
we started with seeds from a language restricted 
domain, the method is able to capture knowledge 
form different domains as well. Besides the 
change in the number of features, the domain of 
the features has also changed form the parlia-
mentary one to others, more general, showing 
that the method will be able to disambiguate sen-
tences where the partial cognates cover different 
types of context.  
Unlike previous work that has done with 
monolingual or bilingual bootstrapping, we tried 
to disambiguate not only words that have senses 
that are very different e.g. plant ? with a sense of 
biological plant or with the sense of factory. In 
our set of partial cognates the French word route 
is a difficult word to disambiguate even for hu-
mans: it has a cognate sense when it refers to a 
maritime or trade route and a false-friend sense 
when it is used as road. The same observation 
applies to client (the cognate sense is client, and 
the false friend sense is customer, patron, or pa-
tient) and to circulation (cognate in air or blood 
circulation, false friend in street traffic).  
447
7 Conclusion and Future Work 
We showed that with simple methods and using 
available tools we can achieve good results in the 
task of partial cognate disambiguation. 
   The accuracy might be increased by using de-
pendencies relations, lemmatization, part-of-
speech tagging ? extract sentences where the par-
tial cognate has the same POS, and other types of 
data representation combined with different se-
mantic tools (e.g. decision lists, rule based sys-
tems).  
In our experiments we use a machine language 
representation ? binary feature values, and we 
show that nonetheless machines are capable of 
learning from new information, using an iterative 
approach, similar to the learning process of hu-
mans. New information was collected and ex-
tracted by classifiers when additional corpora 
were used for training. 
   In addition to the applications that we men-
tioned in Section 1, partial cognates can also be 
useful in Computer-Assisted Language Learning 
(CALL) tools. Search engines for E-Learning can 
find useful a partial cognate annotator. A teacher 
that prepares a test to be integrated into a CALL 
tool can save time by using our methods to 
automatically disambiguate partial cognates, 
even though the automatic classifications need to 
be checked by the teacher.  
In future work we plan to try different repre-
sentations of the data, to use knowledge of the 
relations that exists between the partial cognate 
and the context words, and to run experiments 
when we iterate the MB and BB steps more than 
once. 
References  
Susane Carroll 1992. On Cognates. Second Language 
Research, 8(2):93-119 
Mona Diab and Philip Resnik. 2002. An unsupervised 
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Meeting of the As-
sociation for Computational Linguistics (ACL 
2002), Philadelphia, pp. 255-262. 
S. M. Gass. 1987. The use and acquisition of the sec-
ond language lexicon (Special issue). Studies in 
Second Language Acquisition, 9 (2).  
Jacques B. M. Guy. 1994. An algorithm for identify-
ing cognates in bilingual word lists and its applica-
bility to machine translation. Journal of 
Quantitative Linguistics, 1(1):35-42. 
Marti Hearst 1991. Noun homograph disambiguation 
using local context in large text corpora. 7th An-
nual Conference of the University of Waterloo 
Center for the new OED and Text Research, Ox-
ford. 
W.J.B Van Heuven, A. Dijkstra, and J. Grainger. 
1998.  Orthographic neighborhood effects in bilin-
gual word recognition. Journal of Memory and 
Language 39: 458-483. 
John Hewson 1993. A Computer-Generated Diction-
ary of Proto-Algonquian. Ottawa: Canadian Mu-
seum of Civilization. 
Nancy Ide. 2000 Cross-lingual sense determination: 
Can it work? Computers and the Humanities, 34:1-
2, Special Issue on the Proceedings of the SIGLEX 
SENSEVAL Workshop, pp.223-234. 
Grzegorz Kondrak. 2004. Combining Evidence in 
Cognate Identification. Proceedings of Canadian 
AI 2004: 17th Conference of the Canadian Society 
for Computational Studies of Intelligence, pp.44-
59.  
Grzegorz Kondrak. 2001. Identifying Cognates by 
Phonetic and Semantic Similarity. Proceedings of 
NAACL 2001: 2nd Meeting of the North American 
Chapter of the Association for Computational Lin-
guistics, pp.103-110. 
Raymond LeBlanc and Hubert S?guin. 1996. Les 
cong?n?res homographes et parographes anglais-
fran?ais. Twenty-Five Years of Second Language 
Teaching at the University of Ottawa, pp.69-91.  
Hang Li and Cong Li. 2004. Word translation disam-
biguation using bilingual bootstrap. Computational 
Linguistics, 30(1):1-22. 
John B. Lowe and Martine Mauzaudon. 1994. The 
reconstruction engine: a computer implementation 
of the comparative method. Computational Lin-
guistics, 20:381-417. 
Hakan Ringbom. 1987. The Role of the First Lan-
guage in Foreign Language Learning. Multilingual 
Matters Ltd., Clevedon, England. 
Dan Tufis, Ion Radu, Nancy Ide 2004. Fine-Grained 
Word Sense Disambiguation Based on Parallel 
Corpora, Word Alignment, Word Clustering and 
Aligned WordNets. Proceedings of the 20th Inter-
national Conference on Computational Linguistics, 
COLING 2004, Geneva, pp. 1312-1318. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proceedings of the 33th Annual Meeting of the As-
sociation for Computational Linguistics, Cam-
bridge, MA, pp 189-196. 
448
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 110?111,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Textual Information for Predicting Functional Properties of the Genes 
 
Oana Frunza and Diana Inkpen 
School of Information Technology and Engineering 
University of Ottawa Ottawa, ON, Canada, K1N 6N5 
{ofrunza,diana}@site.uottawa.ca 
 
1 Overview 
This paper is focused on determining which pro-
teins affect the activity of Aryl Hydrocarbon Re-
ceptor (AHR) system when learning a model that 
can accurately predict its activity when single 
genes are knocked out. Experiments with results 
are presented when models are trained on a single 
source of information: abstracts from Medline 
(http://medline.cos.com/) that talk about the genes in-
volved in the experiments. The results suggest that 
AdaBoost classifier with a binary bag-of-words 
representation obtains significantly better results. 
2 Task Description and Data Sets 
The task that we address is a biology-specific task 
considered a competition track for KDDCup2002 
(http://www.biostat.wisc.edu/~craven/kddcup/winners.html).  
   The organizers of the KDD Cup competition 
provided data obtained from experiments per-
formed on a set of yeast strains in which each 
strain contains a single gene that is knocked out (a 
gene sequence in which a single gene is inopera-
tive). Each experiment had associated a discretized 
value of the activity of the AHR system when a 
single gene was knocked out. 3 possible classes 
describe the systems? response. The "nc" label in-
dicates that the activity of the hidden system was 
not significantly different than the baseline (the 
wild-type yeast); the "control" label indicates that 
the activity was significantly different than the 
baseline for the given instance, and that the activity 
of another hidden system (the control) was also 
significantly changed compared to its baseline; the 
"change" label shows that the activity of the hid-
den system was significantly changed, but the ac-
tivity of the control system was not significantly 
changed. 
   The organizers of the KDD Cup evaluate the task 
as a two-class problem with focus on the positive 
class. The first definition is called the ?narrow? 
definition of the positive class and it is specific to 
the knocked-out genes that had an AHR-specific 
effect. In this case the positive class is defined by 
the experiments in which the label of the system is 
?change? and the negative examples are the ex-
periments that consist of those genes with either 
the "nc" or the "control" label. The second defini-
tion consists of those genes labeled with either the 
"change" or the "control" label. The negative class 
consists of those genes labeled with the "nc" label. 
The second partitioning corresponds to the 
?broad? characterization of the positive class 
genes that affect the hidden system.  
   The area under the Receiver Operating Charac-
teristic (ROC) - AUC curve is chosen as an evalua-
tion measure. The global score for the task will be 
the summed AUC values for both the ?narrow? and 
the ?broad? partition of the data. 
   The sources of information provided by the or-
ganizers of the task contain: hierarchical informa-
tion about the function and localization of the 
genes; relational information describing the pro-
tein-protein interactions; and textual information in 
abstracts from Medline that talk about the genes. 
Some characteristics of the data need to be taken 
into consideration in order to make suitable deci-
sions for choosing the trainable system/classifier, 
the representation of the data, etc. Missing infor-
mation is a characteristic of the data set. Not all 
genes had the location and function annotation, the 
protein-protein interaction information, or abstracts 
associated with the gene name. Besides the missing 
information, the high class imbalance is another 
fact that needs to be taken into account.  
   From the data that was released for the KDD 
competition we run experiments only with the 
genes that had associated abstracts. Table 1 pre-
sents a summary of the data sets used in our ex-
periments after considering only the genes that had 
abstracts associated with them. The majority of the 
genes had one abstract, while others had as many 
as 22 abstracts.   
110
Table 1. Summary of the data for our experiments with 
the two definitions of the positive class. In brackets are 
the original sizes of the data sets. 
Narrow Broad Data 
set Pos Neg Pos Neg 
Training 24 
(37) 
1,435 
(2,980) 
51 
(83) 
1,408 
(2,934) 
Test 11 
(19) 
715 
(1,469) 
30 
(43) 
696 
(1,445) 
3 Related Work  
Previous research on the task was done by the 
teams that participated in the KDD Cup 2002. The 
textual information available in the task was con-
sidered as an auxiliary source of information and 
not the primary one, as in this article.  
   The winners of the task, Kowalczyk and Raskutti 
(2002) used the textual information as additional 
features to the ones extracted from other available 
information for the genes. They used a ?bag-of-
words? representation, removed stop words and 
words with low frequency. They used Support 
Vector machine (SVM) as a classifier.  
   Krogel et. al. (2002) used the textual information  
with an information extraction system in order to 
extract missing information (function, localization, 
protein class) for the genes in the released data set.  
   Vogel and Axelrod (2002) used the Medline ab-
stracts to extract predictive keywords, and added 
them to their global system. 
   Our study investigates and suggests a textual rep-
resentation and a trainable model suitable for this 
task and similar tasks in the biomedical domain. 
4 Method  
The method that we propose to solve the biology 
task is using Machine Learning (ML) classifiers 
suitable for a text classification task and various 
feature representations that are known to work well 
for data sets with high class imbalance. The task 
becomes a two-class classification: ?Positive? ver-
sus ?Negative?, with a ?narrow? and ?broad? 
definition for the positive class. As classification 
algorithms we used: Complement Naive Bayes 
(CNB), AdaBoost, and SVM all from the Weka 
toolkit (http://www.cs.waikato.ac.nz/ml/weka/). Similar to 
the evaluation done for the KDD Cup, we consider 
the sum of the 2 AUC measures for the definitions 
of the positive class as an evaluation score. The 
random classifier with an AUC measure of 0.5 is 
considered as a baseline.  
As a representation technique we used binary 
and frequency values for features that are: words 
extracted from the abstracts (bag-of-words (BOW) 
representation), UMLS concepts and UMLS 
phrases identified using the MetaMap system 
(http://mmtx.nlm.nih.gov/), and UMLS relations ex-
tracted from the UMLS metathesaurus. We also 
ran experiments with feature selection techniques.  
   Table 2 presents our best results using AdaBoost 
classifier for BOW, UMLS concepts, and UMLS 
relations representation techniques. ?B? stands for 
binary and ?Freq? stands for frequency counts. 
 
Table 2. Sum of the AUC results for the two classes 
without feature selection.  
Represen- 
tation 
AdaBoost 
(AUC) 
Narrow 
AdaBoost 
(AUC) 
Broad 
Sumed 
AUC 
BOW_B 0.613 0.598 1.211 
BOW_Freq 0.592 0.557 1.149 
UMLS_B 0.571 0.607 1.178 
UMLS_Freq 0.5 0.606 1.106 
UMLS_Rel_B 0.505 0.547 1.052 
UMLS_Rel_Freq 0.5 0.5 1 
5 Discussion and Conclusion 
Looking at the obtained results, a general conclu-
sion can be made: textual information is useful for 
biology-specific tasks. Not only that it can improve 
the results but can also be considered a stand-alone 
source of knowledge in this domain. Without any 
additional knowledge, our result of 1.21 AUC sum 
is comparable with the sum of 1.23 AUC obtained 
by the winners of the KDD competition.  
References  
Adam Kowalczyk and Bhavani Raskutti, 2002. One 
Class SVM for Yeast Regulation Prediction, ACM 
SIGKDD Explorations Newsletter, Volume 4, Issue 
2, pp. 99-100. 
Mark A Krogel, Marcus Denecke, Marco Landwehr, 
and Tobias Scheffer. 2002. Combining data and text 
mining techniques for yeast gene regulation predic-
tion: a case study, ACM SIGKDD Explorations 
Newsletter, Volume 4, Issue 2, pp. 104-105. 
David S. Vogel and Randy C. Axelrod. 2002. Predicting 
the Effects of Gene Deletion, ACM SIGKDD Explo-
rations Newsletter, Volume 4, Issue 2, pp. 101-103.    
111
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 59?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Visual Development Process for  
Automatic Generation of Digital Games Narrative Content  
 
Maria Fernanda Caropreso1 Diana Inkpen1 Shahzad Khan2 Fazel Keshtkar1 
 
1University of Ottawa 
{caropres,diana}@site.uottawa.ca 
akesh081@uottawa.ca 
2DISTIL Interactive 
s.khan2@distilinteractive.com 
 
Abstract 
Users of Natural Language Generation 
systems are required to have sophisti-
cated linguistic and sometimes even pro-
gramming knowledge, which has hin-
dered the adoption of this technology by 
individuals outside the computational 
linguistics research community. We have 
designed and implemented a visual envi-
ronment for creating and modifying NLG 
templates which requires no program-
ming ability and minimum linguistic 
knowledge. It allows specifying tem-
plates with any number of variables and 
dependencies between them. Internally, it 
uses SimpleNLG to provide the linguistic 
background knowledge. We tested the 
performance of our system in the context 
of an interactive simulation game. We 
describe the templates used for testing 
and show examples of sentences that our 
system generates from these templates. 
1 Introduction 
Natural Language Generation (NLG) is the proc-
ess of constructing outputs from non-linguistic 
inputs (Bateman, 2002) (Dalianis, 1996) (Reiter 
and Dale, 2000). 
NLG systems are useful in systems in which 
verbal or textual interaction with the users is re-
quired, as for example Gaming, Robotics, and 
Automatic Help Desks. Using NLG systems in-
stead of manually authored sentences would en-
able the software to adapt the expressed mes-
sages to the context of the conversation, and ex-
press past and future actions that may form this 
interaction. 
However, the use of the available NLG sys-
tems is far from simple. The most complete sys-
tems often require extensive linguistic knowl-
edge. Some systems also require programming 
knowledge. This knowledge cannot be assumed 
for the content and subject matter experts who 
are members of a development team. However, 
these individuals do need to interact with the 
NLG system in order to make use of the message 
generation capability to support their product 
development efforts. It is then necessary to pro-
vide them with an environment that will allow 
them to have access in a simpler way to the fea-
tures they need of a specific NLG system.  
There are two widely adopted approaches to 
NLG, the ?deep-linguistic? and the ?template-
based? (van Deemter et al, 2005). The deep-
linguistic approach attempts to build the sen-
tences up from a wholly logical representation. 
The template-based NLG systems provide scaf-
folding in the form of templates that contain a 
predefined structure and perhaps some of the 
final text.  
SimpleNLG is an NLG system that allows the 
user to specify a sentence by giving its content 
words and its grammatical roles (such as subject 
or verb). SimpleNLG also permits the user to 
specify several features for the main verb, such 
as: tense (present, past or future); whether or not 
it is subjective, progressive, passive or perfect; 
whether or not it is in interrogative form; wheth-
er or not it is negated; and which, if any, modal 
to use (i.e. could, must).While some of these fea-
tures affect only the verb, others affect the struc-
ture of the whole sentence, as for example when 
it has to be expressed in the passive voice.  
SimpleNLG is implemented as a java library 
and it requires java programming knowledge to 
be used. Because of the programming nature of 
SimpleNLG, it allows the user to define flexible 
templates by using programming variables in the 
sentence specification. The variable parts of the 
templates could be filled with different values. 
When templates are defined using SimpleNLG 
they keep all the functionality of the NLG system 
(for example, being able to modify the verb fea-
59
tures or the output format, and making use of the 
grammatical knowledge), while also allowing for 
the variable values to change. 
We have designed an environment that pro-
vides simple access to the use of the SimpleNLG 
system in order to generate sentences with vari-
able parts or templates. We developed this NLG 
Template Authoring Environment guided by the 
need of templates required for generating content 
for digital-based training games at DISTIL Inter-
active1. An early prototype of the tool, with a 
text-only interface, is presented in (Caropreso et 
al., 2009). 
In training games the player is typically pre-
sented with challenging situations and is encour-
aged to practice different strategies at dealing 
with them, in a safe, virtual environment. 
Through tips and feedback, the player develops 
an understanding of the problem and what are the 
successful ways of confronting it (French et al, 
1999). 
In training games there is usually an explosion 
of possible scenarios and situations. The narra-
tive should ideally reflect the past events and 
decisions taken. The considerable amount of tex-
tual information required in order to keep the 
feedback consistent with the updated narrative 
can be a burden on the game designers. It is then 
necessary to include templates that statically 
provide the basic information, combined with 
variable parts that adapt the narrative to the cir-
cumstances. 
The goal of the NLG Template Authoring En-
vironment was to provide the game content de-
signers with an accessible tool they could use to 
create and manipulate the NLG templates, and 
thus generate sentences that would support the 
narrative progression of the game.  
In the rest of this paper we describe our NLG 
Template Authoring Environment, its design, 
implementation and capabilities. We describe the 
templates that we used to test the system and we 
explain the user?s knowledge required in order to 
create them. We finish the paper presenting our 
conclusions and future work. 
2 Template Authoring Environment 
The NLG Template Authoring Environment 
asks for a model sentence and allows the user to 
mark the sections that are variable. For each va-
riable indicated, the user has to specify its type 
(i.e., personal pronoun, possessive pronoun, Em-
                                               
1
 http://www.distilinteractive.com/ 
ployee_type) and which values of that type are 
allowed (i.e., all personal pronouns, or only 
?she? and ?he?). Additionally, the user can also 
indicate dependencies between variable elements 
and information for the verb (i.e., tense, form, 
modals). The system then shows the user all the 
possible sentences that could be generated from 
the given template by calculating all the possible 
combinations of variable values that respect the 
specified dependencies and follow the verb se-
lections. The user can then refine the template by 
changing the given example or the specified va-
riables, dependencies or verb options, in order to 
adjust the generated sentences to the needs of the 
game.  
The NLG Template Authoring Environment 
has been implemented in Java. The SimpleNLG 
library was used to automatically generate cor-
rect sentences and provide the user with the pos-
sibility of exploring different attributes to the 
verb. It has a user-friendly intuitive graphical 
interface, part of which is shown in Figure 1. 
 
Figure 1: Graphical Interface 
 
 
After entering an example sentence and click-
ing on Analyze, the user indicates that a section 
is variable by giving a type or semantic class to 
the word in that section. The values of a semantic 
class are stored in a text file, which allows the 
user to create new semantic classes as needed. 
These files contain all the possible values and 
their respective syntactic information (person, 
number and gender) which will be used for 
agreement with the verb and for dependency be-
tween variables purposes. Restrictions to the val-
ues that a variable can take are also indicated 
60
through the graphical interface. Dependencies 
can be indicated only between already declared 
variables. The main verb and all its options are 
indicated in the section at the bottom of the 
graphical interface. 
In the template shown in Figure 1, the exam-
ple sentence is ?I walk my dog?, ?I? is a variable 
of type personal pronoun, ?walk? is the main 
verb, ?my? is a variable of type possessive pro-
noun, ?dog? is a variable of type animal and 
there is a dependency between ?I? and ?my? 
(which will allow to make their values agree in 
person, number and gender when generating all 
possible combinations). 
In Figure 1 we also see that the user has se-
lected the values ?present and past? for the verb 
tense and ?normal? and ?imperative? for the verb 
form. Therefore, four sentences will be generated 
for each combination of the variables? values 
(one sentence for each combination of the tense 
and form selections). All these sentences will 
have the verb negated and will use the perfect 
tenses (as indicated by the extra verb options). 
3 Testing the NLG Template Authoring 
Environment 
In order to verify the correct functioning of the 
NLG Template Authoring Environment, we se-
lected a set of sentence templates from the game 
?Business in Balance: Implementing an Envi-
ronmental Management System? from DISTIL 
Interactive. The templates were selected manu-
ally, while keeping in mind the need to cover 
different aspects, as for example the number and 
type of the variables and dependencies. The test-
ing of these examples covers for many more 
templates of the same type. The five selected 
sentence templates that form our testing set are 
displayed in Table 1 and are identified in the rest 
of this section by their reference number or order 
in the table.  
Table 1. Testing examples 
Ref.  
number 
Template 
1 The ACTORS (ME/US) could help 
DEPARTMENTS. 
2 The ACTORS IS/ARE now avail-
able to help. 
3 I/WE struggled because of 
MY/OUR lack of knowledge. 
4 I/WE AM/ARE pleased to report 
that I/WE completed the task 
TASKS. 
5 I/WE WAS/WERE not the great-
est choice for keeping things 
moving along quickly. 
 
In these template examples, we show in capi-
tals the variable parts of the templates. ACTORS, 
DEPARTMENTS and TASKS refer to one of several 
possible nouns previously defined for each of the 
classes with those names. The terms in capitals 
separated by a ?/? already display all the ac-
cepted values for that variable (for example 
I/WE represent a variable of type personal pro-
noun which could take only the selected values 
?I? or ?we? and the rest are filtered out). 
The first template example has two variables 
of predefined closed class nouns, ACTORS and 
DEPARTMENTS. The latter is independent, while 
the former has a dependency with a variable of 
type personal pronoun (in objective case form) 
that could only take the values ?me? or ?us?. 
This template is used in the game when the ac-
tor/character available to help is the same ac-
tor/character that is providing the information. 
This template can be successfully generated with 
our system by declaring the variables, restricting 
the values of the pronoun variable, and establish-
ing the dependency. When filtering non-valid 
sentences, the system will eliminate those cases 
where the value?s number of the variable ACTOR 
and the personal pronoun do not agree (i.e., it 
will only allow sentences that use ?me? if the 
actor is singular, and sentences that use ?us?, if 
the actor is plural). When creating this template, 
the user will have to be aware that the main verb 
is ?to help? and indicate ?could? as a modal to be 
used. This is important as otherwise SimpleNLG 
will modify the main verb in order to agree with 
the number of the subject. It is also necessary in 
case some of the options to change the main verb 
are specified. 
Two examples of the generated sentences us-
ing the first template are shown below. 
? The HR Training Manager (me) could 
help the Design Department. 
? The Implementation Team (us) could 
help the Deputy Management Represen-
tative. 
The second template is one that found a prob-
lem with our system and provided us with a rea-
son and an opportunity to improve it. This exam-
ple template also uses a variable of the closed 
class noun ACTOR together with the verb ?to be? 
in the present tense, agreeing in number with the 
actor. It might seem trivial to indicate this de-
pendency between the actor variable and the 
verb. But in our system the verbs are not treated 
as a regular variable (even when their values can 
be variable), but they are left for SimpleNLG to 
find the correct verb form. We needed then to 
61
inform SimpleNLG the number to which the 
verb should agree (by default it would assume 
singular). In this case we needed to inform Sim-
pleNLG that the number to agree with would be 
the number of the variable ACTOR. We also have 
to consider the case when the subject number 
does not depend on a variable and is plural, as 
for example in a template where the subject is 
?The members of DEPARTMENT?. To accom-
modate for these cases, we improved our system 
by asking the user to indicate in a pull down 
menu whether the template?s verb should agree 
with a variable value or it should be always used 
in plural or in singular. (This option is displayed 
in the bottom right corner of the interface and not 
shown in the partial screen shot on Figure 1.) 
The third template presents a dependency be-
tween a variable of type personal pronoun in the 
subjective case form, and a variable of type pos-
sessive pronoun in the complement. Both vari-
ables accept only a pair of their possible values, 
and the dependency between them establishes 
that they have to agree in person, number and 
gender. That is not a problem for our system. 
With respect to the verb, the user has to indicate 
the past tense as the only option.  
In the fourth and fifth template, there is a per-
sonal pronoun variable taking the place of the 
subject, which should agree in person and num-
ber with the verb. This is, as mentioned before, 
left to SimpleNLG to solve. As the subject in 
these cases consists of only a personal pronoun 
and SimpleNLG can detect this fact, no extra 
information is required. In the fourth template, 
there is also a dependency between the personal 
pronoun variable in the subject role and the per-
sonal pronoun variable in the complement. Once 
again the person and number of these two vari-
ables have to agree, and the sentences not satis-
fying this restriction are filtered out by our sys-
tem. Finally, for the fifth template the user is 
forced to specify that the verb ?to be? has to be 
used in its past tense. 
4 Conclusions and Future Work 
We have identified the need for an NLG Tem-
plate Authoring Environment that allows game 
content designers without linguistic and pro-
gramming background to experiment with and 
finally create language templates.  
We have designed and implemented a system 
that allows the user to specify an example sen-
tence together with variables, its dependencies, 
and verb options that complete the template. This 
system shows the user all the possible sentences 
that could be generated with the specified tem-
plate. It can be used to refine the template until it 
satisfies the user?s needs.  
The system makes use of the SimpleNLG java 
library which provides us with correct sentences 
and the possibility of including many verb varia-
tions, such as tense, form and modals. 
We have evaluated our NLG Template Au-
thoring Environment in a set of sentence tem-
plates from a digital-based interactive simulation 
game that covered different characteristics.  
We have implemented a user-friendly intuitive 
graphical interface for the system. The conven-
ience of use of this interface will be evaluated in 
the context of the development of a new game. 
Acknowledgements 
This work is supported by the Ontario Centres of 
Excellence (OCE) and Precarn Incorporated. 
References 
J. A. Bateman. 2002. Natural Language Generation: 
an introduction and open-ended review of the state 
of the art. 
M. F. Caropreso, D. Inkpen, S. Khan and F. Keshtkar. 
2009. Novice Friendly Natural Language Genera-
tion Template Authoring Environment. Proceeding 
of the Canadian Artificial Intelligence Conference 
2009, Kelowna, BC, pp.195-198. 
H. Dalianis. 1996. Concise Natural Language Genera-
tion from Formal Specifications, Ph.D. Thesis, 
(Teknologie Doktorsavhandling), Department of 
Computer and Systems Sciences, Royal Institute of 
Technology/ Stockholm University. Report Series 
No. 96-008, ISSN 1101-8526, SRN SU-
KTH/DSV/R 96/8 SE.  
K. van Deemter, E. Krahmer and M. Theune. 2005. 
Real versus Template-Based Natural Language 
Generation: A False Opposition? In Computational 
Linguistics, 31(1): 15-24.  
D. French, C. Hale, C. Johnson and G. Farr. 1999. 
Internet Based Learning: An introduction and 
framework for higher education and business. Lon-
don, UK: Kogan Page.  
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems (Studies in Natural 
Language Processing), Cambridge University 
Press. 
E. Reiter. 2007. SimpleNlg package: 
http://www.csd.abdn.ac.uk/ereiter/simplnlg 
62
Acquiring Collocations for Lexical Choice between Near-Synonyms
Diana Zaiu Inkpen and Graeme Hirst
Department of Computer Science
University of Toronto
{dianaz,gh}@cs.toronto.edu
Abstract
We extend a lexical knowledge-base of
near-synonym differences with knowl-
edge about their collocational behaviour.
This type of knowledge is useful in the
process of lexical choice between near-
synonyms. We acquire collocations for
the near-synonyms of interest from a cor-
pus (only collocations with the appropri-
ate sense and part-of-speech). For each
word that collocates with a near-synonym
we use a differential test to learn whether
the word forms a less-preferred collo-
cation or an anti-collocation with other
near-synonyms in the same cluster. For
this task we use a much larger corpus
(the Web). We also look at associations
(longer-distance co-occurrences) as a pos-
sible source of learning more about nu-
ances that the near-synonyms may carry.
1 Introduction
Edmonds and Hirst (2002 to appear) developed a
lexical choice process for natural language gener-
ation (NLG) or machine translation (MT) that can
decide which near-synonyms are most appropriate
in a particular situation. The lexical choice process
has to choose between clusters of near-synonyms (to
convey the basic meaning), and then to choose be-
tween the near-synonyms in each cluster. To group
near-synonyms in clusters we trust lexicographers?
judgment in dictionaries of synonym differences.
For example task, job, duty, assignment, chore, stint,
hitch all refer to a one-time piece of work, but which
one to choose depends on the duration of the work,
the commitment and the effort involved, etc.
In order to convey desired nuances of mean-
ing and to avoid unwanted implications, knowledge
about the differences among near-synonyms is nec-
essary. I-Saurus, a prototype implementation of (Ed-
monds and Hirst, 2002 to appear), uses a small num-
ber of hand-built clusters of near-synonyms.
Our goal is to automatically acquire knowledge
about distinctions among near-synonyms from a
dictionary of synonym differences and from other
sources such as free text, in order to build a new lex-
ical resource, which can be used in lexical choice.
Preliminary results on automatically acquiring a lex-
ical knowledge-base of near-synonym differences
were presented in (Inkpen and Hirst, 2001). We ac-
quired denotational (implications, suggestions, de-
notations), attitudinal (favorable, neutral, or pejo-
rative), and stylistic distinctions from Choose the
Right Word (Hayakawa, 1994) (hereafter CTRW)1.
We used an unsupervised decision-list algorithm to
learn all the words used to express distinctions and
then applied information extraction techniques.
Another type of knowledge that can help in the
process of choosing between near-synonyms is col-
locational behaviour, because one must not choose
a near-synonym that does not collocate well with
the other word choices for the sentence. I-Saurus
does not include such knowledge. The focus of
the work we present in this paper is to add knowl-
edge about collocational behaviour to our lexical
knowledge-base of near-synonym differences. The
lexical choice process implemented in I-Saurus gen-
1We are grateful to HarperCollins Publishers, Inc. for per-
mission to use CTRW in this project.
                     July 2002, pp. 67-76.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
erates all the possible sentences with a given mean-
ing, and ranks them according to the degree to which
they satisfy a set of preferences given as input (these
are the denotational, attitudinal, and stylistic nu-
ances mentioned above). We can refine the rank-
ing so that it favors good collocations, and penal-
izes sentences containing words that do not collocate
well.
We acquire collocates of all near-synonyms in
CTRW from free text. We combine several statis-
tical measures, unlike other researchers who rely on
only one measure to rank collocations.
Then we acquire knowledge about less-preferred
collocations and anti-collocations2. For exam-
ple daunting task is a preferred collocation, while
daunting job is less preferred (it should not be used
in lexical choice unless there is no better alternative),
and daunting duty is an anti-collocation (it must not
be used in lexical choice). Like Church et al(1991),
we use the t-test and mutual information. Unlike
them we use the Web as a corpus for this task, we
distinguish three different types of collocations, and
we apply sense disambiguation to collocations.
Collocations are defined in different ways by dif-
ferent researchers. For us collocations consist of
consecutive words that appear together much more
often than by chance. We also include words sep-
arated by a few non-content words (short-distance
co-occurrence in the same sentence).
We are interested in collocations to be used in lex-
ical choice. Therefore we need to extract lexical
collocations (between open-class words), not gram-
matical collocations (which could contain closed-
class words, for example put on). For now, we con-
sider only two-word fixed collocations. In future
work we will consider longer and more flexible col-
locations.
We are also interested in acquiring words that
strongly associate with our near-synonyms, espe-
cially words that associate with only one of the near-
synonyms in the cluster. Using these strong asso-
ciations, we plan to learn about nuances of near-
synonyms in order to validate and extend our lexical
knowledge-base of near-synonym differences.
In our first experiment, described in sections 2
and 3 (with results in section 4, and evaluation in
2This term was introduced by Pearce (2001).
section 5), we acquire knowledge about the collo-
cational behaviour of the near-synonyms. In step 1
(section 2), we acquire potential collocations from
the British National Corpus (BNC)3, combining sev-
eral measures. In section 3 we present: (step2) se-
lect collocations for the near-synonyms in CTRW;
(step 3) filter out wrongly selected collocations us-
ing mutual information on the Web; (step 4) for each
cluster we compose new collocations by combin-
ing the collocate of one near-synonym with the the
other near-synonym, and we apply the differential t-
test to classify them into preferred collocations, less-
preferred collocations, and anti-collocations. Sec-
tion 6 sketches our second experiment, involving
word associations. The last two sections present re-
lated work, and conclusions and future work.
2 Extracting collocations from free text
For the first experiment we acquired collocations for
near-synonyms from a corpus. We experimented
with 100 million words from the Wall Street Journal
(WSJ). Some of our near-synonyms appear very few
times (10.64% appear fewer than 5 times) and 6.87%
of them do not appear at all in WSJ (due to its busi-
ness domain). Therefore we need a more general
corpus. We used the 100 million word BNC. Only
2.61% of our near-synonyms do not occur; and only
2.63% occur between 1 and 5 times.
Many of the near-synonyms appear in more than
one cluster, with different parts-of-speech. We ex-
perimented on extracting collocations from raw text,
but we decided to use a part-of-speech tagged corpus
because we need to extract only collocations rele-
vant for each cluster of near-synonyms. The BNC is
a good choice of corpus for us because it has been
tagged (automatically by the CLAWS tagger).
We preprocessed the BNC by removing all words
tagged as closed-class. To reduce computation time,
we also removed words that are not useful for our
purposes, such as proper names (tagged NP0). If we
keep the proper names, they are likely to be among
the highest-ranked collocations.
There are many statistical methods that can be
used to identify collocations. Four general meth-
ods are presented by Manning and Schu?tze (1999).
The first one, based on frequency of co-occurrence,
3http://www.hcu.ox.ac.uk/BNC/
does not consider the length of the corpus. Part-of-
speech filtering is needed to obtain useful colloca-
tions. The second method considers the means and
variance of the distance between two words, and can
compute flexible collocations (Smadja, 1993). The
third method is hypothesis testing, which uses sta-
tistical tests to decide if the words occur together
with probability higher than chance (it tests whether
we can reject the null hypothesis that the two words
occurred together by chance). The fourth method
is (pointwise) mutual information, an information-
theoretical measure.
We use Ted Pedersen?s Bigram Statistics Pack-
age4. BSP is a suite of programs to aid in analyz-
ing bigrams in a corpus (newer versions allow N-
grams). The package can compute bigram frequen-
cies and various statistics to measure the degree of
association between two words: mutual information
(MI), Dice, chi-square (?2), log-likelihood (LL), and
Fisher?s exact test.
The BSP tools count for each bigram in a corpus
how many times it occurs, and how many times the
first word occurs.
We briefly describe the methods we use in our ex-
periments, for the two-word case. Each bigram xy
can be viewed as having two features represented by
the binary variables X and Y . The joint frequency
distribution of X and Y is described in a contingency
table. Table 1 shows an example for the bigram
daunting task. n11 is the number of times the bi-
gram xy occurs; n12 is the number of times x occurs
in bigrams at the left of words other than y; n21 is
the number of times y occurs in bigrams after words
other that x; and n22 is the number of bigrams con-
taining neither x nor y. In Table 1 the variable X
denotes the presence or absence of daunting in the
first position of a bigram, and Y denotes the pres-
ence or absence of task in the second position of a
bigram. The marginal distributions of X and Y are
the row and column totals obtained by summing the
joint frequencies: n+1 = n11 + n21, n1+ = n11 + n12,
and n++ is the total number of bigrams.
The BSP tool counts for each bigram in a corpus
how many times it occurs, how many times the first
word occurs at the left of any bigram (n+1), and how
many times the second words occurs at the right of
4http://www.d.umn.edu/?tpederse/code.html
y ?y
x n11 = 66 n12 = 54 n1+ = 120
?x n21 = 4628 n22 = 15808937 n2+ = 15813565
n+1 = 4694 n+2 = 15808991 n++ = 15813685
Table 1: Contingency table for daunting task
(x = daunting, y = task).
any bigram (n1+).
Mutual information, I(x;y), compares the prob-
ability of observing words x and word y together (the
joint probability) with the probabilities of observing
x and y independently (the probability of occurring
together by chance) (Church and Hanks, 1991).
I(x;y) = log2
P(x,y)
P(x)P(y)
The probabilities can be approximated by: P(x) =
n+1/n++, P(y) = n1+/n++, P(x,y) = n11/n++.
Therefore:
I(x;y) = log2
n++n11
n+1n1+
The Dice coefficient is related to mutual informa-
tion and it is calculated as:
Dice(x,y) =
2P(x,y)
P(x)+ P(y)
=
2n11
n+1 + n1+
The next methods fall under hypothesis test-
ing methods. Pearson?s Chi-square and Log-
likelihood ratios measure the divergence of ob-
served (ni j) and expected (mi j) sample counts (i =
1,2, j = 1,2). The expected values are for the model
that assumes independence (assumes that the null
hypothesis is true). For each cell in the contingency
table, the expected counts are: mi j = ni+n+ jn++ . The
measures are calculated as (Pedersen, 1996):
?2 = ?i, j
(ni j?mi j)2
mi j
LL = 2 ?i, j
log2 n2i j
mi j
Log-likelihood ratios (Dunning, 1993) are more
appropriate for sparse data than chi-square.
Fisher?s exact test is a significance test that is
considered to be more appropriate for sparse and
skewed samples of data than statistics such as the
log-likelihood ratio or Pearson?s Chi-Square test
(Pedersen, 1996). Fisher?s exact test is computed
by fixing the marginal totals of a contingency table
and then determining the probability of each of the
possible tables that could result in those marginal to-
tals. Therefore it is computationally expensive. The
formula is:
P =
n1+!n2+!n+1!n+2!
n++!n11!n12!n21!n22!
Because these five measures rank collocations in
different ways (as the results in the Appendix will
show), and have different advantages and draw-
backs, we decided to combine them in choosing col-
locations. We choose as potential collocations for
each near-synonym a collocation that is selected by
at least two of the measures. For each measure
we need to choose a threshold T , and consider as
selected collocations only the T highest-ranked bi-
grams (where T can differ for each measure). By
choosing higher thresholds we increase the precision
(reduce the chance of accepting wrong collocations).
By choosing lower thresholds we get better recall.
If we opt for low recall we may not get many col-
locations for some of the near-synonyms. Because
there is no principled way of choosing these thresh-
olds, we prefer to choose lower thresholds (the first
200,000 collocations selected by each measure, ex-
cept Fisher?s measure for which we take all 435,000
collocations ranked 1) and to filter out later (in step
2) the bigrams that are not true collocations, using
mutual information on the Web.
3 Differential collocations
For each cluster of near-synonyms, we now have
the words that occur in preferred collocations with
each near-synonym. We need to check whether these
words collocate with the other near-synonyms in the
same cluster. For example, if daunting task is a pre-
ferred collocation, we check whether daunting col-
locates with the other near-synonyms of task.
We use the Web as a corpus for differential col-
locations. We don?t use the BNC corpus to rank
less-preferred and anti-collocations, because their
absence in BNC may be due to chance. We can as-
sume that the Web (the portion retrieved by search
engines) is big enough that a negative result can be
trusted.
We use an interface to AltaVista search engine to
count how often a collocation is found. (See Table 2
for an example.5) A low number of co-occurrences
indicates a less-preferred collocation. But we also
need to consider how frequent the two words in the
collocation are. We use the differential t-test to find
collocations that best distinguish between two near-
synonyms (Church et al, 1991), but we use the Web
as a corpus. Here we don?t have part-of-speech tags
but this is not a problem because in the previous
step we selected collocations with the right part-of-
speech for the near-synonym. We approximate the
number of occurrences of a word on the Web with
the number of documents containing the word.
The t-test can also be used in the hypothesis test-
ing method to rank collocations. It looks at the mean
and variance of a sample of measurements, where
the null hypothesis is that the sample was drawn
from a normal distribution with mean ?. It measures
the difference between observed (x?) and expected
means, scaled by the variance of the data (s2), which
in turn is scaled by the sample size (N).
t =
x???
?
s2
N
We are interested in the Differential t-test, which
can be used for hypothesis testing of differences. It
compares the means of two normal populations:
t =
x?1? x?2
?
s21
N +
s22
N
Here the null hypothesis is that the average differ-
ence is ? = 0.Therefore x?? ? = ? = x?1? x?2. In the
denominator we add the variances of the two popu-
lations.
If the collocations of interest are xw and yw (or
similarly wx and wy), then we have the approxima-
tions x?1 = s21 = P(x,w) and x?2 = s22 = P(y,w); there-
fore:
t =
P(x,w)?P(y,w)
?
P(x,w)+P(y,w)
n++
=
nxw?nyw
?
nxw + nyw
If w is a word that collocates with one of the near-
synonyms in a cluster, and x is each of the near-
5The search was done on 13 March 2002.
synonyms, we can approximate the mutual informa-
tion relative to w:
P(w,x)
P(x)
=
nwx
nx
where P(w) was dropped because it is the same for
various x (we cannot compute if we keep it, because
we don?t know the total number of bigrams on the
Web).
We use this measure to eliminate collocations
wrongly selected in step 1. We eliminate those with
mutual information lower that a threshold. We de-
scribe the way we chose this threshold (Tmi) in sec-
tion 5.
We are careful not to consider collocations of a
near-synonym with a wrong part-of-speech (our col-
locations are tagged). But there is also the case when
a near-synonym has more than one major sense. In
this case we are likely to retrieve collocations for
senses other than the one required in the cluster. For
example, for the cluster job, task, duty, etc., the col-
location import/N duty/N is likely to be for a differ-
ent sense of duty (the customs sense). Our way of
dealing with this is to disambiguate the sense used
in each collocations (we assume one sense per collo-
cation), by using a simple Lesk-style method (Lesk,
1986). For each collocation, we retrieve instances in
the corpus, and collect the content words surround-
ing the collocations. This set of words is then in-
tersected with the context of the near-synonym in
CTRW (that is the whole entry). If the intersection
is not empty, it is likely that the collocation and the
entry use the near-synonym in the same sense. If the
intersection is empty, we don?t keep the collocation.
In step 3, we group the collocations of each near-
synonym with a given collocate in three classes,
based on the t-test values of pairwise collocations.
We compute the t-test between each collocation and
the collocation with maximum frequency, and the
t-test between each collocation and the collocation
with minimum frequency (see Table 2 for an exam-
ple). Then, we need to determine a set of thresholds
that classify the collocations in the three groups:
preferred collocations, less preferred collocations,
and anti-collocations. The procedure we use in this
step is detailed in section 5.
x Hits MI t max t min
task 63573 0.011662 - 252.07
job 485 0.000022 249.19 22.02
assignment 297 0.000120 250.30 17.23
chore 96 0.151899 251.50 9.80
duty 23 0.000022 251.93 4.80
stint 0 0 252.07 -
hitch 0 0 252.07 -
Table 2: The second column shows the number of
hits for the collocation daunting x, where x is one
of the near-synonyms in the first column. The third
column shows the mutual information, the fourth
column, the differential t-test between the colloca-
tion with maximum frequency (daunting task) and
daunting x, and the last column, the t-test between
daunting x and the collocation with minimum fre-
quency (daunting hitch).
4 Results
We obtained 15,813,685 bigrams. From these,
1,350,398 were distinct and occurred at least 4
times.
We present some of the top-ranked collocations
for each measure in the Appendix. We present the
rank given by each measure (1 is the highest), the
value of the measure, the frequency of the colloca-
tion, and the frequencies of the words in the collo-
cation.
We selected collocations for all 914 clusters in
CTRW (5419 near-synonyms in total). An example
of collocations extracted for the near-synonym task
is:
daunting/A task/N
-- MI 24887 10.8556
-- LL 5998 907.96
-- X2 16341 122196.8257
-- Dice 2766 0.0274
repetitive/A task/N
-- MI 64110 6.7756
-- X2 330563 430.4004
where the numbers are, in order, the rank given by
the measure and the value of the measure.
We filtered out the collocations using MI on the
Web (step 2), and then we applied the differential
t-test (step 3). Table 2 shows the values of MI
between daunting x and x, where x is one of the
near-synonyms of task. It also shows t-test val-
Near-synonyms daunting particular tough
task
? ? ?
job ? ? ?
assignment ?
? ?
chore ? ? ?
duty ?
?
?
stint ? ? ?
hitch ? ? ?
Table 3: Example of results for collocations.
ues between (some) pairs of collocations. Table 3
presents an example of results for differential col-
locations, where
?
marks preferred collocations, ?
marks less-preferred collocations, and ? marks anti-
collocations.
Before proceeding with step 3, we filtered out the
collocations in which the near-synonym is used in
a different sense, using the Lesk method explained
above. For example, suspended/V duty/N is kept
while customs/N duty/N and import/N duty/N are re-
jected. The disambiguation part of our system was
run only for a subset of CTRW, because we have yet
to evaluate it. The other parts of our system were run
for the whole CTRW. Their evaluation is described
in the next section.
5 Evaluation
Our evaluation has two purposes: to get a quanti-
tative measure of the quality of our results, and to
choose thresholds in a principled way.
As described in the previous sections, in step 1
we selected potential collocations from BNC (the
ones selected by at least two of the five measures).
Then, we selected collocations for each of the near-
synonyms in CTRW (step 2). We need to evaluate
the MI filter (step 3), which filters out the bigrams
that are not true collocations, based on their mutual
information computed on the Web. We also need to
evaluate step 4, the three way classification based on
the differential t-test on the Web.
For evaluation purposes we selected three clusters
from CTRW, with a total of 24 near-synonyms. For
these, we obtained 916 collocations from BNC ac-
cording to the method described in section 2.
We had two human judges reviewing these collo-
cations to determine which of them are true colloca-
tions and which are not. We presented the colloca-
tions to the judges in random order, and each collo-
cation was presented twice. The first judge was con-
sistent (judged a collocation in the same way both
times it appeared) in 90.4% of the cases. The second
judge was consistent in 88% of the cases. The agree-
ment between the two judges was 67.5% (computed
in a strict way, that is we considered agreement only
when the two judges had the same opinion including
the cases when they were not consistent). The con-
sistency and agreement figures show how difficult
the task is for humans.
We used the data annotated by the two judges to
build a standard solution, so we can evaluate the
results of our MI filter. In the standard solution
a bigram was considered a true collocation if both
judges considered it so. We used the standard solu-
tion to evaluate the results of the filtering, for various
values of the threshold Tmi. That is, if a bigram had
the value of MI on the Web lower than a threshold
Tmi, it was filtered out. We choose the value of Tmi so
that the accuracy of our filtering program is the high-
est. By accuracy we mean the number of true collo-
cations (as given by the standard solution) identified
by our program over the total number of bigrams we
used in the evaluation. The best accuracy was 70.7%
for Tmi = 0.0017. We used this value of the threshold
when running our programs for all CTRW.
As a result of this first part of the evaluation, we
can say that after filtering collocations based on MI
on the Web, approximately 70.7% of the remaining
bigrams are true collocation. This value is not ab-
solute, because we used a sample of the data for the
evaluation. The 70.7% accuracy is much better than
a baseline (approximately 50% for random choice).
Table 4 summarizes our evaluation results.
Next, we proceeded with evaluating the differ-
ential t-test three-way classifier. For each cluster,
for each collocation, new collocations were formed
from the collocate and all the near-synonyms in the
cluster. In order to learn the classifier, and to evalu-
ate its results, we had the two judges manually clas-
sify a sample data into preferred collocations, less-
preferred collocations, and anti-collocations. We
used 2838 collocations obtained for the same three
clusters from 401 collocations (out of the initial 916)
that remained after filtering. We built a standard so-
lution for this task, based on the classifications of
Step Baseline Our system
Filter (MI on the Web) 50% 70.7%
Dif. t-test classifier 71.4% 84.1%
Table 4: Accuracy of our main steps.
both judges. When the judges agreed, the class was
clear. When they did not agree, we designed sim-
ple rules, such as: when one judge chose the class
preferred collocation, and the other judge chose the
class anti-collocation, the class in the solution was
less-preferred collocation. The agreement between
judges was 80%; therefore we are confident that the
quality of our standard solution is high. We used
this standard solution as training data to learn a de-
cision tree6 for our three-way classifier. The fea-
tures in the decision tree are the t-test between each
collocation and the collocation from the same group
that has maximum frequency on the Web, and the
t-test between the current collocation and the col-
location that has minimum frequency (as presented
in Table 2). We could have set aside a part of the
training data as a test set. Instead, we did 10-fold
cross validation to quantify the accuracy on unseen
data. The accuracy on the test set was 84.1% (com-
pared with a baseline that chooses the most frequent
class, anti-collocations, and achieves an accuracy of
71.4%). We also experimented with including MI
as a feature in the decision tree, and with manually
choosing thresholds (without a decision tree) for the
three-way classification, but the accuracy was lower
than 84.1%.
The three-way classifier can fix some of the mis-
takes of the MI filter. If a wrong collocation re-
mained after the MI filter, the classifier can classify
it in the anti-collocations class.
We can conclude that the collocational knowledge
we acquired has acceptable quality.
6 Word Association
We performed a second experiment, where we
looked for long distance co-occurrences (words that
co-occur in a window of size K). We call these as-
sociations, and they include the lexical collocations
we extracted in section 2.
6We used C4.5, http://www.cse.unsw.edu.au/?quinlan
We use BSP with the option of looking for bi-
grams in a window larger than 2. For example
if the window size is 3, and the text is vaccine/N
cure/V available/A, the extracted bigrams are vac-
cine/N cure/V, cure/V available/A, and vaccine/N
available/A. We would like to choose a large (4?
15) window size; the only problem is the increase
in computation time. We look for associations of a
word in the paragraph, not only in the sentence. Be-
cause we look for bigrams, we may get associations
that occur to the left or to the right of the word. This
is an indication of strong association.
We obtained associations similar to those pre-
sented by Church et al(1991) for the near-synonyms
ship and boat. Church et al suggest that a lexicog-
rapher looking at these associations can infer that a
boat is generally smaller than a ship, because they
are found in rivers and lakes, while the ships are
found in seas. Also, boats are used for small jobs
(e.g., fishing, police, pleasure), whereas ships are
used for serious business (e.g., cargo, war). Our in-
tention is to use the associations to automatically in-
fer this kind of knowledge and to validate acquired
knowledge.
For our purpose we need only very strong associ-
ations, and we don?t want words that associate with
all near-synonyms in a cluster. Therefore we test for
anti-associations using the same method we used in
section 3, with the difference that the query asked to
AltaVista is: x NEAR y (where x and y are the words
of interest).
Words that don?t associate with a near-synonym
but associate with all the other near-synonyms in
a cluster can tell us something about its nuances
of meaning. For example terrible slip is an anti-
association, while terrible associates with mistake,
blunder, error. This is an indication that slip is a
minor error.
Table 5 presents some preliminary results we
obtained with K = 4 (on half the BNC and then
on the Web), for the differential associations of
boat (where ? marks preferred associations, ?
marks less-preferred associations, and ? marks anti-
associations). We used the same thresholds as for
our experiment with collocations.
Near-synonyms fishing club rowing
boat
? ? ?
vessel
?
? ?
craft ? ? ?
ship ? ? ?
Table 5: Example of results for associations.
7 Related work
There has been a lot of work done in extracting col-
locations for different applications. We have already
mentioned some of the most important contributors.
Like Church et al(1991), we use the t-test and
mutual information, but unlike them we use the Web
as a corpus for this task (and a modified form of
mutual information), and we distinguish three types
of collocations (preferred, less-preferred, and anti-
collocations).
We are concerned with extracting collocations for
use in lexical choice. There is a lot of work on
using collocations in NLG (but not in the lexical
choice sub-component). There are two typical ap-
proaches: the use of phrasal templates in the form
of canned phrases, and the use of automatically ex-
tracted collocations for unification-based generation
(McKeown and Radev, 2000).
Statistical NLG systems (such as Nitrogen
(Langkilde and Knight, 1998)) make good use of the
most frequent words and their collocations. But such
a system cannot choose a less-frequent synonym that
may be more appropriate for conveying desired nu-
ances of meaning, if the synonym is not a frequent
word.
Finally, there is work related to ours from the
point of view of the synonymy relation.
Turney (2001) used mutual information to detect
the best answer to questions about synonyms from
Test of English as a Foreign Language (TOEFL) and
English as a Second Language (ESL). Given a prob-
lem word (with or without context), and four alter-
native words, the question is to choose the alterna-
tive most similar in meaning with the problem word.
His work is based on the assumption that two syn-
onyms are likely to occur in the same document (on
the Web). This can be true if the author needs to
avoid repeating the same word, but not true when
the synonym is of secondary importance in a text.
The alternative that has the highest PMI-IR (point-
wise mutual information for information retrieval)
with the problem word is selected as the answer. We
used the same measure in section 3 ? the mutual
information between a collocation and a collocate
that has the potential to discriminate between near-
synonyms. Both works use the Web as a corpus, and
a search engine to estimate the mutual information
scores.
Pearce (2001) improves the quality of retrieved
collocations by using synonyms from WordNet
(Pearce, 2001). A pair of words is considered a
collocation if one of the words significantly prefers
only one (or several) of the synonyms of the other
word. For example, emotional baggage is a good
collocation because baggage and luggage are in the
same synset and ?emotional luggage is not a col-
location. As in our work, three types of colloca-
tions are distinguished: words that collocate well;
words that tend to not occur together, but if they
do the reading is acceptable; and words that must
not be used together because the reading will be un-
natural (anti-collocations). In a similar manner with
(Pearce, 2001), in section 3, we don?t record collo-
cations in our lexical knowledge-base if they don?t
help discriminate between near-synonyms. A differ-
ence is that we use more than frequency counts to
classify collocations (we use a combination of t-test
and MI).
Our evaluation was partly inspired by Evert and
Krenn (2001). They collect collocations of the form
noun-adjective and verb-prepositional phrase. They
build a solution using two human judges, and use
the solution to decide what is the best threshold for
taking the N highest-ranked pairs as true colloca-
tions. In their experiment MI behaves worse that
other measures (LL, t-test), but in our experiment
MI on the Web achieves good results.
8 Conclusions and Future Work
We presented an unsupervised method to acquire
knowledge about the collocational behaviour of
near-synonyms.
Our future work includes improving the way we
combine the five measures for ranking collocations,
maybe by giving more weight to the collocations se-
lected by the log-likelihood ratio. We also plan to
experiment more with disambiguating the senses of
the words in a collocation.
Our long-term goal is to acquire knowledge about
near-synonyms from corpora and other sources, by
bootstrapping with our initial lexical knowledge-
base of near-synonym differences. This includes
validating the knowledge already asserted and learn-
ing more distinctions.
Acknowledgments
We thank Gerald Penn, Olga Vechtomova, and three anonymous
reviewers for their helpful comments on previous drafts of this
paper. We thank Eric Joanis and Tristan Miller for helping with
the judging task. Our work is financially supported by the Nat-
ural Sciences and Engineering Research Council of Canada and
the University of Toronto.
Appendix
The first 10 collocations selected by each mea-
sure are presented below. Note that some of
the measures rank many collocations equally at
rank 1: MI 358 collocations; LL one collocation;
?2 828 collocations; Dice 828 collocations; and
Fisher 435,000 collocations (when the measure is
computed with a precision of 10 digits ? higher
precision is recommended, but the computation
time becomes a problem). The rest of the columns
are: the rank assigned by the measure, the value
of the measure, the frequency of the collocation in
BNC, the frequency of the first word in the first
position in bigrams, and the frequency of the second
word in the second position in bigrams.
Some of the collocations ranked 1 by MI:
source-level/A debugger/N 1 21.9147 4 4 4
prosciutto/N crudo/N 1 21.9147 4 4 4
rumpy/A pumpy/A 1 21.9147 4 4 4
thrushes/N blackbirds/N 1 21.9147 4 4 4
clickity/N clickity/N 1 21.9147 4 4 4
bldsc/N microfilming/V 1 21.9147 4 4 4
chi-square/A variate/N 1 21.9147 4 4 4
long-period/A comets/N 1 21.9147 4 4 4
tranquillizers/N sedatives/N 1 21.9147 4 4 4
one-page/A synopsis/N 1 21.9147 4 4 4
First 10 collocations selected by LL:
prime/A minister/N 1 123548 9464 11223 18825
see/V p./N 2 83195 8693 78213 10640
read/V studio/N 3 67537 5020 14172 5895
ref/N no/N 4 62486 3630 3651 4806
video-taped/A report/N 5 52952 3765 3765 15886
secretary/N state/N 6 51277 5016 10187 25912
date/N award/N 7 48794 3627 8826 5614
hon./A friend/N 8 47821 4094 10345 10566
soviet/A union/N 9 44797 3894 8876 12538
report/N follows/V 10 44785 3776 16463 6056
Some of the collocations ranked 1 by ?2:
lymphokine/V activated/A 1 15813684 5 5 5
config/N sys/N 1 15813684 4 4 4
levator/N depressor/N 1 15813684 5 5 5
nobile/N officium/N 1 15813684 11 11 11
line-printer/N dot-matrix/A 1 15813684 4 4 4
dermatitis/N herpetiformis/N 1 15813684 9 9 9
self-induced/A vomiting/N 1 15813684 5 5 5
horoscopic/A astrology/N 1 15813684 5 5 5
mumbo/N jumbo/N 1 15813684 12 12 12
long-period/A comets/N 1 15813684 4 4 4
Some of the collocations ranked 1 by Dice:
clarinets/N bassoons/N 1 1.00 5 5 5
email/N footy/N 1 1.00 4 4 4
tweet/V tweet/V 1 1.00 5 5 5
garage/parking/N vehicular/A 1 1.00 4 4 4
growing/N coca/N 1 1.00 5 5 5
movers/N seconders/N 1 1.00 5 5 5
elliptic/A integrals/N 1 1.00 8 8 8
viscose/N rayon/N 1 1.00 15 15 15
cause-effect/A inversions/N 1 1.00 5 5 5
first-come/A first-served/A 1 1.00 6 6 6
Some of the collocations ranked 1 by Fisher:
roman/A artefacts/N 1 1.00 4 3148 108
qualitative/A identity/N 1 1.00 16 336 1932
literacy/N education/N 1 1.00 9 252 20350
disability/N pension/N 1 1.00 6 470 2555
units/N transfused/V 1 1.00 5 2452 12
extension/N exceed/V 1 1.00 9 1177 212
smashed/V smithereens/N 1 1.00 5 194 9
climbing/N frames/N 1 1.00 5 171 275
inclination/N go/V 1 1.00 10 53 51663
trading/N connections/N 1 1.00 6 2162 736
References
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analy-
sis. In Uri Zernik, editor, Lexical Acquisition: Using
On-line Resources to Build a Lexicon, pages 115?164.
Lawrence Erlbaum.
Ted Dunning. 1993. Accurate methods for statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
Philip Edmonds and Graeme Hirst. 2002 (to appear).
Near-synonymy and lexical choice. Computational
Linguistics, 28(2).
Stefan Evert and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of the 39th Annual Meeting of
the of the Association for Computational Linguistics
(ACL?2001), Toulouse, France.
S. I. Hayakawa. 1994. Choose the Right Word. Harper-
Collins Publishers.
Diana Zaiu Inkpen and Graeme Hirst. 2001. Build-
ing a lexical knowledge-base of near-synonym differ-
ences. In Proceedings of the Workshop on WordNet
and Other Lexical Resources, Second Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL?2001), Pittsburgh.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of N-grams in generation. In Proceedings of
the International Natural Language Generation Work-
shop, Niagara-on-the-Lake, Ontario.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of SIG-
DOC Conference, Toronto.
Christopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
The MIT Press, Cambridge, Massachusetts.
Kathleen McKeown and Dragomir Radev. 2000. Col-
locations. In R. Dale, H. Moisl, and H. Somers, edi-
tors, Handbook of Natural Language Processing. Mar-
cel Dekker.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In Proceedings of the Workshop on WordNet
and Other Lexical Resources, Second meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Pittsburgh.
Ted Pedersen. 1996. Fishing for exactness. In Proceed-
ings of the South-Central SAS Users Group Confer-
ence (SCSUG-96), Austin, Texas.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
Peter Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), pages 491?502, Freiburg, Germany.
Coling 2010: Poster Volume, pages 303?311,
Beijing, August 2010
Building Systematic Reviews Using Automatic Text Classification 
Techniques  
 
Oana Frunza, Diana Inkpen, and Stan Matwin 
School of Information Technology and Engineering 
University of Ottawa  
{ofrunza,diana,stan}@site.uottawa.ca 
 
 
  Abstract 
The amount of information in medical 
publications continues to increase at a 
tremendous rate. Systematic reviews help 
to process this growing body of informa-
tion. They are fundamental tools for evi-
dence-based medicine. In this paper, we 
show that automatic text classification can 
be useful in building systematic reviews 
for medical topics to speed up the review-
ing process. We propose a per-question 
classification method that uses an ensem-
ble of classifiers that exploit the particular 
protocol of a systematic review. We also 
show that when integrating the classifier 
in the human workflow of building a re-
view the per-question method is superior 
to the global method. We test several 
evaluation measures on a real dataset. 
1 Introduction 
Systematic reviews are the result of a tedious 
process which involves human reviewers to ma-
nually screen references of papers to determine 
their relevance to the review. This process often 
entails reading thousands or even tens of thou-
sands of abstracts from prospective articles. As 
the body of available articles continues to grow, 
this process is becoming increasingly difficult.  
 Common systematic review practices stipu-
late that two reviewers are used at the screening 
phases of a systematic review to review each ab-
stract of the documents retrieved after a simple 
query-based search. After a final decision is 
made for each abstract (the two reviewers decide 
if the abstract is relevant or not to the topic of 
review), in the next phase further analysis (more 
strict screening steps) on the entire article is 
done. A systematic review has to be complete, 
articles that are published on a certain topic and 
are clinically relevant need to be part of the re-
view. This requires near-perfect recall since the 
accidental exclusion of a potentially relevant ab-
stract can have a significantly negative impact on 
the validity of the overall systematic review (Co-
hen et al, 2006). Our goal in this paper is to pro-
pose an automatic system that can help human 
judges in the process of triaging articles by look-
ing only at abstracts and not the entire docu-
ments. This decision step is known as the initial 
screening phase in the protocol of building sys-
tematic reviews, only the abstracts are used as 
source of information.  
One reviewer will still read the entire collec-
tion of abstracts while the other will benefit from 
the help of the system; this reviewer will have to 
label only the articles that will be used to train 
the classifier (ideally a small proportion for 
workload reduction), the rest of the articles will 
be labeled by the classifier.  
 In the systematic review preparation, if at 
least one reviewer agrees to include an abstract, 
the abstract will have the labeled included and it 
will pass to the next screening phase; otherwise, 
it will be discarded. Therefore, the benefit of 
doubt plays an important role in the decision 
process. When we replace one reviewer with the 
automatic classifier, because we keep one human 
judge in the process, the confidence and reliabil-
ity of the review is still higher while the overall 
workload is reduced. The reduction is from the 
time required for two passes through the collec-
tion (for the two humans) to only one pass and 
the smaller part labeled by the reviewer which is 
assisted by the classifier.  Figure 1 presents on 
overview of our proposed workflow.   
303
  
Figure 1. Embedding automatic text classification in 
the process of building a systematic review. 
 
The task that needs to be solved in order to help 
the systematic review process is a text classifica-
tion task intended to classify an abstract as rele-
vant or not relevant to the topic of review. 
 The hypothesis that guides our research is 
that it is possible to save time for the human re-
viewers and obtain good performance levels, 
similar to the ones obtained by humans. In this 
current study we show that we can achieve this 
by building a classification model that is based 
on the natural human workflow used for building 
systematic reviews. We show, on a real data set, 
that a human-machine system obtains the best 
results when an ensemble of classifiers is used as 
the classification model.  
2 Related Work  
The traditional way to collect and triage the ab-
stracts from a systematic review consists in using 
simple query search techniques based on MeSH1 
or keywords terms. The queries are usual Boo-
lean-based and are optimized either for precision 
or for recall. The studies done by Haynes et al 
(1994) show that it is difficult to obtain high per-
formance for both measures.  
 The research done by Aphinyanaphongs and 
Aliferis (2005) is probably the first application of 
automatic text classification to the task of creat-
                                                 
1
 http://www.nlm.nih.gov/mesh/ 
ing systematic reviews. In that paper the authors 
experimented with a variety of text classification 
techniques using the data derived from the ACP 
Journal Club as their corpus. They found that 
support vector machine (SVM) was the best clas-
sifier according to a variety of measures. Further 
work for systematic reviews was done by Cohen 
et al (2006). Their work is mostly focused on the 
elimination of non relevant documents. As their 
main goal is to save work for the reviewers in-
volved in systematic review preparation, they 
define a measure, called work saved over sam-
pling (WSS) that captures the amount of work 
that the reviewers will save with respect to a 
baseline of just sampling for a given value of 
recall. The idea is that a classifier returns, with 
high recall, a set of abstracts, and only those ab-
stracts need to be read to weed out the non-
relevant ones. The savings are measured with 
respect to the number of abstracts that would 
have to be read if a random baseline classifier 
was used. Such baseline corresponds to uni-
formly sampling a given percentage of abstracts 
(equal to the desired recall) from the entire set. In 
Cohen et al (2006), the WSS measure is applied 
to report the reduction in reviewer's work when 
retrieving 95% of the relevant documents; the 
precision was very low.  
 We focus on developing a classifier for sys-
tematic review preparation, relying on character-
istics of the data that were not included in the 
Cohen et al?s (2006), because the questions 
asked in the preparation of the reviews are not 
available, Therefore we cannot perform a direct 
comparison of results here. Also, the data sets 
that they used in their experiments are signifi-
cantly smaller than the one that we used. 
3 The Data Set 
A set of 47,274 abstracts with titles were col-
lected from MEDLINE2 as part of a systematic 
review done by the McMaster University?s Evi-
dence-Based Practice Center using TrialStat 
Corporation?s Systematic Review System 3 , a 
web-based software platform used to conduct 
systematic reviews.  
The initial set of abstracts was collected using 
a set of Boolean search queries that were run for 
                                                 
2
 http://medline.cos.com 
3
 http://www.trialstat.com/ 
304
the specific topic of the systematic review: ?the 
dissemination strategy of health care services 
for elderly people of age 65 and over?.  
In the protocol applied, two reviewers work in 
parallel. They read the entire collection of 47,274 
abstracts and answer a set of questions to deter-
mine if an abstract is relevant or not to the topic 
of review. Examples of questions present in the 
protocol: Is this article about a dissemination 
strategy or a behavioral intervention?; Is this a 
primary study?; Is this a review?; etc. An ab-
stract is not considered to pass to the next screen-
ing phase, when the entire article is available, if 
the two reviewers respond negative to the same 
question for a certain abstract. All other cases of 
possible responses suggest that the abstract will 
be part of the next screening phase. In this paper 
we focus on the initial screening phase, the only 
source of information is the abstract and the title 
of the article, with the main goal to achieve an 
acceptable level of recall not to mistakenly ex-
clude relevant abstracts.  
 From the entire collection of labeled ab-
stracts only 7,173 are relevant. Usually in the 
process of building systematic reviews the num-
ber of non-relevant documents is much higher 
than the number of relevant ones. The initial re-
trieval query is purposefully very broad, so as not 
to miss any relevant papers.  
4 Methods 
The machine learning techniques that could be 
used in the process of automating the creation of 
systematic reviews need to take into account 
some issues that can arise when dealing with 
such tasks. Imbalanced data sets are usually 
what we deal with when building reviews, the 
proportion of relevant articles that end up being 
present in the review is significantly lower com-
pared with the original data set. The benefit of 
doubt will affect the quality of the data used to 
train the classifier, since a certain amount of 
noise is introduced: abstracts that are in fact non-
relevant can be labeled as being relevant in the 
first screening process. The relatively high num-
ber of abstracts involved in the process will make 
the classification algorithms deal with a high 
number of features and the representation tech-
nique should try to capture aspects pertaining of 
the medical domain.    
 
4.1 Representation Techniques 
In our current research, we use three representa-
tion techniques: bag-of-words (BOW), concepts 
from the Unified Medical Language System 
(UMLS), and a combination of both.  
The bag-of-words representation is com-
monly used for text classification and we have 
chosen to use binary feature values. Binary fea-
ture values were shown to out-perform weighted 
values for text classification tasks in the medical 
domain as shown by Cohen et al (2006) and bi-
nary values tend to be more stable in results than 
frequency values for a task similar to ours, as 
shown by Ma (2007). 
We considered feature words delimitated by 
space and simple punctuation marks that ap-
peared at least three times in the training data, 
were not part of a stop words list4, and had a 
length greater than three characters. 30,000 word 
features were extracted. No stemming was used. 
UMLS concepts which are part of the U.S. 
National Library of Medicine 5  (NLM) knowl-
edge repository are identified and extracted form 
the collection of abstracts using the MetaMap6 
system. This conceptual representation helped us 
overcome some of the shortcomings of BOW 
representation, and allowed us to use multi-word 
features, medical knowledge, and higher-level 
meanings of words in context. As Cohen (2008) 
shows, multi-word and medical concept repre-
sentations are suitable to use.  
4.2 Classification Algorithms  
As a classification algorithm we have chosen to 
use the complement naive Bayes (CNB) (Frank 
and Bouckaert, 2006) classifier from the Weka7 
tool. The reason for this choice is that the CNB 
classifier implements state-of-the-art modifica-
tions of the standard multinomial na?ve Bayes 
(MNB) classifier for a classification task with 
highly skewed class distribution (Drummond and 
Holte, 2003). As the systematic reviews data 
usually contain a large majority of not relevant 
abstracts, resulting in a skewness reaching even 
below 1%, it is important to use appropriate clas-
sifiers.  Other classifiers, such as decision tress, 
                                                 
4
 http://www.site.uottawa.ca/~diana/csi5180/StopWords 
5
 http://www.nlm.nih.gov/pubs/factsheets/umls.html 
6
 http://mmtx.nlm.nih.gov/ 
7
 www.cs.waikato.ac.nz/machine learning/weka/ 
305
support vector machine, instance-based learning, 
and boosting, were used but the results obtained 
with CNB were always better. 
4.3 Global Text Classification Method 
The first method that we propose in order to 
solve the text classification task that is intended 
to help a systematic review process is a straight-
forward machine learning approach. We trained a 
classifier, CNB, on a collection of abstracts and 
then evaluated the classifier?s performance on a 
separate test data set. The power of this classifi-
cation technique stands in the ability to use a 
suitable classification algorithm and a good rep-
resentation for the text classification task; Cohen 
et al (2006) also used this approach. We ran-
domly split the data set described in Section 3, 
into a training set and a test set. The two possible 
classes are Included (relevant) or Excluded 
(non relevant). We decided to work with a train-
ing set smaller than the test set because ideally 
good results need to be obtained without using 
too much training data. We have to take into 
consideration that training a classifier for a par-
ticular topic, human effort is required for annota-
tion.  
  Table 1 presents a summary of the data 
along with the class distribution in the training 
and test data sets. We randomly sampled the data 
to build the training and test data sets, and the 
original distribution of 1:5.6 between the two 
classes holds in both sets.  
 
Data 
set 
No. of 
abstracts 
Class distribution 
Included : Excluded (ratio) 
Training 20,000 3,056 : 16,944 (1:5.6) 
Testing 27,274 4,117 : 23,157 (1:5.6) 
Table 1. Training and test data sets. 
 
4.3.1 Feature Selection 
 
Using the global method, we performed experi-
ments with several feature selection algorithms. 
We used only the BOW representation. 
Chi2 is a measure that evaluates the worth of an 
attribute by computing the value of the chi-
squared statistic with respect to the class. We 
selected the top k1 CHI2 features that are exclu-
sively included (appeared only in the training 
abstracts that are classified as Included) and the 
top k2 CHI2 features that are exclusively excluded 
(appeared only in the training abstracts that are 
classified as Excluded) and used them as a rep-
resentation for our data set. We varied the k1 pa-
rameter from 10 to 150 and k2 from 5 to 150 We 
used a minimum of 20 features and a maximum 
of 300. 
InfoGain evaluates the worth of an attribute 
by measuring the information gain with respect 
to the class. We run experiments when we varied 
the number of selected features from 50 to 500. 
We used a number of 50, 100, 150, 250, 300 and 
500 top features.  
Bi-Normal Separation (BNS) is a feature se-
lection technique that measures the separation 
between the threshold occurrences of a feature in 
one of the two classes. The latter measure is de-
scribed in detail in Forman (2002). We used a 
ratio of features that varies from 10 to 150 for the 
most representative features for the Included 
class and from 5 to 150 for the Excluded class. 
For some experiments the number of features for 
the Included class is higher than the number of 
features for the Excluded class. We have chosen 
to do so because we wanted to re-balance the 
imbalance of classes in the training data set. Af-
ter selecting the number of Included and Ex-
cluded features, we used the combination to rep-
resent our entire collection of abstracts.  
We used the implementation from the Weka 
package for the Chi2 and InfoGain and the BNS 
implementation done by Ma (2007).  
4.4 Per-Question Classification Method 
The second method that we propose for solving 
the task takes into account the specifics of the 
systematic review process. It takes advantage of 
the set of questions the reviewers use in the proc-
ess of deciding if an abstract is relevant or not. 
These questions are created in the design step of 
the systematic review and almost all systematic 
reviews have them. By using these questions we 
better emulate how the human judges work when 
building systematic reviews.  
 We have chosen to use only the questions 
that have inclusion/exclusion criteria, there were 
also some opened answer questions involved in 
the review, because they are the ones that are 
important for reviewers to make a decision. To 
collect training data for each question, we used 
the same training and test data set as in the pre-
vious method (but note that not all the abstracts 
306
have answers for all the questions; therefore the 
training set sizes differ for each question). Table 
2 presents the questions and data sets used. 
When we created a training data set for each 
question we removed the abstracts for which we 
had a disagreement between the human experts ? 
two different answers for a specific question, 
they represent noise in the training data.  For 
each of the questions from Table 2, we trained a 
CNB classifier on the corresponding data set.  
 
Question 
(Training : Included class : Excluded class) 
Q1 - Is this article about a dissemination strat-
egy or a behavioural intervention? (14,057:1,145: 
12,912) 
Q2 - Is the population in this article made of indi-
viduals 65-year old or older or does it comprise 
individuals who serve the elderly population needs 
(i.e. health care providers, policy makers, organi-
zations, community)? (15,005:7,360:7,645) 
Q3 - Is this a primary study? (8,825:6,895:1,930) 
Q4 - Is this a review? (6,429:5,640:789)  
Table 2. Data sets for the per-question classification 
method. 
 
We used the same representation for the per-
question classifiers as we did for the global clas-
sifier: BOW, UMLS (the concepts that appeared 
only in the new question-oriented training data 
sets), and the combination BOW+UMLS. We 
used each trained model to obtain a prediction 
for each instance from the test set; therefore each 
test instance was assigned four prediction values 
of 0 or 1. To assign a final class for each test in-
stance, from the prediction of all four classifiers, 
the class of a test instance is decided according to 
one of the following four schemes:  
 1. If any one vote is Excluded, the final class 
of a test instance is Excluded. This is a 1-vote 
scheme. 
       2. If any two votes are Excluded, the final 
class of a test instance is Excluded. This is a 2-
vote scheme. 
 3. If any three votes are Excluded, the final 
class of a test instance is Excluded. This is a 3-
vote scheme.  
 4. If all four votes are Excluded, the final 
class of a test instance is Excluded. This is a 4-
vote scheme.  
 When we combined of the classifiers, we 
gave each classifier an equal importance. 
5 Evaluation Measures and Results 
When performing the evaluation for the task of 
classifying an abstract into one of the two classes 
Included (relevant) or Excluded (non rele-
vant), two objectives are of great importance: 
Objective 1 - ensure the completeness of the sys-
tematic review (maximize the number of relevant 
documents included); Objective 2 - reduce the 
reviewers' workload (maximize the number of 
irrelevant documents excluded).  
 We observe that objective 1 is more impor-
tant than objective 2 and this is why we decided 
to report recall and precision for the Included 
class. We also report F-measure, since we are 
dealing with imbalanced data sets.  
   Besides the standard evaluation measures, 
we report WSS8 measure as well in order to give 
a clearer view of the results we obtain.  
 As baseline for our methods we consider: 
two extreme baselines and a random-baseline 
classifier that takes into account the distribution 
of the two classes in the training data set. The 
baselines results are: Include_All ? a baseline 
that classifies everything in the majority class: 
Recall = 100%, Precision = 15%, F-measure = 
26.2%; WSS = 0% Exclude_All ? a baseline that 
classifies everything as Excluded: Recall = 0%, 
Precision = 100%, F-measure = 64.2%; WSS = 
0% Random baseline: Recall = 8.9%, Precision = 
15.4%, F-measure = 67.8%; WSS = 0.23%. 
5.1 Results for the Global Method 
In this subsection, we present the results obtained 
using our global method with the three represen-
tation techniques and CNB as classification algo-
rithm. To get a clear image of the results we 
show the confusion matrix in Table 3 for the 
reader to better understand the workload reduc-
tion when using classifiers to help the process of 
building systematic reviews.  
BOW features were identified following the 
guidelines presented in Section 3.4 and a number 
of 23,906 features were selected. UMLS con-
cepts were identified using the MetaMap system. 
                                                 
8
 WSS = (TE + FE)/(TE + FE + TI + FI) ? 1+ TI/(TI + FE) 
where T stands for true; F ? false I ? Included class; E- Ex-
cluded class. 
307
  
BOW UMLS BOW+UMLS 
True Inc.  2,692 2,793 2,715 
False Inc. 5,022 8,922 5,086 
True Exc. 18,135 14,235 18,071 
False Exc. 
 1,425 1,324 1,402 
Recall 65.3% 67.8% 65.9% 
Precision 34.9% 23.8% 34.8% 
F-measure 45.5% 35.2% 45.5% 
WSS 37.1% 24.9% 37.3% 
Table 3. Results for the global method. 
 
From the whole training abstracts collection, 
a number of 459 UMLS features were identified. 
Analyzing the results from Table 5, in terms of 
recall, the UMLS representation obtained the 
best recall results, 67.8% for the global method 
but much lower precision, 23.8% than BOW rep-
resentation, 34.9%. The hybrid representation, 
BOW+ UMLS features had similar results with 
the BOW alone. Recall increased a bit for the 
hybrid representation compared to BOW alone, 
0.6% but its value is still not acceptable. We 
conclude that the levels of recall, our main objec-
tive for this task, were not acceptable for a classi-
fier to be used as replacement of a human judge 
in the workflow of building a systematic review. 
The levels of precision that we obtained with the 
global method are acceptable but they cannot 
substitute the low level of recall. Since our major 
focus is recall, we investigated more and we fur-
ther improved our precision scores with the per-
question classification method. 
 
5.1.1 Results for Feature Selection 
 
Table 4 presents the results obtained with our 
feature selection techniques. We decided to re-
port only representative results using CNB as a 
classifier and a specific representation setting. 
The number of features used in the experiment is 
presented in the round brackets. The first number 
represents the number of features extracted from 
the Included class data set while the second 
from the Excluded class data set.  
 Similar experiments were performed when 
using Na?ve Bayes as classifier. The results ob-
tained were opposite to ones obtained for CNB, 
all abstracts were classified as Excluded. We 
believe that this is the case because the CNB 
classifier tries to compensate for the class imbal-
ance and gives more credit to the minority class,  
 
 
Chi2 
(150:150) 
InfoGain 
(300) 
BNS 
(10:8) 
True Inc. 3,819 3,875 2,690 
False Inc. 19,233 19,638 13,905 
True Exc. 3,924 3,518 9,253 
False Exc. 298 242 1,427 
Recall 92.8% 94.1% 65.3% 
Precision 16.6% 16.5% 16.2% 
F-measure 28% 28% 25% 
WSS 8.2% 7.9% 4.5% 
Table 4. Representative results obtained for various 
feature selection techniques. 
  
while the Na?ve Bayes classifier will let the ma-
jority class overwhelm the classifier. 
 Besides the results presented in Table 4, we 
also tried to boost the representative features for 
the Included class hoping to re-balance the im-
balance present in the training data set. To per-
form these experiments we selected the top k 
CHI2 word features and then added to this set of 
features the top k1 CHI2 representative features 
only for the Included class. The parameter k var-
ied from 50 to 100 and the parameter k1 from 30 
to 70. We performed experiments when using the 
original imbalanced training data set and using a 
balanced data set as well, with both CNB and 
Na?ve Bayes classifier. The results obtained for 
these experiments were similar to the ones when 
we used the previous feature selection tech-
niques. There was no significant difference in the 
results compared to the ones in Table 5. 
5.2 Results for the Per-Question Method 
The results for our second method using the four 
voting schemes are presented in Table 5.  
Compared with the global method the results 
obtained by the per-question method, especially 
the ones for 2 votes are the best so far in terms of 
the balance between the two objectives. A large 
number of abstracts that should be excluded are 
classified as Excluded whereas wrongly exclud-
ing very few abstracts that should have been in-
cluded (a lot fewer than in the case of the global 
classification method).  
The 2-votes scheme performs better than the 
1-vote schemes because of potential classifica-
tion errors. When the classifiers for two different 
questions (that look at two different aspects of 
the systematic review topic) are confident that 
the abstract is not relevant, the chance of correct 
308
prediction is higher; a balance between excluding 
an article and keeping it as relevant is achieved. 
When using the classifiers for 3 or 4 questions 
the performance goes down in terms of precision; 
a higher number of abstracts get classified as In-
cluded - some abstracts do not address all target 
question of the review topic.  
 
1-Vote  BOW UMLS BOW+UMLS 
True Inc. 1,262 1,222 1,264 
False Inc. 745 2,266 741 
True Exc. 22,412 20,891 22,416 
False Exc. 2,855 2,895 2,853 
Recall 30.6% 29.6% 30.7% 
Precision 62.8% 35.0% 63.0% 
F-measure 41.2% 32.1% 41.2% 
WSS 23.2% 16.8% 23.3% 
2-Vote BOW UMLS BOW+UMLS 
True Inc. 3,181 2,603 3,283 
False Inc. 9,976 9,505 10,720 
True Exc. 13,181 13,652 12,437 
False Exc. 936 1,514 834 
Recall 77.2% 63.2% 79.7% 
Precision 24.1% 21.5% 23.4% 
F-measure 36.8% 32.0% 36.2% 
WSS 29.0% 18.8% 28.4% 
3-Vote  BOW UMLS BOW+UMLS 
True Inc. 3,898 3,480 3,890 
False Inc. 18,915 16,472 18,881 
True Exc. 4,242 6,685 4,276 
False Exc. 219 637 227 
Recall 94.6% 84.5% 94.4% 
Precision 17.0% 17.4% 17.0% 
F-measure 28.9% 28.9% 28.9% 
WSS 11.0% 11.3% 11.0% 
4-Vote  BOW UMLS BOW+UMLS 
True Inc. 4,085 3,947 4,086 
False Inc. 21,946 20,869 21,964 
True Exc. 1,211 2,288 1,193 
False Exc. 32 170 31 
Recall 99.2% 95.8% 99.2% 
Precision 15.6% 15.9% 15.6% 
F-measure 27.1% 27.2% 27.0% 
WSS 3.7% 4.8% 3.7% 
Table 5. Results for the per-question method for the 
Included class. 
 
For the per-question technique the recall value 
peaked at 99.2% with the 4-vote method BOW 
and BOW+UMLS representation technique. In 
the same time the lowest values of precision for 
the per-question technique, 15.6% is obtained 
with the same experimental setting. It is impor-
tant to aim for a high recall but not to dismiss the 
precision values. The difference of even less than 
2% in precision values can cause the reviewers to 
read additional thousands of documents, as ob-
served in the confusion matrices for 2-vote, 3-
vote and 4-vote methods in Table 5.  
From the confusion matrix in Table 5 for the 
2-vote method and the 3- and 4-vote method we 
observe the high difference in the number of 
documents a reviewer will have to read (the 
falsely included documents). The difference in 
precision from 24.1% for the 2-vote method to 
15.6% for the 4-vote method makes the reviewer 
go through 11,988 additional abstracts.  
The best value for the WSS measure for the 
per-question method is achieved by the 2-vote 
scheme. The result is lower than the one obtained 
by the global method but the recall level is higher 
therefore, we still keep as a potential winner the 
2-vote scheme.  
5.3 Results for Human-Machine Workflow 
In Figure 1, we envisioned the way we can use 
the automatic classifier in the workflow of build-
ing a systematic review. In order to determine the 
performance of the human-machine workflow 
that we propose we computed the recall values 
when the human reviewer?s labels are combined 
with the labels obtained from the classifier. The 
same labeling technique is applied as for the hu-
man-human workflow: if at least one decision for 
an abstract is to include it in the systematic re-
view, then the final label is Included.  
 We also calculated the evaluation measures 
for the two reviewers. The evaluation measures 
for the human judge that is kept in the human-
machine workflow, Reviewer 1 in Figure 1, are 
64.29% for recall and 15.20% for precision. The 
evaluation measures for the reviewer that is to be 
replaced in the human-machine classification, 
Reviewer 2 in Figure 1 are 59.66% for recall and 
15.09% for precision. The recall value for the 
two human judges combined is 85.26% and the 
precision value is 100%. As we can observe the 
recall value for the second reviewer, the one that 
is replaced in the human-classifier workflow is 
low. In Table 6 we present precision and recall 
results for the symbiotic model for both our me-
thods. In these results we can clearly see that the 
2-vote technique is superior to the other voting 
techniques and to the global method. For almost 
the same level of precision the level or recall it is 
much higher. These observations support the fact 
309
that the extra effort spent in identifying the most 
suitable methodology pays off.  
 The fact that we keep a human in the loop 
makes our method acceptable as a workflow for 
building a systematic review.  
 
Method BOW UMLS BOW+ 
UMLS 
Global    17.9/87.7% 17.0/88.6% 17.9/87.7% 
1-Vote 17.1/75.3% 16.5/74.8% 17.1/75.4% 
2-Vote 17.1/91.6% 16.4/86.6% 17.1/92.7% 
3-Vote 15.8/97.9% 15.8/94.2% 15.8/97.8% 
4-Vote 15.3/99.6% 15.4/98.3% 15.3/99.6% 
Table 6. Precision/recall results for the human-
classifier workflow for the Included class. 
6 Discussion 
The global method achieves good results in terms 
of precision while the best recall is obtained by 
the per-question method.   
 The best results for the task were obtained 
using the per-question method with the 2-vote 
scheme with or without UMLS features. The 3-
vote scheme with UMLS representation is close 
to the 2-vote scheme but looking at F-measure 
and WSS results the 2-vote scheme is better. The 
clear distinction between the methods comes 
when we combined the classifiers with the hu-
man judge in the workflow of building reviews. 
The per-question technique is more robust 
and it offers the possibility to choose the desired 
type of performance. If the reviewers are willing 
to read almost the entire collection of documents, 
knowing that the recall is high, then a 3 or 4-vote 
scheme can be the set-up (though the 3 or 4-vote 
method is not likely to achieve 100% recall be-
cause it is very rare that an abstract contain an-
swers to three or four of the questions associated 
with the systematic review). If the reviewers will 
like to read a small collection being confident 
that almost all the abstracts are relevant, then a 1-
vote scheme can be the set-up required. The per-
question method confirms the fact that an en-
semble of classifiers is better than one classifier; 
(Dietterich, 1997).  
When we combine the human and the system 
results we obtain a major improved in terms of 
recall. We base our discussion for the human-
machine results for the experiment that obtained 
the best results, the 2-vote scheme with a 
BOW+UMLS representation technique. When 
combining the human and classifier decisions, 
the precision level decreased a bit compared to 
the one that the machine obtained. We believe 
that this is the case because some of the abstracts 
that the classifier excluded were included by the 
first human reviewer and, with this decision 
process in place, the level of precision dropped. 
Our goal of improving the recall level from 
the first level of screening is achieved, since 
when both the classifier and the human judge are 
integrated in the workflow, the recall level jumps 
from 79.7% to 92.7%. 
We believe that the low level of precision 
that is obtained for the human reviewer, for the 
human-classifier workflow, and for the classifier, 
is due to the fact that we are running experiments 
for the first screening phase when we use only 
the abstracts as source of information and not the 
entire articles.  
We believe that further investigations are re-
quired to fully replace a human reviewer with an 
automatic classifier but the results obtained with 
the per-question method encourage us to believe 
that this is a suitable solution for reaching our 
final goal.  
7 Conclusions and Future Work  
In this paper, we looked at two methods by 
which we envision the way automatic text classi-
fication techniques could help the workflow of 
building systematic reviews.  
The first method is a straight-forward appli-
cation of the representations and learning algo-
rithms that capture the specifics of the data: med-
ical domain, huge number of features, misclassi-
fication, and imbalanced classes.  
We showed that the specifics of the human 
protocol in which systematic reviews are built 
have a positive effect when deployed in an auto-
matic way. We believe that the tedious process 
that is currently used for building systematic re-
views can be lightened by the use of a classifier 
in combination with only one human judge. By 
having a human judge in the loop, we ensure that 
the workflow is reliable and that the system can 
be easily integrated in the workflow.  
 In future work we would like to look into 
ways of improving the results by the way we 
chose the training data set and by integrating 
more domain specific knowledge. We would also 
like to investigate ways by witch we can update 
systematic reviews.  
310
References  
Aphinyanaphongs Y. and Aliferis C. Text Categoriza-
tion Models for Retrieval of High Quality Articles. 
Journal of the American Medical Informatics As-
sociation 2005; 12:207-216. 
Cohen A.M. Optimizing Feature Representation for 
Automated Systematic Review Work Prioritization. 
Proceedings of the AMIA Annual Symposium 
2008; 6:121-126. 
Cohen A.M., Hersh W.R., Peterson K., Yen P.Y. Re-
ducing Workload in Systematic Review Prepara-
tion Using Automated Citation Classification. 
Journal of the American Medical Informatics As-
sociation 2006; 13:206-219. 
Dietterich, T. Machine-Learning Research: Four 
Current Directions. Artificial Intelligence Maga-
zine. 18(4): 97-136 (1997) 
Drummond C. and Holte R.C. C4.5, Class Imbalance, 
and Cost Sensitivity: Why Under-Sampling beats 
Over-Sampling. Proceedings of the Twentieth In-
ternational Conference on Machine Learning: 
Workshop on Learning from Imbalanced Data Sets 
(II), 2003. 
Forman G. Choose Your Words Carefully: An Empiri-
cal Study of Feature Selection Metrics for Text 
Classification. In the Joint Proceedings of the 13th 
European Conference on Machine Learning and 
the 6th European Conference on Principles and 
Practice of Knowledge Discovery in Databases 
(ECML/PKDD), 2002. 
Frank E. and Bouckaert R.R. Naive Bayes for Text 
Classification with Unbalanced Classes. In the 
Proceedings of the 10th European Conference on 
Principles and Practice of Knowledge Discovery in 
Databases, Berlin, Germany, 2006, pp. 503-510. 
Haynes R.B., Wilczynski N., McKibbon K.A., Walker 
C.J., Sinclair J.C. Developing optimal search strat-
egies for detecting clinically sound studies in 
MEDLINE. Journal of the American Medical In-
formatics Association 1994; 1:447-58. 
Kohavi R. and Provost F. Glossary of Terms. Editorial 
for the Special Issue on Applications of Machine 
Learning and the Knowledge Discovery Process 
1998; 30:271-274. 
Ma Y. 2007. Text classification on imbalanced data: 
Application to Systematic Reviews Automation.  
M.Sc. Thesis. University of Ottawa. 
311
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 152?161,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Segmentation Similarity and Agreement
Chris Fournier
University of Ottawa
Ottawa, ON, Canada
cfour037@eecs.uottawa.ca
Diana Inkpen
University of Ottawa
Ottawa, ON, Canada
diana@eecs.uottawa.ca
Abstract
We propose a new segmentation evaluation
metric, called segmentation similarity (S), that
quantifies the similarity between two segmen-
tations as the proportion of boundaries that
are not transformed when comparing them us-
ing edit distance, essentially using edit dis-
tance as a penalty function and scaling penal-
ties by segmentation size. We propose several
adapted inter-annotator agreement coefficients
which use S that are suitable for segmenta-
tion. We show that S is configurable enough
to suit a wide variety of segmentation evalua-
tions, and is an improvement upon the state of
the art. We also propose using inter-annotator
agreement coefficients to evaluate automatic
segmenters in terms of human performance.
1 Introduction
Segmentation is the task of splitting up an item, such
as a document, into a sequence of segments by plac-
ing boundaries within. The purpose of segmenting
can vary greatly, but one common objective is to
denote shifts in the topic of a text, where multiple
boundary types can also be present (e.g., major ver-
sus minor topic shifts). Human-competitive auto-
matic segmentation methods can help a wide range
of computational linguistic tasks which depend upon
the identification of segment boundaries in text.
To evaluate automatic segmentation methods, a
method of comparing an automatic segmenter?s per-
formance against the segmentations produced by hu-
man judges (coders) is required. Current meth-
ods of performing this comparison designate only
one coder?s segmentation as a reference to com-
pare against. A single ?true? reference segmentation
from a coder should not be trusted, given that inter-
annotator agreement is often reported to be rather
poor (Hearst, 1997, p. 54). Additionally, to en-
sure that an automatic segmenter does not over-fit
to the preference and bias of one particular coder,
an automatic segmenter should be compared directly
against multiple coders.
The state of the art segmentation evaluation met-
rics (Pk and WindowDiff) slide a window across a
designated reference and hypothesis segmentation,
and count the number of windows where the number
of boundaries differ. Window-based methods suffer
from a variety of problems, including: i) unequal
penalization of error types; ii) an arbitrarily defined
window size parameter (whose choice greatly af-
fects outcomes); iii) lack of clear intuition; iv) in-
applicability to multiply-coded corpora; and v) re-
liance upon a ?true? reference segmentation.
In this paper, we propose a new method of
comparing two segmentations, called segmentation
similarity 1 (S), that: i) equally penalizes all er-
ror types (unless explicitly configured otherwise);
ii) appropriately responds to scenarios tested; iii) de-
fines no arbitrary parameters; iv) is intuitive; and
v) is adapted for use in a variety of popular inter-
annotator agreement coefficients to handle multiply-
coded corpora; and vi) does not rely upon a ?true?
reference segmentation (it is symmetric). Capitaliz-
ing on the adapted inter-annotator agreement coeffi-
cients, the relative difficulty that human segmenters
have with various segmentation tasks can now be
quantified. We also propose that these coefficients
can be used to evaluate and compare automatic seg-
mentation methods in terms of human agreement.
This paper is organized as follows. In Sec-
tion 2, we review segmentation evaluation and inter-
annotator agreement. In Section 3, we present S and
1A software implementation of segmentation similarity (S)
is available at http://nlp.chrisfournier.ca/
152
inter-annotator agreement coefficient adaptations. In
Section 4, we evaluate S and WindowDiff in vari-
ous scenarios and simulations, and upon a multiply-
coded corpus.
2 Related Work
2.1 Segmentation Evaluation
Precision, recall, and their mean (F?-measure) have
been previously applied to segmentation evaluation.
Precision is the proportion of boundaries chosen that
agree with a reference segmentation, and recall is
the proportion of boundaries chosen that agree with
a reference segmentation out of all boundaries in the
reference and hypothesis (Pevzner and Hearst, 2002,
p. 3). For segmentation, these metrics are unsuitable
because they penalize near-misses of boundaries as
full-misses, causing them to drastically overestimate
the error. Near-misses are prevalent in segmentation
and can account for a large proportion of the errors
produced by a coder, and as inter-annotator agree-
ment often shows, they do not reflect coder error,
but the difficulty of the task.
Pk (Beeferman and Berger, 1999, pp. 198?200)2
is a window-based metric which attempts to solve
the harsh near-miss penalization of precision, recall,
and F?-measure. In Pk, a window of size k, where
k is defined as half of the mean reference segment
size, is slid across the text to compute penalties.
A penalty of 1 is assigned for each window whose
boundaries are detected to be in different segments
of the reference and hypothesis segmentations, and
this count is normalized by the number of windows.
Pevzner and Hearst (2002, pp. 5?10) highlighted
a number of issues with Pk, specifically that: i) False
negatives (FNs) are penalized more than false pos-
itives (FPs); ii) It does not penalize FPs that fall
within k units of a reference boundary; iii) Its sen-
sitivity to variations in segment size can cause it to
linearly decrease the penalty for FPs if the size of
any segments fall below k; and iv) Near-miss errors
are too harshly penalized.
To attempt to mitigate the shortcomings of Pk,
Pevzner and Hearst (2002, p. 10) proposed a
modified metric which changed how penalties were
2Pk is a modification of P? (Beeferman et al, 1997, p. 43).
Other modifications such as TDT Cseg (Doddington, 1998, pp.
5?6) have been proposed, but Pk has seen greater usage.
counted, named WindowDiff (WD). A window of
size k is still slid across the text, but now penal-
ties are attributed to windows where the number of
boundaries in each segmentation differs (see Equa-
tion 1, where b(Rij) and b(Hij) represents the num-
ber of boundaries within the segments in a window
of size k from position i to j, and N the number of
sentences plus one), with the same normalization.
WD(R,H) =
1
N ? k
N?k?
i=1,j=i+k
(|b(Rij)?b(Hij)| > 0) (1)
WindowDiff is able to reduce, but not eliminate,
sensitivity to segment size, gives more equal weights
to both FPs and FNs (FNs are, in effect, penalized
less3), and is able to catch mistakes in both small
and large segments. It is not without issues though;
Lamprier et al (2007) demonstrated that WindowD-
iff penalizes errors less at the beginning and end of
a segmentation (this is corrected by padding the seg-
mentation at each end by size k). Additionally, vari-
ations in the window size k lead to difficulties in in-
terpreting and comparing WindowDiff?s values, and
the intuition of the method remains vague.
Franz et al (2007) proposed measuring perfor-
mance in terms of the number of words that are FNs
and FPs, normalized by the number of word posi-
tions present (see Equation 2).
RFN =
1
N
?
w
FN(w), RFP =
1
N
?
w
FP (w) (2)
RFN and RFP have the advantage that they take
into account the severity of an error in terms of seg-
ment size, allowing them to reflect the effects of er-
roneously missing, or added, words in a segment
better than window based metrics. Unfortunately,
RFN and RFP suffer from the same flaw as preci-
sion, recall, and F?-measure in that they do not ac-
count for near misses.
2.2 Inter-Annotator Agreement
The need to ascertain the agreement and reliabil-
ity between coders for segmentation was recognized
3Georgescul et al (2006, p. 48) note that both FPs and FNs
are weighted by 1/N?k, and although there are ?equiprobable
possibilities to have a [FP] in an interval of k units?, ?the total
number of equiprobable possibilities to have a [FN] in an inter-
val of k units is smaller than (N?k)?, making the interpretation
of a full miss as a FN less probable than as a FP.
153
by Passonneau and Litman (1993), who adapted the
percentage agreement metric by Gale et al (1992,
p. 254) for usage in segmentation. This percentage
agreement metric (Passonneau and Litman, 1993, p.
150) is the ratio of the total observed agreement of a
coder with the majority opinion for each boundary
over the total possible agreements. This measure
failed to take into account chance agreement, or to
less harshly penalize near-misses.
Hearst (1997) collected segmentations from 7
coders while developing the automatic segmenter
TextTiling, and reported mean ? (Siegel and Castel-
lan, 1988) values for coders and automatic seg-
menters (Hearst, 1997, p. 56). Pairwise mean ?
scores were calculated by comparing a coder?s seg-
mentation against a reference segmentation formu-
lated by the majority opinion strategy used in Pas-
sonneau and Litman (1993, p. 150) (Hearst, 1997,
pp. 53?54). Although mean ? scores attempt to
take into account chance agreement, near misses are
still unaccounted for, and use of Siegel and Castel-
lan?s (1988) ? has declined in favour of other coeffi-
cients (Artstein and Poesio, 2008, pp. 555?556).
Artstein and Poesio (2008) briefly touch upon
recommendations for coefficients for segmentation
evaluation, and though they do not propose a mea-
sure, they do conjecture that a modification of a
weighted form of ? (Krippendorff, 1980; Krippen-
dorff, 2004) using unification and WindowDiff may
suffice (Artstein and Poesio, 2008, pp. 580?582).
3 Segmentation Similarity
For discussing segmentation, a segment?s size (or
mass) is measured in units, the error is quantified
in potential boundaries (PBs), and we have adopted
a modified form of the notation used by Artstein and
Poesio (2008), where the set of:
? Items is {i|i ? I} with cardinality i;
? Categories is {k|k ? K} with cardinality k;
? Coders is {c|c ? C} with cardinality c;
? Segmentations of an item i by a coder c is {s|s ?
S}, where when sic is specified with only one sub-
script, it denotes sc, for all relevant items (i); and
? Types of segmentation boundaries is {t|t ? T} with
cardinality t.
3.1 Sources of Dissimilarity
Linear segmentation has three main types of errors:
1. s1 contains a boundary that is off by n PBs in s2;
2. s1 contains a boundary that s2 does not; or
3. s2 contains a boundary that s1 does not.
These types of errors can be seen in Figure 1, and
are conceptualized as a pairwise transposition of a
boundary for error 1, and the insertion or deletion
(depending upon your perspective) of a boundary for
errors 2 and 3. Since we do not designate either seg-
mentation as a reference or hypothesis, we refer to
insertions and deletions both as substitutions.
s1
s2
1 3 2
Figure 1: Types of segmentations errors
It is important to not penalize near misses as full
misses in many segmentation tasks because coders
often agree upon the existence of a boundary, but
disagree upon its exact location. In the previous sce-
nario, assigning a full miss would mean that even a
boundary loosely agreed-upon, as in Figure 1, error
1, would be regarded as completely disagreed-upon.
3.2 Edit Distance
In S, concepts from Damereau-Levenshtein edit dis-
tance (Damereau, 1964; Levenshtein, 1966) are ap-
plied to model segmentation edit distance as two op-
erations: substitutions and transpositions.4 These
two operations represent full misses and near misses,
respectively. Using these two operations, a new
globally-optimal minimum edit distance is applied
to a pair of sequences of sets of boundaries to model
the sources of dissimilarity identified earlier.5
Near misses that are remedied by transposition are
penalized as b PBs of error (where b is the number
of boundaries transposed), as opposed to the 2b PBs
of errors by which they would be penalized if they
were considered to be two separate substitution op-
erations. Transpositions can also be considered over
n > 2 PBs (n-wise transpositions). This is useful
if, for a specific task, near misses of up to n PBs are
not to be penalized as full misses (default n = 2).
The error represented by the two operations can
also be scaled (i.e., weighted) from 1 PB each to a
4Beeferman et al (1997, p. 42) briefly mention using an edit
distance without transpositions, but discard it in favour of P?.
5For multiple boundaries, an add/del operation is added, and
transpositions are considered only within boundary types.
154
fraction. The distance over which an n-wise trans-
position occurred can also be used in conjunction
with the scalar operation weighting so that a transpo-
sition is weighted using the function in Equation 3.
te(n, b) = b? (1/b)
n?2 where n ? 2 and b > 0 (3)
This transposition error function was chosen so
that, in an n-wise transposition where n = 2 PBs
and the number of boundaries transposed b = 2, the
penalty would be 1 PB, and the maximum penalty as
limn?? te(n) would be b PBs, or in this case 2 PBs
(demonstrated later in Figure 5b).
3.3 Method
In S, we conceptualize the entire segmentation, and
individual segments, as having mass (i.e., unit mag-
nitude/length), and quantify similarity between two
segmentations as the proportion of boundaries that
are not transformed when comparing segmentations
using edit distance, essentially using edit distance as
a penalty function and scaling penalties by segmen-
tation size. S is a symmetric function that quantifies
the similarity between two segmentations as a per-
centage, and applies to any granularity or segmenta-
tion unit (e.g., paragraphs, sentences, clauses, etc.).
Consider a somewhat contrived example
containing?for simplicity and brevity?only one
boundary type (t = 1). First, a segmentation must
be converted into a sequence of segment mass
values (see Figure 2).
0 1 2 3 4 5 6
? 1 3 2
Figure 2: Annotation of segmentation mass
Then, a pair of segmentations are converted into
parallel sequences of boundary sets, where each set
contains the types of boundaries present at that po-
tential boundary location (if there is no boundary
present, then the set is empty), as in Figure 3.
s1
s2
1 2 2 3 3 1 2
{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}
1 2 1 2 6 2
{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}
Figure 3: Segmentations annotated with mass and their
corresponding boundary set sequences
The edit distance is calculated by first identify-
ing all potential substitution operations that could
occur (in this case 5). A search for all potential n-
wise transpositions that can be made over n adja-
cent sets between the sequences is then performed,
searching from the beginning of the sequence to the
end, keeping only those transpositions which do not
overlap and which result in transposing the most
boundaries between the sequences (to minimize the
edit distance). In this case, we have only one non-
overlapping 2-wise transposition. We then subtract
the number of boundaries involved in transpositions
between the sequences (2 boundaries) from the num-
ber of substitutions, giving us an edit distance of 4
PBs: 1 transposition PB and 3 substitution PBs.
s1
s2
1 2 2 3 3 1 2
{1} {} {1} {} {1} {} {} {1} {} {} {1} {1} {}
1 2 1 2 6 2
{1} {} {1} {1} {} {1} {} {} {} {} {} {1} {}
Transposition
Sub. Sub. Sub.
Figure 4: Edit operations performed on boundary sets
Edit distance, and especially the number of oper-
ations of each type performed, is useful in identi-
fying the number of full and near misses that have
occurred?which indicates whether one?s choice of
transposition window size n is either too generous
or too harsh. Edit distance as a penalty does not
incorporate information on the severity of an error
with respect to the size of a segment, and is not an
easily comparable value without some form of nor-
malization. To account for these issues, we define
S so that boundary edit distance is used to subtract
penalties for each edit operation that occurs, from
the number of potential boundaries in a segmenta-
tion, normalizing this value by the total number of
potential boundaries in a segmentation.
S(si1, si2) =
t ?mass(i)? t? d(si1, si1, T )
t ?mass(i)? t
(4)
S, as shown in Equation 4, scales the mass of the
item by the cardinality of the set of boundary types
(t) because the edit distance function d(si1, si1, T )
will return a value of [0, t ? mass(i)] PBs, where
t ? Z+?while subtracting the edit distance and t.6
6The number of potential boundaries in a segmentation si
155
The numerator is normalized by the total number
of potential boundaries per boundary type. This re-
sults in a function with a range of [0, 1]. It returns 0
when one segmentation contains no boundaries, and
the other contains the maximum number of possible
boundaries. It returns 1 when both segmentations
are identical.
Using the default configuration of this equation,
S = 9/13 = 0.6923, a very low similarity, which
WindowDiff also agrees upon (1?WD = 0.6154).
The edit-distance function d(si1, si1, T ) can also be
assigned values of the range [0, 1] as scalar weights
(wsub, wtrp) to reduce the penalty attributed to par-
ticular edit operations, and configured to use a trans-
position error function (Equation 3, used by default).
3.4 Evaluating Automatic Segmenters
Coders often disagree in segmentation tasks (Hearst,
1997, p. 56), making it improbable that a single,
correct, reference segmentation could be identified
from human codings. This improbability is the re-
sult of individual coders adopting slightly differ-
ent segmentation strategies (i.e., different granular-
ity). In light of this, we propose that the best avail-
able evaluation strategy for automatic segmentation
methods is to compare performance against multiple
coders directly, so that performance can be quanti-
fied relative to human reliability and agreement.
To evaluate whether an automatic segmenter
performs on par with human performance, inter-
annotator agreement can be calculated with and
without the inclusion of an automatic segmenter,
where an observed drop in the coefficients would
signify that the automatic segmenter does not per-
form as reliably as the group of human coders.7 This
can be performed independently for multiple auto-
matic segmenters to compare them to each other?
assuming that the coefficients model chance agree-
ment appropriately?because agreement is calculated
(and quantifies reliability) over all segmentations.
3.5 Inter-Annotator Agreement
Similarity alone is not a sufficiently insightful mea-
sure of reliability, or agreement, between coders.
with t boundary types is t ?mass(i)? t.
7Similar to how human competitiveness is ascertained by
Medelyan et al (2009, pp. 1324?1325) and Medelyan (2009,
pp. 143?145) by comparing drops in inter-indexer consistency.
Chance agreement occurs in segmentation when
coders operating at slightly different granularities
agree due to their codings, and not their own in-
nate segmentation heuristics. Inter-annotator agree-
ment coefficients have been developed that assume a
variety of prior distributions to characterize chance
agreement, and to attempt to offer a way to iden-
tify whether agreement is primarily due to chance,
or not, and to quantify reliability.
Artstein and Poesio (2008) note that most of a
coder?s judgements are non-boundaries. The class
imbalance caused by segmentations often contain-
ing few boundaries, paired with no handling of near
misses, causes most inter-annotator agreement co-
efficients to drastically underestimate agreement on
segmentations. To allow for agreement coefficients
to account for near misses, we have adapted S for use
with Cohen?s ?, Scott?s pi, Fleiss?s multi-pi (pi?), and
Fleiss?s multi-? (??), which are all coefficients that
range from [Ae/1?Ae , 1], where 0 indicates chance
agreement, and 1 perfect agreement. All four coeffi-
cients have the general form:
?, pi, ??, and pi? =
Aa ? Ae
1? Ae
(5)
For each agreement coefficient, the set of cate-
gories is defined as solely the presence of a bound-
ary (K = {segt|t ? T}), per boundary type (t).
This category choice is similar to those chosen by
Hearst (1997, p. 53), who computed chance agree-
ment in terms of the probability that coders would
say that a segment boundary exists (segt), and the
probability that they would not (unsegt). We have
chosen to model chance agreement only in terms of
the presence of a boundary, and not the absence,
because coders have only two choices when seg-
menting: to place a boundary, or not. Coders do
not place non-boundaries. If they do not make a
choice, then the default choice is used: no boundary.
This default option makes it impossible to determine
whether a segmenter is making a choice by not plac-
ing a boundary, or whether they are not sure whether
a boundary is to be placed.8 For this reason, we
only characterize chance agreement between coders
in terms of one boundary presence category per type.
8This could be modelled as another boundary type, which
would be modelled in S by the set of boundary types T .
156
3.5.1 Scott?s pi
Proposed by Scott (1955), pi assumes that chance
agreement between coders can be characterized as
the proportion of items that have been assigned to
category k by both coders (Equation 7). We cal-
culate agreement (Apia ) as pairwise mean S (scaled
by each item?s size) to enable agreement to quantify
near misses leniently, and chance agreement (Apie )
can be calculated as in Artstein and Poesio (2008).
Apia =
?
i?I mass(i) ? S(si1, si2)?
i?I mass(i)
(6)
Apie =
?
k?K
(
Ppie (k)
)2
(7)
We calculate chance agreement per category as
the proportion of boundaries (segt) assigned by all
coders over the total number of potential boundaries
for segmentations, as shown in Equation 8.
Ppie (segt) =
?
c?C
?
i?I |boundaries(t, sic)|
c ?
?
i?I
(
mass(i)? 1
) (8)
This adapted coefficient appropriately estimates
chance agreement in situations where there no in-
dividual coder bias.
3.5.2 Cohen?s ?
Proposed by Cohen (1960), ? characterizes
chance agreement as individual distributions per
coder, calculated as shown in Equations 9-10 using
our definition of agreement (Apia ) as shown earlier.
A?a = A
pi
a (9)
A?e =
?
k?K
P?e (k|c1) ? P
?
e (k|c2) (10)
We calculate category probabilities as in Scott?s
pi, but per coder, as shown in Equation 11.
P?e (segt|c) =
?
i?I |boundaries(t, sic)|
?
i?I
(
mass(i)? 1
) (11)
This adapted coefficient appropriately estimates
chance agreement for segmentation evaluations
where coder bias is present.
3.5.3 Fleiss?s Multi-pi
Proposed by Fleiss (1971), multi-pi (pi?) adapts
Scott?s pi for multiple annotators. We use Artstein
and Poesio?s (2008, p. 564) proposal for calculat-
ing actual and expected agreement, and because all
coders rate all items, we express agreement as pair-
wise mean S between all coders as shown in Equa-
tions 12-13, adapting only Equation 12.
Api
?
a =
1
(c
2
)
c?1?
m=1
c?
n=m+1
?
i?I mass(i) ? S(sim, sin)
?
i?I
(
mass(i)? 1
) (12)
Api
?
e =
?
k?K
(
Ppie (k)
)2
(13)
3.5.4 Fleiss?s Multi-?
Proposed by Davies and Fleiss (1982), multi-?
(??) adapts Cohen?s ? for multiple annotators. We
use Artstein and Poesio?s (2008, extended version)
proposal for calculating agreement just as in pi?, but
with separate distributions per coder as shown in
Equations 14-15.
A?
?
a = A
pi?
a (14)
A?
?
e =
?
k?K
(
1
(c
2
)
c?1?
m=1
c?
n=m+1
P?e (k|cm) ? P
?
e (k|cn)
)
(15)
3.6 Annotator Bias
To identify the degree of bias in a group of coders?
segmentations, we can use a measure of variance
proposed by Artstein and Poesio (2008, p. 572) that
is quantified in terms of the difference between ex-
pected agreement when chance is assumed to vary
between coders, and when it is assumed to not.
B = Api
?
e ?A
??
e (16)
4 Experiments
To demonstrate the advantages of using S, as op-
posed to WindowDiff (WD), we compare both met-
rics using a variety of contrived scenarios, and then
compare our adapted agreement coefficients against
pairwise meanWD9 for the segmentations collected
by Kazantseva and Szpakowicz (2012).
In this section, because WD is a penalty-based
metric, it is reported as 1?WD so that it is easier
to compare against S values. When reported in this
way, 1?WD and S both range from [0, 1], where 1
represents no errors and 0 represents maximal error.
9Permuted, and with window size recalculated for each pair.
157
0 20 40 60 80 100
0
0.2
0.4
0.6
0.8
1
Number of full misses / false positives
1?WD
S
(a) Increasing the number of full misses,
or FPs, where k = 25 for WD
0 2 4 6 8 10
0.7
0.8
0.9
1
Distance between boundaries in each seg. (units)
1?WD
S(n = 3)
S(n = 5,scale)
S(n = 5,wtrp = 0)
(b) Increasing the distance between two
boundaries considered to be a near miss
until metrics consider them a full miss
0 20 40 60 80 100
0.2
0.4
0.6
0.8
1
Segmentation mass (m)
1?WD
S
k/m
(c) Increasing the mass m of segmenta-
tions configured as shown in Figure 10
showing the effect of k on 1?WD
Figure 5: Responses of 1?WD and S to various segmentation scenarios
4.1 Segmentation Cases
Maximal versus minimal segmentation When
proposing a new metric, its reactions to extrema
must be illustrated, for example when a maximal
segmentation is compared to a minimal segmenta-
tion, as shown in Figure 6. In this scenario, both
1?WD and S appropriately identify that this case
represents maximal error, or 0. Though not shown
here, both metrics also report a similarity of 1.0
when identical segmentations are compared.
s1
s2
14
1 1 1 1 1 1 1 1 1 1 1 1 1 1
Figure 6: Maximal versus minimal seg. masses
Full misses For the most serious source of error,
full misses (i.e., FPs and FNs), both metrics appro-
priately report a reduction in similarity for cases
such as Figure 7 that is very similar (1?WD =
0.8462, S= 0.8461). Where the two metrics differ
is when this type of error is increased.
s1
s2
1 2 2 2 4 2 1
1 2 8 2 1
Figure 7: Full misses in seg. masses
S reacts to increasing full misses linearly, whereas
WindowDiff can prematurely report a maximal
number of errors. Figure 5a demonstrates this ef-
fect, where for each iteration we have taken seg-
mentations of 100 units of mass with one matching
boundary at the first hypothesis boundary position,
and uniformly increased the number of internal hy-
pothesis segments, giving us 1 matching boundary,
and [0, 98] FPs. This premature report of maximal
error (at 7 FP) by WD is caused by the window size
(k = 25) being greater than all of the internal hy-
pothesis segment sizes, making all windows penal-
ized for containing errors.
Near misses When dealing with near misses, the
values of both metrics drop (1?WD = 0.8182,
S = 0.9231), but to greatly varying degrees. In
comparison to full misses, WindowDiff penalizes a
near miss, like that in Figure 8, far more than S.
This difference is due to the distance between the
two boundaries involved in a near miss; S shows,
in this case, 1 PB of error until it is outside of the
n-wise transposition window (where n = 2 PBs),
at which point it is considered an error of not one
transposition, but two substitutions (2 PBs).
s1
s2
6 8
7 7
Figure 8: Near misses in seg. masses
If we wanted to completely forgive near misses
up to n PBs, we could set the weighting of trans-
positions in S to wtrp = 0. This is useful if a spe-
cific segmentation task accepts that near misses are
very probable, and that there is little cost associated
with a near miss in a window of n PBs. We can
also set n to a high number, i.e., 5 PBs, and use the
scaled transposition error (te) function (Equation 3)
to slowly increase the error from b = 1 PB to b = 2
PBs, as shown in Figure 5b, which shows how both
158
Scenario 1: FN, p = 0.5 Scenario 2: FP, p = 0.5 Scenario 3: FP and FN, p = 0.5
(20,30) (15,35) (20,30) (15,35) (20,30) (15,35)
WD 0.2340? 0.0113 0.2292? 0.0104 0.2265? 0.0114 0.2265? 0.0111 0.3635? 0.0126 0.3599? 0.0117
S 0.9801? 0.0006 0.9801? 0.0006 0.9800? 0.0006 0.9800? 0.0006 0.9605? 0.0009 0.9603? 0.0009
(10,40) (5,45) (10,40) (5,45) (10,40) (5,45)
WD 0.2297? 0.0105 0.2206? 0.0079 0.2256? 0.0102 0.2184? 0.0069 0.3516? 0.0110 0.3254? 0.0087
S 0.9799? 0.0007 0.9796? 0.0007 0.9800? 0.0006 0.9796? 0.0007 0.9606? 0.0010 0.9598? 0.0011
Table 1: Stability of mean (with standard deviation) values of WD and S in three different scenarios, each defining
the: probability of a false positive (FP), false negative (FN), or both. Each scenario varies the range of internal
segment sizes (e.g., (20, 30)). Low standard deviation and similar within-scenario means demonstrates low sensitivity
to variations in internal segment size.
metrics react to increases in the distance between a
near miss in a segment of 25 units. These configura-
tions are all preferable to the drop of 1?WD.
4.2 Segmentation Mass Scale Effects
It is important for a segmentation evaluation met-
ric to take into account the severity of an error in
terms of segment size. An error in a 100 unit seg-
ment should be considered less severe than an er-
ror in a 2 unit segment, because an extra boundary
placed within a 100 unit segment (e.g., Figure 9 with
m = 100) could probably indicate a weak boundary,
whereas in a 4 unit segment the probability that an
extra boundary exists right next to two agreed-upon
boundaries should be small for most tasks, meaning
that it is probable that the extra boundary is an error,
and not a weak boundary.
s1
s2
m/4
m/2
m/4
m/4
m/4
m/4
m/4
Figure 9: Two segmentations of mass m with a full miss
To demonstrate that S is sensitive to segment size,
Figure 5c shows how S and 1?WD respond when
comparing segmentations configured as shown in
Figure 10 (containing one match and one full miss)
with linearly increasing mass (4 ? m ? 100).
1?WD will eventually indicate 0.68, whereas S ap-
propriately discounts the error as mass is increased,
approaching 1 as limm??. 1?WD behaves in this
way because of how it calculates its window size pa-
rameter, k, which is plotted as k/m to show how its
value influences 1?WD.
s1
s2
m/4 m? (
m/4)
m/4
m/4
m/2
Figure 10: Two segmentations of mass m compared with
increasing m in Figure 5c (s1 as reference)
4.3 Variation in Segment Sizes
When Pevzner and Hearst (2002) proposed WD,
they demonstrated that it was not as sensitive as
Pk to variations in the size of segments inside a
segmentation. To show this, they simulated how
WD performs upon a segmentation comprised of
1000 segments with four different uniformly dis-
tributed ranges of internal segment sizes (keeping
the mean at approximately 25 units) in compari-
son to a hypothesis segmentation with errors (false
positives, false negatives, and both) uniformly dis-
tributed within segments (Pevzner and Hearst, 2002,
pp. 11?12). 10 trials were performed for each seg-
ment size range and error probability, with 100 hy-
potheses generated per trial. Recreating this simu-
lation, we compare the stability of S in comparison
to WD, as shown in Table 1. We can see that WD
values show substantial within-scenario variation for
each segment size range, and larger standard devia-
tions, than S.
4.4 Inter-Annotator Agreement Coefficients
Here, we demonstrate the adapted inter-annotator
agreement coefficients upon topical paragraph-level
segmentations produced by 27 coders of 20 chapters
from the novel The Moonstone by Wilkie Collins
collected by Kazantseva and Szpakowicz (2012).
Figure 11 shows a heat map of each chapter where
the percentage of coders who agreed upon each po-
tential boundary is represented. Comparing this heat
map to the inter-annotator agreement coefficients in
Table 2 allows us to better understand why certain
chapters have lower reliability.
Chapter 1 has the lowest pi?S score in the table, and
also the highest bias (BS). One of the reasons for
this low reliability can be attributed to the chapter?s
small mass (m) and few coders (|c|), which makes
it more sensitive to chance agreement. Visually, the
159
1
2
3
4
5
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
1.0
0.0
Ch
ap
ter
s
Potential boundary positions (between paragraphs)
Coderagreementperpotentialboundary(%
)
Figure 11: Heat maps for the segmentations of each chap-
ter showing the percentage of coders who agree upon
boundary positions (darker shows higher agreement)
predominance of grey indicates that, although there
are probably two boundaries, their exact location is
not very well agreed upon. In this case, 1?WD
incorrectly indicates the opposite, that this chapter
may have relatively moderate reliability, because it
is not corrected for chance agreement.
1?WD indicates that the lowest reliability is
found in Chapter 19. pi?S indicates that this is one
of the higher agreement chapters, and looking at the
heat map, we can see that it does not contain any
strongly agreed upon boundaries. In this chapter,
there is little opportunity to agree by chance due to
the low number of boundaries (|b|) placed, and be-
cause the judgements are tightly clustered in a fair
amount of mass, the S component of pi?S appropri-
ately takes into account the near misses observed
and gives it a high reliability score.
Chapter 17 received the highest pi?S in the table,
which is another example of how tight clustering of
boundary choices in a large mass leads pi?S to appro-
priately indicate high reliability despite that there are
not as many individual highly-agreed-upon bound-
aries, whereas 1?WD indicates that there is low re-
liability. 1?WD and pi?S both agree, however, that
chapter 16 has high reliability.
Despite WindowDiff?s sensitivity to near misses,
it is evident that its pairwise mean cannot be used
to consistently judge inter-annotator agreement, or
reliability. S demonstrates better versatility when
accounting for near misses, and when used as part
of inter-annotator agreement coefficients, it prop-
erly takes into account chance agreement. Follow-
ing Artstein and Poesio?s (2008, pp. 590?591) rec-
Ch. pi?S ?
?
S BS 1?WD |c| |b| m
1 0.7452 0.7463 0.0039 0.6641? 0.1307 4 13 13
2 0.8839 0.8840 0.0009 0.7619? 0.1743 6 20 15
3 0.8338 0.8340 0.0013 0.6732? 0.1559 4 23 38
4 0.8414 0.8417 0.0019 0.6019? 0.2245 4 25 46
5 0.8773 0.8774 0.0003 0.6965? 0.1106 6 34 42
7 0.8132 0.8133 0.0002 0.6945? 0.1822 6 20 15
8 0.8495 0.8496 0.0006 0.7505? 0.0911 6 48 39
9 0.8104 0.8105 0.0009 0.6502? 0.1319 6 35 33
10 0.9077 0.9078 0.0002 0.7729? 0.0770 6 56 83
11 0.8130 0.8135 0.0022 0.6189? 0.1294 4 73 111
12 0.9178 0.9178 0.0001 0.6504? 0.1277 6 40 102
13 0.9354 0.9354 0.0002 0.5660? 0.2187 6 21 58
14 0.9367 0.9367 0.0001 0.7128? 0.1744 6 35 70
15 0.9344 0.9344 0.0001 0.7291? 0.0856 6 40 97
16 0.9356 0.9356 0.0000 0.8016? 0.0648 6 41 69
17 0.9447 0.9447 0.0002 0.6717? 0.2044 5 23 70
18 0.8921 0.8922 0.0005 0.5998? 0.1614 5 28 59
19 0.9021 0.9022 0.0009 0.4796? 0.2666 5 15 36
20 0.8590 0.8591 0.0003 0.6657? 0.1221 6 21 21
21 0.9286 0.9286 0.0004 0.6255? 0.2003 5 17 60
Table 2: S-based inter-annotator agreements and pairwise
mean 1?WD and standard deviation with the number of
coders, boundaries, and mass per chapter
ommendation, and given the low bias (mean coder
groupBS = 0.0061?0.0035), we propose reporting
reliability using pi? for this corpus, where the mean
coder group pi?S for the corpus is 0.8904 ? 0.0392
(counting 1039 full and 212 near misses).
5 Conclusion and Future Work
We have proposed a segmentation evaluation met-
ric which solves the key problems facing segmenta-
tion analysis today, including an inability to: appro-
priately quantify near misses when evaluating auto-
matic segmenters and human performance; penalize
errors equally (or, with configuration, in a manner
that suits a specific segmentation task); compare an
automatic segmenter directly against human perfor-
mance; require a ?true? reference; and handle mul-
tiple boundary types. Using S, task-specific eval-
uation of automatic and human segmenters can be
performed using multiple human judgements unhin-
dered by the quirks of window-based metrics.
In current and future work, we will show how S
can be used to analyze hierarchical segmentations,
and illustrate how to apply S to linear segmentations
containing multiple boundary types.
Acknowledgments
We thank Anna Kazantseva for her invaluable feed-
back and corpora, and Stan Szpakowicz, Martin Sca-
iano, and James Cracknell for their feedback.
160
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596. MIT Press, Cam-
bridge, MA, USA.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text Segmentation Using Exponential Models.
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, 2:35?46.
Association for Computational Linguistics, Strouds-
burg, PA, USA.
Doug Beeferman and Adam Berger. 1999. Statisti-
cal models for text segmentation. Machine learning,
34(1?3):177?210. Springer Netherlands, NL.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46. Sage, Beverly Hills, CA, USA.
Frederick J. Damerau. 1964. A technique for computer
detection and correction of spelling errors. Commu-
nications of the ACM, 7(3):171?176. Association for
Computing Machinery, Stroudsburg, PA, USA.
Mark Davies and Joseph L. Fleiss. 1982. Measur-
ing agreement for multinomial data. Biometrics,
38(4):1047?1051. Blackwell Publishing Inc, Oxford,
UK.
George R. Doddington. 1998. The topic detection and
tracking phase 2 (TDT2) evaluation plan. DARPA
Broadcast News Transcription and Understanding
Workshop, pp. 223?229. Morgan Kaufmann, Waltham,
MA, USA.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382. American Psychological Association,
Washington, DC, USA.
Martin, Franz, J. Scott McCarley, and Jian-Ming Xu.
2007. User-oriented text segmentation evaluation
measure. Proceedings of the 30th annual international
ACM SIGIR conference on Research and development
in information retrieval, pp. 701?702. Association for
Computing Machinery, Stroudsburg, PA, USA.
William Gale, Kenneth Ward Church, and David
Yarowsky. 1992. Estimating upper and lower bounds
on the performance of word-sense disambiguation pro-
grams. Proceedings of the 30th annual meeting of
the Association for Computational Linguistics, pp.
249?256. Association for Computational Linguistics,
Stroudsburg, PA, USA.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. An analysis of quantitative aspects in
the evaluation of thematic segmentation algorithms.
Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, pp. 144?151. Association for
Computational Linguistics, Stroudsburg, PA, USA.
Marti A. Hearst. 1997. TextTiling: Segmenting Text into
Multi-paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64. MIT Press, Cambridge, MA,
USA.
Anna Kazantseva and Stan Szpakowicz. 2012. Topical
Segmentation: a Study of Human Performance. Pro-
ceedings of Human Language Technologies: The 2012
Annual Conference of the North American Chapter
of the Association for Computational Linguistics. As-
sociation for Computational Linguistics, Stroudsburg,
PA, USA.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology, Chapter 12. Sage, Beverly
Hills, CA, USA.
Klaus Krippendorff. 2004. Content Analysis: An Intro-
duction to Its Methodology, Chapter 11. Sage, Beverly
Hills, CA, USA.
Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and
Frederic Saubion 2007. On evaluation methodologies
for text segmentation algorithms. Proceedings of the
19th IEEE International Conference on Tools with Ar-
tificial Intelligence, 2:19?26. IEEE Computer Society,
Washington, DC, USA.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710. American Institute
of Physics, College Park, MD, USA.
Olena Medelyan. 2009. Human-competitive automatic
topic indexing. PhD Thesis. University of Waikato,
Waikato, NZ.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pp. 1318?1327. Association for Compu-
tational Linguistics, Stroudsburg, PA, USA.
Rebecca J. Passonneau and Diane J. Litman. 1993.
Intention-based segmentation: human reliability and
correlation with linguistic cues. Proceedings of the
31st annual meeting of the Association for Computa-
tional Linguistics, pp. 148?155). Association for Com-
putational Linguistics, Stroudsburg, PA, USA.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36. MIT
Press, Cambridge, MA, USA.
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opinion
Quarterly, 19(3):321?325. American Association for
Public Opinion Research, Deerfield, IL, USA.
Sidney Siegel and N. John Castellan, Jr. 1988. Non-
parametric Statistics for the Behavioral Sciences. 2nd
Edition, Chapter 9.8. McGraw-Hill, New York, USA.
161
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 362?366,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Getting More from Segmentation Evaluation
Martin Scaiano
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
mscai056@uottawa.ca
Diana Inkpen
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
diana@eecs.uottawa.com
Abstract
We introduce a new segmentation evaluation
measure, WinPR, which resolves some of the
limitations of WindowDiff. WinPR distin-
guishes between false positive and false nega-
tive errors; produces more intuitive measures,
such as precision, recall, and F-measure; is in-
sensitive to window size, which allows us to
customize near miss sensitivity; and is based
on counting errors not windows, but still pro-
vides partial reward for near misses.
1 Introduction
WindowDiff (Pevzner and Hearst, 2002) has be-
come the most frequently used measure to evalu-
ate segmentation. Segmentation is the task of di-
viding a stream of data (text or other media) into
coherent units. These units may be motivated top-
ically (Malioutov and Barzilay, 2006), structurally
(Stokes, 2003) (Malioutov et al, 2007) (Jancsary et
al., 2008), or visually (Chen et al, 2008), depending
on the domain and task. Segmentation evaluation
is difficult because exact comparison of boundaries
is too strict; a partial reward is required for close
boundaries.
2 WindowDiff
?The WindowDiff metric is a variant of the Pk mea-
sure, which penalizes false positives and near misses
equally.? (Malioutov et al, 2007). WindowDiff uses
a sliding window over the segmentation; each win-
dow is evaluated as correct or incorrect. WindowD-
iff is effectively 1 ? accuracy for all windows,
but accuracy is sensitive to the balance of positive
and negative data being evaluated. The positive
and negative balance is determined by the window
size. Small windows produce more negatives, thus
WindowDiff recommends using a window size (k)
of half the average segment length. This produces
an almost equal number of positive windows (con-
taining boundaries) and negative windows (without
boundaries).
Equation 1 represents the window size (k), where
N is the total number of sentences (or content units).
Equation 2 is WindowDiff?s traditional definition,
where R is the number of reference boundaries in
the window from i to i+k, and C is the number
of computed boundaries in the same window. The
comparison (> 0) is sometimes forgotten, which
produces strange values not bound between 0 and 1;
thus we prefer equation 3 to represent WindowDiff,
as it emphasizes the comparison.
k =
N
2 * number of segments
(1)
WindowDiff =
1
N ? k
N?k?
i=0
(|Ri,i+k ? Ci,i+k| > 0)(2)
WindowDiff =
1
N ? k
N?k?
i=0
(Ri,i+k 6= Ci,i+k) (3)
Figure 1 illustrates WindowDiff?s sliding win-
dow evaluation. Each rectangle represents a sen-
tence, while the shade indicates to which segment
it truly belongs (reference segmentation). The ver-
tical line represents a computed boundary. This ex-
ample contains a near miss (misaligned boundary).
In this example, we are using a window size of 5.
The columns i, R, C, W represent the window po-
sition, the number of boundaries from the reference
(true) segmentation in the window, the number of
boundaries from the computed segmentation in the
window, and whether the values agree, respectively.
Only windows up to i = 5 are shown, but to process
362
the entire segmentation 8 windows are required.
i R C W
0 0 0 D
1 0 0 D
2 0 1 X
3 1 1 D
4 1 1 D
5 1 0 X
Figure 1: Illustration of counting boundaries in windows
Franz et al (2007) note that WindowDiff does not
allow different segmentation tasks to optimize dif-
ferent aspects, or tolerate different types of errors.
Tasks requiring a uniform theme in a segment might
tolerate false positives, while tasks requiring com-
plete ideas or complete themes might accept false
negatives.
Georgescul et al (2009) note that while Win-
dowDiff technically penalizes false positives and
false negatives equally, false positives are in fact
more likely; a false positive error occurs anywhere
were there are more computed boundaries than
boundaries in the reference, while a false negative
error can only occur when a boundary is missed.
Consider figure 1, only 3 of the 8 windows contain a
boundary; only those 3 windows may have false neg-
atives (a missed boundary), while all other windows
may contain false positives (too many boundaries).
Lamprier et al (2008) note that errors near the
beginning and end of a segmentation are actually
counted slightly less than other errors. Lamprier of-
fers a simple correction for this problem, by adding
k?1 phantom positions, which have no boundaries,
at the beginning and at the end sequence. The ad-
dition of these phantom boundaries allows for win-
dows extending outside the segmentation to be eval-
uated, and thus allowing for each position to be
count k times. Example E in figure 4 in the next
section will illustrate this point. Consider example
D in figure 4; this error will only be accounted for in
the first window, instead of the typical k windows.
Furthermore, tasks may want to adjust sensitiv-
ity or reward for near misses. Naturally, one would
be inclined to adjust the window size, but changing
the window size will change the balance of positive
windows and negative windows. Changing this bal-
ance has a significant impact on how WindowDiff
functions.
Some researchers have questioned what the Win-
dowDiff value tells us; how do we interpret it?
3 WinPR
WinPR is derived from WindowDiff, but differs on
one main point: WinPR evaluates boundary posi-
tions, while WindowDiff evaluates regions (or win-
dows). WinPR is a set of equations (4-7) (Figure 2)
producing a confusion matrix. The confusion matrix
allows for the distinction between false positive and
negative errors, and can be used with Precision, Re-
call, and F-measure. Furthermore, the window size
may be changed to adjust near-miss sensitivity with-
out affecting the the interpretation of the confusion
matrix.
N is the number of content units and k repre-
sents the window size. WinPR includes the Lam-
prier (2008) correction, thus the sum is from 1 ? k
to N instead of 1 to N ? k as with WindowDiff.
min and max refer to the tradition computer sci-
ence functions which select the minimal or maximal
value from a set of two values. True negatives (5)
start with a negative term, which removes the value
of the phantom positions.
Each WinPR equation is a summation over all
windows. To understand the intuition behind each
equation, consider Figure 3. R and C represent the
number of boundaries from the reference and com-
puted segmentations, respectively, in the ith win-
dow, up to a maximum of k. The overlapping region
represents the TPs. The difference is the error, while
the sign of the difference indicates whether they are
FPs or FNs. The WinPR equations select the differ-
ence using the max function, forcing negative val-
ues to 0. The remainder, up to k, represents the TNs.
kCiRi0
C
R
TP
error
TN
Figure 3: WinPR within Window Counting Demostration
Consider how WindowDiff and WinPR handle
the examples in Figure 4. These examples use the
same basic representation as Figure 1 in section 2.
Each segment is 6 units long and the window size is
363
True Positives = TP =
N?
i=1?k
min(Ri,i+k, Ci,i+k) (4)
True Negatives = TN = ?k(k ? 1) +
N?
i=1?k
(k ?max(Ri,i+k, Ci,i+k)) (5)
False Positives = FP =
N?
i=1?k
max(0, Ci,i+k ?Ri,i+k) (6)
False Negatives = FN =
N?
i=1?k
max(0, Ri,i+k ? Ci,i+k) (7)
Figure 2: Equations for the WinPR confusion matrix
3 = (6/2). Each window contains 3 content units,
thus we consider 4 potential boundary positions (the
edges are inclusive).
A) Correct boundary
B) Missed boundary
C) Near boundary
D) Extra boundary
E) Extra boundaries
Figure 4: Example segmentations
Example A provides a baseline for comparison; B
is a false negative (a missed boundary); C is a near
miss; D is an extra boundary at the beginning of the
sequence, providing an example of Lamprier?s criti-
cism. E includes two errors near each other. Notice
how the additional errors in E have have a very small
impact on the WindowDiff value. Table 1 lists the
number of correct and incorrect windows, and the
WindowDiff value for each example.
Example Correct Incorrect WindowDiff
A 10 0 0
B 6 4 0.4
C 8 2 0.2
D 9 1 0.1
E 4 6 0.6
Table 1: WindowDiff values for examples A to E
WindowDiff should penalize an error k times,
once for each window in which it appears, with the
exception of near misses which have partial reward
and penalization. D is only penalized in one win-
dow, because most of the other windows would be
outside the sequence. E contains two errors, but they
are not fully penalized because they appear in over-
lapping windows. Furthermore, using a single met-
ric does not indicate if the errors are false positives
or false negatives. This information is important to
the development of a segmentation algorithm.
If we apply WinPR to examples A-E, we get the
results in Table 2. We will calculate precision and
recall using the WinPR confusion matrix, shown un-
der WinP and WinR respectively. You will note that
we can easily see whether an error is a false posi-
tive or a false negative. As we would expect, false
positives affect precision, and false negatives affect
recall. Near misses manifest as equal parts false pos-
itive and false negative. In example E, each error is
counted, unlike WindowDiff.
Example TP TN FP FN WinP WinR
a 4 40 0 0 1 1.0
b 0 40 0 4 - 0
c 3 40 1 1 0.75 0.75
d 4 36 4 0 0.5 1.0
e 4 32 8 0 0.33 1.0
Table 2: WinPR values for examples A to E
In Table 2, note that each potential boundary posi-
tion is considered k (the window size) times. Thus,
each positive or negative boundary assignment is
counted k times; near misses producing a blend of
values: TP, FP, FN. We refer to the normalized con-
364
fusion matrix (or normalized WinPR), as the con-
fusion matrix divided by the window size. If near
misses are not considered, this confusion matrix
gives the exact count of boundary assignments.
What is not apparent in Table 2, is that WinPR
is insensitive to window size, with the exception of
near misses. Thus adjusting the window size can
be used to adjust the tolerance or sensitivity to near
misses. Large window sizes are more forgiving of
near misses, smaller window size are more strict.
3.1 Near Misses and Window Size
WinPR does not provide any particular values in-
dicating the number of near misses, their distance,
or contribution to the evaluation. Because WinPR?s
window size only affects near miss sensitivity, and
not the positive/negative balance like in WindowD-
iff, we can subtract two normalized confusion ma-
trices using different window sizes. The difference
between the confusion matrices gives the impact of
near misses under different window sizes. Choosing
a very strict window size (k = 1), and subtracting it
from another window size would effectively provide
the contribution of the near misses to the confusion
matrix. In many circumstances, using several win-
dow sizes may be desirable.
3.2 Variations in Segment Size: Validation by
Simulation
We ran numerous tests on artificial segmentation
data composed of 40 segments, with a mean segment
length of 40 content units, and standard deviations
varying from 10 to 120. All tests showed that a false
positive or a false negative error is always penalized
k times, as expected.
3.3 WinPR Applied to a Complete
Segmentation
Using a reference segmentation of 40 segments, we
derived two flawed segments: we added 20 extra
boundaries to one, and removed 18 boundaries from
the other. Both produced WindowDiff values of
0.22, while WinPR provided WinP = 0.66 and WinR
= 1.0 for the addition of boundaries and WinP =
1.00 and WinR = 0.54 for the removal of bound-
aries. WinPR highlights the differences in the na-
ture of the two flawed segmentations, while WinDiff
masks both the number and types of errors.
4 Conclusion
We presented a new evaluation method for segmen-
tation, called WinPR because it produces a confu-
sion matrix from which Precision and Recall can be
derived. WinPR is easy to implement and provides
more detail on the types of errors in a computed seg-
mentation, as compared with the reference. Some of
the major benefits of WinPR, as opposed to Win-
dowDiff are presented below:
1. Distinct counting of false positives and false
negatives, which helps in algorithm selection
for downstream tasks and helps with analysis
and optimization of an algorithm.
2. The confusion matrix is easier to interpret than
a WindowDiff value.
3. WinPR counts errors from boundaries, not win-
dows, thus close errors are not masked
4. Precision, and Recall are easier to understand
than WindowDiff.
5. F-measure is effective when a single value is
required for comparison.
6. WinPR incorporates Lamprier (2008) correc-
tion.
7. Adjusting the window size can customize an
evaluation?s tolerance of near misses
8. WinPR provides a method of detecting the im-
pact of near misses on an evaluation
WinPR counts boundaries, not windows, which
has analytical benefits, but WindowDiff?s counting
of windows provides an evaluation of segmentation
by region. Thus WindowDiff is more appropriate
when an evaluator is less interested in the types and
the number of errors and more interested in the per-
centage of the sequence that is correct.
Acknowledgments
Thanks to Dr. Stan Szpakowicz for all his help refin-
ing the arguments and the presentation of this paper.
Thanks to Anna Kazantseva for months of discus-
sions about segmentation and the evaluation prob-
lems we each faced. Thanks to Natural Sciences and
Engineering Research Council of Canada (NSERC)
for funding our research.
365
References
L Chen, YC Lai, and H Liao. 2008. Movie scene
segmentation using background information. Pattern
Recognition, Jan.
M Franz, J McCarley, and J Xu. 2007. User-oriented
text segmentation evaluation measure. SIGIR ?07 Pro-
ceedings of the 30th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, Jan.
M Georgescul, A Clark, and S Armstrong. 2009. An
analysis of quantitative aspects in the evaluation of the-
matic segmentation algorithms. SigDIAL ?06 Proceed-
ings of the 7th SIGdial Workshop on Discourse and
Dialogue, Jan.
J Jancsary, J Matiasek, and H Trost. 2008. Revealing the
structure of medical dictations with conditional ran-
dom fields. EMNLP ?08 Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Jan.
S Lamprier, T Amghar, and B Levrat. 2008. On evalu-
ation methodologies for text segmentation algorithms.
19th IEEE International Conference on Tools with Ar-
tificial Intelligence - Vol.2, Jan.
I Malioutov and R Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. ACL-44 Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, Jan.
I Malioutov, A Park, R Barzilay, and R Glass. 2007.
Making sense of sound: Unsupervised topic segmen-
tation over acoustic input. Proceeding of the Annual
Meeting of the Association for Computation Linguis-
tics 2007, Jan.
L Pevzner and M Hearst. 2002. A critique and improve-
ment of an evaluation metric for text segmentation.
Computational Linguistics, Jan.
N Stokes. 2003. Spoken and written news story seg-
mentation using lexical chains. Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology: HLT-NAACL2003 Student Re-
search Workshop, Jan.
366
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 380?383, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
uOttawa: System description for SemEval 2013 Task 2 Sentiment    
Analysis in Twitter 
 
 
Hamid Poursepanj, Josh Weissbock, and Diana Inkpen 
School of Electrical Engineering and Computer Science  
University of Ottawa 
Ottawa, K1N6N5, Canada 
{hpour099, jweis035, Diana.Inkpen}@uottawa.ca 
 
  
 
Abstract 
We present two systems developed at the Uni-
versity of Ottawa for the SemEval 2013 Task 2. 
The first system (for Task A) classifies the po-
larity / sentiment orientation of one target word 
in a Twitter message. The second system (for 
Task B) classifies the polarity of whole Twitter 
messages. Our two systems are very simple, 
based on supervised classifiers with bag-of-
words feature representation, enriched with in-
formation from several sources. We present a 
few additional results, besides results of the 
submitted runs. 
1 Introduction 
The Semeval 2013 Task 2 focused on classifying 
Twitter messages (?tweets?) as expressing a posi-
tive opinion, a negative opinion, a neutral opinion, 
or no opinion (objective). In fact, the neutral and 
objective were joined in one class for the require-
ments of the shared task. Task A contained target 
words whose sense had to be classified in the con-
text, while Task B was to classify each text into 
one of the three classes: positive, negative, and 
neutral/objective. The training data that was made 
available for each task consisted in annotated 
Twitter message. There were two test sets for each 
task, one composed of Twitter messages and one 
of SMS message (even if there was no specific 
training data for SMS messages). See more details 
about the datasets in (Wilson et al, 2013). 
 
2 System Description 
We used supervised learning classifiers from We-
ka (Witten and Frank, 2005). Initially we extracted 
simple bag-of-word features (BOW). For the sub-
mitted systems, we also used features calculated 
based on SentiWordNet information (Baccianella 
et al, 2010). SentiWordNet contains positivity, 
negativity, and objectivity scores for each sense of 
a word. We explain below how this information 
was used for each task. 
    As classifiers, we used Support Vector Ma-
chines (SVM) (SMO and libSVM from Weka with 
default values for parameters), because SVM is 
known to perform well on many tasks, and Multi-
nomial Naive Bayes (MNB), because MNB is 
known to perform well on text data and it is faster 
than SVM. 
2.1 Task A 
Our system for Task A involved two parts: the 
expansion of our training data and the classifica-
tion. The expansion was done with information 
from SentiWordNet. Stop words and words that 
appeared only once in the training data were fil-
tered out. Then the classification was completed 
with algorithms from Weka.  
As mentioned, the first task was to expand all of 
the tweets that were provided as training data. This 
was doing using Python and the Python NLTK 
library, as well as SentiWordNet. SentiWordNet 
provides a score of the sentient state for each word 
(for each sense, in case the word has more than 
380
one sense). As an example, the word ?want? can 
mean ?a state of extreme poverty? with the Senti-
WordNet score of (Positive: 0 Objective: 
0.75 Negative: 0.25). The same word could also 
mean ?a specific feeling of desire? with a score of 
(Positive: 0.5 Objective: 0.5 Negative: 0). We also 
used for expansion the definitions and synonyms 
of each word sense, from WordNet. 
The tweets in the training data are labeled with 
their sentiment type (Positive, Negative, Objective 
and Neutral). Neutral and Objective are treated the 
same. The provided training data has the target 
word marked, and also the sentiment orientation of 
the word in the context of the tweeter message. 
These target words were the ones expanded by our 
method. When the target was a multi-word expres-
sion, if the expression was found in WordNet, then 
the expansion was done directly; if not, each word 
was expanded in a similar fashion and concatenat-
ed to the original tweet. These target words were 
looked up in SentiWordNet and matched with the 
definition that had the highest score that also 
matched their sentiment label in the training data.  
 
 
Original Tweet The great Noel Gallagher is about to 
hit the stage in St. Paul. Plenty of 
room here so we're 4th row center. 
Plenty of room. Pretty fired up 
Key Words Great 
Sentiment Positive 
Definition very good; "he did a bully job"; "a 
neat sports car"; "had a great time at 
the party"; "you look simply smash-
ing" 
Synonyms Swell, smashing, slap-up, peachy, 
not_bad, nifty, neat, keen, groovy, 
dandy, cracking, corking, bully, 
bang-up 
Expanded 
Tweet 
The great Noel Gallagher is about to 
hit the stage in St. Paul. Plenty of 
room here so were 4th row center. 
Plenty of room. Pretty fired up  swell 
smashing slap-up peachy not_bad 
nifty neat keen groovy dandy crack-
ing corking bully bang-up very good 
he did a bully job a neat sports car 
had a great time at the party you look 
simply smashing  
Table 1: Example of tweet expansion for Task A 
 
 
The target word?s definition and synonyms were then 
concatenated to the original tweet. No additional 
changes were made to either the original tweet or the 
features that were added from SentiWordNet.  An ex-
ample follows in Table 1. The test data (Twitter and 
SMS) was not expanded, because there are no labels in 
the test data to be able to choose the sense with corre-
sponding sentiment. 
2.2 Task B 
For this task, we used the following resources: 
SentiwordNet (Baccianella et al 2010), the Polari-
ty Lexicon (Wilson et al, 2005), the General In-
quirer (Stone et al, 1966), and the Stanford NLP 
tools (Toutanova et al, 2003) for preprocessing 
and feature selection. The preprocessing of Twitter 
messages is implemented in three steps namely, 
stop-word removal, stemming, and removal of 
words with occurrence frequency of one. Several 
extra features will be used: the number of positive 
words and negative words identified by three lexi-
cal resources mentioned above, the number of 
emoticons, the number of elongated words, and the 
number of punctuation tokens (single or repeated 
exclamation marks, etc.). As for SentiWordNet, 
for each word a score is calculated that shows the 
positive or negative weight of that word. No sense 
disambiguation is done (the first sense is used), but 
the scores are used for the right part-of-speech (in 
case a word has more than one possible part-of-
speech). Part-of-Speech tagging was done with the 
Stanford NLP Tools. As for General Inquirer and 
Polarity Lexicon, we simply used the list positive 
and negative words from these resources in order 
to count how many positive and how many nega-
tive terms appear in a message.  
 
3 Results 
3.1 Task A 
For classification, we first trained on our expanded 
training data using 10-fold cross-validation and 
using the SVM (libSVM) and Multinomial Na-
iveBayes classifiers from Weka, using their default 
settings. The training data was represented as a 
bag of words (BOW). These classifiers were cho-
sen as they have given us good results in the past 
for text classification. The classifiers were run 
with 10-fold cross-validation. See Table 2 for the 
381
results. Without expanding the tweets, the accura-
cy of the SVM classifier was equal to the baseline 
of classifying everything into the most frequent 
class, which was ?positive? in the training data. 
For MNB, the results were lower than the baseline. 
After expanding the tweets, the accuracy increased 
to 73% for SVM and to 80.36% for MNB. We 
concluded that MNB works better for Task A. This 
is why the submitted runs used the MNB model 
that was created from the expanded training data. 
Then we used this to classify the Twitter and SMS 
test data. The average F-score for the positive and 
the negative class for our submitted runs can be 
seen in Table 3, compared to the other systems 
that participated in the task. We report this meas-
ure because it was the official evaluation measure 
used in the task. 
 
 
System SVM MNB 
Baseline 66.32% 66.32% 
BOW features 66.32% 33.23% 
BOW+ text expansion 73.00% 80.36% 
Table 2: Accuracy results for task A by 10-fold cross-
validation on the training data 
 
 
System Tweets SMS 
uOttawa system 0.6020 0.5589 
Median system 0.7489 0.7283 
Best system 0.8893 0.8837 
Table 3:  Results for Task A for the submitted runs 
(Average F-score for positive/negative class) 
    
   The precision, recall and F-score on the Twitter 
and SMS test data for our submitted runs can be 
seen in Tables 4 and 5, respectively. All our sub-
mitted runs were for the ?constrained? task; no 
additional training data was used. 
 
 
Class Precision Recall F-Score 
Positive 0.6934 0.7659 0.7278 
Negative 0.5371 0.4276 0.4762 
Neutral 0.0585 0.0688 0.0632 
Table 4: Results for Tweet test data for Task A, for 
each class. 
 
 
Class Precision Recall F-Score 
Positive 0.5606 0.5705 0.5655 
Negative 0.5998 0.5118 0.5523 
Neutral 0.1159 0.2201 0.1518 
Table 5: Results for SMS test data for Task A, for each 
class. 
3.2 Task B 
First we present results on the training data (10-
fold cross-validation), then we present the results 
for the submitted runs (also without any additional 
training data).  
    Table 6 shows the overall accuracy for BOW 
features for two classifiers, evaluated based on 10-
fold cross validation on the training data, for two 
classifiers: SVM (SMO in Weka) and Multidimen-
sional Na?ve Bays (MNB in Weka). The BOW 
plus SentiWordNet features also include the num-
ber of positive and negative words identified from 
SentiWordNet. The BOW plus extra features rep-
resentation includes the number of positive and 
negative words identified from SentiWordNet, 
General Inquirer, and Polarity Lexicon (six extra 
features). The last row of the table shows the over-
all accuracy for BOW features plus all the extra 
features mentioned in Section 2.2, including in-
formation extracted from SentiWordNet, Polarity 
Lexicon, and General Inquirer. We can see that the 
SentiWordNet features help, and that when includ-
ing all the extra features, the results improve even 
more. We noticed that the features from the Polari-
ty Lexicon contributed the most. When we re-
moved GI, the accuracy did not change much; we 
believe this is because GI has too small coverage. 
 
 
System SVM MNB 
Baseline 48.50% 48.50% 
BOW features 58.75% 59.56% 
BOW+ SentiWordNet 69.43% 63.30% 
BOW+ extra features 82.42% 73.09% 
Table 6: Accuracy results for task B by 10-fold cross-
validation on the training data 
 
    The baseline in Table 6 is the accuracy of a triv-
ial classifier that puts everything in the most fre-
quent class, which is neutral/objective for the 
training data (ZeroR classifier in Weka). 
382
    The results of the submitted runs are in Table 7 
for the two data sets. The features representation 
was BOW plus SentiWordNet information. The 
official evaluation measure is reported (average F-
score for the positive and negative class). The de-
tailed results for each class are presented in Tables 
8 and 9.  
     In Table 7, we added an extra row for a new 
uOttawa system (SVM with BOW plus extra fea-
tures) that uses the best classifier that we designed 
(as chosen based on the experiments on the train-
ing data, see Table 6). This classifier uses SVM 
with BOW and all the extra features. 
 
 
System Tweets SMS 
uOttawa submitted  
system 
0.4251 0.4051 
uOttawa new system 0.8684 0.9140 
Median system 0.5150 0.4523 
Best system 0.6902 0.6846  
Table 7:  Results for Task B for the submitted runs 
(Average F-score for positive/negative). 
 
Class Precision Recall F-score 
Positive 0.6206 0.5089 0.5592 
Negative 0.4845 0.2080 0.2910 
Neutral 0.5357 0.7402 0.6216 
Table 8: Results for each class for task B, for the sub-
mitted system (SVM with BOW plus SentiWordNet 
features) for the Twitter test data. 
 
Class Precision Recall F-score 
Positive 0.4822 0.5508 0.5142 
Negative 0.5643 0.2005 0.2959 
Neutral 0.6932 0.7988 0.7423 
Table 9: Results for each class for task B, for the sub-
mitted system (SVM with BOW plus SentiWordNet 
features) for the SMS test data. 
 
4 Conclusions and Future Work  
In Task A, we expanded upon the Twitter messag-
es from the training data using their keyword?s 
definition and synonyms from SentiWordNet. We 
showed that the expansion helped improve the 
classification performance. In future work, we 
would like to try an SVM using asymmetric soft-
boundaries to try and penalize the classifier for 
missing items in the neutral class, the class with 
the least items in the Task A training data.   
 The overall accuracy of the classifiers for Task 
B increased a lot when we introduced the extra 
features discussed in section 2.2. The overall accu-
racy of SVM increased from 58.75% to 82.42% 
(as measures by cross-validation on the training 
data). When applying this classifier on the two test 
data sets, the results were very surprisingly good 
(even higher that the best system submitted by the 
SemEval participants for Task B1). 
 
References  
Stefano Baccianella, Andrea Esuli and Fabrizio Sebas-
tiani. SentiWordNet 3.0: An Enhanced Lexical Re-
source for Sentiment Analysis and Opinion Mining. 
In Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation 
(LREC'10), Valletta, Malta, May 2010. 
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, 
and Daniel M. Ogilvie. The General Inquirer: A 
computer approach to content analysis. MIT Press, 
1966. 
Kristina Toutanova, Dan Klein, Christopher Manning, 
and Yoram Singer. Feature-Rich Part-of-Speech 
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of HLT-NAACL 2003, pp. 252-259, 2003. 
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, 
Sara Rosenthal, Veselin Stoyanov and Alan Ritter. 
SemEval-2013 Task 2: Sentiment Analysis in Twit-
ter. In Proceedings of the International Workshop on 
Semantic Evaluation SemEval '13, Atlanta, Georgia, 
June 2013. 
Theresa Wilson, Janyce Wiebe and Paul Hoffmann. 
Recognizing contextual polarity in phrase- level sen-
timent analysis. In Proceedings of HLT/ EMNLP 
2005.  
Ian H. Witten and Eibe Frank. Data Mining: Practical 
Machine Learning Tools and Techniques, 2nd edi-
tion, Morgan Kaufmann, San Francisco, 2005. 
 
 
                                                 
1 Computed with the provided scoring script. 
383
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 35?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Corpus-based Method for Extracting Paraphrases of Emotion Terms
Fazel Keshtkar
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
akeshtka@site.uOttawa.ca
Diana Inkpen
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
diana@site.uOttawa.ca
Abstract
Since paraphrasing is one of the crucial tasks
in natural language understanding and gener-
ation, this paper introduces a novel technique
to extract paraphrases for emotion terms, from
non-parallel corpora. We present a bootstrap-
ping technique for identifying paraphrases,
starting with a small number of seeds. Word-
Net Affect emotion words are used as seeds.
The bootstrapping approach learns extraction
patterns for six classes of emotions. We use
annotated blogs and other datasets as texts
from which to extract paraphrases, based on
the highest-scoring extraction patterns. The
results include lexical and morpho-syntactic
paraphrases, that we evaluate with human
judges.
1 Introduction
Paraphrases are different ways to express the same
information. Algorithms to extract and automati-
cally identify paraphrases are of interest from both
linguistic and practical points of view. Many ma-
jor challenges in Natural Language Processing ap-
plications, for example multi-document summariza-
tion, need to avoid repetitive information from the
input documents. In Natural Language Genera-
tion, paraphrasing is employed to create more var-
ied and natural text. In our research, we ex-
tract paraphrases for emotions, with the goal of us-
ing them to automatically-generate emotional texts
(such as friendly or hostile texts) for conversations
between intelligent agents and characters in educa-
tional games. Paraphrasing is applied to generate
text with more variety. To our knowledge, most cur-
rent applications manually collect paraphrases for
specific applications, or they use lexical resources
such as WordNet (Miller et al, 1993) to identify
paraphrases.
This paper introduces a novel method for ex-
tracting paraphrases for emotions from texts. We
focus on the six basic emotions proposed by Ek-
man (1992): happiness, sadness, anger, disgust,
surprise, and fear.
We describe the construction of the paraphrases
extractor. We also propose a k-window algorithm
for selecting contexts that are used in the paraphrase
extraction method. We automatically learn patterns
that are able to extract the emotion paraphrases from
corpora, starting with a set of seed words. We use
data sets such as blogs and other annotated cor-
pora, in which the emotions are marked. We use
a large collection of non-parallel corpora which are
described in Section 3. These corpora contain many
instances of paraphrases different words to express
the same emotion.
An example of sentence fragments for one
emotion class, happiness, is shown in Table 1. From
them, the paraphrase pair that our method will
extract is:
"so happy to see"
"very glad to visit".
In the following sections, we give an overview of
related work on paraphrasing in Section 2. In Sec-
tion 3 we describe the datasets used in this work.
We explain the details of our paraphrase extraction
method in Section 4. We present results of our evalu-
ation and discuss our results in Section 5, and finally
in Section 6 we present the conclusions and future
work.
35
his little boy was so happy to see him
princess and she were very glad to visit him
Table 1: Two sentence fragments (candidate contexts)
from the emotion class happy, from the blog corpus.
2 Related Work
Three main approaches for collecting paraphrases
were proposed in the literature: manual collection,
utilization of existing lexical resources, and corpus-
based extraction of expressions that occur in similar
contexts (Barzilay and McKeown, 2001). Manually-
collected paraphrases were used in natural language
generation (NLG) (Iordanskaja et al, 1991). Langk-
ilde et al (1998) used lexical resources in statistical
sentence generation, summarization, and question-
answering. Barzilay and McKeown (2001) used a
corpus-based method to identify paraphrases from a
corpus of multiple English translations of the same
source text. Our method is similar to this method,
but it extracts paraphrases only for a particular emo-
tion, and it needs only a regular corpus, not a parallel
corpus of multiple translations.
Some research has been done in paraphrase ex-
traction for natural language processing and genera-
tion for different applications. Das and Smith (2009)
presented a approach to decide whether two sen-
tences hold a paraphrase relationship. They ap-
plied a generative model that generates a paraphrase
of a given sentence, then used probabilistic infer-
ence to reason about whether two sentences share
the paraphrase relationship. In another research,
Wang et. al (2009) studied the problem of extract-
ing technical paraphrases from a parallel software
corpus. Their aim was to report duplicate bugs. In
their method for paraphrase extraction, they used:
sentence selection, global context-based and co-
occurrence-based scoring. Also, some studies have
been done in paraphrase generation in NLG (Zhao
et al, 2009), (Chevelu et al, 2009). Bootstrapping
methods have been applied to various natural lan-
guage applications, for example to word sense dis-
ambiguation (Yarowsky, 1995), lexicon construction
for information extraction (Riloff and Jones, 1999),
and named entity classification (Collins and Singer,
1999). In our research, we use the bootstrapping ap-
proach to learn paraphrases for emotions.
3 Data
The text data from which we will extract paraphrases
is composed of four concatenated datasets. They
contain sentences annotated with the six basic emo-
tions. The number of sentences in each dataset
is presented in Table 2. We briefly describe the
datasets, as follows.
3.1 LiveJournal blog dataset
We used the blog corpus that Mishne collected for
his research (Mishne, 2005). The corpus contains
815,494 blog posts from Livejournal 1, a free we-
blog service used by millions of people to create
weblogs. In Livejournal, users are able to option-
ally specify their current emotion or mood. To se-
lect their emotion/mood users can choose from a list
of 132 provided moods. So, the data is annotated
by the user who created the blog. We selected only
the texts corresponding to the six emotions that we
mentioned.
3.2 Text Affect Dataset
This dataset (Strapparava and Mihalcea, 2007) con-
sists of newspaper headlines that were used in the
SemEval 2007-Task 14. It includes a development
dataset of 250 annotated headlines, and a test dataset
of 1000 news headlines. We use all of them. The an-
notations were made with the six basic emotions on
intensity scales of [-100, 100], therefore a threshold
is used to choose the main emotion of each sentence.
3.3 Fairy Tales Dataset
This dataset consists in 1580 annotated sentences
(Alm et al, 2005), from tales by the Grimm brothers,
H.C. Andersen, and B. Potter. The annotations used
the extended set of nine basic emotions of Izard
(1971). We selected only those marked with the six
emotions that we focus on.
3.4 Annotated Blog Dataset
We also used the dataset provided by Aman and Sz-
pakowicz (2007). Emotion-rich sentences were se-
lected from personal blogs, and annotated with the
six emotions (as well as a non-emotion class, that
we ignore here). They worked with blog posts and
collected directly from the Web. First, they prepared
1http://www.livejournalinc.com
36
Dataset Happiness Sadness Anger Disgust Surprise Fear
LiveJournal 7705 1698 4758 1191 1191 3996
TextAffect 334 214 175 28 131 166
Fairy tales 445 264 216 217 113 165
Annotated blog dataset 536 173 115 115 172 179
Table 2: The number of emotion-annotated sentences in each dataset.
Figure 1: High-level view of the paraphrase extraction
method.
a list of seed words for six basic emotion categories
proposed by Ekman (1992). Then, they took words
commonly used in the context of a particular emo-
tion. Finally, they used the seed words for each
category, and retrieved blog posts containing one or
more of those words for the annotation process.
4 Method for Paraphrase Extraction
For each of the six emotions, we run our method
on the set of sentences marked with the correspond-
ing emotion from the concatenated corpus. We
start with a set of seed words form WordNet Af-
fect (Strapparava and Valitutti, 2004), for each emo-
tion of interest. The number of seed words is the fol-
lowing: for happiness 395, for surprise 68, for fear
140, for disgust 50, for anger 250, and for sadness
200. Table 3 shows some of seeds for each category
of emotion.
Since sentences are different in our datasets
and they are not aligned as parallel sentences as
in (Barzilay and McKeown, 2001), our algorithm
constructs pairs of similar sentences, based on the
local context. On the other hand, we assume that,
if the contexts surrounding two seeds look similar,
then these contexts are likely to help in extracting
new paraphrases.
Figure 1 illustrates the high-level architecture of
our paraphrase extraction method. The input to the
method is a text corpus for a emotion category and
a manually defined list of seed words. Before boot-
strapping starts, we run the k-window algorithm on
every sentence in the corpus, in order to construct
candidate contexts. In Section 4.5 we explain how
the bootstrapping algorithm processes and selects
the paraphrases based on strong surrounding con-
texts. As it is shown in Figure 1, our method has
several stages: extracting candidate contexts, using
them to extract patterns, selecting the best patterns,
extracting potential paraphrases, and filtering them
to obtain the final paraphrases.
4.1 Preprocessing
During preprocessing, HTML and XML tags are
eliminated from the blogs data and other datasets,
then the text is tokenized and annotated with part
of speech tags. We use the Stanford part-of-speech
tagger and chunker (Toutanova et al, 2003) to iden-
tify noun and verb phrases in the sentences. In the
next step, we use a sliding window based on the
k-window approach, to identify candidate contexts
that contain the target seeds.
4.2 The k-window Algorithm
We use the k-window algorithm introduced by
Bostad (2003) in order to identify all the tokens
surrounding a specific term in a window with
size of ?k. Here, we use this approach to ex-
tract candidate patterns for each seed, from the
sentences. We start with one seed and truncate
all contexts around the seed within a window of
?k words before and ?k words after the seed,
until all the seeds are processed. For these exper-
iments, we set the value of k to ?5. Therefore
37
Happiness: avidness, glad, warmheartedness, exalt, enjoy, comforting, joviality, amorous, joyful,
like, cheer, adoring, fascinating, happy, impress, great, satisfaction, cheerful, charmed, romantic, joy,
pleased, inspire, good, fulfill, gladness, merry
Sadness: poor, sorry, woeful, guilty, miserable, glooming, bad, grim, tearful, glum, mourning, joyless,
sadness, blue, rueful, hamed, regret, hapless, regretful, dismay, dismal, misery, godforsaken, oppression,
harass, dark, sadly, attrition
Anger: belligerence, envious, aggravate, resentful, abominate, murderously, greedy, hatred, disdain,
envy, annoy, mad, jealousy, huffiness, sore, anger, harass, bother, enraged, hateful, irritating, hostile,
outrage, devil, irritate, angry
Disgust: nauseous, sicken, foul, disgust, nausea, revolt, hideous, horror, detestable, wicked, repel,
offensive, repulse, yucky, repulsive, queasy, obscene, noisome
Surprise: wondrous, amaze, gravel, marvel, fantastic, wonderful, surprising, marvelous, wonderment,
astonish, wonder, admiration, terrific, dumfounded, trounce
Fear: fearful, apprehensively, anxiously, presage, horrified, hysterical, timidity, horrible, timid,
fright, hesitance, affright, trepid, horrific, unassertive, apprehensiveness, hideous, scarey, cruel, panic,
scared, terror, awful, dire, fear, dread, crawl, anxious, distrust, diffidence
Table 3: Some of the seeds from WordNet Affect for each category of emotion.
the longest candidate contexts will have the form
w1, w2, w3, w4, w5, seed, w6, w7, w8, w9, w10, w11.
In the next subsection, we explain what features we
extract from each candidate context, to allow us to
determine similar contexts.
4.3 Feature Extraction
Previous research on word sense disambiguation on
contextual analysis has acknowledged several local
and topical features as good indicators of word prop-
erties. These include surrounding words and their
part of speech tags, collocations, keywords in con-
texts (Mihalcea, 2004). Also recently, other fea-
tures have been proposed: bigrams, named entities,
syntactic features, and semantic relations with other
words in the context.
We transfer the candidate phrases extracted by the
sliding k-window into the vector space of features.
We consider features that include both lexical and
syntactic descriptions of the paraphrases for all pairs
of two candidates. The lexical features include the
sequence of tokens for each phrase in the paraphrase
pair; the syntactic feature consists of a sequence of
part-of-speech (PoS) tags where equal words and
words with the same root and PoS are marked.
For example, the value of the syntactic feature for
the pair ??so glad to see?? and ??very
happy to visit?? is ?RB1 JJ1 TO V B1?
and ?RB1 JJ2 TO V B2?, where indices indicate
Candidate context: He was further annoyed by the jay bird
?PRP VBD RB VBN IN DT NN NN?,65,8,?VBD RB?,?,was,
?,?,?,He/PRP,was/VBD,further/RB,annoyed,by/IN,the/DT,
jay/NN,bird/NN,?,?,jay,?,?IN DT NN?,2,2,0,1
Table 4: An example of extracted features.
word equalities. However, based on the above ev-
idences and our previous research, we also investi-
gate other features that are well suited for our goal.
Table 5 lists the features that we used for paraphrase
extraction. They include some term frequency fea-
tures. As an example, in Table 4 we show extracted
features from a relevant context.
4.4 Extracting Patterns
From each candidate context, we extracted the fea-
tures as described above. Then we learn extraction
patterns, in which some words might be substituted
by their part-of-speech. We use the seeds to build
initial patterns. Two candidate contexts that con-
tain the same seed create one positive example. By
using each initial seed, we can extract all contexts
surrounding these positive examples. Then we se-
lect the stronger ones. We used Collins and Singer
method (Collins and Singer, 1999) to compute the
strength of each example. If we consider x as a con-
text, the strength as a positive example of x is de-
38
Features Description
F1 Sequence of part-of-speech
F2 Length of sequence in bytes
F3 Number of tokens
F4 Sequence of PoS between the seed and the first verb before the seed
F5 Sequence of PoS between the seed and the first noun before the seed
F6 First verb before the seed
F7 First noun before the seed
F8 Token before the seed
F9 Seed
F10 Token after the seed
F11 First verb after the seed
F12 First noun after the seed
F13 Sequence of PoS between the seed and the first verb after the seed
F14 Sequence of PoS between the seed and the first noun after the seed
F15 Number of verbs in the candidate context
F16 Number of nouns in the candidate context
F17 Number of adjective in the candidate context
F18 Number of adverbs in the candidate context
Table 5: The features that we used for paraphrase extraction.
fined as:
Strength(x) = count(x+)/count(x) (1)
In Equation 1, count(x+) is the number of times
context x surrounded a seed in a positive example
and count(x) is frequency of the context x. This
allows us to score the potential pattern.
4.5 Bootstrapping Algorithm for Paraphrase
Extraction
Our bootstrapping algorithm is summarized in Fig-
ure 2. It starts with a set of seeds, which are consid-
ered initial paraphrases. A set of extraction patterns
is initially empty. The algorithm generates candidate
contexts, from the aligned similar contexts. The can-
didate patterns are scored by how many paraphrases
they can extract. Those with the highest scores are
added to the set of extraction patterns. Using the ex-
tended set of extraction patterns, more paraphrase
pairs are extracted and added to the set of para-
phrases. Using the enlarged set of paraphrases, more
extraction patterns are extracted. The process keeps
iterating until no new patterns or no new paraphrases
are learned.
Our method is able to accumulate a large lexi-
con of emotion phrases by bootstrapping from the
manually initialized list of seed words. In each it-
eration, the paraphrase set is expanded with related
phrases found in the corpus, which are filtered by
using a measure of strong surrounding context sim-
ilarity. The bootstrapping process starts by select-
ing a subset of the extraction patterns that aim to
extract the paraphrases. We call this set the pattern
pool. The phrases extracted by these patterns be-
come candidate paraphrases. They are filtered based
on how many patterns select them, in order to pro-
duce the final paraphrases from the set of candidate
paraphrases.
5 Results and Evaluation
The result of our algorithm is a set of extraction pat-
terns and a set of pairs of paraphrases. Some of the
paraphrases extracted by our system are shown in
Table 6. The paraphrases that are considered correct
are shown under Correct paraphrases. As explained
in the next section, two human judges agreed that
these are acceptable paraphrases. The results con-
sidered incorrect by the two judges are shown un-
39
Algorithm 1: Bootstrapping Algorithm.
For each seed for an emotion
Loop until no more paraphrases or no more contexts are learned.
1- Locate the seeds in each sentence
2- Find similar contexts surrounding a pair of two seeds
3- Analyze all contexts surrounding the two seeds to extract
the strongest patterns
4- Use the new patterns to learn more paraphrases
Figure 2: Our bootstrapping algorithm for extracting paraphrases.
der Incorrect paraphrases. Our algorithm learnt 196
extraction patterns and produced 5926 pairs of para-
phrases. Table 7 shows the number of extraction pat-
terns and the number of paraphrase pairs that were
produced by our algorithm for each class of emo-
tions. For evaluation of our algorithm, we use two
techniques. One uses human judges to judge if a
sample of paraphrases extracted by our method are
correct; we also measures the agreement between
the judges (See Section 5.1). The second estimates
the recall and the precision of our method (See Sec-
tion 5.2. In the following subsections we describe
these evaluations.
5.1 Evaluating Correctness with Human
Judges
We evaluate the correctness of the extracted para-
phrase pairs, using the same method as Brazilay and
McKeown (2001). We randomly selected 600 para-
phrase pairs from the lexical paraphrases produced
by our algorithm: for each class of emotion we
selected 100 paraphrase pairs. We evaluated their
correctness with two human judges. They judged
whether the two expressions are good paraphrases
or not.
We provided a page of guidelines for the judges.
We defined paraphrase as ?approximate conceptual
equivalence?, the same definition used in (Barzilay
and McKeown, 2001). Each human judge had to
choose a ?Yes? or ?No? answer for each pair of para-
phrases under test. We did not include example sen-
tences containing these paraphrases. A similar Ma-
chine Translation evaluation task for word-to-word
translation was done in (Melamed, 2001).
Figure 3 presents the results of the evaluation: the
correctness for each class of emotion according to
judge A, and according to judge B. The judges were
graduate students in computational linguistics, na-
tive speakers of English.
We also measured the agreement between the two
judges and the Kappa coefficient (Siegel and Castel-
lan, 1988). If there is complete agreement between
two judges Kappa is 1, and if there is no agreement
between the judges then Kappa = 0. The Kappa
values and the agreement values for our judges are
presented in Figure 4.
The inter-judge agreement over all the para-
phrases for the six classes of emotions is 81.72%,
which is 490 out of the 600 paraphrases pairs in our
sample. Note that they agreed that some pairs are
good paraphrases, or they agreed that some pairs
are not good paraphrases, that is why the numbers
in Figure 4 are higher than the correctness numbers
from Figure 3. The Kappa coefficient compensates
for the chance agreement. The Kappa value over
all the paraphrase pairs is 74.41% which shows a
significant agreement.
Figure 3: The correctness results according the judge A
and judge B, for each class of emotion.
5.2 Estimating Recall
Evaluating the Recall of our algorithm is difficult
due to following reasons. Our algorithm is not able
to cover all the English words; it can only detect
40
Disgust
Correct paraphrases:
being a wicked::getting of evil; been rather sick::feeling rather nauseated;
feels somewhat queasy::felt kind of sick; damn being sick::am getting sick
Incorrect paraphrases:
disgusting and vile::appealing and nauseated; get so sick::some truly disgusting
Fear
Correct paraphrases:
was freaking scared::was quite frightened; just very afraid::just so scared;
tears of fright::full of terror; freaking scary::intense fear;
Incorrect paraphrases:
serious panic attack::easily scared; not necessarily fear::despite your fear
Anger
Correct paraphrases:
upset and angry::angry and pissed; am royally pissed::feeling pretty angry;
made me mad::see me angry; do to torment::just to spite
Incorrect paraphrases:
very pretty annoying::very very angry; bitter and spite::tired and angry
Happiness
Correct paraphrases:
the love of::the joy of; in great mood::in good condition;
the joy of::the glad of; good feeling::good mood
Incorrect paraphrases:
as much eagerness::as many gladness; feeling smart::feel happy
Sadness
Correct paraphrases:
too depressing::so sad; quite miserable::quite sorrowful;
strangely unhappy::so misery; been really down::feel really sad
Incorrect paraphrases:
out of pity::out of misery; akward and depressing::terrible and gloomy
Surprise
Correct paraphrases:
amazement at::surprised by; always wonder::always surprised;
still astounded::still amazed; unexpected surprise::got shocked
Incorrect paraphrases:
passion and tremendous::serious and amazing; tremendous stress::huge shock
Table 6: Examples of paraphrases extracted by our algorithm (correctly and incorrectly).
41
Class of Emotion # Paraphrases Pairs # Extraction Patterns
Disgust 1125 12
Fear 1004 31
Anger 670 47
Happiness 1095 68
Sadness 1308 25
Surprise 724 13
Total 5926 196
Table 7: The number of lexical and extraction patterns produced by the algorithm.
Figure 4: The Kappa coefficients and the agreement be-
tween the two human judges.
paraphrasing relations with words which appeared
in our corpus. Moreover, to compare directly with
an electronic thesaurus such as WordNet is not fea-
sible, because WordNet contains mostly synonym
sets between words, and only a few multi-word ex-
pressions. We decided to estimate recall manually,
by asking a human judge to extract paraphrases by
hand from a sample of text. We randomly selected
60 texts (10 for each emotion class) and asked the
judge to extract paraphrases from these sentences.
For each emotion class, the judge extracted expres-
sions that reflect the emotion, and then made pairs
that were conceptually equivalent. It was not feasi-
ble to ask a second judge to do the same task, be-
cause the process is time-consuming and tedious.
In Information Retrieval, Precision and Recall are
defined in terms of a set of retrieved documents and
a set of relevant documents 2. In the following sec-
tions we describe how we compute the Precision and
Recall for our algorithm compared to the manually
2http://en.wikipedia.org/wiki/
Category of Emotions Precision Recall
Disgust 82.33% 92.91%
Fear 82.64% 88.20%
Anger 93.67% 80.57%
Happiness 82.00% 90.89%
Sadness 82.00% 89.88%
Surprise 79.78% 89.50%
Average 84.23% 88.66%
Table 8: Precision and Recall for a sample of texts, for
each category of emotion, and their average.
extracted paraphrases.
From the paraphrases that were extracted by the
algorithm from the same texts, we counted how
many of them were also extracted by the human
judge. Equation 2 defines the Precision. On av-
erage, from 89 paraphrases extracted by the algo-
rithm, 74 were identified as paraphrases by the hu-
man judge (84.23%). See Table 8 for the values for
all the classes.
P =
#Correctly Retrieved Paraphrases by the Algorithm
All Paraphrases Retrieved by the Algorithm
(2)
For computing the Recall we count how many of
the paraphrases extracted by the human judge were
correctly extracted by the algorithm (Equation 3).
R =
#Correctly Retrieved Paraphrases by the Algorithm
All Paraphrases Retrieved by the Human Judge
(3)
5.3 Discussion and Comparison to Related
Work
To the best of our knowledge, no similar research
has been done in extracting paraphrases for emotion
terms from corpora. However, Barzilay and McKe-
own (2001) did similar work to corpus-based iden-
42
tification of general paraphrases from multiple En-
glish translations of the same source text. We can
compare the pros and cons of our method compared
to their method. The advantages are:
? In our method, there is no requirement for the
corpus to be parallel. Our algorithm uses the
entire corpus together to construct its boot-
strapping method, while in (Barzilay and McK-
eown, 2001) the parallel corpus is needed in or-
der detect positive contexts.
? Since we construct the candidate contexts
based on the k-window approach, there is no
need for sentences to be aligned in our method.
In (Barzilay and McKeown, 2001) sentence
alignment is essential in order to recognize
identical words and positive contexts.
? The algorithm in (Barzilay and McKeown,
2001) has to find positive contexts first, then
it looks for appropriate patterns to extract para-
phrases. Therefore, if identical words do not
occur in the aligned sentences, the algorithm
fails to find positive contexts. But, our al-
gorithm starts with given seeds that allow us
to detect positive context with the k-window
method.
A limitation of our method is the need for the initial
seed words. However, obtaining these seed words
is not a problem nowadays. They can be found in
on line dictionaries, WordNet, and other lexical re-
courses.
6 Conclusion and Future Work
In this paper, we introduced a method for corpus-
based extraction of paraphrases for emotion terms.
We showed a method that used a bootstrapping tech-
nique based on contextual and lexical features and
is able to successfully extract paraphrases using a
non-parallel corpus. We showed that a bootstrapping
algorithm based on contextual surrounding context
features of paraphrases achieves significant perfor-
mance on our data set.
In future work, we will extend this techniques to
extract paraphrases from more corpora and for more
types of emotions. In terms of evaluation, we will
use the extracted paraphrases as features in machine
learning classifiers that classify candidate sentences
into classes of emotions. If the results of the classifi-
cation are good, this mean the extracted paraphrases
are of good quality.
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.
2005. Emotions from text: machine learning for text-
based emotion prediction. In Proceedings of the Hu-
man Language Technology Conference Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005).
Saima Aman and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In TSD, pages 196?
205.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceeding
of ACL/EACL, 2001, Toulouse.
Thorstein Bostad. 2003. Sentence Based Automatic Sen-
timent Classification. Ph.D. thesis, University of Cam-
bridge, Computer Speech Text and Internet Technolo-
gies (CSTIT), Computer Laboratory, Jan.
Jonathan Chevelu, Thomas Lavergne, Yves Lepage, and
Thierry Moudenc. 2009. Introduction of a new para-
phrase generation tool based on Monte-Carlo sam-
pling. In Proceedings of ACL-IJCNLP 2009, Singa-
pore, pages 249?25.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of ACL-IJCNLP 2009,
Singapore, pages 468?476.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6:169?200.
L. Iordanskaja, Richard Kittredget, and Alain Polguere,
1991. Natural Language Generation in Artificial In-
telligence and Computational Linguistics. Kluwer
Academic.
Carroll E. Izard. 1971. The Face of Emotion. Appleton-
Century-Crofts., New York.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL.
Ilya Dan Melamed. 2001. Empirical Methods for Ex-
ploiting Parallel Texts. MIT Press.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Natural Language
Learning (CoNLL 2004), Boston, May.
43
George Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller, 1993. Introduc-
tion to Wordnet: An On-Line Lexical Database. Cog-
nitive Science Laboratory, Princeton University, Au-
gust.
Gilad Mishne. 2005. Experiments with mood classifica-
tion in blog posts. ACM SIGIR.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, page 10441049.
The AAAI Press/MIT Press.
Sidney Siegel and John Castellan, 1988. Non Parametric
Statistics for Behavioral Sciences. . McGraw-Hill.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of the 4th
International Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic, June 2007.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: an affective extension of wordnet. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC 2004),
Lisbon, May 2004, pages 1083?1086.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252?259.
Xiaoyin Wang, David Lo, Jing Jiang, Lu Zhang, and
Hong Mei. 2009. Extracting paraphrases of tech-
nical terms from noisy parallel software corpora. In
Proceedings of ACL-IJCNLP 2009, Singapore, pages
197?200.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics, pages 189?196.
Shiqi Zhao, Xiang Lan, Ting Liu, , and Sheng Li.
2009. Application-driven statistical paraphrase gen-
eration. In Proceedings of ACL-IJCNLP 2009, Singa-
pore, pages 834?842.
44
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 140?146,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Hierarchical versus Flat Classification of Emotions in Text 
 
Diman Ghazi (a), Diana Inkpen (a), Stan Szpakowicz (a, b) 
(a) School of Information Technology and Engineering, University of Ottawa 
(b) Institute of Computer Science, Polish Academy of Sciences 
{dghaz038,diana,szpak}@site.uottawa.ca 
 
 
 
Abstract 
We explore the task of automatic classifica-
tion of texts by the emotions expressed. Our 
novel method arranges neutrality, polarity and 
emotions hierarchically. We test the method 
on two datasets and show that it outperforms 
the corresponding ?flat? approach, which does 
not take into account the hierarchical informa-
tion. The highly imbalanced structure of most 
of the datasets in this area, particularly the two 
datasets with which we worked, has a dramat-
ic effect on the performance of classification. 
The hierarchical approach helps alleviate the 
effect. 
1 Introduction 
Computational approaches to emotion analysis 
have focused on various emotion modalities, but 
there was only limited effort in the direction of 
automatic recognition of emotion in text (Aman, 
2007). 
Oleveres et al(1998), as one of the first works in 
emotion detection in text, uses a simple Natural 
Language Parser for keyword spotting, phrase 
length measurement and emoticon identification. 
They apply a rule-based expert system to construct 
emotion scores based on the parsed text and con-
textual information. However their simple word-
level analysis system is not sufficient when the 
emotion is expressed by more complicated phrases 
and sentences. 
More advanced systems for textual emotion recog-
nition performed sentence-level analysis.  Liu et al 
(2003), proposed an approach aimed at understand-
ing the underlying semantics of language using 
large-scale real-world commonsense knowledge to 
classify sentences into ?basic? emotion categories. 
They developed a commonsense affect model 
enabling the analysis of the affective qualities of 
text in a robust way.  
In SemEval 2007, one of the tasks was carried out 
in an unsupervised setting and the emphasis was on 
the study of emotion in lexical semantics (Strappa-
rava and Mihalcea, 2008; Chaumartin, 2007; Koza-
reva et al, 2007; Katz et al, 2007). Neviarouskaya 
et al(2009) applied a rule-based approach to affect 
recognition from a blog text. However, statistical 
and machine learning approaches have became a 
method of choice for constructing a wide variety of 
NLP applications (Wiebe et al, 2005). 
There has been previous work using statistical 
methods and supervised machine learning, includ-
ing (Aman, 2007; Katz et al, 2007; Alm, 2008; 
Wilson et al, 2009). Most of that research concen-
trated on feature selections and applying lexical 
semantics rather than on different learning 
schemes. In particular, only flat classification has 
been considered. 
According to Kiritchenko et al (2006), ?Hierar-
chical categorization deals with categorization 
problems where categories are organized in hierar-
chies?. Hierarchical text categorization places new 
items into a collection with a predefined hierar-
chical structure. The categories are partially or-
dered, usually from more generic to more specific.  
Koller and Sahami (1997) carried out the first 
proper study of a hierarchical text categorization 
problem in 1997. More work in hierarchical text 
categorization has been reported later. Keshtkar 
and Inkpen (2009) applied a hierarchical approach 
to mood classification: classifying blog posts into 
132 moods. The connection with our work is only 
indirect, because ? even though moods and emo-
tions may seem similar ? their hierarchy structure 
and the classification task are quite different. The 
work reported in (Kiritchenko et al, 2006) is more 
general. It explores two main aspects of hierarchic-
140
al text categorization: learning algorithms and per-
formance evaluation.  
In this paper, we extend our preliminary work 
(Ghazi et al, 2010) on hierarchical classification. 
Hierarchical classification is a new approach to 
emotional analysis, which considers the relation 
between neutrality, polarity and emotion of a text. 
The main idea is to arrange these categories and 
their interconnections into a hierarchy and leverage 
it in the classification process. 
We categorize sentences into six basic emotion 
classes; there also may, naturally, be no emotion in 
a sentence. The emotions are happiness, sadness, 
fear, anger, disgust, and surprise (Ekman, 1992). 
In one of the datasets we applied, we did consider 
the class non-emotional. 
For these categories, we have considered two 
forms of hierarchy for classification, with two or 
three levels. In the two-level method, we explore 
the effect of neutral instances on one dataset and 
the effect of polarity on the other dataset. In the 
three-level hierarchy, we consider neutrality and 
polarity together. 
Our experiments on data annotated with emotions 
show performance which exceeds that of the corre-
sponding flat approach. 
Section 2 of this paper gives an overview of the 
datasets and feature sets. Section 3 describes both 
hierarchical classification methods and their 
evaluation with respect to flat classification results. 
Section 4 discusses future work and presents a few 
conclusions. 
2 Data and Feature Sets 
2.1 Datasets 
The statistical methods typically require training 
and test corpora, manually annotated with respect 
to each language-processing task to be learned 
(Wiebe et al, 2005). One of the datasets in our 
experiments is a corpus of blog sentences anno-
tated with Ekman?s emotion labels (Aman, 2007). 
The second dataset is a sentence-annotated corpus 
resource divided into three parts for large-scale 
exploration of affect in children?s stories (Alm, 
2008). 
In the first dataset, each sentence is tagged by a 
dominant emotion in the sentence, or labelled as 
non-emotional. The dataset contains 173 weblog 
posts annotated by two judges. Table 1 shows the 
details of the dataset. 
In the second dataset, two annotators have anno-
tated 176 stories. The affects considered are the 
same as Ekman?s six emotions, except that the 
surprise class is subdivided into positive surprise 
and negative surprise. We run our experiments on 
only sentences with high agreement- sentences 
with the same affective labels annotated by both 
annotators. That is the version of the dataset which 
merged angry and disgusted instances and com-
bined the positive and negative surprise classes. 
The resulting dataset, therefore, has only five 
classes (Alm, 2008). Table 1 presents more details 
about the datasets, including the range of frequen-
cies for the class distribution (Min is the proportion 
of sentences with the most infrequent class, Max is 
the proportion for sentences with the most frequent 
class.) The proportion of the most frequent class 
also gives us a baseline for the accuracies of our 
classifiers (since the poorest baseline classifier 
could always choose the most frequent class).  
Table 1. Datasets specifications. 
 Domain Size # classes Min-Max% 
Aman?s 
Data set 
Weblogs 2090 7 6-38 % 
Alm?s 
Data set 
Stories 1207 5 9-36% 
2.2 Feature sets 
In (Ghazi et al, 2010), three sets of features ? one 
corpus-based and two lexically-based ? are com-
pared on Aman?s datasets. The first experiment is a 
corpus-based classification which uses unigrams 
(bag-of-words). In the second experiment, classifi-
cation was based on features derived from the 
Prior-Polarity lexicon1 (Wilson et al 2009); the 
features were the tokens common between the 
prior-polarity lexicon and the chosen dataset. In the 
last experiment, we used a combination of the 
emotional lists of words from Roget?s Thesaurus2 
(Aman and Szpakowicz, 2008) and WordNet Af-
fect3 (Strapparava and Valitutti, 2004); we call it 
the polarity feature set.  
                                                 
1 www.cs.pitt.edu/mpqa 
2 The 1987 Penguin?s Roget?s Thesaurus was used. 
3
 www.cse.unt.edu/~rada/affective 
text/data/WordNetAffectEmotioLists.tar.gz 
141
Based on the results and the discussion in (Ghazi et 
al., 2010), we decided to use the polarity feature 
set in our experiments. This feature set has certain 
advantages. It is quite a bit smaller than the uni-
gram features, and we have observed that they ap-
pear to be more meaningful. For example, the 
unigram features include (inevitably non-
emotional) names of people and countries. It is 
also possible to have misspelled tokens in uni-
grams, while the prior-polarity lexicon features are 
well-defined words usually considered as polar. 
Besides, lexical features are known to be more 
domain- and corpus-independent. Last but not 
least, our chosen feature set significantly outper-
forms the third set. 
2.3 Classification 
As a classification algorithm, we use the support 
vector machines (SVM) algorithm with tenfold 
cross-validation as a testing option. It is shown that 
SVM obtains good performance in text classifica-
tion: it scales well to the large numbers of features 
(Kennedy and Inkpen, 2006; Aman, 2007).  
We apply the same settings at each level of the 
hierarchy for our hierarchical approach classifica-
tion.  
In hierarchical categorization, categories are organ-
ized into levels (Kiritchenko et al, 2006). We use 
the hierarchical categories to put more knowledge 
into our classification method as the category hier-
archies are carefully composed manually to repre-
sent our knowledge of the subject. We will achieve 
that in two forms of hierarchy. A two-level hierar-
chy represents the relation of emotion and neutral-
ity in text, as well as the relation of positive and 
negative polarity. These two relations are exam-
ined in two different experiments, each on a sepa-
rate dataset. 
A three-level hierarchy is concerned with the rela-
tion between polarity and emotions along with the 
relation between neutrality and emotion. We as-
sume that, of Ekman's six emotions, happiness be-
longs to the positive polarity class, while the other 
five emotions have negative polarity. This is quite 
similar to the three-level hierarchy of affect labels 
used by Alm (2008). In her diagram, she considers 
happiness and positive surprise as positive, and the 
rest as negative emotions. She has not, however, 
used this model in the classification approach: 
classification experiments were only run at three 
separate affect levels. She also considers positive 
and negative surprise as one Surprise class. 
For each level of our proposed hierarchy, we run 
two sets of experiments. In the first set, we assume 
that all the instances are correctly classified at the 
preceding levels, so we only need to be concerned 
with local mistakes. Because we do not have to 
deal with instances misclassified at the previous 
level, we call these results reference results.  
In the second set of experiments, the methodology 
is different than in (Ghazi et al 2010). In that work 
both training and testing of subsequent levels is 
based on the results of preceding levels. A question 
arises, however: once we have good data available, 
why train on incorrect data which result from mis-
takes at the preceding level? That is why we de-
cided to train on correctly-labelled data and when 
testing, to compute global results by cumulating 
the mistakes from all the levels of the hierarchical 
classification. In other words, classification mis-
takes at one level of the hierarchy carry on as mis-
takes at the next levels. Therefore, we talk of 
global results because we compute the accuracy, 
precision, recall and F-measure globally, based on 
the results at all levels. These results characterize 
the hierarchical classification approach when test-
ing on new sentences: the classifiers are applied in 
a pipeline order: level 1, then level 2 on the results 
of the previous level (then level 3 if we are in the 
three-level setting).   
In the next section, we show the experiments and 
results on our chosen datasets. 
 
3 Results and discussions 
3.1 Two-level classification 
This section has two parts. The main goal of the 
first part is to find out how the presence of neutral 
instances affects the performance of features for 
distinguishing between emotional classes in 
Aman?s dataset. This was motivated by a similar 
work in polarity classification (Wilson et al, 
2009). 
In the second part, we discuss the effect of consid-
ering positive and negative polarity of emotions for 
five affect classes in Alm?s dataset. 
142
3.1.1 Neutral-Emotional 
At the first level, emotional versus non-emotional 
classification tries to determine whether an in-
stance is neutral or emotional. The second step 
takes all instances which level 1 classified as emo-
tional, and tries to classify them into one of Ek-
man's six emotions. Table 2 presents the result of 
experiments and, for comparison, the flat classifi-
cation results. A comparison of the results in both 
experiments with flat classification shows that in 
both cases the accuracy of two-level approach is 
significantly better than the accuracy of flat classi-
fication. 
One of the results worth discussing further is the 
precision of the non-emotional class: it increases 
while recall decreases. We will see the same pat-
tern in further experiments. This happens to the 
classes which used to dominate in flat classifica-
tion but they no longer dominate in hierarchical 
classification. Classifiers tends to give priority to a 
dominant class, so more instances are placed in 
this class; thus, classification achieves low preci-
sion and high recall. Hierarchical methods tend to 
produce higher precision. 
The difference between precision and recall of the 
happiness class in the flat approach and the two-
level approach cannot be ignored. It can be ex-
plained as follows: at the second level there are no 
more non-emotional instances, so the happiness 
class dominates, with 42% of all the instances. As 
explained before, this gives high recall and low 
precision for the happiness class. We hope to ad-
dress this big gap between precision and recall of 
the happiness class in the next experiments, three-
level classification. It separates happiness from the 
other five emotions, so it makes the number of in-
stances of each level more balanced. 
Our main focus is comparing hierarchical and flat 
classification, assuming all the other parameters 
are fixed. We mention, however, the best previous 
results achieved by Aman (2007) on the same data-
set. Her best result was obtained by combining 
corpus-based unigrams, features derived from 
emotional lists of words from Roget?s Thesaurus 
(explained in 2.2) and common words between the 
dataset and WordNetAffect. She also applied SVM 
with tenfold cross validation. The results appear in 
Table 3. 
     Table 3. Aman?s best results on her data set. 
 Precision Recall F-Measure 
happiness 0.813  0.698  0.751  
sadness  0.605  0.416  0.493  
fear  0.868  0.513  0.645  
surprise  0.723  0.409  0.522  
disgust  0.672  0.488  0.566  
anger  0.650  0.436  0.522  
non-emo 0.587  0.625  0.605  
 
 
 
Table 2. Two-level emotional classification on Aman?s dataset (the highest precision, recall, and F-measure val-
ues for each class are shown in bold). The results of the flat classification are repeated for convenience. 
Two-level classification Flat classification 
 Precision Recall F-measure Precision Recall F-measure 
1st level emo non-emo 
0.88 
0.88 
0.85 
0.81 
0.86 
0.84 
-- 
0.54 
-- 
0.87 
-- 
0.67 
2nd level 
reference results 
 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.59 
0.77 
0.91 
0.75 
0.66 
0.72 
0.95 
0.49 
0.49 
0.32 
0.35 
0.33 
0.71 
0.60 
0.63 
0.45 
0.45 
0.46 
0.74 
0.69 
0.82 
0.64 
0.68 
0.67 
0.60 
0.42 
0.49 
0.27 
0.31 
0.26 
0.66 
0.52 
0.62 
0.38 
0.43 
0.38 
Accuracy   68.32%   61.67%  
2-level experi-
ment 
global results 
non-emo 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.88 
0.56 
0.64 
0.75 
0.56 
0.52 
0.55 
0.81 
0.86 
0.42 
0.43 
0.29 
0.29 
0.27 
0.84 
0.68 
0.51 
0.55 
0.38 
0.37 
0.36 
0.54 
0.74 
0.69 
0.82 
0.64 
0.68 
0.67
0.87 
0.60 
0.42 
0.49 
0.27 
0.31 
0.26 
0.67 
0.66 
0.52 
0.62 
0.38 
0.43 
0.38 
Accuracy   65.50%   61.67%  
 
143
 By comparing the reference results in Table 2 with 
Aman?s result shown in Table 3, our results on two  
classes, non-emo and sadness are significantly bet-
ter. Even though recall of our experiments is high-
er for happiness class, the precision makes the F-
measure to be lower. The reason behind the differ-
ence between the precisions is the same as their 
difference between in our hierarchical and flat 
comparisons. As it was also mentioned there we 
hope to address this problem in three-level classifi-
cation. Both precision and recall of the sadness in 
our experiments is higher than Aman?s results. We 
have a higher precision for fear, but recall is 
slightly lower. For the last three classes our preci-
sion is higher while recall is significantly lower.  
 
The size of these three classes, which are the smal-
lest classes in the dataset, appears to be the reason. 
It is possible that the small set of features that we 
are using will recall fewer instances of these 
classes comparing to the bigger feature sets used 
by Aman (2007).  
3.1.2 Negative-Positive polarity 
These experiments have been run on Alm?s dataset 
with five emotion classes. This part is based on the 
assumption that the happiness class is positive and 
the remaining four classes are negative.  
 
 
At the first level, positive versus negative classifi-
cation tries to determine whether an instance bears 
a positive emotion. The second step takes all in-
stances which level 1 classified as negative, and 
tries to classify them into one of the four negative 
classes, namely sadness, fear, surprise and anger-
disgust. The results show a higher accuracy in ref-
erence results while it is slightly lower for global 
results. In terms of precision and recall, however, 
there is a high increase in precision of positive 
(happiness) class while the recall decreases. 
The results show a higher accuracy in reference 
results while it is slightly lower for global results. 
In terms of precision and recall, however, there is a 
high increase in precision of positive (happiness) 
class while the recall decreases. 
We also see a higher F-measure for all classes in 
the reference results. That confirms the consistency 
between the result in Table 2 and Table 4. 
In the global measurements, recall is higher for all 
the classes at the second level, but the F-measure is 
higher only for three classes. 
Here we cannot compare our results with the best 
previous results achieved by Alm (2008), because 
the datasets and the experiments are not the same. 
She reports the accuracy of the classification re-
sults for three sub-corpora separately. She random-
ly selected neutral instances from the annotated 
data and added them to the dataset, which makes it 
Table 4. Two-level emotional classification on Alm?s dataset (the highest precision, recall, and F-measure val-
ues for each class are shown in bold). 
Two-level classification Flat classification 
 Precision Recall F-measure Precision Recall F-measure 
1st level neg pos 
0.81 
0.84 
0.93 
0.64 
0.87 
0.72 
-- 
0.56 
-- 
0.86 
-- 
0.68 
2nd level 
reference results 
 
sadness 
fear 
surprise 
anger 
0.65 
0.59 
0.45 
0.49 
0.68 
0.40 
0.21 
0.73 
0.66 
0.47 
0.29 
0.59 
0.67 
0.59 
0.35 
0.54 
0.53 
0.38 
0.10 
0.43 
0.59 
0.46 
0.16 
0.48 
Accuracy   59.07%   57.41%  
2-level experiment 
global results 
happiness 
sadness 
fear 
surprise 
anger 
0.84 
0.55 
0.45 
0.27 
0.43 
0.64 
0.61 
0.39 
0.21 
0.68 
0.72 
0.58 
0.42 
0.19 
0.53 
0.56 
0.67 
0.59 
0.35 
0.54 
0.86 
0.53 
0.38 
0.10 
0.43 
0.68 
0.59 
0.46 
0.16 
0.48 
Accuracy   56.57%   57.41%  
 
144
different than the data set we used in our experi-
ments.  
3.2 Three-level classification 
In this approach, we go even further: we break the 
seven-class classification task into three levels. 
The first level defines whether the instance is emo-
tional. At the second level the instances defined as 
emotional by the first level will be classified on 
their polarity. At the third level, we assume that the 
instances of happiness class have positive polarity 
and the other five emotions negative polarity. That 
is why we take the negative instances from the 
second level and classify them into the five nega-
tive emotion classes. Table 5 presents the results of  
this classification. The results show that the accu-
racy of both reference results and global results are 
higher than flat classification, but the accuracy of 
the global results is not significantly better. 
At the first and second level, the F-measure of no-
emotion and happiness classes is significantly bet-
ter. At the third level, except in the class disgust, 
we see an increase in the F-measure of all classes 
in comparison with both the two-level and flat 
classification. 
 
Table 5. Three-level emotional classification on Aman?s data-
set (the highest precision, recall, and F-measure values for 
each class are shown in bold) 
?
Three-level Classification 
 Precision Recall F 
1st level emo 
non-emo 
0.88 
0.88 
0.85 
0.81 
0.86 
0.84 
2nd level 
reference results 
positive 
negative 
0.89 
0.79 
0.65 
0.94 
0.75 
0.86 
3rd level 
reference results 
 
 
sadness 
fear 
surprise 
disgust 
anger 
0.63 
0.88 
0.79 
0.42 
0.38 
0.54 
0.52 
0.37 
0.38 
0.71 
0.59 
0.65 
0.50 
0.40 
0.49 
Accuracy   65.5%  
 
3-level experi-
ment 
global results 
non-emo 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.88 
0.77 
0.43 
0.52 
0.46 
0.31 
0.35 
0.81 
0.62 
0.49 
0.4 
0.32 
0.31 
0.55 
0.84 
0.69 
0.46 
0.45 
0.38 
0.31 
0.43 
Accuracy   62.2%  
Also, as shown by the two-level experiments, the 
results of the second level of the reference results 
approach an increase in the precision of the happi-
ness class. That makes the instances defined as 
happiness more precise. 
By comparing the results with Table 3, which is 
the best previous results, we see an increase in the 
precision of happiness class and its F-measure 
consequently; therefore in these results we get a 
higher F-measure for three classes, non-emo, sad-
ness and fear. We get the same F-measure for hap-
piness and slightly lower F-measure for surprise 
but we still have a lower F-measure for the other 
two classes, namely, disgust and anger. The other 
difference is the high increase in the recall value 
for fear. 
4 Conclusions and Future Work 
The focus of this study was a comparison of the 
hierarchical and flat classification approaches to 
emotional analysis and classification. In the emo-
tional classification we noticed that having a 
dominant class in the dataset degrades the results 
significantly. A classifier trained on imbalanced 
data gives biased results for the classes with more 
instances. Our results, based on a novel method, 
shows that the hierarchical classification approach 
is better at dealing with the highly imbalanced 
data. We also saw a considerable improvement in 
the classification results when we did not deal with 
the errors from previous steps and slightly better 
results when we evaluated the results globally. 
In the future, we will consider different levels of 
our hierarchy as different tasks which could be 
handled differently. Each of the tasks has its own 
specification. We can, therefore, definitely benefit 
from analyzing each task separately and defining 
different sets of features and classification methods 
for each task rather than using the same method for 
every task. 
145
References 
Alm, C.: ?Affect in text and speech?, PhD disserta-
tion, University of Illinois at Urbana-
Champaign, Department of Linguistics (2008) 
Aman, S.: ?Identifying Expressions of Emotion in 
Text?, Master's thesis, University of Ottawa, Ot-
tawa, Canada (2007) 
Aman, S., Szpakowicz, S.: ?Using Roget?s Thesau-
rus for Fine-grained Emotion Recognition?. 
Proc. Conf. on Natural Language Processing 
(IJCNLP), Hyderabad, India, 296-302 (2008) 
Chaumartin. F.: ?Upar7: A knowledge-based sys-
tem for headline sentiment tagging?, Proc. Se-
mEval-2007, Prague, Czech Republic, June 
(2007) 
Ekman, P.: ?An Argument for Basic Emotions?, 
Cognition and Emotion, 6, 169-200 (1992) 
Ghazi, D., Inkpen, D., Szpakowicz, S.: ?Hierar-
chical approach to emotion recognition and clas-
sification in texts?, A. Farzindar and V. Keselj 
(eds.), Proc. 23rd Canadian Conference on Ar-
tificial Intelligence, Ottawa, ON.  Lecture Notes 
in Artificial Intelligence 6085, Springer Verlag,  
40?50 (2010) 
Katz, P., Singleton, M., Wicentowski, R.: ?Swat-
mp: the semeval-2007 systems for task 5 and 
task 14?, Proc. SemEval-2007, Prague, Czech 
Republic, June (2007) 
Kennedy, A., Inkpen, D.: ?Sentiment classification 
of movie reviews using contextual valence shif-
ter?, Computational Intelligence 22. 110-125 
(2006) 
Keshtkar, F., Inkpen, D.: ?Using Sentiment Orien-
tation Features for Mood Classification in Blog 
Corpus?, IEEE International Conf. on NLP and 
KE, Dalian, China, Sep. 24-27 (2009) 
Kiritchenko, S., Matwin, S., Nock, R., Famili, 
F.: ?Learning and Evaluation in the Presence of 
Class Hierarchies: Application to Text Categori-
zation?, Lecture Notes in Artificial Intelligence 
4013, Springer, 395-406 (2006) 
Koller, D., Sahami, M.: ?Hierarchically Classify-
ing Documents Using Very Few Words?. Proc. 
International Conference on Machine Learning, 
170-178 (1997) 
Kozareva, Z., Navarro, B., Vazquez, S., Montoyo, 
A.: ?UA-ZBSA: A headline emotion classifica-
tion through web information?, Proc. SemEval-
2007, Prague, Czech Republic, June (2007) 
Liu, H., Lieberman, H., Selker, T.: ?A Model of 
Textual Affect Sensing using Real-World 
Knowledge?. In Proc. IUI 2003, 125-132 (2003) 
Neviarouskaya, A., Prendinger, H., and Ishizuka, 
M.: ?Compositionality Principle in Recognition 
of Fine-Grained Emotions from Text?, In: Pro-
ceedings of Third International Conference on 
Weblogs and Social Media (ICWSM?09), 
AAAI, San Jose, California, US, 278-281 (2009) 
Olveres, J., Billinghurst, M., Savage, J., Holden, 
A.: ?Intelligent, Expressive Avatars?. In Proc. of 
the WECC?98, 47-55 (1998) 
 Strapparava, C., Mihalcea, R.: ?SemEval-2007 
Task 14: Affective Text? (2007) 
Strapparava, C., Mihalcea, R.: ?Learning to Identi-
fy Emotions in Text?, Proc. ACM Symposium 
on Applied computing, Fortaleza, Brazil, 1556-
1560 (2008) 
Wilson, T., Wiebe, J., Hoffmann, P.: ?Recognizing 
contextual polarity: an exploration of features 
for phrase-level sentiment analysis?, Computa-
tional Linguistics 35(3), 399-433 (2009) 
 Wiebe, J., Wilson, T., Cardie, C.: ?Annotating 
Expressions of Opinions and Emotions in Lan-
guage?, Language Resources and Evaluation 39, 
165-210 (2005) 
146
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 91?98,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Extraction of Disease-Treatment Semantic Relations from Biomedical 
Sentences 
 
Oana Frunza and Diana Inkpen 
School of Information Technology and Engineering 
University of Ottawa Ottawa, ON, Canada, K1N 6N5 
{ofrunza,diana}@site.uottawa.ca 
 
  
 
 
Abstract 
This paper describes our study on identi-
fying semantic relations that exist between 
diseases and treatments in biomedical sen-
tences. We focus on three semantic rela-
tions: Cure, Prevent, and Side Effect. The 
contributions of this paper consists in the 
fact that better results are obtained com-
pared to previous studies and the fact that 
our research settings allow the integration 
of biomedical and medical knowledge. 
We obtain 98.55% F-measure for the Cure 
relation, 100% F-measure for the Prevent 
relation, and 88.89% F-measure for the 
Side Effect relation. 
1 Introduction 
Research in the fields of life-science and bio-
medical domain has been the focus of the Natural 
Language Processing (NLP) and Machine Learn-
ing (ML) community for some time now. This 
trend goes very much inline with the direction 
the medical healthcare system is moving to: the 
electronic world. The research focus of scientists 
that work in the filed of computational linguistics 
and life science domains also followed the trends 
of the medicine that is practiced today, an Evi-
dence Based Medicine (EBM). This new way of 
medical practice is not only based on the experi-
ence a healthcare provider acquires as time 
passes by, but on the latest discoveries as well. 
We live in an information explosion era where it 
is almost impossible to find that piece of relevant 
information that we need. With easy and cheep 
access to disk-space we sometimes even find 
challenging to find our stored local documents. It 
should come to no surprise that the global trend 
in domains like biomedicine and not only is to 
rely on technology to identify and upraise infor-
mation. The amount of publications and research 
that is indexed in the life-science domain grows 
almost exponentially (Hunter and Cohen (2006) 
making the task of finding relevant information, 
a hard and challenging task for NLP research.  
The search for information in the life-science 
domain is not only the focus of researchers that 
work in these fields, but the focus of laypeople as 
well. Studies reveal that people are searching the 
web for medical-related articles to be better in-
formed about their health. Ginsberg et al (2009) 
show how a new outbreak of the influenza virus 
can be detected from search engine query data.   
The aim of this paper is to show which NLP 
and ML techniques are suitable for the task of 
identifying semantic relations between diseases 
and treatments in short biomedical texts. The 
value of our work stands in the results we obtain 
and the new feature representation techniques.  
2 Related Work  
The most relevant work for our study is the work 
of Rosario and Hearst (2004). The authors of this 
paper are the ones that created and distributed the 
data set used in our research. The data set is an-
notated with disease and treatments entities and 
with 8 semantic relations between diseases and 
treatments. The main focus of their work is on 
entity recognition ? the task of identifying enti-
ties, diseases and treatments in biomedical text 
sentences. The authors use Hidden Markov 
Models and maximum entropy models to per-
form both the task of entity recognition and of 
relation discrimination. Their representation 
techniques are based on words in context, part-
of-speech information, phrases, and terms from 
MeSH1, a medical lexical knowledge-base. Com-
pared to previous work, our research is focused 
                                                 
1
 http://www.nlm.nih.gov/mesh/meshhome.html 
91
on different representation techniques, different 
classification models, and most importantly in 
obtaining improved results without using the an-
notations of the entities (new data will not have 
them). In previous research, the best results were 
obtained when the entities involved in the rela-
tions were identified and used as features.  
The biomedical literature contains a wealth of 
work on semantic relation extraction, mostly fo-
cused on more biology-specific tasks: subcellu-
lar-location (Craven 1999), gene-disorder asso-
ciation (Ray and Craven 2001), and diseases and 
drugs relations (Srinivasan and Rindflesch 2002, 
Ahlers et al, 2007). 
Text classification techniques combined with a 
Na?ve Bayes classifier and relational learning 
algorithms are methods used by Craven (1999). 
Hidden Markov Models are used in Craven 
(2001), but similarly to Rosario and Hearst 
(2004), the research focus was entity recognition.  
A context based approach using MeSH term 
co-occurrences are used by Srinivasan and Rind-
flesch (2002) for relationship discrimination be-
tween diseases and drugs.  
A lot of work is focused on building rules used 
to extract relation. Feldman et al (2002) use a 
rule-based system to extract relations that are 
focused on genes, proteins, drugs, and diseases. 
Friedman et al (2001) go deeper into building a 
rule-based system by hand-crafting a semantic 
grammar and a set of semantic constraints in or-
der to recognize a range of biological and mo-
lecular relations. 
3 Task and Data Sets 
Our task is focused on identifying disease-
treatment relations in sentences. Three relations: 
Cure, Prevent, and Side Effect, are the main ob-
jective of our work. We are tackling this task by 
using techniques based on NLP and supervised 
ML techniques. We decided to focus on these 
three relations because these are the ones that are 
better represented in the original data set and in 
the end will allow us to draw more reliable con-
clusions. Also, looking at the meaning of all rela-
tions in the original data set, the three that we 
focus on are the ones that could be useful for 
wider research goals and are the ones that really 
entail relations between two entities. In the su-
pervised ML settings the amount of training data 
is a factor that influences the performance; sup-
port for this stands not only in the related work 
performed on the same data set, but in the re-
search literature as well. The aim of this paper is 
to focus on few relations of interest and try to 
identify what predictive model and what repre-
sentation techniques bring the best results of 
identifying semantic relations in short biomedi-
cal texts. We mostly focused on the value that 
the research can bring, rather than on an incre-
mental research. 
As mentioned in the previous section, the data 
set that we use to run our experiments is the one 
of Rosario and Hearst (2004). The entire data set 
is collected from Medline2 2001 abstracts. Sen-
tences from titles and abstracts are annotated 
with entities and with 8 relations, based only on 
the information present in a certain sentence. The 
first 100 titles and 40 abstracts from each of the 
59 Medline 2001 files were used for annotation. 
Table 1, presents the original data set, as pub-
lished in previous research. The numbers in pa-
renthesis represent the training and test set sizes.  
 
Relationship Definition and Example 
Cure 
810 (648, 162) 
TREAT cures DIS 
Intravenous immune globulin for 
recurrent spontaneous abortion 
Only DIS 
616 (492, 124) 
TREAT not mentioned 
Social ties and susceptibility to 
the common cold 
Only TREAT 
166 (132, 34) 
DIS not mentioned 
Flucticasome propionate is safe in 
recommended doses 
Prevent 
63 (50, 13) 
TREAT prevents the DIS 
Statins for prevention of stroke 
Vague 
36 (28, 8) 
Very unclear relationship 
Phenylbutazone and leukemia 
Side Effect 
29 (24, 5) 
DIS is a result of a TREAT 
Malignant mesodermal mixed 
tumor of the uterus following 
irradiation 
NO Cure 
4 (3, 1) 
TREAT does not cure DIS 
Evidence for double resistance to 
permethrin and malathion in head 
lice 
     Total relevant: 1724 (1377, 347) 
Irrelevant 
1771 (1416, 355) 
Treat and DIS not present 
Patients were followed up for 6 
months 
Total: 3495 (2793, 702) 
 Table 1. Original data set.  
     
From this original data set, the sentences that are 
annotated with Cure, Prevent, Side Effect, Only 
DIS, Only TREAT, and Vague are the ones that 
used in our current work. While our main focus 
is on the Cure, Prevent, and Side Effect, we also 
run experiments for all relations such that a di-
rect comparison with the previous work is done.  
                                                 
2
 http://medline.cos.com/ 
92
Table 2 describes the data sets that we created 
from the original data and used in our experi-
ments. For each of the relations of interest we 
have 3 labels attached: Positive, Negative, and 
Neutral. The Positive label is given to sentences 
that are annotated with the relation in question in 
the original data; the Negative label is given to 
the sentences labeled with Only DIS and Only 
TREAT classes in the original data; Neutral label 
is given to the sentences annotated with Vague 
class in the original data set.  
 
Table 2. Our data sets3. 
4 Methodology 
The experimental settings that we follow are 
adapted to the domain of study (we integrate ad-
ditional medical knowledge), yielding for the 
methods to bring improved performance.  
The challenges that can be encountered while 
working with NLP and ML techniques are: find-
ing the suitable model for prediction ? since the 
ML field offers a suite of predictive models (al-
gorithms), the task of finding the suitable one 
relies heavily on empirical studies and knowl-
edge expertise; and finding the best data repre-
sentation ? identifying the right and sufficient 
features to represent the data is a crucial aspect. 
These challenges are addressed by trying various 
predictive algorithms based on different learning 
techniques, and by using various textual repre-
sentation techniques that we consider suitable.  
The task of identifying the three semantic rela-
tions is addressed in three ways: 
       Setting 1: build three models, each focused 
on one relation that can distinguish sentences 
that contain the relation ? Positive label, from 
other sentences that are neutral ? Neutral label, 
and from sentences that do not contain relevant 
information ? Negative label; 
                                                 
3
 The number of sentences available for download is 
not the same as the ones from the original data set, 
published in Rosario and Hearst (?04). 
Setting 2: build three models, each focused on 
one relation that can distinguish sentences that 
contain the relation from sentences that do not 
contain any relevant information. This setting is 
similar to a two-class classification task in which 
instances are labeled either with the relation in 
question ? Positive label, or with non-relevant 
information ? Negative label; 
  Setting 3: build one model that distinguishes the 
three relations ? a three-way classification task 
where each sentence is labeled with one of the 
semantic relations, using the data with all the 
Positive labels. 
The first set of experiments is influenced by 
previous research done by Koppel and Schler 
(2005). The authors claim that for polarity learn-
ing ?neutral? examples help the learning algo-
rithms to better identify the two polarities. Their 
research was done on a corpus of posts to chat 
groups devoted to popular U.S. television and 
posts to shopping.com?s product evaluation page. 
As classification algorithms, a set of 6 repre-
sentative models: decision-based models (Deci-
sion trees ? J48), probabilistic models (Na?ve 
Bayes and complement Na?ve Bayes (CNB), 
which is adapted for imbalanced class distribu-
tion), adaptive learning (AdaBoost), linear classi-
fier (support vector machine (SVM) with poly-
nomial kernel), and a classifier, ZeroR, that al-
ways predicts the majority class in the training 
data used as a baseline. All classifiers are part of 
a tool called Weka4. 
As representation technique, we rely on fea-
tures such as the words in the context, the noun 
and verb-phrases, and the detected biomedical 
and medical entities. In the following subsec-
tions, we describe all the representation tech-
niques that we use.  
4.1 Bag-of-words representation 
 
The bag-of-words (BOW) representation is 
commonly used for text classification tasks. It is 
a representation in which the features are chosen 
among the words that are present in the training 
data. Selection techniques are used in order to 
identify the most suitable words as features. Af-
ter the feature space is identified, each training 
and test instance is mapped into this feature rep-
resentation by giving values to each feature for a 
certain instance. Two feature value representa-
tions are the most commonly used for the BOW 
representation: binary feature values ? the value 
                                                 
4
 http://www.cs.waikato.ac.nz/ml/weka/ 
Train  
          Relation Positive Negative Neutral 
Cure 554 531 25 
Prevent 42 531 25 
SideEffect 20 531 25 
 Test   
Relation Positive Negative Neutral 
Cure 276 266 12 
Prevent 21 266 12 
SideEffect 10 266 12 
93
of a feature is 1 if the feature is present in the 
instance and 0 otherwise, or frequency feature 
values ? the feature value is the number of times 
it appears in an instance, or 0 if it did not appear.  
Taking into consideration the fact that an in-
stance is a sentence, the textual information is 
relatively small. Therefore a frequency value 
representation is chosen. The difference between 
a binary value representation and a frequency 
value representation is not always significant, 
because sentences tend to be short. Nonetheless, 
if a feature appears more than once in a sentence, 
this means that it is important and the frequency 
value representation captures this aspect. 
The selected features are words (not lemma-
tized) delimited by spaces and simple punctua-
tion marks: space, ( , ) , [ , ] , . , ' , _ that ap-
peared at least three times in the training collec-
tion and contain at least an alpha-numeric char-
acter, are not part of an English list of stop 
words5 and are longer than three characters. Stop 
words are function words that appear in every 
document (e.g., the, it, of, an) and therefore do 
not help in classification. The frequency thresh-
old of three is commonly used for text collec-
tions because it removes non-informative fea-
tures and also strings of characters that might be 
the result of a wrong tokenization when splitting 
the text into words. Words that have length of 
one or two characters are not considered as fea-
tures because of two reasons: possible incorrect 
tokenization and problems with very short acro-
nyms in the medical domain that could be highly 
ambiguous (could be a medical acronym or an 
abbreviation of a common word).  
4.2 NLP and biomedical concepts represen-
tation  
The second type of representation is based on 
NLP information ? noun-phrases, verb-phrases 
and biomedical concepts (Biomed). In order to 
extract this type of information from the data, we 
used the Genia6 tagger. The tagger analyzes Eng-
lish sentences and outputs the base forms, part-
of-speech tags, chunk tags, and named entity 
tags. The tagger is specifically tuned for bio-
medical text such as Medline abstracts.  
Figure 1 presents an output example by the 
Genia tagger for the sentence: ?Inhibition of NF-
kappaB activation reversed the anti-apoptotic 
effect of isochamaejasmin.?. The tag O stands 
for Outside, B for Beginning, and I for Inside. 
                                                 
5
 http://www.site.uottawa.ca/~diana/csi5180/StopWords 
6
 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 
Figure 1. Example of Genia tagger output 
Inhibition     Inhibition  NN  B-NP  O 
of       of   IN  B-PP  O  
NF-kappaB NF-kappaB  NN  B-NP B-protein  
activation    activation   NN  I-NP  O  
reversed       reverse  VBD  B-VP  O  
the       the   DT  B-NP  O  
anti-apoptotic anti-apoptotic JJ  I-NP  O  
effect        effect  NN  I-NP  O  
of        of   IN  B-PP  O  
isochamaejasmin isochamaejasmin NN B-NP  O  
.  .   .  O  O 
 
The noun-phrases and verb-phrases identified by 
the tagger are considered as features for our sec-
ond representation technique. The following pre-
processing steps are applied before defining the 
set of final features: remove features that contain 
only punctuation, remove stop-words, and con-
sider valid features only the lemma-based forms 
of the identified noun-phrases, verb-phrases and 
biomedical concepts. The reason to do this is 
because there are a lot of inflected forms (e.g., 
plural forms) for the same word and the lemma-
tized form (the base form of a word) will give us 
the same base form for all the inflected forms.  
4.3 Medical concepts (UMLS) representa-
tion 
In order to work with a representation that pro-
vides features that are more general than the 
words in the abstracts (used in the BOW repre-
sentation), we also used the unified medical lan-
guage system7 (here on UMLS) concept repre-
sentations. UMLS is a knowledge source devel-
oped at the U.S. National Library of Medicine 
(here on NLM) and it contains a meta-thesaurus, 
a semantic network, and the specialist lexicon for 
biomedical domain. The meta-thesaurus is organ-
ized around concepts and meanings; it links al-
ternative names and views of the same concept 
and identifies useful relationships between dif-
ferent concepts. UMLS contains over 1 million 
medical concepts, and over 5 million concept 
names which are hierarchical organized. Each 
unique concept that is present in the thesaurus 
has associated multiple text strings variants 
(slight morphological variations of the concept). 
All concepts are assigned at least one semantic 
type from the semantic network providing a gen-
eralization of the existing relations between con-
cepts. There are 135 semantic types in the 
knowledge base linked through 54 relationships.  
                                                 
7 http://www.nlm.nih.gov/pubs/factsheets/umls.html 
94
In addition to the UMLS knowledge base, 
NLM created a set of tools that allow easier ac-
cess to the useful information. MetaMap8  is a 
tool created by NLM that maps free text to medi-
cal concepts in the UMLS, or equivalently, it 
discovers meta-thesaurus concepts in text. With 
this software, text is processed through a series 
of modules that in the end will give a ranked list 
of all possible concept candidates for a particular 
noun-phrase. For each of the noun phrases that 
the system finds in the text, variant noun phrases 
are generated. For each of the variant noun 
phrases, candidate concepts (concepts that con-
tain the noun phrase variant) from the UMLS 
meta-thesaurus are retrieved and evaluated. The 
retrieved concepts are compared to the actual 
phrase using a fit function that measures the text 
overlap between the actual phrase and the candi-
date concept (it returns a numerical value). The 
best of the candidates are then organized accord-
ing to the decreasing value of the fit function. 
We used the top concept candidate for each iden-
tified phrase in an abstract as a feature.  Figure 2 
presents an example of the output of the Meta-
Map system for the phrase ?to an increased 
risk". The information presented in the brackets, 
the semantic type, ?Qualitative Concept, Quanti-
tative Concept? for the candidate with the fit 
function value 861 is the feature used for our 
UMLS representation. 
 
Figure 2. Example of MetaMap system output 
Meta Candidates (6) 
861 Risk [Qualitative Concept, Quantitative Concept] 
694 Increased (Increased (qualifier value)) [Func-
tional Concept] 
623 Increase (Increase (qualifier value)) [Functional 
Concept] 
601 Acquired (Acquired (qualifier value)) [Temporal 
Concept] 
601 Obtained (Obtained (attribute)) [Functional Con-
cept] 
588 Increasing (Increasing (qualifier value)) [Func-
tional Concept] 
 
Another reason to use a UMLS concept represen-
tation is the concept drift phenomenon that can 
appear in a BOW representation. Especially in 
the medical domain texts, this is a frequent prob-
lem as stated by Cohen et al (2004). New arti-
cles that publish new research on a certain topic 
bring with them new terms that might not match 
the ones that were seen in the training process in 
a certain moment of time.  
                                                 
8
 http://mmtx.nlm.nih.gov/ 
Experiments for the task tackled in our re-
search are performed with all the above-
mentioned representations, plus combinations of 
them. We combine the BOW, UMLS and NLP 
and biomedical concepts by putting all features 
together to represent an instance.   
5 Results 
This section presents the results obtained for the 
task of identifying semantic relations with the 
methods described above. As evaluation meas-
ures we report F-measure and accuracy values. 
The main evaluation metric that we consider is 
the F-measure9, since it is a suitable when the 
data set is imbalanced. We report the accuracy 
measure as well, because we want to compare 
our results with previous work. Table A1 from 
appendix A presents the results that we obtained 
with our methods. The table contains F-measure 
scores for all three semantic relations with the 
three experimental settings proposed for all com-
binations of representation and classification al-
gorithms. In this section, since we cannot report 
all the results for all the classification algorithms, 
we decided to report the classifiers that obtained 
the lower and upper margin of results for every 
representation setting. More detailed descriptions 
for the results are present in appendix A. We 
consider as baseline a classifier that always pre-
dicts the majority class. For the relation Cure the 
F-measure baseline is 66.51%, for Prevent and 
Side Effect 0%. 
The next three figures present the best results 
obtained for the three experimental settings. 
 
Figure 3. Best results for Setting 1. 
85.14%
62.50%
34.48%
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
Cure - BOW +
NLP + Biomed+
UMLS - SMO
Prevent -
UMLS + NLP +
Biomed - SVM
SideEffect -
BOW- NB
Results - Setting1F-measure
 
                                                 
9
 F-measure represents the harmonic mean between 
precision and recall. Precision represents the percent-
age of correctly classified sentences while recall 
represents the percentage of sentences identified as 
relevant by the classifier.  
95
Figure 4. Best results for Setting 2. 
82.00%
84.00%
86.00%
88.00%
90.00%
92.00%
94.00%
96.00%
98.00%
100.00%
Cure -
BOW + 
NLP + 
Biomed+ 
UMLS - NB
Prevent -
BOW + 
NLP + 
Biomed+ 
UMLS - NB
SideEffect 
- BOW + 
NLP + 
Biomed+ 
UMLS -
CNB
98.55% 100%
88.89%
Results - Setting 2
F-measure
 
 
Figure 5. Best results for Setting 3. 
98.55% 100%
88.89%
80.00%
85.00%
90.00%
95.00%
100.00%
Cure -  BOW +
NLP +
Biomed+
UMLS - NB
Prevent - 
BOW + NLP +
Biomed+
UMLS - NB
SideEffect -
BOW + NLP +
Biomed+
UMLS - CNB
Results - Setting 3
F-measure
 
 
6 Discussion 
Our goal was to obtain high performance results 
for the three semantic relations. The first set of 
experiments was influenced by previous work on 
a different task. The results obtained show that 
this setting might not be suitable for the medical 
domain, due to one of the following possible ex-
planations: the number of examples that are con-
sidered as being neutral is not sufficient or not 
appropriate (the neutral examples are considered 
sentences that are annotated with a Vague rela-
tion in the original data); or the negative exam-
ples are not appropriate (the negative examples 
are considered sentences that talk about either 
treatment or about diseases). The results of these 
experiments are shown in Figure 3. As future 
work, we want to run similar setting experiments 
when considering negative examples sentences 
that are not informative, labeled Irrelevant, from 
the original data set, and the neutral examples the 
ones that are considered negative in this current 
experiments.  
In Setting 2, the results are better than in the 
previous setting, showing that the neutral exam-
ples used in the previous experiments confused 
the algorithms and were not appropriate. These 
results validate the fact that the previous setting 
was not the best one for the task. 
The best results for the task are obtained with 
the third setting, when a model is built and 
trained on a data set that contains all sentences 
annotated with the three relations. The represen-
tation and the classification algorithms were able 
to make the distinction between the relations and 
obtained the best results for this task. The results 
are: 98.55% F-measure for the Cure class, 100% 
F-measure for the Prevent class, and 88.89% for 
the Side Effect class.  
Some important observations can be drawn 
from the obtained results: probabilistic and linear 
models combined with informative feature repre-
sentations bring the best results. They are consis-
tent in outperforming the other classifiers in all 
the three settings. AdaBoost classifier was out-
performed by other classifiers, which is a little 
surprising, taking into consideration the fact that 
this classifier tends to work better on imbalanced 
data. BOW is a representation technique that 
even though it is simplistic, most of the times it 
is really hard to outperform. One of the major 
contributions of this work is the fact that the cur-
rent experiments show that additional informa-
tion used in the representation settings brings 
improvements for the task. The task itself is a 
knowledge-charged task and the experiments 
show that classifiers can perform better when 
richer information (e.g. concepts for medical  
ontologies) is provided.  
6.1 Comparison to previous work 
Even though our main focus is on the three rela-
tions mentioned earlier, in order to validate our 
methodology, we also performed the 8-class 
classification task, similar to the one done by 
Rosario and Hearst (2004). Figure 3 presents a 
graphical comparison of the results of our meth-
ods to the ones obtained in the previous work. 
We report accuracy values for these experiments, 
as it was done in the previous work. 
In Figure 3, the first set of bar-results repre-
sents the best individual results for each relation. 
The representation technique and classification 
model that obtains the best results are the ones 
described on the x-axis.  
 
 
 
 
 
96
Figure 3. Comparison of results. 
Results for all semantic relations
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
120.00%
Cu
re
 
-
 
BO
W+
NL
P+
Bio
m
ed
+U
ML
S-C
NB
No
_
Cu
re
Pr
ev
en
t-B
OW
+N
LP
+B
iom
ed
-
CN
B
Va
gu
e 
-
 
BO
W 
+ 
NL
P+
Bi
om
ed
 
-
 
NB
Sid
eE
ffe
ct 
-
BO
W+
NL
P+
Bio
m
ed
-
NB
Tre
ar
m
en
t_O
nly
 
-
BO
W+
NL
P+
Bio
m
ed
-
NB
Dis
ea
se
_
On
ly-
BO
W+
NL
P+
Bi
om
ed
-
J4
8
Irr
ele
va
nt
 
-
 
BO
W+
NL
P+
Bio
m
ed
+U
ML
S-A
da
B
Models
Ac
cu
ra
cy
Best Models
Best Model
Previous Work
 
 
The second series of results represents the 
overall best model that is reported for each rela-
tion. The model reported here is a combination 
of BOW, verb and noun-phrases, biomedical and 
UMLS concepts, with a CNB classifier. 
The third series of results represent the accu-
racy results obtained in previous work by Rosa-
rio and Hearst (2004). As we can see from the 
figure, the best individual models have a major 
improvement over previous results. When a sin-
gle model is used for all relations, our results 
improve the previous ones in four relations with 
the difference varying from: 3 percentage point 
difference (Cure) to 23 percentage point differ-
ence (Prevent). We obtain the same results for 
two semantic relations, No_Cure and Vague and 
we believe that this is the case due to the fact that 
these two classes are significantly under-
represented compared to the other ones involved 
in the task. For the Treatment_Only relation our 
results are outperformed with 1.5 percentage 
points and for the Irrelevant relation with 0.1 
percentage point, only when we use the same 
model for all relations.  
7 Conclusion and Future Work 
We can conclude that additional knowledge and 
deeper analysis of the task and data in question 
are required in order to obtain reliable results. 
Probabilistic models are stable and reliable for 
the classification of short texts in the medical 
domain. The representation techniques highly 
influence the results, common for the ML com-
munity, but more informative representations 
where the ones that consistently obtained the best 
results.  
As future work, we would like to extend the 
experimental methodology when the first setting 
is applied, and to use additional sources of in-
formation as representation techniques. 
 
References  
Ahlers C., Fiszman M., Fushman D., Lang F.-M., 
Rindflesch T. 2007. Extracting semantic predica-
tions from Medline citations for pharmacogenom-
ics. Pacific Symposium on Biocomputing, 12:209-
220. 
Craven M. 1999. Learning to extract relations from 
Medline. AAAI-99 Workshop on Machine Learn-
ing for Information Extraction. 
Feldman R. Regev Y., Finkelstein-Landau M., Hur-
vitz E., and Kogan B. 2002. Mining biomedical lit-
erature using information extraction. Current Drug 
Discovery.  
Friedman C., Kra P., Yu H., Krauthammer M., and 
Rzhetzky A. 2001. Genies: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles. Bioinformatics, 
17(1). 
Ginsberg J., Mohebbi Matthew H., Rajan S. Patel, 
Lynnette Brammer, Mark S. Smolinski & Larry 
Brilliant. 2009. Detecting influenza epidemics 
using search engine query data. Nature 457, 
1012-1014. 
Hunter Lawrence and K. Bretonnel Cohen. 2006. 
Biomedical Language Processing: What?s Beyond 
PubMed? Molecular Cell 21, 589?594. 
Ray S. and Craven M. 2001. Representing sentence 
structure in Hidden Markov Models for informa-
tion extraction. Proceedings of IJCAI-2001. 
Rosario B. and Marti A. Hearst. 2004. Classifying 
semantic relations in bioscience text. Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics, 430. 
 Koppel M. and J. Schler. 2005. Using Neutral Ex-
amples for Learning Polarity, Proceedings of 
IJCAI, Edinburgh, Scotland. 
Srinivasan P. and T. Rindflesch 2002. Exploring text 
mining from Medline. Proceedings of the AMIA 
Symposium.  
 
 
 
 
97
Appendix A. Detailed Results. 
 
Classification Algorithm - F-Measure (%) 
 
 
 
Relation 
 
 
Representation 
Setting1 Setting2 Setting3 
Cure NLP+Biomed AdaB 
ZeroR 
32.22 
66.51 
AdaB 
ZeroR 
35.69 
67.48 
CNB 
SVM 
87.88 
94.85 
 BOW AdaB 
CNB 
63.60 
79.22 
AdaB 
SVM 
67.23 
81.43 
CNB 
NB 
92.57 
96.80 
 UMLS AdaB 
NB 
61.08 
74.73 
AdaB 
NB 
64.78 
76.04 
CNB 
SVM 
88.20 
95.62 
 BOW+UMLS AdaB 
CNB 
56.07 
84.54 
AdaB 
NB 
74.68 
86.48 
J48 
NB 
96.13 
97.50 
 NLP+Biomed 
+UMLS 
AdaB 
NB 
61.08 
75.18 
AdaB 
NB 
64.78 
76.70 
CNB 
SVM 
90.87 
96.58 
 NLP+Biomed 
+BOW 
AdaB 
SVM 
53.04 
78.98 
AdaB 
CNB 
77.46 
81.86 
J48 
NB 
96.14 
97.86 
 NLP+Biomed+ 
BOW+UMLS 
AdaB 
SVM 
53.04 
85.14 
AdaB 
SVM 
72.32 
87.10 
J48 
NB 
96.32 
98.55 
Prevent NLP+Biomed AdaB 
NB 
0 
17.02 
AdaB,J48 
NB 
0 
22.86 
Ada,J48 
CNB 
0 
55.17 
 BOW CNB 
NB 
31.78 
50 
J48 
NB 
0 
61.9 
SVM 
CNB 
50 
89.47 
 UMLS AdaB 
NB 
0 
28.57 
J48 
SVM 
0 
48.28 
J48 
CNB 
0 
68.75 
 BOW+UMLS J48 
NB 
39.02 
57.14 
J48 
NB 
9.09 
75.68 
AdaB 
CNB 
60 
89.47 
 NLP+Biomed 
+UMLS 
AdaB 
SVM 
0 
62.50 
J48 
SVM 
16 
57.69 
J48 
CNB 
0 
97.56 
 NLP+Biomed 
+BOW 
SVM 
NB 
35 
54.90 
J48 
NB 
0 
66.67 
AdaB 
CNB 
64.52 
92.31 
 NLP+Biomed+ 
BOW+UMLS 
J48 
NB 
30.77 
62.30 
J48 
SVM 
0 
77.78 
AdaB,J48 
NB 
64.52 
100 
Side 
Effect 
NLP+Biomed AdaB 
NB,CNB 
0 
7.69 
J48,SVM 
AdaB 
0 
18.18 
AdaB,J48 
CNB 
0 
33.33 
 BOW AdaB 
NB 
0 
34.48 
AdaB,J48 
NB 
0 
50 
Ada,J48 
CNB 
0 
66.67 
 UMLS AdaB,J48,
SVM NB 
0 
22.22 
J48,SVM 
NB 
0 
33.33 
AdaB,J48 
NB,CNB 
0 
46.15 
 BOW+UMLS AdaB,J48 
NB 
0 
21.43 
J48 
NB 
0 
47 
AdaB 
CNB 
0 
75 
 NLP+Biomed+ 
UMLS 
AdaB,J48 
NB 
0 
19.35 
J48 
NB 
0 
31.58 
AdaB.J48 
NB,CNB 
0 
46.15 
 NLP+Biomed+ 
BOW 
AdaB,J48 
NB 
0 
33.33 
J48 
NB 
0 
55.56 
AdaB,J48 
CNB 
0 
88.89 
 NLP+Biomed+ 
BOW+UMLS 
AdaB,J48 
NB 
0 
24 
J48 
NB 
0 
46.15 
AdaB 
CNB 
0 
88.89 
Table A1. Results obtained with our methods. 
The Representation column describes all the feature representation techniques that we tried. The acro-
nym NLP stands from verb and noun-phrase features put together and Biomed for bio-medical con-
cepts (the ones extracted by Genia tagger). The first line of results for every representation technique 
presents the classier that obtained the lowest results, while the second line represents the classifier 
with the best F-measure score. In bold we mark the best scores for all semantic relations in each of the 
three settings. 
 
98
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70?78,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Prior versus Contextual Emotion of a Word in a Sentence
Diman Ghazi
EECS, University of Ottawa
dghaz038@uottawa.ca
Diana Inkpen
EECS, University of Ottawa
diana@eecs.uottawa.ca
Stan Szpakowicz
EECS, University of Ottawa &
ICS, Polish Academy of Sciences
szpak@eecs.uottawa.ca
Abstract
A set of words labelled with their prior emo-
tion is an obvious place to start on the auto-
matic discovery of the emotion of a sentence,
but it is clear that context must also be con-
sidered. No simple function of the labels on
the individual words may capture the overall
emotion of the sentence; words are interre-
lated and they mutually influence their affect-
related interpretation. We present a method
which enables us to take the contextual emo-
tion of a word and the syntactic structure of the
sentence into account to classify sentences by
emotion classes. We show that this promising
method outperforms both a method based on
a Bag-of-Words representation and a system
based only on the prior emotions of words.
The goal of this work is to distinguish auto-
matically between prior and contextual emo-
tion, with a focus on exploring features impor-
tant for this task.
1 Introduction
Recognition, interpretation and representation of af-
fect have been investigated by researchers in the
field of affective computing (Picard 1997). They
consider a wide range of modalities such as affect in
speech, facial display, posture and physiological ac-
tivity. It is only recently that there has been a grow-
ing interest in automatic identification and extraction
of sentiment, opinions and emotions in text.
Sentiment analysis is the task of identifying posi-
tive and negative opinions, emotions and evaluations
(Wilson, Wiebe, and Hoffmann, 2005). Most of the
current work in sentiment analysis has focused on
determining the presence of sentiment in the given
text, and on determining its polarity ? the positive or
negative orientation. The applications of sentiment
analysis range from classifying positive and nega-
tive movie reviews (Pang, Lee, and Vaithyanathan,
2002; Turney, 2002) to opinion question-answering
(Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie,
and Wiebe, 2005). The analysis of sentiment must,
however, go beyond differentiating positive from
negative emotions to give a systematic account of
the qualitative differences among individual emo-
tion (Ortony, Collins, and Clore, 1988).
In this work, we deal with assigning fine-grained
emotion classes to sentences in text. It might seem
that these two tasks are strongly tied, but the higher
level of classification in emotion recognition task
and the presence of certain degrees of similarities
between some emotion labels make categorization
into distinct emotion classes more challenging and
difficult. Particularly notable in this regard are two
classes, anger and disgust, which human annotators
often find hard to distinguish (Aman and Szpakow-
icz, 2007). In order to recognize and analyze affect
in written text ? seldom explicitly marked for emo-
tions ? NLP researchers have come up with a variety
of techniques, including the use of machine learn-
ing, rule-based methods and the lexical approach
(Neviarouskaya, Prendinger, and Ishizuka, 2011).
There has been previous work using statistical
methods and supervised machine learning applied to
corpus-based features, mainly unigrams, combined
with lexical features (Alm, Roth, and Sproat, 2005;
Aman and Szpakowicz, 2007; Katz, Singleton, and
Wicentowski, 2007). The weakness of such methods
70
is that they neglect negation, syntactic relations and
semantic dependencies. They also require large (an-
notated) corpora for meaningful statistics and good
performance. Processing may take time, and anno-
tation effort is inevitably high. Rule-based meth-
ods (Chaumartin, 2007; Neviarouskaya, Prendinger,
and Ishizuka, 2011) require manual creation of rules.
That is an expensive process with weak guaran-
tee of consistency and coverage, and likely very
task-dependent; the set of rules of rule-based af-
fect analysis task (Neviarouskaya, Prendinger, and
Ishizuka, 2011) can differ drastically from what un-
derlies other tasks such as rule-based part-of-speech
tagger, discourse parsers, word sense disambigua-
tion and machine translation.
The study of emotions in lexical semantics was
the theme of a SemEval 2007 task (Strapparava and
Mihalcea, 2007), carried out in an unsupervised set-
ting (Strapparava and Mihalcea, 2008; Chaumartin,
2007; Kozareva et al, 2007; Katz, Singleton, and
Wicentowski, 2007). The participants were encour-
aged to work with WordNet-Affect (Strapparava and
Valitutti, 2004) and SentiWordNet (Esuli and Sebas-
tiani, 2006). Word-level analysis, however, will not
suffice when affect is expressed by phrases which re-
quire complex phrase- and sentence-level analyses:
words are interrelated and they mutually influence
their affect-related interpretation. On the other hand,
words can have more than one sense, and they can
only be disambiguated in context. Consequently, the
emotion conveyed by a word in a sentence can differ
drastically from the emotion of the word on its own.
For example, according to the WordNet-Affect lex-
icon, the word ?afraid? is listed in the ?fear? cate-
gory, but in the sentence ?I am afraid it is going to
rain.? the word ?afraid? does not convey fear.
We refer to the emotion listed for a word in an
emotion lexicon as the word?s prior emotion. A
word?s contextual emotion is the emotion of the sen-
tence in which that word appears, taking the context
into account.
Our method combines several way of tackling the
problem. First, we find keywords listed in WordNet-
Affect and select the sentences which include emo-
tional words from that lexicon. Next, we study the
syntactic structure and semantic relations in the text
surrounding the emotional word. We explore fea-
tures important in emotion recognition, and we con-
happi- sad- anger dis- sur- fear total
ness ness gust prise
398 201 252 53 71 141 1116
Table 1: The distribution of labels in the WordNet-
Affect Lexicon.
sider their effect on the emotion expressed by the
sentence. Finally, we use machine learning to clas-
sify the sentences, represented by the chosen fea-
tures, by their contextual emotion.
We categorize sentences into six basic emotions
defined by Ekman (1992); that has been the choice
of most of previous related work. These emotions
are happiness, sadness, fear, anger, disgust and sur-
prise. There also may, naturally, be no emotion in a
sentence; that is tagged as neutral/non-emotional.
We evaluate our results by comparing our method
applied to our set of features with Support Vec-
tor Machine (SVM) applied to Bag-of-Words, which
was found to give the best performance among su-
pervised methods (Yang and Liu, 1999; Pang, Lee,
and Vaithyanathan, 2002; Aman and Szpakowicz,
2007; Ghazi, Inkpen, and Szpakowicz, 2010). We
show that our method is promising and that it out-
performs both a system which works only with prior
emotions of words, ignoring context, and a system
which applies SVM to Bag-of-Words.
Section 2 of this paper describes the dataset and
resources used. Section 3 discusses the features
which we use for recognizing contextual emotion.
Experiments and results are presented in Section 4.
In Section 5, we conclude and discuss future work.
2 Dataset and Resources
Supervised statistical methods typically require
training data and test data, manually annotated
with respect to each language-processing task to be
learned. In this section, we explain the dataset and
lexicons used in our experiments.
WordNet-Affect Lexicon (Strapparava and Vali-
tutti, 2004). The first resource we require is an
emotional lexicon, a set of words which indicate
the presence of a particular emotion. In our exper-
iments, we use WordNet-Affect, which contains six
lists of words corresponding to the six basic emo-
tion categories. It is the result of assigning a variety
71
Neutral Negative Positive Both
6.9% 59.7% 31.1% 0.3%
Table 2: The distribution of labels in the Prior-Polarity
Lexicon.
of affect labels to each synset in WordNet. Table 1
shows the distribution of words in WordNet-Affect.
Prior-Polarity Lexicon (Wilson, Wiebe, and
Hoffmann, 2009). The prior-polarity subjectivity
lexicon contains over 8000 subjectivity clues col-
lected from a number of sources. To create this
lexicon, the authors began with the list of subjec-
tivity clues extracted by Riloff (2003). The list
was expanded using a dictionary and a thesaurus,
and adding positive and negative word lists from
the General Inquirer.1 Words are grouped into
strong subjective and weak subjective clues; Table 2
presents the distribution of their polarity.
Intensifier Lexicon (Neviarouskaya, Prendinger,
and Ishizuka, 2010). It is a list of 112 modifiers (ad-
verbs). Two annotators gave coefficients for inten-
sity degree ? strengthening or weakening, from 0.0
to 2.0 ? and the result was averaged.
Emotion Dataset (Aman and Szpakowicz,
2007). The main consideration in the selection of
data for emotional classification task is that the data
should be rich in emotion expressions. That is why
we chose for our experiments a corpus of blog sen-
tences annotated with emotion labels, discussed by
Aman and Szpakowicz (2007). Each sentence is
tagged by its dominant emotion, or as non-emotional
if it does not include any emotion. The annotation is
based on Ekman?s six emotions at the sentence level.
The dataset contains 4090 annotated sentences, 68%
of which were marked as non-emotional. The highly
unbalanced dataset with non-emotional sentences as
by far the largest class, and merely 3% in the fear
and surprise classes, prompted us to remove 2000 of
the non-emotional sentences. We lowered the num-
ber of non-emotional sentences to 38% of all the
sentences, and thus reduced the imbalance. Table 3
shows the details of the chosen dataset.
1www.wjh.harvard.edu/?inquirer/
hp sd ag dg sr fr ne total
536 173 179 172 115 115 800 2090
Table 3: The distribution of labels in Aman?s modified
dataset. The labels are happiness, sadness, anger, dis-
gust, surprise, fear, no emotion.
3 Features
The features used in our experiments were motivated
both by the literature (Wilson, Wiebe, and Hoff-
mann, 2009; Choi et al, 2005) and by the explo-
ration of contextual emotion of words in the anno-
tated data. All of the features are counted based on
the emotional word from the lexicon which occurs in
the sentence. For ease of description, we group the
features into four distinct sets: emotion-word fea-
tures, part-of-speech features, sentence features and
dependency-tree features.
Emotion-word features. This set of features are
based on the emotion-word itself.
? The emotion of a word according to WordNet-
Affect (Strapparava and Valitutti, 2004).
? The polarity of a word according to the prior-
polarity lexicon (Wilson, Wiebe, and Hoff-
mann, 2009).
? The presence of a word in a small list of modi-
fiers (Neviarouskaya, Prendinger, and Ishizuka,
2010).
Part-of-speech features. Based on the Stanford
tagger?s output (Toutanova et al, 2003), every word
in a sentence gets one of the Penn Treebank tags.
? The part-of-speech of the emotional word it-
self, both according to the emotion lexicon and
Stanford tagger.
? The POS of neighbouring words in the same
sentence. We choose a window of [-2,2], as it
is usually suggested by the literature (Choi et
al., 2005).
Sentence features. For now we only consider the
number of words in the sentence.
Dependency-tree features. For each emotional
word, we create features based on the parse tree and
its dependencies produced by the Stanford parser
(Marneffe, Maccartney, and Manning, 2006). The
72
dependencies are all binary relations: a grammati-
cal relation holds between a governor (head) and a
dependent (modifier).
According to Mohammad and Turney (2010),2
adverbs and adjectives are some of the most
emotion-inspiring terms. This is not surprising con-
sidering that they are used to qualify a noun or a
verb; therefore to keep the number of features small,
among all the 52 different type of dependencies, we
only chose the negation, adverb and adjective modi-
fier dependencies.
After parsing the sentence and getting the de-
pendencies, we count the following dependency-tree
Boolean features for the emotional word.
? Whether the word is in a ?neg? dependency
(negation modifier): true when there is a nega-
tion word which modifies the emotional word.
? Whether the word is in a ?amod? dependency
(adjectival modifier): true if the emotional
word is (i) a noun modified by an adjective or
(ii) an adjective modifying a noun.
? Whether the word is in a ?advmod? depen-
dency (adverbial modifier): true if the emo-
tional word (i) is a non-clausal adverb or adver-
bial phrase which serves to modify the meaning
of a word, or (ii) has been modified by an ad-
verb.
We also have several modification features based
on the dependency tree. These Boolean features cap-
ture different types of relationships involving the cue
word.3 We list the feature name and the condition on
the cue word w which makes the feature true.
? Modifies-positive: w modifies a positive word
from the prior-polarity lexicon.
? Modifies-negative: w modifies a negative word
from the prior-polarity lexicon.
? Modified-by-positive: w is the head of the de-
pendency, which is modified by a positive word
from the prior-polarity lexicon.
? Modified-by-negative: w is the head of the
dependency, which is modified by a negative
word from the prior-polarity lexicon.
2In their paper, they also explain how they created an emo-
tion lexicon by crowd-sourcing, but ? to the best of our knowl-
edge ? it is not publicly available yet.
3The terms ?emotional word? and ?cue word? are used in-
terchangeably.
hp sd ag dg sr fr ne total
part 1 196 64 64 63 36 52 150 625
part 2 51 18 22 18 9 14 26 158
part 1+ 247 82 86 81 45 66 176 783
part 2
Table 4: The distribution of labels in the portions of
Aman?s dataset used in our experiments, named part 1,
part 2 and part 1+part 2. The labels are happiness, sad-
ness, anger, disgust, surprise, fear, no emotion.
? Modifies-intensifier-strengthen: w modifies a
strengthening intensifier from the intensifier
lexicon.
? Modifies-intensifier-weaken: w modifies a
weakening intensifier from the intensifier lex-
icon.
? Modified-by-intensifier-strengthen: w is the
head of the dependency, which is modified by
a strengthening intensifier from the intensifier
lexicon.
? Modified-by-intensifier-weaken: w is the head
of the dependency, which is modified by a
weakening intensifier from the intensifiers lex-
icon.
4 Experiments
In the experiments, we use the emotion dataset pre-
sented in Section 2. Our main consideration is to
classify a sentence based on the contextual emotion
of the words (known as emotional in the lexicon).
That is why in the dataset we only choose sentences
which contain at least one emotional word accord-
ing to WordNet-Affect. As a result, the number of
sentences chosen from the dataset will decrease to
783 sentences, 625 of which contain only one emo-
tional word and 158 sentences which contain more
than one emotional word. Their details are shown in
Table 4.
Next, we represent the data with the features pre-
sented in Section 3. Those features, however, were
defined for each emotional word based on their con-
text, so we will proceed differently for sentences
with one emotional word and sentences with more
than one emotional word.
? In sentences with one emotional word, we as-
sume the contextual emotion of the emotional
73
word is the same as the emotion assigned to the
sentence by the human annotators; therefore all
the 625 sentences with one emotional word are
represented with the set of features presented
in Section 3 and the sentence?s emotion will be
considered as their contextual emotion.
? For sentences with more than one emotional
word, the emotion of the sentence depends on
all emotional words and their syntactic and se-
mantic relations. We have 158 sentences where
no emotion can be assigned to the contextual
emotion of their emotional words, and all we
know is the dominant emotion of the sentence.
We will, therefore, have two different sets of ex-
periments. For the first set of sentences, the data are
all annotated, so we will take a supervised approach.
For the second set of sentences, we combine super-
vised and unsupervised learning. We train a clas-
sifier on the first set of data and we use the model
to classify the emotional words into their contextual
emotion in the second set of data. Finally, we pro-
pose an unsupervised method to combine the con-
textual emotion of all the emotional words in a sen-
tence and calculate the emotion of the sentence.
For evaluation, we report precision, recall, F-
measure and accuracy to compare the results. We
also define two baselines for each set of experiments
to compare our results with. The experiments are
presented in the next two subsections.
4.1 Experiments on sentences with one
emotional word
In these experiments, we explain first the baselines
and then the results of our experiments on the sen-
tences with only one emotional word.
Baseline
We develop two baseline systems to assess the dif-
ficulty of our task. The first baseline labels the sen-
tences the same as the most frequent class?s emo-
tion, which is a typical baseline in machine learning
tasks (Aman and Szpakowicz, 2007; Alm, Roth, and
Sproat, 2005). This baseline will result in 31% ac-
curacy.
The second baseline labels the emotion of the sen-
tence the same as the prior emotion of the only emo-
tional word in the sentence. The accuracy of this
Precision Recall F
SVM +
Bag-of-
Words
Happiness 0.59 0.67 0.63
Sadness 0.38 0.45 0.41
Anger 0.40 0.31 0.35
Surprise 0.41 0.33 0.37
Disgust 0.51 0.43 0.47
Fear 0.55 0.50 0.52
Non-emo 0.49 0.48 0.48
Accuracy 50.72%
SVM
+ our
features
Happiness 0.68 0.78 0.73
Sadness 0.49 0.58 0.53
Anger 0.66 0.48 0.56
Surprise 0.61 0.31 0.41
Disgust 0.43 0.38 0.40
Fear 0.67 0.63 0.65
Non-emo 0.51 0.53 0.52
Accuracy 58.88%
Logistic
Regres-
sion + our
features
Happiness 0.78 0.82 0.80
Sadness 0.53 0.64 0.58
Anger 0.69 0.62 0.66
Surprise 0.89 0.47 0.62
Disgust 0.81 0.41 0.55
Fear 0.71 0.71 0.71
Non-emo 0.53 0.64 0.58
Accuracy 66.88%
Table 5: Classification experiments on the dataset with
one emotional word in each sentence. Each experiment
is marked by the method and the feature set.
experiment is 51%, remarkably higher than the first
baseline?s accuracy. The second baseline is particu-
larly designed to address the emotion of the sentence
only based on the prior emotion of the emotional
words; therefore it will allow us to assess the dif-
ference between the emotion of the sentence based
on the prior emotion of the words in the sentence
versus the case when we consider the context and its
effect on the emotion of the sentence.
Learning Experiments
In this part, we use two classification algorithms,
Support Vector Machines (SVM) and Logistic Re-
gression (LR), and two different set of features,
the set of features from Section 3 and Bag-of-
Words (unigram). Unigram models have been
widely used in text classification and shown to pro-
vide good results in sentiment classification tasks.
In general, SVM has long been a method of
choice for sentiment recognition in text. SVM has
74
been shown to give good performance in text clas-
sification experiments as it scales well to the large
numbers of features (Yang and Liu, 1999; Pang, Lee,
and Vaithyanathan, 2002; Aman and Szpakowicz,
2007). For the classification, we use the SMO al-
gorithm (Platt, 1998) from Weka (Hall et al, 2009),
setting 10-fold cross validation as a testing option.
We compare applying SMO to two sets of features,
(i) Bag-of-Words, which are binary features defin-
ing whether a unigram exists in a sentence and (ii)
our set of features. In our experiments we use uni-
grams from the corpus, selected using feature selec-
tion methods from Weka.
We also compare those two results with the third
experiment: apply SimpleLogistic (Sumner, Frank,
and Hall, 2005) from Weka to our set of features,
again setting 10-fold cross validation as a testing op-
tion. Logistic regression is a discriminative prob-
abilistic classification model which operates over
real-valued vector inputs. It is relatively slow to train
compared to the other classifiers. It also requires ex-
tensive tuning in the form of feature selection and
implementation to achieve state-of-the-art classifica-
tion performance. Logistic regression models with
large numbers of features and limited amounts of
training data are highly prone to over-fitting (Alias-
i, 2008). Besides, logistic regression is really slow
and it is known to only work on data represented
by a small set of features. That is why we do not
apply SimpleLogistic to Bag-of-Words features. On
the other hand, the number of our features is rela-
tively low, so we find logistic regression to be a good
choice of classifier for our representation method.
The classification results are shown in Table 5.
We note consistent improvement. The results of
both experiments using our set of features signifi-
cantly outperform (on the basis of a paired t-test,
p=0.005) both the baselines and SVM applied to
Bag-of-Words features. We get the best result, how-
ever, by applying logistic regression to our feature
set. The number of our features and the nature of
the features we introduce make them an appropriate
choice of data representation for logistic regression
methods.
4.2 Experiments on sentences with more than
one emotional word
In these experiments, we combine supervised and
unsupervised learning. We train a classifier on the
first set of data, which is annotated, and we use the
model to classify the emotional words in the sec-
ond group of sentences. We propose an unsuper-
vised method to combine the contextual emotion of
the emotional words and calculate the emotion of the
sentence.
Baseline
We develop two baseline systems. The first base-
line labels all the sentences the same: as the emo-
tion of the most frequent class, giving 32% accu-
racy. The second baseline labels the emotion of the
sentence the same as the most frequently occurring
prior-emotion of the emotional words in the sen-
tence. In the case of a tie, we randomly pick one
of the emotions. The accuracy of this experiment
is 45%. Again, as a second baseline we choose a
baseline that is based on the prior emotion of the
emotional words so that we can compare it with the
results based on contextual emotion of the emotional
words in the sentence.
Learning Experiments
For sentences with more than one emotional
word, we represent each emotional word and its con-
text by the set of features explained in section 3. We
do not have the contextual emotion label for each
emotional word, so we cannot train the classifier on
these data. Consequently, we train the classifier on
the part of the dataset which only includes sentences
with one emotional word. In these sentences, each
emotional word is labeled with their contextual emo-
tion ? the same as the sentence?s emotion.
Once we have the classifier model, we get the
probability distribution of emotional classes for each
emotional word (calculated by the logistic regres-
sion function learned from the annotated data). We
add up the probabilities of each class for all emo-
tional words. Finally, we select the class with the
maximum probability. The result, shown in Table 6,
is compared using supervised learning, SVM, with
Bag-of-Words features, explained in previous sec-
tion, with setting 10-fold cross validation as a testing
75
Precision Recall F
SVM +
Bag-of-
Words
Happiness 0.52 0.60 0.54
Sadness 0.35 0.33 0.34
Anger 0.30 0.27 0.29
Surprise 0.14 0.11 0.12
Disgust 0.30 0.17 0.21
Fear 0.44 0.29 0.35
Non-emo 0.23 0.35 0.28
Accuracy 36.71%
Logistic
Regres-
sion +
unsu-
pervised
+ our
features
Happiness 0.63 0.71 0.67
Sadness 0.67 0.44 0.53
Anger 0.50 0.41 0.45
Surprise 1.00 0.22 0.36
Disgust 0.80 0.22 0.34
Fear 0.60 0.64 0.62
Non-emo 0.37 0.69 0.48
Accuracy 54.43%
Table 6: Classification experiments on the dataset with
more than one emotional word in each sentence. Each
experiment is marked by the method and the feature set.
option.4
By comparing the results in Table 6, we can see
that the result of learning applied to our set of fea-
tures significantly outperforms (on the basis of a
paired t-test, p=0.005) both baselines and the result
of SVM algorithm applied to Bag-of-Words features.
4.3 Discussion
We cannot directly compare our results with the pre-
vious results achieved by Aman and Szpakowicz
(2007), because the datasets differ. F-measure, pre-
cision and recall for each class are reported on the
whole dataset, but we only used part of that dataset.
To show how hard this task is, and to see where we
stand, the best result from (Aman and Szpakowicz,
2007) is shown in Table 7.
In our experiments, we showed that our approach
and our features significantly outperform the base-
lines and the SVM result applied to Bag-of-Words.
For the final conclusion, we add one more compar-
ison. As we can see from Table 6, the accuracy
result of applying SVM to Bag-of-Words is really
low. Because supervised methods scale well on large
datasets, one reason could be the size of the data we
use in this experiment; therefore we try to compare
4Since SVM does not return a distribution probability, we
cannot apply SVM to our features in this set of experiments.
Precision Recall F
Happiness 0.813 0.698 0.751
Sadness 0.605 0.416 0.493
Anger 0.650 0.436 0.522
Surprise 0.723 0.409 0.522
Disgust 0.672 0.488 0.566
Fear 0.868 0.513 0.645
Non-emo 0.587 0.625 0.605
Table 7: Aman?s best result on the dataset explained in
Section 2.
the results of the two experiments on all 758 sen-
tences with at least one emotional word.
For this comparison, we apply SVM with Bag-of-
Words features to all of 758 sentences and we get
an accuracy of 55.17%. Considering our features
and methodology, we cannot apply logistic regres-
sion with our features to the whole dataset; therefore
we calculate its accuracy by counting the percent-
age of correctly classified instances in both parts of
the dataset, used in the two experiments, and we get
an accuracy of 64.36%. We also compare the re-
sults with the baselines. The first baseline, which
is the percentage of most frequent class (happiness
in this case), results in 31.5% accuracy. The second
baseline based on the prior emotion of the emotional
words results in 50.13% accuracy. It is notable that
the result of applying LR to our set of features is
still significantly better than the result of applying
SVM to Bag-of-Words and both baselines; this sup-
ports our earlier conclusion. It is hard to compare
the results mentioned thus far, so we have combined
all the results in Figure 1, which displays the accu-
racy obtained by each experiment.
We also looked into our results and assessed the
cases where the contextual emotion is different from
the prior emotion of the emotional word. Consider
the sentence ?Joe said it does not happen that often
so it does not bother him.? Based on the emotion
lexicon, the word ?bother? is classified as angry; so
is the emotion of the sentence if we only consider
the prior emotion of words. In our set of features,
however, we consider the negation in the sentence,
so the sentence is classified as non-emotional rather
than angry. Another interesting sentence is the rather
simple ?You look like her I guess.? Based on the lex-
icon, the word ?like? is in the happy category, while
76
Figure 1: The comparison of accuracy results of all ex-
periments for sentences with one emotional word (part
1), sentences with more than one emotional words (part
2), and sentences with at least one emotional word (part
1+part 2).
the sentence is non-emotional. In this case, the part-
of-speech features play an important role and they
catch the fact that ?like" is not a verb here; it does
not convey a happy emotion and the sentence is clas-
sified as non-emotional.
We also analyzed the errors, and we found some
common errors due to:
? complex sentences or unstructured sentences
which will cause the parser to fail or return in-
correct data, resulting in incorrect dependency-
tree information;
? limited coverage of the emotion lexicon.
These are some of the issues which we would like
to address in our future work.
5 Conclusion and Future Directions
The focus of this study was a comparison of prior
emotion of a word with its contextual emotion, and
their effect on the emotion expressed by the sen-
tence. We also studied features important in recog-
nizing contextual emotion. We experimented with
a wide variety of linguistically-motivated features,
and we evaluated the performance of these fea-
tures using logistic regression. We showed that
our approach and features significantly outperform
the baseline and the SVM result applied to Bag-of-
Words.
Even though the features we presented did quite
well on the chosen dataset, in the future we would
like to show the robustness of these features by ap-
plying them to different datasets.
Another direction for future work will be to ex-
pand our emotion lexicon using existing techniques
for automatically acquiring the prior emotion of
words. Based on the number of instances in each
emotion class, we noticed there is a tight relation
between the number of words in each emotion list
in the emotion lexicon and the number of sentences
that are derived for each emotion class. It follows
that a larger lexicon will have a greater coverage of
emotional expressions.
Last but not least, one of the weaknesses of our
approach was the fact that we could not use all the
instances in the dataset. Again, the main reason was
the low coverage of the emotion lexicon that was
used. The other reason was the limitation of our
method: we had to only choose the sentences that
have one or more emotional words. As future work,
we would like to relax the restriction by using the
root of the sentence (based on the dependency tree
result) as a cue word rather than the emotional word
from the lexicon. So, for sentences with no emo-
tional word, we can calculate all the features regard-
ing the root word rather than the emotional word.
References
Alias-i. 2008. Lingpipe 4.1.0., October.
Alm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.
2005. Emotions from Text: Machine Learning for
Text-based Emotion Prediction. In HLT/EMNLP.
Aman, Saima and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Proc. 10th Inter-
national Conf. Text, Speech and Dialogue, pages 196?
205. Springer-Verlag.
Chaumartin, Fran?ois-Regis. 2007. UPAR7: a
knowledge-based system for headline sentiment tag-
ging. In Proc. 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 422?425.
Choi, Yejin, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proc. Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 355?362.
Ekman, Paul. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3):169?200.
Esuli, Andrea and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Resource
77
for Opinion Mining. In Proc. 5th Conf. on Language
Resources and Evaluation LREC 2006, pages 417?
422.
Ghazi, Diman, Diana Inkpen, and Stan Szpakowicz.
2010. Hierarchical approach to emotion recognition
and classification in texts. In Canadian Conference on
AI, pages 40?50.
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Katz, Phil, Matthew Singleton, and Richard Wicen-
towski. 2007. SWAT-MP: the SemEval-2007 systems
for task 5 and task 14. In Proc. 4th International Work-
shop on Semantic Evaluations, SemEval ?07, pages
308?313.
Kozareva, Zornitsa, Borja Navarro, Sonia V?zquez, and
Andr?s Montoyo. 2007. UA-ZBSA: a headline emo-
tion classification through web information. In Proc.
4th International Workshop on Semantic Evaluations,
SemEval ?07, pages 334?337.
Marneffe, Marie-Catherine De, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Proc.
LREC 2006.
Mohammad, Saif M. and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In Proc.
NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in
Text, CAAGET ?10, pages 26?34.
Neviarouskaya, Alena, Helmut Prendinger, and Mitsuru
Ishizuka. 2010. AM: textual attitude analysis model.
In Proc. NAACL HLT 2010 Workshop on Computa-
tional Approaches to Analysis and Generation of Emo-
tion in Text, pages 80?88.
Neviarouskaya, Alena, Helmut Prendinger, and Mitsuru
Ishizuka. 2011. Affect Analysis Model: novel rule-
based approach to affect sensing from text. Natural
Language Engineering, 17(1):95?135.
Ortony, Andrew, Allan Collins, and Gerald L. Clore.
1988. The cognitive structure of emotions. Cambridge
University Press.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proc. ACL-02 confer-
ence on Empirical methods in natural language pro-
cessing - Volume 10, EMNLP ?02, pages 79?86.
Platt, John C. 1998. Sequential Minimal Optimization:
A Fast Algorithm for Training Support Vector Ma-
chines.
Riloff, Ellen. 2003. Learning extraction patterns for sub-
jective expressions. In Proc. 2003 Conf. on Empirical
Methods in Natural Language Processing, pages 105?
112.
Stoyanov, Veselin, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proc. Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 923?930.
Strapparava, Carlo and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proc. Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 70?74, Prague, Czech Republic, June.
Strapparava, Carlo and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proc. 2008 ACM sym-
posium on Applied computing, SAC ?08, pages 1556?
1560.
Strapparava, Carlo and Alessandro Valitutti. 2004.
WordNet-Affect: an Affective Extension of Word-
Net. In Proc. 4th International Conf. on Language
Resources and Evaluation, pages 1083?1086.
Sumner, Marc, Eibe Frank, and Mark A. Hall. 2005.
Speeding Up Logistic Model Tree Induction. In Proc.
9th European Conference on Principles and Practice
of Knowledge Discovery in Databases, pages 675?
683.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proc. HLT-NAACL, pages 252?259.
Turney, Peter D. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. 40th Annual Meeting on
Association for Computational Linguistics, ACL ?02,
pages 417?424.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proc. HLT-EMNLP, pages 347?
354.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: An Explo-
ration of Features for Phrase-Level Sentiment Analy-
sis. Computational Linguistics, 35(3):399?433.
Yang, Yiming and Xin Liu. 1999. A re-examination
of text categorization methods. In Proc. 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?99, pages 42?49.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proc. 2003 conference on Empirical
methods in natural language processing, EMNLP ?03,
pages 129?136.
78
