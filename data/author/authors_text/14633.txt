Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 134?142,
Beijing, August 2010
Simultaneous Ranking and Clustering of Sentences: A Reinforcement 
Approach to Multi-Document Summarization 
1Xiaoyan Cai, 1Wenjie Li, 1You Ouyang, 2Hong Yan 
1Department of Computing, The Hong Kong Polytechnic University 
{csxcai,cswjli,csyouyang}@comp.polyu.edu.hk 
2Department of Logistics and Maritime Studies, The Hong Kong Polytechnic University 
lgthyan@polyu.edu.hk 
 
 
Abstract 
Multi-document summarization aims to 
produce a concise summary that contains 
salient information from a set of source 
documents. In this field, sentence ranking 
has hitherto been the issue of most concern. 
Since documents often cover a number of 
topic themes with each theme represented 
by a cluster of highly related sentences, 
sentence clustering was recently explored in 
the literature in order to provide more 
informative summaries. Existing cluster-
based ranking approaches applied clustering 
and ranking in isolation. As a result, the 
ranking performance will be inevitably 
influenced by the clustering result. In this 
paper, we propose a reinforcement approach 
that tightly integrates ranking and clustering 
by mutually and simultaneously updating 
each other so that the performance of both 
can be improved. Experimental results on 
the DUC datasets demonstrate its 
effectiveness and robustness. 
1 Introduction 
Automatic multi-document summarization has 
drawn increasing attention in the past with the 
rapid growth of the Internet and information 
explosion. It aims to condense the original text 
into its essential content and to assist in 
filtering and selection of necessary information. 
So far extractive summarization that directly 
extracts sentences from documents to compose 
summaries is still the mainstream in this field. 
Under this framework, sentence ranking is the 
issue of most concern. 
Though traditional feature-based ranking 
approaches and graph-based approaches 
employed quite different techniques to rank 
sentences, they have at least one point in 
common, i.e., all of them focused on sentences 
only, but ignored the information beyond the 
sentence level (referring to Figure 1(a)). 
Actually, in a given document set, there 
usually exist a number of themes (or topics) 
with each theme represented by a cluster of 
highly related sentences (Harabagiu and 
Lacatusu, 2005; Hardy et al, 2002). These 
theme clusters are of different size and 
especially different importance to assist users 
in understanding the content in the whole 
document set. The cluster level information is 
supposed to have foreseeable influence on 
sentence ranking.  
 
Figure 1. Ranking vs. Clustering 
In order to enhance the performance of 
summarization, recently cluster-based ranking 
approaches were explored in the literature 
(Wan and Yang, 2006; Sun et al 2007; Wang 
et al 2008a,b; Qazvinian and Radev, 2008). 
Normally these approaches applied a clustering 
algorithm to obtain the theme clusters first and 
then ranked the sentences within each cluster 
or by exploring the interaction between 
sentences and obtained clusters (referring to 
Figure 1(b)). In other words, clustering and 
ranking are regarded as two independent 
processes in these approaches although the 
cluster-level information has been incorporated 
into the sentence ranking process. As a result, 
Ranking Ranking 
Clustering 
Ranking 
Clustering
(a)                           (b)                           (c) 
134
the ranking performance is inevitably 
influenced by the clustering result.  
To help alleviate this problem, we argue in 
this paper that the quality of ranking and 
clustering can be both improved when the two 
processes are mutually enhanced (referring to 
Figure 1(c)). Based on it, we propose a 
reinforcement approach that updates ranking 
and clustering interactively and iteratively to 
multi-document summarization. The main 
contributions of the paper are three-fold: (1) 
Three different ranking functions are defined 
in a bi-type document graph constructed from 
the given document set, namely global, within-
cluster and conditional rankings, respectively. 
(2) A reinforcement approach is proposed to 
tightly integrate ranking and clustering of 
sentences by exploring term rank distributions 
over the clusters. (3) Thorough experimental 
studies are conducted to verify the 
effectiveness and robustness of the proposed 
approach. 
The rest of this paper is organized as follows. 
Section 2 reviews related work in cluster-based 
ranking. Section 3 defines ranking functions 
and explains reinforced ranking and clustering 
process and its application in multi-document 
summarization. Section 4 presents experiments 
and evaluations. Section 5 concludes the paper.  
2 Related Work 
Clustering has become an increasingly 
important topic with the explosion of 
information available via the Internet. It is an 
important tool in text mining and knowledge 
discovery. Its ability to automatically group 
similar textual objects together enables one to 
discover hidden similarity and key concepts, as 
well as to summarize a large amount of text 
into a small number of groups (Karypis et al, 
2000).  
To summarize a scientific paper, Qazvinian 
and Radev (2008) presented two sentence 
selection strategies based on the clusters which 
were generated by a hierarchical 
agglomeration algorithm applied in the citation 
summary network. One was called C-RR, 
which started with the largest cluster and 
extracted the first sentence from each cluster in 
the order they appeared until the summary 
length limit was reached. The other was called 
C-LexRank, which was similar to C-RR but 
adopted LexRank to rank the sentences within 
each cluster and chose the most salient one. 
Meanwhile, Wan and Yang (2008) proposed 
two models to incorporate the cluster-level 
information into the process of sentence 
ranking for generic summarization. While the 
Cluster-based Conditional Markov Random 
Walk model (ClusterCMRW) incorporated the 
cluster-level information into the text graph 
and manipulated clusters and sentences equally, 
the Cluster-based HITS model (ClusterHITS) 
treated clusters and sentences as hubs and 
authorities in the HITS algorithm.  
Besides, Wang et al (2008) proposed a 
language model to simultaneously cluster and 
summarize documents. Nonnegative 
factorization was performed on the term-
document matrix using the term-sentence 
matrix as the base so that the document-topic 
and sentence-topic matrices could be 
constructed, from which the document clusters 
and the corresponding summary sentences 
were generated simultaneously. 
3 A Reinforcement Approach to 
Multi-document Summarization 
3.1 Document Bi-type Graph 
First of all, let?s introduce the sentence-term 
bi-type graph model for a set of given 
documents D, based on which the algorithm of 
reinforced ranking and clustering is developed. 
Let >=< WEVG ,, , where V is the set of 
vertices that consists of the sentence set 
},,,{ 21 nsssS ?=  and the term set 
},,{ 21 mtttT ,?= , i.e., TSV ?= , E is the set of 
edges that connect the vertices, i.e., 
},|,{ VvvvvE jiji ?><= . W is the adjacency 
matrix in which the element ijw  represents the 
weight of the edge connecting iv  and jv . 
Formally, W can be decomposed into four 
blocks, i.e., SSW , STW , TSW  and TTW , each 
representing a sub-graph of the textual objects 
indicated by the subscripts. W can be written as 
???
?
???
?=
TTTS
STSS
WW
WW
W ,       
where ),( jiWST  is the number of times the 
term jt  appears in the sentence is . )(i,jWSS  is 
135
the number of common terms in the sentences 
is  and js . TSW  is equal to 
T
STW  as the 
relationships between terms and sentences are 
symmetric. For simplification, in this study we 
assume there is no direct relationships between 
terms, i.e., 0=TTW . In the future, we will 
explore effective ways to integrate term 
semantic relationships into the model.  
3.2 Basic Ranking Functions 
Recall that our ultimate goal is sentence 
ranking. As an indispensable part of the 
approach, the basic ranking functions need to 
be defined first.  
3.2.1 Global Ranking (without Clustering) 
Let )( isr  (i=1, 2, ?, n) and )( jtr  (j=1, 2, ?, 
m) denote the ranking scores of the sentence is  
and the term jt  in the whole document set, 
respectively. Based on the assumptions that 
?Highly ranked terms appear in highly ranked 
sentences, while highly ranked sentences 
contain highly ranked terms. Moreover, a 
sentence is ranked higher if it contains many 
terms that appear in many other highly ranked 
sentences.? 
we define  
)(),()1()(),()(
11
j
n
j
SS
m
j
jSTi srjiWtrjiWsr ??
==
???+??= ?? (1) 
and  
)(),()(
1
i
n
i
TSj srijWtr ?
=
?= .      (2) 
For calculation purpose, )( isr  and )( jtr  are 
normalized by  
?
=
?
n
i
i
i
i
sr
sr
sr
1'
' )(
)(
)(  and 
?
=
?
m
j
j
j
j
tr
tr
tr
1'
' )(
)(
)( . 
Equations (1) and (2) can be rewritten using 
the matrix form, i.e.,  
???
???
?
?
?=
?
???+?
??=
||)(||
)(
)(
||)(||
)(
)1(
||)(||
)(
)(
SrW
SrW
Tr
SrW
SrW
TrW
TrW
Sr
TS
TS
SS
SS
ST
ST ??
. (3) 
We call )(Sr  and )(Tr  the ?global ranking 
functions?, because at this moment sentence 
clustering is not yet involved and all the 
sentences/terms in the whole document set are 
ranked together. 
Theorem: The solution to )(Sr  and )(Tr  
given by Equation (3) is the primary 
eigenvector of SSTSST WWW ??+?? )1( ??  and 
STSSTS WWIW ????? ?1))1(( ?? , respectively. 
Proof: Combine Equations (1) and (2), we get 
||)(||
)(
)1(
||)(||
)(
||)(||
)(
)1(
||
||)(||
)(
||
||)(||
)(
SrW
SrW
SrWW
SrWW
SrW
SrW
SrW
SrW
W
SrW
SrW
W
Sr
SS
SS
TSST
TSST
SS
SS
TS
TS
ST
TS
TS
ST
?
???+??
???=
?
???+
?
??
?
??
?=
??
??)(
 
As the iterative process is a power method, 
it is guaranteed that )(Sr  converges to the 
primary eigenvector of +?? TSST WW?  
SSW?? )1( ? . Similarly,  )(Tr  is guaranteed to 
converge to the primary eigenvector of 
STSSTS WWIW ????? ?1))1(( ?? .                      ? 
3.2.2 Local Ranking (within Clusters) 
Assume now K theme clusters have been 
generated by certain clustering algorithm, 
denoted as },,,{ 21 KCCCC ?=  where kC  (k=1, 
2, ?, K) represents a cluster of highly related 
sentences )( kC CS k ?  which contain the terms 
)( kC CT k ? . The sentences and terms within 
the cluster kC  form a cluster bi-type graph 
with the adjacency matrix 
kCW . Let )( kk CC Sr  
and )(
kk CC Tr  denote the ranking scores of kCS  
and 
kCT  within kC . They are calculated by an 
equation similar to Equation (3) by replacing 
the document level adjacency matrix W  with 
the cluster level adjacency matrix 
kCW . We 
call )(
kk CC Sr  and )( kk CC Tr  the ?within-
cluster ranking functions? with respect to the 
cluster kC . They are the local ranking 
functions, in contrast to )(Sr  and )(Tr  that 
rank all the sentences and terms in the whole 
document set D. We believe that it will benefit 
sentence overall ranking when knowing more 
details about the ranking results at the finer 
granularity of theme clusters, instead of at the 
coarse granularity of the whole document set. 
136
3.2.3 Conditional Ranking (across Clusters) 
To facilitate the discovery of rank distributions 
of terms and sentences over all the theme 
clusters, we further define two ?conditional 
ranking functions? )|( kCSr  and )|( kCTr . 
These rank distributions are necessary for the 
parameter estimation during the reinforcement 
process introduced later. The conditional 
ranking score of the term jt  on the cluster kC , 
i.e., )|( kCTr  is directly derived from kCT , i.e., 
=)|( kj Ctr )( jC tr k  if kj Ct ? , and 0)|( =kj Ctr  
otherwise. It is further normalized as  
? =
=
m
j kj
kj
kj
Ctr
Ctr
Ctr
1
)|(
)|(
)|( .   (4) 
Then the conditional ranking score of the 
sentence is  on the cluster kC  is deduced from 
the terms that are included in is , i.e.,  
? ?
?
= =
=
?
?
=
n
i
m
j kjST
m
j kjST
ki
CtrjiW
CtrjiW
Csr
1 1
1
)|(),(
)|(),(
)|( . (5) 
Equation (5) can be interpreted as that the 
conditional rank of is  on kC  is higher if many 
terms in is  are ranked higher in kC . Now we 
have sentence and term conditional ranks over 
all the theme clusters and are ready to 
introduce the reinforcement process.  
3.3 Reinforcement between Within-
Cluster Ranking and Clustering  
The conditional ranks of the term jt  across the 
K theme clusters can be viewed as a rank 
distribution. Then the rank distribution of the 
sentence is  can be considered as a mixture 
model over K conditional rank distributions of 
the terms contained in the sentence is . And the 
sentence is  can be represented as a K-
dimensional vector in the new measure space, 
in which the vectors can be used to guide the 
sentence clustering update. Next, we will 
explain the mixture model of sentence and use 
EM algorithm (Bilmes, 1997) to get the 
component coefficients of the model. Then, we 
will present the similarity measure between 
sentence and cluster, which is used to adjust 
the clusters that the sentences belong to and in 
turn modify within-cluster ranking for the 
sentences in the updated clusters.  
3.3.1 Sentence Mixture Model  
For each sentence  is , we assume that it 
follows the distribution )|( isTr  to generate the 
relationship between the sentence is  and the 
term set T. This distribution can be considered 
as a mixture model over K component 
distributions, i.e. the term conditional rank 
distributions across K theme clusters. We use 
ki,?  to denote the probability that is  belongs 
to kC , then )|( isTr  can be modeled as: 
?
=
?=
K
k
kki CTrsTr
1
i, )|()|( ?  and ?
=
=
K
k
k
1
i, 1? . (6) 
ki,?  can be explained as )|( ik sCp  and 
calculated by the Bayesian equation 
?? )|()|( kiik CspsCp )( kCp , where )|( ki Csp  
is assumed to be )|( ki Csr  obtained from the 
conditional rank of is  on kC  as introduced 
before and )( kCp  is the prior probability. 
3.3.2 Parameter Estimation 
We use EM algorithm to estimate the 
component coefficients ki,?  along with 
)}({ kCp . A hidden variable zC , },,2,1{ Kz ??  
is used to denote the cluster label that a 
sentence term pair ),( ji ts  are from. In addition, 
we make the independent assumption that the 
probability of is  belonging to kC  and the 
probability of jt  belonging to kC  are 
independent, i.e., ?= )|()|,( kikji CspCtsp  
)|( kj Ctp , where )|,( kji Ctsp is the probability 
of is  and jt  both belonging to kC . Similarly, 
)|( kj Ctp  is assumed to be )|( kj Ctr . 
Let ?  be the parameter matrix, which is a 
Kn?  matrix }{ ,kiKn ?=? ?  ;,,1( ni ?=  
),,1 Kk ?= . The best ?  is estimated from the 
relationships observed in the document bi-type 
graph, i.e., STW  and SSW . The likelihood of 
generating all the relationships under the 
parameter ?  can be calculated as:  
????
= == =
???=
???=?
n
i
n
j
jiW
ji
n
i
m
j
jiW
ji
SSSTSSST
SSST ssptsp
WpWpWWL
1 1
),(
1 1
),(
'
)|,()|,(
)|()|(),|(
 
137
where )|,( ?ji tsp  is the probability that is  
and jt  both belong to the same cluster, given 
the current parameter. As )|,( ?ji ssp  does not 
contain variables from ? , we only need to 
consider maximizing the first part of the 
likelihood in order to get the best estimation of 
? . Let )|( STWL ?  be the first part of 
likelihood.  
Taking into account the hidden variable zC , 
the complete log-likelihood can be written as  
( )
( )
( )??
??
??
= =
= =
= =
???=
???=
?=?
n
i
m
j
zjiZST
n
i
m
j
zzji
n
i
m
j
jiW
zjiZST
CptspjiW
CpCtsp
CtspCWL
jiSTW
ST
1 1
1 1
1 1
),(
)|(),(log),(
)|(),|,(log
)|,,(log),|(log
),( . 
In the E-step, given the initial parameter 0? , 
which is set to Kki
10
, =?  for all i and k, the 
expectation of log-likelihood under the current 
distribution of ZC  is: 
???
???
= = =
= = =
?
?=??=?
+?=??=
?=??
n
i
K
k
m
j
jikzkzST
K
k
n
i
m
j
jikzjikST
ZSTWCf
tsCCpCCpjiW
tsCCptspjiW
CWLEQ
STZ
1 1 1
0
1 1 1
0
),|(
0
),,|())|(log(),(
),,|()),(log(),(
),|((log),( 0
 
The conditional distribution in the above 
equation, i.e., ),,|( 0?= jikz tsCCp , can be 
calculated using the Bayesian rule as follows: 
)()|()|(
)|(),|,(
),,|(
000
00
0
kzkjki
kzkzji
jikz
CCpCtpCsp
CCpCCtsp
tsCCp
=?
?=?=?
?=
. (7) 
In the M-Step, we first get the estimation of 
)( kz CCp =  by maximizing the expectation 
),( 0??Q . By introducing a Lagrange 
multiplier ? , we get the equation below. 
?=?=+??=?
? ?
=
0)]1)((),([
)( 1
0
K
k
kz
kz
CCpQ
CCp
?
??
= =
=+?==
n
i
m
j
jikz
kz
ST tsCCpCCp
jiW
1 1
0 0),,|(
)(
1
),( ?  
Thus, the estimation of )( kz CCp =  given 
previous 0?  is  
??
??
= =
= =
?=
==
n
i
m
j
ST
n
i
m
j
jikzST
kz
jiW
tsCCpjiW
CCp
1 1
1 1
0
),(
),,|(),(
)( . (8) 
Then, the parameters ki,?  can be calculated 
with the Bayesian rule as 
?
=
=
==
K
l
lzli
kzki
ki
CCpCsp
CCpCsp
1
,
)()|(
)()|(? .  (9) 
By setting ?=?0 , the whole process can 
be repeated. The updating rules provided in 
Equations (7)-(9) are applied at each iteration. 
Finally ?  will converge to a local maximum. 
A similar estimation process has been adopted 
in (Sun et al, 2009), which was used to 
estimate the component coefficients for author-
conference networks.  
3.3.3 Similarity Measure 
After we get the estimations of the component 
coefficients ki,?  for is  , is  will be represented 
as a K dimensional vector ,,,( 2,1, ?iiis ??=  
),Ki? . The center of each cluster can thus be 
calculated accordingly, which is the mean of 
is  for all is  in the same cluster, i.e., 
|| k
Cs
i
C
C
s
Center kik
?
?= ,      
where || kC  is the size of kC .  
Then the similarity between each sentence 
and each cluster can be calculated as the cosine 
similarity between them, i.e.,  
??
?
==
==
K
l C
K
l i
K
l Ci
ki
lCenterls
lCenterls
Cssim
k
k
1
2
1
2
1
))())(
)()(
),( . (10) 
Finally, each sentence is re-assigned to a 
cluster that is the most similar to the sentence. 
Based on the updated clusters, within-cluster 
ranking is updated accordingly, which triggers 
the next round of clustering refinement. It is 
expected that the quality of clusters should be 
improved during this iterative update process 
since the similar sentences under new 
attributes will be grouped together, and 
meanwhile the quality of ranking will be 
improved along with the better clusters and 
138
thus offers better attributes for further 
clustering.  
3.4 Ensemble Ranking 
The overall sentence ranking function f is 
defined as the ensemble of all the sentence 
conditional ranking scores on the K clusters.  
?
=
?=
K
k
kiki Csrsf
1
)|()( ? ,  (11) 
where k?  is a coefficient evaluating the 
importance of kC . It can be formulated as the 
normalized cosine similarity between a theme 
cluster and the whole document set for generic 
summarization, or between a theme cluster and 
a given query for query-based summarization. 
]1,0[?k?  and ?
=
=
K
k
k
1
1? . 
Figure 2 below summarizes the whole 
process that determines the overall sentence 
ensemble ranking scores.  
Input: The bi-type document graph >=< WETSG ,,? , 
ranking functions, the cluster number K, 1=? , 
001.0=Tre , 10=IterNum . 
Output: sentence final ensemble ranking vector )(Sf . 
1. 0?t ; 
2. Get the initial partition for S, i.e. tkC , Kk ?,2,1= , 
calculate cluster centers t
kC
Center accordingly.  
3. For (t=1; t<IterNum && Tre>? ; t++) 
4.     Calculate the within-cluster ranking )(
kk CC Tr
, 
)(
kCkC
Sr  and the conditional ranking )|( ki Csr ; 
5.     Get new attribute is  for each sentence is , and 
new attribute t
kCCenter  for each cluster 
t
kC ; 
6.     For each sentence is in S 
7.          For k=1 to K 
8.               Calculate similarity value ),( tki Cssim  
9.          End For 
10.        Assign is to 10
+t
kC , ),(maxarg0
t
kik Cssimk =  
11.   End For 
12.   ||max 1 t
kC
t
kCk
CenterCenter ?= +?  
13.   1+? tt  
14. End For 
15. For each sentence is  in S 
16.        For k=1 to K 
17.             ?
=
?=
K
k
kiki Csrsf
1
)|()( ?  
18.        End For 
19. End For 
Figure 2. The Overall Sentence Ranking Algorithm  
3.5 Summary Generation 
In multi-document summarization, the number 
of documents to be summarized can be very 
large. This makes information redundancy 
appears to be more serious in multi-document 
summarization than in single-document 
summarization. Redundancy control is 
necessary. We apply a simple yet effective 
way to choose summary sentences. Each time, 
we compare the current candidate sentence to 
the sentences already included in the summary. 
Only the sentence that is not too similar to any 
sentence in the summary (i.e., the cosine 
similarity between them is lower than a 
threshold) is selected into the summary. The 
iteration is repeated until the length of the 
sentences in the summary reaches the length 
limitation. In this paper, the threshold is set to 
0.7 as always in our past work. 
4 Experiments and Evaluations 
We conduct the experiments on the DUC 2004 
generic multi-document summarization dataset 
and the DUC 2006 query-based multi-
document summarization dataset. According to 
task definitions, systems are required to 
produce a concise summary for each document 
set (without or with a given query description) 
and the length of summaries is limited to 665 
bytes in DUC 2004 and 250 words in DUC 
2006. 
A well-recognized automatic evaluation 
toolkit ROUGE (Lin and Hovy, 2003) is used 
in evaluation. It measures summary quality by 
counting overlapping units between system-
generated summaries and human-written 
reference summaries. We report two common 
ROUGE scores in this paper, namely ROUGE-
1 and ROUGE-2, which base on Uni-gram 
match and Bi-gram match, respectively. 
Documents and queries are pre-processed by 
segmenting sentences and splitting words. Stop 
words are removed and the remaining words 
are stemmed using Porter stemmer.  
4.1 Evaluation of Performance  
In order to evaluate the performance of 
reinforced clustering and ranking approach, we 
compare it with the other three ranking 
approaches: (1) Global-Rank, which does not 
apply clustering and simply relies on the 
139
sentence global ranking scores to select 
summary sentences; (2) Local-Rank, which 
clusters sentences first and then rank sentences 
within each cluster. A summary is generated in 
the same way as presented in (Qazvinian and 
Radev, 2008). The clusters are ordered by 
decreasing size; (3) Cluster-HITS, which also 
clusters sentences first, but then regards 
clusters as hubs and sentences as authorities in 
the HITS algorithm and uses the obtained 
authority scores to rank and select sentences. 
The classical clustering algorithm K-means is 
used where necessary. For query-based 
summarization, the additional query-relevance 
(i.e. the cosine similarity between sentences 
and query) is involved to re-rank the candidate 
sentences chosen by the ranking approaches 
for generic summarization. 
Note that K-means requires a predefined 
cluster number K. To avoid exhaustive search 
for a proper cluster number for each document 
set, we employ the spectra approach 
introduced in (Li et al, 2007) to predict the 
number of the expected clusters. Based on the 
sentence similarity matrix using the 
normalized 1-norm, for its eigenvalues i?  
(i=1,2, ?, n), the ratio )1(/ 21 ?= + ???? ii   is 
defined. If 05.01 >? +ii ??  and i?  is still close 
to 1, then set K=i+1. Tables 1 and 2 below 
compare the performance of the four 
approaches on DUC 2004 and 2006 according 
to the calculated K.  
DUC 2004 ROUGE-1 ROUGE-2 
Reinforced 0.37082 0.08351 
Cluster-HITS 0.36463 0.07632 
Local-Rank 0.36294 0.07351 
Global-Rank 0.35729 0.06893 
Table 1. Results on the DUC 2004 dataset 
DUC 2006 ROUGE-1 ROUGE-2 
Reinforced 0.39531 0.08957 
Cluster-HITS 0.38315 0.08632 
Local-Rank 0.38104 0.08841 
Global-Rank 0.37478 0.08531 
Table 2. Results on the DUC 2006 dataset 
It is not surprised to find that ?Global-Rank? 
shows the poorest performance, when it 
utilizes the sentence level information only 
whereas the other three approaches all 
integrate the additional cluster level 
information in various ways. In addition, as 
results illustrate, the performance of ?Cluster-
HITS? is better than the performance of 
?Local-Rank?. This can be mainly credited to 
the ability of ?Cluster-HITS? to consider not 
only the cluster-level information, but also the 
sentence-to-cluster relationships, which are 
ignored in ?Local-Rank?. It is happy to see that 
the proposed reinforcement approach, which 
simultaneously updates clustering and ranking 
of sentences, consistently outperforms the 
other three approaches. 
4.2 Analysis of Cluster Quality 
Our original intention to propose the 
reinforcement approach is to hope to generate 
more accurate clusters and ranking results by 
mutually refining within-cluster ranking and 
clustering. In order to check and monitor the 
variation trend of the cluster quality during the 
iterations, we define the following measure 
?
?=
?= ??
?=
K
k
K
kll
ji
CsCs
ki
Cs
sssim
Cssim
quan
ljki
ki
1
,1 ,
)
),(min
),(min
( , (12) 
where ),(min ki
Cs
Cssim
ki?
 denotes the distance 
between the cluster center and the border 
sentence in a cluster that is the farthest away 
from the center. The larger it is, the more 
compact the cluster is. ),(min
,
ji
CsCs
sssim
ljki ??
, on 
the other hand, denotes the distance between 
the most distant pair of sentences, one from 
each cluster. The smaller it is, the more 
separated the two clusters are. The distance is 
measured by cosine similarity. As a whole, the 
larger quan means the better cluster quality. 
Figure 3 below plots the values of quan in each 
iteration on the DUC 2004 and 2006 datasets. 
Note that the algorithm converges in less than 
6 rounds and 5 rounds on the DUC 2004 and 
2006 datasets, respectively. The curves clearly 
show the increasment of quan and thus the 
improved cluster quality. 
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
7
7.5
1 2 3 4 5 6IterNum
Q
ua
n
DUC2004 DUC2006
 
Figure 3. Cluster Quality on DUC 2004 and 2006  
140
While quan directly evaluate the quality of 
the generated clusters, we are also quite 
interested in whether the improved clusters 
quality can further enhance the quality of 
sentence ranking and thus consequently raise 
the performance of summarization. Therefore, 
we evaluate the ROUGEs in each iteration as 
well. Figure 4 below illustrates the changes of 
ROUGE-1 and ROUGE-2 result on the DUC 
2004 and 2006 datasets, respectively. Now, we 
have come to the positive conclusion. 
0.29
0.3
0.31
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.4
1 2 3 4 5 6
IterNum
R
O
U
G
E
-1
DUC2004 DUC2006
0.045
0.05
0.055
0.06
0.065
0.07
0.075
0.08
0.085
0.09
0.095
1 2 3 4 5 6
IterNum
R
O
U
G
E-
2
 
Figure 4. ROUGEs on DUC 2004 and 2006  
4.3 Impact of Cluster Numbers 
In previous experiments, the cluster number is 
predicted through the eigenvalues of 1-norm 
normalized sentence similarity matrix. This 
number is just the estimated number. The 
actual number is hard to predict accurately. To 
further examine how the cluster number 
influences summarization, we conduct the 
following additional experiments by varying 
the cluster number. Given a document set, we 
let S denote the sentence set in the document 
set, and set K in the following way: 
|| SK ?= ? ,   (13) 
where )1,0(??  is a ratio controlling the 
expected cluster number. The larger ?  is, the 
more clusters will be produced. ?  ranges from 
0.1 to 0.9 in the experiments. Due to page 
limitation, we only provide the ROUGE-1 and 
ROUGE-2 results of the proposed approach, 
?Cluster-HITS? and ?Local-Rank? on the DUC 
2004 dataset in Figure 5. The similar curves 
are also observed on the 2006 dataset. 
0.355
0.36
0.365
0.37
0.375
0.38
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E
-1
Cluster-HITS Local Rank Reinforced
?  
0.072
0.075
0.078
0.081
0.084
0.087
0.09
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E
-2
?  
Figure 5. ROUGEs vs.? on DUC 2004 
It is shown that (1) the proposed approach 
outperforms ?Cluster-HITS? and ?Local-
Rank? in almost all the cases no matter how 
the cluster number is set; (2) the performances 
of ?Cluster-HITS? and ?Local-Rank? are more 
sensitive to the cluster number and a large 
number of clusters appears to deteriorate the 
performances of both. This is reasonable. 
Actually when ?  getting close to 1, ?Local-
Rank? approaches to ?Global-Rank?. These 
results demonstrate the robustness of the 
proposed approach. 
5 Conclusion 
In this paper, we present a reinforcement 
approach that tightly integrates ranking and 
clustering together by mutually and 
simultaneously updating each other. 
Experimental results demonstrate the 
effectiveness and the robustness of the 
proposed approach. In the future, we will 
explore how to integrate term semantic 
relationships to further improve the 
performance of summarization. 
Acknowledgement 
The work described in this paper was 
supported by an internal grant from the Hong 
Kong Polytechnic University (G-YG80). 
 
141
References 
J. Bilmes. 1997. A Gentle Tutorial on the em 
Algorithm and Its Application to Parameter 
Wstimation for Gaussian Mixture and Hidden 
Markov Models. Technical Report ICSI-TR-97-
02, University of Berkeley. 
Brin, S., and Page, L. 1998. The Anatomy of a 
Large-scale Hypertextual Web Search Engine. In 
Proceedings of WWW1998.. 
Harabagiu S. and Lacatusu F. 2005. Topic Themes 
for Multi-Document Summarization. In 
Proceedings of SIGIR2005. 
Hardy H., Shimizu N., Strzalkowski T., Ting L., 
Wise G. B., and Zhang X. 2002. Cross-
Document Summarization by Concept 
Classification. In Proceedings of SIGIR2002. 
Jon M. Kleinberg. 1999. Authoritative Sources in a 
Hyperlinked Environment. In Proceedings of the 
9th ACM-SIAM Symposium on Discrete 
Algorithms.  
Karypis, George, Vipin Kumar and Michael 
Steinbach. 2000. A Comparison of Document 
Clustering Techniques. KDD workshop on Text 
Mining. 
Lin, C. Y. and Hovy, E. 2000. The Automated 
Acquisition of Topic Signature for Text 
Summarization. In Proceedings of COLING2000.  
Li W.Y., Ng W.K., Liu Y.  and Ong K.L. 2007. 
Enhancing the Effectiveness of Clustering with 
Spectra Analysis. IEEE Transactions on 
Knowledge and Data Engineering (TKDE). 
19(7): 887-902.  
Li, F., Tang, Y., Huang, M., Zhu, X. 2009. 
Answering Opinion Questions with Random 
Walks on Graphs. In Proceedings of ACL2009. 
Otterbacher J., Erkan G. and Radev D. 2005. Using 
RandomWalks for Question-focused Sentence 
Retrieval. In Proceedings of HLT/EMNLP 2005. 
Qazvinian  V. and Radev D. R. 2008. Scientific 
paper summarization using citation summary 
networks. In Proceedings of COLING2008. 
Sun P., Lee J.H., Kim D.H., and Ahn C.M. 2007. 
Multi-Document Using Weighted Similarity 
Between Topic and Clustering-Based Non-
negative Semantic Feature. APWeb/WAIM 
2007. 
Sun Y., Han J., Zhao P., Yin Z., Cheng H., and Wu 
T. 2009. Rankclus: Integrating Clustering with  
 
Ranking for Heterogenous Information Network 
Analysis. In Proceedings of EDBT 2009. 
Wang D.D., Li T., Zhu S.H., Ding Chris. 2008a 
Multi-Document Summarization via Sentence-
Level Semantic Analysis and Symmetric Matrix 
Factorization. In Proceedings of SIGIR2008. 
Wang D.D., Zhu S.H., Li T., Chi Y., and Gong Y.H. 
2008b. Integrating Clustering and Multi-
Document Summarization to Improve Document 
Understanding. In Proceedings of CIKM 2008. 
Wan X. and Yang J. 2006. Improved Affinity Graph 
based Multi-Document Summarization. In 
Proceedings of HLT-NAACL2006. 
Zha H. 2002. Generic Summarization and Key 
Phrase Extraction using Mutual Reinforcement 
Principle and Sentence Clustering. In 
Proceedings of SIGIR2002. 
142
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 56?65,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid
Wayne Xin Zhao?, Jing Jiang?, Hongfei Yan?, Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University, China
?School of Information Systems, Singapore Management University, Singapore
{zhaoxin,yhf}@net.pku.edu.cn, jingjiang@smu.edu.cn, lxm@pku.edu.cn
Abstract
Discovering and summarizing opinions from
online reviews is an important and challeng-
ing task. A commonly-adopted framework
generates structured review summaries with
aspects and opinions. Recently topic mod-
els have been used to identify meaningful re-
view aspects, but existing topic models do
not identify aspect-specific opinion words. In
this paper, we propose a MaxEnt-LDA hy-
brid model to jointly discover both aspects
and aspect-specific opinion words. We show
that with a relatively small amount of train-
ing data, our model can effectively identify as-
pect and opinion words simultaneously. We
also demonstrate the domain adaptability of
our model.
1 Introduction
With the dramatic growth of opinionated user-
generated content, consumers often turn to online
product reviews to seek advice while companies see
reviews as a valuable source of consumer feedback.
How to automatically understand, extract and sum-
marize the opinions expressed in online reviews has
therefore become an important research topic and
gained much attention in recent years (Pang and Lee,
2008). A wide spectrum of tasks have been studied
under review mining, ranging from coarse-grained
document-level polarity classification (Pang et al,
2002) to fine-grained extraction of opinion expres-
sions and their targets (Wu et al, 2009). In partic-
ular, a general framework of summarizing reviews
of a certain product is to first identify different as-
pects (a.k.a. features) of the given product and then
extract specific opinion expressions for each aspect.
For example, aspects of a restaurant may include
food, staff, ambience and price, and opinion expres-
sions for staff may include friendly, rude, etc. Be-
cause of the practicality of this structured summary
format, it has been adopted in several previous stud-
ies (Hu and Liu, 2004; Popescu and Etzioni, 2005;
Brody and Elhadad, 2010) as well as some commer-
cial systems, e.g. the ?scorecard? feature at Bing
shopping1.
Different approaches have been proposed to iden-
tify aspect words and phrases from reviews. Previ-
ous methods using frequent itemset mining (Hu and
Liu, 2004) or supervised learning (Jin and Ho, 2009;
Jin et al, 2009; Wu et al, 2009) have the limitation
that they do not group semantically related aspect
expressions together. Supervised learning also suf-
fers from its heavy dependence on training data. In
contrast, unsupervised, knowledge-lean topic mod-
eling approach has been shown to be effective in au-
tomatically identifying aspects and their representa-
tive words (Titov and McDonald, 2008; Brody and
Elhadad, 2010). For example, words such as waiter,
waitress, staff and service are grouped into one as-
pect.
We follow this promising direction and extend ex-
isting topic models to jointly identify both aspect
and opinion words, especially aspect-specific opin-
ion words. Current topic models for opinion mining,
which we will review in detail in Section 2, still lack
this ability. But separating aspect and opinion words
can be very useful. Aspect-specific opinion words
can be used to construct a domain-dependent senti-
1http://www.bing.com/shopping
56
ment lexicon and applied to tasks such as sentiment
classification. They can also provide more informa-
tive descriptions of the product or service being re-
viewed. For example, using more specific opinion
words such as cozy and romantic to describe the am-
bience aspect in a review summary is more meaning-
ful than using generic words such as nice and great.
To the best of our knowledge, Brody and Elhadad
(2010) are the first to study aspect-specific opinion
words, but their opinion word detection is performed
outside of topic modeling, and they only consider
adjectives as possible opinion words.
In this paper, we propose a new topic modeling
approach that can automatically separate aspect and
opinion words. A novelty of this model is the inte-
gration of a discriminative maximum entropy (Max-
Ent) component with the standard generative com-
ponent. The MaxEnt component allows us to lever-
age arbitrary features such as POS tags to help sepa-
rate aspect and opinion words. Because the supervi-
sion relies mostly on non-lexical features, although
our model is no longer fully unsupervised, the num-
ber of training sentences needed is relatively small.
Moreover, training data can also come from a differ-
ent domain and yet still remain effective, making our
model highly domain adaptive. Empirical evaluation
on large review data sets shows that our model can
effectively identify both aspects and aspect-specific
opinion words with a small amount of training data.
2 Related Work
Pioneered by the work of Hu and Liu (2004), review
summarization has been an important research topic.
There are usually two major tasks involved, namely,
aspect or feature identification and opinion extrac-
tion. Hu and Liu (2004) applied frequent itemset
mining to identify product features without supervi-
sion, and considered adjectives collocated with fea-
ture words as opinion words. Jin and Ho (2009),
Jin et al (2009) and Wu et al (2009) used super-
vised learning that requires hand-labeled training
sentences to identify both aspects and opinions. A
common limitation of these methods is that they do
not group semantically related aspect expressions to-
gether. Furthermore, supervised learning usually re-
quires a large amount of training data in order to per-
form well and is not easily domain adaptable.
Topic modeling provides an unsupervised and
knowledge-lean approach to opinion mining. Titov
and McDonald (2008) show that global topic models
such as LDA (Blei et al, 2003) may not be suitable
for detecting rateable aspects. They propose multi-
grain topic models for discovering local rateable as-
pects. However, they do not explicitly separate as-
pect and opinion words. Lin and He (2009) propose
a joint topic-sentiment model, but topic words and
sentiment words are still not explicitly separated.
Mei et al (2007) propose to separate topic and sen-
timent words using a positive sentiment model and
a negative sentiment model, but both models cap-
ture general opinion words only. In contrast, we
model aspect-specific opinion words as well as gen-
eral opinion words.
Recently Brody and Elhadad (2010) propose to
detect aspect-specific opinion words in an unsuper-
vised manner. They take a two-step approach by first
detecting aspect words using topic models and then
identifying aspect-specific opinion words using po-
larity propagation. They only consider adjectives as
opinion words, which may potentially miss opinion
words with other POS tags. We try to jointly capture
both aspect and opinion words within topic models,
and we allow non-adjective opinion words.
Another line of related work is about how to in-
corporate useful features into topic models (Zhu and
Xing, 2010; Mimno and McCallum, 2008). Our
MaxEnt-LDA hybrid bears similarity to these recent
models but ours is designed for opinion mining.
3 Model Description
Our model is an extension of LDA (Blei et al, 2003)
but captures both aspect words and opinion words.
To model the aspect words, we use a modified ver-
sion of the multi-grain topic models from (Titov and
McDonald, 2008). Our model is simpler and yet still
produces meaningful aspects. Specifically, we as-
sume that there are T aspects in a given collection of
reviews from the same domain, and each review doc-
ument contains a mixture of aspects. We further as-
sume that each sentence (instead of each word as in
standard LDA) is assigned to a single aspect, which
is often true based on our observation.
To understand how we model the opinion words,
let us first look at two example review sentences
57
from the restaurant domain:
The food was tasty.
The waiter was quite friendly.
We can see that there is a strong association of
tasty with food and similarly of friendly with waiter.
While both tasty and friendly are specific to the
restaurant domain, they are each associated with
only a single aspect, namely food and staff, respec-
tively. Besides these aspect-specific opinion words,
we also see general opinion words such as great
in the sentence ?The food was great!? These gen-
eral opinion words are shared across aspects, as op-
posed to aspect-specific opinion words which are
used most commonly with their corresponding as-
pects. We therefore introduce a general opinion
model and T aspect-specific opinion models to cap-
ture these different opinion words.
3.1 Generative Process
We now describe the generative process of the
model. First, we draw several multinomial word dis-
tributions from a symmetric Dirichlet prior with pa-
rameter ?: a background model ?B, a general aspect
model ?A,g, a general opinion model ?O,g, T as-
pect models {?A,t}Tt=1 and T aspect-specific opin-
ion models {?O,t}Tt=1. All these are multinomial
distributions over the vocabulary, which we assume
has V words. Then for each review document d, we
draw a topic distribution ?d?Dir(?) as in standard
LDA. For each sentence s in document d, we draw
an aspect assignment zd,s?Multi(?d).
Now for each word in sentence s of document d,
we have several choices: The word may describe the
specific aspect (e.g. waiter for the staff aspect), or a
general aspect (e.g. restaurant), or an opinion either
specific to the aspect (e.g. friendly) or generic (e.g.
great), or a commonly used background word (e.g.
know). To distinguish between these choices, we in-
troduce two indicator variable, yd,s,n and ud,s,n, for
the nth word wd,s,n. We draw yd,s,n from a multi-
nomial distribution over {0, 1, 2}, parameterized by
pid,s,n. yd,s,n determines whether wd,s,n is a back-
ground word, aspect word or opinion word. We will
discuss how to set pid,s,n in Section 3.2. We draw
ud,s,n from a Bernoulli distribution over {0, 1} pa-
rameterized by p, which in turn is drawn from a sym-
metric Beta(?). ud,s,n determines whether wd,s,n is
general or aspect-specific. We then draw wd,s,n as
T
?
?
B
?
A,t
?
O,t
?
A,g
?
O,g
D
S
N
d,s
x
d,s,n
pi
d,s,n
y
d,s,n
w
d,s,n
u
d,s,n
z
d,s
?
d
{B,O,A}
? p ?
?
Figure 1: The plate notation of our model.
follows:
wd,s,n ?
?
??????
??????
Multi(?B) if yd,s,n = 0
Multi(?A,zd,s) if yd,s,n = 1, ud,s,n = 0
Multi(?A,g) if yd,s,n = 1, ud,s,n = 1
Multi(?O,zd,s) if yd,s,n = 2, ud,s,n = 0
Multi(?O,g) if yd,s,n = 2, ud,s,n = 1
.
Figure 1 shows our model using the plate notation.
3.2 Setting pi with a Maximum Entropy Model
A simple way to set pid,s,n is to draw it from a
symmetric Dirichlet prior. However, as suggested
in (Mei et al, 2007; Lin and He, 2009), fully un-
supervised topic models are unable to identify opin-
ion words well. An important observation we make
is that aspect words and opinion words usually play
different syntactic roles in a sentence. Aspect words
tend to be nouns while opinion words tend to be ad-
jectives. Their contexts in sentences can also be dif-
ferent. But we do not want to use strict rules to sepa-
rate aspect and opinion words because there are also
exceptions. E.g. verbs such as recommend can also
be opinion words.
In order to use information such as POS tags
to help discriminate between aspect and opinion
words, we propose a novel idea as follows: We set
pid,s,n using a maximum entropy (MaxEnt) model
applied to a feature vector xd,s,n associated with
wd,s,n. xd,s,n can encode any arbitrary features we
think may be discriminative, e.g. previous, current
and next POS tags. Formally, we have
p(yd,s,n = l|xd,s,n) = pid,s,nl =
exp (?l ? xd,s,n
)
?2
l?=0 exp
(?l? ? xd,s,n
) ,
58
where {?l}2l=0 denote the MaxEnt model weights
and can be learned from a set of training sentences
with labeled background, aspect and opinion words.
This MaxEnt-LDA hybrid model is partially in-
spired by (Mimno and McCallum, 2008).
As for the features included in x, currently we
use two types of simple features: (1) lexical features
which include the previous, the current and the next
words {wi?1, wi, wi+1}, and (2) POS tag features
which include the previous, the current and the next
POS tags {POSi?1, POSi, POSi+1}.
3.3 Inference
We use Gibbs sampling to perform model inference.
Due to the space limit, we leave out the derivation
details and only show the sampling formulas. Note
that the MaxEnt component is trained first indepen-
dently of the Gibbs sampling procedure, that is, in
Gibbs sampling, we assume that the ? parameters
are fixed.
We use w to denote all the words we observe in
the collection, x to denote all the feature vectors for
these words, and y, z and u to denote all the hidden
variables. First, given the assignment of all other
hidden variables, to sample a value for zd,s, we use
the following formula:
P (zd,s = t|z?(d,s),y,u,w,x) ?
cd(t) + ?
cd(?) + T?
?
( ?
(
cA,t(?) + V ?
)
?
(
cA,t(?) + nA,t(?) + V ?
) ?
V?
v=1
?
(
cA,t(v) + nA,t(v) + ?
)
?
(
cA,t(v) + ?
)
)
?
( ?
(
cO,t(?) + V ?
)
?
(
cO,t(?) + nO,t(?) + V ?
) ?
V?
v=1
?
(
cO,t(v) + nO,t(v) + ?
)
?
(
cO,t(v) + ?
)
)
.
Here cd(t) is the number of sentences assigned to as-
pect t in document d, and cd(?) is the number of sen-
tences in document d. cA,t(v) is the number of times
word v is assigned as an aspect word to aspect t,
and cO,t(v) is the number of times word v is assigned
as an opinion word to aspect t. cA,t(?) is the total num-
ber of times any word is assigned as an aspect word
to aspect t, and cO,t(?) is the total number of times any
word is assigned as an opinion word to aspect t. All
these counts represented by a c variable exclude sen-
tence s of document d. nA,t(v) is the number of times
word v is assigned as an aspect word to aspect t in
sentence s of document d, and similarly, nO,t(v) is the
number of times word v is assigned as an opinion
word to aspect t in sentence s of document d.
Then, to jointly sample values for yd,s,n and
ud,s,n, we have
P (yd,s,n = 0|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?0 ? xd,s,n)?
l? exp(?l? ? xd,s,n)
?
cB(wd,s,n) + ?
cB(?) + V ?
,
P (yd,s,n = l, ud,s,n = b|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?l ? xd,s,n)?
l? exp(?l? ? xd,s,n)
? g(wd,s,n, zd,s, l, b),
where the function g(v, t, l, b) (1 ? v ? V, 1 ? t ?
T, l ? {1, 2}, b ? {0, 1}) is defined as follows:
g(v, t, l, b) =
?
??????????
??????????
cA,t(v) +?
cA,t(?) +V ?
? c(0)+?c(?)+2? if l = 1, b = 0
cO,t(v) +?
cO,t(?) +V ?
? c(0)+?c(?)+2? if l = 2, b = 0
cA,g(v) +?
cA,g(?) +V ?
? c(1)+?c(?)+2? if l = 1, b = 1
cO,g(v) +?
cO,g(?) +V ?
? c(1)+?c(?)+2? if l = 2, b = 1.
.
Here the various c variables denote various counts
excluding the nth word in sentence s of document d.
Due to space limit, we do not give full explanation
here.
4 Experiment Setup
To evaluate our MaxEnt-LDA hybrid model for
jointly modeling aspect and opinion words, we used
a restaurant review data set previously used in (Ganu
et al, 2009; Brody and Elhadad, 2010) and a ho-
tel review data set previously used in (Baccianella
et al, 2009). We removed stop words and used the
Stanford POS Tagger2 to tag the two data sets. Only
reviews that have no more than 50 sentences were
used. We also kept another version of the data which
includes the stop words for the purpose of extracting
the contextual features included in x. Some details
of the data sets are given in Table 1.
For our hybrid model, we ran 500 iterations of
Gibbs sampling. Following (Griffiths and Steyvers,
2004), we fixed the Dirichlet priors as follows: ? =
2http://nlp.stanford.edu/software/tagger.shtml
59
data set restaurant hotel
#tokens 1,644,923 1,097,739
#docs 52,574 14,443
Table 1: Some statistics of the data sets.
data set #sentences #tokens
restaurant 46 634
cell phone 125 4414
DVD player 180 3024
Table 2: Some statistics of the labeled training data.
50/T , ? = 0.1 and ? = 0.5. We also experimented
with other settings of these priors and did not notice
any major difference. For MaxEnt training, we tried
three labeled data sets: one that was taken from the
restaurant data set and manually annotated by us3,
and two from the annotated data set used in (Wu et
al., 2009). Note that the latter two were used for test-
ing domain adaptation in Section 6.3. Some details
of the training sets are shown in Table 2.
In our preliminary experiments, we also tried two
variations of our MaxEnt-LDA hybrid model. (1)
The first is a fully unsupervised model where we
used a uniform Dirichlet prior for pi. We found
that this unsupervised model could not separate as-
pect and opinion words well. (2) The second is a
bootstrapping version of the MaxEnt-LDA model
where we used the predicted values of y as pseudo
labels and re-trained the MaxEnt model iteratively.
We found that this bootstrapping procedure did not
boost the overall performance much and even hurt
the performance a little in some cases. Due to the
space limit we do not report these experiments here.
5 Evaluation
In this section we report the evaluation of our
model. We refer to our MaxEnt-LDA hybrid model
as ME-LDA. We also implemented a local version
of the standard LDA method where each sentence
is treated as a document. This is the model used
in (Brody and Elhadad, 2010) to identify aspects,
and we refer to this model as LocLDA.
Food Staff Order Taking Ambience
chocolate service wait room
dessert food waiter dining
cake staff wait tables
cream excellent order bar
ice friendly minutes place
desserts attentive seated decor
coffee extremely waitress scene
tea waiters reservation space
bread slow asked area
cheese outstanding told table
Table 4: Sample aspects of the restaurant domain using
LocLDA. Note that the words in bold are opinion words
which are mixed with aspect words.
5.1 Qualitative Evaluation
For each of the two data sets, we show four sample
aspects identified by ME-LDA in Table 3 and Ta-
ble 5. Because the hotel domain is somehow similar
to the restaurant domain, we used the labeled train-
ing data from the restaurant domain also for the hotel
data set. From the tables we can see that generally
aspect words are quite coherent and meaningful, and
opinion words correspond to aspects very well. For
comparison, we also applied LocLDA to the restau-
rant data set and present the aspects in Table 4. We
can see that ME-LDA and LocLDA give similar as-
pect words. The major difference between these two
models is that ME-LDA can sperate aspect words
and opinion words, which can be very useful. ME-
LDA is also able to separate general opinion words
from aspect-specific ones, giving more informative
opinion expressions for each aspect.
5.2 Evaluation of Aspects Identification
We also quantitatively evaluated the quality of the
automatically identified aspects. Ganu et al (2009)
provide a set of annotated sentences from the restau-
rant data set, in which each sentence has been as-
signed one or more labels from a gold standard label
set S = {Staff, Food, Ambience, Price, Anecdote,
Misc}. To evaluate the quality of our aspect iden-
tification, we chose from the gold standard labels
three major aspects, namely Staff, Food and Ambi-
ence. We did not choose the other aspects because
(1) Price is often mixed with other aspects such as
Food, and (2) Anecdote and Misc do not show clear
3We randomly selected 46 sentences for manual annotation.
60
Food Staff Order Taking Ambience General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
chocolate good service friendly table seated room small good
dessert best staff attentive minutes asked dining nice well
cake great food great wait told tables beautiful nice
cream delicious wait nice waiter waited bar romantic great
ice sweet waiter good reservation waiting place cozy better
desserts hot place excellent order long decor great small
coffee amazing waiters helpful time arrived scene open bad
tea fresh restaurant rude hour rude space warm worth
bread tasted waitress extremely manager sat area feel definitely
cheese excellent waitstaff slow people finally table comfortable special
Table 3: Sample aspects and opinion words of the restaurant domain using ME-LDA.
Service Room Condition Ambience Meal General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
staff helpful room shower room quiet breakfast good great
desk friendly bathroom small floor open coffee fresh good
hotel front bed clean hotel small fruit continental nice
english polite air comfortable noise noisy buffet included well
reception courteous tv hot street nice eggs hot excellent
help pleasant conditioning large view top pastries cold best
service asked water nice night lovely cheese nice small
concierge good rooms safe breakfast hear room great lovely
room excellent beds double room overlooking tea delicious better
restaurant rude bath well terrace beautiful cereal adequate fine
Table 5: Sample aspects and opinion words of the hotel domain using ME-LDA.
patterns in either word usage or writing styles, mak-
ing it even hard for humans to identify them. Brody
and Elhadad (2010) also only used these three as-
pects for quantitative evaluation. To avoid ambigu-
ity, we used only the single-labeled sentences for
evaluation. About 83% of the labeled sentences have
a single label, which confirms our observation that a
sentence usually belongs to a single aspect.
We first ran ME-LDA and LocLDA each to get
an inferred aspect set T . Following (Brody and El-
hadad, 2010), we set the number of aspects to 14
in both models. We then manually mapped each in-
ferred aspect to one of the six gold standard aspects,
i.e., we created a mapping function f(t) : T ? S.
For sentence s of document d, we first assign it to an
inferred aspect as follows:
t? = argmax
t?T
Nd,s?
n=1
logP (wd,s,n|t).
We then assign the gold standard aspect f(t?) to this
Aspect Method Precision Recall F-1
Staff LocLDA 0.804 0.585 0.677
ME-LDA 0.779 0.540 0.638
Food LocLDA 0.898 0.648 0.753
ME-LDA 0.874 0.787 0.828
Ambience LocLDA 0.603 0.677 0.638
ME-LDA 0.773 0.558 0.648
Table 6: Results of aspects identification on restaurant.
sentence. We then calculated the F-1 score of the
three aspects: Staff, Food and Ambience. The re-
sults are shown in Table 6. Generally ME-LDA has
given competitive results compared with LocLDA.
For Food and Ambience ME-LDA outperformed Lo-
cLDA, while for Staff ME-LDA is a little worse
than LocLDA. Note that ME-LDA is not designed
to compete with LocLDA for aspect identification.
61
5.3 Evaluation of Opinion Identification
Since the major advantage of ME-LDA is its abil-
ity to separate aspect and opinion words, we further
quantitatively evaluated the quality of the aspect-
specific opinion words identified by ME-LDA.
Brody and Elhadad (2010) has constructed a gold
standard set of aspect-specific opinion words for the
restaurant data set. In this gold standard set, they
manually judged eight out of the 14 automatically
inferred aspects they had: J = {Ambiance, Staff,
Food-Main Dishes, Atmosphere-Physical, Food-
Baked Goods, Food-General, Drinks, Service}.
Each word is assigned a polarity score ranging from
-2.0 to 2.0 in each aspect. We used their gold stan-
dard words whose polarity scores are not equal to
zero. Because their gold standard only includes
adjectives, we also manually added more opinion
words into the gold standard set. To do so, we took
the top 20 opinion words returned by our method
and two baseline methods, pooled them together,
and manually judged them. We use precision at n
(P@n), a commonly used metric in information re-
trieval, for evaluation. Because top words are more
important in opinion models, we set n to 5, 10 and
20. For both ME-LDA and BL-1 below, we again
manually mapped each automatically inferred aspect
to one of the gold standard aspects.
Since LocLDA does not identify aspect-specific
opinion words, we consider the following two base-
line methods that can identify aspect-specific opin-
ion words:
BL-1: In this baseline, we start with all adjectives
as candidate opinion words, and use mutual infor-
mation (MI) to rank these candidates. Specifically,
given an aspect t, we rank the candidate words ac-
cording to the following scoring function:
ScoreBL-1(w, t) =
?
v?Vt
p(w, v) log p(w, v)p(w)p(v) ,
where Vt is the set of the top-100 frequent aspect
words from ?A,t.
BL-2: In this baseline, we first use LocLDA to learn
a topic distribution for each sentence. We then as-
sign a sentence to the aspect with the largest proba-
bility and hence get sentence clusters. We manually
map these clusters to the eight gold standard aspects.
Finally, for each aspect we rank adjectives by their
Method P@5 P@10 P@20
ME-LDA 0.825?,? 0.700? 0.569?
BL-1 0.400 0.450 0.469
BL-2 0.725 0.650 0.563
Table 7: Average P@n of aspect-specific opinion words
on restaurant. * and ? indicate that the improvement hy-
pothesis is accepted at confidence level 0.9 respectively
for BL-1 and BL-2.
frequencies in the aspect and treat these as aspect-
specific opinion words.
The basic results in terms of the average precision
at n over the eight aspects are shown in Table 7. We
can see that ME-LDA outperformed the two base-
lines consistently. Especially, for P@5, ME-LDA
gave more than 100% relative improvement over
BL-1. The absolute value of 0.825 for P@5 also
indicates that top opinion words discovered by our
model are indeed meaningful.
5.4 Evaluation of the Association between
Opinion Words and Aspects
The evaluation in the previous section shows that our
model returns good opinion words for each aspect.
It does not, however, directly judge how aspect-
specific those opinion words are. This is because the
gold standard created by (Brody and Elhadad, 2010)
also includes general opinion words. E.g. friendly
and good may both be judged to be opinion words
for the staff aspect, but the former is more specific
than the latter. We suspect that BL-2 has comparable
performance with ME-LDA for this reason. So we
further evaluated the association between opinion
words and aspects by directly looking at how easy
it is to infer the corresponding aspect by only look-
ing at an aspect-specific opinion word. We selected
four aspects for evaluation: Ambiance, Staff, Food-
Main Dishes and Atmosphere-Physical . We chose
these four aspects because they are quite different
from each other and thus manual judgments on these
four aspects can be more objective. For each aspect,
similar to the pooling strategy in IR, we pooled the
top 20 opinion words identified by BL-1, BL-2 and
ME-LDA. We then asked two human assessors to
assign an association score to each of these words
as follows: If the word is closely associated with an
aspect, a score of 2 is given; if it is marginally as-
62
Metrics Dataset BL-2 ME-LDA
nDCG@5 Restaurant 0.647 0.764
Hotel 0.782 0.820
nDCG@10 Restaurant 0.781 0.897
Hotel 0.722 0.789
Table 8: Average nDCG performance of BL-2 and ME-
LDA. Because only four aspects were used for evaluation,
we did not perform statistical significance test. We found
that in all cases ME-LDA outperformed BL-2 for either
all aspects or three out of four aspects.
sociated with an aspect, a score of 1 is given; other-
wise, 0 is given. We calculated the Kappa statistics
of agreement, and we got a quite high Kappa value
of 0.8375 and 0.7875 respectively for the restaurant
data set and the hotel data set. Then for each word
in an aspect, we took the average of the scores of
the two assessors. We used an nDCG-like metric to
compare the performance of our model and of BL-2.
The metric is defined as follows:
nDCG@k(t,M) =
?k
i=1
Score(Mt,i)
log2(i+1)
iDCG@k(t) ,
where Mt,i is the ith aspect-specific opinion word
inferred by method M for aspect t, Score(Mt,i) is
the association score of this word, and iDCG@k(t)
is the score of the ideal DCG measure at k for as-
pect t, that is, the maximum DCG score assuming
an ideal ranking. We chose k = 5 and k = 10. The
average nDCG over the four aspects are presented
in Table 8. We can see that ME-LDA outperformed
BL-2 quite a lot for the restaurant data set, which
conforms to our hypothesis that ME-LDA generates
aspect-specific opinion words of stronger associa-
tion with aspects. For the hotel data set, ME-LDA
outperformed a little. This may be due to the fact
that we used the restaurant training data for the ho-
tel data set.
6 Further Analysis of MaxEnt
In this section, we perform some further evaluation
and analysis of the MaxEnt component in our model.
6.1 Feature Selection
Previous studies have shown that simple POS fea-
tures and lexical features can be very effective for
discovering aspect words and opinion words (Hu
Methods Average F-1
LocLDA 0.690
ME-LDA + A 0.631
ME-LDA + B 0.695
ME-LDA + C 0.705
Table 9: Comparison of the average F-1 using different
feature sets for aspect identification on restaurant.
and Liu, 2004; Jin et al, 2009; Wu et al, 2009;
Brody and Elhadad, 2010). for POS features, since
we observe that aspect words tend to be nouns while
opinion words tend to be adjectives but sometimes
also verbs or other part-of-speeches, we can expect
that POS features should be quite useful. As for lexi-
cal features, words from a sentiment lexicon can also
be helpful in discovering opinion words.
However, lexical features are more diverse so pre-
sumably we need more training data in order to de-
tect useful lexical features. Lexical features are also
more domain-dependent. On the other hand, we hy-
pothesize that POS features are more effective when
the amount of training data is small and/or the train-
ing data comes from a different domain. We there-
fore compare the following three sets of features:
? A: wi?1, wi, wi+1
? B: POSi?1, POSi, POSi+1
? C: A+ B
We show the comparison of the performance in Ta-
ble 9 using the average F-1 score defined in Sec-
tion 5.2 for aspect identification, and in Table 10 us-
ing the average P@n measure defined in Section 5.3
for opinion identification. We can see that Set B
plays the most important part, which conforms to
our hypothesis that POS features are very important
in opinion mining. In addition, we can see that Set C
performs a bit better than Set B, which indicates that
some lexical features (e.g., general opinion words)
may also be helpful. Note that here the training data
is from the same domain as the test data, and there-
fore lexical features are likely to be useful.
6.2 Examine the Size of Labeled Data
As we have seen, POS features play the major role
in discriminating between aspect and opinion words.
Because there are much fewer POS features than
word features, we expect that we do not need many
63
Methods P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + A 0.150 0.200 0.231
ME-LDA + B 0.775 0.688 0.569
ME-LDA + C 0.825 0.700 0.569
Table 10: Comparison of the average P@n using different
feature sets for opinion identification on restaurant.
Method F-1
LocalLDA 0.690
ME-LDA + 10 0.629
ME-LDA + 20 0.692
ME-LDA + 30 0.691
ME-LDA + 40 0.726
ME-LDA + 46 0.705
Table 11: Average F-1 with differen sizes of training data
on restaurant.
labeled sentences to learn the POS-based patterns.
We now examine the sensitivity of the performance
with respect to the amount of labeled data. We gen-
erated four smaller training data sets with 10, 20, 30
and 40 sentences each from the whole training data
set we have, which consists of 46 labeled sentences.
The results are shown in Table 11 and Table 12. We
can see that generally the performance stays above
BL when the number of training sentences is 20 or
more. This indicates that our model needs only a
relatively small number of high-quality training sen-
tences to achieve good results.
6.3 Domain Adaption
Since we find that the MaxEnt supervision relies
more on POS features than lexical features, we also
hypothesize that if the training sentences come from
a different domain the performance can still remain
relatively high. To test this hypothesis, we tried two
Method P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + 10 0.700 0.563 0.488
ME-LDA + 20 0.875 0.650 0.600
ME-LDA + 30 0.825 0.700 0.569
ME-LDA + 40 0.825 0.688 0.581
ME-LDA + 46 0.825 0.700 0.569
Table 12: Average P@n of aspect-specific opinion words
with differen sizes of training data on restaurant.
Method Average F-1
restaurant + B 0.695
restaurant + C 0.705
cell phone + B 0.662
cell phone + C 0.629
DVD player + B 0.686
DVD player + C 0.635
Table 13: Average F-1 performance for domain adaption
on restaurant.
Method P@5 P@10 P@20
restaurant + B 0.775 0.688 0.569
restaurant + C 0.825 0.700 0.569
cell phone + B 0.775 0.675 0.588
cell phone + C 0.750 0.688 0.594
DVD player + B 0.775 0.713 0.575
DVD player + C 0.825 0.663 0.588
Table 14: Average P@n of aspect-specific opinion words
for domain adaption on restaurant.
quite different training data sets, one from the cell
phone domain and the other from the DVD player
domain, both used in (Wu et al, 2009).
We consider two feature sets defined in Sec-
tion 6.1 for domain adaption, namely B and C. The
results are shown in Table 13 and Table 14.
For aspect identification, using out-of-domain
training data performed worse than using in-domain
training data, but the absolute performance is still
decent. And interestingly, we can see that using B
is better than using C, indicating that lexical features
may hurt the performance in the cross-domain set-
ting. It suggests that lexical features are not easily
adaptable across domains for aspect identification.
For opinion identification, we can see that there
is no clear difference between using out-of-domain
training data and using in-domain training data,
which may indicate that our opinion identification
component is robust in domain adaption. Also, we
cannot easily tell whetherB has advantage over C for
opinion identification. One possible reason may be
that those general opinion words are useful across
domains, so lexical features may still be useful for
domain adaption.
64
7 Conclusions
In this paper, we presented a topic modeling ap-
proach that can jointly identify aspect and opinion
words, using a MaxEnt-LDA hybrid. We showed
that by incorporating a supervised, discriminative
maximum entropy model into an unsupervised, gen-
erative topic model, we could leverage syntactic fea-
tures to help separate aspect and opinion words.
We evaluated our model on two large review data
sets from the restaurant and the hotel domains. We
found that our model was competitive in identifying
meaningful aspects compared with previous mod-
els. Most importantly, our model was able to iden-
tify meaningful opinion words strongly associated
with different aspects. We also demonstrated that
the model could perform well with a relatively small
amount of training data or with training data from a
different domain.
Our model provides a principled way to jointly
model both aspects and opinions. One of the future
directions we plan to explore is to use this model
to help sentence-level extraction of specific opinions
and their targets, which previously was only tackled
in a fully supervised manner. Another direction is to
extend the model to support polarity classification.
ACKNOWLEDGMENT
The authors Xin Zhao, Hongfei Yan and Xiaom-
ing Li are partially supported by NSFC under the
grant No. 70903008 and 60933004, CNGI grant No.
2008-122, 863 Program No. 2009AA01Z143, and
the Open Fund of the State Key Laboratory of Soft-
ware Development Environment under Grant No.
SKLSDE-2010KF-03, Beihang University.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews. In
Proceedings of the 31st ECIR.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3.
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
Proceedings of Human Language Technologies: The
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predictions
using review text content. In Proceedings of the 12th
International Workshop on the Web and Databases.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
HMM-based learning framework for web opinion min-
ing. In Proceedings of the 26th International Confer-
ence on Machine Learning.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
OpinionMiner: A novel machine learning system for
web opinion mining and extraction. In Proceedings of
the 15th ACM SIGKDD.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Proceed-
ing of the Eighteenth ACM Conference on Information
and Knowledge Management.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
Modeling facets and opinions in weblogs. In Proceed-
ings of the 16th International Conference on World
Wide Web.
David Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. In Conference on
Uncertainty in Artificial Intelligence.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of the HLT-EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Jun Zhu and Eric P. Xing. 2010. Conditional topic ran-
dom fields. In Proceedings of the 27th International
Conference on Machine Learning.
65
