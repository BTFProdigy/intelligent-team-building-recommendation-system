Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300?308,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Structural, Transitive and Latent Models for Biographic Fact Extraction
Nikesh Garera and David Yarowsky
Department of Computer Science, Johns Hopkins University
Human Language Technology Center of Excellence
Baltimore MD, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
This paper presents six novel approaches
to biographic fact extraction that model
structural, transitive and latent proper-
ties of biographical data. The ensem-
ble of these proposed models substantially
outperforms standard pattern-based bio-
graphic fact extraction methods and per-
formance is further improved by modeling
inter-attribute correlations and distribu-
tions over functions of attributes, achiev-
ing an average extraction accuracy of 80%
over seven types of biographic attributes.
1 Introduction
Extracting biographic facts such as ?Birthdate?,
?Occupation?, ?Nationality?, etc. is a critical step
for advancing the state of the art in information
processing and retrieval. An important aspect of
web search is to be able to narrow down search
results by distinguishing among people with the
same name leading to multiple efforts focusing
on web person name disambiguation in the liter-
ature (Mann and Yarowsky, 2003; Artiles et al,
2007, Cucerzan, 2007). While biographic facts are
certainly useful for disambiguating person names,
they also allow for automatic extraction of ency-
lopedic knowledge that has been limited to man-
ual efforts such as Britannica, Wikipedia, etc.
Such encyploedic knowledge can advance verti-
cal search engines such as http://www.spock.com
that are focused on people searches where one can
get an enhanced search interface for searching by
various biographic attributes. Biographic facts are
also useful for powerful query mechanisms such
as finding what attributes are common between
two people (Auer and Lehmann, 2007).
Figure 1: Goal: extracting attribute-value bio-
graphic fact pairs from biographic free-text
While there are a large quantity of biographic texts
available online, there are only a few biographic
fact databases available1, and most of them have
been created manually, are incomplete and are
available primarily in English.
This work presents multiple novel approaches
for automatically extracting biographic facts such
as ?Birthdate?, ?Occupation?, ?Nationality?, and
?Religion?, making use of diverse sources of in-
formation present in biographies.
In particular, we have proposed and evaluated the
following 6 distinct original approaches to this
1E.g.: http://www.nndb.com, http://www.biography.com,
Infoboxes in Wikipedia
300
task with large collective empirical gains:
1. An improvement to the Ravichandran and
Hovy (2002) algorithm based on Partially
Untethered Contextual Pattern Models
2. Learning a position-based model using ab-
solute and relative positions and sequential
order of hypotheses that satisfy the domain
model. For example, ?Deathdate? very often
appears after ?Birthdate? in a biography.
3. Using transitive models over attributes via
co-occurring entities. For example, other
people mentioned person?s biography page
tend to have similar attributes such as occu-
pation (See Figure 4).
4. Using latent wide-document-context models
to detect attributes that may not be mentioned
directly in the article (e.g. the words ?song,
hits, album, recorded,..? all collectively indi-
cate the occupation of singer or musician in
the article.
5. Using inter-attribute correlations, for filter-
ing unlikely biographic attribute combina-
tions. For example, a tuple consisting of <
?Nationality? = India, ?Religion? = Hindu >
has a higher probability than a tuple consist-
ing of < ?Nationality? = France, ?Religion?
= Hindu >.
6. Learning distributions over functions of at-
tributes, for example, using an age distri-
bution to filter tuples containing improbable
<deathyear>-<birthyear> lifespan values.
We propose and evaluate techniques for exploiting
all of the above classes of information in the next
sections.
2 Related Work
The literature for biography extraction falls into
two major classes. The first one deals with iden-
tifying and extracting biographical sentences and
treats the problem as a summarization task (Cowie
et al, 2000, Schiffman et al, 2001, Zhou et
al., 2004). The second and more closely related
class deals with extracting specific facts such as
?birthplace?, ?occupation?, etc. For this task,
the primary theme of work in the literature has
been to treat the task as a general semantic-class
learning problem where one starts with a few
seeds of the semantic relationship of interest and
learns contextual patterns such as ?<NAME>
was born in <Birthplace>? or ?<NAME> (born
<Birthdate>)? (Hearst, 1992; Riloff, 1996; The-
len and Riloff, 2002; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002; Mann and
Yarowsky, 2003; Jijkoun et al, 2004; Mann and
Yarowsky, 2005; Alfonseca et al, 2006; Pasca et
al., 2006). There has also been some work on ex-
tracting biographic facts directly from Wikipedia
pages. Culotta et al (2006) deal with learning
contextual patterns for extracting family relation-
ships from Wikipedia. Ruiz-Casado et al (2006)
learn contextual patterns for biographic facts and
apply them to Wikipedia pages.
While the pattern-learning approach extends well
for a few biography classes, some of the bio-
graphic facts like ?Gender? and ?Religion? do not
have consistent contextual patterns, and only a
few of the explicit biographic attributes such as
?Birthdate?, ?Deathdate?, ?Birthplace? and ?Oc-
cupation? have been shown to work well in the
pattern-learning framework (Mann and Yarowsky,
2005; Alfonesca, 2006; Pasca et al, 2006).
Secondly, there is a general lack of work that at-
tempts to utilize the typical information sequenc-
ing within biographic texts for fact extraction, and
we show how the information structure of biogra-
phies can be used to improve upon pattern based
models. Furthermore, we also present additional
novel models of attribute correlation and age dis-
tribution that aid the extraction process.
3 Approach
We first implement the standard pattern-based ap-
proach for extracting biographic facts from the raw
prose in Wikipedia people pages. We then present
an array of novel techniques exploiting different
classes of information including partially-tethered
contextual patterns, relative attribute position and
sequence, transitive attributes of co-occurring en-
tities, broad-context topical profiles, inter-attribute
correlations and likely human age distributions.
For illustrative purposes, we motivate each tech-
nique using one or two attributes but in practice
they can be applied to a wide range of attributes
and empirical results in Table 4 show that they
give consistent performance gains across multiple
attributes.
301
4 Contextual Pattern-Based Model
A standard model for extracting biographic facts
is to learn templatic contextual patterns such as
<NAME> ?was born in? <Birthplace>. Such
templatic patterns can be learned using seed ex-
amples of the attribute in question and, there has
been a plethora of work in the seed-based boot-
strapping literature which addresses this problem
(Ravichandran and Hovy, 2002; Thelen and Riloff,
2002; Mann and Yarowsky, 2005; Alfonseca et al,
2006; Pasca et al, 2006)
Thus for our baseline we implemented a stan-
dard Ravichandran and Hovy (2002) pattern
learning model using 100 seed2 examples from
an online biographic database called NNDB
(http://www.nndb.com) for each of the biographic
attributes: ?Birthdate?, ?Birthplace?, ?Death-
date?, ?Gender?, ?Nationality?, ?Occupation? and
?Religion?. Given the seed pairs, patterns for
each attribute were learned by searching for seed
<Name,Attribute Value> pairs in the Wikipedia
page and extracting the left, middle and right con-
texts as various contextual patterns3.
While the biographic text was obtained from
Wikipedia articles, all of the 7 attribute values
used as seed and test person names could not
be obtained from Wikipedia due to incomplete
and unnormalized (for attribute value format) in-
foboxes. Hence, the values for training/evaluation
were extracted from NNDB which provides a
cleaner set of gold truth, and is similar to an ap-
proach utilizing trained annotators for marking up
and extracting the factual information in a stan-
dard format. For consistency, only the people
names whose articles occur in Wikipedia where
selected as part of seed and test sets.
Given the attribute values of the seed names and
their text articles, the probability of a relationship
r(Attribute Name), given the surrounding context
?A1 p A2 q A3?, where p and q are <NAME>
and <Attrib Val> respectively, is given using the
rote extractor model probability as in (Ravichan-
dran and Hovy, 2002; Mann and Yarowsky 2005):
2The seed examples were chosen randomly, with a bias
against duplicate attribute values to increase training diver-
sity. Both the seed and test names and data will be made
available online to the research community for replication
and extension.
3We implemented a noisy model of coreference resolu-
tion by resolving any gender-correct pronoun used in the
Wikipedia page to the title person name of the article. Gender
is also extracted automatically as a biographic attribute.
P (r(p, q)|A1pA2qA3) =
?
x,y?r
c(A1xA2yA3)
?
x,z
c(A1xA2zA3)
Thus, the probability for each contextual pattern
is based on how often it correctly predicts a re-
lationship in the seed set. And, each extracted
attribute value q using the given pattern can thus
be ranked according to the above probability. We
tested this approach for extracting values for each
of the seven attributes on a test set of 100 held-out
names and report Precision, Pseudo-recall and F-
score for each attribute which are computed in the
standard way as follows, for say Attribute ?Birth-
place (bplace)?:
Precisionbplace =
# people with bplace correctly extracted
# of people with bplace extracted
Pseudo-recbplace =
# people with bplace correctly extracted
# of people with bplace in test set
F-scorebplace =
2?Precisionbplace?Pseudo-recbplace
Precisionbplace + Pseudo-recbplace
Since the true values of each attribute are obtained
from a cleaner and normalized person-database
(NNDB), not all the attribute values maybe present
in the Wikipedia article for a given name. Thus,
we also compute accuracy on the subset of names
for which the value of a given attribute is also ex-
plictly stated in the article. This is denoted as:
Acctruth pres =
# people with bplace correctly extracted
# of people with true bplace stated in article
We further applied a domain model for each at-
tribute to filter noisy targets extracted from lex-
ical patterns. Our domain models of attributes
include lists of acceptable values (such as lists
of places, occupations and religions) and struc-
tural constraints such as possible date formats for
?Birthdate? and ?Deathdate?. The rows with sub-
script ?RH02?in Table 4 shows the performance
of this Ravichandran and Hovy (2002) model with
additional attribute domain modeling for each at-
tribute, and Table 3 shows the average perfor-
mance across all attributes.
5 Partially Untethered Templatic
Contextual Patterns
The pattern-learning literature for fact extraction
often consists of patterns with a ?hook? and
?target? (Mann and Yarowsky, 2005). For ex-
ample, in the pattern ?<Name> was born in
<Birthplace>?, ?<NAME>? is the hook and
?<Birthplace>? is the target. The disadvantage
of this approach is that the intervening dually-
tethered patterns can be quite long and highly
variable, such as ?<NAME> was highly influ-
302
Figure 2: Distribution of the observed document
mentions of Deathdate, Nationality and Religion.
ential in his role as <Occupation>?. We over-
come this problem by modeling partially unteth-
ered variable-length ngram patterns adjacent to
only the target, with the only constraint being
that the hook entity appear somewhere in the sen-
tence4. Examples of these new contextual ngram
features include ?his role as <Occupation>? and
?role as <Occupation>?. The pattern probability
model here is essentially the same as in Ravichan-
dran and Hovy, 2002 and just the pattern repre-
sentation is changed. The rows with subscript
?RH02imp? in tables 4 and 3 show performance
gains using this improved templatic-pattern-based
model, yielding an absolute 21% gain in accuracy.
6 Document-Position-Based Model
One of the properties of biographic genres is that
primary biographic attributes5 tend to appear in
characteristic positions, often toward the begin-
ning of the article. Thus, the absolute position
(in percentage) can be modeled explicitly using a
Gaussian parametric model as follows for choos-
ing the best candidate value v? for a given attribute
A:
v? = argmaxv?domain(A)f(posnv|A)
where,
f(posnv|A)
= N (posnv; ??A, ??2A)
= 1
??A
?
2pi
e?(posnv???A)
2/2??A2
4This constraint is particularly viable in biographic text,
which tends to focus on the properties of a single individual.
5We use the hyperlinked phrases as potential values for all
attributes except ?Gender?. For ?Gender? we used pronouns
as potential values ranked according to the their distance from
the beginning of the page.
In the above equation, posnv is the absolute
position ratio (position/length) and ??A, ??A
2 are
the sample mean and variance based on the sam-
ple of correct position ratios of attribute values
in biographies with attribute A. Figure 2, for
example, shows the positional distribution of the
seed attribute values for deathdate, nationality and
religion in Wikipedia articles, fit to a Gaussian
distribution. Combining this empirically derived
position model with a domain model6 of accept-
able attribute values is effective enough to serve
as a stand-alone model.
Attribute Best rank P(Rank)
in seed set
Birthplace 1 0.61
Birthdate 1 0.98
Deathdate 2 0.58
Gender 1 1.0
Occupation 1 0.70
Nationality 1 0.83
Religion 1 0.80
Table 1: Majority rank of the correct attribute
value in the Wikipedia pages of the seed names
used for learning relative ordering among at-
tributes satisfying the domain model
6.1 Learning Relative Ordering in the
Position-Based Model
In practice, for attributes such as birthdate, the
first text pattern satisfying the domain model is
often the correct answer for biographical articles.
Deathdate also tends to occur near the beginning
of the article, but almost always some point
after the birthdate. This motivates a second,
sequence-based position model based on the rank
of the attribute values among other values in the
domain of the attribute, as follows:
v? = argmaxv?domain(A)P (rankv|A)
where P (rankv|A) is the fraction of biographies
having attribute a with the correct value occuring
at rank rankv, where rank is measured according
to the relative order in which the values belonging
to the attribute domain occur from the beginning
6The domain model is the same as used in Section 4 and
remains constant across all the models developed in this paper
303
of the article. We use the seed set to learn the rel-
ative positions between attributes, that is, in the
Wikipedia pages of seed names what is the rank of
the correct attribute.
Table 1 shows the most frequent rank of the correct
attribute value and Figure 3 shows the distribu-
tion of the correct ranks for a sample of attributes.
We can see that 61% of the time the first loca-
tion mentioned in a biography is the individuals?s
birthplace, while 58% of the time the 2nd date
in the article is the deathdate. Thus, ?Deathdate?
often appears as the second date in a Wikipedia
page as expected. These empirical distributions
for the correct rank provide a direct vehicle for
scoring hypotheses, and the rows with ?rel. posn?
as the subscript in Table 4 shows the improvement
in performance using the learned relative order-
ing. Averaging across different attributes, table
3 shows an absolute 11% average gain in accu-
racy of the position-sequence-based models rela-
tive to the improved Ravichandran and Hovy re-
sults achieved here.
Figure 3: Empirical distribution of the relative po-
sition of the correct (seed) answers among all text
phrases satisfying the domain model for ?birth-
place? and ?death date?.
7 Implicit Models
Some of the biographic attributes such as ?Nation-
ality?, ?Occupation? and ?Religion? can be ex-
tracted successfully even when the answer is not
directly mentioned in the biographic article. We
present two such models for doing so in the fol-
lowing subsections:
7.1 Extracting Attributes Transitively using
Neighboring Person-Names
Attributes such as ?Occupation? are transitive in
nature, that is, the people names appearing close
to the target name will tend to have the same
occupation as the target name. Based on this
intution, we implemented a transitive model that
predicts occupation based on consensus voting via
the extracted occupations of neighboring names7
as follows:
v? = argmaxv?domain(A)P (v|A,Sneighbors)
where,
P (v|A,Sneighbors) =
# neighboring names with attrib value v
# of neighboring names in the article
The set of neighboring names is represented
as Sneighbors and the best candidate value for
an attribute A is chosen based on the the fraction
of neighboring names having the same value
for the respective attribute. We rank candidates
according to this probability and the row labeled
?trans? in Table 4 shows that this model helps in
subsantially improving the recall of ?Occupation?
and ?Religion?, yielding a 7% and 3% average
improvement in F-measure respectively, on top of
the position model described in Section 6.
7.2 Latent Model based on Document-Wide
Context Profiles
In addition to modeling cross-entity attribute
transitively, attributes such as ?Occupation? can
also be modeled successfully using a document-
wide context or topic model. For example, the
distribution of words occuring in a biography
7We only use the neighboring names whose attribute
value can be obtained from an encylopedic database. Fur-
thermore, since we are dealing with biographic pages that
talk about a single person, all other person-names mentioned
in the article whose attributes are present in an encylopedia
were considered for consensus voting
304
Figure 4: Illustration of modeling ?occupation? and ?nationality? transitively via consensus from at-
tributes of neighboring names
of a politician would be different from that of
a scientist. Thus, even if the occupation is not
explicitly mentioned in the article, one can infer
it using a bag-of-words topic profile learned from
the seed examples.
Given a value v, for an attribute A, (for ex-
ample v = ?Politician? and A = ?Occupation?),
we learn a centroid weight vector:
Cv = [w1,v, w2,v, ..., wn,v] where,
wt,v = 1N tft,v ? log
|A|
|t?A|
tft,v is the frequency of word t in the articles of People
having attribute A = v
|A| is the total number of values of attribute A
|t ? A| is the total number of values of attribute A, such that
the articles of people having one of those values contain the
term t
N is the total number of People in the seed set
Given a biography article of a test name and
an attribute in question, we compute a similar
word weight vector C ? = [w?1, w
?
2, ..., w
?
n] for
the test name and measure its cosine similarity
to the centroid vector of each value of the given
attribute. Thus, the best value a? is chosen as:
v? =
argmaxv
w?1?w1,v+w
?
2?w2,v+....+w
?
n?wn,v?
w?21 +w
?2
2 +...+w
?2
n
?
w21,v+w
2
2,v+...+w
2
n,v
Tables 3 and 4 show performance using the la-
tent document-wide-context model. We see that
this model by itself gives the top performance
on ?Occupation?, outperforming the best alterna-
tive model by 9% absolute accuracy, indicating
the usefulness of implicit attribute modeling via
broad-context word frequencies.
This latent model can be further extended us-
ing the multilingual nature of Wikipedia. We
take the corresponding German pages of the train-
ing names and model the German word distribu-
tions characterizing each seed occupation. Table
4 shows that English attribute classification can be
successful using only the words in a parallel Ger-
man article. For some attributes, the performance
of latent model modeled via cross-language (noted
as latentCL) is close to that of English suggesting
potential future work by exploiting this multilin-
gual dimension.
It is interesting to note that both the transitive
model and the latent wide-context model do not
rely on the actual ?Occupation? being explicitly
mentioned in the article, they still outperform ex-
305
Occupation Weight Vector
English
Physicist <magnetic:32.7, electromagnetic:18.2, wire: 18.2, electricity: 17.7, optical:14.5, discovered:11.2>
Singer <song:40, hits:30.5, hit:29.6, reggae:23.6, album:17.1, francis:15.2, music:13.8, recorded:13.6, ...>
Politician <humphrey:367.4, soviet: 97.4, votes: 70.6, senate: 64.7, democratic: 57.2, kennedy: 55.9, ...>
Painter <mural:40.0, diego:14.7, paint:14.5, fresco:10.9. paintings:10.9, museum of modern art:8.83, ...>
Auto racing <renault:76.3, championship:32.7. schumacher:32.7, race:30.4, pole:29.1, driver:28.1 >
German
Physicist <faraday:25.4, chemie:7.3, vorlesungsserie:7.2, 1846:5.8, entdeckt:4.5, rotation:3.6 ...>
Singer <song:16.22, jamaikanischen:11.77, platz:7.3, hit: 6.7, solou?nstler:4.5, album:4.1, widmet:4.0, ...>
Politician <konservativen:26.5, wahlkreis:26.5, romano:21.8, stimmen:18.6, gewa?hlt:18.4, ...>
Painter <rivera:32.7, malerin:7.6, wandgema?lde:7.3, kunst:6.75, 1940:5.8, maler:5.1, auftrag:4.5, ...>
Auto racing <team:29.4,mclaren:18.1,teamkollegen:18.1,sieg:11.7, meisterschaft:10.9, gegner:10.9, ...>
Table 2: Sample of occupation weight vectors in English and German learned using the latent model.
plicit pattern-based and position-based models.
This implicit modeling also helps in improving the
recall of less-often directly mentioned attributes
such as a person?s ?Religion?.
8 Model Combination
While the pattern-based, position-based, transitive
and latent models are all stand-alone models, they
can complement each other in combination as they
provide relatively orthogonal sources of informa-
tion. To combine these models, we perform a sim-
ple backoff-based combination for each attribute
based on stand-alone model performance, and the
rows with subscript ?combined? in Tables 3 and 4
shows an average 14% absolute performance gain
of the combined model relative to the improved
Ravichandran and Hovy 2002 model.
9 Further Extensions: Reducing False
Positives
Since the position-and-domain-based models will
almost always posit an answer, one of the prob-
lems is the high number of false positives yielded
by these algorithms. The following subsections in-
troduce further extensions using interesting prop-
erties of biographic attributes to reduce the effect
of false positives.
9.1 Using Inter-Attribute Correlations
One of the ways to filter false positives is by
filtering empirically incompatible inter-attribute
pairings. The motivation here is that the at-
tributes are not independent of each other when
modeled for the same individual. For example,
P(Religion=Hindu | Nationality=India) is higher
than P(Religion=Hindu | Nationality=France) and
Model Fscore Acc
truth
pres
Ravichandran and Hovy, 2002 0.37 0.43
Improved RH02 Model 0.54 0.64
Position-Based Model 0.53 0.75
Combinedabove 3+trans+latent+cl 0.59 0.78
Combined + Age Dist + Corr 0.62 0.80
(+24%) (+37%)
Table 3: Average Performance of different models
across all biographic attributes
similarly we can find positive and negative cor-
relations among other attribute pairings. For im-
plementation, we consider all possible 3-tuples
of (?Nationality?, ?Birthplace?, ?Religion?)8 and
search on NNDB for the presence of the tuple for
any individual in the database (excluding the test
data of course). As an agressive but effective filter,
we filter the tuples for which no name in NNDB
was found containing the candidate 3-tuples. The
rows with label ?combined+corr? in Table 4 and
Table 3 shows substantial performaance gains us-
ing inter-attribute correlations, such as the 7% ab-
solute average gain for Birthplace over the Section
8 combined models, and a 3% absolute gain for
Nationality and Religion.
9.2 Using Age Distribution
Another way to filter out false positives is to con-
sider distributions on meta-attributes, for example:
while age is not explicitly extracted, we can use
the fact that age is a function of two extracted at-
tributes (<Deathyear>-<Birthyear>) and use the
age distribution to filter out false positives for
8The test of joint-presence between these three attributes
were used since they are strongly correlated
306
Figure 5: Age distribution of famous people on the
web (from www.spock.com)
<Birthdate> and<Deathdate>. Based on the age
distribution for famous people9 on the web shown
in Figure 5, we can bias against unusual candi-
date lifespans and filter out completely those out-
side the range of 25-100, as most of the probabil-
ity mass is concentrated in this range. Rows with
subscript ?comb+ age dist? in Table 4 shows the
performance gains using this feature, yielding an
average 5% absolute accuracy gain for Birthdate.
10 Conclusion
This paper has shown six successful novel ap-
proaches to biographic fact extraction using struc-
tural, transitive and latent properties of biographic
data. We first showed an improvement to the stan-
dard Ravichandran and Hovy (2002) model uti-
lizing untethered contextual pattern models, fol-
lowed by a document position and sequence-based
approach to attribute modeling.
Next we showed transitive models exploiting the
tendency for individuals occurring together in an
article to have related attribute values. We also
showed how latent models of wide document con-
text, both monolingually and translingually, can
capture facts that are not stated directly in a text.
Each of these models provide substantial per-
formance gain, and further performance gain is
achived via classifier combination. We also
showed how inter-attribution correlations can be
9Since all the seed and test examples were used from
nndb.com, we use the age distribution of famous people on
the web: http://blog.spock.com/2008/02/08/age-distribution-
of-people-on-the-web/
Attribute Prec P-Rec Fscore Acc
truth
pres
BirthdateRH02 0.86 0.38 0.53 0.88
BirthdateRH02imp 0.52 0.52 0.52 0.67
Birthdaterel. posn 0.42 0.40 0.41 0.93
Birthdatecombined 0.58 0.58 0.58 0.95
Birthdatecomb+age dist 0.63 0.60 0.61 1.00
DeathdateRH02 0.80 0.19 0.30 0.36
DeathdateRH02imp 0.50 0.49 0.49 0.59
Deathdaterel. posn 0.46 0.44 0.45 0.86
Deathdatecombined 0.49 0.49 0.49 0.86
Deathdatecomb+age dist 0.51 0.49 0.50 0.86
BirthplaceRH02 0.42 0.38 0.40 0.42
BirthplaceRH02imp 0.41 0.41 0.41 0.45
Birthplacerel. posn 0.47 0.41 0.44 0.48
Birthplacecombined 0.44 0.44 0.44 0.48
Birthplacecombined+corr 0.53 0.50 0.51 0.55
OccupationRH02 0.54 0.18 0.27 0.26
OccupationRH02imp 0.38 0.34 0.36 0.48
Occupationrel. posn 0.48 0.35 0.40 0.50
Occupationtrans 0.49 0.46 0.47 0.50
Occupationlatent 0.48 0.48 0.48 0.59
OccupationlatentCL 0.48 0.48 0.48 0.54
Occupationcombined 0.48 0.48 0.48 0.59
NationalityRH02 0.40 0.25 0.31 0.27
NationalityRH02imp 0.75 0.75 0.75 0.81
Nationalityrel. posn 0.73 0.72 0.71 0.78
Nationalitytrans 0.51 0.48 0.49 0.49
Nationalitylatent 0.56 0.56 0.56 0.56
NationalitylatentCL 0.55 0.48 0.51 0.48
Nationalitycombined 0.75 0.75 0.75 0.81
Nationalitycomb+corr 0.77 0.77 0.77 0.84
GenderRH02 0.76 0.76 0.76 0.76
GenderRH02imp 0.99 0.99 0.99 0.99
Genderrel. posn 1.00 1.00 1.00 1.00
Gendertrans 0.79 0.75 0.77 0.75
Genderlatent 0.82 0.82 0.82 0.82
GenderlatentCL 0.83 0.72 0.77 0.72
Gendercombined 1.00 1.00 1.00 1.00
ReligionRH02 0.02 0.02 0.04 0.06
ReligionRH02imp 0.55 0.18 0.27 0.45
Religionrel. posn 0.49 0.24 0.32 0.73
Religiontrans 0.38 0.33 0.35 0.48
Religionlatent 0.36 0.36 0.36 0.45
ReligionlatentCL 0.30 0.26 0.28 0.22
Religioncombined 0.41 0.41 0.41 0.76
Religioncombined+corr 0.44 0.44 0.44 0.79
Table 4: Attribute-wise performance comparison
of all the models across several biographic at-
tributes.
modeled to filter unlikely attribute combinations,
and how models of functions over attributes, such
as deathdate-birthdate distributions, can further
constrain the candidate space. These approaches
collectively achieve 80% average accuracy on a
test set of 7 biographic attribute types, yielding a
37% absolute accuracy gain relative to a standard
algorithm on the same data.
307
References
E. Agichtein and L. Gravano. 2000. Snowball: ex-
tracting relations from large plain-text collections.
Proceedings of ICDL, pages 85?94.
E. Alfonseca, P. Castells, M. Okumura, and M. Ruiz-
Casado. 2006. A rote extractor with edit distance-
based generalisation and multi-corpora precision
calculation. Proceedings of COLING-ACL, pages
9?16.
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
semeval-2007 weps evaluation: Establishing a
benchmark for the web people search task. In Pro-
ceedings of SemEval, pages 64?69.
S. Auer and J. Lehmann. 2007. What have Innsbruck
and Leipzig in common? Extracting Semantics from
Wiki Content. Proceedings of ESWC, pages 503?
517.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. Proceedings of COLING-ACL, pages 79?
85.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. Pro-
ceedings of EACL, pages 3?7.
J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000.
Generating personal profiles. The International
Conference On MT And Multilingual NLP.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. Proceedings of
EMNLP-CoNLL, pages 708?716.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrat-
ing probabilistic extraction models and data mining
to discover relations and patterns in text. Proceed-
ings of HLT-NAACL, pages 296?303.
E. Filatova and J. Prager. 2005. Tell me what you do
and I?ll tell you what you are: Learning occupation-
related activities for biographies. Proceedings of
HLT-EMNLP, pages 113?120.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539?545.
V. Jijkoun, M. de Rijke, and J. Mur. 2004. Infor-
mation extraction for question answering: improv-
ing recall through syntactic patterns. Proceedings of
COLING, page 1284.
G.S. Mann and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings of
CoNLL, pages 33?40.
G.S. Mann and D. Yarowsky. 2005. Multi-field in-
formation extraction and cross-document fusion. In
Proceedings of ACL, pages 483?490.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. Proceedings of HLT-
NAACL companion volume, pages 70?72.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the World Wide
Web of Facts Step one: The One-Million Fact Ex-
traction Challenge. Proceedings of AAAI, pages
1400?1405.
D. Ravichandran and E. Hovy. 2002. Learning sur-
face text patterns for a question answering system.
Proceedings of ACL, pages 41?47.
Y. Ravin and Z. Kazi. 1999. Is Hillary Rodham Clin-
ton the President? Disambiguating Names across
Documents. Proceedings of ACL.
M. Remy. 2002. Wikipedia: The Free Encyclopedia.
Online Information Review Year, 26(6).
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. Proceedings of AAAI,
pages 1044?1049.
M. Ruiz-Casado, E. Alfonseca, and P. Castells.
2005. Automatic extraction of semantic relation-
ships for wordnet by means of pattern learning from
wikipedia. Proceedings of NLDB 2005.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2006.
From Wikipedia to semantic relationships: a semi-
automated annotation approach. Proceedings of
ESWC.
B. Schiffman, I. Mani, and K.J. Concepcion. 2001.
Producing biographical summaries: combining lin-
guistic knowledge with corpus statistics. Proceed-
ings of ACL, pages 458?465.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proceedings of EMNLP,
pages 14?21.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text. Proceedings of
ANLP, pages 202?208.
C. Walker, S. Strassel, J. Medero, and K. Maeda. 2006.
Ace 2005 multilingual training corpus. Linguistic
Data Consortium.
R. Weischedel, J. Xu, and A. Licuanan. 2004. A
Hybrid Approach to Answering Biographical Ques-
tions. New Directions In Question Answering, pages
59?70.
M. Wick, A. Culotta, and A. McCallum. 2006. Learn-
ing field compatibilities to extract database records
from unstructured text. In Proceedings of EMNLP,
pages 603?611.
L. Zhou, M. Ticrea, and E. Hovy. 2004. Multidoc-
ument biography summarization. Proceedings of
EMNLP, pages 434?441.
308
Translating Compounds by Learning Component
Gloss Translation Models via Multiple Languages
Nikesh Garera and David Yarowsky
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
This paper presents an approach to the
translation of compound words without the
need for bilingual training text, by mod-
eling the mapping of literal component
word glosses (e.g. ?iron-path?) into flu-
ent English (e.g. ?railway?) across mul-
tiple languages. Performance is improved
by adding component-sequence and learned-
morphology models along with context sim-
ilarity from monolingual text and optional
combination with traditional bilingual-text-
based translation discovery.
1 Introduction
Compound words such as lighthouse and fireplace
are words that are composed of two or more compo-
nent words and are often a challenge for machine
translation due to their potentially complex com-
pounding behavior and ambiguous interpretations
(Rackow et al, 1992). For many languages, such
words form a significant portion of the lexicon and
the compounding process is further complicated by
diverse morphological processes (Levi, 1978) and
the properties of different compound sequences such
as Noun-Noun, Adj-Adj, Adj-Noun, Verb-Verb, etc.
Compounds also tend to have a high type frequency
but a low token frequency which makes their transla-
tion difficult to learn using corpus-based algorithms
(Tanaka and Baldwin, 2003). Furthermore, most of
the literature on compound translation has been re-
stricted to a few languages dealing with compound-
ing phenomena specific to the language in question.
Compound Splitting English Gloss Translation
Input: Distilled glosses from German-English dictionary
Krankenhaus Kranken-Haus sick-house hospital
Regenschirm Regen-Schirm rain-guard umbrella
Wo?rterBuch Wo?rter-Buch words-book dictionary
Eisenbahn Eisen-Bahn iron-path railroad
Input: Distilled glosses from Swedish-English dictionary
Sjukhus Sjhu-Khus sick-house hospital
Ja?rnva?g Ja?rn-va?g iron-path railway
Ordbok Ord-Bok words-book dictionary
Goal: To translate new Albanian compounds
Hekurudhe? Hekur-Udhe? iron-path ???
Table 1: Example lexical resources used in this task and their
application to translating compound words in new languages.
With these challenges in mind, the primary goal of
this work is to improve the coverage of translation
lexicons for compounds, as illustrated in Table 1
and Figure 1, in multiple new languages. We show
how using cross-language compound evidence ob-
tained from bilingual dictionaries can aid in com-
pound translation. A primary motivating idea for
this work is that the literal component glosses for
compound words (such as ?iron path? for railway)
is often replicated in multiple languages, providing
insight into the fluent translation of a similar literal
gloss in a new (often resource-poor) language.
2 Resources Utilized
The only resource utilized for our compound trans-
lation lexicon algorithm is a collection of bilingual
dictionaries. We used bilingual dictionary collec-
tions for 50 languages that were acquired in elec-
tronic form over the Internet or via optical character
recognition (OCR) on paper dictionaries. Note that
no parallel or even monolingual corpora is required,
their use described later in the paper is optional.
403
3 Related Work
The compound-translation literature typically deals
with these steps: 1) Compound splitting, 2) transla-
tion candidate generation and 3) translation candi-
date scoring. Compound splitting is generally done
using translation lexicon lookup and allowing for
different splitting options based on corpus frequency
(Zhang et al, 2000; Koehn and Knight, 2003).
Translation candidate generation is an important
phase and this is where our work differs signifi-
cantly from the previous literature. Most of the pre-
vious work has been focused on generating com-
positional translation candidates, that is, the trans-
lation candidates of the compound words are lexi-
cally composed of the component word translations.
This has been done by either just concatenating the
translations of component words to form a candi-
date (Grefenstette, 1999; Cao and Li, 2002), or us-
ing syntactic templates such as ?E2 in E1?, ?E1 of
E2? to form translation candidates from the transla-
tion of the component words E2 and E1 (Baldwin
and Tanaka, 2004), or using synsets of the compo-
nent word translations to include synonyms in the
compositional candidates (Navigli et al, 2003).
The above class of work in compositional-candidate
generation fails to translate compounds such as
Krankenhaus (hospital) whose component word
translations are Kranken (sick) and Haus (hospital),
and composing sick and house in any order will not
result in the correct translation (hospital). Another
problem with using fixed syntactic templates is that
they are restricted to the specific patterns occurring
in the target language. We show how one can use
the gloss patterns of compounds in multiple other
languages to hypothesize translation candidates that
are not lexically compositional.
4 Approach
Our approach to compound word translation is illus-
trated in Figure 1.
4.1 Splitting compound words and gloss
generation with translation lexicon lookup
We first split a given source word, such as the Al-
banian compound hekurudhe?, into a set of compo-
nent word partitions, such as hekur (iron) and udhe?
(path). Our initial approach is to consider all possi-
ble partitions based on contiguous component words
found in a small dictionary for the language, as in
Goal: To translate this 
Albanian compound word:
udh?
(English gloss)
Compound splitting
using lexicon lookup 
Using small Albanian
 English dictionary
Italian-English dictionary
   ferrovia      --->  ferro    via
   (railroad)   <--- (iron)  (path)
German-English dictionary
   eisenbahn  --->  eisen     bahn 
  (railroad)   <---  (iron)    (path)
Swedish-English dictionary
   j?rnv?g  --->    j?rn          v?g 
  (railway)    <--- (iron)    (path)
Uighur-English dictionary
  t?m?ryol   --->   t?m?r     yol     
 (railroad)    <--- (iron)     (path)
Lookup words in other 
languages that result in
"iron path" after splitting
Candidate 
translations 
of hekurudh?
Other dictionaries
iron path
hekur
hekurudh?
zog bird
udh? path
hekur iron
vadis water
0.19
0.14
0.05
railroad
railway
rail
Algorithm output
for hekurudh?  
(iron) (path)
Figure 1: Illustration of using cross-language evidence us-
ing bilingual dictionaries of different languages for compound
translation
404
Brown (2002) and Koehn and Knight (2003)1. For a
given split, we generate its English glosses by using
all possible English translations of the component
words given in the dictionary of that language2.
4.2 Using cross-language evidence from
different bilingual dictionaries
For many compound words (especially for borrow-
ings), the compounding process is identical across
several languages and the literal English gloss re-
mains the same across these languages. For ex-
ample, the English word railway is translated as a
compound word in many languages, and the English
gloss of those compounds is often ?iron path? or a
similar literal meaning3. Thus knowing the fluent
English translation of the literal gloss ?iron path?
in some relatively resource-rich language provides a
vehicle for the translation from all other languages
sharing that literal gloss4
4.3 Ranking translation candidates
The confidence in the correctness of a mapping be-
tween a literal gloss (e.g. ?iron path?) and fluent
translation (e.g. ?railroad?) can be based on the
number of distinct languages exhibiting this associa-
tion. Thus we rank the candidate translations gener-
ated via different languages as in Figure 1 as fol-
lows: For a given target compound word, say fc
with a set of English glosses G obtained via mul-
tiple splitting options or multiple component word
translations, the translation probability for a candi-
date translation can be computed as:
p(ec|fc) =
?
g?G
p(ec, g|fc)
=
?
g?G
p(g|fc) ? p(ec|g, fc)
=
?
g?G
p(g|fc) ? p(ec|g)
1In order to avoid inflections as component-words we limit
the component-word length to at least three characters.
2The algorithm is allowed to generate multiple glosses ?iron
way,? ?iron road,? etc. based on multiple translations of the
component words. Multiple glosses only add to the number of
translation candidates generated.
3For the gloss, ?iron path?, we found 10 other languages in
which some compound word has the English gloss after split-
ting and component-word translation
4We do assume an existing small translation lexicon in the
target language for the individual component-words, but these
are often higher frequency words and present either in a basic
dictionary or discoverable through corpus-based techniques.
where, p(g|fc) = p(g1|f1) ? p(g2|f2). f1, f2 are
the individual component-words of compound and
g1, g2 are their translations from the existing dic-
tionary. For human dictionaries, p(g|fc) is uni-
form for all g ? G, while variable probabilities
can also be acquired from bitext or other translation
discovery approaches. Also, p(ec|g) =
freq(g,ec)
freq(g) ,
where freq(g, ec) is the number of times the com-
pound word with English gloss g is translated as
ec in the bilingual dictionaries of other languages
and freq(g) is the total number of times the English
gloss appears in these dictionaries.
5 Evaluation using Exact-match
Translation Accuracy
For evaluation, we assess the performance of the
algorithm on the following 10 languages: Alba-
nian, Arabic, Bulgarian, Czech, Farsi, German,
Hungarian, Russian, Slovak and Swedish. We de-
tail both the average performance for these 10 lan-
guages (Avg10), as well as provide individual per-
formance details on Albanian, Bulgarian, German
and Swedish. For each of the compound trans-
lation models, we report coverage (the # of com-
pound words for which a hypothesis was generated
by the algorithm) and Top1/Top10 accuracy. Top1
and Top 10 accuracy are the fraction of words for
which a correct translation (listed in the evaluation
dictionary) appears in the Top 1 and Top 10 trans-
lation candidates respectively, as ranked by the al-
gorithm. Because evaluation dictionaries are often
missing acceptable translations (e.g. railroad rather
than railway), and any deviation from exact-match is
scored as incorrect, these measures will be a lower
bound on acceptable translation accuracy. Also,
target language models can often select effectively
among such hypothesis lists in context.
6 Comparison of different compound
translation models
6.1 A simple model using literal English gloss
concatenation as the translation
Our baseline model is a simple gloss concatenation
model for generating compositional translation can-
didates on the lines of Grefenstette (1999) and Cao
and Li (2002). We take the translations of the in-
dividual component-words (e.g. for the compound
word hekurudhe?, they would be hekur (iron) and
405
udhe? (path)) and hypothesizes three translation can-
didate variants: ?iron path?, ?iron-path? and ?iron-
path?. A test instance is scored as correct if any
of these translation candidates occur in the transla-
tions of hekurudhe? in the bilingual dictionary. This
baseline performance measures how well simple lit-
eral glosses serve as translation candidates. In cases
such as the German compoundNu?schale (nutshell),
which is a simple concatenation of the individual
components Nu?(nut) and Schale (shell), the literal
gloss is correct. For this baseline, if the component-
words have multiple translations, then each of the
possible English gloss is ranked randomly. While
Grefenstette (1999) and Cao and Li (2002) proposed
re-ranking these candidates using web-data, the po-
tential gains of this ranking are limited, as we see in
Table 2 that even the Found Acc. is very low5, that
is for most of the cases the correct translation does
not appear anywhere in the set of English glosses6
Language Cmpnd wrds Top1 Top10 Found
translated Acc. Acc. Acc.
Albanian 4472 (10.11%) 0.001 0.010 0.020
Bulgarian 9093 (12.50%) 0.001 0.015 0.031
German 15731 (29.11%) 0.004 0.079 0.134
Swedish 18316 (31.57%) 0.005 0.068 0.111
Avg10 14228 (17.84%) 0.002 0.030 0.055
Table 2: Baseline performance using unreordered literal En-
glish glosses as translations. The percentages in parentheses
indicate what fraction of all the words in the test (entire) vocab-
ulary were detected and translated as compounds.
6.2 Using bilingual dictionaries
This section describes the results from the model ex-
plained in Section 4. To recap, this model attempts
to translate every test word such that there is at least
one additional language whose bilingual dictionary
supports an equivalent split and literal English gloss,
and bases its translation hypotheses on the consen-
sus fluent translation(s) corresponding to the literal
glosses in these other languages. The performance
is shown in Table 3. The substantial increase in ac-
curacy over the baseline indicates the usefulness of
5Found Acc. is the fraction of examples for which the cor-
rect translation appears anywhere in the n-best list
6One explanation for this could be that for only a small per-
centage of compound words, their dictionary translations are
formed by concatenating their English glosses. Also, Grefen-
stette (1999) reports much higher accuracies for German on this
model because the 724 German test compounds were chosen in
such a way that their correct translation is a concatenation of the
possible component word translations.
such gloss-to-translation guidance from other lan-
guages. The rest of the sections detail our investi-
gation of improvements to this model.
Language Compound words Top1 Top10
translated Acc. Acc.
Albanian 3085 (6.97%) 0.185 0.332
Bulgarian 6719 (9.24%) 0.247 0.416
German 11103 (20.55%) 0.195 0.362
Swedish 12681 (21.86%) 0.188 0.346
Avg10 9320.9 (11.98%) 0.184 0.326
Table 3: Coverage and accuracy for the standard model us-
ing gloss-to-fluent translation mappings learned from bilingual
dictionaries in other languages (in forward order only).
6.3 Using forward and backward ordering for
English gloss search
In our standard model, the literal English gloss for
a source compound word (for example, iron path)
matches glosses in other language dictionaries only
in the identical order. But given that modifier/head
word order often differs between languages, we
test how searching for both orderings (e.g. ?iron
path? and ?path iron?) can improve performance,
as shown in Table 4. The percentages in parentheses
show relative increase from the performance of the
standard model in Section 6.2. We see a substantial
improvement in both coverage and accuracy.
Language Cmpnd wrds Top1 Top10
translated Acc. Acc.
Albanian 3229(+4.67%) .217(+17.30%) .409(+23.19%)
Bulgarian 6806(+1.29%) .255(+3.24%) .442(+6.25%)
German 11346(+2.19%) .199(+2.05%) .388(+7.18%)
Swedish 12970(+2.28%) .189(+0.53%) .361(+4.34%)
Avg10 9603(+3.03%) .193(+4.89%) .362(+11.04%)
Table 4: Performance for looking up English gloss via both
orderings. The percentages in parentheses are relative improve-
ments from the performance in Table 3
.
6.4 Increasing coverage by automatically
discovering compound morphology
For many languages, the compounding process in-
troduces its own morphology (Figure 2). For ex-
ample, in German, the word Gescha?ftsfu?hrer (man-
ager) consists of the lexemes Gescha?ft (business)
and Fu?hrer (guide) joined by the lexeme -s. For the
purposes of these experiments, we will call such lex-
emes fillers or middle glue characters. Koehn and
Knight (2003) used a fixed set of two known fillers s
and es for handling German compounds. To broaden
the applicability of this work to new languages with-
out linguistic guidance, we show how such fillers
406
Gesch?ft s F?hrer
Paterfamilias
Pater Familia s+ + + +
s as Middle Glue
     in German
s as End Glue
      in Latin
Gesch?ftsf?hrer
(Business) (Guide) (Father) (Family)
(Manager) (Household head)
Figure 2: Illustration of compounding morphology using
middle and end glue characters.
can be estimated directly from corpora in different
languages. In additional to fillers, compound can
also introduce morphology at the suffix or prefix of
compounds, for example, in the Latin language, the
lexeme paterfamilias contains the genitive form fa-
milias of the lexeme familia (family), thus s in this
case is referred to as the ?end glue? character. To
Albanian Bulgarian German Swedish
Top 5 Middle Glue Character(s)
j 0.059 O 0.129 s 0.133 s 0.132
s 0.048 N 0.046 n 0.090 l 0.051
t 0.042 H 0.036 k 0.066 n 0.049
r 0.042 E 0.025 h 0.042 t 0.045
i 0.038 A 0.025 f 0.037 r 0.035
Top 5 End Glue Character(s)
m 0.146 T 0.124 n 0.188 a 0.074
t 0.079 EH 0.092 t 0.167 g 0.073
s 0.059 H 0.063 en 0.130 t 0.059
k 0.048 M 0.049 e 0.069 e 0.057
r 0.037 AM 0.047 d 0.043 d 0.057
Table 5: Top 5 middle glues (fillers) and end glues discovered
for each language along with their probability scores.
augment the splitting step outlined in Section 4.1,
we allow deletion of up to two middle characters
and two end characters. Then, for each glue candi-
date (for example es), we estimate its probability as
the relative frequency of unique hypothesized com-
pound words successfully using that particular glue.
We rank the set of glues by their probability and take
the top 10 middle and end glues for each language.
A sample of glues discovered for some of the lan-
guages are shown in Table 5. The performance for
the morphology step is shown in Table 6. The rela-
tive percentage improvements are with respect to the
previous Section 6.3. We observe significant gain in
coverage as the flexibility of glue process allows dis-
covery of more compounds.
6.5 Re-ranking using context vector projection
We may further improve performance by re-ranking
candidate translations based on the goodness of se-
mantic ?fit? between two words, as measured by
Language Cmpnd wrds Top1 Top10
translated Acc. Acc.
Albanian 3272(+1.33%) .214(-1.38%) .407(-0.49%)
Bulgarian 7211(+5.95%) .258(+1.18%) .443(+0.23%)
German 13372(+17.86%) .200(+0.50%) .391(+0.77%)
Swedish 15094(+16.38%) .190(+0.53%) .363(+0.55%)
Avg10 10273(+6.98%) .194(+0.52%) .363(+0.28%)
Table 6: Performance for increasing coverage by including
compounding morphology. The percentages in parentheses are
relative improvements from the performance in Table 4
.
their context similarity. This can be accomplished as
in Rapp (1999) and Schafer and Yarowsky (2002) by
creating bag-of-words context vectors around both
the source and target language words and then pro-
jecting the source vectors into the (English) target
space via the current small translation dictionary.
Once in the same language space, source words and
their translation hypotheses are compared via co-
sine similarity using their surrounding context vec-
tors. We performed this experiment for German
and Swedish and report average accuracies with and
without this addition in Table 7. For monolingual
corpora, we used the German and Swedish side of
the Europarl corpus (Koehn, 2005) consisting of ap-
proximately 15 million and 21 million words respec-
tively. We were able to project context vectors for
an average of 4224.5 words in the two languages
among all the possible compound words detected in
Section 6.4. The poor Eurpoarl coverage could be
due to the fact that compound words are generally
technical words with low Europarl corpus frequency,
especially in parliamentary proceedings. We believe
that the small performance gains here are due to
these limitations of the monolingual corpora.
Method Top1avg Top10avg
Original ranking 0.196 0.388
Comb. with Context Sim 0.201 0.391
Table 7: Average performance on German and Swedish with
and without using context vector similarity from monolingual
corpora.
6.6 Using phrase-tables if a parallel corpus is
available
All previous results in this paper have been for trans-
lation lexicon discovery without the need for paral-
lel bilingual text (bitext), which is often in limited
supply for lower-resource languages. However, it
is useful to assess how this translation lexicon dis-
407
covery work compares with traditional bitext-based
lexicon induction (and how well the approaches can
be combined). For this purpose, we used phrase ta-
bles learned by the standard statistical MT Toolkit
Moses (Koehn et al, 2007). We tested the phrase-
table accuracy on two languages, one for which we
had a lot of parallel data available (German-English
Europarl corpus with approx. 15 million words) and
one for which we had relatively little parallel data
(Czech-English news-commentary corpus with ap-
prox. 1 million words). This was done to see how
the amount of parallel data available affects the ac-
curacy and coverage of compound translation. Table
8 shows the performance for this experiment. For
German, we see a significant improvement in accu-
racy and for Czech a small improvement in Top1 but
a decline in Top10 accuracy. Note that these ac-
curacies are still quite low as compared to general
performance of phrase tables in an end-to-end MT
system because we are measuring exact-match ac-
curacy on a generally more challenging and often-
lower-frequency lexicon subset. The third row in
Table 8 for each of the languages shows that if one
had a parallel corpus available, its n-best list can be
combined with the n-best list of Bilingual Dictio-
naries algorithm to provide much higher consensus
accuracy gains using weighted voting.
Method # of words Top1 Top10
translated Acc. Acc.
German
BiDict 13372 0.200 0.391
Parallel Corpus SMT 3281 0.423 0.576
Parallel + BiDict 3281 0.452 0.579
Czech
BiDictthresh=1 3455 0.276 0.514
Parallel Corpus SMT 309 0.285 0.404
Parallel + BiDict 309 0.359 0.599
Table 8: Performance of this paper?s BiDict approach com-
pared with and augmented with traditional statistical MT learn-
ing from bitext.
7 Quantifying the Role of Cross-languages
7.1 Coverage/Accuracy Trade off
The number of languages offering a translation hy-
pothesis for a given literal English gloss is a use-
ful parameter for measuring confidence in the algo-
rithm?s selection. The more distinct languages ex-
hibiting a translation for the gloss, the higher like-
lihood that the majority translation will be correct
Coverage/Accuracy Tradeoff
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 200 400 600 800 1000 1200 1400 1600
# of words translated as compounds
E
x
a
c
t
 
m
a
t
c
h
 
a
c
c
u
r
a
c
y
Avg Top 1 Acc.
Avg Top 10 Acc.
>= 8
>= 5
>= 4
>= 3
>= 6
>= 14
>= x: threshold for 
# of languages
Figure 3: Coverage/Accuracy trade off curve by incrementing
the minimum number of languages exhibiting a candidate trans-
lation for the source-word?s literal English gloss. Accuracy here
is the Top1 accuracy averaged over all 10 test languages.
rather than noise. Varying this parameter yields the
coverage/accuracy trade off as shown in Figure 3.
7.2 Varying size of bilingual dictionaries
Figure 4 illustrates how the size of the bilingual
dictionaries used for providing cross-language evi-
dence affects translation performance. In order to
take both coverage and accuracy into account, per-
formance measure used was the F-score which is
a harmonic average of Precision (the accuracy on
the subset of words that could be translated) and
Psuedo-recall (which is the correctly translated frac-
tion out of total words that could be translated using
100% of the dictionary size). We can see in Figure 4
that increasing the percentage of dictionary size7 al-
ways helps without plateauing, suggesting substan-
tial extrapolation potential from large dictionaries.
7.3 Greedy vs Random Selection of Utilized
Languages
A natural question for our compound translation al-
gorithm is how does the choice of additional lan-
guages affect performance. We report two experi-
ments on this question. A simple experiment is to
use bilingual dictionaries of randomly selected lan-
guages and test the performance of K-randomly se-
lected languages8, incrementing K until it is the full
set of 50 languages. The dashed lines in Figures 5
7Each run of choosing a percentage of dictionary size was
averaged over 10 runs
8Each run of randomly selecting K languages was averaged
over 10 runs.
408
00.050.1
0.150.20.25
0.30.35
0 10 20 30 40 50 60 70 80 90 100% of dictionary used
F- score Top 1Top 10
Figure 4: F-measure performance given varying sizes of the
bilingual dictionaries used for cross-language evidence (as a
percentage of words randomly utilized from each dictionary).
00.020.040.060.08
0.10.120.140.160.18
0.2
0 10 20 30 40 50# of languages utilized (K)F-
score (Top 1) K-RandomK-Greedy
Figure 5: Top-1 match F-score performance utilizing K lan-
guages for cross-language evidence, for both a random K lan-
guages and greedy selection of the most effective K languages
(typically the closest or largest dictionaries)
00.050.10.15
0.20.250.30.35
0 10 20 30 40 50# of languages utilized (K)F-s
core (Top 10) K-RandomK-Greedy
Figure 6: The performance relationship detailed in Figure 5
caption for Top-10 match F-score.
and 6 show this trend. The performance is measured
by F-score as in section 7.1, where Pseudo-Recall
here is the fraction of correct candidates out of the
total candidates that could be translated had we used
bilingual dictionaries of all the languages. We can
see that adding random bilingual dictionaries helps
improve the performance in a close to linear fashion.
Furthermore, we observe that certain contributing
languages are much more effective than others (e.g.
Arabic/Farsi vs. Arabic/Czech). We use a greedy
heuristic for ranking an additional cross-language,
that is the number of test words for which the correct
English translation can be provided by the bilingual
dictionary of the respective cross-language. Figures
5 and 6 show that greedy selection of the most ef-
fective K utilized languages using this heuristic sub-
stantially accelerates performance. In fact, beyond
the best 10 languages, performance plateaus and ac-
tually decreases slightly, indicating that increased
noise is outweighing increased coverage.
Albanian Arabic
Russian 0.067 0.116 Farsi 0.051 0.090
+Spanish 0.100 0.169 +Spanish 0.059 0.111
+Bulgarian 0.119 0.201 +French 0.077 0.138
Bulgarian Czech
Russian 0.186 0.294 Slovak 0.177 0.289
+Hungarian 0.190 0.319 +Russian 0.222 0.368
+Swedish 0.203 0.339 +Hungarian 0.235 0.407
Farsi German
Arabic 0.031 0.047 Dutch 0.130 0.228
+Dutch 0.038 0.070 +Swedish 0.191 0.316
+Spanish 0.044 0.079 +Hungarian 0.204 0.355
Hungarian Russian
Swedish 0.073 0.108 Bulgarian 0.185 0.250
+Dutch 0.103 0.158 +Hungarian 0.199 0.292
+German 0.117 0.182 +Swedish 0.216 0.319
Slovak Swedish
Czech 0.145 0.218 German 0.120 0.188
+Russian 0.168 0.280 +Hungarian 0.152 0.264
+Hungarian 0.176 0.300 +Dutch 0.182 0.309
Table 9: Illustrating 3-best cross-languages obtained for each
test language (shown in bold). Each row shows the effect of
adding the respective cross-language to the set of languages in
the rows above it and the corresponding F-scores (Top 1 and
Top 10) achieved.
7.4 Languages found using Greedy selection
Table 9 shows the sets of the most effective three
cross-languages per test language selected using the
greedy heuristic explained in previous section. Un-
surprisingly, related languages tend to help more
than distant languages. For example, Dutch is most
409
effective for the test language German, and Slovak is
most effective for Czech. We can also see interest-
ing symmetries between related languages, for ex-
ample: Farsi is the top language used for test lan-
guage Arabic and vice-versa. Such symmetries can
also be seen for other pairs of related languages such
as (Czech, Slovak) and (Russian, Bulgarian). Thus,
related languages are most helpful and they can be
related in several ways such as etymologically, cul-
turally and physically (such as Hungarian contact
with the Germanic languages). The second point
to note is that languages having large dictionaries
also tend to be especially helpful, even when un-
related. This can be seen by the presence of Hun-
garian in top three cross-languages for most of the
test languages. This is likely because Hungarian was
one of the largest dictionaries and hence can provide
good coverage for obtaining translation candidates
of rarer or technical compounds, which may have
more language universal literal glosses.
8 Conclusion
This paper has shown that successful translation
of compounds can be achieved without the need
for bilingual training text, by modeling the map-
ping of literal component-word glosses (e.g. ?iron-
path?) into fluent English (e.g. ?railway?) across
multiple languages. An interesting property of us-
ing such cross-language evidence is that one does
need to restrict the candidate translations to compo-
sitional (or ?glossy?) translations, as our model al-
lows the successful generation of more fluent non-
compositional translations. We further show im-
proved performance by adding component-sequence
and learned-morphology models along with context
similarity from monolingual text and optional com-
bination with traditional bilingual-text-based trans-
lation discovery. These models show consistent per-
formance gains across 10 diverse test languages.
9 Acknowledgments
We thank Chris Callison-Burch for providing access
to phrase tables and giving valuable comments on
this work as well as suggesting useful additional ex-
periments. We also thankMarkus Dreyer for helping
with German examples and David Smith for giving
valuable comments on initial version of the paper.
References
T. Baldwin and T. Tanaka. 2004. Translation by Machine
of Complex Nominals: Getting it Right. Proceedings
of the ACL-2004 Workshop on Multiword Expressions,
pages 24?31.
R.D. Brown. 2002. Corpus-driven splitting of compound
words. Proceedings of TMI-2002.
Y. Cao and H. Li. 2002. Base Noun Phrase translation
using web data and the EM algorithm. Proceedings of
COLING-Volume 1, pages 1?7.
G. Grefenstette. 1999. The World Wide Web as a Re-
source for Example-Based Machine Translation Tasks.
In ASLIB?99 Translating and the Computer 21.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. Proceedings of the EACL-Volume
1, pages 187?193.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
of ACL, companion volume, pages 177?180.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. MT Summit X.
J.N. Levi. 1978. The Syntax and Semantics of Complex
Nominals.
R. Navigli, P. Velardi, and A. Gangemi. 2003. Ontology
learning and its application to automated terminology
translation. Intelligent Systems, IEEE, 18(1):22?31.
U. Rackow, I. Dagan, and U. Schwall. 1992. Auto-
matic translation of noun compounds. Proceedings of
COLING-Volume 4, pages 1249?1253.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
Proceedings of ACL, pages 519?526.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. Proceedings of COLING, pages 1?7.
C. Schafer and D. Yarowsky. 2004. Exploiting aggregate
properties of bilingual dictionaries for distinguishing
senses of English words and inducing English sense
clusters. Proceedings of ACL-2004, pages 118?121.
T. Tanaka and T. Baldwin. 2003. Noun-Noun Compound
Machine Translation: A Feasibility Study on Shallow
Processing. Proceedings of the ACL-2003 Workshop
on Multiword Expressions, pages 17?24.
J. Zhang, J. Gao, and M. Zhou. 2000. Extraction of
Chinese compound words: an experimental study on
a very large corpus. Proceedings of the Second Work-
shop on Chinese Language Processing, pages 132?
139.
410
Minimally Supervised Multilingual Taxonomy and
Translation Lexicon Induction
Nikesh Garera and David Yarowsky
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
We present a novel algorithm for the acqui-
sition of multilingual lexical taxonomies (in-
cluding hyponymy/hypernymy, meronymy
and taxonomic cousinhood), from monolin-
gual corpora with minimal supervision in the
form of seed exemplars using discriminative
learning across the major WordNet seman-
tic relationships. This capability is also ex-
tended robustly and effectively to a second
language (Hindi) via cross-language projec-
tion of the various seed exemplars. We also
present a novel model of translation dic-
tionary induction via multilingual transitive
models of hypernymy and hyponymy, us-
ing these induced taxonomies. Candidate
lexical translation probabilities are based on
the probability that their induced hyponyms
and/or hypernyms are translations of one an-
other. We evaluate all of the above models
on English and Hindi.
1 Introduction
Taxonomy resources such as WordNet are limited
or non-existent for most of the world?s languages.
Building a WordNet manually from scratch requires
a huge amount of human effort and for rare lan-
guages the required human and linguistic resources
may simply not be available. Most of the automatic
approaches for extracting semantic relations (such as
hyponyms) have been demonstrated for English and
some of them rely on various language-specific re-
sources (such as supervised training data, language-
specific lexicosyntactic patterns, shallow parsers,
(grenade)
haathagolaa (explosive)
baaruuda
(bomb)
bama
(gun)
banduuka
explosivegrenade bomb gun
weapon
Induced Hindi Hypernymy (with glosses)
Induced English Hypernymy
hathiyaara
(weapon)
Figure 1: Goal: To induce multilingual taxonomy relation-
ships in parallel in multiple languages (such as Hindi and En-
glish) for information extraction and machine translation pur-
poses.
etc.). This paper presents a language independent
approach for inducing taxonomies such as shown
in Figure 1 using limited supervision and linguis-
tic resources. We propose a seed learning based ap-
proach for extracting semantic relations (hyponyms,
meronyms and cousins) that improves upon existing
induction frameworks by combining evidence from
multiple semantic relation types. We show that us-
ing a joint model for extracting different semantic
relations helps to induce more relation-specific pat-
terns and filter out the generic patterns1 . The pat-
1By generic patterns, we mean patterns that cannot distin-
guish between different semantic relations. For example, the
465
terns can then be used for extracting new wordpairs
expressing the relation. Note that the only training
data used in the algorithm are the few seed pairs re-
quired to start the bootstrapping process, which are
relatively easy to obtain. We evaluate the taxonomy
induction algorithm on English and a second lan-
guage (Hindi) and show that it can reliably and accu-
rately induce taxonomies in two diverse languages.
We further show how having induced parallel tax-
onomies in two languages can be used for augment-
ing a translation dictionary between those two lan-
guages. We make use of the automatically induced
hyponym/hypernym relations in each language to
create a transitive ?bridge? for dictionary induction.
Specifically, the dictionary induction task relies on
the key observation that words in two languages (e.g.
English and Hindi) have increased probabilities of
being translations of each other if their hypernyms
or hyponyms are translations of one another.
2 Related Work
While manually created WordNets for English (Fell-
baum, 1998) and Hindi (Narayan, 2002) have been
made available, a lot of time and effort is required
in building such semantic taxonomies from scratch.
Hence several automatic corpus based approaches
for acquiring lexical knowledge have been proposed
in the literature. Much of this work has been done
for English based on using a few evocative fixed
patterns including ?X and other Ys?, ?Y such as
X?, as in the classic work by Hearst (1992). The
problems with using a few fixed patterns is the of-
ten low coverage of such patterns; thus there is a
need for discovering additional informative patterns
automatically. There has been a plethora of work
in the area of information extraction using automat-
ically derived patterns contextual patterns for se-
mantic categories (e.g. companies, locations, time,
person-names, etc.) based on bootstrapping from
a small set of seed words (Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Thelen and Riloff,
2002; Ravichandran and Hovy, 2002; Hasegawa et
al. 2004; Etzioni et al 2005; Pas?ca et al 2006).
This framework has been also shown to work for ex-
tracting semantic relations between entities: Pantel
et al (2004) proposed an approach based on edit-
pattern ?X and Y? is a generic pattern whereas the pattern ?Y
such as X? is a hyponym-specific pattern
distance to learn lexico-POS patterns for is-a and
part-of relations. Girju et al (2003) used 100 seed
words from WordNet to extract patterns for part-of
relations. While most of the above pattern induction
work has been shown to work well for specific rela-
tions (such as ?birthdates, companies, etc.?), Section
3.1 explains why directly applying seed learning for
semantic relations can result in high recall but low
precision patterns, a problem also noted by Pantel
and Pennacchiotti (2006). Furthermore, much of
the semantic relation extraction work has focused
on extracting a particular relation independently of
other relations. We show how this problem can be
solved by combining evidence from multiple rela-
tions in Section 3.2. Snow et al(2006) also de-
scribe a probablistic framework for combining ev-
idence using constraints from hyponymy and cousin
relations. However, they use a supervised logistic
regression model. Moreover, their features rely on
parsing dependency trees which may not be avail-
able for most languages.
The key contribution of this work is using evidence
from multiple relationship types in the seed learning
framework for inducing these relationships and con-
ducting a multilingual evaluation for the same. We
further show how extraction of semantic relations in
multiple languages can be applied to the task of im-
proving a dictionary between those languages.
3 Approach
To be able to automatically create taxonomies such
as WordNet, it is useful to be able to learn not only
hyponymy/hyponymy directly, but also the addi-
tional semantic relationships of meronymy and tax-
onomic cousinhood. Specifically, given a pair of
words (X, Y), the task is to answer the following
questions: 1. Is X a hyponym of Y (e.g. weapon,
gun)? 2. Is X a part/member of Y (e.g. trigger, gun)?
3. Is X a cousin/sibling2 of Y (e.g. gun, missile)? 4.
Do none of the above 3 relations apply but X is ob-
served in the context of Y (e.g. airplane,accident)?3
We will refer to class 4 as ?other?.
2Cousins/siblings are words that share a close common hy-
pernym
3Note that this does not imply X is unrelated or indepen-
dent of Y. On the contrary, the required sentential co-occurence
implies a topic similarity. Thus, this is a much harder class to
distinguish from classes 1-3 than non co-occuring unrelatedness
(such as gun, protazoa) and hence was included in the evalua-
tion.
466
Rank English Hindi
1 Y, the X Y aura X
(Gloss: Y and X)
2 Y and X Y va X
(Gloss: Y in addition to X)
3 X and other Y Y ne X
(Gloss: Y (case marker) X)
4 X and Y X ke Y
(Gloss: X?s Y)
5 Y, X Y me.n X
(Gloss: Y in X)
Table 1: Naive pattern scoring: Hyponymy patterns ranked by
their raw corpus frequency scores.
3.1 Independently Bootstrapping Lexical
Relationship Models
Following the pattern induction framework of
Ravichandran and Hovy (2002), one of the ways
of extracting different semantic relations is to learn
patterns for each relation independently using seeds
of that relation and extract new pairs using the
learned patterns. For example, to build an inde-
pendent model of hyponymy using this framework,
we collected approximately 50 seed exemplars of
hyponym pairs and extracted all the patterns that
match with the seed pairs4. As in Ravichandran
and Hovy (2002), the patterns were ranked by cor-
pus frequency and a frequency threshold was set to
select the final patterns. These patterns were then
used to extract new word pairs expressing the hy-
ponymy relation by finding word pairs that occur
with these patterns in an unlabeled corpus. How-
ever, the problem with this approach is that generic
patterns (like ?X and Y?) occur many times in a
corpus and thus low-precision patterns may end up
with high cumulative scores. This problem is illus-
trated more clearly in Table 1, which shows a list
of top five hyponymy patterns (ranked by their cor-
pus frequency) using this approach. We overcome
this problem by exploiting the multi-class nature of
our task and combine evidence from multiple rela-
tions in order to learn high precision patterns (with
high conditional probabilities) for each relation. The
key idea is to weed out the patterns that occur in
4A pattern is the ngrams occurring between the seedpair
(also called gluetext). The length of the pattern was thresholded
to 15 words.
Rank English Hindi
1 Y like X X aura anya Y
(Gloss: X and other Y)
2 Y such as X Y, X
(Gloss: Y, X)
3 X and other Y X jaise Y
(Gloss: X like Y)
4 Y and X Y tathaa X
(Gloss: Y or X)
5 Y, including X X va anya Y
(Gloss: X and other Y)
Table 2: Patterns for hypernymy class reranked using ev-
idence from other classes. Patterns distributed fairly evenly
across multiple relationship types (e.g. ?X and Y?) are dep-
recated more than patterns focused predominantly on a single
relationship type (e.g. ?Y such as X?).
more than one semantic relation and keep the ones
that are relation-specific5 , thus using the relations
meronymy, cousins and other as negative evidence
for hyponymy and vice versa. Table 2 shows the pat-
tern ranking by using the model developed in Sec-
tion 3.2 that makes use of evidence from different
classes. We can see more hyponymy specific pat-
terns ranked at the top6 suggesting the usefulness of
this method in finding class-specific patterns.
3.2 A minimally supervised multi-class
classifier for identifying different semantic
relations
First, we extract a list of patterns from an unla-
beled corpus7 independently for each relationship
type (class) using the seeds8 for the respective class
as in Section 3.1.9 In order to develop a multi-
5In the actual algorithm, we will not be entirely weeding
out the common patterns but will estimate the conditional class
probabilities for each pattern: p(class|pattern)
6It is interesting to see in Table 2 that the top learned Hindi
hyponymy patterns seem to be translations of the English pat-
terns suggested by Hearst (1992). This leads to an interesting
future work question: Are the most effective hyponym patterns
in other languages usually translations of the English hyponym
patterns proposed by Hearst (1992) and what are frequent ex-
ceptions?
7Unlabeled monolingual corpora were used for this task, the
English corpus was the LDC Gigaword corpus and the Hindi
corpus was newswire text extracted from the web containing a
total of 64 million words.
8The number of seeds used for classes {hyponym,
meronym, cousin, other} were {48,40,49,50} for English and
were {32,58,31,35} for Hindi respectively. A sample of seeds
used is shown in Table 5.
9We retained only the patterns that had seed frequency
greater one for extracting new word pairs. The total number
467
Hypo. Mero. Cous. Other
X of the Y 0 0.66 0.04 0.3
Y, especially X 1 0 0 0
Y, whose X 0 1 0 0
X and other Y 0.63 0.08 0.18 0.11
X and Y 0.23 0.3 0.33 0.14
Table 3: A sample of patterns and their relationship type
probabilities P (class|pattern) extracted at the end of training
phase for English.
Hypo. Mero. Cous. Other
X aura anya Y 1 0 0 0
(X and other Y)
X aura Y 0.09 0.09 0.71 0.11
(X and Y)
X jaise Y 1 0 0 0
(X like Y)
X va Y 0.11 0 0.89 0
(X and Y)
Y kii X 0.33 0.67 0 0
(Y?s X)
Table 4: A sample of patterns and their class probabilities
P (class|pattern) extracted at the end of training phase for
Hindi.
class probabilistic model, we obtain the probability
of each class c given the pattern p as follows:
P (c|p) = seedfreq(p,c)?
c? seedfreq(p,c
?
)
where seedfreq(p, c) is the number of seeds of class
c that were found with the pattern p in an unlabeled
corpus. A sample of the P (class|pattern) tables
for English and Hindi are shown in the Tables 3 and
4 respectively. It is clear how occurrence of a pattern
in multiple classes can be used for finding reliable
patterns for a particular class. For example, in Table
3: although the pattern ?X and Y? will get a higher
seed frequency than the pattern ?Y, especially X?,
the probability P (?X and Y ??|hyponymy) is much
lower than P (?Y, especially X ??|hyponymy),
since the pattern ?Y, especially X? is unlikely to oc-
cur with seeds of other relations.
Now, instead of using the seedfreq(p, c) as the
score for a particular pattern with respect to a
class, we can rescore patterns using the probabilities
P (class|pattern). Thus the final score for a pattern
of retained patterns across all classes for {English,Hindi} were
{455,117} respectively.
p with respect to class c is obtained as:
score(p, c) = seedfreq(p, c) ? P (c|p)
We can view this equation as balancing recall and
precision, where the first term is the frequency of
the pattern with respect to seeds of class c (repre-
senting recall), and the second term represents the
relation-specificness of the pattern with respect to
class c (representing precision). We recomputed the
score for each pattern in the above manner and ob-
tain a ranked list of patterns for each of the classes
for English and Hindi. Now, to extract new pairs
for each class, we take all the patterns with a seed
frequency greater than 2 and use them to extract
word pairs from an unlabeled corpus. The semantic
class for each extracted pair is then predicted using
the multi-class classifier as follows: Given a pair of
words (X1, X2), note all the patterns that matched
with this pair in the unlabeled corpus, denote this set
as P. Choose the predicted class c? for this pair as:
c? = argmaxc
?
p?P score(p, c)
3.3 Evaluation of the Classification Task
Over 10,000 new word relationship pairs were ex-
tracted based on the above algorithm. While it is
hard to evaluate all the extracted pairs manually, one
can certainly create a representative smaller test set
and evaluate performance on that set. The test set
was created by randomly identifying word pairs in
WordNet and newswire corpora and annotating their
correct semantic class relationships. Test set con-
struction was done entirely independently from the
algorithm application, and hence some of the test
pairs were missed entirely by the learning algorithm,
yielding only partial coverage.
The total number of test examples including all
classes were 200 and 140 for English and Hindi test-
sets respectively. The overall coverage10 on these
test-sets was 81% and 79% for English and Hindi
respectively. Table 6 reports the overall accuracy11
for the 4-way classification using different patterns
scoring methods. Baseline 1 is scoring patterns by
their corpus frequency as in Ravichandran and Hovy
(2002), Baseline 2 is another intutive method of
10Coverage is defined as the percentage of the test cases that
were present in the unlabeled corpus, that is, cases for which an
answer was given.
11Accuracy on a particular set of pairs is defined as the per-
centage of pairs in that set whose class was correctly predicted.
468
English Hindi
Seed Pairs Model Predictions Seed Pairs Model Predictions
tool,hammer gun,weapon khela,Tenisa kaa.ngresa,paarTii
(game,tennis) (congress,party)
Hypernym currency,yen hockey,sport appraadha,hatyaa passporTa,kaagajaata
(crime,murder) (passport,document)
metal,copper cancer,disease jaanvara,bhaaga a.ngrejii,bhaashhaa
(animal,tiger) (English,language)
wheel,truck room,hotel u.ngalii,haatha jeba,sharTa
(finger,hand) (pocket,shirt)
Meronym headline,newspaper bark,tree kamaraa,aspataala kaptaana,Tiima
(room,hospital) (captain,team)
wing,bird lens,camera ma.njila,imaarata darvaaja,makaana
(floor,building) (door,house)
dollar,euro guitar,drum bhaajapa,kaa.ngresa peTrola,Diijala
(bjp,congress) (petrol,diesel)
Cousin heroin,cocaine history, geography Hindii,a.ngrejii Daalara,rupayaa
(Hindi,English) (dollar,rupee)
helicopter,submarine diabetes,arthritis basa,Traka talaaba,nadii
(bus,truck) (pond,river)
Table 5: A sample of seeds used and model predictions for each class for the taxonomy induction task. For each of the model
predictions shown above, its Hyponym/Meronym/Cousin classification was correctly assigned by the model.
scoring patterns by the number of seeds they ex-
tract. The third row in Table 6 indicates the result
of rescoring patterns by their class conditional prob-
abilties, giving the best accuracy.
While this method yields some improvement over
other baselines, the main point to note here is that
the pattern-based methods which have been shown
to work well for English also perform reasonably
well on Hindi, inspite of the fact that the size of the
unlabeled corpus available for Hindi was 15 times
smaller than for English.
Table 7 shows detailed accuracy results for each re-
lationship type using the model developed in sec-
tion 3.2. It is also interesting to see in Table 8 that
most of the confusion is due to ?other? class being
classified as ?cousin? which is expected as cousin
words are only weakly semantically related and uses
more generic patterns such as ?X and Y? which can
often be associated with the ?other? class as well.
Strongly semantically clear classes like Hypernymy
and Meronymy seem to be well discriminated as
their induced patterns are less likely to occur in other
relationship types.
Model English Hindi
Accuracy Accuracy
Baseline 1
[RH02] 65% 63%
Baseline 2 seedfreq 70% 65%
seedfreq ? P (c|p) 73% 66%
Table 6: Overall accuracy for 4-way classification
{hypernym,meronym,cousin,other} using different pattern
scoring methods.
English Hindi
Total Cover. Acc. Total Cover. Acc.
Hypr. 83 74% 97% 59 82% 75%
Mero. 41 81% 88% 33 63% 81%
Cous. 42 91% 55% 23 91% 71%
Other 34 85% 31% 25 80% 20%
Overall 200 81% 73% 140 79% 66%
Table 7: Test set coverage and accuracy results for inducing
different semantic relationship types.
English Hindi
Hypo. Mero. Cous. Oth. Hypo. Mero. Cous. Oth.
Hypo. 59 1 1 0 36 1 10 1
Mero. 1 28 1 3 0 17 4 0
Cous. 14 3 21 0 6 0 15 0
Other 7 3 10 9 1 4 11 4
Table 8: Confusion matrix for English (left) Hindi (right) for
the four-way classification task
469
baaruuda
hathiyaara
bama
[via induced
hypernymy]
bomb explosive grenadegun
weapon
banduuka
hyponymy]
[via induced
Goal: To learn this translation
haathagolaa
[via existing dictionary entries or previous induced translations]
EnglishHindi
Figure 2: Illustration of the models of using induced hyponymy and hypernymy for translation lexicon induction.
4 Improving a partial translation
dictionary
In this section, we explore the application of
automatically generated multilingual taxonomies
to the task of translation dictionary induction. The
hypothesis is that a pair of words in two languages
would have increased probability of being transla-
tions of each other if their hypernyms or hyponyms
are translations of one another.
As illustrated in Figure 2, the probability that
weapon is a translation of the Hindi word hathiyaara
can be decomposed into the sum of the probabilities
that their hyponyms in both languages (as induced
in Section 3.2) are translations of each other. Thus:
PH?>E (WE |WH) =
?
i Phyper (WE |Eng(Hi)) Phypo(Hi|WH)
for induced hyponyms Hi of the source word
WH , and using an existing (and likely very incom-
plete) Hindi-English dictionary to generate Eng(Hi)
for these hyponyms, and the corresponding induced
hypernyms of these translations in English.12. We
conducted a very preliminary evaluation of this idea
for obtaining English translations of a set of 25
12One of the challenges of inducing a dictionary via using a
corpus based taxonomy is sense disambiguation of the words to
be translated. In the current model, the more dominant sense
(in terms of corpus frequency of its hyponyms) is likely to get
selected by this approach. While the current model can still
help in getting translations of the dominant sense, possible fu-
ture work would be to cluster all the hyponyms according to
contextual features such that each cluster can represent the hy-
ponyms for a particular sense. The current dictionary induction
model can then be applied again using the hyponym clusters to
distinguish different senses for translation.
Hindi words. The Hindi candidate hyponym space
had been pruned of function words and non-noun
words. The likely English translation candidates
for each Hindi word were ranked according to the
probability PH?>E(WE|WH).
The first column of Table 9 shows the stand-alone
performance for this model on the dictionary induc-
tion task. This standalone model has a reasonably
good accuracy for finding the correct translation in
the Top 10 and Top 20 English candidates.
Accuracy Accuracy Accuracy
(uni-d) (bi-d) bi-d + Other
Top 1 20% 36% 36%
Top 5 56% 64% 72%
Top 10 72% 72% 80%
Top 20 84% 84% 84%
Table 9: Accuracy on Hindi to English word translation using
different transitive hypernym algorithms. The additional model
components in the bi-d(irectional) plus Other model are only
used to rerank the top 20 candidates of the bidirectional model,
and are hence limited to its top-20 performance.
This approach can be further improved by also im-
plementing the above model in the reverse direction
and computing the P (WH |WEi) for each of the
English candidates Ei. We did so and computed
P (WH |WEi) for top 20 English candidate trans-
lations. The final score for an English candidate
translation given a Hindi word was combined by
a simple average of the two directions, that is, by
summing P (WEi |WH) + P (WH |WEi).
The second column of Table 9 shows how this
bidirectional approach helps in getting the right
470
translations in Top 1 and Top 5 as compared to the
unidirectional approach. Table 10 shows a sample
Correctly translated Incorrectly translated
aujaara vishaya
(tool) (topic)
biimaarii saamana
(disease) (stuff)
hathiyaara dala
(weapon) (group,union)
dastaaveja tyohaara
(documents) (festival)
aparaadha jagaha
(crime) (position,location)
Table 10: A sample of correct and incorrect translations using
transitive hypernymy/hyponym word translation induction
of correct and incorrect translations generated
by the above model. It is interesting to see that
the incorrect translations seem to be the words
that are very general (like ?topic?, ?stuff?, etc.)
and hence their hyponym space is very large and
diffuse, resulting in incorrect translations.While the
columns 1 and 2 of Table 9 show the standalone
application of our translation dictionary induction
method, we can also combine our model with
existing work on dictionary induction using other
translation induction measures such as using relative
frequency similarity in multilingual corpora and
using cross-language context similarity between
word co-occurrence vectors (Schafer and Yarowsky,
2002).We implemented the above dictionary induc-
tion measures and combined the taxonomy based
dictionary induction model with other measures by
just summing the two scores13. The preliminary
results for bidirectional hypernym/hyponym +
other features are shown in column 3 of Table
9. The results show that the hypernym/hyponym
features can be a useful orthogonal source of lexical
similarity in the translation-induction model space.
While the model shown in Figure 2 proposes
inducing translations of hypernyms, one can also go
in the other direction and induce likely translation
candidates for hyponyms by knowing the translation
of hypernyms. For example, to learn that rifle is
a likely translation candidate of the Hindi word
13after renormalizing each of the individual score to be in the
range 0 to 1.
raaiphala, is illustrated in Figure 3. But because
there is a much larger space of hyponyms for
weapon in this direction, the output serves more to
reduce the entropy of the translation candidate space
when used in conjunction with other translation
induction similarity measures. We would expect the
application of additional similarity measures to this
greatly narrowed and ranked hypothesis space to
yield improvement in future work.
5 Conclusion
This paper has presented a novel minimal-resource
algorithm for the acquisition of multilingual lex-
ical taxonomies (including hyponymy/hypernymy
and meronymy). The algorithm is based on cross
language projection of various monolingual indica-
tors of these taxonomic relationships in free text
and via bootstrapping thereof. Using only 31-58
seed examples, the algorithm achieves accuracies of
73% and 66% for English and Hindi respectively on
the tasks of hyponymy/meronomy/cousinhood/other
model induction. The robustness of this approach
is shown by the fact that the unannotated Hindi de-
velopment corpus was only 1/15th the size of the
utilized English corpus. We also present a novel
model of unsupervised translation dictionary induc-
tion via multilingual transitive models of hypernymy
and hyponymy, using these induced taxonomies and
evaluated on Hindi-English. Performance starting
from no multilingual dictionary supervision is quite
promising.
References
E. Agichtein and L. Gravano. 2000. Snowball: extract-
ing relations from large plain-text collections. In Pro-
ceedings of the 5th ACM International Conference on
Digital Libraries, pages 85?94.
M. J. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. Knowitnow: Fast, scalable information extrac-
tion from the web. In Proceedings of EMNLP/HLT-05,
pages 563?570.
S. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of ACL-99, pages 120?126.
B. Carterette, R. Jones, W. Greiner, and C. Barr. 2006. N
semantic classes are harder than two. In Proceedings
of ACL/COLING-06, pages 49?56.
471
raaiphala missile grenade bomb
rifle
weapon
(hypothesis space)
[via inducedhathiyaara
or previous induced translations]
[via existing dictionary entries
hypernymy]
[via induced
hyponymy]
Hindi English
Goal: To learn this translation
Figure 3: Reducing the space of likely translation candidates of the word raaiphala by inducing its hypernym, using a partial
dictionary to look up the translation of hypernym and generating the candidate translations as induced hyponyms in English space.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artif. Intell., 165(1):91?
134.
C. Fellbaum. 1998. WordNet: An electronic lexical
database.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning semantic constraints for the automatic dis-
covery of part-whole relations. In Proceedings of
HLT/NAACL-03, pages 1?8.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 21(1):83?135.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2005. En-
glish Gigaword Second Edition. Linguistic Data Con-
sortium, catalog number LDC2005T12.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discov-
ering relations among named entities from large cor-
pora. In Proceedings of ACL-04, pages 415?422.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING-
92, pages 539?545.
D. Narayan, D. Chakrabarty, P. Pande, and P. Bhat-
tacharyya. 2002. An Experience in Building the Indo
WordNet-a WordNet for Hindi. International Confer-
ence on Global WordNet.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the web: Fact extraction in the fast lane. In Proceed-
ings of ACL/COLING-06, pages 809?816.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting
semantic relations. In Proceedings of ACL/COLING-
06, pages 113?120.
P. Pantel and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
HLT/NAACL-04, pages 321?328.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. Towards
terascale knowledge acquisition. In Proceedings of
COLING-04.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of ACL-02, pages 41?47.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of AAAI/IAAI-99, pages 474?479.
E. Riloff and J. Shepherd. 1997. A corpus-based ap-
proach for building semantic lexicons. CoRR, cmp-
lg/9706013.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. In Proceedings of CONLL-02, pages 146?
152.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of ACL/COLING-06, pages 801?808.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
contexts. In Proceedings of EMNLP-02, pages 214?
221.
D. Widdows. 2003. Unsupervised methods for devel-
oping taxonomies by combining syntactic and statisti-
cal information. In Proceedings of HLT/NAACL-03,
pages 197?204.
472
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 710?718,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Modeling Latent Biographic Attributes in Conversational Genres
Nikesh Garera and David Yarowsky
Department of Computer Science, Johns Hopkins University
Human Language Technology Center of Excellence
Baltimore MD, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
This paper presents and evaluates several
original techniques for the latent classifi-
cation of biographic attributes such as gen-
der, age and native language, in diverse
genres (conversation transcripts, email)
and languages (Arabic, English). First,
we present a novel partner-sensitive model
for extracting biographic attributes in con-
versations, given the differences in lexi-
cal usage and discourse style such as ob-
served between same-gender and mixed-
gender conversations. Then, we explore
a rich variety of novel sociolinguistic and
discourse-based features, including mean
utterance length, passive/active usage, per-
centage domination of the conversation,
speaking rate and filler word usage. Cu-
mulatively up to 20% error reduction is
achieved relative to the standard Boulis
and Ostendorf (2005) algorithm for classi-
fying individual conversations on Switch-
board, and accuracy for gender detection
on the Switchboard corpus (aggregate) and
Gulf Arabic corpus exceeds 95%.
1 Introduction
Speaker attributes such as gender, age, dialect, na-
tive language and educational level may be (a)
stated overtly in metadata, (b) derivable indirectly
from metadata such as a speaker?s phone number
or userid, or (c) derivable from acoustic proper-
ties of the speaker, including pitch and f0 contours
(Bocklet et al, 2008). In contrast, the goal of
this paper is to model and classify such speaker
attributes from only the latent information found
in textual transcripts. In particular, we are inter-
ested in modeling and classifying biographic at-
tributes such as gender and age based on lexi-
cal and discourse factors including lexical choice,
mean utterance length, patterns of participation
in the conversation and filler word usage. Fur-
thermore, a speaker?s lexical choice and discourse
style may differ substantially depending on the
gender/age/etc. of the speaker?s interlocutor, and
hence improvements may be achived via dyadic
modeling or stacked classifiers.
There has been substantial work in the sociolin-
guistics literature investigating discourse style dif-
ferences due to speaker properties such as gender
(Coates, 1997; Eckert, McConnell-Ginet, 2003).
Analyzing such differences is not only interesting
from the sociolinguistic and psycholinguistic point
of view of language understanding, but also from
an engineering perspective, given the goal of pre-
dicting latent author/speaker attributes in various
practical applications such as user authenticaion,
call routing, user and population profiling on so-
cial networking websites such as facebook, and
gender/age conditioned language models for ma-
chine translation and speech recogntition. While
most of the prior work in sociolinguistics has been
approached from a non-computational perspec-
tive, Koppel et al (2002) employed the use of a
linear model for gender classification with manu-
ally assigned weights for a set of linguistically in-
teresting words as features, focusing on a small de-
velopment corpus. Another computational study
for gender classification using approximately 30
weblog entries was done by Herring and Paolillo
(2006), making use of a logistic regression model
to study the effect of different features.
While small-scale sociolinguistic studies on
monologues have shed some light on important
features, we focus on modeling attributes from
spoken conversations, building upon the work of
710
Boulis and Ostendorf (2005) and show how gen-
der and other attributes can be accurately predicted
based on the following original contributions:
1. Modeling Partner Effect: A speaker may
adapt his or her conversation style depending
on the partner and we show how conditioning
on the predicted partner class using a stacked
model can provide further performance gains
in gender classification.
2. Sociolinguistic features: The paper explores
a rich set of lexical and non-lexical features
motivated by the sociolinguistic literature for
gender classification, and show how they
can effectively augment the standard ngram-
based model of Boulis and Ostendorf (2005).
3. Application to Arabic Language: We also re-
port results for Arabic language and show
that the ngram model gives reasonably high
accuracy for Arabic as well. Furthmore, we
also get consistent performance gains due to
partner effect and sociolingusic features, as
observed in English.
4. Application to Email Genre: We show how
the models explored in this paper extend to
email genre, showing the wide applicability
of general text-based features.
5. Application to new attributes: We show how
the lexical model of Boulis and Ostendorf
(2005) can be extended to Age and Native
vs. Non-native prediction, with further im-
provements gained from our partner-sensitive
models and novel sociolinguistic features.
2 Related Work
Much attention has been devoted in the sociolin-
guistics literature to detection of age, gender, so-
cial class, religion, education, etc. from conversa-
tional discourse and monologues starting as early
as the 1950s, making use of morphological fea-
tures such as the choice between the -ing and
the -in variants of the present participle ending
of the verb (Fisher, 1958), and phonological fea-
tures such as the pronounciation of the ?r? sound
in words such as far, four, cards, etc. (Labov,
1966). Gender differences has been one of the
primary areas of sociolinguistic research, includ-
ing work such as Coates (1998) and Eckert and
McConnell-Ginet (2003). There has also been
some work in developing computational models
based on linguistically interesting clues suggested
by the sociolinguistic literature for detecting gen-
der on formal written texts (Singh, 2001; Koppel
et al, 2002; Herring and Paolillo, 2006) but it has
been primarily focused on using a small number of
manually selected features, and on a small number
of formal written texts. Another relevant line of
work has been on the blog domain, using a bag of
words feature set to discriminate age and gender
(Schler et al, 2006; Burger and Henderson, 2006;
Nowson and Oberlander, 2006).
Conversational speech presents a challenging do-
main due to the interaction of genders, recognition
errors and sudden topic shifts. While prosodic fea-
tures have been shown to be useful in gender/age
classification (e.g. Shafran et al, 2003), their work
makes use of speech transcripts along the lines of
Boulis and Ostendorf (2005) in order to build a
general model that can be applied to electronic
conversations as well. While Boulis and Osten-
dorf (2005) observe that the gender of the part-
ner can have a substantial effect on their classifier
accuracy, given that same-gender conversations
are easier to classify than mixed-gender classifi-
cations, they don?t utilize this observation in their
work. In Section 5.3, we show how the predicted
gender/age etc. of the partner/interlocutor can
be used to improve overall performance via both
dyadic modeling and classifier stacking. Boulis
and Ostendorf (2005) have also constrained them-
selves to lexical n-gram features, while we show
improvements via the incorporation of non-lexical
features such as the percentage domination of the
conversation, degree of passive usage, usage of
subordinate clauses, speaker rate, usage profiles
for filler words (e.g. ?umm?), mean-utterance
length, and other such properties.
We also report performance gains of our models
for a new genre (email) and a new language (Ara-
bic), indicating the robustness of the models ex-
plored in this paper. Finally, we also explore and
evaluate original model performance on additional
latent speaker attributes including age and native
vs. non-native English speaking status.
3 Corpus Details
Consistent with Boulis and Ostendorf (2005), we
utilized the Fisher telephone conversation corpus
(Cieri et al, 2004) and we also evaluated per-
formance on the standard Switchboard conversa-
tional corpus (Godfrey et al, 1992), both collected
and annotated by the Linguistic Data Consortium.
In both cases, we utilized the provided metadata
711
(including true speaker gender, age, native lan-
guage, etc.) as only class labels for both train-
ing and evaluation, but never as features in the
classification. The primary task we employed was
identical to Boulis and Ostendorf (2005), namely
the classification of gender, etc. of each speaker
in an isolated conversation, but we also evaluate
performance when classifying speaker attributes
given the combination of multiple conversations
in which the speaker has participated. The Fisher
corpus contains a total of 11971 speakers and each
speaker participated in 1-3 conversations, result-
ing in a total of 23398 conversation sides (i.e. the
transcript of a single speaker in a single conversa-
tion). We followed the preprocessing steps and ex-
perimental setup of Boulis and Ostendorf (2005)
as closely as possible given the details presented
in their paper, although some details such as the
exact training/test partition were not currently ob-
tainable from either the paper or personal commu-
nication. This resulted in a training set of 9000
speakers with 17587 conversation sides and a test
set of 1000 speakers with 2008 conversation sides.
The Switchboard corpus was much smaller and
consisted of 543 speakers, with 443 speakers used
for training and 100 speakers used for testing, re-
sulting in a total of 4062 conversation sides for
training and 808 conversation sides for testing.
4 Modeling Gender via Ngram features
(Boulis and Ostendorf, 2005)
As our reference algorithm, we used the current
state-of-the-art system developed by Boulis and
Ostendorf (2005) using unigram and bigram fea-
tures in a SVM framework. We reimplemented
this model as our reference for gender classifica-
tion, further details of which are given below:
4.1 Training Vectors
For each conversation side, a training example was
created using unigram and bigram features with
tf-idf weighting, as done in standard text classi-
fication approaches. However, stopwords were re-
tained in the feature set as various sociolinguis-
tic studies have shown that use of some of the
stopwords, for instance, pronouns and determin-
ers, are correlated with age and gender. Also, only
the ngrams with frequency greater than 5 were re-
tained in the feature set following Boulis and Os-
tendorf (2005). This resulted in a total of 227,450
features for the Fisher corpus and 57,914 features
for the Switchboard corpus.
Female Male
Fisher Corpus
husband -0.0291 my wife 0.0366
my husband -0.0281 wife 0.0328
oh -0.0210 uh 0.0284
laughter -0.0186 ah 0.0248
have -0.0169 er 0.0222
mhm -0.0169 i i 0.0201
so -0.0163 hey 0.0199
because -0.0160 you doing 0.0169
and -0.0155 all right 0.0169
i know -0.0152 man 0.0160
hi -0.0147 pretty 0.0156
um -0.0141 i see 0.0141
boyfriend -0.0134 yeah i 0.0125
oh my -0.0124 my girlfriend 0.0114
i have -0.0119 thats thats 0.0109
but -0.0118 mike 0.0109
children -0.0115 guy 0.0109
goodness -0.0114 is that 0.0108
yes -0.0106 basically 0.0106
uh huh -0.0105 shit 0.0102
Switchboard Corpus
oh -0.0122 wife 0.0078
laughter -0.0088 my wife 0.0077
my husband -0.0077 uh 0.0072
husband -0.0072 i i 0.0053
have -0.0069 actually 0.0051
uhhuh -0.0068 sort of 0.0041
and i -0.0050 yeah i 0.0041
feel -0.0048 got 0.0039
umhum -0.0048 a 0.0038
i know -0.0047 sort 0.0037
really -0.0046 yep 0.0036
women -0.0043 the 0.0036
um -0.0042 stuff 0.0035
would -0.0039 yeah 0.0034
children -0.0038 pretty 0.0033
too -0.0036 that that 0.0032
but -0.0035 guess 0.0031
and -0.0034 as 0.0029
wonderful -0.0032 is 0.0028
yeah yeah -0.0031 i guess 0.0028
Table 1: Top 20 ngram features for gender, ranked by the
weights assigned by the linear SVM model
4.2 Model
After extracting the ngrams, a SVM model was
trained via the SVMlight toolkit (Joachims, 1999)
using the linear kernel with the default toolkit
settings. Table 1 shows the most discriminative
ngrams for gender based on the weights assigned
by the linear SVM model. It is interesting that
some of the gender-correlated words proposed by
sociolinguistics are also found by this empirical
approach, including the frequent use of ?oh? by fe-
males and also obvious indicators of gender such
as ?my wife? or ?my husband?, etc. Also, named
entity ?Mike? shows up as a discriminative uni-
gram, this maybe due to the self-introduction at
the beginning of the conversations and ?Mike?
being a common male name. For compatibility
with Boulis and Ostendorf (2005), no special pre-
712
Figure 1: The effect of varying the amount of each con-
versation side utilized for training, based on the utilized % of
each conversation (starting from their beginning).
processing for names is performed, and they are
treated as just any other unigrams or bigrams1.
Furthermore, the ngram-based approach scales
well with varying the amount of conversation uti-
lized in training the model as shown in Figure 1.
The ?Boulis and Ostendorf, 05? rows in Table 3
show the performance of this reimplemented al-
gorithm on both the Fisher (90.84%) and Switch-
board (90.22%) corpora, under the identical train-
ing and test conditions used elsewhere in our paper
for direct comparison with subsequent results2.
5 Effect of Partner?s Gender
Our original contribution in this section is the suc-
cessful modeling of speaker properties (e.g. gen-
der/age) based on the prior and joint modeling of
the partner speaker?s gender/age in the same dis-
course. The motivation here is that people tend
to use stronger gender-specific, age-specific or
dialect-specific word/phrase usage and discourse
properties when speaking with someone of a sim-
ilar gender/age/dialect than when speaking with
someone of a different gender/age/dialect, when
they may adapt a more neutral speaking style.
Also, discourse properties such as relative use
of the passive and percentage of the conversa-
tion dominated may vary depending on the gen-
der or age relationship with the speaking partner.
We employ several varieties of classifier stacking
and joint modeling to be effectively sensitive to
these differences. To illustrate the significance of
1A natural extension of this work, however, would be to
do explicit extraction of self introductions and then do table-
lookup-based gender classification, although we did not do
so for consistency with the reference algorithm.
2The modest differences with their reported results may
be due to unreported details such as the exact training/test
splits or SVM parameterizations, so for the purposes of as-
sessing the relative gain of our subsequent enhancements
we base all reported experiments on the internally-consistent
configurations as (re-)implemented here.
Fisher Corpus
Same gender conversations 94.01
Mixed gender conversations 84.06
Switchboard Corpus
Same gender conversations 93.22
Mixed gender conversations 86.84
Table 2: Difference in Gender classification accuracy be-
tween mixed gender and same gender conversations using the
reference algorithm
Classifying speaker?s and partner?s
gender simultaneously
Male-Male 84.80
Female-Female 81.96
Male-Female 15.58
Female-Male 27.46
Table 3: Performance for 4-way classification of the entire
conversation into (mm, ff, mf, fm) classes using the reference
algorithm on Switchboard corpus.
the ?partner effect?, Table 2 shows the difference
in the standard algorithm performance between
same-gender conversations (when gender-specific
style flourishes) and mixed-gender conversations
(where more neutral styles are harder to classify).
Table 3 shows the classwise performance of classi-
fying the entire conversation into four possible cat-
egories. We can see that the mixed-gender cases
are also significantly harder to classify on a con-
versation level granularity.
5.1 Oracle Experiment
To assess the potential gains from full exploita-
tion of partner-sensitive modeling, we first report
the result from an oracle experiment, where we
assume we know whether the conversation is ho-
mogeneous (same gender) or heterogeneous (dif-
ferent gender). In order to effectively utilize this
information, we classify both the test conversa-
tion side and the partner side, and if the classi-
fier is more confident about the partner side then
we choose the gender of the test conversation side
based on the heterogeneous/homogeneous infor-
mation. The overall accuracy improves to 96.46%
on the Fisher corpus using this oracle (from
90.84%), leading us to the experiment where the
oracle is replaced with a non-oracle SVM model
trained on a subset of training data such that all test
conversation sides (of the speaker and the partner)
are excluded from the training set.
5.2 Replacing Oracle by a Homogeneous vs
Heterogenous Classifier
Given the substantial improvement using the Or-
acle information, we initially trained another bi-
713
nary classifier for classifying the conversation as
mixed or single-gender. It turns out that this task
is much harder than the single-side gender clas-
sification, task and achieved only a low accuracy
value of 68.35% on the Fisher corpus. Intuitively,
the homogeneous vs. hetereogeneous partition re-
sults in a much harder classification task because
the two diverse classes of male-male and female-
female conversations are grouped into one class
(?homogeneous?) resulting in linearly insepara-
ble classes3. This subsequently lead us to create
two different classifiers for conversations, namely,
male-male vs rest and female-female vs rest4 used
in a classifier combination framework as follows:
5.3 Modeling partner via conditional model
and whole-conversation model
The following classifiers were trained and each of
their scores was used as a feature in a meta SVM
classifier:
1. Male-Male vs Rest: Classifying the entire
conversation (using test speaker and partner?s
sides) as male-male or other5.
2. Female-Female vs Rest: Classifying the en-
tire conversation (using test speaker and part-
ner?s sides) as female-female or other.
3. Conditional model of gender given most
likely partner?s gender: Two separate clas-
sifiers were trained for classifying the gen-
der of a given conversation side, one where
the partner is male and other where the part-
ner is female. Given a test conversation side,
we first choose the most likely gender of the
partner?s conversation side using the ngram-
based model6 and then choose the gender of
the test conversation side using the appropri-
ate conditional model.
4. Ngram model as explained in Section 4.
The row labeled ?+ Partner Model? in Table 4
shows the performance gain obtained via this
meta-classifier incorporating conversation type
and partner-conditioned models.
3Even non-linear kernels were not able to find a good clas-
sification boundary
4We also explored training a 3-way classifier, male-male,
female-female, mixed and the results were similar to that of
the binarized setup
5For classifying the conversations as male-male vs rest or
female-female vs rest, all the conversations with either the
speaker or the partner present in any of the test conversations
were eliminated from the training set, thus creating a disjoint
training and test conversation partitions.
6All the partner conversation sides of test speakers were
removed from the training data and the ngram-based model
was retrained on the remaining subset.
Figure 2: Empirical differences in sociolinguistic features
for Gender on the Switchboard corpus
6 Incorporating Sociolinguistic Features
The sociolinguistic literature has shown gender
differences for speakers due to features such as
speaking rate, pronoun usage and filler word us-
age. While ngram features are able to reason-
ably predict speaker gender due to their high detail
and coverage and the overall importance of lexical
choice in gender differences while speaking, the
sociolinguistics literature suggests that other non-
lexical features can further help improve perfor-
mance, and more importantly, advance our under-
standing of gender differences in discourse. Thus,
on top of the standard Boulis and Ostendorf (2005)
model, we also investigated the following features
motivated by the sociolinguistic literature on gen-
der differences in discourse (Macaulay, 2005):
1. % of conversation spoken: We measured the
speaker?s fraction of conversation spoken via
three features extracted from the transcripts:
% of words, utterances and time.
2. Speaker rate: Some studies have shown that
males speak faster than females (Yuan et
al., 2006) as can also be observed in Fig-
ure 2 showing empirical data obtained from
Switchboard corpus. The speaker rate was
measured in words/sec., using starting and
ending time-stamps for the discourse.
3. % of pronoun usage: Macaulay (2005) argues
that females tend to use more third-person
male/female pronouns (he, she, him, her and
his) as compared to males.
4. % of back-channel responses such as
?(laughter)? and ?(lipsmacks)?.
5. % of passive usage: Passives were detected
by extracting a list of past-participle verbs
from Penn Treebank and using occurences of
?form of ?to be? + past participle?.
714
6. % of short utterances (<= 3 words).
7. % of modal auxiliaries, subordinate clauses.
8. % of ?mm? tokens such as ?mhm?, ?um?,
?uh-huh?, ?uh?, ?hm?, ?hmm?,etc.
9. Type-token ratio
10. Mean inter-utterance time: Avg. time taken
between utterances of the same speaker.
11. % of ?yeah? occurences.
12. % of WH-question words.
13. % Mean word and utterance length.
The above classes resulted in a total of 16 sociolin-
guistic features which were added based on feature
ablation studies as features in the meta SVM clas-
sifier along with the 4 features as explained previ-
ously in Section 5.3.
The rows in Table 4 labeled ?+ (any sociolinguis-
tic feature)? show the performance gain using the
respective features described in this section. Each
row indicates an additive effect in the feature ab-
lation, showing the result of adding the current so-
ciolinguistic feature with the set of features men-
tioned in the rows above.
7 Gender Classification Results
Table 4 combines the results of the experiments re-
ported in the previous sections, assessed on both
the Fisher and Switchboard corpora for gender
classification. The evaluation measure was the
standard classifier accuracy, that is, the fraction of
test conversation sides whose gender was correctly
predicted. Baseline performance (always guessing
female) yields 57.47% and 51.6% on Fisher and
Switchboard respectively. As noted before, the
standard reference algorithm is Boulis and Osten-
dorf (2005), and all cited relative error reductions
are based on this established standard, as imple-
mented in this paper. Also, as a second reference,
performance is also cited for the popular ?Gender
Genie?, an online gender-detector7, based on the
manually weighted word-level sociolinguistic fea-
tures discussed in Argamon et al (2003). The ad-
ditional table rows are described in Sections 4-6,
and cumulatively yield substantial improvements
over the Boulis and Ostendorf (2005) standard.
7.1 Aggregating results over per-speaker via
consensus voting
While Table 4 shows results for classifying the
gender of the speaker on a per conversation ba-
sis (to be consistent and enable fair comparison
7http://bookblog.net/gender/genie.php
Model Acc. Error
Reduc.
Fisher Corpus (57.5% of sides are female)
Gender Genie 55.63 -384%
Ngram (Boulis & Ostendorf, 05) 90.84 Ref.
+ Partner Model 91.28 4.80%
+ % of ?yeah? 91.33
+ % of (laughter) 91.38
+ % of short utt. 91.43
+ % of auxiliaries 91.48
+ % of subord-clauses, ?mm? 91.58
+ % of Participation (in utt.) 91.63
+ % of Passive usage 91.68 9.17%
Switchboard Corpus (51.6% of sides are female)
Gender Genie 55.94 -350%
Ngram (Boulis & Ostendorf, 05) 90.22 Ref.
+ Partner Model 91.58 13.91%
+ Speaker rate, % of fillers 91.71
+ Mean utt. len., % of Ques. 91.96
+ % of Passive usage 92.08
+ % of (laughter) 92.20 20.25%
Table 4: Results showing improvement in accuracy of gen-
der classifier using partner-model and sociolinguistic features
Model Acc. Error
Reduc.
Fisher Corpus
Ngram (Boulis & Ostendorf, 05) 90.50 Ref.
+ Partner Model 91.60 11.58%
+ Socioling. Features 91.70 12.63%
Switchboard Corpus
Ngram (Boulis & Ostendorf, 05) 92.78 Ref.
+ Partner Model 93.81 14.27%
+ Socioling. Features 96.91 57.20%
Table 5: Aggregate results on a ?per-speaker? basis via ma-
jority consensus on different conversations for the respective
speaker. The results on Switchboard are significantly higher
due to more conversations per speaker as compared to the
Fisher corpus
with the work reported by Boulis and Ostendorf
(2005)), all of the above models can be easily
extended to per-speaker evaluation by pooling in
the predictions from multiple conversations of the
same speaker. Table 5 shows the result of each
model on a per-speaker basis using a majority vote
of the predictions made on the individual conver-
sations of the respective speaker. The consen-
sus model when applied to Switchboard corpus
show larger gains as it has 9.38 conversations per
speaker on average as compared to 1.95 conversa-
tions per speaker on average in Fisher. The results
715
on Switchboard corpus show a very large reduc-
tion in error rate of more than 57% with respect to
the standard algorithm, further indicating the use-
fulness of the partner-sensitive model and richer
sociolinguistic features when more conversational
evidence is available.
8 Application to Arabic Language
It would be interesting to see how the Boulis and
Ostendorf (2005) model along with the partner-
based model and sociolinguistic features would
extend to a new language. We used the LDC Gulf
Arabic telephone conversation corpus (Linguistic
Data Consortium, 2006). The training set con-
sisted of 499 conversations, and the test set con-
sisted of 200 conversations. Each speaker partic-
ipated in only one conversation, resulting in the
same number of training/test speakers as conver-
sations, and thus there was no overlap in speak-
ers/partners between training and test sets. Only
non-lexical sociolinguistic features were used for
Arabic in addition to the ngram features. The re-
sults for Arabic are shown in table 6. Based on
prior distribution, always guessing the most likely
class for gender (?male?) yielded 52.5% accuracy.
We can see that the Boulis and Ostendorf (2005)
model gives a reasonably high accuracy in Arabic
as well. More importantly, we also see consistent
performance gains via partner modeling and so-
ciolinguistic features, indicating the robustness of
these models and achieving final accuracy of 96%.
9 Application to Email Genre
A primary motivation for using only the speaker
transcripts as compared to also using acoustic
properties of the speaker (Bocklet et al, 2008) was
to enable the application of the models to other
new genres. In order to empirically support this
motivation, we also tested the performance of the
models explored in this paper on the Enron email
corpus (Klimt and Yang, 2004). We manually an-
notated the sender?s gender on a random collec-
tion of emails taken from the corpus. The resulting
training and test sets after preprocessing for header
information, reply-to?s, forwarded messages con-
sisted of 1579 and 204 emails respectively.
In addition to ngram features, a subset of so-
ciolinguistic features that could be extracted for
email were also utilized. Based on the prior dis-
tribution, always guessing the most likely class
(?male?) resulted in 63.2% accuracy. We can see
from Table 7 that the Boulis and Ostendorf (2005)
Model Acc. Error
Reduc.
Gulf Arabic (52.5% sides are male)
Ngram (Boulis & Ostendorf, 05) 92.00 Ref.
+ Partner Model 95.00
+ Mean word len. 95.50
+ Mean utt. len. 96.00 50.00%
Table 6: Gender classification results for a new
language (Gulf Arabic) showing consistent im-
provement gains via partner-model and sociolin-
guistic features.
Model Acc. Error
Reduc.
Enron Email Corpus (63.2% sides are male)
Ngram (Boulis & Ostendorf, 05) 76.78 Ref.
+ % of subor-claus., Mean 80.19
word len., Type-token ratio
+ % of pronouns. 80.50 16.02%
Table 7: Application of Ngram model and soci-
olinguistic features for gender classification in a
new genre (Email)
model based on lexical features yields a reason-
able performance with further improvements due
to the addition of sociolingustic features, resulting
in 80.5% accuracy.
10 Application to New Attributes
While gender has been studied heavily in the lit-
erature, other speaker attributes such as age and
native/non-native status also correlate highly with
lexical choice and other non-lexical features. We
applied the ngram-based model of Boulis and Os-
tendorf (2005) and our improvements using our
partner-sensitive model and richer sociolinguistic
features for a binary classification of the age of the
speaker, and classifying into native speaker of En-
glish vs non-native.
Corpus details for Age and Native Language:
For age, we used the same training and test speak-
ers from Fisher corpus as explained for gender in
section 3 and binarized into greater-than or less-
than-or-equal-to 40 for more parallel binary eval-
uation. For predicting native/non-native status, we
used the 1156 non-native speakers in the Fisher
corpus and pooled them with a randomly selected
equal number of native speakers. The training and
test partitions consisted of 2000 and 312 speakers
respectively, resulting in 3267 conversation sides
for training and 508 conversation sides for testing.
716
Age >= 40 Age < 40
well 0.0330 im thirty -0.0266
im forty 0.0189 actually -0.0262
thats right 0.0160 definitely -0.0226
forty 0.0158 like -0.0223
yeah well 0.0153 wow -0.0189
uhhuh 0.0148 as well -0.0183
yeah right 0.0144 exactly -0.0170
and um 0.0130 oh wow -0.0143
im fifty 0.0126 everyone -0.0137
years 0.0126 i mean -0.0132
anyway 0.0123 oh really -0.0128
isnt 0.0118 mom -0.0112
daughter 0.0117 im twenty -0.0110
well i 0.0116 cool -0.0108
in fact 0.0116 think that -0.0107
whether 0.0111 so -0.0107
my daughter 0.0111 mean -0.0106
pardon 0.0110 pretty -0.0106
gee 0.0109 thirty -0.0105
know laughter 0.0105 hey -0.0103
this 0.0102 right now -0.0100
oh 0.0102 cause -0.0096
young 0.0100 im actually -0.0096
in 0.0100 my mom -0.0096
when they 0.0100 kinda -0.0095
Table 8: Top 25 ngram features for Age ranked by weights
assigned by the linear SVM model
Results for Age and Native/Non-Native:
Based on the prior distribution, always guessing
the most likely class for age ( age less-than-or-
equal-to 40) results in 62.59% accuracy and al-
ways guessing the most likely class for native lan-
guage (non-native) yields 50.59% accuracy.
Table 9 shows the results for age and native/non-
native speaker status. We can see that the ngram-
based approach for gender also gives reasonable
performance on other speaker attributes, and more
importantly, both the partner-model and sociolin-
guistic features help in reducing the error rate on
age and native language substantially, indicating
their usefulness not just on gender but also on
other diverse latent attributes.
Table 8 shows the most discriminative ngrams for
binary classification of age, it is interesting to see
the use of ?well? right on top of the list for older
speakers, also found in the sociolinguistic studies
for age (Macaulay, 2005). We also see that older
speakers talk about their children (?my daughter?)
and younger speakers talk about their parents (?my
mom?), the use of words such as ?wow?, ?kinda?
and ?cool? is also common in younger speakers.
To give maximal consistency/benefit to the Boulis
and Ostendorf (2005) n-gram-based model, we did
not filter the self-reporting n-grams such as ?im
forty? and ?im thirty?, putting our sociolinguistic-
literature-based and discourse-style-based features
at a relative disadvantage.
Model Accuracy
Age (62.6% of sides have age <= 40)
Ngram Model 82.27
+ Partner Model 82.77
+ % of passive, mean inter-utt. time 83.02
, % of pronouns
+ % of ?yeah? 83.43
+ type/token ratio, + % of lipsmacks 83.83
+ % of auxiliaries, + % of short utt. 83.98
+ % of ?mm? 84.03
(Reduction in Error) (9.93%)
Native vs Non-native (50.6% of sides are non-native)
Ngram 76.97
+ Partner 80.31
+ Mean word length 80.51
(Reduction in Error) (15.37%)
Table 9: Results showing improvement in the accuracy of
age and native language classification using partner-model
and sociolinguistic features
11 Conclusion
This paper has presented and evaluated several
original techniques for the latent classification of
speaker gender, age and native language in diverse
genres and languages. A novel partner-sensitve
model shows performance gains from the joint
modeling of speaker attributes along with partner
speaker attributes, given the differences in lexical
usage and discourse style such as observed be-
tween same-gender and mixed-gender conversa-
tions. The robustness of the partner-model is sub-
stantially supported based on the consistent per-
formance gains achieved in diverse languages and
attributes. This paper has also explored a rich va-
riety of novel sociolinguistic and discourse-based
features, including mean utterance length, pas-
sive/active usage, percentage domination of the
conversation, speaking rate and filler word usage.
In addition to these novel models, the paper also
shows how these models and the previous work
extend to new languages and genres. Cumula-
tively up to 20% error reduction is achieved rel-
ative to the standard Boulis and Ostendorf (2005)
algorithm for classifying individual conversations
on Switchboard, and accuracy for gender detection
on the Switchboard corpus (aggregate) and Gulf
Arabic exceeds 95%.
Acknowledgements
We would like to thank Omar F. Zaidan for valu-
able discussions and feedback during the initial
stages of this work.
717
References
S. Argamon, M. Koppel, J. Fine, and A.R. Shimoni.
2003. Gender, genre, and writing style in formal
written texts. Text-Interdisciplinary Journal for the
Study of Discourse, 23(3):321?346.
T. Bocklet, A. Maier, and E. No?th. 2008. Age Determi-
nation of Children in Preschool and Primary School
Age with GMM-Based Supervectors and Support
Vector Machines/Regression. In Proceedings of
Text, Speech and Dialogue; 11th International Con-
ference, volume 1, pages 253?260.
C. Boulis and M. Ostendorf. 2005. A quantitative
analysis of lexical differences between genders in
telephone conversations. Proceedings of ACL, pages
435?442.
J.D. Burger and J.C. Henderson. 2006. An ex-
ploration of observable features related to blogger
age. In Computational Approaches to AnalyzingWe-
blogs: Papers from the 2006 AAAI Spring Sympo-
sium, pages 15?20.
C. Cieri, D. Miller, and K. Walker. 2004. The
Fisher Corpus: a resource for the next generations
of speech-to-text. In Proceedings of LREC.
J. Coates. 1998. Language and Gender: A Reader.
Blackwell Publishers.
Linguistic Data Consortium. 2006. Gulf Arabic Con-
versational Telephone Speech Transcripts.
P. Eckert and S. McConnell-Ginet. 2003. Language
and Gender. Cambridge University Press.
J.L. Fischer. 1958. Social influences on the choice of a
linguistic variant. Word, 14:47?56.
JJ Godfrey, EC Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. Proceedings of ICASSP, 1.
S.C. Herring and J.C. Paolillo. 2006. Gender and
genre variation in weblogs. Journal of Sociolinguis-
tics, 10(4):439?459.
J. Holmes and M. Meyerhoff. 2003. The Handbook of
Language and Gender. Blackwell Publishers.
H. Jing, N. Kambhatla, and S. Roukos. 2007. Extract-
ing social networks and biographical facts from con-
versational speech transcripts. Proceedings of ACL,
pages 1040?1047.
B. Klimt and Y. Yang. 2004. Introducing the En-
ron corpus. In First Conference on Email and Anti-
Spam (CEAS).
M. Koppel, S. Argamon, and A.R. Shimoni. 2002.
Automatically Categorizing Written Texts by Au-
thor Gender. Literary and Linguistic Computing,
17(4):401?412.
W. Labov. 1966. The Social Stratification of English
in New York City. Center for Applied Linguistics,
Washington DC.
H. Liu and R. Mihalcea. 2007. Of Men, Women, and
Computers: Data-Driven Gender Modeling for Im-
proved User Interfaces. In International Conference
on Weblogs and Social Media.
R.K.S. Macaulay. 2005. Talk that Counts: Age, Gen-
der, and Social Class Differences in Discourse. Ox-
ford University Press, USA.
S. Nowson and J. Oberlander. 2006. The identity of
bloggers: Openness and gender in personal weblogs.
Proceedings of the AAAI Spring Symposia on Com-
putational Approaches to Analyzing Weblogs.
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. Pro-
ceedings of the AAAI Spring Symposia on Computa-
tional Approaches to Analyzing Weblogs.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice sig-
natures. Proceedings of ASRU, pages 31?36.
S. Singh. 2001. A pilot study on gender differences in
conversational speech on lexical richness measures.
Literary and Linguistic Computing, 16(3):251?264.
718
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 357?360,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Arabic Cross-Document Coreference Detection
Asad Sayeed,
1,2
Tamer Elsayed,
1,2
Nikesh Garera,
1,6
David Alexander,
1,3
Tan Xu,
1,4
Douglas W. Oard,
1,4,5
David Yarowsky,
1,6
Christine Piatko
1
1
Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore,
MD, USA?
2
Dept. of Computer Science, University of Maryland, College Park, MD,
USA?
3
BBN Technologies, Cambridge, MA, USA?
4
College of Information Studies,
University of Maryland, College Park, MD, USA?
5
UMIACS, University of Maryland, College
Park, MD, USA?
6
Dept. of Computer Science, Johns Hopkins University, Baltimore, MD, USA
{asayeed,telsayed}@cs.umd.edu, ngarera@cs.jhu.edu, dalexand@bbn.com,
{tanx,oard}@umd.edu, yarowsky@cs.jhu.edu, Christine.Piatko@jhuapl.edu
Abstract
We describe a set of techniques for Ara-
bic cross-document coreference resolu-
tion. We compare a baseline system of
exact mention string-matching to ones that
include local mention context information
as well as information from an existing
machine translation system. It turns out
that the machine translation-based tech-
nique outperforms the baseline, but local
entity context similarity does not. This
helps to point the way for future cross-
document coreference work in languages
with few existing resources for the task.
1 Introduction
Our world contains at least two noteworthy
George Bushes: President George H. W. Bush and
President George W. Bush. They are both fre-
quently referred to as ?George Bush.? If we wish
to use a search engine to find documents about
one of them, we are likely also to find documents
about the other. Improving our ability to find all
documents referring to one and none referring to
the other in a targeted search is a goal of cross-
document entity coreference detection. Here we
describe some results from a system we built to
perform this task on Arabic documents. We base
our work partly on previous work done by Bagga
and Baldwin (Bagga and Baldwin, 1998), which
has also been used in later work (Chen and Mar-
tin, 2007). Other work such as Lloyd et al (Lloyd,
2006) focus on techniques specific to English.
The main contribution of this work to cross-
document coreference lies in the conditions under
which it was done. Even now, there is no large-
scale resource?in terms of annotated data?for
cross-document coreference in Arabic as there is
in English (e.g. WebPeople (Artiles, 2008)). Thus,
we employed techniques for high-performance
processing in a resource-poor environment. We
provide early steps in cross-document coreference
detection for resource-poor languages.
2 Approach
We treat cross-document entities as a set of graphs
consisting of links between within-document enti-
ties. The graphs are disjoint. Each of our systems
produces a list of such links as within-document
entity pairs (A,B). We obtain within-document
entities by running the corpus through a within-
document coreference resolver?in this case, Serif
from BBN Technologies.
To create the entity clusters, we use a union-
find algorithm over the pairs. If links (A,B)
and (C,B) appear in the system output, then
{A,B,C} are one entity. Similarly, if (X,Y )
and (Z, Y ) appear in the output, then it will find
that {X,Y, Z} are one entity. If the algorithm
later discovers link (B,Z) in the system output, it
will decide that {A,B,C,X, Y, Z} are an entity.
This is efficiently implemented via a hash table
whose keys and values are both within-document
entity IDs, allowing the implementation of easily-
searched linked lists.
2.1 The baseline system
The baseline system uses a string matching cri-
terion to determine whether two within-document
entities are similar enough to be considered as part
of the same cross-document entity. Given within-
document entities A and B, the criterion is imple-
mented as follows:
1. Find the mention strings {a
1
, a
2
, . . .} and
357
{b
1
, b
2
, . . .} of A and B, respectively that are
the longest for that within-document entity
in the given document. (There may be more
than one longest mention of equal length for
a given entity.)
2. If any longest mention strings a
n
and b
m
exist
such that a
n
= b
m
(exact string match), then
A and B are considered to be part of the same
cross-document entity. Otherwise, they are
considered to be different entities.
When the system decides that two within-
document entities are connected as a single cross-
document entity, it emits a link between within-
document entities A and B represented as the pair
(A, B). We maintain a list of such links, but we
omit all links between within-document entities in
the same document.
The output of the system is a list of pairwise
links. The following two experimental systems
also produce lists of pairwise links. Union is per-
formed between the baseline system?s list and the
lists produced by the other systems to create lists
of pairs that include the information in the base-
line. However, each of the following systems?
outputs are merged separately with the baseline.
By including the baseline results in each system,
we are able to clarify the potential of each addi-
tional technique to improve performance over a
technique that is cheap to run under any circum-
stances, especially given that our experiments are
focused on increasing the number of links in an
Arabic context where links are likely to be dis-
rupted by spelling variations.
2.2 Translingual projection
We implement a novel cross-language approach
for Arabic coreference resolution by expanding
the space of exact match comparisons to approxi-
mate matches of English translations of the Arabic
strings. The intuition for this approach is that of-
ten the Arabic strings of the same named entity
may differ due to misspellings, titles, or aliases
that can be corrected in the English space. The
English translations were obtained using a stan-
dard statistical machine translation system (Chi-
ang, 2007; Li, 2008) and then compared using an
alias match.
The algorithm below describes the approach,
applied to any Arabic named entities that fail the
baseline string-match test:
1. For a given candidate Arabic named entity
pair (A,B), we project them into English by
translating the mentions using a standard sta-
tistical machine translation toolkit. Using the
projected English pair, say, (A
?
, B
?
) we per-
form the following tests to determine whether
A and B are co-referent:
(a) We do an exact string-match test in the
English space using the projected enti-
ties (A
?
, B
?
). The exact string match test
is done exactly as in the baseline system,
using the set of longest named entities in
their respective co-reference chains.
(b) If (A
?
, B
?
) fail in the exact string-match
test as in the baseline, then we test
whether they belong to a list of high con-
fidence co-referent named-entity pairs
1
precomputed for English using alias-
lists derived from Wikipedia.
(c) If (A
?
, B
?
) fails (a) and (b) then (A,B)
is deemed as non-coreferent.
While we hypothesize that translingual projection
via English should help in increasing recall since
it can work with non-exact string matches, it may
also help in increasing precision based on the as-
sumption that a name of American or English ori-
gin might have different variants in Arabic and that
translating to English can help in merging those
variants, as shown in figure 1.
 ????? ??????
 ????? 
?????? 
 ??????? 
 ???????
(Ms. Aisha)
(Aisha)
(Clenton)
(Clinton)
(Cilinton)
Aisha
Aisha
Clinton
Clinton
Clinton
Translate
via SMT
Figure 1: Illustration of translingual projection
method for resolving Arabic named entity strings
via English space. The English strings in paren-
theses indicate the literal glosses of the Arabic
strings prior to translation.
2.3 Entity context similarity
The context of mentions can play an important role
in merging or splitting potential coreferent men-
1
For example: (Sean Michael Waltman, Sean Waltman)
are high confidence-matches even though they are not an
exact-string match.
358
tions. We hypothesize that two mentions in two
different documents have a good chance of refer-
ring to the same entity if they are mentioned in
contexts that are topically very similar. A way of
representing a mention context is to consider the
words in the mention?s neighborhood. The con-
text of a mention can be defined as the words that
surround the mention in a window of n (50 in our
experiments) tokens centered by the mention. In
our experiments, we used highly similar contexts
to link mentions that might be coreferent.
Computing context similarity between every
pair of large number of mentions requires a highly
scalable and efficient mechanism. This can be
achieved using MapReduce, a distributed comput-
ing framework (Dean, 2004)
Elsayed et al (Elsayed, 2008) proposed an ef-
ficient MapReduce solution for the problem of
computing the pairwise similarity matrix in large
collections. They considered a ?bag-of-words?
model where similarity of two documents d
i
and d
j
is measured as follows: sim(d
i
, d
j
) =
?
t?d
i
?d
j
w
t,d
i
? w
t,d
j
, where w(t, d) is the weight
of term t in document d. A term contributes to
each pair that contains it. The list of documents
that contain a term is what is contained in the post-
ings of an inverted index. Thus, by processing
all postings, the pairwise similarity matrix can be
computed by summing term contributions. We use
the MapReduce framework for two jobs, inverted
indexing and pairwise similarity.
Elsayed et al suggested an efficient df-cut strat-
egy that eliminates terms that appear in many doc-
uments (having high df ) and thus contribute less
in similarity but cost in computation (e.g., a 99%
df-cut means that the most frequent 1% of the
terms were discarded). We adopted that approach
for computing similarities between the contexts
of two mentions. The processing unit was rep-
resented as a bag of n words in a window sur-
rounding each mention of a within-document en-
tity. Given a relatively small mention context, we
used a high df-cut value of 99.9%.
3 Experiments
We performed our experiments in the context of
the Automatic Content Extraction (ACE) eval-
uation of 2008, run by the National Institute
of Standards and Technology (NIST). The eval-
uation corpus contained approximately 10,000
documents from the following domains: broad-
cast conversation transcripts, broadcast news tran-
scripts, conversational telephone speech tran-
scripts, newswire, Usenet Newsgroup/Discussion
Groups, and weblogs. Systems were required to
process the large source sets completely. For per-
formance measurement after the evaluation, NIST
selected 412 of the Arabic source documents out
of the larger set (NIST, 2008).
For development purposes we used the NIST
ACE 2005 Arabic data with within-document
ground truth. This consisted of 1,245 documents.
We also used exactly 12,000 randomly selected
documents from the LDC Arabic Gigaword Third
Edition corpus, processed through Serif. The Ara-
bic Gigaword corpus was used to select a thresh-
old of 0.4956 for the context similarity technique
via inspection of (A,B) link scores by a native
speaker of Arabic.
It must be emphasized that there was no ground
truth available for this task in Arabic. Performing
this task in the absence of significant training or
evaluation data is one emphasis of this work.
3.1 Evaluation measures
We used NIST?s scoring techniques to evaluate the
performance of our systems. Scoring for the ACE
evaluation is done using an scoring script provided
by NIST which produces many kinds of statistics.
NIST mainly uses a measure called the ACE value,
but it also computes B-cubed.
B-Cubed represents the task of finding cross-
document entities in the following way: if a user
of the system is searching for a particular Bush
and finds document D, he or she should be able to
find all of the other documents with the same Bush
in them as links from D?that is, cross-document
entities represent graphs connecting documents.
Bagga and Baldwin are able to define precision,
recall, and F-measure over a collection of docu-
ments in this way.
The ACE Value represents a score similar to
B-Cubed, except that every mention and within-
document entity is weighted in NIST?s specifica-
tion by a number of factors. Every entity is worth 1
point, a missing entity worth 0, and attribute errors
are discounted by multiplying by a factor (0.75 for
CLASS, 0.5 for TYPE, and 0.9 for SUBTYPE).
Before scoring can be accomplished, the enti-
ties found by the system must be mapped onto
those found in the reference provided by NIST.
The ACE scorer does this document-by-document,
359
selecting the mapping that produces the highest
score. A description of the evaluation method and
entity categorization is available at (NIST, 2008).
3.2 Results and discussion
The results of running the ACE evaluation script
on the system output are shown in table 1. The
translingual projection system achieves higher
scores than all other systems on all measures. Al-
though it achieves only a 2 point improvement
over the baseline ACE value, it should be noted
that this represents a substantial number of at-
tributes per cross-document entity that it is getting
right.
Thresh B-Cubed ACE
System hold Prec Rec F Val.
Baseline 37.5 44.1 40.6 19.2
TrnsProj 38.4 44.8 41.3 21.2
CtxtSim 0.2 37.6 35.2 36.4 15.9
CtxtSim 0.3 37.4 43.8 40.3 18.9
CtxtSim 0.4 37.5 44.1 40.6 19.3
CtxtSim 0.4956 37.5 44.1 40.6 19.3
CtxtSim 0.6 37.5 44.1 40.6 19.2
Table 1: Scores from ACE evaluation script.
On the other hand, as the context similarity
threshold increases, we notice that the B-Cubed
measures reach identical values with the baseline
but never exceed it. But as it decreases, it loses
B-Cubed recall and ACE value.
While two within-document entities whose
longest mention strings match exactly and are le-
gitimately coreferent are likely to be mentioned in
the same contexts, it seems that a lower (more lib-
eral) threshold introduces spurious links and cre-
ates a different entity clustering.
Translingual projection appears to include links
that exact string matching in Arabic does not?
part of its purpose is to add close matches to those
found by exact string matching. It is able to in-
clude these links partly because it allows access to
resources in English that are not available for Ara-
bic such as Wikipedia alias lists.
4 Conclusions and Future Work
We have evaluated and discussed a set of tech-
niques for cross-document coreference in Arabic
that can be applied in the absence of significant
training and evaluation data. As it turns out, an
approach based on machine translation is slightly
better than a string-matching baseline, across all
measures. It worked by using translations from
Arabic to English in order to liberalize the string-
matching criterion, suggesting that using further
techniques via English to discover links may be
a fruitful future research path. This also seems
to suggest that a Bagga and Baldwin-style vector-
space model may not be the first approach to pur-
sue in future work on Arabic.
However, varying other parameters in the con-
text similarity approach should be tried in order
to gain a fuller picture of performance. One of
them is the df-cut of the MapReduce-based sim-
ilarity computation. Another is the width of the
word token window we used?we may have used
one that is too tight to be better than exact Arabic
string-matching.
References
Javier Artiles and Satoshi Sekine and Julio Gonzalo
2008. Web People Search?Results of the first eval-
uation and the plan for the second. WWW 2008.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. COLING-ACL 1998.
Y. Chen and J. Martin. 2007. Towards robust unsuper-
vised personal name disambiguation. EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. OSDI.
T. Elsayed and J. Lin and D. W. Oard. 2008. Pair-
wise Document Similarity in Large Collections with
MapReduce. ACL/HLT.
Z. Li and S. Khudanpur. 2008. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. ACL SSST.
L. Lloyd and Andrew Mehler and Steven Skiena. 2006.
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching.
NIST. 2008. Automatic Content Extraction 2008 Eval-
uation Plan (ACE08).
360
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 37?44, New York City, June 2006. c?2006 Association for Computational Linguistics
Resolving and Generating Definite Anaphora
by Modeling Hypernymy using Unlabeled Corpora
Nikesh Garera and David Yarowsky
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
We demonstrate an original and success-
ful approach for both resolving and gen-
erating definite anaphora. We propose
and evaluate unsupervised models for ex-
tracting hypernym relations by mining co-
occurrence data of definite NPs and po-
tential antecedents in an unlabeled cor-
pus. The algorithm outperforms a stan-
dard WordNet-based approach to resolv-
ing and generating definite anaphora. It
also substantially outperforms recent re-
lated work using pattern-based extraction
of such hypernym relations for corefer-
ence resolution.
1 Introduction
Successful resolution and generation of definite
anaphora requires knowledge of hypernym and hy-
ponym relationships. For example, determining the
antecedent to the definite anaphor ?the drug? in text
requires knowledge of what previous noun-phrase
candidates could be drugs. Likewise, generating a
definite anaphor for the antecedent ?Morphine? in
text requires both knowledge of potential hypernyms
(e.g. ?the opiate?, ?the narcotic?, ?the drug?, and
?the substance?), as well as selection of the most ap-
propriate level of generality along the hypernym tree
in context (i.e. the ?natural? hypernym anaphor).
Unfortunately existing manual hypernym databases
such as WordNet are very incomplete, especially
for technical vocabulary and proper names. Word-
Nets are also limited or non-existent for most of the
world?s languages. Finally, WordNets also do not
include notation of the ?natural? hypernym level for
anaphora generation, and using the immediate par-
ent performs quite poorly, as quantified in Section 5.
In first part of this paper, we propose a novel ap-
proach for resolving definite anaphora involving hy-
ponymy relations. We show that it performs substan-
tially better than previous approaches on the task of
antecedent selection. In the second part we demon-
strate how this approach can be successfully ex-
tended to the problem of generating a natural def-
inite NP given a specific antecedent.
In order to explain the antecedent selection task for
definite anaphora clearly, we provide the follow-
ing example taken from the LDC Gigaword corpus
(Graff et al, 2005).
(1)...pseudoephedrine is found in an allergy treat-
ment, which was given to Wilson by a doctor when
he attended Blinn junior college in Houston. In a
unanimous vote, the Norwegian sports confedera-
tion ruled that Wilson had not taken the drug to en-
hance his performance...
In the above example, the task is to resolve
the definite NP the drug to its correct antecedent
pseudoephedrine, among the potential antecedents
<pseudoephedrine, allergy, blinn, college, hous-
ton, vote, confederation, wilson>. Only Wilson can
be ruled out on syntactic grounds (Hobbs, 1978).
To be able to resolve the correct antecedent from
the remaining potential antecedents, the system re-
quires the knowledge that pseudoephedrine is a
drug. Thus, the problem is to create such a knowl-
edge source and apply it to this task of antecedent
selection. A total of 177 such anaphoric examples
37
were extracted randomly from the LDC Gigaword
corpus and a human judge identified the correct an-
tecedent for the definite NP in each example (given a
context of previous sentences).1 Two human judges
were asked to perform the same task over the same
examples. The agreement between the judges was
92% (of all 177 examples), indicating a clearly de-
fined task for our evaluation purposes.
We describe an unsupervised approach to this task
that extracts examples containing definite NPs from
a large corpus, considers all head words appearing
before the definite NP as potential antecedents and
then filters the noisy<antecedent, definite-NP> pair
using Mutual Information space. The co-occurence
statistics of such pairs can then be used as a mecha-
nism for detecting a hypernym relation between the
definite NP and its potential antecedents. We com-
pare this approach with a WordNet-based algorithm
and with an approach presented by Markert and Nis-
sim (2005) on resolving definite NP coreference that
makes use of lexico-syntactic patterns such as ?X
and Other Ys? as utilized by Hearst (1992).
2 Related work
There is a rich tradition of work using lexical and se-
mantic resources for anaphora and coreference res-
olution. Several researchers have used WordNet as
a lexical and semantic resource for certain types of
bridging anaphora (Poesio et al, 1997; Meyer and
Dale, 2002). WordNet has also been used as an im-
portant feature in machine learning of coreference
resolution using supervised training data (Soon et
al., 2001; Ng and Cardie, 2002). However, sev-
eral researchers have reported that knowledge incor-
porated via WordNet is still insufficient for definite
anaphora resolution. And of course, WordNet is not
available for all languages and is missing inclusion
of large segments of the vocabulary even for cov-
ered languages. Hence researchers have investigated
use of corpus-based approaches to build a Word-
Net like resource automatically (Hearst, 1992; Cara-
1The test examples were selected as follows: First, all
the sentences containing definite NP ?The Y? were extracted
from the corpus. Then, the sentences containing instances
of anaphoric definite NPs were kept and other cases of defi-
nite expressions (like existential NPs ?The White House?,?The
weather?) were discarded. From this anaphoric set of sentences,
177 sentence instances covering 13 distinct hypernyms were
randomly selected as the test set and annotated for the correct
antecedent by human judges.
ballo, 1999; Berland and Charniak, 1999). Also,
several researchers have applied it to resolving dif-
ferent types of bridging anaphora (Clark, 1975).
Poesio et al (2002) have proposed extracting lexical
knowledge about part-of relations using Hearst-style
patterns and applied it to the task of resolving bridg-
ing references. Poesio et al (2004) have suggested
using Google as a source of computing lexical dis-
tance between antecedent and definite NP for mere-
ological bridging references (references referring to
parts of an object already introduced). Markert et al
(2003) have applied relations extracted from lexico-
syntactic patterns such as ?X and other Ys? for Other-
Anaphora (referential NPs with modifiers other or
another) and for bridging involving meronymy.
There has generally been a lack of work in the exist-
ing literature for automatically building lexical re-
sources for definite anaphora resolution involving
hyponyms relations such as presented in Example
(1). However, this issue was recently addressed by
Markert and Nissim (2005) by extending their work
on Other-Anaphora using lexico syntactic pattern ?X
and other Y?s to antecedent selection for definite NP
coreference. However, our task is more challeng-
ing since the anaphoric definite NPs in our test set
include only hypernym anaphors without including
the much simpler cases of headword repetition and
other instances of string matching. For direct eval-
uation, we also implemented their corpus-based ap-
proach and compared it with our models on identical
test data.
We also describe and evaluate a mechanism for com-
bining the knowledge obtained from WordNet and
the six corpus-based approaches investigated here.
The resulting models are able to overcome the weak-
nesses of a WordNet-only model and substantially
outperforms any of the individual models.
3 Models for Lexical Acquisition
3.1 TheY-Model
Our algorithm is motivated by the observation that in
a discourse, the use of the definite article (?the?) in a
non-deictic context is primarily licensed if the con-
cept has already been mentioned in the text. Hence a
sentence such as ?The drug is very expensive? gen-
erally implies that either the word drug itself was
previously mentioned (e.g. ?He is taking a new drug
for his high cholesterol.?) or a hyponym of drug was
38
previously mentioned (e.g. ?He is taking Lipitor for
his high cholesterol.?). Because it is straightforward
to filter out the former case by string matching, the
residual instances of the phrase ?the drug? (without
previous mentions of the word ?drug? in the dis-
course) are likely to be instances of hypernymic def-
inite anaphora. We can then determine which nouns
earlier in the discourse (e.g. Lipitor) are likely an-
tecedents by unsupervised statistical co-occurrence
modeling aggregated over the entire corpus. All we
need is a large corpus without any anaphora annota-
tion and a basic tool for noun tagging and NP head
annotation. The detailed algorithm is as follows:
1. Find each sentence in the training corpus that
contains a definite NP (?the Y?) and does not
contain ?a Y?, ?an Y? or other instantiations of
Y2 appearing before the definite NP within a
fixed window.3
2. In the sentences that pass the above definite NP
and a/an test, regard all the head words (X) oc-
curring in the current sentence before the defi-
nite NP and the ones occurring in previous two
sentences as potential antecedents.
3. Count the frequency c(X,Y) for each pair ob-
tained in the above two steps and pre-store it in
a table.4 The frequency table can be modified
to give other scores for pair(X,Y) such as stan-
dard TF-IDF and Mutual Information scores.
4. Given a test sentence having an anaphoric def-
inite NP Y, consider the nouns appearing be-
fore Y within a fixed window as potential an-
tecedents. Rank the candidates by their pre-
computed co-occurence measures as computed
in Step 3.
Since we consider all head words preceding the defi-
nite NP as potential correct antecedents, the raw fre-
quency of the pair (X ,Y ) can be very noisy. This
can be seen clearly in Table 1, where the first col-
umn shows the top potential antecedents of definite
NP the drug as given by raw frequency. We nor-
malize the raw frequency using standard TF-IDF
2While matching for both ?the Y? and ?a/an Y?, we also ac-
count for Nouns getting modified by other words such as adjec-
tives. Thus ?the Y? will still match to ?the green and big Y?.
3Window size was set to two sentences, we also experi-
mented with a larger window size of five sentences and the re-
sults obtained were similar.
4Note that the count c(X,Y) is asymmetric
Rank Raw freq TF-IDF MI
1 today kilogram amphetamine
2 police heroin cannabis
3 kilogram police cocaine
4 year cocaine heroin
5 heroin today marijuana
6 dollar trafficker pill
7 country officer hashish
8 official amphetamine tablet
Table 1: A sample of ranked hyponyms proposed for
the definite NP The drug by TheY-Model illustrat-
ing the differences in weighting methods.
Acc Acctag Av Rank
MI 0.531 0.577 4.82
TF-IDF 0.175 0.190 6.63
Raw Freq 0.113 0.123 7.61
Table 2: Results using different normalization tech-
niques for the TheY-Model in isolation. (60 million
word corpus)
and Mutual Information scores to filter the noisy
pairs.5 In Table 2, we report our results for an-
tecedent selection using Raw frequency c(X,Y), TF-
IDF 6 and MI in isolation. Accuracy is the fraction
of total examples that were assigned the correct an-
tecedent and Accuracytag is the same excluding the
examples that had POS tagging errors for the cor-
rect antecedent.7 Av Rank is the rank of the true
antecedent averaged over the number of test exam-
ples.8 Based on the above experiment, the rest of
this paper assumesMutual Information scoring tech-
nique for TheY-Model.
5Note that MI(X,Y ) = log P (X,Y )P (X)P (Y ) and this is directly
proportional to P (Y |X) = c(X,Y )c(X) for a fixed Y . Thus, we
can simply use this conditional probability during implementa-
tion since the definite NP Y is fixed for the task of antecedent
selection.
6For the purposes of TF-IDF computation, document fre-
quency df(X) is defined as the number of unique definite NPs
for which X appears as an antecedent.
7Since the POS tagging was done automatically, it is possi-
ble for any model to miss the correct antecedent because it was
not tagged correctly as a noun in the first place. There were 14
such examples in the test set and none of the model variants can
find the correct antecdent in these instances.
8Knowing average rank can be useful when a n-best ranked
list from coreference task is used as an input to other down-
stream tasks such as information extraction.
39
Acc Acctag Av Rank
TheY+WN 0.695 0.755 3.37
WordNet 0.593 0.644 3.29
TheY 0.531 0.577 4.82
Table 3: Accuracy and Average Rank showing com-
bined model performance on the antecedent selec-
tion task. Corpus Size: 60 million words.
3.2 WordNet-Model (WN)
Because WordNet is considered as a standard re-
source of lexical knowledge and is often used in
coreference tasks, it is useful to know how well
corpus-based approaches perform as compared to
a standard model based on the WordNet (version
2.0).9 The algorithm for the WordNet-Model is as
follows:
Given a definite NP Y and its potential antecedent
X, choose X if it occurs as a hyponym (either direct
or indirect inheritance) of Y. If multiple potential an-
tecedents occur in the hierarchy of Y, choose the one
that is closest in the hierarchy.
3.3 Combination: TheY+WordNet Model
Most of the literature on using lexical resources
for definite anaphora has focused on using individ-
ual models (either corpus-based or manually build
resources such as WordNet) for antecedent selec-
tion. Some of the difficulties with using WordNet is
its limited coverage and its lack of empirical rank-
ing model. We propose a combination of TheY-
Model andWordNet-Model to overcome these prob-
lems. Essentially, we rerank the hypotheses found
in WordNet-Model based on ranks of TheY-model
or use a backoff scheme if WordNet-Model does not
return an answer due to its limited coverage. Given
a definite NP Y and a set of potential antecedents Xs
the detailed algorithm is specified as follows:
1. Rerank with TheY-Model: Rerank the potential
antecedents found in the WordNet-Model ta-
ble by assiging them the ranks given by TheY-
Model. If TheY-Model does not return a rank
for a potential antecedent, use the rank given by
9We also computed the accuracy using a weaker baseline,
namely, selecting the closest previous headword as the correct
antecedent. This recency based baseline obtained a low accu-
racy of 15% and hence we used the stronger WordNet based
model for comparison purposes.
the WordNet-Model. Now pick the top ranked
antecedent after reranking.
2. Backoff: If none of the potential antecedents
were found in the WordNet-Model then pick
the correct antecedent from the ranked list of
The-Y model. If none of the models return an
answer then assign ranks uniformly at random.
The above algorithm harnesses the strength of
WordNet-Model to identify good hyponyms and the
strength of TheY-model to identify which are more
likely to be used as an antecedent. Note that this
combination algorithm can be applied using any
corpus-based technique to account for poor-ranking
and low-coverage problems of WordNet and the
Sections 3.4, 3.5 and 3.6 will show the results for
backing off to a Hearst-style hypernym model. Ta-
ble 4 shows the decisions made by TheY-model,
WordNet-Model and the combined model for a sam-
ple of test examples. It is interesting to see how both
the models mutually complement each other in these
decisions. Table 3 shows the results for the models
presented so far using a 60 million word training text
from the Gigaword corpus. The combined model re-
sults in a substantially better accuracy than the indi-
vidual WordNet-Model and TheY-Model, indicating
its strong merit for the antecedent selection task.10
3.4 OtherY-Modelfreq
This model is a reimplementation of the corpus-
based algorithm proposed by Markert and Nissim
(2005) for the equivalent task of antecedent selec-
tion for definite NP coreference. We implement their
approach of using the lexico-syntactic pattern X and
A* other B* Y{pl} for extracting (X,Y) pairs.The A*
and B* allow for adjectives or other modifiers to be
placed in between the pattern. The model presented
in their article uses the raw frequency as the criteria
for selecting the antecedent.
3.5 OtherY-ModelMI (normalized)
We normalize the OtherY-Model using Mutual In-
formation scoring method. Although Markert and
Nissim (2005) report that using Mutual Information
performs similar to using raw frequency, Table 5
shows that using Mutual Information makes a sub-
stantial impact on results using large training cor-
pora relative to using raw frequency.
10The claim is statistically significant with a p < 0.01 ob-
tained by sign-test
40
Summary Keyword True TheY Truth WordNet Truth TheY+WN Truth
(Def. Ana) Antecedent Choice Rank Choice Rank Choice Rank
Both metal gold gold 1 gold 1 gold 1
correct sport soccer soccer 1 soccer 1 soccer 1
TheY-Model drug steroid steroid 1 NA NA steroid 1
helps drug azt azt 1 medication 2 azt 1
WN-Model instrument trumpet king 10 trumpet 1 trumpet 1
helps drug naltrexone alcohol 14 naltrexone 1 naltrexone 1
Both weapon bomb artillery 3 NA NA artillery 3
incorrect instrument voice music 9 NA NA music 9
Table 4: A sample of output from different models on antecedent selection (60 million word corpus).
3.6 Combination: TheY+OtherYMI Model
Our two corpus-based approaches (TheY and Oth-
erY) make use of different linguistic phenomena and
it would be interesting to see whether they are com-
plementary in nature. We used a similar combina-
tion algorithm as in Section 3.3 with the WordNet-
Model replaced with the OtherY-Model for hyper-
nym filtering, and we used the noisy TheY-Model
for reranking and backoff. The results for this ap-
proach are showed as the entry TheY+OtherYMI in
Table 5. We also implemented a combination (Oth-
erY+WN) of Other-Y model and WordNet-Model
by replacing TheY-Model with OtherY-Model in the
algorithm described in Section 3.3. The respective
results are indicated as OtherY+WN entry in Table
5.
4 Further Anaphora Resolution Results
Table 5 summarizes results obtained from all the
models defined in Section 3 on three different sizes
of training unlabeled corpora (from Gigaword cor-
pus). The models are listed from high accuracy to
low accuracy order. The OtherY-Model performs
particularly poorly on smaller data sizes, where cov-
erage of the Hearst-style patterns maybe limited,
as also observed by Berland and Charniak (1999).
We further find that the Markert and Nissim (2005)
OtherY-Model and our MI-based improvement do
show substantial relative performance growth at in-
creased corpus sizes, although they still underper-
form our basic TheY-Model at all tested corpus
sizes. Also, the combination of corpus-based mod-
els (TheY-Model+OtherY-model) does indeed per-
forms better than either of them in isolation. Fi-
nally, note that the basic TheY-algorithm still does
Acc Acctag Av Rank
60 million words
TheY+WN 0.695 0.755 3.37
OtherYMI+WN 0.633 0.687 3.04
WordNet 0.593 0.644 3.29
TheY 0.531 0.577 4.82
TheY+OtherYMI 0.497 0.540 4.96
OtherYMI 0.356 0.387 5.38
OtherYfreq 0.350 0.380 5.39
230 million words
TheY+WN 0.678 0.736 3.61
OtherYMI+WN 0.650 0.705 2.99
WordNet 0.593 0.644 3.29
TheY+OtherYMI 0.559 0.607 4.50
TheY 0.519 0.564 4.64
OtherYMI 0.503 0.546 4.37
OtherYfreq 0.418 0.454 4.52
380 million words
TheY+WN 0.695 0.755 3.47
OtherYMI+WN 0.644 0.699 3.03
WordNet 0.593 0.644 3.29
TheY+OtherYMI 0.554 0.601 4.20
TheY 0.537 0.583 4.26
OtherYMI 0.525 0.571 4.20
OtherYfreq 0.446 0.485 4.36
Table 5: Accuracy and Average Rank of Models de-
fined in Section 3 on the antecedent selection task.
41
relatively well by itself on smaller corpus sizes,
suggesting its merit on resource-limited languages
with smaller available online text collections and the
unavailability of WordNet. The combined models
of WordNet-Model with the two corpus-based ap-
proaches still significantly (p < 0.01) outperform
any of the other individual models.11
5 Generation Task
Having shown positive results for the task of an-
tecedent selection, we turn to a more difficult task,
namely generating an anaphoric definite NP given
a nominal antecedent. In Example (1), this would
correspond to generating ?the drug? as an anaphor
knowing that the antecedent is pseudoephedrine.
This task clearly has many applications: current gen-
eration systems often limit their anaphoric usage to
pronouns and thus an automatic system that does
well on hypernymic definite NP generation can di-
rectly be helpful. It also has strong potential appli-
cation in abstractive summarization where rewriting
a fluent passage requires a good model of anaphoric
usage.
There are many interesting challenges in this prob-
lem: first of all, there maybe be multiple acceptable
choices for definite anaphor given a particular an-
tecedent, complicating automatic evaluation. Sec-
ond, when a system generates a definite anaphora,
the space of potential candidates is essentially un-
bounded, unlike in antecdent selection, where it is
limited only to the number of potential antecedents
in prior context. In spite of the complex nature
of this problem, our experiments with the human
judgements, WordNet and corpus-based approaches
show a simple feasible solution. We evaluate our
automatic approaches based on exact-match agree-
ment with definite anaphora actually used in the cor-
pus (accuracy) and also by agreement with definite
anaphora predicted independently by a human judge
in an absence of context.
11Note that syntactic co-reference candidate filters such as
the Hobbs algorithm were not utilized in this study. To assess
the performance implications, the Hobbs algorithm was applied
to a randomly selected 100-instance subset of the test data. Al-
though the Hobbs algorithm frequently pruned at least one of
the coreference candidates, in only 2% of the data did such can-
didate filtering change system output. However, since both of
these changes were improvements, it could be worthwhile to
utilize Hobbs filtering in future work, although the gains would
likely be modest.
5.1 Human experiment
We extracted a total of 103 <true antecedent, defi-
nite NP> pairs from the set of test instances used in
the resolution task. Then we asked a human judge (a
native speaker of English) to predict a parent class
of the antecedent that could act as a good definite
anaphora choice in general, independent of a par-
ticular context. Thus, the actual corpus sentence
containing the antecedent and definite NP and its
context was not provided to the judge. We took
the predictions provided by the judge and matched
them with the actual definite NPs used in the corpus.
The agreement between corpus and the human judge
was 79% which can thus be considered as an upper
bound of algorithm performance. Table 7 shows a
sample of decisions made by the human and how
they agree with the definite NPs observed in the cor-
pus. It is interesting to note the challenge of the
sense variation and figurative usage. For example,
?corruption? is refered to as a ?tool? in the actual
corpus anaphora, a metaphoric usage that would be
difficult to predict unless given the usage sentence
and its context. However, a human agreement of
79% indicate that such instances are relatively rare
and the task of predicting a definite anaphor with-
out its context is viable. In general, it appears from
our experiements that humans tend to select from
a relatively small set of parent classes when gener-
ating hypernymic definite anaphora. Furthermore,
there appears to be a relatively context-independent
concept of the ?natural? level in the hypernym hi-
erarchy for generating anaphors. For example, al-
though <?alkaloid?, ?organic compound?, ?com-
pound?, ?substance?, ?entity?> are all hypernyms
of ?Pseudoephederine? in WordNet, ?the drug?
appears to be the preferred hypernym for definite
anaphora in the data, with the other alternatives be-
ing either too specific or too general to be natural.
This natural level appears to be difficult to define by
rule. For example, using just the immediate parent
hypernym in the WordNet hierarchy only achieves
4% match with the corpus data for definite anaphor
generation.
5.2 Algorithms
The following sections presents our corpus-based al-
gorithms as more effective alternatives.
42
Agreement Agreement
w/ human w/ corpus
judge
TheY+OtherY+WN 47% 46%
OtherY +WN 43% 43%
TheY+WN 42% 37%
TheY +OtherY 39% 36%
OtherY 39% 36%
WordNet 4% 4%
Human judge 100% 79%
Corpus 79% 100%
Table 6: Agreement of different generation models
with human judge and with definite NP used in the
corpus.
5.2.1 Individual Models
For the corpus-based approaches, the TheY-Model
and OtherY-Model were trained in the same manner
as for the antecedent selection task. The only differ-
ence was that in the generation case, the frequency
statistics were reversed to provide a hypernym given
a hyponym. Additionally, we found that raw fre-
quency outperformed either TF-IDF or Mutual In-
formation and was used for all results in Table 6.
The stand-alone WordNet model is also very simple:
Given an antecedent, we lookup its direct hypernym
(using first sense) in the WordNet and use it as the
definite NP, for lack of a better rule for preferred hy-
pernym location.
5.2.2 Combining corpus-based approaches and
WordNet
Each of the corpus-based approaches was combined
with WordNet resulting in two different models as
follows: Given an antecedent X, the corpus-based
approach looks up in its table the hypernym of X,
for example Y, and only produces Y as the output if
Y also occurs in the WordNet as hypernym. Thus
WordNet is used as a filtering tool for detecting vi-
able hypernyms. This combination resulted in two
models: ?TheY+WN? and ?OtherY+WN?.
We also combined all the three approaches, ?TheY?,
?OtherY? and WordNet resulting in a single model
?TheY+OtherY+WN?. This was done as follows: We
first combine the models ?TheY? and ?OtherY? using
a backoff model. The first priority is to use the hy-
Antecedent Corpus Human TheY+OtherY
Def Ana Choice +WN
racing sport sport sport
azt drug drug drug
missile weapon weapon weapon
alligator animal animal animal
steel metal metal metal
osteporosis disease disease condition
grenade device weapon device
baikonur site city station
corruption tool crime activity
Table 7: Sample of decisions made by hu-
man judge and our best performing model
(TheY+OtherY+WN) on the generation task.
pernym from the model ?OtherY?, if not found then
use the hypernym from the model ?TheY?. Given a
definite NP from the backoff model, apply theWord-
Net filtering technique, specifically, choose it as the
correct definite NP if it also occurs as a hypernym in
the WordNet hierarchy of the antecedent.
5.3 Evaluation of Anaphor Generation
We evaluated the resulting algorithms from Section
5.2 on the definite NP prediction task as described
earlier. Table 6 shows the agreement of the algo-
rithm predictions with the human judge as well as
with the definite NP actually observed in the corpus.
It is interesting to see that WordNet by itself per-
forms very poorly on this task since it does not have
any word-specific mechanism to choose the correct
level in the hierarchy and the correct word sense for
selecting the hypernym. However, when combined
with our corpus-based approaches, the agreement
increases substantially indicating that the corpus-
based approaches are effectively filtering the space
of hypernyms that can be used as natural classes.
Likewise, WordNet helps to filter the noisy hyper-
nyms from the corpus predictions. Thus, this inter-
play between the corpus-based and WordNet alo-
rithm works out nicely, resulting in the best model
being a combination of all three individual models
and achieving a substantially better agreement with
both the corpus and human judge than any of the in-
dividual models. Table 7 shows decisions made by
this algorithm on a sample test data.
43
6 Conclusion
This paper provides a successful solution to the
problem of incomplete lexical resources for definite
anaphora resolution and further demonstrates how
the resources built for resolution can be naturally ex-
tended for the less studied task of anaphora genera-
tion. We first presented a simple and noisy corpus-
based approach based on globally modeling head-
word co-occurrence around likely anaphoric definite
NPs. This was shown to outperform a recent ap-
proach byMarkert and Nissim (2005) that makes use
of standard Hearst-style patterns extracting hyper-
nyms for the same task. Even with a relatively small
training corpora, our simple TheY-model was able
to achieve relatively high accuracy, making it suit-
able for resource-limited languages where annotated
training corpora and full WordNets are likely not
available. We then evaluated several variants of this
algorithm based on model combination techniques.
The best combined model was shown to exceed 75%
accuracy on the resolution task, beating any of the
individual models. On the much harder anaphora
generation task, where the stand-alone WordNet-
based model only achieved an accuracy of 4%, we
showed that our algorithms can achieve 35%-47%
accuracy on blind exact-match evaluation, thus mo-
tivating the use of such corpus-based learning ap-
proaches on the generation task as well.
Acknowledgements
Thanks to Charles Schafer for sharing his tools on
POS/Headword tagging for the Gigaword corpus.
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguis-
tics, pages 57?64.
S. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
H. H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, pages 169?174.
D. Connoly, J. D. Burger, and D. S. Day. 1997. A ma-
chine learning approach to anaphoric reference. In
Proceedings of the International Conference on New
Methods in Language Processing, pages 133?144.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2005. En-
glish Gigaword Second Edition. Linguistic Data Con-
sortium, catalog number LDC2005T12.
S. Harabagiu, R. Bunescu, and S. J. Maiorano. 2001.
Text and knowledge mining for coreference resolu-
tion. In Proceedings of the Second Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 55?62.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545.
J. Hobbs. 1978. Resolving pronoun references. Lingua,
44:311?338.
K. Markert and M. Nissim. 2005. Comparing knowl-
edge sources for nominal anaphora resolution. Com-
putational Linguistics, 31(3):367?402.
K. Markert, M. Nissim, and N. N. Modjeska. 2003. Us-
ing the web for nominal anaphora resolution. In Pro-
ceedings of the EACL Workshop on the Computational
Treatment of Anaphora, pages 39?46.
J. Meyer and R. Dale. 2002. Mining a corpus to sup-
port associative anaphora resolution. In Proceedings
of the Fourth International Conference on Discourse
Anaphora and Anaphor Resolution.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111.
M. Poesio, R. Vieira, and S. Teufel. 1997. Resolving
bridging references in unrestricted text. In Proceed-
ings of the ACL Workshop on Operational Factors in
Robust Anaphora, pages 1?6.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Viera. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proccedings of the Third Con-
ference on Language Resources and Evaluation, pages
1220?1224.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004. Learning to resolve bridging references. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 143?150.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
M. Strube, S. Rapp, and C. Mu?ller. 2002. The influ-
ence of minimum edit distance on reference resolution.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 312?
319.
R. Vieira and M. Poesio. 2000. An empirically-based
system for processing definite descriptions. Computa-
tional Linguistics, 26(4):539?593.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Corefer-
ence resolution using competition learning approach.
In Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 176?183.
44
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 199?202,
Prague, June 2007. c?2007 Association for Computational Linguistics
JHU1 : An Unsupervised Approach to Person Name Disambiguation
using Web Snippets
Delip Rao Nikesh Garera David Yarowsky
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218
{delip, ngarera, yarowsky}@cs.jhu.edu
Abstract
This paper presents an approach to person
name disambiguation using K-means clus-
tering on rich-feature-enhanced document
vectors, augmented with additional web-
extracted snippets surrounding the polyse-
mous names to facilitate term bridging. This
yields a significant F-measure improvement
on the shared task training data set. The pa-
per also illustrates the significant divergence
between the properties of the training and
test data in this shared task, substantially
skewing results. Our system optimized on
F0.2 rather than F0.5 would have achieved
top performance in the shared task.
1 Introduction
Being able to automatically distinguish between
John Doe, the musician, and John Doe, the actor, on
the Web is a task of significant importance with ap-
plications in IR and other information management
tasks. Mann and Yarowsky (2004) used bigograph-
ical data annotated with named entitities and per-
form fusion of extracted information across multiple
documents. Bekkerman and McCallum (2005) stud-
ied the problem in a social network setting exploit-
ing link topology to disambiguate namesakes. Al-
Kamha and Embley (2004) used a combination of
attributes (like zipcodes, state, etc.), links, and page
similarity to derive the name clusters while Wan et.
al. (2005) used lexical features and named entities.
2 Approaches
Our framework focuses on the K-means clustering
model using both bag of words as features and vari-
ous augumented feature sets. We experimented with
several similarity functions and chose Pearson?s cor-
relation coefficient1 as the distance measure for clus-
tering. The weights for the features were set to the
term frequency of their respective words in the doc-
ument.2
2.1 Submitted system: Clustering using Web
Snippets
We queried the Google search engine with the
target person names and extracted up to the top
one thousand results. For each result we also
extracted the snippet associated with it. An example
is shown below in Figure 2.1. As can be seen the
Figure 1: Google snippet for ?Dekang Lin?
snippets contain high quality, low noise features that
could be used to improve the performance of the
system. Each snippet was treated as a document and
1This performs better than the standard measures like Eu-
clidean and Cosine with K-means clustering on this data.
2We found that using TF weights instead of TF-IDF weights
gives a better performance on this task.
199
clustered along with the supplied documents. This
process is illustrated in Figure 2. The following
example illustrates how these web snippets can
improve performance by lexical transitivity. In
this hypothetical example, a short test document
contains a Canadian postal code (T6G 2H1) not
found in any of the training documents. However,
there may exist an additional web page not in the
training or test data which contains both this term
and also overlap with other terms in the training data
(e.g. 492-9920), serving as an effective transitive
bridge between the two.
Training Document 1 492-9920, not(T6G 2H1)
Web Snippet 2 both 492-9920, T6G 2H1
Test Document 3 T6G 2H1, not(492-9920)
Thus K-means clustering is likely to cluster the
three documents above together while without this
transitive bridge the association between training
and test documents is much less strong. The final
clustering of the test data is simply a projection with
the training documents and web snippets removed.
Projection of test documents
Initial clusters of web snippets + test documents
Web snippet document
Test document
Figure 2: Clustering using Web Snippets
2.2 Baselines
In this section we describe several trivial baselines:
1. Singletons: A clustering where each cluster
has only one document hence number of clus-
ters is same as the number of documents.
2. One Cluster: A clustering with only one clus-
ter containing all documents.
3. Random: A clustering scheme which parti-
tions the documents uniformly at random into
K clusters, where the value of K were the op-
timal K on the training and test data.
These results are summarized in Table 1. Note that
all average F-scores mentioned in this table and the
rest of the paper are microaverages obtained by av-
eraging the purity and invese purity over all names
and then calculating the F-score.
Train Test
Baseline F0.2 F0.5 F0.2 F0.5
Singletons .676 .511 .843 .730
One Cluster .688 .638 .378 .327
Random .556 .493 .801 .668
Table 1: Baseline performance
2.3 K-means on Bag of Words model
The standard unaugumented Bag of Words model
achieves F0.5 of 0.666 on training data, as shown
in Table 2.
2.4 Part of speech tag features
We then consider only terms that are nouns (NN,
NNP) and adjectives (JJ) with the intuition that
most of the content bearing words and descriptive
words that disambiguate a person would fall in these
classes. The result then improves to 0.67 on the
training data.
2.5 Rich features
Another variant of this system, that we call Rich-
Feats, gives preferential weighting to terms that are
immediately around all variants of the person name
in question, place names, occupation names, and
titles. For marking up place names, occupation
names, and titles we used gazetteer3 lookup with-
out explicit named entity disambiguation. The key-
words that appeared in the HTML tag <META ..>
were also given higher weights. This resulted in an
F0.5 of 0.664.
2.6 Snippets from the Web
The addition of web snippets as described in Sec-
tion 2.1 yeilds a significant F0.5 improvement to
0.72.
3Totalling 19646 terms, gathered from publicly available re-
sources on the web. Further details are available on request.
200
2.7 Snippets and Rich features
This is a combination of the models mentioned in
Sections 2.5 and 2.6. This model combination re-
sulted in a slight degradation of performance over
snippets by themselves on the training data but a
slight improvement on test data.
Model K F0.2 F0.5
Vanilla BOW 10% 0.702 0.666
BOW + PoS 10% 0.706 0.670
BOW + RichFeats 10% 0.700 0.664
Snippets 10 0.721 0.718
Snippets + RichFeats 10 0.714 0.712
Table 2: Performance on Training Data
3 Selection of Parameters
The main parameter for K-means clustering is
choosing the number of clusters, K. We optimized
K over the training data varying K from 10%,
20%,? ? ?,100% of the number of documents as well
as varying absolute K values from 10, 20, ? ? ? to 100
documents.4 The evaluation score of F-measure can
be highly sensitive to this parameter K, as shown
in Table 3. The value of K that gives the best F-
measure on training set using vanilla bag of words
(BOW) model is K = 10%, however we see in Ta-
ble 3 that this value of K actually performs much
worse on the test data as compared to other K val-
ues.
4 Training/Test discrepancy and
re-evaluation using cross validation on
test data
Table 4 compares cluster statistics between the train-
ing and test data. This data was derived from Artiles
et. al (2007). The large difference between aver-
age number of clusters in training and test sets in-
dicates that the parameter K, optimized on training
set cannot be transferred to test set as these two sets
belong to a very different distribution. This can be
emprically seen in Table 3 where applying the best
K on training results in a significant performance
4We discard the training and test documents that have no text
content, thus the absolute value K = 10 and percentage value K
= 10% can result in different K?s, even if name had originally
100 documents to begin with.
drop on test set given this divergence when param-
eters are optimized for F0.5 (although performance
does transfer well when parameters are optimized on
F0.2). This was observed in our primary evaluation
system which was optimized for F0.5 and resulted in
a low official score of F0.5 = .53 and F0.2 = .65.
Train Test
K F0.2 F0.5 F0.2 F0.5
10% .702 .666 .527 .600
20% .716 .644 .617 .630
30% .724 .631 .683 .676
40% .724 .618 .728 .705
50% .732 .614 .762 .724
60% .731 .601 .798 .747
70% .730 .593 .832 .766
80% .732 .586 .855 .773
90% .714 .558 .861 .764
100% .670 .502 .843 .730
Table 3: Selecting the optimal parameter on training
data and application to test data
Thus an interesting question is to measure per-
formance when parameters are chosen on data shar-
ing the distributional character of the test data rather
than the highly divergent training set. To do this, we
used a standard 2-fold cross validation to estimate
clustering parameters from a held-out, alternate-half
portion of the test data5, which more fairly repre-
sents the character of the other half of the test data
than does the very different training data. We di-
vide the test set into two equal halves (taking first
fifteen names alphabetically in one set and the rest
in another). We optimize K on the first half, test
on the other half and vice versa. We report the two
K-values and their corresponding F-measures in Ta-
ble 5 and we also report the average in order to com-
pare it with the results on the test set obtained using
K optimized on training. Further, we also report
what would be oracle best K, that is, if we optimize
K on the entire test data 6. We can see in Table 5
that how optimizing K on a devlopment set with
5This also prevents overfitting as the two halves for training
and testing are disjoint.
6By oracle best K we mean the K obtained by optimizing
over the entire test data. Note that, the oracle best K is just
for comparison because it would be unfair to claim results by
optimizing K on the entire test set, all our claimed results for
different models are based on 2-fold cross validation.
201
same distribution as test set can give us F-measure
in the range of 77%, a significant increase as com-
pared to the F-measure obtained by optimizing K on
given training data. Further, Table 5, also indicates
results by a custom clustering method, that takes the
best K-means clustering using vanilla bag of words
model, retains the largest cluster and splits all the
other clusters into singleton clusters. This method
gives an improved 2-fold F-measure score over the
simple bag of words model, implying that most of
the namesakes in test data have one (or few) domi-
nant cluster and a lot of singleton clusters. Table 6
shows a full enumeration of model variance under
this cross validated test evaluation. POS and Rich-
Feats yield small gains, and a best F0.5 performance
of .776.
Data set cluster size # of clusters
Mean Variance Mean Variance
Train 5.4 144.0 10.8 146.3
Test 3.1 26.5 45.9 574.1
Table 4: Cluster statistics from the test and training
data
Data set K F0.2 F0.5
F0.5 Best K on train 10% .702 .666
F0.2 Best K on train 10 .707 .663
Best K on train 10% .527 .560
applied to test 10 .540 .571
2Fold on Test 80 .847 .748
80% .862 .793
.854* .771*
2Fold on Single 80 .847 .749
Largest Cluster 80 .866 .795
.856* .772*
Oracle on Test 80 .858 .774
Table 5: Comparision of training and test results us-
ing Vanilla Bag-of-words model. The values indi-
cated with * represent the average value.
5 Conclusion
We presented a K-means clustering approach for the
task of person name disambiguation using several
augmented feature sets including HTML meta fea-
tures, part-of-speech-filtered features, and inclusion
of additional web snippets extracted from Google
to facilitate term bridging. The latter showed sig-
nificant empirical gains on the training data. Best
Model K F0.2 F0.5
Vanilla BOW 80/ .847/.862 .749/.793
80% Avg = .854 Avg = .771
BOW + PoS 80%/ .844/.865 .749/.795
80% Avg = .854 Avg = .772
BOW 80%/ .847/.868 .754/.798
RichFeats 80% Avg = .858 Avg = .776
Snippets 50%/ .842/.875 .746/.800
50% Avg = .859 Avg = .773
Snippets + 40%/ .836/.874 .750/.798
RichFeats 50% Avg = .855 Avg = .774
Table 6: Performance on 2Fold Test Data
performance on test data, when parameters are op-
timized for F0.2 on training (Table 3), yielded a top
performing F0.2 of .855 on test data (and F0.5=.773
on test data). We also explored the striking discrep-
ancy between training and test data characteristics
and showed how optimizing the clustering param-
eters on given training data does not transfer well
to the divergent test data. To control for similar
training and test distributional characteristics, we re-
evaluated our test results estimating clustering pa-
rameters from alternate held-out portions of the test
set. Our models achieved cross validated F0.5 of .77-
.78 on test data for all feature combinations, further
showing the broad strong performance of these tech-
niques.
References
Reema Al-Kamha and David W. Embley. 2004. Grouping
search-engine returned citations for person-name queries. In
Proceedings of the 6th annual ACM international workshop
on Web information and data management, pages 96?103.
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2007. Eval-
uation: Establishing a benchmark for the web people search
task. In Proceedings of Semeval 2007, Association for Com-
putational Linguistics.
Ron Bekkerman and Andrew McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In Pro-
ceedings of the 14th international conference on World Wide
Web, pages 463?470.
Gideon S. Mann and David Yarowsky. 2004. Unsupervised
personal name disambiguation. In Proceedings of the sev-
enth conference on Natural language learning (CONLL),
pages 33?40.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding. 2005.
Person resolution in person search results: Webhawk. In
Proceedings of the 14th ACM international conference on
Information and knowledge management, pages 163?170.
202
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129?137,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Translation Lexicon Induction from Monolingual Corpora via
Dependency Contexts and Part-of-Speech Equivalences
Nikesh Garera, Chris Callison-Burch, David Yarowsky
Department of Computer Science, Johns Hopkins University
Baltimore MD, USA
{ngarera,ccb,yarowsky}@cs.jhu.edu
Abstract
This paper presents novel improvements
to the induction of translation lexicons
from monolingual corpora using multilin-
gual dependency parses. We introduce a
dependency-based context model that in-
corporates long-range dependencies, vari-
able context sizes, and reordering. It pro-
vides a 16% relative improvement over
the baseline approach that uses a fixed
context window of adjacent words. Its
Top 10 accuracy for noun translation is
higher than that of a statistical translation
model trained on a Spanish-English par-
allel corpus containing 100,000 sentence
pairs. We generalize the evaluation to
other word-types, and show that the per-
formance can be increased to 18% rela-
tive by preserving part-of-speech equiva-
lencies during translation.
1 Introduction
Recent trends in machine translation illustrate that
highly accurate word and phrase translations can be
learned automatically given enough parallel training
data (Koehn et al, 2003; Chiang, 2007). However,
large parallel corpora exist for only a small frac-
tion of the world?s languages, leading to a bottleneck
for building translation systems in low-density lan-
guages such as Swahili, Uzbek or Punjabi. While
parallel training data is uncommon for such lan-
guages, more readily available resources include
small translation dictionaries, comparable corpora,
and large amounts of monolingual data.
The marked difference in the availability of
monolingual vs parallel corpora has led several
researchers to develop methods for automatically
learning bilingual lexicons, either by using mono-
lingual corpora (Rapp, 1999; Koehn and Knight,
2002; Schafer and Yarowsky, 2002; Haghighi et al,
2008) or by exploiting the cross-language evidence
of closely related ?bridge? languages that have more
resources (Mann and Yarowsky, 2001).
This paper investigates new ways of learning
translations from monolingual corpora. We extend
the Rapp (1999) model of context vector projection
using a seed lexicon. It is based on the intuition that
translations will have similar lexical context, even in
unrelated corpora. For example, in order to translate
the word ?airplane?, the algorithm builds a context
vector which might contain terms such as ?passen-
gers?, ?runway?, ?airport?, etc. and words in tar-
get language that have their translations (obtained
via seed lexicon) in surrounding context can be con-
sidered as likely translations. We extend the basic
approach by formulating a context model that uses
dependency trees. The use of dependencies has the
following advantages:
? Long distance dependencies allow associated
words to be included in the context vector even
if they fall outside of the fixed-window used in
the baseline model.
? Using relationships like parent and child in-
stead of absolute positions alleviates problems
when projecting vectors between languages
with different word orders.
? It achieves better performance than baseline
context models across the board, and better
performance than statistical translation models
on Top-10 accuracy for noun translation when
trained on identical data.
129
We further show that an extension based on part-
of-speech clustering can give similar accuracy gains
for learning translations of all word-types, deepen-
ing the findings of previous literature which mainly
focused on translating nouns (Rapp, 1999; Koehn
and Knight, 2002; Haghighi et al, 2008).
2 Related Work
The literature on translation lexicon induction for
low-density languages falls in to two broad cate-
gories: 1) Effectively utilizing similarity between
languages by choosing a high-resource ?bridge? lan-
guage for translation (Mann and Yarowsky, 2001;
Schafer and Yarowsky, 2002) and 2) Extracting
noisy clues (such as similar context) from mono-
lingual corpora with help of a seed lexicon (Rapp,
1999; Koehn and Knight, 2002; Schafer and
Yarowsky, 2002, Haghighi et al, 2008). The lat-
ter category is more relevant to this work and is ex-
plained in detail below.
The idea of words with similar meaning having
similar contexts in the same language comes from
the Distributional Hypothesis (Harris, 1985) and
Rapp (1999) was the first to propose using context of
a given word as a clue to its translation. Given a Ger-
man word with an unknown translation, a German
context vector is constructed by counting its sur-
rounding words in a monolingual German corpus.
Using an incomplete bilingual dictionary, the counts
of the German context words with known transla-
tions are projected onto an English vector. The pro-
jected vector for the German word is compared to
the vectors constructed for all English words using
a monolingual English corpus. The English words
with the highest vector similarity are treated as trans-
lation candidates. The original work employed a rel-
atively large bilingual dictionary containing approx-
imately 16,000 words and tested only on a small col-
lection of 100 manually selected nouns.
Koehn and Knight (2002) tested this idea on a
larger test set consisting of the 1000 most frequent
words from a German-English lexicon. They also
incorporated clues such as frequency and ortho-
graphic similarity in addition to context. Schafer
and Yarowsky, (2002) independently proposed us-
ing frequency, orthographic similarity and also
showed improvements using temporal and word-
burstiness similarity measures, in addition to con-
text. Haghighi et al, (2008) made use of contex-
tual and orthographic clues for learning a generative
model from monolingual corpora and a seed lexicon.
All of the aforementioned work defines context
similarity in terms of the adjacent words over a win-
dow of some arbitary size (usually 2 to 4 words), as
initially proposed by Rapp (1999). We show that the
model for surrounding context can be improved by
using dependency information rather than strictly re-
lying on adjacent words, based on the success of de-
pendency trees for monolingual clustering and dis-
ambiguation tasks (Lin and Pantel, 2002; Pado and
Lapata, 2007) and the recent developments in multi-
lingual dependency parsing literature (Buchholz and
Marsi, 2006; Nivre et al, 2007).
We further differentiate ourselves from previous
work by conducting a second evaluation which ex-
amines the accuracy of translating all word types,
rather than just nouns. While the straightforward ap-
plication of context-based model gives a lower over-
all accuracy than nouns alone, we show how learn-
ing a mapping of part-of-speech tagsets between the
source and target language can result in comparable
performance to that of noun translation.
3 Translation by Context Vector
Projection
This section details how translations are discovered
from monolingual corpora through context vector
projection. Section 3.1 defines alternative ways of
modeling context vectors, and including baseline
models and our dependency-based model.
The central idea of Rapp?s method for learning
translations is that of context vector projection and
vector similarity. The goodness of semantic ?fit? of
candidate translations is measured as the vector sim-
ilarity between two words. Those vectors are drawn
from two different languages, so the vector for one
word must first be projected onto the language space
of the other. The algorithm for creating, projecting
and comparing vectors is described below, and illus-
trated in Figure 1.
Algorithm:
1. Extract context vectors:
Given a word in source language, say sw, create
a vector using the surrounding context words
and call this reference source vector rssw for
130
Figure 1: Illustration of (Rapp, 1999) model for translating spanish word ?crecimiento (growth)? via dependency context vectors
extracted from respective monolingual corpora as explained in Section 3.1.2
source word sw. The actual composition of this
vector varies depending on how the surround-
ing context is modeled. The context model is
independent of the algorithm, and various mod-
els are explained in later sections.
2. Project reference source vector:
Project all the source vector words contained in
the projection dictionary onto the vector space
for the target language, retaining the counts
from source corpus. This vector now exists in
the target language space and is called the ref-
erence target vector rtsw . This vector may be
sparse, depending on how complete the bilin-
gual dictionary is, because words without dic-
tionary entires will receive zero counts in the
reference target vector.
3. Rank candidates by vector similarity:
For each word twi in the target language a con-
text vector is created using the target language
monolingual corpora as in Step 1. Compute a
similarity score between the context vector of
twi = ?ci1, ci2, ...., cin? and reference target vec-
tor rtsw = ?r1, r2, ...., rn?. The word with the
maximum similarity score t?wi is chosen as thecandidate translation of sw.
The vector similarity can be computed in a
number of ways. Our setup we used cosine
similarity:
t?wi = argmaxtwi ci1?r1+ci2?r2+....+cin?rn?c2i1+c2i2+...+c2in?r21+r22+...+r2n
Rapp (1999) used l1-norm metric after nor-
malizing the vectors to unit length, Koehn and
Knight (2002) used Spearman rank order cor-
relation, and Schafer and Yarowsky (2002) use
cosine similarity. We found that cosine simi-
larity gave the best results in our experimental
conditions. Other similarity measures may be
used equally well.
3.1 Models of Context
We compared several context models. Empirical re-
sults for their ability to find accurate translations are
given in Section 5.
3.1.1 Baseline model
In the baseline model, the context is computed
using adjacent words as in (Rapp,1999; Koehn
and Knight, 2002; Schafer and Yarowsky, 2002;
Haghighi et al, 2008). Given a word in source lan-
guage, say sw, count all its immediate context words
appearing in a window of four words. The counts
are collected seperately for each position by keeping
track of four seperate vectors for positions -2, -1, +1
and +2. Thus each vector is a sparse vector, having
the # of dimensions as the size of source language
vocabulary. Each dimension is also reweighted by
multiplying the inverse document frequency (IDF)
131
Figure 2: Illustration of using dependency trees to model richer contexts for projection
as in the standard TF.IDF weighting scheme1. These
vectors are then concatenated into a single vector,
having dimension four times the size of the vocabu-
lary. This vector is called the reference source vector
rssw for source word sw.
3.1.2 Modeling context using dependency trees
We use dependency parsing to extend the con-
text model. Our context vectors use contexts derived
from head-words linked by dependency trees instead
of using the immediate adjacent lexical words. The
use of dependency trees for modeling contexts has
been shown to help in monolingual clustering tasks
of finding words with similar meaning (Lin and Pan-
tel, 2002) and we show how they can be effectively
used for translation lexicon induction.
Position Adjacent Dependency
Context Context
-2 para camino
-1 el para
+1 y prosperidad, y, el
+2 la econo?mica
Table 1: Contrasting context words derived from the adjacent
vs dependency models for the above example
The four vectors for positions -1, +1, -2 and +2
in the baseline model get mapped to immediate par-
ent (-1), immediate child (+1), grandparent (-2) and
grandchild (+2). An example of using the depen-
dency tree context is shown in Figure 2, and the de-
pendency context is shown in contrast with the ad-
jacent context in Table 1, showing the selection of
more salient words by using the dependencies.
Note that while we are limiting to four positions
in the tree, it does not imply that only a maximum of
four context words are selected since the word can
have multiple immediate children depending upon
the dependency parse of the sentence. Hence, this
approach allows for a dynamic context size, with the
1In order to compute the IDF, while there were no clear doc-
ument boundaries in our corpus, a virtual document boundary
was created by binning after every 1000 words.
number of context words varying with the number of
children and parents at the two levels.
Another advantage of this method is that it al-
leviates the reordering problem as we use tree po-
sitions (consisting of head-words) as compared to
the adjacent position in the baseline context model.
For example, if the source spanish word to be trans-
lated was ?prosperidad?, then in the example shown
in Figure 2, in case of adjacent context, the con-
text word ?econo?mica? will show up in +1 position
in Spanish and -1 position in English (as adjectives
come before nouns in English) but in case of depen-
dency context, the adjective will be the child of noun
and hence will show up in +1 position in both lan-
guages. Thus, we do not need to use a bag of word
model as in Section 3 in order to avoid learning the
explicit mapping that adjectives and nouns in Span-
ish and English are reversed.
4 Experimental Design
For our initial set of experiments we compared sev-
eral different vector-based context models:
? Adjbow ? A baseline model which used bag of
words model with a fixed window of 4 words,
two on either side of the word to be translated.
? Adjposn ? A second baseline that used a fixed
window of 4 words but which took positional
into account.
? Depbow ? A dependency model which did not
distinguish between grandparent, parent, child
and grandparent relations, analogous to the bag
of words model.
? Depposn ? A dependency model which did in-
clude such relationships, and was analogous to
the position-based baseline.
? Depposn + rev ? The above Depposn model ap-
plied in both directions (Spanish-to-English
and English-to-Spanish) using their sum as the
final translation score.
We contrasted the accuracy of the above methods,
which use monolingual corpora, with a statistical
132
model trained on bilingual parallel corpora. We re-
fer to that model as Mosesen-es-100k, because it was
trained using the Moses toolkit (Koehn et al, 2007).
4.1 Training Data
All context models were trained on a Spanish cor-
pus containing 100,000 sentences with 2.13 million
words and an English corpus containing 100,000
sentences with 2.07 million words. The Spanish cor-
pus was parsed using the MST dependency parser
(McDonald et al, 2005) trained using dependency
trees generated from the the English Penn Treebank
(Marcus et al, 1993) and Spanish CoNLL-X data
(Buchholz and Marsi, 2006).
So that we could directly compare against sta-
tistical translation models, our Spanish and English
monolingual corpora were drawn from the Europarl
parallel corpus (Koehn, 2005). The fact that our
two monolingual corpora are taken from a parallel
corpus ensures that the assumption that similar con-
texts are a good indicator of translation holds. This
assumption underlies in all work of translation lex-
icon induction from comparable monolingual cor-
pora, and here we strongly bias toward that assump-
tion. Despite the bias, the comparison of different
context models holds, since all models are trained
on the same data.
4.2 Evaluation Criterion
The models were evaluated in terms of exact-match
translation accuracy of the 1000 most frequent
nouns in a English-Spanish dictionary. The accuracy
was calculated by counting how many mappings ex-
actly match one of the entries in the dictionary. This
evaluation criterion is similar to the setup used by
Koehn and Knight (2002). We compute the Top N
accuracy in the standard way as the number of Span-
ish test words whose Top N English translation can-
didates contain a lexicon translation entry out of the
total number of Spanish words that can be mapped
correctly using the lexicon entries. Thus if ?crec-
imiento, growth? is the correct mapping based on the
lexicon entries, the translation for ?crecimiento? will
be counted as correct if ?growth? occurs in the Top
N English translation candidates for ?crecimiento?.
Note that the exact-match accuracy is a conser-
vative estimate as it is possible that the algorithm
may propose a reasonable translation for the given
camino
Depposn Cntxt Model Adjbow Cntxt Model
way 0.124 intentions 0.22
solution 0.097 way 0.21
steps 0.094 idea 0.20
path 0.093 thing 0.20
debate 0.085 faith 0.18
account 0.082 steps 0.17
means 0.080 example 0.17
work 0.079 news 0.16
approach 0.074 work 0.16
issue 0.073 attitude 0.15
Table 2: Top 10 translation candidates for the spanish word
?camino (way)? for the best adjacent context model (Adjbow)
and best dependency context model (Depposn). The bold English
terms show the acceptable translations.
Figure 3: Precision/Recall curve showing superior perfor-
mance of dependency context model as compared to adjacent
context at different recall points. Precision is the fraction of
tested Spanish words with Top 1 translation correct and Recall
is fraction of the 1000 Spanish words tested upon.
Spanish word but is marked incorrect if it does not
exist in the lexicon. Because it would be intractable
to compare each projected vector against the vectors
for all possible English words, we limited ourselves
to comparing the projected vector from each Spanish
word against the vectors for the 1000 most frequent
English nouns, following along the lines of previ-
ous work (Koehn and Knight, 2002; Haghighi et al,
2008).
5 Results
Table 3 gives the Top 1 and Top 10 accuracy for
each of the models on their ability to translate Span-
ish nouns into English. Examples of the top 10
translations using the best performing baseline and
dependency-based models are shown in Table 2. The
baseline models Adjposn and Adjbow differ in that the
133
Model AccTop 1 AccTop 10
Adjbow 35.3% 59.8%
Adjposn 20.9% 46.9%
Depbow 41.0% 62.0%
Depposn 41.0% 64.1%
Depposn + rev 42.9% 65.5%
Mosesen-es-100k 56.4% 62.7%
Table 3: Performance of various context-based models
learned from monolingual corpora and phrase-table learned
from parallel corpora on Noun translation.
latter disregards the position information in the con-
text vector and simply uses a bag of words instead.
Table 3 shows that Adjbow gains using this simplifi-
cation. A bag of words vector approach pools counts
together, which helps to reduce data sparsity. In
the position based model the vector is four times as
long. Additionally, the bag of words model can help
when there is local re-ordering between the two lan-
guages. For instance, Spanish adjectives often fol-
low nouns whereas in English the the ordering is
reversed. Thus, one can either learn position map-
pings, that is, position +1 for adjectives in Spanish is
the same as position -1 in English or just add the the
word counts from different positions into one com-
mon vector as considered in the bag of words ap-
proach.
Using dependency trees also alleviates the prob-
lem of position mapping between source and target
language. Table 3 shows the performance using the
dependency based models outperforms the baseline
models substantially. Comparing Depbow to Depposn
shows that ignoring the tree depth and treating it as
a bag of words does not increase the performance.
This contrasts with the baseline models. The de-
pendency positions account for re-ordering automat-
ically. The precision-recall curve in Figure 3 shows
that the dependency-based context performs better
than adjancet context at almost all recall levels.
The Mosesen-es-100k model shows the performance
of the statistical translation model trained on a bilin-
gual parallel corpus. While the system performs best
in Top 1 accuracy, the dependency context-based
model that ignores the sentence alignments surpris-
ingly performs better in case of Top 10 accuracy,
showing substantial promise.
While computing the accuracy using the phrase-
table learned from parallel corpora (Mosesen-es-100k),
the translation probabilities from both directions
(p(es|en) and p(en|es)) were used to rank the can-
didates. We also apply the monolingual context-
based model in the reverse direction (from English
to Spanish) and the row with label Depposn + rev in
Table 3 shows further gains using both directions.
Spanish English Sim Is present
Score in lexicon
sen?ores gentlemen 0.99 NO
xenofobia xenophobia 0.87 YES
diversidad diversity 0.73 YES
chipre cyprus 0.66 YES
mujeres women 0.65 YES
alemania germany 0.65 YES
explotacio?n exploitation 0.63 YES
hombres men 0.62 YES
repu?blica republic 0.60 YES
racismo racism 0.59 YES
comercio commerce 0.58 YES
continente continent 0.53 YES
gobierno government 0.52 YES
israel israel 0.52 YES
francia france 0.52 YES
fundamento certainty 0.51 NO
suecia sweden 0.50 YES
tra?fico space 0.49 NO
televisio?n tv 0.48 YES
francesa portuguese 0.48 NO
Table 4: List of 20 most confident mappings using the de-
pendency context based model for noun translation. Note that
although the first mapping is the correct one, it was not present
in the lexicon used for evaluation and hence is marked as incor-
rect.
6 Further Extensions: Generalizing to
other word types via tagset mapping
Most of the previous literature on this problem fo-
cuses on evaluating on nouns (Rapp, 1999; Koehn
and Knight 2002; Haghighi et al, 2008). However
the vector projection approach is general, and should
be applicable to other word-types as well. We eval-
uated the models with new test set containing 1000
most frequent words (not just nouns) in the English-
Spanish lexicon.
We used the dependency-based context model to
create translations for this new set. The row labeled
Depposn in Table 5 shows that the accuracy on this
set is lower when compared to evaluating only on
nouns. The main reason for lower accuracy is that
closed class words are often the most frequent and
tend to have a wide range of contexts resulting in
reasonable translation for most words include open
class words via the context model. For instance, the
English preposition ?to? appears as the most confi-
dent translation for 147 out of the 1000 Spanish test
134
Figure 4: Illustration of using part-of-speech tag mapping to
restrict candidate space of translations
words and in none (rightly so) after restricting the
translations by part-of-speech categories.
This problem can be greatly reduced by making
use of the intuition that part-of-speech is often pre-
served in translation, thus the space of possible can-
didate translation can be largely reduced based on
the part-of-speech restrictions. For example, a noun
in source language will usually be translated as noun
in target language, determiner will be translated as
determiner and so on. This idea is more clearly il-
lustrated in in Figure 4. We do not impose a hard
restriction but rather compute a ranking based on
the conditional probability of candidate translation?s
part-of-speech tag given source word?s tag.
An interesting problem in using part-of-speech re-
strictions is that corpora in different languages have
been tagged using widely different tagsets and the
following subsection explains this problem in detail:
6.1 Mapping Part-of-Speech tagsets in
different languages
The English tagset was derived from the Penn tree-
bank consisting of 53 tags (including punctuation
markers) and the Spanish tagset was derived from
the Cast3LB dataset consisting of 57 tags but there
is a large difference in the morphological and syn-
tactic features marked by the tagset. For example,
the Spanish tagset as different tags for masculine and
feminine nouns and also has a different tag for coor-
dinated nouns, all of which need to be mapped to the
singular or plural noun category available in English
tagset. Figure 5 shows an illustration of the mapping
problem between the Spanish and English POS tags.
Figure 5: Illustration of mapping Spanish part-of-speech
tagset to English tagset. The tagsets vary greatly in notation and
the morphological/syntactic constituents represented and need
to be mapped first, using the algorithm described in Section 6.1.
We now describe an empirical approach for learn-
ing the mapping between tagsets using the English-
Spanish projection dictionary used in the monolin-
gual context-based models for translation. Given a
small English-Spanish bilingual dictionary and a n-
best list of part-of-speech tags for each word in the
dictionary2, we compute conditional probability of
translating a source word with pos tag sposi to a tar-
get with pos tag tposj as follows:
p(tposj |sposi) =
c(sposi , tposj )
c(sposi) =?
sw?S, tw?T p(sposi |sw) ? p(tposj |tw) ? Idict(sw, tw)?
sw?S p(sposi |sw)
where
? S and T are the source and target vocabulary in
the seed dictionary, with sw and tw being any
of the words in the respective sets.
? p(sposi |sw), p(tposj |tw) are obtained using rel-
ative frequencies in a part-of-speech tagged
corpus in the source/target languages respec-
tively, and are used as soft counts.
? Idict(sw, tw) is the indicator function with
value 1 if the pair (sw, tw) occurs in the seed
dictionary and 0 otherwise.
In essence, the mapping between tagsets is
learned using the known translations from a small
dictionary.
Given a source word sw to translate, its most
likely tag s?pos, and the most likely mapping of this
tag into English t?pos computed as above, the transla-
tion candidates with part-of-speech tag t?pos are con-
sidered for comparison with vector similarity and
2The n-best part-of-speech tag list for any word in the dic-
tionary was derived using the relative frequencies in a part-of-
speech annotated corpora in the respective languages
135
Figure 6: Precision/Recall curve showing superior perfor-
mance of using part-of-speech equivalences for translating all
word-types. Precision is the fraction of tested Spanish words
with Top 1 translation correct and Recall is fraction of the 1000
Spanish words tested upon.
the other candidates with tposj 6= t?pos are discarded
from the candidate space. Figure 4 shows an exam-
ple of restricting the candidate space using POS tags.
Model AccTop 1 AccTop 10
Depposn 35.1% 62.9%
+ POS 41.3% 66.4%
Table 5: Performance of dependency context-based model
along with addition of part-of-speech mapping model on trans-
lating all word-types.
The row labeled +POS in Table 5 shows the part-
of-speech tags provides substantial gain as com-
pared to direct application of dependency context-
based model and is also comparable to the accuracy
obtained evaluating just on nouns in Table 3.
7 Conclusion
This paper presents a novel contribution to the stan-
dard context models used when learning transla-
tion lexicons from monolingual corpora by vector
projection. We show that using contexts based on
dependency parses can provide more salient con-
texts, allow for dynamic context size, and account
for word reordering in the source and target lan-
guage. An exact-match evaluation shows 16% rela-
tive improvement by using a dependency-based con-
text model over the standard approach. Furthermore,
we show that our model, which is trained only on
monolingual corpora, outperforms the standard sta-
Spanish English Sim Is present
Score in lexicon
sen?ores gentlemen 0.99 NO
chipre cyprus 0.66 YES
mujeres women 0.65 YES
alemania germany 0.65 YES
hombres men 0.62 YES
expresar express 0.60 YES
racismo racism 0.59 YES
interior internal 0.55 YES
gobierno government 0.52 YES
francia france 0.52 YES
cultural cultural 0.51 YES
suecia sweden 0.50 YES
fundamento basis 0.48 YES
francesa french 0.48 YES
entre between 0.47 YES
origen origin 0.46 YES
tra?fico traffic 0.45 YES
de of 0.44 YES
social social 0.43 YES
ruego thank 0.43 NO
Table 6: List of 20 most confident mappings using the depen-
dency context with the part-of-speech mapping model translat-
ing all word-types. Note that although the second best mapping
in Table4 for noun-translation is for xenofobia with score 0.87,
xenofobia is not among the 1000 most frequent words (of all
word-types) and thus is not in this test set.
tistical MT approach to learning phrase tables when
trained on the same amount of sentence-aligned par-
allel corpora, when evaluated on Top 10 accuracy.
As a second contribution, we go beyond previ-
ous literature which evaluated only on nouns. We
showed how preserving a word?s part-of-speech in
translation can improve performance. We further
proposed a solution to an interesting sub-problem
encountered on the way. Since part-of-speeech
tagsets are not identical across two languages, we
propose a way of learning their mapping automat-
ically. Restricting candidate space based on this
learned tagset mapping resulted in 18% improve-
ment over the direct application of context-based
model to all word-types.
Dependency trees help improve the context for
translation substantially and their use opens up the
question of how the context can be enriched further
making use of the hidden structure that may provide
clues for a word?s translation. We also believe that
the problem of learning the mapping between tagsets
in two different languages can be used in general for
other NLP tasks making use of projection of words
and its morphological/syntactic properties between
languages.
136
References
S. Buchholz and E. Marsi. 2006. Conll-X shared task
on multilingual dependency parsing. Proceedings of
CoNLL, pages 189?210.
Y. Cao and H. Li. 2002. Base Noun Phrase translation
using web data and the EM algorithm. Proceedings of
COLING-Volume 1, pages 1?7.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
P. Fung and L.Y. Yee. 1998. An IR Approach for
Translating New Words from Nonparallel, Compara-
ble Texts. Proceedings of ACL, 36:414?420.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. Proceedings of ACL-HLT, pages 771?779.
Z. Harris. 1985. Distributional structure. Katz, J. J. (ed.),
The Philosophy of Linguistics, pages 26?47.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. Proceedings of
ACL Workshop on Unsupervised Lexical Acquisition,
pages 9?16.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL-
HLT, pages 48?54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
of ACL, companian volume, pages 177?180.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. MT Summit X.
D. Lin and P. Pantel. 2002. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(04):343?360.
G.S. Mann and D. Yarowsky. 2001. Multipath transla-
tion lexicon induction via bridge languages. Proceed-
ings of NAACL, pages 151?158.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19(2):313?330.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. Proceedings of EMNLP-HLT, pages
523?530.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007
shared task on dependency parsing. Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL,
pages 915?932.
S. Pado and M. Lapata. 2007. Dependency-Based Con-
struction of Semantic Space Models. Computational
Linguistics, 33(2):161?199.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
Proceedings of ACL, pages 519?526.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. Proceedings of COLING, pages 1?7.
137
