Back Transliteration from Japanese to English 
Using Target English Context  
Isao Goto?, Naoto Kato??, Terumasa Ehara???, and Hideki Tanaka? 
?NHK Science and Technical 
Research Laboratories  
1-11-10 Kinuta, Setagaya,  
Tokyo, 157-8510, Japan 
goto.i-es@nhk.or.jp 
tanaka.h-ja@nhk.or.jp 
??ATR Spoken Language Trans-
lation Research Laboratories 
2-2-2 Hikaridai, Keihanna  
Science City, Kyoto, 619-0288, 
Japan 
naoto.kato@atr.jp 
???Tokyo University of  
Science, Suwa  
5000-1, Toyohira, Chino,  
Nagano, 391-0292, Japan 
eharate@rs.suwa.tus.
ac.jp 
 
Abstract 
This paper proposes a method of automatic 
back transliteration of proper nouns, in which 
a Japanese transliterated-word is restored to 
the original English word. The English words 
are created from a sequence of letters; thus 
our method can create new English words that 
are not registered in dictionaries or English 
word lists. When a katakana character is con-
verted into English letters, there are various 
candidates of alphabetic characters. To ensure 
adequate conversion, the proposed method 
uses a target English context to calculate the 
probability of an English character or string 
corresponding to a Japanese katakana charac-
ter or string. We confirmed the effectiveness 
of using the target English context by an ex-
periment of personal-name back translitera-
tion.  
1 Introduction 
In transliteration, a word in one language is con-
verted into a character string of another language 
expressing how it is pronounced. In the case of 
transliteration into Japanese, special characters 
called katakana are used to show how a word is 
pronounced. For example, a personal name and 
its transliterated word are shown below.  
Cunningham         ?????
(ka ni n ga mu)
[Transliteration]
 
Here, the italic alphabets are romanized Japanese 
katakana characters.  
New transliterated words such as personal 
names or technical terms in katakana are not al-
ways listed in dictionaries. It would be useful for 
cross-language information retrieval if these 
words could be automatically restored to the 
original English words.  
Back transliteration is the process of restoring 
transliterated words to the original English words. 
Here is a problem of back transliteration.  
?                ?????????
(English word) (ku ra cchi fi ? ru do)
[Back transliteration]
 
There are many ambiguities to restoring a 
transliterated katakana word to its original Eng-
lish word. For example, should "a" in "ku ra cchi 
fi ? ru do" be converted into the English letter of 
"a" or "u" or some other letter or string? Trying 
to resolve the ambiguity is a difficult problem, 
which means that back transliteration to the cor-
rect English word is also difficult.  
Using the pronunciation of a dictionary or lim-
iting output English words to a particular English 
word list prepared in advance can simplify the 
problem of back transliteration. However, these 
methods cannot produce a new English word that 
is not registered in a dictionary or an English 
word list. Transliterated words are mainly proper 
nouns and technical terms, and such words are 
often not registered. Thus, a back transliteration 
framework for creating new words would be very 
useful.  
A number of back transliteration methods for 
selecting English words from an English pronun-
ciation dictionary have been proposed. They in-
clude Japanese-to-English (Knight and Graehl, 
1998) 1 , Arabic-to-English (Stalls and Knight, 
                                                          
1 Their English letter-to-sound WFST does not convert Eng-
lish words that are not registered in a pronunciation diction-
ary.  
1998), and Korean-to-English (Lin and Chen, 
2002).  
There are also methods that select English 
words from an English word list, e.g., Japanese-
to-English (Fujii and Ishikawa, 2001) and Chi-
nese-to-English (Chen et al, 1998).  
Moreover, there are back transliteration meth-
ods capable of generating new words, there are 
some methods for back transliteration from Ko-
rean to English (Jeong et al, 1999; Kang and 
Choi, 2000).  
These previous works did not take the target 
English context into account for calculating the 
plausibility of matching target characters with the 
source characters.  
This paper presents a method of taking the tar-
get English context into account to generate an 
English word from a Japanese katakana word. 
Our character-based method can produce new 
English words that are not listed in the learning 
corpus.  
This paper is organized as follows. Section 2 
describes our method. Section 3 describes the 
experimental set-up and results. Section 4 dis-
cusses the performance of our method based on 
the experimental results. Section 5 concludes our 
research. 
2 Proposed Method 
2.1 Advantage of using English context 
First we explain the difficulty of back translitera-
tion without a pronunciation dictionary. Next, we 
clarify the reason for the difficulty. Finally, we 
clarify the effect using English context in back 
transliteration.  
In back transliteration, an English letter or 
string is chosen to correspond to a katakana char-
acter or string. However, this decision is difficult. 
For example, there are cases that an English letter 
"u" corresponds to "a" of katakana, and there are 
cases that the same English letter "u" does not 
correspond to the same "a" of katakana. "u" in 
Cunningham corresponds to "a" in katakana and 
"u" in Bush does not correspond to "a" in kata-
kana. It is difficult to resolve this ambiguity 
without the pronunciation registered in a diction-
ary.  
The difference in correspondence mainly 
comes from the difference of the letters around 
the English letter "u." The correspondence of an 
English letter or string to a katakana character or 
string varies depending on the surrounding char-
acters, i.e., on its English context.  
Thus, our back transliteration method uses the 
target English context to calculate the probability 
of English letters corresponding to a katakana 
character or string.  
2.2 Notation and conversion-candidate 
lattice  
We formulate the word conversion process as a 
unit conversion process for treating new words. 
Here, the unit is one or more characters that form 
a part of characters of the word.  
A katakana word, K, is expressed by equation 
2.1 with "^" and "$" added to its start and end, 
respectively.  
1
0 0 1 1...
m
mk k k k
+
+= =K  (2.1) 
0 ^k = , 1 $mk + =  (2.2) 
where jk  is the j-th character in the katakana 
word, and m is the number of characters except 
for "^" and "$" and 10
mk +  is a character string 
from 0k  to 1mk + .  
We use katakana units constructed of one or 
more katakana characters. We denote a katakana 
unit as ku. For any ku, many English units, eu, 
could be corresponded as conversion-candidates. 
The ku's and eu's are generated using a learning 
corpus in which bilingual words are separated 
into units and every ku unit is related an eu unit.  
{ }EL  denotes the lattice of all eu's correspond-
ing to ku's covering a Japanese word. Every eu is 
a node of the lattice and each node is connected 
with next nodes. { }EL  has a lattice structure start-
ing from "^" and ending at "$." Figure 1 shows an 
example of { }EL  corresponding to a katakana 
word "?????????(ki ru shu shu ta i 
n)." In the figure, each circle represents one eu. 
A character string linking individual character 
units in the paths 1 2( , ,.., )d qp p p p?  between "^" 
and "$" in { }EL  becomes a conversion candidate, 
where q is the number of paths between "^" and 
"$" in { }EL . 
We get English word candidates by joining eu's 
from "^" to "$" in { }EL . We select a certain path, 
pd, in { }EL . The number of character units 
cchi
?(ki)
chi
ci
cki
cy
k
ke
khi
ki
kie
kii
ky
qui
ch
che
chou
chu
s
sc
sch
schu
sh
?(ru) ??(shu) ??(shu) ?(ta) ?(i) ?(n)
ta
tad
tag
te
ter
tha
ti
to
tta
tu
e
hi
i
ji
y
yeh
yi
m
mon
mp
n
ne
ng
ngh
nin
nn
nne
nt
nw
t
??(tai)
l
ld
le
les
lew
ll
lle
llu
lou
lu
r
rc
rd
re
rg
roo
rou
rr
rre
rt
lu
she
shu
su
sy
sz
ch
che
chou
chu
s
sc
sch
schu
sh
she
shu
su
sy
sz
taj
tay
tey
ti
tie
ty
tye? ? ?
? ? ?
? ? ? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
^ $
 
Figure 1: Example of lattice { }EL  of conversion candidates units. 
 
except for "^" and "$" in pd is expressed as ( )dn p . 
The character units in pd are numbered from start 
to end.  
The English word, E, resulting from the con-
version of a katakana word, K, for pd is expressed 
as follows: 
1
0 0 1 1..
m
mk k k k
+
+= =K  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= ku = ku ku ku , (2.3) 
( ) 1
0 0 1 ( ) 1..
d
d
l p
l pe e e e
+
+= =E  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= eu = eu eu eu , (2.4) 
0 0 0 0 ^k e= = = =ku eu ,  
1 ( ) 1 ( ) 1 ( ) 1 $d d dm l p n p n pk e+ + + += = = =ku eu ,  (2.5) 
where ej is the j-th character in the English word. 
( )dl p  is the number of characters except for "^" 
and "$" in the English word. ( ) 10 d
n p +eu  for each pd 
in { }EL  in equation 2.4 becomes the candidate 
English word. ( ) 10 d
n p +ku  in equation 2.3 shows the 
sequence of katakana units. 
2.3 Probability models using target Eng-
lish context 
To determine the corresponding English word for 
a katakana word, the following equation 2.6 must 
be calculated: 
? arg max ( | )P
E
E = E K . (2.6) 
Here, E?  represents an output result. 
To use the English context for calculating the 
matching of an English unit with a katakana unit, 
the above equation is transformed into Equation 
2.7 by using Bayes? theorem. 
? arg max ( ) ( | )P P=
E
E E K E  (2.7) 
Equation 2.7 contains a translation model in 
which an English word is a condition and kata-
kana is a result.  
The word in the translation model ( | )P K E  in 
Equation 2.7 is broken down into character units 
by using equations 2.3 and 2.4. 
{
}
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1 ( ) 1
0 0 0
? arg max ( )
( , , | )
arg max ( )
( | , , )
( | , ) ( | )
d d
n p n pd d
d d
n p n pd d
d d d
n p n p
n p n p
n p n p n p
P
P
P
P
P P
+ +
+ +
+ +
+ +
+ + +
=
?
=
?
?
? ?
? ?
E
eu ku
E
eu ku
E E
K ku eu E
E
K ku eu E
ku eu E eu E
 (2.8) 
( ) 1
0
dn p +eu  includes information of E. K is only 
affected by ( ) 1
0
dn p +ku . Thus equation 2.8 can be 
rewritten as follows:  
( ) 1 ( ) 1
0 0
( ) 1
0
( ) 1 ( ) 1 ( ) 1
0 0 0 .
? argmax ( )
( | )
( | ) ( | )
d
n p n pd d
d d d
n p
n p n p n p
P
P
P P
+ +
+
+ + +
???
???
=
?
?
? ?
E
eu ku
E E
K ku
ku eu eu E
 (2.9) 
( ) 1
0( | )
dn pP +K ku  is 1 when the string of K and 
( ) 1
0
dn p +ku  is the same, and the strings of the 
( ) 1
0
dn p +ku  of all paths in the lattice and the string 
of the K is the same. Thus, ( ) 1
0( | )
dn pP +K ku  is al-
ways 1.  
We approximate the sum of paths by selecting 
the maximum path.  
( ) 1 ( ) 1
0 0
( ) 1
0
? arg max ( ) ( | )
( | )
d d
d
n p n p
n p
P P
P
+ +
+
?
?
E
E E ku eu
eu E
 
 (2.10) 
We show an instance of each probability 
model with a concrete value as follows: 
 
( )
(^Crutchfield$)
P
P
E , 
 
( ) 1
0( | )
(^ | ^ / )
( ) ( / / / / / )
dn pP
P
ku ra cchi fi ru do ku ra cchi fi ru do
+
? ?
K ku
?????????$ ?/?/??/???/?/?/$ , 
 
( ) 1 ( ) 1
0 0( | )
(^ / | ^ / C/ru/tch/fie/l/d / $)
( / / / / / )
d dn p n pP
P
ku ra cchi fi ru do
+ +
?
ku eu
?/?/??/???/?/?/$ , 
 
( ) 1
0( | )
(^ / C/ru/tch/fie/ld / $ | ^Crutchfield$)
dn pP
P
+eu E . 
 
We broke down the language model ( )P E  in 
equation 2.10 into letters.  
( ) 1
1
1
( | )( )
dl p
j
j j a
j
P e eP
+
?
?
=
? ?E  (2.11) 
Here, a is a constant. Equation 2.11 is an (a+1)-
gram model of English letters.  
Next, we approximate the translation model 
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  and the chunking model 
( ) 1
0( | )
dn pP +eu E . For this, we use our previously 
proposed approximation technique (Goto et al, 
2003). The outline of the technique is shown as 
follows.  
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  is approximated by reduc-
ing the condition.  
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | , )
d d
d
d
n p n p
n p
n pi
i
i
P
P
+ +
+
+?
=
= ?
ku eu
ku ku eu
 
( ) 1
( ) 1
( ) ( ) 1
1
( | , , )
dn p
start i
i start i b i end i
i
P e e
+
?
? +
=
? ? ku eu
 (2.12)
 
where start(i) is the first position of the i-th char-
acter unit eui, while end(i) is the last position of 
the i-th character unit eui; and b is a constant. 
Equation 2.12 takes English context ( ) 1( )
start i
start i be
?
?  and 
( ) 1end ie +  into account.  
Next, the chunking model ( ) 10( | )d
n pP +eu E  is 
transformed. All chunking patterns of ( ) 10 d
l pe +=E  
into ( ) 10 d
n p +eu  are denoted by each l(pd)+1 point 
between l(pd)+2 characters that serve or do not 
serve as delimiters. eu0 and ( ) 1dn p +eu  are deter-
mined in advance. l(pd)-1 points remain ambigu-
ous. We represent the value that is delimiter or is 
non-delimiter between ej and ej+1 by zj. We call 
the zj delimiter distinction.  { delimiternon-delimiterjz =  (2.13) 
Here, we show an example of English units us-
ing zj.  
(e1 e2 e3 e4  e5  e6 e7 e8 e9  e10 e11)
C r u t c h f i e l d
(z1  z2  z3  z4  z5  z6 z7  z8  z9  z10)
/ / / /
1  0  1  0  0  1  0  0  1  1
English:
Values of zj:
/
 
 
In this example, a delimiter of zj is represented by 
1 and a non-delimiter is represented by 0.  
The chunking model is transformed into a 
processing per character by using zj. And we re-
duce the condition.  
( ) 1
0
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | )
( | , )
d
d d
d
d
n p
l p l p
l p
l pj
j
j
P
P z e
P z z e
+
? +
?
+?
=
=
= ?
eu E
 
( ) 1
1 1
1
1
( | , )
dl p
j j
j j c j c
j
P z z e
?
? +
? ? ?
=
? ?  (2.14) 
The conditional information of the English 
1j
j ce
+
?  is as many as c characters and 1 character 
before and after zj, respectively. The conditional 
information of 1 1
j
j cz
?
? ?  is as many as c+1 delimiter 
distinctions before zj.  
By using equation 2.11, 2.12, and 2.14, equa-
tion 2.10 becomes as follows:  
( ) 1
1
1
( )
( ) 1
( ) ( ) 1
1
( ) 1
1 1
1
1
? arg max ( | )
( | , , )
( | , ).
d
d
d
l p
j
j j a
j
n p
start i
i start i b i end i
i
l p
j j
j j c j c
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ? ?
=
?
?
?
?
?
?
E
E
ku eu
 (2.15) 
Equation 2.15 is the equation of our back 
transliteration method.  
2.4 Beam search solution for context 
sensitive grammar 
Equation 2.15 includes context-sensitive gram-
mar. As such, it can not be carried out efficiently. 
In decoding from the head of a word to the tail, 
eend(i)+1 in equation 2.15 becomes context-
sensitive. Thus we try to get approximate results 
by using a beam search solution. To get the re-
sults, we use dynamic programming. Every node 
of eu in the lattice keeps the N-best results evalu-
ated by using a letter of eend(i)+1 that gives the 
maximum probability in the next letters. When 
the results of next node are evaluated for select-
ing the N-best, the accurate probabilities from the 
previous nodes are used.  
2.5 Learning probability models based 
on the maximum entropy method 
The probability models are learned based on the 
maximum entropy method. This makes it possi-
ble to prevent data sparseness relating to the 
model as well as to efficiently utilize many con-
ditions, such as context, simultaneously. We use 
the Gaussian Prior (Chen and Rosenfeld, 1999) 
smoothing method for the language model. We 
use one Gaussian variance. We use the value of 
the Gaussian variance that minimizes the test 
set's perplexity.  
The feature functions of the models based on 
the maximum entropy method are defined as 
combinations of letters. In addition, we use 
vowel, consonant, and semi-vowel classes for the 
translation model. We manually define the com-
binations of the letter positions such as ej and ej-1. 
The feature functions consist of the letter combi-
nations that meet the combinations of the letter 
positions and are observed at least once in the 
learning data.  
2.6 Corpus for learning 
A Japanese-English word list aligned by unit was 
used for learning the translation model and the 
chunking model and for generating the lattice of 
conversion candidates. The alignment was done 
by semi-automatically. A romanized katakana 
character usually corresponds to one or several 
English letters or strings. For example, a roman-
ized katakana character "k" usually corresponds 
to an English letter "c," "k," "ch," or "q." With 
such heuristic rules, the Japanese-English word 
corpus could be aligned by unit and the align-
ment errors were corrected manually.  
3 Experiment 
3.1 Learning data and test data 
We conducted an experiment on back translitera-
tion using English personal names. The learning 
data used in the experiment are described below. 
The Dictionary of Western Names of 80,000 
People2 was used as the source of the Japanese-
English word corpus. We chose the names in al-
phabet from A to Z and their corresponding kata-
kana. The number of distinct words was 39,830 
for English words and 39,562 for katakana words. 
The number of English-katakana pairs was 
83,0573. We related the alphabet and katakana 
character units in those words by using the 
method described in section 2.6. We then used 
the corpus to make the translation and the chunk-
ing models and to generate a lattice of conversion 
candidates. 
The learning of the language model was car-
ried out using a word list that was created by 
merging two word lists: an American personal-
                                                          
2 Published by Nichigai Associates in Japan in 1994.  
3  This corpus includes many identical English-katakana 
word pairs.  
name list4, and English head words of the Dic-
tionary of Western Names of 80,000 people. The 
American name list contains frequency informa-
tion for each name; we also used the frequency 
data for the learning of the language model. A 
test set for evaluating the value of the Gaussian 
variance was created using the American name 
list. The list was split 9:1, and we used the larger 
data for learning and the smaller data for evaluat-
ing the parameter value.  
The test data is as follows. The test data con-
tained 333 katakana name words of American 
Cabinet officials, and other high-ranking officials, 
as well as high-ranking governmental officials of 
Canada, the United Kingdom, Australia, and 
New Zealand (listed in the World Yearbook 2002 
published by Kyodo News in Japan). The English 
name words that were listed along with the corre-
sponding katakana names were used as answer 
words. Words that included characters other than 
the letters A to Z were excluded from the test 
data. Family names and First names were not 
distinguished.  
3.2 Experimental models 
We used the following methods to test the indi-
vidual effects of each factor of our method.  
? Method A 
Used a model that did not take English context 
into account. The plausibility is expressed as fol-
lows: 
( )
1
? arg max ( | )
dn p
i i
i
P
=
= ?
E
E eu ku . (3.1) 
? Method B 
Used our language model and a translation model 
that did not consider English context. The con-
stant a = 3 in the language model. The plausibil-
ity is expressed as follows: 
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu . 
 (3.2) 
? Method C 
Applied our chunking model to method B, with c 
= 3 in the chunking model. The plausibility is 
expressed as follows: 
                                                          
4  Prepared from the 1990 Census conducted by the U.S. 
Department of Commerce. Available at 
http://www.census.gov/genealogy/names/ . The list includes 
91,910 distinct words.  
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu  
( ) 1
1 4
4 3
1
( | , ).
dl p
j j
j j j
j
P z z e
?
? +
? ?
=
? ?  (3.3) 
? Method D 
Used our translation model that considered Eng-
lish context, but not the chunking model. b = 3 in 
the translation model. The plausibility is ex-
pressed as follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
? arg max ( | )
| , , .
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
P e e
P e e
+
?
?
=
?
? +
=
=
?
?
?
E
E
ku eu
 (3.4) 
? Method E 
Used our language model, translation model, and 
chunking model. The plausibility is expressed as 
follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
( ) 1
1 4
4 3
1
? arg max ( | )
| , ,
( | , ).
d
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
l p
j j
j j j
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ?
=
=
?
?
?
?
?
E
E
ku eu  
 (3.5) 
3.3 Results 
Table 1 shows the results of the experiment5 on 
back transliteration from Japanese katakana to 
English. The conversion was determined to be 
successful if the generated English word agreed 
perfectly with the English word in the test data. 
Table 2 shows examples of back transliterated 
words.  
Method A B C D E 
Top 1 23.7 57.4 61.6 63.1 66.4
Top 2 34.8 69.1 72.4 71.8 74.2
Top 3 42.9 73.6 76.6 75.4 79.3
Top 5 54.1 77.5 79.9 80.8 83.5
Top 10 63.4 82.0 85.3 86.5 87.7
Table 1: Ratio (%) of including the answer word 
in high-ranking words.  
                                                          
5 For model D and E, we used N=50 for the beam search 
solution. In addition, we kept paths that represented parts of 
words existing in the learning data.  
Japanese katakana 
(romanized katakana) 
Created English 
??????? 
(a shu ku ro fu to) 
Ashcroft 
????????? 
(ki ru shu shu ta i n) 
Kirschstein 
????? 
(su pe n sa -) 
Spencer 
 
???? 
(pa u e ru) 
Powell 
 
????? 
(pu ri n shi pi) 
Principi 
 
Table 2: Example of English words produced.  
4 Discussion 
The correct-match ratio of the method E for the 
first-ranked words was 66%. Its correct-match 
ratio for words up to the 10th rank was 87%.  
Regarding the top 1 ranked words, method B 
that used a language model increase the ratio 33-
points from method A that did not use a language 
model. This demonstrates the effectiveness of the 
language model. 
Also for the top 1 ranked words, method C 
which adopted the chunking model increase the 
ratio 4-points from method B that did not adopt 
the chunking model in the top 1 ranked words. 
This indicates the effectiveness of the chunking 
model. 
Method D that used a translation model taking 
English context into account had a ratio 5-points 
higher in top 1 ranked words than that of method 
B that used a translation model not taking Eng-
lish context into account. This demonstrates the 
effectiveness of the language model.  
Method E gave the best ratio. Its ratio for the 
top 1 ranked word was 42-points higher than 
method A's.  
These results demonstrate the effectiveness of 
using English context for back transliteration.  
5 Conclusion 
This paper described a method for Japanese to 
English back transliteration. Unlike conventional 
methods, our method uses a target English con-
text to calculate the plausibility of matching be-
tween English and katakana. Our method can 
treat English words that do not exist in learning 
data. We confirmed the effectiveness of our 
method in an experiment using personal names. 
We will apply this technique to cross-language 
information retrieval.  
References  
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, 
and Shih-Chung Tsai. 1998. Proper Name Transla-
tion in Cross-Language Information Retrieval. 36th 
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference 
on Computational Linguistics, pp.232-236. 
Stanley F. Chen, Ronald Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMU-CS-99-108, Carnegie Mel-
lon University.  
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating Names and Technical Terms in Arabic Text. 
COLING/ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Teru-
masa Ehara. 2003. Transliteration Considering 
Context Information based on the Maximum En-
tropy Method. Machine Translation Summit IX, 
pp.125-132. 
Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung Lee, 
and Key-Sun Choi. 1999. Automatic Identification 
and Back-Transliteration of Foreign Words for In-
formation Retrieval. Information Processing and 
Management, Vol.35, No.4, pp.523-540. 
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic 
Transliteration and Back-Transliteration by Deci-
sion Tree Learning. International Conference on 
Language Resources and Evaluation. pp.1135-1411. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, Vol.24, 
No.4, pp.599-612. 
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward 
Machine Transliteration by Learning Phonetic 
Similarity. 6th Conference on Natural Language 
Learning, pp.139-145.  
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japa-
nese/English Cross-Language Information Re-
trieval: Exploration of Query Translation and 
Transliteration. Computers and the Humanities, 
Vol.35, No.4, pp.389-420. 
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39?47,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Syntax-Driven Sentence Revision for Broadcast News Summarization 
 
Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa, 
Tadashi Kumano and Naoto Kato 
NHK Science and Technology Research Labs. 
1-10-11, Kinuta, Setagaya-ku, Tokyo, Japan 
{tanaka.h-ja,kinoshita.a-ek,kobayakawa-t.ko,kumano.t-eq,kato.n-ga}@nhk.or.jp 
 
Abstract 
We propose a method of revising lead sentences in 
a news broadcast. Unlike many other methods pro-
posed so far, this method does not use the corefer-
ence relation of noun phrases (NPs) but rather, 
insertion and substitution of the phrases modifying 
the same head chunk in lead and other sentences. 
The method borrows an idea from the sentence 
fusion methods and is more general than those 
using NP coreferencing as ours includes them. We 
show in experiments the method was able to find 
semantically appropriate revisions thus demon-
strating its basic feasibility. We also show that that 
parsing errors mainly degraded the sentential com-
pleteness such as grammaticality and redundancy.  
1 Introduction 
We address the problem of revising the lead sen-
tence in a broadcast news text to increase the 
amount of background information in the lead. 
This is one of the draft and revision approaches 
to summarization, which has received keen atten-
tion in the research community. Unlike many 
other methods that directly utilize noun phrase 
(NP) coreference (Nenkova 2008; Mani et al 
1999), we propose a method that employs inser-
tion and substitution of phrases that modify the 
same chunk in the lead and other sentences. We 
also show its effectiveness in a revision experi-
ment.  
As is well known, the extractive summary that 
has been extensively studied from the early days 
of summarization history (Luhn, 1958) suffers 
from various drawbacks. These include the prob-
lems of a break in cohesion in the summary text 
such as dangling anaphora and a sudden shift in 
topic.  
To ameliorate these problems, the idea of revis-
ing the extracted sentences was proposed in a 
single document summarization study. Jing and 
McKeown (1999; 2000) found that human sum-
marization can be traced back to six cut-and-
paste operations of a text and proposed a revision 
method consisting of sentence reduction and 
combination modules with a sentence extraction 
part. Mani and colleagues (1999) proposed a 
summarization system based on ?draft and revi-
sion? together with sentence extraction. The re-
vision part is achieved with the sentence aggre-
gation and smoothing modules. 
The cohesion break problem becomes particu-
larly conspicuous in multi-document summariza-
tion. To ameliorate this, revision of the extracted 
sentences is also thought to be effective, and 
many ideas and methods have been proposed so 
far. For example, Otterbacher and colleagues 
(2002) analyzed manually revised extracts and 
factored out cohesion problems. Nenkova (2008) 
proposed a revision idea that utilizes noun 
coreference with linguistic quality improvements 
in mind.  
Other than the break in cohesion, multi-
document summarization faces the problem of 
information overlap particularly when the docu-
ment set consists of similar sentences. Barzilay 
and McKeown (2005) proposed an idea called 
sentence fusion that integrates information in 
overlapping sentences to produce a non-
overlapping summary sentence. Their algorithm 
firstly analyzes the sentences to obtain the de-
pendency trees and sets a basis tree by finding 
the centroid of the dependency trees. It next 
augments the basis tree with the sub-trees in oth-
er sentences and finally prunes the predefined 
constituents. Their algorithm was further modi-
fied and applied to the German biographies by 
Filippova and Strube (2008).  
Like the work of Jing and McKeown (2000) and 
Mani et al (1999), our work was inspired by the 
summarization method used by human abstrac-
tors. Actually, our abstractors first extract impor-
tant sentences, which is called lead identification, 
and then revise them, which is referred to as 
phrase elaboration or specification. In this paper, 
we concentrate on the revision part.   
Our work can be viewed as an application of the 
sentence fusion method to the draft and revision 
39
approach to a single Japanese news document 
summarization. Actually, our dependency struc-
ture alignment is almost the same as that of 
Filippova and Strube (2008), and our lead sen-
tence plays the role of a basis tree in the Barzilay 
and McKeown approach (2005). Though the idea 
of sentence fusion was developed mainly for 
suppressing the overlap in multi-document sum-
marization, we consider this effective in aug-
menting the extracts in a single-document sum-
marization task where we face less overlap 
among sentences. 
Before explaining the method in detail, we will 
briefly introduce the Japanese dependency 1  
structure on which our idea is based. The de-
pendency structure is constructed based on the 
bunsetsu chunk, which we call ?chunk? for sim-
plicity. The chunk usually consists of one con-
tent-bearing word and a series of function words. 
All the chunks in a sentence except for the last 
one modify a chunk in the right direction. We 
call the modifying chunk the modifier and the 
modified chunk the head. We usually span a di-
rected edge from a modifier chunk to the head 
chunk 2 . Our dependency tree has no syntactic 
information such as subject or object. 
                                                
2 Broadcast news summarization 
Tanaka et al (2005) showed that most Japanese 
broadcast news texts are written with a three-part 
structure, i.e., the lead, body, and supplement. 
The most important information is succinctly 
mentioned in the lead, which is the opening sen-
tence(s) of a news story, referred to as an ?arti-
cle? here. Proper names and details are some-
times avoided in favor of more abstract expres-
sions such as ?big insurance company.? The lead 
is then detailed in the body by answering who, 
what, when, where, why, and how, and proper 
names only alluded to in the lead appear here. 
Necessary information that was not covered in 
the lead or the body is placed in the supplement.  
The research also reports that professional news 
abstractors who are hired for digital text services 
summarize articles in a two-step approach. First, 
they identify the lead sentences and set it (them) 
as the starting point of the summary. As the av-
erage lead length is 95 characters and the al-
 
1 This is the kakari-uke (modifier-modifiee) relation of 
Japanese, which differs from the conventional dependency 
relation. We use the term dependency for convenience in 
this paper. 
2 This is the other way around compared to the English de-
pendency such as in Barzilay and McKeown (2005).  
lowed summary length is about 115 characters 
(or 150 characters depending on the screen de-
sign), they revise the lead sentences using ex-
pressions from the remainder of the story.   
We see here that the extraction and revision 
strategy that has been extensively studied by 
many researchers for various reasons was actu-
ally applied by human abstractors, and therefore, 
the strategy can be used as a real summarization 
model. Inspired by this, we decided to study a 
news summarization system based on the above 
approach. To develop a complete summarization 
system, we have to solve three problems: 1) 
identifying the lead, body, and supplement struc-
ture in each article, 2) finding the lead revision 
candidates, and 3) generating a final summary by 
selecting and combining the candidates. 
We have already studied problem 1) and showed 
that automatic recognition of three tags with a 
decision tree algorithm reached a precision over 
92% (Tanaka et al 2007). We then moved to 
problem 2), which we discuss extensively in the 
rest of this paper.  
3 Manual lead revision experiment 
To see how problem 2) in the previous section 
could be solved, we conducted a manual lead-
revision experiment. We asked a native Japanese 
speaker to revise the lead sentences of 15 news 
articles using expressions from the body section 
of each article with cut-and-paste operations (in-
sertion and substitution) of bunsetsu chunk se-
quences. We refer to chunk sequences as phrases. 
We also asked the reviser to find as many revi-
sions as possible.  
In the interview with her, we found that she took 
advantage of the syntactic structure to revise the 
lead sentences. Actually, she first searched for 
the ?same? chunks in the lead and the body and 
checked whether the modifier phrases to these 
chunks could be used for revision. To see what 
makes these chunks the ?same,? we compared 
the syntactic head chunk of the lead and body 
phrases used for substitution and insertion. 
Table 1 summarizes the results of the compari-
son in three categories: perfect match, partial 
match (content word match), and different. 
The table indicates that nearly half of the head 
chunks were exactly the same, and the rest con-
tained some differences. The second row shows 
the number where the syntactic heads had the 
same content words but not the same function 
words. The pair ??? kaidan-shi ?talked? and
?????? kaidan-shi-mashi-ta ?talked? is an 
40
  Ins. Sub. Total 
1) Perfect 9 6 15
2) Partial 6 6 12
3) Different 1 6 7
 Total 16 18 34
Lead
IAEA? 
of the IAEA
???? 
the team
??? 
at Korea 
??????
arrived 
Table 1. Degree of syntactic head agreement 
example. These are the  syntactic and aspectual 
variants of the same verb ???? kaidan-suru 
?talk.?  
The third row represents cases where the syntac-
tic heads had no common surface words. We 
found that even in this case, though, the syntactic 
heads were close in some way. In one example, 
there was accordance in the distant heads, for 
instance, in the pair ?????  mitsuka-tta  
?found? and ??? ichibu-no ?part of.? In this 
case, we can find the chunk ????? mit-
suka-tta ?found? at a short edge distance from ?
?? ichibu-no ?part of.?  Based on the findings, 
we devised a lead sentence revision algorithm. 
4 Revision algorithm 
4.1 Concept 
We explain here the concept of our algorithm 
and show an example in Figure 1. We have a 
lead sentence and a body sentence, both of which 
have the ?same? syntactic head chunk, ????
??, touchaku-shima-shi-ta, ?arrived.?  
The head chunk of the lead has two phrases (un-
derlined with thick lines in Figure 1) that directly 
modify the head. We call such a phrase a maxi-
mum phrase of a head3. Like the lead sentence, 
the body sentence also has two maximum phras-
es. In the following part, we use the term phrase 
to refer to a maximum phrase for simplicity. 
By comparing the phrases in Figure 1, we notice 
that the following operations can add useful in-
formation to the lead sentence; 1) inserting the 
first phrase of the body will supply the fact the 
visit was on the 4th, 2) substituting the first 
phrase of the lead with the second one in the 
body adds the detail of the IAEA team. This re-
vision strategy was employed by the human re-
viser mentioned in section 2, and we consider 
this to be effective because our target document 
has a so-called inverse pyramid structure (Robin 
and McKeown 1996), in which the first sentence 
is elaborated by the following sentences. 
                                                 
3 To be more precise, a maximum phrase is defined as the 
maximum chunk sequence on a dependency path of a head. 
 
 
Figure 1. Concept of revision algorithm 
Further analyzing the above fact, we devised the 
lead sentence revision algorithm below. We pre-
sent the outline here and discuss the details in the 
next section. We suppose an input pair of a lead 
and a body sentence that are syntactically ana-
lyzed. 
1) Trigger search 
We search for the ?same? chunks in the lead 
and body sentences. We call the ?same? 
chunks triggers as they give the starting point 
to the revision. 
2) Phrase alignment 
We identify the maximum phrases of each 
trigger, and these phrases are aligned according 
to a similarity metric. 
3) Substitution 
If a body phrase has a corresponding phrase in 
the lead, and the body phrase is richer in in-
formation, we substitute the body phrase for 
the lead phrase.  
4) Insertion 
If a body phrase has no counterpart in the lead, 
that is, the phrase is floating, we insert it into 
the lead sentence. 
Our method inserts and substitutes any type of 
phrase that modifies the trigger and therefore has 
no limitation in syntactic type. Although NP 
elaboration such as in (Nenkova 2008) is of great 
importance, there are other useful syntactic types 
for revision. An example is the adverbial phrase 
insertion of time and location. The insertion of 
the phrase 4? yokka ?on the 4th? in figure 1 in-
deed adds useful information to the lead sentence.     
4.2 Algorithm 
The overall flow of the revision algorithm is 
shown in Algorithm 1. The inputs are a lead and 
a body sentence that are syntactically parsed, 
which are denoted by L and B respectively. 
The whole algorithm starts with the all-trigger 
search in step 1. Revision candidates are then 
found for each trigger pair in the main loop from 
steps 2 to 6.  The revision for each trigger pair is  
IAEA? 
of the IAEA 
??? 
inspectors 
??????
arrived
5?? 
five 
Body
4?? 
on the 4th
insertion substitution
maximum phrase 
41
Algorithm 14 (Left figures are the step numbers.) 
1: find all trigger pairs between L and B and  
              store them in T.  
 T={(l, b) ; l b, l?L and b?B } ?
2: for all (l, b) ? T do 
  find l?s max phrases and store in Pl. 
  Pl={pl ; pl ? max phrase of l} 
3:  do the same for trigger b 
  Pb={pb ; pb ? max phrase of b} 
4:  align phrases in Pl and Pb and store  
   result in A 
   A={( pl, pb) ; pl  pb ,  ?
     pl ? Pl, pb ? Pb } 
5:  for all (pl, pb) ? A do 
   follow Table 2 
  end for 
6: end for 
 
Body  
pb =?  pb? ?  
pl =?  4: no op. 1: insertion Lead 
pl  ? ? 3: no op. 2: substitution 
Table 2. Operations for step 5 
found based on the idea in the previous section in 
steps 4 and 5. Now we explain the main parts. 
? Step 1: trigger chunk pair search  
We first detect the trigger pairs in step 1 that are 
the base of the revision process. What then can 
be a trigger pair that yields correct revisions? We 
roughly define trigger pairs as the ?coreferential? 
chunk pairs of all parts of speech, i.e., the parts 
of speech that point to the same entity, event, 
action, change, and so on.  
Notice that the term coreferential is used in an 
extended way as it is usually used to describe the 
phenomena in noun group pairs (Mitkov, 2002).  
The chunk ?????? touchaku-shimashita 
?arrived? and IAEA? IAEA-no ?of the IAEA? 
in Figure 1 are examples.  
Identifying our coreferential chunks is even 
harder than the conventional coreference resolu-
tion, and we made a simplifying assumption as in 
Nenkova (2008) with some additional conditions 
that were obtained through our preliminary ex-
periments.  
(1) Assumption: Two chunks having the same 
surface forms are coreferential. 
(2) Conditions for light verb (noun) chunks: 
Agreement of modifying verbal nous is fur-
                                                 ??
4 The sign a b means the chunk ?a? and ?b? are triggers.  
The sign p q means the phrases ?p? and ?q? are aligned. 
ther required for chunks whose content 
words consist only of light verbs such as ?
? aru ?be? and ?? naru ?become?: these 
chunks themselves have little lexical mean-
ing. The agreement is checked with the 
hand-crafted rules. Similar checks are ap-
plied to chunks whose content words consist 
only of light nouns such as ?? koto (?koto? 
makes the previous verb a noun) . 
(3) Conditions for verb inflections: a chunk that 
contains a verb usually ends with a function 
word series that indicates a variety of infor-
mation such as inflection type, dependency 
type, tense, aspect, and modality. Some in-
formation such as tense and aspect is vital to 
decide the coreference relation (exchanging 
the modifier phrases ?arrive? and ?will ar-
rive? will likely bring about inconsistency in 
meaning), although some is not. We are in 
the process of categorizing function words 
that do not affect the coreference relation and 
temporally adopted the empirically obtained 
rule: the difference in verb inflection be-
tween the te-form (predicate modifying 
form) and dictionary form (sentence end 
form) can be ignored.     
? Step 4: phrase alignment 
We used the surface form agreement for similar-
ity evaluation. We applied several metrics and 
explain them one by one. 
1) Chunk similarity t, s 
t, s : x, y? chunk [0, 1]. ?
Function t is the Dice coefficient between the 
set of content words in x and those in y. The 
same coefficient calculated with all words 
(function and content words) is denoted as s. 
2) Phrase absorption ratio 
a : px, py? phrases  [0, 1] ?
This is the function that indicates how many 
chunks in phrase px is represented in py and is 
calculated with t as in, 
?
? ?
=
x
ypx
py
x
yx yxtp
ppa )),((max
1
:),( . 
3) Alignment  quality 
With the above two functions, the alignment qual-
lity is evaluated by the function 
g : px, py ? phrases ?  [0, 1] 
],1,0[
),,()1(),(:),(
?
?+=
?
?? yxsppappg yxyx  
where the shorter phrase is set to px so that 
yx pp < . The variables x and y are the last 
42
chunks in px and py, respectively. Intuitively, 
the function evaluates how many chunks in the 
shorter phrase px are represented in py and how 
similar the last chunks are. The last chunk in a 
phrase, especially the function words in the 
chunk, determines the syntactic character of 
the phrase, and we measured this value with 
the second term of the alignment quality. The 
parameter ? is decided empirically, which was 
set at 0.375 in this paper. 
In alignment, we calculated the score for all 
possible phrase combinations and then greed-
ily selected the pair with the highest score. We 
set the minimum alignment score at 0.185; 
those pairs with scores lower than this value 
were not aligned. 
? Step 5 (Table 2, case 1): insertion 
Step 5 starts either an insertion or substitution 
process, as in Table 2. If pb  (body phrase is 
not null) and pl =  (lead phrase is null) in Table 
2, the insertion process starts.  
? ?
?
In this process, we check the following.  
1) Redundancy check 
Insertion may cause redundancy in informa-
tion. As a matter of fact, redundancy often 
happens when there is an error in syntactic 
analysis. Suppose there are the same lead and 
body phrases that modify the same chunks in 
the lead and body sentences. If the lead phrase 
fails to modify the correct chunk because of an 
error, the body phrase loses the chance to be 
aligned to the lead phrase since they belong to 
different trigger chunks. As a result, the body 
phrase becomes a floating phrase and is in-
serted into the lead chunk, which duplicates 
the same phrase.  
To prevent this, we evaluate the degree of du-
plication with the phrase absorption ratio a 
and allow phrase insertion when the score is 
below a predefined threshold ? : we allow in-
sertion when 
? ),( bb pLpa < ,? phrase, L : lead sentence, 
is satisfied. 
2) Discourse coherence check 
Blind phrase insertion may invite a break in 
cohesion in a lead sentence.  This frequently 
happens when the inserted phrase has words 
that require an antecedent. We then prepared a 
list of words that contain such context-
requiring words and forbid phrase insertions 
that contain words that are on the list.  This list 
contains the pronoun family such as ?? ko-
kono ?this? and special adjectives such as ?? 
chigau ?different.?  
3) Insertion point decision 
The body phrase should be inserted at the 
proper position in the lead sentence to main-
tain the syntactic consistency. Because we 
dealt with single-phrase insertion here, we 
employed a simple heuristics.  
Since the Japanese dependency edge spans 
from left to right as we mentioned in section 1, 
we considered that the right phrase of the in-
serted phrase is important to keep the new de-
pendency from the inserted phrase to the trig-
ger chunk. Because we already know the 
phrase alignment status at this stage, we fol-
low the next steps to determine the insertion 
position in the lead of the insertion phrase. 
A) In the body sentence, find the nearest right 
substitution phrase pr of the insertion 
phrase. 
B) Find the pr?s aligned phrase in the lead prL. 
C) Insert the phrase to the left of the prL. 
D) If there is no pr, insert the phrase to the left 
to the trigger. 
? Step 5 (Table 2, case 2): substitution 
If pb ?  ?  and pl ? ?  in Table 2, the substitu-
tion process starts. This process first checks if 
each aligned phrase pair contains the same chunk 
other than the present trigger. If there is such a 
chunk, the substitution phrase is reduced to the 
subtree from the present trigger to the identical 
chunk. The newly found identical chunks are in 
trigger table T, and the remaining part will be 
evaluated later in the main loop. Owing to the 
phrase partitioning, we can avoid phrase substi-
tutions which are in an inclusive relation.  
The substitution candidate goes through three 
checks: information increase, redundancy, and 
discourse cohesion. As the latter two are almost 
the same as those in the insertion, we explain 
here the information increase. This involves 
checking whether the number of chunks in the 
body phrase is greater than that in the aligned 
lead phrase. This is based on the simple assump-
tion that elaboration requires more words. 
5 Revision experiments 
5.1 Data and evaluation steps 
? Purpose 
We conducted a lead revision experiment with 
three purposes. The first one was to empirically 
evaluate the validity of our simplified assump-
43
tions: trigger identification and concreteness in-
crease evaluation. For trigger identification, we 
basically viewed the identical chunks as triggers 
and added some amendments for light verbs 
(nouns) and verb inflections. For the check of an 
increase in concreteness, we assumed that 
phrases with more chunks were more concrete. 
However, these simplifications should be veri-
fied in experiments. 
The second purpose was to check the validity of 
using the revision phrases only in body sentences 
and not in the supplemental sentences. 
The last one was to determine how ineffective 
the result is if the syntactic parsing fails. With 
these purposes in mind, we designed our experi-
ment as follows.  
? Data  
A total of 257 articles from news programs 
broadcast on 20 Jan., 20 Apr., and 20 July in 
2004 were tagged with lead, body, and supple-
ment tags by a native Japanese evaluator. The 
articles were morphologically analyzed by Me-
cab (Kudo et al, 2003) and syntactically parsed 
by Cabocha (Kudo and Matsumoto, 2002). 
? Evaluator and evaluation detail 
We prepared an evaluation interface that presents 
a lead with one revision point (insertion or sub-
stitution) that was obtained using the body and 
supplemental sentences to an evaluator. 
A Japanese native speaker evaluated the results 
one by one with the above interface. We planned 
a linguistic evaluation like DUC2005 (Hoa Trang, 
2005). Since their five-type evaluation is in-
tended for multi-document summarization, 
whereas our task is single-document summariza-
tion, and we are interested in evaluating our 
questions mentioned above, we carried out the 
evaluation as follows. In future, we plan to in-
crease the number of evaluation items and the 
number of evaluators.  
Concreteness Score 
Decreased 0 
Unchanged 1 
Increased 2 
Table 3. Evaluation of increased concreteness 
Completeness Required operations Score
Poor More than 2 0
Acceptable One 1
Perfect None 2
Table 4. Sentential completeness 
 
E1) The evaluator judged if the revision was ob-
tained from the lead and body sentences with 
or without parsing errors. Here, errors that did 
not affect the revision were not considered.  
E2) Second, she checked whether the revision 
was semantically correct or revised informa-
tion matching the fact described in the lead 
sentence. Here, she did not care about the 
grammaticality or the improvements in con-
creteness of the revision; if the revision was 
problematic but manually correctable, it was 
judged as OK. This step evaluated the correct-
ness of the trigger selection; wrong triggers, 
i.e., those referring to different facts produce 
semantically inconsistent revisions as they mix 
up different facts. 
The following evaluation was done for those 
judged correct in evaluation step E2, as we found 
that revisions that were semantically inconsistent 
with the lead?s facts were often too difficult to 
evaluate further.  
E3) Third, she evaluated the change in concrete-
ness after revision with the revisions that 
passed evaluation E2. She judged whether or 
not the revision increased the concreteness of 
the lead in three categories (Table 3). 
Notice that original lead sentences are sup-
posed to have an average score of 1. 
E4) Last, she checked the sentential complete-
ness of the revision result that passed evalua-
tion E2. They still contained problems such as 
grammatical errors and improper insertion po-
sition. Rather than evaluating these items sepa-
rately, we measured them together for senten-
tial completeness. At this time, we measured in 
terms of the number of operations (insertion, 
deletion, substitution) needed to make the sen-
tence complete5.  
As shown in Table 4, revisions requiring more 
than two operations are categorized as ?poor,? 
those requiring one operation are ?acceptable,? 
and those requiring no operations are ?perfect.? 
We employed this measure because we found 
that grading detailed items such as grammatical-
ity and insertion positions at fine levels was 
rather difficult. We also found that native Japa-
nese speakers can correct errors easily. Notice 
the lead sentences are perfect and are supposed 
                                                 
5 This was not an automatic process and may not be perfect. 
The evaluator simulated the correction in mind and judged 
whether it was done with one action. 
44
to have an average score of 2 in sentential com-
pleteness. Since the revision does not improve 
the completeness further but elicits defects such 
as grammatical errors, it usually produces a score 
below 2. Some examples of the results with their 
scores are shown below. The underlined parts are 
the inserted body chunk phrases, and the paren-
thesized parts are the deleted lead chunks. 
1) Concreteness 2, Completeness 2 
?????????????????
?????????????????
????????????? 
minkan-dantai-no ?private 
organization?, korea-
society-nado-ga ?Korea Soci-
ety and others?, shusai-suru  
?sponsored?, chousen-hantou-
heiwa-forumu-ni  ?Peace Fo-
rum in Korean Peninsula?, 
(moyooshi-ni ?event?), 
shusseki-suru ?attend? 
2) Concreteness 1, Completeness 2 
?????????????????
???? 
buhin-ni ?to the parts? ki-
retsu-ga ?cracks?, haitte-
iru-no-ga ?being there? (), 
mitsuka-tta ?found? 
3) Concreteness 2, Completeness 0 
?????????????????
???????????????? 
Herikoputa-kara ?from a hel-
icopter?, chijou-niju-
metoru-no-takasa-kara ?from 
20 meters high? (), rakka-
shi ?fell and?, shibou-
shima-shita ?killed? 
Example 1 is the perfect substitution and had 
scores of 2 for both concreteness increase and 
completeness. Actually, the originally vaguely 
mentioned term ?event? was replaced by a more 
concrete phrase with proper names, ?Korean Pen-
insula Peace Forum sponsored by Korea Society 
and others.? Notice that this can be achieved by 
NP coreference based methods if they can iden-
tify that these two different phrases are corefer-
ential. Our method does this through the depend-
ency on the same trigger ???? shusseki-suru 
?attend.? 
Example 2 is a perfect sentence, but its concrete-
ness stayed at the same level. As a result, the 
scores were 1 for concreteness increase and 2 for 
completeness. 
 Incorrect Correct Cor. Ratio 
Succ. 70 353 0.83Parse
Fail. 31 149 0.83
Body 50 464 0.90Sent.
Supp. 51 38 0.43
Table 5. Results of semantic correctness 
Score 0 1 2 Ave.
Succ. 0 55 298 1.84Parse
Fail. 1 19 129 1.86
Body 1 61 402 1.86Sent.
Supp. 0 13 25 1.66
Table 6. Results of concreteness increase 
Score 0 1 2 Ave.
Succ. 78 60 215 1.39Parse
Fail. 66 55 28 0.74
Body 120 110 234 1.25Sent.
Supp. 24 5 9 0.61
Table 7. Results of sentential completeness 
Actually, the original sentence that meant ?They 
found a crack in the parts? was revised to ?They 
found there was a crack in the parts,? which did 
not add useful information. Example 3 has a 
grammatical problem although the revision sup-
plied useful information.As a result, it had scores 
of 2 for concreteness increase and 0 for com-
pleteness. The added kara-case phrase (from 
phrase) ????????????? chijou-
niju-metoru-no-takasa-kara ?from 20 meters 
high? is useful, but since the original sentence 
already has the kara-case ???????? 
herikoputa-kara ?from helicopter,? the insertion 
invited a double kara-case, which is forbidden in 
Japanese. To correct the error, we need at least 
two operations, and thus, a completeness score of 
0 was assigned. 
5.2 Results of experiments 
Table 5 presents the results of evaluation E2, the 
semantic correctness with the parsing status of 
evaluation E1 and the source sentence category 
from which the phrases for revision were ob-
tained. Columns 2 and 3 list the number of revi-
sions (insertions and substitutions) that were cor-
rect and incorrect and column 4 shows the cor-
rectness ratio. We obtained a total of 603 revi-
sions and found that 30% (180/603) of them 
were derived with syntactic errors. 
The semantic correctness ratio was unchanged 
regardless of the parsing success. On the contrary, 
it was affected by the source sentence type. The 
correctness ratio with the supplemental sentence 
45
was significantly6 lower than that with the body 
sentence. Table 6 lists the results of the con-
creteness improvements with the parsing status 
and the source sentence type. Columns 2, 3 and 4 
list the number of revisions that fell in the scores 
(0-2) listed in the first row. The average score in 
this table again was not affected by the parsing 
failure but was significantly affected by the 
source sentence category. The result with the 
supplement sentences was significantly worse 
than that with body sentences. 
Table 7 lists the results of the sentential com-
pleteness in the same fashion as Table 6. The 
sentential completeness was significantly wors-
ened by both the parsing failure and source sen-
tence category.  
These results indicate that the answers to the 
questions posed at the beginning of this section 
are as follows. From the semantic correctness 
evaluation, we infer that our trigger selection 
strategy worked well especially when the source 
sentence category was limited to the body.  
From the concreteness-increase evaluation, the 
assumption that we made also worked reasonably 
well when the source sentence category was lim-
ited to the body.  
The effect of parsing was much more limited 
than we had anticipated in that it did not degrade 
either the semantic correctness or the concrete-
ness improvements. Parsing failure, however, 
degraded the sentential completeness of the re-
vised sentences. This seems quite reasonable: 
parsing errors elicit problems such as wrong 
phrase attachment and wrong maximum phrase 
identification. The revisions with these errors 
invite incomplete sentences that need corrections.  
It is worth noting that cases sometimes occurred 
where a parsing error did not cause any problem 
in the revision. We found that the phrases gov-
erned by a trigger pair in many cases were quite 
similar, and therefore, the parser makes the same 
error. In that case, the errors are often offset and 
cause no problems superficially. 
We consider that the sentential completeness 
needs further improvements to make an auto-
matic summarization system, although the se-
mantic correctness and concreteness increase are 
at an almost satisfactory level. Our dependency-
based revision is expected to be potentially use-
ful to develop a summarization system. 
                                                 
6 In this section, the ?significance? was tested with the 
Mann-Whitney U test with Fisher?s exact probability. We 
set the significance level at 5%.  
6 Future work  
Several problems remain to be solved, which will 
be addressed in future work. Obviously, we need 
to improve the parsing accuracy that degraded 
the sentential completeness in our experiments. 
Although we did not quantitatively evaluate the 
errors in phrase insertion position and redun-
dancy, we could see these happening in the re-
vised sentences because of the inaccurate parsing. 
Apart from this, we need to further refine the 
following problems.  
Regarding the trigger selection, one particular 
problem we faced was the mixture of statements 
of different politicians in a news article. The 
statements were often included as direct quota-
tions that end with the chunk ????? nobe-
mashi-ta ?said.? Our system takes the chunk as 
the trigger and does not care whose statements 
they are; thus, it ended up mixing them up. A 
similar problem happened when we had two dif-
ferent female victims of an incident in an article. 
Since our system has no means to distinguish 
them, the modifier phrases about these women 
were mixed up. 
We think that we can improve our method by 
applying more general language generation tech-
niques. An example is the kara-case collision that 
we explained in example 3 in section 5.1. The 
essence of the problem is that the added content 
is useful, but there is a grammatical problem. In 
other words, ?what to say? is ok but ?how to 
say? needs refinement. This particular problem 
can be solved by doing the case-collision check, 
and by synthesizing the colliding phrases into 
one. These can be better treated in the generation 
framework.  
7 Conclusion 
We proposed a lead sentence revision method 
based on the operations of phrases that have the 
same head in the lead and other sentences.  This 
method is a type of sentence fusion and is more 
general than methods that use noun phrase 
coreferencing in that it can add phrases of any 
syntactic type. We described the algorithm and 
the rules extensively, conducted a lead revision 
experiment, and showed that the algorithm was 
able to find semantically appropriate revisions. 
We also showed that parsing errors mainly de-
grade the sentential completeness such as gram-
maticality and repetition. 
 
46
Reference 
Regina Barzilay and Kathleen R. McKeown. 2005. 
Sentence Fusion for Multidocument News Summa-
rization. Computational Linguistics. 31(3): 298-
327. 
Katja Filippova and Michael Strube. 2008. Sentence 
Fusion via Dependency Graph Compression.  proc. 
of the EMNLP 2008: 177-185 
Hongyan Jing and Kathleen R. McKeown. 1999. The 
Decomposition of Human-Written Summary Sen-
tences. proc. of the 22nd International Conference 
on Research and Development in Information Re-
trieval  SIGIR 99: 129-136. 
Hongyan Jing and Kathleen R. McKeown. 2000. Cut 
and Paste Based Text Summarization, proc. of the 
1st meeting of the North American Chapter of the 
Association for Computational Linguistics: 178-
185. 
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. Proc. 
of the 6th Conference on Natural Language Learn-
ing 2002: 63-69. 
Taku Kudo, Kaoru Yamamoto and Yuji Matsumoto. 
2004. Applying Conditional Random Fields to Jap-
anese Morphological Analysis, proc. of the 
EMNLP 2004: 230-237. 
H. P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. Advances in Automatic Text Sum-
marization. The MIT Press: 15-21. 
Inderjeet Mani, Barbara Gates, and Eric Bloedorn.  
1999. Improving Summaries by Revising Them. 
Proc. of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.: 558-565. 
Ruslan Mitkov 2002, Anaphora Resolution, Pearson 
Education. 
Ani Nenkova. 2008. Entity-driven Rewrite for Multi-
document Summarization, proc. of the 3rd Interna-
tional Joint Conference on Natural Language Gen-
eration: 118-125. 
Jahna C. Otterbacher, Dragomir R. Radev, and Airong 
Luo 2002, Revisions that Improve Cohesion in 
Multi-document Summaries: A Preliminary Study. 
Proc. of the ACL-02 Workshop on Automatic 
Summarization: 27-36. 
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision-
based model for summary generation. Artificial In-
telligence.  85: 135-179. 
 
47
