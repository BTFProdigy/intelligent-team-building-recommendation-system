Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1311?1321, Dublin, Ireland, August 23-29 2014.
Group based Self Training for E-Commerce Product Record Linkage
Wayne Xin Zhao
1,2
, Yuexin Wu
2
, Hongfei Yan
2
and Xiaoming Li
2
1
School of Information, Renmin University of China, China
2
School of Electronic Engineering and Computer Science, Peking University, China
batmanfly@gmail.com, wuyuexin@gmail.com,
yhf1029@gmail.com, lxm@pku.edu.cn
Abstract
In this paper, we study the task of product record linkage across multiple e-commerce web-
sites. We solve this task via a semi-supervised approach and adopt the self-training algorithm for
learning with little labeled data. In previous self-training algorithms, the learner tries to convert
the most confidently predicted unlabeled examples of each class into labeled training examples.
However, they evaluate the confidence of an instance only based on the individual evidence from
the instance. The correlation among data instances is rarely considered.
To address it, we develop a novel variant of the self-training algorithm by leveraging the data
characteristics for the task of product record linkage. We joint consider a candidate linked pair
and its corresponding correlated pairs as a group at the selection of pseudo labeled data. We
propose a novel confidence evaluation method for a group of instances, and incorporate it as a
re-ranking step in the self-training algorithm. We evaluate the novel self-training algorithm on
two large datasets constructed based on real e-commerce Websites. We adopt several competitive
methods as comparisons and perform extensive experiments. The results show that our method
outperforms these baselines that do not consider data correlation.
1 Introduction
Recent years have witnessed the rapid development of online e-commerce business, e.g. Amazon and
eBay, which raises the need for better storing, organizing and analyzing the large amount of product
records. An important task is how to effectively link product records across multiple databases or web-
sites. This task serves as a fundamental step for many applications. For example, it will be useful to
provide entity-oriented search and product comparison analysis in eBay, where record linkage can help
to unify the corresponding records (i.e. records from different sellers) given a product. Record linkage has
been shown to be important in many fields, including biology (Needleman and Wunsch, 1970), database
(Neiling, 2006) and text mining (Goiser and Christen, 2006; Bilenko and Mooney, 2003). In this paper,
we mainly focus on the task of product record linkage for online e-commerce websites, but our method
is easy to be extended to other data sources and tasks.
Early studies on record linkage were mainly based on the classical probabilistic approach develope-
d by Fellegi and Sunter (1969), furthermore it was improved by the application of the expectation-
maximization (EM) algorithm (Winkler, 1988) and the use of approximate string comparison algorithms
(Christen, 2006; Winkler, 2006). The early work was not flexible to incorporate rich information. The
development of machine learning techniques in the late 1990s provides a new approach for record link-
age, and it has become the mainstream methodology for this task. The task of record linkage is usually
re-casted as the record pair classification problem, i.e. whether a record pair refers to the same entity or
not (Elfeky et al., 2002; Neiling, 2006; Tejada et al., 2002; Nahm et al., 2002). Supervised methods can
also be used to learn distance measures for approximate string comparisons (Bilenko and Mooney, 2003;
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1311
Cohen et al., 2003). Although supervised techniques often achieve good linkage quality, they are largely
limited by the availability of the training data.
To address this problem, semi-supervised learning approaches aim to make good use of a small portion
of labeled and a large amount of unlabeled data to build a better classifier (Yarowsky, 1995). Self-training
is a commonly used algorithm for semi-supervised learning, where in each iteration the learner converts
the most confidently predicted unlabeled examples of each class into labeled training examples. It has
been successfully applied to many tasks, such as sentiment analysis (He and Zhou, 2011; Riloff et al.,
2003) and object detection from images (Rosenberg et al., 2005).
In this paper, we solve the task of product record linkage via a semi-supervised approach and adopt
the flexible self-training framework for learning with little labeled data. We propose a novel variant of
the self-training algorithm by incorporating the correlation existing in the data instances, which is rarely
studied in previous studies. To introduce our idea, we first present an illustrative example in Figure 1.
There are two databases D and D
?
, and we have three records r
1
, r
2
, r
3
? D and another three records
r
?
1
, r
?
2
, r
?
3
? D
?
. Furthermore, we assume r
1
and r
?
1
refer to the same product. We can see that r
1
is
involved in three candidate pairs, i.e. (r
1
, r
?
1
), (r
1
, r
?
2
) and (r
1
, r
?
3
). Similarly, r
?
1
is involved in three
candidate pairs, i.e. (r
?
1
, r
1
), (r
?
1
, r
2
) and (r
?
1
, r
3
). Usually, each individual database does not contain
duplicate records, once we know r
1
is linked to r
?
1
, we can infer the rest candidate pairs should not be
linked. In other words, only if we are confident that no pair in the set {(r
1
, r
?
2
), (r
1
, r
?
3
), (r
2
, r
?
1
), (r
3
, r
?
1
)}
is not linked, r
1
is likely to be linked with r
?
1
.
?
?
?
?
?
?
Figure 1: An illustrative example for correlation among record pairs. The real line denotes the real linkage
relation and the dash line denotes the candidate linkage relation.
For the task of record linkage, the number of positive instances (i.e. linked record pairs) are usually
much less than that of negative instances. We mainly consider the confidence evaluation of the candidate
positive instance. By following the above idea, given a candidate linked pair, we treat all the correlated
record pairs together as a group and evaluate the linkage confidence based on the evidence of all record
pairs in this group, i.e. group confidence evaluation. We incorporate the group confidence evaluation
into the self-training algorithm as a re-ranking step. Interestingly, once we have identified a linked pair,
the rest correlated record pairs can be naturally judged as negative instances. We evaluate the novel
self-training algorithm on two large datasets constructed based on real e-commerce Websites. We adopt
several competitive methods as comparisons and perform extensive experiments. The results show that
our method outperforms these baselines that do not consider data correlation.
2 Related Work
We have briefly described the supervised approaches for record linkage in the introduction. Now we
discuss other related studies, including unsupervised clustering techniques, genetic programming based
approaches and linking based on more complex constraints.
Unsupervised clustering techniques have been investigated both for improved blocking (Cohen and
Richman, 2002; McCallum et al., 2000) and for automatic record pair classification (Elfeky et al., 2002).
Usually, such techniques do not perform not as well as supervised approaches.
Most recently, genetic programming (GP) (Koza et al., 1999) has also been utilized to the task of
record linkage. GenLink (Isele and Bizer, 2012) is a GP-based supervised learning algorithm in order
to learn linkage rules from a set of existing reference links, which also suffers from the problem of
lack of labeled data. Ngomo and Lyko (2013) evaluated linear and boolean classifiers against classifiers
1312
computed by using genetic programming for the record linkage problem. Their experiments showed that
both approaches did not perform well on real data.
Some other studies exploit more complex constraints that include relationships between different entity
types to link all types of entities in coordination (Bhattacharya and Getoor, 2007; Dong et al., 2005; On
et al., 2007). The usage of such constraints can indeed help to get better linkage results, but is in many
cases domain-dependent. We try to develop an approach which can be applicable across domains.
In order to address the problem of limited labeled data, we mainly consider the semi-supervised ap-
proaches. There are rarely semi-supervised approaches specially for the record linkage problem. Some
studies on improving self-training algorithms are related to our work. Self-training with editing (Li and
Zhou, 2005) can help to reduce mislabeled pseudo training examples, and reserved self-training (Guan
and Yang, 2013) is designed for handling imbalanced data. We have very different focus with theirs, i.e.
incorporating the instance correlations into learning algorithms, which can applied to other self-training
variants.
3 Problem Definition
In this section, we first introduce the preliminary related to our task. Then we formally define our studied
task.
Product record. A product record r is characterized by a referred product entity e and a set of attribute
values V = {(v
i
)}
i
, where v
i
denotes the value of the ith attribute in r. We use r.e and r.V to index
the product entity and attribute value set of the record r respectively. A product record corresponds to a
unique product entity but a product entity can map to multiple product records across multiple databases.
Attribute values are represented as strings, i.e. a sequence of characters. An attribute of a product might
correspond to different descriptive text across websites.
Product record linkage. The task of product record linkage is to judge whether two product records refer
to the same product entity. Given two product records r and r
?
, we aim to judge whether r.e is the same
to r
?
.e. Usually, r and r
?
come from different product databases. Although different product databases
can have different attributes for the same product and different attribute names for the same attribute, we
make an assumption about the task: candidate record pairs share the same set of attributes. It is relatively
easy to automatically identify common attributes and align attributes (H?arder et al., 1999; Rundensteiner,
1999; Hassanzadeh et al., 2013), which is not our focus in this paper. We mainly study product record
linkage under the same set of attributes, and this assumption makes our study more focused. If r and r
?
refer to the same product entity, denoted by r ? r
?
; otherwise, we denote it by r 6? r
?
.
4 A General Machine Learning based Approach
Given a product type, as we mentioned above, we assume that it corresponds to a specific set of attributes,
and all the product records share the same set of attributes but possibly with different descriptive text for
attribute values. In this section, we further present a general supervised approach with similarity features.
4.1 Defining the similarity function
Given two product records r and r
?
, we can obtain the similarity between their descriptive text of an at-
tribute by using a similarity function. The major intuition is that if two records refer to the same product,
they should have similar text for the same attribute, i.e. the similarity function should return a large sim-
ilarity value. Let f(?, ?) denote a similarity function, which takes two text strings and returns a similarity
value within the interval [0, 1] for these two strings. As revealed in (Bilenko and Mooney, 2003), differ-
ent attributes or fields may need different similarity functions to achieve best similarity evaluation. Thus,
instead of fixing a single similarity function, we consider using the following widely used similarity
functions: 1) Exact match; 2) Cosine similarity; 3) Jaccard coefficient; 4) K-Gram similarity (Kondrak,
2005); 5) Levenshtein similarity (Levenshtein, 1966); 6) Affine Gap similarity (Needleman and Wunsch,
1970).
1313
4.2 The learning framework
Based on these similarity functions, we propose a general learning framework for product record linkage
by using similarity values of different fields as features.
Given a product type, we assume that there are A attributes and K similarity functions. For two records
r and r
?
, we can obtain a similarity feature vector x = [x
a,k
]
A
i=1
,
K
k=1
, which is indexed by an attribute and
a similarity function: x
a,k
denotes the similarity of the ath attribute between r and r
?
by using the kth
similarity function. Furthermore, each feature vector x will correspond to a unique binary label y which
indicates that r and r
?
refer to the same product entity. Given a set of record pairs and their linkage labels
{(x, y)}, we can learn a classifier which is able to predict the linkage label given the similarity feature
vector of two records. To this end, we have reformulated the task of product record linkage as a binary
classification problem. Any classifiers can be used for this task. In what follows, we will use instances
and candidate pairs alternatively.
5 Group based Self-Training
In the above, we have presented a supervised learning approach for product record linkage. The approach
is easy to apply in practice, however, the performance is largely limited by the availability of training
data. For our current task, i.e. product record linkage, the generation of labeled data becomes even much
harder: there are usually many product types and it is infeasible to create a large amount of labeled data
for each type. Although it is difficult to obtain labeled data, we can easily obtain sufficient unlabeled data.
Thus, in this paper, we study the task of product record linkage in a semi-supervised setting by leveraging
both the learning ability of the classifiers and the usefulness of the large amount of unlabeled data. We
propose a novel group based self-training algorithm for product record linkage. Before introducing our
method, we first introduce the general self-training algorithm.
5.1 The general self-training algorithm
Self-training is a semi-supervised learning algorithm. It starts training on labeled data only, after each
iteration, the most confidently predicted unlabeled samples would be incorporated as new labeled data,
i.e. pseudo labeled data, decided by confidence scores from the classifier. After several iterations, it is
expected to get a better classifier trained with both labeled data and pseudo labeled data. The general
procedure of self-training algorithm is summarized in Algorithm 1.
Algorithm 1: The general procedure of the self-training algorithm.
1 Input: labeled dataset L, unlabeled dataset U , the classifier C.
2 U
?
? S randomly selected examples from U , S is usually set to 0.5 ? |U|;
3 repeat
4 Training the classifier: Use L to train C, and label the examples in U
?
;
5 Selecting pseudo labeled data: Select T most confidently classified examples from U
?
and add them to L;
6 Filling unlabeled data: Refill U
?
with examples from U , to keep U
?
at a constant size of S examples.
7 until I iterations or U = ?;
8 return The extended labeled dataset L and the trained classifier C.
We can see that self-training is a wrapper algorithm by taking a classifier as the learning component,
and it has three major steps in an iteration: 1) training classifier; 2) selecting pseudo labeled data; and
3) filling unlabeled data. Among the three steps, the most important step is the pseudo labeled data
selection. Previously, the most commonly used method is to select the top confident instances of the
classifier, and it is easy to see that the performance of self-training relies on the learning ability of the
embedded classifier.
5.2 Group confidence evaluation
Recall that each instance is a pair of product records (r, r
?
) and their label indicates whether they should
be linked or not. Let P
L
(r, r
?
) denote the confidence that r and r
?
refer to the same product entity (linked
1314
confidence), and P
N
(r, r
?
) denote the confidence that r and r
?
refer to different product entities (non-
linked confidence). P
L
(r, r
?
) and P
N
(r, r
?
) can be estimated by the confidence scores from the classifier.
In the task of product record linkage, there are usually more negative instances, i.e. the number of non-
linked pairs is much more than that of linked pairs. Thus, we mainly study the confidence of a candidate
positive instance. The standard self-training algorithm selects top ranked positive instances according
to the confidence scores estimated by the classifier, i.e. we select the instances with large linked con-
fidence P
L
(?, ?). However, when applied to product record linkage, it ignores important characteristics
underlying the data, which will be potentially helpful to the task.
Let us examine the illustrative example in Figure 1. Recall that r
1
and r
?
1
refer to the same product,
i.e. r
1
? r
?
1
. We can see that r
1
is involved in three candidate pairs, i.e. (r
1
, r
?
1
), (r
1
, r
?
2
) and (r
1
, r
?
3
).
Similarly, r
?
1
is involved in three candidate pairs, i.e. (r
?
1
, r
1
), (r
?
1
, r
2
) and (r
?
1
, r
3
). We totally have a set
of five candidate pairs, i.e. {(r
1
, r
?
1
), (r
1
, r
?
2
), (r
1
, r
?
3
), (r
2
, r
?
1
), (r
3
, r
?
1
)}. Here we follow the assumption
of the one-to-one mapping, i.e. given two databases, a product record can link to at most one record in
the other database. By leveraging the correlation among candidate pairs, with r
1
? r
?
1
, we can infer the
rest four candidate pairs must not be linked, i.e. r
1
6? r
?
2
, r
1
6? r
?
3
, r
2
6? r
?
1
, r
3
6? r
?
1
. Next, we formally
characterize the above idea and present the algorithm. Given two databases D and D
?
, let C ? D ? D
?
denote the candidate pair set where two product records in a pair come from D and D
?
respectively.
Consider a candidate pair (r, r
?
) ? C, where r ? D, r
?
? D
?
. We consider the following two sets:
S
r
= {(r, b)|(r, b) ? C, b ? D
?
and b 6= r
?
} and S
r
?
= {(a, r
?
)|(a, r
?
) ? C, a ? D and a 6= r}.
Intuitively, if we know r ? r
?
, then all the pairs in both S
r
and S
r
?
must not be linked. Thus, we define
the conflicting set of pair (r, r
?
) as S
r,r
?
cfl
= S
r
? S
r
?
.
With the definition of the conflicting set, let us reconsider the pseudo labeled data selection. The
straightforward way is to evaluate each instance with their linked confidence P
L
() from the classifier.
However, it oversimplifies the data dependence and does not make use of the correlated characteristics.
Consider an instance, which is a record pair (r, r
?
), we can have the following two properties:
? If r ? r
?
, then ?(a, b) ? S
r,r
?
cfl
, we have a 6? b;
? If ?(a, b) ? S
r,r
?
cfl
and a ? b, then we have r 6? r
?
.
The above properties suggest that it should be helpful to consider the correlation among instances
when evaluating the confidence of a positive instance, i.e. a candidate linked record pair. Intuitively, if
two records refer to the same product entity, they should have large linked confidence and their conflicting
pairs should have large non-linked confidence. We propose to use the following method to evaluate the
linkage confidence between r and r
?
Conf(r, r
?
) = P
L
(r, r
?
)
(
?
(a,b)?S
r,r
?
cfl
P
N
(a, b)
)
1/M
, (1)
where M = |S
r,r
?
cfl
|, P
L
(?, ?) and P
N
(?, ?) are positive and negative confidence scores estimated by
the classifier respectively. Note that we take the geometric mean of the non-linked confidence of these
conflicting pairs, which is to reduce the affect of large outlier values and the varying size of the conflict
sets. We treat a candidate linked pair and all the candidate pairs in its conflicting set as a group. The group
confidence evaluation consists of two intuitions: 1) the confidence that two records should be linked; 2)
the confidence that any pair of records in the conflicting set must not be linked. We have taken these two
aspects into a unified evaluation score.
5.3 The proposed self-training algorithm
In this part, we present the novel self-training algorithm based on the group confidence evaluation. We
have the similar steps with the general self-training algorithm in Algorithm 1. The major focus is to mod-
ify the step of pseudo labeled data selection. As mentioned above, we mainly consider the confidence
evaluation of positive instances. Our method for pseudo labeled data selection is three-step process:
1315
? Select top T
?
most confidently classified positive examples by the classifier;
? Rerank these T
?
examples by the group confidence scores defined in Equation 1;
? Select top T examples from the reranked T
?
examples (T ? T
?
) as pseudo positive instances and
their corresponding conflicting instances in the conflicting sets as pseudo negative instances.
We select positive instances not only based on the instance itself but also their corresponding conflict-
ing instances: if we have high confidence about a positive instance, then the confidence of their conflict-
ing instances being negative should be high, too. Next, we present the detailed group based self-training
algorithm in Algorithm 2.
Algorithm 2: The procedure of the group based self-training algorithm.
1 Input: labeled dataset L, unlabeled dataset U , the classifier C.
2 U
?
? S randomly selected examples from U ;
3 repeat
4 Training the classifier: Use L to train C, and label the examples in U
?
;
5 Selecting pseudo labeled data selection:
? Select T
?
most confident positive examples from U
?
and add them to L;
? Calculate the group confidence scores for the T
?
examples according to Equation 1.
? Rerank these T
?
examples by their group confidence scores and add top T examples to L as the pseudo positive
instances.
? For each of the T examples, add their conflicting instances to L into as the pseudo negative instances.
Filling unlabeled data: Refill U
?
with examples from U , to keep U
?
at a constant size of S examples.
6 until I iterations or U = ?;
7 return The extended labeled dataset L and the trained classifier C.
On one hand, our group based self-training algorithm naturally exploits the correlation among data
instances and evaluate the confidence scores in a broader view, which avoids the decision conflicts caused
by the data dependence. On the other hand, we focus on evaluating the confidence of being a positive
instance, which further reduces the bias from imbalanced data distribution. Thus, it is expected to achieve
better performance in the task of product record linkage.
Most classifiers can provide the estimated confidence scores P
L
() (i.e. for a positive instance) and
P
N
() (i.e. for a negative instance): Maximum-Entropy models output the conditional probabilities of an
instance for each class (Berger et al., 1996); the Decision Tree C4.5 algorithm is also able to compute
the probability distribution over different classes for each instance (Quinlan, 1993).
6 Experiments
6.1 Construction of the test collection
We test our method on two real e-commerce datasets respectively from Jingdong
1
and eTao
2
. Jingdong is
the largest B2C e-commerce company and eTao is one of the largest product search portals in China. Due
to the extremely large product databases, it is infeasible to generate training data on each product type
for these two product databases. We consider two popular kinds of products: laptop and camera. These
two kinds of products cover a considerable amount of brands and models, especially suitable for the test
of record linkage. Both Jindong and eTao have set up specific categories for these two kinds of products
respectively, thus we can easily crawl the product records under the corresponding category label. To
generate linked record datasets, we first manually align attributes (i.e. fields) for these two kinds between
Jindong and eTao. We summarize the numbers of aligned fields and some example fields in Table 1. Not
all the records contain the information for all the fields, we set the value of the empty field to a ?NULL?
string.
1
http://www.jd.com
2
http://www.etao.com
1316
We adopt a blocking approach (Baxter et al., 2003) to automatically generate a set of candidate pairs,
i.e. a record in Jindong is to be linked with a record in eTao. This approach consider all pairwise links
between Jindong records and eTao records for the same kind of product. If there exists at least one com-
mon word in the field of brand or model between a record pair, we consider it to be a candidate pair. The
automatic method generates 20,094 candidate pairs and 12,157 candidate pairs respectively for LAPTOP
and CAMERA. Then we invite professional workers from an e-commerce company to link records across
these two product databases. Instead of examining all the candidate pairs, the labeling process adopts a
product-oriented way to generate the gold standard. Given a product record of a database, the annotator
first identifies the product entity that the record refers to, then she looks for the corresponding record in
another database. In the annotation process, Web access is available all the time. Annotators can make
use of the search engines of Jindong and eTao to accelerate the product lookup. A linked record pair is
treated as a positive instance. Finally, we identify 501 linkable products (i.e. 501 positive instances) in
LAPTOP dataset, and 478 linkable products (i.e. 478 positive instances) in CAMERA dataset. All the
other candidate pairs are automatically labeled as negative. We present the the data statistics in Table 1.
Dataset
# positive # negative
# fields Example fields
instances instances
LAPTOP 501 19593 10 OS, screen size, CPU type, ram size
CAMERA 478 11679 11 lens type, sensor type, focal length, aperture size
Table 1: Basic statistics of datasets.
6.2 Experimental setup
For each kind of product, we divide the dataset into two parts, i.e. a training set and a test set. In order
to examine different methods in a semi-supervised setting, we keep a small amount of instances in the
training set, and we assume all the methods can use of the data (without labels) in the test set. There are
more negative instances, we mainly consider the amount of positive instances, and the number of positive
instances is called as the number of seeds. We randomly generate the training set with the given number
of seeds. Once we add one positive instance into the training set, we add all the its conflicting instances
into the training set. This is to reduce the correlation between training instances and testing instances for
a fair comparison. In later experiments, given the seed number, we will generate ten random training sets
and take the average of ten runs as the final performance. In later experiments, we do not explicitly report
the number of negative instances unless needed.
We adopt three widely used evaluation metrics for the classification task: Precision, Recall and the
F-measure
3
.
We compare the following methods for the task of product record linkage:
? Supervised Classifier (SC): the standard supervised classifier, which does not consider the unlabeled
data at all.
? Traditional Self-Training (t-ST): the traditional self-training method in Algorithm 1 which adds an
equal amount of samples of each class in pseudo labeled selection at each iteration.
? Proportional Self-Training (p-ST): the traditional self-training method in Algorithm 1 but add sam-
ples according to the class distribution at each iteration.
? Simple Group Based Self-Training (s-ST): a simplified version of our approach without the group
confidence valuation, which directly selects samples of high confidence scores estimated from the
classifier together with their conflicting pairs as negative samples at each iteration.
? Group Based Self-Training (g-ST): the proposed group based self-training algorithm in Algorithm 2,
which uses the group confidence evaluation method to select pseudo positive instances.
3
http:/en.wikipedia.org/wiki/Precision and recall
1317
Recall all the methods rely on the wrapped classifier. We select two classic but very different classi-
fiers: the Maximum Entropy model (MaxEnt) and the Decision Tree C4.5 (Tree). We implement these
two classifiers using the machine learning toolkit Weka
4
. We use the six similarity functions to obtain
similarity values between two records on each field as features. All the self-training based methods run
ten iterations and at each iteration they add the same number of positive instances, i.e. 30. Differen-
t methods select pseudo negative instances differently. t-ST does not consider the correlation between
data instances, and it adds top 30 confident negative instances. p-ST adds top 30 ?
#negative instances
#positive instances
con-
fident negative instances. Both p-ST and g-ST take all the conflicting instances of the selected pseudo
positive instances as the negative instances. We present the average numbers of pseudo negative instances
at an iteration in Table 2. As will be revealed later, although p-ST adds more negative instances, g-ST
performs much better than p-ST, which indicates simply adding more negative instances might not lead
to better performance. We do not perform specific preprocessing steps to make the data balanced (e.g.
under-sampling or over-sampling), and we find the data distribution does not significantly affect the
performance of the classifiers on our dataset.
Dataset t-ST p-ST s-ST g-ST
LAPTOP 30 950 845 854
CAMERA 30 655 569 584
Table 2: Average numbers of pseudo negative instances selected at each iteration.
6.3 Results and analysis
Overall performance comparison. To test the performance under weak supervision, we first set the
seed number to 30, which nearly takes up a proportion of 5% of the labeled data. We present the results
of different methods in Table 3 and Table 4. We first examine the performance of the baselines. We can
see that semi-supervised learning is very effective to improve over the the supervised classifier when the
amount of training data is small. It is interesting to see that s-ST performs best among all the baselines.
Recall that the major difference between s-ST and other baselines is that it select the conflicting pairs
of the pseudo positive instances as the negative instances. It indicates that it is important to consider the
correlation among the data instances. In addition, Decision Tree seems to be more competitive than Max-
imum Entropy Model for product record linkage. Then we take our group based self-training algorithm
into comparison. In terms of F1 measure, we can see that it is consistently better than all the baselines
on two datasets respectively by using two different classifiers. It is worth looking into the performance
comparison on precision and recall. We can see that (1) s-ST and g-ST yield better results in terms of
precision while the other baselines yield better results in terms of recall; (2) our method g-ST largely
improves over the best baseline s-ST. It is not surprising to have these observations since that our group
evaluation method is more careful at the selection of pseudo positive instance: it considers the evidence
from the conflicting instances.
Methods MaxEnt Decision Tree
P R F1 P R F1
SC 0.246 0.910 0.382 0.301 0.931 0.454
t-ST 0.264 0.925 0.411 0.328 0.921 0.484
p-ST 0.350 0.831 0.487 0.412 0.887 0.539
s-ST 0.979 0.632 0.767 0.909 0.754 0.823
g-ST 0.936 0.742 0.826 0.912 0.843 0.876
Table 3: Results on LAPTOP dataset.
Parameter tuning. In the above, we have shown the results of different methods with 30 positive in-
stances. The number of seeds is particularly important for self-training algorithms, and we want to ex-
4
http://www.cs.waikato.ac.nz/ml/weka
1318
Methods MaxEnt Decision Tree
P R F1 P R F1
SC 0.387 0.891 0.540 0.493 0.965 0.652
t-ST 0.352 0.892 0.504 0.537 0.963 0.677
p-ST 0.501 0.871 0.626 0.573 0.942 0.700
s-ST 0.931 0.479 0.632 0.962 0.570 0.716
g-ST 0.917 0.574 0.706 0.965 0.588 0.731
Table 4: Results on CAMERA dataset.
amine how it affects the performance of these methods. By varying the number of seeds from 10 to 50
with a step of 10, we present the F1 results in Figure 2 on two datasets by using two classifiers. We can
see that our method is consistently better than baselines with the varying of the seed number. Especially,
our method still works well when there is little labeled data, i.e. #seeds = 10. With a weaker classifier,
i.e. MaxEnt, our method yields more improvement than that with Tree. Besides the seed number, there
are another two factors which potentially affect the performance: (1) the iteration number and (2) the
number of pseudo positive instances selected at each iteration. We also examine the tuning results of
these two parameters and find our method is consistently better than s-ST with the varying of these two
factors. These results show that our method is very effective and it is of high stability and practicability.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(a) LAPTOP, MaxEnt
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(b) LAPTOP, Tree
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(c) CAMERA, MaxEnt
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(d) CAMERA, Tree
Figure 2: Performance comparison with varying seed numbers (i.e. # of positive instances).
1319
7 Conclusion
In this paper, we develop a novel variant of the self-training algorithm by leveraging the data characteris-
tic for the task of product record linkage. We joint consider a candidate linked pair and its corresponding
correlated pairs as a group, at the selection of pseudo labeled data. We propose a confidence evaluation
method for a group of instances, and incorporate it as a re-ranking step in the self-training algorithm. We
evaluate the novel self-training algorithm on two large datasets constructed based on real e-commerce
Websites. We adopt several competitive methods as comparisons and perform extensive experiments.
The results show that our method outperforms these baselines that do not consider data correlation. We
also carefully examine the affects of various parameters, and the tuning results indicate the stability and
robustness of our method.
The major contribution and novelty of this paper is the novel group confidence evaluation to model
the correlation existing in data. Although we develop the idea in the setting of self-training algorithms,
it will be promising to be applied in other learning algorithms, i.e. active learning.
Acknowledgements
We thank the anonymous reviewers for his/her thorough review and highly appreciate the comments.
This work was partially supported by the National Key Basic Research Program (973 Program) of China
under grant No. 2014CB340403, 2014CB340405 and NSFC Grant 61272340. Xin Zhao was supported
by MSRA PhD fellowship. Xin Zhao and Yuexin Wu contributed equally to this work and should be
considered as joint first authors. Xin Zhao is the corresponding author.
References
Rohan Baxter, Peter Christen, and Tim Churches. 2003. A comparison of fast blocking methods for record linkage.
In ACM SIGKDD, volume 3, pages 25?27. Citeseer.
Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational linguistics, 22(1):39?71.
Indrajit Bhattacharya and Lise Getoor. 2007. Collective entity resolution in relational data. ACM Transactions on
Knowledge Discovery from Data (TKDD), 1(1):5.
Mikhail Bilenko and Raymond J Mooney. 2003. Adaptive duplicate detection using learnable string similarity
measures. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 39?48. ACM.
Peter Christen. 2006. A comparison of personal name matching: Techniques and practical issues. In Data Mining
Workshops, 2006. ICDM Workshops 2006. Sixth IEEE International Conference on, pages 290?294. IEEE.
William W Cohen and Jacob Richman. 2002. Learning to match and cluster large high-dimensional data sets for
data integration. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 475?480. ACM.
William W Cohen, Pradeep D Ravikumar, Stephen E Fienberg, et al. 2003. A comparison of string distance
metrics for name-matching tasks. In IIWeb, volume 2003, pages 73?78.
Xin Dong, Alon Halevy, and Jayant Madhavan. 2005. Reference reconciliation in complex information spaces. In
Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 85?96. ACM.
Mohamed G Elfeky, Vassilios S Verykios, and Ahmed K Elmagarmid. 2002. Tailor: A record linkage toolbox. In
Data Engineering, 2002. Proceedings. 18th International Conference on, pages 17?28. IEEE.
Ivan P Fellegi and Alan B Sunter. 1969. A theory for record linkage. Journal of the American Statistical Associa-
tion, 64(328):1183?1210.
Karl Goiser and Peter Christen. 2006. Towards automated record linkage. In Proceedings of the fifth Australasian
conference on Data mining and analystics-Volume 61, pages 23?31. Australian Computer Society, Inc.
Zhiguang Liu Xishuang Dong Yi Guan and Jinfeng Yang. 2013. Reserved self-training: A semi-supervised senti-
ment classification method for chinese microblogs.
1320
Theo H?arder, G?unter Sauter, and Joachim Thomas. 1999. The intrinsic problems of structural heterogeneity and
an approach to their solution. The VLDB Journal, 8(1):25?43.
Oktie Hassanzadeh, Ken Q Pu, Soheil Hassas Yeganeh, Ren?ee J Miller, Lucian Popa, Mauricio A Hern?andez,
and Howard Ho. 2013. Discovering linkage points over web data. Proceedings of the VLDB Endowment,
6(6):445?456.
Yulan He and Deyu Zhou. 2011. Self-training from labeled features for sentiment analysis. Information Process-
ing & Management, 47(4):606?616.
Robert Isele and Christian Bizer. 2012. Learning expressive linkage rules using genetic programming. Proceed-
ings of the VLDB Endowment, 5(11):1638?1649.
Grzegorz Kondrak. 2005. N-gram similarity and distance. In String Processing and Information Retrieval, pages
115?126. Springer.
John R Koza, Forrest H Bennett III, and Oscar Stiffelman. 1999. Genetic programming as a Darwinian invention
machine. Springer.
Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
Ming Li and Zhi-Hua Zhou. 2005. Setred: Self-training with editing. In Advances in Knowledge Discovery and
Data Mining, pages 611?621. Springer.
Andrew McCallum, Kamal Nigam, and Lyle H Ungar. 2000. Efficient clustering of high-dimensional data sets
with application to reference matching. In Proceedings of the sixth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 169?178. ACM.
Un Yong Nahm, Mikhail Bilenko, and Raymond J Mooney. 2002. Two approaches to handling noisy variation in
text mining. In Proceedings of the ICML-2002 workshop on text learning (TextML2002), pages 18?27. Citeseer.
Saul B Needleman and Christian D Wunsch. 1970. A general method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443?453.
Mattis Neiling. 2006. Identification of real-world objects in multiple databases. In From Data and Information
Analysis to Knowledge Engineering, pages 63?74. Springer.
Axel-Cyrille Ngonga Ngomo and Klaus Lyko. 2013. Unsupervised learning of link specifications: Deterministic
vs. non-deterministic. Ontology Matching, page 25.
Byung-Won On, Nick Koudas, Dongwon Lee, and Divesh Srivastava. 2007. Group linkage. In Data Engineering,
2007. ICDE 2007. IEEE 23rd International Conference on, pages 496?505. IEEE.
John Ross Quinlan. 1993. C4. 5: programs for machine learning, volume 1. Morgan kaufmann.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern boot-
strapping. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume
4, pages 25?32. Association for Computational Linguistics.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. 2005. Semi-supervised self-training of object detec-
tion models.
Elke Rundensteiner. 1999. Special issue on data transformation. IEEE Techn. Bull. Data Engineering, 22(1).
Sheila Tejada, Craig A Knoblock, and Steven Minton. 2002. Learning domain-independent string transformation
weights for high accuracy object identification. In Proceedings of the eighth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 350?359. ACM.
William E Winkler. 1988. Using the em algorithm for weight computation in the fellegi-sunter model of record
linkage. In Proceedings of the Section on Survey Research Methods, American Statistical Association, volume
667, page 671.
William E Winkler. 2006. Overview of record linkage and current research directions. In Bureau of the Census.
Citeseer.
David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of
the 33rd annual meeting on Association for Computational Linguistics, pages 189?196. Association for Com-
putational Linguistics.
1321
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 56?65,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid
Wayne Xin Zhao?, Jing Jiang?, Hongfei Yan?, Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University, China
?School of Information Systems, Singapore Management University, Singapore
{zhaoxin,yhf}@net.pku.edu.cn, jingjiang@smu.edu.cn, lxm@pku.edu.cn
Abstract
Discovering and summarizing opinions from
online reviews is an important and challeng-
ing task. A commonly-adopted framework
generates structured review summaries with
aspects and opinions. Recently topic mod-
els have been used to identify meaningful re-
view aspects, but existing topic models do
not identify aspect-specific opinion words. In
this paper, we propose a MaxEnt-LDA hy-
brid model to jointly discover both aspects
and aspect-specific opinion words. We show
that with a relatively small amount of train-
ing data, our model can effectively identify as-
pect and opinion words simultaneously. We
also demonstrate the domain adaptability of
our model.
1 Introduction
With the dramatic growth of opinionated user-
generated content, consumers often turn to online
product reviews to seek advice while companies see
reviews as a valuable source of consumer feedback.
How to automatically understand, extract and sum-
marize the opinions expressed in online reviews has
therefore become an important research topic and
gained much attention in recent years (Pang and Lee,
2008). A wide spectrum of tasks have been studied
under review mining, ranging from coarse-grained
document-level polarity classification (Pang et al,
2002) to fine-grained extraction of opinion expres-
sions and their targets (Wu et al, 2009). In partic-
ular, a general framework of summarizing reviews
of a certain product is to first identify different as-
pects (a.k.a. features) of the given product and then
extract specific opinion expressions for each aspect.
For example, aspects of a restaurant may include
food, staff, ambience and price, and opinion expres-
sions for staff may include friendly, rude, etc. Be-
cause of the practicality of this structured summary
format, it has been adopted in several previous stud-
ies (Hu and Liu, 2004; Popescu and Etzioni, 2005;
Brody and Elhadad, 2010) as well as some commer-
cial systems, e.g. the ?scorecard? feature at Bing
shopping1.
Different approaches have been proposed to iden-
tify aspect words and phrases from reviews. Previ-
ous methods using frequent itemset mining (Hu and
Liu, 2004) or supervised learning (Jin and Ho, 2009;
Jin et al, 2009; Wu et al, 2009) have the limitation
that they do not group semantically related aspect
expressions together. Supervised learning also suf-
fers from its heavy dependence on training data. In
contrast, unsupervised, knowledge-lean topic mod-
eling approach has been shown to be effective in au-
tomatically identifying aspects and their representa-
tive words (Titov and McDonald, 2008; Brody and
Elhadad, 2010). For example, words such as waiter,
waitress, staff and service are grouped into one as-
pect.
We follow this promising direction and extend ex-
isting topic models to jointly identify both aspect
and opinion words, especially aspect-specific opin-
ion words. Current topic models for opinion mining,
which we will review in detail in Section 2, still lack
this ability. But separating aspect and opinion words
can be very useful. Aspect-specific opinion words
can be used to construct a domain-dependent senti-
1http://www.bing.com/shopping
56
ment lexicon and applied to tasks such as sentiment
classification. They can also provide more informa-
tive descriptions of the product or service being re-
viewed. For example, using more specific opinion
words such as cozy and romantic to describe the am-
bience aspect in a review summary is more meaning-
ful than using generic words such as nice and great.
To the best of our knowledge, Brody and Elhadad
(2010) are the first to study aspect-specific opinion
words, but their opinion word detection is performed
outside of topic modeling, and they only consider
adjectives as possible opinion words.
In this paper, we propose a new topic modeling
approach that can automatically separate aspect and
opinion words. A novelty of this model is the inte-
gration of a discriminative maximum entropy (Max-
Ent) component with the standard generative com-
ponent. The MaxEnt component allows us to lever-
age arbitrary features such as POS tags to help sepa-
rate aspect and opinion words. Because the supervi-
sion relies mostly on non-lexical features, although
our model is no longer fully unsupervised, the num-
ber of training sentences needed is relatively small.
Moreover, training data can also come from a differ-
ent domain and yet still remain effective, making our
model highly domain adaptive. Empirical evaluation
on large review data sets shows that our model can
effectively identify both aspects and aspect-specific
opinion words with a small amount of training data.
2 Related Work
Pioneered by the work of Hu and Liu (2004), review
summarization has been an important research topic.
There are usually two major tasks involved, namely,
aspect or feature identification and opinion extrac-
tion. Hu and Liu (2004) applied frequent itemset
mining to identify product features without supervi-
sion, and considered adjectives collocated with fea-
ture words as opinion words. Jin and Ho (2009),
Jin et al (2009) and Wu et al (2009) used super-
vised learning that requires hand-labeled training
sentences to identify both aspects and opinions. A
common limitation of these methods is that they do
not group semantically related aspect expressions to-
gether. Furthermore, supervised learning usually re-
quires a large amount of training data in order to per-
form well and is not easily domain adaptable.
Topic modeling provides an unsupervised and
knowledge-lean approach to opinion mining. Titov
and McDonald (2008) show that global topic models
such as LDA (Blei et al, 2003) may not be suitable
for detecting rateable aspects. They propose multi-
grain topic models for discovering local rateable as-
pects. However, they do not explicitly separate as-
pect and opinion words. Lin and He (2009) propose
a joint topic-sentiment model, but topic words and
sentiment words are still not explicitly separated.
Mei et al (2007) propose to separate topic and sen-
timent words using a positive sentiment model and
a negative sentiment model, but both models cap-
ture general opinion words only. In contrast, we
model aspect-specific opinion words as well as gen-
eral opinion words.
Recently Brody and Elhadad (2010) propose to
detect aspect-specific opinion words in an unsuper-
vised manner. They take a two-step approach by first
detecting aspect words using topic models and then
identifying aspect-specific opinion words using po-
larity propagation. They only consider adjectives as
opinion words, which may potentially miss opinion
words with other POS tags. We try to jointly capture
both aspect and opinion words within topic models,
and we allow non-adjective opinion words.
Another line of related work is about how to in-
corporate useful features into topic models (Zhu and
Xing, 2010; Mimno and McCallum, 2008). Our
MaxEnt-LDA hybrid bears similarity to these recent
models but ours is designed for opinion mining.
3 Model Description
Our model is an extension of LDA (Blei et al, 2003)
but captures both aspect words and opinion words.
To model the aspect words, we use a modified ver-
sion of the multi-grain topic models from (Titov and
McDonald, 2008). Our model is simpler and yet still
produces meaningful aspects. Specifically, we as-
sume that there are T aspects in a given collection of
reviews from the same domain, and each review doc-
ument contains a mixture of aspects. We further as-
sume that each sentence (instead of each word as in
standard LDA) is assigned to a single aspect, which
is often true based on our observation.
To understand how we model the opinion words,
let us first look at two example review sentences
57
from the restaurant domain:
The food was tasty.
The waiter was quite friendly.
We can see that there is a strong association of
tasty with food and similarly of friendly with waiter.
While both tasty and friendly are specific to the
restaurant domain, they are each associated with
only a single aspect, namely food and staff, respec-
tively. Besides these aspect-specific opinion words,
we also see general opinion words such as great
in the sentence ?The food was great!? These gen-
eral opinion words are shared across aspects, as op-
posed to aspect-specific opinion words which are
used most commonly with their corresponding as-
pects. We therefore introduce a general opinion
model and T aspect-specific opinion models to cap-
ture these different opinion words.
3.1 Generative Process
We now describe the generative process of the
model. First, we draw several multinomial word dis-
tributions from a symmetric Dirichlet prior with pa-
rameter ?: a background model ?B, a general aspect
model ?A,g, a general opinion model ?O,g, T as-
pect models {?A,t}Tt=1 and T aspect-specific opin-
ion models {?O,t}Tt=1. All these are multinomial
distributions over the vocabulary, which we assume
has V words. Then for each review document d, we
draw a topic distribution ?d?Dir(?) as in standard
LDA. For each sentence s in document d, we draw
an aspect assignment zd,s?Multi(?d).
Now for each word in sentence s of document d,
we have several choices: The word may describe the
specific aspect (e.g. waiter for the staff aspect), or a
general aspect (e.g. restaurant), or an opinion either
specific to the aspect (e.g. friendly) or generic (e.g.
great), or a commonly used background word (e.g.
know). To distinguish between these choices, we in-
troduce two indicator variable, yd,s,n and ud,s,n, for
the nth word wd,s,n. We draw yd,s,n from a multi-
nomial distribution over {0, 1, 2}, parameterized by
pid,s,n. yd,s,n determines whether wd,s,n is a back-
ground word, aspect word or opinion word. We will
discuss how to set pid,s,n in Section 3.2. We draw
ud,s,n from a Bernoulli distribution over {0, 1} pa-
rameterized by p, which in turn is drawn from a sym-
metric Beta(?). ud,s,n determines whether wd,s,n is
general or aspect-specific. We then draw wd,s,n as
T
?
?
B
?
A,t
?
O,t
?
A,g
?
O,g
D
S
N
d,s
x
d,s,n
pi
d,s,n
y
d,s,n
w
d,s,n
u
d,s,n
z
d,s
?
d
{B,O,A}
? p ?
?
Figure 1: The plate notation of our model.
follows:
wd,s,n ?
?
??????
??????
Multi(?B) if yd,s,n = 0
Multi(?A,zd,s) if yd,s,n = 1, ud,s,n = 0
Multi(?A,g) if yd,s,n = 1, ud,s,n = 1
Multi(?O,zd,s) if yd,s,n = 2, ud,s,n = 0
Multi(?O,g) if yd,s,n = 2, ud,s,n = 1
.
Figure 1 shows our model using the plate notation.
3.2 Setting pi with a Maximum Entropy Model
A simple way to set pid,s,n is to draw it from a
symmetric Dirichlet prior. However, as suggested
in (Mei et al, 2007; Lin and He, 2009), fully un-
supervised topic models are unable to identify opin-
ion words well. An important observation we make
is that aspect words and opinion words usually play
different syntactic roles in a sentence. Aspect words
tend to be nouns while opinion words tend to be ad-
jectives. Their contexts in sentences can also be dif-
ferent. But we do not want to use strict rules to sepa-
rate aspect and opinion words because there are also
exceptions. E.g. verbs such as recommend can also
be opinion words.
In order to use information such as POS tags
to help discriminate between aspect and opinion
words, we propose a novel idea as follows: We set
pid,s,n using a maximum entropy (MaxEnt) model
applied to a feature vector xd,s,n associated with
wd,s,n. xd,s,n can encode any arbitrary features we
think may be discriminative, e.g. previous, current
and next POS tags. Formally, we have
p(yd,s,n = l|xd,s,n) = pid,s,nl =
exp (?l ? xd,s,n
)
?2
l?=0 exp
(?l? ? xd,s,n
) ,
58
where {?l}2l=0 denote the MaxEnt model weights
and can be learned from a set of training sentences
with labeled background, aspect and opinion words.
This MaxEnt-LDA hybrid model is partially in-
spired by (Mimno and McCallum, 2008).
As for the features included in x, currently we
use two types of simple features: (1) lexical features
which include the previous, the current and the next
words {wi?1, wi, wi+1}, and (2) POS tag features
which include the previous, the current and the next
POS tags {POSi?1, POSi, POSi+1}.
3.3 Inference
We use Gibbs sampling to perform model inference.
Due to the space limit, we leave out the derivation
details and only show the sampling formulas. Note
that the MaxEnt component is trained first indepen-
dently of the Gibbs sampling procedure, that is, in
Gibbs sampling, we assume that the ? parameters
are fixed.
We use w to denote all the words we observe in
the collection, x to denote all the feature vectors for
these words, and y, z and u to denote all the hidden
variables. First, given the assignment of all other
hidden variables, to sample a value for zd,s, we use
the following formula:
P (zd,s = t|z?(d,s),y,u,w,x) ?
cd(t) + ?
cd(?) + T?
?
( ?
(
cA,t(?) + V ?
)
?
(
cA,t(?) + nA,t(?) + V ?
) ?
V?
v=1
?
(
cA,t(v) + nA,t(v) + ?
)
?
(
cA,t(v) + ?
)
)
?
( ?
(
cO,t(?) + V ?
)
?
(
cO,t(?) + nO,t(?) + V ?
) ?
V?
v=1
?
(
cO,t(v) + nO,t(v) + ?
)
?
(
cO,t(v) + ?
)
)
.
Here cd(t) is the number of sentences assigned to as-
pect t in document d, and cd(?) is the number of sen-
tences in document d. cA,t(v) is the number of times
word v is assigned as an aspect word to aspect t,
and cO,t(v) is the number of times word v is assigned
as an opinion word to aspect t. cA,t(?) is the total num-
ber of times any word is assigned as an aspect word
to aspect t, and cO,t(?) is the total number of times any
word is assigned as an opinion word to aspect t. All
these counts represented by a c variable exclude sen-
tence s of document d. nA,t(v) is the number of times
word v is assigned as an aspect word to aspect t in
sentence s of document d, and similarly, nO,t(v) is the
number of times word v is assigned as an opinion
word to aspect t in sentence s of document d.
Then, to jointly sample values for yd,s,n and
ud,s,n, we have
P (yd,s,n = 0|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?0 ? xd,s,n)?
l? exp(?l? ? xd,s,n)
?
cB(wd,s,n) + ?
cB(?) + V ?
,
P (yd,s,n = l, ud,s,n = b|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?l ? xd,s,n)?
l? exp(?l? ? xd,s,n)
? g(wd,s,n, zd,s, l, b),
where the function g(v, t, l, b) (1 ? v ? V, 1 ? t ?
T, l ? {1, 2}, b ? {0, 1}) is defined as follows:
g(v, t, l, b) =
?
??????????
??????????
cA,t(v) +?
cA,t(?) +V ?
? c(0)+?c(?)+2? if l = 1, b = 0
cO,t(v) +?
cO,t(?) +V ?
? c(0)+?c(?)+2? if l = 2, b = 0
cA,g(v) +?
cA,g(?) +V ?
? c(1)+?c(?)+2? if l = 1, b = 1
cO,g(v) +?
cO,g(?) +V ?
? c(1)+?c(?)+2? if l = 2, b = 1.
.
Here the various c variables denote various counts
excluding the nth word in sentence s of document d.
Due to space limit, we do not give full explanation
here.
4 Experiment Setup
To evaluate our MaxEnt-LDA hybrid model for
jointly modeling aspect and opinion words, we used
a restaurant review data set previously used in (Ganu
et al, 2009; Brody and Elhadad, 2010) and a ho-
tel review data set previously used in (Baccianella
et al, 2009). We removed stop words and used the
Stanford POS Tagger2 to tag the two data sets. Only
reviews that have no more than 50 sentences were
used. We also kept another version of the data which
includes the stop words for the purpose of extracting
the contextual features included in x. Some details
of the data sets are given in Table 1.
For our hybrid model, we ran 500 iterations of
Gibbs sampling. Following (Griffiths and Steyvers,
2004), we fixed the Dirichlet priors as follows: ? =
2http://nlp.stanford.edu/software/tagger.shtml
59
data set restaurant hotel
#tokens 1,644,923 1,097,739
#docs 52,574 14,443
Table 1: Some statistics of the data sets.
data set #sentences #tokens
restaurant 46 634
cell phone 125 4414
DVD player 180 3024
Table 2: Some statistics of the labeled training data.
50/T , ? = 0.1 and ? = 0.5. We also experimented
with other settings of these priors and did not notice
any major difference. For MaxEnt training, we tried
three labeled data sets: one that was taken from the
restaurant data set and manually annotated by us3,
and two from the annotated data set used in (Wu et
al., 2009). Note that the latter two were used for test-
ing domain adaptation in Section 6.3. Some details
of the training sets are shown in Table 2.
In our preliminary experiments, we also tried two
variations of our MaxEnt-LDA hybrid model. (1)
The first is a fully unsupervised model where we
used a uniform Dirichlet prior for pi. We found
that this unsupervised model could not separate as-
pect and opinion words well. (2) The second is a
bootstrapping version of the MaxEnt-LDA model
where we used the predicted values of y as pseudo
labels and re-trained the MaxEnt model iteratively.
We found that this bootstrapping procedure did not
boost the overall performance much and even hurt
the performance a little in some cases. Due to the
space limit we do not report these experiments here.
5 Evaluation
In this section we report the evaluation of our
model. We refer to our MaxEnt-LDA hybrid model
as ME-LDA. We also implemented a local version
of the standard LDA method where each sentence
is treated as a document. This is the model used
in (Brody and Elhadad, 2010) to identify aspects,
and we refer to this model as LocLDA.
Food Staff Order Taking Ambience
chocolate service wait room
dessert food waiter dining
cake staff wait tables
cream excellent order bar
ice friendly minutes place
desserts attentive seated decor
coffee extremely waitress scene
tea waiters reservation space
bread slow asked area
cheese outstanding told table
Table 4: Sample aspects of the restaurant domain using
LocLDA. Note that the words in bold are opinion words
which are mixed with aspect words.
5.1 Qualitative Evaluation
For each of the two data sets, we show four sample
aspects identified by ME-LDA in Table 3 and Ta-
ble 5. Because the hotel domain is somehow similar
to the restaurant domain, we used the labeled train-
ing data from the restaurant domain also for the hotel
data set. From the tables we can see that generally
aspect words are quite coherent and meaningful, and
opinion words correspond to aspects very well. For
comparison, we also applied LocLDA to the restau-
rant data set and present the aspects in Table 4. We
can see that ME-LDA and LocLDA give similar as-
pect words. The major difference between these two
models is that ME-LDA can sperate aspect words
and opinion words, which can be very useful. ME-
LDA is also able to separate general opinion words
from aspect-specific ones, giving more informative
opinion expressions for each aspect.
5.2 Evaluation of Aspects Identification
We also quantitatively evaluated the quality of the
automatically identified aspects. Ganu et al (2009)
provide a set of annotated sentences from the restau-
rant data set, in which each sentence has been as-
signed one or more labels from a gold standard label
set S = {Staff, Food, Ambience, Price, Anecdote,
Misc}. To evaluate the quality of our aspect iden-
tification, we chose from the gold standard labels
three major aspects, namely Staff, Food and Ambi-
ence. We did not choose the other aspects because
(1) Price is often mixed with other aspects such as
Food, and (2) Anecdote and Misc do not show clear
3We randomly selected 46 sentences for manual annotation.
60
Food Staff Order Taking Ambience General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
chocolate good service friendly table seated room small good
dessert best staff attentive minutes asked dining nice well
cake great food great wait told tables beautiful nice
cream delicious wait nice waiter waited bar romantic great
ice sweet waiter good reservation waiting place cozy better
desserts hot place excellent order long decor great small
coffee amazing waiters helpful time arrived scene open bad
tea fresh restaurant rude hour rude space warm worth
bread tasted waitress extremely manager sat area feel definitely
cheese excellent waitstaff slow people finally table comfortable special
Table 3: Sample aspects and opinion words of the restaurant domain using ME-LDA.
Service Room Condition Ambience Meal General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
staff helpful room shower room quiet breakfast good great
desk friendly bathroom small floor open coffee fresh good
hotel front bed clean hotel small fruit continental nice
english polite air comfortable noise noisy buffet included well
reception courteous tv hot street nice eggs hot excellent
help pleasant conditioning large view top pastries cold best
service asked water nice night lovely cheese nice small
concierge good rooms safe breakfast hear room great lovely
room excellent beds double room overlooking tea delicious better
restaurant rude bath well terrace beautiful cereal adequate fine
Table 5: Sample aspects and opinion words of the hotel domain using ME-LDA.
patterns in either word usage or writing styles, mak-
ing it even hard for humans to identify them. Brody
and Elhadad (2010) also only used these three as-
pects for quantitative evaluation. To avoid ambigu-
ity, we used only the single-labeled sentences for
evaluation. About 83% of the labeled sentences have
a single label, which confirms our observation that a
sentence usually belongs to a single aspect.
We first ran ME-LDA and LocLDA each to get
an inferred aspect set T . Following (Brody and El-
hadad, 2010), we set the number of aspects to 14
in both models. We then manually mapped each in-
ferred aspect to one of the six gold standard aspects,
i.e., we created a mapping function f(t) : T ? S.
For sentence s of document d, we first assign it to an
inferred aspect as follows:
t? = argmax
t?T
Nd,s?
n=1
logP (wd,s,n|t).
We then assign the gold standard aspect f(t?) to this
Aspect Method Precision Recall F-1
Staff LocLDA 0.804 0.585 0.677
ME-LDA 0.779 0.540 0.638
Food LocLDA 0.898 0.648 0.753
ME-LDA 0.874 0.787 0.828
Ambience LocLDA 0.603 0.677 0.638
ME-LDA 0.773 0.558 0.648
Table 6: Results of aspects identification on restaurant.
sentence. We then calculated the F-1 score of the
three aspects: Staff, Food and Ambience. The re-
sults are shown in Table 6. Generally ME-LDA has
given competitive results compared with LocLDA.
For Food and Ambience ME-LDA outperformed Lo-
cLDA, while for Staff ME-LDA is a little worse
than LocLDA. Note that ME-LDA is not designed
to compete with LocLDA for aspect identification.
61
5.3 Evaluation of Opinion Identification
Since the major advantage of ME-LDA is its abil-
ity to separate aspect and opinion words, we further
quantitatively evaluated the quality of the aspect-
specific opinion words identified by ME-LDA.
Brody and Elhadad (2010) has constructed a gold
standard set of aspect-specific opinion words for the
restaurant data set. In this gold standard set, they
manually judged eight out of the 14 automatically
inferred aspects they had: J = {Ambiance, Staff,
Food-Main Dishes, Atmosphere-Physical, Food-
Baked Goods, Food-General, Drinks, Service}.
Each word is assigned a polarity score ranging from
-2.0 to 2.0 in each aspect. We used their gold stan-
dard words whose polarity scores are not equal to
zero. Because their gold standard only includes
adjectives, we also manually added more opinion
words into the gold standard set. To do so, we took
the top 20 opinion words returned by our method
and two baseline methods, pooled them together,
and manually judged them. We use precision at n
(P@n), a commonly used metric in information re-
trieval, for evaluation. Because top words are more
important in opinion models, we set n to 5, 10 and
20. For both ME-LDA and BL-1 below, we again
manually mapped each automatically inferred aspect
to one of the gold standard aspects.
Since LocLDA does not identify aspect-specific
opinion words, we consider the following two base-
line methods that can identify aspect-specific opin-
ion words:
BL-1: In this baseline, we start with all adjectives
as candidate opinion words, and use mutual infor-
mation (MI) to rank these candidates. Specifically,
given an aspect t, we rank the candidate words ac-
cording to the following scoring function:
ScoreBL-1(w, t) =
?
v?Vt
p(w, v) log p(w, v)p(w)p(v) ,
where Vt is the set of the top-100 frequent aspect
words from ?A,t.
BL-2: In this baseline, we first use LocLDA to learn
a topic distribution for each sentence. We then as-
sign a sentence to the aspect with the largest proba-
bility and hence get sentence clusters. We manually
map these clusters to the eight gold standard aspects.
Finally, for each aspect we rank adjectives by their
Method P@5 P@10 P@20
ME-LDA 0.825?,? 0.700? 0.569?
BL-1 0.400 0.450 0.469
BL-2 0.725 0.650 0.563
Table 7: Average P@n of aspect-specific opinion words
on restaurant. * and ? indicate that the improvement hy-
pothesis is accepted at confidence level 0.9 respectively
for BL-1 and BL-2.
frequencies in the aspect and treat these as aspect-
specific opinion words.
The basic results in terms of the average precision
at n over the eight aspects are shown in Table 7. We
can see that ME-LDA outperformed the two base-
lines consistently. Especially, for P@5, ME-LDA
gave more than 100% relative improvement over
BL-1. The absolute value of 0.825 for P@5 also
indicates that top opinion words discovered by our
model are indeed meaningful.
5.4 Evaluation of the Association between
Opinion Words and Aspects
The evaluation in the previous section shows that our
model returns good opinion words for each aspect.
It does not, however, directly judge how aspect-
specific those opinion words are. This is because the
gold standard created by (Brody and Elhadad, 2010)
also includes general opinion words. E.g. friendly
and good may both be judged to be opinion words
for the staff aspect, but the former is more specific
than the latter. We suspect that BL-2 has comparable
performance with ME-LDA for this reason. So we
further evaluated the association between opinion
words and aspects by directly looking at how easy
it is to infer the corresponding aspect by only look-
ing at an aspect-specific opinion word. We selected
four aspects for evaluation: Ambiance, Staff, Food-
Main Dishes and Atmosphere-Physical . We chose
these four aspects because they are quite different
from each other and thus manual judgments on these
four aspects can be more objective. For each aspect,
similar to the pooling strategy in IR, we pooled the
top 20 opinion words identified by BL-1, BL-2 and
ME-LDA. We then asked two human assessors to
assign an association score to each of these words
as follows: If the word is closely associated with an
aspect, a score of 2 is given; if it is marginally as-
62
Metrics Dataset BL-2 ME-LDA
nDCG@5 Restaurant 0.647 0.764
Hotel 0.782 0.820
nDCG@10 Restaurant 0.781 0.897
Hotel 0.722 0.789
Table 8: Average nDCG performance of BL-2 and ME-
LDA. Because only four aspects were used for evaluation,
we did not perform statistical significance test. We found
that in all cases ME-LDA outperformed BL-2 for either
all aspects or three out of four aspects.
sociated with an aspect, a score of 1 is given; other-
wise, 0 is given. We calculated the Kappa statistics
of agreement, and we got a quite high Kappa value
of 0.8375 and 0.7875 respectively for the restaurant
data set and the hotel data set. Then for each word
in an aspect, we took the average of the scores of
the two assessors. We used an nDCG-like metric to
compare the performance of our model and of BL-2.
The metric is defined as follows:
nDCG@k(t,M) =
?k
i=1
Score(Mt,i)
log2(i+1)
iDCG@k(t) ,
where Mt,i is the ith aspect-specific opinion word
inferred by method M for aspect t, Score(Mt,i) is
the association score of this word, and iDCG@k(t)
is the score of the ideal DCG measure at k for as-
pect t, that is, the maximum DCG score assuming
an ideal ranking. We chose k = 5 and k = 10. The
average nDCG over the four aspects are presented
in Table 8. We can see that ME-LDA outperformed
BL-2 quite a lot for the restaurant data set, which
conforms to our hypothesis that ME-LDA generates
aspect-specific opinion words of stronger associa-
tion with aspects. For the hotel data set, ME-LDA
outperformed a little. This may be due to the fact
that we used the restaurant training data for the ho-
tel data set.
6 Further Analysis of MaxEnt
In this section, we perform some further evaluation
and analysis of the MaxEnt component in our model.
6.1 Feature Selection
Previous studies have shown that simple POS fea-
tures and lexical features can be very effective for
discovering aspect words and opinion words (Hu
Methods Average F-1
LocLDA 0.690
ME-LDA + A 0.631
ME-LDA + B 0.695
ME-LDA + C 0.705
Table 9: Comparison of the average F-1 using different
feature sets for aspect identification on restaurant.
and Liu, 2004; Jin et al, 2009; Wu et al, 2009;
Brody and Elhadad, 2010). for POS features, since
we observe that aspect words tend to be nouns while
opinion words tend to be adjectives but sometimes
also verbs or other part-of-speeches, we can expect
that POS features should be quite useful. As for lexi-
cal features, words from a sentiment lexicon can also
be helpful in discovering opinion words.
However, lexical features are more diverse so pre-
sumably we need more training data in order to de-
tect useful lexical features. Lexical features are also
more domain-dependent. On the other hand, we hy-
pothesize that POS features are more effective when
the amount of training data is small and/or the train-
ing data comes from a different domain. We there-
fore compare the following three sets of features:
? A: wi?1, wi, wi+1
? B: POSi?1, POSi, POSi+1
? C: A+ B
We show the comparison of the performance in Ta-
ble 9 using the average F-1 score defined in Sec-
tion 5.2 for aspect identification, and in Table 10 us-
ing the average P@n measure defined in Section 5.3
for opinion identification. We can see that Set B
plays the most important part, which conforms to
our hypothesis that POS features are very important
in opinion mining. In addition, we can see that Set C
performs a bit better than Set B, which indicates that
some lexical features (e.g., general opinion words)
may also be helpful. Note that here the training data
is from the same domain as the test data, and there-
fore lexical features are likely to be useful.
6.2 Examine the Size of Labeled Data
As we have seen, POS features play the major role
in discriminating between aspect and opinion words.
Because there are much fewer POS features than
word features, we expect that we do not need many
63
Methods P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + A 0.150 0.200 0.231
ME-LDA + B 0.775 0.688 0.569
ME-LDA + C 0.825 0.700 0.569
Table 10: Comparison of the average P@n using different
feature sets for opinion identification on restaurant.
Method F-1
LocalLDA 0.690
ME-LDA + 10 0.629
ME-LDA + 20 0.692
ME-LDA + 30 0.691
ME-LDA + 40 0.726
ME-LDA + 46 0.705
Table 11: Average F-1 with differen sizes of training data
on restaurant.
labeled sentences to learn the POS-based patterns.
We now examine the sensitivity of the performance
with respect to the amount of labeled data. We gen-
erated four smaller training data sets with 10, 20, 30
and 40 sentences each from the whole training data
set we have, which consists of 46 labeled sentences.
The results are shown in Table 11 and Table 12. We
can see that generally the performance stays above
BL when the number of training sentences is 20 or
more. This indicates that our model needs only a
relatively small number of high-quality training sen-
tences to achieve good results.
6.3 Domain Adaption
Since we find that the MaxEnt supervision relies
more on POS features than lexical features, we also
hypothesize that if the training sentences come from
a different domain the performance can still remain
relatively high. To test this hypothesis, we tried two
Method P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + 10 0.700 0.563 0.488
ME-LDA + 20 0.875 0.650 0.600
ME-LDA + 30 0.825 0.700 0.569
ME-LDA + 40 0.825 0.688 0.581
ME-LDA + 46 0.825 0.700 0.569
Table 12: Average P@n of aspect-specific opinion words
with differen sizes of training data on restaurant.
Method Average F-1
restaurant + B 0.695
restaurant + C 0.705
cell phone + B 0.662
cell phone + C 0.629
DVD player + B 0.686
DVD player + C 0.635
Table 13: Average F-1 performance for domain adaption
on restaurant.
Method P@5 P@10 P@20
restaurant + B 0.775 0.688 0.569
restaurant + C 0.825 0.700 0.569
cell phone + B 0.775 0.675 0.588
cell phone + C 0.750 0.688 0.594
DVD player + B 0.775 0.713 0.575
DVD player + C 0.825 0.663 0.588
Table 14: Average P@n of aspect-specific opinion words
for domain adaption on restaurant.
quite different training data sets, one from the cell
phone domain and the other from the DVD player
domain, both used in (Wu et al, 2009).
We consider two feature sets defined in Sec-
tion 6.1 for domain adaption, namely B and C. The
results are shown in Table 13 and Table 14.
For aspect identification, using out-of-domain
training data performed worse than using in-domain
training data, but the absolute performance is still
decent. And interestingly, we can see that using B
is better than using C, indicating that lexical features
may hurt the performance in the cross-domain set-
ting. It suggests that lexical features are not easily
adaptable across domains for aspect identification.
For opinion identification, we can see that there
is no clear difference between using out-of-domain
training data and using in-domain training data,
which may indicate that our opinion identification
component is robust in domain adaption. Also, we
cannot easily tell whetherB has advantage over C for
opinion identification. One possible reason may be
that those general opinion words are useful across
domains, so lexical features may still be useful for
domain adaption.
64
7 Conclusions
In this paper, we presented a topic modeling ap-
proach that can jointly identify aspect and opinion
words, using a MaxEnt-LDA hybrid. We showed
that by incorporating a supervised, discriminative
maximum entropy model into an unsupervised, gen-
erative topic model, we could leverage syntactic fea-
tures to help separate aspect and opinion words.
We evaluated our model on two large review data
sets from the restaurant and the hotel domains. We
found that our model was competitive in identifying
meaningful aspects compared with previous mod-
els. Most importantly, our model was able to iden-
tify meaningful opinion words strongly associated
with different aspects. We also demonstrated that
the model could perform well with a relatively small
amount of training data or with training data from a
different domain.
Our model provides a principled way to jointly
model both aspects and opinions. One of the future
directions we plan to explore is to use this model
to help sentence-level extraction of specific opinions
and their targets, which previously was only tackled
in a fully supervised manner. Another direction is to
extend the model to support polarity classification.
ACKNOWLEDGMENT
The authors Xin Zhao, Hongfei Yan and Xiaom-
ing Li are partially supported by NSFC under the
grant No. 70903008 and 60933004, CNGI grant No.
2008-122, 863 Program No. 2009AA01Z143, and
the Open Fund of the State Key Laboratory of Soft-
ware Development Environment under Grant No.
SKLSDE-2010KF-03, Beihang University.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews. In
Proceedings of the 31st ECIR.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3.
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
Proceedings of Human Language Technologies: The
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predictions
using review text content. In Proceedings of the 12th
International Workshop on the Web and Databases.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
HMM-based learning framework for web opinion min-
ing. In Proceedings of the 26th International Confer-
ence on Machine Learning.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
OpinionMiner: A novel machine learning system for
web opinion mining and extraction. In Proceedings of
the 15th ACM SIGKDD.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Proceed-
ing of the Eighteenth ACM Conference on Information
and Knowledge Management.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
Modeling facets and opinions in weblogs. In Proceed-
ings of the 16th International Conference on World
Wide Web.
David Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. In Conference on
Uncertainty in Artificial Intelligence.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of the HLT-EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Jun Zhu and Eric P. Xing. 2010. Conditional topic ran-
dom fields. In Proceedings of the 27th International
Conference on Machine Learning.
65
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1245?1254, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Learning for Coreference Resolution with Markov Logic
Yang Song1, Jing Jiang2, Wayne Xin Zhao3, Sujian Li1, Houfeng Wang1
1Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
2School of Information Systems, Singapore Management University, Singapore
3School of Electronics Engineering and Computer Science, Peking University, China
{ysong, lisujian, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg, batmanfly@gmail.com
Abstract
Pairwise coreference resolution models must
merge pairwise coreference decisions to gen-
erate final outputs. Traditional merging meth-
ods adopt different strategies such as the best-
first method and enforcing the transitivity con-
straint, but most of these methods are used
independently of the pairwise learning meth-
ods as an isolated inference procedure at the
end. We propose a joint learning model which
combines pairwise classification and mention
clustering with Markov logic. Experimen-
tal results show that our joint learning sys-
tem outperforms independent learning sys-
tems. Our system gives a better performance
than all the learning-based systems from the
CoNLL-2011 shared task on the same dataset.
Compared with the best system from CoNLL-
2011, which employs a rule-based method,
our system shows competitive performance.
1 Introduction
The task of noun phrase coreference resolution is to
determine which mentions in a text refer to the same
real-world entity. Many methods have been pro-
posed for this problem. Among them the mention-
pair model (McCarthy and Lehnert, 1995) is one of
the most influential ones and can achieve the state-
of-the-art performance (Bengtson and Roth, 2008).
The mention-pair model splits the task into three
parts: mention detection, pairwise classification and
mention clustering. Mention detection aims to iden-
tify anaphoric noun phrases, including proper nouns,
common noun phrases and pronouns. Pairwise clas-
sification takes a pair of detected anaphoric noun
phrase candidates and determines whether they re-
fer to the same entity. Because these classification
decisions are local, they do not guarantee that can-
didate mentions are partitioned into clusters. There-
fore a mention clustering step is needed to resolve
conflicts and generate the final mention clusters.
Much work has been done following the mention-
pair model (Soon et al 2001; Ng and Cardie, 2002).
In most work, pairwise classification and mention
clustering are done sequentially. A major weak-
ness of this approach is that pairwise classification
considers only local information, which may not be
sufficient to make correct decisions. One way to
address this weakness is to jointly learn the pair-
wise classification model and the mention cluster-
ing model. This idea has been explored to some
extent by McCallum and Wellner (2005) using con-
ditional undirected graphical models and by Finley
and Joachims (2005) using an SVM-based super-
vised clustering method.
In this paper, we study how to use a different
learning framework, Markov logic (Richardson and
Domingos, 2006), to learn a joint model for both
pairwise classification and mention clustering un-
der the mention-pair model. We choose Markov
logic because of its appealing properties. Markov
logic is based on first-order logic, which makes
the learned models readily interpretable by humans.
Moreover, joint learning is natural under the Markov
logic framework, with local pairwise classification
and global mention clustering both formulated as
weighted first-order clauses. In fact, Markov logic
has been previously used by Poon and Domingos
(2008) for coreference resolution and achieved good
1245
results, but it was used for unsupervised coreference
resolution and the method was based on a different
model, the entity-mention model.
More specifically, to combine mention cluster-
ing with pairwise classification, we adopt the com-
monly used strategies (such as best-first clustering
and transitivity constraint), and formulate them as
first-order logic formulas under the Markov logic
framework. Best-first clustering has been previously
studied by Ng and Cardie (2002) and Bengtson and
Roth (2008) and found to be effective. Transitivity
constraint has been applied to coreference resolution
by Klenner (2007) and Finkel and Manning (2008),
and also achieved good performance.
We evaluate Markov logic-based method on the
dataset from CoNLL-2011 shared task. Our ex-
periment results demonstrate the advantage of joint
learning of pairwise classification and mention clus-
tering over independent learning. We examine
best-first clustering and transitivity constraint in our
methods, and find that both are very useful for coref-
erence resolution. Compared with the state of the
art, our method outperforms a baseline that repre-
sents a typical system using the mention-pair model.
Our method is also better than all learning systems
from the CoNLL-2011 shared task based on the re-
ported performance. Even with the top system from
CoNLL-2011, our performance is still competitive.
In the rest of this paper, we first describe a stan-
dard pairwise coreference resolution system in Sec-
tion 2. We then present our Markov logic model for
pairwise coreference resolution in Section 3. Exper-
imental results are given in Section 4. Finally we
discuss related work in Section 5 and conclude in
Section 6.
2 Standard Pairwise Coreference
Resolution
In this section, we describe standard learning-based
framework for pairwise coreference resolution. The
major steps include mention detection, pairwise
classification and mention clustering.
2.1 Mention Detection
For mention detection, traditional methods include
learning-based and rule-based methods. Which kind
of method to choose depends on specific dataset. In
this paper, we first consider all the noun phrases
in the given text as candidate mentions. With-
out gold standard mention boundaries, we use a
well-known preprocessing tool from Stanford?s NLP
group1 to extract noun phrases. After obtaining all
the extracted noun phrases, we also use a rule-based
method to remove some erroneous candidates based
on previous studies (e.g. Lee et al(2011), Uryupina
et al(2011)). Some examples of these erroneous
candidates include stop words (e.g. uh, hmm), web
addresses (e.g. http://www.google.com),
numbers (e.g. $9,000) and pleonastic ?it? pronouns.
2.2 Pairwise Classification
For pairwise classification, traditional learning-
based methods usually adopt a classification model
such as maximum entropy models and support vec-
tor machines. Training instances (i.e. positive and
negative mention pairs) are constructed from known
coreference chains, and features are defined to rep-
resent these instances.
In this paper, we build a baseline system that uses
maximum entropy models as the classification algo-
rithm. For generation of training instances, we fol-
low the method of Bengtson and Roth (2008). For
each predicted mention m, we generate a positive
mention pair between m and its closest preceding
antecedent, and negative mention pairs by pairing m
with each of its preceding predicted mentions which
are not coreferential with m. To avoid having too
many negative instances, we impose a maximum
sentence distance between the two mentions when
constructing mention pairs. This is based on the in-
tuition that for each anaphoric mention, its preced-
ing antecedent should appear quite near it, and most
coreferential mention pairs which have a long sen-
tence distance can be resolved using string match-
ing. During the testing phase, we generate men-
tion pairs for each mention candidate with each of
its preceding mention candidates and use the learned
model to make coreference decisions for these men-
tion pairs. We also impose the sentence distance
constraint and use string matching for mention pairs
with a sentence distance exceeding the threshold.
1http://nlp.stanford.edu/software/corenlp.shtml
1246
2.3 Mention Clustering
After obtaining the coreferential results for all men-
tion pairs, some clustering method should be used to
generate the final output. One strategy is the single-
link method, which links all the mention pairs that
have a prediction probability higher than a threshold
value. Two other alternative methods are the best-
first clustering method and clustering with the tran-
sitivity constraint. Best-first clustering means that
for each candidate mention m, we select the best
one from all its preceding candidate mentions based
on the prediction probabilities. A threshold value
is given to filter out those mention pairs that have a
low probability to be coreferential. Transitivity con-
straint means that if a and b are coreferential and
b and c are coreferential, then a and c must also be
coreferential. Previous work has found that best-first
clustering and transitivity constraint-based cluster-
ing are better than the single-link method. Finally
we remove all the singleton mentions.
3 Markov Logic for Pairwise Coreference
Resolution
In this section, we present our method for joint
learning of pairwise classification and mention clus-
tering using Markov logic. For mention detection,
training instance generation and postprocessing, our
method follows the same procedures as described in
Section 2. In what follows, we will first describe
the basic Markov logic networks (MLN) framework,
and then introduce the first-order logic formulas we
use in our MLN including local formulas and global
formulas which perform pairwise classification and
mention clustering respectively. Through this way,
these two isolated parts are combined together, and
joint learning and inference can be performed in a
single framework. Finally we present inference and
parameter learning methods.
3.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic (Richardson and Domingos,
2006; Riedel, 2008). A Markov logic network con-
sists of a set of first-order clauses (which we will re-
fer to as formulas in the rest of the paper) just like in
first-order logic. However, different from first-order
logic where a formula represents a hard constraint,
in an MLN, these constraints are softened and they
can be violated with some penalty. An MLN M
is therefore a set of weighted formulas {(?i, wi)}i,
where ?i is a first order formula andwi is the penalty
(the formula?s weight). These weighted formulas
define a probability distribution over sets of ground
atoms or so-called possible worlds. Let y denote a
possible world, then we define p(y) as follows:
p(y) = 1
Z
exp
(
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y)
)
. (1)
Here each c is a binding of free variables in ?i to
constants. Each f?ic represents a binary feature func-
tion that returns 1 if the ground formula we get by
replacing the free variables in ?i with the constants
in c under the given possible world y is true, and 0
otherwise. n?i denotes the number of free variables
of a formula ?i. Cn?i is the set of all bindings for the
free variables in ?i. Z is a normalization constant.
This distribution corresponds to a Markov network
where nodes represent ground atoms and factors rep-
resent ground formulas.
Each formula consists of a set of first-order predi-
cates, logical connectors and variables. Take the fol-
lowing formula as one example:
(?i, wi) : headMatch(a, b)?(a ?= b) ? coref (a, b).
The formula above indicates that if two different
candidate mentions a and b have the same head
word, then they are coreferential. Here a and b are
variables which can represent any candidate men-
tion, headMatch and coref are observed predicate
and hidden predicate respectively. An observed
predicate is one whose value is known from the ob-
servations when its free variables are assigned some
constants. A hidden predicate is one whose value is
not known from the observations. From this exam-
ple, we can see that headMatch is an observed pred-
icate because we can check whether two candidate
mentions have the same head word. coref is a hid-
den predicate because this is something we would
like to predict.
3.2 Formulas
We use two kinds of formulas for pairwise classi-
fication and mention clustering, respectively. For
1247
describing the attributes ofmi
mentionType(i,t) mi has mention type NAM(named entities), NOM(nominal) or PRO(pronouns).
entityType(i,e) mi has entity type PERSON, ORG, GPE or UN...
genderType(i,g) mi has gender type MALE, FEMALE, NEUTRAL or UN.
numberType(i,n) mi has number type SINGULAR, PLURAL or UN.
hasHead(i,h) mi has head word h, here h can represent all possible head words.
firstMention(i) mi is the first mention in its sentence.
reflexive(i) mi is reflexive.
possessive(i) mi is possessive.
definite(i) mi is definite noun phrase.
indefinite(i) mi is indefinite noun phrase.
demonstrative(i) mi is demonstrative.
describing the attributes of relations betweenmj andmi
mentionDistance(j,i,m) Distance between mj and mi in mentions.
sentenceDistance(j,i,s) Distance between mj and mi in sentences.
bothMatch(j,i,b) Gender and number of both mj and mi match: AGREE YES, AGREE NO
and AGREE UN).
closestMatch(j,i,c) mj is the first agreement in number and gender when looking backward
from mi: CAGREE YES, CAGREE NO and CAGREE UN.
exactStrMatch(j,i) Exact strings match between mj and mi.
pronounStrMatch(j,i) Both are pronouns and their strings match.
nopronounStrMatch(j,i) Both are not pronouns and their strings match.
properStrMatch(j,i) Both are proper names and their strings match.
headMatch(j,i) Head word strings match between mj and mi.
subStrMatch(j,i) Sub-word strings match between mj and mi.
animacyMatch(j,i) Animacy types match between mj and mi.
nested(j,i) mj/i is included in mi/j .
c command(j,i) mj/i C-Commands mi/j .
sameSpeaker(j,i) mj and mi have the same speaker.
entityTypeMatch(j,i) Entity types match between mj and mi.
alias(j,i) mj/i is an alias of mi/j .
srlMatch(j,i) mj and mi have the same semantic role.
verbMatch(j,i) mj and mi have semantic role for the same verb.
Table 1: Observed predicates.
pairwise classification, because the decisions are lo-
cal, we use a set of local formulas. For mention
clustering, we use global formulas to implement
best-first clustering or transitivity constraint. We
naturally combine pairwise classification with men-
tion clustering via local and global formulas in the
Markov logic framework, which is the essence of
?joint learning? in our work.
3.2.1 Local Formulas
A local formula relates any observed predicates to
exactly one hidden predicate. For our problem, we
define a list of observed predicates to describe the
properties of individual candidate mentions and the
relations between two candidate mentions, shown in
Table 1. For our problem, we have only one hidden
predicate, i.e. coref. Most of our local formulas are
from existing work (e.g. Soon et al(2001), Ng and
Cardie (2002), Sapena et al(2011)). They are listed
in Table 2, where the symbol ?+? indicates that for
every value of the variable preceding ?+? there is a
separate weight for the corresponding formula.
3.2.2 Global Formulas
Global formulas are designed to add global con-
straints for hidden predicates. Since in our problem
there is only one hidden predicate, i.e. coref, our
global formulas incorporate correlations among dif-
ferent ground atoms of the coref predicates. Next we
will show the best-first and transitivity global con-
straints. Note that we treat them as hard constraints
so we do not set any weights for these global formu-
las.
1248
Lexical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? exactStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? pronounStrMatch (j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? properStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nopronounStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? headMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? subStrMatch(j,i) ? j ?= i ? coref(j,i)
hasHead(j,h1+) ? hasHead(i,h2+) ? j ?= i ? coref(j,i)
Grammatical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? genderType(j,g1+) ? genderType(i,g2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? numberType(j,n1+) ? numberType(i,n2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? bothMatch(j,i,b+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? closestMatch(j,i,c+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? animacyMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nested(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? c command(j,i) ? j ?= i ? coref(j,i)
(mentionType(j,t1+) ? mentionType(i,t2+)) ? j ?= i ? coref(j,i)
(reflexive(j) ? reflexive(i)) ? j ?= i ? coref(j,i)
(possessive(j) ? possessive(i)) ? j ?= i ? coref(j,i)
(definite(j) ? definite(i)) ? j ?= i ? coref(j,i)
(indefinite(j) ? indefinite(i)) ? j ?= i ? coref(j,i)
(demonstrative(j) ? demonstrative(i)) ? j ?= i ? coref(j,i)
Distance and position Features
mentionType(j,t1+) ? mentionType(i,t2+) ? sentenceDistance(j,i,s+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? mentionDistance (j,i,m+) ? j ?= i ? coref(j,i)
(firstMention(j) ? firstMention(i)) ? j ?= i ? coref(j,i)
Semantic Features
mentionType(j,t1+) ? mentionType(i,t2+) ? alias(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? sameSpeaker(j,i) ? j?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? entityTypeMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? srlMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? verbMatch(j,i) ? j ?= i ? coref(j,i)
(entityType(j,e1+) ? entityType(i,e2+)) ? j ?= i ? coref(j,i)
Table 2: Local Formulas.
Best-First constraint:
coref(j, i) ? ?coref(k, i) ?j, k < i(k ?= j) (2)
Here we assume that coref(j,i) returns true if can-
didate mentions j and i are coreferential and false
otherwise. Therefore for each candidate mention i,
we should only select at most one candidate mention
j to return true for the predicate coref(j,i) from all its
preceding candidate mentions.
Transitivity constraint:
coref(j, k)?coref(k, i)?j < k < i ? coref(j, i) (3)
coref(j, k)?coref(j, i)?j < k < i ? coref(k, i) (4)
coref(j, i)?coref(k, i)?j < k < i ? coref(j, k) (5)
With the transitivity constraint, it means for given
mentions j, k and i, if any two pairs of them are
coreferential, then the third pair of them should be
also coreferential.
We use best-first clustering and transitivity con-
straint in our joint learning model respectively. De-
tailed comparisons between them will be shown in
Section 4.
3.3 Inference
We use MAP inference which is implemented by In-
teger Linear Programming (ILP). Its objective is to
maximize a posteriori probability as follows. Here
we use x to represent all the observed ground atoms
and y to represent the hidden ground atoms. For-
mally, we have
y? = argmax
y
p(y|x) ? argmax
y
s(y, x),
where
s(y, x) =
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y, x). (6)
Each hidden ground atom can only takes a value of
either 0 or 1. And global formulas should be satis-
fied as hard constraints when inferring the best y?. So
1249
the problem can be easily solved using ILP. Detailed
introduction about transforming groundMarkov net-
works in Markov logic into an ILP problem can be
found in (Riedel, 2008).
3.4 Parameter Learning
For parameter learning, we employ the online
learner MIRA (Crammer and Singer, 2003), which
establishes a large margin between the score of the
gold solution and all wrong solutions to learn the
weights. This is achieved by solving the quadratic
program as follows
min ? wt ?wt?1 ? . (7)
s.t. s(yi, xi)? s(y?, xi) ? L(yi, y?)
?y? ?= yi, (yi, xi) ? D
Here D = {(yi, xi)}Ni=1 represents N training in-
stances (each instance represents one single docu-
ment in the dataset) and t represents the number of
iterations. In our problem, we adopt 1-best MIRA,
which means that in each iteration we try to find wt
which can guarantee the difference between the right
solution yi and the best solution y? (i.e. the one with
the highest score s(y?, xi), equivalent to y? in Section
3.3)) is at least as big as the loss L(yi, y?), while
changing wt?1 as little as possible. The number of
false ground atoms of coref predicate is selected as
loss function in our experiments. Hard global con-
straints (i.e. best-first clustering or transitivity con-
straint) must be satisfied when inferring the best y?
in each iteration, which can make learned weights
more effective.
4 Experiments
In this section, we will first describe the dataset and
evaluation metrics we use. We will then present the
effect of our joint learning method, and finally dis-
cuss the comparison with the state of the art.
4.1 Data Set
We use the dataset from the CoNLL-2011 shared
task, ?Modeling Unrestricted Coreference in
OntoNotes? (Pradhan et al 2011)2. It uses the En-
glish portion of the OntoNotes v4.0 corpus. There
are three important differences between OntoNotes
2http://conll.cemantix.org/2011/
and another well-known coreference dataset from
ACE. First, OntoNotes does not label any singleton
entity cluster, which has only one reference in the
text. Second, only identity coreference is tagged in
OntoNotes, but not appositives or predicate nomi-
natives. Third, ACE only considers mentions which
belong to ACE entity types, whereas OntoNotes
considers more entity types. The shared task is to
automatically identify both entity coreference and
event coreference, although we only focus on entity
coreference in this paper. We don?t assume that
gold standard mention boundaries are given. So we
develop a heuristic method for mention detection.
See details in Section 2.1.
The training set consists of 1674 documents from
newswire, magazine articles, broadcast news, broad-
cast conversations and webpages, and the develop-
ment set consists of 202 documents from the same
source. For training set, there are 101264 mentions
from 26612 entities. And for development set, there
are 14291 mentions from 3752 entities (Pradhan et
al., 2011).
4.2 Evaluation Metrics
We use the same evaluation metrics as used in
CoNLL-2011. Specifically, for mention detection,
we use precision, recall and the F-measure. A men-
tion is considered to be correct only if it matches
the exact same span of characters in the annotation
key. For coreference resolution, MUC (Vilain et al
1995), B-CUBED (Bagga and Baldwin, 1998) and
CEAF-E (Luo, 2005) are used for evaluation. The
unweighted average F score of them is used to com-
pare different systems.
4.3 The Effect of Joint Learning
To assess the performance of our method, we set up
several variations of our system to compare with the
joint learning system. The MLN-Local system uses
only the local formulas described in Table 2 with-
out any global constraints under the MLN frame-
work. By default, the MLN-Local system uses the
single-link method to generate clustering results.
The MLN-Local+BF system replaces the single-link
method with best-first clustering to infer mention
clustering results after learning the weights for all
the local formulas. The MLN-Local+Trans sys-
tem replaces the best-first clustering with transitivity
1250
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Local 62.52 74.75 68.09 56.07 65.55 60.44 65.67 72.95 69.12 45.55 37.19 40.95 56.84
MLN-Local+BF 65.74 73.2 69.27 56.79 64.08 60.22 65.71 74.18 69.69 47.29 40.53 43.65 57.85
MLN-Local+Trans 68.49 70.32 69.40 57.16 60.98 59.01 66.97 72.90 69.81 46.96 43.34 45.08 57.97
MLN-Joint(BF) 64.36 75.25 69.38 55.47 66.95 60.67 64.14 77.75 70.29 50.47 39.85 44.53 58.50
MLN-Joint(Trans) 64.46 75.37 69.49 55.48 67.15 60.76 64.00 78.11 70.36 50.63 39.84 44.60 58.57
Table 3: Comparison between different MLN-based systems, using 10-fold cross validation on the training dataset.
constraint. The MLN-Joint system is a joint model
for both pairwise classification and mention cluster-
ing. It can combine either best-first clustering or en-
forcing transitivity constraint with pairwise classifi-
cation, and we denote these two variants of MLN-
Joint as MLN-Joint(BF) and MLN-Joint(Trans) re-
spectively.
To compare the performance of the various sys-
tems above, we use 10-fold cross validation on
the training dataset. We empirically find that our
method has a fast convergence rate, to learn the
MLN model, we set the number of iterations to be
10.
The performance of these compared systems is
shown in Table 3. To provide some context for
the performance of this task, we report the median
average F-score of the official results of CoNLL-
2011, which is 50.12 (Pradhan et al 2011). We can
see that MLN-Local achieves an average F-score of
56.84, which is well above the median score. When
adding best-first or transitivity constraint which
is independent of pairwise classification, MLN-
Local+BF and MLN-Local+Trans achieve better re-
sults of 57.85 and 57.97. Most of all, we can see
that the joint learning model (MLN-Joint(BF) or
MLN-Joint(Trans)) significantly outperforms inde-
pendent learning model (MLN-Local+BF or MLN-
Local+Trans) no matter whether best-first clustering
or transitivity constraint is used (based on a paired 2-
tailed t-test with p < 0.05) with the score of 58.50
or 58.57, which shows the effectiveness of our pro-
posed joint learning method.
Best-first clustering and transitivity constraint
are very useful in Markov logic framework, and
both MLN-Local and MLN-Joint benefit from them.
For MLN-Joint, these two clustering methods re-
sult in similar performance. But actually, transi-
tivity is harder than best-first, because it signifi-
cantly increases the number of formulas for con-
straints and slows down the learning process. In
our experiments, we find that MLN-Joint(Trans)3 is
much slower than MLN-Joint(BF). Overall, MLN-
Joint(BF) has a good trade-off between effectiveness
and efficiency.
4.4 Comparison with the State of the Art
In order to compare our method with the state-of-
the-art systems, we consider the following systems.
We implemented a traditional pairwise coreference
system using Maximum Entropy as the base classi-
fier and best-first clustering to link the results. We
used the same set of local features in MLN-Joint.
We refer to this system as MaxEnt+BF. To replace
best-first clustering with transitivity constraint, we
have another system named as MaxEnt+Trans. We
also consider the best 3 systems from CoNLL-2011
shared task. Chang?s system uses ILP to perform
best-first clustering after training a pairwise corefer-
ence model. Sapena?s system uses a relaxation label-
ing method to iteratively perform function optimiza-
tion for labeling each mention?s entity after learning
the weights for features under a C4.5 learner. Lee?s
system is a purely rule-based one. They use a battery
of sieves by precision (from highest to lowest) to it-
eratively choose antecedent for each mention. They
obtained the highest score in CoNLL-2011.
Table 4 shows the comparisons of our system with
the state-of-the-art systems on the development set
of CoNLL-2011. From the results, we can see that
our joint learning systems are obviously better than
3For MLN-Joint(Trans), not all training instances can be
learnt in a reasonable amount of time, so we set up a time out
threshold of 100 seconds. If the model cannot response in 100
seconds for some training instance, we remove it from the train-
ing set.
1251
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Joint(BF) 67.33 72.94 70.02 58.03 64.05 60.89 67.11 73.88 70.33 47.6 41.92 44.58 58.60
MLN-Joint(Trans) 67.28 72.88 69.97 58.00 64.10 60.90 67.12 74.13 70.45 47.70 41.96 44.65 58.67
MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19
MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31
Lee?s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60
Sapena?s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61
Chang?s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35
Table 4: Comparisons with state-of-the-art systems on the development dataset.
MaxEnt+BF and MaxEnt+Trans. They also out-
perform the learning-based systems of Sapena et al
(2011) and Chang et al(2011), and perform com-
petitively with Lee?s system (Lee et al 2011). Note
that Lee?s system is purely rule-based, while our
methods are developed in a theoretically sound way,
i.e., Markov logic framework.
5 Related Work
Supervised noun phrase coreference resolution has
been extensively studied. Besides the mention-pair
model, two other commonly used models are the
entity-mention model (Luo et al 2004; Yang et al
2008) and ranking models (Denis and Baldridge,
2008; Rahman and Ng, 2009). Interested readers
can refer to the literature review by Ng (2010).
Under the mention-pair model, Klenner (2007)
and Finkel and Manning (2008) applied Integer Lin-
ear Programming (ILP) to enforce transitivity on the
pairwise classification results. Chang et al(2011)
used the same ILP technique to incorporate best-first
clustering and generate the mention clusters. In all
these studies, however, mention clustering is com-
bined with pairwise classification only at the infer-
ence stage but not at the learning stage.
To perform joint learning of pairwise classifi-
cation and mention clustering, in (McCallum and
Wellner, 2005), each mention pair corresponds to
a binary variable indicating whether the two men-
tions are coreferential, and the dependence between
these variables is modeled by conditional undirected
graphical models. Finley and Joachims (2005) pro-
posed a general SVM-based framework for super-
vised clustering that learns item-pair similarity mea-
sures, and applied the framework to noun phrase
coreference resolution. In our work, we take a differ-
ent approach and apply Markov logic. As we have
shown in Section 3, given the flexibility of Markov
logic, it is straightforward to perform joint learning
of pairwise classification and mention clustering.
In recent years, Markov logic has been widely
used in natural language processing problems (Poon
and Domingos, 2009; Yoshikawa et al 2009; Che
and Liu, 2010). For coreference resolution, the most
notable one is unsupervised coreference resolution
by Poon and Domingos (2008). Poon and Domin-
gos (2008) followed the entity-mention model while
we follow the mention-pair model, which are quite
different approaches. To seek good performance in
an unsupervised way, Poon and Domingos (2008)
highly rely on two important strong indicators:
appositives and predicate nominatives. However,
OntoNotes corpus (state-of-art NLP data collection)
on coreference layer for CoNLL-2011 has excluded
these two conditions of annotations (appositives and
predicate nominatives) from their judging guide-
lines. Compared with it, our methods are more ap-
plicable for real dataset. Huang et al(2009) used
Markov logic to predict coreference probabilities
for mention pairs followed by correlation cluster-
ing to generate the final results. Although they also
perform joint learning, at the inference stage, they
still make pairwise coreference decisions and clus-
ter mentions sequentially. Unlike their method, We
formulate the two steps into a single framework.
Besides combining pairwise classification and
mention clustering, there has also been some work
that jointly performs mention detection and coref-
erence resolution. Daume? and Marcu (2005) de-
veloped such a model based on the Learning as
1252
Search Optimization (LaSO) framework. Rahman
and Ng (2009) proposed to learn a cluster-ranker
for discourse-new mention detection jointly with
coreference resolution. Denis and Baldridge (2007)
adopted an Integer Linear Programming (ILP) for-
mulation for coreference resolution which models
anaphoricity and coreference as a joint task.
6 Conclusion
In this paper we present a joint learning method with
Markov logic which naturally combines pairwise
classification and mention clustering. Experimental
results show that the joint learning method signifi-
cantly outperforms baseline methods. Our method
is also better than all the learning-based systems in
CoNLL-2011 and reaches the same level of perfor-
mance with the best system.
In the future we will try to design more global
constraints and explore deeper relations between
training instances generation and mention cluster-
ing. We will also attempt to introduce more predi-
cates and transform structure learning techniques for
MLN into coreference problems.
Acknowledgments
Part of the work was done when the first author
was a visiting student in the Singapore Manage-
ment University. And this work was partially sup-
ported by the National High Technology Research
and Development Program of China(863 Program)
(No.2012AA011101), the National Natural Science
Foundation of China (No.91024009, No.60973053,
No.90920011), and the Specialized Research Fund
for the Doctoral Program of Higher Education of
China (Grant No. 20090001110047).
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference pro-
tocols for coreference resolution. In CoNLL Shared
Task, pages 40?44, Portland, Oregon, USA. Associa-
tion for Computational Linguistics.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 161?169.
Tsinghua University Press.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
III Hal Daume? and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 97?104, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
EMNLP, pages 660?669.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
ACL (Short Papers), pages 45?48. The Association for
Computer Linguistics.
T. Finley and T. Joachims. 2005. Supervised clustering
with support vector machines. In International Con-
ference on Machine Learning (ICML), pages 217?224.
Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jia-
jun Chen. 2009. Coreference resolution using markov
logic networks. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing: 10th Interna-
tional Conference, CICLing 2009.
M. Klenner. 2007. Enforcing consistency on coreference
sets. In RANLP.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kamb-
hatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of the ACL, pages 135?142.
1253
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT/EMNLP, pages 25?
32.
Andrew McCallum and Ben Wellner. 2005. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems, pages 905?912. MIT Press.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In ACL, pages 1396?
1411. The Association for Computer Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP, pages 650?659.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1?10.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of map inference for markov logic. In UAI,
pages 468?475. AUAI Press.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35?39, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61?65, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Marc B. Vilain, John D. Burger, John S. Aberdeen, Den-
nis Connolly, and Lynette Hirschman. 1995. Amodel-
theoretic coreference scoring scheme. In MUC, pages
45?52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In ACL, pages 843?851. The Association for
Computer Linguistics.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In ACL/AFNLP,
pages 405?413. The Association for Computer Lin-
guistics.
1254
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1466?1477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Identifying Event-related Bursts via Social Media Activities
Wayne Xin Zhao?, Baihan Shu?, Jing Jiang?, Yang Song?, Hongfei Yan?? and Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,baihan.shu,yhf1029}@gmail.com,ysong@pku.edu.cn
jingjiang@smu.edu.sg, lxm@pku.edu.cn
Abstract
Activities on social media increase at a dra-
matic rate. When an external event happens,
there is a surge in the degree of activities re-
lated to the event. These activities may be
temporally correlated with one another, but
they may also capture different aspects of an
event and therefore exhibit different bursty
patterns. In this paper, we propose to iden-
tify event-related bursts via social media activ-
ities. We study how to correlate multiple types
of activities to derive a global bursty pattern.
To model smoothness of one state sequence,
we propose a novel function which can cap-
ture the state context. The experiments on a
large Twitter dataset shows our methods are
very effective.
1 Introduction
Online social networks (e.g., Twitter, Facebook,
Myspace) significantly influence the way we live.
Activities on social media increase at a dramatic
rate. Millions of users engage in a diverse range
of routine activities on social media such as posting
blog messages, images, videos or status messages,
as well as interacting with items generated by oth-
ers such as forwarding messages. When an event
interesting to a certain group of individuals takes
place, there is usually a surge in the degree of ac-
tivities related to the event (e.g., a sudden explosion
of tweets). Since social media activities may indi-
cate the happenings of external events, can we lever-
age on the rich social media activities to help iden-
tify meaningful external events? This is the research
problem we study in this paper. By external events,
we refer to real-world events that happen external to
the online space.
?Corresponding author.
2 4 6 8 100
2040
6080
100120
140
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(a) Query=?Amazon.?
2 4 6 8 100
50
100
150
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(b) Query=?Eclipse?.
Figure 1: The amount of activities within a 10-hour
window for two queries. Three types of activities
are considered: (1) posting a tweet (upward triangle),
(2) retweet (downward triangle), (3) posting a URL-
embedded tweet (excluding retweet) (filled circle). As
explained in Table 1, both bursts above are noisy.
Mining events from text streams is usually
achieved by detecting bursty patterns (Swan and Al-
lan, 2000; Kleinberg, 2003; Fung et al 2005). How-
ever, previous work has mostly focused on tradi-
tional text streams such as scientific publications and
news articles. There is still a lack of systematic in-
vestigations into the problem of identifying event-
related bursty patterns via social media activities.
There are at least two basic characteristics of social
media that make the problem more interesting and
challenging.
First, social media involve various types of activ-
ities taking place in real time. These activities may
be temporally correlated with one another, but they
may also capture different aspects of an event and
therefore exhibit different bursty patterns. Most of
previous methods (Swan and Allan, 2000; Klein-
berg, 2003; Fung et al 2005) deal with a single type
of textual activities. When applied to social media,
they oversimplify the complex nature of online so-
1466
Bursty Activity Time # in Sr # in Su # in St Noisy?
Sr,St 23:00?23:59, Nov. 23, 2009 108 5 147 Y
See Fig. 1(b) [Query=eclipse] major bursty reason: The tweet from Robert Pattinson ?@twilight: from rob cont .
- i hope you are looking forward to eclipse as much as i am .? has been retweeted many times.
Su,St 07:00?07:59, Jul. 25, 2009 6 122 133 Y
See Fig. 1(a) [Query=Amazon] major bursty reason: Advertisement tweets like ?@fitnessjunkies amazon.com deals
: http://tinyurl.com/lakz3h.? have been posted many times.
St,Su,Sr 09:00?9:59, Oct. 9, 2009 1562 423 2848 N
[Query=Nobel] major bursty reason: The news ?Obama won Nobel Peace Prize? flood Twitter.
Table 1: Examples of bursts. The first two bursts are judged as noise since they do not correspond to any meaningful
external events. In fact, the reasons why a burst appears in social media can be quite diverse. In this paper, we only
focus on event-related bursts. St denotes posting a tweet, Su denotes posting a url-embedded tweet, and Sr denotes
retweet.
cial activities, and therefore they may not be well
suitable to social media. Let us consider a moti-
vating example. Figure 1 shows the change of the
amount of activities of three different types over a
10-hour time window for two queries. If we consider
only the total number of tweets, we can see that for
both queries there is a burst. However, neither of the
two bursts corresponds to a real-world event. The
first burst was caused by the broadcast of an adver-
tisement from several Twitter bots, and the second
burst was caused by numerous retweets of a status
update of a movie star1. The detailed explanations
of why the two bursts are noisy are also shown in
Table 1. On the other hand, interestingly, we can see
that not all the activity streams display noisy bursty
patterns at the same time. It indicates that we may
make use of multiple views of different activities
to detect event-related bursts. The intuition is that
using multiple types of activities may help learn a
better global picture of event-related bursty patterns.
Learning may also be more resistant to noisy bursts.
Second, in social media, burst detection is chal-
lenged by irregular, unpredictable and spurious
noisy bursts. To overcome this challenge, a reason-
able assumption is that a burst corresponding to a
real event should not fluctuate too much within a
relatively short time window. To illustrate it, we
present an example in Figure 2, in which we first
use a simple threshold method to detect bursts and
then analyze the effect of local smoothness. In par-
ticular, if the amount of activities at a certain time
is above a pre-defined threshold, we set its state to
1, which indicates a bursty state. Otherwise, we set
the state to 0. Figure 2(a) shows that for the query
?Eclipse,? with a threshold of 50, the state sequence
for the time window we consider is ?0000100000.?
1The reasons for these bursts were revealed by manually
checking the tweets during the corresponding periods.
1 2 3 4 5 6 7 8 9 100
50
100
150 Correct0000000000Threshold0000100000
(a) Query=?Eclipse?.
1 2 3 4 5 6 7 8 9 100
5001000
15002000
25003000 Correct0000011111Threshold0000011111
(b) Query=?Nobel?.
Figure 2: Analysis of the effect of local smoothness on
threshold method. It shows two examples of threshold
methods for burst detection in a 10-hour window. The
red line denotes the bursty threshold. If the number of
activities is above the threshold in one time interval, the
state of this time interval is judge as bursty. Detailed de-
scriptions of these cases are shown in Table 1.
Although there is a burst in this sequence, its dura-
tion is very short. In fact, this is the first example
shown in Table 1, which is a noisy burst. In con-
trast, in Figure 2(b), the state sequence for the query
?Nobel? is ?0000011111,? in which the longer and
smoother burst corresponds to a true event. A good
function for evaluating the smoothness of a state se-
quence should be able to discriminate these cases
and model the context of state sequences effectively.
With its unique characteristics and challenges,
there is an emergent need to deeply study the prob-
lem of event-related burst detection via social me-
dia activities. In this paper, we conduct a system-
atic investigation on this problem. We formulate
this problem as burst detection from time series of
social media activities. We develop an optimiza-
tion model to learn bursty patterns based on multiple
types of activities. We propose to detect bursts by
considering both local state smoothness and correla-
tion across multiple streams. We define a function to
1467
quantitatively measure local smoothness of one sin-
gle state sequence. We systematically evaluate three
types of activities for burst detection on a large Twit-
ter dataset and analyze different properties of these
three streams for burst detection.
2 Problem Definition
Before formally introducing our problems, we first
define some basic concepts.
Activity: An activity refers to some type of action
that users perform when they are interested in some
topic or event.
Activity Stream: An activity stream of length
N and type m is a sequence of numbers
(nm1 , n
m
2 , ..., n
m
N ), where each n
m
i denotes the
amount of activities of type m that occur during the
ith time interval.
Query: A queryQ is a sequence of terms q1, ..., q|Q|
which can represent the information needs of users.
For example, an example query related to President
Obama is ?barack obama.?
Event-related Burst: Given a query Q, an event-
related burst is defined as a period [ts, te] in which
some event related with Q takes place, where ts and
te are the start timestamp and end timestamp of the
event period respectively. During the event period
the amount of activities is significantly higher than
average.
Based on these definitions, our task is to try to
identify event-related bursts via multiple social me-
dia activity streams.
3 Identifying Event-related Bursts from
Social Media
In this section, we discuss how to identify event-
related bursts via social media activities. For-
mally, given a query Q, we first build M ac-
tivity streams related with Q on T timestamps:
{(nm1 , ..., n
m
T )}
M
m=1. The definition of activity in our
methods is very general; it includes various types of
social media activities, including textual and non-
textual activities, e.g., a click on a shared photo and
a link formation between two users.
Given the input, we try to infer a state sequence
over these T timestamps: z = (z1, ..., zT ), where
zi is 1 or 0. 1 indicates a time point within a burst
while 0 indicates a non-bursty time point.
3.1 Modeling a Single Activity Stream
3.1.1 Generation function
In probability theory and statistics, the Poisson
distribution2 is a discrete probability distribution
that can measure the probability of a given number
of ?activities? occurring in a fixed time interval. We
use the Poisson distribution to study the probability
of observing the number of social media activities,
and we treat one hour as one time interval in this
paper.
Homogeneous Poisson Distribution The genera-
tive probability of the ith number in one activity
stream of type m is defined as f(nmi , i, z
m
i ) =
(?zmi
)n
m
i exp(??zmi
)
nmi !
, where ?0 is the (normal) expec-
tation of the number of activities in one time inter-
val. If one state is bursty, it would emit activities
with a faster rate and result in a larger expectation
?1. We can set ?1 = ?0 ? ?, where ? > 1.
Heterogeneous Poisson Distribution The two-state
machine in (Kleinberg, 2003) used two global refer-
ences for all the time intervals: one for bursty and
the other for non-bursty. In our experiments, we ob-
serve temporal patterns of user behaviors, i.e., activ-
ities in some hours are significantly more than those
in the others. Instead of using fixed global rates ?0
and ?1, we try to model temporal patterns of user
behaviors by parameterizing ?(?) with the time in-
dex. By following (Ihler et al 2006), we use a
set of hour-specific rates {?1,h}24h=1 and {?0,h}
24
h=1.
3
Given a time index h, we set ?0,h to be the expecta-
tion of the number of activities in hth time interval
every day, then we have ?1,h = ?0,h ? ?. In this
paper, ? is empirally set as 1.5.
3.1.2 Smoothness of a State Sequence
For burst detection, the major aim is to identify
steady and meaningful bursts and to discard tran-
sient and spurious bursts. Given a state sequence
z1z2...zT , to quantitatively measure the smoothness
and compactness of it, we introduce some measures.
One simple method is to count the number of
change in the state sequence. Formally, we use the
following formula:
g1(z) = T ?
T?1?
i=1
I(zi 6= zi+1), (1)
2http://en.wikipedia.org/wiki/Poisson distribution
3We can also make the rates both day-specific and hour-
specific, i.e., {?(?),d,h}h?{1,...,24},d?{1,...,7}.
1468
where T is length of the state sequence and I(?)
is an indicator function which returns 1 only if the
statement is true. Let us take the state sequence
?0000100000? (shown in Figure 2(a)) as an example
to see how g1 works. State changes 0pos=4 ? 1pos=5
and 1pos=5 ? 0pos=6 each incur a cost of 1, there-
fore g1(0000100000) = 10 ? 2 = 8. Similarly, we
can get g1(0000000000) = 10. There is a cost dif-
ference between these two sequences, i.e., ?g1 = 2.
Kleinberg (2003) uses state transition probabilities
to model the smoothness of state sequences. With
simple derivations, we can show that Kleinberg?s
model essentially also uses a cost function that is
linear in terms of the number of state changes in a
sequence, and therefore similar to g1.
In social media, very short noisy bursts like
?0000100000? are very frequent. To discard such
noises, we may multiply g1 by a big cost factor to
punish short-term fluctuations. However, it is not
sensitive to the state context4 and may affect the
detection of meaningful bursts. For example, state
change 0pos=4 ? 1pos=5 in ?0000111100? would
receive the same cost as that of 0pos=4 ? 1pos=5
in ?0000100000? although the later is more like a
noise.
To better measure the smoothness of a state se-
quence , we propose a novel context-sensitive func-
tion, which sums the square of the length of the max-
imum subsequences in which all states are the same.
Formally, we have
g2(z1, z2, ..., zT ) =
?
si<ei
(ei ? si + 1)
2, (2)
where si and ei are the start index and end in-
dex of the ith subsequence respectively. To define
?maximum?, we have the constraints zsi = zsi+1 =
... = zei , zsi?1 6= zsi , zei 6= zei+1. For example,
g2(0000000000)= 102 = 100, g2(0000100000)=
42 + 12 + 52 = 42, we can see that ?g2 =
100 ? 42 = 58, which is significantly larger than
?g1(= 2). g2 rewards the continunity of state se-
quences while punish the fluctuating changes, and it
is context-sensitive. State change 0pos=4 ? 1pos=5
in ?0000111100? receives a cost of 4,5 which is
4Here context refers to the window of hidden state se-
quences.
5Indeed, g2 is not designed for a single state change but for
the overall smoothness patterns, so we choose a referring se-
quence generated by making the corresponding state negative to
compute the cost, i.e., |g2(0000011110)?g2(0000001110)| =
4.
much smaller than that of 0pos=4 ? 1pos=5 in
?0000100000?. g2 is also sensitive to the po-
sition of state changes, e.g., g2(0000100000) 6=
g2(0100000000).
3.2 Burst Detection from a Single Activity
Stream
Given an activity stream (nm1 , ..., n
m
T ), we would
like to infer a state sequence over these T times-
tamps, i.e., to find out the most possible state se-
quence z = (zm1 , ..., z
m
T ) based on the data, where
zmi = 1 or 0. We formulate this problem as
an optimization problem. The cost of a state se-
quence includes two parts: generation of activities
and smoothness of the state sequence. The objective
function is to find a state sequence which incurs the
minimum cost. Formally, we define the total cost
function as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i )
? ?? ?
generating cost
+
(
? ?(zm1 , ..., z
m
T ) ? ?1
)
? ?? ?
smoothness cost
,
(3)
where ?1 > 0 is a scaling factor which balance these
two parts. ?(?) function is the smoothness function,
and we can set it as either g1(?) or g2(?).
To seek the optimal state sequence, we can min-
imize Equation 3. However, exact inference is hard
due to the exponential search space. Instead of ex-
amining the smoothness of the whole state sequence,
we propose to measure the smoothness of all the L-
length subsequences, so called ?local smoothness?.
The assumption is that the states in a relatively short
time window should not change too much. The new
objective function is defined as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i ) (4)
?
(
?
i?L
?(zmi , ..., z
m
i+L?1)
)
? ?1.
The objective function in Equation 4 can be
solved efficiently by a dynamic programming algo-
rithm shown in Algorithm 1. The time complexity
of this algorithm is O(T ? 2L). Note that the meth-
ods we present in Equation 4 and Algorithm 1 are
quite general. They are independent of the concrete
forms of f(?) and ?(?), which leaves room for flexi-
ble adaptation or extension in specific tasks. In pre-
vious methods (Kleinberg, 2003), L is often fixed as
1469
2. Indeed, as shown in Figure 2, in some cases, we
may need a longer window to infer the global pat-
terns. In our model, L can be tuned based on real
datasets. We can seek a trade-off between efficiency
and length of context windows.
Algorithm 1: Dynamic Programming for Equation 4.
d[i][s][zi...zi?L+1] denotes the minimum cost of the first1
i timestamps with the state subsequence: zi...zi?L+1 and
zi = s;
set d[0][?][?] = 0;2
set c1[i] = log f(nmi , i, z
m
i );3
set c2[i] = ?(zi, ..., zi?L+1);4
b, b?: previous and current state window are represented as5
L-bit binary numbers;
for i = 1 to T do6
for s = 0 to 1 do7
for b = 0 to 2L ? 1 do8
b? = (b << 1|s)&(1 << L? 1);9
d[i][s][b?]??10
min(d[i][s][b?], d[i?1][s][b]+c1[i]+c2[i]);
end11
end12
end13
3.3 Correlating Multiple Activity Streams
In this section, we discuss how to correlate multi-
ple activity streams to learn a global bursty patterns.
The hidden state sequences corresponding to these
activity streams are not fully independent. An ex-
ternal event may intricate surges in multiple activity
streams simultaneously.
We propose to correlate multiple activity streams
in an optimization model. The idea is that activ-
ity streams related with one query might be depen-
dent, i.e., the states of multiple activity streams on
the same timestamp tend to be the same6; if not,
it would incur a cost. To implement this idea, we
develop an optimization model. For convenience,
we call the states of each activity stream as ?local
states? while the overall states learnt from multiple
activity streams as ?global states?.
The idea is that although various activity streams
are different in the scale of frequencies, they tend to
share similar trend patterns. We incorporate the cor-
relation between local states on the same timestamp.
6In our experiments, we compute the cross correlation be-
tween different streams with a lag factor ?, we find the cross
correlation achieves maximum consistantly when ? = 0.
Formally, we have
Cost(Z) =
M?
m=1
{
?
T?
i=1
log f(nmi , i, z
m
i )
?
?
i?L
?(zmi , ..., z
m
i+L?1) ? ?1
}
+
T?
i=1
?
m1,m2
I(zm1i 6= z
m2
i ) ? ?2, (5)
where I(?) is indicator function, and ?2 is the cost
when a pair of states are different across multiple
streams on the same timestamp.
The objective function in Equation 5 can be
solved by a dynamic programming algorithm pre-
sented in Algorithm 2. The time complexity of this
algorithm is O(T ? 2M ?L+M ). Generally, L can be
set as one small value, e.g., L =2 to 6, and we can
select just a few representative activity streams, i.e.,
M =2 to 6. In this case, the algorithm can be effi-
cient.
Algorithm 2: Dynamic Programming for Equation 5.
d[i][z1i ...z
1
i?L+1; ...; z
M
i ...z
M
i?L+1] denotes the minimum1
cost of the first i timestamps with the local state
subsequence zmi ...z
m
i?L+1 in the mth stream;
set d[0][...] = 0;2
bl, bl
?
: previous and current state windows represented as3
M ? L-bit binary numbers;
c[i, bl, bl
?
] denotes all the cost in the tth timestamp;4
for i = 1 to T do5
for bl = 0 to 2M?L ? 1 do6
deriving current local state sequences bl
?
from bl;7
d[i][b?l]??8
min(d[i][bl
?
], d[i? 1][bl] + c[i, bl, bl
?
]);
end9
end10
Given M types of activity streams, we can get
M (local) state sequences {(zm1 , ..., z
m
T )}
M
m=1. The
next question is how to learn a global state sequence
(zG1 , ..., z
G
T ) based on local state sequences. Here we
give a few options:
CONJUNCT: we set a global state zi as bursty if
all local states are bursty, i.e., zGi = ?
M
m=1z
m
i .
DISJUNCT: we set a global state zi as bursty if
one of the local states is bursty, i.e., zGi = ?
M
m=1z
m
i .
BELIEF: we set a global state zi as the most con-
fident local state, i.e., zGi = argmaxmbelief(z
m
i ).
The belief(?) function can be defined as the ratio be-
tween generating costs from states zmi and 1 ? z
m
i :
belief(zmi ) =
f(nmi ,i,z
m
i )
f(nmi ,i,1?z
m
i )
.
1470
Table 2: Basic statistics of our golden test collection.
# of queries 17
Aver. # of event-related bursts per query 19
Min. bursty interval 3 hours
Max. bursty interval 163 hours
Aver. bursty interval 17.8 hours
L2G: we treat the states of one local stream as the
global states.
4 Experiments
4.1 Construction of Test Collection
We test our algorithms on a large Twitter dataset,
which contains about 200 million tweets and ranges
from July, 2009 to December 2009. We manually
constructed a list of 17 queries that have high vol-
umes of relevant tweets during this period. These
queries have a very broad coverage of topics. Exam-
ple queries are ?Barack Obama?, ?Apple?, ?Earth-
quake?, ?F1? and ?Nobel Prize?. For each query, we
invite two senior graduate students to manually iden-
tify their golden bursty intervals, and each bursty in-
terval is represented as a pair of timestamps in terms
of hours. Specifically, to generate the golden stan-
dard, given a query, the judges first manually gen-
erate a candidate list of external events7; then for
each event, they look into the tweets within the cor-
responding period and check whether there is a surge
on the frequency of tweets. If so, the judges fur-
ther determine the start timepoint and end timepoint
of it. If there is a conflict, a third judge will make
the final decision. We used Cohen?s kappa coeffi-
cient to measure the agreement of between the first
two judges, which turned out to be 0.67, indicating a
good level of agreement8. We present basic statistics
of the test collection in Table 2.
4.2 Evaluation Metrics
Before introducing our evaluation metrics, we first
define the Bursty Interval Overlap Ratio (BIOR)
BIOR(f,X ) =
?
f ??X ?l(f, f
?)
L(f)
,
f is a bursty interval, ?l(f, f ?) is the length of
overlap between f ? and f , L(f) is the length of
7We refer to some gold news resources, e.g., Google News
and Yahoo! News.
8http://en.wikipedia.org/wiki/Cohen?s kappa
Figure 3: Examples to illustrate BIOR. X0, X1
and X2 are three sets of bursty intervals. X0
and X2 consist of one interval, and X1 consists of
two intervals. BIOR(f,X0)=1, BIOR(f,X1)=0.5 and
BIOR(f,X2)=0.5.
bursty period of f . X is a set of bursty intervals,
BIOR measures the proportion of the timestamps in
f which are covered by one of bursty intervals in
X . We use BIOR to measure partial match of inter-
vals, because a system may not return all the exact
bursty intervals9. We show some examples of BIOR
in Figure 3.
We use modified Precision, Recall and F as ba-
sic measures. Given one query, P, R and F can be
defined as follows
R =
?
f?B I
(
1
|Mf |
BIOR(f,M) > 0.5
)
|B|
,
P =
1
|M|
?
f ??M
(BIOR(f ?,B)),
F =
2? P ?R
P + R
,
where M is the set of bursty intervals identified
by one candidate method, B is the set of bursty in-
tervals in golden standards, and Mf is the set of in-
tervals which overlap with f in M. We incorporate
the factor 1|Mf | in Recall to penalize the incontin-
uous coverage of the golden interval, and we also
require that the overlap ratio with penalized factor
is higher than a threshold of 0.5. Given two sets of
bursty intervals which have the same value of BIOR,
we prefer the one with fewer intervals. In Figure 3,
we can easily derive X1 and X2 have the same value
9A simple evaluation method is that we label each one hour
time slot as being part of a burst or not and compare with the
gold standard. However, in our experiments, we find that some
methods tend to break one meaningful burst into small parts and
easier to be affected by small fluctuations although they may
have a good coverage of bursty points. This is why we adopt a
different evaluation approach.
1471
Table 3: Average cross-correlation between different
streams.
St Sr Su
St 1 0.830235 0.851514
Sr 0.830235 1 0.59905
Su 0.851514 0.59905 1
of BIOR, when computing Recall, we prefer X2 to
X1 since X2 consists of only one complete inter-
val whileX1 consists of two inconsecutive intervals.
I(?) is an indicator function which returns 1 only if
the statement if true. In our experiments, we use the
average of R, P and F over all test queries.
4.3 Experiment Setup
Selecting activity streams
We consider three types of activity streams in
Twitter: 1) posting a tweet, denoted as St; 2) for-
warding a tweet (retweet), denoted as Sr; 3) post-
ing a URL-embedded tweet, denoted as Su. It is
natural to test the performance of St in discover-
ing bursty patterns, while Su and Sr measure the
influence of external events on users in Twitter in
two different aspects. Sr: An important convention
in Twitter is the ?retweeting? mechanism, through
which users can actively spread the news or related
information; Su: Another characteristic of Twitter is
that the length of tweets is limited to 140 characters,
which constrains the capacity of information. Users
often embed a URL link in the tweets to help others
know more about the corresponding information.
We compute the average cross correlation be-
tween different activity streams for these 17 queries
in our test collection, and we summarize the results
in Table 3. We can see that both Sr and Su have a
high correlation with St, and Sr has a relatively low
correlation with Su. 10
Methods for comparisons
S(?): using Equation 4 and considers a single ac-
tivity stream, namely St, Su and Sr.
MBurst(?): using Equation 5 and considers mul-
tiple activity streams.
To compare our methods with previous methods,
we adopt the following baselines:
StateMachine: This is the method proposed
in (Kleinberg, 2003). We use heterogeneous Poisson
10We also consider the frequencies of unique users by hours,
however, we find it has a extremely high correlation coefficient
with St, about 0.99, so we do not incorporate it.
function as generating functions instead of binomial
function Cnk because sometimes it is difficult to get
the exact total number n in social media.
Threshold: If we find that the count in one time
interval is higher than a predefined threshold, it is
treated as a burst. The threshold is set as 1.5 times
of the average number.
PeakFinding: This is the method proposed
in (Marcus et al 2011), which aims to automatically
discover peaks from tweets.
Binomial: This is the method proposed in (Fung et
al., 2007a), which uses a cumulative binomial distri-
bution with a base probability estimated by remov-
ing abnormal frequencies.
As for multiple-stream burst detection, to the best
of our knowledge, the only existing work is pro-
posed by (Yao et al 2010), which is supervised and
requires a considerable amount of training time, so
we do not compare our work with it. We compare
our method with the following heuristic baselines:
SimpleConjunct: we first find the optimal state se-
quences for each single activity stream. We then de-
rive a global state sequence by taking the conjunc-
tion of all local states.
SimpleDisjunct: we first find the optimal state se-
quences for each single activity stream, and then we
derive a global state sequence by take the disjunction
of all local states.
Another possible baseline is that we first merge
all the activities, then apply the single-stream algo-
rithm. However, in our data set, we find that the
number of activities in St is significantly larger than
that of the two types. St dominantly determines the
final performance, so we do not incorporate it here
as a comparison.
4.4 Experimental Results
Preliminary results on a single stream
We first examine the performance of our proposed
method on a single stream. Note that, our method
in Equation 4 has two merits: 1) the length of lo-
cal window can be tuned on different datasets; 2) a
novel state smoothness function is adopted.
We set the ? function in Equation 4 respectively
as g1 and g2, and apply our proposed methods to
three streams (St,Sr,Su) mentioned above. Note
that, when L = 2 and ? = g1, our method becomes
the algorithm in (Kleinberg, 2003). We tune the pa-
rameter ?1 in Equation 4 from 2 to 20 with a step of
2. We record the best F performance and compute
1472
the corresponding standard deviation. In Table 5, we
can observe that 1) streams St and Sr perform better
than Su; 2) the length of local window significantly
affects the performance; 3) g2 is much better than g1
in our proposed burst detection algorithm; 4) gen-
erally speaking, a longer window size (L = 3, 4)
performs better than the most common used size 2
in (Kleinberg, 2003).
We can see that our proposed method is more ef-
fective than the other baselines. The major reason is
that none of these methods consider state smooth-
ness in a systematic way. In our preliminary ex-
periments, we find that these baselines usually out-
put a lot of bursts, most of which are broken mean-
ingful bursts. To overcome this, baseline method
StateMachine (g1 + L = 2) requires larger ? and
?1, which may discard relatively small meaningful
bursts; while our proposed single stream method
(g2 + L = 3, 4) tends to identify steady and con-
secutive bursts through the help of longer context
window and context sensitive smoothness function
g2, it is more suitable to be applied to social media
for burst detection.
Compared with the other baselines, (Kleinberg,
2003) is still one good and robust baseline since it
models the state smoothness partially. These prelim-
inary findings indicate that state smoothness is very
important for burst detection, and the length of state
context window will affect the performance signifi-
cantly.
To get a deep analysis of the performance of dif-
ferent streams, we set up three classes, and each
class corresponds to a single stream. Since for each
query, we can obtain multiple results in different ac-
tivity streams, we further categorize the 17 anno-
tated queries to the stream which leads to the opti-
mal performance on that query. Interestingly, we can
see: 1) the url stream gives better performance on
queries about big companies because users in Twit-
ter usually talk about the release of new products
or important evolutionary news via url-embedded
tweets; 2) the retweet stream gives better perfor-
mance on queries which correspond to unexpected
or significant events, e.g., diasters. It is consistent
with our intuitions that users in Twitter do actively
spread such information. Combining previous anal-
ysis of Table 5, overall we find the retweet stream is
more capable to identify bursts which correspond to
significant events.
Table 4: Categorization of 17 queries according to the
optimal performance.
Streams Queries
url Apple,Microsoft,Nokia, climate
retweet bomb,crash,earthquake,typhoon,
F1,Google,Olympics
all tweet Amazon, eclipse, Lakers,
NASA, Nobel Prize, Barack Obama
Table 5: Performance (average F) on a single stream.
???? indicates that the improvement our proposed single-
stream methodg2,L=4 over all the other baselines is ac-
cepted at the confidence level of 0.95, i.e., StateMachine,
PeakingFinding, Binomial and Threshold.
? L St Sr Su
4 0.545/0.015 0.543/0.037 0.451/0.036
g2 3 0.536/0.013 0.549??/0.019 0.464/0.025
2 0.468/0.055 0.542/0.071 0.455/0.045
4 0.513/0.059 0.546/0.058 0.465/0.047
g1 3 0.469/0.055 0.542/0.071 0.455/0.045
2 0.396/0.043 0.489/0.074 0.374/0.035
StateMachine 0.396 0.489 0.374
PeakFinding 0.410 0.356 0.302
Binomial 0.315 0.420 0.341
Threshold 0.195 0.181 0.175
Preliminary results on multiple streams
After examining the basic results on a single
stream, we continue to evaluate the performance of
our proposed models on multiple activity streams.
For MBurst in Equation 5, we have three parame-
ters to set, namely L, ?1 and ?2. We do a grid search
for both ?1 and ?2 from 1 to 12 with a step of 1, and
we also examine the performance when L = 2, 3, 4.
We can see that MBurst has four candidate meth-
ods to derive global states from local states; for L2G,
we use the states of St as the final states, and we em-
pirically find that it performs best compared with the
other two streams in L2G.
Recall that our proposed single-stream method
is better than all the other single-stream baselines,
so here single-best denotes our method in Equa-
tion 4 (? = g2, L = 4) on Sr. For SimpleConjunct
and SimpleDisjunct, we first find the optimal state
sequences for each single activity stream using our
proposed method in Equation 4 (? = g2, L = 4),
and then we derive a global state sequence by take
the conjunction or disjunction of all local states re-
spectively.
Besides the best performance, we further compute
the average of the top 10 results of each method
by tuning parameters to check the average perfor-
1473
Table 6: Performance (average F) on multiple streams.
??? indicates that the improvement our proposed
multiple-stream method over our proposed single-stream
method at the confidence level of 0.9 in terms of average
performance.
Methods best average
single-best (g2 + Sr) 0.549 0.526
SimpleConjunct 0.548 -
SimpleDisjunct 0.465 -
MBurst+CONJUNCTr,t,u 0.555 0.548
MBurst+DISJUNCTr,t,u 0.576 0.570?
MBurst+BELIEFr,t,u 0.568 0.561
MBurst+L2Gr,t,u(t) 0.574 0.567
MBurst+L2Gr,t,u(r) 0.560 0.558
mance. The average performance can show the sta-
bility of models in some degree. If one model out-
puts the maximum in a very limited set of parame-
ters, it may not work well in real data, especially in
social media.
In Table 6, we can seeMBurst+DISJUNCTr,t,u
gives the best performance. MBurst performs
consistently better than single-best which is a very
strong single-stream method, especially for average
performance. MBurst+DISJUNCTr,t,u has an im-
provement of average performance over single-best
by 8.4%. And simply combining three different
streams may hit results (SimpleConjunct and Sim-
pleDisjunct). It indicates that MBurst is more sta-
ble and shows a higher performance.
For different methods to derive global bursty pat-
terns, we can see that MBurst+DISJUNCT per-
forms best while MBurst+CONJUNCT performs
worst. Interestingly, however, SimpleConjunct is
better than SimpleDisjunct, the major reason is that
MBurst performs a local-state correlation of mul-
tiple activity streams to correct possible noisy fluc-
tuations from single streams before the conjunction
or disjunction of local states. After such correlation,
the performance of each activity stream should im-
prove. To see this, we present the optimal results of a
single stream without/with local-state correlation in
Table 7. Local-state correlation significantly boosts
the performance of a single stream. Indeed, we find
that the step of local-state correlation is more impor-
tant for our multiple stream algorithm than the step
of how to derive global states based on local states.
We test our MBurst algorithm with the setting:
T = 4416, L = 4 and M = 3, and for all the test
Table 7: Comparison between the optimal results of a
single stream with/without local-state correlation.
all retweet retweet url
without 0.536 0.549 0.464
with 0.574 0.560 0.547
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?1
Average
 F
 
 MBurst+orsingle?best
(a) ?2 = 4, varying ?1.
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?2
Average
 F
 
 MBurst+orsingle?best
(b) ?1 = 11, varying ?2.
Figure 4: Parameter sensitivity of MBurst + DIS-
JUNCT.
queries, our algorithm can respond in 2 seconds 11,
which is efficient to be deployed in social media.
Parameter sensitivity
We have shown the performance of different pa-
rameter settings for single stream algorithm in Ta-
ble 5. Next, we check parameter sensitivity in
MBurst. In our experiments, we find a longer lo-
cal window (L = 3, 4) is better than L = 2, so
we first set L = 4, then we select parameter set-
tings of ?2 = 4 and ?1 = 11, which give best per-
formance for MBurst+DISJUNCT. We vary one
with the other fixed to see how one single parame-
ter affects the performance. The results are shown in
Figure 4, and we can see MBurst+DISJUNCT is
consistently better than single-best.
5 Related Work
Our work is related to burst detection from text
streams. Pioneered by the automaton model pro-
posed in (Kleinberg, 2003), many techniques have
been proposed for burst detection such as the ?2-
test based method (Swan and Allan, 2000), the
parameter-free method (Fung et al 2005) and mov-
ing average method (Vlachos et al 2004). Our work
is related to the applications of these burst detection
algorithms for event detection (He et al 2007; Fung
et al 2007b; Shan et al 2012; Zhao et al 2012).
11All experiments are tested in a Mac PC, 2.4GHz Intel Core
2 Duo.
1474
Some recent work try to identify hot trends (Math-
ioudakis and Koudas, 2010; Zubiaga et al 2011;
Budak et al 2011; Naaman et al 2011) or make
use of the burstiness (Sakaki et al 2010; Aramki
et al 2011; Marcus et al 2011) in social media.
However, few of these methods consider modeling
the local smoothness of one state sequence in a sys-
tematic way and often use a fixed window length of
2.
Little work considers making use of different
types of social media activities for burst detection.
(Yao et al 2010; Kotov et al 2011; Wang et al
2007; Wang et al 2009) conducted some prelim-
inary studies of mining correlated bursty patterns
from multiple sources. However, they either highly
relies on high-quality training datasets or require a
considerable amount of training time. Online social
activities are dynamic, with a large number of new
items generated continuously. In such a dynamic
setting, burst detection algorithms should effectively
collect evidence, efficiently adjust prediction models
and respond to the users as social media activities
evolve. Therefore it is not suitable to deploy such
algorithms in social media.
Our work is also similar to studies which aim
to mine and leverage knowledge from social me-
dia (Mathioudakis et al 2010; Ruiz et al 2012;
Morales et al 2012). We share the common point
with these studies that we try to utilize the under-
lying rich knowledge in social media, while our fo-
cus of this work is quite different from theirs, i.e., to
identify event-related bursts.
Another line of related research is Twitter related
studies (Kwak et al 2010; Sakaki et al 2010). Our
proposed methods can provide event-related bursts
for downstream applications.
6 Conclusion
In this paper, we propose to identify event-related
bursts via social media activities. We propose one
optimization model to correlate multiple activity
streams to learn the bursty patterns. To better mea-
sure local smoothness of the state sequence, we pro-
pose a novel state cost function. We test our meth-
ods in a large Twitter dataset. The experiment re-
sults show that our methods are both effective and
efficient. Our work can provide a preliminary un-
derstanding of the correlation between the happen-
ings of events and the degree of online social media
activities.
Finally, we present a few promising directions
which may potentially improve or enrich current
work.
1) Variable-length context. In this paper, L is a
pre-determined parameter which controls the size of
context window. It cannot be modified when the al-
gorithm runs. A large L will significantly increases
the algorithm complexity, and we may not need a
large L for all the states in a Markov chain. This
problem can be addressed by using the variable-
length hidden Markov model (Wang et al 2006),
which is able to learn the ?minimum? context length
for accurately determining each state.
2) Incorporation of more useful features. Our
current model mainly considers temporal variations
of streaming data and searches the surge patterns ex-
isting in it. In some cases, simple frequency infor-
mation may not be capable to identify all the mean-
ingful bursts. It can be potentially useful to leverage
up more features to help filter out noisy bursts, e.g.,
semantic information (Zhao et al 2010).
3) Modeling multi-modality data. We have ex-
amined our multi-stream algorithm by using three
different activity streams. These streams are textual-
based. It will be interesting to check our algorithm in
multi-modality data streams. E.g., in Facebook, we
may collect a stream consisting of the daily frequen-
cies of photo sharing and another stream consisting
of the daily frequencies of text status updates.
4) Evaluation of the identified bursts. In most
of previous work, they seldom construct a gold stan-
dard for quantitative test, instead they qualitatively
evaluate their methods. In our work, we invite hu-
man judges to generate the gold standard. It is time-
consuming, and the bias from human judges cannot
be completely eliminated although more judges can
be invited. A possible evaluation method is to exam-
ine the identified bursts in downstream applications,
e.g., event detection.
Acknowledgement
This work is partially supported by NSFC Grant
61073082, 60933004 and 70903008. Xin Zhao is
supported by Google PhD Fellowship (China). We
thank the insightful comments from Junjie Yao and
the anonymous reviewers.
1475
References
Eiji Aramki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568?1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Ceren Budak, Divyakant Agrawal, and Amr El Abbadi.
2011. Structural trend analysis for online social net-
works. Proc. VLDB Endow., 4, July.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007a. Time-dependent event hierar-
chy construction. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?07.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007b. Time-dependent event hierarchy
construction. In SIGKDD.
Qi He, Kuiyu Chang, and Ee-Peng Lim. 2007. Analyz-
ing feature trajectories for event detection. In SIGIR.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD, pages 207?216, New
York, NY, USA. ACM.
J. Kleinberg. 2003. Bursty and hierarchical structure in
streams. Data Mining and Knowledge Discovery.
Alexander Kotov, ChengXiang Zhai, and Richard Sproat.
2011. Mining named entities with temporally corre-
lated bursts from multilingual web news streams. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM, pages
237?246.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a
news media? In WWW ?10: Proceedings of the 19th
international conference on World wide web, pages
591?600.
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: aggregating and visualizing
microblogs for event exploration. In Proceedings of
the 2011 annual conference on Human factors in com-
puting systems, CHI ?11.
Michael Mathioudakis and Nick Koudas. 2010. Twit-
termonitor: trend detection over the twitter stream.
In Proceedings of the 2010 international conference
on Management of data, SIGMOD ?10, pages 1155?
1158.
Michael Mathioudakis, Nick Koudas, and Peter Marbach.
2010. Early online identification of attention gather-
ing items in social media. In Proceedings of the third
ACM international conference on Web search and data
mining, WSDM ?10, pages 301?310, New York, NY,
USA. ACM.
Gianmarco De Francisci Morales, Aristides Gionis, and
Claudio Lucchese. 2012. From chatter to headlines:
harnessing the real-time web for personalized news
recommendation. In WSDM, pages 153?162.
Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twitter.
JASIST, 62(5):902?918.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aris-
tides Gionis, and Alejandro Jaimes. 2012. Correlat-
ing financial time series with micro-blogging activity.
pages 513?522.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. WWW, pages 851?860,
New York, NY, USA. ACM.
Dongdong Shan, Wayne Xin Zhao, Rishan Chen, Shu
Baihan, Hongfei Yan, and Xiaoming Li. 2012.
Eventsearch: A system for event discovery and re-
trieval on multi-type historical data. In KDD?12, De-
mostration.
Russell Swan and James Allan. 2000. Automatic gener-
ation of overview timelines. In SIGIR.
Michail Vlachos, Christopher Meek, Zografoula Vagena,
and Dimitrios Gunopulos. 2004. Identifying similari-
ties, periodicities and bursts for online search queries.
In SIGMOD.
Yi Wang, Lizhu Zhou, Jianhua Feng, JianyongWang, and
Zhi-Qiang Liu. 2006. Mining complex time-series
data by learning markovian models. In Proceedings
of the Sixth International Conference on Data Min-
ing, ICDM, pages 1136?1140, Washington, DC, USA.
IEEE Computer Society.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, WSDM, pages 192?201.
Junjie Yao, Bin Cui, Yuxin Huang, and Xin Jin. 2010.
Temporal and social context based burst detection
from folksonomies. In AAAI.
Wayne Xin Zhao, Jing Jiang, Jing He, Dongdong Shan,
Hongfei Yan, and Xiaoming Li. 2010. Context mod-
eling for ranking and tagging bursty features in text
1476
streams. In Proceedings of the 19th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?10.
Wayne Xin Zhao, Rishan Chen, Kai Fan, Hongfei Yan,
and Xiaoming Li. 2012. A novel burst-based text
representation model for scalable event detection. In
ACL?12.
Arkaitz Zubiaga, Damiano Spina, V??ctor Fresno, and
Raquel Mart??nez. 2011. Classifying trending topics:
a typology of conversation triggers on twitter. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM.
1477
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Topical Keyphrase Extraction from Twitter
Wayne Xin Zhao? Jing Jiang? Jing He? Yang Song? Palakorn Achananuparp?
Ee-Peng Lim? Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,peaceful.he,songyangmagic}@gmail.com,
{jingjiang,eplim,palakorna}@smu.edu.sg, lxm@pku.edu.cn
Abstract
Summarizing and analyzing Twitter content is
an important and challenging task. In this pa-
per, we propose to extract topical keyphrases
as one way to summarize Twitter. We propose
a context-sensitive topical PageRank method
for keyword ranking and a probabilistic scor-
ing function that considers both relevance and
interestingness of keyphrases for keyphrase
ranking. We evaluate our proposed methods
on a large Twitter data set. Experiments show
that these methods are very effective for topi-
cal keyphrase extraction.
1 Introduction
Twitter, a new microblogging website, has attracted
hundreds of millions of users who publish short
messages (a.k.a. tweets) on it. They either pub-
lish original tweets or retweet (i.e. forward) oth-
ers? tweets if they find them interesting. Twitter
has been shown to be useful in a number of appli-
cations, including tweets as social sensors of real-
time events (Sakaki et al, 2010), the sentiment pre-
diction power of Twitter (Tumasjan et al, 2010),
etc. However, current explorations are still in an
early stage and our understanding of Twitter content
still remains limited. How to automatically under-
stand, extract and summarize useful Twitter content
has therefore become an important and emergent re-
search topic.
In this paper, we propose to extract keyphrases
as a way to summarize Twitter content. Tradition-
ally, keyphrases are defined as a short list of terms to
summarize the topics of a document (Turney, 2000).
It can be used for various tasks such as document
summarization (Litvak and Last, 2008) and index-
ing (Li et al, 2004). While it appears natural to use
keyphrases to summarize Twitter content, compared
with traditional text collections, keyphrase extrac-
tion from Twitter is more challenging in at least two
aspects: 1) Tweets are much shorter than traditional
articles and not all tweets contain useful informa-
tion; 2) Topics tend to be more diverse in Twitter
than in formal articles such as news reports.
So far there is little work on keyword or keyphrase
extraction from Twitter. Wu et al (2010) proposed
to automatically generate personalized tags for Twit-
ter users. However, user-level tags may not be suit-
able to summarize the overall Twitter content within
a certain period and/or from a certain group of peo-
ple such as people in the same region. Existing work
on keyphrase extraction identifies keyphrases from
either individual documents or an entire text collec-
tion (Turney, 2000; Tomokiyo and Hurst, 2003).
These approaches are not immediately applicable
to Twitter because it does not make sense to ex-
tract keyphrases from a single tweet, and if we ex-
tract keyphrases from a whole tweet collection we
will mix a diverse range of topics together, which
makes it difficult for users to follow the extracted
keyphrases.
Therefore, in this paper, we propose to study the
novel problem of extracting topical keyphrases for
summarizing and analyzing Twitter content. In other
words, we extract and organize keyphrases by top-
ics learnt from Twitter. In our work, we follow the
standard three steps of keyphrase extraction, namely,
keyword ranking, candidate keyphrase generation
379
and keyphrase ranking. For keyword ranking, we
modify the Topical PageRank method proposed by
Liu et al (2010) by introducing topic-sensitive score
propagation. We find that topic-sensitive propaga-
tion can largely help boost the performance. For
keyphrase ranking, we propose a principled proba-
bilistic phrase ranking method, which can be flex-
ibly combined with any keyword ranking method
and candidate keyphrase generation method. Ex-
periments on a large Twitter data set show that
our proposed methods are very effective in topical
keyphrase extraction from Twitter. Interestingly, our
proposed keyphrase ranking method can incorporate
users? interests by modeling the retweet behavior.
We further examine what topics are suitable for in-
corporating users? interests for topical keyphrase ex-
traction.
To the best of our knowledge, our work is the
first to study how to extract keyphrases from mi-
croblogs. We perform a thorough analysis of the
proposed methods, which can be useful for future
work in this direction.
2 Related Work
Our work is related to unsupervised keyphrase ex-
traction. Graph-based ranking methods are the
state of the art in unsupervised keyphrase extrac-
tion. Mihalcea and Tarau (2004) proposed to use
TextRank, a modified PageRank algorithm to ex-
tract keyphrases. Based on the study by Mihalcea
and Tarau (2004), Liu et al (2010) proposed to de-
compose a traditional random walk into multiple
random walks specific to various topics. Language
modeling methods (Tomokiyo and Hurst, 2003) and
natural language processing techniques (Barker and
Cornacchia, 2000) have also been used for unsuper-
vised keyphrase extraction. Our keyword extraction
method is mainly based on the study by Liu et al
(2010). The difference is that we model the score
propagation with topic context, which can lower the
effect of noise, especially in microblogs.
Our work is also related to automatic topic label-
ing (Mei et al, 2007). We focus on extracting topical
keyphrases in microblogs, which has its own chal-
lenges. Our method can also be used to label topics
in other text collections.
Another line of relevant research is Twitter-
related text mining. The most relevant work is
by Wu et al (2010), who directly applied Tex-
tRank (Mihalcea and Tarau, 2004) to extract key-
words from tweets to tag users. Topic discovery
from Twitter is also related to our work (Ramage et
al., 2010), but we further extract keyphrases from
each topic for summarizing and analyzing Twitter
content.
3 Method
3.1 Preliminaries
Let U be a set of Twitter users. Let C =
{{du,m}
Mu
m=1}u?U be a collection of tweets gener-
ated by U , where Mu is the total number of tweets
generated by user u and du,m is the m-th tweet of
u. Let V be the vocabulary. du,m consists of a
sequence of words (wu,m,1, wu,m,2, . . . , wu,m,Nu,m)
where Nu,m is the number of words in du,m and
wu,m,n ? V (1 ? n ? Nu,m). We also assume
that there is a set of topics T over the collection C.
Given T and C, topical keyphrase extraction is to
discover a list of keyphrases for each topic t ? T .
Here each keyphrase is a sequence of words.
To extract keyphrases, we first identify topics
from the Twitter collection using topic models (Sec-
tion 3.2). Next for each topic, we run a topical
PageRank algorithm to rank keywords and then gen-
erate candidate keyphrases using the top ranked key-
words (Section 3.3). Finally, we use a probabilis-
tic model to rank the candidate keyphrases (Sec-
tion 3.4).
3.2 Topic discovery
We first describe how we discover the set of topics
T . Author-topic models have been shown to be ef-
fective for topic modeling of microblogs (Weng et
al., 2010; Hong and Davison, 2010). In Twit-
ter, we observe an important characteristic of tweets:
tweets are short and a single tweet tends to be about
a single topic. So we apply a modified author-topic
model called Twitter-LDA introduced by Zhao et al
(2011), which assumes a single topic assignment for
an entire tweet.
The model is based on the following assumptions.
There is a set of topics T in Twitter, each represented
by a word distribution. Each user has her topic inter-
ests modeled by a distribution over the topics. When
a user wants to write a tweet, she first chooses a topic
based on her topic distribution. Then she chooses a
380
1. Draw ?B ? Dir(?), pi ? Dir(?)
2. For each topic t ? T ,
(a) draw ?t ? Dir(?)
3. For each user u ? U ,
(a) draw ?u ? Dir(?)
(b) for each tweet du,m
i. draw zu,m ? Multi(?u)
ii. for each word wu,m,n
A. draw yu,m,n ? Bernoulli(pi)
B. draw wu,m,n ? Multi(?B) if
yu,m,n = 0 and wu,m,n ?
Multi(?zu,m) if yu,m,n = 1
Figure 1: The generation process of tweets.
bag of words one by one based on the chosen topic.
However, not all words in a tweet are closely re-
lated to the topic of that tweet; some are background
words commonly used in tweets on different topics.
Therefore, for each word in a tweet, the user first
decides whether it is a background word or a topic
word and then chooses the word from its respective
word distribution.
Formally, let ?t denote the word distribution for
topic t and ?B the word distribution for background
words. Let ?u denote the topic distribution of user
u. Let pi denote a Bernoulli distribution that gov-
erns the choice between background words and topic
words. The generation process of tweets is described
in Figure 1. Each multinomial distribution is gov-
erned by some symmetric Dirichlet distribution pa-
rameterized by ?, ? or ?.
3.3 Topical PageRank for Keyword Ranking
Topical PageRank was introduced by Liu et al
(2010) to identify keywords for future keyphrase
extraction. It runs topic-biased PageRank for each
topic separately and boosts those words with high
relevance to the corresponding topic. Formally, the
topic-specific PageRank scores can be defined as
follows:
Rt(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
Rt(wj)+ (1??)Pt(wi),
(1)
where Rt(w) is the topic-specific PageRank score
of word w in topic t, e(wj , wi) is the weight for the
edge (wj ? wi), O(wj) =
?
w? e(wj , w
?) and ?
is a damping factor ranging from 0 to 1. The topic-
specific preference value Pt(w) for each word w is
its random jumping probability with the constraint
that
?
w?V Pt(w) = 1 given topic t. A large Rt(?)
indicates a word is a good candidate keyword in
topic t. We denote this original version of the Topi-
cal PageRank as TPR.
However, the original TPR ignores the topic con-
text when setting the edge weights; the edge weight
is set by counting the number of co-occurrences of
the two words within a certain window size. Tak-
ing the topic of ?electronic products? as an exam-
ple, the word ?juice? may co-occur frequently with a
good keyword ?apple? for this topic because of Ap-
ple electronic products, so ?juice? may be ranked
high by this context-free co-occurrence edge weight
although it is not related to electronic products. In
other words, context-free propagation may cause the
scores to be off-topic.
So in this paper, we propose to use a topic context
sensitive PageRank method. Formally, we have
Rt(wi) = ?
?
j:wj?wi
et(wj , wi)
Ot(wj)
Rt(wj)+(1??)Pt(wi).
(2)
Here we compute the propagation from wj to wi in
the context of topic t, namely, the edge weight from
wj to wi is parameterized by t. In this paper, we
compute edge weight et(wj , wi) between two words
by counting the number of co-occurrences of these
two words in tweets assigned to topic t. We denote
this context-sensitive topical PageRank as cTPR.
After keyword ranking using cTPR or any other
method, we adopt a common candidate keyphrase
generation method proposed by Mihalcea and Tarau
(2004) as follows. We first select the top S keywords
for each topic, and then look for combinations of
these keywords that occur as frequent phrases in the
text collection. More details are given in Section 4.
3.4 Probabilistic Models for Topical Keyphrase
Ranking
With the candidate keyphrases, our next step is to
rank them. While a standard method is to simply
aggregate the scores of keywords inside a candidate
keyphrase as the score for the keyphrase, here we
propose a different probabilistic scoring function.
Our method is based on the following hypotheses
about good keyphrases given a topic:
381
Figure 2: Assumptions of variable dependencies.
Relevance: A good keyphrase should be closely re-
lated to the given topic and also discriminative. For
example, for the topic ?news,? ?president obama? is
a good keyphrase while ?math class? is not.
Interestingness: A good keyphrase should be inter-
esting and can attract users? attention. For example,
for the topic ?music,? ?justin bieber? is more inter-
esting than ?song player.?
Sometimes, there is a trade-off between these two
properties and a good keyphrase has to balance both.
Let R be a binary variable to denote relevance
where 1 is relevant and 0 is irrelevant. Let I be an-
other binary variable to denote interestingness where
1 is interesting and 0 is non-interesting. Let k denote
a candidate keyphrase. Following the probabilistic
relevance models in information retrieval (Lafferty
and Zhai, 2003), we propose to use P (R = 1, I =
1|t, k) to rank candidate keyphrases for topic t. We
have
P (R = 1, I = 1|t, k)
= P (R = 1|t, k)P (I = 1|t, k, R = 1)
= P (I = 1|t, k, R = 1)P (R = 1|t, k)
= P (I = 1|k)P (R = 1|t, k)
= P (I = 1|k)?
P (R = 1|t, k)
P (R = 1|t, k) + P (R = 0|t, k)
= P (I = 1|k)?
1
1 + P (R=0|t,k)P (R=1|t,k)
= P (I = 1|k)?
1
1 + P (R=0,k|t)P (R=1,k|t)
= P (I = 1|k)?
1
1 + P (R=0|t)P (R=1|t) ?
P (k|t,R=0)
P (k|t,R=1)
= P (I = 1|k)?
1
1 + P (R=0)P (R=1) ?
P (k|t,R=0)
P (k|t,R=1)
.
Here we have assumed that I is independent of t and
R given k, i.e. the interestingness of a keyphrase is
independent of the topic or whether the keyphrase is
relevant to the topic. We have also assumed that R
is independent of t when k is unknown, i.e. without
knowing the keyphrase, the relevance is independent
of the topic. Our assumptions can be depicted by
Figure 2.
We further define ? = P (R=0)P (R=1) . In general we
can assume that P (R = 0)  P (R = 1) because
there are much more non-relevant keyphrases than
relevant ones, that is, ?  1. In this case, we have
logP (R = 1, I = 1|t, k) (3)
= log
(
P (I = 1|k)?
1
1 + ? ? P (k|t,R=0)P (k|t,R=1)
)
? log
(
P (I = 1|k)?
P (k|t, R = 1)
P (k|t, R = 0)
?
1
?
)
= logP (I = 1|k) + log
P (k|t, R = 1)
P (k|t, R = 0)
? log ?.
We can see that the ranking score logP (R = 1, I =
1|t, k) can be decomposed into two components, a
relevance score log P (k|t,R=1)P (k|t,R=0) and an interestingness
score logP (I = 1|k). The last term log ? is a con-
stant and thus not relevant.
Estimating the relevance score
Let a keyphrase candidate k be a sequence of
words (w1, w2, . . . , wN ). Based on an independent
assumption of words given R and t, we have
log
P (k|t, R = 1)
P (k|t, R = 0)
= log
P (w1w2 . . . wN |t, R = 1)
P (w1w2 . . . wN |t, R = 0)
=
N?
n=1
log
P (wn|t, R = 1)
P (wn|t, R = 0)
. (4)
Given the topic model ?t previously learned for
topic t, we can set P (w|t, R = 1) to ?tw, i.e. the
probability of w under ?t. Following Griffiths and
Steyvers (2004), we estimate ?tw as
?tw =
#(Ct, w) + ?
#(Ct, ?) + ?|V|
. (5)
Here Ct denotes the collection of tweets assigned to
topic t, #(Ct, w) is the number of times w appears in
Ct, and #(Ct, ?) is the total number of words in Ct.
P (w|t, R = 0) can be estimated using a smoothed
background model.
P (w|R = 0, t) =
#(C, w) + ?
#(C, ?) + ?|V|
. (6)
382
Here #(C, ?) denotes the number of words in the
whole collection C, and #(C, w) denotes the number
of times w appears in the whole collection.
After plugging Equation (5) and Equation (6) into
Equation (4), we get the following formula for the
relevance score:
log
P (k|t, R = 1)
P (k|t, R = 0)
=
?
w?k
(
log
#(Ct, w) + ?
#(C, w) + ?
+ log
#(C, ?) + ?|V|
#(Ct, ?) + ?|V|
)
=
(?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?, (7)
where ? = #(C,?)+?|V|#(Ct,?)+?|V| and |k| denotes the number
of words in k.
Estimating the interestingness score
To capture the interestingness of keyphrases, we
make use of the retweeting behavior in Twitter. We
use string matching with RT to determine whether
a tweet is an original posting or a retweet. If a
tweet is interesting, it tends to get retweeted mul-
tiple times. Retweeting is therefore a stronger indi-
cator of user interests than tweeting. We use retweet
ratio |ReTweetsk||Tweetsk| to estimate P (I = 1|k). To prevent
zero frequency, we use a modified add-one smooth-
ing method. Finally, we get
logP (I = 1|k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
. (8)
Here |ReTweetsk| and |Tweetsk| denote the num-
bers of retweets and tweets containing the keyphrase
k, respectively, and lavg is the average number of
tweets that a candidate keyphrase appears in.
Finally, we can plug Equation (7) and Equa-
tion (8) into Equation (3) and obtain the following
scoring function for ranking:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(9)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?.
#user #tweet #term #token
13,307 1,300,300 50,506 11,868,910
Table 1: Some statistics of the data set.
Incorporating length preference
Our preliminary experiments with Equation (9)
show that this scoring function usually ranks longer
keyphrases higher than shorter ones. However, be-
cause our candidate keyphrase are extracted without
using any linguistic knowledge such as noun phrase
boundaries, longer candidate keyphrases tend to be
less meaningful as a phrase. Moreover, for our task
of using keyphrases to summarize Twitter, we hy-
pothesize that shorter keyphrases are preferred by
users as they are more compact. We would there-
fore like to incorporate some length preference.
Recall that Equation (9) is derived from P (R =
1, I = 1|t, k), but this probability does not allow
us to directly incorporate any length preference. We
further observe that Equation (9) tends to give longer
keyphrases higher scores mainly due to the term
|k|?. So here we heuristically incorporate our length
preference by removing |k|? from Equation (9), re-
sulting in the following final scoring function:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(10)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
.
4 Experiments
4.1 Data Set and Preprocessing
We use a Twitter data set collected from Singapore
users for evaluation. We used Twitter REST API1
to facilitate the data collection. The majority of the
tweets collected were published in a 20-week period
from December 1, 2009 through April 18, 2010. We
removed common stopwords and words which ap-
peared in fewer than 10 tweets. We also removed all
users who had fewer than 5 tweets. Some statistics
of this data set after cleaning are shown in Table 1.
We ran Twitter-LDA with 500 iterations of Gibbs
sampling. After trying a few different numbers of
1http://apiwiki.twitter.com/w/page/22554663/REST-API-
Documentation
383
topics, we empirically set the number of topics to
30. We set ? to 50.0/|T | as Griffiths and Steyvers
(2004) suggested, but set ? to a smaller value of 0.01
and ? to 20. We chose these parameter settings be-
cause they generally gave coherent and meaningful
topics for our data set. We selected 10 topics that
cover a diverse range of content in Twitter for eval-
uation of topical keyphrase extraction. The top 10
words of these topics are shown in Table 2.
We also tried the standard LDA model and the
author-topic model on our data set and found that
our proposed topic model was better or at least com-
parable in terms of finding meaningful topics. In ad-
dition to generating meaningful topics, Twitter-LDA
is much more convenient in supporting the compu-
tation of tweet-level statistics (e.g. the number of
co-occurrences of two words in a specific topic) than
the standard LDA or the author-topic model because
Twitter-LDA assumes a single topic assignment for
an entire tweet.
4.2 Methods for Comparison
As we have described in Section 3.1, there are three
steps to generate keyphrases, namely, keyword rank-
ing, candidate keyphrase generation, and keyphrase
ranking. We have proposed a context-sensitive top-
ical PageRank method (cTPR) for the first step of
keyword ranking, and a probabilistic scoring func-
tion for the third step of keyphrase ranking. We now
describe the baseline methods we use to compare
with our proposed methods.
Keyword Ranking
We compare our cTPR method with the original
topical PageRank method (Equation (1)), which rep-
resents the state of the art. We refer to this baseline
as TPR.
For both TPR and cTPR, the damping factor is
empirically set to 0.1, which always gives the best
performance based on our preliminary experiments.
We use normalized P (t|w) to set Pt(w) because our
preliminary experiments showed that this was the
best among the three choices discussed by Liu et al
(2010). This finding is also consistent with what Liu
et al (2010) found.
In addition, we also use two other baselines for
comparison: (1) kwBL1: ranking by P (w|t) = ?tw.
(2) kwBL2: ranking by P (t|w) = P (t)?
t
w?
t? P (t
?)?t?w
.
Keyphrase Ranking
We use kpRelInt to denote our relevance and inter-
estingness based keyphrase ranking function P (R =
1, I = 1|t, k), i.e. Equation (10). ? and ? are em-
pirically set to 0.01 and 500. Usually ? can be set to
zero, but in our experiments we find that our rank-
ing method needs a more uniform estimation of the
background model. We use the following ranking
functions for comparison:
? kpBL1: Similar to what is used by Liu et al
(2010), we can rank candidate keyphrases by
?
w?k f(w), where f(w) is the score assigned
to word w by a keyword ranking method.
? kpBL2: We consider another baseline ranking
method by
?
w?k log f(w).
? kpRel: If we consider only relevance but
not interestingness, we can rank candidate
keyphrases by
?
w?k log
#(Ct,w)+?
#(C,w)+? .
4.3 Gold Standard Generation
Since there is no existing test collection for topi-
cal keyphrase extraction from Twitter, we manually
constructed our test collection. For each of the 10
selected topics, we ran all the methods to rank key-
words. For each method we selected the top 3000
keywords and searched all the combinations of these
words as phrases which have a frequency larger than
30. In order to achieve high phraseness, we first
computed the minimum value of pointwise mutual
information for all bigrams in one combination, and
we removed combinations having a value below a
threshold, which was empirically set to 2.135. Then
we merged all these candidate phrases. We did not
consider single-word phrases because we found that
it would include too many frequent words that might
not be useful for summaries.
We asked two judges to judge the quality of the
candidate keyphrases. The judges live in Singapore
and had used Twitter before. For each topic, the
judges were given the top topic words and a short
topic description. Web search was also available.
For each candidate keyphrase, we asked the judges
to score it as follows: 2 (relevant, meaningful and in-
formative), 1 (relevant but either too general or too
specific, or informal) and 0 (irrelevant or meaning-
less). Here in addition to relevance, the other two
criteria, namely, whether a phrase is meaningful and
informative, were studied by Tomokiyo and Hurst
384
T2 T4 T5 T10 T12 T13 T18 T20 T23 T25
eat twitter love singapore singapore hot iphone song study win
food tweet idol road #singapore rain google video school game
dinner blog adam mrt #business weather social youtube time team
lunch facebook watch sgreinfo #news cold media love homework match
eating internet april east health morning ipad songs tomorrow play
ice tweets hot park asia sun twitter bieber maths chelsea
chicken follow lambert room market good free music class world
cream msn awesome sqft world night app justin paper united
tea followers girl price prices raining apple feature math liverpool
hungry time american built bank air marketing twitter finish arsenal
Table 2: Top 10 Words of Sample Topics on our Singapore Twitter Dateset.
(2003). We then averaged the scores of the two
judges as the final scores. The Cohen?s Kappa co-
efficients of the 10 topics range from 0.45 to 0.80,
showing fair to good agreement2. We further dis-
carded all candidates with an average score less than
1. The number of the remaining keyphrases for each
topic ranges from 56 to 282.
4.4 Evaluation Metrics
Traditionally keyphrase extraction is evaluated using
precision and recall on all the extracted keyphrases.
We choose not to use these measures for the fol-
lowing reasons: (1) Traditional keyphrase extraction
works on single documents while we study topical
keyphrase extraction. The gold standard keyphrase
list for a single document is usually short and clean,
while for each Twitter topic there can be many
keyphrases, some are more relevant and interesting
than others. (2) Our extracted topical keyphrases are
meant for summarizing Twitter content, and they are
likely to be directly shown to the users. It is there-
fore more meaningful to focus on the quality of the
top-ranked keyphrases.
Inspired by the popular nDCG metric in informa-
tion retrieval (Ja?rvelin and Keka?la?inen, 2002), we
define the following normalized keyphrase quality
measure (nKQM) for a methodM:
nKQM@K =
1
|T |
?
t?T
?K
j=1
1
log2(j+1)
score(Mt,j)
IdealScore(K,t)
,
where T is the set of topics, Mt,j is the j-
th keyphrase generated by method M for topic
2We find that judgments on topics related to social me-
dia (e.g. T4) and daily life (e.g. T13) tend to have a higher
degree of disagreement.
t, score(?) is the average score from the two hu-
man judges, and IdealScore(K,t) is the normalization
factor?score of the top K keyphrases of topic t un-
der the ideal ranking. Intuitively, ifM returns more
good keyphrases in top ranks, its nKQM value will
be higher.
We also use mean average precision (MAP) to
measure the overall performance of keyphrase rank-
ing:
MAP =
1
|T |
?
t?T
1
NM,t
|Mt|?
j=1
NM,t,j
j
1(score(Mt,j) ? 1),
where 1(S) is an indicator function which returns
1 when S is true and 0 otherwise, NM,t,j denotes
the number of correct keyphrases among the top j
keyphrases returned byM for topic t, and NM,t de-
notes the total number of correct keyphrases of topic
t returned byM.
4.5 Experiment Results
Evaluation of keyword ranking methods
Since keyword ranking is the first step for
keyphrase extraction, we first compare our keyword
ranking method cTPR with other methods. For each
topic, we pooled the top 20 keywords ranked by all
four methods. We manually examined whether a
word is a good keyword or a noisy word based on
topic context. Then we computed the average num-
ber of noisy words in the 10 topics for each method.
As shown in Table 5, we can observe that cTPR per-
formed the best among the four methods.
Since our final goal is to extract topical
keyphrases, we further compare the performance
of cTPR and TPR when they are combined with a
keyphrase ranking algorithm. Here we use the two
385
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
kpBL1 TPR 0.5015 0.54331 0.5611 0.5715 0.5984
kwBL1 0.6026 0.5683 0.5579 0.5254 0.5984
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6279
cTPR 0.6109 0.6218 0.6139 0.6062 0.6608
kpBL2 TPR 0.7294 0.7172 0.6921 0.6433 0.6379
kwBL1 0.7111 0.6614 0.6306 0.5829 0.5416
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6545
cTPR 0.7491 0.7429 0.6930 0.6519 0.6688
Table 3: Comparisons of keyphrase extraction for cTPR and baselines.
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
cTPR+kpBL1 0.61095 0.62182 0.61389 0.60618 0.6608
cTPR+kpBL2 0.74913 0.74294 0.69303 0.65194 0.6688
cTPR+kpRel 0.75361 0.74926 0.69645 0.65065 0.6696
cTPR+kpRelInt 0.81061 0.75184 0.71422 0.66319 0.6694
Table 4: Comparisons of keyphrase extraction for different keyphrase ranking methods.
kwBL1 kwBL2 TPR cTPR
2 3 4.9 1.5
Table 5: Average number of noisy words among the top
20 keywords of the 10 topics.
baseline keyphrase ranking algorithms kpBL1 and
kpBL2. The comparison is shown in Table 3. We
can see that cTPR is consistently better than the three
other methods for both kpBL1 and kpBL2.
Evaluation of keyphrase ranking methods
In this section we compare keypharse ranking
methods. Previously we have shown that cTPR is
better than TPR, kwBL1 and kwBL2 for keyword
ranking. Therefore we use cTPR as the keyword
ranking method and examine the keyphrase rank-
ing method kpRelInt with kpBL1, kpBL2 and kpRel
when they are combined with cTPR. The results are
shown in Table 4. From the results we can see the
following: (1) Keyphrase ranking methods kpRelInt
and kpRel are more effective than kpBL1 and kpBL2,
especially when using the nKQM metric. (2) kpRe-
lInt is better than kpRel, especially for the nKQM
metric. Interestingly, we also see that for the nKQM
metric, kpBL1, which is the most commonly used
keyphrase ranking method, did not perform as well
as kpBL2, a modified version of kpBL1.
We also tested kpRelInt and kpRel on TPR, kwBL1
and kwBL2 and found that kpRelInt and kpRel are
consistently better than kpBL2 and kpBL1. Due to
space limit, we do not report all the results here.
These findings support our assumption that our pro-
posed keyphrase ranking method is effective.
The comparison between kpBL2 with kpBL1
shows that taking the product of keyword scores is
more effective than taking their sum. kpRel and
kpRelInt also use the product of keyword scores.
This may be because there is more noise in Twit-
ter than traditional documents. Common words (e.g.
?good?) and domain background words (e.g. ?Sin-
gapore?) tend to gain higher weights during keyword
ranking due to their high frequency, especially in
graph-based method, but we do not want such words
to contribute too much to keyphrase scores. Taking
the product of keyword scores is therefore more suit-
able here than taking their sum.
Further analysis of interestingness
As shown in Table 4, kpRelInt performs better
in terms of nKQM compared with kpRel. Here we
study why it worked better for keyphrase ranking.
The only difference between kpRel and kpRelInt is
that kpRelInt includes the factor of user interests. By
manually examining the top keyphrases, we find that
the topics ?Movie-TV? (T5), ?News? (T12), ?Music?
(T20) and ?Sports? (T25) particularly benefited from
kpRelInt compared with other topics. We find that
well-known named entities (e.g. celebrities, politi-
cal leaders, football clubs and big companies) and
significant events tend to be ranked higher by kpRe-
lInt than kpRel.
We then counted the numbers of entity and event
keyphrases for these four topics retrieved by differ-
ent methods, shown in Table 6 . We can see that
in these four topics, kpRelInt is consistently better
than kpRel in terms of the number of entity and event
keyphrases retrieved.
386
T2 T5 T10 T12 T20 T25
chicken rice adam lambert north east president obama justin bieber manchester united
ice cream jack neo rent blk magnitude earthquake music video champions league
fried chicken american idol east coast volcanic ash lady gaga football match
curry rice david archuleta east plaza prime minister taylor swift premier league
chicken porridge robert pattinson west coast iceland volcano demi lovato f1 grand prix
curry chicken alexander mcqueen bukit timah chile earthquake youtube channel tiger woods
beef noodles april fools street view goldman sachs miley cyrus grand slam(tennis)
chocolate cake harry potter orchard road coe prices telephone video liverpool fans
cheese fries april fool toa payoh haiti earthquake song lyrics final score
instant noodles andrew garcia marina bay #singapore #business joe jonas manchester derby
Table 7: Top 10 keyphrases of 6 topics from cTPR+kpRelInt.
Methods T5 T12 T20 T25
cTPR+kpRel 8 9 16 11
cTPR+kpRelInt 10 12 17 14
Table 6: Numbers of entity and event keyphrases re-
trieved by different methods within top 20.
On the other hand, we also find that for some
topics interestingness helped little or even hurt the
performance a little, e.g. for the topics ?Food? and
?Traffic.? We find that the keyphrases in these top-
ics are stable and change less over time. This may
suggest that we can modify our formula to handle
different topics different. We will explore this direc-
tion in our future work.
Parameter settings
We also examine how the parameters in our model
affect the performance.
?: We performed a search from 0.1 to 0.9 with a
step size of 0.1. We found ? = 0.1 was the optimal
parameter for cTPR and TPR. However, TPR is more
sensitive to ?. The performance went down quickly
with ? increasing.
?: We checked the overall performance with
? ? {400, 450, 500, 550, 600}. We found that ? =
500 ? 0.01|V| gave the best performance gener-
ally for cTPR. The performance difference is not
very significant between these different values of ?,
which indicates that the our method is robust.
4.6 Qualitative evaluation of cTPR+kpRelInt
We show the top 10 keyphrases discovered by
cTPR+kRelInt in Table 7. We can observe that these
keyphrases are clear, interesting and informative for
summarizing Twitter topics.
We hypothesize that the following applications
can benefit from the extracted keyphrases:
Automatic generation of realtime trendy phrases:
For exampoe, keyphrases in the topic ?Food? (T2)
can be used to help online restaurant reviews.
Event detection and topic tracking: In the topic
?News? top keyphrases can be used as candidate
trendy topics for event detection and topic tracking.
Automatic discovery of important named entities:
As discussed previously, our methods tend to rank
important named entities such as celebrities in high
ranks.
5 Conclusion
In this paper, we studied the novel problem of topical
keyphrase extraction for summarizing and analyzing
Twitter content. We proposed the context-sensitive
topical PageRank (cTPR) method for keyword rank-
ing. Experiments showed that cTPR is consistently
better than the original TPR and other baseline meth-
ods in terms of top keyword and keyphrase extrac-
tion. For keyphrase ranking, we proposed a prob-
abilistic ranking method, which models both rele-
vance and interestingness of keyphrases. In our ex-
periments, this method is shown to be very effec-
tive to boost the performance of keyphrase extrac-
tion for different kinds of keyword ranking methods.
In the future, we may consider how to incorporate
keyword scores into our keyphrase ranking method.
Note that we propose to rank keyphrases by a gen-
eral formula P (R = 1, I = 1|t, k) and we have made
some approximations based on reasonable assump-
tions. There should be other potential ways to esti-
mate P (R = 1, I = 1|t, k).
Acknowledgements
This work was done during Xin Zhao?s visit to the
Singapore Management University. Xin Zhao and
Xiaoming Li are partially supported by NSFC under
387
the grant No. 60933004, 61073082, 61050009 and
HGJ Grant No. 2011ZX01042-001-001.
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In Pro-
ceedings of the 13th Biennial Conference of the Cana-
dian Society on Computational Studies of Intelligence:
Advances in Artificial Intelligence, pages 40?52.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in Twitter. In Proceedings of
the First Workshop on Social Media Analytics.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Transactions on Information Systems, 20(4):422?446.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic
relevance models based on document and query gener-
ation. Language Modeling and Information Retrieval,
13.
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin Chen.
2004. Incorporating document keyphrases in search
results. In Proceedings of the 10th Americas Confer-
ence on Information Systems.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the Workshop on Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17?24.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong
Sun. 2010. Automatic keyphrase extraction via topic
decomposition. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 366?376.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing or-
der into texts. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing micorblogs with topic models. In Pro-
ceedings of the 4th International Conference on We-
blogs and Social Media.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International World Wide Web Conference.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33?40.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the 4th International
Conference on Weblogs and Social Media.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, (4):303?336.
Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. TwitterRank: finding topic-sensitive influential
twitterers. In Proceedings of the third ACM Interna-
tional Conference on Web Search and Data Mining.
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. Au-
tomatic generation of personalized annotation tags for
twitter users. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 689?692.
Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Lim Ee-
Peng, Hongfei Yan, and Xiaoming Li. 2011. Compar-
ing Twitter and traditional media using topic models.
In Proceedings of the 33rd European Conference on
Information Retrieval.
388
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 43?47,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Novel Burst-based Text Representation Model
for Scalable Event Detection
Wayne Xin Zhao?, Rishan Chen?, Kai Fan?, Hongfei Yan?? and Xiaoming Li??
?School of Electronics Engineering and Computer Science, Peking University, China
?State Key Laboratory of Software, Beihang University, China
{batmanfly,tsunamicrs,fankaicn,yhf1029}@gmail.com, lxm@pku.edu.cn
Abstract
Mining retrospective events from text streams
has been an important research topic. Classic
text representation model (i.e., vector space
model) cannot model temporal aspects of doc-
uments. To address it, we proposed a novel
burst-based text representation model, de-
noted as BurstVSM. BurstVSM corresponds
dimensions to bursty features instead of terms,
which can capture semantic and temporal in-
formation. Meanwhile, it significantly reduces
the number of non-zero entries in the repre-
sentation. We test it via scalable event de-
tection, and experiments in a 10-year news
archive show that our methods are both effec-
tive and efficient.
1 Introduction
Mining retrospective events (Yang et al, 1998; Fung
et al, 2007; Allan et al, 2000) has been quite an im-
portant research topic in text mining. One standard
way for that is to cluster news articles as events by
following a two-step approach (Yang et al, 1998):
1) represent document as vectors and calculate simi-
larities between documents; 2) run the clustering al-
gorithm to obtain document clusters as events.1 Un-
derlying text representation often plays a critical role
in this approach, especially for long text streams. In
this paper, our focus is to study how to represent
temporal documents effectively for event detection.
Classical text representation methods, i.e., Vector
SpaceModel (VSM), have a few shortcomings when
dealing with temporal documents. The major one is
that it maps one dimension to one term, which com-
pletely ignores temporal information, and therefore
VSM can never capture the evolving trends in text
streams. See the example in Figure 1, D1 and D2
?Corresponding author.
1Post-processing may be also needed on the preliminary
document clusters to refine the results.
!" !#
$%&'
()*+*
,-./01#223 ,-./01#224
Figure 1: A motivating example. D1 and D2 are news
articles about U.S. presidential election respectively in
years 2004 and 2008.
may have a high similarity based on VSM due to the
presence of some general terms (e.g., ?election?) re-
lated to U.S. presidential election, although general
terms correspond to events in different periods (i.e.,
November 2004 and November 2008). Temporal
information has to be taken into consideration for
event detection. Another important issue is scala-
bility, with the increasing of the number in the text
stream, the size of the vocabulary, i.e., the number
of dimensions in VSM, can be very large, which re-
quires a considerable amount of space for storage
and time for downstream processing.
To address these difficulties, in this paper, we pro-
pose a burst based text representation method for
scalable event detection. The major novelty is to nat-
urally incorporate temporal information into dimen-
sions themselves instead of using external time de-
caying functions (Yang et al, 1998). We instantiate
this idea by using bursty features as basic representa-
tion units of documents. In this paper, bursty feature
refers to a sudden surge of the frequency of a single
term in a text stream, and it is represented as the term
itself together with the time interval during which
the burst takes place. For example, (Olympic,
Aug-08-2008, Aug-24-2008)
2 can be regarded
as a bursty feature. We also call the term in a bursty
2Beijing 2008 Olympic Games
43
feature its bursty term. In our model, each dimen-
sion corresponds to a bursty feature, which contains
both temporal and semantic information. Bursty fea-
tures capture and reflect the evolving topic trends,
which can be learnt by searching surge patterns in
stream data (Kleinberg, 2003). Built on bursty fea-
tures, our representation model can well adapt to text
streams with complex trends, and therefore provides
a more reasonable temporal document representa-
tion. We further propose a split-cluster-merge algo-
rithm to generate clusters as events. This algorithm
can run a mutli-thread mode to speed up processing.
Our contribution can be summarized as two as-
pects: 1) we propose a novel burst-based text rep-
resentation model, to our best knowledge, it is the
first work which explicitly incorporates temporal in-
formation into dimensions themselves; 2) we test
this representation model via scalable event detec-
tion task on a very large news corpus, and extensive
experiments show the proposed methods are both ef-
fective and efficient.
2 Burst-based Text Representation
In this section, we describe the proposed burst-based
text representation model, denoted as BurstVSM. In
BurstVSM, each document is represented as one
vector as in VSM, while the major novelty is that one
dimension is mapped to one bursty feature instead
of one term. In this paper, we define a bursty fea-
ture f as a triplet (wf , tfs , t
f
e ), where w is the bursty
term and ts and te are the start and end timestamps
of the bursty interval (period). Before introducting
BurstVSM, we first discuss how to identify bursty
features from text streams.
2.1 Burst Detection Algorithm
We follow the batch mode two-state automaton
method from (Kleinberg, 2003) for bursty feature
detection.3 In this model, a stream of documents
containing a term w are assumed to be generated
from a two-state automaton with a low frequency
state q0 and a high frequency state q1. Each state
has its own emission rate (p0 and p1 respectively),
and there is a probability for changing state. If an
interval of high states appears in the optimal state
sequence of some term, this term together with this
interval is detected as a bursty feature. To obtain
all bursty features in text streams, we can perform
burst detection on each term in the vocabulary. In-
stead of using a fixed p0 and p1 in (Kleinberg, 2003),
by following the moving average method (Vlachos
3The news articles in one day is treated as a batch.
et al, 2004) ,we parameterize p0 and p1 with the
time index for each batch, formally, we have p0(t)
and p1(t) for the tth batch. Given a term w, we
use a sliding window of length L to estimate p0(t)
and p1(t) for the tth batch as follows: p0(t) =?
j?Wt
Nj,w?
j?Wt
Nj
and p1(t) = p0(t) ? s, where Nj,w and
Nj are w ?s document frequency and the total num-
ber of documents in jth batch respectively. s is a
scaling factor lager than 1.0, indicating state q1 has
a faster rate, and it is empirically set as 1.5. Wt is a
time interval [max(t?L/2, 0), min(t+L/2, N)], and
the length of moving window L is set as 180 days.
All the other parts remain the same as in (Kleinberg,
2003). Our detection method is denoted as TVBurst.
2.2 Burst based text representation models
We apply TVBurst to all the terms in our vocabu-
lary to identify a set of bursty features, denoted as
B. Given B, a document di(t) with timestamp t is
represented as a vector of weights in bursty feature
dimensions:
di(t) = (di,1(t), di,2(t), ..., di,|B|(t)).
We define the jth weight of di as follows
di,j =
?
tf-idfi,wBj , if t ? [t
Bj
s , t
Bj
e ] ,
0, otherwise.
When the timestamp of di is in the bursty inter-
val of Bj and contains bursty term wBj , we set up
the weight using common used tf-idf method. In
BurstVSM, each dimension is mapped to one bursty
feature, and it considers both semantic and temporal
information. One dimension is active only when the
document falls in the corresponding bursty interval.
Usually, a document vector in BurstVSM has only
a few non-zero entries, which makes computation of
document similarities more efficient in large datasets
compared with traditional VSM.
The most related work to ours is the boostVSM
introduced by (He et al, 2007b), it proposes to
weight different term dimensions with correspond-
ing bursty scores. However, it is still based on term
dimensions and fails to deal with terms with mul-
tiple bursts. Suppose that we are dealing with a
text collection related with U.S. presidential elec-
tions, Fig. 2 show sample dimensions for these three
methods. In BurstVSM, one term with multiple
bursts will be naturally mapped to different dimen-
sions. For example, two bursty features ( presiden-
tial, Nov., 2004) and ( presidential, Nov., 2008 ) cor-
respond to different dimensions in BurstVSM, while
44
Figure 2: One example for comparisons of different rep-
resentation methods. Terms in red box correspond to
multiple bursty periods.
Table 1: Summary of different representation models.
Here dimension reduction refers to the reduction of non-
zero entries in representation vector.
semantic temporal dimension trend
information information reduction modeling
VSM ? ? ? bad
boostVSM ? partially ? moderate
BurstVSM ? ? ? good
VSM and boostVSM cannot capture such temporal
differences. Some methods try to design time de-
caying functions (Yang et al, 1998), which decay
the similarity with the increasing of time gap be-
tween two documents. However, it requires efforts
for function selection and parameters tuning. We
summarize these discussions in Table 1.
3 split-cluster-merge algorithm for event
detection
In this section, we discuss how to cluster documents
as events. Since each document can be represented
as a burst-based vector, we use cosine function to
compute document similarities. Due to the large size
of our news corpus, it is infeasible to cluster all the
documents straightforward. We develop a heuristic
clustering algorithm for event detection, denoted as
split-cluster-merge, which includes three main steps,
namely split, cluster and merge. The idea is that we
first split the dataset into small parts, then cluster
the documents of each part independently and finally
merge similar clusters from two consecutive parts.
In our dataset, we find that most events last no more
than one month, so we split the dataset into parts by
months. After splitting, clustering can run in paral-
lel for different parts (we useCLUTO4 as the cluster-
ing tool), which significantly reduces total time cost.
For merge, we merge clusters in consecutive months
with an empirical threshold of 0.5. The final clusters
4www.cs.umn.edu/k?arypis/cluto
are returned as identified events.
4 Evaluation
4.1 Experiment Setup
We used a subset of 68 millon deduplicated
timestamped web pages generated from this
archive (Huang et al, 2008). Since our major focus
is to detect events from news articles, we only keep
the web pages with keyword ?news? in URL field.
The final collection contains 11, 218, 581 articles
with total 1, 730, 984, 304 tokens ranging from 2000
to 2009. We run all the experiments on a 64-bit linux
server with four Quad-Core AMD Opteron(tm) Pro-
cessors and 64GB of RAM. For split-cluster-merge
algorithm, we implement the cluster step in a multi-
thread mode, so that different parts can be processed
in parallel.
4.2 Construction of test collection
We manually construct the test collection for event
detection. To examine the effectiveness of event de-
tection methods in different grains, we consider two
type of events in terms of the number of relevant
documents, namely significant events and moder-
ate events. A significant event is required to have
at least 300 relevant docs, and a moderate event is
required to have 10 ? 100 relevant docs. 14 grad-
uate students are invited to generate the test collec-
tion, starting with a list of 100 candidate seed events
by referring to Xinhua News.5 For one target event,
the judges first construct queries with temporal con-
straints to retrieve candidate documents and then
judge wether they are relevant or not. Each doc-
ument is assigned to three students, and we adopt
the majority-win strategy for the final judgment. Fi-
nally, by removing all candidate seed events which
neither belong to significant events nor moderate
events, we derive a test collection consisting of 24
significant events and 40 moderate events.6
4.3 Evaluation metrics and baselines
Similar to the evaluation in information retrieval ,
given a target event, we evaluate the quality of the
returned ?relevant? documents by systems. We use
average precision, average recall and mean average
precision(MAP) as evaluation metrics. A difference
is that we do not have queries, and the output of a
system is a set of document clusters. So for a sys-
tem, given an event in golden standard, we first se-
lect the cluster (the system generates) which has the
5http://news.xinhuanet.com/english
6For access to the code and test collection, contact Xin Zhao
via batmanfly@gmail.com.
45
Table 2: Results of event detection. Our proposed method is better than all the other baselines at confidence level 0.9.
Signifcant Events Moderate Events
P R F MAP P R F MAP
timemines-?2(nouns) 0.52 0.2 0.29 0.11 0.22 0.27 0.24 0.09
timemines-?2(NE) 0.61 0.18 0.28 0.08 0.27 0.25 0.26 0.13
TVBurst+boostVSM 0.67 0.44 0.53 0.31 0.22 0.39 0.28 0.13
swan+BurstVSM 0.74 0.56 0.64 0.48 0.39 0.54 0.45 0.38
kleiberg+BurstVSM 0.68 0.63 0.65 0.52 0.35 0.53 0.42 0.36
TVBurst+BurstVSM 0.78 0.69 0.73 0.63 0.4 0.61 0.48 0.39
Table 3: Comparisons of average intra-class and inter-
class similarity.
Significant Events Moderate Events
Methods Intra Inter Intra Inter
TVBurst+boostVSM 0.234 0.132 0.295 0.007
TVBurst+BurstVSM 0.328 0.014 0.480 0.004
most relevant documents, then sort the documents
in the descending order of similarities with the clus-
ter centroid and finally compute P, R ,F and MAP in
this cluster. We perform Wilcoxon signed-rank test
for significance testing.
We used the event detection method in (Swan
and Allan, 2000) as baseline, denoted as timemines-
?2. As (Swan and Allan, 2000) suggested, we
tried two versions: 1) using all nouns and 2) us-
ing all named entities. Recall that BurstVSM re-
lies on bursty features as dimensions, we tested dif-
ferent burst detection algorithms in our proposed
BurstVSM model, including swan (Swan and Al-
lan, 2000), kleinberg (Kleinberg, 2003) and our pro-
posed TVBurst algorithm.
4.4 Experiment results
Preliminary results. In Table 2, we can see that 1)
BurstVSM with any of these three burst detection al-
gorithms is significantly better than timemines-?2,
suggesting our event detection method is very ef-
fective; 2) TVBurst with BurstVSM gives the best
performance, which suggests using moving average
base probability will improve the performance of
burst detection. We use TVBurst as the default burst
detection algorithm in later experiments.
Then we compare the performance of differ-
ent text representation models for event detection,
namely BurstVSM and boostVSM (He et al, 2007b;
He et al, 2007a).7 For different representation mod-
els, we use split-cluster-merge as clustering algo-
rithm. Table 2 shows that BurstVSM is much ef-
fecitve than boostVSM for event detection. In fact,
we empirically find boostVSM is appropriate for
7We use the same parameter settings in the original paper.
Table 4: Comparisons of observed runtime and storage.
boostVSM BurstVSM
Aver. # of non-zero entries per doc 149 14
File size for storing vectors (gigabytes) 3.74 0.571
Total # of merge 10,265,335 9,801,962
Aver. cluster cost per month (sec.) 355 55
Total merge cost (sec.) 2,441 875
Total time cost (sec.) 192,051 4,851
clustering documents in a coarse grain (e.g., in topic
level) but not for event detection.
Intra-class and inter-class similarities. In our
methods, event detection is treated as document
clustering. It is very important to study how similari-
ties affect the performance of clustering. To see why
our proposed representation methods are better than
boostVSM, we present the average intra-class simi-
larity and inter-class similarity for different events in
Table 3.8 We can see BurstVSM results in a larger
intra-class similarity and a smaller inter-class simi-
larity than boostVSM.
Analysis of the space/time complexity. We fur-
ther analyze the space/time complexity of different
representation models. In Table 4. We can see that
BurstVSM has much smaller space/time cost com-
pared with boostVSM, and meanwhile it has a better
performance for event detection (See Table 2). In
burst-based representation, one document has fewer
non-zero entries.
Acknowledgement. The core idea of this work
is initialized and developped by Kai Fan. This
work is partially supported by HGJ 2010 Grant
2011ZX01042-001-001, NSFC Grant 61073082 and
60933004. Xin Zhao is supported by Google PhD
Fellowship (China). We thank the insightful com-
ments from Junjie Yao, Jing Liu and the anony-
mous reviewers. We have developped an online Chi-
nese large-scale event search engine based on this
work, visit http://sewm.pku.edu.cn/eventsearch for
more details.
8For each event in our golden standard, we have two clus-
ters: relevant documents and non-relevant documents(within
the event period).
46
References
James Allan, Victor Lavrenko, and Hubert Jin. 2000.
First story detection in TDT is hard. In Proceedings
of the ninth international conference on Information
and knowledge management.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007. Time-dependent event hierarchy
construction. In SIGKDD.
Q. He, K. Chang, and E. P. Lim. 2007a. Using burstiness
to improve clustering of topics in news streams. In
ICDM.
Qi He, Kuiyu Chang, Ee-Peng Lim, and Jun Zhang.
2007b. Bursty feature representation for clustering
text streams. In SDM.
L. Huang, L. Wang, and X. Li. 2008. Achieving both
high precision and high recall in near-duplicate detec-
tion. In CIKM.
J. Kleinberg. 2003. Bursty and hierarchical structure in
streams. Data Mining and Knowledge Discovery.
Russell Swan and James Allan. 2000. Automatic gener-
ation of overview timelines. In SIGIR.
Michail Vlachos, Christopher Meek, Zografoula Vagena,
and Dimitrios Gunopulos. 2004. Identifying similari-
ties, periodicities and bursts for online search queries.
In SIGMOD.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998.
A study of retrospective and on-line event detection.
In SIGIR.
47
