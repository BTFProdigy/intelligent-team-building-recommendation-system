Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 735?744,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Syntactic Models for Structural Word Insertion and Deletion  
Arul Menezes and Chris Quirk 
Microsoft Research 
One Microsoft Way, Redmond, WA 98052, USA 
{arulm, chrisq}@microsoft.com 
 
 
 
 
Abstract 
An important problem in translation neglected 
by most recent statistical machine translation 
systems is insertion and deletion of words, 
such as function words, motivated by linguistic 
structure rather than adjacent lexical context. 
Phrasal and hierarchical systems can only 
insert or delete words in the context of a larger 
phrase or rule. While this may suffice when 
translating in-domain, it performs poorly when 
trying to translate broad domains such as web 
text.  Various syntactic approaches have been 
proposed that begin to address this problem by 
learning lexicalized and unlexicalized rules. 
Among these, the treelet approach uses 
unlexicalized order templates to model 
ordering separately from lexical choice. We 
introduce an extension to the latter that allows 
for structural word insertion and deletion, 
without requiring a lexical anchor, and show 
that it produces gains of more than 1.0% BLEU 
over both phrasal and baseline treelet systems 
on broad domain text. 
1 Introduction 
Among the phenomena that are modeled poorly by 
modern SMT systems is the insertion and deletion 
of words, such as function words, that are 
motivated by the divergent linguistic structure 
between source and target language. To take the 
simplest of examples, the English noun compound 
?file name? would typically be translated into 
Spanish as ?nombre de archivo?, which requires 
the insertion of the preposition ?de?. Conversely, 
when translating from Spanish to English, the ?de? 
must be deleted. At first glance, the problem may 
seem trivial, yet the presence and position of these 
function words can have crucial impact on the 
adequacy and fluency of translation. 
In particular, function words are often used to 
denote key semantic information. They may be 
used to denote case information, in languages such 
as Japanese. Failing to insert the proper case 
marker may render a sentence unreadable or 
significantly change its meaning. Learning these 
operations can be tricky for MT models best suited 
to contiguous word sequences. From a fluency 
standpoint, proper insertion of determiners and 
prepositions can often make the difference between 
laughably awkward output and natural sounding 
translations; consider the output ?it?s a cake piece? 
as opposed to ?it?s a piece of cake?. 
Furthermore, since missing or spurious function 
words can confuse the target language model, 
handling these words properly can have an impact 
beyond the words themselves. 
This paper focuses on methods of inserting and 
deleting words based on syntactic cues, to be used 
in the context of a syntax-informed translation 
system. While the models we build are relatively 
simple and the underlying templates are easy to 
extract, they add significant generalization ability 
to the base translation system, and result in 
significant gains. 
2 Background 
As a motivating example, let us return to the 
English/Spanish pair ?file name? and ?nombre de 
archivo?. In principle, we would want a machine 
translation system to be capable of learning the 
following general transformation: 
 ? NOUN1 NOUN2?  ? NOUN2 de NOUN1? (1) 
Yet even this simple example is beyond the 
capabilities of many common approaches. 
The heavily lexicalized approaches of phrasal 
systems (Koehn et al, 2003), are inherently 
incapable of this generalization. As a proxy, they 
735
acquire phrase pairs such as ?nombre de archivo? 
 ?file name?, ?nombre de?  ?name? and ?de 
archivo?  ?file?. Note that the inserted word is 
attached to adjacent context word(s). When the test 
set vocabulary has significant overlap with the 
training vocabulary, the correct translation can 
often be assembled based on the head or the 
modifying noun. However, as we show in this 
paper, this is woefully inadequate when translating 
truly out-of-domain input. 
In principle, phrase-based translation systems 
may employ insertion phrase pairs such as 
 ?[NULL]?  ?de? (2) 
but the ungrounded nature of this transformation 
makes its use during decoding difficult. Since there 
are no constraints on where such a rule may apply 
and the rule does not consume any input words, the 
decoder must attempt these rules at every point in 
the search. 
The reverse operation 
 ?de?  ?[NULL]? (3) 
is more feasible to implement, though again, there 
is great ambiguity ? a source word may be deleted 
at any point during the search, with identical target 
results. Few systems allow this operation in 
practice. Estimating the likelihood of this operation 
and correctly identifying the contexts in which it 
should occur remain challenging problems. 
Hierarchical systems, such as (Chiang, 2005) in 
principle have the capacity to learn insertions and 
deletions grounded by minimal lexical cues. 
However, the extracted rules use a single non-
terminal. Hence, to avoid explosive ambiguity, 
they are constrained to contain at least one aligned 
pair of words. This restriction successfully limits 
computational complexity at a cost of 
generalization power. 
Syntax-based approaches provide fertile context 
for grounding insertions and deletions. Often we 
may draw a strong correspondence between 
function words in one language and syntactic 
constructions in another. For instance, the syntactic 
approach of Marcu et al (2006) can learn 
unlexicalized rules that insert function words in 
isolation, such as: 
 NP(NN:x0 NN:x1)  x1 de  x0 (4) 
However, as discussed in (Wang, Knight & 
Marcu, 2007), joint modeling of structure and 
lexical choice can exacerbate data sparsity, a 
problem that they attempt to address by tree 
binarization. Nevertheless, as we show below, 
unlexicalized structural transformation rules such 
as (1) and (4) that allow for insertion of isolated 
function words, are essential for good quality 
translation of truly out-of-domain test data.  
In the treelet translation approach (Menezes & 
Quirk, 2007), lexical choice and syntactic re-
ordering are modeled separately using lexicalized 
treelets and unlexicalized order templates. We 
discuss this approach in more detail in Section 4. 
In Section 5, we describe how we extend this 
approach to allow for structural insertion and 
deletion, without the need for content word 
anchors. 
3 Related Work 
There is surprisingly little prior work in this area. 
We previously (Menezes & Quirk, 2005) explored 
the use of deletion operations such as (3) above, 
but these were not grounded in any syntactic 
context, and the estimation was somewhat 
heuristic1. 
The tuple translation model of Crego et al 
(2005), a joint model over source and target 
translations, also provides a means of deleting 
words. In training, sentence pairs such as ?nombre 
de archivo? / ?file name? are first word aligned, 
then minimal bilingual tuples are identified, such 
as ?nombre / name?, ?de / NULL? and ?archivo / 
file?. The tuples may involve deletion of words by 
allowing an empty target side, but do not allow 
insertion tuples with an empty source side. These 
inserted words are bound to an adjacent neighbor. 
An n-gram model is trained over the tuple 
sequences. As a result, deletion probabilities have 
the desirable property of being conditioned on 
adjacent context, yet this context is heavily 
lexicalized, therefore unlikely to generalize well. 
More recently, Li et. al. (2008) describe three 
models for handling ?single word deletion? (they 
discuss, but do not address, word insertion). The 
first model uses a fixed probability of deletion 
                                                          
1
 We assigned channel probabilities based on the sum of the 
Model1 probability of the source word being aligned to NULL 
or one of a list of "garbage collector" words. This exploits the 
property of Model1 that certain high-frequency words tend to 
act as "garbage collectors" for words that should remain 
unaligned. 
736
P(NULL), independent of the source word, 
estimated by counting null alignments in the 
training corpus. The second model estimates a 
deletion probability per-word, P(NULL|w), also 
directly from the aligned corpus, and the third 
model trains an SVM to predict the probability of 
deletion given source language context 
(neighboring and dependency tree-adjacent words 
and parts-of-speech). All three models give large 
gains of 1.5% BLEU or more on Chinese-English 
translation. It is interesting to note that the more 
sophisticated models provide a relatively small 
improvement over the simplest model in-domain, 
and no benefit out-of-domain.  
4  Dependency treelet translation 
As a baseline, we use the treelet translation 
approach (which we previously described in 
Menezes & Quirk, 2007), a linguistically syntax-
based system leveraging a source parser. It first 
unifies lexicalized treelets and unlexicalized 
templates to construct a sentence-specific set of 
synchronous rewrite rules. It then finds the highest 
scoring derivation according to a linear 
combination of models. We briefly review this 
system before describing our current extension. 
4.1 The treelet translation model 
Sentence-specific rewrite rules are constructed by 
unifying information from three sources: a 
dependency parse of the input sentence, a set of 
treelet translation pairs, and a set of unlexicalized 
order templates. Dependency parses are 
represented as trees: each node has a lexical label 
and a part of speech, as well as ordered lists of pre- 
and post-modifiers.  
A treelet represents a connected subgraph of a 
dependency tree; treelet translation pairs consist 
of source and target treelets and a node alignment. 
This alignment is represented by indices: each 
node is annotated with an integer alignment index. 
A source node and a target node are aligned iff they 
have the same alignment index. For instance: 
  ((old1/JJ) man2/NN)  (hombre2 (viejo1)) (5) 
 (man1/NN)  (hombre1) (6) 
Order templates are unlexicalized transduction 
rules that describe the reorderings, insertions and 
deletions associated with a single group of nodes 
that are aligned together. For instance: 
 ((x0:/DT) (x1:/JJ) 1/NN)  ((x0) 1 (x1)) (7) 
 ((x0:/DT) (x1:/JJ) 1/NN)  ((x0) (x1) 1) (8) 
 ((x0:/DT) 1/NN)  ((x0) 1) (9) 
 ((x0:/RB) 1/JJ)  ((x0) 1) (10) 
Each node is either a placeholder or a variable. 
Placeholders, such as 1/NN on the source side or 
1 on the target side, have alignment indices and 
constraints on their parts-of-speech on the source 
side, but are unconstrained lexically (represented 
by the ). These unify at translation time with 
lexicalized treelet nodes with matching parts-of-
speech and alignment.  
Variables, such as x0:/DT on the source side 
and x0: on the target side, also have parts-of-
speech constraints on the source side. Variables are 
used to indicate where rewrite rules are recursively 
applied to translate subtrees. Thus each variable 
label such as x0, must occur exactly once on each 
side. 
In effect, a template specifies how all the 
children of a given source node are reordered 
during translation. If translation were a word-
replacement task, then templates would be just 
simple, single-level tree transducers. However, in 
the presence of one-to-many and many-to-one 
translations and unaligned words,  templates may 
span multiple levels in the tree.  
As an example, order template (7) indicates that 
an NN with two pre-modifying subtrees headed by 
DT and JJ may be translated by using a single 
word translation of the NN, placing the translation 
of the DT subtree as a pre-modifier, and placing 
the translation of the JJ subtree as a post-modifier. 
As discussed below, this template can unify with 
the treelet (6) to produce the following rewrite 
rule: 
 ((x0:DT) (x1:JJ) man/NN)  
 ((x0) hombre (x1)) (11) 
Matching: A treelet translation pair matches an 
input parse iff there is a unique correspondence 
between the source side of the treelet pair and a 
connected subgraph of the input parse.  
An order template matches an input parse iff 
there is a unique correspondence between the 
source side of the template and the input parse, 
with the additional restriction that all children of 
input nodes that correspond to placeholder 
737
template nodes must be included in the 
correspondence. For instance, order template (7) 
matches the parse 
 ((the/DT) (young/JJ) colt/NN) (12) 
but not the parse 
  ((the/DT) (old/JJ) (grey/JJ) mare/NN) (13) 
Finally, an order template matches a treelet 
translation pair at a given node iff, on both source 
and target sides, there is a correspondence between 
the treelet translation nodes and template nodes 
that is consistent with their tree structure and 
alignments. Furthermore, all placeholder nodes in 
the template must correspond to some treelet node. 
Constructing a sentence-specific rewrite rule is 
then a process of unifying each treelet with a 
matching combination of order templates with 
respect to an input parse.  Each treelet node must 
be unified with one and only one order template 
placeholder node. Unifying under these constraints 
produces a rewrite rule that has a one-to-one 
correspondence between variables in source and 
target. For instance, given the input parse: 
 ((the/DT) ((very/RB) old/JJ) man/NN)  (14) 
we can create a rewrite rule from the treelet 
translation pair (5) by unifying it with the order 
template (7), which matches at the node man and 
its descendents, and template (10), which matches 
at the node old, to produce the following sentence-
specific rewrite rule:  
 ((the/DT) ((x1: /RB) old/JJ) man/NN)  
 ((el) hombre ((x1) viejo)) (15) 
Note that by using different combinations of 
order templates, a single treelet can produce 
multiple rewrite rules. Also, note how treelet 
translation pairs capture contextual lexical 
translations but are underspecified with respect to 
ordering, while order templates separately capture 
arbitrary reordering phenomena yet are 
underspecified lexically. Keeping lexical and 
ordering information orthogonal until runtime 
allows for the production of novel transduction 
rules never actually seen in the training corpus, 
leading to improved generalization power. 
 Decoding: Given a set of sentence-specific 
rewrite rules, a standard beam search algorithm is 
used to find the highest scoring derivation. 
Derivations are scored according to a linear 
combination of models. 
4.2 Training 
The process of extracting treelet translation pairs 
and order templates begins with parallel sentences. 
First, the sentence pairs are word segmented on 
both sides, and the source language sentences are 
parsed. Next, the sentence pairs are word aligned 
and the alignments are used to project a target 
language dependency tree. 
Treelet extraction: From each sentence pair , 
with the alignment relation ~, a treelet translation 
pair consisting of the source treelet  	  and the 
target treelet 
 	  is extracted iff: 
(1) There exist    and   
 such that  ~ . 
(2) For all   , and    such that ~,    iff 
  
. 
Order template extraction is attempted starting 
from each node Sroot in the source whose parent is 
not also aligned to the same target word(s). We 
identify Troot, the highest target node aligned to 
Sroot. We initialize the sets S0 as {Sroot} and T0 as 
{Troot}.  We expand S0 to include all nodes 
adjacent to some element of S0 that are (a) 
unaligned, or (b) aligned to some node in T0. The 
converse is applied to T0. This expansion is 
repeated until we reach a fixed point. Together, S0 
and T0 make up the placeholder nodes in the 
extracted order template. We then create one 
variable in the order template for each direct child 
of nodes in S0 and T0 that is not already included in 
the order template. Iff there is a one-to-one word 
alignment correspondence between source and 
target variables, then a template is extracted. This 
restriction leads to clean templates, at the cost of 
excluding all templates involving extraposition. 
5 Insertion/deletion order templates 
In this paper, we extend our previous work to 
allow for insertion and deletion of words, by 
allowing unaligned lexical items as part of the 
otherwise unlexicalized order templates. 
Grounding insertions and deletions in templates 
rather than treelets has two major benefits. First, 
insertion and deletion can be performed even in the 
absence of specific lexical context, leading to 
greater generalization power. Secondly, this 
increased power is tempered by linguistically 
738
informative unlexicalized context. Rather than 
proposing insertions and deletions in any arbitrary 
setting, we are guided by specific syntactic 
phenomena. For instance, when translating English 
noun compounds into Spanish, we often must 
include a preposition; this generalization is 
naturally captured using just parts-of-speech. 
The inclusion of lexical items in order templates 
affects the translation system in only a few places: 
dependency tree projection, order template 
extraction, and rewrite rule construction at runtime. 
Dependency tree projection: During this step of 
the baseline treelet system, unaligned words are by 
default attached low, to the lowest aligned 
neighbor. Although this worked well in 
conjunction with the discriminative order model, it 
prevents unaligned nodes from conditioning on 
relevant context in order templates. Therefore, we 
change the default attachment of unaligned nodes 
to be to the highest aligned neighbor; informal 
experiments showed that this did not noticeably 
impact translation quality in the baseline system. 
For example, consider the source parse and aligned 
target sentence: 
 ((calibrated1/JJ) (camera2/NN) file3/NN) 
 archivo3 de4 c?mara2 calibrado1 (16) 
Using the baseline projection algorithm would 
produce this target dependency tree: 
 (archivo3 ((de4) c?mara2) (calibrado1)) (17) 
Instead, we attach unaligned words high: 
 (archivo3 (de4) (c?mara2) (calibrado1)) (18) 
Order template extraction: In addition to the 
purely unlexicalized templates extracted from each 
training sentence, we also allow templates that 
include lexical items for each unaligned token. For 
each point in the original extraction procedure, 
where S0 or T0 contain unaligned nodes, we now 
extract two templates: The original unlexicalized 
template, and a new template in which only the 
unaligned node(s) contain the specific lexical 
item(s). From the example sentence pair (16), 
using the projected parse (18) we would extract the 
following two templates: 
 ((x0:/JJ) (x1:/NN) 1/NN)   
 (1 (2) (x1) (x0)) (19) 
 ((x0:/JJ) (x1:/NN) 1/NN)   
 (1 (de2) (x1) (x0)) (20) 
Template matching and unification: We extend 
the template matching against the input parse to 
require that any lexicalized source template nodes 
match the input exactly. When matching templates 
to treelet translation pairs, any unaligned treelet 
nodes must be consistent with the corresponding 
template node (i.e. the template node must be 
unlexicalized, or the lexical items must match). On 
the other hand, lexicalized template nodes do not 
need to match any treelet nodes -- insertions or 
deletions may now come from the template alone. 
Consider the following example input parse: 
 ((digital/JJ) (camera/NN)  
     (file/NN) extension/NN) (21) 
The following treelet translation pair provides a 
contextual translation for some of the children, 
including the insertion of one necessary 
preposition: 
 ((file1/NN) extension2/NN)      
     (extension2 (de3) (archivo1)) (22) 
The following order template can provide relative 
ordering information between nodes as well as 
insert the remaining prepositions: 
 ((x0:/JJ) (x1:/NN) (x2:/NN) 1/NN)     
      (1 (de2) (x2) (de3) (x0) (x1)) (23) 
The unification of this template and treelet is 
somewhat complex: the first inserted de is agreed 
upon by both template and treelet, whereas the 
second is inserted by the template alone. This 
results in the following novel rewrite rule: 
 ((x0:/JJ) (x1: /NN) (file) extension)     
      (extension (de) (archivo) (de) (x0) (x1))   (24) 
These relatively minimal changes produce a 
powerful contextualized model of insertion and 
deletion.  
Parameter estimation: The underlying treelet 
system includes a template probability estimated 
by relative frequency. We estimate our lexicalized 
templates in the same way. However early 
experiments showed that this feature alone was not 
enough to allow even common insertions, since the 
probability of even the most common insertion 
templates is much lower than that of unlexicalized 
templates. To improve the modeling capability, we 
included two additional feature functions: a count 
of structurally inserted words, and a count of 
structurally deleted words.  
739
6 Example 
Consider the following English test sentence and 
corresponding Spanish human translation: 
September is National Cholesterol Education 
Month 
Septiembre es el Mes Nacional para la 
Educaci?n sobre el Colesterol 
The baseline treelet system without structural 
insertions translates this sentence as: 
Septiembre es Nacional Colesterol Educaci?n 
Mes 
Not only is the translation missing the appropriate 
articles and prepositions, but also in their absence, 
it fails to reorder the content words correctly. 
Without the missing prepositions, the language 
model does not show a strong preference among 
various orderings of "nacional" "colesterol" 
"educaci?n" and "mes". 
Using structural insertion templates, the highest 
scoring translation of the sentence is now: 
Septiembre es el Mes Nacional de Educaci?n de 
colesterol 
Although the choice of prepositions is not the same 
as the reference, the fluency is much improved and 
the translation is quite understandable. Figure 6.1, 
lists the structural insertion templates that are used 
to produce this translation, and shows how they are 
unified with treelet translation pairs to produce 
sentence-specific rewrite rules, which are in turn 
composed during decoding to produce this 
translation. 
7 Experiments 
We evaluated the translation quality of the system 
using the BLEU metric (Papineni et al, 2002). We 
compared three systems: (a) a standard phrasal 
system using a decoder based on Pharaoh, (Koehn 
et al, 2003), (b) A baseline treelet system using 
unlexicalized order templates and (c) The present 
work, which adds structural insertion and deletion 
templates.  
7.1 Data 
We report results for two language pairs, English-
Spanish and English- Japanese. For English-
Spanish we use two training sets: (a) the Europarl 
corpus provided by the NAACL 2006 Statistical 
Machine Translation workshop (b) a ?general-
domain? data set that includes a broad spectrum of 
data such as governmental data, general web data 
and technical corpora.  
September
NN
is
VB
National
JJ
Cholesterol
NN
Education
NN
Month
NN
Input dependency tree
x2x0 de2 de3 x1
x0:*
JJ
x1:*
NN
x2:*
NN
*1
NN
x2mes x0 de de x1
x0:*
JJ
x1:*
NN
x2:*
NN
month
NN
septiembre es x1
x1:*
NN
is
VB
september
NN
x0 *1 x1
x0:*
NN
*1
VB
x1:*
NN
septiembre es
september
NN
is
VB
mes
month
NN
Treelets Templates Rewrite Rules
el2 el
 
Figure 6.1: Example sentence, matching treelets, structural insertion templates and unified rewrite rules 
740
For English-Japanese we use only the ?general-
domain? data set.  
 Sentence 
pairs 
Tokens Phr 
size 
MERT 
data 
Europarl E-S 730K 15M 7 Europarl 
General E-S 3.7M 41M 4 Web 
General E-J 2.6M 16M 4 Web 
Table 7.1 Training data 
For English-Spanish we report results using the 
four test sets listed in Table 7.2. For English-
Japanese we use only the web test set. The first 
two tests are from the 2006 SMT workshop and the 
newswire test is from the 2008 workshop. The web 
test sets were selected from a random sampling of 
English web sites, with target language translations 
provided by professional translation vendors. All 
test sets have one reference translation.  
 Domain Sentence pairs 
eu-test Europarl  2000 
nc-test News commentary 1064 
News News wire 2051 
Web General web text 5000 
Table 7.2 Test data 
7.2 Models 
The baseline treelet translation system uses all the 
models described in Menezes & Quirk (2007), 
namely:  
? Treelet log probabilities, maximum likelihood 
estimates with absolute discounting.  
? Forward and backward lexical weighting, 
using Model-1 translation log probabilities. 
? Trigram language model using modified 
Kneser-Ney smoothing.  
? Word and phrase count feature functions. 
? Order template log probabilities, maximum 
likelihood estimates, absolute discounting. 
? Count of artificial source order templates.2   
? Discriminative tree-based order model. 
The present work does not use the discriminative 
tree-based order model3 but adds: 
                                                          
2
 When no template is compatible with a treelet, the decoder 
creates an artificial template that preserves source order. This 
count feature allows MERT to deprecate the use of such 
templates. This is analogous to the glue rules of Chiang 
(2005). 
? Count of structural insertions: This counts only 
words inserted via templates, not lexical 
insertions via treelets. 
? Count of structural deletions: This counts only 
words deleted via templates, not lexical 
deletions via treelets. 
The comparison phrasal system was constructed 
using the same alignments and the heuristic 
combination described in (Koehn et al, 2003). 
This system used a standard set of models: 
? Direct and inverse log probabilities, both 
relative frequency and lexical weighting. 
? Word count, phrase count. 
? Trigram language model log probability. 
? Length based distortion model. 
? Lexicalized reordering model. 
7.3 Training 
We parsed the source (English) side of the corpus 
using NLPWIN, a broad-coverage rule-based 
parser able to produce syntactic analyses at varying 
levels of depth (Heidorn, 2000). For the purposes 
of these experiments, we used a dependency tree 
output with part-of-speech tags and unstemmed, 
case-normalized surface words. For word 
alignment we used a training regimen of five 
iterations of Model 1, followed by five iterations of 
a word-dependent HMM model (He, 2007) in both 
directions. The forward and backward alignments 
were combined using a dependency tree-based 
heuristic combination. The word alignments and 
English dependency tree were used to project a 
target tree. From the aligned tree pairs we 
extracted treelet and order template tables. 
For the Europarl systems, we use a 
phrase/treelet size of 7 and train model weights 
using 2000 sentences of Europarl data. For the 
?general-domain? systems, we use a phrase/treelet 
size of 4, and train model weights using 2000 
sentences of web data. 
For any given corpus, all systems used the same 
treelet or phrase size (see Table 7.1) and the same 
trigram language model. Model weights were 
trained separately for each system, data set and 
experimental condition, using minimum error rate 
training to maximize BLEU (Och, 2003). 
                                                                                           
3
 In our experiments, we find that the impact of this model is 
small in the presence of order templates; also, it degrades the 
overall speed of the decoder. 
741
8 Results and Discussion 
Tables 8.1 and 8.4 compare baseline phrasal and 
treelet systems with systems that use various types 
of insertion and deletion templates. 
English-Japanese: As one might expect, the use 
of structural insertion and deletion has the greatest 
impact when translating between languages such as 
English and Japanese that show significant 
structural divergence. In this language pair, both 
insertions and deletions have an impact, for a total 
gain of 1.1% BLEU over the baseline treelet 
system, and 3.6% over the phrasal system. To aid 
our understanding of the system, we tabulated the 
most commonly inserted and deleted words when 
translating from English into Japanese in Tables 
8.2 and 8.3 respectively. Satisfyingly, most of the 
insertions and deletions correspond to well-known 
structural differences between the languages. For 
instance, in English the thematic role of a noun 
phrase, such as subject or object, is typically 
indicated by word order, whereas Japanese uses 
case markers to express this information. Hence, 
case markers such as ??? and ??? need to be 
inserted. Also, when noun compounds are 
translated, an intervening postposition such as ??? 
is usually needed. Among the most common 
deletions are ?the? and ?a?. This is because 
Japanese does not have a notion of definiteness. 
Similarly, pronouns are often dropped in Japanese. 
English-Spanish: We note, in Table 8.4 that 
even between such closely related languages, 
structural insertions give us noticeable 
improvements over the baseline treelet system. On 
the smaller Europarl training corpus the 
improvements range from 0.5% to 1.1% BLEU. 
On the larger training corpus we find that for the 
more in-domain governmental4 and news test sets, 
the effect is smaller or even slightly negative, but 
                                                          
4
 The "general domain" training corpus is a superset of the 
Europarl training set, therefore, the Europarl tests sets are "in-
domain" in both cases. 
on the very broad web test set we still see an 
improvement of about 0.7% BLEU. 
As one might expect, as the training data size 
increases, the generalization power of structural 
insertion and deletions becomes less important 
when translating in-domain text, as more insertions 
and deletions can be handled lexically. 
Nevertheless, the web test results indicate that if 
one hopes to handle truly general input the need 
for structural generalizations remains.  
Unlike in English-Japanese, when translating 
from English to Spanish, structural deletions are 
less helpful. Used in isolation or in combination 
with insertion templates they have a slightly 
negative and/or insignificant impact in all cases. 
We hypothesize that when translating from English 
into Spanish, more words need to be inserted than 
deleted. Conversely, when translating in the 
reverse direction, deletion templates may play a 
bigger role. We were unable to test the reverse 
direction because our syntax-based systems depend 
on a source language parser. In future work we 
hope to address this.  
 % BLEU 
Phrasal 13.41 
Baseline treelet 15.89 
+Deletion only 16.00 
+Insertion only 16.16 
+Deletion and Insertion 17.01 
Table 8.1: English-Japanese system comparisons 
Word Count %age Type 
? 
2844 42% Postposition 
? 
1637 24% Postposition/case marker 
? 
630 9.3% Postposition/case marker 
? 
517 7.6% Punctuation 
? 
476 7.0% Postposition 
?? 
266 3.9% Light verb 
? 
101 1.5% Postposition 
? 
68 1.0% Postposition 
?? 
27 0.40% Light verb 
? 
26 0.38% Punctuation 
? 
19 0.28% Question marker 
Table 8.2: E-J: Most commonly inserted words 
Word Count %age Type 
the 875 59% Definite article 
- 159 11% Punctuation 
a 113 7.7% Indefinite article 
you 53 3.6% Pronoun 
it 53 3.6% Pronoun 
that 26 1.8% Conjunction, Pronoun 
" 23 1.6% Punctuation 
in 16 1.1% Preposition 
. 10 0.68% Punctuation 
's 10 0.68% Possessive 
I 9 0.61% Pronoun 
Table 8.3: E-J: Most commonly deleted words 
 
742
In table 8.5 and 8.6, we list the words most 
commonly inserted and deleted when translating 
the web test using the general English-Spanish 
system. As in English-Japanese, we find that the 
insertions are what one would expect on linguistic 
grounds. However, deletions are used much less 
frequently than insertions and also much less 
frequently than they are in English-Japanese. Only 
53 words are structurally deleted in the 5000 
sentence test set, as opposed to 4728 structural 
insertions. Furthermore, the most common deletion 
is of quotation marks, which is incorrect in most 
cases, even though such deletion is evidenced in 
the training corpus5.  
On the other hand, the next most common 
deletions ?I? and ?it? are linguistically well 
grounded, since Spanish often drops pronouns. 
9 Conclusions and Future Work 
We have presented an extension of the treelet 
translation method to include order templates with 
structural insertion and deletion, which improves 
translation quality under a variety of scenarios, 
particularly between structurally divergent 
languages. Even between closely related 
languages, these operations significantly improve 
the generalizability of the system, providing 
benefit when handling out-of-domain test data. 
Our experiments shed light on a little-studied 
area of MT, but one that is nonetheless crucial for 
high quality broad domain translation. Our results 
affirm the importance of structural insertions, in 
particular, when translating from English into other 
                                                          
5
  In many parallel corpora, quotes are not consistently 
preserved between source and target languages.  
languages, and the importance of both insertions 
and deletions when translating between divergent 
languages. In future, we hope to study translations 
from other languages into English to study the role 
of deletions in such cases.  
References 
Chiang, David. A hierarchical phrase-based model for 
statistical machine translation. ACL 2005.  
Crego, Josep, Jos? Mari?o and Adri? de Gispert. 
Reordered search and tuple unfolding for Ngram-
based SMT. MT Summit 2005. 
He, Xiaodong. Using Word Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. Workshop on 
Statistical Machine Translation, 2007 
de 3509 74% Preposition 
la 555 12% Determiner 
el 250 5.3% Determiner 
se 77 1.6% Reflexive pronoun 
que 63 1.3% Relative pronoun 
los 63 1.3% Determiner 
del 57 1.2% Preposition+Determiner 
, 42 0.89% Punctuation 
a 30 0.63% Preposition 
en 21 0.44% Preposition 
lo 9 0.19% Pronoun 
las 6 0.13% Determiner 
Table 8.5: E-S: Most commonly inserted words 
" 38 72% Punctuation 
I 5 9.4% Pronoun 
it 2 3.8% Pronoun 
, 2 3.8% Punctuation 
- 2 3.8% Punctuation 
Table 8.6: E-S: Most commonly deleted words 
  EU-devtest EU-test NC-test Newswire Web test 
EUROPARL E-S       
 Phrasal 27.9 28.5 24.7 17.7 17.0 
 Baseline treelet 27.65 28.38 27.00 18.46 18.71 
 +Deletion only 27.66 28.39 26.97 18.46 18.64 
 +Insertion only 28.23 28.93 28.10 19.08 19.43 
 +Deletion and Insertion 28.27 29.08 27.82 18.98 19.19 
GENERAL E-S       
 Phrasal 28.79 29.19 29.45 21.12 27.91 
 Baseline treelet 28.67 29.33 32.49 21.90 27.42 
 +Deletion only 28.67 29.27 32.25 21.69 27.47 
 +Insertion only 28.90 29.70 32.53 21.84 28.30 
 +Deletion and Insertion 28.34 29.41 32.66 21.70 27.95 
Table 8.4: English-Spanish system comparisons, %BLEU 
 
743
Heidorn, George. ?Intelligent writing assistance?. In 
Dale et al Handbook of Natural Language 
Processing, Marcel Dekker. 2000 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase based translation. NAACL 2003. 
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Hailei 
Zhang. An Empirical Study in SourceWord Deletion 
for Phrase-based Statistical Machine Translation. 
Workshop on Statistical Machine Translation, 2008 
Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and 
Kevin Knight. SPMT: Statistical Machine 
Translation with Syntactified Target Language 
Phrases. EMNLP-2006. 
Menezes, Arul, and Chris Quirk. Microsoft Research 
Treelet translation system: IWSLT evaluation. 
International Workshop on Spoken Language 
Translation, 2005 
Menezes, Arul, and Chris Quirk. Using Dependency 
Order Templates to Improve Generality in 
Translation. Workshop on Statistical Machine 
Translation, 2007 
Och, Franz Josef. Minimum error rate training in 
statistical machine translation. ACL 2003. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evaluation 
of machine translation. ACL 2002. 
Wang, Wei, Kevin Knight and Daniel Marcu. 
Binarizing Syntax Trees to Improve Syntax-Based 
Machine Translation Accuracy. EMNLP-CoNLL, 
2007 
744
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 8?9,
Vancouver, October 2005.
MindNet: an automatically-created lexical resource 
 
Lucy Vanderwende, Gary Kacmarcik, Hisami Suzuki, Arul Menezes 
Microsoft Research 
Redmond, WA 98052, USA 
{lucyv, garykac, hisamis, arulm}@microsoft.com 
 
Abstract 
We will demonstrate MindNet, a lexical resource 
built automatically by processing text.  We will 
present two forms of MindNet: as a static lexical 
resource, and, as a toolkit which allows MindNets 
to be built from arbitrary text.  We will also intro-
duce a web-based interface to MindNet lexicons 
(MNEX) that is intended to make the data con-
tained within MindNets more accessible for explo-
ration.  Both English and Japanese MindNets will 
be shown and will be made available, through 
MNEX, for research purposes. 
1 MindNet 
A MindNet is a collection of semantic relations 
that is automatically extracted from text data using 
a broad coverage parser. Previous publications on 
MindNet (Suzuki et al, 2005, Richardson et al, 
1998, Vanderwende 1995) have focused on the 
effort required to build a MindNet from the data 
contained in Japanese and English lexicons. 
Semantic Relations 
The semantic relations that are stored in MindNet 
are directed, labeled relationships between two 
words; see Table 1:  
Attributive Manner Source 
Cause Means Synonym 
Goal Part Time 
Hypernym Possessor TypicalObject 
Location Result TypicalSubject 
Table 1: A sampling of the semantic relations stored in 
MindNet 
 
These semantic relations are obtained from the 
Logical Form analysis of our broad coverage 
parser NLPwin (Heidorn, 2000).  The Logical 
Form is a labeled dependency analysis with func-
tion words removed.  We have not completed an 
evaluation of the quality of the extracted semantic 
relations.  Anecdotally, however, the quality varies 
according to the relation type, with Hypernym and 
grammatical relations TypicalSubject and Typi-
calObj being reliable, while relations such as Part 
and Purpose are less reliable. By making MindNet 
available, we solicit feedback on the utility of these 
labeled relationships, especially in contrast to sim-
ple co-occurrence statistics and to the heavily used 
hypernymy and synonymy links. Furthermore, we 
solicit feedback on the level of accuracy which is 
tolerable for specific applications. 
Semantic Relation Structures 
We refer to the hierarchical collection of semantic 
relations (semrels) that are automatically extracted 
from a source sentence as a semrel structure. Each 
semrel structure contains all of the semrels ex-
tracted from a single source sentence.  A semrel 
structure can be viewed from the perspective of 
each unique word that occurs in the structure; we 
call these inverted structures.  They contain the 
same information as the original, but with a differ-
ent word placed at the root of the structure. An ex-
ample semrel structure for the definition of 
swallow is given in Figure 1a, and its inversion, 
from the perspective of wing is given in Figure 1b: 
 
swallow           wing 
 Hyp bird           PartOf bird 
       Part wing             Attrib small 
       Attrib small          HypOf swallow 
 
Figure 1a and b: Figure 1a is the semrel structure for the 
definition of swallow1, Figure 1b the inversion on wing. 
2 MNEX 
MNEX (MindNet Explorer) is the web-based inter-
face to MindNet that is designed to facilitate 
browsing MindNet structure and relations. MNEX 
displays paths based on the word or words that the 
                                                          
1
 Swallow: a small bird with wings (LDOCE).  Definition 
abbreviated for purposes of exposition.   
8
user enters. A path is a set of links that connect one 
word to another within either a single semrel struc-
ture or by combining fragments from multiple 
semrel structures.  Paths are weighted for compari-
son (Richardson, 1997). Currently, either one or 
two words can be specified and we allow some 
restrictions to refine the path search.  A user can 
restrict the intended part of speech of the words 
entered, and/or the user can restrict the paths to 
include only the specified relation. When two 
words are provided, the UI returns a list of the 
highest ranked paths between those two words. 
When only one word is given, then all paths from 
that word are ranked and displayed.  Figure 2 
shows the MNEX interface, and a query requesting 
all paths from the word bird, restricted to Noun 
part of speech, through the Part relation:  
 
 
Figure 2: MNEX output for ?bird (Noun) Part? query 
3 Relation to other work 
For English, WordNet is the most widely used 
knowledgebase. Aside from being English-only, 
this database was hand-coded and significant effort 
is required to create similar databases for different 
domains and languages. Projects like EuroWord-
Net address the monolingual aspect of WordNet, 
but these databases are still labor intensive to cre-
ate.  On the other hand, the quality of the informa-
tion contained in a WordNet (Fellbaum et al, 
1998) is very reliable, exactly because it was 
manually created.  FrameNet (Baker et al, 1998) 
and OpenCyc are other valuable resources for Eng-
lish, also hand-created, that contain a rich set of 
relations between words and concepts. Their use is 
still being explored as they have been made avail-
able only recently. For Japanese, there are also 
concept dictionaries providing semantic relations, 
similarly hand-created, e.g., EDR and Nihongo 
Goi-taikei (NTT). 
The demonstration of MindNet will highlight 
that this resource is automatically created, allowing 
domain lexical resources to be built quickly, albeit 
with lesser accuracy.  We are confident that this is 
a trade-off worth making in many cases, and en-
courage experimentation in this area.  MNEX al-
lows the exploration of the rich set of relations 
through which paths connecting words are linked. 
4 References 
Baker, Collin F., Fillmore, Charles J., and Lowe, John 
B. (1998): The Berkeley FrameNet project. in Pro-
ceedings of the COLING-ACL, Montreal, Canada. 
Fellbaum, C. (ed). 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press. 
Heidorn, G. 2000. Intelligent writing assistance. in 
R.Dale, H.Moisl and H.Somers (eds.), A Handbook 
of Natural Langauge Processing: Techniques and 
Applications for the Processing of Language as Text. 
New York: Marcel Dekker. 
National Institute of Information and Communications 
Technology. 2001. EDR Electronic Dictionary Ver-
sion 2.0 Technical Guide. 
NTT Communications Science Laboratories. 1999. Goi-
Taikei - A Japanese Lexicon. Iwanami Shoten. 
OpenCyc. Available at: http://www.cyc.com/opencyc. 
Richardson, S.D. 1997, Determining Similarity and In-
ferring Relations in a Lexical Knowledge Base. PhD. 
dissertation, City University of New York. 
Richardson, S.D., W. B. Dolan, and L. Vanderwende. 
1998. MindNet: Acquiring and Structuring Semantic 
Information from Text, In Proceedings of ACL-
COLING. Montreal, pp. 1098-1102. 
Suzuki, H., G. Kacmarcik, L. Vanderwende and A. 
Menezes. 2005. Mindnet and mnex. In Proceedings 
of the 11th Annual meeting of the Society of Natural 
Language Processing (in Japanese).  
Vanderwende, L. 1995. Ambiguity in the acquisition of 
lexical information. In Proceedings of the AAAI 
1995 Spring Symposium Series, symposium on rep-
resentation and acquisition of lexical knowledge, 
174-179. 
9
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9?16,
New York, June 2006. c?2006 Association for Computational Linguistics
Do we need phrases? Challenging the conventional wisdom in Statistical 
Machine Translation 
Chris Quirk and Arul Menezes 
Microsoft Research 
One Microsoft Way 
Redmond, WA  98052  USA 
{chrisq,arulm}@microsoft.com 
 
Abstract 
We begin by exploring theoretical and 
practical issues with phrasal SMT, several 
of which are addressed by syntax-based 
SMT. Next, to address problems not 
handled by syntax, we propose the 
concept of a Minimal Translation Unit 
(MTU) and develop MTU sequence 
models. Finally we incorporate these 
models into a syntax-based SMT system 
and demonstrate that it improves on the 
state of the art translation quality within a 
theoretically more desirable framework. 
1. Introduction 
The last several years have seen phrasal statistical 
machine translation (SMT) systems outperform 
word-based approaches by a wide margin (Koehn 
2003). Unfortunately the use of phrases in SMT is 
beset by a number of difficult theoretical and 
practical problems, which we attempt to 
characterize below. Recent research into syntax-
based SMT (Quirk and Menezes 2005; Chiang 
2005) has produced promising results in 
addressing some of the problems; research 
motivated by other statistical models has helped 
to address others (Banchs et al 2005). We refine 
and unify two threads of research in an attempt to 
address all of these problems simultaneously. 
Such an approach proves both theoretically more 
desirable and empirically superior. 
In brief, Phrasal SMT systems employ phrase 
pairs automatically extracted from parallel 
corpora. To translate, a source sentence is first 
partitioned into a sequence of phrases I = s1?sI. 
Each source phrase si is then translated into a 
target phrase ti. Finally the target phrases are 
permuted, and the translation is read off in order. 
Beam search is used to approximate the optimal 
translation. We refer the reader to Keohn et al 
(2003) for a detailed description. Unless 
otherwise noted, the following discussion is 
generally applicable to Alignment Template 
systems (Och and Ney 2004) as well. 
1.1. Advantages of phrasal SMT 
Non-compositionality 
Phrases capture the translations of idiomatic and 
other non-compositional fixed phrases as a unit, 
side-stepping the need to awkwardly reconstruct 
them word by word. While many words can be 
translated into a single target word, common 
everyday phrases such as the English password 
translating as the French mot de passe cannot be 
easily subdivided. Allowing such translations to 
be first class entities simplifies translation 
implementation and improves translation quality. 
Local re-ordering 
Phrases provide memorized re-ordering decisions. 
As previously noted, translation can be 
conceptually divided into two steps: first, finding 
a set of phrase pairs that simultaneously covers 
the source side and provides a bag of translated 
target phrases; and second, picking an order for 
those target phrases. Since phrase pairs consist of 
memorized substrings of the training data, they 
are very likely to produce correct local re-
orderings. 
Contextual information 
Many phrasal translations may be easily 
subdivided into word-for-word translation, for 
instance the English phrase the cabbage may be 
translated word-for-word as le chou. However we 
note that la is also a perfectly reasonable word-
for-word translation of the, yet la chou is not a 
grammatical French string. Even when a phrase 
appears compositional, the incorporation of 
contextual information often improves translation 
9
quality. Phrases are a straightforward means of 
capturing local context.  
1.2. Theoretical problems with phrasal SMT 
Exact substring match; no discontiguity 
Large fixed phrase pairs are effective when an 
exact match can be found, but are useless 
otherwise. The alignment template approach 
(where phrases are modeled in terms of word 
classes instead of specific words) provides a 
solution at the expense of truly fixed phrases. 
Neither phrasal SMT nor alignment templates 
allow discontiguous translation pairs. 
Global re-ordering 
Phrases do capture local reordering, but provide 
no global re-ordering strategy, and the number of 
possible orderings to be considered is not 
lessened significantly. Given a sentence of n 
words, if the average target phrase length is 4 
words (which is unusually high), then the re-
ordering space is reduced from n! to only (n/4)!: 
still impractical for exact search in most 
sentences. Systems must therefore impose some 
limits on phrasal reordering, often hard limits 
based on distance as in Koehn et al (2003) or 
some linguistically motivated constraint, such as 
ITG (Zens and Ney, 2004). Since these phrases 
are not bound by or even related to syntactic 
constituents, linguistic generalizations (such as 
SVO becoming SOV, or prepositions becoming 
postpositions) are not easily incorporated into the 
movement models. 
Probability estimation 
To estimate the translation probability of a phrase 
pair, several approaches are used, often 
concurrently as features in a log-linear model. 
Conditional probabilities can be estimated by 
maximum likelihood estimation. Yet the phrases 
most likely to contribute important translational 
and ordering information?the longest ones?are 
the ones most subject to sparse data issues. 
Alternately, conditional phrasal models can be 
constructed from word translation probabilities; 
this approach is often called lexical weighting 
(Vogel et al 2003). This avoids sparse data 
issues, but tends to prefer literal translations 
where the word-for-word probabilities are high 
Furthermore most approaches model phrases as 
bags of words, and fail to distinguish between 
local re-ordering possibilities. 
Partitioning limitation 
A phrasal approach partitions the sentence into 
strings of words, making several questionable 
assumptions along the way. First, the probability 
of the partitioning is never considered. Long 
phrases tend to be rare and therefore have sharp 
probability distributions. This adds an inherent 
bias toward long phrases with questionable MLE 
probabilities (e.g. 1/1 or 2/2). 1 
Second, the translation probability of each 
phrase pair is modeled independently. Such an 
approach fails to model any phenomena that reach 
across boundaries; only the target language model 
and perhaps whole-sentence bag of words models 
cross phrase boundaries. This is especially 
important when translating into languages with 
agreement phenomena. Often a single phrase does 
not cover all agreeing modifiers of a headword; 
the uncovered modifiers are biased toward the 
most common variant rather than the one agreeing 
with its head. Ideally a system would consider 
overlapping phrases rather than a single 
partitioning, but this poses a problem for 
generative models: when words are generated 
multiple times by different phrases, they are 
effectively penalized. 
1.3. Practical problem with phrases: size 
In addition to the theoretical problems with 
phrases, there are also practical issues. While 
phrasal systems achieve diminishing returns due 
                                                          
1
 The Alignment Template approach differs slightly here. 
Phrasal SMT estimates the probability of a phrase pair as: 
=
'
)',(
),()|(
t
tscount
tscount
st?  
The Alignment Template method incorporates a loose 
partitioning probability by instead estimating the probability 
as (in the special case where each word has a unique class): 
)(
),()|(
scount
tscount
stp =  
Note that these counts could differ significantly. Picture a 
source phrase that almost always translates into a 
discontiguous phrase (e.g. English not becoming French ne 
? pas), except for the rare occasion where, due to an 
alignment error or odd training data, it translates into a 
contiguous phrase (e.g. French ne parle pas). Then the first 
probability formulation of ne parle pas given not would be 
unreasonably high. However, this is a partial fix since it 
again suffers from data sparsity problems, especially on 
longer templates where systems hope to achieve the best 
benefits from phrases. 
10
to sparse data, one does see a small incremental 
benefit with increasing phrase lengths. Given that 
storing all of these phrases leads to very large 
phrase tables, many research systems simply limit 
the phrases gathered to those that could possibly 
influence some test set. However, this is not 
feasible for true production MT systems, since the 
data to be translated is unknown. 
2. Previous work 
2.1. Delayed phrase construction 
To avoid the major practical problem of phrasal 
SMT?namely large phrase tables, most of which 
are not useful to any one sentence?one can 
instead construct phrase tables on the fly using an 
indexed form of the training data (Zhang and 
Vogel 2005; Callison-Burch et al 2005). 
However, this does not relieve any of the 
theoretical problems with phrase-based SMT. 
2.2. Syntax-based SMT 
Two recent systems have attempted to address the 
contiguity limitation and global re-ordering 
problem using syntax-based approaches. 
Hierarchical phrases 
Recent work in the use of hierarchical phrases 
(Chiang 2005) improves the ability to capture 
linguistic generalizations, and also removes the 
limitation to contiguous phrases. Hierarchical 
phrases differ from standard phrases in one 
important way: in addition to lexical items, a 
phrase pair may contain indexed placeholders, 
where each index must occur exactly once on 
each side. Such a formulation leads to a formally 
syntax-based translation approach, where 
translation is viewed as a parallel parsing problem 
over a grammar with one non-terminal symbol. 
This approach significantly outperforms a phrasal 
SMT baseline in controlled experimentation. 
Hierarchical phrases do address the need for 
non-contiguous phrases and suggest a powerful 
ordering story in the absence of linguistic 
information, although this reordering information 
is bound in a deeply lexicalized form. Yet they do 
not address the phrase probability estimation 
problem; nor do they provide a means of 
modeling phenomena across phrase boundaries. 
The practical problems with phrase-based 
translation systems are further exacerbated, since 
the number of translation rules with up to two 
non-adjacent non-terminals in a 1-1 monotone 
sentence pair of n source and target words is 
O(n6), as compared to O(n2) phrases. 
Treelet Translation 
Another means of extending phrase-based 
translation is to incorporate source language 
syntactic information. In Quirk and Menezes 
(2005) we presented an approach to phrasal SMT 
based on a parsed dependency tree representation 
of the source language. We use a source 
dependency parser and project a target 
dependency tree using a word-based alignment, 
after which we extract tree-based phrases 
(?treelets?) and train a tree-based ordering model. 
We showed that using treelets and a tree-based 
ordering model results in significantly better 
translations than a leading phrase-based system 
(Pharaoh, Koehn 2004), keeping all other models 
identical. 
Like the hierarchical phrase approach, treelet 
translation succeeds in improving the global re-
ordering search and allowing discontiguous 
phrases, but does not solve the partitioning or 
estimation problems. While we found our treelet 
system more resistant to degradation at smaller 
phrase sizes than the phrase-based system, it 
nevertheless suffered significantly at very small 
phrase sizes. Thus it is also subject to practical 
problems of size, and again these problems are 
exacerbated since there are potentially an 
exponential number of treelets. 
2.3. Bilingual n-gram channel models 
To address on the problems of estimation and 
partitioning, one recent approach transforms 
channel modeling into a standard sequence 
modeling problem (Banchs et al 2005). Consider 
the following aligned sentence pair in Figure 1a. 
In such a well-behaved example, it is natural to 
consider the problem in terms of sequence 
models. Picture a generative process that 
produces a sentence pair in left to right, emitting a 
pair of words in lock step. Let M = ? m1, ?, mn ? 
be a sequence of word pairs mi = ? s, t ?. Then one 
can generatively model the probability of an 
aligned sentence pair using techniques from n-
gram language modeling: 
11

=
?
?
=
?
?
=
=
k
i
i
nii
k
i
i
i
mmP
mmP
MPATSP
1
1
1
1
1
)|(
)|(
)(),,(
 
 When an alignment is one-to-one and 
monotone, this definition is sufficient. However 
alignments are seldom purely one-to-one and 
monotone in practice; Figure 1b displays common 
behavior such as one-to-many alignments, 
inserted words, and non-monotone translation. To 
address these problems, Banchs et al (2005) 
suggest defining tuples such that: 
(1) the tuple sequence is monotone, 
(2) there are no word alignment links between 
two distinct tuples, 
(3) each tuple has a non-NULL source side, 
which may require that target words 
aligned to NULL are joined with their 
following word, and 
(4) no smaller tuples can be extracted without 
violating these constraints. 
Note that M is now a sequence of phrase pairs 
instead of word pairs. With this adjusted 
definition, even Figure 1b can be generated using 
the same process using the following tuples: 
m1 = ? the, l? ? 
m2 = ? following example, exemple suivant ? 
m3 = ? renames, change le nom ? 
m4 = ? the, de la ? 
m5 = ? table, table ? 
There are several advantages to such an 
approach. First, it largely avoids the partitioning 
problem; instead of segmenting into potentially 
large phrases, the sentence is segmented into 
much smaller tuples, most often pairs of single 
words. Furthermore the failure to model a 
partitioning probability is much more defensible 
when the partitions are much smaller. Secondly, 
n-gram language model probabilities provide a 
robust means of estimating phrasal translation 
probabilities in context that models interactions 
between all adjacent tuples, obviating the need for 
overlapping mappings. 
These tuple channel models still must address 
practical issues such as model size, though much 
work has been done to shrink language models 
with minimal impact to perplexity (e.g. Stolcke 
1998), which these models could immediately 
leverage. Furthermore, these models do not 
address the contiguity problem or the global 
reordering problem. 
3. Translation by MTUs 
In this paper, we address all four theoretical 
problems using a novel combination of our 
syntactically-informed treelet approach (Quirk 
and Menezes 2005) and a modified version of 
bilingual n-gram channel models (Banchs et al 
2005). As in our previous work, we first parse the 
sentence into a dependency tree. After this initial 
parse, we use a global search to find a candidate 
that maximizes a log-linear model, where these 
candidates consist of a target word sequence 
annotated with a dependency structure, a word 
alignment, and a treelet decomposition.  
We begin by exploring minimal translation 
units and the models that concern them. 
3.1. Minimal Translation Units 
Minimal Translation Units (MTUs) are related to 
the tuples of Banchs et al (2005), but differ in 
several important respects. First, we relieve the 
restriction that the MTU sequence be monotone. 
This prevents spurious expansion of MTUs to 
incorporate adjacent context only to satisfy 
monotonicity. In the example, note that the 
previous algorithm would extract the tuple 
?following example, exemple suivant? even though 
the translations are mostly independent. Their 
partitioning is also context dependent: if the 
sentence did not contain the words following or 
suivant, then ? example, exemple ? would be a 
single MTU. Secondly we drop the requirement 
that no MTU have a NULL source side. While 
some insertions can be modeled in terms of 
adjacent words, we believe more robust models 
can be obtained if we consider insertions as 
 
(a) Monotone aligned sentence pair 
 
 
(b) More common non-monotone aligned sentence pair 
 
Figure 1. Example aligned sentence pairs. 
12
independent units. In the end our MTUs are 
defined quite simply as pairs of source and target 
word sets that follow the given constraints: 
(1) there are no word alignment links between 
distinct MTUs, and 
(2) no smaller MTUs can be extracted without 
violating the previous constraint. 
Since our word alignment algorithm is able to 
produce one-to-one, one-to-many, many-to-one, 
one-to-zero, and zero-to-one translations, these 
act as our basic units. As an example, let us 
consider example (1) once again. Using this new 
algorithm, the MTUs would be: 
m1 = ? the, l? ? 
m2 = ? following, suivant ? 
m3 = ? example, exemple ? 
m4 = ? renames, change le nom ? 
m5 = ? NULL, de ? 
m6 = ? the, la ? 
m7 = ? table, table ? 
A finer grained partitioning into MTUs further 
reduces the data sparsity and partitioning issues 
associated with phrases. Yet it poses issues in 
modeling translation: given a sequence of MTUs 
that does not have a monotone segmentation, how 
do we model the probability of an aligned 
translation pair? We propose several solutions, 
and use each in a log-linear combination of 
models. 
First, one may walk the MTUs in source order, 
ignoring insertion MTUs altogether. Such a 
model is completely agnostic of the target word 
order; instead of generating an aligned source 
target pair, it generates a source sentence along 
with a bag of target phrases. This approach 
expends a great deal of modeling effort in 
regenerating the source sentence, which may not 
be altogether desirable, though it does condition 
on surrounding translations. Also, it can be 
evaluated on candidates before orderings are 
considered. This latter property may be useful in 
two-stage decoding strategies where translations 
are considered before orderings. 
Secondly, one may walk the MTUs in target 
order, ignoring deletion MTUs. Where the source-
order MTU channel model expends probability 
mass generating the source sentence, this model 
expends a probability mass generating the target 
sentence and therefore may be somewhat 
redundant with the target language model. 
Finally, one may walk the MTUs in 
dependency tree order. Let us assume that in 
addition to an aligned source-target candidate 
pair, we have a dependency parse of the source 
side. Where the past models conditioned on 
surface adjacent MTUs, this model conditions on 
tree adjacent MTUs. Currently we condition only 
on the ancestor chain, where parent1(m) is the 
parent MTU of m, parent2(m) is the grandparent 
of m, and so on: 
))(|()(),,( 11 mparentmPMPATSP n
Mm
?
?
??=  
This model hopes to capture information 
completely distinct from the other two models, 
such as translational preferences contingent on the 
head, even in the presence of long distance 
dependencies. Note that it generates unordered 
dependency tree pairs.  
All of these models can be trained from a 
parallel corpus that has been word aligned and the 
source side dependency parsed. We walk through 
each sentence extracting MTUs in source, target, 
and tree order. Standard n-gram language 
modeling tools can be used to train MTU 
language models. 
3.2. Decoding 
We employ a dependency tree-based beam search 
decoder to search the space of translations. First 
the input is parsed into a dependency tree 
  English French English Japanese 
Training Sentences 300,000 500,000 
 Words 4,441,465 5,198,932 7,909,198 9,379,240 
 Vocabulary 63,343 59,290 79,029 95,813 
 Singletons 35,328 29,448 44,111 52,911 
Development test Sentences 200 200 
 Words 3,045 3,456 3,436 4,095 
Test Sentences 2,000 2,000 
 Words 30,010 34,725 35,556 3,855 
 OOV rate 5.5% 4.6% 6.9% 6.8% 
Table 4.1 Data characteristics 
13
structure. For each input node in the dependency 
tree, an n-best list of candidates is produced. 
Candidates consist of a target dependency tree 
along with a treelet and word alignment. The 
decoder generally assumes phrasal cohesion: 
candidates covering a substring (not subsequence) 
of the input sentence produce a potential substring 
(not subsequence) of the final translation. In 
addition to allowing a DP / beam decoder, this 
allows us to evaluate string-based models (such as 
the target language model and the source and 
target order MTU n-gram models) on partial 
candidates. This decoder is unchanged from our 
previous work: the MTU n-gram models are 
simply incorporated as feature functions in the 
log-linear combination. In the experiments section 
the MTU models are referred to as model set (1). 
3.3. Other translation models 
Phrasal channel models 
We can estimate traditional channel models using 
maximum likelihood or lexical weighting: 
? ?
? ?
?
?
? ? ?
? ? ?
?
?
=
=
=
=
)(),(
InverseM1
)(),(
DirectM1
)(),(
InverseMLE
)(),(
DirectMLE
)|(),,(
)|(),,(
)(*,
),(),,(
,*)(
),(),,(
Atreelets s t
Atreelets t s
Atreelets
Atreelets
tspATSf
stpATSf
c
cATSf
c
cATSf
?? ? ?
?? ? ?
??
??
?
??
?
??
 
We use word probability tables p(t | s) and p(s | t) 
estimated by IBM Model 1 (Brown et al 1993). 
Such models can be built over phrases if used in a 
phrasal decoder or over treelets if used in a treelet 
decoder. These models are referred to as set (2). 
Word-based models 
A target language model using modified Kneser-
Ney smoothing captures fluency; a word count 
feature offsets the target LM preference for 
shorter selections; and a treelet/phrase count helps 
bias toward translations using fewer phrases. 
These models are referred to as set (3). 
|)(|),,(
||),,(
)|(),,(
tphrasecoun
wordcount
||
1
1
targetLM
AtreeletsATSf
TATSf
ttPATSf
T
i
i
nii
=
=
= ?
=
?
?
 
Syntactic models 
As in Quirk and Menezes (2005), we include a 
linguistically-informed order model that predicts 
the head-relative position of each node 
independently, and a tree-based bigram target 
language model; these models are referred to as 
set (4). 
?
?
?
?
=
=
Tt
Tt
tparenttPATSf
ATStpositionPATSf
))(|(),,(
),,|)((),,(
treeLM
order
  
4. Experimental setup 
We evaluate the translation quality of the system 
using the BLEU metric (Papineni et al, 02) under 
a variety of configurations. As an additional 
baseline, we compare against a phrasal SMT 
decoder, Pharaoh (Koehn et al 2003).  
4.1. Data 
Two language pairs were used for this 
comparison: English to French, and English to 
Japanese. The data was selected from technical 
software documentation including software 
manuals and product support articles; Table 4.1 
presents the major characteristics of this data. 
4.2. Training 
We parsed the source (English) side of the 
corpora using NLPWIN, a broad-coverage rule-
based parser able to produce syntactic analyses at 
varying levels of depth (Heidorn 2002). For the 
purposes of these experiments we used a 
dependency tree output with part-of-speech tags 
and unstemmed surface words. Word alignments 
were produced by GIZA++ (Och and Ney 2003) 
with a standard training regimen of five iterations 
of Model 1, five iterations of the HMM Model, 
and five iterations of Model 4, in both directions. 
These alignments were combined heuristically as 
described in our previous work. 
We then projected the dependency trees and 
used the aligned dependency tree pairs to extract 
treelet translation pairs, train the order model, and 
train MTU models. The target language models 
were trained using only the target side of the 
corpus. Finally we trained model weights by 
maximizing BLEU (Och 2003) and set decoder 
optimization parameters (n-best list size, timeouts 
14
etc) on a development test set of 200 held-out 
sentences each with a single reference translation. 
Parameters were individually estimated for each 
distinct configuration. 
Pharaoh 
The same GIZA++ alignments as above were 
used in the Pharaoh decoder (Koehn 2004). We 
used the heuristic combination described in (Och 
and Ney 2003) and extracted phrasal translation 
pairs from this combined alignment as described 
in (Koehn et al, 2003). Aside from MTU models 
and syntactic models (Pharaoh uses its own 
ordering approach), the same models were used: 
MLE and lexical weighting channel models, 
target LM, and phrase and word count. Model 
weights were also trained following Och (2003). 
5. Results 
We begin with a broad brush comparison of 
systems in Table 5.1. Throughout this section, 
treelet and phrase sizes are measured in terms of 
MTUs, not words. By default, all systems 
(including Pharaoh) use treelets or phrases of up 
to four MTUs, and MTU bigram models. The first 
results reiterate that the introduction of 
discontiguous mappings and especially a 
linguistically motivated order model (model set 
(4)) can improve translation quality. Replacing 
the standard channel models (model set (2)) with 
MTU bigram models (model set (1)) does not 
appear to degrade quality; it even seems to boost 
quality on EF. Furthermore, the information in the 
MTU models appears somewhat orthogonal to the 
phrasal models; a combination results in 
improvements for both language pairs. 
The experiments in Table 5.2 compare quality 
using different orders of MTU n-gram models. 
(Treelets containing up to four MTUs were still 
used as the basis for decoding; only the order of 
the MTU n-gram models was adjusted.) A 
unigram model performs surprisingly well. This 
supports our intuition that atomic handling of 
non-compositional multi-word translations is a 
major contribution of phrasal SMT. Furthermore 
bigram models increase translation quality 
supporting the claim that local context is another 
contribution. Models beyond bigrams had little 
impact presumably due to sparsity and smoothing. 
Table 5.3 explores the impact of using different 
phrase/treelet sizes in decoding. We see that 
adding MTU models makes translation more 
resilient given smaller phrases. The poor 
performance at size 1 is not particularly 
surprising: both systems require insertions to be 
lexically anchored: the only decoding operation 
allowed is translation of some visible source 
phrase, and insertions have no visible trace. 
6. Conclusions 
In this paper we have teased apart the role of 
 EF EJ 
Phrasal decoder (Pharaoh) 
  Model sets (2),(3) 45.8?2.0 32.9?0.9 
Treelet decoder, without discontiguous mappings 
  Model sets (2),(3) 45.1?2.1 33.2?0.9 
  Model sets (2),(3),(4) 48.4?2.0 34.8?0.9 
Treelet decoder, with discontiguous mappings 
  Model sets (2),(3) 46.4?2.1 34.3?0.9 
  Model sets (2),(3),(4) 48.7?2.1 34.9?0.9 
  Model sets (1),(3),(4) 49.6?2.1 33.9?0.8 
  Model sets (1)-(4) 50.5?2.1 36.2?0.9 
 
Table 5.1. Broad system comparison. 
 EF EJ 
Treelet decoder, model sets (1),(3),(4) 
  MTU unigram 47.8?2.1 33.2?0.9 
  MTU bigram 49.6?2.1 33.9?0.8 
  MTU trigram 49.9?2.0 34.0?0.9 
  MTU 4-gram 49.6?2.1 34.1?0.9 
Treelet decoder, model sets (1)-(4)  
  MTU unigram 48.6?2.1 34.3?1.0 
  MTU bigram 50.5?2.1 36.2?0.9 
  MTU trigram 48.9?2.0 36.1?0.9 
  MTU 4-gram 50.4?2.0 36.2?1.0 
 
Table 5.2. Varying MTU n-gram model order. 
Table 5.3. Varying phrase / treelet size. 
 
 Phrasal decoder 
model sets (2),(3) 
Treelet decoder: MTU bigram 
model sets (1),(3),(4) 
Treelet decoder: MTU bigram 
model sets (1)-(4) 
Size EF EJ EF EJ EF EJ 
1 32.6?1.8 20.5?0.7 26.3?1.3 15.4?0.7 29.8?1.4 16.7?0.7 
2 40.4?1.9 29.7?0.7 48.7?2.1 32.4?0.9 47.7?2.1 33.8?0.8 
3 44.3?2.1 30.7?0.9 48.5?2.0 34.6?0.9 48.5?2.0 35.1?0.9 
4 45.8?2.0 32.9?0.9 49.6?2.1 33.9?0.8 50.5?2.1 36.2?0.9 
 
15
phrases and handled each contribution via a 
distinct model best suited to the task. Non-
compositional translations stay as MTU phrases. 
Context and robust estimation is provided by 
MTU-based n-gram models. Local and global 
ordering is handled by a tree-based model. 
The first interesting result is that at normal 
phrase sizes, augmenting an SMT system with 
MTU n-gram models improves quality; whereas 
replacing the standard phrasal channel models by 
the more theoretically sound MTU n-gram 
channel models leads to very similar 
performance. 
Even more interesting are the results on smaller 
phrases. A system using very small phrases (size 
2) and MTU bigram models matches (English-
French) or at least approaches (English-Japanese) 
the performance of the baseline system using 
large phrases (size 4). While this work does not 
yet obviate the need for phrases, we consider it a 
promising step in that direction. 
An immediate practical benefit is that it allows 
systems to use much smaller phrases (and hence 
smaller phrase tables) with little or no loss in 
quality. This result is particularly important for 
syntax-based systems, or any system that allows 
discontiguous phrases. Given a fixed length limit, 
the number of surface phrases extracted from any 
sentence pair of length n where all words are 
uniquely aligned is O(n), but the number of 
treelets is potentially exponential in the number of 
children; and the number of rules with two gaps 
extracted by Chiang (2005) is potentially O(n3). 
Our results using MTUs suggest that such 
systems can avoid unwieldy, poorly estimated 
long phrases and instead anchor decoding on 
shorter, more tractable knowledge units such as 
MTUs, incorporating channel model information 
and contextual knowledge with an MTU n-gram 
model. 
Much future work does remain. From 
inspecting the model weights of the best systems, 
we note that only the source order MTU n-gram 
model has a major contribution to the overall 
score of a given candidate. This suggests that the 
three distinct models, despite their different walk 
orders, are somewhat redundant. We plan to 
consider other approaches for conditioning on 
context. Furthermore phrasal channel models, in 
spite of the laundry list of problems presented 
here, have a significant impact on translation 
quality. We hope to replace them with effective 
models without the brittleness and sparsity issues 
of heavy lexicalization. 
References 
Banchs, Rafael, Josep Crego, Adri? de Gispert, Patrik 
Lambert, and Jose Mari?o. 2005. Statistical machine 
translation of Euparl data by using bilingual n-grams. In 
Proceedings of ACL Workshop on Building and Using 
Parallel Texts. 
Brown, Peter, Vincent Della Pietra, Stephen Della Pietra, and 
Robert Mercer. 1993. The mathematics of statistical 
machine translation: parameter estimation. Computational 
Linguistics 19(2): 263-311. 
Callison-Burch, Chris, Colin Bannard, and Josh Schroeder. 
2005. Scaling phrase-based machine translation to larger 
corpora and longer phrases. In Proceedings of ACL. 
Chiang, David. 2005. A hierarchical phrase-based model for 
statistical machine translation. In Proceedings of ACL. 
Heidorn, George. 2000. ?Intelligent writing assistance?. In 
Dale et al Handbook of Natural Language Processing, 
Marcel Dekker. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. 
Statistical phrase based translation. In Proceedings of 
NAACL. 
Koehn, Philipp. 2004. Pharaoh: A beam search decoder for 
phrase-based statistical machine translation models. In 
Proceedings of AMTA. 
Och, Franz Josef and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Och, Franz Josef and Hermann Ney. 2004. The Alignment 
Template approach to statistical machine translation, 
Computational Linguistics, 30(4):417-450.  
Och, Franz Josef. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of ACL. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evaluation of 
machine translation. In Proceedings of ACL. 
Quirk, Chris and Arul Menezes. 2005. Dependency tree 
translation: syntactically-informed phrasal SMT. In 
Proceedings of ACL. 
Stolcke, Andreas. 1998. Entropy-based pruning of backoff 
language models. In Proceedings of DARPA Broadcast 
News Transcription and Understanding. 
Vogel, Stephan, Ying Zhang, Fei Huang, Alicia Tribble, 
Ashish Venugopal, Bing Zhao, Alex Waibel. 2003. The 
CMU statistical machine translation system. In 
Proceedings of MT Summit. 
Zens, Richard, and Hermann Ney. 2003. A comparative 
study on reordering constraints in statistical machine 
translation. In Proceedings of ACL. 
Zhang, Ying and Stephan Vogel. 2005. An efficient phrase-
to-phrase alignment model for arbitrarily long phrase and 
large corpora. In Proceedings of EAMT. 
16
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33?40,
New York, June 2006. c?2006 Association for Computational Linguistics
Effectively Using Syntax for Recognizing False Entailment
Rion Snow
Computer Science Department
Stanford University
Stanford, CA 94305
rion@cs.stanford.edu
Lucy Vanderwende and Arul Menezes
Microsoft Research
One Microsoft Way
Redmond, WA 98027
{lucyv,arulm}@microsoft.com
Abstract
Recognizing textual entailment is a chal-
lenging problem and a fundamental com-
ponent of many applications in natural
language processing. We present a novel
framework for recognizing textual entail-
ment that focuses on the use of syntactic
heuristics to recognize false entailment.
We give a thorough analysis of our sys-
tem, which demonstrates state-of-the-art
performance on a widely-used test set.
1 Introduction
Recognizing the semantic equivalence of two frag-
ments of text is a fundamental component of many
applications in natural language processing. Recog-
nizing textual entailment, as formulated in the recent
PASCAL Challenge 1, is the problem of determining
whether some text sentence T entails some hypothe-
sis sentence H .
The motivation for this formulation was to iso-
late and evaluate the application-independent com-
ponent of semantic inference shared across many ap-
plication areas, reflected in the division of the PAS-
CAL RTE dataset into seven distinct tasks: Informa-
tion Extraction (IE), Comparable Documents (CD),
Reading Comprehension (RC), Machine Translation
(MT), Information Retrieval (IR), Question Answer-
ing (QA), and Paraphrase Acquisition (PP).
1http://www.pascal-network.org/Challenges/RTE. The ex-
amples given throughout this paper are from the first PASCAL
RTE dataset, described in Section 6.
The RTE problem as presented in the PASCAL
RTE dataset is particularly attractive in that it is a
reasonably simple task for human annotators with
high inter-annotator agreement (95.1% in one inde-
pendent labeling (Bos and Markert, 2005)), but an
extremely challenging task for automated systems.
The highest accuracy systems on the RTE test set
are still much closer in performance to a random
baseline accuracy of 50% than to the inter-annotator
agreement. For example, two high-accuracy systems
are those described in (Tatu and Moldovan, 2005),
achieving 60.4% accuracy with no task-specific in-
formation, and (Bos and Markert, 2005), which
achieves 61.2% task-dependent accuracy, i.e. when
able to use the specific task labels as input.
Previous systems for RTE have attempted a wide
variety of strategies. Many previous approaches
have used a logical form representation of the text
and hypothesis sentences, focusing on deriving a
proof by which one can infer the hypothesis logical
form from the text logical form (Bayer et al, 2005;
Bos and Markert, 2005; Raina et al, 2005; Tatu and
Moldovan, 2005). These papers often cite that a ma-
jor obstacle to accurate theorem proving for the task
of textual entailment is the lack of world knowledge,
which is frequently difficult and costly to obtain and
encode. Attempts have been made to remedy this
deficit through various techniques, including model-
building (Bos and Markert, 2005) and the addition
of semantic axioms (Tatu and Moldovan, 2005).
Our system diverges from previous approaches
most strongly by focusing upon false entailments;
rather than assuming that a given entailment is false
until proven true, we make the opposite assump-
33
tion, and instead focus on applying knowledge-free
heuristics that can act locally on a subgraph of syn-
tactic dependencies to determine with high confi-
dence that the entailment is false. Our approach is
inspired by an analysis of the RTE dataset that sug-
gested a syntax-based approach should be approxi-
mately twice as effective at predicting false entail-
ment as true entailment (Vanderwende and Dolan,
2006). The analysis implied that a great deal of syn-
tactic information remained unexploited by existing
systems, but gave few explicit suggestions on how
syntactic information should be applied; this paper
provides a starting point for creating the heuristics
capable of obtaining the bound they suggest2.
2 System Description
Similar to most other syntax-based approaches to
recognizing textual entailment, we begin by rep-
resenting each text and hypothesis sentence pair
in logical forms. These logical forms are gener-
ated using NLPWIN3, a robust system for natural
language parsing and generation (Heidorn, 2000).
Our logical form representation may be consid-
ered equivalently as a set of triples of the form
RELATION(nodei, nodej), or as a graph of syntac-
tic dependencies; we use both terminologies inter-
changeably. Our algorithm proceeds as follows:
1. Parse each sentence with the NLPWIN parser,
resulting in syntactic dependency graphs for the
text and hypothesis sentences.
2. Attempt an alignment of each content node in
the dependency graph of the hypothesis sen-
tence to some node in the graph of the text sen-
tence, using a set of heuristics for alignment
(described in Section 3).
3. Using the alignment, apply a set of syntactic
heuristics for recognizing false entailment (de-
scribed in Section 4); if any match, predict that
the entailment is false.
2(Vanderwende and Dolan, 2006) suggest that the truth or
falsehood of 48% of the entailment examples in the RTE test set
could be correctly identified via syntax and a thesaurus alone;
thus by random guessing on the rest of the examples one might
hope for an accuracy level of 0.48 + 0.522 = 74%.3To aid in the replicability of our experiments, we have
published the NLPWIN logical forms for all sentences from
the development and test sets in the PASCAL RTE dataset at
http://research.microsoft.com/nlp/Projects/RTE.aspx.
lemma: freepos: Verbfeatures: Past,Pass,T1,Proposition
lemma: _Xpos: PronTsub
lemma: hostagepos: Nounfeatures: Plur,Humn,Count,Anim,Conc,Humn_sr
Tobj
lemma: sixpos: Adjfeatures: Quant,Plur,Num,Value 6Lops
lemma: Iraqpos: Nounfeatures: Sing,PrprN,Pers3,Cntry
Locn_in
Figure 1: Logical form produced by NLPWIN for
the sentence ?Six hostages in Iraq were freed.?
4. If no syntactic heuristic matches, back off to
a lexical similarity model (described in section
5.1), with an attempt to align detected para-
phrases (described in section 5.2).
In addition to the typical syntactic information pro-
vided by a dependency parser, the NLPWIN parser
provides an extensive number of semantic features
obtained from various linguistic resources, creating
a rich environment for feature engineering. For ex-
ample, Figure 1 (from Dev Ex. #616) illustrates the
dependency graph representation we use, demon-
strating the stemming, part-of-speech tagging, syn-
tactic relationship identification, and semantic fea-
ture tagging capabilities of NLPWIN.
We define a content node to be any node whose
lemma is not on a small stoplist of common stop
words. In addition to content vs. non-content nodes,
among content nodes we distinguish between en-
tities and nonentities: an entity node is any node
classified by the NLPWIN parser as being a proper
noun, quantity, or time.
Each of the features of our system were developed
from inspection of sentence pairs from the RTE de-
velopment data set, and used in the final system only
if they improved the system?s accuracy on the de-
velopment set (or improved F-score if accuracy was
unchanged); sentence pairs in the RTE test set were
left uninspected and used for testing purposes only.
3 Linguistic cues for node alignment
Our syntactic heuristics for recognizing false entail-
ment rely heavily on the correct alignment of words
and multiword units between the text and hypothesis
logical forms. In the notation below, we will con-
sider h and t to be nodes in the hypothesis H and
34
Hypothesis: ??Hepburn, who won four Oscars...??
Text: ??Hepburn, a four-time Academy Award winner...??
HepburnNoun winVerbTsub
HepburnNoun
Stringmatch
OscarNounTobj
winnerNoun
Derivationalform match
fourAdjLops
Academy_AwardNoun
Synonymmatch
four-timeAdj
Valuematch
Appostn
Attrib
Mod
Figure 2: Example of synonym, value, and deriva-
tional form alignment heuristics, Dev Ex. #767
text T logical forms, respectively. To accomplish
the task of node alignment we rely on the following
heuristics:
3.1 WordNet synonym match
As in (Herrera et al, 2005) and others, we align
a node h ? H to any node t ? T that has both
the same part of speech and belongs to the same
synset in WordNet. Our alignment considers mul-
tiword units, including compound nouns (e.g., we
align ?Oscar? to ?Academy Award? as in Figure 2),
as well as verb-particle constructions such as ?set
off? (aligned to ?trigger? in Test Ex. #1983).
3.2 Numeric value match
The NLPWIN parser assigns a normalized numeric
value feature to each piece of text inferred to cor-
respond to a numeric value; this allows us to align
?6th? to ?sixth? in Test Ex. #1175. and to align ?a
dozen? to ?twelve? in Test Ex. #1231.
3.3 Acronym match
Many acronyms are recognized using the syn-
onym match described above; nonetheless, many
acronyms are not yet in WordNet. For these cases we
have a specialized acronym match heuristic which
aligns pairs of nodes with the following properties:
if the lemma for some node h consists only of cap-
italized letters (with possible interceding periods),
and the letters correspond to the first characters of
some multiword lemma for some t ? T , then we
consider h and t to be aligned. This heuristic allows
us to align ?UNDP? to ?United Nations Develop-
ment Programme? in Dev Ex. #357 and ?ANC? to
?African National Congress? in Test Ex. #1300.
3.4 Derivational form match
We would like to align words which have the same
root form (or have a synonym with the same root
form) and which possess similar semantic meaning,
but which may belong to different syntactic cate-
gories. We perform this by using a combination of
the synonym and derivationally-related form infor-
mation contained within WordNet. Explicitly our
procedure for constructing the set of derivationally-
related forms for a node h is to take the union of all
derivationally-related forms of all the synonyms of
h (including h itself), i.e.:
DERIV(h) = ?s?WN-SYN(h)WN-DERIV(s)
In addition to the noun/verb derivationally-related
forms, we detect adjective/adverb derivationally-
related forms that differ only by the suffix ?ly?.
Unlike the previous alignment heuristics, we do
not expect that two nodes aligned via derivationally-
related forms will play the same syntactic role in
their respective sentences. Thus we consider two
nodes aligned in this way to be soft-aligned, and we
do not attempt to apply our false entailment recog-
nition heuristics to nodes aligned in this way.
3.5 Country adjectival form / demonym match
As a special case of derivational form match, we
soft-align matches from an explicit list of place
names, adjectival forms, and demonyms4; e.g.,
?Sweden? and ?Swedish? in Test Ex. #1576.
3.6 Other heuristics for alignment
In addition to these heuristics, we implemented a hy-
ponym match heuristic similar to that discussed in
(Herrera et al, 2005), and a heuristic based on the
string-edit distance of two lemmas; however, these
heuristics yielded a decrease in our system?s accu-
racy on the development set and were thus left out
of our final system.
4 Recognizing false entailment
The bulk of our system focuses on heuristics for
recognizing false entailment. For purposes of no-
tation, we define binary functions for the existence
4List of adjectival forms and demonyms based on the list at:
http://en.wikipedia.org/wiki/List of demonyms
35
Unaligned Entity: ENTITY(h) ? ?t.?ALIGN(h, t) ? False.
Negation Mismatch: ALIGN(h, t) ? NEG(t) 6= NEG(h) ? False.
Modal Mismatch: ALIGN(h, t) ? MOD(t) ? ?MOD(h) ? False.
Antonym Match: ALIGN(h1, t1) ? REL(h0, h1) ? REL(t0, t1) ? LEMMA(t0) ? ANTONYMS(h0) ? False
Argument Movement: ALIGN(h1, t1) ? ALIGN(h2, t2) ? REL(h1, h2) ? ?REL(t1, t2) ? REL ? {SUBJ, OBJ, IND} ? False
Superlative Mismatch: ?(SUPR(h1) ? (ALIGN(h1, t1) ? ALIGN(h2, t2) ? REL1(h2, h1) ? REL1(t2, t1)
??t3.(REL2(t2, t3) ? REL2 ? {MOD,POSSR,LOCN} ? REL2(h2, h3) ? ALIGN(h3, t3))) ? False
Conditional Mismatch: ALIGN(h1, t1) ? ALIGN(h2, t2) ? COND ? PATH(t1, t2) ? COND /? PATH(h1, h2) ? False
Table 1: Summary of heuristics for recognizing false entailment
of each semantic node feature recognized by NLP-
WIN; e.g., if h is negated, we state that NEG(h) =
TRUE. Similarly we assign binary functions for
the existence of each syntactic relation defined over
pairs of nodes. Finally, we define the function
ALIGN(h, t) to be true if and only if the node h ? H
has been ?hard-aligned? to the node t ? T using one
of the heuristics in Section 3. Other notation is de-
fined in the text as it is used. Table 1 summarizes all
heuristics used in our final system to recognize false
entailment.
4.1 Unaligned entity
If some node h has been recognized as an entity (i.e.,
as a proper noun, quantity, or time) but has not been
aligned to any node t, we predict that the entailment
is false. For example, we predict that Test Ex. #1863
is false because the entities ?Suwariya?, ?20 miles?,
and ?35? in H are unaligned.
4.2 Negation mismatch
If any two nodes (h, t) are aligned, and one (and
only one) of them is negated, we predict that the en-
tailment is false. Negation is conveyed by the NEG
feature in NLPWIN. This heuristic allows us to pre-
dict false entailment in the example ?Pertussis is not
very contagious? and ?...pertussis, is a highly conta-
gious bacterial infection? in Test Ex. #1144.
4.3 Modal auxiliary verb mismatch
If any two nodes (h, t) are aligned, and t is modified
by a modal auxiliary verb (e.g, can, might, should,
etc.) but h is not similarly modified, we predict that
the entailment is false. Modification by a modal aux-
iliary verb is conveyed by the MOD feature in NLP-
WIN. This heuristic allows us to predict false en-
tailment between the text phrase ?would constitute
a threat to democracy?, and the hypothesis phrase
?constitutes a democratic threat? in Test Ex. #1203.
4.4 Antonym match
If two aligned noun nodes (h1, t1) are both subjects
or both objects of verb nodes (h0, t0) in their re-
spective sentences, i.e., REL(h0, h1)? REL(t0, t1)?
REL ? {SUBJ,OBJ}, then we check for a verb
antonym match between (h0, t0). We construct
the set of verb antonyms using WordNet; we con-
sider the antonyms of h0 to be the union of the
antonyms of the first three senses of LEMMA(h0),
or of the nearest antonym-possessing hypernyms if
those senses do not themselves have antonyms in
WordNet. Explicitly our procedure for constructing
the antonym set of a node h0 is as follows:
1. ANTONYMS(h0) = {}
2. For each of the first three listed senses s of
LEMMA(h0) in WordNet:
(a) While |WN-ANTONYMS(s)| = 0
i. s ? WN-HYPERNYM(s)
(b) ANTONYMS(h0) ? ANTONYMS(h0) ?
WN-ANTONYMS(s)
3. return ANTONYMS(h0)
In addition to the verb antonyms in WordNet, we
detect the prepositional antonym pairs (before/after,
to/from, and over/under). This heuristic allows us to
predict false entailment between ?Black holes can
lose mass...? and ?Black holes can regain some of
their mass...? in Test Ex. #1445.
4.5 Argument movement
For any two aligned verb nodes (h1, t1), we con-
sider each noun child h2 of h1 possessing any of
36
Hypothesis Text
killVerb
Prime MinisterRobert MalvalNoun
Tobj
AristideNoun
Tsub
killVerb
Prime MinisterRobert MalvalNoun
AristideNoun
Tsub
conferenceNoun
Tobj
 
 
   
 
callVerb
Attrib
conferenceNoun
TobjTsub
 
 
   
 
Port-au-PrinceNoun
Locn_in
Figure 3: Example of object movement signaling
false entailment
the subject, object, or indirect object relations to
h1, i.e., there exists REL(h1, h2) such that REL ?
{SUBJ, OBJ, IND}. If there is some node t2 such that
ALIGN(h2, t2), but REL(t1, t2) 6= REL(h1, h2), then
we predict that the entailment is false.
As an example, consider Figure 3, representing
subgraphs from Dev Ex. #1916:
T : ...U.N. officials are also dismayed that Aristide killed a con-
ference called by Prime Minister Robert Malval...
H: Aristide kills Prime Minister Robert Malval.
Here let (h1, t1) correspond to the aligned verbs
with lemma kill, where the object of h1 has lemma
Prime Minister Robert Malval, and the object of t1
has lemma conference. Since h2 is aligned to some
node t2 in the text graph, but ?OBJ(t1, t2), the sen-
tence pair is rejected as a false entailment.
4.6 Superlative mismatch
If some adjective node h1 in the hypothesis is iden-
tified as a superlative, check that all of the following
conditions are satisfied:
1. h1 is aligned to some superlative t1 in the text
sentence.
2. The noun phrase h2 modified by h1 is aligned
to the noun phrase t2 modified by t1.
3. Any additional modifier t3 of the noun phrase
t2 is aligned to some modifier h3 of h2 in the
hypothesis sentence (reverse subset match).
If any of these conditions are not satisfied, we pre-
dict that the entailment is false. This heuristic allows
us to predict false entailment in (Dev Ex. #908):
T : Time Warner is the world?s largest media and Internet com-
pany.
H: Time Warner is the world?s largest company.
Here ?largest media and Internet company? in T
fails the reverse subset match (condition 3) to
?largest company? in H .
4.7 Conditional mismatch
For any pair of aligned nodes (h1, t1), if there ex-
ists a second pair of aligned nodes (h2, t2) such
that the shortest path PATH(t1, t2) in the depen-
dency graph T contains the conditional relation,
then PATH(h1, h2) must also contain the conditional
relation, or else we predict that the entailment is
false. For example, consider the following false en-
tailment (Dev Ex. #60):
T : If a Mexican approaches the border, he?s assumed to be try-
ing to illegally cross.
H: Mexicans continue to illegally cross border.
Here, ?Mexican? and ?cross? are aligned, and the
path between them in the text contains the condi-
tional relation, but does not in the hypothesis; thus
the entailment is predicted to be false.
4.8 Other heuristics for false entailment
In addition to these heuristics, we additionally im-
plemented an IS-A mismatch heuristic, which at-
tempted to discover when an IS-A relation in the hy-
pothesis sentence was not implied by a correspond-
ing IS-A relation in the text; however, this heuristic
yielded a loss in accuracy on the development set
and was therefore not included in our final system.
5 Lexical similarity and paraphrase
detection
5.1 Lexical similarity using MindNet
In case none of the preceding heuristics for rejec-
tion are applicable, we back off to a lexical sim-
ilarity model similar to that described in (Glick-
man et al, 2005). For every content node h ? H
37
not already aligned by one of the heuristics in Sec-
tion 3, we obtain a similarity score MN(h, t) from a
similarity database that is constructed automatically
from the data contained in MindNet5 as described in
(Richardson, 1997). Our similarity function is thus:
sim(h, t) =
?
??
??
1 if ANY-ALIGN(h, t)
MN(h, t) if MN(h, t) > min
min otherwise
Where the minimum score min is a parameter
tuned for maximum accuracy on the development
set; min = 0.00002 in our final system. We then
compute the entailment score:
score(H,T ) = 1|H|
?
h?H
max
t?T
sim(h, t)
This approach is identical to that used in (Glick-
man et al, 2005), except that we use alignment
heuristics and MindNet similarity scores in place
of their web-based estimation of lexical entailment
probabilities, and we take as our score the geomet-
ric mean of the component entailment scores rather
than the unnormalized product of probabilities.
5.2 Measuring phrasal similarity using the web
The methods discussed so far for alignment are lim-
ited to aligning pairs of single words or multiple-
word units constituting single syntactic categories;
these are insufficient for the problem of detecting
more complicated paraphrases. For example, con-
sider the following true entailment (Dev Ex. #496):
T : ...Muslims believe there is only one God.
H: Muslims are monotheistic.
Here we would like to align the hypothesis phrase
?are monotheistic? to the text phrase ?believe there
is only one God?; unfortunately, single-node align-
ment aligns only the nodes with lemma ?Muslim?.
In this section we describe the approach used in our
system to approximate phrasal similarity via distrib-
utional information obtained using the MSN Search
search engine.
We propose a metric for measuring phrasal simi-
larity based on a phrasal version of the distributional
hypothesis: we propose that a phrase template Ph
5http://research.microsoft.com/mnex
(e.g. ?xh are monotheistic?) has high semantic simi-
larity to a template Pt (e.g. ?xt believe there is only
one God?), with possible ?slot-fillers? xh and xt, re-
spectively, if the overlap of the sets of observed slot-
fillers Xh ?Xt for those phrase templates is high in
some sufficiently large corpus (e.g., the Web).
To measure phrasal similarity we issue the sur-
face text form of each candidate phrase template as
a query to a web-based search engine, and parse the
returned sentences in which the candidate phrase oc-
curs to determine the appropriate slot-fillers. For ex-
ample, in the above example, we observe the set of
slot-fillers Xt = {Muslims, Christians, Jews, Saiv-
ities, Sikhs, Caodaists, People}, and Xh ? Xt =
{Muslims, Christians, Jews, Sikhs, People}.
Explicitly, given the text and hypothesis logical
forms, our algorithm proceeds as follows to compute
the phrasal similarity between all phrase templates
in H and T :
1. For each pair of aligned single node and un-
aligned leaf node (t1, tl) (or pair of aligned
nodes (t1, t2)) in the text T :
(a) Use NLPWIN to generate a surface text
string S from the underlying logical form
PATH(t1, t2).
(b) Create the surface string template phrase
Pt by removing from S the lemmas corre-
sponding to t1 (and t2, if path is between
aligned nodes).
(c) Perform a web search for the string Pt.
(d) Parse the resulting sentences containing
Pt and extract all non-pronoun slot fillers
xt ? Xt that satisfy the same syntactic
roles as t1 in the original sentence.
2. Similarly, extract the slot fillers Xh for each
discovered phrase template Ph in H .
3. Calculate paraphrase similarity as a function of
the overlap between the slot-filler sets Xt and
Xh, i.e: score(Ph, Pt) = |Xh?Xt||Xt| .
We then incorporate paraphrase similarity within the
lexical similarity model by allowing, for some un-
aligned node h ? Ph, where t ? Pt:
sim(h, t) = max(MN(h, t), score(Ph, Pt))
38
Our approach to paraphrase detection is most similar
to the TE/ASE algorithm (Szpektor et al, 2004), and
bears similarity to both DIRT (Lin and Pantel, 2001)
and KnowItAll (Etzioni et al, 2004). The chief
difference in our algorithm is that we generate the
surface text search strings from the parsed logical
forms using the generation capabilities of NLPWIN
(Aikawa et al, 2001), and we verify that the syn-
tactic relations in each discovered web snippet are
isomorphic to those in the original candidate para-
phrase template.
6 Results and Discussion
In this section we present the final results of our sys-
tem on the PASCAL RTE-1 test set, and examine our
features in an ablation study. The PASCAL RTE-1
development and test sets consist of 567 and 800 ex-
amples, respectively, with the test set split equally
between true and false examples.
6.1 Results and Performance Comparison on
the PASCAL RTE-1 Test Set
Table 2 displays the accuracy and confidence-
weighted score6 (CWS) of our final system on each
of the tasks for both the development and test sets.
Our overall test set accuracy of 62.50% rep-
resents a 2.1% absolute improvement over the
task-independent system described in (Tatu and
Moldovan, 2005), and a 20.2% relative improve-
ment in accuracy over their system with respect to
an uninformed baseline accuracy of 50%.
To compute confidence scores for our judgments,
any entailment determined to be false by any heuris-
tic was assigned maximum confidence; no attempts
were made to distinguish between entailments re-
jected by different heuristics. The confidence of
all other predictions was calculated as the ab-
solute value in the difference between the output
score(H,T ) of the lexical similarity model and the
threshold t = 0.1285 as tuned for highest accu-
racy on our development set. We would expect a
higher CWS to result from learning a more appro-
priate confidence function; nonetheless our overall
6As in (Dagan et al, 2005) we compute the confidence-
weighted score (or ?average precision?) over n examples
{c1, c2, ..., cn} ranked in order of decreasing confidence as
cws = 1n
?n
i=1
(#correct-up-to-rank-i)
i
Dev Set Test Set
Task acc cws acc cws
CD 0.8061 0.8357 0.7867 0.8261
RC 0.5534 0.5885 0.6429 0.6476
IR 0.6857 0.6954 0.6000 0.6571
MT 0.7037 0.7145 0.6000 0.6350
IE 0.5857 0.6008 0.5917 0.6275
QA 0.7111 0.7121 0.5308 0.5463
PP 0.7683 0.7470 0.5200 0.5333
All 0.6878 0.6888 0.6250 0.6534
Table 2: Summary of accuracies and confidence-
weighted scores, by task
Alignment Feature Dev Test
Synonym Match 0.0106 0.0038
Derivational Form 0.0053 0.0025
Paraphrase 0.0053 0.0000
Lexical Similarity 0.0053 0.0000
Value Match 0.0017 0.0013
Acronym Match 0.0017 0.0013
Adjectival Form7 0.0000 0.0063
False Entailment Feature Dev Test
Negation Mismatch 0.0106 0.0025
Argument Movement 0.0070 0.0250
Conditional Mismatch 0.0053 0.0037
Modal Mismatch 0.0035 0.0013
Superlative Mismatch 0.0035 -0.0025
Entity Mismatch 0.0018 0.0063
Table 3: Feature ablation study; quantity is the ac-
curacy loss obtained by removal of single feature
test set CWS of 0.6534 is higher than previously-
reported task-independent systems (however, the
task-dependent system reported in (Raina et al,
2005) achieves a CWS of 0.686).
6.2 Feature analysis
Table 3 displays the results of our feature ablation
study, analyzing the individual effect of each feature.
Of the seven heuristics used in our final system
for node alignment (including lexical similarity and
paraphrase detection), our ablation study showed
7As discussed in Section 2, features with no effect on devel-
opment set accuracy were included in the system if and only if
they improved the system?s unweighted F-score.
39
that five were helpful in varying degrees on our test
set, but that removal of either MindNet similarity
scores or paraphrase detection resulted in no accu-
racy loss on the test set.
Of the six false entailment heuristics used in the
final system, five resulted in an accuracy improve-
ment on the test set (the most effective by far was
the ?Argument Movement?, resulting in a net gain
of 20 correctly-classified false examples); inclusion
of the ?Superlative Mismatch? feature resulted in a
small net loss of two examples.
We note that our heuristics for false entailment,
where applicable, were indeed significantly more ac-
curate than our final system as a whole; on the set of
examples predicted false by our heuristics we had
71.3% accuracy on the training set (112 correct out
of 157 predicted), and 72.9% accuracy on the test set
(164 correct out of 225 predicted).
7 Conclusion
In this paper we have presented and analyzed a sys-
tem for recognizing textual entailment focused pri-
marily on the recognition of false entailment, and
demonstrated higher performance than achieved by
previous approaches on the widely-used PASCAL
RTE test set. Our system achieves state-of-the-
art performance despite not exploiting a wide ar-
ray of sources of knowledge used by other high-
performance systems; we submit that the perfor-
mance of our system demonstrates the unexploited
potential in features designed specifically for the
recognition of false entailment.
Acknowledgments
We thank Chris Brockett, Michael Gamon, Gary
Kacmarick, and Chris Quirk for helpful discussion.
Also, thanks to Robert Ragno for assistance with
the MSN Search API. Rion Snow is supported by
an NDSEG Fellowship sponsored by the DOD and
AFOSR.
References
Takako Aikawa, Maite Melero, Lee Schwartz, and Andi
Wu. 2001. Multilingual Sentence Generation. In
Proc. of 8th European Workshop on Natural Language
Generation.
Samuel Bayer, John Burger, Lisa Ferro, John Henderson,
and Alexander Yeh. 2005. MITRE?s Submissions to
the EU Pascal RTE Challenge. In Proc. of the PASCAL
Challenges Workshop on RTE 2005.
Johan Bos and Katja Markert. 2005. Recognizing Tex-
tual Entailment with Logical Inference. In Proc. HLT-
EMNLP 2005.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In Proceedings of the PASCAL Challenges
Workshop on RTE 2005.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2004.
Web-scale information extraction in KnowItAll. In
Proc. WWW 2004.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
Web Based Probabilistic Textual Entailment. In Proc.
of the PASCAL Challenges Workshop on RTE 2005.
George E. Heidorn. 2000. Intelligent Writing Assis-
tance. In R. Dale, H. Moisl, and H. Somers (eds.),
A Handbook of Natural Language Processing: Tech-
niques and Applications for the Processing of Lan-
guage as Text. Marcel Dekker, New York. 181-207.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo. 2005.
Textual Entailment Recognision Based on Depen-
dency Analysis and WordNet. In Proc. of the PASCAL
Challenges Workshop on RTE 2005.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proc. KDD 2001.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proc. AAAI 2005.
Stephen D. Richardson. 1997. Determining Similarity
and Inferring Relations in a Lexical Knowledge Base.
Ph.D. thesis, The City University of New York.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura
Coppola. 2004. Scaling Web-based Acquisition of
Entailment Relations. In Proc. EMNLP 2004.
Marta Tatu and Dan Moldovan. 2005. A Semantic Ap-
proach to Recognizing Textual Entailment. In Proc.
HLT-EMNLP 2005.
Lucy Vanderwende and William B. Dolan. 2006. What
Syntax Can Contribute in the Entailment Task. In
MLCW 2005, LNAI 3944, pp. 205?216. J. Quinonero-
Candela et al (eds.). Springer-Verlag.
40
Proceedings of the 43rd Annual Meeting of the ACL, pages 271?279,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Dependency Treelet Translation: Syntactically Informed Phrasal SMT  
Chris Quirk, Arul Menezes Colin Cherry 
Microsoft Research University of Alberta 
One Microsoft Way Edmonton, Alberta 
Redmond, WA 98052 Canada T6G 2E1 
{chrisq,arulm}@microsoft.com colinc@cs.ualberta.ca 
 
Abstract 
We describe a novel approach to 
statistical machine translation that 
combines syntactic information in the 
source language with recent advances in 
phrasal translation. This method requires a 
source-language dependency parser, target 
language word segmentation and an 
unsupervised word alignment component. 
We align a parallel corpus, project the 
source dependency parse onto the target 
sentence, extract dependency treelet 
translation pairs, and train a tree-based 
ordering model. We describe an efficient 
decoder and show that using these tree-
based models in combination with 
conventional SMT models provides a 
promising approach that incorporates the 
power of phrasal SMT with the linguistic 
generality available in a parser.  
1. Introduction 
Over the past decade, we have witnessed a 
revolution in the field of machine translation 
(MT) toward statistical or corpus-based methods. 
Yet despite this success, statistical machine 
translation (SMT) has many hurdles to overcome. 
While it excels at translating domain-specific 
terminology and fixed phrases, grammatical 
generalizations are poorly captured and often 
mangled during translation (Thurmair, 04).  
1.1. Limitations of string-based phrasal SMT 
State-of-the-art phrasal SMT systems such as 
(Koehn et al, 03) and (Vogel et al, 03) model 
translations of phrases (here, strings of adjacent 
words, not syntactic constituents) rather than 
individual words. Arbitrary reordering of words is 
allowed within memorized phrases, but typically 
only a small amount of phrase reordering is 
allowed, modeled in terms of offset positions at 
the string level. This reordering model is very 
limited in terms of linguistic generalizations. For 
instance, when translating English to Japanese, an 
ideal system would automatically learn large-
scale typological differences: English SVO 
clauses generally become Japanese SOV clauses, 
English post-modifying prepositional phrases 
become Japanese pre-modifying postpositional 
phrases, etc. A phrasal SMT system may learn the 
internal reordering of specific common phrases, 
but it cannot generalize to unseen phrases that 
share the same linguistic structure. 
In addition, these systems are limited to 
phrases contiguous in both source and target, and 
thus cannot learn the generalization that English 
not may translate as French ne?pas except in the 
context of specific intervening words.  
1.2. Previous work on syntactic SMT1 
The hope in the SMT community has been that 
the incorporation of syntax would address these 
issues, but that promise has yet to be realized. 
One simple means of incorporating syntax into 
SMT is by re-ranking the n-best list of a baseline 
SMT system using various syntactic models, but 
Och et al (04) found very little positive impact 
with this approach. However, an n-best list of 
even 16,000 translations captures only a tiny 
fraction of the ordering possibilities of a 20 word 
sentence; re-ranking provides the syntactic model 
no opportunity to boost or prune large sections of 
that search space.  
Inversion Transduction Grammars (Wu, 97), or 
ITGs, treat translation as a process of parallel 
parsing of the source and target language via a 
synchronized grammar. To make this process 
                                                        
1
 Note that since this paper does not address the word alignment problem 
directly, we do not discuss the large body of work on incorporating syntactic 
information into the word alignment process. 
 
271
computationally efficient, however, some severe 
simplifying assumptions are made, such as using 
a single non-terminal label. This results in the 
model simply learning a very high level 
preference regarding how often nodes should 
switch order without any contextual information. 
Also these translation models are intrinsically 
word-based; phrasal combinations are not 
modeled directly, and results have not been 
competitive with the top phrasal SMT systems.  
Along similar lines, Alshawi et al (2000) treat 
translation as a process of simultaneous induction 
of source and target dependency trees using head-
transduction; again, no separate parser is used. 
Yamada and Knight (01) employ a parser in the 
target language to train probabilities on a set of 
operations that convert a target language tree to a 
source language string. This improves fluency 
slightly (Charniak et al, 03), but fails to 
significantly impact overall translation quality. 
This may be because the parser is applied to MT 
output, which is notoriously unlike native 
language, and no additional insight is gained via 
source language analysis.  
Lin (04) translates dependency trees using 
paths. This is the first attempt to incorporate large 
phrasal SMT-style memorized patterns together 
with a separate source dependency parser and 
SMT models. However the phrases are limited to 
linear paths in the tree, the only SMT model used 
is a maximum likelihood channel model and there 
is no ordering model. Reported BLEU scores are 
far below the leading phrasal SMT systems. 
MSR-MT (Menezes & Richardson, 01) parses 
both source and target languages to obtain a 
logical form (LF), and translates source LFs using 
memorized aligned LF patterns to produce a 
target LF. It utilizes a separate sentence 
realization component (Ringger et al, 04) to turn 
this into a target sentence. As such, it does not use 
a target language model during decoding, relying 
instead on MLE channel probabilities and 
heuristics such as pattern size. Recently Aue et al 
(04) incorporated an LF-based language model 
(LM) into the system for a small quality boost. A 
key disadvantage of this approach and related 
work (Ding & Palmer, 02) is that it requires a 
parser in both languages, which severely limits 
the language pairs that can be addressed. 
2. Dependency Treelet Translation 
In this paper we propose a novel dependency tree-
based approach to phrasal SMT which uses tree-
based ?phrases? and a tree-based ordering model 
in combination with conventional SMT models to 
produce state-of-the-art translations.  
Our system employs a source-language 
dependency parser, a target language word 
segmentation component, and an unsupervised 
word alignment component to learn treelet 
translations from a parallel sentence-aligned 
corpus. We begin by parsing the source text to 
obtain dependency trees and word-segmenting the 
target side, then applying an off-the-shelf word 
alignment component to the bitext.  
The word alignments are used to project the 
source dependency parses onto the target 
sentences. From this aligned parallel dependency 
corpus we extract a treelet translation model 
incorporating source and target treelet pairs, 
where a treelet is defined to be an arbitrary 
connected subgraph of the dependency tree. A 
unique feature is that we allow treelets with a 
wildcard root, effectively allowing mappings for 
siblings in the dependency tree. This allows us to 
model important phenomena, such as not ?   
ne?pas. We also train a variety of statistical 
models on this aligned dependency tree corpus, 
including a channel model and an order model.  
To translate an input sentence, we parse the 
sentence, producing a dependency tree for that 
sentence. We then employ a decoder to find a 
combination and ordering of treelet translation 
pairs that cover the source tree and are optimal 
according to a set of models that are combined in 
a log-linear framework as in (Och, 03).  
This approach offers the following advantages 
over string-based SMT systems: Instead of 
limiting learned phrases to contiguous word 
sequences, we allow translation by all possible 
phrases that form connected subgraphs (treelets) 
in the source and target dependency trees. This is 
a powerful extension: the vast majority of 
surface-contiguous phrases are also treelets of the 
tree; in addition, we gain discontiguous phrases, 
including combinations such as verb-object, 
article-noun, adjective-noun etc. regardless of the 
number of intervening words. 
272
Another major advantage is the ability to 
employ more powerful models for reordering 
source language constituents. These models can 
incorporate information from the source analysis. 
For example, we may model directly the 
probability that the translation of an object of a 
preposition in English should precede the 
corresponding postposition in Japanese, or the 
probability that a pre-modifying adjective in 
English translates into a post-modifier in French. 
2.1. Parsing and alignment 
We require a source language dependency parser 
that produces unlabeled, ordered dependency 
trees and annotates each source word with a part-
of-speech (POS). An example dependency tree is 
shown in Figure 1. The arrows indicate the head 
annotation, and the POS for each candidate is 
listed underneath. For the target language we only 
require word segmentation.  
To obtain word alignments we currently use 
GIZA++ (Och & Ney, 03). We follow the 
common practice of deriving many-to-many 
alignments by running the IBM models in both 
directions and combining the results heuristically. 
Our heuristics differ in that they constrain many-
to-one alignments to be contiguous in the source 
dependency tree. A detailed description of these 
heuristics can be found in Quirk et al (04).  
2.2. Projecting dependency trees 
Given a word aligned sentence pair and a source 
dependency tree, we use the alignment to project 
the source structure onto the target sentence. One-
to-one alignments project directly to create a 
target tree isomorphic to the source. Many-to-one 
alignments project similarly; since the ?many? 
source nodes are connected in the tree, they act as 
if condensed into a single node. In the case of 
one-to-many alignments we project the source 
node to the rightmost2 of the ?many? target words, 
and make the rest of the target words dependent 
on it. 
                                                        
2
 If the target language is Japanese, leftmost may be more appropriate. 
Unaligned target words3 are attached into the 
dependency structure as follows: assume there is 
an unaligned word tj in position j. Let i < j and k 
> j be the target positions closest to j such that ti 
depends on tk or vice versa: attach tj to the lower 
of ti or tk. If all the nodes to the left (or right) of 
position j are unaligned, attach tj to the left-most 
(or right-most) word that is aligned. 
The target dependency tree created in this 
process may not read off in the same order as the 
target string, since our alignments do not enforce 
phrasal cohesion. For instance, consider the 
projection of the parse in Figure 1 using the word 
alignment in Figure 2a. Our algorithm produces 
the dependency tree in Figure 2b. If we read off 
the leaves in a left-to-right in-order traversal, we 
do not get the original input string: de d?marrage 
appears in the wrong place. 
A second reattachment pass corrects this 
situation. For each node in the wrong order, we 
reattach it to the lowest of its ancestors such that 
it is in the correct place relative to its siblings and 
parent. In Figure 2c, reattaching d?marrage to et 
suffices to produce the correct order.  
                                                        
3
 Source unaligned nodes do not present a problem, with the exception that if 
the root is unaligned, the projection process produces a forest of target trees 
anchored by a dummy root.  
startup properties and options
Noun Noun Conj Noun
 
Figure 1. An example dependency tree. 
startup properties and options
propri?t?s et options de d?marrage
 
(a) Word alignment. 
 
 
startup properties and options
propri?t?s de d?marrage et options
 
 
 (b) Dependencies after initial projection. 
 
 
startup properties and options
propri?t?s et options de d?marrage
 
(c) Dependencies after reattachment step. 
 
Figure 2. Projection of dependencies. 
273
2.3. Extracting treelet translation pairs 
From the aligned pairs of dependency trees we 
extract all pairs of aligned source and target 
treelets along with word-level alignment linkages, 
up to a configurable maximum size. We also keep 
treelet counts for maximum likelihood estimation.  
2.4. Order model 
Phrasal SMT systems often use a model to score 
the ordering of a set of phrases. One approach is 
to penalize any deviation from monotone 
decoding; another is to estimate the probability 
that a source phrase in position i translates to a 
target phrase in position j (Koehn et al, 03). 
We attempt to improve on these approaches by 
incorporating syntactic information. Our model 
assigns a probability to the order of a target tree 
given a source tree. Under the assumption that 
constituents generally move as a whole, we 
predict the probability of each given ordering of 
modifiers independently. That is, we make the 
following simplifying assumption (where c is a 
function returning the set of nodes modifying t): 
?
?
=
Tt
TStcorderTSTorder ),|))((P(),|)(P(
 
Furthermore, we assume that the position of each 
child can be modeled independently in terms of a 
head-relative position: 
),|),(P(),|))((P(
)(
TStmposTStcorder
tcm
?
?
=  
Figure 3a demonstrates an aligned dependency 
tree pair annotated with head-relative positions; 
Figure 3b presents the same information in an 
alternate tree-like representation. 
We currently use a small set of features 
reflecting very local information in the 
dependency tree to model P(pos(m,t) | S, T): 
? The lexical items of the head and modifier. 
? The lexical items of the source nodes aligned 
to the head and modifier. 
? The part-of-speech ("cat") of the source nodes 
aligned to the head and modifier. 
? The head-relative position of the source node 
aligned to the source modifier. 4 
As an example, consider the children of 
propri?t? in Figure 3. The head-relative positions 
                                                        
4
 One can also include features of siblings to produce a Markov ordering 
model. However, we found that this had little impact in practice. 
of its modifiers la and Cancel are -1 and +1, 
respectively. Thus we try to predict as follows: 
P(pos(m1) = -1 | 
lex(m1)="la", lex(h)="propri?t?", 
lex(src(m1))="the", lex(src(h)="property", 
cat(src(m1))=Determiner, cat(src(h))=Noun, 
position(src(m1))=-2) ? 
P(pos(m2) = +1 | 
lex(m2)="Cancel", lex(h)="propri?t?", 
lex(src(m2))="Cancel", lex(src(h))="property", 
cat(src(m2))=Noun, cat(src(h))=Noun, 
position(src(m2))=-1) 
The training corpus acts as a supervised training 
set: we extract a training feature vector from each 
of the target language nodes in the aligned 
dependency tree pairs. Together these feature 
vectors are used to train a decision tree 
(Chickering, 02). The distribution at each leaf of 
the DT can be used to assign a probability to each 
possible target language position. A more detailed 
description is available in (Quirk et al, 04). 
2.5. Other models 
Channel Models: We incorporate two distinct 
channel models, a maximum likelihood estimate 
(MLE) model and a model computed using 
Model-1 word-to-word alignment probabilities as 
in (Vogel et al, 03). The MLE model effectively 
captures non-literal phrasal translations such as 
idioms, but suffers from data sparsity. The word-
the-2 Cancel-1 property-1 uses these-1 settings+1
la-1 propri?t?-1 Cancel+1 utilise ces-1 param?tres+1
 
(a) Head annotation representation 
 
uses
property-1              settings+1
the-2 Cancel-1                 these-1
la-1             Cancel+1         ces-1
propri?t?-1                        param?tres+1
utilise
 
(b) Branching structure representation. 
 
Figure 3.  Aligned dependency tree pair, annotated with 
head-relative positions 
274
to-word model does not typically suffer from data 
sparsity, but prefers more literal translations.  
Given a set of treelet translation pairs that 
cover a given input dependency tree and produce 
a target dependency tree, we model the 
probability of source given target as the product 
of the individual treelet translation probabilities: 
we assume a uniform probability distribution over 
the decompositions of a tree into treelets.  
Target Model: Given an ordered target language 
dependency tree, it is trivial to read off the surface 
string. We evaluate this string using a trigram 
model with modified Kneser-Ney smoothing.  
Miscellaneous Feature Functions: The log-linear 
framework allows us to incorporate other feature 
functions as ?models? in the translation process. 
For instance, using fewer, larger treelet translation 
pairs often provides better translations, since they 
capture more context and allow fewer possibilities 
for search and model error. Therefore we add a 
feature function that counts the number of phrases 
used. We also add a feature that counts the 
number of target words; this acts as an 
insertion/deletion bonus/penalty.  
3. Decoding 
The challenge of tree-based decoding is that the 
traditional left-to-right decoding approach of 
string-based systems is inapplicable. Additional 
challenges are posed by the need to handle 
treelets?perhaps discontiguous or overlapping?
and a combinatorially explosive ordering space.  
Our decoding approach is influenced by ITG 
(Wu, 97) with several important extensions. First, 
we employ treelet translation pairs instead of 
single word translations. Second, instead of 
modeling rearrangements as either preserving 
source order or swapping source order, we allow 
the dependents of a node to be ordered in any 
arbitrary manner and use the order model 
described in section 2.4 to estimate probabilities. 
Finally, we use a log-linear framework for model 
combination that allows any amount of other 
information to be modeled.  
We will initially approach the decoding 
problem as a bottom up, exhaustive search. We 
define the set of all possible treelet translation 
pairs of the subtree rooted at each input node in 
the following manner: A treelet translation pair x 
is said to match the input dependency tree S iff 
there is some connected subgraph S? that is 
identical to the source side of x. We say that x 
covers all the nodes in S? and is rooted at source 
node s, where s is the root of matched subgraph 
S?.  
We first find all treelet translation pairs that 
match the input dependency tree. Each matched 
pair is placed on a list associated with the input 
node where the match is rooted. Moving bottom-
up through the input dependency tree, we 
compute a list of candidate translations for the 
input subtree rooted at each node s, as follows:  
Consider in turn each treelet translation pair x 
rooted at s. The treelet pair x may cover only a 
portion of the input subtree rooted at s. Find all 
descendents s' of s that are not covered by x, but 
whose parent s'' is covered by x. At each such 
node s'' look at all interleavings of the children of 
s'' specified by x, if any, with each translation t' 
from the candidate translation list5 of each child 
s'. Each such interleaving is scored using the 
models previously described and added to the 
candidate translation list for that input node. The 
resultant translation is the best scoring candidate 
for the root input node. 
As an example, see the example dependency 
tree in Figure 4a and treelet translation pair in 4b. 
This treelet translation pair covers all the nodes in 
4a except the subtrees rooted at software and is. 
                                                        
5
 Computed by the previous application of this procedure to s' during the 
bottom-up traversal. 
installed
software is on
the computer
your
 
 (a) Example input dependency tree. 
installed
on
computer
your
votre
ordinateur
sur
install?s
 
(b) Example treelet translation pair. 
 
Figure 4.  Example decoder structures. 
275
We first compute (and cache) the candidate 
translation lists for the subtrees rooted at software 
and is, then construct full translation candidates 
by attaching those subtree translations to install?s 
in all possible ways. The order of sur relative to 
install?s is fixed; it remains to place the translated 
subtrees for the software and is. Note that if c is 
the count of children specified in the mapping and 
r is the count of subtrees translated via recursive 
calls, then there are (c+r+1)!/(c+1)! orderings. 
Thus (1+2+1)!/(1+1)! = 12 candidate translations 
are produced for each combination of translations 
of the software and is. 
3.1. Optimality-preserving optimizations 
Dynamic Programming 
Converting this exhaustive search to dynamic 
programming relies on the observation that 
scoring a translation candidate at a node depends 
on the following information from its 
descendents: the order model requires features 
from the root of a translated subtree, and the 
target language model is affected by the first and 
last two words in each subtree. Therefore, we 
need to keep the best scoring translation candidate 
for a given subtree for each combination of (head, 
leading bigram, trailing bigram), which is, in the 
worst case, O(V5), where V is the vocabulary size. 
The dynamic programming approach therefore 
does not allow for great savings in practice 
because a trigram target language model forces 
consideration of context external to each subtree.  
Duplicate elimination 
To eliminate unnecessary ordering operations, we 
first check that a given set of words has not been 
previously ordered by the decoder. We use an 
order-independent hash table where two trees are 
considered equal if they have the same tree 
structure and lexical choices after sorting each 
child list into a canonical order. A simpler 
alternate approach would be to compare bags-of-
words. However since our possible orderings are 
bound by the induced tree structure, we might 
overzealously prune a candidate with a different 
tree structure that allows a better target order.  
3.2. Lossy optimizations 
The following optimizations do not preserve 
optimality, but work well in practice. 
N-best lists 
Instead of keeping the full list of translation 
candidates for a given input node, we keep a top-
scoring subset of the candidates. While the 
decoder is no longer guaranteed to find the 
optimal translation, in practice the quality impact 
is minimal with a list size ? 10 (see Table 5.6).  
Variable-sized n-best lists: A further speedup 
can be obtained by noting that the number of 
translations using a given treelet pair is 
exponential in the number of subtrees of the input 
not covered by that pair. To limit this explosion 
we vary the size of the n-best list on any recursive 
call in inverse proportion to the number of 
subtrees uncovered by the current treelet. This has 
the intuitive appeal of allowing a more thorough 
exploration of large treelet translation pairs (that 
are likely to result in better translations) than of 
smaller, less promising pairs.  
Pruning treelet translation pairs 
Channel model scores and treelet size are 
powerful predictors of translation quality. 
Heuristically pruning low scoring treelet 
translation pairs before the search starts allows 
the decoder to focus on combinations and 
orderings of high quality treelet pairs.  
? Only keep those treelet translation pairs with 
an MLE probability above a threshold t. 
? Given a set of treelet translation pairs with 
identical sources, keep those with an MLE 
probability within a ratio r of the best pair.  
? At each input node, keep only the top k treelet 
translation pairs rooted at that node, as ranked 
first by size, then by MLE channel model 
score, then by Model 1 score. The impact of 
this optimization is explored in Table 5.6.  
Greedy ordering 
The complexity of the ordering step at each node 
grows with the factorial of the number of children 
to be ordered. This can be tamed by noting that 
given a fixed pre- and post-modifier count, our 
order model is capable of evaluating a single 
ordering decision independently from other 
ordering decisions. 
One version of the decoder takes advantage of 
this to severely limit the number of ordering 
possibilities considered. Instead of considering all 
interleavings, it considers each potential modifier 
position in turn, greedily picking the most 
276
probable child for that slot, moving on to the next 
slot, picking the most probable among the 
remaining children for that slot and so on. 
The complexity of greedy ordering is linear, 
but at the cost of a noticeable drop in BLEU score 
(see Table 5.4). Under default settings our system 
tries to decode a sentence with exhaustive 
ordering until a specified timeout, at which point 
it falls back to greedy ordering. 
4. Experiments 
We evaluated the translation quality of the system 
using the BLEU metric (Papineni et al, 02) under 
a variety of configurations. We compared against 
two radically different types of systems to 
demonstrate the competitiveness of this approach:  
? Pharaoh: A leading phrasal SMT decoder 
(Koehn et al, 03). 
? The MSR-MT system described in Section 1, 
an EBMT/hybrid MT system.  
4.1. Data 
We used a parallel English-French corpus 
containing 1.5 million sentences of Microsoft 
technical data (e.g., support articles, product 
documentation). We selected a cleaner subset of 
this data by eliminating sentences with XML or 
HTML tags as well as very long (>160 characters) 
and very short (<40 characters) sentences. We 
held out 2,000 sentences for development testing 
and parameter tuning, 10,000 sentences for 
testing, and 250 sentences for lambda training. 
We ran experiments on subsets of the training 
data ranging from 1,000 to 300,000 sentences. 
Table 4.1 presents details about this dataset. 
4.2. Training 
We parsed the source (English) side of the corpus 
using NLPWIN, a broad-coverage rule-based 
parser developed at Microsoft Research able to 
produce syntactic analyses at varying levels of 
depth (Heidorn, 02). For the purposes of these 
experiments we used a dependency tree output 
with part-of-speech tags and unstemmed surface 
words.  
For word alignment, we used GIZA++, 
following a standard training regimen of five 
iterations of Model 1, five iterations of the HMM 
Model, and five iterations of Model 4, in both 
directions.  
We then projected the dependency trees and 
used the aligned dependency tree pairs to extract 
treelet translation pairs and train the order model 
as described above. The target language model 
was trained using only the French side of the 
corpus; additional data may improve its 
performance. Finally we trained lambdas via 
Maximum BLEU (Och, 03) on 250 held-out 
sentences with a single reference translation, and 
tuned the decoder optimization parameters (n-best 
list size, timeouts etc) on the development test set. 
Pharaoh 
The same GIZA++ alignments as above were 
used in the Pharaoh decoder. We used the 
heuristic combination described in (Och & Ney, 
03) and extracted phrasal translation pairs from 
this combined alignment as described in (Koehn 
et al, 03). Except for the order model (Pharaoh 
uses its own ordering approach), the same models 
were used: MLE channel model, Model 1 channel 
model, target language model, phrase count, and 
word count. Lambdas were trained in the same 
manner (Och, 03). 
MSR-MT 
MSR-MT used its own word alignment approach 
as described in (Menezes & Richardson, 01) on 
the same training data. MSR-MT does not use 
lambdas or a target language model. 
5. Results 
We present BLEU scores on an unseen 10,000 
sentence test set using a single reference 
translation for each sentence. Speed numbers are 
the end-to-end translation speed in sentences per 
minute. All results are based on a training set size 
of 100,000 sentences and a phrase size of 4, 
except Table 5.2 which varies the phrase size and 
Table 5.3 which varies the training set size. 
  English French 
Training Sentences 570,562 
 Words 7,327,251 8,415,882 
 Vocabulary 72,440 80,758 
 Singletons 38,037 39,496 
Test Sentences 10,000 
 Words 133,402 153,701 
Table 4.1 Data characteristics 
277
Results for our system and the comparison 
systems are presented in Table 5.1. Pharaoh 
monotone refers to Pharaoh with phrase 
reordering disabled. The difference between 
Pharaoh and the Treelet system is significant at 
the 99% confidence level under a two-tailed 
paired t-test. 
 BLEU Score Sents/min 
Pharaoh monotone 37.06 4286 
Pharaoh 38.83 162 
MSR-MT 35.26 453 
Treelet 40.66 10.1 
Table 5.1 System comparisons  
Table 5.2 compares Pharaoh and the Treelet 
system at different phrase sizes. While all the 
differences are statistically significant at the 99% 
confidence level, the wide gap at smaller phrase 
sizes is particularly striking. We infer that 
whereas Pharaoh depends heavily on long phrases 
to encapsulate reordering, our dependency tree-
based ordering model enables credible 
performance even with single-word ?phrases?. We 
conjecture that in a language pair with large-scale 
ordering differences, such as English-Japanese, 
even long phrases are unlikely to capture the 
necessary reorderings, whereas our tree-based 
ordering model may prove more robust. 
Max. size Treelet BLEU Pharaoh BLEU 
1  37.50 23.18  
2 39.84 32.07  
3 40.36 37.09  
4 (default) 40.66 38.83  
5 40.71 39.41  
6 40.74 39.72  
Table 5.2 Effect of maximum treelet/phrase size 
Table 5.3 compares the same systems at different 
training corpus sizes. All of the differences are 
statistically significant at the 99% confidence 
level. Noting that the gap widens at smaller 
corpus sizes, we suggest that our tree-based 
approach is more suitable than string-based 
phrasal SMT when translating from English into 
languages or domains with limited parallel data. 
We also ran experiments varying different 
system parameters. Table 5.4 explores different 
ordering strategies, Table 5.5 looks at the impact 
of discontiguous phrases and Table 5.6 looks at 
the impact of decoder optimizations such as 
treelet pruning and n-best list size. 
Ordering strategy BLEU  Sents/min  
No order model (monotone) 35.35 39.7 
Greedy ordering 38.85 13.1 
Exhaustive (default) 40.66 10.1 
Table 5.4 Effect of ordering strategies 
 BLEU Score  Sents/min 
Contiguous only 40.08  11.0 
Allow discontiguous 40.66 10.1 
Table 5.5 Effect of allowing treelets that correspond to 
discontiguous phrases 
 BLEU Score  Sents/min  
Pruning treelets   
  Keep top 1 28.58  144.9 
  ? top 3 39.10 21.2 
  ? top 5 40.29 14.6 
  ? top 10 (default) 40.66 10.1 
  ? top 20 40.70 3.5 
  Keep all 40.29 3.2 
N-best list size    
  1-best 37.28 175.4 
  5-best 39.96 79.4 
  10-best 40.42 23.3 
  20-best (default) 40.66 10.1 
  50-best 39.39 3.7 
Table 5.6 Effect of optimizations  
6. Discussion  
We presented a novel approach to syntactically-
informed statistical machine translation that 
leverages a parsed dependency tree representation 
of the source language via a tree-based ordering 
model and treelet phrase extraction. We showed 
that it significantly outperforms a leading phrasal 
SMT system over a wide range of training set 
sizes and phrase sizes. 
Constituents vs. dependencies: Most attempts at 
 1k 3k 10k 30k 100k 300k 
Pharaoh 17.20  22.51  27.70  33.73  38.83  42.75  
Treelet 18.70 25.39 30.96 35.81 40.66 44.32 
Table 5.3 Effect of training set size on treelet translation and comparison system  
 
278
syntactic SMT have relied on a constituency 
analysis rather than dependency analysis. While 
this is a natural starting point due to its well-
understood nature and commonly available tools, 
we feel that this is not the most effective 
representation for syntax in MT. Dependency 
analysis, in contrast to constituency analysis, 
tends to bring semantically related elements 
together (e.g., verbs become adjacent to all their 
arguments) and is better suited to lexicalized 
models, such as the ones presented in this paper.  
7. Future work 
The most important contribution of our system is 
a linguistically motivated ordering approach 
based on the source dependency tree, yet this 
paper only explores one possible model. Different 
model structures, machine learning techniques, 
and target feature representations all have the 
potential for significant improvements.  
Currently we only consider the top parse of an 
input sentence. One means of considering 
alternate possibilities is to build a packed forest of 
dependency trees and use this in decoding 
translations of each input sentence. 
As noted above, our approach shows particular 
promise for language pairs such as English-
Japanese that exhibit large-scale reordering and 
have proven difficult for string-based approaches. 
Further experimentation with such language pairs 
is necessary to confirm this. Our experience has 
been that the quality of GIZA++ alignments for 
such language pairs is inadequate. Following up 
on ideas introduced by (Cherry & Lin, 03) we 
plan to explore ways to leverage the dependency 
tree to improve alignment quality.  
References 
Alshawi, Hiyan, Srinivas Bangalore, and Shona 
Douglas. Learning dependency translation models 
as collections of finite-state head transducers. 
Computational Linguistics, 26(1):45?60, 2000. 
Aue, Anthony, Arul Menezes, Robert C. Moore, Chris 
Quirk, and Eric Ringger. Statistical machine 
translation using labeled semantic dependency 
graphs. TMI 2004. 
Charniak, Eugene, Kevin Knight, and Kenji Yamada. 
Syntax-based language models for statistical 
machine translation. MT Summit 2003. 
Cherry, Colin and Dekang Lin. A probability model to 
improve word alignment. ACL 2003. 
Chickering, David Maxwell. The WinMine Toolkit. 
Microsoft Research Technical Report: MSR-TR-
2002-103. 
Ding, Yuan and Martha Palmer. Automatic learning of 
parallel dependency treelet pairs. IJCNLP 2004. 
Heidorn, George. (2000). ?Intelligent writing 
assistance?. In Dale et al Handbook of Natural 
Language Processing, Marcel Dekker. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase based translation. NAACL 2003. 
Lin, Dekang. A path-based transfer model for machine 
translation. COLING 2004. 
 Menezes, Arul and Stephen D. Richardson. A best-
first alignment algorithm for automatic extraction of 
transfer mappings from bilingual corpora. DDMT 
Workshop, ACL 2001. 
Och, Franz Josef and Hermann Ney. A systematic 
comparison of various statistical alignment models, 
Computational Linguistics, 29(1):19-51, 2003.  
Och, Franz Josef. Minimum error rate training in 
statistical machine translation. ACL 2003. 
Och, Franz Josef, et al A smorgasbord of features for 
statistical machine translation. HLT/NAACL 2004. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. BLEU: a method for automatic 
evaluation of machine translation. ACL 2002. 
Quirk, Chris, Arul Menezes, and Colin Cherry. 
Dependency Tree Translation. Microsoft Research 
Technical Report: MSR-TR-2004-113. 
Ringger, Eric, et al Linguistically informed statistical 
models of constituent structure for ordering in 
sentence realization. COLING 2004. 
 Thurmair, Gregor. Comparing rule-based and 
statistical MT output. Workshop on the amazing 
utility of parallel and comparable corpora, LREC, 
2004. 
 Vogel, Stephan, Ying Zhang, Fei Huang, Alicia 
Tribble, Ashish Venugopal, Bing Zhao, and Alex 
Waibel. The CMU statistical machine translation 
system. MT Summit 2003. 
Wu, Dekai. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377?403, 1997. 
Yamada, Kenji and Kevin Knight. A syntax-based 
statistical translation model. ACL, 2001. 
279
Overcoming the customization bottleneck using example-based MT
Stephen D. Richardson, William B. Dolan, Arul Menezes, Monica Corston-Oliver?
Microsoft Research ?Butler Hill Group
One Microsoft Way 4610 Wallingford Ave. N.
Redmond, WA 98052 Seattle WA 98103
{steveri, billdol, arulm}@microsoft.com moco@butlerhill.com
Abstract
We describe MSR-MT, a large-scale
hybrid machine translation system
under development for several
language pairs. This system?s ability to
acquire its primary translation
knowledge automatically by parsing a
bilingual corpus of hundreds of
thousands of sentence pairs and
aligning resulting logical forms
demonstrates true promise for
overcoming the so-called MT
customization bottleneck. Trained on
English and Spanish technical prose, a
blind evaluation shows that MSR-MT?s
integration of rule-based parsers,
example based processing, and
statistical techniques produces
translations whose quality exceeds that
of uncustomized commercial MT
systems in this domain.
1 Introduction
Commercially available machine translation
(MT) systems have long been limited in their
cost effectiveness and overall utility by the need
for domain customization. Such customization
typically includes identifying relevant
terminology (esp. multi-word collocations),
entering this terminology into system lexicons,
and making additional tweaks to handle
formatting and even some syntactic
idiosyncrasies. One of the goals of data-driven
MT research has been to overcome this
customization bottleneck through automated or
semi-automated extraction of translation
knowledge from bilingual corpora.
To address this bottleneck, a variety of
example based machine translation (EBMT)
systems have been created and described in the
literature. Some of these employ parsers to
produce dependency structures for the sentence
pairs in aligned bilingual corpora, which are
then aligned to obtain transfer rules or examples
(Meyers et al 2000; Watanabe et al 2000).
Other systems extract and use examples that are
represented as linear patterns of varying
complexity (Brown 1999; Watanabe and Takeda
1998; Turcato et al 1999).
For some EBMT systems, substantial
collections of examples are also manually
crafted or at least reviewed for correctness after
being identified automatically (Watanabe et al
2000; Brown 1999; Franz et al 2000). The
efforts that report accuracy results for fully
automatic example extraction (Meyers et al
2000; Watanabe et al 2000) do so for very
modest amounts of training data (a few thousand
sentence pairs). Previous work in this area thus
raises the possibility that manual review or
crafting is required to obtain example bases of
sufficient coverage and accuracy to be truly
useful.
Other variations of EBMT systems are
hybrids that integrate an EBMT component as
one of multiple sources of transfer knowledge
(in addition to other transfer rule or knowledge
based components) used during translation
(Frederking et al 1994; Takeda et al 1992).
To our knowledge, commercial quality MT
has so far been achieved only through years of
effort in creating hand-coded transfer rules.
Systems whose primary source of translation
knowledge comes from an automatically created
example base have not been shown capable of
matching or exceeding the quality of
commercial systems.
This paper reports on MSR-MT, an MT
system that attempts to break the customization
bottleneck by exploiting example-based (and
some statistical) techniques to automatically
acquire its primary translation knowledge from a
bilingual corpus of several million words. The
system leverages the linguistic generality of
existing rule-based parsers to enable broad
coverage and to overcome some of the
limitations on locality of context characteristic
of data-driven approaches. The ability of MSR-
MT to adapt automatically to a particular
domain, and to produce reasonable translations
for that domain, is validated through a blind
assessment by human evaluators. The quality of
MSR-MT?s output in this one domain is shown
to exceed the output quality of two highly rated
(though not domain-customized) commercially
available MT systems.
We believe that this demonstration is the first
in the literature to show that automatic training
methods can produce a commercially viable
level of translation quality.
2 MSR-MT
MSR-MT is a data-driven hybrid MT system,
combining rule-based analysis and generation
components with example-based transfer. The
automatic alignment procedure used to create
the example base relies on the same parser
employed during analysis and also makes use of
its own small set of rules for determining
permissible alignments. Moderately sized
bilingual dictionaries, containing only word
pairs and their parts of speech, provide
translation candidates for the alignment
procedure and are also used as a backup source
of translations during transfer. Statistical
techniques supply additional translation pair
candidates for alignment and identify certain
multi-word terms for parsing and transfer.
The robust, broad-coverage parsers used by
MSR-MT were created originally for
monolingual applications and have been used in
commercial grammar checkers.1 These parsers
produce a logical form (LF) representation that
is compatible across multiple languages (see
section 3 below). Parsers now exist for seven
languages (English, French, German, Spanish,
Chinese, Japanese, and Korean), and active
development continues to improve their
accuracy and coverage.
1 Parsers for English, Spanish, French, and German
provide linguistic analyses for the grammar checker
in Microsoft Word.
Figure 1. MSR-MT architecture.
Generation components are currently being
developed for English, Spanish, Chinese, and
Japanese. Given the automated learning
techniques used to create MSR-MT transfer
components, it should theoretically be possible,
provided with appropriate aligned bilingual
corpora, to create MT systems for any language
pair for which we have the necessary parsing
and generation components. In practice, we
have thus far created systems that translate into
English from all other languages and that
translate from English to Spanish, Chinese, and
Japanese. We have experimented only
preliminarily with Korean and Chinese to
Japanese.
Results from our Spanish-English and
English-Spanish systems are reported at the end
of this paper. The bilingual corpus used to
produce these systems comes from Microsoft
manuals and help text. The sentence alignment
of this corpus is the result of using a commercial
translation memory (TM) tool during the
translation process.
The architecture of MSR-MT is presented in
Figure 1. During the training phase, source and
target sentences from the aligned bilingual
corpus are parsed to produce corresponding LFs.
The normalized word forms resulting from
parsing are also fed to a statistical word
association learner (described in section 4.1),
which outputs learned single word translation
pairs as well as a special class of multi-word
pairs. The LFs are then aligned with the aid of
translations from a bilingual dictionary and the
learned single word pairs (section 4.2). Transfer
mappings that result from LF alignment, in the
form of linked source and target LF segments,
are stored in a special repository known as
MindNet (section 4.3). Additionally, the learned
multi-word pairs are added to the bilingual
dictionary for possible backup use during
translation and to the main parsing lexicon to
improve parse quality in certain cases.
At runtime, MSR-MT?s analysis parses
source sentences with the same parser used for
source text during the training phase (section
5.1). The resulting LFs then undergo a process
known as MindMeld, which matches them
against the LF transfer mappings stored in
MindNet (section 5.2). MindMeld also links
segments of source LFs with corresponding
target LF segments stored in MindNet. These
target LF segments are stitched together into a
single target LF during transfer, and any
translations for words or phrases not found
during MindMeld are searched for in the
updated bilingual dictionary and inserted in the
target LF (section 5.3). Generation receives the
target LF as input, from which it produces a
target sentence (section 5.4).
3 Logical form
MSR-MT?s broad-coverage parsers produce
conventional phrase structure analyses
augmented with grammatical relations (Heidorn
et al 2000). Syntactic analyses undergo further
processing in order to derive logical forms
(LFs), which are graph structures that describe
labeled dependencies among content words in
the original input. LFs normalize certain
syntactic alternations (e.g. active/passive) and
resolve both intrasentential anaphora and long-
distance dependencies.
MT has proven to be an excellent application
for driving the development of our LF
representation. The code that builds LFs from
syntactic analyses is shared across all seven of
the languages under development. This shared
architecture greatly simplifies the task of
aligning LF segments (section 4.2) from
different languages, since superficially distinct
constructions in two languages frequently
collapse onto similar or identical LF
representations. Even when two aligned
sentences produce divergent LFs, the alignment
and generation components can count on a
consistent interpretation of the representational
machinery used to build the two. Thus the
meaning of the relation Topic, for instance, is
consistent across all seven languages, although
its surface realizations in the various languages
vary dramatically.
4 Training MSR-MT
This section describes the two primary
mechanisms used by MSR-MT to automatically
extract translation mappings from parallel
corpora and the repository in which they are
stored.
4.1 Statistical learning of single word-
and multi-word associations
The software domain that has been our
primary research focus contains many words
and phrases that are not included in our general-
domain lexicons. Identifying translation
correspondences between these unknown words
and phrases across an aligned dataset can
provide crucial lexical anchors for the alignment
algorithm described in section 4.2.
In order to identify these associations, source
and target text are first parsed, and normalized
word forms (lemmas) are extracted. In the
multi-word case, English ?captoid? processing is
exploited to identify sequences of related,
capitalized words. Both single word and multi-
word associations are iteratively hypothesized
and scored by the algorithm under certain
constraints until a reliable set of each is
obtained.
Over the English/Spanish bilingual corpus
used for the present work, 9,563 single word and
4,884 multi-word associations not already
known to our system were identified using this
method.
Moore (2001) describes this technique in
detail, while Pinkham & Corston-Oliver (2001)
describes its integration with MSR-MT and
investigates its effect on translation quality.
4.2 Logical form alignment
As described in section 2, MSR-MT acquires
transfer mappings by aligning pairs of LFs
obtained from parsing sentence pairs in a
bilingual corpus. The LF alignment algorithm
first establishes tentative lexical
correspondences between nodes in the source
and target LFs using translation pairs from a
bilingual lexicon. Our English/Spanish lexicon
presently contains 88,500 translation pairs,
which are then augmented with single word
translations acquired using the statistical method
described in section 4.1. After establishing
possible correspondences, the algorithm uses a
small set of alignment grammar rules to align
LF nodes according to both lexical and
structural considerations and to create LF
transfer mappings. The final step is to filter the
mappings based on the frequency of their source
and target sides. Menezes & Richardson (2001)
provides further details and an evaluation of the
LF alignment algorithm.
The English/Spanish bilingual training
corpus, consisting largely of Microsoft manuals
and help text, averaged 14.1 words per English
sentence. A 2.5 million word sample of English
data contained almost 40K unique word forms.
The data was arbitrarily split in two for use in
our Spanish-English and English-Spanish
systems. The first sub-corpus contains over
208,000 sentence pairs and the second over
183,000 sentence pairs. Only pairs for which
both Spanish and English parsers produce
complete, spanning parses and LFs are currently
used for alignment. Table 1 provides the
number of pairs used and the number of transfer
mappings extracted and used in each case.
Spanish-
English
English-
Spanish
Total sentence pairs 208,730 183,110
Sentence pairs used 161,606 138,280
Transfer mappings
extracted
1,208,828 1,001,078
Unique, filtered
mappings used
58,314 47,136
Table 1. English/Spanish transfer mappings from
LF alignment
4.3 MindNet
The repository into which transfer mappings
from LF alignment are stored is known as
MindNet. Richardson et al (1998) describes
how MindNet began as a lexical knowledge
base containing LF-like structures that were
produced automatically from the definitions and
example sentences in machine-readable
dictionaries. Later, MindNet was generalized,
becoming an architecture for a class of
repositories that can store and access LFs
produced for a variety of expository texts,
including but not limited to dictionaries,
encyclopedias, and technical manuals.
For MSR-MT, MindNet serves as the
optimal example base, specifically designed to
store and retrieve the linked source and target
LF segments comprising the transfer mappings
extracted during LF alignment. As part of daily
regression testing for MSR-MT, all the sentence
pairs in the combined English/Spanish corpus
are parsed, the resulting spanning LFs are
aligned, and a separate MindNet for each of the
two directed language pairs is built from the LF
transfer mappings obtained. These MindNets
are about 7MB each in size and take roughly 6.5
hours each to create on a 550 Mhz PC.
5 Running MSR-MT
MSR-MT translates sentences in four processing
steps, which were illustrated in Figure 1 and
outlined in section 2 above. These steps are
detailed using a simple example in the following
sections.
5.1 Analysis
The input source sentence is parsed with the
same parser used on source text during MSR-
MT?s training. The parser produces an LF for
the sentence, as described in section 3. For the
example LF in Figure 2, the Spanish input
sentence is Haga clic en el bot?n de opci?n. In
English, this is literally Make click in the button
of option. In fluent, translated English, it is
Click the option button.
Figure 2. LF produced for Haga clic en el bot?n
de opci?n.
5.2 MindMeld
The source LF produced by analysis is next
matched by the MindMeld process to the source
LF segments that are part of the transfer
mappings stored in MindNet. Multiple transfer
mappings may match portions of the source LF.
MindMeld attempts to find the best set of
matching transfer mappings by first searching
for LF segments in MindNet that have matching
lemmas, parts of speech, and other feature
information. Larger (more specific) mappings
are preferred to smaller (more general)
mappings. In other words, transfers with context
will be matched preferentially, but the system
will fall back to the smaller transfers when no
matching context is found. Among mappings of
equal size, MindMeld prefers higher-frequency
mappings. Mappings are also allowed to match
overlapping portions of the source LF so long as
they do not conflict in any way.
After an optimal set of matching transfer
mappings is found, MindMeld creates Links on
nodes in the source LF to copies of the
corresponding target LF segments retrieved
from the mappings. Figure 3 shows the source
LF for the example sentence with additional
Links to target LF segments. Note that Links
for multi-word mappings are represented by
linking the root nodes (e.g., hacer and click) of
the corresponding segments, then linking an
asterisk (*) to the other source nodes
participating in the multi-word mapping (e.g.,
usted and clic). Sublinks between
corresponding individual source and target
nodes of such a mapping (not shown in the
figure) are also created for use during transfer.
Figure 3. Linked LF for Haga clic en el bot?n de
opci?n.
5.3 Transfer
The responsibility of transfer is to take a linked
LF from MindMeld and create a target LF that
will be the basis for the target translation. This
is accomplished through a top down traversal of
the linked LF in which the target LF segments
pointed to by Links on the source LF nodes are
stitched together. When stitching together LF
segments from possibly complex multi-word
mappings, the sublinks set by MindMeld
between individual nodes are used to determine
correct attachment points for modifiers, etc.
Default attachment points are used if needed.
Also, a very small set of simple, general, hand-
coded transfer rules (currently four for English
to/from Spanish) may apply to fill current (and
we hope, temporary) gaps in learned transfer
mappings.
In cases where no applicable transfer
mapping was found during MindMeld, the
nodes in the source LF and their relations are
simply copied into the target LF. Default (i.e.,
most commonly occurring) single word
translations may still be found in the MindNet
for these nodes and inserted in the target LF, but
if not, translations are obtained, if possible, from
the same bilingual dictionary used during LF
alignment.
Figure 4 shows the target LF created by
transfer from the linked LF shown in Figure 3.
Figure 4. Target LF for Click the option button.
5.4 Generation
A rule-based generation component maps
from the target LF to the target string (Aikawa
et al 2001). The generation components for the
target languages currently handled by MSR-MT
are application-independent, having been
designed to apply to a range of tasks, including
question answering, grammar checking, and
translation. In its application to translation,
generation has no information about the source
language for a given input LF, working
exclusively with the information passed to it by
the transfer component. It uses this information,
in conjunction with a monolingual (target
language) dictionary to produce its output. One
generic generation component is thus sufficient
for each language.
In some cases, transfer produces an
unmistakably ?non-native? target LF. In order to
correct some of the worst of these anomalies, a
small set of source-language independent rules
is applied prior to generation. The need for such
rules reflects deficiencies in our current data-
driven learning techniques during transfer.
6 Evaluating MSR-MT
In evaluating progress, we have found no
effective alternative to the most obvious
solution: periodic, blind human evaluations
focused on translations of single sentences. The
human raters used for these evaluations work for
an independent agency and played no
development role building the systems they test.
Each language pair under active development is
periodically subjected to the evaluation process
described in this section.
6.1 Evaluation Methodology
For each evaluation, five to seven evaluators
are asked to evaluate the same set of 200 to 250
blind test sentences. For each sentence, raters
are presented with a reference sentence in the
target language, which is a human translation of
the corresponding source sentence. In order to
maintain consistency among raters who may
have different levels of fluency in the source
language, raters are not shown the source
sentence. Instead, they are presented with two
machine-generated target translations presented
in random order: one translation by the system
to be evaluated (the experimental system), and
another translation by a comparison system (the
control system). The order of presentation of
sentences is also randomized for each rater in
order to eliminate any ordering effect.
Raters are asked to make a three-way choice.
For each sentence, raters may choose one of the
two automatically translated sentences as the
better translation of the (unseen) source
sentence, assuming that the reference sentence
represents a perfect translation, or, they may
indicate that neither of the two is better. Raters
are instructed to use their best judgment about
the relative importance of fluency/style and
accuracy/content preservation. We chose to use
this simple three-way scale in order to avoid
making any a priori judgments about the
relative importance of these parameters for
subjective judgments of quality. The three-way
scale also allows sentences to be rated on the
same scale, regardless of whether the
differences between output from system 1 and
system 2 are substantial or negligible.
The scoring system is similarly simple; each
judgment by a rater is represented as 1 (sentence
from experimental system judged better), 0
(neither sentence judged better), or -1 (sentence
from control system judged better). For each
sentence, the score is the mean of all raters?
judgments; for each comparison, the score is the
mean of the scores of all sentences.
6.2 Evaluation results
Although work on MSR-MT encompasses a
number of language pairs, we focus here on the
evaluation of just two, Spanish-English and
English-Spanish. Training data was held
constant for each of these evaluations.
6.2.1 Spanish-English over time
Spanish-English
systems
Mean preference
score (7 raters)
Sample
size
MSR-MT 9/00
vs.
MSR-MT 12/00
0.30 ? 0.09
(at 0.95)
200
sentences
MSR-MT 12/00
vs.
MSR-MT 4/01
0.28 ? 0.07
(at 0.99)
250
sentences
This table summarizes two evaluations
tracking progress in MSR-MT?s Spanish-
English (SE) translation quality over a seven
month development period. The first evaluation,
with seven raters, compared a September 2000
version of the system to a December 2000
version. The second evaluation, carried out by
six raters, examined progress between
December 2000 and April 2001.
A score of -1 would mean that raters
uniformly preferred the control system, while a
score of 1 would indicate that all raters preferred
the comparison system for all sentences. In each
of these evaluations, all raters significantly
preferred the comparison, or newer, version of
MSR-MT, as reflected in the mean preference
scores of 0.30 and 0.28. These numbers confirm
that the system made considerable progress over
a relatively short time span.
6.2.2 Spanish-English vs. alternative system
Spanish-English
systems
Mean preference
score (7 raters)
Sample
size
MSR-MT 9/00 vs.
Babelfish
-0.23 ? 0.12
(at 0.95)
200
sentences
MSR-MT 12/00
vs. Babelfish
0.11 ? 0.10
(at 0.95)
200
sentences
MSR-MT 4/01 vs.
Babelfish
0.32 ? 0.11
(at .99)
250
sentences
This table summarizes our comparison of
MSR-MT?s Spanish-English (SE) output to the
output of Babelfish (http://world.altavista.com/).
Three separate evaluations were performed, in
order to track MSR-MT?s progress over seven
months. The first two evaluations involved
seven raters, while the third involved six.
The shift in the mean preference score from
-0.23 to 0.32 shows clear progress against
Babelfish; by the second evaluation, raters very
slightly preferred MSR-MT in this domain. By
April, all six raters strongly preferred MSR-MT.
6.2.3 English-Spanish vs. alternative system
English-Spanish
systems
Mean preference
score (5 raters)
Sample
size
MSR-MT 2/01
vs. L&H
0.078 ? 0.13
(at 0.95)
250
sentences
MSR-MT 4/01
vs. L&H
0.19 ? 0.14
(at 0.99)
250
sentences
The evaluations summarized in this table
compared February and April 2001 versions of
MSR-MT?s English-Spanish (ES) output to the
output of the Lernout & Hauspie (L&H) ES
system (http://officeupdate.lhsl.com/) for 250
source sentences. Five raters participated in the
first evaluation, and six in the second.
The mean preference scores show that by
April, MSR-MT was strongly preferred over
L&H. Interestingly, though, one rater who
participated in both evaluations maintained a
slight but systematic preference for L&H?s
translations. Determining which aspects of the
translations might have caused this rater to
behave differently from the others is a topic for
future investigation.
6.3 Discussion
These results document steady progress in
the quality of MSR-MT?s output over a
relatively short time. By April 2001, both the SE
and ES versions of the system had surpassed
Babelfish in translation quality for this domain.
While these versions of MSR-MT are the most
fully developed, the other language pairs under
development are also progressing rapidly.
In interpreting our results, it is important to
keep in mind that MSR-MT has been
customized to the test domain, while the
Babelfish and Lernout & Hauspie systems have
not.2 This certainly affects our results, and
2Babelfish was chosen for these comparisons only
after we experimentally compared its output to that
of the related Systran system augmented with its
computer domain dictionary. Surprisingly, the
means that our comparisons have a certain
asymmetry. As our work progresses, we hope to
evaluate MSR-MT against a quality bar that is
perhaps more meaningful: the output of a
commercial system that has been hand-
customized for a specific domain.
The asymmetrical nature of our comparison
cuts both ways, however. Customization
produces better translations, and a system that
can be automatically customized has an inherent
advantage over one that requires laborious
manual customization. Comparing an
automatically-customized version of MSR-MT
to a commercial system which has undergone
years of hand-customization will represent a
comparison that is at least as asymmetrical as
those we have presented here.
We have another, more concrete, purpose in
regularly evaluating our system relative to the
output of systems like Babelfish and L&H: these
commercial systems serve as (nearly) static
benchmarks that allow us to track our own
progress without reference to absolute quality.
7 Conclusions and Future Work
This paper has described MSR-MT, an
EBMT system that produces MT output whose
quality in a specific domain exceeds that of
commercial MT systems, thus attacking head-on
the customization bottleneck. This work
demonstrates that automatic data-driven
methods can provide commercial-quality MT.
In future work we hope to demonstrate that
MSR-MT can be rapidly adapted to very
different semantic domains, and that it can
compete in translation quality even with
commercial systems that have been hand-
customized to a particular domain.
Acknowledgements
We would like to acknowledge the efforts of
the MSR NLP group in carrying out this work.
References
Aikawa, T., M. Melero, L. Schwartz, and A. Wu
2001 ?Multilingual natural language
generation,? Proceedings of 8th European
Workshop on Natural Language Generation,
Toulouse.
generic SE Babelfish engine produced slightly better
translations of our technical data.
Brown, R. 1999. ?Adding linguistic knowledge
to a lexical example-based translation
system,? Proceedings of TMI 99.
Franz, A., K. Horiguchi, L. Duan, D. Ecker, E.
Koontz, and K. Uchida 2000. ?An integrated
architecture for example-based machine
translation,? Proceedings of COLING2000.
Frederking, R., S. Nirenburg, D. Farwell, S.
Helmreich, E. Hovy, K. Knight, S. Beale, C.
Domashnev, D. Attardo, D. Grannes, and R.
Brown 1994. ?Integrating translations from
multiple sources within the Pangloss Mark
III machine translation system,? Proceedings
of AMTA94.
Heidorn, G., K. Jensen, S. Richardson, and A.
Viesse 2000. In R. Dale, H. Moisl and H.
Somers (eds) Handbook of Natural Language
Processing. Marcel Dekker Inc.
Meyers, A., M. Kosaka, and R. Grishman. 2000.
?Chart-based transfer rule application in
machine translation,? Proceedings of
COLING98.
Menezes, A. and S. Richardson 2001. ?A best-
first alignment algorithm for automatic
extraction of transfer mappings from
bilingual corpora,? Proceedings of the
Workshop on Data-Driven Machine
Translation, ACL 2001.
Moore, R. 2001 ?Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships Among Words,?
Proceedings of the Workshop on Data-
Driven Machine Translation, ACL 2001.
Pinkham, J and M. Corston-Oliver 2001
?Adding Domain Specificity to an MT
system,? Proceedings of the Workshop on
Data-Driven Machine Translation, ACL
2001.
Richardson, S. D., W. Dolan, and L.
Vanderwende 1998. ?MindNet: Acquiring
and Structuring Semantic Information from
Text,? Proceedings of COLING-ACL ?98,
Montreal.
Takeda, K., N. Uramoto, T. Nasukawa, and T.
Tsutsumi 1992. ?Shalt 2?a symmetric
machine translation system with conceptual
transfer,? Proceedings of COLING92.
Turcato, D., P. McFetridge, F. Popowich, and J.
Toole 1999. ?A unified example-based and
lexicalist approach to machine translation,?
Proceedings of TMI 99.
Watanabe, W. Kurohashi, S. and E. Aramaki
2000. ?Finding structural correspondences
from bilingual parsed corpus for corpus-
based translation,? Proceedings of
COLING2000.
Watanabe, H. and K. Takeda 1998. ?A pattern-
based machine translation system extended
by example-based processing,? Proceedings
of COLING98.
A best-first alignment algorithm for automatic extraction of transfer
mappings from bilingual corpora
Arul Menezes and Stephen D. Richardson
Microsoft Research
One Microsoft Way
Redmond, WA 98008, USA
arulm@microsoft.com
steveri@microsoft.com
Abstract
Translation systems that automatically
extract transfer mappings (rules or
examples) from bilingual corpora have
been hampered by the difficulty of
achieving accurate alignment and
acquiring high quality mappings. We
describe an algorithm that uses a best-
first strategy and a small alignment
grammar to significantly improve the
quality of the transfer mappings
extracted. For each mapping,
frequencies are computed and sufficient
context is retained to distinguish
competing mappings during translation.
Variants of the algorithm are run
against a corpus containing 200K
sentence pairs and evaluated based on
the quality of resulting translations.
1 Introduction
A machine translation system requires a
substantial amount of translation knowledge
typically embodied in bilingual dictionaries,
transfer rules, example bases, or a statistical
model. Over the last decade, research has
focused on the automatic acquisition of this
knowledge from bilingual corpora. Statistical
systems build translation models from this data
without linguistic analysis (Brown, 1993).
Another class of systems, including our own,
parses sentences in parallel sentence-aligned
corpora to extract transfer rules or examples
(Kaji, 1992) (Meyers, 2000) (Watanabe, 2000).
These systems typically obtain a predicate-
argument or dependency structure for source
and target sentences, which are then aligned,
and from the resulting alignment, lexical and
structural translation correspondences are
extracted, which are then represented as a set of
rules or an example-base for translation.
However, before this method of knowledge
acquisition can be fully automated, a number of
issues remain to be addressed. The alignment
and transfer-mapping acquisition procedure
must acquire rules with very high precision. It
must be robust against errors introduced by
parsing and sentence-level alignment, errors
intrinsic to the corpus, as well as errors resulting
from the alignment procedure itself. The
procedure must also produce transfer mappings
that provide sufficient context to enable the
translation system utilizing these mappings to
choose the appropriate translation for a given
context.
In this paper, we describe the alignment and
transfer-acquisition algorithm used in our
machine translation system, which attempts to
address the issues raised above. This system
acquires transfer mappings by aligning pairs of
logical form structures (LFs) similar to those
described by Jensen (1993). These LFs are
obtained by parsing sentence pairs from a
sentence-aligned bilingual corpus. (The problem
of aligning parallel corpora at the sentence level
has been addressed by Meyers (1998b) Chen
(1993) and others and is beyond the scope of
this paper).
We show that alignment using a best-first
strategy in conjunction with a small alignment
grammar improves the alignment and the quality
of the acquired transfer mappings.
hacer
usted
informaci?n
hiperv?nculo hiperv?nculo
clic
direcci?n
click
Hyperlink_Information
you
address
hyperlink
de de
en en
under
Dsub Dobj
Mod
Dsub
Dobj
Figure 1a: Lexical correspondences Figure 1b: Alignment Mappings
hacer
usted
informaci?n
hiperv?nculo hiperv?nculo
clic
direcci?n
click
Hyperlink_Information
you
address
hyperlink
de de
en en
under
Dsub Dobj
Mod
Dsub
Dobj
2 Logical Form
A Logical Form (LF) is an unordered graph
representing the relations among the most
meaningful elements of a sentence. Nodes are
identified by the lemma of a content word and
directed, labeled arcs indicate the underlying
semantic relations. Logical Forms are intended
to be as independent as possible of specific
languages and their grammars. In particular,
Logical Forms from different languages use the
same relation types and provide similar analyses
for similar constructions. The logical form
abstracts away from such language-particular
aspects of a sentence as constituent order,
inflectional morphology, and certain function
words.
Figure 1a illustrates the LFs for the
following Spanish sentence and its
corresponding English translation, which we use
in example below.
En Informaci?n del hiperv?nculo, haga clic en
la direcci?n del hiperv?nculo.
Under Hyperlink Information, click the
hyperlink address.
3 Alignment
We consider an alignment of two logical forms
to be a set of mappings, such that each mapping
is between a node or set of nodes (and the
relations between them) in the source LF and a
node or set of nodes (and the relations between
them) in the target LF, where no node
participates in more than one such mapping. In
other words, we allow one-to-one, one-to-many,
many-to-one and many-to-many mappings but
the mappings do not overlap.
Our alignment algorithm proceeds in two
phases. The first phase establishes tentative
lexical correspondences between nodes in the
source and target LFs. The second phase aligns
nodes based on these lexical correspondences as
well as structural considerations. The algorithm
starts from the nodes with the tightest lexical
correspondence (?best-first?) and works outward
from these anchor points.
We first present the algorithm, and then
illustrate how it applies to the sentence-pair in
Figure-1.
3.1 Finding tentative lexical
correspondences
We use a bilingual lexicon that merges data
from several sources (CUP, 1995), (SoftArt,
1995), (Langenscheidt, 1997), and inverts
target-to-source dictionaries to improve
coverage. Our Spanish-English lexicon contains
88,500 translation pairs. We augment this with
19,762 translation correspondences acquired
using statistical techniques described by Moore
(2001).
Like Watanabe (2000) and Meyers (2000),
we use a lexicon to establish initial tentative
word correspondences. However, we have found
that even a relatively large bilingual dictionary
has only moderately good coverage for our
purposes. Hence, we pursue an aggressive
matching strategy for establishing tentative
word correspondences. Using the bilingual
dictionary together with the derivational
morphology component in our system
(Pentheroudakis, 1993), we find direct
translations, translations of morphological bases
and derivations, and base and derived forms of
translations. Fuzzy string matching is also used
to identify possible correspondences. We have
found that aggressive over-generation of
correspondences at this phase is balanced by the
more conservative second phase and results in
improved overall alignment quality.
We also look for matches between
components of multi-word expressions and
individual words. This allows us to align such
expressions that may have been analyzed as a
single lexicalized entity in one language but as
separate words in the other.
3.2 Aligning nodes
Our alignment procedure uses the tentative
lexical correspondences established above, as
well as structural cues, to create affirmative
node alignments. A set of alignment grammar
rules licenses only linguistically meaningful
alignments. The rules are ordered to create the
most unambiguous alignments (?best?) first and
use these to disambiguate subsequent
alignments. The algorithm and the alignment
grammar rules are intended to be applicable
across multiple languages. The rules were
developed while working primarily with a
Spanish-English corpus, but have also been
applied to other language pairs such as French,
German, and Japanese to/from English.
The algorithm is as follows:
1. Initialize the set of unaligned source and
target nodes to the set of all source and
target nodes respectively.
2. Attempt to apply the alignment rules in the
specified order, to each unaligned node or
set of nodes in source and target. If a rule
fails to apply to any unaligned node or set of
nodes, move to the next rule.
3. If all rules fail to apply to all nodes, exit. No
more alignment is possible. (Note: some
nodes may remain unaligned).
4. When a rule applies, mark the nodes or sets
of nodes to which it applied as aligned to
each other and remove them from the lists
of unaligned source and target nodes
respectively. Go to step 2 and apply rules
again, starting from the first rule.
The alignment grammar currently consists
of 18 rules. Below we provide the specification
for some of the most important rules.
1. Bidirectionally unique translation: A set of
contiguous source nodes S and a set of
contiguous target nodes T such that every
node in S has a lexical correspondence with
every node in T and with no other target
node, and every node in T has a lexical
correspondence with every node in S and
with no other source node. Align S and T to
each other.
2. Translation + Children: A source node S
and a target node T that have a lexical
correspondence, such that each child of S
and T is already aligned to a child of the
other. Align S and T to each other.
3. Translation + Parent: A source node S and
a target node T that have a lexical
correspondence, such that a parent Ps of S
has already been aligned to a parent Pt of T.
Align S and T to each other.
4. Verb+Object to Verb: A verb V1 (from
either source or target), that has child O that
is not a verb, but is already aligned to a verb
V2, and either V2 has no unaligned parents,
or V1 and V2 have children aligned to each
other. Align V1+O to V2.
5. Parent + relationship: A source node S and
a target node T, with the same part-of-
speech, and no unaligned siblings, where a
parent Ps of S is already aligned to a parent
Pt of T, and the relationship between Ps and
S is the same as that between Pt and T.
Align S and T to each other.
6. Child + relationship: Analogous to previous
rule but based on previously aligned
children instead of parents.
Note that rules 4-6 do not exploit lexical
correspondence, relying solely on relationships
between nodes being examined and previously
aligned nodes.
3.3 Alignment Example
In this section, we illustrate the application of
the alignment procedure to the example in
Figure 1. In the first phase, using the bilingual
lexicon, we identify the lexical correspondences
depicted in Figure-1a as dotted lines. Note that
each of the two instances of hiperv?nculo has
two ambiguous correspondences, and that while
the correspondence from Informaci?n to
Hyperlink Information is unique, the reverse is
not. Note also that neither the monolingual nor
bilingual lexicons have been customized for this
domain. For example, there is no entry in either
lexicon for Hyperlink_Information. This unit has
been assembled by general-purpose "Captoid"
grammar rules. Similarly, lexical
correspondences established for this unit are
based on translations found for its individual
components, there being no lexicon entry for the
captoid as a whole.
In the next phase, the alignment rules apply
to create alignment mappings depicted in
Figure-1b as dotted lines.
Rule-1: Bidirectionally unique translation,
applies in three places, creating alignment
mappings between direcci?n and address,
usted and you, and clic and click. These are
the initial ?best? alignments that provide the
anchors from which we will work outwards to
align the rest of the structure.
Rule-3: Translation + Parent, applies next to
align the instance of hiperv?nculo that is the
child of direcci?n to hyperlink, which is the
child of address. We leverage a previously
created alignment (direcci?n to address) and
the structure of the logical form to resolve the
ambiguity present at the lexical level.
Rule-1 now applies (where previously it did not)
to create a many-to-one mapping between
informaci?n and hiperv?nculo to
Hyperlink_Information. The uniqueness
condition in this rule is now met because the
ambiguous alternative was cleared away by
the prior application of Rule-3.
Rule-4: Verb+Object to Verb applies to rollup
hacer with its object clic, since the latter is
already aligned to a verb. This produces the
many-to-one alignment of hacer and clic to
click
4 Acquiring Transfer Mappings
Figure-2 shows the transfer mappings derived
from the alignment example in Figure-1.
informaci?n
hiperv?nculo
de Hyperlink_Information
direcci?n
hiperv?nculo
de
address
hyperlink
Mod
hacer
Pron) clic
enDsub Dobj
(Noun)
hacer
Pron) clic
enDsub Dobj DobjDsub
click
direcci?n address(Pron)
informaci?n
hiperv?nculo
de
Hyperlink_Inform ation
under
(Verb)en
(Verb)
direcci?n address
hiperv?nculo hyperlink
DobjDsub
click
(Noun)(Pron)
Figure-2 : Transfer mappings acquired
4.1 Transfer mappings with context
Each mapping created during alignment forms
the core of a family of mappings emitted by the
transfer mapping acquisition procedure. The
alignment mapping by itself represents a
minimal transfer mapping with no context. In
addition, we emit multiple variants, each one
expanding the core mapping with varying types
and amounts of local context.
We use linguistic constructs such as noun
and verb phrases to provide the boundaries for
the context we include. For example, the
transfer mapping for an adjective is expanded to
include the noun it modifies; the mapping for a
modal verb is expanded to include the main
verb; the mapping for a main verb is expanded
to include its object; mappings for collocations
of nouns are emitted individually and as a
whole. Mappings may include ?wild card? or
under-specified nodes, with a part of speech, but
no lemma, as shown in Figure 2.
4.2 Alignment Post-processing
After we have acquired transfer mappings from
our entire training corpus, we compute
frequencies for all mappings. We use these to
resolve conflicting mappings, i.e. mappings
where the source sides of the mapping are
identical, but the target sides differ. Currently
we resolve the conflict by simply picking the
most frequent mapping. Note that this does not
imply that we are committed to a single
translation for every word across the corpus,
since we emitted each mapping with different
types and amounts of context (see section 4.1).
Ideally at least one of these contexts serves to
disambiguate the translation. The conflicts being
resolved here are those mappings where the
necessary context is not present.
A drawback of this approach is that we are
relying on a priori linguistic heuristics to ensure
that we have the right context. Our future work
plans to address this by iteratively searching for
the context that serves to optimally
disambiguate (across the entire training corpus)
between conflicting mappings.
4.2.1 Frequency Threshold
During post-processing we also apply a
frequency threshold, keeping only mappings
seen at least N times (where N is currently 2).
This frequency threshold greatly improves the
speed of the runtime system, with negligible
impact on translation quality (see section 5.6).
5 Experiments and Results
5.1 Evaluation methodology
In the evaluation process, we found that various
evaluation metrics of alignment in isolation bore
very little relationship to the quality of the
translations produced by a system that used the
results of such alignment. Since it is the overall
translation quality that we care about, we use the
output quality (as judged by humans) of the MT
system incorporating the transfer mappings
produced by an alignment algorithm (keeping all
other aspects of the system constant) as the
metric for that algorithm.
5.2 Translation system
Our translation system (Richardson, 2001)
begins by parsing an input sentence and
obtaining a logical form. We then search the
transfer mappings acquired during alignment,
for mappings that match portions of the input
LF. We prefer larger (more specific) mappings
to smaller (more general) mappings. Among
mappings of equal size, we prefer higher-
frequency mappings. We allow overlapping
mappings that do not conflict. The lemmas in
any portion of the LF not covered by a transfer
mapping are translated using the same bilingual
dictionary employed during alignment, or by a
handful of hard-coded transfer rules (see Section
5.7 for a discussion of the contribution made by
each of these components). Target LF fragments
from matched transfer mappings and default
dictionary translations are stitched together to
form an output LF. From this, a rule-based
generation component produces an output
sentence.
The system provides output for every input
sentence. Sentences for which spanning parses
are not found are translated anyway, albeit with
lower quality.
5.3 Training corpus
We use a sentence-aligned Spanish-English
training corpus consisting of 208,730 sentence
pairs mostly from technical manuals. The data
was already aligned at the sentence-level since it
was taken from sentence-level translation
memories created by human translators using a
commercial translation-memory product. This
data was parsed and aligned at the sub-sentence
level by our system, using the techniques
described in this paper. Our parser produces a
parse in every case, but in each language
roughly 15% of the parses produced are ?fitted?
or non-spanning. Since we have a relatively
large training corpus, we apply a conservative
heuristic and only use in alignment those
sentence-pairs that produced spanning parses in
both languages. In this corpus 161,606 pairs (or
77.4% of the corpus) were used. This is a
substantially larger training corpus than those
used in previous work on learning transfer
mappings from parsed data. Table-1 presents
some data on the mappings extracted from this
corpus using Best-First.
Total Sentence pairs 208,730
Sentence pairs used 161,606
Number of transfer mappings 1,202,828
Transfer mappings per pair 7.48
Num. unique transfer mappings 437,479
Num. unique after elim. conflicts 369,067
Num. unique with frequency > 1 58,314
Time taken to align entire corpus
(on a 800MHz PC)
74 minutes
Alignment speed 35.6 sent/s
Table-1: Best-first alignment of training corpus
5.4 Experiments
In each experiment we used 5 human evaluators
in a blind evaluation, to compare the translations
produced by the test system with those produced
by a comparison system. Evaluators were
presented, for each sentence, with a reference
human translation and with the two machine
translations in random order, but not the original
source language sentence. They were asked to
pick the better overall translation, taking into
account both content and fluency. They were
allowed to choose ?Neither? if they considered
both translations equally good or equally bad.
All the experiments were run with our
Spanish-English system. The test sentences were
randomly chosen from unseen data from the
same domain. Experiment-1 used 200 sentences
and each sentence was evaluated by all raters.
Sentences were rated better for one system or
the other if a majority of the raters agreed.
Experiments 2-4 used 500 sentences each, but
each sentence was rated by a single rater.
In each experiment, the test system was the
system described in section 5.2, loaded with
transfer mappings acquired using the techniques
described in this paper (hereafter ?Best-First?).
5.5 Comparison systems
In the first experiment the comparison system is
a highly rated commercial system, Babelfish
(http://world.altavista.com).
Each of the next three experiments varies
some key aspect of Best-First in order to explore
the properties of the algorithm.
5.5.1 Bottom Up
Experiment-2 compares Best-First to the
previous algorithm we employed, which used a
bottom-up approach, similar in spirit to that used
by Meyers (1998a).
This algorithm follows the procedure
described in section 3.1 to establish tentative
lexical correspondences. However, it does not
use an alignment grammar, and relies on a
bottom-up rather than a best-first strategy. It
starts by aligning the leaf nodes and proceeds
upwards, aligning nodes whose child nodes have
already aligned. Nodes that do not align are
skipped over, and later rolled-up with ancestor
nodes that have successfully aligned.
5.5.2 No Context
Experiment-3 uses a comparison algorithm that
differs from Best First in that it retains no
context (see section 4.1) when emitting transfer
mappings.
5.5.3 No Threshold
The comparison algorithm used in Experiment-4
differs from Best First in that the frequency
threshold (see section 4.2.1) is not applied, i.e.
all transfer mappings are retained.
Comparison
System
Num. sentences
Best-First rated
better
Num. sentences
comparison
system rated better
Num. sentences
neither rated better
Net percentage
improvement
Babelfish 93 (46.5%) 73 (36.5%) 34 (17%) 10.0%
Bottom-Up 224 (44.8%) 111 (22.2%) 165 (33%) 22.6%
No-Context 187 (37.4%) 69 (13.8%) 244 (48.8%) 23.6%
No-Threshold 112 (22.4%) 122 (24.4%) 266 (53.2%) -2.0%
Table-2: Translation Quality
5.6 Discussion
The results of the four experiments are
presented in Table-2.
Experiment-1 establishes that the algorithm
presented in this paper automatically acquires
translation knowledge of sufficient quantity and
quality as to enable translations that exceed the
quality of a highly rated traditional MT system.
Note however that Babelfish/Systran was not
customized to this domain.
Experiment-2 shows that Best-First
produces transfer mappings resulting in
significantly better translations than Bottom-Up.
Using Best-First produced better translations for
a net of 22.6% of the sentences.
Experiment-3 shows that retaining sufficient
context in transfer mappings is crucial to
translation quality, producing better translations
for a net of 23.6% of the sentences.
Experiment-4 shows that the frequency
threshold hurts translation quality slightly (a net
loss of 2%), but as Table-3 shows it results in a
much smaller (approx. 6 times) and faster
(approx 45 times) runtime system.
Num
mappings
Translation speed
(500 sentences)
Best-First 58,314 173s (0.34s/sent)
No-Threshold 359,528 8059s (17s/sent)
Table-3: Translation Speed (500 sentences)
5.7 Transfer mapping coverage
Using end-to-end translation quality as a metric
for alignment leaves open the question of how
much of the translation quality derives from
alignment versus other sources of translation
knowledge in our system, such as the bilingual
dictionary, or the 2 hand-coded transfer rules in
our system. To address this issue we measured
the contribution of each using a 3264-sentence
test set. Table-4 presents the results. The first
column indicates the total number of words in
each category. The next four columns indicate
the percentage translated using each knowledge
source, and the percentage not translated
respectively.
As the table shows, the vast majority of
content words get translated using transfer-
mappings obtained via alignment.
Our alignment algorithm does not explicitly
attempt to learn transfer mappings for pronouns,
but pronouns are sometimes included in transfer
mappings when they form part of the context
that is included with each mapping (see section
4.1). The 31.89% of pronoun translations that
the table indicates as coming from alignment
fall into this category.
Our algorithm does try to learn transfer
mappings for prepositions and conjunctions,
which are represented in the Logical Form as
labels on arcs (see Figure-1). Mappings for
prepositions and conjunctions always include
the nodes on both ends of this arc. These
mappings may translate a preposition in the
source language to a preposition in the target
language, or to an entirely different relationship,
such as direct object, indirect object, modifier
etc.
As the table shows, the system is currently
less successful at learning transfer mappings for
prepositions and conjunctions than it is for
content words.
As a temporary measure we have 2 hand-
coded transfer rules that apply to prepositions,
which account for 8.4% of such transfers. We
intend for these to eventually be replaced by
mappings learned from the data.
Number of
instances
Alignment Dictionary Rules Not
translated
Content words 21,245 93.50% 4.10% 0% 2.4%
Pronouns 2,158 31.89% 68.20% 0% 0%
Prepositions/Conjunctions 6,640 32.00% 59.70% 8.4% 0%
Table-4: Coverage of transfer mappings, dictionary & rules
6 Conclusions and Future Work
We proposed an algorithm for automatically
acquiring high-quality transfer mappings from
sentence-aligned bilingual corpora using an
alignment grammar and a best-first strategy.
We reported the results of applying the
algorithm to a substantially larger training
corpus than that used in previously reported
work on learning transfer mappings from parsed
data.
We showed that this approach produces
transfer mappings that result in translation
quality comparable to a commercial MT system
for this domain.
We also showed that a best-first, alignment-
grammar based approach produced better results
than a bottom-up approach, and that retaining
context in the acquired transfer mappings is
essential to translation quality.
We currently rely on a priori linguistic
heuristics to try to provide the right context for
each transfer mapping. In future work, we plan
to use machine-learning techniques to determine
the extent of the context that optimally
disambiguates between conflicting mappings.
References
Peter Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer, 1993. ?The
mathematics of statistical machine translation?
Computational Linguistics, 19:263-312
Cambridge University Press (1995), McCarthy, M.
ed., Cambridge Word Selector
Stanley F. Chen, 1993. ?Aligning sentences in
bilingual corpora using lexical information?
Proceedings of ACL 1993
Karen Jensen, 1993. ?PEGASUS: Deriving argument
structures after syntax.? In Natural Language
Processing: The PLNLP Approach. Kluwer
Academic Publishers, Boston, MA.
Hiroyuki Kaji, Yuuko Kida, and Yasutsugu
Morimoto, 1992. ?Learning Translation Templates
from Bilingual Text? Proceedings of COLING
1992
Langenscheidt Publishers 1997, The Langenscheidt
Pocket Spanish Dictionary
Adam Meyers, Roman Yangarber, Ralph Grishman,
Catherine Macleod, and Antonio Moreno-
Sandoval, 1998a. ?Deriving transfer rules from
dominance-preserving alignments?, Proceedings
of COLING 1998
Adam Meyers, Michiko Kosaka and Ralph
Grishman, 1998b. ?A multilingual procedure for
dictionary-based sentence alignment? Proceedings
of AMTA 98
Adam Meyers, Michiko Kosaka and Ralph
Grishman, 2000. ?Chart-based transfer rule
application in machine translation? Proceedings of
COLING 2000
Robert C. Moore 2001, ?Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships among Words?
Proceedings of the Workshop on Data-Driven
Machine Translation, ACL 2001
Joseph Pentheroudakis and Lucretia Vanderwende
1993, ?Automatically identifying morphological
relations in machine-readable dictionaries? Ninth
Annual conference of the University of Waterloo
Center for the new OED and Text Research
Stephen D. Richardson, William Dolan, Monica
Corston-Oliver, and Arul Menezes 2001,
?Overcoming the customization bottleneck using
example-based MT?, Workshop on Data-Driven
Machine Translation, ACL 2001
SoftArt Inc (1995) Soft-Art translation dictionary.
Version 7
Hideo Watanabe, Sado Kurohashi, and Eiji Aramaki,
2000. ?Finding Structural Correspondences from
Bilingual Parsed Corpus for Corpus-based
Translation? Proceedings of COLING 2000
English-Japanese Example-Based Machine Translation Using Abstract 
Linguistic Representations 
 
Chris Brockett, Takako Aikawa, Anthony Aue, Arul Menezes, Chris Quirk  
and Hisami Suzuki 
Natural Language Processing Group, Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{chrisbkt,takakoa,anthaue,arulm,chrisq,hisamis}@microsoft.com 
 
 
Abstract 
This presentation describes an example- 
based English-Japanese machine trans- 
lation system in which an abstract 
linguistic representation layer is used to 
extract and store bilingual translation 
knowledge, transfer patterns between 
languages, and generate output strings. 
Abstraction permits structural neutral-
izations that facilitate learning of trans-
lation examples across languages with 
radically different surface structure charac-
teristics, and allows MT development to 
proceed within a largely language- 
independent NLP architecture. Com-
parative evaluation indicates that after 
training in a domain the English-Japanese 
system is statistically indistinguishable 
from a non-customized commercially 
available MT system in the same domain. 
Introduction 
In the wake of the pioneering work of Nagao 
(1984), Brown et al (1990) and Sato and 
Nagao (1990), Machine Translation (MT) 
research has increasingly focused on the issue 
of how to acquire translation knowledge from 
aligned parallel texts. While much of this 
research effort has focused on acquisition of 
correspondences between individual lexical 
items or between unstructured strings of words, 
closer attention has begun to be paid to the 
learning of structured phrasal units: Yamamoto 
and Matsumoto (2000), for example, describe a 
method for automatically extracting correspon-
dences between dependency relations in 
Japanese and English. Similarly, Imamura 
(2001a, 2001b) seeks to match corresponding 
Japanese and English phrases containing 
information about hierarchical structures, 
including partially completed parses. 
 
Yamamoto and Matsumoto (2000) explicitly 
assume that dependency relations between 
words will generally be preserved across 
languages. However, when languages are as 
different as Japanese and English with respect 
to their syntactic and informational structures, 
grammatical or dependency relations may not 
always be preserved: the English sentence ?the 
network failed? has quite a different 
grammatical structure from its Japanese 
translation equivalent ??????????
???? ?a defect arose in the network.? One 
issue for example-based MT, then, is to capture 
systematic divergences through generic 
learning applicable to multiple language pairs. 
 
In this presentation we describe the MSR-MT 
English-Japanese system, an example-based 
MT system that learns structured phrase-sized 
translation units. Unlike the systems discussed 
in Yamamoto and Matsumoto (2000) and 
Imamura (2001a, 2001b), MSR-MT places the 
locus of translation knowledge acquisition at a 
greater level of abstraction than surface 
relations, pushing it into a semantically- 
motivated layer called LOGICAL FORM (LF) 
(Heidorn 2000; Campbell & Suzuki 2002a, 
2002b). Abstraction has the effect of 
neutralizing (or at least minimizing) differences 
in word order and syntactic structure, so that 
mappings between structural relations 
associated with lexical items can readily be 
acquired within a general MT architecture. 
 
In Section 1 below, we present an overview of 
the characteristics of the system, with special 
reference to English-Japanese MT. Section 2 
discusses a class of structures learned through 
phrase alignment, Section 3 presents the results 
of comparative evaluation, and Section 4 some 
factors that contributed to the evaluation results. 
Section 5 addresses directions for future work. 
 
  
1 The MSR-MT System 
The MSR-MT English-Japanese system is a 
hybrid example-based machine translation 
system that employs handcrafted broad- 
coverage augmented phrase structure grammars 
for parsing, and statistical and heuristic 
techniques to capture translation knowlege and 
for transfer between languages. The parsers are 
general purpose: the English parser, for 
example, forms the core of the grammar 
checkers used in Microsoft Word (Heidorn 
2000). The Japanese grammar utilizes much of 
the same codebase, but contains language- 
specific grammar rules and additional features 
owing to the need for word-breaking in 
Japanese (Suzuki et al 2000; Kacmarcik et al 
2000). These parsers are robust in that if the 
analysis grammar fails to find an appropriate 
parse, it outputs a best-guess ?fitted? parse. 
 
System development is not confined to 
English-Japanese: MSR-MT is part of a 
broader natural language processing project 
involving three Asian languages (Japanese, 
Chinese, and Korean) and four European 
languages (English, French, German, and 
Spanish). Development of the MSR-MT 
systems proceeds more or less simultaneously 
across these languages and in multiple 
directions, including Japanese-English. The 
Spanish-English version of MSR-MT has been 
described in Richardson et al 2001a, Richardson 
et al2001b, and the reader is referred to these 
papers for more information concerning 
algorithms employed during phrase alignment. 
A description of the French-Spanish MT 
system is found in Pinkham & Smets. 2002. 
 
1.1 Training Data 
MSR-MT requires that a large corpus of 
aligned sentences be available as examples for 
training. For English-Japanese MT, the system 
currently trains on a corpus of approximately 
596,000 pre-aligned sentence pairs. About 
274,000 of these are sentence pairs extracted 
from Microsoft technical documentation that 
had been professionally translated from 
English into Japanese. The remaining 322,000 
are sentence examples or sentence fragments 
extracted from electronic versions of student 
dictionaries.1  
1.2  Logical Form 
MSR-MT employs a post-parsing layer of 
semantic representation called LOGICAL FORM 
(LF) to handle core components of the 
translation process, namely acquisition and 
storage of translation knowledge, transfer 
between languages, and generation of target 
output. LF can be viewed as a representation of 
the various roles played by the content words 
after neutralizing word order and local 
morphosyntactic variation (Heidorn 2000; 
Campbell & Suzuki 2002a; 2002b). These can 
be seen in the Tsub (Typical Subject) and Tobj 
(Typical Object) relations in Fig. 1 in the 
sentence ?Mary eats pizza? and its Japanese 
counterpart. The graphs are simplified for 
expository purposes. 
Although our hypothesis is that equivalent 
sentences in two languages will tend to 
resemble each other at LF more than they do in 
the surface parse, we do not adopt a na?ve 
reductionism that would attempt to make LFs 
completely identical. In Fig. 2, for example, the 
LFs of the quantified nouns differ in that the 
Japanese LF preserves the classifier, yet are 
similar enough that learning the mapping 
between the two structures is straightforward. 
It will be noted that since the LF for each 
language stores words or morphemes of that 
language, this level of representation is not in 
any sense an interlingua. 
 
                                                   
1 Kodansha?s Basic English-Japanese Dictionary, 
1999; Kenkyusha?s New College Japanese-English 
Dictionary, 4th Edition, 1995 ; and Kenkyusha?s 
New College English-Japanese Dictionary, 6th 
Edition, 1994. 
 
 
Fig. 1  Canonical English and Japanese 
Logical Forms 
 
 
1.3  Mapping Logical Forms 
In the training phase, MSR-MT learns transfer 
mappings from the sentence-aligned bilingual 
corpus. First, the system deploys the 
general-purpose parsers to analyze the English 
and Japanese sentence pairs and generate LFs 
for each sentence. In the next step, an LF 
alignment algorithm is used to match source 
language and target language LFs at the 
sub-sentence level. 
 
The LF alignment algorithm first establishes 
tentative lexical correspondences between 
nodes in the source and target LFs on the basis 
of lexical matching over dictionary information 
and approximately 31,000 ?word associations,? 
that is, lexical mappings extracted from the 
training corpora using statistical techniques 
based on mutual information (Moore 2001). 
From these possible lexical correspondences, 
the algorithm uses a small grammar of 
(language-pair-independent) rules to align LF 
nodes on lexical and structural principles. The 
aligned LF pairs are then partitioned into 
smaller aligned LF segments, with individual 
node mappings captured in a relationship we 
call ?sublinking.? Finally, the aligned LF 
segments are filtered on the basis of frequency, 
and compiled into a database known as a 
Mindnet. (See Menezes & Richardson 2001 for a 
detailed description of this process.) 
 
The Mindnet is a general-purpose database of 
semantic information (Richardson et al 1998) 
that has been repurposed as the primary 
repository of translation information for MT 
applications. The process of building the 
Mindnet is entirely automated; there is no 
human vetting of candidate entries. At the end 
of a typical training session, 1,816,520 transfer 
patterns identified in the training corpus may 
yield 98,248 final entries in the Mindnet. Only 
the output of successful parses is considered 
for inclusion, and each mapping of LF 
segments must have been encountered twice in 
the corpus before it is incorporated into the 
Mindnet. 
 
In the Mindnet, LF segments from the source 
language are represented as linked to the 
corresponding LF segment from the target 
languages. These can be seen in Figs. 3 and 4, 
discussed below in Section 2. 
1.4  Transfer and Generation 
At translation time, the broad-coverage source 
language parser processes the English input 
sentence, and creates a source-language LF. 
This LF is then checked against the Mindnet 
entries. 2  The best matching structures are 
extracted and stitched together determinist-
ically into a new target-language ?transferred 
LF? that is then submitted to the Japanese 
system for generation of the output string. 
 
The generation module is language-specific 
and used for both monolingual generation and 
MT. In the context of MT, generation takes as 
input the transferred LF and converts it into a 
basic syntactic tree. A small set of heuristic 
rules preprocesses the transferred LF to 
?nativize? some structural differences, such as 
pro-drop phenomena in Japanese. A series of 
core generation rules then applies to the LF tree, 
transforming it into a Japanese sentence string. 
Generation rules operate on a single tree only, 
are application-independent and are developed 
in a monolingual environment (see Aikawa et 
al. 2001a, 2001b for further details.) 
Generation of inflectional morphology is also 
handled in this component. The generation 
component has no explicit knowledge of the 
source language. 
 
2 Acquisition of Complex Structural 
Mappings 
The generalization provided by LF makes it 
possible for MSR-MT to handle complex 
structural relations in cases where English and 
Japanese are systematically divergent. This is 
                                                   
2 MSR-MT resorts to lexical lookup only when a 
term is not found in the Mindnet. The handcrafted 
dictionary is slated for replacement by purely 
statistically generated data.  
 
 
Fig. 2  Cross-Linguistic Variation in Logical 
Form 
 
illustrated by the sample training pair in the 
lefthand column of Table 1. In Japanese, 
inanimate nouns tend to be avoided as subjects 
of transitive verbs; the word ?URL?, which is 
subject in the English sentence, thus 
corresponds to an oblique relation in the 
Japanese. (The Japanese sentence, although a 
natural and idiomatic translation of the English,  
is literally equivalent to ?one can access public 
folders with this URL.?)   
 
Nonetheless, mappings turn out to be learnable 
even where the information is structured so 
radically differently. Fig. 3 shows the Mindnet 
entry for ?provide,? which is result of training 
on sentence pairs like those in the lefthand 
column of Table 1. The system learns not only 
the mapping between the phrase ?provide 
access? and the potential form of ???? 
?access?, but also the crucial sublinking of the 
Tsub node of the English sentence and the node 
headed by ?  (underspecified for semantic 
role) in the Japanese. At translation time the 
system is able to generalize on the basis of the 
functional roles stored in the Mindnet; it can 
substitute lexical items to achieve a relatively 
natural translation of similar sentences such as 
shown in the right-hand side of Table 1.  
Differences of the kind seen in Fig 3 are 
endemic in our Japanese and English corpora. 
Fig. 4 shows part of the example Mindnet entry 
for the English word ?fail? referred to in the 
Introduction, which exhibits another mismatch 
in grammatical roles somewhat similar to that 
in observed in Fig. 3. Here again, the lexical 
matching and generic alignment heuristics have 
allowed the match to be captured into the 
Mindnet. Although the techniques employed 
may have been informed by analysis of 
language-specific data, they are in principle of 
general application. 
 
 
3 Evaluation 
In May 2002, we compared output of the 
MSR-MT English-Japanese system with a 
commercially available desktop MT system.3 
                                                   
3 Toshiba?s The Honyaku Office v2.0 desktop MT 
system was selected for this purpose. The Honyaku 
is a trademark of the Toshiba Corporation. Another 
desktop system was also considered for evaluation; 
however, comparative evaluation with that system 
indicated that the Toshiba system performed 
marginally, though not significantly, better on our 
technical documentation.  
 
Training Data Translation Output  
This URL provides access to public folders. 
 
This computer provides access to the internet. 
 
?? URL ?????? ????? 
????????? 
????????????????? 
????????? 
 
Table 1.  Sample Input and Output 
Fig. 3.  Part of the Mindnet Entry for ?provide? 
 
 
Fig. 4.  Part of the Mindnet Entry for ?fail? 
 
A total of 238 English-Japanese sentence pairs 
were randomly extracted from held-out 
software manual data of the same kinds used 
for training the system. 4  The Japanese 
sentences, which had been translated by human 
translators, were taken as reference sentences 
(and were assumed to be correct translations). 
The English sentences were then translated by 
the two MT systems into Japanese for blind 
evaluation performed by seven outside vendors 
unfamiliar with either system?s characteristics. 
 
No attempt was made to constrain or modify 
the English input sentences on the basis of 
length or other characteristics. Both systems 
provided a translation for each sentence.5  
 
For each of the Japanese reference sentences, 
evaluators were asked to select which 
translation was closer to the reference sentence. 
A value of +1 was assigned if the evaluator 
considered MSR-MT output sentence better 
and ?1 if they considered the comparison 
system better. If two translated sentences were 
considered equally good or bad in comparison 
                                                   
4  250 sentences were originally selected for 
evaluation; 12 were later discarded when it was 
discovered by evaluators that the Japanese reference 
sentences (not the input sentences) were defective 
owing to the presence of junk characters (mojibake) 
and other deficiencies.  
5 In MSR-MT, Mindnet coverage is sufficiently 
complete with respect to the domain that an 
untranslated sentence normally represents a 
complete failure to parse the input, typically owing 
to excessive length. 
to the reference, a value of 0 was assigned. On 
this metric, MSR-MT scored slightly worse 
than the comparison system rating of ?0.015. 
At a two-way confidence measure of +/?0.16, 
the difference between the systems is 
statistically insignificant. By contrast, an 
earlier evaluation conducted in October 2001 
yielded a score of ?0.34 vis-?-vis the 
comparison system. 
 
In addition, the evaluators were asked to rate 
the translation quality on an absolute scale of 1 
through 4, according to the following criteria: 
 
1. Unacceptable: Absolutely not comprehen- 
sible and/or little or no information trans- 
ferred accurately. 
2. Possibly Acceptable: Possibly compre- 
hensible (given enough context and/or 
time to work it out); some information 
transferred accurately. 
3. Acceptable: Not perfect, but definitely 
comprehensible, and with accurate transfer 
of all important information. 
4. Ideal: Not necessarily a perfect translation, 
but grammatically correct, and with all 
information accurately transferred. 
 
On this absolute scale, neither system 
performed exceptionally well: MSR-MT scored 
an average 2.25 as opposed to 2.32 for the 
comparison system. Again, the difference 
between the two is statistically insignificant. It 
should be added that the comparison presented 
here is not ideal, since MSR-MT was trained 
principally on technical manual sentences, 
 Evaluation 
Date 
Transfers 
per Sentence 
Nodes  
Per Transfer
 
 Oct. 2001 5.8 1.6  
 May 2002 6.7 2.0  
Table 2. Number of Transfers and Nodes Transferred per Sentence 
 
 Evaluation Date Word Class Total From 
Mindnet 
From 
Dictionary
Untranslated  
 Prepositions 410 17.1% 77.1% 5.9%  
 
Oct. 2001  
(250 sentences) Content Lemmas 2124 88.4% 7.8% 3.9%  
 Prepositions 842 61.9% 37.5% 0.6%  
 
May 2002 
(520 sentences) Content Lemmas 4429 95.9% 1.5% 2.6%  
Table 3.  Sources of Different Word Classes at Transfer 
 
while the comparison system was not 
specifically tuned to this corpus. Accordingly 
the results of the evaluation need to be 
interpreted narrowly, as demonstrating that:  
l  A viable example-based English-Japanese 
MT system can be developed that applies 
general-purpose alignment rules to semantic 
representations; and  
l  Given general-purpose grammars, a 
representation of what the sentence means, 
and suitable learning techniques, it is 
possible to achieve in a domain, results 
analogous with those of a mature 
commercial product, and within a relatively 
short time frame. 
4 Discussion 
It is illustrative to consider some of the factors 
that contributed to these results. Table 2 shows 
the number of transfers per sentence and the 
number of LF nodes per transfer in versions of 
the system evaluated in October 2001 and May 
2002. Not only is the MSR-MT finding more 
LF segments in the Mindnet, crucially the 
number of nodes transferred has also grown. 
An average of two connected nodes are now 
transferred with each LF segment, indicating 
that the system is increasingly learning its 
translation knowledge in terms of complex 
structures rather than simple lexical 
correspondences. 
 
It has been our experience that the greater 
MSR-MT?s reliance on the Mindnet, the better 
the quality of its output. Table 2 shows the 
sources of selected word classes in the two 
systems. Over time, reliance on the Mindnet 
has increased overall, while reliance on 
dictionary lookup has now diminished to the 
point where, in the case of content words, it 
should be possible to discard the handcrafted 
dictionary altogether and draw exclusively on 
the contextualized resources of the Mindnet 
and statistically-generated lexical data. Also 
striking in Table 2 is the gain shown in 
preposition handling: a majority of English 
prepositions are now being transferred only in 
the context of LF structures found in the 
Mindnet. 
 
The important observation underlying the gains 
shown in these tables is that they have 
primarily been obtained either as the result of 
LF improvements in English or Japanese (i.e., 
from better sentence analysis or LF 
construction), or as a result of generic 
improvements to the algorithms that map 
between LF segments (notably better 
coindexation and improved learning of 
mappings involving lexical attributes). In the 
latter case, although certain modifications may 
have been driven by phenomena observed 
between Japanese and English, the heuristics 
apply across all seven languages on which our 
group is currently working. Adaptation to the 
case of Japanese-English MT usually takes the 
form of loosening rather than tightening of 
constraints.  
 
 
5 Future Work 
Ultimately it is probably desirable that the 
system?s mean absolute score should approach 
3 (Acceptable) within the training domain: this 
is a high quality bar that is not attained by 
off-the-shelf systems. Much of the work will be 
of a general nature: improving the parses and 
LF structures of source and target languages 
will bring automatic benefits to both alignment 
of structured phrases and runtime translation. 
For example, efforts are currently underway to 
redesign LF to better represent scopal 
properties of quantifiers and negation 
(Campbell & Suzuki 2002a, 2002b). 
 
Work to improve the quality of alignment and 
transfer is ongoing within our group. In 
addition to improvement of alignment itself, 
we are also exploring techniques to ensure that 
the transferred LF is consistent with known 
LFs in the target language, with the eventual 
goal of obviating the need for heuristic rules 
used in preprocessing generation. Again, these 
improvements are likely to be system-wide and 
generic, and not specific to the 
English-Japanese case. 
 
 
Conclusions 
Use of abstract semantically-motivated 
linguistic representations (Logical Form) 
permits MSR-MT to align, store, and translate 
sentence patterns reflecting widely varying 
syntactic and information structures in 
Japanese and English, and to do so within the 
framework of a general-purpose NLP 
architecture applicable to both European 
languages and Asian languages. 
 
Our experience with English-Japanese example 
based MT suggests that the problem of MT 
among Asian languages may be recast as a 
problem of implementing a general represen- 
tation of structured meaning across languages 
that neutralizes differences where possible, and 
where this is not possible, readily permits 
researchers to identify general-purpose 
techniques of bridging the disparities that are 
viable across multiple languages. 
 
Acknowledgements 
We would like to thank Bill Dolan and Rich 
Campbell for their comments on a draft of this 
paper. Our appreciation also goes to the 
members of the Butler Hill Group for their 
assistance with conducting evaluations. 
References  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001a. Multilingual sentence generation. In 
Proceedings of 8th European Workshop on 
Natural Language Generation, Toulouse, France.  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001b. Sentence generation for multilingual 
machine translation. In Proceedings of the MT 
Summit VIII, Santiago de Compostela, Spain.  
Brown, P. F.,  J. Cocke, S. A. D. Pietra, V. J. D. 
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, 
and P. S. Roossin. 1990. A statistical approach to 
machine translation. Computational Linguistics, 
16(2): 79-85. 
Campbell, R. and H. Suzuki. 2002a. Language- 
neutral representation of syntactic structure. In 
Proceedings of the First International Workshop 
on Scalable Natural Language Understanding 
(SCANALU 2002), Heidelberg, Germany. 
Campbell, R. and H. Suzuki. 2002b. Language- 
Neutral Syntax: An Overview. Microsoft Research 
Techreport: MSR-TR-2002-76. 
Heidorn, G. 2000. Intelligent writing assistance. In 
R. Dale, H. Moisl and H. Somers (eds.), A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker, New York. 
pp. 181-207.  
Imamura, K. 2001a. Application of translation 
knowledge acquired by ierarchical phrase 
alignment. In Proceedings of TMI.  
Imamura, K. 2001b. Hierarchical phrase alignment 
harmonized with parsing. In Proceedings of 
NLPRS, Tokyo, Japan, pp 377-384.  
Kacmarcik, G., C. Brockett, and H. Suzuki. 2000. 
Robust segmentation of Japanese text into a 
lattice for parsing. In Proceedings of COLING 
2000, Saarbrueken, Germany, pp. 390-396. 
Menezes, A. and S. D. Richardson. 2001. A 
best-first alignment algorithm for automatic 
extraction of transfer mappings from bilingual 
corpora. In Proceedings of the Workshop on 
Data-driven Machine Translation at 39th Annual 
Meeting of the Association for Computational 
Linguistics, Toulouse, France, pp. 39-46. 
Moore, R. C. 2001. Towards a simple and accurate 
statistical approach to learning translation 
relationships among words," in Proceedings, 
Workshop on Data-driven Machine Translation, 
39th Annual Meeting and 10th Conference of the 
European Chapter, Association for 
Computational Linguistics, Toulouse, France, pp. 
79-86. 
Nagao, M. 1984. A framework of a mechanical 
translation between Japanese and English by 
analogy principle. In A. Elithorn. and R. Bannerji 
(eds.) Artificial and Human Intelligence.  Nato 
Publications. pp. 181-207.   
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro. 2001. Rapid Assembly of a Large-scale 
French-English MT system. In Proceedings of the 
MT Summit VIII, Santiago de Compostela, Spain. 
Pinkham, J., and M. Smets. 2002. Machine 
translation without a bilingual dictionary. In 
Proceedings of the 9th International Conference 
on Theoretical and Methodological Issues in 
Machine Translation. Kyoto, Japan, pp. 146-156. 
Richardson, S. D., W. B. Dolan, A. Menezes, and M. 
Corston-Oliver. 2001. Overcoming the 
customization bottleneck using example-based 
MT. In Proceedings, Workshop on Data-driven 
Machine Translation, 39th Annual Meeting and 
10th Conference of the European Chapter, 
Association for Computational Linguistics. 
Toulouse, France, pp. 9-16. 
Richardson, S. D., W. B. Dolan, A. Menezes, and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods. In 
Proceedings of MT Summit VIII, Santiago De 
Compostela, Spain, pp. 293-298.  
Richardson, S. D., W. B. Dolan, and L. 
Vanderwende. 1998 MindNet: Acquiring and 
structuring semantic information from text, 
ACL-98. pp. 1098-1102. 
Sato, S. and Nagao M. 1990. Toward 
memory-based translation. In Proceedings of 
COLING 1990, Helsinki, Finland, pp. 247-252. 
Suzuki, H., C. Brockett, and G. Kacmarcik. 2000. 
Using a broad-coverage parser for word-breaking 
in Japanese. In Proceedings of COLING 2000, 
Saarbrueken, Germany, pp. 822-827.  
Yamamoto K., and Y Matsumoto. 2000. 
Acquisition of phrase-level bilingual 
correspondence using dependency structure. In 
Proceedings of COLING 2000, Saarbrueken, 
Germany, pp. 933-939.  
Proceedings of the Workshop on Statistical Machine Translation, pages 158?161,
New York City, June 2006. c?2006 Association for Computational Linguistics
Microsoft Research Treelet Translation System:
NAACL 2006 Europarl Evaluation
Arul Menezes, Kristina Toutanova and Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{arulm,kristout,chrisq}@microsoft.com
Abstract
The  Microsoft  Research  translation  system  is  a
syntactically  informed  phrasal  SMT  system  that
uses  a  phrase  translation  model  based  on
dependency treelets and a global reordering model
based  on  the  source  dependency  tree.  These
models  are  combined  with  several  other
knowledge  sources  in  a  log-linear  manner.  The
weights of the individual components in the log-
linear model  are set  by an automatic  parameter-
tuning method.  We give a brief  overview of the
components  of  the  system  and  discuss  our
experience with the Europarl data translating from
English to Spanish.
1. Introduction
The  dependency  treelet  translation  system
developed at MSR is a statistical MT system that
takes  advantage  of  linguistic  tools,  namely  a
source language dependency parser,  as well as a
word alignment component. [1]
To  train  a  translation  system,  we  require  a
sentence-aligned parallel  corpus.  First  the source
side is parsed to obtain dependency trees. Next the
corpus  is  word-aligned,  and  the  source
dependencies  are  projected  onto  the  target
sentences  using  the  word  alignments.  From  the
aligned dependency corpus we extract  all  treelet
translation pairs,  and train an order model and a
bi-lexical dependency model.
To translate, we parse the input sentence, and
employ  a  decoder  to  find  a  combination  and
ordering of treelet translation pairs that cover the
source tree and are optimal according to a set of
models.  In  a  now-common generalization  of  the
classic  noisy-channel  framework,  we  use  a  log-
linear combination of models [2], as in below:
translation?S , F ,? ?=argmax
T {?f ?F ? f f ?S ,T ?}
Such an approach toward translation scoring has
proven  very  effective  in  practice,  as  it  allows  a
translation system to incorporate information from
a  variety  of  probabilistic  or  non-probabilistic
sources.  The weights  ? = {  ?f } are selected by
discriminatively training against held out data.
2. System Details
A brief word on notation: s and t represent source
and target lexical nodes; S and T represent source
and target trees; s and t represent source and target
treelets  (connected  subgraphs  of  the  dependency
tree).  The  expression  ?t? T refers  to  all  the
lexical items in the target language tree T and |T|
refers to the count of lexical items in  T. We use
subscripts to indicate selected words: Tn represents
the n
th
 lexical item in an in-order traversal of T.
2.1. Training
We  use  the  broad  coverage  dependency  parser
NLPWIN  [3]  to  obtain  source  language
dependency  trees,  and  we  use  GIZA++  [4]  to
produce  word  alignments.  The  GIZA++ training
regimen  and  parameters  are  tuned  to  optimize
BLEU [5] scores on held-out data. Using the word
alignments,  we  follow a  set  of  dependency  tree
projection  heuristics  [1]  to  construct  target
dependency  trees,  producing  a  word-aligned
parallel  dependency  tree  corpus.  Treelet
translation pairs are extracted by enumerating all
source treelets (to a maximum size) aligned to a
target treelet.
2.2. Decoding
We use a tree-based decoder, inspired by dynamic
programming. It searches for an approximation of
158
the n-best translations of each subtree of the input
dependency  tree.  Translation  candidates  are
composed from treelet  translation pairs  extracted
from the training corpus. This process is described
in more detail in [1].
2.3. Models
2.3.1. Channel models
We  employ  several  channel  models:  a  direct
maximum likelihood estimate of the probability of
target  given  source,  as  well  as  an  estimate  of
source given target and target given source using
the word-based IBM Model 1 [6]. For MLE, we
use  absolute  discounting  to  smooth  the
probabilities:
PMLE ? t?s ?= c ? s , t ???c ? s ,* ?
Here,  c represents  the  count  of  instances  of  the
treelet pair  ?s, t? in the training corpus, and  ? is
determined empirically.
For Model 1 probabilities we compute the sum
over all possible alignments of the treelet without
normalizing for length. The calculation of source
given  target  is  presented  below;  target  given
source is calculated symmetrically.
PM1? t?s ?=?t?t ?s?s P ? t?s ?
2.3.2. Bilingual n-gram channel models
Traditional  phrasal  SMT systems  are  beset  by a
number of theoretical problems, such as the ad hoc
estimation  of  phrasal  probability,  the  failure  to
model  the  partition  probability,  and  the  tenuous
connection  between  the  phrases  and  the
underlying  word-based  alignment  model.  In
string-based  SMT  systems,  these  problems  are
outweighed by the key role played by phrases in
capturing  ?local?  order.  In  the  absence  of  good
global  ordering  models,  this  has  led  to  an
inexorable  push  towards  longer  and  longer
phrases, resulting in serious practical problems of
scale, without, in the end, obviating the need for a
real global ordering story.
In [13] we discuss these issues in greater detail
and  also  present  our  approach  to  this  problem.
Briefly,  we  take  as  our  basic  unit  the  Minimal
Translation Unit (MTU) which we define as a set
of source and target word pairs such that there are
no word alignment links between distinct MTUs,
and  no  smaller  MTUs  can  be  extracted  without
violating the previous constraint.  In other words,
these are the minimal non-compositional phrases.
We then build models based on n-grams of MTUs
in  source  string,  target  string  and  source
dependency  tree  order.  These  bilingual  n-gram
models  in  combination  with  our  global  ordering
model allow us to use shorter phrases without any
loss  in  quality,  or  alternately  to  improve  quality
while keeping phrase size constant.
As an example,  consider the aligned sentence
pair in Figure 1. There are seven MTUs:
m1 = <we should / hemos>
m2 = <NULL / de>
m3 = <follow / cumplir>
m4 = <the / el>
m5 = <Rio / Rio>
m6 = <agenda / programa>
m7 = <NULL / de>
We can then predict the probability of each MTU
in the context of (a) the previous MTUs in source
order,  (b) the previous MTUs in target order,  or
(c) the ancestor MTUs in the tree. We consider all
of these traversal orders, each acting as a separate
feature function in the log linear combination. For
source and target traversal order we use a trigram
model, and a bigram model for tree order.
2.3.3. Target language models
We  use  both  a  surface  level  trigram  language
model  and a dependency-based bigram language
model  [7],  similar  to  the  bilexical  dependency
modes  used  in  some  English  Treebank  parsers
(e.g. [8]).
Psurf ?T ?=?i=1
?T?
Ptrisurf ?T i?T i?2 ,T i?1 ?
Pbilex ?T ?=?i=1
?T?
Pbidep ?T i?parent ?T i ??
Ptrisurf is a Kneser-Ney smoothed trigram language
model  trained  on  the  target  side  of  the  training
corpus,  and  Pbilex is  a  Kneser-Ney  smoothed
we??2 should??1 follow the??2 Rio??1 agenda?+1
hemos??1 de?+1 cumplir el??1 programa?+1 de??1 R?o?+1
Figure 1: Aligned dependency tree pair, annotated with head-
relative positions
159
bigram language model trained on target language
dependencies  extracted  from the aligned  parallel
dependency tree corpus.
2.3.4. Order model
The  order  model  assigns  a  probability  to  the
position  (pos)  of  each target  node relative  to its
head based on information in both the source and
target trees:
Porder ?order ?T ??S ,T ?=?t?T P ? pos ? t , parent ? t ???S ,T ?
Here, position is modeled in terms of closeness to
the head in the dependency tree. The closest pre-
modifier  of  a  given  head  has  position  -1;  the
closest  post-modifier  has  a  position  1.  Figure  1
shows an example dependency tree pair annotated
with head-relative positions.
We use a small set of features reflecting local
information in the dependency tree to model P(pos
(t,parent(t)) | S, T):
? Lexical items of t and parent(t), the parent of t
in the dependency tree.
? Lexical items of the source nodes aligned to t
and head(t).
? Part-of-speech  ("cat")  of  the  source  nodes
aligned to the head and modifier.
? Head-relative  position  of  the  source  node
aligned to the source modifier. 
These  features  along  with  the  target  feature  are
gathered  from  the  word-aligned  parallel
dependency  tree  corpus  and  used  to  train  a
statistical  model.  In  previous  versions  of  the
system,  we trained a decision tree model  [9].  In
the  current  version,  we  explored  log-linear
models. In addition to providing a different way of
combining  information  from  multiple  features,
log-linear models allow us to model the similarity
among different classes (target positions), which is
advantageous for our task.
 We  implemented  a  method  for  automatic
selection  of  features  and  feature  conjunctions  in
the log-linear model. The method greedily selects
feature  conjunction  templates  that  maximize  the
accuracy  on  a  development  set.  Our  feature
selection  study  showed  that  the  part-of-speech
labels of the source nodes aligned to the head and
the modifier and the head-relative position of the
source  node  corresponding  to  the  modifier  were
the  most  important  features.  It  was  useful  to
concatenate the part-of-speech of the source head
with  every  feature.  This  effectively  achieves
learning  of  separate  movement  models  for  each
source head category. Lexical information on the
pairs  of  head  and  dependent  in  the  source  and
target was also very useful.
To model the similarity among different target
classes  and  to  achieve  pooling  of  data  across
similar classes, we added multiple features of the
target position. These features let our model know,
for  example,  that  position  -5  looks  more  like
position  -6  than  like  position  3.  We  added  a
feature  ?positive?/?negative?  which  is  shared  by
all  positive/negative  positions.  We  also  added  a
feature looking at the displacement of a position in
the target from the corresponding position in the
source  and  features  which  group  the  target
positions  into  bins.  These  features  of  the  target
position are combined with features of the input.
This  model  was  trained  on  the  provided
parallel  corpus.  As  described  in  Section  2.1  we
parsed the source sentences,  and projected target
dependencies.  Each  head-modifier  pair  in  the
resulting target trees constituted a training instance
for the order model.
The  score  computed  by  the  log-linear  order
model is used as a single feature in the overall log-
linear  combination  of  models  (see  Section  1),
whose  parameters  were  optimized  using
MaxBLEU  [2].  This  order  model  replaced  the
decision tree-based model described in [1]. 
We compared  the  decision  tree  model  to  the
log-linear  model  on  predicting  the  position  of  a
modifier  using  reference  parallel  sentences,
independent of the full MT system. The decision
tree  achieved  per  decision  accuracy  of  69%
whereas  the  log-linear  model  achieved  per
decision accuracy of 79%.
1
 In the context of the
full  MT system,  however,  the  new order  model
provided  a  more  modest  improvement  in  the
BLEU score of 0.39%.
2.3.5. Other models
We include two pseudo-models that help balance
certain biases inherent in our other models.
? Treelet  count.  This  feature  is  a  count  of
treelets  used  to  construct  the  candidate.  It
acts as a bias toward translations that use a
smaller  number  of  treelets;  hence  toward
larger  sized  treelets  incorporating  more
context.
? Word count. We also include a count of the
words  in  the  target  sentence.  This  feature
1
 The per-decision accuracy numbers were obtained on
different (random) splits of training and test data.
160
helps  to  offset  the  bias  of  the  target
language model toward shorter sentences.
3. Discussion
We participated in  the English to  Spanish  track,
using  the  supplied  bilingual  data  only.  We used
only the target side of the bilingual corpus for the
target  language  model,  rather  than  the  larger
supplied  language  model.  We  did  find  that
increasing the target language order from 3 to 4
had a noticeable impact on translation quality. It is
likely that a larger target language corpus would
have an impact, but we did not explore this.
BLEU
Baseline treelet system 27.60
Add bilingual MTU models 28.42
Replace DT order model with log-linear model 28.81
Table 1: Results on development set
We found  that  the  addition of  bilingual  n-gram
based  models  had  a  substantial  impact  on
translation  quality.  Adding  these  models  raised
BLEU scores about 0.8%, but anecdotal evidence
suggests  that  human-evaluated  quality  rose  by
much more than the BLEU score difference would
suggest. In general, we felt that in this corpus, due
to the great diversity in translations for the same
source language words and phrases, and given just
one reference translation, BLEU score correlated
rather  poorly  with  human  judgments.  This  was
borne out in the human evaluation of the final test
results.  Humans  ranked  our  system  first  and
second,  in-domain  and  out-of-domain
respectively, even though it was in the middle of a
field of ten systems by BLEU score. Furthermore,
n-gram  channel  models  may  provide  greater
robustness. While our BLEU score dropped 3.61%
on out-of-domain data, the average BLEU score of
the other nine competing systems dropped 5.11%.
4. References
[1] Quirk,  C.,  Menezes,  A.,  and Cherry,  C.,  "Dependency
Tree Translation: Syntactically Informed Phrasal SMT",
Proceedings of ACL 2005, Ann Arbor, MI, USA, 2005.
[2] Och, F.  J.,  and Ney, H.,  "Discriminative  Training and
Maximum  Entropy  Models  for  Statistical  Machine
Translation",  Proceedings  of  ACL  2002,  Philadelphia,
PA, USA, 2002.
[3] Heidorn, G., ?Intelligent writing assistance?, in Dale et
al.  Handbook of Natural Language Processing, Marcel
Dekker, 2000.
[4] Och, F.  J.,  and Ney H.,  "A Systematic Comparison of
Various  Statistical  Alignment  Models",  Computational
Linguistics, 29(1):19-51, March 2003.
[5] Papineni,  K.,  Roukos,  S.,  Ward,  T.,  and  Zhu,  W.-J.,
"BLEU: a method for automatic evaluation of machine
translation",  Proceedings  of  ACL  2002,  Philadelphia,
PA, USA, 2002.
[6] Brown,  P.  F.,  Della Pietra,  S.,  Della Pietra,  V. J.,  and
Mercer, R. L., "The Mathematics of Statistical Machine
Translation:  Parameter  Estimation",  Computational
Linguistics 19(2): 263-311, 1994.
[7] Aue,  A.,  Menezes,  A.,  Moore,  R.,  Quirk,  C.,  and
Ringger,  E.,  "Statistical  Machine  Translation  Using
Labeled Semantic Dependency Graphs." Proceedings of
TMI 2004, Baltimore, MD, USA, 2004.
[8] Collins,  M.,  "Three  generative,  lexicalised  models  for
statistical  parsing",  Proceedings of ACL 1997,  Madrid,
Spain, 1997.
[9] Chickering,  D.M.,  "The  WinMine  Toolkit",  Microsoft
Research  Technical  Report  MSR-TR-2002-103,
Redmond, WA, USA, 2002.
[10] Och,  F.  J.,  Gildea,  D.,  Khudanpur,  S.,  Sarkar,  A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D.,
Eng,  K.,  Jain,  V.,  Jin,  Z.,  and  Radev,  D.,  "A
Smorgasbord  of  Features  for  Statistical  Machine
Translation". Proceedings of HLT/NAACL 2004, Boston,
MA, USA, 2004.
[11] Bender,  O.,  Zens,  R.,  Matsuov,  E.  and  Ney,  H.,
"Alignment  Templates:  the  RWTH  SMT  System".
IWSLT Workshop at INTERSPEECH 2004, Jeju Island,
Korea, 2004.
[12] Och, F. J., "Minimum Error Rate Training for Statistical
Machine  Translation",  Proceedings  of  ACL  2003,
Sapporo, Japan, 2003.
[13] Quirk,  C  and  Menezes,  A,  ?Do  we  need  phrases?
Challenging  the  conventional  wisdom  in  Statistical
Machine  Translation?,  Proceedings  of  HLT/NAACL
2006, New York, NY, USA, 2006
161
Proceedings of the Second Workshop on Statistical Machine Translation, pages 1?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
Using Dependency Order Templates to Improve Generality in 
Translation  
Arul Menezes and Chris Quirk
Microsoft Research 
One Microsoft Way, Redmond, WA 98052, USA 
{arulm, chrisq}@microsoft.com 
 
Abstract 
Today's statistical machine translation 
systems generalize poorly to new 
domains. Even small shifts can cause 
precipitous drops in translation quality. 
Phrasal systems rely heavily, for both 
reordering and contextual translation, on 
long phrases that simply fail to match out-
of-domain text. Hierarchical systems 
attempt to generalize these phrases but 
their learned rules are subject to severe 
constraints. Syntactic systems can learn 
lexicalized and unlexicalized rules, but the 
joint modeling of lexical choice and 
reordering can narrow the applicability of 
learned rules. The treelet approach models 
reordering separately from lexical choice, 
using a discriminatively trained order 
model, which allows  treelets to apply 
broadly, and has shown better 
generalization to new domains, but suffers 
a factorially large search space. We 
introduce a new reordering model based 
on dependency order templates, and show 
that it outperforms both phrasal and treelet 
systems on in-domain and out-of-domain 
text, while limiting the search space. 
1 Introduction 
Modern phrasal SMT systems such as (Koehn et 
al., 2003) derive much of their power from being 
able to memorize and use long phrases. Phrases 
allow for non-compositional translation, local 
reordering and contextual lexical choice. 
However the phrases are fully lexicalized, which 
means they generalize poorly to even slightly out-
of-domain text. In an open competition (Koehn & 
Monz, 2006) systems trained on parliamentary 
proceedings were tested on text from 'news 
commentary' web sites, a very slightly different 
domain. The 9 phrasal systems in the English to 
Spanish track suffered an absolute drop in BLEU 
score of between 4.4% and 6.34% (14% to 27% 
relative). The treelet system of Menezes et al 
(2006) fared somewhat better but still suffered an 
absolute drop of 3.61%.  
Clearly there is a need for approaches with 
greater powers of generalization. There are 
multiple facets to this issue, including handling of 
unknown words, new senses of known words etc. 
In this work, we will focus on the issue of 
reordering, i.e. can we learn how to transform the 
sentence structure of one language into the 
sentence structure of another, in a way that is not 
tied to a specific domain or sub-domains, or 
indeed, sequences of individual words.   
An early attempt at greater generality in a 
purely phrasal setting was the alignment template 
approach (Och & Ney 2004); newer approaches 
include formally syntactic (Chiang 2005), and 
linguistically syntactic approaches (Quirk et al 
2005), (Huang et al 2006). In the next section, we 
examine these representative approaches to the 
reordering problem. 
2 Related Work 
Our discussion of related work will be grounded 
in the following tiny English to Spanish example, 
where the training set includes:  
a very old book 
un libro m?s  antiguo 
a  book  very old1 
the old man 
el  hombre viejo 
the man    old 
it is very important 
es muy  importante 
is very important 
                                                           
1 English gloss of Spanish sentences in italics. 
1
and the test sentence and reference translation are 
a very old man 
un hombre muy  viejo 
a  man    very old 
Note that while the first training pair has the 
correct structure for the test sentence, most of the 
contextually correct lexical choices come from 
the other two pairs. 
2.1 Phrasal translation, Alignment templates 
The relevant phrases (i.e. those that match the test 
sentence) extracted from these training pairs are 
shown in Table 2.1. Only phrases up to size 3 are 
shown. The ones in italics are 'correct' in that they 
can lead to the reference translation. Note that 
none of the multi-word phrases lead to the 
reference, so the local reordering often captured 
in the phrasal model is no help at all in ordering 
this sentence. The system is unable to learn the 
correct structure from the first sentence because 
the words are wrong, and from the second 
sentence even though the phrase old man has the 
right words in the right order, it does not lead to 
the reference translation because the translation of 
very cannot be inserted in the right place.  
a un 
very m?s 
old antiguo 
very old m?s antiguo 
old viejo 
man hombre 
old man hombre viejo 
very muy 
Table 2.1: Relevant extracted phrases 
Looking at this as a sparse data issue we might 
suspect that generalization could solve the 
problem. The alignment template approach (Och 
& Ney, 2004) uses word classes rather than 
lexical items to model phrase translation. Yet this 
approach loses the advantage of context-sensitive 
lexical selection: the word translation model 
depends only on the word classes to subcategorize 
for translations, which leads to less accurate 
lexical choice in practice (Zens & Ney, 2004). 
2.2 Hierarchical translation 
Hierarchical systems (Chiang, 2005) induce a 
context-free grammar with one non-terminal 
directly from the parallel corpus, with the 
advantage of not requiring any additional 
knowledge source or tools, such as a treebank or a 
parser. However this can lead to an explosion of 
rules. In order to make the problem tractable and 
avoid spurious ambiguity, Chiang restricts the 
learned rules in several ways. The most 
problematic of these is that every rule must have 
at least one pair of aligned words, and that 
adjacent non-terminals are not permitted on the 
source side. In Table 2.2 we show the additional 
hierarchical phrases that would be learned from 
our training pairs under these restrictions. Again 
only those applicable to the test sentence are 
shown and the 'correct' rules, i.e. those that lead to 
the reference, are italicized. 
X1 old X1 antiguo 
very X1 m?s X1 
very old X1 X1 m?s antiguo 
X1 old X2 X2 X1 antiguo 
very X1 X2 X2 m?s X1 
X1 man hombre X1 
old X1 X1 viejo 
X1 old man X1 hombre viejo 
X1 very X1 muy 
very X2 muy X2 
X1 very X2 X1 muy X2 
Table 2.2: Additional hierarchical phrases 
Note that even though from the first pair, we learn 
several rules with the perfect reordering for the 
test sentence, they do not lead to the reference 
because they drag along the contextually incorrect 
lexical choices. From the second pair, we learn a 
rule (X1 old man) that has the right contextual 
word choice, but  does not lead to the reference, 
because the paucity of the grammar's single non-
terminal causes this rule to incorrectly imply that 
the translation of very be placed before hombre. 
2.3 Constituency tree transduction 
An alternate approach is to use linguistic 
information from a parser. Transduction rules 
between Spanish strings and English trees can be 
learned from a word-aligned parallel corpus with 
parse trees on one side (Graehl & Knight, 2004). 
Such rules can be used to translate from Spanish 
to English by searching for the best English 
language tree for a given Spanish language string 
(Marcu et al, 2006). Alternately English trees 
produced by a parser can be transduced to 
2
Spanish strings using the same rules (Huang et al, 
2006). Translation rules may reach beyond one 
level in the syntax tree; this extended domain of 
locality allows many phenomena including both 
lexicalized and unlexicalized rules. However 
reordering and translation are modeled jointly, 
which may exacerbate data sparsity. Furthermore 
it forces the system to pick between unlexicalized 
rules that capture reordering and lexicalized rules 
that model context-sensitive translation. 
For instance, the following rules can be 
extracted from the first sentence of the corpus: 
 
r1: un x1 x2 ? NP(DT(a) ADJP:x2 NN:x1) 
r2: x1 x2 ? ADJP(RB:x1 JJ:x2) 
  
Although together they capture the necessary 
reordering for our test sentence pair, they do not 
allow for context sensitive translations of the 
ambiguous terms very and old; each must be 
selected independently. Disappointingly, no 
single constituency tree transduction rule derived 
from this corpus translates old man as hombre 
viejo in a single step on the test sentence: the 
syntactic structures are slightly different, but the 
difference is sufficient to prevent matching. 2 
Again we note that phrases provide utility by 
capturing both reordering and context. While xRS 
                                                           
2 Marcu et al (2006) and Zollmann et al (2006) recognize 
this problem and attempt to alleviate it by grafting surface 
phrases into constituency trees by various methods. 
rules provide an elegant and powerful model of 
reordering, they come with a potential cost in 
context-sensitive translation.  
2.4 Dependency treelet translation 
We previously described (Quirk et al 2005) a 
linguistically syntax-based system that parses the 
source language, uses word-based alignments to 
project a target dependency tree, and extracts 
paired dependency tree fragments (treelets) 
instead of surface phrases.  In contrast to the xRS 
approach, ordering is very loosely coupled with 
translation via a separate discriminatively trained 
dependency tree-based order model. The switch 
to a dependency parse also changes the 
conditioning information available for translation: 
related lexical items are generally adjacent, rather 
than separated by a path of unlexicalized non-
terminals. In effect, by using a looser matching 
requirement, treelets retain the context-sensitive 
lexical choice of phrases: treelets must only be a 
connected subgraph of the input sentence to be 
applicable; some children may remain uncovered. 
Figure 2.2 shows source dependency parses 
and projected target dependencies for our training 
data; Figure 2.3 shows the treelet pairs that this 
system would extract that match the input 
a very old book
DT RB JJ NN
ADJP
NP
un libro m?s antiguo
the old man
DT JJ NN
NP
el hombre viejo
it is very important
PN VB RB JJ
ADJP
VP
S
es muy importante  
Figure 2.1:  Constituency parses 
 
Figure 2.2: Dependency trees for training pairs 
 
Figure 2.3: Relevant extracted treelets 
3
sentence (treelets of size 1 are not shown).  The 
second treelet supplies the order of viejo with 
respect to its head, and unlike the case with xRS 
rules, we can use this to make the correct 
contextual word choice. The difference is that 
because xRS rules provide both reordering and 
word choice, each rule must match all of the 
children at any given tree node. On the other 
hand, treelets are allowed to match more loosely. 
The translations of the unmatched children (un 
and muy in this case) are placed by exploring all 
possible orderings and scoring them with both 
order model and language model. Although this 
effectively decouples lexical selection from 
ordering, it comes at a huge cost in search space 
and translation quality may suffer due to search 
error. However, as mentioned in Section 1, this 
approach is able to generalize better to out-of-
domain data than phrasal approaches. Koehn and 
Monz (2006) also include a human evaluation, in 
which this system ranked noticeably higher than 
one might have predicted from its BLEU score.    
3 Dependency Order Templates 
The Dependency Order Templates approach 
leverages the power of the xR rule formalism, 
while avoiding the problems mentioned in Section 
2.3, by constructing the rules on the fly from two 
separately matched components: (a) Dependency 
treelet translation pairs described in Section 2.4 
that capture contextual lexical translations but are 
underspecified with respect to ordering, and (b) 
Order templates, which are unlexicalized rules 
(over dependency, rather than constituency trees) 
that capture reordering phenomena. 
Formally, an order template is an unlexicalized 
transduction rule mapping dependency trees 
containing only parts of speech to unlexicalized 
target language trees (see Figure 4.1b). 
Given an input sentence, we combine relevant 
treelet translation pairs and order templates to 
construct lexicalized transduction rules for that 
sentence, and then decode using standard 
transduction approaches. By keeping lexical and 
ordering information orthogonal until runtime, we 
can produce novel transduction rules not seen in 
the training corpus. This allows greater 
generalization capabilities than the constituency 
tree transduction approaches of Section 2.3. 
As compared to the treelet approach described 
in Section 2.4, the generalization capability is 
somewhat reduced. In the treelet system all 
reorderings are exhaustively evaluated, but the 
size of the search space necessitates tight pruning, 
leading to significant search error. By contrast, in 
the order template approach we consider only 
reorderings that are captured in some order 
template. The drastic reduction in search space 
leads to an overall improvement, not only in 
decoding speed, but also in translation quality due 
to reduced search error. 
3.1 Extracting order templates 
For each pair of parallel training sentences, we 
parse the source sentence, obtain a source 
dependency tree, and use GIZA++ word 
alignments to project a target dependency tree as 
described in Quirk et al (2005).  
Given this pair of aligned source and target 
dependency trees, we recursively extract one 
order template for each pair of aligned non-leaf 
source and target nodes. In the case of multi-word 
alignments, all contiguous 3  aligned nodes are 
added to the template. Next we recursively add 
child nodes as follows: For each node in the 
template, add all its children. For each such child, 
if it is aligned, stop recursing, if it is unaligned, 
recursively add its children.     
On each template node we remove the lexical 
items; we retain the part of speech on the source 
nodes (we do not use target linguistic features). 
We also keep node alignment information4. The 
resulting aligned source and target sub-graphs 
comprise the order template. Figure 4.1b lists the 
order templates extracted from the training pairs 
in Figure 2.1 that capture all the patterns 
necessary to correctly reorder the test sentence. 
4 Decoding 
Decoding is treated as a problem of syntax-
directed transduction. Input sentences are 
segmented into a token stream, annotated with 
part-of-speech information, and parsed into 
                                                           
3 If a multi-word alignment is not contiguous in either source 
or target dependency tree no order template is extracted. 
4 If a source or target node aligns to a tree node outside the 
template, the template breaks phrasal cohesion and is 
currently discarded. We intend to address these 'structural 
divergence' patterns in future work. 
4
unlabeled dependency trees. At each node in the 
input dependency tree we first find the set of 
matching treelet pairs: A pair matches if its source 
side corresponds to a connected subgraph of the 
input tree. Next we find matching order 
templates: order templates must also match a 
connected subgraph of the input tree, but in 
addition, for each input node, the template must 
match either all or none of its children 5 . 
Compatible combinations of treelets and order 
templates are merged to form xR rules. Finally, 
we search for the best transduction according to 
the constructed xR rules as scored by a log-linear 
combination of models (see Section 5). 
4.1 Compatibility 
A treelet and an order template are considered 
compatible if the following conditions are met: 
The treelet and the matching portions of the 
template must be structurally isomorphic. Every 
treelet node must match an order template node. 
Matching nodes must have the same part of 
speech. Unaligned treelet nodes must match an 
unaligned template node. Aligned treelet nodes 
must match aligned template nodes. Nodes that 
are aligned to each other in the treelet pair must 
match template nodes that are aligned to each 
other. 
4.2 Creating transduction rules 
Given a treelet, we can form a set of tree 
transduction rules as follows. We iterate over 
each source node n in the treelet pair; let s be the 
corresponding node in the input tree (identified 
during the matching). If, for all children of s there 
is a corresponding child of n, then this treelet 
specifies the placement of all children and no 
changes are necessary. Otherwise we pick a 
template that matched at s and is compatible with 
the treelet. The treelet and template are unified to 
produce an updated rule with variables on the 
source and target sides for each uncovered child 
of s. When all treelet nodes have been visited, we 
are left with a transduction rule that specifies the 
translation of all nodes in the treelet and contains 
variables that specify the placement of all 
                                                           
5 This is so the resulting rules fit within the xR formalism. At 
each node, a rule either fully specifies its ordering, or 
delegates the translation of the subtree to other rules.  
uncovered nodes. Due to the independence of 
ordering and lexical information, we may produce 
novel transduction rules not seen in the training 
corpus. Figure 4.1 shows this process as it applies 
to the test sentence in Section 2. 
If, at any node s, we cannot find a matching 
template compatible with the current treelet, we 
create an artificial source order template, which 
simply preserves the source language order in the 
target translation. We add a feature function that 
counts the number of such templates and train its 
weight during minimum error rate training. 
4.3 Transduction using xR rules 
In the absence of a language model or other 
contextually dependent features, finding the 
highest scoring derivation would be a simple 
dynamic program (Huang et al 2006) 6.However 
exact search using an ? -gram language model 
leads to split states for each ? -gram context. 
Instead we use an approximate beam search 
moving bottom-up in the tree, much like a CKY 
parser. Candidates in this search are derivations 
with respect to the transducer. 
Each transduction rule ?  has a vector of 
variables ???,? ??? . Each variable is associated 
with an input node ????. For each input node ?, 
we keep a beam of derivations ????. Derivations 
are represented as a pair ??, ??  where ?  is a 
transduction rule and ? ? ?? is a vector with one 
integer for each of the ?  variables in ? . The 
interpretation is that the complete candidate can 
be constructed by recursively substituting for each 
                                                           
6 Like Chiang (2005) we only search for the yield of the most 
likely derivation, rather than the most likely yield. 
Figure 4.1: Merging templates and treelets 
5
??? ?  ??? ????  the candidate constructed from 
the ?? th entry in the beam ?????????.  
Figure 4.2 describes the transduction process. 
Since we approach decoding as xR transduction, 
the process is identical to that of constituency-
based algorithms (e.g. Huang and Chiang, 2007). 
There are several free parameters to tune: 
? Beam size ? Maximum number of candidates 
per input node (in this paper we use 100) 
? Beam threshold ? maximum range of scores 
between top and bottom scoring candidate 
(we use a logprob difference of 30) 
? Maximum combinations considered ? To 
bound search time, we can stop after a 
specified number of elements are popped off 
the priority queue (we use 5000) 
5 Models 
We use all of the Treelet models we described in 
Quirk et al (2005) namely:  
? Treelet table with translation probabilities 
estimated using maximum likelihood, with 
absolute discounting.  
? Discriminative tree-based order model. 
? Forward and backward lexical weighting, 
using Model-1 translation probabilities. 
? Trigram language model using modified 
Kneser-Ney smoothing.  
? Word and phrase count feature functions. 
In addition, we introduce the following: 
? Order template table, with template 
probabilities estimated using maximum 
likelihood, with absolute discounting. 
? A feature function that counts the number of 
artificial source order templates (see below) 
used in a candidate. 
The models are combined in a log-linear 
framework, with weights trained using minimum 
error rate training to optimize the BLEU score. 
6 Experiments 
We evaluated the translation quality of the system 
using the BLEU metric (Papineni et al, 2002). 
We compared our system to Pharaoh, a leading 
phrasal SMT decoder (Koehn et al, 2003), and 
our treelet system. We report numbers for English 
to Spanish. 
6.1 Data 
We used the Europarl corpus provided by the 
NAACL 2006 Statistical Machine Translation 
workshop. The target language model was trained 
using only the target side of the parallel corpus. 
The larger monolingual corpus was not utilized.  
The corpus consists of European Parliament 
proceedings, 730,740 parallel sentence pairs of 
English-Spanish, amounting to about 15M words 
in each language. The test data consists of 2000 
sentences each of development (dev), 
development-test (devtest) and test data (test) 
from the same domain. There is also a separate set 
of 1064 test sentences (NC-test) gathered from 
"news commentary" web sites.  
6.2 Training 
We parsed the source (English) side of the corpus 
using NLPWIN, a broad-coverage rule-based 
parser able to produce syntactic analyses at 
varying levels of depth (Heidorn, 2002). For the 
purposes of these experiments we used a 
dependency tree output with part-of-speech tags 
and unstemmed, case-normalized surface words. 
For word alignment we used GIZA++, under a 
training regimen of five iterations of Model 1, 
five iterations of HMM, and five iterations of 
Model 4, in both directions. The forward and 
backward alignments were symmetrized using a 
tree-based heuristic combination. The word 
GetTranslationBeam(?) // memoized 
    prioq ? ? 
    beam ? ? 
    for ? ? ???? 
        Enqueue(prioq, ??, ??, EarlyScore(??, ??)) 
    while Size(prioq) ? 0 
        ??, ?? ? PopBest(prioq) 
        AddToBeam(beam, ??, ??, TrueScore(??, ??)) 
        for ? in 1. . |?| 
            Enqueue(prioq, ??, ? ? ???, 
                 EarlyScore(??, ? ? ???)) 
    return beam 
EarlyScore(??, ??) 
    ? ? RuleScore(?) 
    for ? in 1. . |?| 
        ? ? InputNode(GetVariable (?, ?)) 
        beam ? GetTranslationBeam(?) 
        ? ? ? ?TrueScore(GetNthEntry(beam, ??)) 
    return ? 
Figure 4.2: Beam tree transduction 
6
alignments and English dependency tree were 
used to project a target tree. From the aligned tree 
pairs we extracted a treelet table and an order 
template table.  
The comparison treelet system was identical 
except that no order template model was used. 
The comparison phrasal system was 
constructed using the same GIZA++ alignments 
and the heuristic combination described in (Och 
& Ney, 2003). Except for the order models 
(Pharaoh uses a penalty on the deviance from 
monotone), the same models were used. 
All systems used a treelet or phrase size of 7 
and a trigram language model. Model weights 
were trained separately for all 3 systems using 
minimum error rate training to maximize BLEU 
(Och, 2003) on the development set (dev). Some 
decoder pruning parameters were tuned on the 
development test (devtest). The test and NC-test 
data sets were not used until final tests. 
7 Results 
We present the results of our system comparisons 
in Table 7.1 and Figure 7.1 using three different 
test sets: The in-domain development test data 
(devtest), the in-domain blind test data (test) and 
the out-of-domain news commentary test data 
(NC-test). All differences (except phrasal vs. 
template on devtest), are statistically significant at 
the p>=0.99 level under the bootstrap resampling 
test. Note that while the systems are quite 
comparable on the in-domain data, on the out-of-
domain data the phrasal system's performance 
drops precipitously, whereas the performance of 
the treelet and order template systems drops much 
less, outperforming the phrasal system by 2.7% 
and 3.46% absolute BLEU. 
 devtest test NC-test
Phrasal 0.2910 0.2935 0.2354
Treelet 0.2819 0.2981 0.2624
Template 0.2896 0.3045 0.2700
Table 7.1: System Comparisons across domains 
Further insight may be had by comparing the 
recall 7  for different n-gram orders (Table 7.2). 
The phrasal system suffers a greater decline in the 
higher order n-grams than the treelet and template 
                                                           
7 n-gram precision cannot be directly compared across output 
from different systems due to different levels of 'brevity' 
systems, indicating that latter show improved 
generality in reordering. 
  1gm 2gm 3gm 4gm 
Test Phrasal 0.61 0.35 0.23 0.15 
 treelet 0.62 0.36 0.23 0.15 
 template 0.62 0.36 0.24 0.16 
NC-test phrasal 0.58 0.30 0.17 0.10 
 treelet 0.60 0.33 0.20 0.12 
 template 0.61 0.34 0.20 0.13 
Table 7.2: n-gram recall across domains 
7.1 Treelet vs. Template systems 
As described in Section 3.1, the order templates 
restrict the broad reordering space of the treelet 
system. Although in theory this might exclude 
reorderings necessary for some translations, Table 
7.3 shows that in practice, the drastic search space 
reduction allows the decoder to explore a wider 
beam and more rules, leading to reduced search 
error and increased translation speed. (The topK 
parameter is the number of phrases explored for 
each span, or rules/treelets for each input node.) 
 Devtest 
BLEU 
Sents. 
per sec 
Pharaoh, beam=100, topK=20 0.2910 0.94 
Treelet, beam=12, topK=5 0.2819 0.21 
Template, beam=100, topK=20 0.2896 0.56 
Table 7.3: Performance comparisons 
Besides the search space restriction, the other 
significant change in the template system is to 
include MLE template probabilities as an 
 
Figure 7.1: In-domain vs. Out-of-domain BLEU 
23
24
25
26
27
28
29
30
31
development in-domain out-of-domain
Phrasal Treelet Order Template
7
additional feature function. Given that the 
template system operates over rules where the 
ordering is fully specified, and that most tree 
transduction systems use MLE rule probabilities 
to model both lexical selection and reordering, 
one might ask if the treelet system's 
discriminatively trained order model is now 
redundant. In Table 7.4 we see that this is not the 
case.8 (Differences are significant at p>=0.99.) 
 devtest test NC-test 
MLE model only 0.2769 0.2922 0.2512 
Discriminative and 
MLE models 
0.2896 0.3045 0.2700 
Table 7.4: Templates and discriminative order model 
Finally we examine the role of frequency 
thresholds in gathering templates. In Table 7.5 it 
may be seen that discarding singletons reduces 
the table size by a factor of 5 and improves 
translation speed with negligible degradation in 
quality. 
 devtest 
BLEU 
Number of 
templates 
Sentences 
per sec. 
No threshold 0.2898 752,165 0.40 
Threshold=1 0.2896 137,584 0.56 
Table 7.5: Effect of template count cutoffs 
8 Conclusions and Future Work 
We introduced a new model of Dependency Order 
Templates that provides for separation of lexical 
choice and reordering knowledge, thus allowing 
for greater generality than the phrasal and xRS 
approaches, while drastically limiting the search 
space as compared to the treelet approach. We 
showed BLEU improvements over phrasal of over 
1% in-domain and nearly 3.5% out-of-domain. As 
compared to the treelet approach we showed an 
improvement of about 0.5%, but a speedup of 
nearly 3x, despite loosening pruning parameters.  
Extraposition and long distance movement still 
pose a serious challenge to syntax-based machine 
translation systems. Most of the today's search 
algorithms assume phrasal cohesion. Even if our 
search algorithms could accommodate such 
movement, we don't have appropriate models to 
                                                           
8 We speculate that other systems using transducers with 
MLE probabilities may also benefit from additional 
reordering models. 
account for such phenomena. Our system already 
extracts extraposition templates, which are a step 
in the right direction, but may prove too sparse 
and brittle to account for the range of phenomena.  
References 
Chiang, David. A hierarchical phrase-based model for 
statistical machine translation. ACL 2005.  
Galley, Michel, Mark Hopkins, Kevin Knight, and Daniel 
Marcu. What?s in a translation rule? HLT-NAACL 2004. 
Graehl, Jonathan and Kevin Knight. Training Tree 
Transducers. NAACL 2004. 
Heidorn, George. ?Intelligent writing assistance?. In Dale et 
al. Handbook of Natural Language Processing, Marcel 
Dekker. (2000) 
Huang, Liang, Kevin Knight, and Aravind Joshi. Statistical 
Syntax-Directed Translation with Extended Domain of 
Locality. AMTA 2006 
Huang, Liang and David Chiang. Faster Algorithms for 
Decoding with Integrated Language Models.  ACL 2007 
(to appear) 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase based translation. NAACL 2003. 
Koehn, Philipp and Christof Monz. Manual and automatic 
evaluation of machine translation between european 
languages. Workshop on Machine Translation, NAACL 
2006. 
Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and Kevin 
Knight. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-2006. 
Menezes, Arul, Kristina Toutanova and Chris Quirk. 
Microsoft Research Treelet translation system: NAACL 
2006 Europarl evaluation. Workshop on Machine 
Translation, NAACL 2006 
Och, Franz Josef and Hermann Ney. A systematic 
comparison of various statistical alignment models, 
Computational Linguistics, 29(1):19-51 (2003).  
Och, Franz Josef. Minimum error rate training in statistical 
machine translation. ACL 2003. 
Och, Franz Josef and Hermann Ney: The Alignment 
Template Approach to Statistical Machine Translation. 
Computational Linguistics 30 (4): 417-449 (2004) 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing 
Zhu. BLEU: a method for automatic evaluation of 
machine translation. ACL 2002. 
Quirk, Chris, Arul Menezes, and Colin Cherry. Dependency 
Tree Translation: Syntactically informed phrasal SMT. 
ACL 2005 
Zens, Richard and Hermann Ney. Improvements in phrase-
based statistical machine translation. HLT-NAACL 2004  
8
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1577?1586,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Social Text Normalization using Contextual Graph Random Walks
Hany Hassan
Microsoft Research
Redmond, WA
hanyh@microsoft.com
Arul Menezes
Microsoft Research
Redmond, WA
arulm@microsoft.com
Abstract
We introduce a social media text normal-
ization system that can be deployed as a
preprocessing step for Machine Transla-
tion and various NLP applications to han-
dle social media text. The proposed sys-
tem is based on unsupervised learning of
the normalization equivalences from unla-
beled text. The proposed approach uses
Random Walks on a contextual similarity
bipartite graph constructed from n-gram
sequences on large unlabeled text corpus.
We show that the proposed approach has a
very high precision of (92.43) and a rea-
sonable recall of (56.4). When used as
a preprocessing step for a state-of-the-art
machine translation system, the translation
quality on social media text improved by
6%. The proposed approach is domain and
language independent and can be deployed
as a preprocessing step for any NLP appli-
cation to handle social media text.
1 Introduction
Social Media text is usually very noisy and con-
tains a lot of typos, ad-hoc abbreviations, pho-
netic substitutions, customized abbreviations and
slang language. The social media text is evolving
with new entities, words and expressions. Natural
language processing and understanding systems
such as Machine Translation, Information Extrac-
tion and Text-to-Speech are usually trained and
optimized for clean data; therefore such systems
would face a challenging problem with social me-
dia text.
Various social media genres developed distinct
characteristics. For example, SMS developed a
nature of shortening messages to avoid multiple
keystrokes. On the other hand, Facebook and in-
stant messaging developed another genre where
more emotional expressions and different abbre-
viations are very common. Somewhere in be-
tween, Twitter?s statuses come with some brevity
similar to SMS along with the social aspect of
Facebook. On the same time, various social me-
dia genres share many characteristics and typo
styles. For example, repeating letters or punctu-
ation for emphasizing and emotional expression
such as ??goooood morniiing??. Using phonetic
spelling in a generalized way or to reflect a lo-
cal accent; such as ??wuz up bro?? (what is up
brother). Eliminating vowels such as ??cm to c
my luv??. Substituting numbers for letters such as
??4get?? (forget) , ??2morrow?? (tomorrow), and
??b4?? (before). Substituting phonetically sim-
ilar letters such as ??phone?? (fon). Slang ab-
breviations which usually abbreviates multi-word
expression such as ??LMS?? (like my status) ,
??idk?? (i do not know), ??rofl?? (rolling on floor
laughing).
While social media genres share many charac-
teristics, they have significant differences as well.
It is crucial to have a solution for text normaliza-
tion that can adapt to such variations automati-
cally. We propose a text normalization approach
using an unsupervised method to induce normal-
ization equivalences from noisy data which can
adapt to any genre of social media.
In this paper, we focus on providing a solu-
tion for social media text normalization as a pre-
processing step for NLP applications. However,
this is a challenging problem for several reasons.
First, it is not straightforward to define the Out-of-
Vocabulary (OOV) words. Traditionally, an OOV
word is defined as a word that does not exist in
the vocabulary of a given system. However, this
definition is not adequate for the social media text
which has a very dynamic nature. Many words
and named entities that do not exist in a given vo-
cabulary should not be considered for normaliza-
tion. Second, same OOV word may have many
1577
appropriate normalization depending on the con-
text and on the domain. Third, text normalization
as a preprocessing step should have very high pre-
cision; in other words, it should provide conser-
vative and confident normalization and not over-
correct. Moreover, the text normalization should
have high recall, as well, to have a good impact on
the NLP applications.
In this paper, we introduce a social media text
normalization system which addresses the chal-
lenges mentioned above. The proposed system is
based on constructing a lattice from possible nor-
malization candidates and finding the best normal-
ization sequence according to an n-gram language
model using a Viterbi decoder. We propose an
unsupervised approach to learn the normalization
candidates from unlabeled text data. The proposed
approach uses RandomWalks on a contextual sim-
ilarity graph constructed form n-gram sequences
on large unlabeled text corpus. The proposed ap-
proach is very scalable, accurate and adaptive to
any domain and language. We evaluate the ap-
proach on the normalization task as well as ma-
chine translation task.
The rest of this paper is organized as follows:
Section(2) discusses the related work, Section(3)
introduces the text normalization system and the
baseline candidate generators, Section(4) intro-
duces the proposed graph-based lexicon induction
approach, Section(5) discusses the experiments
and output analysis, and finally Section(6) con-
cludes and discusses future work.
2 Related Work
Early work handled the text normalization prob-
lem as a noisy channel model where the normal-
ized words go through a noisy channel to produce
the noisy text. (Brill and Moore, 2000) introduced
an approach for modeling the spelling errors as
a noisy channel model based on string to string
edits. Using this model gives significant perfor-
mance improvements compared to previously pro-
posed models. (Toutanova and Moore, 2002) im-
proved the string to string edits model by mod-
eling pronunciation similarities between words.
(Choudhury et al, 2007) introduced a supervised
HMM channel model for text normalization which
has been expanded by (Cook and Stevenson, 2009)
to introduce unsupervised noisy channel model
using probabilistic models for common abbrevi-
ation and various spelling errors types. Some
researchers used Statistical Machine Translation
approach for text normalization; formalizing the
problem as a translation from the noisy forms to
the normalized forms. (Aw et al, 2006) proposed
an approach for normalizing Short Messaging Ser-
vice (SMS) texts by translating it into normal-
ized forms using Phrase-based SMT techniques on
character level. The main drawback of these ap-
proaches is that the noisy channel model cannot
accurately represent the errors types without con-
textual information.
More recent approaches tried to handle the text
normalization problem using normalization lexi-
cons which map the noisy form of the word to a
normalized form. For example, (Han et al, 2011)
proposed an approach using a classifier to identify
the noisy words candidate for normalization; then
using some rules to generate lexical variants and a
small normalization lexicon. (Gouws et al, 2011)
proposed an approach using an impoverished nor-
malization lexicon based on string and distribu-
tional similarity along with a dictionary lookup
approach to detect noisy words. More recently,
(Han et al, 2012) introduced a similar approach
by generating a normalization lexicon based on
distributional similarity and string similarity. This
approach uses pairwise similarity where any two
words that share the same context are considered
as normalization equivalences. The pairwise ap-
proach has a number of limitations. First, it does
not take into account the relative frequencies of
the normalization equivalences that might share
different contexts. Therefore, the selection of the
normalization equivalences is performed on pair-
wise basis only and is not optimized over the
whole data. Secondly, the normalization equiva-
lences must appear in the exact same context to
be considered as a normalization candidate. These
limitations affect the accuracy and the coverage of
the produced lexicon.
Our approach also adopts a lexicon based ap-
proach for text normalization, we construct a lat-
tice from possible normalization candidates and
find the best normalization sequence according
to an n-gram language model using a Viterbi de-
coder. The normalization lexicon is acquired from
unlabeled data using random walks on a contex-
tual similarity graph constructed form n-gram se-
quences on large unlabeled text corpus. Our ap-
proach has some similarities with (Han et al,
2012) since both approaches utilize a normaliza-
1578
tion lexicon acquired form unlabeled data using
distributional and string similarities. However, our
approach is significantly different since we acquire
the lexicon using random walks on a contextual
similarity graph which has a number of advantages
over the pairwise similarity approach used in (Han
et al, 2012). Namely, the acquired normalization
equivalence are optimized globally over the whole
data, the rare equivalences are not considered as
good candidates unless there is a strong statistical
evidence across the data, and finally the normal-
ization equivalences may not share the same con-
text. Those are clear advantages over the pairwise
similarity approach and result in a lexicon with
higher accuracy as well as wider coverage. Those
advantages will be clearer when we describe the
proposed approach in details and during evalua-
tion and comparison to the pairwise approach.
3 Text Normalization System
In this paper, we handle text normalization as a
lattice scoring approach, where the translation is
performed from noisy text as the source side to
the normalized text as the target side. Unlike con-
ventional MT systems, the translation table is not
learned from parallel aligned data; instead it is
modeled by the graph-based approach of lexicon
generation as we will describe later. We construct
a lattice from possible normalization candidates
and find the best normalization sequence accord-
ing to an n-gram language model using a Viterbi
decoder.
In this paper, we restrict the normalization lexi-
con to one-to-one word mappings, we do not con-
sider multi words mapping for the lexicon induc-
tion. To identify OOV candidates for normaliza-
tion; we restrict proposing normalization candi-
dates to the words that we have in our induced
normalization lexicon only. This way, the system
would provide more confident and conservative
normalization. We move the problem of identi-
fying OOV words to training time; at training time
we use soft criteria to identify OOV words.
3.1 Baseline Normalization Candidates
Generation
We experimented with two normalization candi-
date generators as baseline systems. The first is a
dictionary based spelling correction similar to As-
pell1. In this experiment we used the spell checker
1http://aspell.net/
to generate all possible candidates for OOV words
and then applied the Viterbi decoder on the con-
structed lattice to score the best correction candi-
dates using a language model.
Our second candidates generator is based on
a trie approximate string matching with K errors
similar to the approach proposed in (Chang et al,
2010), where K errors can be caused by substi-
tution, insertion, or deletion operations. In our
implementation, we customized the errors opera-
tions to accommodate the nature of the social me-
dia text. Such as lengthening, letter substitution,
letter-number substitution and phonetic substitu-
tion. This approach overcomes the main problem
of the dictionary-based approach which is provid-
ing inappropriate normalization candidates to the
errors styles in the social media text.
As we will show in the experiments in
Section(5), dictionary-based normalization meth-
ods proved to be inadequate for social media do-
main normalization for many reasons. First, they
provide generic corrections which are inappropri-
ate for social media text. Second, they usually pro-
vide corrections with the minimal edit distance for
any word or named entity regardless of the nature
of the words. Finally, the previous approaches do
not take into account the dynamics of the social
media text where new terms can be introduced on
a daily basis.
4 Normalization Lexicons using
Graph-based Random Walks
4.1 Bipartite Graph Representation
The main motivation of this approach is that
normalization equivalences share similar context;
which we call contextual similarity. For instance,
assume 5-gram sequences of words, two words
may be normalization equivalences if their n-gram
context shares the same two words on the left and
the same two words on the right. In other words,
they are sharing a wild card pattern such as (word
1 word 2 * word 4 word 5).
This contextual similarity can be represented as
a bipartite graph with the first partite representing
the words and the second partite representing the
n-gram contexts that may be shared by words. A
word node can be either normalized word or noisy
word. Identifying if a word is normalized or noisy
(candidate for normalization) is crucial since this
decision limits the candidate noisy words to be
normalized. We adopted a soft criteria for iden-
1579
C2
making4
makin
2 mking1
tkin
1
C3 23
C1 taking1
takin21
C4 145
Figure 1: Bipartite Graph Representation, left
nodes represent contexts, gray right nodes repre-
sent the noisy words and white right nodes rep-
resent the normalized words. Edge weight is the
co-occurrence count of a word and its context.
tifying noisy words. A vocabulary is constructed
from a large clean corpus. Any word that does not
appear in this vocabulary more than a predefined
threshold (i.e. 10 times) is considered as a can-
didate for normalization (noisy word). Figure(1)
shows a sample of the bipartite graphG(W,C,E),
where noisy words are shown as gray nodes.
Algorithm 4.1: CONSTRUCTBIPARTITE(text)
comment: Construct Bipartite Graph
output (G(W,C,E))
comment: Extract all n-gram sequences
Ngrams? EXTRACTNGRAMS(TextCorpus)
for each n ? Ngrams
do?
????????????
????????????
comment: Check for center word
if ISNOISY(CenterWord)
W ? ADDSOURCENODE(CenterWord)
else
W ? ADDABSORBINGNODE(CenterWord)
comment: add the context pattern
C ? ADD(Context)
comment: edge weight
E ? ADD(Context,Word, count)
The bipartite graph, G(W,C,E), is composed
of W which includes all nodes representing nor-
malized words and noisy words, C which includes
all nodes representing shared context, and finally
E which represents the edges of the graph con-
necting word nodes and context nodes. The weight
on the edge is simply the number of occurrences
of a given word in a context. While construct-
ing the graph, we identify if a node represents a
noisy word (N) (called source node) or a normal-
ized word (M) (called absorbing node). The bi-
partite graph is constructed using the procedure in
Algorithm(4.1).
4.2 Lexicon generation using Random Walks
Our proposed approach uses Markov Random
Walks on the bipartite graph in Figure(1) as de-
fined in (Norris, 1997). The main objective is to
identify pairs of noisy and normalized words that
can be considered as normalization equivalences.
In principal, this is similar to using random walks
for semi-supervised label propagation which has
been introduced in (Szummer and Jaakkola, 2002)
and then used in many other applications. For
example, (Hughes and Ramage, 2007) used ran-
dom walks on Wordnet graph to measure lexical
semantic relatedness between words. (Das and
Petrov, 2011) used graph-based label propagation
for cross-lingual knowledge transfers to induce
POS tags between two languages. (Minkov and
Cohen, 2012) introduced a path constrained graph
walk algorithm given a small number of labeled
examples to assess nodes relatedness in the graph.
In this paper, we apply the label propagation ap-
proach to the text normalization problem.
Consider a random walk on the bipartite graph
G(W,C,E) starting at a noisy word (source
node) and ending at a normalized word (absorb-
ing node). The walker starts from any source
node Ni belonging to the noisy words then move
to any other connected node Mj with probability
Pij . The transition between each pair of nodes
is defined by a transition probability Pij which
represents the normalized probability of the co-
occurrence counts of the word and the correspond-
ing context. Though the counts are symmetric, the
probability is not symmetric. This is due to the
probability normalization which is done according
to the nodes connectivity. Therefore, the transition
probability between any two nodes i, j is defined
as:
Pij = Wij/
?
?k
Wik (1)
For any non-connected pair of nodes, Pij =0. It
is worth noting that due to the bipartite graph rep-
resentation; any word node, either noisy (source)
or normalized (absorbing), is only connected to
context nodes and not directly connected to any
other word node.
1580
The algorithm repeats independent random
walks for K times where the walks traverse the
graph randomly according to the transition prob-
ability distribution in Eqn(1); each walk starts
from the source noisy node and ends at an absorb-
ing normalized node, or consumes the maximum
number of steps without hitting an absorbing node.
For any random walk the number of steps taken
to traverse between any two nodes is called the
hitting time (Norris, 1997). Therefore, the hit-
ting time between a noisy and a normalized pair
of nodes (n,m) with a walk r is hr(n,m). We
define the cost between the two nodes as the aver-
age hitting time H(n,m) of all walks that connect
those two nodes:
H(n,m) =
?
?r
hr(n,m)/R (2)
Consider the bipartite graph in Figure(1), as-
sume a random walk starting at the source node
representing the noisy word ?tkin? then moves to
the context node C1 then to the absorbing node
representing the normalized word ?taking?. This
random walk will associate ?tkin? with ?taking?
with a walk of two steps (hits). Another random
walk that can connect the two words is [?tkin?
? C4? ?takin?? C1? ?taking?], which has
4 steps (hits). In this case, the cost of this pair
of nodes is the average number of hits connecting
them which is 3.
It is worth noting that the random walks are
selected according to the transition probability in
Eqn(1); therefore, the more probable paths will be
picked more frequently. The same pair of nodes
can be connected with many walks of various steps
(hits), and the same noisy word can be connected
to many other normalized words.
We define the contextual similarity probabil-
ity of a normalization equivalence pair n,m as
L(n,m). Which is the relative frequency of the
average hitting of those two nodes, H(n,m), and
all other normalized nodes linked to that noisy
word. Thus L(n,m), is calculated as:
L(n,m) = H(n,m)/
?
i
H(n,mi) (3)
Furthermore, we add another similarity cost be-tween a noisy word and a normalized word based
on the lexical similarity cost, SimCost(n,m),
which we will describe in the next section. Thefinal cost associated with a pair is:
Cost(n,m) = ?1L(n,m) + ?2SimCost(n,m) (4)
Algorithm 4.2: INDUCELEXICON(G)
output (Lexicon)
INIT((Lexicon))
for each n ?W ? G(W,C,E)
do?
???????????????????
???????????????????
comment: for noisy nodes only
if ISNOISY(n)?
????????
????????
INIT(Rn)
comment: do K random walks
for i? 0 to K
do
Rn? RANDOMWALK(n)
comment: Calculate Avg. hits and normalize
Ln? NORMALIZE(Rn)
comment: Calculate Lexical Sim Cost
Ln? SIMCOST(Ln)
Ln? PRUNE(Ln)
Lexicon? ADD(Ln)
We used uniform interpolation, both ?1 and ?2
equals 1. The final Lexicon is constructed using
those entries and if needed we prune the list to take
top N according to the cost above. The algorithm
is outlined in 4.2.
4.3 Lexical Similarity Cost
We use a similarity function proposed in (Con-
tractor et al, 2010) which is based on Longest
Common Subsequence Ratio (LCSR) (Melamed,
1999). This cost function is defined as the ratio
of LCSR and Edit distance between two strings as
follows:
SimCost(n,m) = LCSR(n,m)/ED(n,m) (5)
LCSR(n,m) = LCS(n,m)/MaxLenght(n,m) (6)
We have modified the Edit Distance calculation
ED(n,m) to be more adequate for social media text.
The edit distance is calculated between the conso-
nant skeleton of the two words; by removing all
vowels, we used Editex edit distance as proposed
in (Zobel and Philip, 1996), repetition is reduced
to a single letter before calculating the edit dis-
tance, and numbers in the middle of words are sub-
stituted by their equivalent letters.
5 Experiments
5.1 Training and Evaluation Data
We collected large amount of social media data to
generate the normalization lexicon using the ran-
1581
dom walk approach. The data consists of 73 mil-
lion Twitter statuses. All tweets were collected
from March/April 2012 using the Twitter Stream-
ing APIs2. We augmented this data with 50 mil-
lion sentences of clean data from English LDC Gi-
gaword corpus 3. We combined both data, noisy
and clean, together to induce the normalization
dictionary from them. While the Gigaword clean
data was used to train the language model to score
the normalized lattice.
We constructed a test set of 1000 sentences of
social media which had been corrected by a na-
tive human annotator, the main guidelines were to
normalize noisy words to its corresponding clean
words in a consistent way according to the evi-
dences in the context. We will refer to this test
set as SM-Test. Furthermore, we developed a test
set for evaluating the effect of the normalization
system when used as a preprocessing step for Ma-
chine translation. The machine translation test set
is composed of 500 sentences of social media En-
glish text translated to normalized Spanish text by
a bi-lingual translator.
5.2 Evaluating Normalization Lexicon
Generation
We extracted 5-gram sequences from the com-
bined noisy and clean data; then we limited the
space of noisy 5-gram sequences to those which
contain only one noisy word as the center word
and all other words, representing the context, are
not noisy. As we mentioned before, we identify
whether the word is noisy or not by looking up
a vocabulary list constructed from clean data. In
these experiments, the vocabulary is constructed
from the Language Model data (50M sentences of
the English Gigaword corpus). Any word that ap-
pears less than 10 times in this vocabulary is con-
sidered noisy and candidate for normalization dur-
ing the lexicon induction process. It is worth not-
ing that our notion of noisy word does not mean it
is an OOV that has to be corrected; instead it in-
dicates that it is candidate for correction but may
be opted not to be normalized if there is no con-
fident normalization for it. This helps to maintain
the approach as a high precision text normaliza-
tion system which is highly preferable as an NLP
preprocessing step.
We constructed a lattice using normalization
2https://dev.twitter.com/docs/streaming-apis
3http://www.ldc.upenn.edu/Catalog/LDC2011T07
candidates and score the best Viterbi path with 5-
gram language model. We experimented with two
candidate generators as baseline systems, namely
the dictionary-based spelling correction and the
trie approximate match with K errors; where K=3.
For both candidate generators the cost function for
a given candidate is calculated using the lexical
similarity cost in Eqn(5). We compared those ap-
proaches with our newly proposed unsupervised
normalization lexicon induction; for this case the
cost for a candidate is the combined cost of the
contextual similarity probability and the lexical
similarity cost as defined in Eqn(4). We examine
the effect of data size and the steps of the random
walks on the accuracy and the coverage of the in-
duced dictionary.
We constructed the bipartite graph with the n-
gram sequences as described in Algorithm 4.1.
Then the Random Walks Algorithm in 4.2 is ap-
plied with 100 walks. The total number of word
nodes is about 7M nodes and the total number
of context nodes is about 480M nodes. We used
MapReduce framework to implement the pro-
posed technique to handle such large graph. We
experimented with the maximum number of ran-
dom walk steps of 2, 4 and 6; and with different
portions of the data as well. Finally, we pruned
the lexicon to keep the top 5 candidates per noisy
word.
Table(1) shows the resulting lexicons from dif-
ferent experiments.
Lexicon Lexicon Data Steps
Lex1 123K 20M 4
Lex2 281K 73 M 2
Lex3 327K 73M 4
Lex4 363K 73M 6
Table 1: Generated Lexicons, steps are the Ran-
dom Walks maximum steps.
As shown in Table(1), we experimented with
different data sizes and steps of the random walks.
The more data we have the larger the lexicon we
get. Also larger steps increase the induced lexi-
con size. A random walk step size of 2 means that
the noisy/normalized pair shares the same context;
while a step size of 4 or more means that they may
not share the same context. Next, we will exam-
ine the effect of lexicon size on the normalization
task.
1582
5.3 Text Normalization Evaluation
We experimented different candidate generators
and compared it to the unsupervised lexicon ap-
proach. Table(2) shows the precision and recall on
a the SM-Test set.
System Candidates Precision Recall F-Measure
Base1 Dict 33.9 15.1 20.98
Base2 Trie 26.64 27.65 27.13
RW1 Lex1 88.76 59.23 71.06
RW2 Lex2 90.66 54.06 67.73
RW3 Lex3 92.43 56.4 70.05
RW4 Lex4 90.87 60.73 72.8
Table 2: Text Normalization with different lexi-
cons
In Table(2), the first baseline is using a dictio-
nary based spell checker; which gets low precision
and very low recall. Similarly the trie approximate
string match is doing a similar job with better re-
call though the precision is worst. Both of the
baseline approaches are inadequate for social me-
dia text since both will try to correct any word that
is similar to a word in the dictionary. The Trie ap-
proximate match is doing better job on the recall
since the approximate match is based on phonetic
and lexical similarities.
On the other hand, the induced normalization
lexicon approach is doing much better even with
a small amount of data as we can see with sys-
tem RW1 which uses Lex1 generated from 20M
sentences and has 123K lexicon entry. Increas-
ing the amount of training data does impact the
performance positively especially the recall. On
the other hand, increasing the number of steps has
a good impact on the recall as well; but with a
considerable impact on the precision. It is clear
that increasing the amount of data and keeping the
steps limit at ??4?? gives better precision and cov-
erage as well. This is a preferred setting since the
main objective of this approach is to have better
precision to serve as a reliable preprocessing step
for Machine Translation and other NLP applica-
tions.
5.4 Comparison with Pairwise Similarity
We present experimental results to compare our
proposed approach with (Han et al, 2012) which
used pairwise contextual similarity to induce a
normalization lexicon of 40K entries, we will refer
to this lexicon as HB-Dict. We compare the per-
formance of HB-Dict and our induced dictionary
(system RW3). We evaluate both system on SM-
Test test set and on (Han et al, 2012) test set of
548 sentences which we call here HB-Test.
System Precision Recall F-Measure
SM-Test
HB-Dict 71.90 26.30 38.51
RW3 92.43 56.4 70.05
HB-Test
HB-Dict 70.0 17.9 26.3
RW3 85.37 56.4 69.93
Table 3: Text Normalization Results
As shown in Table(3), RW3 system signifi-
cantly outperforms HB-Dict system with the lex-
icon from (Han et al, 2012) on both test sets for
both precision and recall. The contextual graph
random walks approach helps in providing high
precision lexicon since the sampling nature of the
approach helps in filtering out unreliable normal-
ization equivalences. The random walks will tra-
verse more frequent paths; which would lead to
more probable normalization equivalence. On the
other hand, the proposed approach provides high
recall as well which is hard to achieve with higher
precision. Since the proposed approach deploys
random walks to sample paths that can traverse
many steps, this relaxes the constraints that the
normalization equivalences have to share the same
context. Instead a noisy word may share a con-
text with another noisy word which in turn shares
a context with a clean equivalent normalization
word. Therefore, we end up with a lexicon that
have much higher recall than the pairwise simi-
larity approach since it explores equivalences be-
yond the pairwise relation. Moreover, the random
walk sampling emphasis the more frequent paths
and hence provides high precision lexicon.
5.5 Output Analysis
Table(4) shows some examples of the induced nor-
malization equivalences, the first part shows good
examples where vowels are restored and phonetic
similar words are matched. Remarkably the cor-
rection ??viewablity?? to ??visibility?? is interest-
ing since the system picked the more frequent
form. Moreover, the lexicon contains some entries
with foreign language words normalized to its En-
glish translation. On the other hand, the lexicon
has some bad normalization such as ??unrecycled
?? which should be normalized to ??non recycled??
but since the system is limited to one word cor-
rection it did not get it. Another interesting bad
normalization is ??tutting?? which is new type of
1583
dancing and should not be corrected to ??tweet-
ing??.
Noisy Clean Remarks
tnght tonight Vowels restored
darlin darling g restored
urung orange phonetic similarity
viewablity visibility good correction
unrecycled recycled negation ignored
tutting tweeting tutting is dancing type
Table 4: Lexicon Samples
Table 5 lists a number of examples and their
normalization using both Baseline1 and RW3. At
the first example, RW3 got the correct normaliza-
tion as ?interesting? which apparently is not the
one with the shortest edit distance, though it is
the most frequent candidate at the generated lex-
icon. The baseline system did not get it right; it
got a wrong normalization with shorter edit dis-
tance. Example(2) shows the same effect by get-
ting ?cuz? normalized to ?because?. At Exam-
ple(3), both the baseline and RW3 did not get
the correct normalization of ?yur? to ?you are?
which is currently a limitation in our system since
we only allow one-to-one word mapping in the
generated lexicons not one-to-many or many-to-
many. At Example(4), RW3 did not normalize
?dure? to ?sure? ; however the baseline normal-
ized it by mistake to ?dare?. This shows a char-
acteristic of the proposed approach; it is very con-
servative in proposing normalization which is de-
sirable as a preprocessing step for NLP applica-
tions. This limitation can be marginalized by pro-
viding more data for generating the lexicon. Fi-
nally, Example 4 shows also that the system nor-
malize ?gr8?which is mainly due to having a flex-
ible similarity cost during the normalization lexi-
con construction.
1. Source: Mad abt dt so mch intesting
Baseline1: Mad at do so much ingesting
RW3: Mad about that so much interesting
2. Source: i?l do cuz ma parnts r ma lyf
Baseline1: I?ll do cut ma parents r ma life
RW3: I?ll do because my parents are my life
3. Source: yur cuuuuute
Baseline1: yur cuuuuute
RW3: your cute
4. Source: I?m dure u will get a gr8 score
Baseline1: I?m dare you will get a gr8 score
RW3: I?m dure you will get a great score
Table 5: Normalization Examples
5.6 Machine Translation Task Evaluation
The final evaluation of the text normalization sys-
tem is an extrinsic evaluation where we evaluate
the effect of the text normalization task on a so-
cial media text translating from English to Span-
ish using a large scale translation system trained
on general domain data. The system is trained
on English-Spanish parallel data from WMT 2012
evaluation 4. The data consists of about 5M paral-
lel sentences on news, europal and UN data. The
system is a state of the art phrase based system
similar to Moses (Hoang et al, 2007). We used
The BLEU score (Papineni et al, 2002) to evaluate
the translation accuracy with and without the nor-
malization. Table(6) shows the translation evalua-
tion with different systems. The translation with
normalization was improved by about 6% from
29.02 to 30.87 using RW3 as a preprocessing step.
System BLEU Impreovemnet
No Normalization 29.02 0%
Baseline1 29.13 0.37%
HB-Dict 29.76 3.69%
RW3 30.87 6.37%
Table 6: Translation Results
6 Conclusion and Future Work
We introduced a social media text normalization
system that can be deployed as a preprocessor
for MT and various NLP applications to han-
dle social media text. The proposed approach is
very scalable, adaptive to any domain and lan-
guage. We show that the proposed unsupervised
approach provides a normalization system with
very high precision and a reasonable recall. We
compared the system with conventional correction
approaches and with recent previous work; and we
showed that it highly outperforms other systems.
Finally, we have used the system as a preprocess-
ing step for a machine translation system which
improved the translation quality by 6%.
As an extension to this work, we will extend the
approach to handle many-to-many normalization
pairs; also we plan to apply the approach to more
languages. Furthermore, the approach can be eas-
ily extended to handle similar problems such as ac-
cent restoration and generic entity normalization.
4http://www.statmt.or/wmt12
1584
Acknowledgments
We would like to thank Lee Schwartz and Will
Lewis for their help in constructing the test sets
and in the error analysis. We would also like to
thank the anonymous reviewers for their helpful
and constructive comments.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 3340, Sydney, Australia.
Eric Brill and Robert C. Moore. 2000. An improved er-
ror model for noisy channel spelling correction, In
ACL 2000: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, En-
glewood Cliffs, NJ, USA.
Ye-In Chang and Jiun-Rung Chen and Min-Tze Hsu
2010. A hash trie filter method for approximate
string matching in genomic databases Applied In-
telligence, 33:1, pages 21:38, Springer US.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu
2007. Investigation and modeling of the structure of
texting language. International Journal of Document
Analysis and Recognition, vol. 10, pp. 157:174.
Danish Contractor and Tanveer Faruquie and Venkata
Subramaniam 2010. Unsupervised cleansing of
noisy text. In COLING ?10 Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 189:196.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization.. In
CALC 09: Proceedings of the Workshop on Compu-
tational Approaches to Linguistic Creativity, pages
71:78, Boulder, USA.
Dipanjan Das and Slav Petrov 2011 Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600:609, Portland, Oregon
Stephan Gouws, Dirk Hovy, and Donald Metzler.
2011. Unsupervised mining of lexical variants from
noisy text. In Proceedings of the First workshop on
Unsupervised Learning in NLP, pages 82:90, Edin-
burgh, Scotland.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a twit-
ter. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL-HLT 2011),
pages 368:378, Portland, Oregon, USA.
Bo Han and Paul Cook and Timothy Baldwin 2012.
Automatically Constructing a Normalisation Dic-
tionary for Microblogs. Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012), pages
421:432, Jeju Island, Korea.
Hieu Hoang and Alexandra Birch and Chris Callison-
burch and Richard Zens and Rwth Aachen and
Alexandra Constantin and Marcello Federico and
Nicola Bertoldi and Chris Dyer and Brooke Cowan
andWade Shen and Christine Moran and Ondrej Bo-
jar 2007. Moses: Open source toolkit for statistical
machine translation.
Thad Hughes and Daniel Ramage 2007. Lexical se-
mantic relatedness with random graph walks Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing EMNLP, pp. 581589,
Prague
Fei Liu and Fuliang Weng and Bingqing Wang and
Yang Liu 2011. Insertion, Deletion, or Substi-
tution? Normalizing Text Messages without Pre-
categorization nor Supervision Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 19:24, Portland, Oregon
Dan Melamed 1999. Bitext Maps and Alignment via
Pattern Recognition. In Computational Linguistics,
25, pages 107:130.
Einat Minkov and William Cohen Graph Based
Similarity Measures for Synonym Extraction from
Parsed Text In Proceedings of the TextGraphs work-
shop 2012
J. Norris 1997. Markov Chains. Cambridge Univer-
sity Press.
Kishore Papineni and Salim Roukos and Todd Ward
and Wei-jing Zhu 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. in Pro-
ceedings of ACL-2002: 40th Annual meeting of the
Association for Computational Linguistics. , pages
311:318.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
Normalization of non-standard words. 2001.
Xu Sun and Jianfeng Gao and Daniel Micol and Chris
Quirk 2010. Learning Phrase-Based Spelling Error
Models from Clickthrough Data. Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 266:274, Sweeden.
Martin Szummer and Tommi 2002. Partially labeled
classification with markov random walks. In Ad-
vances in Neural Information Processing Systems,
pages 945:952.
1585
Kristina Toutanova and Robert C. Moore. Pronunci-
ation modeling for improved spelling correction..
2002. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
, pages 144151, Philadelphia, USA.
Justin Zobel and Philip Dart 1996. Phonetic string
matching: Lessons from information retrieval. in
Proceedings of the Eighteenth ACM SIGIR Inter-
national Conference on Research and Development
in Information Retrieval, pages 166:173, Zurich,
Switzerland.
1586
