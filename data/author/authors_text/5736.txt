Combining Linguistic and Machine Learning Techniques for Email
Summarization
Smaranda Muresan
Dept. of Computer Science
Columbia University
500 West 120 Street
New York, NY, 10027
smara@cs.columbia.edu
Evelyne Tzoukermann
Bell Laboratories
Lucent Technologies
700 Mountain Avenue
Murray Hill, NJ, 07974
evelyne@lucent.com
Judith L. Klavans
Columbia University
Center for Research on
Information Access
535 West 114th Street
New York, NY 10027
klavans@cs.columbia.edu
Abstract
This paper shows that linguistic tech-
niques along with machine learning
can extract high quality noun phrases
for the purpose of providing the gist
or summary of email messages. We
describe a set of comparative experi-
ments using several machine learning
algorithms for the task of salient noun
phrase extraction. Three main conclu-
sions can be drawn from this study: (i)
the modifiers of a noun phrase can be
semantically as important as the head,
for the task of gisting, (ii) linguistic fil-
tering improves the performance of ma-
chine learning algorithms, (iii) a combi-
nation of classifiers improves accuracy.
1 Introduction
In this paper we present a comparative study of
symbolic machine learning models applied to nat-
ural language task of summarizing email mes-
sages through topic phrase extraction.
Email messages are domain-general text, they
are unstructured and not always syntactically well
formed. These characteristics raise challenges for
automatic text processing, especially for the sum-
marization task. Our approach to email summa-
rization, implemented in the GIST-IT system, is
to identify topic phrases, by first extracting noun
phrases as candidate units for representing doc-
ument meaning and then using machine learning
algorithms to select the most salient ones.
The comparative evaluation of several machine
learning models in the settings of our experiments
indicates that : (i) for the task of gisting the mod-
ifiers of the noun phrase are equally as important
as the head, (ii) noun phrases are better than n-
grams for the phrase-level representation of the
document, (iii) linguistic filtering enhances ma-
chine learning techniques, (iv) a combination of
classifiers improves accuracy.
Section 2 of the paper outlines the machine
learning aspect of extracting salient noun phrases,
emphasizing the features used for classifica-
tion and the symbolic machine learning models
used in the comparative experiments. Section
3 presents the linguistic filtering steps that im-
prove the accuracy of the machine learning algo-
rithms. Section 4 discusses in detail our conclu-
sions stated above.
2 Machine Learning for Content
Extraction
Symbolic machine learning has been applied suc-
cessfully in conjunction with many NLP applica-
tions (syntactic and semantic parsing, POS tag-
ging, text categorization, word sense disambigua-
tion) as reviewed by Mooney and Cardie (1999).
We used machine learning techniques for finding
salient noun phrases that can represent the sum-
mary of an email message. This section describes
the three steps involved in this classification task:
1) what representation is appropriate for the infor-
mation to be classified as relevant or non-relevant
(candidate phrases), 2) which features should be
associated with each candidate, 3) which classifi-
cation models should be used.
Case 1
CNP: scientific/JJ and/CC technical/JJ articles/NNS
SNP1: scientific/JJ articles/NNS
SNP2: technical/JJ articles/NNS
Case 2
CNP: scientific/JJ thesauri/NNS and databases/NNS
SNP1: scientific/JJ thesauri/NNS
SNP2: scientific/JJ databases/NNS
Case 3
CNP: physics/NN and/CC biology/NN skilled/JJ researchers/NNS
SNP1: physics/NN skilled/JJ researchers/NNS
SNP2: biology/NN skilled/JJ researchers/NNS
Table 1: Resolving Coordination of NPs
2.1 Candidate Phrases
Of the major syntactic constituents of a sentence,
e.g. noun phrases, verb phrases, and prepositional
phrases, we assume that noun phrases (NPs) carry
the most contentful information about the doc-
ument, a well-supported hypothesis (Smeaton,
1999; Wacholder, 1998).
As considered by Wacholder (1998), the sim-
ple NPs are the maximal NPs that contain pre-
modifiers but not post-nominal constituents such
as prepositions or clauses. We chose simple NPs
for content representation because they are se-
mantically and syntactically coherent and they are
less ambiguous than complex NPs. For extracting
simple noun phrases we first used Ramshaw and
Marcus?s base NP chunker (Ramshaw and Mar-
cus, 1995). The base NP is either a simple NP or
a coordination of simple NPs. We used heuristics
based on POS tags to automatically split the co-
ordinate NPs into simple ones, properly assigning
the premodifiers. Table 1 presents some coordi-
nate NPs (CNP) encountered in our data collec-
tion and the results of our algorithm which split
them into simple NPs (SNP1 and SNP2).
2.2 Features used for Classification
The choice of features used to represent the can-
didate phrases has a strong impact on the accu-
racy of the classifiers (e.g. the number of exam-
ples needed to obtain a given accuracy on the test
data, the cost of classification). For our classifica-
tion task of determining if a noun phrase is salient
or not to the document meaning, we chose a set of
nine features.
Several studies rely on the linguistic intuition
that the head of the noun phrase makes a greater
contribution to the semantics of the nominal
group than the modifiers. However, for some
specific tasks in NLP , the head is not necessar-
ily the most semantically important part of the
noun phrase. In analyzing email messages from
the perspective of finding salient NPs, we claim
that the modifier(s) of the noun phrase - usually
nominal modifiers(s), often have as much seman-
tic content as the head. This opinion is also sup-
ported in the work of Strzalkowski et al (1999),
where syntactic NPs are captured for the goal
of extracting their semantic content but are pro-
cessed as an ?ordered? string of words rather than
a syntactic unit. Thus we introduce as a sepa-
rate feature in the feature vector, a new TF*IDF
measure which consider the NP as a sequence of
equally weighted elements, counting individually
the modifier(s) and the head.
Consider the following list of simple NPs se-
lected as candidates:
1. conference workshop announcement
2. international conference
3. workshop description
4. conference deadline
In the case of the first noun phrase, for exam-
ple, its importance is found in the two noun mod-
ifiers: conference and workshop as much as in
the head announcement, due to their presence as
heads or modifiers in the candidate NPs 2-4. Our
new feature will be: TF  IDF
conference
+TF 
IDF
workshop
+ TF  IDF
announcement
. Giving
these linguistic observations we divided the set of
features into three groups, as we mentioned also
in (Tzoukermann et al, 2001): 1) one associated
with the head of the noun phrase; 2) one associ-
ated with the whole NP and 3) one that represents
the new TF*IDF measure discussed above.
2.2.1 Features associated with the Head
We choose two features to characterize the
head of the noun phrases:
 head tfidf: the TF*IDF measure of the
head of the candidate NP. For the NP in
example (1) this feature will be TF 
IDF
announcement
.
 head focc: The position of the first occur-
rence of the head in text (the number of
words that precede the first occurrence of the
head divided by the total number of words in
the document).
2.2.2 Features associated with the whole NP
We select six features that we consider relevant
in determining the relative importance of the noun
phrase:
 np tfidf: the TF*IDF measure of
the whole NP. For the NP in the
example (1) this feature will be
TF  IDF
conference workshop announcement
.
 np focc: The position of the first occurrence
of the noun phrase in the document.
 np length words: Noun phrase length mea-
sured in number of words, normalized by di-
viding it with the total number of words in
the candidate NP list.
 np length chars: Noun phrase length mea-
sured in number of characters, normalized
by dividing it with the total number of char-
acters in the candidate NPs list.
 sent pos: Position of the noun phrase in the
sentence: the number of words that precede
the noun phrase, divided by sentence length.
For noun phrases in the subject line (which
are usually short and will be affected by this
measure), we consider the maximum length
of sentence in document as the normalization
factor.
 par pos: Position of noun phrase in para-
graph, same as sent pos, but at the paragraph
level.
2.2.3 Feature that considers all constituents
of the NP equally weighted
One of the important hypotheses we tested in
this work is that both the modifiers and the head
of NP contribute equally to its salience. Thus we
consider mh tfidf as an additional feature in the
feature vector.
 mh tfidf: the new TF*IDF measure that
takes also into consideration the importance
of the modifiers. In our example the value of
this feature will be : TF  IDF
conference
+
TF IDF
workshop
+TF IDF
announcement
In computing the TF*IDF measures (head tfidf,
np tfidf, mh tfidf), specific weights, w
i
, were as-
signed to account for the presence in the email
subject line and/or headlines in the email body.
 w
i1
: presence in the subject line and head-
line
 w
i2
: presence in the subject line
 w
i3
: presence in headlines where w
i1
> w
i2
> w
i3
.
These weights were manually chosen after a set
of experiments, but we plan to use a regression
method to automatically learn them.
2.3 Symbolic Machine Learning Models
We compared three symbolic machine learning
paradigms (decision trees, rule induction and de-
cision forests) applied to the task of salient NP
extraction, evaluating five classifiers.
2.3.1 Decision Tree Classifiers
Decision trees classify instances represented as
feature vectors, where internal nodes of the tree
test one or several attributes of the instance and
where the leaves represent categories. Depending
on how the test is performed at each node, there
exists two types of decision tree classifiers: axis
parallel and oblique. The axis-parallel decision
trees check at each node the value of a single at-
tribute. If the attributes are numeric, the test has
the form x
i
> t, where x
i
is one of the attribute
of an instance and t is the threshold. Oblique de-
cision trees test a linear combination of attributes
at each internal node:
n
X
i=1
a
i
x
i
+ a
n+1
> 0
where a
i
; :::; a
n+1
are real-valued coefficients.
We compared the performance of C4.5, an axis-
parallel decision tree classifier (Quinlan, 1993)
and OC1, an oblique decision tree classifier
(Murthy et al, 1993).
2.3.2 Rule Induction Classifiers
In rule induction, the goal is to learn the small-
est set of rules that capture all the generalisable
knowledge within the data. Rule induction clas-
sification is based on firing rules on a new in-
stance, triggered by the matching feature values
to the left-hand side of the rules. Rules can be of
various normal forms and can be ordered. How-
ever, the appropriate ordering can be hard to find
and the key point of many rule induction algo-
rithms is to minimize the search strategy through
the space of possible rule sets and orderings. For
our task, we test the effectiveness of two rule
induction algorithms : C4.5rules that form pro-
duction rules from unpruned decision tree, and
a fast top-down propositional rule learning sys-
tem, RIPPER (Cohen, 1995). Both algorithms
first construct an initial model and then iteratively
improve it. C4.5rules improvement strategy is a
greedy search, thus potentially missing the best
rule set. Furthermore, as discussed in (Cohen,
1995), for large noisy datasets RIPPER starts with
an initial model of small size, while C4.5rules
starts with an over-large initial model. This means
that RIPPER?s search is more efficient for noisy
datasets and thus is more appropriate for our data
collection. It also allows the user to specify the
loss ratio, which indicates the ratio of the cost of
false positives to the cost of false negatives, thus
allowing a trade off between precision and recall.
This is crucial for our analysis since we deal with
sparse data due to the fact that in a document the
number of salient NPs is much smaller than the
number of irrelevant NPs.
2.3.3 Decision Forest Classifier
Decision forests are a collection of decision
trees together with a combination function. We
test the performance of DFC (Ho, 1998), a deci-
sion forest classifier that systematically constructs
decision trees by pseudo-randomly selecting sub-
sets of components of feature vectors. The advan-
tage of this classifier is that it combines a set of
different classifiers in order to improve accuracy.
It implements different splitting functions. In the
setting of our evaluation we tested the informa-
tion gain ratio (similar to the one used by Quinlan
in C4.5). An augmented feature vector (pairwise
sums, differences, and products of features) was
used for this classifier.
3 Linguistic Knowledge Enhances
Machine Learning
Not all simple noun phrases are equally
important to reflect document meaning.
Boguraev and Kennedy (1999) discuss the
issue that for the task of document gisting, topical
noun phrases are usually noun-noun compounds.
In our work, we rely on ML techniques to decide
which are the salient NPs, but we claim that a
shallow linguistic filtering applied before the
learning process improves the accuracy of the
classifiers. We performed four filtering steps:
1. Inflectional morphological processing:
Grouping inflectional variants together can
help especially in case of short documents
(which is sometimes the case for email
messages). English nouns have only two
kinds of regular inflection: a suffix for
the plural mark and another suffix for the
possessive one.
2. Removing unimportant modifiers: In this
second step we remove the determiners that
accompany the nouns and also the auxil-
iary words most and more that form the pe-
riphrastic forms of comparative and superla-
tive adjectives modifying the nouns (e.g.
?the most complex morphology? will be fil-
tered to ?complex morphology?).
3. Removing common words: We used a list
of 571 common words used in IR systems
in order to further filter the list of candi-
date NPs. Thus, words like even, following,
every, are eliminated from the noun phrase
structure.
4. Removing empty nouns: Words like lot,
group, set, bunch are considered empty
heads. For example the primary concept of
the noun phrases like ?group of students?,
?lots of students? or ?bunch of students?
is given by the noun ?students?. We ex-
tracted all the nouns that appear in front of
the preposition ?of? and then sorted them by
frequency of appearance. A threshold was
then used to select the final list (Klavans et
al., 1990). Three different data collections
were used: the Brown corpus, the Wall Street
Journal, and a set of 4000 email messages
(most of them related to a conference orga-
nization). We generated a set of 141 empty
nouns that we used in this forth step of the
filtering process.
4 Results and Discussion
One important step in summarization is the dis-
covery of the relevant information from the source
text. Our approach was to extract the salient NPs
using linguistic knowledge and machine learning
techniques. Our evaluation corpus consists of a
collection of email messages which is heteroge-
neous in genre, length, and topic. We used 2,500
NPs extracted from 51 email messages as a train-
ing set and 324 NPs from 8 messages for testing.
Each NP was manually tagged for saliency by one
human judge. We are planning to add more judges
in the future and measure the interuser agreement.
This section outlines a comparative evaluation
of five classifiers using two feature settings on the
task of extracting salient NPs from email mes-
sages. The evaluation shows the following im-
portant results:
Result 1. In the context of gisting, the head-
modifier relationship is an ordered relation be-
tween semantically equal elements.
We evaluate the impact of adding mh tfidf (see
section 2.2), as an additional feature in the feature
vector. This is shown in Table 2 in the different
feature vectors fv1 and fv2. The first feature vec-
tor, fv1, contains the features in sections 2.2.1 and
2.2.2, while fv2 includes as an additional feature
mh tfidf.
As can be seen from Table 3, the results of eval-
uating these two feature settings using five differ-
ent classifiers, show that fv2 performed better than
fv1. For example, the DFC classifier shows an in-
crease both in precision and recall. This allows us
to claim that in the context of gisting, the syntactic
head of the noun phrase is not always the seman-
tic head, and modifiers can have also an important
role.
One advantage of the rule-induction algorithms
is that their output is easily interpretable by hu-
mans. Analyzing C4.5rules output, we gain an
insight on the features that contribute most in the
classification process. In case of fv1, the most im-
portant features are: the first appearance of the
NP and its head (np focc, head focc), the length
of NP in number of words (np length words) and
the tf*idf measure of the whole NP and its head
(np tfidf, head tfidf ). For example:
 IF head focc <= 0.0262172 AND np tfidf
> 0.0435465 THEN Relevant
 IF np focc <= 0.912409 AND
np length words > 0.0242424 THEN
Relevant
 IF head tfidf <= 0.0243452 AND np tfidf
<= 0.0435465 AND np length words <=
0.0242424 then Not relevant
In case of fv2, the new feature m tfidf impacts
the rules for both Relevant and Not relevant cat-
egories. It supercedes the need for np tfidf and
head tfidf, as can be seen also from the rules be-
low:
 IF mh tfidf > 0.0502262 AND np focc <=
0.892585 THEN Relevant
 IF mh tfidf > 0.0180134 AND
np length words > 0.0260708 THEN
Relevant
 IF mh tfidf <= 0.0223546 AND
np length words <= 0.0260708 THEN
Not relevant
 IF mh tfidf <= 0.191205 AND np focc >
0.892585 THEN Not relevant
Feature vector 1 (fv1)
head focc head tfidf np focc np tfidf np length chars np length words par pos sent pos
Feature vector 2 (fv2)
head focc head tfidf mh tfidf np focc np tfidf np length chars np length words par pos sent pos
Table 2: Two feature settings to evaluate the impact of mh tfidf
C4.5 OC1 C4.5 rules Ripper DFC
p r p r p r p r p r
fv1 73.3% 78.6% 73.7% 93% 73.7% 88.5% 83.6% 71.4% 80.3% 83.5%
fv2 70% 88.9% 82.3% 88% 73.7% 95% 85.7% 78.8% 85.7% 87.9%
Table 3: Evaluation of two feature vectors using five classifiers
Result 2. Classifiers? performance depends
on the characteristics of the corpus, and com-
bining classifiers improves accuracy
This result was postulated by evaluating the
performance of five different classifiers in the task
of extracting salient noun phrases. As measures
of performance we use precision and recall . The
evaluation was performed according to what de-
gree the output of the classifiers corresponds to
the user judgments and the results are presented
in Table 3.
We first compare two decision tree classifiers:
one which uses as the splitting function only a sin-
gle feature (C4.5) and the other, the oblique tree
classifier (OC1) which at each internal node tests
a linear combination of features. Table 3 shows
that OC1 outperforms C4.5.
Columns 4 and 5 from Table 3 show the rela-
tive performance of RIPPER and C4.5rules. As
discussed in (Cohen, 1995), RIPPER is more ap-
propriate for noisy and sparse data collection than
C4.5rules. Table 3 shows that RIPPER performs
better than C4.5rules in terms of precision.
Finally, we investigate whether a combination
of classifiers will improve performance. Thus we
choose the Decision Forest Classifier, DFC, to
perform our test. DFC obtains the best results,
as can be seen from column 6 of Table 3.
Result 3. Linguistic filtering is an important
step in extracting salient NPs
As seen from Result 2, the DFC performed best
in our task, so we chose only this classifier to
present the impact of linguistic filtering. Table
4 shows that linguistic filtering improves preci-
sion and recall, having an important role espe-
cially on fv2, where the new feature, mh tfidf was
used (from 69.2% precision and 56.25% recall to
85.7% precision and 87.9% recall).
without filtering with filtering
precision recall precision recall
fv1 75% 75% 80.3% 83.5%
fv2 69.2% 56.25% 85.7% 87.9%
Table 4: Evaluation of linguistic filtering
This is explained by the fact that the filter-
ing presented in section 3 removed the noise in-
troduced by unimportant modifiers, common and
empty nouns.
Result 4. Noun phrases are better candi-
dates than n-grams
Presenting the gist of an email message by
phrase extraction addresses one obvious question:
are noun-phrases better than n-grams for repre-
senting the document content? To answer this
question we compared the results of our system,
GIST-IT, that extracts linguistically well moti-
vated phrasal units, with KEA output, that ex-
tracts bigrams and trigrams as key phrases using
a Na?ive Bayes model (Witten et al, 1999). Table
5 shows the results on one email message. The
n-gram approach of KEA system extracts phrases
like sort of batch, extracting lots, wn, and even
URLs that are unlikely to represent the gist of a
document. This is an indication that the linguis-
tically motivated GIST-IT phrases are more use-
ful for document gisting. In future work we will
perform also a task-based evaluation of these two
GIST-IT KEA
perl module wordnet interface module
?wn? command line program sort of batch
simple easy perl interface WordNet data
wordnet.pm module accesses the WordNet
wordnet system lots of WordNet
query perl module WordNet perl
wordnet QueryData
wordnet package wn
wordnet relation perl module
command line extracting
wordnet data use this module
included man page extracting lots
free software WordNet system
querydata www.cogsci.princeton.edu
Table 5: Salient phrase extraction with GIST-IT vs. KEA on one email message
approaches, to test usability.
5 Related Work
Machine learning has been successfully applied
to different natural language tasks, including text
summarization. A document summary is seen
as a succinct and coherent prose that captures
the meaning of the text. Prior work in docu-
ment summarization has been mostly based on
sentence extraction. Kupiec et al (1995) use ma-
chine learning for extracting the most impor-
tant sentences of the document. But extrac-
tive summarization relies on the properties of
source text that emails typically do not have:
coherence, grammaticality, well defined struc-
ture. Berger and Mittal (2000) present a summa-
rization system, named OCELOT that provides
the gist of the web documents based on proba-
bilistic models. Their approach is closed related
with statistical machine translation.
As discussed in (Boguraev and Kennedy,
1999), the meaning of ?summary? should be ad-
justed depending on the information management
task for which it is used. Key phrases, for ex-
ample, can be seen as semantic metadata that
summarize and characterize documents (Witten
et al, 1999; Turney, 2000). These approaches
select a set of candidate phrases (bigrams or tri-
grams) and then apply Na?ive Bayes learning to
classify them as key phrases or not. But deal-
ing only with n-grams does not always provide
good output in terms of a summary. In (Bogu-
raev and Kennedy, 1999) the ?gist? of a document
is seen as a sequence of salient objects, usually
topical noun phrases, presented in a highlighted
context. Their approach is similar to extracting
technical terms (Justeson and Katz, 1995). Noun
phrases are used also in IR task (Strzalkowski et
al., 1999; Smeaton, 1999; Sparck Jones, 1999).
The work of Strzalkowski et al (1999) supports
our hypothesis that for some NLP tasks (gisting,
IR) the head+modifier relation of a noun phrase is
in fact an ordered relation between semantically
equally important elements.
6 Conclusions and Future Work
In this paper we presented a novel technique for
document gisting suitable for domain and genre
independent collections such as email messages.
The method extracts simple noun phrases using
linguistic techniques and then uses machine learn-
ing to classify them as salient for the document
content. The contributions of this work are:
1. From a linguistic standpoint, we demon-
strated that the modifiers of a noun phrase
can be as semantically important as the head
for the task of gisting.
2. From a machine learning standpoint, we
evaluated the power and limitation of sev-
eral classifiers: decision trees, rule induc-
tion, and decision forests classifiers.
3. We proved that linguistic knowledge can en-
hance machine learning by evaluating the
impact of linguistic filtering before applying
the learning scheme.
The study, the evaluation, and the results pro-
vide experimental grounds for research not only
in summarization, but also in information extrac-
tion and topic detection.
References
A.L. Berger and V.O. Mittal. 2000. OCELOT:A sys-
tem for summarizing web pages. In Proceedings of
the 23rd Anual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 144?151, Athens, Greece.
B. Boguraev and C. Kennedy. 1999. Salience-based
content characterisation of text documents. In In-
terjit Mani and T. Maybury, Mark, editors, Ad-
vances in Automatic Text Summarization, pages 99?
111. The MIT Press.
W. Cohen. 1995. Fast effective rule induction. In
Machine-Learning: Proceedings of the Twelfth In-
ternational Conference.
T.K. Ho. 1998. The random subspace method
for constructing decision forests. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(8):832?844.
J. Justeson and S. Katz. 1995. Technical terminol-
ogy: Some linguistic properties and an algorithm
for identification in text. Natural Language Engi-
neering, (1):9?27.
J.L. Klavans, M.S. Chodorow, and N. Wacholder.
1990. From dictionary to knowledge base via
taxonomy. In Proceedings of the Sixth Confer-
ence of the University of Waterloo Centre for the
New Oxford English Dictionary and Text Research:
Electronic Text Research, University of Waterloo,
Canada.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings on the
18th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68?73, Seattle,WA.
R.J Mooney and C. Cardie. 1999. Symbolic ma-
chine learning for natural language processing. In
ACL?99 Tutorial.
S.K. Murthy, S. Kasif, S. Salzberg, and R. Beigel.
1993. OC1: Randomized induction of oblique de-
cision trees. In Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
322?327, Washington, D.C.
J.R Quinlan. 1993. C4.5: Program for Machine
Learning. Morgan Kaufmann Publisher, San Ma-
teo, California.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Pro-
ceedings of Third ACL Workshop on Very Large
Corpora.
A. Smeaton. 1999. Using NLP or NLP resources
for information retrieval tasks. In Tomek Strza-
lkowski, editor, Natural Language Information Re-
trieval. Kluwer, Boston, MA.
K. Sparck Jones. 1999. What is the role for NLP in
text retrieval. In Tomek Strzalkowski, editor, Nat-
ural Language Information Retrieval, pages 1?12.
Kluwer, Boston, MA.
T. Strzalkowski, F. Lin, J. Wang, and J. Perez-
Carballo. 1999. Evaluating natural language pro-
cessing techniques in information retrieval. In
Tomek Strzalkowski, editor, Natural Language In-
formation Retrieval. Kluwer, Boston, MA.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336,
May.
E Tzoukermann, S Muresan, and J.L. Klavans.
2001. GIST-IT: Summarizing email using linguis-
tic knowledge and machine learning. In Proceeding
of the HLT and KM Workshop, EACL/ACL 2001.
N. Wacholder. 1998. Simplex NPS sorted by head:
A method for identifying significant topics within
a document. In Proceedings of the COLING-ACL
Workshop on the Computational Treatment of Nom-
inals, Montreal, Canada.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In Proceedings of
DL?99, pages 254?256.
GIST-IT: Summarizing Email Using Linguistic Knowledge and Machine 
Learning  
 
Evelyne Tzoukermann 
Bell Labs, Lucent 
Technologies 
700 Mountain Avenue 
Murray Hill, NJ, 07974, USA 
evelyne@lucent.com 
Smaranda Muresan 
Columbia University  
500 W 120th Street 
New York, NY, 10027, USA 
smara@cs.columbia.edu 
Judith L. Klavans 
Columbia University 
Center for Research on 
Information Access 
535 W 114th Street 
New York, NY, 10027, USA 
klavans@cs.columbia.edu 
 
Abstract  
We present a system for the automatic 
extraction of salient information from 
email messages, thus providing the gist of 
their meaning.   Dealing with email raises 
several challenges that we address in this 
paper:  heterogeneous data in terms of 
length and topic. Our method combines 
shallow linguistic processing with 
machine learning to extract phrasal units 
that are representative of email content. 
The GIST-IT application is fully 
implemented and embedded in an active 
mailbox platform.  Evaluation was 
performed over three machine learning 
paradigms.  
Introduction 
The volume of email messages is huge and 
growing.  A qualitative and quantitative study of 
email overload [Whittaker and Sidner (1996)] 
shows that people receive a large number of 
email messages each day (~ 49) and that 21% of 
their   inboxes (about 334 messages) are long 
messages (over 10 Kbytes).  Therefore 
summarization techniques adequate for real-
world applications are of great interest and need 
[Berger and Mittal (2000), McKeown and Radev 
(1995), Kupiec et al(1995), McKeown et al
(1999), Hovy (2000)].  
  
In this paper we present GIST-IT, an 
automatic email message summarizer that will 
convey to the user the gist of the document 
through topic phrase extraction, by combining 
linguistic and machine learning techniques. 
 Email messages and web documents raise 
several challenges to automatic text 
processing, and the summarization task 
addresses most of them: they are free-style 
text, not always syntactically or 
grammatically well-formed, domain and 
genre independent, of variable length and on 
multiple topics.  Furthermore, due to the lack 
of well-formed syntactic and grammatical 
structures, the granularity of document 
extracts presents another level of complexity.  
In our work, we address the extraction 
problem at phrase-level [Ueda et al(2000), 
Wacholder et al(2000)], identifying salient 
information that is spread across multiple 
sentences and paragraphs.           
Our novel approach first extracts simple 
noun phrases as candidate units for 
representing document meaning and then 
uses machine learning algorithms to select 
the most prominent ones.  This combined 
method allows us to generate an informative, 
generic, ?at-a-glance? summary.  
In this paper, we show: (a) the efficiency 
of the linguistic approach for phrase 
extraction in comparing results with and 
without filtering techniques,  (b) the 
usefulness of vector representation in 
determining proper features to identify 
contentful information, (c) the benefit of 
using a new measure of TF*IDF for the noun 
phrase and its constituents, (d) the power of 
machine learning systems in evaluating 
several classifiers in order to select the one 
performing the best for this task. 
1 Related work  
Traditionally a document summary is seen as a 
small, coherent prose that renders to the user the 
important meaning of the text. In this framework 
most of the research has focused on extractive 
summaries at sentence level. However, as 
discussed in [Boguraev and Kennedy (1999)], 
the meaning of ?summary? should be adjusted 
depending on the information management task 
for which it is used. Key phrases, for example, 
can be seen as semantic metadata that 
summarize and characterize documents [Witten 
et al(1999), Turney (1999)]. These approaches 
select a set of candidate phrases (sequence of 
one, two or three consecutive stemmed, non-stop 
words) and then apply machine learning 
techniques to classify them as key phrases or 
not. But dealing only with n-grams does not 
always provide good output in terms of a 
summary (see discussion in Section 5.4).      
 Wacholder (1998) proposes a linguistically-
motivated method for the representation of the 
document aboutness: ?head clustering?. A list of 
simple noun phrases is first extracted, clustered 
by head and then ranked by the frequency of the 
head. Klavans et al(2000) report on the 
evaluation of ?usefulness? of head clustering in 
the context of browsing applications, in terms of 
quality and coverage.  
  Other researchers have used noun-phrases 
quite successfully for information retrieval task  
[Strzalkowski et al(1999), Sparck-Jones 
(1999)]. Strzalkowski et al(1999) uses head + 
modifier pairs as part of a larger system 
which constitutes the ?stream model? that is 
used for information retrieval. They treat the 
head-modifier relationship as an ?ordered 
relation between otherwise equal elements?, 
emphasizing that for some tasks, the syntactic 
head of the NP is not necessarily a semantic 
head, and the modifier is not either 
necessarily a semantic modifier and that the 
opposite is often true. Using a machine 
learning approach, we proved this hypothesis 
for the task of gisting.  
 Berger and Mittal (2000) present a 
summarization system named OCELOT, 
based on probabilistic models, which 
provides the gist of web documents. Like 
email messages, web documents are also very 
heterogeneous and their unstructured nature 
pose equal difficulties.  
In this paper, we propose a novel 
technique for summarization that combines 
the linguistic approach of extracting simple 
noun phrases as possible candidates for 
document extracts, and the use of machine 
learning algorithms to automatically select 
the most salient ones. 
2 System architecture 
The input to GIST-IT is a single email 
message. The architecture, presented in 
Figure 1 consists of four distinct functional 
components.  The first module is an email 
preprocessor developed for Text-To-Speech 
 
HPDLO 
PHVVDJH 
(  0DLO 3UHS 
7RNHQL]DWLRQ 
6LPSOH 13  
([WUDFWLRQ  
13 ILOWHULQJ 
13 ([WUDFWLRQ DQG )LOWHULQJ 8QLW 
)HDWXUH  
VHOHFWLRQ 
)HDWXUH  
VHOHFWLRQ 
&ODVVLILFDWLRQ  
0RGHO 
13  
FODVVLILFDWLRQ 
JLVW RI HPDLO  
PHVVDJH 
SUHVHQWDWLRQ 
0/ 8QLW 
 
Figure 1 System Architecture 
applications. The second component is a shallow 
text processing unit, which is actually a pipeline 
of modules for extraction and filtering of simple 
NP candidates.  The third functional component 
is a machine learning unit, which consists of a 
feature selection module and a text classifier. 
This module uses a training set and a testing set 
that were devided from our email corpus.  In 
order to test the performance of GIST-IT on the 
task of summarization, we use a heterogeneous 
collection of email messages in genre, length, 
and topic.  We represent each email as a set of 
NP feature vectors.  We used 2,500 NPs 
extracted from 51 email messages as a training 
set and 324 NPs from 8 messages for testing. 
Each NP was manually tagged for saliency by 
one of the authors and we are planning to add 
more judges in the future. The final module 
deals with presentation of the gisted email 
message. 
2.1 The Email Preprocessor 
This module uses finite-state transducer 
technology in order to identify message content.  
Information at the top of the message related to 
?From/To/Date'' as well as the signature block 
are separated from the message content. 
2.2 Candidate Simple Noun Phrase Extraction and 
Filtering Unit 
This module performs shallow text processing 
for extraction and filtering of simple NP 
candidates, consisting of a pipeline of three 
modules: text tokenization, NP extraction, and 
NP filtering. Since the tool was created to 
preprocess email for speech output, some of the 
text tokenization suitable for speech is not 
accurate for text processing and some 
modifications needed to be implemented (e.g. 
email preprocessor splits acronyms like DLI2 
into DLI 2).  The noun phrase extraction module 
uses Brill's POS tagger [Brill (1992)]and a base 
NP chunker [Ramshaw and Marcus (1995)]. 
After analyzing some of these errors, we 
augmented the tagger lexicon from our training 
data and we added lexical and contextual rules 
to deal mainly with incorrect tagging of gerund 
endings. In order to improve the accuracy of 
classifiers we perform linguistic filtering, as 
discussed in detail in Section 3.1.2. 
2.3 Machine Learning Unit 
The first component of the ML unit is the 
feature selection module to compute NP 
vectors.  In the training phase, a model for 
identifying salient simple NPs is created.  
The training data consist of a list of feature 
vectors already classified as salient/non-
salient by the user.  Thus we rely on user-
relevance judgments to train the ML unit. In 
the extraction phase this unit will classify 
relevant NPs using the model generated 
during training.  We applied three machine 
learning paradigms (decision trees, rule 
induction algorithms, and decision forest) 
evaluating three different classifiers.  
2.4 Presentation 
The presentation of the message gist is a 
complex user interface issue with its 
independent set of problems.   Depending on 
the application and its use, one can think of 
different presentation techniques.  The gist of 
the message could be the set of NPs or the set 
of sentences in which these NPs occur so that 
the added context would make it more 
understandable to the user. We do not address 
in this work the disfluency that could occur in 
listing a set of extracted sentences, since the 
aim is to deliver to the user the very content 
of the message even in a raw fashion.   GIST-
IT is to be used in an application where the 
output is synthesized speech.   The focus of 
this paper is on extracting content with GIST-
IT, although presentation is a topic for future 
research.  
3 Combining Linguistic Knowledge and 
Machine Learning for Email Gisting 
We combine symbolic machine learning and 
linguistic processing in order to extract the 
salient phrases of a document.   Out of the 
large syntactic constituents of a sentence, e.g. 
noun phrases, verb phrases, and prepositional 
phrases, we assume that noun phrases (NPs) 
carry the most contentful information about 
the document, even if sometimes the verbs 
are important too, as reported in the work by 
[Klavans and Kan (1998)]. The problem is 
that no matter the size of a document, the 
number of informative noun phrases is very 
small comparing with the number of all noun 
phrases, making selection a necessity. Indeed, in 
the context of gisting, generating and presenting 
the list of all noun phrases, even with adequate 
linguistic filtering, may be overwhelming. Thus, 
we define the extraction of important noun 
phrases as a classification task, applying 
machine learning techniques to determine which 
features associated with the candidate NPs 
classify them as salient vs. non-salient.  We 
represent the document -- in this case an email 
message -- as a set of candidate NPs, each of 
them associated with a feature vector used in the 
classification model.  We use a number of 
linguistic methods both in the extraction and in 
the filtering of candidate noun phrases, and in 
the selection of the features.  
   
3.1 Candidate NPs 
Noun phrases were extracted using Ramshaw 
and Marcus's base NP chunker [Ramshaw and 
Marcus (1995)].  The base NP is either a simple 
NP as defined by Wacholder (1998) or a 
conjunction of two simple NPs.  Since the 
feature vectors used in the classifier scheme are 
simple NPs we used different heuristics to 
automatically split the conjoined NPs (CNP) 
into simple ones (SNP), properly assigning the 
premodifiers. Table 1 presents such an example: 
 
CNP: physics/NN and/CC biology/NN skilled/JJ  
researchers/NNS 
SNP1:  physics/NN skilled/JJ researchers/NNS 
SNP2: biology/NN skilled/JJ researchers/NNS 
Table 1 Splitting Complex NPs into Simple NPs  
3.1.2 Filtering simple NPs   
Since not all simple noun phrases are equally 
important to reflect the document meaning, we 
use well-defined linguistic properties to extract 
only those NPs (or parts of NPs) that have a 
greater chance to render the salient information. 
By introducing this level of linguistic filtering 
before applying the learning scheme, we 
improve the accuracy of the classifiers, thus 
obtaining better results (see discussion in 
sections 4.1.3 and 5.3). We performed four 
filtering steps: 
1. Inflectional morphological processing. 
English nouns have only two kinds of inflection: 
an affix that marks plural and an affix that 
marks possessive.
 
2. Removing unimportant modifiers. In this 
second step we remove the determiners that 
accompany the nouns and also the auxiliary 
words most and more that form the 
periphrastic forms of comparative and 
superlative adjectives modifying the nouns.
 
3. Remove common words. We used a list of 
571 common words used in IR systems in 
order to further filter the list of candidate 
NPs. Thus, words like even, following, every, 
are eliminated from the noun phrase 
structure. (i.e. ?even more detailed 
information? and ?detailed information? will 
also be grouped together). 
 
4. Remove ?empty? nouns. Words like lot, 
group, set, bunch are considered ?empty? 
nouns in the sense that they have no 
contribution to the noun phrase meaning. For 
example the meaning of the noun phrases like 
?group of students?,  ?lots of students? or 
?bunch of students? is given by the noun 
?students?. In order not to bias the extraction 
of empty nouns we used three different data 
collections: Brown corpus, Wall Street 
Journal, and a set of 4000 email messages 
(most of which were collected during a 
conference organization). Our algorithm was 
a simple one: we extracted all the nouns that 
appear in front of the preposition ?of? and 
then sorted them by frequency of appearance 
in all three corpora and used a threshold to 
select the final list. We generated a set of 141 
empty nouns that we used in this forth step of 
filtering process.   
3.2 Feature Selection 
We select a set of nine features that fall into 
three categories: linguistic, statistical 
(frequency-based) and positional. These 
features capture information about the 
relative importance of NPs to the document 
meaning.  
Several studies rely on linguistic intuition 
that the head of the noun phrase makes a 
greater contribution to the semantics of the 
nominal group than the modifiers. For some 
NLP tasks, the head is not necessarily the 
most important item of the noun phrase.  In 
analyzing email messages from the 
perspective of finding salient NPs, we claim 
that the constituents of the NP have often as 
much semantic content as the head.  This 
opinion is also supported in the work of 
[Strzalkowski et al(1999)]. In many cases, the 
meaning of the NP is given equally by 
modifier(s) -- usually nominal modifiers(s) -- 
and head.  Consider the following list of simple 
NPs selected as candidates: 
(1) ?conference workshop announcement? 
(2) ?international conference? 
(3) ?workshop description? 
(4) ?conference deadline? 
In the case of noun phrase (1) the importance of 
the noun phrase is found in the two noun 
modifiers: conference and   workshop as much 
as in the head announcement. We test this 
empirical observation by introducing as a 
separate feature in the feature vector, a new 
TF*IDF measure that counts for both the 
modifiers and the head of the noun phrase, thus 
seeing the NP as a sequence of equally weighted 
elements.  For the example above the new 
feature will be: 
TF*IDFconference + TF*IDFworkshop + TF*IDFannouncement 
We divided the set of features into three 
groups: one associated with the head of the noun 
phrase, one associated with the whole NP and 
one that represents the new TF*IDF measure 
discussed above.  Since we want to use this 
technique on other types of documents, all 
features are independent of the text type or 
genre.  For example, in the initial selection of 
our attributes we introduced as separate features 
the presence or the absence of NPs in the subject 
line of the email and in the headline of the body. 
Kilander (1996) pointed out that users estimate 
that ?subject lines can be useful, but also 
devastating if their importance is overly 
emphasized?.  Based on this study and also on 
our goal to provide a method that is domain and 
genre independent we decided not to consider 
the subject line and the headlines as separate 
features, but rather as weights included in the 
TF*IDF measures as presented below.  Another 
motivation for this decision is that in email 
processing the correct identification of headlines 
is not always clear. 
3.2.1 Features associated with the Head 
We choose two features to characterize the head 
of the noun phrases: 
head_tfidf ? the TF*IDF measure of the 
head of the candidate NP. 
head_focc - The first occurrence of the head 
in text (the numbers of words that precede the 
head divided by the total number of words in 
the document).  
3.2.2 Features associated with the whole 
NP 
We select six features that we consider 
relevant in association with the whole NP:  
np_tfidf ? the TF*IDF measure associated 
with the whole NP.  
np_focc - The first occurrence of the noun 
phrase in the document.  
np_length_words - Noun phrase length 
measured in number of words, normalized by 
dividing it with the total numbers of words in 
the candidate NPs list. 
np_length_chars - Noun phrase length 
measured in number of characters, 
normalized by dividing it with the total 
numbers of characters in the candidate NPs 
list. 
sent_pos - Position of the noun phrase in 
sentence: the number of words that precede 
the noun phrase, divided by the sentence 
length. For noun phrases in the subject line 
and headlines (which are usually short and 
will be affected by this measure), we consider 
the maximum length of sentence in document 
as the normalization factor.  
par_pos - Position of noun phrase in 
paragraph, same as sent_pos, but at the 
paragraph level. 
3.2.3 Feature that considers all constituents 
of the NP equally weighted 
m_htfidf - the new TF*IDF measure that 
take into consideration the importance of the 
modifiers.  
In computing the TF*IDF measures 
(head_tfidf, np_tfidf, m_tfidf), weights wi, 
were assigned to account for the presence in 
the subject line and/or headline.  
wi1 ? if the head appears both in the subject 
line and headline; 
wi2 ? if the head appears only in the subject 
line; 
wi3 ? if the head appears only in headlines 
 where wi1 > wi2 > wi3. 
These weights were manually chosen after 
a set of experiments, but we plan to use either 
a regression method or explore with genetic 
algorithms to automatically learn them. 
3.3 Three Paradigms of Supervised Machine 
Learning  
Symbolic machine learning is used in 
conjunction with many NLP applications 
(syntactic and semantic parsing, POS tagging, 
text categorization, word sense disambiguation).     
In this paper we compare three symbolic 
learning techniques applied to the task of salient 
NP extraction: decision tree, rule induction 
learning and decision forests.   
We tested the performance of an axis-parallel 
decision tree, C4.5 [Quinlan (1993)]; a rule 
learning system RIPPER [Cohen (1995)] and a 
decision forest classifier (DFC) [Ho (1998)]. 
RIPPER allows the user to specify the loss ratio, 
which indicates the ratio of the cost of a false 
positive to the cost of a false negative, thus 
allowing the trade off between precision and 
recall. This is crucial for our analysis since we 
deal with sparse data set (in a document the 
number of salient NPs is much smaller than the 
number of irrelevant NPs). Finally we tried to 
prove that a combination of classifiers might 
improve accuracy, increasing both precision and 
recall. The Decision Forest Classifier (DFC) 
uses an algorithm for systematically 
constructing decision trees by pseudo-randomly 
selecting subsets of components of feature 
vectors. It implements different splitting 
functions.  In the setting of our evaluation we 
tested the information gain ratio (similar to the 
one used by Quinlan in C4.5). An augmented 
feature vector (pairwise sums, differences, and 
products of features) was used for this classifier. 
4 Evaluation and Experimental Results 
Since there are many different summaries for 
each document, evaluating summaries is a 
difficult problem. Extracting the salient noun 
phrases is the first key step in the summarization 
method that we adopt in this paper. Thus, we 
focus on evaluating the performance of GIST-IT 
on this task, using three classification schemes 
and two different feature settings. 
4.1 Evaluation Scheme 
There are several questions that we address in 
this paper: 
4.1.1 What features or combination of 
features are important in determining the 
degree of salience of an NP?   
Following our assumption that each 
constituent of the noun phrase is equally 
meaningful, we evaluate the impact of adding 
m_htfidf
 (see section 3.2.3), as an additional 
feature in the feature vector.  This is shown in 
Table 2 in the different feature vectors fv1 
and fv2. 
 
fv1-  head_focc  head_tfidf np_focc np_tfidf   
        np_length_words  np_length_chars par_pos sent_pos 
fv2 - head_focc  head_tfidf  m_htfidf  np_focc np_tfidf  
         np_length_words np_length_chars par_pos sent_pos 
Table 2 Two feature settings to evaluate the 
impact of m_htfidf 
4.1.2 What classification scheme is more 
adequate to our task? 
We evaluate the performance of three 
different classifiers in the task of extracting 
salient noun phrases.  As measures of 
performance we use precision (p) and recall 
(r).  The evaluation was performed according 
to what degree the output of the classifiers 
corresponds to the user judgments.  
 
C4.5 Ripper     DFC  Feature 
vectors p  r p r p r 
fv1 73.3 78.6 83.6 71.4 80.3 83.5 
fv2 70 88.9 85.7 78.8 85.7 87.9 
Table 3 Evaluation of two feature vectors using 
three classifiers 
 
Table 3 shows our results that answer 
these two questions. The table rows represent 
the two feature vectors we are comparing, 
and the columns correspond to the three 
classifiers chosen for the evaluation.   
4.1.3 Is linguistic filtering an important step 
in extracting salient NPs? 
In the third evaluation we analyse the impact 
of linguistic filtering on the classifier?s 
performance. It turns out that results show 
major improvements, from 69.2% to 85.7% 
for precision of fv2, and from 56.25% to 
87.9% for recall of fv2.  For detailed results, 
see [Muresan et al (2001)]. 
4.1.4 After the filtering and classification, are 
noun phrases good candidates for representing 
the gist of an email message? 
In order to answer this question, we compare 
the output of GIST-IT on one email with the 
results of KEA system [Witten et al(1999)] that 
uses a 'bag-of-words' approach to key phrase 
extraction (see Table 4). 
 
module     
sort of batch 
WordNet data 
 accesses   
the WordNet     
lots of WordNet       
WordNet perl          
QueryData 
wn 
perl module 
extracting      
use this module 
extracting lots      
WordNet system  
www.cogsci.princeton.e
du 
Perl module wordne 
interface  
'wn' command line program   
simple easy perl interface      
included man page 
wordnet                 
wordnet.pm module 
wordnet system      
wordnet package 
query perl module     
command line  
wordnet relation    
wordnet data   
free software        
querydata        
Table 4 KEA (left)  vs GIST-IT output (right) 
5  Discussion of results 
The results shown indicate that best system 
performance reached 87.9% recall and 85.7% 
precision.  Although these results are very high, 
judging NP relevance is a complex and highly 
variable task.  In the future, we will extend the 
gold standard with more judges, more data, and 
thus a more precise standard for measurement. 
5.1 The right selection of features 
Feature selection has a decisive impact on 
overall performance. As seen in Table 2, fv2 has 
m_htfidf
 as an additional feature, and its 
performance shown in Table 3 is superior to fv1; 
the DFC classifier shows an increase both in 
precision and recall. These results support the 
original hypothesis that in the context of gisting, 
the syntactic head of the noun phrase is not 
always the semantic head, and modifiers can 
also have an important role.  
5.2 Different classification models 
The effectiveness of different classification 
schemes in the context of our task is discussed 
here. As shown in Table 3, C4.5 performs well 
especially in terms of recall. RIPPER, as 
discussed in [Cohen (1995)], is more appropriate 
for noisy and sparse data collection than 
C4.5, showing an improvement in precision. 
Finally, DFC which is a combination of 
classifiers, shows  improved performance. 
The classifier was run with an augumented 
feature vector that included pairwise sums, 
differences and products of the features.  
5.3 Impact of linguistic knowledge 
As shown in previous section, DFC 
performed best in our task, so we chose only 
this classifier to present the impact of 
linguistic knowledge. Linguistic filtering 
improved precision and recall, having an 
important role especially on fv2, where the 
new feature m_tfidf was used. This is 
explained by the fact that the filtering 
presented in section 3.1.2 removed the noise 
introduced by unimportant modifiers, 
common and empty nouns, thus giving this 
new feature a larger impact.   
5.4 Noun phrases are better than n-grams   
Presenting the gist of an email message by 
phrase extraction addresses one obvious 
question: can any phrasal extract represent 
the content of a document, or must a well 
defined linguistic phrasal structure be used? 
To answer this question we compare the 
results of our system that extract 
linguistically principled phrasal units, with 
KEA output, that extracts bigrams and 
trigrams as key phrases [Witten et al(1999)].   
Table 4 shows the results of the KEA system. 
Due to the n-gram approach, KEA output 
contains phrases like sort of batch, extracting 
lots, wn, and even urls that are unlikely to 
represent the gist of a document. 
 
Conclusion and future work 
In this paper we presented a novel technique 
for document gisting suitable for domain and 
genre independent collections such as email 
messages.  The method extracts simple noun 
phrases using linguistic techniques and then 
use machine learning to classify them as 
salient for the document content.  We 
evaluated the system in different 
experimental settings using three 
classification models. In analyzing the 
structure of NPs, we demonstrated that the 
modifiers of a noun phrase can be 
semantically as important as the head for the 
task of gisting. GIST-IT is fully implemented, 
evaluated, and embedded in an application, 
which allows user to access a set of information 
including email, finances, etc.  
  We plan to extend our work by taking 
advantage of structured email, by classifying 
messages into folders, and then by applying 
information extraction techniques.  Since NPs 
and machine learning techniques are domain and 
genre independent, we plan to test GIST-IT on 
different data collections (e.g. web pages), and 
for other knowledge management tasks, such as   
document indexing or query refinement. 
Additionally, we plan to test the significance of 
the output for the user, i.e. whether the system 
provide informative content and adequate gist of 
the message. 
References  
Berger, A.L and Mittal, V.O (2000). OCELOT:A system for 
summarizing web pages. In Proceedings of the 23rd 
Annual International ACM SIGIR, Athens, Greece, pp 
144-151.  
Brill, E. (1992).  A Simple Rule-based Part of Speech 
Tagger. In Proceedings of the Third Conference on 
ANLP. Trento, Italy; 1992 
Boguraev, B. and Kennedy, C. (1999). Salience-based 
content characterisation of text documents. In I. Mani 
and T. Maybury, M., editors, Advances in Automatic 
Text Summarization, pp 99-111. The MIT Press.  
Cohen, W. (1995). Fast Effective Rule Induction. Machine-
Learning: Proceedings of the Twelfth International 
Conference.  
Ho, T.K (1998). The random subspace method for 
constructing decision forests. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 20(8). 
Hovy, E.H (2000). Automated Text Summarization. In R. 
Mitkov, editor,  Oxford University Handbook of 
Computational Linguistics. Oxford Univ. Press. 
Kilander, F. (1996). Properties of electronic texts for 
classification purposes as suggested by users. 
Klavans, J.L., Wacholder, N. and Evans, D.K. (2000) 
Evaluation of computational linguistic techniques for 
identifying significant topics for browsing applications. 
In Proceedings (LREC-2000), Athens. Greece. 
Klavans, J.L. and Kan, M-Y. (1998).Role of verbs in 
document analysis. In proceedings of COLING/ACL  98. 
Kupiec, J., Pedersen, J. and Chen, F. (1995). A trainable 
document summarizer. In Proceedings of the 18th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp 
68-73, Seattle, WA. 
McKeown, K.R,  Klavans, J.L, Hatzivassiloglou, V.,  
Barzilay, R. and Eskin, E. (1999). Towards 
multidocument summarization by reformulation: 
Progress and prospects. In Proceedings of AAAI'99. 
McKeown, K.R and Radev, D.R (1995). Generating 
summaries of multiple news articles. In Proceedings 
of the 18th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, pp 74-82, Seattle, WA. 
Muresan, S., Tzoukermann, E. and Klavans, J.L. 
(2001). Email Summarization Using Linguistic and 
Machine Learning Techniques. In Proceedings of 
CoNLL 2001 ACL Workshop, Toulouse, France. 
Murthy, S.K., Kasif, S., Salzberg, S. and Beigel, R. 
(1993). OC1: Randomized Induction of Oblique 
Decision Trees. Proceedings of the Eleventh National 
Conference on Artificial Intelligence, pp. 322--327, 
Washington, D.C. 
Quinlan, J.R (1993). C4.5: Program for Machine 
Learning. Morgan Kaufmann. 
Ramshaw, L.A. and Marcus, M.P. (1995). Text 
Chunking Using Transformation-Based Learning. In 
Proceedings of Third ACL Workshop on Very Large 
Corpora, MIT. 
Sparck-Jones, K. (1999). What Is The Role of NLP in 
Text Retrieval. In T. Strzalkowski, editor, Natural 
Language Information Retrieval. Kluwer, Boston, 
MA.    
Strzalkowski, T., Lin, F., Wang, J., and Perez-Carballo, 
J. (1999). Evaluating natural language  processing 
techniques for information retrieval. In T. 
Strzalkowski, editor, Natural Language Information 
Retrieval. Kluwer, Boston, MA.    
Turney, P.D. (2000). Learning algorithms for 
keyphrase exraction. Information Retrieval, 2(4): pp 
303-336. 
Ueda, Y., Oka M., Koyama T. and Miyauchi T (2000). 
Toward the "at-a-glance" summary: Phrase-
representation summarization method. In 
Proceedings of COLING 2000.   
Wacholder, N. (1998). Simplex NPS sorted by head: a 
method for identifying significant topics within a 
document, In Proceedings of the COLING-ACL 
Workshop on the Computational Treatment of 
Nominals. 
Whittaker, S. and Sidner, C. Email overload: Exploring 
personal information management of email. In 
Proceedings of CHI?96. p. 276-283. NY:ACM Press 
Witten, I.H, Paynter, G.W., Frank E., Gutwin C. and 
Nevill-Manning, C.G (1999). KEA: Practical 
automatic keyphrase extraction. In Proceedings of 
DL'99, pp 254-256. 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 832?839,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Grammar Approximation by Representative Sublanguage:
A New Model for Language Learning
Smaranda Muresan
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
smara@umiacs.umd.edu
Owen Rambow
Center for Computational Learning Systems
Columbia University
New York, NY 10027, USA
rambow@cs.columbia.edu
Abstract
We propose a new language learning model
that learns a syntactic-semantic grammar
from a small number of natural language
strings annotated with their semantics, along
with basic assumptions about natural lan-
guage syntax. We show that the search space
for grammar induction is a complete gram-
mar lattice, which guarantees the uniqueness
of the learned grammar.
1 Introduction
There is considerable interest in learning computa-
tional grammars.1 While much attention has focused
on learning syntactic grammars either in a super-
vised or unsupervised manner, recently there is a
growing interest toward learning grammars/parsers
that capture semantics as well (Bos et al, 2004;
Zettlemoyer and Collins, 2005; Ge and Mooney,
2005).
Learning both syntax and semantics is arguably
more difficult than learning syntax alone. In for-
mal grammar learning theory it has been shown that
learning from ?good examples,? or representative
examples, is more powerful than learning from all
the examples (Freivalds et al, 1993). Haghighi and
Klein (2006) show that using a handful of ?proto-
1This research was supported by the National Science Foun-
dation under Digital Library Initiative Phase II Grant Number
IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs).
We would like to thank Judith Klavans for her contributions
over the course of this research, Kathy McKeown for her in-
put, and several anonymous reviewers for very useful feedback
on earlier drafts of this paper.
types? significantly improves over a fully unsuper-
vised PCFG induction model (their prototypes were
formed by sequences of POS tags; for example, pro-
totypical NPs were DT NN, JJ NN).
In this paper, we present a new grammar formal-
ism and a new learning method which together ad-
dress the problem of learning a syntactic-semantic
grammar in the presence of a representative sample
of strings annotated with their semantics, along with
minimal assumptions about syntax (such as syntac-
tic categories). The semantic representation is an
ontology-based semantic representation. The anno-
tation of the representative examples does not in-
clude the entire derivation, unlike most of the ex-
isting syntactic treebanks. The aim of the paper is to
present the formal aspects of our grammar induction
model.
In Section 2, we present a new grammar formal-
ism, called Lexicalized Well-Founded Grammars,
a type of constraint-based grammars that combine
syntax and semantics. We then turn to the two main
results of this paper. In Section 3 we show that
our grammars can always be learned from a set of
positive representative examples (with no negative
examples), and the search space for grammar in-
duction is a complete grammar lattice, which guar-
antees the uniqueness of the learned grammar. In
Section 4, we propose a new computationally effi-
cient model for grammar induction from pairs of ut-
terances and their semantic representations, called
Grammar Approximation by Representative Sublan-
guage (GARS). Section 5 discusses the practical use
of our model and Section 6 states our conclusions
and future work.
832
2 Lexicalized Well-Founded Grammars
Lexicalized Well-Founded Grammars (LWFGs) are
a type of Definite Clause Grammars (Pereira and
Warren, 1980) where: (1) the Context-Free Gram-
mar backbone is extended by introducing a par-
tial ordering relation among nonterminals (well-
founded) 2) each string is associated with a
syntactic-semantic representation called semantic
molecule; 3) grammar rules have two types of con-
straints: one for semantic composition and one for
ontology-based semantic interpretation.
The partial ordering among nonterminals allows
the ordering of the grammar rules, and thus facili-
tates the bottom-up induction of these grammars.
The semantic molecule is a syntactic-semantic
representation of natural language strings   	

where  (head) encodes the information required
for semantic composition, and  (body) is the ac-
tual semantic representation of the string. Figure 1
shows examples of semantic molecules for an ad-
jective, a noun and a noun phrase. The represen-
tations associated with the lexical items are called
elementary semantic molecules (I), while the rep-
resentations built by the combination of others are
called derived semantic molecules (II). The head
of the semantic molecule is a flat feature structure,
having at least two attributes encoding the syntac-
tic category of the associated string, cat, and the
head of the string, head. The set of attributes is
finite and known a priori for each syntactic cate-
gory. The body of the semantic molecule is a flat,
ontology-based semantic representation. It is a log-
ical form, built as a conjunction of atomic predi-
cates ffProceedings of ACL-08: HLT, pages 1012?1020,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generalizing Word Lattice Translation
Christopher Dyer?, Smaranda Muresan, Philip Resnik?
Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony, smara, resnik AT umd.edu
Abstract
Word lattice decoding has proven useful in
spoken language translation; we argue that it
provides a compelling model for translation of
text genres, as well. We show that prior work
in translating lattices using finite state tech-
niques can be naturally extended to more ex-
pressive synchronous context-free grammar-
based models. Additionally, we resolve a
significant complication that non-linear word
lattice inputs introduce in reordering mod-
els. Our experiments evaluating the approach
demonstrate substantial gains for Chinese-
English and Arabic-English translation.
1 Introduction
When Brown and colleagues introduced statistical
machine translation in the early 1990s, their key in-
sight ? harkening back to Weaver in the late 1940s ?
was that translation could be viewed as an instance
of noisy channel modeling (Brown et al, 1990).
They introduced a now standard decomposition that
distinguishes modeling sentences in the target lan-
guage (language models) from modeling the rela-
tionship between source and target language (trans-
lation models). Today, virtually all statistical trans-
lation systems seek the best hypothesis e for a given
input f in the source language, according to
e? = arg max
e
Pr(e|f) (1)
An exception is the translation of speech recogni-
tion output, where the acoustic signal generally un-
derdetermines the choice of source word sequence
f . There, Bertoldi and others have recently found
that, rather than translating a single-best transcrip-
tion f , it is advantageous to allow the MT decoder to
consider all possibilities for f by encoding the alter-
natives compactly as a confusion network or lattice
(Bertoldi et al, 2007; Bertoldi and Federico, 2005;
Koehn et al, 2007).
Why, however, should this advantage be limited
to translation from spoken input? Even for text,
there are often multiple ways to derive a sequence
of words from the input string. Segmentation of
Chinese, decompounding in German, morpholog-
ical analysis for Arabic ? across a wide range
of source languages, ambiguity in the input gives
rise to multiple possibilities for the source word se-
quence. Nonetheless, state-of-the-art systems com-
monly identify a single analysis f during a prepro-
cessing step, and decode according to the decision
rule in (1).
In this paper, we go beyond speech translation
by showing that lattice decoding can also yield im-
provements for text by preserving alternative anal-
yses of the input. In addition, we generalize lattice
decoding algorithmically, extending it for the first
time to hierarchical phrase-based translation (Chi-
ang, 2005; Chiang, 2007).
Formally, the approach we take can be thought of
as a ?noisier channel?, where an observed signal o
gives rise to a set of source-language strings f ? ?
F(o) and we seek
e? = arg max
e
max
f ??F(o)
Pr(e, f ?|o) (2)
= arg max
e
max
f ??F(o)
Pr(e)Pr(f ?|e, o) (3)
= arg max
e
max
f ??F(o)
Pr(e)Pr(f ?|e)Pr(o|f ?).(4)
Following Och and Ney (2002), we use the maxi-
mum entropy framework (Berger et al, 1996) to di-
rectly model the posterior Pr(e, f ?|o) with parame-
ters tuned to minimize a loss function representing
1012
the quality only of the resulting translations. Thus,
we make use of the following general decision rule:
e? = arg max
e
max
f ??F(o)
M?
m=1
?m?m(e, f
?, o) (5)
In principle, one could decode according to (2)
simply by enumerating and decoding each f ? ?
F(o); however, for any interestingly large F(o) this
will be impractical. We assume that for many in-
teresting cases of F(o), there will be identical sub-
strings that express the same content, and therefore
a lattice representation is appropriate.
In Section 2, we discuss decoding with this model
in general, and then show how two classes of trans-
lation models can easily be adapted for lattice trans-
lation; we achieve a unified treatment of finite-state
and hierarchical phrase-based models by treating
lattices as a subcase of weighted finite state au-
tomata (FSAs). In Section 3, we identify and solve
issues that arise with reordering in non-linear FSAs,
i.e. FSAs where every path does not pass through
every node. Section 4 presents two applications of
the noisier channel paradigm, demonstrating sub-
stantial performance gains in Arabic-English and
Chinese-English translation. In Section 5 we discuss
relevant prior work, and we conclude in Section 6.
2 Decoding
Most statistical machine translation systems model
translational equivalence using either finite state
transducers or synchronous context free grammars
(Lopez, to appear 2008). In this section we discuss
the issues associated with adapting decoders from
both classes of formalism to process word lattices.
The first decoder we present is a SCFG-based de-
coder similar to the one described in Chiang (2007).
The second is a phrase-based decoder implementing
the model of Koehn et al (2003).
2.1 Word lattices
A word lattice G = ?V,E? is a directed acyclic
graph that formally is a weighted finite state automa-
ton (FSA). We further stipulate that exactly one node
has no outgoing edges and is designated the ?end
node?. Figure 1 illustrates three classes of word
lattices.
0
1x 2a
y
3bc
0 1
ax
?
2b 3dc
0 1a 2b 3c
Figure 1: Three examples of word lattices: (a) sentence,
(b) confusion network, and (c) non-linear word lattice.
A word lattice is useful for our purposes because
it permits any finite set of strings to be represented
and allows for substrings common to multiple mem-
bers of the set to be represented with a single piece
of structure. Additionally, all paths from one node to
another form an equivalence class representing, in
our model, alternative expressions of the same un-
derlying communicative intent.
For translation, we will find it useful to encode
G in a chart based on a topological ordering of the
nodes, as described by Cheppalier et al (1999). The
nodes in the lattices shown in Figure 1 are labeled
according to an appropriate numbering.
The chart-representation of the graph is a triple of
2-dimensional matrices ?F,p,R?, which can be con-
structed from the numbered graph. Fi,j is the word
label of the jth transition leaving node i. The cor-
responding transition cost is pi,j . Ri,j is the node
number of the node on the right side of the jth tran-
sition leaving node i. Note that Ri,j > i for all i, j.
Table 1 shows the word lattice from Figure 1 repre-
sented in matrix form as ?F,p,R?.
0 1 2
a 1 1 b 1 2 c 1 3
a 13 1 b 1 2 c
1
2 3
x 13 1 d
1
2 3
 13 1
x 12 1 y 1 2 b
1
2 3
a 12 2 c
1
2 3
Table 1: Topologically ordered chart encoding of the
three lattices in Figure 1. Each cell ij in this table is a
triple ?Fij ,pij ,Rij?
1013
2.2 Parsing word lattices
Chiang (2005) introduced hierarchical phrase-based
translation models, which are formally based
on synchronous context-free grammars (SCFGs).
Translation proceeds by parsing the input using the
source language side of the grammar, simultane-
ously building a tree on the target language side via
the target side of the synchronized rules. Since de-
coding is equivalent to parsing, we begin by present-
ing a parser for word lattices, which is a generaliza-
tion of a CKY parser for lattices given in Cheppalier
et al (1999).
Following Goodman (1999), we present our lat-
tice parser as a deductive proof system in Figure 2.
The parser consists of two kinds of items, the first
with the form [X ? ? ? ?, i, j] representing rules
that have yet to be completed and span node i to
node j. The other items have the form [X, i, j] and
indicate that non-terminal X spans [i, j]. As with
sentence parsing, the goal is a deduction that covers
the spans of the entire input lattice [S, 0, |V | ? 1].
The three inference rules are: 1) match a terminal
symbol and move across one edge in the lattice 2)
move across an -edge without advancing the dot in
an incomplete rule 3) advance the dot across a non-
terminal symbol given appropriate antecedents.
2.3 From parsing to MT decoding
A target language model is necessary to generate flu-
ent output. To do so, the grammar is intersected with
an n-gram LM. To mitigate the effects of the combi-
natorial explosion of non-terminals the LM intersec-
tion entails, we use cube-pruning to only consider
the most promising expansions (Chiang, 2007).
2.4 Lattice translation with FSTs
A second important class of translation models in-
cludes those based formally on FSTs. We present a
description of the decoding process for a word lattice
using a representative FST model, the phrase-based
translation model described in Koehn et al (2003).
Phrase-based models translate a foreign sentence
f into the target language e by breaking up f into
a sequence of phrases f
I
1, where each phrase f i can
contain one or more contiguous words and is trans-
lated into a target phrase ei of one or more contigu-
ous words. Each word in f must be translated ex-
actly once. To generalize this model to word lattices,
it is necessary to choose both a path through the lat-
tice and a partitioning of the sentence this induces
into a sequence of phrases f
I
1. Although the number
of source phrases in a word lattice can be exponen-
tial in the number of nodes, enumerating the possible
translations of every span in a lattice is in practice
tractable, as described by Bertoldi et al (2007).
2.5 Decoding with phrase-based models
We adapted the Moses phrase-based decoder to
translate word lattices (Koehn et al, 2007). The
unmodified decoder builds a translation hypothesis
from left to right by selecting a range of untrans-
lated words and adding translations of this phrase to
the end of the hypothesis being extended. When no
untranslated words remain, the translation process is
complete.
The word lattice decoder works similarly, only
now the decoder keeps track not of the words that
have been covered, but of the nodes, given a topo-
logical ordering of the nodes. For example, assum-
ing the third lattice in Figure 1 is our input, if the
edge with word a is translated, this will cover two
untranslated nodes [0,1] in the coverage vector, even
though it is only a single word. As with sentence-
based decoding, a translation hypothesis is complete
when all nodes in the input lattice are covered.
2.6 Non-monotonicity and unreachable nodes
The changes described thus far are straightfor-
ward adaptations of the underlying phrase-based
sentence decoder; however, dealing properly with
non-monotonic decoding of word lattices introduces
some minor complexity that is worth mentioning. In
the sentence decoder, any translation of any span of
untranslated words is an allowable extension of a
partial translation hypothesis, provided that the cov-
erage vectors of the extension and the partial hypoth-
esis do not intersect. In a non-linear word lattice,
a further constraint must be enforced ensuring that
there is always a path from the starting node of the
translation extension?s source to the node represent-
ing the nearest right edge of the already-translated
material, as well as a path from the ending node of
the translation extension?s source to future translated
spans. Figure 3 illustrates the problem. If [0,1] is
translated, the decoder must not consider translating
1014
Axioms:
[X ? ??, i, i] : w
(X
w
?? ??, ??) ? G, i ? [0, |V | ? 2]
Inference rules:
[X ? ? ? Fj,k?, i, j] : w
[X ? ?Fj,k ? ?, i,Rj,k] : w ? pj,k
[X ? ? ? ?, i, j] : w
[X ? ? ? ?, i,Rj,k] : w ? pj,k
Fj,k = 
[Z ? ? ?X?, i, k] : w1 [X ? ??, k, j] : w2
[Z ? ?X ? ?, i, j] : w1 ? w2
Goal state:
[S ? ??, 0, |V | ? 1]
Figure 2: Word lattice parser for an unrestricted context free grammar G.
0 1x
2
ay
Figure 3: The span [0, 3] has one inconsistent covering,
[0, 1] + [2, 3].
[2,3] as a possible extension of this hypothesis since
there is no path from node 1 to node 2 and therefore
the span [1,2] would never be covered. In the parser
that forms the basis of the hierarchical decoder de-
scribed in Section 2.3, no such restriction is neces-
sary since grammar rules are processed in a strictly
left-to-right fashion without any skips.
3 Distortion in a non-linear word lattice
In both hierarchical and phrase-based models, the
distance between words in the source sentence is
used to limit where in the target sequence their trans-
lations will be generated. In phrase based transla-
tion, distortion is modeled explicitly. Models that
support non-monotonic decoding generally include
a distortion cost, such as |ai ? bi?1 ? 1| where ai is
the starting position of the foreign phrase f i and bi?1
is the ending position of phrase f i?1 (Koehn et al,
2003). The intuition behind this model is that since
most translation is monotonic, the cost of skipping
ahead or back in the source should be proportional
to the number of words that are skipped. Addition-
ally, a maximum distortion limit is used to restrict
0 1x
2a y3
b cd
Figure 4: Distance-based distortion problem. What is the
distance between node 4 to node 0?
the size of the search space.
In linear word lattices, such as confusion net-
works, the distance metric used for the distortion
penalty and for distortion limits is well defined;
however, in a non-linear word lattice, it poses the
problem illustrated in Figure 4. Assuming the left-
to-right decoding strategy described in the previous
section, if c is generated by the first target word, the
distortion penalty associated with ?skipping ahead?
should be either 3 or 2, depending on what path is
chosen to translate the span [0,3]. In large lattices,
where a single arc may span many nodes, the possi-
ble distances may vary quite substantially depending
on what path is ultimately taken, and handling this
properly therefore crucial.
Although hierarchical phrase-based models do
not model distortion explicitly, Chiang (2007) sug-
gests using a span length limit to restrict the win-
dow in which reordering can take place.1 The de-
coder enforces the constraint that a synchronous rule
learned from the training data (the only mechanism
by which reordering can be introduced) can span
1This is done to reduce the size of the search space and be-
cause hierarchical phrase-based translation models are inaccu-
rate models of long-distance distortion.
1015
Distance metric MT05 MT06
Difference 0.2943 0.2786
Difference+LexRO 0.2974 0.2890
ShortestP 0.2993 0.2865
ShortestP+LexRO 0.3072 0.2992
Table 2: Effect of distance metric on phrase-based model
performance.
maximally ? words in f . Like the distortion cost
used in phrase-based systems, ? is also poorly de-
fined for non-linear lattices.
Since we want a distance metric that will restrict
as few local reorderings as possible on any path,
we use a function ?(a, b) returning the length of the
shortest path between nodes a and b. Since this func-
tion is not dependent on the exact path chosen, it can
be computed in advance of decoding using an all-
pairs shortest path algorithm (Cormen et al, 1989).
3.1 Experimental results
We tested the effect of the distance metric on trans-
lation quality using Chinese word segmentation lat-
tices (Section 4.1, below) using both a hierarchical
and phrase-based system modified to translate word
lattices. We compared the shortest-path distance
metric with a baseline which uses the difference in
node number as the distortion distance. For an ad-
ditional datapoint, we added a lexicalized reorder-
ing model that models the probability of each phrase
pair appearing in three different orientations (swap,
monotone, other) in the training corpus (Koehn et
al., 2005).
Table 2 summarizes the results of the phrase-
based systems. On both test sets, the shortest path
metric improved the BLEU scores. As expected,
the lexicalized reordering model improved transla-
tion quality over the baseline; however, the improve-
ment was more substantial in the model that used the
shortest-path distance metric (which was already a
higher baseline). Table 3 summarizes the results of
our experiment comparing the performance of two
distance metrics to determine whether a rule has ex-
ceeded the decoder?s span limit. The pattern is the
same, showing a clear increase in BLEU for the
shortest path metric over the baseline.
Distance metric MT05 MT06
Difference 0.3063 0.2957
ShortestP 0.3176 0.3043
Table 3: Effect of distance metric on hierarchical model
performance.
4 Exploiting Source Language Alternatives
Chinese word segmentation. A necessary first
step in translating Chinese using standard models
is segmenting the character stream into a sequence
of words. Word-lattice translation offers two possi-
ble improvements over the conventional approach.
First, a lattice may represent multiple alternative
segmentations of a sentence; input represented in
this way will be more robust to errors made by the
segmenter.2 Second, different segmentation granu-
larities may be more or less optimal for translating
different spans. By encoding alternatives in the in-
put in a word lattice, the decision as to which granu-
larity to use for a given span can be resolved during
decoding rather than when constructing the system.
Figure 5 illustrates a lattice based on three different
segmentations.
Arabic morphological variation. Arabic orthog-
raphy is problematic for lexical and phrase-based
MT approaches since a large class of functional el-
ements (prepositions, pronouns, tense markers, con-
junctions, definiteness markers) are attached to their
host stems. Thus, while the training data may pro-
vide good evidence for the translation of a partic-
ular stem by itself, the same stem may not be at-
tested when attached to a particular conjunction.
The general solution taken is to take the best pos-
sible morphological analysis of the text (it is of-
ten ambiguous whether a piece of a word is part
of the stem or merely a neighboring functional el-
ement), and then make a subset of the bound func-
tional elements in the language into freestanding to-
kens. Figure 6 illustrates the unsegmented Arabic
surface form as well as the morphological segmen-
tation variant we made use of. The limitation of this
approach is that as the amount and variety of train-
ing data increases, the optimal segmentation strat-
egy changes: more aggressive segmentation results
2The segmentation process is ambiguous, even for native
speakers of Chinese.
1016
01
?
2
??
4
????
?
3
?
??
?
5
?
6
??
?
7
"
8
?
9
??
?
10
?
11
??
?
12
"
Figure 5: Sample Chinese segmentation lattice using three segmentations.
in fewer OOV tokens, but automatic evaluation met-
rics indicate lower translation quality, presumably
because the smaller units are being translated less
idiomatically (Habash and Sadat, 2006). Lattices al-
low the decoder to make decisions about what gran-
ularity of segmentation to use subsententially.
4.1 Chinese Word Segmentation Experiments
In our experiments we used two state-of-the-art Chi-
nese word segmenters: one developed at Harbin
Institute of Technology (Zhao et al, 2001), and
one developed at Stanford University (Tseng et al,
2005). In addition, we used a character-based seg-
mentation. In the remaining of this paper, we use cs
for character segmentation, hs for Harbin segmenta-
tion and ss for Stanford segmentation. We built two
types of lattices: one that combines the Harbin and
Stanford segmenters (hs+ss), and one which uses
all three segmentations (hs+ss+cs).
Data and Settings. The systems used in these
experiments were trained on the NIST MT06 Eval
corpus without the UN data (approximatively 950K
sentences). The corpus was analyzed with the three
segmentation schemes. For the systems using word
lattices, the training data contained the versions of
the corpus appropriate for the segmentation schemes
used in the input. That is, for the hs+ss condition,
the training data consisted of two copies of the cor-
pus: one segmented with the Harbin segmenter and
the other with the Stanford segmenter.3 A trigram
English language model with modified Kneser-Ney
smoothing (Kneser and Ney, 1995) was trained on
the English side of our training data as well as por-
tions of the Gigaword v2 English Corpus, and was
used for all experiments. The NIST MT03 test set
was used as a development set for optimizing the in-
terpolation weights using minimum error rate train-
3The corpora were word-aligned independently and then
concatenated for rule extraction.
ing (Och, 2003). The testing was done on the NIST
2005 and 2006 evaluation sets (MT05, MT06).
Experimental results: Word-lattices improve
translation quality. We used both a phrase-based
translation model, decoded using our modified ver-
sion of Moses (Koehn et al, 2007), and a hierarchi-
cal phrase-based translation model, using our modi-
fied version of Hiero (Chiang, 2005; Chiang, 2007).
These two translation model types illustrate the ap-
plicability of the theoretical contributions presented
in Section 2 and Section 3.
We observed that the coverage of named entities
(NEs) in our baseline systems was rather poor. Since
names in Chinese can be composed of relatively
long strings of characters that cannot be translated
individually, when generating the segmentation lat-
tices that included cs arcs, we avoided segmenting
NEs of type PERSON, as identified using a Chinese
NE tagger (Florian et al, 2004).
The results are summarized in Table 4. We see
that using word lattices improves BLEU scores both
in the phrase-based model and hierarchical model as
compared to the single-best segmentation approach.
All results using our word-lattice decoding for the
hierarchical models (hs+ss and hs+ss+cs) are sig-
nificantly better than the best segmentation (ss).4
For the phrase-based model, we obtain significant
gains using our word-lattice decoder using all three
segmentations on MT05. The other results, while
better than the best segmentation (hs) by at least
0.3 BLEU points, are not statistically significant.
Even if the results are not statistically significant
for MT06, there is a high decrease in OOV items
when using word-lattices. For example, for MT06
the number of OOVs in the hs translation is 484.
4Significance testing was carried out using the bootstrap re-
sampling technique advocated by Koehn (2004). Unless other-
wise noted, all reported improvements are signficant at at least
p < 0.05.
1017
surface wxlAl ftrp AlSyf kAn mEZm AlDjyj AlAElAmy m&ydA llEmAd .
segmented w- xlAl ftrp Al- Syf kAn mEZm Al- Djyj Al- AElAmy m&ydA l- Al- EmAd .
(English) During the summer period , most media buzz was supportive of the general .
Figure 6: Example of Arabic morphological segmentation.
The number of OOVs decreased by 19% for hs+ss
and by 75% for hs+ss+cs. As mentioned in Section
3, using lexical reordering for word-lattices further
improves the translation quality.
4.2 Arabic Morphology Experiments
We created lattices from an unsegmented version of
the Arabic test data and generated alternative arcs
where clitics as well as the definiteness marker and
the future tense marker were segmented into tokens.
We used the Buckwalter morphological analyzer and
disambiguated the analysis using a simple unigram
model trained on the Penn Arabic Treebank.
Data and Settings. For these experiments we
made use of the entire NIST MT08 training data,
although for training of the system, we used a sub-
sampling method proposed by Kishore Papineni that
aims to include training sentences containing n-
grams in the test data (personal communication).
For all systems, we used a 5-gram English LM
trained on 250M words of English training data.
The NIST MT03 test set was used as development
set for optimizing the interpolation weights using
MER training (Och, 2003). Evaluation was car-
ried out on the NIST 2005 and 2006 evaluation sets
(MT05, MT06).
Experimental results: Word-lattices improve
translation quality. Results are presented in Table
5. Using word-lattices to combine the surface forms
with morphologically segmented forms significantly
improves BLEU scores both in the phrase-based and
hierarchical models.
5 Prior work
Lattice Translation. The ?noisier channel? model
of machine translation has been widely used in spo-
ken language translation as an alternative to select-
ing the single-best hypothesis from an ASR system
and translating it (Ney, 1999; Casacuberta et al,
2004; Zhang et al, 2005; Saleem et al, 2005; Ma-
tusov et al, 2005; Bertoldi et al, 2007; Mathias,
2007). Several authors (e.g. Saleem et al (2005)
and Bertoldi et al (2007)) comment directly on
the impracticality of using n-best lists to translate
speech.
Although translation is fundamentally a non-
monotonic relationship between most language
pairs, reordering has tended to be a secondary con-
cern to the researchers who have worked on lattice
translation. Matusov et al (2005) decodes monoton-
ically and then uses a finite state reordering model
on the single-best translation, along the lines of
Bangalore and Riccardi (2000). Mathias (2007)
and Saleem et al (2004) only report results of
monotonic decoding for the systems they describe.
Bertoldi et al (2007) solve the problem by requiring
that their input be in the format of a confusion net-
work, which enables the standard distortion penalty
to be used. Finally, the system described by Zhang
et al (2005) uses IBM Model 4 features to translate
lattices. For the distortion model, they use the maxi-
mum probability value over all possible paths in the
lattice for each jump considered, which is similar
to the approach we have taken. Mathias and Byrne
(2006) build a phrase-based translation system as a
cascaded series of FSTs which can accept any input
FSA; however, the only reordering that is permitted
is the swapping of two adjacent phrases.
Applications of source lattices outside of the do-
main of spoken language translation have been far
more limited. Costa-jussa` and Fonollosa (2007) take
steps in this direction by using lattices to encode
multiple reorderings of the source language. Dyer
(2007) uses confusion networks to encode mor-
phological alternatives in Czech-English translation,
and Xu et al (2005) takes an approach very similar
to ours for Chinese-English translation and encodes
multiple word segmentations in a lattice, but which
is decoded with a conventionally trained translation
model and without a sophisticated reordering model.
The Arabic-English morphological segmentation
lattices are similar in spirit to backoff translation
models (Yang and Kirchhoff, 2006), which consider
alternative morphological segmentations and simpli-
1018
MT05 MT06
(Source Type) BLEU BLEU
cs 0.2833 0.2694
hs 0.2905 0.2835
ss 0.2894 0.2801
hs+ss 0.2938 0.2870
hs+ss+cs 0.2993 0.2865
hs+ss+cs.lexRo 0.3072 0.2992
MT05 MT06
(Source Type) BLEU BLEU
cs 0.2904 0.2821
hs 0.3008 0.2907
ss 0.3071 0.2964
hs+ss 0.3132 0.3006
hs+ss+cs 0.3176 0.3043
(a) Phrase-based model (b) Hierarchical model
Table 4: Chinese Word Segmentation Results
MT05 MT06
(Source Type) BLEU BLEU
surface 0.4682 0.3512
morph 0.5087 0.3841
morph+surface 0.5225 0.4008
MT05 MT06
(Source Type) BLEU BLEU
surface 0.5253 0.3991
morph 0.5377 0.4180
morph+surface 0.5453 0.4287
(a) Phrase-based model (b) Hierarchical model
Table 5: Arabic Morphology Results
fications of a surface token when the surface token
can not be translated.
Parsing and formal language theory. There has
been considerable work on parsing word lattices,
much of it for language modeling applications in
speech recognition (Ney, 1991; Cheppalier and Raj-
man, 1998). Additionally, Grune and Jacobs (2008)
refines an algorithm originally due to Bar-Hillel for
intersecting an arbitrary FSA (of which word lattices
are a subset) with a CFG. Klein and Manning (2001)
formalize parsing as a hypergraph search problem
and derive an O(n3) parser for lattices.
6 Conclusions
We have achieved substantial gains in translation
performance by decoding compact representations
of alternative source language analyses, rather than
single-best representations. Our results generalize
previous gains for lattice translation of spoken lan-
guage input, and we have further generalized the
approach by introducing an algorithm for lattice
decoding using a hierarchical phrase-based model.
Additionally, we have shown that although word
lattices complicate modeling of word reordering, a
simple heuristic offers good performance and en-
ables many standard distortion models to be used
directly with lattice input.
Acknowledgments
This research was supported by the GALE program
of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-0001. The authors wish
to thank Niyu Ge for the Chinese named-entity anal-
ysis, Pi-Chuan Chang for her assistance with the
Stanford Chinese segmenter, and Tie-Jun Zhao and
Congui Zhu for making the Harbin Chinese seg-
menter available to us.
References
S. Bangalore and G. Riccardi. 2000. Finite state models
for lexical reordering in spoken language translation.
In Proc. Int. Conf. on Spoken Language Processing,
pages 422?425, Beijing, China.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Comput. Linguist., 22(1):39?71.
N. Bertoldi and M. Federico. 2005. A new decoder for
spoken language translation based on confusion net-
works. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
translation by confusion network decoding. In Pro-
ceeding of ICASSP 2007, Honolulu, Hawaii, April.
P.F. Brown, J. Cocke, S. Della-Pietra, V.J. Della-Pietra,
F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16:79?85.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M. Vilar,
S. Barrachina, I. Garcia-Varea, D. Llorens, C. Mar-
1019
tinez, S. Molau, F. Nevado, M. Pastor, D. Pico, A. San-
chis, and C. Tillmann. 2004. Some approaches to
statistical and finite-state speech-to-speech translation.
Computer Speech & Language, 18(1):25?47, January.
J. Cheppalier and M. Rajman. 1998. A generalized CYK
algorithm for parsing stochastic CFG. In Proceedings
of the Workshop on Tabulation in Parsing and Deduc-
tion (TAPD98), pages 133?137, Paris, France.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL?99), pages 95?104.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 263?270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
T.H. Cormen, C. E. Leiserson, and R. L. Rivest, 1989.
Introduction to Algorithms, pages 558?565. The MIT
Press and McGraw-Hill Book Company.
M. Costa-jussa` and J.A.R. Fonollosa. 2007. Analy-
sis of statistical and morphological classes to gener-
ate weighted reordering hypotheses on a statistical ma-
chine translation system. In Proc. of the Second Work-
shop on SMT, pages 171?176, Prague.
C. Dyer. 2007. Noisier channel translation: translation
from morphologically complex languages. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, Prague, June.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proc. of HLT-NAACL 2004, pages 1?8.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25:573?605.
D. Grune and C.J. H. Jacobs. 2008. Parsing as intersec-
tion. Parsing Techniques, pages 425?442.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
D. Klein and C. D. Manning. 2001. Parsing with hyper-
graphs. In Proceedings of IWPT 2001.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of IEEE
Internation Conference on Acoustics, Speech, and Sig-
nal Processing, pages 181?184.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
2003, pages 48?54.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Edinburgh
system description for the 2005 IWSLT speech trans-
lation evaluation. In Proc. of IWSLT 2005, Pittsburgh.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Annual Meeting
of the Association for Computation Linguistics (ACL),
Demonstration Session, pages 177?180, Jun.
P. Koehn. 2004. Statistical significance tests for machine
translation evluation. In Proc. of the 2004 Conf. on
EMNLP, pages 388?395.
A. Lopez. to appear 2008. Statistical machine transla-
tion. ACM Computing Surveys.
L. Mathias and W. Byrne. 2006. Statistical phrase-
based speech translation. In IEEE Conf. on Acoustics,
Speech and Signal Processing.
L. Mathias. 2007. Statistical Machine Translation
and Automatic Speech Recognition under Uncertainty.
Ph.D. thesis, The Johns Hopkins University.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proceedings of Interspeech 2005.
H. Ney. 1991. Dynamic programming parsing for
context-free grammars in continuous speech recogni-
tion. IEEE Transactions on Signal Processing, 39(2).
H. Ney. 1999. Speech translation: Coupling of recogni-
tion and translation. In Proc. of ICASSP, pages 517?
520, Phoenix.
F. Och and H. Ney. 2002. Discriminitive training
and maximum entropy models for statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 295?302.
S. Saleem, S.-C. Jou, S. Vogel, and T. Schulz. 2005. Us-
ing word lattice information for a tighter coupling in
speech translation systems. In Proc. of ICSLP, Jeju
Island, Korea.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHANWorkshop on Chinese Lan-
guage Processing.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005, Pittsburgh.
M. Yang and K. Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected
languages. In Proceedings of the EACL 2006, pages
41?48.
R. Zhang, G. Kikui, H. Yamamoto, and W. Lo. 2005.
A decoding algorithm for word lattice translation in
speech translation. In Proceedings of the 2005 Inter-
national Workshop on Spoken Language Translation.
T. Zhao, L. Yajuan, Y. Muyun, and Y. Hao. 2001. In-
creasing accuracy of chinese segmentation with strat-
egy of multi-step processing. In J Chinese Information
Processing (Chinese Version), volume 1, pages 13?18.
1020
Coling 2010: Poster Volume, pages 885?893,
Beijing, August 2010
A Learnable Constraint-based Grammar Formalism
Smaranda Muresan
School of Communication and Information
Rutgers University
smuresan@rci.rutgers.edu
Abstract
Lexicalized Well-Founded Grammar
(LWFG) is a recently developed syntactic-
semantic grammar formalism for deep
language understanding, which balances
expressiveness with provable learnability
results. The learnability result for LWFGs
assumes that the semantic composition
constraints are learnable. In this paper,
we show what are the properties and
principles the semantic representation and
grammar formalism require, in order to
be able to learn these constraints from
examples, and give a learning algorithm.
We also introduce a LWFG parser as a
deductive system, used as an inference
engine during LWFG induction. An
example for learning a grammar for noun
compounds is given.
1 Introduction
Recently, several machine learning approaches
have been proposed for mapping sentences to their
formal meaning representations (Ge and Mooney,
2005; Zettlemoyer and Collins, 2005; Muresan,
2008; Wong and Mooney, 2007; Zettlemoyer and
Collins, 2009). However, only few of them in-
tegrate the semantic representation with a gram-
mar formalism: ?-expressions and Combinatory
Categorial Grammars (CCGs) (Steedman, 1996)
are used by Zettlemoyer and Collins (2005;2009),
and ontology-based representations and Lexical-
ized Well-Founded Grammars (LWFGs) (Mure-
san and Rambow, 2007) are used by Muresan
(2008).
An advantage of the LWFG formalism, com-
pared to most constraint-based grammar for-
malisms developed for deep language understand-
ing, is that it is accompanied by a learnability
guarantee, the search space for LWFG induc-
tion being a complete grammar lattice (Muresan
and Rambow, 2007). Like other constraint-based
grammar formalisms, the semantic structures in
LWFG are composed by constraint solving, se-
mantic composition being realized through con-
straints at the grammar rule level. Moreover, se-
mantic interpretation is also realized through con-
straints at the grammar rule level, providing ac-
cess to meaning during parsing.
However, the learnability result given by Mure-
san and Rambow (2007) assumed that the gram-
mar constraints were learnable. In this paper we
present the properties and principles of the seman-
tic representation and grammar formalism that al-
low us to learn the semantic composition con-
straints. These constraints are a simplified version
of ?path equations? (Shieber et al, 1983), and we
present an algorithm for learning these constraints
from examples (Section 5). We also present a
LWFG parser as a deductive system (Shieber et
al., 1995) (Section 3). The LWFG parser is used
as an innate inference engine during LWFG learn-
ing, and we present an algorithm for learning
LWFGs from examples (Section 4). A discussion
and an example of learning a grammar for noun
compounds are given is Section 6.
2 Lexicalized Well-Founded Grammars
Lexicalized Well-Founded Grammar (LWFG) is
a recently developed formalism that balances
expressiveness with provable learnability results
(Muresan and Rambow, 2007). LWFGs are a type
of Definite Clause Grammars (Pereira and War-
ren, 1980) in which (1) the context-free back-
bone is extended by introducing a partial ordering
relation among nonterminals, 2) grammar non-
terminals are augmented with strings and their
syntactic-semantic representations, called seman-
tic molecules, and (3) grammar rules can have
885
1. Syntagmas containing elementary semantic molecules
a. (w1, `h1b1
?)= (laser,0
B
B
B
B
B
@
2
6
4
cat noun
head X1
mod X2
3
7
5
D
X1.isa = laser, X2.P1=X1
E
1
C
C
C
C
C
A
) b. (w2, `h2b2
?)=(printer,0
B
B
B
B
B
@
2
6
4
cat noun
nr sg
head X3
3
7
5
D
X3.isa = printer
E
1
C
C
C
C
C
A
)
2. Syntagmas containing a derived semantic molecule
(w, `hb
?)=(laser printer,0
B
B
B
B
B
@
2
6
4
cat nc
nr sg
head X
3
7
5
D
X1.isa = laser, X .P1=X1, X .isa=printer
E
1
C
C
C
C
C
A
)
3. Constraint Grammar Rule
NC(w,
`h
b
?
) ? Noun(w1,
`h1
b1
?
), Noun(w2,
`h2
b2
?
) : ?c(h, h1, h2),?onto(b)
?c(h, h1, h2) = {h.cat = nc, h1.cat = noun, h2.cat = noun, h.head = h1.mod, h.head = h2.head, h.nr = h2.nr}
?onto(b) returns ?X1.isa = laser,X.instr = X1,X.isa = printer?
Figure 1: Syntagmas containing elementary semantic molecules (1) and a derived semantic molecule
(2); A constraint grammar rule together with the semantic composition and ontology-based interpreta-
tion constraints, ?c and ?onto (3)
two types of constraints, one for semantic com-
position and one for semantic interpretation. The
first property allows LWFG learning from a small
set of examples. The last two properties make
LWFGs a type of syntactic-semantic grammars.
Definition 1. A semantic molecule associated
with a natural language string w, is a syntactic-
semantic representation, w? = (hb
), where h
(head) encodes compositional information, while
b (body) is the actual semantic representation of
the string w.
Grammar nonterminals are augmented with
pairs of strings and their semantic molecules.
These pairs are called syntagmas, and are denoted
by ? = (w,w?) = (w, (hb
)
).
Examples of semantic molecules for the nouns
laser and printer and the noun-noun compound
laser printer are given in Figure 1. When as-
sociated with lexical items, semantic molecules
are called elementary semantic molecules. When
semantic molecules are built by the combina-
tion of others, they are called derived semantic
molecules. Formally, the semantic molecule head,
h, is a one-level feature structure (i.e., values are
atomic), while the semantic molecule body, b, is a
logical form built as a conjunction of atomic pred-
icates ?concept?.?attr? = ?concept?, where vari-
ables are either concept or slot identifiers in an on-
tology.1
1The body of a semantic molecule is called OntoSeR and
Muresan and Rambow (2007) formally defined
LWFGs, and we present here a slight modification
of their definition.
Definition 2. A Lexicalized Well-Founded Gram-
mar (LWFG) is a 7-tuple, G = ??,??, NG,
, PG, P?, S?, where:
1. ? is a finite set of terminal symbols.
2. ?? is a finite set of elementary seman-
tic molecules corresponding to the terminal
symbols.
3. NG is a finite set of nonterminal symbols.
NG?? = ?. We denote pre(NG) ? NG, the
set of pre-terminals (a.k.a, parts of speech)
4.  is a partial ordering relation among non-
terminals.
5. PG is the set of constraint grammar rules. A
constraint grammar rule is written A(?) ?
B1(?1), . . . , Bn(?n) : ?(??), where A,Bi ?
NG, ?? = (?, ?1, ..., ?n) such that ? =
(w,w?), ?i = (wi, wi?), 1 ? i ? n,w =
w1 ? ? ?wn, w? = w?1 ? ? ? ? ? w?n, and ? is the
composition operator for semantic molecules
(more details about the composition oper-
ator are given in Section 5). For brevity,
we denote a rule by A ? ? : ?, where
A ? NG, ? ? N+G . P? is the set of con-
straint grammar rules whose left-hand side
are pre-terminals, A(?) ?, A ? pre(NG).
is a flat ontology-based semantic representation.
886
We use the notation A ? ? for this gram-
mar rules. In LWFG due to partial ordering
among nonterminals we can have ordered
constraint grammar rules and non-ordered
constraint grammar rules (both types can be
recursive or non-recursive). A grammar rule
A(?) ? B1(?1), . . . , Bn(?n) : ?(??), is an
ordered rule, if for all Bi, we have A  Bi.
In LWFGs, each nonterminal symbol is a
left-hand side in at least one ordered non-
recursive rule and the empty string cannot be
derived from any nonterminal symbol.
6. S ? NG is the start nonterminal symbol, and
?A ? NG, S  A (we use the same notation
for the reflexive, transitive closure of ).
The partial ordering relationmakes the set of
nonterminals well-founded2 , which allows the or-
dering of the grammar rules, as well as the order-
ing of the syntagmas generated by LWFGs. This
ordering allow LWFG learning from a small set of
representative examples (Muresan and Rambow,
2007) (P? is not learned).
An example of a LWFG rule is given in Fig-
ure 1(3). Nonterminals are augmented with syn-
tagmas. Moreover, in LWFG the semantic com-
position and interpretation are realized via con-
straints at the grammar rule level (?(??) in Defi-
nition 2). More precisely, syntagma composition
means string concatenation (w = w1w2) and se-
mantic molecule composition ((hb
)
=
(h1
b1
)
?
(h2
b2
))
?- where the bodies of semantic molecules are
concatenated through logical conjunction (b =
(b1, b2)?, where ? is a variable substitution ? =
{X2/X,X3/X}), while the semantic molecules
heads are composed through compositional con-
straints ?c(h, h1, h2), which are a simplified ver-
sion of ?path equations? (Shieber et al, 1983) (see
Figure 1(3)). During LWFG learning, composi-
tional constraints ?c are learned together with the
grammar rules. Semantic interpretation, which
is ontology-based in LWFG, is also encoded as
constraints at the grammar rule level ? ?onto
? providing access to meaning during parsing.
?onto(b) constraints are applied to the body of
the semantic molecule corresponding to the syn-
2 should not be confused with information ordering de-
rived from flat feature structures
tagma associated with the left-hand side nonter-
minal. The ontology-based constraints are not
learned; rather, ?onto is a general predicate that
succeed or fail as a result of querying an ontology
? when it succeeds, it instantiates the variables
of the semantic representation with concepts/slots
in the ontology (see the example in Figure 1(3)).
2.1 Derivation in LWFG
The derivation in LWFG is called ground syn-
tagma derivation, and it can be seen as the
bottom up counterpart of the usual derivation.
Given a LWFG, G, the ground syntagma deriva-
tion relation, ?G?, is defined as: A??
A?G??
(if ? =
(w,w?), w ? ?, w? ? ??, i.e., A ? pre(NG, ),
and Bi?G??i, i=1,...,n, A(?)?B1(?1),...,Bn(?n) : ?(??)
A?G??
.
The set of all syntagmas generated by a gram-
mar G is L?(G) = {?|? = (w,w?), w ?
?+, ?A ? NG, A ?G? ?}. Given a LWFG G,
E? ? L?(G) is called a sublanguage of G. Ex-
tending the notation, given a LWFG G, the set of
syntagmas generated by a rule (A? ? : ?) ? PG
is L?(A ? ? : ?) = {?|? = (w,w?), w ?
?+, (A ? ? : ?) ?G? ?}, where (A ? ? : ?) ?G?
? denotes the ground derivation A ?G? ? obtained
using the rule A ? ? : ? in the last derivation
step.
3 LWFG Parsing as Deduction
Following Shieber (1995), we present the Lexical-
ized Well-Founded Grammar parser as a deduc-
tive proof system in Table 1. The items of the
logic are of the form [i, j, ?ij , A ? ? ? ??A],
where A ? ?? : ?A is a grammar rule, ?A ?
the constraints corresponding to the grammar rule
whose left-hand side nonterminal is A? can be
true, ? shows how much of the right-hand side
of the rule has been recognized so far, i points to
the parent node where the rule was invoked, and j
points to the position in the input that the recogni-
tion has reached. We use the following notations:
?Rij = (wRij ,
(hRij
bRij
)
) are syntagmas corresponding
to the partially parsed right-hand side of a rule;
?Lij = (wLij ,
(hLij
bLij
)
) are ground-derived syntagmas
(i.e., they are augmenting the left-hand side non-
887
Item form [i, j, ?ij , A? ? ? ??A] 1 ? i, j ? n+ 1, A ? NG, ?? ? N?G
the ?A constraint can be true
Axioms [i, i+ 1, ?Lii+1, Bi ? ?] 1 ? i ? n,Bi ? pre(NG), Bi ? ?Lii+1 ? P?
Goals [i, j, ?Lij , A? ??A?] 1 ? i, j ? n+ 1, A ? NG, ? ? N+G
Inference Rules
Prediction [i,j,?Lij ,B???B?][i,i,?Rii,A??B??A] ?A? B? : ?
A? (A? B? : ?A) ? PG
?Rii = ?? (i.e., wRii = , bRii = true and hRii = ?)
Completion [i,j,?Rij ,A?? ? B ? ?A] [j,k,?Ljk,B?? ?B ?][i,k,?Rik,A?? B ? ? ?A] ?
R
ik = ?Rij ? ?Ljk, where
wRik = wRijwLjk, bRik = bRijbLjk, hRik = hRij ? hLjk
Constraint [i,j,?Rij ,A????A][i,j,?Lij ,A???A?] ??
A is satisfiable ? ?Lij = ?(?Rij)
Table 1: LWFG parsing as deductive system
terminal of a LWFG rule). The goal items are
of the form [i, j, ?Lij , A ? ??A?], where ?Lij is
ground-derived from the rule A? ? : ?A.
Compared to the deductive system in (Shieber
et al, 1995), the LWFG parser has the follow-
ing characteristics: each item is augmented with
a syntagma; the Constraint rule is a new infer-
ence rule, and the goal items are associated to
every nonterminal in the grammar, not only to
the start symbol (i.e., LWFG parser is a robust
parser). The Constraint inference rule is the only
one that obtains an inactive edge3, from an active
edge by executing the grammar constraint ?A (the
? is shifted across the constraint). By applying the
Constraint rule as the last inference rule we obtain
the ground-derived syntagmas ?Lij . Thus, the goal
items are obtained only after the Constraint rule is
applied. During this inference rule we have that
?Lij = ?(?Rij), where ? is defined by: wLij = wRij ,
bLij = bRij?ij , and hLij = ?(hRij). The substitution
?ij and the function ? are implicitly contained in
the grammar constraint ?Ac (hLij , hRij) (see Section
5 for details)
Definition 3 (Robust parsing provability). Robust
parsing provability corresponds to reaching the
goal item: `rp A(?Lij) iff [i, j, ?Lij , A? ??A?].
Thus, we can notice that the ground syntagma
derivation is equivalent to robust parsing provabil-
ity, i.e., A ?G? ? iff G `rp A(?).
3We use Kay?s terminology: items are edges, where the
axioms and goals are inactive edges having ? at the end,
while the rest are active edges (Kay, 1986).
4 Learning LWFGs
The theoretical learning model for LWFG induc-
tion, Grammar Approximation by Representative
Sublanguage (GARS), together with a learnability
theorem was introduced in (Muresan and Ram-
bow, 2007). LWFG?s learning framework char-
acterizes the ?importance? of substructures in the
model not simply by frequency, but rather lin-
guistically, by defining a notion of ?representa-
tive examples? that drives the acquisition process.
Informally, representative examples are ?building
blocks? from which larger structures can be in-
ferred via reference to a larger generalization cor-
pus referred to as representative sublanguage in
(Muresan and Rambow, 2007). The GARS model
uses a polynomial algorithm for LWFG learning
that take advantage of the building blocks nature
of representative examples.
The LWFG induction algorithm belongs to the
class of Inductive Logic Programming methods
(ILP), based on entailment (Muggleton, 1995;
Dzeroski, 2007). At each step a new constraint
grammar rule is learned from the current repre-
sentative example, ?. Then this rule is added to
the grammar rule set. The process continues until
all the representative examples are covered. We
describe below the process of learning a grammar
rule from the current representative example:
1. Most Specific Grammar Rule Generation.
In the first step, the most specific grammar
rule is generated from the current represen-
tative example ?. The category annotated
888
STEP 1 (Most Specific Grammar Rule Generation)
STEP 2 (Grammar Rule Generalization)
(laser printer, 
CANDIDATE GRAMMAR RULES
laser printer
Performance CriteriaBEST RULE 
((laser printer) manual)(desktop (laser printer))
K ? Background KnowledgeLexicon 
(laser, )
Previously learned grammar rules
cat   nc
                                 )
(printer, )
? = (w, (hb)) - Current representative example
a) chunks={[NA(laser), Noun(laser)], [NC(printer),Noun(printer)]}
rg1 NC ? Noun Noun:?c4 (score=1)rg2 NC ? NA Noun:?c5 (score=2)
b) r: NC(w, (hb)) ? Noun(w1, (h1b1)) Noun(w2, (h2b2)):?c4(h, h1, h2)?c4(h, h1, h2) = {h.cat = nc, h1.cat = noun, h2.cat = noun,
E? - Representative Sublanguage
NC ? NA NC:?c7
rg4 NC ? NA NC:?c7 (score=3)rg3 NC ? Noun NC:?c6 (score=2)
Noun ?
cat nounhead X1mod X2?X1.isa = laser,X2.Y = X1?cat noun
?X3.isa = printer?
NA ? Noun:?c1
?B.isa = laser, A.P1 = B,A.isa = printer?
NA ? NA NA:?c2NC ? Noun:?c3
nr sghead A
head X3nr sgNoun ?
h.head = h1.mod, h.head = h2.head, h.nr = h2.nr}
Figure 2: Example of Grammar Rule Learning
in the representative example gives the
left-hand-side nonterminal, while a robust
parser returns the minimum number of
chunks covering the representative example.
The categories of the chunks give the non-
terminals of the right-hand side of the most
specific rule. For example, in Figure 2, given
the representative example laser printer
annotated with its semantic molecule, and
the background knowledge containing the
already learned rules NA ? Noun : ?c1 ,
NA ? NA NA : ?c2 , NC ? Noun : ?c3
the robust parser generates the chunks
corresponding to the noun laser and the
noun printer: [NA(laser),Noun(laser)]
and [NC(printer),Noun(printer)], re-
spectively. The most specific rule is
NC ? Noun Noun : ?c4 , where the
left-hand side nonterminal is given by the
category of the representative example, in
this case nc. Compositional constraints ?c4
are learned as well. In section 5 we give
the algorithm for learning these constraints,
and several properties and principles that are
needed in order for these constraints to be
learnable.
2. Grammar Rule Generalization. In the sec-
ond step, this most specific rule is gener-
alized, obtaining a set of candidate gram-
mar rules (the generalization step is the in-
verse of the derivation step used to define
the complete grammar lattice search space in
(Muresan and Rambow, 2007)). The perfor-
mance criterion in choosing the best gram-
mar rule among these candidate hypotheses
is the number of examples in the representa-
tive sublanguage E? (generalization corpus)
that can be parsed using the candidate gram-
mar rule, rgi in the last ground derivation
step, together with the previous learned rules,
i.e., |E??L?(rgi)|. In Figure 2 given the rep-
resentative sublanguage E?={ laser printer,
laser printer manual, desktop laser printer}
the learner will generalize to the recursive
rule NC ? NA NC : ?7, since only this
rule can parse all the examples in E?.
5 Learnable Composition Constraints
In LWFG, the semantic structures are composed
by constraint solving, rather than functional ap-
plication (with lambda expressions and lambda re-
duction). This section presents the properties and
principles that guarantee the learnability of the
compositional constraints,?c, and presents an al-
gorithm to generate these constraints from exam-
ples, which is a key result for LWFG learnability.
The information for semantic composition is
encoded in the head of semantic molecules. There
are three types of attributes that belong to the se-
mantic molecule head h: category attributes Ach,
variable attributes Avh, and feature attributes Afh.
Thus, Ah = Ach ? Avh ? Afh and Ach,Avh,Afh arepairwise disjoint. For example, in Figure 1 for the
noun-noun compound laser printer, we have that
Ach = {cat}, Afh = {nr}, and Avh = {head},while for the noun laser we have that Ach1 =
{cat}, Afh1 = ?, andAvh1 = {head,mod} (nounscan be modifiers of other nouns, so their represen-
tation is similar to that of an adjective).
We describe in turn each of these types of at-
tributes and their corresponding principles. All
principles, except the first and the last mirror
principles in other constraint-based linguistic for-
malisms, such as HPSG (Pollard and Sag, 1994).
The category attributes Ach are state at-
tributes, and their value set gives the category of
the semantic molecule. There is one attribute, cat
? Ach, which is mandatory and whose value is the
name of the category (e.g., h.cat = nc in Figure
889
1). The category of a semantic molecule can be
given by: 1) the cat attribute alone, or 2) the cat
attribute together with other state attributes in Ach
which are syntactic-semantic markers.
Principle 1 (Category Name Principle). The cat-
egory name h.cat of a syntagma ? = (w, (hb
)
) is
the same as the grammar nonterminal augmented
with syntagma ?.
When learning a LWFG rule from an example
?, the above principle allows us to determine the
nonterminal in the left-hand side of the grammar
rule. For example, when learning the LWFG rule
from the syntagma corresponding to laser printer
in Figure 2, the nonterminal in the left-hand side
of the LWFG rule is NC since h.cat = nc.
The variable attributes Avh are attributes
whose values are logical variables and represent
the semantic valence of the molecule, which al-
lows the binding of the semantic representations.
These logical variables appear in the semantic
molecule body as well. For example, in Figure
1(2) for the noun-noun compound laser printer,
the value of the variable attribute head ? Avh is
a variable X , which appears also in the body of
the semantic molecule ?X1.isa = laser,X.P1 =
X1, X.isa = printer?. It can be noticed that the
semantic molecule body contains other variables
as well (X1, P1). However, only the variables
present in the semantic molecule head as well (X)
will participate in further composition.
Principle 2 (Semantic Representation Binding
Principle). All the logical variables that the body
b of a semantic molecule corresponding to a syn-
tagma ? = (w, (hb
)
), share with other syntagmas,
are at the same time values of the variable at-
tributes (Avh) of the semantic molecule head.
There is one variable attribute, head ? Avh that
represents the head of a syntagma, giving the fol-
lowing principle:
Principle 3 (Semantic Head Principle). Given a
syntagma ? = (w, (hb
)
) ground derived from a
grammar rule, r, there exists one and only one
syntagma ?i = (wi,
(hi
bi
)
) corresponding to a non-
terminal Bi in rule r?s right-hand side, which
has the same value of the attribute head, i.e.,
h.head = hi.head.
The feature attributes Afh are the attributeswhose values express the specific properties of the
semantic molecules (e.g., number, person).
Principle 4 (Feature Inheritance Principle). If
?i = (wi,
(hi
bi
)
) is the semantic head of a ground-
derived syntagma ? = (w, (hb
)
), then all fea-
ture attributes of ? inherit the values of the cor-
responding attributes that belong to the seman-
tic head ?i. That is, if h.head = hi.head , then
h.f = hi.f , ?f ? Afh ?Afhi .
Besides this principle, the feature attributes are
used for category agreement. The categories that
enter in agreement are maximum projection cat-
egories. This linguistic knowledge about agree-
ment is used in the form of the following princi-
ple:
Principle 5 (Feature Agreement Principle). The
agreeing categories and the agreement features
are a-priori given based on linguistic knowledge,
and are applied only at the semantic head level.
Given all the above principles, we can now for-
mulate the general Composition Principle:
Principle 6 (Composition Principle). A syntagma
? = (w,w?) corresponding to the left-hand side
nonterminal of a grammar rule is obtained by
string concatenation (w = w1 . . . wn) and the
composition of semantic molecules corresponding
to the nonterminals from the rule right-hand side:
w? =
(h
b
)
= (w1 ? ? ?wn)? = w?1 ? ? ? ? ? w?n
=
(h1
b1
)
? ? ? ? ?
(hn
bn
)
=
(h1 ? ? ? ? ? hn
?b1, . . . , bn??
)
The composition of the semantic molecule bod-
ies is realized through conjunction after the ap-
plication of a variable substitution ?. The body
variable specialization substitution ? is the most
general unifier (mgu) of b and b1, . . . , bn, s.t
b = (b1, . . . , bn)?. It is a particular form of the
commonly used substitution (Lloyd, 2003), i.e.,
a finite set of the form {X1/Y1, . . . , Xm/Ym},
whereX1, . . . , Xm, Y1, . . . , Ym are variables, and
X1, . . . , Xm are distinct.
The composition of the semantic molecule
heads is realized by a set of constraints
?c(h, h1..., hn), which is a system of equations
890
similar to ?path equations? (Shieber et al, 1983;
van Noord, 1993), but applied to flat feature struc-
tures:
?
?
?
hi.c = ct
hi.vi = hj .vj
hi.f = ct or
hi.f = hj .f
?
?
? where
0 ? i, j ? n, i 6= j
c ? Achi
vi ? Avhi , vj ? A
v
hj
f ? Afhi , f ? A
f
hj
When learning a LWFG rule from a repre-
sentative example ? as in Figure 2, the robust
parser returns the minimum number of chunks,
n, covering ?. The body variable substitution ?
is fully determined by the representative exam-
ple as mgu of b and b1, . . . , bn, and the compo-
sitional constraints ?c(h, h1, . . . , hn) are learned
using Alg 1. For example, in Figure 2, when
learning from the representative example corre-
sponding to the string laser printer, we have that
? = {X1/B,X2/A,X3/A, Y/P1}.
In Alg 1 we use the notation ?0 = (w0,
(h0
b0
)
) to
denote the representative example ?.
Alg 1: Learn Constraints(?0, ?1, . . . , ?n)
?i = (wi,
`hi
bi
?
), 0 ? i ? n
?c ? ?
? ? mgu(b0, (b1, . . . , bn))
foreach 0 ? i ? n ? c ? Achi do1 if hi.c = c1 then
?c ? ?c ? {hi.c = c1}
foreach 0 ? i, j ? n ? i 6= j ?X/Y ? ??2
vi ? Avhi ? vj ? Avhj doif hi.vi = X ? hj .vj = Y then
?c ? ?c ? {hi.vi = hj .vj}
if hs.head = h0.head, 1 ? s ? n then3
foreach f ? Afh0 ? Afhs doif h0.f = c1 ? hs.f = c1 then
?c ? ?c ? {h0.f = hs.f}
if hs.cat = cs ? hi.cat = ci ? agr(cs, ci),
1 ? i ? n then
foreach f ? agrFeatures(cs, ci) do
if hs.f = c1 ? hi.f = c1 then
?c ? ?c ? {hs.f = hi.f}
for all other f ? Afhi , 0 ? i ? n do4 /*i.e., if we are not in case 3 */
if hi.f = c1 then
?c ? ?c ? {hi.f = c1}
return ?c /*i.e., ?c(h0, h1, . . . , hn) */
In the first step, the constraints corresponding
to category attributes are fully determined by the
values of these attributes that appear in the se-
mantic molecule heads of ?0, . . . ?n. In Figure
2, when learning the most specific rule r from
the representative example laser printer, the set
of constraints {h.cat = nc, h1.cat = noun, h2 =
noun} ? ?c4 are the constraints corresponding
to category attributes. In the second step, the con-
straints corresponding to variable attributes are
fully determined by the variables in the substitu-
tion ? that also appear as values of variable at-
tributes hi.vi, hj .vj , where 0 ? i, j ? n and
i 6= j. In Figure 2, only {X2/A,X3/A} ? ?
will be used, generating the set of constraints
{h.head = h1.mod, h.head = h2.head} ? ?c4 .
In the third step, the values of the feature at-
tributes which obey Principles 4 and 5 are gen-
eralized ? agr(cs, ci) is the predicate which gives
us the agreement between the categories cs and
ci (e.g., the subject agrees with the verb), and
agrFeatures(cs, ci) gives us the set of feature at-
tributes that participate in agreement (e.g., nr,
pers, case). In Figure 2, the set of constraints
{h.nr = h2.nr} ? ?c4 represents the general-
ization of the feature attribute values for nr, using
Principle 4 . For all features attributes besides the
ones that obey the above two principles, the gener-
ated constraints keep the particular values of these
attributes (step 4 of Alg 1).
6 Examples
The LWFG formalism allows us to learn gram-
mars for deep language understanding from ex-
amples. Instead of writing syntactic-semantic
grammar by hand (both rules and constraints),
we need to provide only a small set of repre-
sentative examples ? strings and their semantic
molecules. Qualitative experiments on learning
LWFGs showed that complex linguistic construc-
tions can be learned and covered, such as com-
plex noun phrases, relative clauses and reduced
relative clauses, finite and non-finite verbal con-
structions (including, tense, aspect, negation, and
subject-verb agreement), and raising and control
constructions (Muresan and Rambow, 2007). In
Figure 3 we show an example of learning a LWFG
grammar for noun-noun compounds. The first
four examples (1-4) are representative examples,
while the last four examples are used for gener-
891
A. Learning Examples:
1. (laser,0
B
B
B
B
B
@
2
6
4
cat na
head A
mod B
3
7
5
D
A.isa = laser, B.P1=A
E
1
C
C
C
C
C
A
) 5. (laser printer manual,0
B
B
B
B
B
@
2
6
4
cat na
head A
mod B
3
7
5
D
C.isa = laser, D.P1=C, D.isa=printer,A.P2=D, A.isa=manual, B.P3=A
E
1
C
C
C
C
C
A
)
2. (laser printer,0
B
B
B
B
B
@
2
6
4
cat na
head A
mod B
3
7
5
D
C.isa = laser, A.P1=C, A.isa=printer, B.P2=A
E
1
C
C
C
C
C
A
) 6. (desktop laser printer,0
B
B
B
B
B
@
2
6
4
cat na
head A
mod B
3
7
5
D
C.isa = desktop, A.P1=C, D.isa=laser,A.P2=D, A.isa=printer, B.P3=A
E
1
C
C
C
C
C
A
)
3. (printer,0
B
B
B
B
B
@
2
6
4
cat nc
nr sg
head A
3
7
5
D
A.isa = printer
E
1
C
C
C
C
C
A
) 7. (laser printer manual,0
B
B
B
B
B
@
2
6
4
cat nc
nr sg
head A
3
7
5
D
B.isa = laser, C.P1=B, C.isa=printer, A.P2=C, A.isa=manual
E
1
C
C
C
C
C
A
)
4. (laser printer,0
B
B
B
B
B
@
2
6
4
cat nc
nr sg
head A
3
7
5
D
B.isa = laser, A.P1=B, A.isa=printer
E
1
C
C
C
C
C
A
) 8. (desktop laser printer,0
B
B
B
B
B
@
2
6
4
cat nc
nr sg
head A
3
7
5
D
B.isa = desktop, A.P1=B, C.isa=laser, A.P2=C, A.isa=printer
E
1
C
C
C
C
C
A
)
B. Learned LWFG Rules:
NA(w,
?h
b
?
)? Noun(w1 ,
?h1
b1
?
) : ?c1 (h, h1) , where ?c1 (h, h1) =
8
>
<
>
:
h.cat = na
h1.cat = noun
h.head = h1.head
h.mod = h1.mod
9
>
=
>
;
NA(w,
?h
b
?
)? NA(w1,
?h1
b1
?
), NA(w2,
?h2
b2
?
) : ?c2 (h, h1, h2) where ?c2 (h, h1, h2) =
8
>
>
><
>
>
>:
h.cat = na
h1.cat = na
h2.cat = na
h.head = h1.mod
h.head = h2.head
h.mod = h2.mod
9
>
>
>=
>
>
>;
NC(w,
?h
b
?
)? Noun(w1,
?h1
b1
?
) : ?c3 (h, h1) , where ?c3 (h, h1) =
8
>
<
>
:
h.cat = nc
h1.cat = noun
h.head = h1.head
h.nr = h1.nr
9
>
=
>
;
NC(w,
?h
b
?
)? NA(w1,
?h1
b1
?
), NC(w2,
?h2
b2
?
) : ?c4 (h, h1, h2) where ?c4 (h, h1, h2) =
8
>
>
><
>
>
>:
h.cat = nc
h1.cat = na
h2.cat = nc
h.head = h1.mod
h.head = h2.head
h.nr = h2.nr
9
>
>
>=
>
>
>;
Figure 3: Learning LWFG Rules for Noun-Noun Compounds
alization (5-8). The learned grammar rules, in-
cluding the learned composition constraints are
also shown. The first two LWFG rules ground de-
rive syntagmas for noun adjuncts, while the last
two rules ground derive syntagmas for noun com-
pounds. For example, ?desktop laser printer? can
be either a fully-formed noun compound (cate-
gory nc), or it can be further combined with the
noun ?invoice? to obtain ?desktop laser printer in-
voice?, case in which it is a noun adjunct (cate-
gory na). The learned rule for noun adjuncts is
both left and right recursive, accounting for both
left and right-branching noun compounds. Even
though we can obtain overgeneralization in syn-
tax, the ontology-based interpretation constraint
at the rule level will prune some erroneous parses.
Preliminary results in the medical domain show
that ?onto can help remove erroneous parses even
when using just a weak ontological model (se-
mantic roles of verbs, prepositions, attributes of
adjectives and adverbs, but no synonymy, or hi-
erarchy of concepts or roles). However, more ex-
periments need to be run for reporting quantitative
results.
7 Conclusions
We have presented the properties and princi-
ples that the semantic representation integrated
in LWFG requires so that the semantic compo-
sitional constraints are learnable from examples.
These properties together with Alg 1 give a the-
oretical result that in conjunction with the learn-
ability result of Muresan and Rambow (2007)
show that LWFG is a learnable constraint-based
grammar formalism that can be used for deep lan-
guage understanding. Instead of writing grammar
rules and constraints by hand, one needs to pro-
vide only a small set of annotated examples.4
4The author acknowledges the support of the NSF (SGER
grant IIS-0838801). Any opinions, findings, or conclusions
are those of the author, and do not necessarily reflect the
views of the funding organization.
892
References
Dzeroski, Saso. 2007. Inductive logic programming in
a nutshell. In Getoor, Lise and Ben Taskar, editors,
Introduction to Statistical Relational Learning. The
MIT Press.
Ge, Ruifang and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of CoNLL-2005.
Kay, M. 1986. Algorithm schemata and data struc-
tures in syntactic processing. In Readings in natural
language processing, pages 35?70. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
Lloyd, John W. 2003. Logic for Learning: Learn-
ing Comprehensible Theories from Structured Data.
Springer, Cognitive Technologies Series.
Muggleton, Stephen. 1995. Inverse Entailment and
Progol. New Generation Computing, Special Issue
on Inductive Logic Programming, 13(3-4):245?286.
Muresan, Smaranda and Owen Rambow. 2007. Gram-
mar approximation by representative sublanguage:
A new model for language learning. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL).
Muresan, Smaranda. 2008. Learning to map text
to graph-based meaning representations via gram-
mar induction. In Coling 2008: Proceedings of
the 3rd Textgraphs workshop on Graph-based Al-
gorithms for Natural Language Processing, pages
9?16, Manchester, UK, August. Coling 2008 Orga-
nizing Committee.
Neumann, Gu?nter and Gertjan van Noord. 1994. Re-
versibility and self-monitoring in natural language
generation. In Strzalkowski, Tomek, editor, Re-
versible Grammar in Natural Language Processing,
pages 59?96. Kluwer Academic Publishers, Boston.
Pollard, Carl and Ivan Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago, Illinois.
Shieber, Stuart, Hans Uszkoreit, Fernando Pereira,
Jane Robinson, and Mabry Tyson. 1983. The
formalism and implementation of PATR-II. In
Grosz, Barbara J. and Mark Stickel, editors, Re-
search on Interactive Acquisition and Use of Knowl-
edge, pages 39?79. SRI International, Menlo Park,
CA, November.
Shieber, Stuart, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1-2):3?
36.
Steedman, Mark. 1996. Surface Structure and Inter-
pretation. The MIT Press.
van Noord, Gertjan. 1993. Reversibility in Natural
Language Processing. Ph.D. thesis, University of
Utrecht.
Wong, Yuk Wah and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007).
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI-05.
Zettlemoyer, Luke and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Association for
Computational Linguistics (ACL?09).
893
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 581?586,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying Sarcasm in Twitter: A Closer Look 
 
 
Roberto Gonz?lez-Ib??ez Smaranda Muresan Nina Wacholder 
 
School of Communication & Information 
Rutgers, The State University of New Jersey 
4 Huntington St, New Brunswick, NJ 08901 
{rgonzal, smuresan, ninwac}@rutgers.edu 
 
 
 
 
 
 
 
Abstract 
Sarcasm transforms the polarity of an ap-
parently positive or negative utterance into 
its opposite. We report on a method for 
constructing a corpus of sarcastic Twitter 
messages in which determination of the 
sarcasm of each message has been made by 
its author. We use this reliable corpus to 
compare sarcastic utterances in Twitter to 
utterances that express positive or negative 
attitudes without sarcasm. We investigate 
the impact of lexical and pragmatic factors 
on machine learning effectiveness for iden-
tifying sarcastic utterances and we compare 
the performance of machine learning tech-
niques and human judges on this task. Per-
haps unsurprisingly, neither the human 
judges nor the machine learning techniques 
perform very well. 
1 Introduction 
Automatic detection of sarcasm is still in its infan-
cy. One reason for the lack of computational mod-
els has been the absence of accurately-labeled 
naturally occurring utterances that can be used to 
train machine learning systems. Microblogging 
platforms such as Twitter, which allow users to 
communicate feelings, opinions and ideas in short 
messages and to assign labels to their own messag-
es, have been recently exploited in sentiment and 
opinion analysis (Pak and Paroubek, 2010; Davi-
dov et al, 2010). In Twitter, messages can be an-
notated with hashtags such as #bicycling, #happy 
and #sarcasm. We use these hashtags to build a 
labeled corpus of naturally occurring sarcastic, 
positive and negative tweets.  
    In this paper, we report on an empirical study on 
the use of lexical and pragmatic factors to distin-
guish sarcasm from positive and negative senti-
ments expressed in Twitter messages. The 
contributions of this paper include i) creation of a 
corpus that includes only sarcastic utterances that 
have been explicitly identified as such by the com-
poser of the message; ii) a report on the difficulty 
of distinguishing sarcastic tweets from tweets that 
are straight-forwardly positive or negative. Our 
results suggest that lexical features alone are not 
sufficient for identifying sarcasm and that pragmat-
ic and contextual features merit further study. 
2 Related Work 
Sarcasm and irony are well-studied phenomena in  
linguistics, psychology and cognitive science 
(Gibbs, 1986; Gibbs and Colston 2007; Kreuz and 
Glucksberg, 1989; Utsumi, 2002). But in the text 
mining literature, automatic detection of sarcasm is 
considered a difficult problem (Nigam & Hurst, 
2006 and Pang & Lee, 2008 for an overview) and 
has been addressed in only a few studies. In the 
context of spoken dialogues, automatic detection 
of sarcasm has relied primarily on speech-related 
cues such as laughter and prosody (Tepperman et 
al., 2006). The work most closely related to ours is 
that of Davidov et al (2010), whose objective was 
to identify sarcastic and non-sarcastic utterances in 
Twitter and in Amazon product reviews. In this 
paper, we consider the somewhat harder problem 
581
of distinguishing sarcastic tweets from non-
sarcastic tweets that directly convey positive and 
negative attitudes (we do not consider neutral ut-
terances at all).  
 Our approach of looking at lexical features for 
identification of sarcasm was inspired by the work 
of Kreuz and Caucci (2007). In addition, we also 
look at pragmatic features, such as establishing 
common ground between speaker and hearer 
(Clark and Gerring, 1984), and emoticons. 
3 Data 
In Twitter, people (tweeters) post messages of up 
to 140 characters (tweets). Apart from plain text, a 
tweet can contain references to other users 
(@<user>), URLs, and hashtags (#hashtag) which 
are tags assigned by the user to identify topic 
(#teaparty, #worldcup) or sentiment (#angry, 
#happy, #sarcasm). An example of a tweet is:  
?@UserName1 check out the twitter feed on 
@UserName2 for a few ideas :) http://xxxxxx.com 
#happy #hour?.  
   To build our corpus of sarcastic (S), positive (P) 
and negative (N) tweets, we relied on the annota-
tions that tweeters assign to their own tweets using 
hashtags. Our assumption is that the best judge of 
whether a tweet is intended to be sarcastic is the 
author of the tweet. As shown in the following sec-
tions, human judges other than the tweets? authors, 
achieve low levels of accuracy when trying to clas-
sify sarcastic tweets; we therefore argue that using 
the tweets labeled by their authors using hashtag 
produces a better quality gold standard. We used a 
Twitter API to collect tweets that include hashtags 
that express sarcasm (#sarcasm, #sarcastic), direct 
positive sentiment (e.g., #happy, #joy, #lucky), and 
direct negative sentiment (e.g., #sadness, #angry, 
#frustrated), respectively. We applied automatic 
filtering to remove retweets, duplicates, quotes, 
spam, tweets written in languages other than Eng-
lish, and tweets with URLs.  
To address the concern of Davidov et al 
(2010) that tweets with #hashtags are noisy, we 
automatically filtered all tweets where the hashtags 
of interest were not located at the very end of the 
message. We then performed a manual review of 
the filtered tweets to double check that the remain-
ing end hashtags were not part of the message. We 
thus eliminated messages about sarcasm such as ?I 
really love #sarcasm? and kept only messages that 
express sarcasm, such as ?lol thanks. I can always 
count on you for comfort :) #sarcasm?.  
Our final corpus consists of 900 tweets in each 
of the three categories, sarcastic, positive and 
negative. Examples of tweets in our corpus that are 
labeled with the #sarcasm hashtag include the fol-
lowing: 
 
1) @UserName That must suck.   
2) I can't express how much I love shopping 
on black Friday.                 
3) @UserName that's what I love about Mi-
ami. Attention to detail in preserving his-
toric landmarks of the past. 
4) @UserName im just loving the positive 
vibes out of that! 
 
The sarcastic tweets are primarily negative (i.e., 
messages that sound positive but are intended to 
convey a negative attitude) as in Examples 2-4, but 
there are also some positive messages (messages 
that sound negative but are apparently intended to 
be understood as positive), as in Example 1. 
4 Lexical and Pragmatic Features 
In this section we address the question of whether 
it is possible to empirically identify lexical and 
pragmatic factors that distinguish sarcastic, posi-
tive and negative utterances. 
Lexical Factors. We used two kinds of lexical fea-
tures ? unigrams and dictionary-based. The dictio-
nary-based features were derived from i) 
Pennebaker et al?s LIWC (2007) dictionary, which 
consists of a set of 64 word categories grouped into 
four general classes: Linguistic Processes (LP) 
(e.g., adverbs, pronouns), Psychological Processes 
(PP) (e.g., positive and negative emotions), Per-
sonal Concerns (PC) (e.g, work, achievement), and 
Spoken Categories (SC) (e.g., assent, non-
fluencies); ii) WordNet Affect (WNA) (Strappara-
va and Valitutti, 2004); and iii) list of interjections 
(e.g., ah, oh, yeah)1, and punctuations (e.g., !, ?). 
The latter are inspired by results from Kreuz and 
Caucci (2007). We merged all of the lists into a 
single dictionary. The token overlap between the 
words in combined dictionary and the words in the 
tweets was 85%. This demonstrates that lexical 
coverage is good, even though tweets are well 
                                                 
1
 http://www.vidarholen.net/contents/interjections/ 
582
known to contain many words that do not appear in 
standard dictionaries.  
Pragmatic Factors. We used three pragmatic fea-
tures: i) positive emoticons such as smileys; ii) 
negative emoticons such as frowning faces; and iii) 
ToUser, which marks if a tweets is a reply to 
another tweet (signaled by <@user> ).  
Feature Ranking.  To measure the impact of fea-
tures on discriminating among the three categories, 
we used two standard measures: presence and fre-
quency of the factors in each tweet. We did a 3-
way comparison of Sarcastic (S), Positive (P), and 
Negative (N) messages (S-P-N); as well as 2-way 
comparisons of i) Sarcastic and Non-Sarcastic (S-
NS);  ii) Sarcastic and Positive (S-P) and Sarcastic 
and Negative (S-N). The NS tweets were obtained 
by merging 450 randomly selected positive and 
450 negative tweets from our corpus.  
We ran a ?2 test to identify the features that were 
most useful in discriminating categories. Table 1 
shows the top 10 features based on presence of all 
dictionary-based lexical factors plus the pragmatic 
factors. We refer to this set of features as LIWC+. 
S-P-N S-NS S-N S-P 
Negemo(PP) 
Posemo(PP) 
Smiley(Pr) 
Question 
Negate(LP) 
Anger(PP) 
Present(LP) 
Joy(WNA) 
Swear(PP) 
AuxVb(LP)  
Posemo(PP) 
Present(LP) 
Question 
ToUser(Pr) 
Affect(PP)  
Verbs(LP) 
AuxVb(LP) 
Quotation 
Social(PP) 
Ingest(PP)  
Posemo(PP) 
Negemo(PP) 
Joy(WNA) 
Affect(PP) 
Anger(PP) 
Sad(PP) 
Swear(PP) 
Smiley(Pr) 
Body(PP) 
Frown(Pr)  
Question    
Present(LP) 
ToUser(Pr) 
Smiley(Pr) 
AuxVb(LP) 
Ipron(LP)   
Negate(LP) 
Verbs(LP) 
Time(PP) 
Negemo(PP)  
 
Table 1: 10 most discriminating features in LIWC+ 
for each task 
In all of the tasks, negative emotion (Negemo), 
positive emotion (Posemo), negation (Negate), 
emoticons (Smiley, Frown), auxiliary verbs 
(AuxVb), and punctuation marks are in the top 10 
features. We also observe indications of a possible 
dependence among factors that could differentiate 
sarcasm from both positive and negative tweets: 
sarcastic tweets tend to have positive emotion 
words like positive tweets do (Posemo is a signifi-
cant feature in S-N but not in S-P), while they use 
more negation words like  negative tweets do (Ne-
gate is an important feature for S-P). Table 1 also 
shows that the pragmatic factor ToUser is impor-
tant in sarcasm detection. This is an indication of 
the possible importance of features that indicate 
common ground in sarcasm identification.  
5 Classification Experiments 
In this section we investigate the usefulness of lex-
ical and pragmatic features in machine learning to 
classify sarcastic, positive and negative Tweets. 
    We used two standard classifiers often employed 
in sentiment classification: support vector machine 
with sequential minimal optimization (SMO) and 
logistic regression (LogR). For features we used: 
1) unigrams; 2) presence of dictionary-based lexi-
cal and pragmatic factors (LIWC+_P); and 3) fre-
quency of dictionary-based lexical and pragmatic 
factors (LIWC+_F). We also trained our models 
with bigrams and trigrams; however, results using 
these features did not report better results than uni-
grams and LICW+. The classifiers were trained on 
balanced datasets (900 instances per class) and 
tested through five-fold cross-validation. 
In Table 2, shaded cells indicate the best accura-
cies for each class, while bolded values indicate 
the best accuracies per row. In the three-way clas-
sification (S-P-N), SMO with unigrams as features 
outperformed SMO with LIWC+_P and LIWC+_F 
as features. Overall SMO outperformed LogR. The 
best accuracy of 57% is an indication of the diffi-
culty of the task.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We also performed several two-way classifica-
tion experiments. For the S-NS classification the 
best results were again obtained using SMO with 
Class Features SMO LogR 
S-
P-
N
 
Unigrams 57.22 49.00 
LIWC+_F 55.59 55.56 
LIWC+_P 55.67 55.59 
S-
N
S 
Unigrams 65.44 60.72 
LIWC+_F 61.22 59.83 
LIWC+_P 62.78 63.17 
S-
P 
Unigrams 70.94 64.83 
LIWC+_F 66.39 67.44 
LIWC+_P 67.22 67.83 
S-
N
 
Unigrams 69.17 64.61 
LIWC+_F 68.56 67.83 
LIWC+_P 68.33 68.67 
P-
N
 
Unigrams 74.67 72.39 
LIWC+_F 74.94 75.89 
LIWC+_P 75.78 75.78 
 
Table 2: Classifiers accuracies using 5-fold cross-
validation, in percent. 
583
unigrams as features (65.44%). For S-P and S-N 
the best accuracies were close to 70%. Overall, our 
best result (75.89%) was achieved in the polarity-
based classification P-N. It is intriguing that the 
machine learning systems have roughly equal dif-
ficulty in separating sarcastic tweets from positive 
tweets and from negative tweets.  
These results indicate that the lexical and prag-
matic features considered in this paper do not pro-
vide sufficient information to accurately 
differentiate sarcastic from positive and negative 
tweets. This may be due to the inherent difficulty 
of distinguishing short utterances in isolation, 
without use of contextual evidence.  
In the next section we explore the inherent diffi-
culty of identifying sarcastic utterances by compar-
ing human performance and classifier 
performance.  
6 Comparison against Human Perfor-
mance 
To get a better sense of how difficult the task of 
sarcasm identification really is, we conducted three 
studies with human judges (not the authors of this 
paper). In the first study, we asked three judges to 
classify 10% of our S-P-N dataset (90 randomly 
selected tweets per category) into sarcastic, posi-
tive and negative. In addition, they were able to 
indicate if they were unsure to which category 
tweets belonged and to add comments about the 
difficulty of the task. 
In this study, overall agreement of 50% was 
achieved among the three judges, with a Fleiss? 
Kappa value of 0.4788 (p<.05). The mean accuracy 
was 62.59% (7.7) with 13.58% (13.44) uncertainty. 
When we considered only the 135 of 270 tweets on 
which all three judges agreed, the accuracy, com-
puted over to the entire gold standard test set, fell 
to 43.33%2. We used the accuracy when the judges 
                                                 
2
 The accuracy on the set they agreed on (135  out of 270 
tweets) was 86.67%. 
agree (43.33%) and the average accuracy (62.59%) 
as a human baseline interval (HBI).  
We trained our SMO and LogR classifiers on 
the other 90% of the S-P-N. The models were then 
evaluated on 10% of the S-P-N dataset that was 
also labeled by humans. Classification accuracy 
was similar to results obtained in the previous sec-
tion. Our best result -- an accuracy of 57.41%-- 
was achieved using SMO and LIWC+_P (Table 3: 
S-P-N). The highest value in the established HBI 
achieved a slightly higher accuracy; however, 
when compared to the bottom value of the same 
interval, our best result significantly outperformed 
it.  It is intriguing that the difficulty of distinguish-
ing sarcastic utterances from positive ones and 
from negative ones was quite similar.  
In the second study, we investigated how well 
human judges performed on the two-way classifi-
cation task of labeling sarcastic and non-sarcastic 
tweets. We asked three other judges to classify 
10% of our S-NS dataset (i.e, 180 tweets) into sar-
castic and non-sarcastic. Results showed an 
agreement of 71.67% among the three judges with 
a Fleiss? Kappa value of 0.5861 (p<.05). The aver-
age accuracy rate was 66.85% (3.9) with 0.37% 
uncertainty (0.64). When we considered only cases 
where all three judges agreed, the accuracy, again 
computed over the entire gold standard test set, fell 
to 59.44% 3 . As shown in Table 3 (S-NS: 10% 
tweets), the HBI was outperformed by the automat-
ic classification using unigrams (68.33%) and 
LIWC+_P (67.78%) as features.  
Based on recent results which show that non-
linguistic cues such as emoticons are helpful in 
interpreting non-literal meaning such as sarcasm 
and irony in user generated content (Derks et al, 
2008; Carvalho et al, 2009), we explored how 
much emoticons help humans to distinguish sarcas-
tic from positive and negative tweets. For this test, 
we created a new dataset using only tweets with 
emoticons. This dataset consisted of 50 sarcastic 
                                                 
3
 The accuracy  on the set they agreed on (129 out of 180 
tweets) was 82.95%. 
 
 
Ta sk S ? N ? P    (10% data set) S ? NS (10% dataset) S ? NS (100 tweets + emoticons) 
HBI [43.33%-62.59%] [59.44% - 66.85%] [70% - 73%] 
Test Features SMO LogR SMO LogR SMO Log R 
1 Unigrams 55.92 46.66 68.33 57.78 71.00 66.00 
2 LIWC+_F 54.07 54.81 62.78 61.11 60.00 58.00 
3 LIWC+_P 57.41 57.04 67.78 67.22 51.00 53.00 
 
Table 3: Classifiers accuracies against humans? accuracies in three classification tasks. 
584
tweets and 50 non-sarcastic tweets (25 P and 25 
N). Two human judges classified the tweets using 
the same procedure as above. For this task judges 
achieved an overall agreement of 89% with Co-
hen?s Kappa value of 0.74 (p<.001). The results 
show that emoticons play an important role in 
helping people distinguish sarcastic from non-
sarcastic tweets. The overall accuracy for both 
judges was 73% (1.41) with uncertainty of 10% 
(1.4). When all judges agreed, the accuracy was 
70% when computed relative the entire gold stan-
dard set4  
Using our trained model for S-NS from the pre-
vious section, we also tested our classifiers on this 
new dataset. Table 3 (S-NS: 100 tweets) shows 
that our best result (71%) was achieved by SMO 
using unigrams as features. This value is located 
between the extreme values of the established HBI. 
These three studies show that humans do not 
perform significantly better than the simple auto-
matic classification methods discussed in this pa-
per. Some judges reported that the classification 
task was hard. The main issues judges identified 
were the lack of context and the brevity of the 
messages. As one judge explained, sometimes it 
was necessary to call on world knowledge such as 
recent events in order to make judgments about 
sarcasm. This suggests that accurate automatic 
identification of sarcasm on Twitter requires in-
formation about interaction between the tweeters 
such as common ground and world knowledge.  
7 Conclusion  
In this paper we have taken a closer look at the 
problem of automatically detecting sarcasm in 
Twitter messages. We used a corpus annotated by 
the tweeters themselves as our gold standard; we 
relied on the judgments of tweeters because of the 
relatively poor performance of human coders at 
this task.  We semi-automatically cleaned the cor-
pus to address concerns about corpus noisiness 
raised in previous work. We explored the contribu-
tion of linguistic and pragmatic features of tweets 
to the automatic separation of sarcastic messages 
from positive and negative ones; we found that the 
three pragmatic features ? ToUser, smiley and 
frown ? were among the ten most discriminating 
features in the classification tasks (Table 1).  
                                                 
4
 The accuracy on the set they agreed on (83 out of 100 
tweets) was 83.13%. 
We also compared the performance of automatic 
and human classification in three different studies. 
We found that automatic classification can be as 
good as human classification; however, the accura-
cy is still low. Our results demonstrate the difficul-
ty of sarcasm classification for both humans and 
machine learning methods. 
The length of tweets as well as the lack of expli-
cit context makes this classification task quite dif-
ficult. In future work, we plan to investigate the 
impact of contextual features such as common 
ground. 
Finally, the low performance of human coders in 
the classification task of sarcastic tweets suggests 
that gold standards built by using labels given by 
human coders other than tweets? authors may not 
be reliable. In this sense we believe that our ap-
proach to create the gold standard of sarcastic 
tweets is more suitable in the context of Twitter 
messages. 
Acknowledgments  
We thank all those who participated as coders in 
our human classification task. We also thank the 
anonymous reviewers for their insightful com-
ments. 
References  
Carvalho, P., Sarmento, S., Silva, M. J., and de Oliveira, 
E. 2009. Clues for detecting irony in user-generated 
contents: oh...!! it's "so easy" ;-). In Proceeding of 
the 1st international CIKM workshop on Topic-
sentiment analysis for mass opinion (TSA '09). 
ACM, New York, NY, USA, 53-56. 
Clark, H. and Gerrig, R. 1984. On the pretence theory of 
irony. Journal of Experimental Psychology: Gener-
al, 113:121?126. D.C.  
Davidov, D., Tsur, O., and Rappoport, A. 2010. Semi-
Supervised Recognition of Sarcastic Sentences in 
Twitter and Amazon, Dmitry Proceeding of Compu-
tational Natural Language Learning (ACL-CoNLL). 
Derks, D., Bos, A. E. R., and Grumbkow, J. V. 2008. 
Emoticons and Online Message Interpretation. Soc. 
Sci. Comput. Rev., 26(3), 379-388. 
Gibbs, R. 1986. On the psycholinguistics of sarcasm. 
Journal of Experimental Psychology: General, 
105:3?15. 
Gibbs, R. W. and Colston H. L. eds. 2007. Irony in 
Language and Thought. Routledge (Taylor and 
Francis), New York. 
585
Kreuz, R. J. and Glucksberg, S. 1989. How to be sarcas-
tic: The echoic reminder theory of verbal irony. 
Journal of Experimental Psychology: General, 
118:374-386. 
Kreuz, R. J. and Caucci, G. M. 2007. Lexical influences 
on the perception of sarcasm. In Proceedings of the 
Workshop on Computational Approaches to Figura-
tive Language (pp. 1-4). Rochester, New York: As-
sociation for Computational. 
LIWC Inc. 2007. The LIWC application. Retrieved May 
10, 2010, from 
http://www.liwc.net/liwcdescription.php. 
Nigam, K. and Hurst, M. 2006. Towards a Robust Me-
tric of Polarity. In Computing Attitude and Affect in 
Text: Theory and Applications (pp. 265-279). Re-
trieved February 22, 2010, from 
http://dx.doi.org/10.1007/1-4020-4102-0_20.   
Pak, A. and Paroubek, P. 2010. Twitter as a Corpus for 
Sentiment Analysis and Opinion Mining, in 
'Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation 
(LREC'10)' , European Language Resources Associ-
ation (ELRA), Valletta, Malta 
Pang, B. and Lee, L. 2008. Opinion Mining and Senti-
ment Analysis. Now Publishers Inc, July. 
Pennebaker, J.W., Francis, M.E., & Booth, R.J. (2001). 
Linguistic Inquiry and Word Count (LIWC): 
LIWC2001 (this includes the manual only). Mah-
wah, NJ: Erlbaum Publishers 
Strapparava, C. and Valitutti, A. 2004. Wordnet-affect: 
an affective extension of wordnet. In Proceedings of 
the 4th International Conference on Language Re-
sources and Evaluation, Lisbon. 
Tepperman, J., Traum, D., and Narayanan, S. 2006. 
Yeah right: Sarcasm recognition for spoken dialogue 
systems. In InterSpeech ICSLP, Pittsburgh, PA. 
Utsumi, A. 2000. Verbal irony as implicit display of 
ironic environment: Distinguishing ironic utterances 
from nonirony. Journal of Pragmatics, 32(12):1777?
1806. 
 
586
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 9?16
Manchester, August 2008
Learning to Map Text to Graph-based Meaning Representations via
Grammar Induction
Smaranda Muresan
Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
smara@umiacs.umd.edu
Abstract
We argue in favor of using a graph-based
representation for language meaning and
propose a novel learning method to map
natural language text to its graph-based
meaning representation. We present a
grammar formalism, which combines syn-
tax and semantics, and has ontology con-
straints at the rule level. These constraints
establish links between language expres-
sions and the entities they refer to in the
real world. We present a relational learning
algorithm that learns these grammars from
a small representative set of annotated ex-
amples, and show how this grammar in-
duction framework and the ontology-based
semantic representation allow us to di-
rectly map text to graph-based meaning
representations.
1 Introduction
Recent work (Wong and Mooney, 2007; Zettle-
moyer and Collins, 2005; He and Young, 2006)
has developed learning algorithms for the problem
of mapping sentences to their underlying semantic
representations. These semantic representations
vary from ?-expressions (Bos et al, 2004; Zettle-
moyer and Collins, 2005; Wong and Mooney,
2007) to DB query languages and command-like
languages (RoboCup Coach Language, CLang)
(Ge and Mooney, 2005).
In this paper we focus on an ontology-based
semantic representation which allows us to en-
code the meaning of a text as a direct acyclic
graph. Recently, there is a growing interest
on ontology-based NLP, starting from efforts in
defining ontology-based semantic representations
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(Nirenburg and Raskin, 2004), to using ontologi-
cal resources in NLP applications, such as ques-
tion answering (Basili et al, 2004; Beale et al,
2004), and building annotated corpora, such as the
OntoNotes project (Hovy et al, 2006).
There are three novel properties to ontology-
based semantics that we propose in this paper:
? There is a direct link between the ontology
and the grammar through constraints at the
grammar rule level. These ontology con-
straints enable access to meaning during lan-
guage processing (parsing and generation).
? Our ontology-based semantic representation
is expressive enough to capture various phe-
nomena of natural language, yet restric-
tive enough to facilitate grammar learning.
The representation encodes both ontological
meaning (concepts and relations among con-
cepts) and extra-ontological meaning, such as
voice, tense, aspect, modality.
? Our representation and grammar learning
framework allow a direct mapping of text to
its meaning, encoded as a direct acyclic graph
(DAG). We consider that ?understanding? a
text is the ability to correctly answer, at the
conceptual level, all the questions asked w.r.t
to that text, and thus Meaning = Text + all
Questions/Answers w.r.t that Text. Under this
assumption, obtaining the meaning of a text
is reduced to a question answering process,
which in our framework is a DAG matching
problem.
First, we review our grammar formalism intro-
duced in (Muresan, 2006; Muresan and Rambow,
2007), called Lexicalized Well-Founded Gram-
mars. Second, we present a relational learning al-
gorithm for inducing these grammars from a rep-
resentative sample of strings annotated with their
semantics, along with minimal assumptions about
9
I. Semantic Molecules
a. (major/adj)?= 0
B
B
B
B
B
B
B
@
h
1
2
6
4
cat adj
head X
1
mod X
2
3
7
5
b
1
D
X
1
.isa = major, X
2
.Y=X
1
E
1
C
C
C
C
C
C
C
A
b. (damage/noun)?= 0
B
B
B
B
B
B
B
@
h
2
2
6
4
cat noun
nr sg
head X
3
3
7
5
b
2
D
X
3
.isa = damageE
1
C
C
C
C
C
C
C
A
c. (major damage)?= 0
B
B
B
B
B
B
@
h
2
6
4
cat n
nr sg
head X
3
7
5
b
D
X
1
.isa = major, X.Y=X
1
, X.isa=damageE
1
C
C
C
C
C
C
A
II. Constraint Grammar Rule
N(w,
?
h
b
?
) ? Adj(w
1
,
?
h
1
b
1
?
), N(w
2
,
?
h
2
b
2
?
) : ?
c
(h, h
1
, h
2
), ?
o
(b)
?
c
(h, h
1
, h
2
) = {h.cat = n, h.head = h
1
.mod, h.head = h
2
.head, h.nr = h
2
.nr, h
1
.cat = adj, h
2
.cat = n}
?
o
(b) returns ?X
1
.isa = major, X.degree = X
1
, X.isa = damage?
Figure 1: Examples of three semantic molecules (I), and a constraint grammar rule together with the
semantic composition and ontology-based interpretation constraints, ?
c
and ?
o
(II)
syntax. Then, we describe the levels of represen-
tation we use to go from utterances to their graph-
based meaning representations, and show how our
representation is suitable to define the meaning of
an utterance/text through answers to questions. As
a proof of concept we discuss how our framework
can be used to acquire terminological knowledge
from natural language definitions and to query this
knowledge using wh-questions.
2 Grammar Formalism
Lexicalized Well-Founded Grammars (LWFGs)
introduced in (Muresan, 2006; Muresan and Ram-
bow, 2007) are a type of Definite Clause Gram-
mars (Pereira and Warren, 1980) where: (1) the
context-free backbone is extended by introducing
a partial ordering relation among nonterminals (the
basis for ?well-founded?); (2) each string is as-
sociated with a syntactic-semantic representation
called a semantic molecule; and (3) grammar rules
have two types of constraints: one for semantic
composition and one for ontology-based semantic
interpretation. The last two properties allow us to
have a syntactic-semantic grammar. The ontology
constraints provide access to meaning during lan-
guage learning, parsing and generation. The first
property allows us to learn these grammars from a
small set of annotated examples.
The semantic molecule is a syntactic-semantic
representation of natural language strings w? =
(
h
b
), where h (head) encodes the information re-
quired for semantic composition, and b (body) is
the actual semantic representation of the string.
Figure 1 gives examples of semantic molecules for
an adjective, a noun and a noun phrase, as pre-
sented in (Muresan and Rambow, 2007).
The head h of the semantic molecule is a flat
feature structure (i.e., feature values are atomic),
having at least two attributes that encode the syn-
tactic category of the associated string, cat, and
the head of the string, head. In addition, attributes
for agreement and other grammatical features can
be present (e.g., nr, pers for number and person).
The set of attributes is finite and known a-priori for
each syntactic category. Being a one-level feature
structure, no recursive or embedded structures are
allowed (unlike other grammar formalisms such as
HPSG, LFG), which makes this representation ap-
pealing for a learning framework. Recursion in the
grammar is obtained through the recursive gram-
mar rules and the composition constraint.
The body, b, of a semantic molecule is a flat rep-
resentation, called OntoSeR (Ontology-based Se-
mantic Representation). No embedding of pred-
icates is allowed, as in Minimal Recursion Se-
mantics (MRS) (Copestake et al, 1999). Unlike
MRS, OntoSeR is a logical form built as a con-
junction of atomic predicates ?concept?.?attr? =
?concept?, where variables are either concept or
slot (attr) identifiers in an ontology. For example,
the adjective major is represented as ?X
1
.isa =
major,X
2
.Y = X
1
?, which says that the meaning
of an adjective is a concept X
1
(X
1
.isa = major)
that is the value of a property of another concept
X
2
(X
2
.Y = X
1
) in the ontology.
A LWFG specifies one or more semantic
molecules for each string that can be parsed by
the grammar. The lexicon of a LWFG consists of
words paired with their semantic molecules shown
in Figure 1(Ia and Ib). In addition to the lexicon, a
LWFG has a set of constraint grammar rules. An
example of a LWFG rule is given in Figure 1(II).
Grammar nonterminals are augmented with pairs
of strings and their semantic molecules. These
pairs are called syntagmas, and are denoted by
? = (w,w
?
) = (w,
(
h
b
)
). This rule generates the
syntagma corresponding to major damage whose
semantic molecule is given in Figure 1(Ic). There
are two types of constraints at the grammar rule
level ? one for semantic composition (how the
10
meaning of a natural language expression is com-
posed from the meaning of its parts) and one for
ontology-based semantic interpretation. The com-
position constraints ?
c
are applied to the heads of
the semantic molecules, the bodies being just con-
catenated. Figure 1 shows that the body of the se-
mantic molecule for major damage is a concate-
nation of the bodies of the adjective major and
noun damage, together with a variable substitu-
tion. This variable substitution {X
2
/X,X
3
/X} is
a result of ?
c
, which is a system of equations ?
a simplified version of ?path equations? (Shieber
et al, 1983), because the heads are flat feature
structures. These constraints are learned together
with the grammar rules. The ontology-based con-
straints ?
o
represent the validation on the ontol-
ogy, and are applied to the body of the semantic
molecule associated with the left-hand side non-
terminal. The ontology-based interpretation is not
done during the composition operation, but after-
words. Thus, for example, the head of the noun
phrase major damage does not need to store the
slot Y , a fact that allows us to use flat feature
structures to represent the head of the semantic
molecules. The ontology-based constraints are not
learned; rather, ?
o
is a general predicate applied
to the logical form semantic representation which
fully contains all the required information needed
for validation on the ontology. Thus, it is indepen-
dent of grammatical categories. This predicate can
succeed or fail as a result of querying the ontology
? when it succeeds, it instantiates the variables of
the semantic representation with concepts/slots in
the ontology (Y = degree). For example, given
the phrase major damage, ?
o
succeeds and returns
?X
1
.isa = major,X.degree = X
1
, X.isa =
damage?, while given major birth it fails.
3 Grammar Learning Algorithm
Unlike stochastic grammar learning for syntac-
tic parsing (e.g., (Collins, 1999)), LWFG is well
suited to learning from reduced-size training data.
Furthermore, unlike previous formalisms used for
deeper representations (e.g, HPSG, LFG), our
LWFG formalism is characterized by a formal
guarantee of polynomial learnability (Muresan,
2006).
A key to these properties is the partial order-
ing among grammar nonterminals, i.e., the set of
nonterminals is well-founded. This partial order-
ing among nonterminals allows us to define the
representative examples of a LWFG, and to learn
LWFGs from this small set of examples. The rep-
resentative examples E
R
of a LWFG, G, are the
simplest syntagmas ground-derived by the gram-
mar G ? i.e., for each grammar rule, there ex-
ists a syntagma which is ground-derived from it in
the minimum number of steps. Informally, repre-
sentative examples are building blocks from which
larger structures can be inferred via reference to a
larger corpus E
?
which can be only weakly anno-
tated (i.e., bracketed), or unannotated. This larger
corpus, E
?
, is used for generalization during learn-
ing (Figure 2).
The theoretical learning model is Grammar
Approximation by Representative Sublanguage
(GARS) introduced in (Muresan, 2006; Muresan
and Rambow, 2007). We proved that the search
space for grammar induction is a complete gram-
mar lattice, and we gave a learnability theorem for
LWFG induction. The GARS model uses a poly-
nomial algorithm for LWFG learning that takes
advantage of the building blocks nature of repre-
sentative examples. The learning algorithm be-
longs to the class of Inductive Logic Programming
methods (ILP), based on entailment (Muggleton,
1995; Dzeroski, 2007). Unlike existing ILP meth-
ods that use randomly-selected examples, our al-
gorithm learns from a set of representative exam-
ples allowing a polynomial efficiency for learn-
ing a syntactico-semantic constraint-based gram-
mar, suitable to capture large fragments of natural
language (Muresan, 2006).
The LWFG induction algorithm is a cover set al
gorithm, where at each step a new constraint gram-
mar rule is learned from the current representative
example, ? ? E
R
. Then this rule is added to the
grammar rule set. The process continues until all
the representative examples are covered. We de-
scribe below the process of learning a grammar
rule from the current representative example, illus-
trated as well in Figure 2.
Step 1. In the first step, the most specific gram-
mar rule is generated from the current represen-
tative example. The category name annotated
in the representative example gives the name of
the left-hand-side nonterminal (?predicate inven-
tion?, in ILP terminology), while a robust parser
returns the minimum number of chunks cover-
ing the representative example. The categories
of the chunks give the nonterminals of the right-
hand side of the most specific rule. For ex-
11
cat adj
head X1
mod X2
cat noun
head X3
nr     sg
cat   n
head X
nr     sg
<X1.isa=major, X.Y=X1, X.isa=X1>
)(major damage, 
N A N: 
major damage
very beautiful painting
loud clear noise
N
N
N
Adj   Noun:
A  Noun: 
A  N: 
(score=1)
(score=2)
(score=3)
CANDIDATE GRAMMAR RULES
r1
r3
r2
N Adj  Noun:
Adj (major, )
<X1.isa=major, X2.Y=X1>
(damage,  
<X3.isa=damage>
)Noun
BACKGROUND KNOWLEDGE
Performance Criteria
CURRENT REPRESENTATIVE  EXAMPLE
MOST SPECIFIC CONSTRAINT GRAMMAR RULE
REPRESENTATIVE SUBLANGUAGE
BEST RULE 
STEP 1 (ROBUST PARSING)
chunks={[Adj(major), A(major)],[Noun(damage), N(damage)]}
A
A
N
Adj:
Adv A:
Noun:
N ......
STEP 2 (RULE GENERALIZATION)
=
h.nr=h2.nr, h1.cat=adj, h2.cat=noun }
 {h.cat=n, h.head=h1.mod, h.head=h2.head,
PSfrag replacements
r
i
?
E
?
r
?
c1
?
c2
?
c3
?
c4
?
c4
?
c4
?
c5
?
c6
?
c6
Figure 2: An iteration step of the learning algorithm
ample, in Figure 2, given the representative ex-
ample major damage annotated with its seman-
tic molecule, and the background knowledge con-
taining the already learned rules A ? Adj
and N ? Noun,1 the robust parser generates
the chunks corresponding to the adjective major
and the noun damage: [Adj(major),A(major)] and
[Noun(damage),N(damage)], respectively. The
most specific rule generated is thus N ?
Adj Noun : ?
c4
, where the left hand side nonter-
minal is given by the category of the representative
example, in this case n. The compositional con-
straints ?
c4
are learned as well. It can be seen that
the annotation of the representative example does
not require us to provide ontology-specific roles or
concepts. Thus, grammar learning is general, and
can be done using a small, generic lexicon.
Step 2. In the second step, this most specific rule is
generalized, obtaining a set of candidate grammar
rules. The performance criterion in choosing the
best grammar rule among these candidate hypothe-
ses is the number of the examples in the represen-
tative sublanguage E
?
(generalization corpus) that
can be parsed using the candidate grammar rule to-
gether with the previous learned rules. In Figure
2 given the representative sublanguage E
?
={ ma-
jor damage, loud clear noise, very beautiful paint-
ing} the learner will generalize to the recursive
rule N ? A N : ?
6
, since only this rule can parse
1For readability, we only show the context-free backbone
of the grammar rules, and ?
o
are not discussed since they are
not learned.
all the examples in E
?
.
4 Levels of Representation
In order to transform natural language utterances
to knowledge, we consider three levels of repre-
sentation: the utterance level, the text level and the
ontology level. In Section 4.4 we show that these
levels of representation allow us to define meaning
as Meaning=Text+all Questions/Answers w.r.t that
Text, using a DAG matching approach.
4.1 Utterance-level Representation
At the utterance level, the semantic representation
corresponds directly to a syntagma ? after the on-
tology constraint ?
o
is applied. This representa-
tion is called Ontology-based Semantic Represen-
tation OntoSeR. At this level, the attrIDs are in-
stantiated with values of the slots from the ontol-
ogy, while the conceptIDs remain variables to al-
low further composition to take place. At OntoSeR
level we can exploit the reversibility of the gram-
mar, since this representation is used during pars-
ing/generation.
In Figure 3 we show the semantic represen-
tation OntoSeR for the utterance Hepatitis B is
an acute viral hepatitis caused by a virus that
tends to persist in the blood serum, obtained using
our parser in conjunction with our learned gram-
mar. The composition constraints bind the con-
ceptID variables, while the ontology constraint in-
stantiates the attrID variables with values of slots
in the ontology. The ontology constraint can be
12
Hepatitis B is an acute viral hepatitis caused by a virus that tends to persist in the blood serum.
OntoSeR = ?(A.name=hepatitisB)
HepatitisB
, (A.tense=pr)
is
, (A.det=an)
an
, (B.is a=acute, A.duration=B)
acute
,
(C.is a=viral, A.kind of=C)
viral
, (A.is a=hepatitis)
hepatitis
, (D.vft=ed, D.voice=pas, D.is a=cause, D.ag=E,
D.th=A)
caused
, (ag.is a=by, D.ag=E)
by
, (E.det=a)
a
, (E.is a=virus)
virus
, (E.is a=that)
that
, (F.tense=pr, F.is a=tend,
F.no ag=E, F.prop=G)
tends
, (G.vft=to, G.is a=persist, G.th=E)
to persist
, (loc.is a=in, G.loc=H)
in
, (H.det=the)
the
,
(I.is a=blood, H.of=I)
blood
, (H.is a=serum)
serum
?
TKR
?29.name= hepatitisB ?33.det= virus
?29.tense= pr ?33.is_a= that
?20.det= an ?34.tense= pr
?30.is_a= acute ?34.is_a= tend
?29.duration=?2 ?34.no_role=?33
?31.is_a= viral ?34.prop=?35
?29.kind_of=?3 ?35.vft= to
?29.is_a= hepatitis ?35.is_a= persist
?32.vft= ed ?35.th=?33
?32.voice= pas loc.is_a= in
?32.is_a= cause ?35.loc=?36
?32.ag=?5 ?36.det= the
?32.th=?1 ?37.is_a= blood
ag.is_a= by ?36.of=?37
?32.ag=?33 ?36.is_a= serum
?33.det= a
OKR
#viral#acute
#hepatitisB #virus33
#cause32 #persist35
#tend34
#serum36
#blood
th ag
duration kind_of of
th loc
prop
#hepatitis
sub
Figure 3: Example of an utterance and its levels of representation
seen as a local semantic interpretation at the ut-
terance/grammar rule level, providing access to
meaning during parsing/generation. In this pa-
per, this semantic interpretation is based only on
a weak ?ontological model?. For the verb the-
matic roles we considered the thematic roles de-
rived from Dorr?s LCS Database (e.g., ag=agent,
th=theme, prop=proposition) (Dorr, 1997). For
adjectives and adverbs we took the roles (prop-
erties) from WordNet (Miller, 1990). For prepo-
sitions we considered the LCS Database. We
also have manually added specific/dummy seman-
tic roles when they were not present in these re-
sources (e.g., of between blood and serum).
The example in Figure 3 shows the output of
our parser in conjunction with the learned gram-
mar for a definitional sentence that contains several
linguistic phenomena such as copula to-be predica-
tive, reduced relative clauses (caused by ...), rel-
ative clauses (virus that ...), raising construction
(tends to persist, where virus is not the argument
of tends but the argument of persist), and noun
compounds (blood serum). For readability, we in-
dicate what part of OntoSeR corresponds to each
lexical item. It can be noticed that OntoSeR con-
tains representations of both ontological meaning
(concepts and relations among concepts) as well as
extra-ontological meaning such as tense and voice
(D.voice = pas; F.tense = pr).
4.2 Text-level Representation
The text-level representation TKR, or discourse
level representation, represents asserted represen-
tations. ConceptIDs become constants, and no
composition can happen at this level. However, we
still have (indirect) reversibility, since TKR repre-
sents all the asserted OntoSeRs. Therefore, all the
information needed for reversibility is still present.
Figure 3 shows an example of the TKR for the
above utterance.
4.3 Ontology-level Representation
Ontology-level knowledge representation OKR is
obtained after task-specific interpretation, which
can be seen as a global semantic interpretation.
OKR is a directed acyclic graph (DAG) G =
(V,E). Edges, E, are either semantic roles given
by verbs, prepositions, adjectives and adverbs,
or extra-ontological meaning properties, such as
tense, aspect, modality, negation. Vertices, V are
either concepts (corresponding to nouns, verbs,
adjectives, adverbs, pronouns, cf. Quine?s crite-
rion (Sowa, 1999, page 496)), or values of the
extra-ontological properties such as present cor-
responding to tense property. In this paper, the
task-specific interpretation is geared mainly to-
wards terminological interpretation. We filter from
OntoSeR determiners and some verb forms, such
as tense, aspect, since temporal relations appear
less in terminological knowledge than in factual
13
knowledge. However, we treat modals and nega-
tion, as they are relevant for terminological knowl-
edge. An example of OKR for the above utterance
is given in Figure 3.
We consider both concepts (e.g., #acute,
#blood), and instances of concepts (e.g., #virus33,
#cause32). Concepts are denoted in OKR by
#name concept, and they form a hierarchy of con-
cepts based on the subsume relation (sub), which
is the inverse of the is a relation. An instance of
a concept is denoted by the name of a concept fol-
lowed by the instance number (e.g., #virus33). A
concept and an instance of this concept are two dif-
ferent vertices in OKR, having the same name. At
the OKR level we assume the principle of concept
identity which means that there is a bijection be-
tween a vertex in OKR and a referent. For exam-
ple, if we do not have pronoun resolution, the pro-
noun and the noun it refers to will be represented
as two separate vertices in the graph. Currently,
our semantic interpreter implements only a weak
concept identity principle which facilitates struc-
ture sharing and inheritance.
To give these two properties we first introduce
some notations. A DAG is called rooted at a vertex
u ? V , if there exists a path from u to each vertex
of the DAG. We have the following definition:
Definition 1. Two subDAGs rooted at two vertices
u, u
? are equal if the set of the adjacent vertices to
u and u? respectively, are equal and if the edges in-
cident from u and u? have the same semantic roles
as labels.
Property 1 (Structure Sharing). In an OKR, all
vertices u, u? ? V with the same name, and whose
subDAGs are equal are identical (i.e., the same
vertex in OKR).
Using a hash table, there is a linear algorithm
O(|V | + |E|) which transforms an OKR to an
equivalent OKR which satisfies Property 1. In Fig-
ure 4 it can be seen that the OKRs of Hepatitis
A and Hepatitis B share the representation corre-
sponding to blood serum (i.e., blood serum is the
same concept instance and due to Property 1 we
have that #serum36=#serum27 and thus they have
the same vertex in the OKR).
Property 2 (Inheritance). A concept in a hierarchy
of concepts can be linked by the sub relation only
to its parent(s), and not to any other ancestors. A
subDAG defining a property of a concept from the
hierarchy of concepts can be found only once in
the OKR at the level of the most general concept
that has this property.
For terminological knowledge we have that any
instance of a concept is a concept, and the defi-
nition is the naming of a concept instance. For
example, the definition of Hepatitis B, is an in-
stance of a concept #hepatitis which has additional
attributes acute, viral and caused by a virus that
tends to persist in the blood serum. Thus, an
additional instance of concept #hepatitis is cre-
ated, which is named #hepatitisB. The fact that
we can have the definition as a naming of a con-
cept instance is facilitated also by our treatment
of copula to-be at the OntoSeR level (A.name =
hepatitisB, . . . , A.is a = hepatitis in Figure 3)
4.4 Meaning as Answers to Questions
We consider that ?understanding? a text is the abil-
ity to correctly answer, at the conceptual level,
all the questions asked w.r.t to that text, and thus
Meaning = Text + all Questions/Answers w.r.t that
Text. In our framework we consider the principle
of natural language as problem formulation, and
not problem solving. Thus, we can represent at
OKR level a paradox formulation in natural lan-
guage, even if the reasoning about its solution can-
not be emphasized. Our levels of representations
allow us to define the meaning of questions, an-
swers and utterances using a DAG matching ap-
proach.
Definition 2. The meaning of a question, q, with
respect to an utterance/discourse, is the set of all
answers that can be directly obtained from that ut-
terance/discourse. The semantic representation of
a question is a subgraph of the utterance graph
where the wh-word substitutes the answer con-
cept(s).
Definition 3. The answer to a question is the con-
cept that matches the wh-word through the DAG
matching algorithm between the question?s sub-
DAG and the utterance/discourse DAG.
Definition 4. The meaning of an utterance u is the
set of all questions that can be asked w.r.t that ut-
terance, together with their answers.
Unlike meaning as truth conditions, where the
problem of meaning equivalence is reduced to
logical form equivalence, in our case meaning
equivalence is reduced to semantic equivalence of
DAGs/subDAGs which obey the concept identity
principle (weak, or strong). The matching algo-
14
rithm obtains the same answers to questions, rela-
tive to semantic equivalent DAGs. If we consider
only the weak concept identity principle given by
Properties 1 and 2, the problem is reduced to
DAG/subDAG identity.
5 Discussion
The grammar formalism, learning model and our
ontology-based representation allow us to directly
map text to graph-based meaning representations.
Our method relies on a general grammar learn-
ing framework and a task-specific semantic inter-
preter. Learning is done based on annotated ex-
amples that do not contain ontology-specific roles
or concepts as we saw in Section 3, and thus our
learning framework is general. We can use any
ontology, depending on the application. The task-
specific semantic interpreter we are currently using
is targeted for terminological knowledge, and uses
a weak ?ontological model? based on admissibility
relations we can find at the level of lexical entries
and a weak concept identity principle.
In (Muresan, 2006) we showed that our gram-
mar formalism and induction model allow us to
learn diverse and complex linguistic phenomena:
complex noun phrases (e.g., noun compounds,
nominalization), prepositional phrases, reduced
relative clauses, finite and non-finite verbal con-
structions (including, tense, aspect, negation), co-
ordination, copula to be, raising and control con-
structions, and rules for wh-questions (including
long-distance dependencies).
In this section we discuss the processes
of knowledge acquisition and natural language
querying, by presenting an example of construct-
ing terminological knowledge from definitions of
hepatitis, Hepatitis A and Hepatitis B. The defi-
nitional text and OKRs are presented in Figure 4,
OKR being shown only for the last two definitions
for readability reasons. A question and answer re-
lated to the resulting OKR are also given.
The definiendum is always a concept, and it is
part of the sub hierarchy. The concepts in the sub
hierarchy are presented in bold in Figure 4. In ad-
dition to the concepts that are defined, we can also
have concepts that are referred (i.e., they are part
of the definiens), if they do not have any modifi-
cation (e.g., #blood in definition of Hepatitis A,
and Hepatitis B). If a referred concept has modi-
fications, it is represented as an instance of a con-
cept in OKR. As a consequence, various verbal-
izations of concept properties can be differentiated
in OKR, allowing us to obtain direct answers that
are specific to each verbalization. For example, the
term virus appears in the definition of both Hepati-
tis A and Hepatitis B. In OKR, they are two differ-
ent instances of a concept, #virus25 and #virus33,
since they have different modifications: persists
in the blood serum, does not persists in the blood
serum, respectively. These modifications are an es-
sential part of the differentia of the two concepts
#hepatitisA and #hepatitisB, causing the distinc-
tion between the two. When we ask the question
What is caused by a virus that persists in the blood
serum? we obtain only the correct answer #hepati-
tisB (Figure 4).
Another important aspect that shows the ade-
quacy of our representation for direct acquisition
and query is the OKR-equivalences that we ob-
tain for different syntactic forms. They are related
mainly to verbal constructions. Among OKR-
equivalences we have: 1) active and passive con-
structions; 2) -ed and -ing verb forms in reduced
relative clauses are equivalent to passive/active
verbal constructions; 3) constructions involving
raising verbs, where we can take advantage of the
fact that the controller is not the semantic argument
of the raising verb (e.g., in the definition of Hep-
atitis B we have . . . caused by a virus that tends to
persist in the blood serum, while the question can
be asked without the raising verb What is caused
by a virus that persists in the blood serum?; see
Figure 4).
Besides acquisition of terminological knowl-
edge, our grammar and semantic interpreter facil-
itates natural language querying of the acquired
knowledge base, by treatment of wh-questions.
Querying is a DAG matching problem, where the
wh-word is matched to the answer concept.
6 Conclusions
This paper has presented a learning framework
to automatically map natural language to graph-
based meaning representations via grammar in-
duction. We presented an ontology-based seman-
tic representation that allows us to define meaning
as Meaning=Text+all Questions/Answers w.r.t that
Text, using a DAG matching approach.
In the future, we plan to extend this work in two
main directions. First, we plan to use a stronger
semantic context with hierarchies of concepts and
semantic roles, selectional restrictions, as well as
15
1. Hepatitis is a disease caused by infectious or toxic agents and
characterized by jaundice, fever and liver enlargement.
2. Hepatitis A is an acute but benign viral hepatitis caused by a virus
that does not persist in the blood serum.
3. Hepatitis B is an acute viral hepatitis caused by a virus that tends
to persist in the blood serum.
#persist26 #cause24
y #virus25
#disease
#hepatitis
#hepatitisA #hepatitisB
#benign
#acute
#viral
#serum27
#tend34
#persist35
#virus33
#cause32
#blood
loc
neg th ag
th
sub
sub sub
dur
atio
n
duration
kind_ofbenignity kind_of
ag
prop
th
loc
of
th
Q1: What is caused by a virus that persists in the
blood serum?
#serum #virus #what
#cause#persist
#blood
th ag th
of
loc
A1: #hepatitisB
#hepatitis
#hepatitisB
#acute #viral
#cause32
#virus33 #serum27
#blood
#persist35
sub
duration kind_of
th ag th loc
of
Figure 4: Acquisition/Query of terminological knowledge
semantic equivalences based on synonymy and
anaphora. The second direction is to enhance the
ontology with probabilities.
References
Basili, Roberto, Dorte H. Hansen, Patrizia Paggio,
Maria Teresa Pazienza, and Fabio Zanzotto. 2004. On-
tological resources and question answering. In Workshop
on Pragmatics of Question Answering, held jointly with
NAACL 2004.
Beale, Stephen, Benoit Lavoie, Marjorie McShane, Sergei
Nirenburg, and Tanya Korelsky. 2004. Question answer-
ing using ontological semantics. In ACL 2004: Second
Workshop on Text Meaning and Interpretation.
Bos, Johan, Stephen Clark, Mark Steedman, James R. Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage seman-
tic representations from a CCG parser. In Proceedings of
COLING-04.
Collins, Michael. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Copestake, Ann, Dan Flickinger, Ivan A. Sag, and Carl Pol-
lard. 1999. Minimal Recursion Semantics: An introduc-
tion.
Dorr, Bonnie J. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine trans-
lation. Machine Translation, 12(4):271?322.
Dzeroski, Saso. 2007. Inductive logic programming in a nut-
shell. In Getoor, Lise and Ben Taskar, editors, Introduction
to Statistical Relational Learning. The MIT Press.
Ge, Ruifang and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics. In
Proceedings of CoNLL-2005.
He, Yulan and Steve Young. 2006. Spoken language un-
derstanding using the hidden vector state model. Speech
Communication Special Issue on Spoken Language Under-
standing in Conversational Systems, 48(3-4).
Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of HLT-NAACL 2006.
Miller, George. 1990. WordNet: An on-line lexical database.
Journal of Lexicography, 3(4):235?312.
Muggleton, Stephen. 1995. Inverse Entailment and Progol.
New Generation Computing, Special Issue on Inductive
Logic Programming, 13(3-4):245?286.
Muresan, Smaranda and Owen Rambow. 2007. Grammar ap-
proximation by representative sublanguage: A new model
for language learning. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguistics
(ACL).
Muresan, Smaranda. 2006. Learning constraint-based gram-
mars from representative examples: Theory and applica-
tions. Technical report, PhD Thesis, Columbia University.
Nirenburg, Sergei and Victor Raskin. 2004. Ontological Se-
mantics. MIT Press.
Pereira, Fernando C. and David H.D Warren. 1980. Definite
Clause Grammars for language analysis. Artificial Intelli-
gence, 13:231?278.
Shieber, Stuart, Hans Uszkoreit, Fernando Pereira, Jane
Robinson, and Mabry Tyson. 1983. The formalism and
implementation of PATR-II. In Grosz, Barbara J. and
Mark Stickel, editors, Research on Interactive Acquisition
and Use of Knowledge, pages 39?79. SRI International,
Menlo Park, CA, November.
Sowa, John F. 1999. Knowledge Representation: Logical,
Philosophical, and Computational Foundations. Brooks
Cole Publishing Co., Pacific Grove, CA.
Wong, Yuk Wah and Raymond Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda cal-
culus. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL-2007).
Zettlemoyer, Luke S. and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proceedings of
UAI-05.
16
Workshop on Computational Linguistics for Literature, pages 1?7,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Computational Analysis of Referring Expressions in 
Narratives of Picture Books 
 
 
Choonkyu Lee Smaranda Muresan Karin Stromswold 
Department of Psychology Library and Information Science Department Department of Psychology 
Rutgers Center for Cognitive Science School of Communication and Information Rutgers Center for Cognitive Science 
Rutgers University ? New Brunswick Rutgers University ? New Brunswick Rutgers University ? New Brunswick 
choonkyu@eden.rutgers.edu smuresan@rci.rutgers.edu karin@ruccs.rutgers.edu 
 
 
 
 
 
 
Abstract 
This paper discusses successes and failures of 
computational linguistics techniques in the 
study of how inter-event time intervals in a 
story affect the narrator?s use of different 
types of referring expressions. The success 
story shows that a conditional frequency dis-
tribution analysis of proper nouns and pro-
nouns yields results that are consistent with 
our previous results ? based on manual coding 
? that the narrator?s choice of referring ex-
pression depends on the amount of time that 
elapsed between events in a story. Unfortu-
nately, the less successful story indicates that 
state-of-the-art coreference resolution systems 
fail to achieve high accuracy for this genre of 
discourse. Fine-grained analyses of these fail-
ures provide insight into the limitations of cur-
rent coreference resolution systems, and ways 
of improving them. 
1 Introduction 
In theories of information structure in extended 
discourse, various factors of discourse salience 
have been proposed as determinants of information 
?newness? vs. ?givenness? (e.g., Prince, 1981). 
Based on evidence from speakers? choice of differ-
ent types of referring expressions in referring back 
to a previously introduced discourse referent, 
scholars have discovered effects of (a) ?referential 
distance? (Giv?n, 1992), a text-based measure of 
distance between the antecedent and the re-
mention in terms of number of intervening clauses; 
(b) topic-prominence of the referent in the previous 
mention (Brennan, 1995); (c) presence of another 
candidate referent (?competitor?) in linguistic or 
visual context (Arnold and Griffin, 2007), among 
others. In re-mentioning individuals, one can, for 
example, simply repeat names or use anaphoric 
devices, such as definite descriptions and pronouns. 
In our work, we have been investigating the role 
of mental representation of nonlinguistic situation-
al dimensions of the storyline (e.g., Zwaan, 1999) 
as an additional factor of salience in discourse or-
ganization. From the five situational dimensions of 
the event-indexing model (Zwaan and Radvansky, 
1998), we have focused on the time dimension. In 
a narrative elicitation study (Lee and Stromswold, 
submitted; Lee, 2012), we presented picture se-
quences from three wordless picture books in Mer-
cer Mayer?s ?Boy, Dog, Frog series? (Mayer, 
1969; Mayer, 1974; Mayer and Mayer, 1975), and 
had 8 adults estimate the inter-event intervals in 
story time between consecutive scenes with no lin-
guistic stimuli, and had a different group of native 
English-speaking adults write stories to go along 
with the pictures. The 36 adults wrote a total of 58 
written narratives, which consisted of 2778 sen-
tences and 38936 word tokens (48 sentences and 
671 word tokens per narrative on average). The use 
of wordless picture books allows fixed target con-
tent and clear visual availability of the characters 
and their actions. 
In our previous analysis (Lee and Stromswold, 
submitted) of the effect of inter-event time inter-
vals on the narrator?s referential choice in referring  
1
        S1) Finally though, the boy starts to get tired and de-
cides to crawl into bed. His dog joins him and soon they 
are asleep. The boy forgot to put a lid on the bottle, and 
Mr. Frog is sneaking out!  
S2) When the boy wakes up in the morning, he sees that 
Mr. Frog is gone. He is very upset that he lost his new 
friend. 
 
Figure 1. Sample ?Long Interval? Between Scenes S1 
and S2 (Mean Estimate: 6h 48m 45s). 
 
back to characters, we manually annotated critical 
sentences selected on the basis of the eight longest 
(mean duration = 1 hour 7 minutes 2 seconds; 
henceforth, ?Long Intervals?) and the eight shortest 
(mean duration = 10 seconds; henceforth, ?Short 
Intervals?) estimated intervals. Examples of a Long 
Interval and a Short Interval between scenes are 
given in Figures 1 and 2, together with sample cor-
responding narratives. For each of the 58 narratives, 
we analyzed the first sentence after a Long and 
Short Interval. Our coding of referring expressions 
involved frequency counts (ranging from 0 to 3) of 
instances of each of our Referential Types ? Proper 
Names (e.g., Mr. Frog), Definite Descriptions (e.g., 
the frog), and Pronouns (e.g., he) ? per critical sen-
tence. We found a significant interaction between 
Interval and Referential Type in both a chi-square 
test of association and an analysis of variance, and 
the effect generally held across participants. Our 
finding demonstrated that narrators used Proper 
Names more after Long Intervals than after Short 
Intervals in story time, and more singular-referent 
Pronouns after Short Intervals than after Long In-
tervals. 
Addressing the issue of the effect of inter-event 
interval on referential choice on a larger scale re-
quires accurate automatic methods for identifica-
tion of Referential Types and coreference 
resolution for the narratives.  In this paper we first 
present a simple computational method for analyz-
ing the entire scene descriptions after the Long and 
       S3) After staring at the frog for two minutes he says 
"Ribbittttttt" and she screams and  throws her fork into 
the air, and falls back in her chair. Charles gets scared 
by her screaming and jumps off her plate into the air.  
S4) Luckily, he lands safely into a man's drink. He is 
mid-conversation with a beautiful lady and doesn't feel 
the new addition to his martini. 
 
Figure 2. Sample ?Short Interval? Between Scenes S3 
and S4 (Mean Estimate: 3s). 
 
Short Intervals to study how inter-event intervals 
affect referential choice, focusing on Proper Nouns 
and Pronouns. Our results from the automatic 
methods are consistent with the results obtained 
using manual coding of the critical sentences. Se-
cond, we present an annotation study of nine narra-
tives with coreference chains, and also discuss the 
performance of two state-of-the-art coreference 
resolution systems on a sample of our data. 
2 Inter-event Interval Effect on Referring 
Expressions: A Basic Computational 
Approach 
In order to address the question of how inter-event 
intervals affect the choice of referring expressions, 
we analyzed the frequency of Pronouns and Proper 
Nouns in scenes following the Long and Short In-
tervals.  The results in Table 1 are consistent with 
our previous results obtained based on manual cod-
ing of the critical sentences only: The ?Long Inter-
val? (LI) scenes and the ?Short Interval? (SI) scenes 
diverge in relative frequencies of our target part-
of-speech tags ? Pronouns (nominal (PRP) and 
possessive (PRP$) forms) vs. Proper Names (NNP).  
One can observe that there are generally higher 
frequencies of Proper Names for the scenes after 
the Long Intervals compared to the Short Intervals, 
not only in absolute number but in relative propor-
tion to Pronouns as well. A noticeable exception, 
Scene 3 of One Frog Too Many (Mayer and  
2
Book Scene# PRP PRP$ NNP 
Frog 
Goes 
to 
Dinner 
4 (LI) 62 56 106 
5 (LI) 54 37 96 
21 (LI) 87 60 120 
9 (SI) 45 22 27 
13 (SI) 50 44 50 
14 (SI) 40 21 40 
One 
Frog 
Too 
Many 
8 (LI) 33 33 55 
19 (LI) 63 42 90 
20 (LI) 60 29 88 
3 (SI) 70 65 158 
15 (SI) 69 50 73 
23 (SI) 1 2 2 
Frog, 
Where 
Are 
You? 
2 (LI) 89 70 143 
3 (LI) 70 65 158 
18 (SI) 64 56 86 
19 (SI) 63 42 90 
Table 1. Scene-based Frequencies of Pronouns and 
Proper Names after the 16 Long and Short Intervals. 
 
Mayer, 1975), is a very early scene in the picture 
book, with many character introductions and dis-
course-newness (Prince, 1981). Even with this ex-
ception included, the association between Interval 
(Long vs. Short) and Referential Type (Pronouns 
vs. Proper Names) was significant in a new analy-
sis based on the entire scene descriptions, rather 
than just the first sentences for these scenes [?2(1) 
= 9.50, p = .0021]. The significant effect of Inter-
val reveals that Proper Names were more common-
ly used after Long Intervals than after Short 
Intervals, and Pronouns were more commonly used 
after Short Intervals than after Long Intervals. 
The exception in Scene 3 of One Frog Too 
Many suggests, however, that excluding first few 
mentions in a coreference chain from analysis may 
reveal a stronger effect of Interval on referential 
type of re-mentions (although one mention for in-
troducing a character does not always establish 
discourse-givenness from the narrator?s perspec-
tive (Clancy, 1980)). Successful automatic 
coreference resolution would facilitate this analysis 
as well. 
3 Annotation of Referring Expressions in 
Narratives of Picture Books 
In order to provide descriptive statistics of refer-
ring expressions in our narratives of pictures books 
and to test the performance of coreference systems 
automatically in the future, we annotated 9 narra-
tives manually with coreference chains (3 narra-
tives for each of the 3 pictures books, with each 
narrative written by a different writer). Only ani-
mate entities, or characters in the stories, were con-
sidered. We used the MMAX2 annotation tool 
(M?ller and Strube, 2006). A coreference schema 
is available from the Heidelberg Text Corpus 
(HTC, Malaka and Zipf, 2000) sample directory 
included in the MMAX2 package. The HTC sche-
ma allows marking a mention in terms of the dis-
course entity or coreference chain it corresponds to, 
as well as ?np_form? (what type of (pro)nominal it 
is), ?grammatical_role? (subject/object/other) and 
?semantic_class? (abstract/human/physical ob-
ject/other). We imported the HTC schema to anno-
tate the mention level in terms of coreference, and 
also created a ?scene? level for our picture-book 
narratives.  
The narratives were annotated by the authors of 
this paper independently in the initial version, and 
with adjudication for the final version. As the ref-
erents were very clear in the narratives for the pic-
ture books, there was only one case of initial 
disagreement in the authors? coreference decisions. 
Table 2 shows statistics related to these 9 narra-
tives. 
 
Table 2. Descriptive Statistics for Each Narrative. 
 
The density of referring expressions is very high 
(~22% of tokens/words in a story are referring ex-
pressions). Densities are also consistent across nar-
ratives: Narrative #7, which was by far the longest 
one with 1109 words, also showed a very high 
density (24%). Numbers of coreference chains are 
also consistent within each target picture book re-
gardless of writer or narrative length: 8, 5, and 7 
for One Frog Too Many (Mayer and Mayer, 1975); 
13, 12, and 11 for Frog, Where Are You? (Mayer, 
1969); and 23, 21, and 26 for Frog Goes to Dinner 
(Mayer, 1974). Table 2 also shows that the longest 
3
chain contains 60 mentions, and the average chain 
has about 8 mentions. 
4 Performance of Coreference Resolution 
Systems on Narratives of Picture Books 
In computational linguistics, the increasing availa-
bility of annotated coreference corpora has led to 
developments in machine learning approaches to 
automatic coreference resolution (see Ng, 2010). 
The task of automatic NP coreference resolution is 
to determine ?which NPs in a text [?] refer to the 
same real-world entity? (Ng, 2010, p. 1396). Suc-
cessful coreference resolution often requires real-
world knowledge of public figures, entity relation-
ships, and aliases, beyond linguistic parameters 
such as number and gender features. 
In this paper, we have chosen two coreference 
resolution systems: Stanford?s Multi-Pass Sieve 
Coreference Resolution System (Lee et al, 2011) 
(henceforth, Stanford dcoref) and ARKref 
(O?Connor and Heilman, 2011). Stanford dcoref 
consists of an initial mention-detection module, the 
main coreference resolution module, and task-
specific post-processing. In this system, global in-
formation about the text is shared across mentions 
in the same cluster in the form of attributes such as 
gender and number. This system received the high-
est scores at a recent CoNLL shared task (Pradhan 
et al, 2011), which the authors attributed to the 
initial high-recall component (in mention detec-
tion) followed by high-precision classifiers in the 
coreference resolution sieves. ARKref is a syntac-
tically rich, rule-based within-document 
coreference system very similar to (the syntactic 
components of) Haghighi and Klein (2009). 
We analyzed in depth the performance of these 
systems on one of our narratives for Frog Goes to 
Dinner (Mayer, 1974). We expected automatic 
coreference resolution systems to show poorer per-
formance when applied to our written narratives 
than that reported in the literature, because most of 
these systems have been trained on newswire, blog, 
or conversation corpora, which ? though quite a 
heterogeneous set in themselves ? are not similar 
to our written narrative data. Some of the most 
noteworthy particularities of our written narrative 
collection include (a) fictional content, in which 
animals occur frequently and are greatly anthro-
pomorphized, (b) an imaginary target audience of a 
limited age range (six- to eight-year-olds), and (c) 
clear scene-by-scene demarcation in the writing 
process, with a new text input box for each new 
scene in a picture book. The first point, in particu-
lar, may limit the utility of named entity recogni-
tion (NER) and WordNet relations among 
nominals in the preprocessing steps prior to 
coreference resolution. As we discuss below, pre-
processing errors in parsing and NER did in fact 
contribute to coreference precision errors. 
Our written narratives had a lot of singleton 
mentions for secondary characters and plural com-
binations of characters. We thus evaluated the per-
formance based on the B3 measure proposed by 
Bagga and Baldwin (1998), rather than the link-
based MUC (Vilain et al, 1995).  
We computed the B3 with equal weighting for 
all mentions. Stanford dcoref achieved B3 scores of 
0.78 Precision, 0.43 Recall and 0.55 F1, while 
ARKref scores were 0.67 for precision, 0.45 for 
recall, and 0.54 for F1. Stanford dcoref includes a 
post-processing module in which singletons are 
removed, which partially contributes to the low 
recall score for the system. 
4.1 Qualitative analysis of coreference output 
In this section, we discuss the errors from both 
ARKref and Stanford dcoref in depth. The 
coreference outputs from both ARKref and Stan-
ford dcoref demonstrate that preprocessing errors 
can lead to errors downstream for coreference 
resolution. Misparsing is one of the serious issues. 
For example, in ARKref?s output for our sample 
narrative (for Frog Goes to Dinner), the third-
person singular verb waves in Billy waves goodbye 
(Scene 6) and Froggy waves goodbye (Scene 7) 
was misparsed as a plural nominal and thus a 
headword of a mention for a discourse entity, and 
these two instances were marked as coreferent. Lee 
et al also acknowledged misparsing as a major 
problem for Stanford dcoref. 
A few surprising errors in the ARKref output in-
clude (a) marking the woman and him in the same 
clause as coreferent despite the gender mismatch, 
and (b) leaving the lady as a singleton and starting 
a new coreference chain for her in the same clause. 
It is strange that the explicitly anaphoric pronoun 
mention did not lead ARKref to link it to the iden-
tified mention the lady. 
Other noteworthy errors common to both sys-
tems? outputs were the following: 
4
(1) inconsistent mention detection and 
coreference resolution for mentions of the frog 
character with Froggy; 
(2) failure to recognize cataphora in Without 
knowing Froggy?s in [his]i saxophone, [the saxo-
phone player]i tries to blow harder? and linking 
the pronoun to Froggy instead; 
(3) starting a new coreference chain at Scene 4 
at the mention of Billy when the referent (the boy) 
has been already introduced as Billy Smith in Scene 
1; 
(4) the same type of error for another character 
(the frog) at an indefinite NP a frog in She is so 
shocked that there is a frog in her salad. 
With regard to error (1), preprocessing results in 
the Stanford dcoref output reveal some NER errors 
in which Froggy was mislabeled as an ?organiza-
tion,? which, along with the absence of Froggy in 
the name gazetteer for the system (Lee et al, 2011), 
would lead to both precision and recall errors for 
Froggy, as we observed. 
Error (3) reveals the potential pitfall of overreli-
ance on headwords for mention/discourse-new de-
tection, which leads these systems to miss the 
internal structure to people?s names ? namely, 
[first name + last name] for the same person, 1 
which then can be re-mentioned using just the first 
name. Although in news articles and other formal 
writing it is typical to mention a person by the last 
name (e.g., Obama rather than Barack) as long as 
the referent is clear, stories, conversations, and 
other less formal genres would make more fre-
quent use of first names of individuals for re-
mention compared to other genres.  Because the 
importance of coreference resolution is not limited 
to formal writing, coreference resolution systems 
need to incorporate name-specific knowledge, ei-
ther in preprocessing stages such as parsing and 
NER or in coreference resolution after the prepro-
cessing. 
Error (4) is not as undesirable as the other ones: 
Even for a human annotator, it is more difficult to 
make a coreference decision for a case like this one, 
in which the fact that the salad-eating lady was 
shocked would come about similarly for any frog, 
not just Froggy. Although there does not seem to 
be a rule for classifying an indefinite NP as denot-
                                                          
1 Application to East Asian languages would need to adjust to 
the opposite ?family name + given name? sequence, often even 
in English transliteration (e.g., Kim Jong-il). 
ing a new entity,2 training on a large corpus would 
lead to such a tendency because indefinites usually 
do indicate discourse-newness introducing a new 
discourse referent. 
In another narrative for the same picture book, 
there were two definite NPs (the woman and the 
waiter) for which the definiteness was due to the 
visual availability of the referent in the scene or a 
bridging inference (restaurant ? waiter) rather than 
a previous mention. Definiteness may lead 
coreference systems to prefer assigning the men-
tion in question to an existing coreference chain 
rather than creating a new chain, but ARKref pro-
cessed both of these possibly misleading definite 
NPs successfully by creating a new coreference 
chain, and Stanford dcoref got one right and made 
a recall error for the other. On the other hand, re-
ferring to different secondary male characters simi-
larly as the man did lead to a spurious coreference 
chain linking all of these mentions. 
5 Conclusion and Future Directions 
With the NLP tools discussed above, possibilities 
abound for interesting research on narratives. 
Based on scene-based segmentation of narratives 
written for fixed target picture sequences, one can 
collect various kinds of linguistic and nonlinguistic 
data associated with the picture sequences and 
conduct regression analysis to see which factor has 
the most predictive value for linguistic variation 
such as Referential Type choice. Important factors 
include temporal and thematic (dis)continuity in 
the target content (McCoy and Strube, 1999; Vonk 
et al, 1992), and discourse salience factors (Prince, 
1981), for which we have collected measures in 
our previous work. 
Our Interval Effect finding lends support to 
McCoy and Strube?s (1999) intuition underlying 
their referring-expression generation system, for 
which they used reference time change in dis-
course as a major predictor of referential type. 
Gaining further insight into the impact of time 
change in content on referential choice in naturally 
occurring discourse can thus lead to a predictive 
model of referring expressions as well. 
In the future, we plan to use ?semantic_class? at-
tributes and features such as ANIMACY in the 
                                                          
2 According to Lee et al (2011), Stanford dcoref correctly 
recognizes coreference in appositive constructions with an 
indefinite NP after the first mention. 
5
HTC schema as our task-specific filters for select-
ing just story characters. Moreover, we plan to ex-
plore other state-of-the-art coreference systems 
such as CherryPicker (Rahman and Ng, 2009). The 
NLP tools and techniques discussed above can be 
applied to cross-document coreference resolution 
as well (see Bagga and Baldwin, 1998, for discus-
sion of a meta document), although training the 
systems for narratives like ours would involve 
much more manual annotation and supervision, 
particularly because different authors usually as-
sign different names to a given character. In order 
to limit the amount of manual annotation, unsuper-
vised methods for coreference resolution (Ng, 
2008; Poon and Domingos, 2008; Haghighi and 
Klein, 2007) could be used. This, however, would 
require a larger number of picture books and hu-
man-produced narratives. 
Coreference is far from a simple phenomenon, 
both for theory and application. Nevertheless, ul-
timately it would be desirable to improve the au-
tomatic coreference resolution systems in ways 
that reflect corpus-linguistic and psycholinguistic 
findings ? e.g., referential distance effects (Giv?n, 
1992), and the privileged status in memory of dis-
course entities in the immediately preceding clause 
(Clark and Sengul, 1979). The goal would be to 
represent as many of the interacting factors in ref-
erential choice as possible, with a weighting 
scheme or a ranking algorithm sensitive to these 
multiple factors. 
References  
Jennifer E. Arnold and Zenzi M. Griffin. 2007. The ef-
fect of additional characters on choice of referring 
expression: Everyone counts. Journal of Memory and 
Language, 56: 521-536.  
Amit Bagga and Breck Baldwin. 1998. Algorithms for 
scoring coreference chains. In Proceedings of LREC 
Workshop on Linguistic Coreference, pages 563-566. 
Susan Brennan. 1995. Centering attention in discourse. 
Language and Cognitive Processes, 10: 137-167.  
Patricia M. Clancy. 1980. Referential choice in English 
and Japanese narrative discourse. In Wallace L. 
Chafe, editor, The Pear Stories: Cognitive, Cultural, 
and Linguistic Aspects of Narrative Production. 
Ablex, Norwood, NJ. 
Herbert H. Clark and C. J. Sengul. 1979. In search of 
referents for nouns and pronouns. Memory and Cog-
nition, 7(1): 35-41. 
Thomas Giv?n. 1992. The grammar of referential co-
herence as mental processing instructions. Linguistics, 
30:5-55. 
Aria Haghighi and Dan Klein. 2007. Unsupervised 
coreference resolution in a nonparametric Bayesian 
model. In Proceedings of ACL 2007, pages 848?855. 
Aria Haghighi and Dan Klein. 2009. Simple coreference 
resolution with rich syntactic and semantic features. 
In Proceedings of EMNLP 2009, pages 1152?1161. 
Choonkyu Lee. 2012. Situation model and salience. The 
LSA 2012 Special Session on Information Structure 
and Discourse: In Memory of Ellen F. Prince. Port-
land, Oregon. 
Choonkyu Lee and Karin Stromswold. submitted. Situa-
tion model and accessibility: Referring expressions in 
narrative production. 
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathan-
ael Chambers, Mihai Surdeanu, and Dan Jurafsky. 
2011. Stanford?s multi-pass sieve coreference resolu-
tion system at the CoNLL-2011 Shared Task. In Pro-
ceedings of the CoNLL-2011 Shared Task, pages 28-
34. 
Rainer Malaka and Alexander Zipf. 2000. Deep Map: 
Challenging IT research in the framework of a tourist 
information system. In Daniel R. Fesenmaier, Stefan 
Klein, and Dimitrios Buhalis, editors, Information 
and Communication Technologies in Tourism 2000: 
Proceedings of the International Conference in Bar-
celona, Spain, pages 15-27. Springer, Wien. 
Mercer Mayer. 1969. Frog, Where Are You? Penguin 
Books, New York. 
Mercer Mayer. 1974. Frog Goes to Dinner. Penguin 
Books, New York. 
Mercer Mayer and Marianna Mayer. 1975. One Frog 
Too Many. Penguin Books, New York. 
Kathleen F. McCoy and Michael Strube. 1999. Taking 
time to structure discourse: Pronoun generation be-
yond accessibility. In Proceedings of the Twenty-
First Annual Conference of the Cognitive Science 
Society, pages 378-383. Lawrence Erlbaum Associ-
ates, Mahwah, NJ. 
Christoph M?ller and Michael Strube. 2006. Multi-level 
annotation of linguistic data with MMAX2. In Sabine 
Braun, Kurt Kohn, and Joybrato Mukherjee, editors, 
Corpus Technology and Language Pedagogy. New 
Resources, New Tools, New Methods, pages 197-214. 
Peter Lang, Frankfurt. 
Vincent Ng. 2009. Unsupervised models for coreference 
resolution. In Proceedings of EMNLP 2008, pages 
640-649. 
Vincent Ng. 2010. Supervised noun phrase coreference 
research: The first fifteen years. In Proceedings of 
ACL 2010, pages 1396-1411. 
Brendan O?Connor and Michael Heilman. 2011. 
ARKref is a Noun Phrase Coreference System. Web-
site at http://www.ark.cs.cmu.edu/ARKref/ 
6
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. 
In Proceedings of EMNLP 2008, pages 650-659. 
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, 
Martha Palmer, Ralph Weischedel, and Nianwen Xue. 
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of 
CoNLL 2011. 
Ellen Prince. 1981. Toward a taxonomy of given-new 
information. In Peter Cole, editor, Radical Pragmat-
ics, pages 223-256. Academic Press, New York. 
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of 
EMNLP 2009, pages 968-977. 
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings 
of the 6th Message Understanding Conference, pages 
45-52. 
Wietske Vonk, Lettica G. M. M. Hustinx, and Wim H. 
G. Simons. 1992. The use of referential expressions 
in structuring discourse. Language and Cognitive 
Processes, 7(3/4): 301-333. 
Rolf A. Zwaan. 1999. Situation models: The mental 
leap into imagined worlds. Current Directions in 
Psychological Science, 8(1):15-18. 
Rolf A. Zwaan and Gabriel A. Radvansky. 1998. Situa-
tion models in language comprehension and memory. 
Psychological Bulletin, 123(2):162-185. 
7
Workshop on Semantic Interpretation in an Actionable Context, NAACL-HLT 2012, pages 1?6,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Learning to Interpret Natural Language Instructions
Monica Babes?-Vroman+, James MacGlashan?, Ruoyuan Gao+, Kevin Winner?
Richard Adjogah?, Marie desJardins?, Michael Littman+ and Smaranda Muresan++
? Department of Computer Science and Electrical Engineering
University of Maryland, Baltimore County
+ Computer Science Department, Rutgers University
++ School of Communication and Information, Rutgers University
Abstract
This paper addresses the problem of training
an artificial agent to follow verbal instructions
representing high-level tasks using a set of in-
structions paired with demonstration traces of
appropriate behavior. From this data, a map-
ping from instructions to tasks is learned, en-
abling the agent to carry out new instructions
in novel environments.
1 Introduction
Learning to interpret language from a situated con-
text has become a topic of much interest in recent
years (Branavan et al, 2009; Branavan et al, 2010;
Branavan et al, 2011; Clarke et al, 2010; Chen
and Mooney, 2011; Vogel and Jurafsky, 2010; Gold-
wasser and Roth, 2011; Liang et al, 2011; Atrzi and
Zettlemoyer, 2011; Tellex et al, 2011). Instead of
using annotated training data consisting of sentences
and their corresponding logical forms (Zettlemoyer
and Collins, 2005; Kate and Mooney, 2006; Wong
and Mooney, 2007; Zettlemoyer and Collins, 2009;
Lu et al, 2008), most of these approaches leverage
non-linguistic information from a situated context
as their primary source of supervision. These ap-
proaches have been applied to various tasks such as
following navigational instructions (Vogel and Ju-
rafsky, 2010; Chen and Mooney, 2011; Tellex et
al., 2011), software control (Branavan et al, 2009;
Branavan et al, 2010), semantic parsing (Clarke et
al., 2010; Liang et al, 2011) and learning to play
games based on text (Branavan et al, 2011; Gold-
wasser and Roth, 2011).
In this paper, we present an approach to inter-
preting language instructions that describe complex
multipart tasks by learning from pairs of instruc-
tions and behavioral traces containing a sequence
of primitive actions that result in these instructions
being properly followed. We do not assume a one-
to-one mapping between instructions and primitive
actions. Our approach uses three main subcom-
ponents: (1) recognizing intentions from observed
behavior using variations of Inverse Reinforcement
Learning (IRL) methods; (2) translating instructions
to task specifications using Semantic Parsing (SP)
techniques; and (3) creating generalized task speci-
fications to match user intentions using probabilis-
tic Task Abstraction (TA) methods. We describe
our system architecture and a learning scenario. We
present preliminary results for a simplified version
of our system that uses a unigram language model,
minimal abstraction, and simple inverse reinforce-
ment learning.
Early work on grounded language learning used
features based on n-grams to represent the natural
language input (Branavan et al, 2009; Vogel and
Jurafsky, 2010). More recent methods have relied
on a richer representation of linguistic data, such as
syntactic dependency trees (Branavan et al, 2011;
Goldwasser and Roth, 2011) and semantic templates
(Tellex et al, 2011) to address the complexity of the
natural language input. Our approach uses a flexi-
ble framework that allows us to incorporate various
degrees of linguistic knowledge available at differ-
ent stages in the learning process (e.g., from depen-
dency relations to a full-fledged semantic model of
the domain learned during training).
1
2 System Architecture
We represent tasks using the Object-oriented
Markov Decision Process (OO-MDP) formal-
ism (Diuk et al, 2008), an extension of Markov De-
cision Processes (MDPs) to explicitly capture rela-
tionships between objects. Specifically, OO-MDPs
add a set of classes C, each with a set of attributes
TC . Each OO-MDP state is defined by an unordered
set of instantiated objects. In addition to these ob-
ject definitions, an OO-MDP also defines a set of
propositional functions that operate on objects. For
instance, we might have a propositional function
toyIn(toy, room) that operates on an object
belonging to class ?toy? and an object belonging to
class ?room,? returning true if the specified ?toy?
object is in the specific ?room? object. We extend
OO-MDPs to include a set of propositional function
classes (F) associating propositional functions that
describe similar properties. In the context of defin-
ing a task corresponding to a particular goal, an OO-
MDP defines a subset of states ? ? S called ter-
mination states that end an action sequence and that
need to be favored by the task?s reward function.
Example Domain. To illustrate our approach, we
present a simple domain called Cleanup World, a 2D
grid world defined by various rooms that are con-
nected by open doorways and can contain various
objects (toys) that the agent can push around to dif-
ferent positions in the world. The Cleanup World
domain can be represented as an OO-MDP with four
object classes: agent, room, doorway, and toy, and a
set of propositional functions that specify whether
a toy is a specific shape (such as isStar(toy)),
the color of a room (such as isGreen(room)),
whether a toy is in a specific room (toyIn(toy,
room)), and whether an agent is in a specific room
(agentIn(room)). These functions belong to
shape, color, toy position or agent position classes.
2.1 Interaction among IRL, SP and TA
The training data for the overall system is a set of
pairs of verbal instructions and behavior. For exam-
ple, one of these pairs could be the instruction Push
the star to the green room with a demonstration of
the task being accomplished in a specific environ-
ment containing various toys and rooms of different
colors. We assume the availability of a set of fea-
tures for each state represented using the OO-MDP
propositional functions descibed previously. These
features play an important role in defining the tasks
to be learned. For example, a robot being taught
to move furniture around would have information
about whether or not it is currently carrying a piece
of furniture, what piece of furniture it needs to be
moving, which room it is currently in, which room
contains each piece of furniture, etc. We present
briefly the three components of our system (IRL, SP
and TA) and how they interact with each other dur-
ing learning.
Inverse Reinforcement Learning. Inverse Re-
inforcement Learning (Abbeel and Ng, 2004) ad-
dresses the task of learning a reward function from
demonstrations of expert behavior and information
about the state-transition function. Recently, more
data-efficient IRL methods have been proposed,
including the Maximum Likelihood Inverse Rein-
forcement Learning (Babes?-Vroman et al, 2011)
or MLIRL approach, which our system builds on.
Given even a small number of trajectories, MLIRL
finds a weighting of the state features that (locally)
maximizes the probability of these trajectories. In
our system, these state features consist of one of the
sets of propositional functions provided by the TA
component. For a given task and a set of sets of
state features, MLIRL evaluates the feature sets and
returns to the TA component its assessment of the
probabilities of the various sets.
Semantic Parsing. To address the problem of
mapping instructions to semantic parses, we use
a constraint-based grammar formalism, Lexical-
ized Well-Founded Grammar (LWFG), which has
been shown to balance expressiveness with practical
learnability results (Muresan and Rambow, 2007;
Muresan, 2011). In LWFG, each string is associ-
ated with a syntactic-semantic representation, and
the grammar rules have two types of constraints: one
for semantic composition (?c) and one for seman-
tic interpretation (?i). The semantic interpretation
constraints, ?i, provide access to a semantic model
(domain knowledge) during parsing. In the absence
of a semantic model, however, the LWFG learnabil-
ity result still holds. This fact is important if our
agent is assumed to start with no knowledge of the
task and domain. LWFG uses an ontology-based se-
mantic representation, which is a logical form repre-
2
sented as a conjunction of atomic predicates. For ex-
ample, the representation of the phrase green room
is ?X1.is=green, X.P1 = X1, X.isa=room?. The
semantic representation specifies two concepts?
green and room?connected through a property
that can be uninstantiated in the absence of a seman-
tic model, or instantiated via the ?i constraints to
the property name (e.g, color) if such a model is
present.
During the learning phase, the SP component, us-
ing an LWFG grammar that is learned offline, pro-
vides to TA the logical forms (i.e., the semantic
parses, or the unlabeled dependency parses if no se-
mantic model is given) for each verbal instruction.
For example, for the instruction Move the chair into
the green room, the semantic parser knows initially
that move is a verb, chair and room are nouns, and
green is an adjective. It also has grammar rules of
the form S ? Verb NP PP: ?c1,?i1,1 but it has
no knowledge of what these words mean (that is, to
which concepts they map in the domain model). For
this instruction, the LWFG parser returns the logical
form:
?(X1.isa=move, X1.Arg1= X2)move,
(X2.det=the)the, (X2.isa=chair)chair,
(X1.P1 = X3, P2.isa=into)into, (X3.det=the)the,
(X4.isa=green, X3.P2 = X2)green,
(X3.isa=room)room?.
The subscripts for each atomic predicate in-
dicate the word to which that predicate corre-
sponds. This logical form corresponds to the
simplified logical form move(chair1,room1),
P1(room1,green), where predicate P1 is unin-
stantiated. A key advantage of this framework is that
the LWFG parser has access to the domain (seman-
tic) model via ?i constraints. As a result, when TA
provides feedback about domain-specific meanings
(i.e., groundings), the parser can incorporate those
mappings via the ?i constraints (e.g., move might
map to the predicate blockToRoom with a certain
probability).
Task Abstraction. The termination conditions
for an OO-MDP task can be defined in terms of the
propositional functions. For example, the Cleanup
1For readability, we show here just the context-free back-
bone, without the augmented nonterminals or constraints.
World domain might include a task that requires the
agent to put a specific toy (t1) in a specific room
(r1). In this case, the termination states would be
defined by states that satisfy toyIn(t1, r1) and the
reward function would be defined as Ra(s, s?) =
{1 : toyIn(ts
?
1 , r
s?
1 );?1 : otherwise}. However,
such a task definition is overly specific and cannot
be evaluated in a new environment that contains dif-
ferent objects. To remove this limitation, we define
abstract task descriptions using parametric lifted re-
ward and termination functions. A parametric lifted
reward function is a first-order logic expression in
which the propositional functions defining the re-
ward can be selected as parameters. This repre-
sentation allows much more general tasks to be de-
fined; these tasks can be evaluated in any environ-
ment that contains the necessary object classes. For
instance, the reward function for an abstract task
that encourages an agent to take a toy of a certain
shape to a room of a certain color (resulting in a re-
ward of 1) would be represented as Ra(s, s?) = {1 :
?ts??toy?rs??roomP1(t) ? P2(r) ? toyIn(t, r);?1 :
otherwise}, where P1 is a propositional function
that operates on toy objects and P2 is a propositional
function that operates on room objects. An analo-
gous definition can be made for termination condi-
tions. Given the logical forms provided by SP, TA
finds candidate tasks that might match each logi-
cal form, along with a set of possible groundings
of those tasks. A grounding of an abstract task is
the set of propositional functions to be applied to
the specific objects in a given training instance. TA
then passes these grounded propositional functions
as the features to use in IRL. (If there are no can-
didate tasks, then it will pass all grounded proposi-
tional functions of the OO-MDP to IRL.) When IRL
returns a reward function for these possible ground-
ings and their likelihoods of representing the true re-
ward function, TA determines whether any abstract
tasks it has defined might match. If not, TA will
either create a new abstract task that is consistent
with the received reward functions or it will modify
one of its existing definitions if doing so does not
require significant changes. With IRL indicating the
intended goal of a trace and with the abstract task in-
dicating relevant parameters, TA can then inform SP
of the task/domain specific meanings for the logical
forms.
3
A Learning from Scratch Scenario. Our sys-
tem is trained using a set of sentence?trajectory
pairs ((S1, T1), ..., (SN , TN )). Initially, the sys-
tem does not know what any of the words mean
and there are no pre-existing abstract tasks. Let?s
assume that S1 is Push the star into the green
room.This sentence is first processed by the SP com-
ponent, yielding the following logical forms: L1
is push(star1, room1), amod(room1, green) and
L2 is push(star1), amod(room1, green),
into(star1, room1). These logical forms and
their likelihoods are passed to the TA compo-
nent, and TA induces incomplete abstract tasks,
which define only the number and kinds of ob-
jects that are relevant to the corresponding re-
ward function. TA can send to IRL a set of
features involving these objects, together with T1,
the demonstration attached to S1. This set of
features might include: agentTouchToy(t1),
toyIn(t1, r1), toyIn(t1, r2), agentIn(r1). IRL
sends back a weighting of the features, and TA
can select the subset of features that have the
highest weights (e.g, (1.91, toyIn(t1, r1)), (1.12,
agentTouchToy(t1)), (0.80, agentIn(r1)). Us-
ing information from SP and IRL, TA can now create
a new abstract task, perhaps called blockToRoom,
adjust the probabilities of the logical forms based on
the relevant features obtained from IRL, and send
these probabilities back to SP, enabling it to adjust
its semantic model.
The entire system proceeds iteratively. While it is
designed, not all features are fully implemented to
be able to report experimental results. In the next
section, we present a simplified version of our sys-
tem and show preliminary results.
3 A Simplified Model and Experiments
In this section, we present a simplified version of our
system with a unigram language model, inverse rein-
forcement learning and minimal abstraction. We call
this version Model 0. The input to Model 0 is a set
of verbal instructions paired with demonstrations of
appropriate behavior. It uses an EM-style algorithm
(Dempster et al, 1977) to estimate the probability
distribution of words conditioned on reward func-
tions (the parameters). With this information, when
the system receives a new command, it can behave
in a way that maximizes its reward given the pos-
terior probabilities of the possible reward functions
given the words.
Algorithm 1 shows our EM-style Model 0. For
all possible reward?demonstration pairs, the E-step
of EM estimates zji = Pr(Rj |(Si, Ti)), the prob-
ability that reward function Rj produced sentence-
trajectory pair (Si, Ti), This estimate is given by the
equation below:
zji = Pr(Rj |(Si, Ti)) =
Pr(Rj)
Pr(Si, Ti)
Pr((Si, Ti)|Rj)
=
Pr(Rj)
Pr(Si, Ti)
Pr(Ti|Rj)
?
wk?Si
Pr(wk|Rj)
where Si is the ith sentence, Ti is the trajectory
demonstrated for verbal command Si, and wk is an
element in the set of all possible words (vocabulary).
If the reward functions Rj are known ahead of time,
Pr(Ti|Rj) can be obtained directly by solving the
MDP and estimating the probability of trajectory Ti
under a Boltzmann policy with respect to Rj . If the
Rjs are not known, EM can estimate them by run-
ning IRL during the M-step (Babes?-Vroman et al,
2011).
The M-step in Algorithm 1 uses the current esti-
mates of zji to further refine the probabilities xkj =
Pr(wk|Rj):
xkj = Pr(wk|Rj) =
1
X
?wk?Si Pr(Rj |Si) + 
?iN(Si)zji + 
where  is a smoothing parameter, X is a normalizing
factor andN(Si) is the number of words in sentence
Si.
To illustrate our Model 0 performance, we se-
lected as training data six sentences for two tasks
(three sentences for each task) from a dataset we
have collected using Amazon Mechanical Turk for
the Cleanup Domain. We show the training data
in Figure 1. We obtained the reward function for
each task using MLIRL, computed the Pr(Ti|Rj),
then ran Algorithm 1 and obtained the parameters
Pr(wk|Rj). After this training process, we pre-
sented the agent with a new task. She is given the
instruction SN : Go to green room. and a starting
state, somewhere in the same grid. Using parame-
ters Pr(wk|Rj), the agent can estimate:
4
Algorithm 1 EM-style Model 0
Input: Demonstrations {(S1, T1), ..., (SN , TN )},
number of reward functions J , size of vocabulary
K.
Initialize: x11, . . . , xJK , randomly.
repeat
E Step: Compute
zji =
Pr(Rj)
Pr(Si,Ti)
Pr(Ti|Rj)
?
wk?Si xkj .
M step: Compute
xkj = 1X
?wk?Si Pr(Rj |Si)+
?iN(Si)zji+
.
until target number of iterations completed.
Pr(SN |R1) =
?
wk?SN Pr(wk|R1) = 8.6 ? 10
?7,
Pr(SN |R2) =
?
wk?SN Pr(wk|R2) = 4.1 ? 10
?4,
and choose the optimal policy corresponding to re-
ward R2, thus successfully carrying out the task.
Note that R1 and R2 corresponded to the two tar-
get tasks, but this mapping was determined by EM.
We illustrate the limitation of the unigram model by
telling the trained agent to Go with the star to green,
(we label this sentence S?N ). Using the learned
parameters, the agent computes the following esti-
mates:
Pr(S?N |R1) =
?
wk?S?N
Pr(wk|R1) = 8.25? 10?7,
Pr(S?N |R2) =
?
wk?S?N
Pr(wk|R2) = 2.10? 10?5.
The agent wrongly chooses reward R2 and goes to
the green room instead of taking the star to the green
room. The problem with the unigram model in this
case is that it gives too much weight to word fre-
quencies (in this case go) without taking into ac-
count what the words mean or how they are used
in the context of the sentence. Using the system de-
scribed in Section 2, we can address these problems
and also move towards more complex scenarios.
4 Conclusions and Future Work
We have presented a three-component architecture
for interpreting natural language instructions, where
the learner has access to natural language input and
demonstrations of appropriate behavior. Our future
work includes fully implementing the system to be
able to build abstract tasks from language informa-
tion and feature relevance.
Figure 1: Training data for 2 tasks: Taking the star to the
green room (left) and Going to the green room (right).
Acknowledgments
The authors acknowledge the support of the Na-
tional Science Foundation (collaborative grant IIS-
00006577 and IIS-1065195). The authors thank the
anonymous reviewers for their feedback. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this paper are those of the authors, and
do not necessarily reflect the views of the funding
organization.
References
Pieter Abbeel and Andrew Ng. 2004. Apprenticeship
learning via inverse reinforcement learning. In Pro-
ceedings of the Twenty-First International Conference
in Machine Learning (ICML 2004).
Yoav Atrzi and Luke Zettlemoyer. 2011. Bootstrapping
semantic parsers for conversations. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing.
Monica Babes?-Vroman, Vukosi Marivate, Kaushik Sub-
ramanian, and Michael Littman. 2011. Apprentice-
ship learning about multiple intentions. In Proceed-
ings of the Twenty Eighth International Conference on
Machine Learning (ICML 2011).
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
5
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ?09.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: Learning
to map high-level instructions to commands. In Asso-
ciation for Computational Linguistics (ACL 2010).
S.R.K. Branavan, David Silver, and Regina Barzilay.
2011. Learning to win by reading manuals in a monte-
carlo framework. In Association for Computational
Linguistics (ACL 2011).
David L. Chen and Raymond J. Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of the 25th AAAI
Conference on Artificial Intelligence (AAAI-2011).,
pages 859?865.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the Association
for Computational Linguistics (ACL 2010).
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
Carlos Diuk, Andre Cohen, and Michael Littman. 2008.
An object-oriented representation for efficient rein-
forcement learning. In Proceedings of the Twenty-
Fifth International Conference on Machine Learning
(ICML-08).
Dan Goldwasser and Dan Roth. 2011. Learning from
natural instructions. In Proceedings of the Twenty-
Second International Joint Conference on Artificial In-
telligence.
Rohit J. Kate and Raymond J. Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, ACL-
44.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Association for Computational Linguistics (ACL
2011).
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?08.
Smaranda Muresan and Owen Rambow. 2007. Grammar
approximation by representative sublanguage: A new
model for language learning. In Proceedings of ACL.
Smaranda Muresan. 2011. Learning for deep language
understanding. In Proceedings of IJCAI-11.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew Walter, Ashis Gopal Banerjee, Seth Teller,
and Nicholas Roy. 2011. Understanding natural
language commands for robotic navigation and mo-
bile manipulation. In Proceedings of the Twenty-Fifth
AAAI Conference on Articifical Intelligence.
Adam Vogel and Dan Jurafsky. 2010. Learning to follow
navigational directions. In Association for Computa-
tional Linguistics (ACL 2010).
Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2007).
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of UAI-05.
Luke Zettlemoyer and Michael Collins. 2009. Learning
context-dependent mappings from sentences to logical
form. In Proceedings of the Association for Computa-
tional Linguistics (ACL?09).
6
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 54?60,
Baltimore, Maryland USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Surprisal as a Predictor of Essay Quality
Gaurav Kharkwal
Department of Psychology
Center for Cognitive Science
Rutgers University, New Brunswick
gaurav.kharkwal@gmail.com
Smaranda Muresan
Department of Computer Science
Center for Computational Learning Systems
Columbia University
smara@ccls.columbia.edu
Abstract
Modern automated essay scoring systems
rely on identifying linguistically-relevant
features to estimate essay quality. This
paper attempts to bridge work in psy-
cholinguistics and natural language pro-
cessing by proposing sentence process-
ing complexity as a feature for automated
essay scoring, in the context of English
as a Foreign Language (EFL). To quan-
tify processing complexity we used a psy-
cholinguistic model called surprisal the-
ory. First, we investigated whether es-
says? average surprisal values decrease
with EFL training. Preliminary results
seem to support this idea. Second, we in-
vestigated whether surprisal can be effec-
tive as a predictor of essay quality. The
results indicate an inverse correlation be-
tween surprisal and essay scores. Overall,
the results are promising and warrant fur-
ther investigation on the usability of sur-
prisal for essay scoring.
1 Introduction
Standardized testing continues to be an integral
part of modern-day education, and an important
area of research in educational technologies is the
development of tools and methodologies to facil-
itate automated evaluation of standardized tests.
Unlike multiple-choice questions, automated eval-
uation of essays presents a particular challenge.
The specific issue is the identification of a suitable
evaluation rubric that can encompass the broad
range of responses that may be received.
Unsurprisingly then, much emphasis has been
placed on the development of Automated Essay
Scoring (henceforth, AES) systems. Notable AES
systems include Project Essay Grade (Page, 1966;
Ajay et al., 1973), ETS?s e-rater
R
? (Burstein et al.,
1998; Attali and Burstein, 2006), Intelligent Es-
say Assessor
TM
(Landauer et al., 2003), BETSY
(Rudner and Liang, 2002), and Vantage Learn-
ing?s IntelliMetric
TM
(Elliot, 2003). The common
thread in most modern AES systems is the iden-
tification of various observable linguistic features,
and the development of computational models that
combine those features for essay evaluation.
One aspect of an essay?s quality that almost all
AES systems do not yet fully capture is sentence
processing complexity. The ability to clearly and
concisely convey information without requiring
undue effort on the part of the reader is one hall-
mark of good writing. Decades of behavioral re-
search on language comprehension has suggested
that some sentence structures are harder to com-
prehend than others. For example, passive sen-
tences, such as the girl was pushed by the boy,
are known to be harder to process than semanti-
cally equivalent active sentences, such as the boy
pushed the girl (Slobin, 1966; Forster and Ol-
brei, 1972; Davison and Lutz, 1985; Kharkwal and
Stromswold, 2013). Thus, it is likely that the over-
all processing complexity of the sentence struc-
tures used in an essay could influence its perceived
quality.
One reason why sentence processing complex-
ity has not yet been fully utilized is the lack
of a suitable way of quantifying it. This paper
proposes the use of a psycholinguistic model of
sentence comprehension called surprisal theory
(Hale, 2001; Levy, 2008) to quantify sentence pro-
cessing complexity. The rest of the paper is orga-
nized as follows. Section 2 describes the surprisal
theory, and discusses its applicability in modeling
sentence processing complexity. Section 3 details
our investigation on whether essays? average sur-
prisal values decrease following English as a For-
eign Language training. Section 4 presents a study
where we investigated whether surprisal can be ef-
fective as a predictor of essay quality. Lastly, Sec-
54
The judge who angered the criminal slammed the gavel Mean
5.64 6.94 6.93 11.60 2.32 9.19 16.92 1.94 4.68 7.35
The judge who the criminal angered slammed the gavel Mean
5.64 6.94 6.93 4.20 9.21 13.73 16.65 2.21 4.69 7.80
Table 1: Surprisal values of two example relative-clause sentences. The values were computed using a
top-down parser by Roark et al. (2009) trained on the Wall Street Journal corpus.
tion 5 concludes the paper.
2 Surprisal Theory
The surprisal theory (Hale, 2001; Levy, 2008)
estimates the word-level processing complexity
as the negative log-probability of a word given
the preceding context (usually, preceding syntac-
tic context). That is:
Complexity(w
i
) ? ? logP (w
i
|w
1...i?1
, CONTEXT)
Essentially, the surprisal model measures pro-
cessing complexity at a word as a function of
how unexpected the word is in its context. Sur-
prisal is minimized (i.e. approaches zero) when a
word must appear in a given context (i.e., when
P (w
i
|w
1...i?1
, CONTEXT) = 1), and approaches
infinity as a word becomes less and less likely.
Crucially, the surprisal theory differs from n-gram
based approaches by using an underlying language
model which includes a lexicon and a syntactic
grammar (the language model is usually a Prob-
abilistic Context-Free Grammar, but not restricted
to it).
To better understand surprisal, consider the fol-
lowing two example sentences:
(1) The judge who angered the criminal slammed
the gavel.
(2) The judge who the criminal angered slammed
the gavel.
Both sentences are center-embedded relative
clause sentences that differ in whether the subject
or the object is extracted from the relative clause.
Critically, they both share the same words differ-
ing only in their relative order. Behavioral stud-
ies have found that object-extracted relative clause
sentences (2) are harder to process than subject-
extracted relative clause sentences (1) (King and
Just, 1991; Gordon et al., 2001; Grodner and
Gibson, 2005; Staub, 2010; Traxler et al., 2002;
Stromswold et al., 1996). The surprisal values at
each word position of the two example sentences
are shown in Table 1.
As we can see from Table 1, the mean surprisal
value is greater for the object-extracted relative
clause sentence. Hence, the surprisal theory cor-
rectly predicts greater processing cost for that sen-
tence. Furthermore, it allows for a finer-grained
analysis of where the processing cost might occur,
specifically at the onset of the relative clause (the)
and the end (angered). Other differences, such as
greatest difficulty at the main verb are shared with
the subject-extracted relative clause, and are plau-
sible because both sentences are center-embedded.
These predictions are consistent with patterns ob-
served in behavioral studies (Staub, 2010).
In addition to relative clauses, the surprisal the-
ory has been used to model various other behav-
ioral findings (Levy, 2008; Levy and Keller, 2012).
Moreover, corpora analyses examining surprisal?s
effectiveness revealed a high correlation between
word-level suprisal values and the corresponding
reading times, which act as a proxy for processing
difficulties (Demberg and Keller, 2008; Boston et
al., 2008; Frank, 2009; Roark et al., 2009).
Thus, the surprisal theory presents itself as an
effective means of quantifying processing com-
plexity of sentences, and words within them. Next,
we discuss a series of evaluations that we per-
formed to determine whether surprisal values re-
flect quality of written essays.
3 Experiment 1
In the first experiment, we investigate whether an
essay?s mean surprisal value decreases after suit-
able English as a Foreign Language (EFL) educa-
tional training. Here, we make the assumption that
EFL training improves a person?s overall writing
quality, and that surprisal value acts as a proxy for
writing quality.
55
Total Syntactic Lexical
Topic Term Mean SD Mean SD Mean SD
Analysis
Term 1 6.34 3.32 2.37 1.86 3.97 3.24
Term 2 6.28 3.30 2.34 1.85 3.94 3.23
Arg.
Term 1 6.24 3.29 2.34 1.85 3.90 3.23
Term 2 6.15 3.36 2.28 1.85 3.87 3.24
Table 2: Means and standard deviations of total surprisal, syntactic surprisal, and lexical surprisal for
Analysis and Argumentation essays
3.1 Corpus
We used the Uppsala Student English corpus pro-
vided by the Department of English at Uppsala
University (Axelsson, 2000). The corpus con-
tained 1,489 essays written by 440 Swedish uni-
versity students of English at three different lev-
els. The total number of words was 1,221,265,
and the average length of an essay was 820 words.
The essays were written on a broad range of top-
ics, and their lengths were limited to be between
700-800 words. The topics were divided based on
student education level, with 5 essay topics written
by first-term students, 8 by second-term students,
and 1 by third-term students.
To facilitate comparison, we chose similar top-
ics from the first and second-term sets. We thus
had two sets of essays. The first set consisted
of Analysis essays which are written as a causal
analysis of some topic, such as ?television and its
impact on people.? The second set consisted of
Argumentation essays where students argue for or
against a topic or viewpoint. We further imposed
the restriction that only essays written by the same
student in both terms were selected. That is, if a
student wrote an essay on a chosen topic in the first
term, but not the second, or vice-versa, their essay
was not considered. This selection resulted in 38
pairs of Analysis essays and 20 pairs of Argumen-
tation essays across the two terms, for a total of
116 essays.
3.2 Computing Surprisal
We computed the surprisal value of each word
in an essay by using a broad-coverage top-down
parser developed by Roark et al. (2009). The
parser was trained on sections 02-24 of the Wall
Street Journal corpus of the Penn Treebank (Mar-
cus et al., 1993). Essentially, the parser com-
putes a word?s surprisal value as the negative log-
probability of the word given the preceding words
using prefix probabilities. Thus, the surprisal
value of the i
th
word is calculated as:
SURPRISAL(w
i
) = ? log
PrefixProb(w
1...i
)
PrefixProb(w
1...i?1
)
Moreover, it decomposes each word?s surprisal
value into two components: syntactic surprisal
and lexical surprisal. Syntactic surprisal measures
the degree of unexpectedness of the part-of-speech
category of a word given the word?s sentential con-
text. On the other hand, lexical surprisal measures
the degree of unexpectedness of the word itself
given its sentential context and a part-of-speech
category.
For every essay, we measured the syntactic, lex-
ical, and total (i.e., summed) surprisal values for
each word. Subsequently, the averages of the three
surprisal values were computed for every essay,
and those means were used for further analyses.
Henceforth, surprisal values for an essay refers to
their mean surprisal values.
3.3 Results and Discussion
Table 2 reports the means and standard deviations
of the three surprisal measures of the essays.
1
As
can be seen, there seems to be a reduction in all
three surprisal values across terms, and second
term essays tend to have a lower mean surprisal
than first term essays. To analyze these differ-
ences, we computed linear mixed-effect regression
models (Baayen, 2008; Baayen et al., 2008) for the
two essay categories. Each model included Term
as a fixed factor and Student as a random intercept.
While our analysis shows that essays in the sec-
ond term have an overall mean surprisal values
less than than essays in the first term, these differ-
ences were not statistically significant. There are a
number of factors that could have influenced these
results. We made an assumption that only a single
term of EFL training could significantly improve
1
It is important to note here that these means and standard
deviations are computed on mean surprisal values per essays
and not surprisal values at individual words.
56
Total Syntactic Lexical
Score Mean SD Mean SD Mean SD
Low 6.22 0.39 2.46 0.22 3.76 0.29
Medium 6.10 0.34 2.35 0.17 3.75 0.26
High 6.09 0.28 2.27 0.14 3.82 0.24
Table 3: Means and standard deviations of total surprisal, syntactic surprisal, and lexical surprisal for the
three different essay score levels
essay quality, and hence decrease overall surprisal
values of essays. However, it is likely that a sin-
gle term of training is insufficient, and perhaps the
lack of a significant difference between surprisal
values reflects no improvement in essay quality
across the two terms. Unfortunately, these essays
were not previously scored, and thus we were un-
able to assess whether essay quality improved over
terms.
4 Experiment 2
In the second experiment, we directly examined
whether surprisal values are related to essay qual-
ity by using a dataset of pre-scored essays.
4.1 Corpus
For this experiment, we used a corpus of essays
written by non-native English speakers. These es-
says are a part of the Educational Testing Service?s
corpus which was used in the first shared task in
Native Language Identification (Blanchard et al.,
2013)
2
.
The corpus consisted of 12,100 essays, with a
total number of 4,142,162 words, and the average
length of an essay was 342 words. The essays
were on 8 separate topics, which broadly asked
students to argue for or against a topic or a view-
point. Each essay was labeled with an English lan-
guage proficiency level (High, Medium, or Low)
based on the judgments of human assessment spe-
cialists. The distribution of the essays per score-
category was: Low = 1,325; Medium = 6,533; and
High = 4,172. In order to ensure an equitable com-
parison, and to balance each group, we decided to
choose 1,325 essays per score-category, for a total
of 3,975 essays.
4.2 Computing Surprisal
As in Experiment 1, for every essay we measured
the syntactic, lexical, and total surprisal values for
each word. We computed the averages of the three
2
Copyright
c
? 2014 ETS. www.ets.org
surprisal values, and used those means for further
analysis.
4.3 Results and Discussion
Table 3 reports the means and standard deviations
of the three surprisal values for every essay per
score-category. We analyzed the differences be-
tween the means using linear mixed-effects regres-
sion models (Baayen, 2008; Baayen et al., 2008).
Essay Score was treated as a fixed effect and Es-
say Topic was included as a random intercept. The
results indicate that Low-scoring essays had a sig-
nificantly greater mean total surprisal value than
Medium or High-scoring essays. However, the dif-
ference in mean total surprisal values for Medium
and High-scoring essays was not significant. On
the other hand, for syntactic and lexical surprisal,
the means for all three essay score levels were sig-
nificantly different from one another.
We further evaluated the three surprisal values
by performing a correlation test between them and
the essay scores. Table 4 reports the output of the
correlation tests. All three surprisal values were
found to be significantly inversely correlated with
essay scores. However, only syntactic surprisal
obtained a correlation coefficient of a sufficiently
large magnitude of 0.39.
A similar evaluation was performed by Attali
and Burstein (2006) in their evaluation of the
features used in ETS?s e-rater system. Interest-
ingly, the magnitude of the correlation coefficient
for syntactic surprisal reported here is within the
range of coefficients corresponding to e-rater?s
features when they were correlated with TOEFL
essay scores (see Attali and Burstein, 2006, Table
2). Granted, a direct comparison between coef-
ficients is not recommended as the datasets used
were different, such a finding is still promising.
Overall, the results shed a positive light on the use
of surprisal, specifically syntactic surprisal, as a
feature for automated essay scoring.
Despite the promising pattern of our results,
57
Dep Var ? t-value p-value
Total -.15 -9.87 < .001
Syntactic -.39 -26.53 < .001
Lexical .08 5.35 < .001
Table 4: Pearson?s R coefficients between the three surprisal values and the essay scores
they must be taken with a grain of salt. The dataset
that we used did not contain the actual scores of
the essays, and we had to work with broad classi-
fications of essay scores into Low, Medium, and
High score levels. A possible avenue of future
work is to test whether these results hold when us-
ing finer-grain essays scores.
5 Conclusions and Future Work
We proposed the use of the surprisal theory to
quantify sentence processing complexity for use
as a feature in essay scoring. The results are en-
couraging, and warrant further evaluation of sur-
prisal?s effectiveness in determining essay quality.
One point of concern is that the relationship
between mean surprisal values and essay scores
is likely to vary depending on the general qual-
ity of the essays. Here, we used a corpus of es-
says written by non-native English speakers, and
as such, these essays are bound to be of a lower
overall quality than essays written by native En-
glish speakers. For example, consider the fol-
lowing, somewhat questionable, sentences chosen
from the subset of essays having a High score:
(3) Some people might think that traveling in a
group led by a tour guide is a good way.
(4) This is possible only if person understands
ideas and concept.
(5) It is an important decision, how to plan your
syllabus.
These examples suggest that even high-scoring
essays written by non-native English speakers may
not necessarily be flawless, and as such, gram-
matical acceptability may play a crucial role in
determining their overall quality. Therefore, it
is possible that for lower-quality essays, high
surprisal values reflect the presence of gram-
matical errors. On the other hand, for better-
written essays, moderate-to-high surprisal values
may reflect structural variability, which arguably
is preferable to monotonous essays with simpler
sentence structures. Thus, it is likely that the re-
lation between surprisal values and essay scores
depends on the overall quality of the essays in
general. For an equitable evaluation, further tests
will need to determine surprisal?s efficacy over a
broader range of essays.
Another critical point is the choice of corpus
used to compute surprisal. Whatever choice is
made essentially dictates and constrains the gram-
mar of the language under consideration. Here, we
used the WSJ corpus and, thus, implicitly made an
assumption about the underlying language model.
Therefore, in our case, a good essay, i.e. one with
a lower surprisal score, would be one which is
stylistically closer to the WSJ corpus. Future work
will need to investigate the role played by the un-
derlying language model, with special emphasis
on evaluating language models that are specific to
the task at hand. In other words, it would be in-
teresting to compare a surprisal model that is built
using a collection of previous essays with a sur-
prisal model that uses a broader language model.
Lastly, our evaluations were aimed at determin-
ing whether surprisal can be an effective predictor
of essay quality. Further tests will need to evaluate
how well the measure contributes to essay score
predictions when compared to related approaches
that rely on non-syntactic language models, such
as n-grams. Moreover, future work will need to
determine whether adding mean surprisal values to
an AES system results in a performance improve-
ment.
Acknowledgments
We are indebted to ETS for sharing their data with
us, and supporting us through this project. This
work would not be possible without their help. We
are also thankful to the reviewers for their help-
ful and encouraging comments. The opinions set
forth in this publication are those of the author(s)
and not ETS.
References
Helen B. Ajay, P. I. Tillett, and Ellis B. Page. 1973.
Analysis of essays by computer (AEC-II). Final
58
Report to the National Center for Educational Re-
search and Development for Project, (8-0101).
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater
R
? v. 2. The Journal of Technol-
ogy, Learning and Assessment, 4(3).
Margareta W. Axelsson. 2000. USE ? the Uppsala Stu-
dent English corpus: An instrument for needs anal-
ysis. ICAME Journal, 24:155?157.
Harald R. Baayen, Douglas J. Davidson, and Dou-
glas M. Bates. 2008. Mixed-effects modeling with
crossed random effects for subjects and items. Jour-
nal of memory and language, 59(4):390?412.
Harald R. Baayen. 2008. Analyzing linguistic data:
A practical introduction to statistics using R. Cam-
bridge University Press.
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A corpus of non-native english. Edu-
cational Testing Service.
Marisa Boston, John Hale, Reinhold Kliegl, Umesh
Patil, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using
the potsdam sentence corpus. Journal of Eye Move-
ment Research, 2(1):1?12.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
and Martin Chodorow. 1998. Enriching automated
essay scoring using discourse marking. In Proceed-
ings of the Workshop on Discourse Relations and
Discourse Marking, pages 206?210.
Alice Davison and Richard Lutz. 1985. Measuring
syntactic complexity relative to discourse context.
In David R. Dowty, Lauri Karttunen, and Arnold M.
Zwicky, editors, Natural language parsing: Psy-
chological, computational, and theoretical perspec-
tives, pages 26?66. Cambridge: Cambridge Univer-
sity Press.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193?210.
Scott Elliot, 2003. Automated essay scoring: a cross
disciplinary approach, chapter IntelliMetric: From
here to validity, pages 71?86. Lawrence Erlbaum
Associates, Mahwah, NJ.
Kenneth Forster and Ilmar Olbrei. 1972. Seman-
tic heuristics and syntactic analysis. Cognition,
2(3):319?347.
Stefan L Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In Proceedings of the 31st annual
conference of the cognitive science society, pages
1139?1144. Cognitive Science Society Austin, TX.
Peter C. Gordon, Randall Hendrick, and Marcus John-
son. 2001. Memory interference during language
processing. Journal of Experimental Psychology:
Learning, Memory and Cognition, 27:1411?1423.
Daniel Grodner and Edward Gibson. 2005. Conse-
quences of the serial nature of linguistic input for
sentenial complexity. Cognitive Science, 29(2):261?
290.
John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, vol-
ume 2, pages 159?166, Pittsburgh, PA.
Gaurav Kharkwal and Karin Stromswold. 2013.
Good-enough language processing: Evidence from
sentence-video matching. Journal of psycholinguis-
tic research, 43(1):1?17.
Jonathan King and Marcel A. Just. 1991. Individ-
ual differences in sentence processing: The role of
working memory. Journal of Memory and Lan-
guage, 30:580?602.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz, 2003. Automated essay scoring: a cross dis-
ciplinary approach, chapter Automated scoring and
annotation of essays with the Iintelligent Essay As-
sessor, pages 87?112. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ.
Roger Levy and Frank Keller. 2012. Expectation
and locality effects in german verb-final structures.
Journal of Memory and Language.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126?1177.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational linguistics, 19(2):313?330.
Ellis B. Page. 1966. The imminence of grading essays
by computer. Phi Delta Kappan, 47(5):238?243.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 324?333. Association for Com-
putational Linguistics.
Lawrence M. Rudner and Tahung Liang. 2002. Au-
tomated essay scoring using Bayes? theorem. The
Journal of Technology, Learning and Assessment,
1(2).
Dan Slobin. 1966. Grammatical transformations
and sentence comprehension in childhood and adult-
hood. Journal of Verbal Learning and Verbal Be-
havior, 5(3):219?227.
59
Adrian Staub. 2010. Eye movements and process-
ing difficulty in object relative clauses. Cognition,
116(1):71?86.
Karin Stromswold, David Caplan, Nathaniel Alpert,
and Scott Rauch. 1996. Localization of syntac-
tic comprehension by position emission tomogra-
phy. Brain and Language, 52:452?473.
Matthew J. Traxler, Robin K. Morris, and Rachel E.
Seely. 2002. Processing subject and object relative
clauses: Evidence from eye movements. Journal of
Memory and Language, 47:69?90.
60
Proceedings of the First Workshop on Argumentation Mining, pages 39?48,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Analyzing Argumentative Discourse Units in Online Interactions
Debanjan Ghosh* Smaranda Muresan? Nina Wacholder* Mark Aakhus* Matthew Mitsui**
*School of Communication and Information, Rutgers University
?Center of Computational Learning Systems, Columbia University
**Department of Computer Science, Rutgers University
debanjan.ghosh|ninwac|aakhus|mmitsui@rutgers.edu, smara@ccls.columbia.edu
Abstract
Argument mining of online interactions is
in its infancy. One reason is the lack of
annotated corpora in this genre. To make
progress, we need to develop a principled
and scalable way of determining which
portions of texts are argumentative and
what is the nature of argumentation. We
propose a two-tiered approach to achieve
this goal and report on several initial stud-
ies to assess its potential.
1 Introduction
An increasing portion of information and opin-
ion exchange occurs in online interactions such
as discussion forums, blogs, and webpage com-
ments. This type of user-generated conversation-
al data provides a wealth of naturally occurring
arguments. Argument mining of online interac-
tions, however, is still in its infancy (Abbott et al.,
2011; Biran and Rambow, 2011; Yin et al., 2012;
Andreas et al., 2012; Misra and Walker, 2013).
One reason is the lack of annotated corpora in this
genre. To make progress, we need to develop a
principled and scalable way of determining which
portions of texts are argumentative and what is the
nature of argumentation.
We propose a multi-step coding approach
grounded in findings from argumentation re-
search on managing the difficulties of coding ar-
guments (Meyers and Brashers, 2010). In the first
step, trained expert annotators identify basic ar-
gumentative features (coarse-grained analysis) in
full-length threads. In the second step, we explore
the feasibility of using crowdsourcing and novice
annotators to identify finer details and nuances of
the basic argumentative units focusing on limited
thread context. Our coarse-grained scheme for ar-
gumentation is based on Pragmatic Argumentation
Theory (PAT) (Van Eemeren et al., 1993; Hutchby,
Figure 1: Argumentative annotation of an Online
Thread
2013; Maynard, 1985). PAT states that an argu-
ment can arise at any point when two or more
actors engage in calling out and making prob-
lematic some aspect of another actor?s prior con-
tribution for what it (could have) said or meant
(Van Eemeren et al., 1993). The argumentative
relationships among contributions to a discussion
are indicated through links between what is tar-
geted and how it is called-out. Figure 1 shows
an example of two Callouts that refer back to the
same Target.
The annotation task performed by the trained
annotators includes three subtasks that Peldszus
and Stede (2013a) identify as part of the argu-
ment mining problem: 1) Segmentation, 2) Seg-
ment classification, and 3) Relationship identifi-
cation. In the language of Peldszus and Stede
(2013a), Callouts and Targets are the basic Argu-
ment Discourse Units (ADUs) that are segmented,
classified, and linked. There are two key advan-
tages of our coarse-grained annotation scheme:
1) It does not initially prescribe what constitutes
an argumentative text; 2) It makes it possible for
Expert Annotators (EAs) to find ADUs in long
39
threads. Assigning finer grained (more complex)
labels would have unduly increased the already
heavy cognitive load for the EAs. In Section
2 we present the corpus, describe the annotation
scheme and task, calculate Inter Annotator Agree-
ment (IAA), and propose a hierarchical clustering
approach to identify text segments that the EAs
found easier or harder to annotate.
In Section 3, we report on two Amazon
Mechanical Turk (MTurk) experiments, which
demonstrate that crowdsourcing is a feasible way
to obtain finer grained annotations of basic ADUs,
especially on the text segments that were easier
for the EAs to code. In the first crowd sourc-
ing study, the Turkers (the workers at MTurk,
who we consider novice annotators) assigned la-
bels (Agree/Disagree/Other) to the relations be-
tween Callout and Target identified by the EAs.
In the second study, Turkers labeled segments of
Callouts as Stance or Rationale. Turkers saw only
a limited context of the threaded discussion, i.e.
a particular Callout-Target pair identified by the
EA(s) who had analyzed the entire thread. In addi-
tion we report on initial classification experiments
to detect agreement/disagreement, with the best
F1 of 66.9% for the Agree class and 62.6% for the
Disagree class.
2 Expert Annotation for Coarse-Grained
Argumentation
Within Pragmatic Argumentation Theory, argu-
mentation refers to the ways in which people (seek
to) make some prior action or antecedent event
disputable by performing challenges, contradic-
tions, negations, accusations, resistance, and other
behaviors that call out a ?Target?, a prior action
or event. In this section, we present the corpus,
the annotation scheme based on PAT and the an-
notation task, the inter-annotator agreement, and a
method to identify which pieces of text are easier
or harder to annotate using a hierarchical cluster-
ing approach.
2.1 Corpus
Our corpus consists of blog comments posted as
responses to four blog postings selected from a
dataset crawled from Technorati between 2008-
2010
1
. We selected blog postings in the general
topic of technology and considered only postings
1
http://technorati.com/blogs/directory/
that had more than 200 comments. For the an-
notation we selected the first one hundred com-
ments on each blog together with the original post-
ing. Each blog together with its comments con-
stitutes a thread. The topics of each thread are:
Android (comparison of features of iPhone and
Android phones), iPad (the usefulness of iPads),
Twitter (the usefulness of Twitter as a microblog-
ging platform), and Layoffs (downsizing and out-
sourcing efforts of technology companies). We re-
fer to these threads as the argumentative corpus.
We plan to make the corpus available to the re-
search community.
2.2 Annotation Scheme and Expert
Annotation Task
The coarse-grained annotation scheme for argu-
mentation is based on the concept of Callout and
Target of Pragmatic Argumentation Theory. The
experts? annotation task was to identify expres-
sions of Callout and their Targets while also indi-
cating the links between them. We prepared a set
of guidelines with careful definitions of all techni-
cal terms. The following is an abbreviated excerpt
from the guidelines:
? Callout: A Callout is a subsequent action
that selects (i.e., refers back to) all or some
part of a prior action (i.e., Target) and com-
ments on it in some way. In addition to re-
ferring back to the Target, a Callout explic-
itly includes either one or both of the fol-
lowing: Stance (indication of attitude or posi-
tion relative to the Target) and Rationale (ar-
gument/justification/explanation of the Stance
taken).
? Target: A Target is a part of a prior action that
has been called out by a subsequent action.
Fig. 1 shows two examples of Callouts from
two comments referring back to the same Target.
Annotators were instructed to mark any text seg-
ment (from words to entire comments) that sat-
isfied the definitions above. A single text seg-
ment could be a Target and a Callout. To per-form
the expert annotation, we hired five graduate stu-
dents who had a strong background in humanities
and who received extensive training for the task.
The EAs performed three annotation subtasks
mentioned by Peldszus and Stede (2013a): Seg-
mentation (identify the Argumentative Dis-course
40
Thread A1 A2 A3 A4 A5
Android 73 99 97 118 110
iPad 68 86 85 109 118
Layoffs 71 83 74 109 117
Twitter 76 102 70 113 119
Avg. 72 92.5 81.5 112.3 116
Table 1: Number of Callouts by threads and EA
Thread F1 EM F1 OM ?
Android 54.4 87.8 0.64
iPad 51.2 86.0 0.73
Layoffs 51.9 87.5 0.87
Twitter 53.8 88.5 0.82
Table 2: IAA for 5 EA: F1 and alpha values per
thread
Units (ADUs) including their boundaries), Seg-
ment classification (label the roles of the ADUs,
in this case Callout and Target) and relation iden-
tification (indicate the link between a Callout and
the most recent Target to which is a response).
The segmentation task, which Artstein and Poe-
sio (2008) refer to as the unitization problem, is
particularly challenging. Table 1 shows extensive
variation in the number of ADUs (Callout in this
case) identified by the EAs for each of the four
threads. Annotator A1 identified the fewest Call-
outs (72) while A4 and A5 identified the most
(112.3 and 116, respectively). Although these dif-
ferences could be due to the issues with training,
we interpret the consistent variation among coders
as an indication that judges can be characterized
as ?lumpers? or ?splitters?. What lumpers con-
sidered a single long unit was treated as two (or
more) shorter units by splitters. This is an example
of the problem of annotator variability discussed
in (Peldszus and Stede, 2013b). Similar behavior
was noticed for Targets.
2
2.3 Inter Annotator Agreement
Since the annotation task includes the segmen-
tation step, to measure the IAA we have to ac-
count for fuzzy boundaries. Thus, we con-sider
two IAA metrics usually used in literature for
such cases: the information retrieval (IR) in-spired
precision-recall (P/R/F1) measure (Wiebe et al.,
2005) and Krippendorff?s ? (Krippendorff, 2004).
We present here the main results; a detailed dis-
cussion of the IAA is left for a different paper. Fol-
lowing Wiebe et al. (2005), to calculate P/R/F1 for
two annotators, one annotator?s ADUs are selected
2
Due to space limitations, here and in the rest of this paper
we report only on Callouts.
as the gold standard. If more than two annotators
are employed, the IAA is the average of the pair-
wise P/R/F1. To determine if two annotators have
selected the same text span to represent an ADU,
we use the two methods of Somasundaran et al.
(2008): exact match (EM) - text spans that vary
at the start or end point by five characters or less,
and overlap match (OM) - text spans that have at
least 10% of same overlapping characters. Table 2
shows the F1 measure for EM and OM for the five
EAs on each of the four threads. As expected, the
F1 measures are much lower for EM than for OM.
For the second IAA metric, we implement
Krippendorff?s ? (Krippendorff, 2004), where the
character overlap between any two annotations
and the gap between them are utilized to mea-
sure the expected disagreement and the observed
disagreement. Table 2 shows ? values for each
thread, which means significant agreement.
While the above metrics show reasonable agree-
ment across annotators, they do not tell us what
pieces of text are easier or harder to annotate. In
the next section we report on a hierarchical cluster-
ing technique that makes it possible to assess how
difficult it is to identify individual text segments as
Callouts.
2.4 Clustering of Callout ADUs
We use a hierarchical clustering technique (Hastie
et al., 2009) to cluster ADUs that are variants of
the same Callout. Each ADU starts in its own clus-
ter. The start and end points of each ADU are uti-
lized to identify overlapping characters in pairs of
ADUs. Then, using a ?bottom up? clustering ap-
proach, two ADUs (in this case, pairs of Callouts)
that share overlapping characters are merged into
a cluster. This process continues until no more
text segments can be merged. Clusters with five
overlapping ADUs include a text segment that all
five annotators have labeled as a Callout, while
clusters with one ADU indicates that only one an-
notator classified the text segment as a Callout
(see Table 3). These numbers provide information
about what segments of text are easier or harder to
code. For instance, when a cluster contains only
two ADUs, it means that three of the five anno-
tators did not label the text segment as a Callout.
Our MTurk study of Stance/Rationale (Sec. 3.2)
could highlight one reason for the variation ? some
coders consider a segment of text as Callout when
an implicit Stance is present, while others do not.
41
# Of EAs Callout Target
5 I disagree too. some things they get right, some
things they do not.
the iPhone is a truly great design.
I disagree too . . . they do not. That happened because the iPhone is a truly
great design.
I disagree too. But when we first tried the iPhone it felt natural
immediately . . . iPhone is a truly great design.
Hi there, I disagree too . . . they do not. Same as
OSX.
?Same as above-
I disagree too. . . Same as OSX . . . no problem. ?Same as above-
2 Like the reviewer said . . . (Apple) the industry
leader.. . . Good luck with that (iPhone clones).
Many of these iPhone . . . griping about issues
that will only affect them once in a blue moon
Like the reviewer said. . . (Apple) the industry
leader.
Many of these iPhone. . .
1 Do you know why the Pre . . . various hand-
set/builds/resolution issues?
Except for games?? iPhone is clearly dominant
there.
Table 3: Examples of Callouts lusters and their corresponding Targets
Thread # of Clusters # of EA ADUs per cluster
5 4 3 2 1
Android 91 52 16 11 7 5
Ipad 88 41 17 7 13 10
Layoffs 86 41 18 11 6 10
Twitter 84 44 17 14 4 5
Table 4: Number of clusters for each cluster type
Table 4 shows the number of Callout clusters in
each thread. The number of clusters with five and
four annotators shows that in each thread there are
Callouts that are plausibly easier to identify. On
the other hand, the clusters selected by only one
or two annotators are harder to identify.
3 Crowdsourcing for Fine-grained
Argumentation
To understand better the nature of the ADUs, we
conducted two studies asking Turkers to perform
finer grained analysis of Callouts and Targets. Our
first study asked five Turkers to label the relation
between a Callout and its corresponding Target
as Agree, Disagree, or Other. The Other relation
may be selected in a situation where the Callout
has no relationship with the Target (e.g., a pos-
sible digression) or is in a type of argumentative
relationship that is difficult to classify as either
Agreement or Disagreement. The second study
asked five Turkers to identify Stance and Ratio-
nale in Callouts identified by EAs. As discussed
in Section 2, by definition, a Callout contains an
explicit instance of Stance, Rationale or both. In
both of these crowdsourcing studies the Turkers
were shown only a limited portion of the threaded
discussion, i.e. the Callout-Target pairs that the
EAs had linked.
Crowdsourcing is becoming a popular mecha-
nism to collect annotations and other type of data
for natural language processing research (Wang
and Callison-Burch, 2010; Snow et al., 2008;
Chen and Dolan, 2011; Post et al., 2012). Crowd-
sourcing platforms such as Amazon Mechanical
Turk (MTurk) provide a flexible framework to sub-
mit various types of NLP tasks where novice anno-
tators (Turkers) can generate content (e.g., transla-
tions, paraphrases) or annotations (labeling) in an
inexpensive way and with limited training. MTurk
also provides researchers with the ability to con-
trol the quality of the Turkers, based on their past
performances. Section 3.1 and 3.2 describe our
two crowdsourcing studies for fine grain argumen-
tation annotation.
3.1 Crowdsourcing Study 1: Labeling the
Relation between Callout and Target
In this study, the Turkers? task was to assign a rela-
tion type between a Callout and its associated Tar-
get. The choices were Agree, Disagree, or Other.
Turkers were provided with detailed instructions,
including multiple examples of Callout and Target
pairs and their relation type. Each HIT (Human
Intelligence Task, in the language of MTurk) con-
tained one Callout-Target pair and Turkers were
paid 2 cents per HIT. To assure a level of qual-
ity control, only qualified Turkers were allowed
to perform the task (i.e., Master level with more
than 95% approval rate and at least 500 approved
HITs).
For this experiment, we randomly selected a
Callout from each cluster, along with its corre-
sponding Target. Our assumption is that all Call-
out ADUs in a given cluster have the same relation
type to their Targets (see Table 3). While this as-
sumption is logical, we plan to fully investigate it
42
in future work by running an MTurk experiment
on all the Callout ADUs and their corresponding
Targets.
We utilized Fleiss? kappa (Fleiss, 1971) to
compute IAA between the Turkers (every HIT
was completed by five Turkers). Kappa is be-
tween 0.45-0.55 for each thread showing moder-
ate agreement between the Turkers (Landis et al.,
1977). These agreement results are in line with the
agreement noticed in previous studies on agree-
ment/disagreement annotations in online interac-
tions (Bender et al., 2011; Abbott et al., 2011).
To select a gold standard for the relation type, we
used majority voting. That is, if three or more
Turkers agreed on a label, we selected that label
as the gold standard. In cases where there was
no majority, we assigned the label Other. The to-
tal number of Callouts that are in agreement and
in disagreement with Targets are 143 and 153, re-
spectively.
Table 5 shows the percentage of each
type of relation identified by Turkers
(Agree/Disagree/Other) for clusters annotated by
different number of EAs. The results suggest
that there is a correlation between text segments
that are easier or harder to annotate by EAs with
the ability of novice annotators to identify an
Agree/Disagree relation type between Callout and
Target. For example, Turkers generally discovered
Agree/Disagree relations between Callouts and
their Targets when the Callouts are part of those
clusters that are annotated by a higher number
of EAs. Turkers identified 57% as showing
a disagreement relation between Callout and
Target, and 39% as showing an agreement relation
(clusters with 5 EAs). For those clusters, only
4% of the Callouts are labeled as having an Other
relation with the Target. For clusters selected
by fewer EAs, however, the number of Callouts
having a relation with the Target labeled as Other
is much higher (39% for clusters with two EAs
and 32% for clusters with one EA). These results
show that those Callouts that are easier to discover
(i.e., identified by all five EAs) mostly have a
relation with the Target (Agree or Disagree) that
is clearly expressed and thus recognizable to the
Turkers. Table 5 also shows that in some cases
even if some EAs agreed on a piece of text to be
considered as a Callout, the novice annotators
assigned the Other relation to the Callout and Tar-
get ADUs. There are two possible explanations:
Relation label # of EA ADUs per cluster
5 4 3 2 1
Agree 39.36 43.33 42.50 35.48 48.39
Disagree 56.91 31.67 32.50 25.81 19.35
Other 3.72 25.00 25.00 38.71 32.26
Table 5: Percentage of Relation labels per EA
cluster type
either the novice annotators could not detect an
implicit agreement or disagreement and thus they
selected Other, or there are other types of relations
besides Agreement and Disagreement between
Callouts and their corresponding Targets. We
plan to extend this study to other fine grained
relation types in future work. In the next section
we discuss the results of building a supervised
classifier to predict the Agree or Disagree relation
type between Callout/Target pairs.
3.1.1 Predicting the Agree/Disagree Relation
Label
We propose a supervised learning setup to clas-
sify the relation types of Callout-Target pairs. The
classification categories are the labels collected
from the MTurk experiment. We only consider
the Agree and Disagree categories since the Other
category has a very small number of instances
(53). Based on the annotations from the Turkers,
we have 143 Agree and 153 Disagree training in-
stances.
We first conducted a simple baseline exper-
iment to check whether participants use words
or phrases to express explicit agreement or dis-
agreement such as ?I agree?, ?I disagree?. We
collected two small lists (twenty words each)
of words from Merriam-Webster dictionary that
explicitly represent agreement and disagreement
Stances. The agreement list contains the word
?agree? and its synonyms such as ?accept?, ?con-
cur?, and ?accede?. The disagreement list con-
tains the word ?disagree? and synonyms such as
?differ? and ?dissent?. We then checked whether
the text of the Callouts contains these explicit
agreement/disagreement markers. Note, that these
markers are utilized as rules and no statistical
learning is involved in this stage of experiment.
The first row of the Table 6 represents the base-
line results. Though the precision is high for
agreement category, the recall is quite low and that
results in a poor overall F1 measure. This shows
that even though markers like ?agree? or ?disagree?
43
Features Category P R F1
Baseline Agree 83.3 6.9 12.9
Disagree 50.0 5.2 9.5
Unigrams Agree 57.9 61.5 59.7
Disagree 61.8 58.2 59.9
MI-based unigram Agree 60.1 66.4 63.1
Disagree 65.2 58.8 61.9
LexF Agree 61.4 73.4 66.9
Disagree 69.6 56.9 62.63
Table 6: Classification of Agree/Disagree
are very precise, they occur in less than 15% of
all the Callouts expressing agreement or disagree-
ment.
For the next set of experiments we used a super-
vised machine learning approach for the two-way
classification (Agree/Disagree). We use Support
Vector Machines (SVM) as our machine-learning
algorithm for classification as implemented in
Weka (Hall et al., 2009) and ran 10-fold cross val-
idation. As a SVM baseline, we first use all un-
igrams in Callout and Target as features (Table
6, Row 2). We notice that the recall improves
significantly when compared with the rule-based
method. To further improve the classification ac-
curacy, we use Mutual Information (MI) to se-
lect the words in the Callouts and Targets that are
likely to be associated with the categories Agree
and Disagree, respectively. Specifically, we sort
each word based on its MI value and then se-
lect the first 180 words in each of the two cate-
gories to represent our new vocabulary set of 360
words. The feature vector includes only words
present in the MI list. Compared to the all uni-
grams baseline, the MI-based unigrams improve
the F1 by 4% (Agree) and 2% (Disagree) (Table
6). The MI approach discovers the words that
are highly associated with Agree/Disagree cate-
gories and these words turn to be useful features
for classification. In addition, we consider several
types of lexical features (LexF) inspired by previ-
ous work on agreement and disagreement (Galley
et al., 2004; Misra and Walker, 2013).
? Sentiment Lexicon (SL): Two features are de-
signed using a sentiment lexicon (Hu and Liu,
2004) where the first feature represents the num-
ber of times the Callout and the Target contain a
positive emotional word and the second feature
represents the number of the negative emotional
words.
? Initial unigrams in Callout (IU): Instead of
using all unigrams in the Callout and Target,
Features Category P R F1
LexF Agree 61.4 73.4 66.9
Disagree 69.6 56.9 62.6
LexF-SL Agree 60.6 74.1 66.7
Disagree 69.4 54.9 61.3
LexF-IU Agree 58.1 69.9 63.5
Disagree 65.3 52.9 58.5
LexF-LO Agree 57.2 74.8 64.8
Disagree 67.0 47.7 55.7
Table 7: Importance of Lexical Features
we only select the first words from the Call-
out (maximum ten). The assumption is that the
stance is generally expressed at the beginning
of a Callout. We used the same MI-based tech-
nique to filter any sparse words.
? Lexical Overlap and Length (LO): This set of
features represents the lexical overlap between
the Callout and the Target and the length of each
ADU.
Table 6 shows that using all these types of
lexical features improves the F1 score for both
categories as compared to the MI-based unigram
features. Table 7 shows the impact of remov-
ing each type of lexical features. From these re-
sults it seems that initial unigrams of Callout (IU)
and lexical overlap (LO) are useful features: re-
moving each of them lowers the results for both
Agree/Disagree categories. In future work, we
plan to explore context-based features such as the
thread structure, and semantic features such as
WordNet-based semantic similarity. We also hy-
pothesize that with additional training instances
the ML approaches will achieve better results.
3.2 Crowdsourcing Study 2: Analysis of
Stance and Rationale
In the second study aimed at identifying the ar-
gumentative nature of the Callouts identified by
the expert annotators, we focus on identifying the
Stance and Rationale segments of a Callout. Since
the presence of at least an explicit Stance or Ra-
tionale was part of the definition of a Callout, we
selected these two argumentation categories as our
finer-grained scheme for this experiment.
Given a pair of Callout and Target ADUs, five
Turkers were asked to identify the Stance and Ra-
tionale segments in the Callout, including the ex-
act boundaries of the text segments. Identifying
Stance and Rationale is a difficult task and thus,
we also asked Turkers to mark the level of diffi-
culty in the identification task. We provided the
44
Diff Number of EAs per cluster
5 4 3 2 1
VE 22.11 22.38 20.25 16.67 10.71
E 28.55 24.00 24.02 28.23 20.00
M 19.69 17.87 20.72 19.39 23.57
D 11.50 10.34 11.46 9.52 12.86
VD 7.02 5.61 4.55 4.42 6.43
TD 11.13 19.79 19.00 21.77 26.33
Table 8: Difficulty judgments by Turkers com-
pared to number of EAs who selected a cluster
Turkers with a scale of difficulty (similar to a Lik-
ert scale), where the Turkers have to choose one
of the following: very easy (VE), easy (E), moder-
ate (M), difficult (D), very difficult (VD), too diffi-
cult to code (TD). Turkers were instructed to select
the too difficult to code choice only in cases where
they felt it was impossible to detect a Stance or
Rationale in the Callout.
The Turkers were provided with detailed in-
structions including examples of Stance and Ra-
tionale annotations for multiple Callouts and only
highly qualified Turkers were allowed to perform
the task. Unlike the previous study, we also ran a
pre-screening testing phase and only Turkers that
passed the screening were allowed to complete the
tasks. Because of the difficult nature of the anno-
tation task, we paid ten cents per HIT.
For the Stance/Rationale study, we considered
all the Callouts in each cluster along with the asso-
ciated Targets. We selected all the Callouts from
each cluster because of variability in the bound-
aries of ADUs, i.e., in the segmentation process.
One benefit of this crowdsourcing experiment is
that it helps us understand better what the variabil-
ity means in terms of argumentative structure. For
example, one EA might mark a text segment as a
Callout only when it expresses a Stance, while an-
other EA might mark as Callout a larger piece of
text expressing both the Stance and Rationale (See
examples of Clusters in Table 3). We leave this
deeper analysis as future work.
Table 8 shows there is a correlation between
the number of EAs who selected a cluster and the
difficulty level Turkers assigned to identifying the
Stance and Rationale elements of a Callout. This
table shows that for more than 50% of the Callouts
that are identified by 5 EAs, the Stance and Ra-
tionale can be easily identified (refer to the ?VE?
and ?E? rows), where as in the case of Callouts
that are identified by only 1 EA, the number is
just 31%. Similarly, more than 26% of the Call-
Diff Number of EAs per cluster
5 4 3 2 1
E 81.04 70.76 60.98 63.64 25.00
M 7.65 7.02 17.07 6.06 25.00
D 5.91 5.85 7.32 9.09 12.50
TD 5.39 16.37 14.63 21.21 37.50
Table 9: Difficulty judgment (majority voting)
outs in that same category (1 EA) were labeled as
?Too difficult to code?, indicating that the Turk-
ers could not identify either a Stance or Rationale
in the Callout. These numbers are comparable to
what our first crowdsourcing study showed for the
Agree/Disagree/Other relation identification (Ta-
ble 5). Table 9 shows results where we selected
overall difficulty level by majority voting. We
combined the easy and very easy categories to the
category easy (E) and the difficult and very diffi-
cult categories to the category difficult (D) for a
simpler presentation.
Table 9 also shows that more than 80% of the
time, Turkers could easily identify Stance and/or
Rationale in Callouts identified by 5 EAs, while
they could perform the finer grained analysis eas-
ily only 25% of time for Callouts identified by a
single EA. Only 5% of Callouts identified by all
5 EAs were considered too difficult to code by the
Turkers (i.e., the novice annotators could not iden-
tify a Stance or a Rationale). In contrast, more
than 37% of Callouts annotated only by 1 EA were
considered too difficult to code by the novice an-
notators. Table 10 presents some of the examples
of Stance and Rationale pairs as selected by the
Turkers along with the difficulty labels.
4 Related Work
Primary tasks for argument analysis are to seg-
ment the text to identify ADUs, detect the roles
of each ADUs, and to establish the relationship
between the ADUs (Peldszus and Stede, 2013a).
Similarly, Cohen (1987) presented a computa-
tional model of argument analysis where the struc-
ture of each argument is restricted to the claim and
evidence relation. Teufel et al. (2009) introduce
the argumentative zoning (AZ) idea that identifies
important sections of scientific articles and later
Hachey and Grover (2005) applied similar idea of
AZ to summarize legal documents. Wyner et al.
(2012) propose a rule-based tool that can high-
light potential argumentative sections of text ac-
cording to discourse cues like ?suppose? or ?there-
fore?. They tested their system on product reviews
45
Target Callout Stance Rationale Difficulty
the iPhone is a truly
great design.
I disagree too. some
things they get right,
some things they do
not.
I. . . too Some things . . . do not Easy
the dedicated ?Back?
button
that back button is key.
navigation is actually
much easier on the an-
droid.
That back button is key Navigation
is. . . android
Moderate
It?s more about the fea-
tures and apps and An-
droid seriously lacks on
latter.
Just because the iPhone
has a huge amount of
apps, doesn?t mean
they?re all worth
having.
? Just because the iPhone
has a huge amount of
apps, doesn?t mean
they?re all worth
having.
Difficult
I feel like your com-
ments about Nexus One
is too positive . . .
I feel like your poor
grammar are to obvious
to be self thought...
? ? Too difficult to
code
Table 10: Examples of Callout/Target pairs with difficulty level (majority voting)
(Canon Camera) from Amazon e-commerce site.
Relatively little attention has so far been de-
voted to the issue of building argumentative cor-
pora from naturally occurring texts (Peldszus and
Stede, 2013a; Feng and Hirst, 2011). However,
(Reed et al., 2008; Reed and Rowe, 2004) have
developed the Araucaria project that maintains
an online repository of arguments (AraucariaDB),
which recently has been used as research cor-
pus for several automatic argumentation analyses
(Palau and Moens, 2009; Wyner et al., 2010; Feng
and Hirst, 2011). Our work contributes a new prin-
cipled method for building annotated corpora for
online interactions. The corpus and guidelines will
also be shared with the research community.
Another line of research that is correlated with
ours is recognition of agreement/disagreement
(Misra and Walker, 2013; Yin et al., 2012; Ab-
bott et al., 2011; Andreas et al., 2012; Galley et
al., 2004; Hillard et al., 2003) and classification of
stances (Walker et al., 2012; Somasundaran and
Wiebe, 2010) in online forums. For future work,
we can utilize textual features (contextual, depen-
dency, discourse markers), relevant multiword ex-
pressions and topic modeling (Mukherjee and Liu,
2013), and thread structure (Murakami and Ray-
mond, 2010; Agrawal et al., 2003) to improve the
Agree/Disagree classification accuracy.
Recently, Cabrio and Villata (2013) proposed
a new direction of argumentative analysis where
the authors show how arguments are associated
with Recognizing Textual Entailment (RTE) re-
search. They utilized RTE approach to detect the
relation of support/attack among arguments (en-
tailment expresses a ?support? and contradiction
expresses an ?attack?) on a dataset of arguments
collected from online debates (e.g., Debatepedia).
5 Conclusion and Future Work
To make progress in argument mining for online
interactions, we need to develop a principled and
scalable way to determine which portions of texts
are argumentative and what is the nature of argu-
mentation. We have proposed a two-tiered ap-
proach to achieve this goal. As a first step we
adopted a coarse-grained annotation scheme based
on Pragmatic Argumentation Theory and asked
expert annotators to label entire threads using this
scheme. Using a clustering technique we iden-
tified which pieces of text were easier or harder
for the Expert Annotators to annotate. Then we
showed that crowdsourcing is a feasible approach
to obtain annotations based on a finer grained ar-
gumentation scheme, especially on text segments
that were easier for the Expert Annotators to la-
bel as being argumentative. While more qualita-
tive analysis of these results is still needed, these
results are an example of the potential benefits of
our multi-step coding approach.
Avenues for future research include but are not
limited to: 1) analyzing the differences between
the stance and rationale annotations among the
novice annotators; 2) improving the classification
accuracies of the Agree/Disagree classifier using
more training data; 3) using syntax and seman-
tics inspired textual features and thread structure;
and 4) developing computational models to detect
Stance and Rationale.
46
Acknowledgements
Part of this paper is based on work supported by
the DARPA-DEFT program for the first two au-
thors. The views expressed are those of the au-
thors and do not reflect the official policy or po-
sition of the Department of Defense or the U.S.
Government.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E
Fox Tree, Robeson Bowmani, and Joseph King.
2011. How can you say such things?!?: Recogniz-
ing disagreement in informal political argument. In
Proceedings of the Workshop on Languages in So-
cial Media, pages 2?11. Association for Computa-
tional Linguistics.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In
Proceedings of the 12th international conference on
World Wide Web, pages 529?535. ACM.
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagree-
ment in threaded discussion. In LREC, pages 818?
822.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Emily M Bender, Jonathan T Morgan, Meghan Oxley,
Mark Zachry, Brian Hutchinson, Alex Marin, Bin
Zhang, and Mari Ostendorf. 2011. Annotating so-
cial acts: Authority claims and alignment moves in
wikipedia talk pages. In Proceedings of the Work-
shop on Languages in Social Media, pages 48?57.
Association for Computational Linguistics.
Or Biran and Owen Rambow. 2011. Identifying jus-
tifications in written dialogs by classifying text as
argumentative. International Journal of Semantic
Computing, 5(04):363?381.
Elena Cabrio and Serena Villata. 2013. A natural
language bipolar argumentation approach to support
users in online debate interactions. Argument &
Computation, 4(3):209?230.
David L Chen and William B Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 190?200.
Association for Computational Linguistics.
Robin Cohen. 1987. Analyzing the structure of ar-
gumentative discourse. Computational linguistics,
13(1-2):11?24.
Vanessa Wei Feng and Graeme Hirst. 2011. Clas-
sifying arguments by scheme. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 987?996. Association
for Computational Linguistics.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 669. Association for Computational Lin-
guistics.
Ben Hachey and Claire Grover. 2005. Automatic le-
gal text summarisation: experiments with summary
structuring. In Proceedings of the 10th international
conference on Artificial intelligence and law, pages
75?84. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10?
18.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
T Hastie, J Friedman, and R Tibshirani. 2009. The
elements of statistical learning, volume 2. Springer.
Dustin Hillard, Mari Ostendorf, and Elizabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: Training with unlabeled
data. In Proceedings of the 2003 Conference of
the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology: companion volume of the Proceedings
of HLT-NAACL 2003?short papers-Volume 2, pages
34?36. Association for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Ian Hutchby. 2013. Confrontation talk: Arguments,
asymmetries, and power on talk radio. Routledge.
Klaus Krippendorff. 2004. Measuring the reliability
of qualitative text analysis data. Quality & quantity,
38:787?800.
J Richard Landis, Gary G Koch, et al. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, 33(1):159?174.
Douglas W Maynard. 1985. How children start argu-
ments. Language in society, 14(01):1?29.
47
Renee A Meyers and Dale Brashers. 2010. Extend-
ing the conversational argument coding scheme: Ar-
gument categories, units, and coding procedures.
Communication Methods and Measures, 4(1-2):27?
45.
Amita Misra and Marilyn A Walker. 2013. Topic in-
dependent identification of agreement and disagree-
ment in social media dialogue. In Proceedings of
the SIGDIAL 2013 Conference, pages 41?50. Asso-
ciation for Computational Linguistics.
Arjun Mukherjee and Bing Liu. 2013. Discovering
user interactions in ideological discussions. In Pro-
ceedings of the 51st Annual Meeting on Association
for Computational Linguistics, pages 671?681. Cite-
seer.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose?: classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the 12th international conference on ar-
tificial intelligence and law, pages 98?107. ACM.
Andreas Peldszus and Manfred Stede. 2013a. From ar-
gument diagrams to argumentation mining in texts:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence (IJCINI), 7(1):1?31.
Andreas Peldszus and Manfred Stede. 2013b. Ranking
the annotators: An agreement study on argumenta-
tion structure. In Proceedings of the 7th linguistic
annotation workshop and interoperability with dis-
course, pages 196?204.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 401?409. Association for Computational
Linguistics.
Chris Reed and Glenn Rowe. 2004. Araucaria: Soft-
ware for argument analysis, diagramming and repre-
sentation. International Journal on Artificial Intelli-
gence Tools, 13(04):961?979.
Chris Reed, Raquel Mochales Palau, Glenn Rowe, and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the 6th
conference on language resources and evaluation-
LREC 2008, pages 91?100.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124. Association
for Computational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2008. Discourse level opinion relations: An
annotation study. In Proceedings of the 9th SIGdial
Workshop on Discourse and Dialogue, pages 129?
137. Association for Computational Linguistics.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 3-Volume 3, pages
1493?1502. Association for Computational Linguis-
tics.
Frans H Van Eemeren, Rob Grootendorst, Sally Jack-
son, and Scott Jacobs. 1993. Reconstructing argu-
mentative discourse. University of Alabama Press.
Marilyn A Walker, Pranav Anand, Rob Abbott, Jean
E Fox Tree, Craig Martell, and Joseph King. 2012.
That is your evidence?: Classifying stance in on-
line political debate. Decision Support Systems,
53(4):719?729.
Rui Wang and Chris Callison-Burch. 2010. Cheap
facts and counter-facts. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 163?167. Association for Computa-
tional Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165?210.
Adam Wyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Semantic
processing of legal texts, pages 60?79. Springer.
Adam Wyner, Jodi Schneider, Katie Atkinson, and
Trevor JM Bench-Capon. 2012. Semi-automated
argumentative analysis of online product reviews. In
COMMA, pages 43?50.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and
disagreement classification in online debates. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
pages 61?69. Association for Computational Lin-
guistics.
48
LAW VIII - The 8th Linguistic Annotation Workshop, pages 120?128,
Dublin, Ireland, August 23-24 2014.
Annotating Multiparty Discourse: Challenges for Agreement Metrics
Nina Wacholder* Smaranda Muresan? Debanjan Ghosh* Mark Aakhus*
*School of Communication and Information, Rutgers University
?Center for Computational Learning Systems, Columbia University
ninwac|debanjan.ghosh|aakhus@rutgers.edu, smara@ccls.columbia.edu
Abstract
To computationally model discourse phenomena such as argumentation we need corpora with
reliable annotation of the phenomena under study. Annotating complex discourse phenomena
poses two challenges: fuzziness of unit boundaries and the need for multiple annotators. We show
that current metrics for inter-annotator agreement (IAA) such as P/R/F1 and Krippendorff?s ?
provide inconsistent results for the same text. In addition, IAA metrics do not tell us what parts of
a text are easier or harder for human judges to annotate and so do not provide sufficiently specific
information for evaluating systems that automatically identify discourse units. We propose a
hierarchical clustering approach that aggregates overlapping text segments of text identified by
multiple annotators; the more annotators who identify a text segment, the easier we assume that
the text segment is to annotate. The clusters make it possible to quantify the extent of agreement
judges show about text segments; this information can be used to assess the output of systems
that automatically identify discourse units.
1 Introduction
Annotation of discourse typically involves three subtasks: segmentation (identification of discourse units,
including their boundaries), segment classification (labeling the role of discourse units) and relation iden-
tification (indicating the link between the discourse units) (Peldszus and Stede, 2013a). The difficulty
of achieving an Inter-Annotator Agreement (IAA) of .80, which is generally accepted as good agree-
ment, is compounded in studies of discourse annotations since annotators must unitize, i.e. identify the
boundaries of discourse units (Artstein and Poesio, 2008). The inconsistent assignment of boundaries in
annotation of discourse has been noted at least since Grosz and Sidner (1986) who observed that although
annotators tended to identify essentially the same units, the boundaries differed slightly. The need for
annotators to identify the boundaries of text segments makes measurement of IAA more difficult because
standard coefficients such as ? assume that the units to be coded have been identified before the coding
begins (Artstein and Poesio, 2008). A second challenge for measuring IAA for discourse annotation is
associated with larger numbers of annotators. Because of the many ways that ideas are expressed in hu-
man language, using multiple annotators to study discourse phenomena is important. Such an approach
capitalizes on the aggregated intuitions of multiple coders to overcome the potential biases of any one
coder and helps identify limitations in the coding scheme, thus adding to the reliability and validity of
the annotation study. The more annotators, however, the harder it is to achieve an IAA of .80 (Bayerl and
Paul, 2011). What to annotate also depends, among other characteristics, on the phenomenon of interest,
the text being annotated, the quality of the annotation scheme and the effectiveness of training. But even
if these are excellent, there is natural variability in human judgment for a task that involves subtle dis-
tinctions about which competent coders disagree. An accurate computational model should reflect this
variability (Aakhus et al., 2013).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
120
# Type Statement
Target I?m going to quit the iphone and switch to an android phone because I
can no long (sic) put up with the AT&T service contract
Callout I am going to switch too
Callout There is no point quitting the iphone because of the service package,
just jail break it and use the provider you want
Table 1: Examples of Callouts and Targets
Figure 1: Cluster where 3 judges identify a core
We propose an approach for overcoming these challenges based on evidence from an annotation study
of arguments in online interactions. Our scheme for argumentation is based on Pragmatic Argumentation
Theory (PAT) (Van Eemeren et al., 1993; Hutchby, 2013; Maynard, 1985). PAT states that argument can
arise at any point when two or more actors engage in calling out and making problematic some aspect
of another actor?s prior contribution for what it (could have) said or meant (Van Eemeren et al., 1993).
The argumentative relationships among contributions to a discussion are indicated through links between
what is targeted and how it is called out. Table 1 shows two Callouts that refer back to the same Target.
Callouts and Targets are Argumentative Discourse Units (ADUs) in the sense of Peldszus and Stede
(2013a), ?minimal units of analysis . . . inspired . . . by a . . . relation-based discourse theory? (p.20). In our
case the theory is PAT. Callouts are related to Targets by a relationship that we may refer to as Response,
though we do not discuss the Response relationship in this paper.
The hierarchical clustering technique that we propose systematically identifies clusters of ADUs; each
cluster contains a core of overlapping text that two or more judges have identified. Figure 1 shows
a schematic example of a cluster with a core identified by three judges. The variation in boundaries
represents the individual judges? differing intuitions; these differences reflect natural variation of human
judgments about discourse units. We interpret differences in the number (or percentage) of judges that
identify a core as evidence of how hard or easy a discourse unit is to recognize.
The contributions of this paper are two-fold. First, we show that methods for assessing IAA, such as
the information retrieval inspired (P/R/F1) approach (Wiebe et al., 2005) and Krippendorff?s ? (Krip-
pendorff, 1995; Krippendorff, 2004b), which was developed for content analysis in the social sciences,
provide inconsistent results when applied to segmentations involving fuzzy boundaries and multiple
coders.
In addition, these metrics do not tell us which parts of a text are easier or harder to annotate, or help
choose a reliable gold standard. Our second contribution is a new method for assessing IAA using hier-
archical clustering to find parts of text that are easier or harder to annotate. These clusters could serve as
the basis for assessing the performance of systems that automatically identify ADUs - the system would
be rewarded for identifying ADUs that are easier for people to recognize and penalized for identifying
ADUs that are relatively hard for people to recognize.
2 Annotation Study of Argumentative Discourse Units: Callouts and Targets
In this section, we describe the annotation study we conducted to determine whether trained human
judges can reliably identify Callouts and Targets. The main annotation task was to find Callouts and the
Targets to which they are linked and unitize them, i.e., assign boundaries to each ADU. As mentioned
above, these are the steps for argument mining delineated in Peldszus and Stede (2013a). The design of
121
the study was consistent with the conditions for generating reliable annotations set forth in Krippendorff
(2004a, p. 217).
We selected five blog postings from a corpus crawled from Technorati (technorati.com) between 2008-
2010; the comments contain many disputes. We used the first 100 comments on each blog as our corpus,
along with the original posting. We refer to each blog and the associated comments as a thread.
The complexity of the phenomenon required the perspective of multiple independent annotators, de-
spite the known difficulty in achieving reliable IAA with more than two annotators. For our initial
study, in which our goal was to obtain naturally occurring examples of Callouts and Targets and assess
the challenges of reliably identifying them, we engaged five graduate students with a strong humanities
background. The coding was performed with the open-source Knowtator software (Ogren, 2006). All
five judges annotated all 100 comments in all five threads. While the annotation process was under way,
annotators were instructed not to communicate with each other about the study.
The annotators? task was to find each instance of a Callout, determine the boundaries, link the Callout
to the most recent Target and determine the boundaries of the Target. We prepared and tested a set of
guidelines with definitions and examples of key concepts. The following is an adapted excerpt from the
guidelines:
? Callout: A Callout is (a part of) a subsequent action that selects (a part of) a prior action and marks
and comments on it in some way. In Table 1, Statements 2 and 3 are both Callouts, i.e., they perform
the action of calling out on Statement 1. Statement 2 calls out the first part of Statement 1 dealing
with switching phones. Statement 3 calls out all of Statement 1 ? both what?s proposed and the
rationale for the disagreement.
? Target: A Target is a part of a prior action that has been called out by a subsequent action. Statement
1 is a Target of Statements 2 and 3. But Statements 2 and 3 link to different parts of Statement 1, as
described above.
? Response: A link between Callout and Target that occurs when a subsequent action refers back to
(is a response to) a prior action.
Annotators were instructed to mark any text segment (from words to entire comments) that satisfied
the definitions above. A single text segment could be a Target and a Callout. To save effort on a difficult
task, judges were asked only to annotate the most recent plausible Target. We plan to study chains of
responses in future work.
Prior to the formal study, each annotator spent approximately eight hours in training, spread over about
two weeks, under the supervision of a PhD student who had helped to develop the guidelines. Training
materials included the guidelines and postings and comments from Technorati that were not used in the
formal study. Judges were reminded that our research goal was to find naturally occurring examples of
Callouts and Targets and that the research team did not know in advance what were the right answers
? the subjects? job was to identify Callouts and Targets that satisfied the definitions in the guidelines.
In response to the judges? questions, the guidelines were iteratively updated: definitions were reviewed,
additional examples were added, and a list of FAQs was developed
1
.
Table 2 shows the wide range of results among the annotators for Callouts that illustrates a problem to
be addressed when assessing reliability for multiple annotators.
Averaged over all five threads, A1 identified the fewest Callouts (66.8) while A4 and A5 identified
the most (107 and 109, respectively). Furthermore, the number of annotations assigned by A4 and A5
to each corpus is consistently higher than those of the other annotators, while the number of annotations
A1 assigned to each thread is consistently lower than that of all of the other annotators. Although these
differences could be due to issues with training, we interpret the consistent variation among coders as
potential evidence of two distinct types of behavior: some judges are ?lumpers? who consider a text
string as a single unit; others are ?splitters? who treat the same text string as two (or more) distinct units.
The high degree of variability among coders is consistent with the observations of Peldszus and Stede
1
The corpus, annotations and guidelines are available at <http://wp.comminfo.rutgers.edu/salts/projects/opposition/>.
122
Thread A1 A2 A3 A4 A5
Android 73 99 97 118 110
Ban 46 73 66 86 83
iPad 68 86 85 109 118
Layoffs 71 83 74 109 117
Twitter 76 102 70 113 119
Avg. 66.8 88.6 78.4 107 109.4
Table 2: Callouts per annotator per thread
(2013b). These differences could be due to issues with training and individual differences among coders,
but even so, the variability highlights an important challenge for calculating IAA with multiple coders
and fuzzy unit boundaries.
3 Some Problems of Unitization Reliability with Existing IAA Metrics
In this section we discuss two state-of-the-art metrics frequently used for measuring IAA for discourse
annotation and we show that these methods offer limited informativeness when text boundaries are fuzzy
and there are multiple judges. These methods are the information retrieval inspired precision-recall
(P/R/F1) metrics used in Wiebe and her collaborators? important work on sentiment analysis (Wiebe
et al., 2005; Somasundaran et al., 2008) and Krippendorff?s ?, a variant of the ? family of IAA coef-
ficients specifically designed to handle fuzzy boundaries and multiple annotators (Krippendorff, 1995;
Krippendorff, 2004b). Krippendorff?s ? determines IAA based on observed disagreement relative to
expected agreement and calculates differences in annotators? judgments. Although it is possible to use
number of words or even clauses to measure IAA, we use length in characters both for consistency with
Wiebe?s approach and because Krippendorff (2004b, pp.790-791) recommends using ?. . . the smallest
distinguishable length, for example the characters in text. . .? to measure IAA. We next show the results
of using P/R/F and Krippendorff?s ? to measure IAA for our annotation study and provide examples of
some challenges that need to be addressed.
3.1 Precision, Recall and F measures
Implementing P/R/F1 requires a gold standard annotation against which the other annotations can be
compared. P/R/F1 is calculated here, following (Wiebe et al., 2005), as follows: the units selected by
one annotator are taken as the gold standard and the remaining annotators are calculated against the
selected gold standard. To determine whether annotators selected the same text span, two different types
of matches were considered, as in Somasundaran et al. (2008): exact matches and overlap matches
(variation of their lenient match):
? Exact Matches (EM): Text spans that vary at the start or end point by five characters or less are
considered an exact match. This minor relaxation of exact matching (Somasundaran et al., 2008)
compensates for minor inconsistencies such as whether a judge included a sentence ending punctu-
ation mark in the unit.
? Overlap Matches (OM): Any overlap between text spans of more than 10% of the total number of
characters is considered a match. OM is weaker than EM but still an indicator of shared judgments
by annotators.
Tables 3 and 5 and Tables 4 and 6 show the P/R/F1-based IAA using EM and OM respectively. The
results are averaged across all five threads. Besides average P/R/F1 we also show Max F1 and Min F1,
which represent the maximum and minimum F1 relative to a particular annotator used as gold standard.
These tables show that the results vary greatly. Among the reasons for the variation are the following:
? Results are sensitive to which annotator is selected as the gold standard. In Table 4, pairing A4 with
the judge who agrees maximally produces an F measure of 90.2 while pairing A4 with the annotator
who agrees minimally produces an F measure of 73.3. In Tables 3 and 4, if we select A4 as the gold
standard we get the most variation; selecting A3 produces the least.
123
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 40.7 57.7 47.8 60 36.7
A2 51.7 51.2 51.4 58.3 43
A3 54.2 57.8 55.9 61.4 47.9
A4 59.7 49.1 53.9 61.4 47.3
A5 55 45.6 49.9 58.3 36.7
Table 3: Callouts: EM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 67.4 95.7 79.1 86.8 73.3
A2 85 83.7 84.3 88.7 76.1
A3 82.7 88 85.2 88.7 80.9
A4 92.7 76.8 84 90.2 73.3
A5 91.4 75.1 82.4 89.6 74
Table 4: Callouts: OM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 24.1 34.6 28.4 34.5 18.7
A2 26.9 24.7 25.7 37.6 18.7
A3 35.2 35.1 35.1 48.4 19.4
A4 37.3 34.5 35.8 50.4 22.1
A5 36.9 31.4 33.9 50.4 19.9
Table 5: Targets: EM P/R/F1 over 5 threads
Ann Avg P Avg R Avg F1 MaxF1 Min F1
A1 60.1 86.5 70.9 76.1 64.2
A2 74.5 69.4 71.9 79.6 62.9
A3 75.9 74.5 75.1 80.1 67.7
A4 78.1 71.5 74.6 84.2 64
A5 83.8 70.3 76.4 83.8 67.2
Table 6: Targets: OM P/R/F1 over 5 threads
? The type of matching matters. As expected, OM, which is less strict than EM, produces substantially
higher F1 scores both for Callouts (Tables 3 and 4 ) and Targets (Tables 5 and 6).
? Different phenomena are associated with different levels of difficulty of annotation. The F1 scores
for Targets are considerably lower than the F1 scores for Callouts. We suspect that Callouts are
easier to recognize since they are often introduced with standard expressions that signal agreement
or disagreement such as ?yes?, ?no?, ?I agree?, or ?I disagree?. Targets, on the other hand, generally
lack such distinguishing lexical features.
We also observe differences across threads. For example, the Ban thread seems harder to annotate
than the other threads. Figure 2 and 3 show IAA results for OM for Callout and Target annotations for
annotators A1 and A5 respectively, across the five threads. We chose A1 and A5 because in general
A1 annotated the fewest Callouts and A5 annotated the most Callouts in the corpus. These figures show
different annotator behavior. For instance, for both Callout and Target annotations, A1 has higher average
R than P, while A5 has higher P but lower R. Figures 2 and 3 hint that the Ban thread is harder to annotate
than the others.
The examples in this section show two downsides to the P/R/F1 metric. First, the scores do not reflect
the extent to which two annotations match. This is crucial information for fuzzy boundary matching, be-
cause the agreement between two annotations can be over only a few characters or over the full length of
the selected text. Second, the variation across multiple judges demonstrates the disadvantage of arbitrary
selection of a gold standard set of annotations against which to measure IAA.
3.2 Krippendorff?s ?
Krippendorff?s ? calculates IAA based on the observed and expected disagreement between annotators.
We use the version of Kripendorff?s ? discussed in Krippendorff (2004b) which takes into account mul-
tiple annotators and fuzzy boundaries. Detailed proof and an explanation of the calculation can be found
in (Krippendorff, 2004b; Krippendorff, 1995).
Thread F1 Krippendorff?s ?
Android 87.8 0.64
Ban 85.3 0.75
iPad 86.0 0.73
Layoffs 87.5 0.87
Twitter 88.5 0.82
Table 7: F1 and ? for all 5 threads
Thread Rank by IAA (Descending)
F1 K?s ?
Twitter Layoffs
Android Twitter
Layoffs Ban
iPad iPad
Ban Android
Table 8: Threads ranked by IAA in descending order
Comparison of ? and P/R/F1 metrics shows that they generate inconsistent results that are difficult to
interpret. For example, in Table 7, the F1 measure for Callouts indicates lower agreement on the Ban
thread in comparison to Android while ? suggests higher agreement on the Ban subcorpus relative to the
124
Figure 2: IAA metrics per thread when A1 is gold standard (Left: Callout. Right: Target.)
Figure 3: IAA metrics per thread when A5 is gold standard ( Left: Callout. Right: Target.)
Android subcorpus. The inconsistencies are also apparent in Table 8, which ranks threads in descending
order of IAA. For example, the Android corpus receives the highest IAA using F1 but the lowest using
?.
We do not show the results for Krippendorff?s ? for Targets for the following reason. Relevant units
from a continuous text string are assigned to categories by individual annotators. But identification of
Targets is dependent on (temporally secondary to) identification of Callouts. In multiple instances we
observe that an annotator links multiple Callouts to two or more overlapping Targets. Depending on
the Callout, the same unit (i.e., text segment) can represent an annotation (a Target) or a gap between
two Targets. Computation of ? is based on the overlapping characters of the annotations and the gaps
between the annotations. Naturally, if a single text string is assigned different labels (i.e. annotation
or a gap between annotations) in different annotations, ? does not produce meaningful results. The
inapplicability of Krippendorff?s ? to Targets is a significant limitation for its use in discourse annotation
(To save space we only show results for Callouts in subsequent tables.)
The examples in Section 3 show a fundamental limitation of both P/R/F1 and Krippendorff?s ?: They
do not pinpoint the location in a document where the extent of variation can be observed. This limits the
usefulness of these measures for studying the discourse phenomenon of interest and for analyzing the
impact of factors such as text difficulty, corpus and judges on IAA. The impact of these factors on IAA
also makes it hard to pick gold standard examples on a principled basis.
4 Hierarchical Clustering of Discourse Units
In this section we introduce a clustering approach that aggregates overlapping annotations, thereby mak-
ing it possible to quantify agreement among annotators within a cluster. Then we show examples of
clusters from our annotation study in which the extent of annotator support for a core reflects how hard
or easy an ADU is for human judges to identify. The hierarchical clustering technique (Hastie et al.,
2009) assumes that overlapping annotations by two or more judges constitutes evidence of the approxi-
mate location of an instance of the phenomenon of interest. In our case, this is the annotation of ADUs
that contain overlapping text. Each ADU starts in its own cluster. The start and end points of each ADU
are utilized to identify overlapping characters in pairs of ADUs. Then, using a bottom-up clustering
125
# Annots Text selected
A1, A2, A3,
A4, A5
I remember Apple telling people give the UI and the keyboard a month
and you?ll get used to it. Plus all the commercials showing the interface.
So, no, you didn?t just pick up the iPhone and know how to use it. It
was pounded into to you.
Table 9: A cluster in which all five judges agreement on the boundaries of the ADU
# Annots Text selected
A1 I?m going to agree that my experience required a bit of getting used to
. . .
A2, A3, A4 I?m going to agree that my experience required a bit of getting used to
. . . I had arrived to the newly minted 2G Gmail and browsing
A5 I?m going to agree that my experience required a bit of getting used to
. . . I had arrived to the newly minted 2G Gmail and browsing. Great
browser on the iPhone but . . . Opera Mini can work wonders
Table 10: A cluster in which all 5 annotators agree on the core but disagree on the closing boundary of
the ADU
technique, pairs of clusters (e.g. pairs of Callout ADUs) with overlapping text strings are merged as they
move up in the hierarchy. An ADU that does not overlap with ADUs identified by any other judge will
remain in its own cluster.
Aggregating overlapping annotations makes it possible to quantify agreement among the annotators
within a cluster. Table 9 shows an example of a cluster that contains five annotations; all five annotators
assign identical unit boundaries, which means that there is a single core, with no variation in the extent of
the ADU. Table 9 thus shows an optimal case ? there is complete agreement among the five annotators.
We take this as strong evidence that the text string in Table 9 is an instance of a Callout that is relatively
easy to identify.
But of course, natural language does not make optimal annotation easy (even if coders were perfect).
Table 10 shows a cluster in which all five annotators agree on the core (shown in italics) but do not
agree about the boundaries of the ADU. A1 picked the shortest text segment. A2, A3 and A4 picked the
same text segment as A1 but they also included the rest of the sentence, up to the word ?browsing?. In
A5?s judgment, the ADU is still longer - it also includes the sentence ?Great browser . . . work wonders.?
Although not as clear-cut as the examples in Table 9, the fact that in Table 10 all annotators chose
overlapping text is evidence that the core has special status in the context of in an annotation task where it
is known that even expert annotators disagree about borders. Examples like those in Table 10 can be used
to study the reasons for variation in the judges? assignment of boundaries. Besides ease of recognition
of an ADU and differing human intuitions, the instructions in the guidelines or characteristics of the
Callouts may be also having an effect.
Table 11 shows a more complex annotation pattern in a cluster. Annotators A1 and A2 agree on the
boundaries of the ADU, but their annotation does not overlap with A4 at all. A3?s boundaries subsume
all other annotations. But because A4?s boundaries do not overlap with those of A1 and A2, technically
this cluster has no core (a text segment included in all ADUs in a cluster). 5% or less of the clusters
have this problem. To handle the absence of a core in this type of cluster, we split the clusters that fit this
pattern into multiple ?overlapping? clusters, that is, we put A1, A2, and A3 into one cluster and we put
A3 and A4 into another cluster. Using this splitting technique, we get two cores, each selected by two
judges: i) ?actually the only . . . app?s developer? from the cluster containing A1, A2, and A3 (shown in
italics) and ii) ?I think it hilarious . . . another device? from the cluster containing A3 and A4 (shown in
bold). The disagreement of the judges in identifying the Callout suggests that judges have quite different
judgments about boundaries of the Callouts.
Table 12 and 13 respectively show the number of clusters with overlapping annotations for Callouts
for each thread before and after splitting. The splitting process has only a small impact on results. The
number of clusters with five and four annotators shows that in each corpus there are Callouts that are
evidently easier to identify. On the other hand, clusters selected by only two or three judges are harder to
126
# Annots Text selected
A1, A2 Actually the only one responsible for the YouTube and Twitter multitask-
ing is the app?s developer
A3 Actually the . . . app?s developer. The Facebook app allows you to watch
videos posted by . . . I think it hilarious that people complain about
features that arent even available on another device
A4 I think it hilarious that people complain about features that arent
even available on another device
Table 11: A cluster with 2 cores, each selected by 2 judges
identify. The clusters containing a text string picked by only one annotator are hardest to identify. This
may be an indication that this text string is not a good example of a Callout, though it also could be an
indication that the judge is particularly good at recognizing subtly expressed Callouts. The clustering
technique thus scaffolds deeper examination of annotation behavior and annotation/concept refinement.
Table 13 also shows that overall, the number of clusters with five or four annotators is well over 50% for
each thread except Ban, even when we exclude the clusters with an ADU identified by only one judge.
This is another hint that the IAA in this thread should be much lower than in the other threads. (See also
Figures 2 and 3).
Thread # of Clusters Annots in each cluster
5 4 3 2 1
Android 91 52 16 11 7 5
Ban 89 25 18 12 20 14
Ipad 88 41 17 7 13 10
Layoffs 86 41 18 11 6 10
Twitter 84 44 17 14 4 5
Table 12: Callouts: Clusters before splitting process
Thread # of Clusters Annots in each cluster
5 4 3 2 1
Android 93 51 15 14 8 5
Ban 91 25 19 12 21 14
iPad 89 41 16 9 13 10
Layoffs 89 40 17 14 8 10
Twitter 87 43 15 20 4 5
Table 13: Callouts: Clusters after splitting process
The clusters with cores supported by four or five annotators show strong annotator agreement and are
very strong candidates for a gold standard, regardless of the IAA for the entire thread. Clusters with
an ADU selected by only one annotator are presumably harder to annotate and are more likely than
other clusters not to be actual instances of the ADU. This information can be used to assess the output
of systems that automatically identify discourse units. For example a system could be penalized more
for missing to identifying ADUs on which all five annotators agree on the boundaries, as in Table 9;
the penalty would be decreased for not identifying ADUs on which fewer annotators agree. Qualitative
analysis may help discover the reason for the variation in strength of clusters, thereby supporting our
ability to interpret IAA and to create accurate computational models of human judgments about dis-
course units. As a related research, PAT and the clustering technique discussed in this paper allow the
development of a finer-grained annotation scheme to analyze the type of links between Target-Callout
(e.g., Agree/Disagree/Other), and the nature of Callouts (e.g., Stance/Rationale) (Ghosh et al., 2014).
5 Conclusion and Future Work
Reliability of annotation studies is important both as part of the demonstration of the validity of the
phenomena being studied and also to support accurate computational modeling of discourse phenomena.
The nature of ADUs, with their fuzzy boundaries, makes it hard to achieve IAA of .80 or higher. Fur-
thermore, the use of a single figure for IAA is a little like relying on an average to convey the range of
variation of a set of numbers. The contributions of this paper are i) to provide concrete examples of the
difficulties of using state of the art metrics like P/R/F1 and Krippendorff?s ? to assess IAA for ADUs
and ii) to open up a new approach to studying IAA that can help us understand how factors like coder
variability and text difficulty affect IAA. Our approach supports reliable identification of discourse units
independent of the overall IAA of the document.
127
References
Mark Aakhus, Smaranda Muresan, and Nina Wacholder. 2013. Integrating natural language processing and
pragmatic argumentation theories for argumentation support. pages 1?12.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Petra Saskia Bayerl and Karsten Ingmar Paul. 2011. What determines inter-coder agreement in manual annota-
tions? a meta-analytic investigation. Computational Linguistics, 37(4):699?725.
Debanjan Ghosh, Smaranda Muresan, Nina Wacholder, Mark Aakhus, and Matthew Mitsui. 2014. Analyzing
argumentative discourse units in online interactions. In Proceedings of the First Workshop on Argumentation
Mining, pages 39?48, Baltimore, Maryland, June. Association for Computational Linguistics.
Barbara J Grosz and Candace L Sidner. 1986. Attention, intentions, and the structure of discourse. Computational
linguistics, 12(3):175?204.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. 2009. The elements
of statistical learning, volume 2. Springer.
Ian Hutchby. 2013. Confrontation talk: Arguments, asymmetries, and power on talk radio. Routledge.
Klaus Krippendorff. 1995. On the reliability of unitizing continuous data. Sociological Methodology, pages
47?76.
Klaus Krippendorff. 2004a. Content analysis: An introduction to its methodology. Sage.
Klaus Krippendorff. 2004b. Measuring the reliability of qualitative text analysis data. Quality & quantity, 38:787?
800.
Douglas W Maynard. 1985. How children start arguments. Language in society, 14(01):1?29.
Philip V Ogren. 2006. Knowtator: a prot?eg?e plug-in for annotated corpus construction. In Proceedings of the
2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology: companion volume: demonstrations, pages 273?275. Association for Computational
Linguistics.
Andreas Peldszus and Manfred Stede. 2013a. From argument diagrams to argumentation mining in texts: A
survey. International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1?31.
Andreas Peldszus and Manfred Stede. 2013b. Ranking the annotators: An agreement study on argumentation
structure. In Proceedings of the 7th linguistic annotation workshop and interoperability with discourse, pages
196?204.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2008. Discourse level opinion relations: An
annotation study. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129?137.
Association for Computational Linguistics.
Frans H Van Eemeren, Rob Grootendorst, Sally Jackson, and Scott Jacobs. 1993. Reconstructing argumentative
discourse. University of Alabama Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language. Language resources and evaluation, 39(2-3):165?210.
128
