Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 17?22,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Verb Noun Construction MWE Token Supervised Classification
Mona T. Diab
Center for Computational Learning Systems
Columbia University
mdiab@ccls.columbia.edu
Pravin Bhutada
Computer Science Department
Columbia University
pb2351@columbia.edu
Abstract
We address the problem of classifying multi-
word expression tokens in running text. We
focus our study on Verb-Noun Constructions
(VNC) that vary in their idiomaticity depend-
ing on context. VNC tokens are classified as
either idiomatic or literal. We present a super-
vised learning approach to the problem. We ex-
periment with different features. Our approach
yields the best results to date on MWE clas-
sification combining different linguistically mo-
tivated features, the overall performance yields
an F-measure of 84.58% corresponding to an F-
measure of 89.96% for idiomaticity identification
and classification and 62.03% for literal identifi-
cation and classification.
1 Introduction
In the literature in general a multiword expression
(MWE) refers to a multiword unit or a colloca-
tion of words that co-occur together statistically
more than chance. A MWE is a cover term for
different types of collocations which vary in their
transparency and fixedness. MWEs are pervasive
in natural language, especially in web based texts
and speech genres. Identifying MWEs and under-
standing their meaning is essential to language un-
derstanding, hence they are of crucial importance
for any Natural Language Processing (NLP) appli-
cations that aim at handling robust language mean-
ing and use. In fact, the seminal paper (Sag et al,
2002) refers to this problem as a key issue for the
development of high-quality NLP applications.
For our purposes, a MWE is defined as a collo-
cation of words that refers to a single concept, for
example - kick the bucket, spill the beans, make a
decision, etc. An MWE typically has an idiosyn-
cratic meaning that is more or different from the
meaning of its component words. AnMWEmean-
ing is transparent, i.e. predictable, in as much
as the component words in the expression relay
the meaning portended by the speaker composi-
tionally. Accordingly, MWEs vary in their de-
gree of meaning compositionality; composition-
ality is correlated with the level of idiomaticity.
An MWE is compositional if the meaning of an
MWE as a unit can be predicted from the mean-
ing of its component words such as in make a
decision meaning to decide. If we conceive of
idiomaticity as being a continuum, the more id-
iomatic an expression, the less transparent and the
more non-compositional it is. Some MWEs are
more predictable than others, for instance, kick the
bucket, when used idiomatically to mean to die,
has nothing in common with the literal meaning
of either kick or bucket, however, make a decision
is very clearly related to to decide. Both of these
expressions are considered MWEs but have vary-
ing degrees of compositionality and predictability.
Both of these expressions belong to a class of id-
iomatic MWEs known as verb noun constructions
(VNC). The first VNC kick the bucket is a non-
decomposable VNC MWE, the latter make a deci-
sion is a decomposable VNC MWE. These types
of constructions are the object of our study.
To date, most research has addressed the prob-
lem of MWE type classification for VNC expres-
sions in English (Melamed, 1997; Lin, 1999;
Baldwin et al, 2003; na Villada Moiro?n and
Tiedemann, 2006; Fazly and Stevenson, 2007;
Van de Cruys and Villada Moiro?n, 2007; Mc-
Carthy et al, 2007), not token classification. For
example: he spilt the beans on the kitchen counter
is most likely a literal usage. This is given away by
the use of the prepositional phrase on the kitchen
counter, as it is plausable that beans could have
literally been spilt on a location such as a kitchen
counter. Most previous research would classify
spilt the beans as idiomatic irrespective of con-
textual usage. In a recent study by (Cook et al,
2008) of 53 idiom MWE types used in different
contexts, the authors concluded that almost half of
them had clear literal meaning and over 40% of
their usages in text were actually literal. Thus, it
would be important for an NLP application such
as machine translation, for example, when given
a new VNC MWE token, to be able to determine
whether it is used idiomatically or not as it could
potentially have detrimental effects on the quality
of the translation.
17
In this paper, we address the problem of MWE
classification for verb-noun (VNC) token con-
structions in running text. We investigate the bi-
nary classification of an unseen VNC token ex-
pression as being either Idiomatic (IDM) or Lit-
eral (LIT). An IDM expression is certainly an
MWE, however, the converse is not necessarily
true. To date most approaches to the problem of
idiomaticity classification on the token level have
been unsupervised (Birke and Sarkar, 2006; Diab
and Krishna, 2009b; Diab and Krishna, 2009a;
Sporleder and Li, 2009). In this study we carry
out a supervised learning investigation using sup-
port vector machines that uses some of the features
which have been shown to help in unsupervised
approaches to the problem.
This paper is organized as follows: In Section
2 we describe our understanding of the various
classes of MWEs in general. Section 3 is a sum-
mary of previous related research. Section 4 de-
scribes our approach. In Section 5 we present the
details of our experiments. We discuss the results
in Section 6. Finally, we conclude in Section 7.
2 Multi-word Expressions
MWEs are typically not productive, though they
allow for inflectional variation (Sag et al, 2002).
They have been conventionalized due to persis-
tent use. MWEs can be classified based on their
semantic types as follows. Idiomatic: This cat-
egory includes expressions that are semantically
non-compositional, fixed expressions such as king-
dom come, ad hoc, non-fixed expressions such
as break new ground, speak of the devil. The
VNCs which we are focusing on in this paper fall
into this category. Semi-idiomatic: This class
includes expressions that seem semantically non-
compositional, yet their semantics are more or less
transparent. This category consists of Light Verb
Constructions (LVC) such as make a living and
Verb Particle Constructions (VPC) such as write-
up, call-up. Non-Idiomatic: This category in-
cludes expressions that are semantically compo-
sitional such as prime minister, proper nouns such
as New York Yankees and collocations such as ma-
chine translation. These expressions are statisti-
cally idiosyncratic. For instance, traffic light is
the most likely lexicalization of the concept and
would occur more often in text than, say, traffic
regulator or vehicle light.
3 Related Work
Several researchers have addressed the problem of
MWE classification (Baldwin et al, 2003; Katz
and Giesbrecht, 2006; Schone and Juraksfy, 2001;
Hashimoto et al, 2006; Hashimoto and Kawa-
hara, 2008). The majority of the proposed research
has been using unsupervised approaches and have
addressed the problem of MWE type classifica-
tion irrespective of usage in context (Fazly and
Stevenson, 2007; Cook et al, 2007). We are
aware of two supervised approaches to the prob-
lem: work by (Katz and Giesbrecht, 2006) and
work by (Hashimoto and Kawahara, 2008).
In Katz and Giesbrecht (2006) (KG06) the au-
thors carried out a vector similarity comparison
between the context of an MWE and that of the
constituent words using LSA to determine if the
expression is idiomatic or not. The KG06 is sim-
ilar in intuition to work proposed by (Fazly and
Stevenson, 2007), however the latter work was un-
supervised. KG06 experimented with a tiny data
set of only 108 sentences corresponding to one
MWE idiomatic expression.
Hashimoto and Kawahara (2008) (HK08) is the
first large scale study to our knowledge that ad-
dressed token classification into idiomatic versus
literal for Japanese MWEs of all types. They ap-
ply a supervised learning framework using sup-
port vector machines based on TinySVM with a
quadratic kernel. They annotate a web based cor-
pus for training data. They identify 101 idiom
types each with a corresponding 1000 examples,
hence they had a corpus of 102K sentences of an-
notated data for their experiments. They exper-
iment with 90 idiom types only for which they
had more than 50 examples. They use two types
of features: word sense disambiguation (WSD)
features and idiom features. The WSD features
comprised some basic syntactic features such as
POS, lemma information, token n-gram features,
in addition to hypernymy information on words as
well as domain information. For the idiom fea-
tures they were mostly inflectional features such
as voice, negativity, modality, in addition to adja-
cency and adnominal features. They report results
in terms of accuracy and rate of error reduction.
Their overall accuracy is of 89.25% using all the
features.
4 Our Approach
We apply a supervised learning framework to
the problem of both identifying and classifying a
MWE expression token in context. We specifically
focus on VNC MWE expressions. We use the an-
notated data by (Cook et al, 2008). We adopt a
chunking approach to the problem using an Inside
Outside Beginning (IOB) tagging framework for
performing the identification of MWE VNC to-
kens and classifying them as idiomatic or literal
in context. For chunk tagging, we use the Yam-
18
Cha sequence labeling system.1 YamCha is based
on Support Vector Machines technology using de-
gree 2 polynomial kernels.
We label each sentence with standard IOB tags.
Since this is a binary classification task, we have 5
different tags: B-L (Beginning of a literal chunk),
I-L (Inside of a literal chunk), B-I (Beginning an
Idiomatic chunk), I-I (Inside an Idiomatic chunk),
O (Outside a chunk). As an example a sentence
such as John kicked the bucket last Friday will be
annotated as follows: John O, kicked B-I, the I-I,
bucket I-I, last O, Friday O. We experiment with
some basic features and some more linguistically
motivated ones.
We experiment with different window sizes for
context ranging from ?/+1 to ?/+5 tokens be-
fore and after the token of interest. We also em-
ploy linguistic features such as character n-gram
features, namely last 3 characters of a token, as
a means of indirectly capturing the word inflec-
tional and derivational morphology (NGRAM).
Other features include: Part-of-Speech (POS)
tags, lemma form (LEMMA) or the citation form
of the word, and named entity (NE) information.
The latter feature is shown to help in the unsuper-
vised setting in recent work (Diab and Krishna,
2009b; Diab and Krishna, 2009a). In general all
the linguistic features are represented as separate
feature sets explicitly modeled in the input data.
Hence, if we are modeling the POS tag feature for
our running example the training data would be
annotated as follows: {John NN O, kicked VBD
B-I, the Det I-I, bucket NN I-I, last ADV O, Friday
NN O }. Likewise adding the NGRAM feature
would be represented as follows: {John NN ohn
O, kicked VBD ked B-I, the Det the I-I, bucket NN
ket I-I, last ADV ast O, Friday NN day O.} and so
on.
With the NE feature, we followed the same rep-
resentation as the other features as a separate col-
umn as expressed above, referred to as Named
Entity Separate (NES). For named entity recogni-
tion (NER) we use the BBN Identifinder software
which identifies 19 NE tags.2 We have two set-
tings for NES: one with the full 19 tags explic-
itly identified (NES-Full) and the other where we
have a binary feature indicating whether a word
is a NE or not (NES-Bin). Moreover, we added
another experimental condition where we changed
the words? representation in the input to their NE
class, Named Entity InText (NEI). For example for
the NEI condition, our running example is repre-
sented as follows: {PER NN ohn O, kicked VBD
ked B-I, the Det the I-I, bucket NN ket I-I, last ADV
1http://www.tado-chasen.com/yamcha
2http://www.bbn.com/identifinder
ast O, DAY NN day O}, where John is replaced by
the NE ?PER? .
5 Experiments and Results
5.1 Data
We use the manually annotated standard data
set identified in (Cook et al, 2008). This data
comprises 2920 unique VNC-Token expressions
drawn from the entire British National Corpus
(BNC).3 The BNC contains 100M words of multi-
ple genres including written text and transcribed
speech. In this set, VNC token expressions are
manually annotated as idiomatic, literal or un-
known. We exclude those annotated as unknown
and those pertaining to the Speech part of the
data leaving us with a total of 2432 sentences cor-
responding to 53 VNC MWE types. This data
has 2571 annotations,4 corresponding to 2020 Id-
iomatic tokens and 551 literal ones. Since the data
set is relatively small we carry out 5-fold cross val-
idation experiments. The results we report are av-
eraged over the 5 folds per condition. We split
the data into 80% for training, 10% for testing and
10% for development. The data used is the tok-
enized version of the BNC.
5.2 Evaluation Metrics
We use F?=1 (F-measure) as the harmonic mean
between (P)recision and (R)ecall, as well as accu-
racy to report the results.5 We report the results
separately for the two classes IDM and LIT aver-
aged over the 5 folds of the TEST data set.
5.3 Results
We present the results for the different features
sets and their combination. We also present results
on a simple most frequent tag baseline (FREQ) as
well as a baseline of using no features, just the
tokenized words (TOK). The baseline is basically
tagging all identified VNC tokens in the data set as
idiomatic. It is worth noting that the baseline has
the advantage of gold identification of MWE VNC
token expressions. In our experimental conditions,
identification of a potential VNC MWE is part of
what is discovered automatically, hence our sys-
tem is penalized for identifying other VNC MWE
3http://www.natcorp.ox.ac.uk/
4A sentence can have more than one MWE expression
hence the number of annotations exceeds the number of sen-
tences.
5We do not think that accuracy should be reported in gen-
eral since it is an inflated result as it is not a measure of error.
All words identified as O factor into the accuracy which re-
sults in exaggerated values for accuracy. We report it only
since it the metric used by previous work.
19
tokens that are not in the original data set.6
In Table 2 we present the results yielded per fea-
ture and per condition. We experimented with dif-
ferent context sizes initially to decide on the opti-
mal window size for our learning framework, re-
sults are presented in Table 1. Then once that is
determined, we proceed to add features.
Noting that a window size of ?/+3 yields the
best results, we proceed to use that as our context
size for the following experimental conditions. We
will not include accuracy since it above 96% for all
our experimental conditions.
All the results yielded by our experiments out-
perform the baseline FREQ. The simple tokenized
words baseline (TOK) with no added features with
a context size of ?/+3 shows a significant im-
provement over the very basic baseline FREQwith
an overall F measure of 77.04%.
Adding lemma information or POS or NGRAM
features all independently contribute to a better
solution, however combining the three features
yields a significant boost in performance over the
TOK baseline of 2.67% absolute F points in over-
all performance.
Confirming previous observations in the liter-
ature, the overall best results are obtained by
using NE features. The NEI condition yields
slightly better results than the NES conditions
in the case when no other features are being
used. NES-Full significantly outperforms NES-
Bin when used alone especially on literal classi-
fication yielding the highest results on this class
of phenomena across the board. However when
combined with other features, NES-Bin fares bet-
ter than NES-Full as we observe slightly less per-
formance when comparing NES-Full+L+N+P and
NES-Bin+L+N+P.
Combining NEI+L+N+P yields the highest re-
sults with an overall F measure of 84.58% a sig-
nificant improvement over both baselines and over
the condition that does not exploit NE features,
L+N+P. Using NEI may be considered a form
of dimensionality reduction hence the significant
contribution to performance.
6 Discussion
The overall results strongly suggest that using lin-
guistically interesting features explicitly has a pos-
itive impact on performance. NE features help
the most and combining them with other features
6We could have easily identified all VNC syntactic con-
figurations corresponding to verb object as a potential MWE
VNC assuming that they are literal by default. This would
have boosted our literal score baseline, however, for this in-
vestigation, we decided to strictly work with the gold stan-
dard data set exclusively.
yields the best results. In general performance
on the classification and identification of idiomatic
expressions yielded much better results. This may
be due to the fact that the data has a lot more id-
iomatic token examples for training. Also we note
that precision scores are significantly higher than
recall scores especially with performance on lit-
eral token instance classification. This might be an
indication that identifying when an MWE is used
literally is a difficult task.
We analyzed some of the errors yielded in our
best condition NEI+L+N+P. The biggest errors are
a result of identifying other VNC constructions
not annotated in the training and test data as VNC
MWEs. However, we also see errors of confusing
idiomatic cases with literal ones 23 times, and the
opposite 4 times.
Some of the errors where the VNC should have
been classified as literal however the system clas-
sified them as idiomatic are kick heel, find feet,
make top. Cases of idiomatic expressions erro-
neously classified as literal are for MWE types hit
the road, blow trumpet, blow whistle, bit a wall.
The system is able to identify new VNC MWE
constructions. For instance in the sentence On the
other hand Pinkie seemed to have lost his head to
a certain extent perhaps some prospects of mak-
ing his mark by bringing in something novel in
the way of business, the first MWE lost his head
is annotated in the training data, however making
his mark is newly identified as idiomatic in this
context.
Also the system identified hit the post as a
literal MWE VNC token in As the ball hit the
post the referee blew the whistle, where blew the
whistle is a literal VNC in this context and it iden-
tified hit the post as another literal VNC.
7 Conclusion
In this study, we explore a set of features that con-
tribute to VNC token expression binary supervised
classification. The use of NER significantly im-
proves the performance of the system. Using NER
as a means of dimensionality reduction yields the
best results. We achieve a state of the art perfor-
mance of an overall F measure of 84.58%. In the
future we are looking at ways of adding more so-
phisticated syntactic and semantic features from
WSD. Given the fact that we were able to get more
interesting VNC data automatically, we are cur-
rently looking into adding the new data to the an-
notated pool after manual checking.
20
IDM-F LIT-F Overall F Overall Acc.
?/+1 77.93 48.57 71.78 96.22
?/+2 85.38 55.61 79.71 97.06
?/+3 86.99 55.68 81.25 96.93
?/+4 86.22 55.81 80.75 97.06
?/+5 83.38 50 77.63 96.61
Table 1: Results in %s of varying context window size
IDM-P IDM-R IDM-F LIT-P LIT-R LIT-F Overall F
FREQ 70.02 89.16 78.44 0 0 0 69.68
TOK 81.78 83.33 82.55 71.79 43.75 54.37 77.04
(L)EMMA 83.1 84.29 83.69 69.77 46.88 56.07 78.11
(N)GRAM 83.17 82.38 82.78 70 43.75 53.85 77.01
(P)OS 83.33 83.33 83.33 77.78 43.75 56.00 78.08
L+N+P 86.95 83.33 85.38 72.22 45.61 55.91 79.71
NES-Full 85.2 87.93 86.55 79.07 58.62 67.33 82.77
NES-Bin 84.97 82.41 83.67 73.49 52.59 61.31 79.15
NEI 89.92 85.18 87.48 81.33 52.59 63.87 82.82
NES-Full+L+N+P 89.89 84.92 87.34 76.32 50 60.42 81.99
NES-Bin+L+N+P 90.86 84.92 87.79 76.32 50 60.42 82.33
NEI+L+N+P 91.35 88.42 89.86 81.69 50 62.03 84.58
Table 2: Final results in %s averaged over 5 folds of test data using different features and their combina-
tions
8 Acknowledgement
The first author was partially funded by DARPA
GALE andMADCAT projects. The authors would
like to acknowledge the useful comments by two
anonymous reviewers who helped in making this
publication more concise and better presented.
References
Timothy Baldwin, Collin Bannard, Takakki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions, pages 89?96, Morristown, NJ, USA.
J. Birke and A. Sarkar. 2006. A clustering approach for
nearly unsupervised recognition of nonliteral lan-
guage. In Proceedings of EACL, volume 6, pages
329?336.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of the Work-
shop on A Broader Perspective on Multiword Ex-
pressions, pages 41?48, Prague, Czech Republic,
June. Association for Computational Linguistics.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2008. The VNC-Tokens Dataset. In Proceedings of
the LREC Workshop on Towards a Shared Task for
Multiword Expressions (MWE 2008), Marrakech,
Morocco, June.
Mona Diab and Madhav Krishna. 2009a. Handling
sparsity for verb noun MWE token classification. In
Proceedings of the Workshop on Geometrical Mod-
els of Natural Language Semantics, pages 96?103,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Mona Diab and Madhav Krishna. 2009b. Unsuper-
vised classification for vnc multiword expressions
tokens. In CICLING.
Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-
tinguishing subtypes of multiword expressions us-
ing linguistically-motivated statistical measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9?16, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Chikara Hashimoto and Daisuke Kawahara. 2008.
Construction of an idiom corpus and its applica-
tion to idiom identification based on WSD incor-
porating idiom-specific features. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 992?1001, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006. Japanese idiom recognition: Drawing a line
between literal and idiomatic meanings. In Proceed-
ings of the COLING/ACL 2006 Main Conference
21
Poster Sessions, pages 353?360, Sydney, Australia,
July. Association for Computational Linguistics.
Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 12?19, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
pages 317?324, Univeristy of Maryland, College
Park, Maryland, USA.
Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 369?379, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Dan I. Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of the 2nd Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP?97),
pages 97?108, Providence, RI, USA, August.
Bego na Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL-06
Workshop on Multiword Expressions in a Multilin-
gual Context, pages 33?40, Morristown, NJ, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Pro-
ceedings of the Third International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, pages 1?15, London, UK. Springer-Verlag.
Patrick Schone and Daniel Juraksfy. 2001. Is
knowledge-free induction of multiword unit dictio-
nary headwords a solved problem? In Proceedings
of Empirical Methods in Natural Language Process-
ing, pages 100?108, Pittsburg, PA, USA.
C. Sporleder and L. Li. 2009. Unsupervised Recog-
nition of Literal and Non-Literal Use of Idiomatic
Expressions. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 754?762. Association for Computational Lin-
guistics.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pages 25?32,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
22
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 840?848,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning about Voice Search for Spoken Dialogue Systems 
Rebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio2,  
Joshua B. Gordon4, Pravin Bhutada4 
1Center for Computational Learning Systems, Columbia University 
2Department of Computer Science, Hunter College of The City University of New York 
3Department of Computer Science, The Graduate Center of The City University of New York 
4Department of Computer Science, Columbia University 
becky@cs.columbia.edu, susan.epstein@hunter.cuny.edu, tligorio@gc.cuny.edu, 
joshua@cs.columbia.edu, pravin.bhutada@gmail.com 
Abstract 
In a Wizard-of-Oz experiment with multiple 
wizard subjects, each wizard viewed automated 
speech recognition (ASR) results for utterances 
whose interpretation is critical to task success: 
requests for books by title from a library data-
base. To avoid non-understandings, the wizard 
directly queried the application database with 
the ASR hypothesis (voice search). To learn 
how to avoid misunderstandings, we investi-
gated how wizards dealt with uncertainty in 
voice search results. Wizards were quite suc-
cessful at selecting the correct title from query 
results that included a match. The most suc-
cessful wizard could also tell when the query 
results did not contain the requested title. Our 
learned models of the best wizard?s behavior 
combine features available to wizards with 
some that are not, such as recognition confi-
dence and acoustic model scores.  
1 Introduction 
Wizard-of-Oz (WOz) studies have long been used 
for spoken dialogue system design. In a relatively 
new variant, a subject (the wizard) is presented 
with real or simulated automated speech recogni-
tion (ASR) to observe how people deal with incor-
rect speech recognition output (Rieser, Kruijff-
Korbayov?, & Lemon, 2005; Skantze, 2003; 
Stuttle, Williams, & Young, 2004; Williams & 
Young, 2003, 2004; Zollo, 1999). In these experi-
ments, when a wizard could not interpret the ASR 
output (non-understanding), she rarely asked users 
to repeat themselves. Instead, the wizard found 
other ways to continue the task.  
This paper describes an experiment that pre-
sented wizards with ASR results for utterances 
whose interpretation is critical to task success: re-
quests for books from a library database, identified 
by title. To avoid non-understandings, wizards 
used voice search (Wang et al, 2008): they direct-
ly queried the application database with ASR out-
put. To investigate how to avoid errors in 
understanding (misunderstandings), we examined 
how wizards dealt with uncertainty in voice search 
results. When the voice search results included the 
requested title, all seven of our wizards were likely 
to identify it. One wizard, however, recognized far 
better than the others when the voice search results 
did not contain the requested title. The experiment 
employed a novel design that made it possible to 
include system features in models of wizard beha-
vior. The principal result is that our learned models 
of the best wizard?s behavior combine features that 
are available to wizards with some that are not, 
such as recognition confidence and acoustic model 
scores. 
The next section of the paper motivates our ex-
periment. Subsequent sections describe related 
work, the dialogue system and embedded wizard 
infrastructure, experimental design, learning me-
thods, and results. We then discuss how to general-
ize from the results of our study for spoken 
dialogue system design. We conclude with a sum-
mary of results and their implications. 
2 Motivation 
Rather than investigate full dialogues, we ad-
dressed a single type of turn exchange or adjacency 
pair (Sacks et al, 1974): a request for a book by its 
840
title. This allowed us to collect data exclusively 
about an utterance type critical for task success in 
our application domain. We hypothesized that low-
level features from speech recognition, such as 
acoustic model fit, could independently affect 
voice search confidence. We therefore applied a 
novel approach, embedded WOz, in which a wizard 
and the system together interpret noisy ASR. 
To address how to avoid misunderstandings, we 
investigated how wizards dealt with uncertainty in 
voice search returns. To illustrate what we mean 
by uncertainty, if we query our book title database 
with the ASR hypothesis: 
ROLL DWELL 
our voice search procedure returns, in this order: 
CROMWELL 
ROBERT LOWELL 
ROAD TO WEALTH 
The correct title appears last because of the score it 
is assigned by the string similarity metric we use.  
Three factors motivated our use of voice search 
to interpret book title requests: noisy ASR, un-
usually long query targets, and high overlap of the 
vocabulary across different query types (e.g., au-
thor and title) as well as with non-query words in 
caller utterances (e.g., ?Could you look up . . .?).  
First, accurate speech recognition for a real-
world telephone application can be difficult to 
achieve, given unpredictable background noise and 
transmission quality. For example, the 68% word 
error rate (WER) for the fielded version of Let?s 
Go Public! (Raux et al, 2005) far exceeded its 
17% WER under controlled conditions. Our appli-
cation handles library requests by telephone, and 
would benefit from robustness to noisy ASR. 
Second, the book title field in our database dif-
fers from the typical case for spoken dialogue sys-
tems that access a relational database. Such 
systems include travel booking (Levin et al, 2000), 
bus route information (Raux et al, 2006), restau-
rant guides (Johnston et al, 2002; Komatani et al, 
2005), weather (Zue et al, 2000) and directory 
services (Georgila et al, 2003). In general for these 
systems, a few words are sufficient to retrieve the 
desired attribute value, such as a neighborhood, a 
street, or a surname. Mean utterance length in a 
sample of 40,000 Let?s Go Public! utterances, for 
example, is 2.4 words. The average book title 
length in our database is 5.4 words. 
Finally, our dialogue system, CheckItOut, al-
lows users to choose whether to request books by 
title, author, or catalogue number. The database 
represents 5028 active patrons (with real borrow-
ing histories and preferences but fictitious personal 
information), 71,166 book titles and 28,031 au-
thors. Though much smaller than a database for a 
directory service application (Georgila et al, 
2003), this is much larger than that of many current 
research systems. For example, Let?s Go Public! 
accesses a database with 70 bus routes and 1300 
place names. Titles and author names contribute 
50,394 words to the vocabulary, of which 57.4% 
occur only in titles, 32.1% only in author names, 
and 10.5% in both. Many book titles (e.g., You See 
I Haven?t Forgotten, You Never Know) have a high 
potential for confusability with non-title phrases in 
users? book requests. Given the longer database 
field and the confusability of the book title lan-
guage, integrating voice search is likely to have a 
relatively larger impact in CheckItOut.  
We seek to minimize non-understandings and 
misunderstandings for several reasons. First, user 
corrections in both situations have been shown to 
be more poorly recognized than non-correction ut-
terances (Litman et al, 2006). Non-understandings 
typically result in re-prompting the user for the 
same information. This often leads to hyper-
articulation and concomitant degradation in recog-
nition performance. Second, users seem to prefer 
systems that minimize non-understandings and mi-
sunderstandings, even at the expense of dialogue 
efficiency. Users of the TOOT train information 
spoken dialogue system preferred system-initiative 
to mixed- or user-initiative, and preferred explicit 
confirmation to implicit or no confirmation 
(Litman & Pan, 1999). This was true despite the 
fact that a mixed-initiative, implicit confirmation 
strategy led to fewer turns for the same task. Most 
of the more recent work on spoken dialogue sys-
tems focuses on mixed-initiative systems in labora-
tory settings. Still, recent work suggests that while 
mixed- or user-initiative is rated highly in usability 
studies, under real usage it ?fails to provide [a] ro-
bust enough interface? (Turunen et al, 2006). In-
corporating accurate voice search into spoken 
dialogue systems could lead to fewer non-
understandings and fewer misunderstandings. 
3 Related Work 
Our approach to noisy ASR contrasts with many 
other information-seeking and transaction-based 
dialogue systems. Those systems typically perform 
841
natural language understanding on ASR output be-
fore database query with techniques that try to im-
prove or expand ASR output. None that we know 
of use voice search. For one directory service ap-
plication, users spell the first three letters of sur-
names, and then ASR results are expanded using 
frequently confused phones (Georgila et al, 2003). 
A two-pass recognition architecture added to Let?s 
Go Public! improved concept recognition in post-
confirmation user utterances (Stoyanchev & Stent, 
2009). In (Komatani et al, 2005), a shallow se-
mantic interpretation phase was followed by deci-
sion trees to classify utterances as relevant either to 
query type or to specific query slots, to narrow the 
set of possible interpretations. CheckItOut is most 
similar in spirit to the latter approach, but relies on 
the database earlier, and only for semantic interpre-
tation, not to also guide the dialogue strategy. 
Our approach to noisy ASR is inspired by pre-
vious WOz studies with real (Skantze, 2003; Zollo, 
1999) or simulated ASR (Kruijff-Korbayov? et al, 
2005; Rieser et al, 2005; Williams & Young, 
2004). Simulation makes it possible to collect di-
alogues without building a speech recognizer, and 
to control for WER. In the studies that involved 
task-oriented dialogues, wizards typically focused 
more on the task and less on resolving ASR errors 
(Williams & Young, 2004; Skantze, 2003; Zollo, 
1999). In studies more like the information-seeking 
dialogues addressed here, an entirely different pat-
tern is observed (Kruijff-Korbayov? et al, 2005; 
Rieser et al, 2005). 
Zollo collected seven dialogues with different 
human-wizard pairs to develop an evacuation plan. 
The overall WER was 30%. Of the 227 cases of 
incorrect ASR, wizard utterances indicated a fail-
ure to understand for only 35% of them. Wizards 
ignored words not salient in the domain and hy-
pothesized words based on phonetic similarity. In 
(Skantze, 2003), both users and wizards knew 
there was no dialogue system; 44 direction-finding 
dialogues were collected with 16 subjects. Despite 
a WER of 43%, the wizard operators signaled mis- 
understanding only 5% of the time, in part because 
they often ignored ASR errors and continued the 
dialogue. For the 20% of non-understandings, op-
erators continued a route description, asked a task-
related question, or requested a clarification.  
Williams and Young collected 144 dialogues 
simulating tourist requests for directions and other 
negotiations. WER was constrained to be high, 
medium, or low. Under medium WER, a task-
related question in response to a non-understanding 
or misunderstanding led to full understanding more 
often than explicit repairs. Under high WER, how-
ever, the reverse was true. Misunderstandings sig-
nificantly increased when wizards followed non-
understandings or misunderstandings with a task-
related question instead of a repair. 
In (Rieser et al, 2005), wizards simulated a 
multimodal MP3 player application with access to 
a database of 150K music albums. Responses 
could be presented verbally or graphically. In the 
noisy transcription condition, wizards made clarifi-
cation requests about twice as often as that found 
in similar human-human dialogue.  
In a system like CheckItOut, user utterances that 
request database information must be understood. 
We seek an approach that would reduce the rate of 
misunderstandings observed for high WER in 
(Williams & Young, 2004) and the rate of clarifi-
cation requests observed in (Rieser et al, 2005). 
4 CheckItOut and Embedded Wizards 
CheckItOut is modeled on library transactions at 
the Andrew Heiskell Braille and Talking Book Li-
brary, a branch of the New York Public Library 
and part of the National Library of Congress. Bor-
rowing requests are handled by telephone. Books, 
mainly in a proprietary audio format, travel by 
mail. In a dialogue with CheckItOut, a user identi-
fies herself, requests books, and is told which are 
available for immediate shipment or will go on re-
serve. The user can request a book by catalogue 
number, title, or author. 
CheckItOut builds on the Olympus/RavenClaw 
framework (Bohus & Rudnicky, 2009) that has 
been the basis for about a dozen dialogue systems 
in different domains, including Let?s Go Public! 
(Raux et al, 2005). Speech recognition relies on 
PocketSphinx. Phoenix, a robust context-free 
grammar (CFG) semantic parser, handles natural 
language understanding (Ward & Issar, 1994). The 
Apollo interaction manager (Raux & Eskenazi, 
2007) detects utterance boundaries using informa-
tion from speech recognition, semantic parsing, 
and Helios, an utterance-level confidence annotator 
(Bohus & Rudnicky, 2002). The dialogue manager 
is implemented in RavenClaw. 
842
To design CheckItOut?s dialogue manager, we 
recorded 175 calls (4.5 hours) from patrons to li-
brarians. We identified 82 book request calls, tran-
scribed them, aligned the utterances with the 
speech signal, and annotated the transcripts for di-
alogue acts. Because active patrons receive 
monthly newsletters listing new titles in the desired 
formats, patrons request specific items with ad-
vance knowledge of the author, title, or catalogue 
number. Most book title requests accurately repro-
duce the exact title, the title less an initial deter-
miner (?the,? ?a?), or a subtitle.  
We exploited the Galaxy message passing archi-
tecture of Olympus/RavenClaw to insert a wizard 
server into CheckItOut. The hub passes messages 
between the system and a wizard?s graphical user 
interface (GUI), allowing us to collect runtime in-
formation that can be included in models of wi-
zards? actions.  
For speech recognition, CheckItOut relies on 
PocketSphinx 0.5, a Hidden Markov Model-based 
recognizer. Speech recognition for this experiment, 
relied on the freely available Wall Street Journal 
?read speech? acoustic models. We did not adapt 
the models to our population or to spontaneous 
speech, thus insuring that wizards would receive 
relatively noisy recognition output.  
We built trigram language models from the 
book titles using the CMU Statistical Language 
Modeling Toolkit. Pilot tests with one male and 
one female native speaker indicated that a lan-
guage model based on 7500 titles would yield 
WER in the desired range. (Average WER for the 
book title requests in our experiment was 71%.) To 
model one aspect of the real world useful for an ac-
tual system, titles with below average circulation 
were eliminated. An offline pilot study had demon-
strated that one-word titles were easy for wizards, 
so we eliminated those as well. A random sample 
of 7,500 was chosen from the remaining 19,708 
titles to build the trigram language model. 
We used Ratcliff/Obersherhelp (R/O) to meas-
ure the similarity of an ASR string to book titles in 
the database (Ratcliff & Metzener, 1988). R/O cal-
culates the ratio r of the number of matching cha-
racters to the total length of both strings, but 
requires O(r2) time on average and O(r3) time in 
the worst case. We therefore computed an upper 
bound on the similarity of a title/ASR pair prior to 
full R/O to speed processing.  
5 Experimental Design 
In this experiment, a user and a wizard sat in sepa-
rate rooms where they could not overhear one 
another. Each had a headset with microphone and a 
GUI. Audio input on the wizard?s headset was dis-
abled. When the user requested a title, the ASR 
hypothesis for the title appeared on the wizard?s 
GUI. The wizard then selected the ASR hypothesis 
to execute a voice search against the database.  
Given the ASR and the query return, the wi-
zard?s task was to guess which candidate in the 
query return, if any, matched the ASR hypothesis. 
Voice search accessed the full backend of 71,166 
titles. The custom query designed for the experi-
ment produced four types of return, in real time, 
based on R/O scores: 
? Singleton: a single best candidate (R/O ? 0.85) 
? AmbiguousList: two to five moderately good 
candidates (0.85 > R/O ? 0.55) 
? NoisyList: six to ten poor but non-random can-
didates (0.55 > R/O ? 0.40) 
? Empty: No candidate titles (max R/O < 0.40) 
In pilot tests, 5%-10% of returns were empty ver-
sus none in the experiment. The distribution of 
other returns was: 46.7% Singleton, 50.5% Ambi-
guousList, and 2.8% NoisyList. 
Seven undergraduate computer science majors 
at Hunter College participated. Two were non-
native speakers of English (one Spanish, one Ro-
manian). Each of the possible 21 pairs of students 
met for five trials. During each trial, one student 
served as wizard and the other as user for a session 
of 20 title cycles. They immediately reversed roles 
for a second session, as discussed further below. 
The experiment yielded 4172 title cycles rather 
than the full 4200, because users were permitted to 
end sessions early. All titles were selected from the 
7500 used to construct the language model.  
Each user received a printed list of 20 titles and 
a brief synopsis of each book. The acoustic quality 
of titles read individually from a list is unlikely to 
approximate that of a patron asking for a specific 
title. Therefore, immediately before each session, 
the user was asked to read a synopsis of each book, 
and to reorder the titles to reflect some logical 
grouping, such as genre or topic. Users requested 
titles in this new order that they had created.  
Participants were encouraged to maximize a ses-
sion score, with a reward for the experiment win-
ner. Scoring was designed to foster cooperative 
843
strategies. The wizard scored +1 for a correctly 
identified title, +0.5 for a thoughtful question, and 
-1 for an incorrect title. The user scored +0.5 for a 
successfully recognized title. User and wizard 
traded roles for the second session, to discourage 
participants from sabotaging the others? scores.  
The wizard?s GUI presented a real-time live 
feed of ASR hypotheses, weighted by grayscale to 
reflect acoustic confidence. Words in each candi-
date title that matched a word in the ASR appeared 
darker: dark black for Singleton or AmbiguousList, 
and medium black for NoisyList. All other words 
were in grayscale in proportion to the degree of 
character overlap. The wizard queried the database 
with a recognition hypothesis for one utterance at a 
time, but could concatenate successive utterances, 
possibly with some limited editing.  
After a query, the wizard?s GUI displayed can-
didate matches in descending order of R/O score. 
The wizard had four options: make a firm choice of 
a candidate, make a tentative choice, ask a ques-
tion, or give up to end the title cycle. Questions 
were recorded. The wizard?s GUI showed the suc-
cess or failure of each title cycle before the next 
one began. The user?s GUI posted the 20 titles to 
be read during the session. On the GUI, the user 
rated the wizard?s title choices as correct or incor-
rect. Titles were highlighted green if the user 
judged a wizard?s offered title correct, red if incor-
rect, yellow if in progress, and not highlighted if 
still pending. The user also rated the wizard?s 
questions. Average elapsed time for each 20-title 
session was 15.5 minutes. 
A questionnaire similar to the type used in 
PARADISE evaluations (Walker et al, 1998) was 
administered to wizards and users for each pair of 
sessions. On a 5-point Likert scale, the average re-
sponse to the question ?I found the system easy to 
use this time? was 4 (sd=0; 4=Agree), indicating 
that participants were comfortable with the task. 
All other questions received an average score of 
Neutral (3) or Disagree (2). For example, partici-
pants were neutral (3) regarding confidence in 
guessing the correct title, and disagreed (2) that 
they became more confident as time went on. 
6 Learning Method and Goals 
To model wizard actions, we assembled 60 fea-
tures that would be available at run time. Part of 
our task was to detect their relative independence, 
meaningfulness, and predictive ability. Features 
described the wizard?s GUI, the current title ses-
sion, similarity between ASR and candidates, ASR 
relevance to the database, and recognition and con-
fidence measures. Because the number of voice 
search returns varied from one title to the next, fea-
tures pertaining to candidates were averaged.  
We used three machine-learning techniques to 
predict wizards? actions: decision trees, linear re-
gression, and logistic regression. All models were 
produced with the Weka data mining package, us-
ing 10-fold cross-validation (Witten & Frank, 
2005). A decision tree is a predictive model that 
maps feature values to a target value. One applies a 
decision tree by tracing a path from the root (the 
top node) to a leaf, which provides the target value. 
Here the leaves are the wizard actions: firm choice, 
tentative choice, question, or give up. The algo-
rithm used is a version of C4.5 (Quinlan, 1993), 
where gain ratio is the splitting criterion. 
To confirm the learnability and quality of the 
decision tree models, we also trained logistic re-
gression and linear regression models on the same 
data, normalized in [0, 1]. The logistic regression 
model predicts the probability of wizards? actions 
by fitting the data to a logistic curve. It generalizes 
the linear model to the prediction of categorical da-
ta; here, categories correspond to wizards? actions. 
The linear regression models represent wizards? 
actions numerically, in decreasing value: firm 
choice, tentative choice, question, give up.  
Although analysis of individual wizards has not 
been systematic in other work, we consider the 
variation in human performance significant. Be-
cause we seek excellent, not average, teachers for 
CheckItOut, our focus is on understanding good 
wizardry. Therefore, we learned two kinds of mod-
els with each of the three methods: the overall 
model using data from all of our wizards, and indi-
vidual wizard models.  
Preliminary cross-correlation confirmed that 
many of the 60 features were heavily interdepen-
dent. Through an initial manual curation phase, we 
isolated groups of features with R2 > 0.5. When 
these groups referenced semantically similar fea-
tures, we selected a single representative from the 
group and retained only that one. For example, the 
features that described similarity between hypo-
theses and candidates were highly correlated, so 
we chose the most comprehensive one: the number 
of exact word matches. We also grouped together 
844
and represented by a single feature: three features 
that described the gaps between exact word 
matches, three that described the data presented to 
the wizard, nine that described various system con-
fidence scores, and three that described the user?s 
speaking rate. This left 28 features.  
Next we ran CfsSubsetEval, a supervised 
attribute selection algorithm for each model 
(Witten & Frank, 2005). This greedy, hill-climbing 
algorithm with backtracking evaluates a subset of 
attributes by the predictive ability of each feature 
and the degree of redundancy among them. This 
process further reduced the 28 features to 8-12 fea-
tures per model. Finally, to reduce overfitting for 
decision trees, we used pruning and subtree rising. 
For linear regression we used the M5 method, re-
peatedly removing the attribute with the smallest 
standardized coefficient until there was no further 
improvement in the error estimate given by the 
Akaike information criterion. 
7 Results 
Table 1 shows the number of title cycles per wi-
zard, the raw session score according to the formu-
la given to the wizards, and accuracy. Accuracy is 
the proportion of title cycles where the wizard 
found the correct title, or correctly guessed that the 
correct title was not present (asked a question or 
gave up). Note that score and accuracy are highly 
correlated (R=0.91, p=0.0041), indicating that the 
instructions to participants elicited behavior con-
sistent with what we wanted to measure. 
Wizards clearly differed in performance, large-
ly due to their response when the candidate list did 
not include the correct title. Analysis of variance 
with wizard as predictor and accuracy as the de-
pendent variable is highly significant (p=0.0006); 
significance is somewhat greater (p=0.0001) where 
session score is the dependent variable. Table 2 
shows the distribution of correct actions: to offer a 
candidate at a given position in the query return 
(Returns 1 through 9), or to ask a question or give 
up. As reflected in Table 2, a baseline accuracy of 
about 65% could be achieved by offering the first 
return. The fifth column of Table 1 shows how of-
ten wizards did that (Offered Return 1), and clearly 
illustrates that those who did so most often (W3 
and W6) had accuracy results closest to the base-
line. The wizard who did so least often (W4) had 
the highest accuracy, primarily because she more 
often correctly offered no title, as shown in the last 
column of Table 1. We conclude that a spoken di-
alogue system would do well to emulate W4. 
Overall, our results in modeling wizards? actions 
were uniform across the three learning methods, 
gauged by accuracy and F measure. For the com-
bined wizard data, logistic regression had an accu-
racy of 75.2%, and F measures of 0.83 for firm 
choices and 0.72 for tentative choices; the decision 
tree accuracy was 82.2%, and the F measures for 
firm versus tentative choices were respectively 
0.82 and 0.71. The decision tree had a root mean 
squared error of 0.306, linear regression 0.483. Ta-
ble 3 shows the accuracy and F measures on firm 
choices for the decision trees by individual wizard, 
along with the numbers of attributes and nodes per 
Table 1. Raw session score, accuracy, proportion of offered titles that were listed first in the query return, and 
frequency of correct non-offers for seven participants. 
 
Participant Cycles Session Score Accuracy Offered Return 1 Correct Non-Offers 
W4 600 0.7585 0.8550 0.70 0.64 
W5 600 0.7584 0.8133 0.76 0.43 
W7 599 0.6971 0.7346 0.76 0.14 
W1 593 0.6936 0.7319 0.79 0.16 
W2 599 0.6703 0.7212 0.74 0.10 
W3 581 0.6648 0.6954 0.81 0.20 
W6 600 0.6103 0.6950 0.86 0.03 
Table 2. Distribution of correct actions 
 
Correct Action N % 
Return 1 2722 65.2445 
Return 2 126 3.0201 
Return 3 56 1.3423 
Return 4 46 1.1026 
Return 5 26 0.6232 
Return 7 7 0.1678 
Return 8 1 0.0002 
Return 9 2 0.0005 
Question or Giveup 1186 28.4276 
Total 4172 1.0000 
845
tree. Although relatively few attributes appeared in 
any one tree, most attributes appeared in multiple 
nodes. W1 was the exception, with a very small 
pruned tree of 7 nodes. 
Accuracy of the decision trees does not correlate 
with wizard rank. In general, the decision trees 
could consistently predict a confident choice (0.80 
? F ? 0.87), but were less consistent on a tentative 
choice (0.60 ? F ? 0.89), and could predict a ques-
tion only for W4, the wizard with the highest accu-
racy and greatest success at detecting when the 
correct title was not in the candidates.  
What wizards saw on the GUI, their recent suc-
cess, and recognizer confidence scores were key 
attributes in the decision trees. The five features 
that appeared most often in the root and top-level 
nodes of all tree models reported in Table 3 were: 
? DisplayType of the return (Singleton, Ambi-
guous List, NoisyList) 
? RecentSuccess, how often the wizard chose the 
correct title within the last three title cycles 
? ContiguousWordMatch, the maximum number 
of contiguous exact word matches between a 
candidate and the ASR hypothesis (averaged 
across candidates) 
? NumberOfCandidates, how many titles were re-
turned by the voice search 
? Confidence, the Helios confidence score 
DisplayType, NumberOfCandidates and Conti-
guousWordMatch pertain to what the wizard could 
see on her GUI. (Recall that DisplayType is distin-
guished by font darkness, as well as by number of 
candidates.) The impact of RecentSuccess might 
result not just from the wizard?s confidence in her 
current strategy, but also from consistency in the 
user?s speech characteristics. The Helios confi-
dence annotation uses a learned model based on 
features from the recognizer, the parser, and the di-
alogue state. Here confidence primarily reflects 
recognition confidence; due to the simplicity of our 
grammar, parse results only indicate whether there 
is a parse. In addition to these five features, every 
tree relied on at least one measure of similarity be-
tween the hypothesis and the candidates.  
W4 achieved superior accuracy: she knew when 
to offer a title and when not to. In the learned tree 
for W4, if the DisplayType was NoisyList, W4 
asked a question; if DisplayType was Ambiguous-
List, the features used to predict W4?s action in-
cluded the five listed above, along with the acous-
tic model score, word length of the ASR, number 
of times the wizard had asked the user to repeat, 
and the maximum size of the gap between words in 
the candidates that matched the ASR hypothesis. 
To focus on W4?s questioning behavior, we 
trained an additional decision tree to learn how W4 
chose between two actions: offering a title versus 
asking a question. This 37-node, 8-attribute tree 
was based on 600 data points, with F=0.91 for 
making an offer and F=0.68 for asking a question. 
The tree is distinctive in that it splits at the root on 
the number of frames in the ASR. If the ASR is 
short (as measured both by the number of recogni-
tion frames and the words), W4 asks a question 
when DisplayType = AmbiguousList or NoisyList, 
either RecentSuccess ? 1 or ContiguousWord-
Match = 0, and the acoustic model score is low. 
Note that shorter titles are more confusable. If the 
ASR is long, W4 asks a question when Conti-
guousWordMatch ? 1, RecentSuccess ? 2, and ei-
ther CandidateDisplay = NoisyList, or Confidence 
is low, and there is a choice of titles. 
8 Discussion 
Our experiment addressed whether voice search 
can compensate for incorrect ASR hypotheses and 
permit identification of a user?s desired book, giv-
en a request by title. The results show that with 
high WER, a baseline dialogue strategy that always 
offers the highest-ranked database return can nev-
ertheless achieve moderate accuracy. This is true 
even with the relatively simplistic measure of simi-
larity between the ASR hypothesis and candidate 
titles used here. As a result, we have integrated 
voice search into CheckItOut, along with a linguis-
tically motivated grammar for book titles. Our cur-
rent Phoenix grammar relies on CFG rules 
automatically generated from dependency parses 
of the book titles, using the MICA parser 
Table 3. Learning results for wizards 
 
Tree Rank Nodes Attributes Accuracy F firm 
W4 1  55 12 75.67 0.85 
W5 2  21 10 76.17 0.85 
W1 3  7 8 80.44 0.87 
W7 4  45 11 73.62 0.83 
W3 5  33 10 77.42 0.84 
W2 6  35 10 78.49 0.85 
W6 7  23 10 85.19 0.80 
 
846
(Bangalore et al, 2009). As described in (Gordon 
& Passonneau, 2010), a book title parse can con-
tain multiple title slots that consume discontinuous 
sequences of words from the ASR hypothesis, thus 
accommodating noisy ASR. For the voice search 
phase, we now concatenate the words consumed by 
a sequence of title slots. We are also experimenting 
with a statistical machine learning approach that 
will replace or complement the semantic parsing. 
Computers clearly do some tasks faster and 
more accurately than people, including database 
search. To benefit from such strengths, a dialogue 
system should also accommodate human prefe-
rences in dialogue strategy. Previous work has 
shown that user satisfaction depends in part on task 
success, but also on minimizing behaviors that can 
increase task success but require the user to correct 
the system (Litman et al, 2006). 
The decision tree that models W4 has lower ac-
curacy than other models? (see Table 3), in part be-
cause her decisions had finer granularity. A spoken 
dialogue system could potentially do as well as or 
better than the best human at detecting when the 
title is not present, given the proper training data. 
To support this, a dataset could be created that was 
biased toward a larger proportion of cases where 
not offering a candidate is the correct action.  
9 Conclusion and Current Work 
This paper presents a novel methodology that em-
beds wizards in a spoken dialogue system, and col-
lects data for a single turn exchange. Our results 
illustrate the merits of ranking wizards, and learn-
ing from the best. Our wizards were uniformly 
good at choosing the correct title when it was 
present, but most were overly eager to identify a 
title when it was not among the candidates. In this 
respect, the best wizard (W4) achieved the highest 
accuracy because she demonstrated a much greater 
ability to know when not to offer a title. We have 
shown that it is feasible to replicate this ability in a 
model learned from features that include the pres-
entation of the search results (length of the candi-
date list, amount of word overlap of candidates 
with the ASR hypothesis), recent success at select-
ing the correct candidate, and measures pertaining 
to recognition results (confidence, acoustic model 
score, speaker rate). If replicated in a spoken di-
alogue system, such a model could support integra-
tion of voice search in a way that avoids 
misunderstandings. We conclude that learning 
from embedded wizards can exploit a wider range 
of relevant features, that dialogue managers can 
profit from access to more fine-grained representa-
tions of user utterances, and that machine learners 
should be selective about which people to model. 
That wizard actions can be modeled using sys-
tem features bodes well for future work. Our next 
experiment will collect full dialogues with embed-
ded wizards whose actions will again be restricted 
through an interface. This time, NLU will integrate 
voice search with the linguistically motivated CFG 
rules for book titles described earlier, and a larger 
language model and grammar for database entities. 
We will select wizards who perform well during 
pilot tests. Again, the goal will be to model the 
most successful wizards, based upon data from 
recognition results, NLU, and voice search results. 
Acknowledgements 
This research was supported by the National 
Science Foundation under IIS-0745369, IIS-
084966, and IIS-0744904. We thank the anonym-
ous reviewers, the Heiskell Library, our CMU col-
laborators, our statistical wizard Liana Epstein, and 
our enthusiastic undergraduate research assistants. 
References 
Bangalore, Srinivas; Bouillier, Pierre; Nasr, Alexis; 
Rambow, Owen; Sagot, Benoit (2009). MICA: a 
probabilistic dependency parser based on tree 
insertion grammars. Application Note. Human 
Language Technology and North American Chapter 
of the Association for Computational Linguistics, 
pp. 185-188.  
Bohus, D.; Rudnicky, A.I. (2009). The RavenClaw 
dialog management framework: Architecture and 
systems. Computer Speech and Language, 23(3), 
332-361. 
Bohus, Daniel; Rudnicky, Alex (2002). Integrating 
multiple knowledge sources for utterance-level 
confidence annotation in the CMU Communicator 
spoken dialog system (Technical Report No. CS-
190): Carnegie Mellon University. 
Georgila, Kallirroi; Sgarbas, Kyrakos; Tsopanoglou, 
Anastasios; Fakotakis, Nikos; Kokkinakis, George 
(2003). A speech-based human-computer interaction 
system for automating directory assistance services. 
International Journal of Speech Technology, Special 
Issue on Speech and Human-Computer Interaction, 
6(2), 145-59. 
847
Gordon, Joshua, B.; Passonneau, Rebecca J. (2010). An 
evaluation framework for natural language 
understanding in spoken dialogue systems. Seventh 
International Conference on Language Resources 
and Evaluation (LREC). 
Johnston, Michael; Bangalore, Srinivas; Vasireddy, 
Gunaranjan; Stent, Amanda; Ehlen, Patrick; Walker, 
Marilyn A., et al (2002). MATCH--An architecture 
for multimodal dialogue systems. Proceedings of the 
40th Annual Meeting of the Association for 
Computational Linguistics, pp. 376-83.  
Komatani, Kazunori; Kanda, Naoyuki; Ogata, Tetsuya; 
Okuno, Hiroshi G. (2005). Contextual constraints 
based on dialogue models in database search task 
for spoken dialogue systems. The Ninth European 
Conference on Speech Communication and 
Technology (Eurospeech), pp. 877-880.  
Kruijff-Korbayov?, Ivana; Blaylock, Nate; 
Gerstenberger, Ciprian; Rieser, Verena; Becker, 
Tilman; Kaisser, Michael, et al (2005). An 
experiment setup for collecting data for adaptive 
output planning in a multimodal dialogue system. 
10th European Workshop on Natural Language 
Generation (ENLG), pp. 191-196.  
Levin, Esther; Narayanan, Shrikanth; Pieraccini, 
Roberto; Biatov, Konstantin; Bocchieri, E.; De 
Fabbrizio, Giuseppe, et al (2000). The AT&T-
DARPA Communicator Mixed-Initiative Spoken 
Dialog System. Sixth International Conference on 
Spoken Dialogue Processing (ICLSP), pp. 122-125.  
Litman, Diane; Hirschberg, Julia; Swerts, Marc (2006). 
Characterizing and predicting corrections in spoken 
dialogue systems. Computational Linguistics, 32(3), 
417-438. 
Litman, Diane; Pan, Shimei (1999). Empirically 
evaluating an adaptable spoken dialogue system. 7th 
International Conference on User Modeling (UM), 
pp. 55-46.  
Quinlan, J. Ross (1993). C4.5: Programs for Machine 
Learning. San Mateo, CA: Morgan Kaufmann. 
Ratcliff, John W.; Metzener, David (1988). Pattern 
Matching: The Gestalt Approach. Dr. Dobb's 
Journal, 46 
Raux, Antoine; Bohus, Dan; Langner, Brian; Black, 
Alan W.; Eskenazi, Maxine (2006). Doing research 
on a deployed spoken dialogue system: one year of 
Let's Go! experience. Ninth International 
Conference on Spoken Language Processing 
(Interspeech/ICSLP).  
Raux, Antoine; Eskenazi, Maxine (2007). A Multi-layer 
architecture for semi-synchronous event-driven 
dialogue management.IEEE Workshop on 
Automatic Speech Recognition and Understanding 
(ASRU 2007), Kyoto, Japan. 
Raux, Antoine; Langner, Brian; Black, Alan W.; 
Eskenazi, Maxine (2005). Let's Go Public! Taking a 
spoken dialog system to the real world.Interspeech 
2005 (Eurospeech), Lisbon, Portugal. 
Rieser, Verena; Kruijff-Korbayov?, Ivana; Lemon, 
Oliver (2005). A corpus collection and annotation 
framework for learning multimodal clarification 
strategies. Sixth SIGdial Workshop on Discourse 
and Dialogue, pp. 97-106.  
Sacks, Harvey; Schegloff, Emanuel A.; Jefferson, Gail 
(1974). A simplest systematics for the organization 
of turn-taking for conversation. Language, 50(4), 
696-735. 
Skantze, Gabriel (2003). Exploring human error 
handling strategies: Implications for Spoken 
Dialogue Systems. Proceedings of ISCA Tutorial 
and Research Workshp on Error Handling in Spoken 
Dialogue Systems, pp. 71-76.  
Stoyanchev, Svetlana; Stent, Amanda (2009). 
Predicting concept types in user corrections in 
dialog. Proceedings of the EACL Workshop SRSL 
2009, the Second Workshop on Semantic 
Representation of Spoken Language, pp. 42-49.  
Turunen, Markku; Hakulinen, Jaakko; Kainulainen, 
Anssi (2006). Evaluation of a spoken dialogue 
system with usability tests and long-term pilot 
studies. Ninth International Conference on Spoken 
Language Processing (Interspeech 2006 - ICSLP). 
Walker, M A.; Litman, D, J.; Kamm, C. A.; Abella, A. 
(1998). Evaluating Spoken Dialogue Agents with 
PARADISE: Two Case Studies. Computer Speech 
and Language, 12, 317-348. 
Wang, Ye-Yi; Yu, Dong; Ju, Yun-Cheng; Acero, Alex 
(2008). An introduction to voice search. IEEE 
Signal Process. Magazine, 25(3). 
Ward, Wayne; Issar, Sunil (1994). Recent improvements 
in the CMU spoken language understanding 
system.ARPA Human Language Technology 
Workshop, Plainsboro, NJ. 
Williams, Jason D.; Young, Steve (2004). 
Characterising Task-oriented Dialog using a 
Simulated ASR Channel. Eight International 
Conference on Spoken Language Processing 
(ICSLP/Interspeech), pp. 185-188.  
Witten, Ian H.; Frank, Eibe (2005). Data Mining: 
Practical Machine Learning Tools and Techniques 
(2nd ed.). San Francisco: Morgan Kaufmann. 
Zollo, Teresa (1999). A study of human dialogue 
strategies in the presence of speech recognition 
errors. Proceedings of AAAI Fall Symposium on 
Psychological Models of Communication in 
Collaborative Systems, pp. 132-139.  
Zue, Victor; Seneff, Stephanie; Glass, James; Polifroni, 
Joseph; Pao, Christine; Hazen, Timothy J., et al 
(2000). A Telephone-based conversational interface 
for weather information. IEEE Transactions on 
Speech and Audio Processing, 8, 85-96. 
 
848
