Proceedings of NAACL HLT 2009: Short Papers, pages 49?52,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Modeling Dialogue Structure with  Adjacency Pair Analysis and Hidden Markov Models  Kristy Elizabeth Boyer*1 Robert Phillips1,2 Eun Young Ha1 Michael D.  Wallis1,2 Mladen A.  Vouk1 James C. Lester1  1Department of Computer Science North Carolina State University Raleigh, NC, USA  2Applied Research Associates Raleigh, NC, USA  *keboyer@ncsu.edu  Abstract 
Automatically detecting dialogue structure within corpora of human-human dialogue is the subject of increasing attention.  In the do-main of tutorial dialogue, automatic discovery of dialogue structure is of particular interest because these structures inherently represent tutorial strategies or modes, the study of which is key to the design of intelligent tutor-ing systems that communicate with learners through natural language.  We propose a methodology in which a corpus of human-human tutorial dialogue is first manually an-notated with dialogue acts.  Dependent adja-cency pairs of these acts are then identified through ?2 analysis, and hidden Markov mod-eling is applied to the observed sequences to induce a descriptive model of the dialogue structure.       
1 Introduction Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven ap-proaches (e.g., Bangalore et al, 2006).  Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (Forbes-Riley et al, 2007), but also contribute to our understanding of 
the cognitive and affective processes involved in learning through tutoring (VanLehn et al, 2007).        Although traditional top-down approaches (e.g., Cade et al, 2008) and some empirical work on analyzing the structure of tutorial dialogue (Forbes-Riley et al, 2007) have yielded significant results, the field is limited by the lack of an auto-matic, data-driven approach to identifying dialogue structure.  An empirical approach to identifying tutorial dialogue strategies, or modes, could ad-dress this limitation by providing a mechanism for describing in succinct probabilistic terms the tuto-rial strategies that actually occur in a corpus.      Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al, 2000), we pro-pose a system that uses HMMs to capture the structure of tutorial dialogue implicit within se-quences of already-tagged dialogue acts.  This ap-proach operates on the premise that at any given point in the tutorial dialogue, the collaborative in-teraction is in a dialogue mode that characterizes the nature of the exchanges between tutor and stu-dent.  In our model, a dialogue mode is defined by a probability distribution over the observed sym-bols (e.g., dialogue acts and adjacency pairs).      Our previous work has noted some limitations of first-order HMMs as applied to sequences of individual dialogue acts (Boyer et al, in press).  Chief among these is that HMMs allow arbitrarily frequent transitions between hidden states, which does not conform well to human intuition about how tutoring strategies are applied.  Training an HMM on a sequence of adjacency pairs rather than individual dialogue acts is one way to generate a 
49
more descriptive model without increasing model complexity more than is required to accommodate the expanded set of observation symbols.  To this end, we apply the approach of Midgley et al (2006) for empirically identifying significant adja-cency pairs within dialogue, and proceed by treat-ing adjacency pairs as atomic units for the purposes of training the HMM.   2 Corpus Analysis This analysis uses a corpus of human-human tuto-rial dialogue collected in the domain of introduc-tory computer science.  Forty-three learners interacted remotely with a tutor through a key-board-to-keyboard remote learning environment yielding 4,864 dialogue moves.    The tutoring corpus was manually tagged with dialogue acts designed to capture the salient char-acteristics of the tutoring process (Table 1).  Tag Act Example Q Question Where should I  declare i? EQ Evaluation Question How does that look? S Statement You need a  closing brace. G Grounding Ok.  EX Extra-Domain You may use  your book. PF Positive Feedback Yes, that?s right. LF Lukewarm Feedback Sort of. NF Negative Feedback No, that?s not right. Table 1. Dialogue Act Tags  The correspondence between utterances and dia-logue act tags is one-to-one.  Compound utterances (i.e., a single utterance comprising more than one dialogue act) were split by the primary annotator prior to the inter-rater reliability study.1      The importance of adjacency pairs is well-established in natural language dialogue (e.g., Schlegoff & Sacks, 1973), and adjacency pair analysis has illuminated important phenomena in tutoring as well (Forbes-Riley et al, 2007).  For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.  How-ever, as noted in (Midgley et al, 2006), in order to                                                            1 Details of the study procedure used to collect the corpus, as well as Kappa statistics for inter-rater reliability, are reported in (Boyer et al, 2008). 
establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher prob-ability of the second member occurring.  For this analysis we utilize a ?2 test for independence of the categorical variables acti and acti+1 for all two-way combinations of dialogue act tags.  Only pairs in which speaker(acti)?speaker(acti+1) were consid-ered.  Other dialogue acts were treated as atomic elements in subsequent analysis, as discussed in Section 3.  Table 2 displays a list of the dependent pairs sorted by descending (unadjusted) statistical significance; the subscript indicates tutor (t) or stu-dent (s).  acti acti+1 P(acti+1|       acti) P(acti+1|    ?acti) ?2 val p-val EQs PFt 0.48 0.07 654 <0.0001 Gs Gt 0.27 0.03 380 <0.0001 EXs EXt 0.34 0.03 378 <0.0001 EQt PFs 0.18 0.01 322 <0.0001 EQt Ss 0.24 0.03 289 <0.0001 EQs LFt 0.13 0.01 265 <0.0001 Qt Ss 0.65 0.04 235 <0.0001 EQt LFs 0.07 0.00 219 <0.0001 Qs St 0.82 0.38 210 <0.0001 EQs NFt 0.08 0.01 207 <0.0001 EXt EXs 0.19 0.02 177 <0.0001 NFs Gt 0.29 0.03 172 <0.0001 EQt NFs 0.11 0.01 133 <0.0001 Ss Gt 0.16 0.03 95 <0.0001 Ss PFt 0.30 0.10 90 <0.0001 St Gs 0.07 0.04 36 <0.0001 PFs Gt 0.14 0.04 34 <0.0001 LFs Gt 0.22 0.04 30 <0.0001 St EQs 0.11 0.07 29 <0.0001 Gt EXs 0.07 0.03 14 0.002 St Qs 0.07 0.05 14 0.0002 Gt Gs 0.10 0.05 9 0.0027 EQt EQs 0.13 0.08 8 0.0042 Table 2. Dependent Adjacency Pairs 3 HMM on Adjacency Pair Sequences The keyboard-to-keyboard tutorial interaction re-sulted in a sequence of utterances that were anno-tated with dialogue acts.  We have hypothesized that a higher-level dialogue structure, namely the tutorial dialogue mode, overlays the observed dia-logue acts.  To build an HMM model of this struc-
50
ture we treat dialogue mode as a hidden variable and train a hidden Markov model to induce the dialogue modes and their associated dialogue act emission probability distributions.    An adjacency pair joining algorithm (Figure 1) was applied to each sequence of dialogue acts.  This algorithm joins pairs of dialogue acts into atomic units according to a priority determined by the strength of the adjacency pair dependency.  Sort adjacency pair list L by descending statistical significance For each adjacency pair (act1, act2) in L         For each dialogue act sequence (a1, a2, ?, an)          in the corpus                 Replace all pairs (ai=act1, ai+1=act2) with a                 new single act (act1act2) Figure 1.  Adjacency Pair Joining Algorithm     Figure 2 illustrates the application of the adja-cency pair joining algorithm on a sequence of dia-logue acts.  Any dialogue acts that were not grouped into adjacency pairs at the completion of the algorithm are treated as atomic units in the HMMianalysis.   Original Dialogue Act Sequence: Qs - St - LFt - St - St - Gs - EQs - LFt - St - St - Qs - St After Adjacency Pair Joining Algorithm: QsSt - LFt - St - StGs - EQsLFt - St - St - QsSt Figure 2.  DA Sequence Before/After Joining     The final set of observed symbols consists of 39 tags: 23 adjacency pairs (Table 2) plus all individ-ual dialogue acts augmented with a tag for the speaker (Table 1).      It was desirable to learn n, the best number of hidden states, during modeling rather than specify-ing this value a priori.  To this end, we trained and ten-fold cross-validated seven models (each featur-ing randomly-initialized parameters) for each number of hidden states n from 2 to 15, inclusive.2  The average log-likelihood was computed across all seven models for each n, and this average log-                                                           2 n=15 was chosen as an initial maximum number of states because it comfortably exceeded our hypothesized range of 3 to 7 (informed by the tutoring literature).  The Akaike Infor-mation Criterion measure steadily worsened above n = 5, con-firming no need to train models with n > 15. 
likelihood ln was used to compute the Akaike In-formation Criterion, a maximum-penalized likeli-hood estimator that penalizes more complex models (Scott, 2002).  The best fit was obtained with n=4 (Figure 3).  The transition probability distribution among hidden states is depicted in Figure 4, with the size of the nodes indicating rela-tive frequency of each hidden state; specifically, State 0 accounts for 63% of the corpus, States 1 and 3 account for approximately 15% each, and State 2 accounts for 7%.  
  Figure 3.  Dialogue Act Emission Probability  Distribution by Dialogue Mode3 4 Discussion and Future Work This exploratory application of hidden Markov models involves training an HMM on a mixed in-put sequence consisting of both individual dialogue acts and adjacency pairs.  The best-fit HMM con-sists of four hidden states whose emission symbol probability distributions lend themselves to inter-pretation as tutorial dialogue modes.  For example, State 0 consists primarily of tutor statements and positive feedback, two of the most common dia-logue  acts  in our corpus.  The transition probabili- 
51
 Figure 4.  Transition Probability Distribution4  ties also reveal that State 0 is highly stable; a self-transition is most likely with probability 0.835.  State 3 is an interactive state featuring student re-flection in the form of questions, statements, and requests for feedback.  The transition probabilities show that nearly 60% of the time the dialogue transitions from State 3 to State 0; this may indi-cate that after establishing what the student does or does not know in State 3, the tutoring switches to a less collaborative ?teaching? mode represented by State 0.        Future evaluation of the HMM presented here will include comparison with other types of graphical models.  Another important step is to correlate the dialogue profile of each tutoring ses-sion, as revealed by the HMM, to learning and af-fective outcomes of the tutoring session.  This type of inquiry can lead directly to design recommenda-tions for tutorial dialogue systems that aim to maximize particular learner outcomes.  In addition, leveraging knowledge of the task state as well as surface-level utterance content below the dialogue act level are promising directions for refining the descriptive and predictive power of these models.      Acknowledgements  This research was supported by the National Science Foundation under Grants REC-0632450, IIS-0812291, CNS-0540523, and GRFP.  Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the 
authors and do not necessarily reflect the views of the National Science Foundation.  References Boyer, K.E., Phillips, R., Wallis, M., Vouk, M., & Lester, J. (2008).  Balancing cognitive and moti-vational scaffolding in tutorial dialogue.  Pro-ceedings of the 9th International Conference on Intelligent Tutoring Systems, Montreal, Canada, 239-249. Boyer, K.E., Ha, E.Y., Wallis, M., Phillips, R., Vouk, M. & Lester, J. (in press).  Discovering tutorial dialogue strategies with hidden Markov models.  To appear in Proceedings of the 14th International Conference on Artificial Intelligence in Educa-tion, Brighton, U.K. Bangalore, S., DiFabbrizio, G., Stent, A. (2006).  Learning the structure of task-driven human-human dialogs.  Proceedings of ACL, Sydney, Australia, 201-208. Cade, W., Copeland, J., Person, N., & D'Mello, S. (2008). Dialog modes in expert tutoring. Proceed-ings of the 9th International Conference on Intel-ligent Tutoring Systems, Montreal, Canada, 470-479.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context de-pendencies for adaptive system development. Proceedings of NAACL HLT, Companion Volume, 41-44.  Midgley, T. D., Harrison, S., & MacNish, C. (2007). Empirical verification of adjacency pairs using dialogue segmentation. Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, 104-108.  Schlegoff, E.A., Sacks, H. (1973).  Opening up clos-ings.  Semiotica, 8(4), 289-327. Scott, S. L. (2002). Bayesian methods for hidden Markov models: Recursive computing in the 21st century. Journal of the American Statistical Asso-ciation, 97(457), 337-351. Stolcke, A., Coccaro, N., Bates, R., Taylor, P., Van Ess-Dykema, C., Ries, K., Shirberg, E., Jurafsky, D., Martin, R., Meteer, M. (2000).  Dialog act modeling for automatic tagging and recognition of conversational speech.  Computational Linguistics 26(3), 339-373. VanLehn, K., Graesser, A., Jackson, G.T., Jordan, P., Olney, A., Rose, C.P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  
52
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 53?61,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learner Characteristics and Feedback in Tutorial Dialogue 
 
 
Kristy Elizabeth 
Boyera 
Robert  
Phillipsab 
Michael D. 
Wallisab 
Mladen A. 
Vouka 
James C. 
Lestera 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates, Inc. 
Raleigh, North Carolina, USA 
{keboyer, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
 
 
Abstract 
Tutorial dialogue has been the subject of in-
creasing attention in recent years, and it has 
become evident that empirical studies of hu-
man-human tutorial dialogue can contribute 
important insights to the design of computa-
tional models of dialogue.  This paper reports 
on a corpus study of human-human tutorial 
dialogue transpiring in the course of problem-
solving in a learning environment for intro-
ductory computer science.  Analyses suggest 
that the choice of corrective tutorial strategy 
makes a significant difference in the outcomes 
of both student learning gains and self-
efficacy gains.  The findings reveal that tuto-
rial strategies intended to maximize student 
motivational outcomes (e.g., self-efficacy 
gain) may not be the same strategies that 
maximize cognitive outcomes (i.e., learning 
gain).  In light of recent findings that learner 
characteristics influence the structure of tuto-
rial dialogue, we explore the importance of 
understanding the interaction between learner 
characteristics and tutorial dialogue strategy 
choice when designing tutorial dialogue sys-
tems.  
1 Introduction 
Providing intelligent tutoring systems (ITSs) with 
the ability to engage learners in rich natural lan-
guage dialogue has been a goal of the ITS commu-
nity since the inception of the field.  Tutorial 
dialogue has been studied in the context of a num-
ber of systems devised to support a broad range of 
conversational phenomena.  Systems such as 
CIRCSIM (Evens and Michael 2006), BEETLE (Zinn 
et al 2002), the Geometry Explanation Tutor 
(Aleven et al 2003), Why2/Atlas (VanLehn et al 
2002), ITSpoke (Litman et al 2006), SCOT (Pon-
Barry et al 2006), ProPL (Lane and VanLehn 
2005) and AutoTutor (Graesser et al 2003) support 
research that has begun to the see the emergence of 
a core set of foundational requirements for mixed-
initiative natural language interaction that occurs in 
the kind of tutorial dialogue investigated here.  
Moreover, recent years have witnessed the appear-
ance of corpus studies empirically investigating 
speech acts in tutorial dialogue (Marineau et al 
2000), dialogues? correlation with learning 
(Forbes-Riley et al 2005, Core et al 2003, Ros? et 
al. 2003, Katz et al 2003), student uncertainty in 
dialogue (Liscombe et al 2005, Forbes-Riley and 
Litman 2005), and comparing text-based and spo-
ken dialogue (Litman et al 2006). 
     Recent years have also seen the emergence of a 
broader view of learning as a complex process in-
volving both cognitive and affective states.  To 
empirically explore these issues, a number of ITSs 
such as AutoTutor (Jackson et al 2007), Betty?s 
Brain (Tan and Biswas 2006), ITSpoke (Forbes-
Riley et al 2005), M-Ecolab (Rebolledo-Mendez 
et al 2006), and MORE (del Soldato and Boulay 
1995) are being used as platforms to investigate the 
impact of tutorial interactions on affective and mo-
tivational outcomes (e.g., self-efficacy) along with 
purely cognitive measures (i.e., learning gains).  A 
central problem in this line of investigation is iden-
53
tifying tutorial strategies (e.g., Graesser et al 
1995) that can appropriately balance the tradeoffs 
between cognitive and affective student outcomes 
(Lepper et al 1993).  While a rich set of cognitive 
and affective tutorial strategies is emerging (e.g., 
Porayska-Pomsta et al 2004), the precise nature of 
the interdependence between these types of strate-
gies is not well understood.  In addition, it may be 
the case that different populations of learners en-
gage in qualitatively different forms of dialogue.  
Students with particular characteristics may have 
specific dialogue profiles, and knowledge of such 
profiles could inform the design of tutorial systems 
whose strategies leverage the characteristics of the 
target population.  The extent to which different 
tutorial strategies, and specific instances of them in 
certain contexts, may be used to enhance tutorial 
effectiveness is an important question to designers 
of ITSs.    
     Given that human-human tutorial dialogue of-
fers a promising model for effective communica-
tion (Chi et al 2001), our methodology is to study 
naturally occurring tutorial dialogues in a task-
oriented learning environment to investigate the 
relationship between the structure of tutorial dia-
logue, the characteristics of learners, and the im-
pact of cognitive and motivational corrective 
tutorial strategies on learning and self-efficacy 
(Boyer et al in press).  A text-based dialogue inter-
face was incorporated into a learning environment 
for introductory computer science.  In the envi-
ronment, students undertook a programming task 
and conversed with human tutors while designing, 
implementing, and testing Java programs.    
     The results of the study suggest that the choice 
of corrective tutorial strategy has a significant im-
pact on the learning gains and self-efficacy of stu-
dents.  These findings reinforce those of other 
studies (e.g., Lepper et al 1993, Person et al 1995, 
Keller et al 1983) that indicate that some cognitive 
and motivational goals may be at odds with one 
other because a tutorial strategy designed to maxi-
mize one set of goals (e.g., cognitive goals) can 
negatively impact the other.  We contextualize our 
findings in light of recent results that learner char-
acteristics such as self-efficacy influence the struc-
ture of task-oriented tutorial dialogue (Boyer et al 
2007), and may therefore produce important inter-
action effects when considered alongside tutorial 
strategy.    
     This paper is organized as follows.  Section 2 
describes the corpus study, including experimental 
design and tagging of dialogue and student prob-
lem-solving actions.  Section 3 presents analysis 
and results.  Discussion and design implications 
are considered in Section 4, and concluding re-
marks follow in Section 5.  
 
2 Corpus Study 
The corpus was gathered by logging text-based 
dialogues between tutors and novice computer sci-
ence students.  The learning task was to complete a 
Java programming problem that required students 
to apply fundamental concepts such as iteration, 
modularization, and sequential-access data struc-
tures.  This study was conducted to compare the 
impact of certain corrective cognitive and motiva-
tional tutorial strategies on student learning and 
self-efficacy in human-human tutoring.  Specifi-
cally, the study considered the motivational strate-
gies of praise and reassurance (Lepper et al 1993) 
and the category of informational tutorial utter-
ances termed cognitive feedback (Porayska-Pomsta 
et al 2004, Tan and Biswas 2006) that followed 
questionable student problem-solving action.  Fol-
lowing the approach of Forbes-Riley (2005) and 
others (Marineau et al 2000), utterances from a 
corpus of human-human tutorial dialogues were 
annotated with dialogue acts.  Then, adopting the 
approach proposed by Ohlsson et al (2007), statis-
tical modeling techniques were employed to quan-
tify the relative impact of these different tutorial 
strategies on the outcomes of interest (in this case, 
learning and self-efficacy gains).     
 
2.1 Experimental Design 
Subjects were students enrolled in an introductory 
computer science course and were primarily 
freshman or sophomore engineering majors in dis-
ciplines such as mechanical, electrical, and com-
puter engineering. 
     The corpus was gathered from tutor-student 
interactions between 43 students and 14 tutors dur-
ing a two-week study.  Tutors and students were 
completely blind to each other?s characteristics as 
they worked together remotely from separate labs.  
Tutors observed student problem-solving actions 
54
(e.g., programming, scrolling, executing programs) 
in real time.  Tutors had varying levels of tutoring 
experience, and were not instructed about specific 
tutorial strategies. 
     Subjects first completed a pre-survey including 
items about self-efficacy, attitude toward computer 
science, and attitude toward collaboration.  Sub-
jects then completed a ten item pre-test over spe-
cific topic content.  The tutorial session was 
controlled at 55 minutes for all subjects, after 
which subjects completed a post-survey and post-
test containing variants of the items on the pre- 
versions.   
 
2.2 Problem-Solving Tagging 
The raw corpus contains 4,864 dialogue moves:  
1,528 student utterances and 3,336 tutor utterances.  
As a chronology of tutorial dialogue interleaved 
with student problem-solving (programming) ac-
tions that took place during the tutoring sessions, 
the corpus contains 29,996 programming key-
strokes and 1,277 periods of scrolling ? all per-
formed by students.  Other problem-solving 
actions, such as opening and closing files or run-
ning the program, were sparse and were therefore 
eliminated from the analyses.  Of the 3,336 tutor 
utterances, 1,243 occur directly after ?question-
able? student problem-solving action.  (The notion 
of ?questionable? is defined below.)  This subset of 
tutorial utterances serves as the basis for the tuto-
rial strategy comparison. 
     Student problem-solving actions were logged 
throughout tutoring sessions.  Two actions were 
under consideration for the analysis:  typing in the 
programming interface and scrolling in the pro-
gram editor window.  To interpret the raw logged 
student problem-solving actions, these events were 
automatically tagged using a heuristic measure for 
correctness: if a problem-solving action was a pro-
gramming keystroke (character) that survived until 
the end of the session, this event was tagged prom-
ising, to indicate it was probably correct.  If a prob-
lem-solving act was a programming keystroke 
(character) that did not survive until the end of the 
session, the problem-solving act was tagged ques-
tionable.  Both these heuristics are based on the 
observation that in this tutoring context, students 
solved the problem in a linear fashion and tutors 
did not allow students to proceed past a step that 
had incorrect code in place.  Finally, periods of 
consecutive scrolling were also marked question-
able because in a problem whose entire solution 
fits on one printed page, scrolling was almost uni-
formly undertaken by a student who was confused 
and looking for answers in irrelevant skeleton code 
provided to support the programming task.   
 
2.3 Dialogue Act Tagging 
Because utterances communicate through two 
channels, a cognitive channel and a motiva-
tional/affective channel, each utterance was 
annotated with both a required cognitive dialogue 
tag (Table 1) and an optional motiva-
tional/affective dialogue tag (Table 2).  While no 
single standardized dialogue act tag set has been 
identified for tutorial dialogue, the tags applied 
here were drawn from several schemes in the tuto-
rial dialogue and broader dialogue literature.  A 
coding scheme for tutorial dialogue in the domain 
of qualitative physics influenced the creation of the 
tag set (Forbes-Riley et al 2005), as did the four-
category scheme (Marineau et al 2000).  A more 
expansive general dialogue act tag set alo contrib-
uted commonly occurring acts (Stolcke et al 
2000).  The motivational tags were drawn from 
work by Lepper (1993) on motivational strategies 
of human tutors.   
     Table 1 displays the cognitive subset of this 
dialogue act tag set, while Table 2 displays the mo-
tivational/affective tags.  It should be noted that a 
cognitive tag was required for each utterance, 
while a motivational/affective tag was applied only 
to the subset of utterances that communicated in 
that channel.  If an utterance constituted a strictly 
motivational/affective act, its cognitive channel 
was tagged with EX (EXtra-domain) indicating 
there was no relevant cognitive content.  On the 
other hand, some utterances had both a cognitive 
component and a motivational/affective compo-
nent.  For example, a tutorial utterance of, ?That 
looks great!? would have been tagged as positive 
feedback (PF) in the cognitive channel, and as 
praise (P) in the motivational/affective channel.  In 
contrast, the tutorial move ?That?s right,? would be 
tagged as positive feedback (PF) in the cognitive 
channel and would not be annotated with a motiva-
tional/affective tag.  Table 3 shows an excerpt 
from the corpus with dialogue act tags applied. 
55
     The entire corpus was tagged by a single human 
annotator, with a second tagger marking 1,418 of 
the original 4,864 utterances.  The resulting kappa 
statistics were 0.76 in the cognitive channel and 
0.64 in the motivation channel.   
3 Analysis and Results 
Overall, these tutoring sessions were effective: 
they yielded learning gains (difference between 
posttest and pretest) with mean 5.9% and median 
7.9%, which were statistically significant 
(p=0.038), and they produced self-efficacy gains
Table 1:  Cognitive Channel Dialogue Acts 
56
(difference between pre-survey and post-survey 
scores) with mean 12.1% and median 12.5%, 
which were also statistically significant 
(p<0.0001).  Analyses revealed that statistically 
significant relationships hold between tutorial 
strategy and learning, as well as between tutorial 
strategy and self-efficacy gains.   
 
3.1 Analysis 
First, the values of learning gain and self-efficacy 
gain were grouped into binary categories (?Low?, 
?High?) based on the median value.  We then ap-
plied multiple logistic regression with the gain 
category as the predicted value.  Tutorial strategy, 
incoming self-efficacy rating, and pre-test score 
were predictors in the model.  The binarization 
approach followed by multiple logistic regression 
was chosen over multiple linear regression on a 
continuous response variable because the learning 
instruments (10 items each) and self-efficacy ques-
tionnaires (5 items each) yielded few distinct val-
ues of learning gain, meaning the response variable 
(learning gain and self-efficacy gain, respectively) 
would not have been truly continuous in nature.  
Logistic regression is used for binary response 
variables; it computes the odds of a particular out-
come over another (e.g., ?Having high learning 
gain versus low learning gain?) given one value of 
the predictor variable over another (e.g., ?The cor-
rective tutorial strategy chosen was positive cogni-
tive feedback instead of praise?). 
 
Table 2:  Motivational/Affective Channel Dialogue Acts 
57
3.2 Results 
After accounting for the effects of pre-test score 
and incoming self-efficacy rating (both of which 
were significant in the model with p<0.001), ob-
servations containing tutorial encouragement were 
56% less likely to result in high learning gain than 
observations without explicit tutorial encourage-
ment (p=0.001).  On the other hand, an analogous 
model of self-efficacy gain revealed that tutorial 
encouragement was 57% more likely to result in 
high self-efficacy gain compared to tutorial re-
sponses that had no explicit praise or reassurance 
(p=0.054).  These models suggested that the pres-
ence of tutorial encouragement in response to 
questionable student problem-solving action may 
enhance self-efficacy gain but detract from learn-
ing gain. 
    Another significant finding was that observa-
tions in which the tutor used cognitive feedback 
plus praise were associated with 40% lower likeli-
hood of high learning gain than observations in 
which the tutor used purely cognitive feedback.  
No impact was observed on self-efficacy gain.  
These results suggest that in response to question-
able student problem-solving action, to achieve 
learning gains, purely cognitive feedback is pre-
ferred over cognitive feedback plus praise, while 
self-efficacy gain does not appear to be impacted 
either way. 
     Among students with low incoming self-
efficacy, observations in which the tutor employed 
a standalone motivational act were 300% as likely 
to be in the high self-efficacy gain group as obser-
vations in which the tutor employed a purely cog-
nitive statement or a cognitive statement combined 
with encouragement (p=0.039).  In contrast, among 
students with high initial self-efficacy, a purely 
motivational tactic resulted in 90% lower odds of 
being in the high self-efficacy gain group.  These 
results suggest that standalone praise or reassur-
ance may be useful for increasing self-efficacy 
gain among low initial self-efficacy students, but 
may decrease self-efficacy gain in high initial self-
efficacy students.   
     Considering strictly cognitive feedback, posi-
tive feedback resulted in 190% increased odds of 
high student self-efficacy gain compared to the 
other cognitive strategies (p=0.0057).  Positive 
cognitive feedback did not differ significantly from 
other types of cognitive strategies in a Chi-square 
comparison with respect to learning gains 
(p=0.390).  The models thus suggest when dealing 
with questionable student problem-solving action, 
positive cognitive feedback is preferable to other 
types of cognitive feedback for eliciting self-
efficacy gains, but this type of feedback is not 
Table 3:  Dialogue Excerpts 
58
found to be better or worse than other cognitive 
feedback for effecting learning gains. 
 
4 Discussion 
The study found that the presence of direct tutorial 
praise or encouragement in response to question-
able student problem-solving action increased the 
odds that the student reported high self-efficacy 
gain while lowering the odds of high learning gain.  
The study also found that, with regard to learning 
gains, purely cognitive feedback was preferable to 
cognitive feedback with an explicitly motivational 
component.  These empirical findings are consis-
tent with theories of Lepper et al (1993) who 
found that some cognitive and affective goals in 
tutoring are ?at odds.?  As would be predicted, the 
results also echo recent quantitative results from 
other tutoring domains such as qualitative physics 
(Jackson et al 2007) and river ecosystems (Tan 
and Biswas 2006) that, in general, overt motiva-
tional feedback contributes to motivation but cog-
nitive feedback matters more for learning.   
      Of the corrective tutorial strategies that were 
exhibited in the corpus, positive cognitive feed-
back emerged as an attractive approach for re-
sponding to plausibly incorrect student problem-
solving actions.  Responding positively (e.g., 
?Right?) to questionable student actions is an ex-
ample of indirect correction, which is recognized 
as a polite strategy (e.g., Porayska-Pomsta et al 
2004).  A qualitative investigation of this phe-
nomenon revealed that in the corpus, tutors gener-
ally followed positive feedback in this context with 
more substantive cognitive feedback to address the 
nature of the student?s error.  As such, the positive 
feedback approach seems to have an implicit, yet 
perceptible, motivational component while retain-
ing its usefulness as cognitive feedback. 
    This study found that explicit motivational acts, 
when applied as corrective tutorial approaches, had 
different impacts on different student subgroups.  
Students with low initial self-efficacy appeared to 
benefit more from praise and reassurance than stu-
dents with high initial self-efficacy.  In a prior cor-
pus study to investigate the impact of learner 
characteristics on tutorial dialogue (Boyer et al 
2007), we also found that learners from different 
populations exhibited significantly different dia-
logue profiles.  For instance, high self-efficacy 
students made more declarative statements, or as-
sertions, than low self-efficacy students.  In addi-
tion, tutors paired with high self-efficacy students 
gave more conversational acknowledgments than 
tutors paired with low self-efficacy students, de-
spite the fact that tutors were not made aware of 
any learner characteristics before the tutoring ses-
sion.  Additional dialogue profile differences 
emerged between high and low-performing stu-
dents, as well as between males and females.  To-
gether these two studies suggest that learner 
characteristics influence the structure of tutorial 
dialogue, and that the choice of tutorial strategy 
may impact student subgroups in different ways.             
 
5 Conclusion 
The work reported here represents a first step to-
ward understanding the effects of learner charac-
teristics on task-oriented tutorial dialogue and the 
use of feedback.  Results suggest that positive cog-
nitive feedback may prove to be an appropriate 
strategy for responding to questionable student 
problem-solving actions in task-oriented tutorial 
situations because of its potential for addressing 
the sometimes competing cognitive and affective 
needs of students.  For low self-efficacy students, it 
was found that direct standalone encouragement 
can be used to bolster self-efficacy, but care must 
be used in correctly diagnosing student self-
efficacy because the same standalone encourage-
ment does not appear helpful for high self-efficacy 
students.  These preliminary findings highlight the 
importance of understanding the interaction be-
tween learner characteristics and tutorial strategy 
as it relates to the design of tutorial dialogue sys-
tems. 
     Several directions for future work appear prom-
ising.  First, it will be important to explore the in-
fluence of learner characteristics on tutorial 
dialogue in the presence of surface level informa-
tion about students? utterances.  This line of inves-
tigation is of particular interest given recent results 
indicating that lexical cohesion in tutorial dialogue 
with low-performing students is found to be highly 
correlated with learning (Ward and Litman 2006).   
Second, while the work reported here has consid-
ered a limited set of motivational dialogue acts, 
namely praise and reassurance, future work should 
target an expanded set of affective dialogue acts to 
59
facilitate continued exploration of motivational and 
affective phenomena in this context.  Finally, the 
current results reflect human-human tutoring 
strategies that proved to be effective; however, it 
remains to be seen whether these same strategies 
can be successfully employed in tutorial dialogue 
systems.  Continuing to identify and empirically 
compare the effectiveness of alternative tutorial 
strategies will build a solid foundation for choos-
ing and implementing strategies that consider 
learner characteristics and successfully balance the 
cognitive and affective concerns surrounding the 
complex processes of teaching and learning 
through tutoring. 
Acknowledgments 
The authors wish to thank Scott McQuiggan and 
the members of the Intellimedia Center for Intelli-
gent Systems for their ongoing intellectual contri-
butions, and the Realsearch Group at NC State 
University for extensive project development sup-
port.  This work was supported in part by the Na-
tional Science Foundation through Grant REC-
0632450, an NSF Graduate Research Fellowship, 
and the STARS Alliance Grant CNS-0540523.  
Any opinions, findings, conclusions or recommen-
dations expressed in this material are those of the 
author(s) and do not necessarily reflect the views 
of the National Science Foundation.  Support was 
also provided by North Carolina State University 
through the Department of Computer Science and 
the Office of the Dean of the College of Engineer-
ing.   
 
References  
Kristy Elizabeth Boyer, Robert Phillips, Michael Wallis, 
Mladen Vouk, and James Lester.  In press.  Balanc-
ing cognitive and motivational scaffolding in tutorial 
dialogue.  To appear in Proceedings of the 9th Inter-
national Conference on Intelligent Tutoring Systems. 
Kristy Elizabeth Boyer, Mladen Vouk, and James Les-
ter.  2007.  The influence of learner characteristics on 
task-oriented tutorial dialogue.  Proceedings of 
AIED, pp. 127-134.  IOS Press. 
Vincent Aleven, Kenneth R. Koedinger, and Octav 
Popescu.  2003.  A tutorial dialog system to support 
self-explanation: Evaluation and open questions.  
Proceedings of the 11th International Conference on 
Artificial Intelligence in Education, pp. 39-46.  Am-
sterdam.  IOS Press. 
Vincent Aleven, Bruce McLaren, Ido Roll, and Kenneth 
Koedinger.  2004.  Toward tutoring help seeking: 
Applying cognitive modeling to meta-cognitive 
skills.  J. C. Lester, R. M. Vicari, and F. Paragua?u 
(Eds.), Proceedings of the 7th International Confer-
ence on Intelligent Tutoring Systems, pp. 227-239.  
Berlin: Springer Verlag. 
Albert Bandura.  2006.  Guide for constructing self-
efficacy scales.  T. Urdan and F. Pajares (Eds.): Self-
Efficacy Beliefs of Adolescents, pp. 307-337.  Infor-
mation Age Publishing, Greenwich, Connecticut. 
Michelene T. H. Chi, Nicholas De Leeuw, Mei-Hung 
Chiu, and Christian LaVancher.  1994.  Eliciting self-
explanations improves understanding.  Cognitive Sci-
ence, 18:439-477. 
Michelene T. H. Chi, Stephanie A. Siler, Heisawn 
Jeong, Takashi Yamauchi, and Robert G. Hausmann.  
2001.  Learning from human tutoring.  Cognitive Sci-
ence, 25(4):471-533. 
Mark G. Core, Johanna D. Moore, and Claus Zinn.  
2003.  The role of initiative in tutorial dialogue.  Pro-
ceedings of the Tenth Conference on European 
Chapter of the Association for Computational Lin-
guistics, pp. 67-74. 
Teresa del Soldato and Benedict du Boulay.  1995.  Im-
plementation of motivational tactics in tutoring sys-
tems.  Journal of Artificial Intelligence in Education, 
6(4):337-378.  Association for the Advancement of 
Computing in Education, USA. 
Martha Evens and Joel Michael.  2006.  One-on-One 
Tutoring by Humans and Computers.  Mahwah, New 
Jersey: Lawrence Erlbaum Associates. 
Kate Forbes-Riley and Diane Litman.  2005.  Using 
bigrams to identify relationships between student cer-
tainness states and tutor responses in a spoken dia-
logue corpus.  Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue.  Lisbon, Por-
tugal. 
Kate Forbes-Riley, Diane Litman, Alison Huettner, and 
Arthur Ward.  2005.  Dialogue-learning correlations 
in spoken dialogue tutoring.  Looi, C-k., Mccalla, G., 
Bredeweg, B., Breuker, J. (Eds.): Proceedings of 
AIED, pp. 225-232.  IOS Press. 
Arthur C. Graesser, George T. Jackson, Eric Mathews, 
Heather H. Mitchell, Andrew Olney, Mathew Ven-
tura, Patrick Chipman, Donald R. Franceschetti, 
Xiangen Hu, Max M. Louwerse, Natalie K. Person, 
and the Tutoring Research Group.  2003.  
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog.  Proceed-
ings of the Twenty-Fifth Annual Conference of the 
Cognitive Science Society, pp. 474-479. 
Arthur C. Graesser, Natalie K. Person, and Joseph P. 
Magliano.  1995.  Collaborative dialogue patterns in 
naturalistic one-to-One tutoring.  Applied Cognitive 
Psychology, 9(6):495-522.  John Wiley & Sons, Ltd. 
60
G. Tanner Jackson and Art Graesser.  2007.  Content 
matters: An investigation of feedback categories 
within an ITS.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:127-134.  
IOS Press. 
Sandra Katz, David Allbritton, and John Connelly.  
2003.  Going beyond the problem given: How human 
tutors use post-solution discussions to support trans-
fer.  International Journal of Artificial Intelligence in 
Education, 13:79-116. 
John M. Keller.  1983.  Motivational design of instruc-
tion.  Reigeluth, C.M. (Ed.): Instructional-Design 
Theories and Models: An Overview of Their Current 
Status, pp. 383-429.  Lawrence Erlbaum Associates, 
Inc., Hillsdale, NJ. 
H. Chad Lane and Kurt VanLehn.  2005.  Teaching the 
tacit knowledge of programming to novices with 
natural language tutoring.  Computer Science Educa-
tion, 15:183-201. 
Mark R. Lepper, Maria Woolverton, Donna L. Mumme, 
and Jean-Luc Gurtner.  1993.  Motivational tech-
niques of expert human tutors: Lessons for the design 
of computer-based tutors.  Lajoie, S.P., Derry, S. J. 
(Eds.): Computers as Cognitive Tools, pp. 75-105. 
Lawrence Erlbaum Associates, Inc., Hillsdale NJ. 
Jackson Liscombe, Julia Hirschberg, and Jennifer J. 
Venditti.  2005.  Detecting certainness in spoken tu-
torial dialogues.  Proceedings of Interspeech, 2005. 
Diane J. Litman, Carolyn P. Ros?, Kate Forbes-Riley, 
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man.  2006.  Spoken versus typed human and com-
puter dialogue tutoring.  International Journal of 
Artificial Intelligence in Education, 16:145-170. 
Johanna Marineau, Peter Wiemer-Hastings, Derek 
Harter, Brent Olde, Patrick Chipman, Ashish Kar-
navat, Victoria Pomeroy, Sonya Rajan, Art Graesser, 
and the Tutoring Research Group.  2000.  Classifica-
tion of speech acts in tutorial dialog.  Proceedings of 
the Workshop on Modeling Human Teaching Tactics 
and Strategies of ITS 2000, pp. 65-71.  Montreal, 
Canada. 
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, 
Davide Fossati, Xin Lu, and Trina C. Kershaw.  
2007.  Beyond the code-and-count analysis of tutor-
ing dialogues.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:349-356.  
IOS Press. 
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and 
Arthur C. Graesser.  1995.  Pragmatics and peda-
gogy: Conversational rules and politeness strategies 
may inhibit effective tutoring.  Cognition and In-
struction, 13(2):161-188.  Lawrence Erlbaum Asso-
ciates, Inc., Hillsdale, NJ. 
Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt, 
Brady Clark, and Stanley Peters.  2006.  Responding 
to student uncertainty in spoken tutorial dialogue sys-
tems.  International Journal of Artificial Intelligence 
in Education, 16:171-194. 
Ka?ka Porayska-Pomsta and Helen Pain.  2004.  Provid-
ing cognitive and affective scaffolding through teach-
ing strategies:  Applying linguistic politeness to the 
educational context.  J.C. Lester, Vicari, R. M., Para-
gua?u, F. (Eds.): Proceedings of ITS 2004, LNCS 
3220:77-86.  Springer-Verlag Berlin / Heidelberg. 
Genaro Rebolledo-Mendez, Benedict du Boulay, and 
Rosemary Luckin.  2006.  Motivating the learner: an 
empirical evaluation. Ikeda, M., Ashlay, K. D., Chan, 
T.-W. (Eds.): Proceedings of ITS 2006,  LNCS 
4053:545-554.  Springer Verlag Berlin / Heidelberg. 
Carolyn P. Ros?, Dumisizwe Bhembe, Stephanie Siler, 
Ramesh Srivastava, and Kurt VanLehn.  2003.  The 
role of why questions in effective human tutoring.  
Hoppe, U., Verdejo, F., Kay, J. (Eds.): Proceedings 
of AIED 2003, pp. 55-62.  IOS Press. 
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth 
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol Van Ess-Dykema, and 
Marie Meteer.  Dialogue act modeling for automatic 
tagging and recognition of conversational speech.  
2000.  Computational Linguistics, 26:339-373. 
Jason Tan and Gautam Biswas.  2006.  The role of feed-
back in preparation for future learning:  A case study 
in learning by teaching environments.  Ikeda, M., 
Ashley, K., Chan, T.-W. (Eds.): Proceedings of ITS 
2006, LNCS 4053:370-381. Springer-Verlag Berlin / 
Heidelberg. 
Kurt VanLehn, Pamela W. Jordan, Carolyn P. Ros?, 
Dumisizwe Bhembe, Michael Bottner, Andy Gaydos, 
Maxim Makatchev, Umarani Pappuswamy, Michael 
Ringenberg, Antonio Roque, Stephanie Siler, and 
Ramesh Srivastava.  2002.  The architecture of 
Why2-Atlas: A coach for qualitative physics essay 
writing.  Proceedings of the 6th International Con-
ference on Intelligent Tutoring Systems, LNCS 
2363:158-167. 
Arthur Ward and Diane Litman.  2006.  Cohesion and 
learning in a tutorial spoken dialog system.  Proceed-
ings of the 19th International FLAIRS (Florida Artifi-
cial Intelligence Research Society) Conference.  
Melbourne Beach, FL. 
Claus Zinn, Johanna D. Moore, and Mark G. Core.  
2002.  A 3-tier planning architecture for managing 
tutorial dialogue.  Intelligent Tutoring Systems, Sixth 
International Conference.  LNCS 2363:574-584.  
Springer-Verlag, London, UK. 
 
 
61
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 19?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Inferring Tutorial Dialogue Structure with Hidden Markov Modeling 
 
 
Kristy 
Elizabeth  
  Boyera 
Eun Young  
 Haa 
     Robert  
Phillipsab 
 Michael     
     D.  
  Wallisab 
Mladen A.  
 Vouka 
James C.  
 Lestera 
 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates 
Raleigh, NC, USA 
 
{keboyer, eha, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
Abstract 
The field of intelligent tutoring systems has 
seen many successes in recent years.  A 
significant remaining challenge is the 
automatic creation of corpus-based tutorial 
dialogue management models.  This paper 
reports on early work toward this goal.  We 
identify tutorial dialogue modes in an 
unsupervised fashion using hidden Markov 
models (HMMs) trained on input 
sequences of manually-labeled dialogue 
acts and adjacency pairs.  The two best-fit 
HMMs are presented and compared with 
respect to the dialogue structure they 
suggest; we also discuss potential uses of 
the methodology for future work. 
1 Introduction 
 
The field of intelligent tutoring systems has made 
great strides toward bringing the benefits of one-
on-one tutoring to a wider population of learners.  
Some intelligent tutoring systems, called tutorial 
dialogue systems, support learners by engaging in 
rich natural language dialogue, e.g., (Graesser et 
al. 2003; Zinn, Moore & Core 2002; Evens & 
Michael 2006; Aleven, Koedinger & Popescu 
2003; Litman et al 2006; Arnott, Hastings & 
Allbritton 2008; VanLehn et al 2002).  However, 
creating these systems comes at a high cost: it 
entails handcrafting each pedagogical strategy the 
tutor might use and then realizing these strategies 
in a dialogue management framework that is also 
custom-engineered for the application.  It is hoped 
that the next generation of these systems can 
leverage corpora of tutorial dialogue in order to 
provide more robust dialogue management models 
that capture the discourse phenomena present in 
effective natural language tutoring.   
The structure of tutorial dialogue has 
traditionally been studied by manually examining 
corpora and focusing on cognitive and 
motivational aspects of tutorial strategies (e.g., 
Lepper et al 1993; Graesser, Person & Magliano 
1995).  While these approaches yielded 
foundational results for the field, such analyses 
suffer from two serious limitations:  manual 
approaches are not easily scalable to different or 
larger corpora, and the rigidity of handcrafted 
dialogue structure tagging schemes may not 
capture all the phenomena that occur in practice.   
In contrast, the stochastic nature of dialogue 
lends itself to description through probabilistic 
models.  In tutorial dialogue, some early work has 
adapted language processing techniques, namely n-
gram analyses, to examine human tutors? responses 
to student uncertainty (Forbes-Riley & Litman 
2005), as well as to find correlations between local 
tutoring strategies and student outcomes (Boyer et 
al. 2008).  However, this work is limited by its 
consideration of small dialogue windows. 
Looking at a broader window of turns is often 
accomplished by modeling the dialogue as a 
Markov decision process.  With this approach, 
19
techniques such as reinforcement learning can be 
used to compare potential policies in terms of 
effectiveness for student learning.  Determining 
relevant feature sets (Tetreault & Litman 2008) 
and conducting focussed experiments for localized 
strategy effectiveness (Chi et al 2008) are active 
areas of research in this line of investigation.  
These approches often fix the dialogue structures 
under consideration in order to compare the 
outcomes associated with those structures or the 
features that influence policy choice.    
    In contrast to treating dialogue structure as a 
fixed entity, one approach for modeling the 
progression of complete dialogues involves 
learning the higher-level structure in order to infer 
succinct probabilistic models of the interaction.  
For example, data-driven approaches for 
discovering dialogue structure have been applied to 
corpora of human-human task-oriented dialogue 
using general models of task structure (Bangalore, 
Di Fabbrizio & Stent 2006).  Encouraging results 
have emerged from using a general model of the 
task structure to inform automatic dialogue act 
tagging as well as subtask segmentation.  
    Our current work examines a modeling 
technique that does not require a priori knowledge 
of the task structure:  specifically, we propose to 
use hidden Markov models (HMMs) (Rabiner 
1989) to capture the structure of tutorial dialogue 
implicit within sequences of tagged dialogue acts.  
Such probablistic inference of discourse structure 
has been used in recent work with HMMs for topic 
identification (Barzilay & Lee 2004) and related 
graphical models for segmenting multi-party 
spoken discourse (Purver et al 2006).  
Analogously, our current work focuses on 
identifying dialogic structures that emerge during 
tutorial dialogue.  Our approach is based on the 
premise that at any given point in the tutorial 
dialogue, the collaborative interaction is ?in? a 
dialogue mode (Cade et al 2008) that characterizes 
the nature of the exchanges between tutor and 
student; these modes correspond to the hidden 
states in the HMM.  Results to date suggest that 
meaningful descriptive models of tutorial dialogue 
can be generated by this simple stochastic 
modeling technique.  This paper focuses on the 
comparison of two first-order HMMs:  one trained 
on sequences of dialogue acts, and the second 
trained on sequences of adjacency pairs.   
 
2 Corpus Analysis 
The HMMs were trained on a corpus of human-
human tutorial dialogue collected in the domain of 
introductory computer science.  Forty-three 
learners interacted remotely with one of fourteen 
tutors through a keyboard-to-keyboard remote 
learning environment yielding 4,864 dialogue 
moves. 
2.1 Dialogue Act Tagging 
The tutoring corpus was manually tagged with 
dialogue acts designed to capture the salient 
characteristics of the tutoring process (Table 1). 
 
Tag Act Example 
Q Question Where should I  
Declare i? 
EQ Evaluation Question How does that look? 
S Statement You need a  
closing brace. 
G Grounding Ok.  
EX Extra-Domain You may use  
your book. 
PF Positive Feedback Yes, that?s right. 
LF Lukewarm Feedback Sort of. 
NF Negative Feedback No, that?s not right. 
Table 1. Dialogue Act Tags 
 
    The correspondence between utterances and 
dialogue act tags is one-to-one; compound 
utterances were split by the primary annotator prior 
to the inter-rater reliability study.1  This dialogue 
act tagging effort produced sequences of dialogue 
acts that have been used in their un-altered forms 
to train one of the two HMMs presented here 
(Section 3).      
2.2 Adjacency Pair Identification 
In addition to the HMM trained on sequences of 
individual dialogue acts, another HMM was 
trained on sequences of dialogue act adjacency 
pairs.  The importance of adjacency pairs is well-
established in natural language dialogue (e.g., 
Schlegoff & Sacks 1973), and adjacency pair 
analysis has illuminated important phenomena in 
tutoring as well (Forbes-Riley et al 2007).  The 
                                                           
1 Details of the study procedure used to collect the corpus, as 
well as Kappa statistics for inter-rater reliability, are reported 
in (Boyer et al 2008). 
20
intuition behind adjacency pairs is that certain 
dialogue acts naturally occur together, and by 
grouping these acts we capture an exchange 
between two conversants in a single structure.  
This formulation is of interest for our purposes 
because when treating sequences of dialogue acts 
as a Markov process, with or without hidden states, 
the addition of adjacency pairs may offer a 
semantically richer observation alphabet.   
    To find adjacency pairs we utilize a ?2 test for 
independence of the categorical variables acti and 
acti+1 for all sequential pairs of dialogue acts that 
occur in the corpus.  Only pairs in which 
speaker(acti) ? speaker(acti+1) were considered.  
Table 2 displays a list of all dependent adjacency 
pairs sorted by descending (unadjusted) statistical 
significance; the subscript on each dialogue act tag 
indicates tutor (t) or student (s). 
    An adjacency pair joining algorithm was applied 
to join statistically significant pairs of dialogue 
acts (p<0.01) into atomic units according to a 
priority determined by the strength of the statistical 
significance.  Dialogue acts that were ?left out? of 
adjacency pair groupings were treated as atomic 
elements in subsequent analysis.  Figure 1 
illustrates the application of the adjacency pair 
joining algorithm on a sequence of dialogue acts 
from the corpus. 
 
 
Figure 1.  DA Sequence Before/After Joining 
3 HMM of Dialogue Structure 
A hidden Markov model is defined by three 
constituents:  1) the set of hidden states (dialogue 
modes), each characterized by its emission 
probability distribution over the possible 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
 
acti acti+1 
P(acti+1|   
    acti) 
P(acti+1| 
   ?acti) 
?2 
val p-val 
EQs PFt 0.48 0.07 654 <0.0001 
Gs Gt 0.27 0.03 380 <0.0001 
EXs EXt 0.34 0.03 378 <0.0001 
EQt PFs 0.18 0.01 322 <0.0001 
EQt Ss 0.24 0.03 289 <0.0001 
EQs LFt 0.13 0.01 265 <0.0001 
Qt Ss 0.65 0.04 235 <0.0001 
EQt LFs 0.07 0.00 219 <0.0001 
Qs St 0.82 0.38 210 <0.0001 
EQs NFt 0.08 0.01 207 <0.0001 
EXt EXs 0.19 0.02 177 <0.0001 
NFs Gt 0.29 0.03 172 <0.0001 
EQt NFs 0.11 0.01 133 <0.0001 
Ss Gt 0.16 0.03 95 <0.0001 
Ss PFt 0.30 0.10 90 <0.0001 
St Gs 0.07 0.04 36 <0.0001 
PFs Gt 0.14 0.04 34 <0.0001 
LFs Gt 0.22 0.04 30 <0.0001 
St EQs 0.11 0.07 29 <0.0001 
Gt EXs 0.07 0.03 14 0.002 
St Qs 0.07 0.05 14 0.0002 
Gt Gs 0.10 0.05 9 0.0027 
EQt EQs 0.13 0.08 8 0.0042 
Table 2. All Dependent Adjacency Pairs 
 
hidden states, and 3) the initial hidden state 
(dialogue mode) probability distribution.   
3.1  Discovering Number of Dialogue Modes 
In keeping with the goal of automatically 
discovering dialogue structure, it was desirable to 
learn n, the best number of hidden states for the 
HMM, during modeling.  To this end, we trained 
and ten-fold cross-validated seven models, each 
featuring randomly-initialized parameters, for each 
number of hidden states n from 2 to 15, inclusive.2  
The average log-likelihood fit from ten-fold cross-
                                                           
2 n=15 was chosen as an initial maximum number of states 
because it comfortably exceeded our hypothesized range of 3 
to 7 (informed by the tutoring literature).  The Akaike 
Information Criterion measure steadily worsened above n = 5, 
confirming no need to train models with n > 15. 
21
validation was computed across all seven models 
for each n, and this average log-likelihood ln was 
used to compute the Akaike Information Criterion, 
a maximum-penalized likelihood estimator that 
prefers simpler models (Scott 2002).  This 
modeling approach was used to train HMMs on 
both the dialogue act and the adjacency pair input 
sequences. 
3.2  Best-Fit Models 
The input sequences of individual dialogue acts 
contain 16 unique symbols because each of the 8 
dialogue act tags (Table 1) was augmented with a 
label of the speaker, either tutor or student.  The 
best-fit HMM for this input sequence contains 
nDA=5 hidden states.  The adjacency pair input 
sequences contain 39 unique symbols, including all 
dependent adjacency pairs (Table 2) along with all 
individual dialogue acts because each dialogue act 
occurs at some point outside an adjacency pair.  
The best-fit HMM for this input sequence contains 
nAP=4 hidden states.  In both cases, the best-fit 
number of dialogue modes implied by the hidden 
states is within the range of what is often 
considered in traditional tutorial dialogue analysis 
(Cade et al 2008; Graesser, Person & Magliano 
1995).   
4 Analysis 
Evaluating the impact of grouping the dialogue 
acts into adjacency pairs requires a fine-grained 
examination of the generated HMMs to gain 
insight into how each model interprets the student 
sessions.     
4.1 Dialogue Act HMM 
Figure 2 displays the emission probability 
distributions for the dialogue act HMM.  State 0DA, 
Tutor Lecture,3 is strongly dominated by tutor 
statements with some student questions and 
positive tutor feedback.  State 1DA constitutes 
Grounding/Extra-Domain, a conversational state 
consisting of acknowledgments, backchannels, and 
discussions that do not relate to the computer 
science task.  State 2DA, Student Reflection, 
                                                           
3 For simplicity, the states of each HMM have been named 
according to an intuitive interpretation of the emission 
probability distribution.   
generates student evaluation questions, statements, 
and positive and negative feedback.  State 3DA is 
comprised of tutor utterances, with positive 
feedback occurring most commonly followed by 
statements, grounding, lukewarm feedback, and 
negative feedback.  This state is interpreted as a 
Tutor Feedback mode.  Finally, State 4DA, Tutor 
Lecture/Probing, is characterized by tutor 
statements and evaluative questions with some 
student grounding statements.   
 
 
Figure 2.  Emission Probability Distributions for 
Dialogue Act HMM 
 
    The state transition diagram (Figure 3) illustrates 
that Tutor Lecture (0DA) and Grounding/Extra-
Domain (1DA) are stable states whose probability of 
self-transition is high:  0.75 and 0.79, respectively.  
Perhaps not surprisingly, Student Reflection (2DA) 
is most likely to transition to Tutor Feedback (3DA) 
with probability 0.77.  Tutor Feedback (3DA) 
transitions to Tutor Lecture (0DA) with probability 
0.60, Tutor Lecture/Probing (4DA) with probability 
0.26, and Student Reflection (2DA) with probability 
0.09.  Finally, Tutor Lecture/Probing (4DA) very 
often transitions to Student Reflection (2DA) with 
probability 0.82. 
 
22
 
Figure 3. Transition diagram for dialogue act HMM 
4.2 Adjacency Pair HMM 
Figure 4 displays the emission probability 
distributions for the HMM that was trained on the 
input sequences of adjacency pairs.  State 0AP, 
Tutor Lecture, consists of tutorial statements, 
positive feedback, and dialogue turns initiated by 
student questions.  In this state, student evaluation 
questions occur in adjacency pairs with positive 
tutor feedback, and other student questions are 
answered by tutorial statements.  State 1AP, Tutor 
Evaluation, generates primarily tutor evaluation 
questions, along with the adjacency pair of tutorial 
statements followed by student acknowledgements.  
State 2AP generates conversational grounding and 
extra-domain talk; this Grounding/Extra-Domain 
state is dominated by the adjacency pair of student 
grounding followed by tutor grounding.  State 3AP 
is comprised of several adjacency pairs:  student 
questions followed by tutor answers, student 
statements with positive tutor feedback, and 
student evaluation questions followed by positive 
feedback.  This Question/Answer state also 
generates some tutor grounding and student 
evaluation questions outside of adjacency pairs.   
 
 
Figure 4.  Emission Probability Distributions for 
Adjacency Pair HMM 
 
 
Figure 5. Transition diagram for adjacency pair HMM 
0DA 
3DA 
2DA 
1DA 
4DA 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
0AP 
3AP 
2AP 
1AP 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
23
 4.3 Dialogue Mode Sequences 
In order to illustrate how the above models fit the 
data, Figure 6 depicts the progression of dialogue 
modes that generate an excerpt from the corpus. 
 
 
Figure 6.  Best-fit sequences of hidden states 
In both models, the most commonly-occurring 
dialogue mode is Tutor Lecture, which generates 
45% of observations in the dialogue act model and 
around 60% in the adjacency pair model.  
Approximately 15% of the dialogue act HMM 
observations are fit to each of states Student 
Reflection, Tutor Feedback, and Tutor 
Lecture/Probing.  This model spends the least 
time, around 8%, in Grounding/Extra Domain.  
The adjacency pair model fits approximately 15% 
of its observations to each of Tutor Evaluation and 
Question/Answer, with around 8% in 
Grounding/Extra-Domain.   
4.4 Model Comparison 
While the two models presented here describe the 
same corpus, it is important to exercise caution 
when making direct structural comparisons.  The 
models contain neither the same number of hidden 
states nor the same emission symbol alphabet; 
therefore, our comparison will be primarily 
qualitative.  It is meaningful to note, however, that 
the adjacency pair model with nAP=4 achieved an 
average log-likelihood fit on the training data that 
was 5.8% better than the same measure achieved 
by the dialogue act model with nDA=5, despite the 
adjacency pair input sequences containing greater 
than twice the number of unique symbols.4   
                                                           
4 This comparison is meaningful because the models depicted 
here provided the best fit among all sizes of models trained for 
the same input scenario. 
    Our qualitative comparison begins by examining 
the modes that are highly similar in the two 
models.  State 2AP generates grounding and extra-
domain statements, as does State 1DA.  These two 
states both constitute a Grounding/Extra-Domain 
dialogue mode.  One artifact of the tutoring study 
design is that all sessions begin in this state due to 
a compulsory greeting that signaled the start of 
each session.  More precisely, the initial state 
probability distribution for each HMM assigns 
probability 1 to this state and probability 0 to all 
other states.     
    Another dialogue mode that is structurally 
similar in the two models is Tutor Lecture, in 
which the majority of utterances are tutor 
statements.  This mode is captured in State 0 in 
both models, with State 0AP implying more detail 
than State 0DA because it is certain in the former 
that some of the tutor statements and positive 
feedback occurred in response to student questions.  
While student questions are present in State 0DA, no 
such precise ordering of the acts can be inferred, as 
discussed in Section 1.    
    Other states do not have one-to-one 
correspondence between the two models.  State 
2DA, Student Reflection, generates only student 
utterances and the self-transition probability for the 
state is very low; the dialogue usually visits State 
2DA for one turn and then transitions immediately 
to another state.  Although this aspect of the model 
reflects the fact that students rarely keep the floor 
for more than one utterance at a time in the corpus, 
such quick dialogue mode transitions are 
inconsistent with an intuitive understanding of 
tutorial dialogue modes as meta-structures that 
usually encompass more than one dialogue turn.  
This phenomenon is perhaps more accurately 
captured in the adjacency pair model.  For 
example, the dominant dialogue act of State 2DA is 
a student evaluation question (EQs).  In contrast, 
these dialogue acts are generated as part of an 
adjacency pair by State 3AP; this model joins the 
student questions with subsequent positive 
feedback from the tutor rather than generating the 
question and then transitioning to a new dialogue 
mode.  Further addressing the issue of frequent 
state transitions is discussed as future work in 
Section 6. 
     
24
5 Discussion and Limitations 
Overall, the adjacency pair model is preferable for 
our purposes because its structure lends itself more 
readily to interpretation as a set of dialogue modes 
each of which encompasses more than one 
dialogue move.  This structural property is 
guaranteed by the inclusion of adjacency pairs as 
atomic elements.  In addition, although the set of 
emission symbols increased to include significant 
adjacency pairs along with all dialogue acts, the 
log-likelihood fit of this model was slightly higher 
than the same measure for the HMM trained on the 
sequences of dialogue acts alone.  The remainder 
of this section focuses on properties of the 
adjacency pair model. 
    One promising result of this early work emerges 
from the fact that by applying hidden Markov 
modeling to sequences of adjacency pairs, 
meaningful dialogue modes have emerged that are 
empirically justified.  The number of these 
dialogue modes is consistent with what researchers 
have traditionally used as a set of hypothesized 
tutorial dialogue modes.  Moreover, the 
composition of the dialogue modes reflects some 
recognizable aspects of tutoring sessions:  tutors 
teach through the Tutor Lecture mode and give 
feedback on student knowledge in a Tutor 
Evaluation mode.  Students ask questions and state 
their own perception of their knowledge in a 
Question/Answer mode.  Both parties engage in 
?housekeeping? talk containing such things as 
greetings and acknowledgements, and sometimes, 
even in a controlled environment, extra-domain 
conversation occurs between the conversants in the 
Grounding/Extra-Domain mode.   
    Although the tutorial modes discovered may not 
map perfectly to sets of handcrafted tutorial 
dialogue modes from the literature (e.g., Cade et 
al. 2008), it is rare for such a perfect mapping to 
exist even between those sets of handcrafted 
modes.  In addition, the HMM framework allows 
for succinct probabilistic description of the 
phenomena at work during the tutoring session:  
through the state transition matrix, we can see the 
back-and-forth flow of the dialogue among its 
modes. 
6 Conclusions and Future Work 
Automatically learning dialogue structure is an 
important step toward creating more robust tutorial 
dialogue management systems.  We have presented 
two hidden Markov models in which the hidden 
states are interpreted as dialogue modes for task-
oriented tutorial dialogue.  These models were 
learned in an unsupervised fashion from manually-
labeled dialogue acts.  HMMs offer concise 
stochastic models of the complex interaction 
patterns occurring in natural language tutorial 
dialogue.  The evidence suggests this 
methodology, which as presented requires only a 
sequence of dialogue acts as input, holds promise 
for automatically discovering the structure of 
tutorial dialogue.   
    Future work will involve conducting evaluations 
to determine the benefits gained by using HMMs 
compared to simpler statistical models.  In 
addition, it is possible that more general types of 
graphical models will prove useful in overcoming 
some limitations of HMMs, such as their arbitrarily 
frequent state transitions, to more readily capture 
the phenomena of interest.  The descriptive insight 
offered by these exploratory models may also be 
increased by future work in which the input 
sequences are enhanced with information about the 
surface-level content of the utterance.  In addition, 
knowledge of the task state within the tutoring 
session can be used to segment the dialogue in 
meaningful ways to further refine model structure.   
    It is also hoped that these models can identify 
empirically-derived tutorial dialogue structures that 
can be associated with measures of effectiveness 
such as student learning (Soller & Stevens 2007).  
These lines of investigation could inform the 
development of next-generation natural language 
tutorial dialogue systems.   
Acknowledgments 
Thanks to Marilyn Walker and Dennis Bahler for 
insightful early discussions on the dialogue and machine 
learning aspects of this work, respectively.  This 
research was supported by the National Science 
Foundation under Grants REC-0632450, IIS-0812291, 
CNS-0540523, and GRFP.  Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not necessarily 
reflect the views of the National Science Foundation. 
25
 
References 
Aleven, V., K. Koedinger, and O. Popescu. 2003. A 
tutorial dialog system to support self-explanation: 
Evaluation and open questions. Proceedings of the 
11th International Conference on Artificial 
Intelligence in Education: 39-46. 
Arnott, E., P. Hastings, and D. Allbritton. 2008. 
Research methods tutor: Evaluation of a dialogue-
based tutoring system in the classroom. Behavioral 
Research Methods 40(3): 694-698. 
Bangalore, S., Di Fabbrizio, G., and Stent, A. 2006. 
Learning the structure of task-driven human-human 
dialogs.  Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the ACL: 201-208. 
Barzilay, R., and Lee, L. 2004. Catching the drift: 
Probabilistic content models, with applications to 
generation and summarization.  Proceedings of 
NAACL HLT: 113?120. 
Boyer, K. E., Phillips, R., Wallis, M., Vouk, M., and 
Lester, J. 2008. Balancing cognitive and 
motivational scaffolding in tutorial dialogue.  
Proceedings of the 9th International Conference on 
Intelligent Tutoring Systems: 239-249. 
Cade, W., Copeland, J., Person, N., and D'Mello, S. 
2008. Dialog modes in expert tutoring.  Proceedings 
of the 9th International Conference on Intelligent 
Tutoring Systems: 470-479. 
Chi, M., Jordan, P., VanLehn, K., and Hall, M. 2008. 
Reinforcement learning-based feature selection for 
developing pedagogically effective tutorial dialogue 
tactics.  Proceedings of the 1st International 
Conference  on Educational Data Mining: 258-265. 
Evens, M., and J. Michael. 2006. One-on-one tutoring 
by humans and computers. Lawrence Erlbaum 
Associates, Mahwah, New Jersey. 
Forbes-Riley, K., and Litman, D. J. 2005. Using 
bigrams to identify relationships between student 
certainness states and tutor responses in a spoken 
dialogue corpus. Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue: 87-96. 
Forbes-Riley, K., Rotaru, M., Litman, D. J., and 
Tetreault, J. 2007. Exploring affect-context 
dependencies for adaptive system development. 
Proceedings of NAACL HLT: 41-44. 
Graesser, A., G. Jackson, E. Mathews, H. Mitchell, A. 
Olney, M. Ventura, and P. Chipman. 2003. 
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog. 
Proceedings of the Twenty-Fifth Annual Conference 
of the Cognitive Science Society: 1-6. 
Graesser, A. C., N. K. Person, and J. P. Magliano. 1995. 
Collaborative dialogue patterns in naturalistic one-
to-one tutoring. Applied Cognitive Psychology 9(6): 
495?522. 
Lepper, M. R., M. Woolverton, D. L. Mumme, and J. L. 
Gurtner. 1993. Motivational techniques of expert 
human tutors: Lessons for the design of computer-
based tutors. Pages 75-105 in S. P. Lajoie, and S. J. 
Derry, editors. Computers as cognitive tools. 
Lawrence Erlbaum Associates, Hillsdale, New 
Jersey. 
Litman, D. J., C. P. Ros?, K. Forbes-Riley, K. VanLehn, 
D. Bhembe, and S. Silliman. 2006. Spoken versus 
typed human and computer dialogue tutoring. 
International Journal of Artificial Intelligence in 
Education 16(2): 145-170. 
Purver, M., Kording, K. P., Griffiths, T. L., and 
Tenenbaum, J. B. 2006. Unsupervised topic 
modelling for multi-party spoken discourse.  
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL: 17-24. 
Rabiner, L. R. 1989. A tutorial on hidden Markov 
models and selected applications in speech 
recognition. Proceedings of the IEEE 77(2): 257-
286. 
Schlegoff, E., and H. Sacks. 1973. Opening up closings. 
Semiotica 7(4): 289-327. 
Scott, S. L. 2002. Bayesian methods for hidden Markov 
models: Recursive computing in the 21st century. 
Journal of the American Statistical Association 
97(457): 337-352. 
Soller, A., and R. Stevens. 2007. Applications of 
stochastic  analyses for collaborative learning and 
cognitive assessment. Pages 217-253 in G. R. 
Hancock, and K. M. Samuelsen, editors. Advances 
in latent variable mixture models. Information Age 
Publishing. 
Tetreault, J. R., and D. J. Litman. 2008. A 
reinforcement learning approach to evaluating state 
representations in spoken dialogue systems. Speech 
Communication 50(8-9): 683-696. 
VanLehn, K., P. W. Jordan, C. P. Rose, D. Bhembe, M. 
Bottner, A. Gaydos, M. Makatchev, U. 
Pappuswamy, M. Ringenberg, and A. Roque. 2002. 
The architecture of Why2-atlas: A coach for 
qualitative physics essay writing. Proceedings of 
Intelligent Tutoring Systems Conference: 158?167. 
Zinn, C., Moore, J. D., and Core, M. G. 2002. A 3-tier 
planning architecture for managing tutorial dialogue.  
Proceedings of the 6th International Conference on 
Intelligent Tutoring Systems: 574-584. 
26
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 66?73,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Leveraging Hidden Dialogue State to Select Tutorial Moves  Kristy Elizabeth    Boyera Robert    Phillipsab Eun Young Haa Michael D.    Wallisab Mladen A.   Vouka James C. Lestera   aDepartment of Computer Science, North Carolina State University bApplied Research Associates Raleigh, NC, USA  {keboyer, rphilli, eha, mdwallis, vouk, lester}@ncsu.edu    Abstract 
A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context. Corpus-based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 1 Introduction A central challenge for dialogue systems is selecting appropriate system dialogue moves (Bangalore, Di Fabbrizio, & Stent, 2008; Frampton & Lemon, 2009; Young et al, 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, & Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 
derived from observing human tutors (e.g., Aleven, McLaren, Roll, & Koedinger, 2004; Evens & Michael, 2006; Graesser, Chipman, Haynes, & Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, & Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al, 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al, 2008; Hardy et al, 2006; Young et al, 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, & Hall, 2008; Tetreault & Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors.  The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact by the user (the student). The modification of this artifact, in our case a computer program, is the focus of the dialogues. Our corpus consists of textual dialogue utterances and a separate synchronous stream of 
66
task actions. Our goal is to extract a data-driven dialogue management model from the corpus, as evidenced by predicting system (tutor) dialogue acts.  In this paper, we present an annotation approach that addresses dialogue utterances and task actions, and we propose a unified sequential representation for these separate synchronous streams of events. We explore the predictive power of three stochastic models ? first-order Markov models, hidden Markov models, and hierarchical hidden Markov models ? for predicting tutor dialogue acts in the unified sequences. By leveraging these models to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, & Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton & Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, & Litman, 2007; Henderson, Lemon, & Georgila, 2008; Heeman, 2007; Young et al, 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault & Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al, 2008).  While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al, 2006) and catalogue ordering (Bangalore et al, 2008). Like the parse-based models of Bangalore et al, our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio & Mikheev, 1998).  
For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens & Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007; Forbes-Riley & Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, & Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al; we explore first-order Markov models, which are equivalent to bigram models, for predicting tutor dialogue acts.  In addition, we present HMMs and HHMMs trained on our corpus. We found that both of these models outperformed the bigram model for predicting tutor moves. 3 Corpus and Annotation The corpus was collected during a human-human tutoring study in which tutors and students worked to solve an introductory computer programming problem (Boyer et al, in press). The dialogues were effective: on average, students exhibited a 7% absolute gain from pretest to posttest (N=48, paired t-test p<0.0001).  The corpus contains 48 textual dialogues with a separate, synchronous task event stream. Tutors and students collaborated to solve an introductory computer programming problem using an online tutorial environment with shared workspace viewing and textual dialogue. Each student participated in exactly one tutoring session. The corpus contains 1,468 student utterances, 3,338 tutor utterances, and 3,793 student task actions. In order to build the dialogue model, we annotated the corpus with dialogue act tags and task annotation labels. 3.1 Dialogue Act Annotation  We have developed a dialogue act tagset inspired by schemes for conversational speech (Stolcke et al, 2000), task-oriented dialogue (Core & Allen, 1997), and tutoring (Litman & Forbes-Riley, 2006). The dialogue act tags are displayed in Table 1. Overall reliability on 10% of the corpus for two annotators was ?=0.80.   
67
 Table 1. Dialogue act tags 
DA? Description?
Stu.?Rel.?Freq.?
Tut.?Rel.?Freq.? ??ASSESSING?QUESTION?(AQ)? Request?for?feedback?on?task?or?conceptual?utterance.? .20? .11? .91?EXTRA?DOMAIN?(EX)? Asides?not?relevant?to?the?tutoring?task.? .08? .04? .79?GROUNDING?(G)? Acknowledgement/thanks? .26? .06? .92?LUKEWARM?CONTENT?FEEDBACK?(LCF)? Negative?assessment?with?explanation.? .01? .03? .53?LUKEWARM?FEEDBACK?(LF)? Lukewarm?assessment?of?task?action?or?conceptual?utterance.? .02? .03? .49?NEGATIVE?CONTENT?FEEDBACK?(NCF)?
Negative?assessment?with?explanation.? .01? .10? .61?
NEGATIVE?FEEDBACK?(NF)? Negative?assessment?of?task?action?or?conceptual?utterance.? .05? .02? .76?POSITIVE?CONTENT?FEEDBACK?(PCF)? Positive?assessment?with?explanation.? .02? .03? .43?POSITIVE?FEEDBACK?(PF)? Positive?assessment?of?task?action?or?conceptual?utterance.? .09? .16? .81?QUESTION?(Q)? Task?or?conceptual?question.? .09? .03? .85?STATEMENT?(S)? Task?or?conceptual?assertion.? .16? .41? .82?
3.2 Task Annotation The dialogues focused on the task of solving an introductory computer programming problem. The task actions were recorded as a separate but synchronous event stream. This stream included 97,509 keystroke-level user task events. These events were manually aggregated and annotated for subtask structure and then for correctness. The task annotation scheme was hierarchical, reflecting the nested nature of the subtasks. An excerpt from the task annotation scheme is depicted in Figure 1; the full scheme contains 66 leaves. The task annotation scheme was designed to reflect the different depth of possible subtasks nested within the overall task. Each labeled task action was also judged for correctness according to the requirements of the task, with categories CORRECT, BUGGY, INCOMPLETE, and DISPREFERRED (technically 
correct but not accomplishing the pedagogical goals of the task). Each group of task keystrokes that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced (giving judges maximum flexibility to apply the tags). To ensure a conservative reliability statistic, all unmatched subtask tags were treated as disagreements. The resulting unweighted kappa statistic was ?simple= 0.58, but the weighted Kappa ?weighted=0.86 is more meaningful because it takes into account the ordinal nature of the labels that result from sequential subtasks. On task actions for which the two judges agreed on subtask tag, the agreement statistic for correctness was ?simple=0.80. 
 Figure 1. Portion of task annotation scheme 3.3 Adjacency Pair Joining Some dialogue acts establish an expectation for another dialogue act to occur next (Schegloff & Sacks, 1973). Our previous work has found that identifying the statistically significant adjacency pairs in a corpus and joining them as atomic observations prior to model building produces more interpretable descriptive models. The models reported here were trained on hybrid sequences of dialogue acts and adjacency pairs. A full description of the adjacency pair identification methodology and joining algorithm is reported in (Boyer et al, 2009). A partial list of the most highly statistically significant adjacency pairs, 
68
which for this work include task actions, is displayed in Table 2.   Table 2. Subset of significant adjacency pairs CORRECTTASKACTION?CORRECTTASKACTION;??EXTRADOMAINS?EXTRADOMAINT;?GROUNDINGS?GROUNDINGT;?ASSESSINGQUESTIONT?POSITIVEFEEDBACKS;??ASSESSINGQUESTIONS?POSITIVEFEEDBACKT;?QUESTIONT?STATEMENTS;?ASSESSINGQUESTIONT?STATEMENTS;?EXTRADOMAINT?EXTRADOMAINS;?QUESTIONS?STATEMENTT;?NEGATIVEFEEDBACKS?GROUNDINGT;?INCOMPLETETASKACTION?INCOMPLETETASKACTION;?POSITIVEFEEDBACKS?GROUNDINGT;??BUGGYTASKACTION?BUGGYTASKACTION 4 Models We learned three types of models using cross-validation with systematic sampling of training and testing sets. 
4.1 First-Order Markov Model The simplest model we discuss is the first-order Markov model (MM), or bigram model (Figure 2). A MM that generates observation (state) sequence o1o2?ot is defined in the following way. The observation symbols are drawn from the alphabet ?={?1, ?2, ?, ?M}, and the initial probability distribution is ?=[?i] where ?i is the probability of a sequence beginning with observation symbol ?i. The transition probability distribution is A=[aij], where aij is the probability of observation j occurring immediately after observation i. 
 Figure 2. Time-slice topology of MM  We trained MMs on our corpus of dialogue acts and task events using ten-fold cross-validation to produce a model that could be queried for the next predicted tutorial dialogue act given the history.  
4.2 Hidden Markov Model A hidden Markov model (HMM) augments the MM framework, resulting in a doubly stochastic structure (Rabiner, 1989). For a first-order HMM, the observation symbol alphabet is defined as above, along with a set of hidden states S={s1,s2,?,sN}. The transition and initial probability distributions are defined analogously to MMs, except that they operate on hidden states 
rather than on observation symbols (Figure 3). That is, ?=[?i] where ?i is the probability of a sequence beginning in hidden state si. The transition matrix is A=[aij], where aij is the probability of the model transitioning from hidden state i to hidden state j. This framework constitutes the first stochastic layer of the model, which can be thought of as modeling hidden, or unobservable, structure. The second stochastic layer of the model governs the production of observation symbols: the emission probability distribution is B=[bik] where bik is the probability of state i emitting observation symbol k. 
 Figure 3. Time-slice topology of HMM  The notion that dialogue has an overarching unobservable structure that influences the observations is widely accepted. In tutoring, this overarching structure may correspond to tutorial strategies. We have explored HMMs? descriptive power for extracting these strategies (Boyer et al, 2009), and this paper explores the hypothesis that HMMs provide better predictive power than MMs on our dialogue sequences. We trained HMMs on the corpus using the standard Baum-Welch expectation maximization algorithm and applied state labels that reflect post-hoc interpretation (Figure 4).  
 Figure 4. Portion of learned HMM 
69
4.3 Hierarchical Hidden Markov Model Hierarchical hidden Markov models (HHMMs) allow for explicit representation of multilevel stochastic structure. A complete formal definition of HHMMs can be found in (Fine, Singer, & Tishby, 1998), but here we present an informal description.  HHMMs include two types of hidden states: internal nodes, which do not produce observation symbols, and production nodes, which do produce observations. An internal node includes a set of substates that correspond to its potential children, S={s1, s2, ?, sN}, each of which is itself the root of an HHMM. The initial probability distribution ?=[?i] for each internal node governs the probability that the model will make a vertical transition to substate si from this internal node; that is, that this internal node will produce substate si as its leftmost child. Horizontal transitions are governed by a transition probability distribution similar to that described above for flat HMMs. Production nodes are defined by their observation symbol alphabet and an emission probability distribution over the symbols; HHMMs do not require a global observation symbol alphabet. The generative topology of our HHMMs is illustrated in Figure 5. 
 Figure 5. Generative topology of HHMM  HHMMs of arbitrary topology can be trained using a generalized version of the Baum-Welch algorithm (Fine et al, 1998). Our HHMMs featured a pre-specified model topology based on known task/subtask structure. A Bayesian view of a portion of the best-fit HHMM is depicted in Figure 6.  This model was trained using five-fold cross-validation to address the absence of symbols from the training set that were present in the testing set, a sparsity problem that arose from splitting the data hierarchically. 
Figure 6. Portion of learned HHMM 
70
5 Results We trained and tested MMs, HMMs, and HHMMs on the corpus and compared prediction accuracy for tutorial dialogue acts by providing the model with partial sequences from the test set and querying for the next tutorial move. The baseline prediction accuracy for this task is 41.1%, corresponding to the most frequent tutorial dialogue act (STATEMENT). As depicted in Figure 7, a first-order MM performed worse than baseline (p<0.001)1 at 27% average prediction accuracy (
? 
? ? MM=6%). HMMs performed better than baseline (p<0.0001), with an average accuracy of 48% (
? 
? ? HMM=3%). HHMMs averaged 57% accuracy, significantly higher than baseline (p=0.002) but weakly significantly higher than HMMs (p=0.04), and with high variation (
? 
? ? HHMM=23%). 
 Figure 7. Average prediction accuracies of three model types on tutor dialogue acts  To further explore the performance of the HHMMs, Figure 8 displays their prediction accuracy on each of six labeled subtasks. These subtasks correspond to the top level of the hierarchical task/subtask annotation scheme. The UNDERSTAND THE PROBLEM subtask corresponds to the initial phase of most tutoring sessions, in which the student and tutor agree to some extent on a problem-solving plan. Subtasks 1, 2, and 3 account for the implementation and debugging of three distinct modules within the learning task, and Subtask 4 involves testing and assessing the student?s finalized program. The EXTRA-DOMAIN subtask involves side conversations whose topics are outside of the domain.  The HHMM performed as well as or better (p<0.01) than baseline on the first three in-domain subtasks. The performance on SUBTASK 4 was not distinguishable from baseline (p=0.06); relatively few students reached this subtask. The model did                                                 1 All p-values in this section were produced by two-sample one-tailed t-tests with unequal sample variances. 
not outperform baseline (p=0.40) for the UNDERSTAND THE PROBLEM subtask, and qualitative inspection of the corpus reveals that the dialogue during this phase of tutoring exhibits limited regularities between students.  
 Figure 8. Average prediction accuracies of HHMMs by subtask 6 Discussion The results support our hypothesis that HMMs, because of their capacity for explicitly representing dialogue structure at an abstract level, perform better than MMs for predicting tutor moves. The results also suggest that explicitly modeling hierarchical task structure can further improve prediction accuracy of the model. The below-baseline performance of the bigram model illustrates that in our complex task-oriented domain, an immediately preceding event is not highly predictive of the next move. While this finding may not hold for conversational dialogue or some task-oriented dialogue with a more balanced distribution of utterances between speakers, the unbalanced nature of our tutoring sessions may not be as easily captured.  In our corpus, tutor utterances outnumber student utterances by more than two to one. This large difference is due to the fact that tutors frequently guided students and provided multi-turn explanations, the impetus for which are not captured in the corpus, but rather, involve external pedagogical goals. The MM, or bigram model, has no mechanism for capturing this layer of stochastic behavior. On the other hand, the HMM can account for unobserved influential variables, and the HHMM can do so to an even greater extent by explicitly modeling task/subtask structure. Considering the performance of the HHMM on individual subtasks reveals interesting properties of our dialogues. First, the HHMM is unable to outperform baseline on the UNDERSTAND THE PROBLEM subtask. To address this issue, our ongoing work investigates taking into account 
71
student characteristics such as incoming knowledge level and self-confidence. On all four in-domain subtasks, the HHMM achieved a 30% to 50% increase over baseline. For extra-domain dialogues, which involve side conversations that are not task-related, the HHMM achieved 86% prediction accuracy on tutor moves, which constitutes a 115% improvement over baseline. This high accuracy may be due in part to the fact that out-of-domain asides were almost exclusively initiated by the student, and tutors rarely engaged in such exchanges beyond providing a single response. This regularity likely facilitated prediction of the tutor?s dialogue moves during out-of-domain talk. We are aware of only one recent project that reports extensively on predicting system actions from a corpus of human-human dialogue. Bangalore et al?s (2008) flat task/dialogue model in a catalogue-ordering domain achieved a prediction accuracy of 55% for system dialogue acts, a 175% improvement over baseline. When explicitly modeling the hierarchical task/subtask dialogue structure, they report a prediction accuracy of 35.6% for system moves, approximately 75% above baseline (Bangalore & Stent, 2009). These findings were obtained by utilizing a variety of lexical and syntactic features along with manually annotated dialogue acts and task/subtask labels. In comparison, our HHMM achieved an average 42% improvement over baseline using only annotated dialogue acts and task/subtask labels. In ongoing work we are exploring the utility of additional features for this prediction task. Our best model performed better than baseline by a significant margin. The absolute prediction accuracy achieved by the HHMM was 57% across the corpus, which at first blush may appear too low to be of practical use. However, the choice of tutorial move involves some measure of subjectivity, and in many contexts there may be no uniquely appropriate dialogue act. Work in other domains has dealt with this uncertainty by maintaining multiple hypotheses (Wright Hastie, Poesio, & Isard, 2002) and by mapping to clustered sets of moves rather than maintaining policies for each possible system selection (Young et al, 2009). Such approaches may prove useful in our domain as well, and may help to more fully realize 
the potential of a learned dialogue management model.  7 Conclusion and Future Work Learning models that predict system moves within a corpus is a first step toward building fully data-driven dialogue management models. We have presented Markov models, hidden Markov models, and hierarchical hidden Markov models trained on sequences of manually annotated dialogue acts and task events. Of the three models, the hierarchical models appear to perform best in our domain, which involves an intrinsically hierarchical task/subtask structure.  The models? performance points to promising future work that includes utilizing additional lexical and syntactic features along with fixed user (student) characteristics within a hierarchical hidden Markov modeling framework. More broadly, the results point to the importance of considering task structure when modeling a complex domain such as those that often accompany task-oriented tutoring. Finally, a key direction for data-driven dialogue management models involves learning unsupervised dialogue act and task classification models.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Ai, H., Tetreault, J. R., & Litman, D. J. (2007). Comparing user simulation models for dialog strategy learning. Proceedings of NAACL HLT, Companion Volume, Rochester, New York. 1-4.  Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking: Applying cognitive modeling to meta-cognitive skills. Proceedings of ITS, 227-239.  Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  
72
Bangalore, S., & Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the EACL, 94-102.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of NAACL HLT (Short Papers), 19-26. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (In press). Characterizing the effectiveness of tutorial dialogue with hidden Markov models. Proceedings of ITS, Pittsburgh, Pennsylvania.  Chi, M., Jordan, P., VanLehn, K., & Hall, M. (2008). Reinforcement learning-based feature selection for developing pedagogically effective tutorial dialogue tactics. Proceedings of EDM, Montreal, Canada. 258-265.  Chi, M., Jordan, P., VanLehn, K., & Litman, D. (2009). To elicit or to tell: Does it matter? Proceedings of AIED, 197-204.  Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.   Evens, M., & Michael, J. (2006). One-on-one tutoring by humans and computers. Mahwah, New Jersey: Lawrence Erlbaum Associates. Fine, S., Singer, Y., & Tishby, N. (1998). The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1), 41-62.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context dependencies for adaptive system development. Proceedings of NAACL HLT (Short Papers), 41-44.  Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Graesser, A. C., Chipman, P., Haynes, B. C., & Olney, A. (2005). AutoTutor: An intelligent tutoring system with mixed-initiative dialogue. IEEE Transactions on Education, 48(4), 612-618.  Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354-373.  Heeman, P. A. (2007). Combining reinforcement learning with information-state update rules. Proceedings of NAACL HLT, 268-275.  
Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487-511.  Jordan, P., Makatchev, M., Pappuswamy, U., VanLehn, K., & Albacete, P. (2006). A natural language tutorial dialogue system for physics. Proceedings of FLAIRS, 521-526.  Kersey, C., Di Eugenio, B., Jordan, P., & Katz, S. (2009). KSC-PaL: A peer learning agent that encourages students to take the initiative. Proceedings of the NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Boulder, Colorado. 55-63.  Levin, E., Pieraccini, R., & Eckert, W. (2000). A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11-23.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Poesio, M., & Mikheev, A. (1998). The predictive power of game structure in dialogue act recognition: Experimental results using maximum entropy estimation. Proceedings of ICSLP, 90-97.  Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Schegloff, E., & Sacks, H. (1973). Opening up closings. Semiotica, 7(4), 289-327.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373.  Tetreault, J. R., & Litman, D. J. (2008). A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech Communication, 50(8-9), 683-696.  VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.  
73
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297?305,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Dialogue Act Modeling in a Complex Task-Oriented Domain 
  Kristy Elizabeth Boyer Eun Young Ha Robert Phillips* Michael D. Wallis* Mladen A. Vouk James C. Lester  Department of Computer Science, North Carolina State University Raleigh, North Carolina, USA  *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA  {keboyer,?eha,?rphilli,?mdwallis,?vouk,?lester}@ncsu.edu? Abstract 
Classifying the dialogue act of a user utterance is a key functionality of a dialogue management system. This paper presents a data-driven dialogue act classifier that is learned from a corpus of human textual dialogue. The task-oriented domain involves tutoring in computer programming exercises. While engaging in the task, students generate a task event stream that is separate from and in parallel with the dialogue. To deal with this complex task-oriented dialogue, we propose a vector-based representation that encodes features from both the dialogue and the hierarchically structured task for training a maximum likelihood classifier. This classifier also leverages knowledge of the hidden dialogue state as learned separately by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al, 2008; Frampton & Lemon, 2009; Hardy et al, 2006; Sridar et al, 2009; Young et al, 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which 
provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al, 2009; Stolcke et al, 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al, 2008; Chotimongkol, 2008; Hardy et al, 2006). Our work adopts this approach for a corpus of human-human dialogue in a task-oriented tutoring domain. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact, in our case a computer program, by the user during the course of the dialogue. Our corpus consists of human-human textual dialogue utterances and a separate, parallel stream of user-generated task actions. We utilize structural features including task/subtask, speaker, and hidden dialogue state along with lexical and syntactic features to interpret user (student) utterances.  This paper makes three contributions. First, it addresses representational issues in creating a dialogue model that integrates task actions with hierarchical task/subtask structure. The task is captured within a separate synchronous event stream that exists in parallel with the dialogue. Second, this paper explores the performance of dialogue act classifiers using different lexical/syntactic and structural feature sets. This comparison includes one model trained entirely on lexical/syntactic features, an important step toward robust unsupervised dialogue act tagging 
297
(Sridhar et al, 2009). Finally, it investigates whether the addition of HMM and task/subtask features improves the performance of the dialogue act classifiers. The findings support this hypothesis for three student dialogue moves, each with important implications for tutorial dialogue.  2 Related Work A variety of modeling approaches have been investigated for statistical dialogue act classification, including sequential approaches and vector-based classifiers. Sequential approaches typically formulate dialogue as a Markov chain in which an observation depends on a finite number of preceding observations. HMM-based approaches make use of the Markov assumption in a doubly stochastic framework that allows fitting optimal dialogue act sequences using the Viterbi algorithm (Rabiner, 1989; Stolcke et al, 2000). Like this work, the approach reported here adopts a first-order Markov formulation to train an HMM on sequences of dialogue acts, but the prediction of this HMM is subsequently encoded in a feature vector for training a vector-based classifier. Vector-based approaches, such as maximum entropy modeling, also frequently take into account both lexical/syntactic and structural features. Lexical and syntactic cues are extracted from local utterance context, while structural features involve longer dialogue act sequences and, in task-oriented domains, task/subtask history. Work by Bangalore et al (2008) on learning the structure of human-human dialogue in a catalogue-ordering domain (also extended to the Maptask and Switchboard corpora) utilizes features including words, part of speech tags, supertags, and named entities, and structural features including dialogue acts and task/subtask labels. In order to perform incremental decoding of dialogue acts and task/subtask structure, they take a greedy approach that does not require the search of complete dialogue sequences. Our work also accomplishes left-to-right incremental interpretation with a greedy approach. Our feature vectors differ from the aforementioned work slightly with respect to lexical/syntactic features and notably in the addition of a set of structural features generated by a separately trained HMM, as described in Section 4.2.  Recent work has explored the use of lexical, syntactic, and prosodic features for online dialogue act tagging (Sridhar et al, 2009); that 
work explores the notion that structural (history) features could be omitted altogether from incremental left-to-right decoding, resulting in computationally inexpensive and robust dialogue act classification. Although our textual dialogue does not feature prosodic cues, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach.   Like Bangalore et al (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti?s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al, 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al, 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al, 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multi-party discourse, also implicitly capture a task structure (Purver et al, 2006).  Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer?s name. The fulfillment of this subtask occurs entirely through the dialogue, and the resulting artifact (a completed order) is produced by the system. In contrast, our task involves the user constructing a solution to a computer programming problem. The fulfillment of this task occurs partially in the dialogue through tutoring, and partially in a separate synchronous stream of user-driven task actions about which the tutor must reason. The stream of user-driven task actions produces an artifact of value in itself (a functioning computer program), and that artifact is the subject of much of the dialogue. We propose a representation that integrates task actions and dialogue acts from these streams into a shared vector-based representation, and we investigate the use of the resulting structural, lexical, and syntactic features for dialogue act classification.  
298
3 Corpus and Annotation The corpus was collected during a controlled human-human tutoring study in which tutors and students worked through textual dialogue to solve an introductory computer programming problem. The dialogues were effective: on average, students exhibited significant learning and self-confidence gains (Boyer et al, 2009).   The corpus contains 48 dialogues each with a separate, synchronous task event stream as depicted in Excerpt 1 of the appendix. There is exactly one dialogue (tutoring session) per student. The corpus captures approximately 48 hours of dialogue and contains 1,468 student utterances and 3,338 tutor utterances. Because the dialogue was textual, utterance segmentation consisted of splitting at existing sentence boundaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1  The corpus was manually annotated with dialogue act labels and task/subtask features. Lexical and syntactic features were extracted automatically. The remainder of this section describes the manual annotation. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by schemes for conversational speech (Stolcke et al, 2000) and task-oriented dialogue (Core & Allen, 1997). It was also influenced by tutoring-specific tagsets (Litman & Forbes-Riley, 2006). Inter-rater reliability for the dialogue act tagging on 10% of the corpus selected via stratified (by tutor) random sampling was ?=0.80. The dialogue act tags, their relative frequencies, and their individual kappa scores from manual annotation are displayed in Table 1.  3.2 Task Annotation All task actions were generated by the student while implementing the solution to an introductory computer programming problem in Java. These task actions were recorded as a separate event stream in parallel with the dialogue corpus. This stream included 97,509 keystroke-level user task events, which were manually aggregated into task/subtask event clusters and annotated for subtask structure and then for correctness. A total of 3,793 aggregated                                                 1 Automatic segmentation is a challenging problem in itself and is left to future work. 
student subtask actions were identified through manual annotation. The task annotation scheme is hierarchical, reflecting the nested nature of the subtasks. A subset of this task annotation scheme is depicted in Figure 1. In the models reported in this paper, the 66 leaves of the task/subtask hierarchy were encoded in the input feature vectors.   Table 1. Student dialogue acts Student?Dialogue?Act? Rel.?Freq.? Human???ACKNOWLEDGMENT?(ACK)? .17? .90?REQUEST?FOR?FEEDBACK?(RF)? .20? .91?EXTRA?DOMAIN?(EX)? .08? .79?GREETING?(GR)? .04? .92?UNCERTAIN?FEEDBACK?WITH?ELABORATION?(UE)? .01? .53?UNCERTAIN?FEEDBACK?(U)? .02? .49?NEGATIVE?FEEDBACK?WITH?ELABORATION?(NE)? .01? .61?NEGATIVE?FEEDBACK?(N)? .05? .76?POSITIVE?FEEDBACK?WITH?ELABORATION?(PE)? .02? .43?POSITIVE?FEEDBACK?(P)? .09? .81?QUESTION?(Q)? .09? .85?STATEMENT?(S)? .16? .82?THANKS?(T)? .05? 1?
 Each group of task events that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. The judge aggregated the raw task keystrokes and tagged the task/subtask hierarchy for each cluster. (Please see Excerpt 1 in the appendix.) A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced, an approach that was intended to give judges maximum flexibility to cluster task actions and subsequently apply the tags. All unmatched subtask tags were treated as disagreements. The resulting kappa statistic at the leaves was ?= 0.58. However, we also observe that the sequential nature of the subtasks within the larger task produces an ordinal relationship between subtasks. For example, in Figure 1, the ?distance? between subtasks 1-a and 1-b can be thought of as ?less than? the distance between subtasks 1-a vs. 3-d because those subtasks are farther from each other within the larger task. The weighted Kappa statistic (Artstein & Poesio, 2008) takes into account such an ordinal relationship and its implicit distance function. The weighted Kappa is 
299
?weighted=0.86, which indicates acceptable inter-rater reliability on the task/subtask annotation. 
 Figure 1. Portion of task annotation scheme  Along with its tag for hierarchical subtask structure, each task event was also judged for correctness according to the requirements of the task as depicted in Table 2. The agreement statistic for correctness was calculated for task events on which the two judges agreed on subtask tag. The resulting unweighted agreement statistic for correctness was ?=0.80.  Table 2. Task correctness labels  Label? Description?CORRECT? Fully? satisfying? the? requirements? of?the? learning? task.? Does? not? require?tutorial?remediation.?BUGGY? Violating? the? requirements? of? the?learning?task.?Often?requires?tutorial?remediation.?INCOMPLETE? Not? violating,? but? not? yet? fully?satisfying,? the? requirements? of? the?learning? task.? May? require? tutorial?remediation.?DISPREFERRED? Technically? satisfying? the?requirements? of? the? learning? task,?but? not? adhering? to? its? pedagogical?intentions.? Usually? requires? tutorial?remediation.?4 Features The vector-based representation for training the dialogue act classifiers integrates several sources of features: lexical and syntactic features, and structural features that include dialogue act labels, task/subtask labels, and set of hidden dialogue state prediction features.   
4.1 Lexical and Syntactic Features Lexical and syntactic features were automatically extracted from the utterances using the Stanford Parser default tokenizer and part of speech (pos) tagger (De Marneffe et al, 2006). The parser created both phrase structure trees and typed dependencies for individual sentences. From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Typed dependencies between pairs of words were extracted from each sentence. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). The pos features were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers.   4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/subtask features (Boyer et al, 2009). These HMMs performed significantly better than bigram models for predicting human tutor moves (Boyer et al, 2010), which indicates that the hidden dialogue state leveraged by the HMMs has predictive value even in the presence of ?true? (manually annotated) dialogue act labels. Therefore, we hypothesized that an HMM could also improve the performance of models to classify student dialogue acts. To explore this hypothesis, we trained an HMM utilizing the methodology described in (Boyer et al, 2009) and used it to generate hidden dialogue state predictions in the form of a probability distribution over possible user utterances at each step in the dialogue. This set of stochastic features was subsequently passed to the classifier as part of the input vector (Figure 2).  4.3 Input Vectors The features were combined into a shared vector-based representation for training the classifier. As depicted in Table 3, the components of the 
300
feature vector include binary existence vectors for lexical and syntactic features for the current (target) utterance as well as for three utterances of left context (this left context may include both tutor and student utterances, which are distinguished by a separate indicator for the speaker). The task/subtask and correctness history features encode the separate stream of task events. There is no one-to-one correspondence between these history features and the left-hand dialogue context, because several task events could have occurred between a pair of dialogue events (or vice versa). This distinction is indicated in the table by the representation of dialogue time steps as [t, t-1,?] and task history steps as [task(t), task(t-1),?]. In total, the feature vectors included 11,432 attributes that were made available for feature selection.  
 Figure 2. Generation of hidden dialogue state prediction features 5 Experiments This section describes the learning of maximum likelihood vector-based models for classification of user dialogue acts. In addition to investigating the accuracy of the overall model, we also performed experiments regarding the utility of feature types for discriminating between particular dialogue acts of interest.    The classifiers are based on logistic regression, which learns a discriminant for each pair of dialogue acts by assigning weights in a maximum likelihood fashion. 2  The logistic regression models were learned using the Weka machine learning toolkit (Hall et al, 2009). For                                                 2 In general, the model that maximizes likelihood also maximizes entropy under the same constraints (Berger et al, 1996).  
feature selection, we performed attribute subset evaluation with a best-first approach that greedily searched the space of possible features using a hill climbing approach with backtracking. The prediction accuracy of the classifiers was determined through ten-fold cross-validation on the corpus, and the results below are presented in terms of prediction accuracy (number of correct classifications divided by total number of classifications) as well as by the kappa statistic, which adjusts for expected agreement by chance.    Table 3. Feature vectors 
Feature?vector?f? Description?[wt,1,?wt,|w|, pt,1,?,pt,|p|, dt,1,?,dt,|d|, st,1,?,st,|s|] 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?current?target?utterance?t??[wt-k,1,?wt-k,|w|, pt-k,1,?,pt-k,|p|, dt-k,1,?,dt-k,|d|, st-k,1,?,st-k,|s|]  where k=1,?,3 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?three?utterances?of?left?context?
[p(o1),?,p(o|S|)] Probability?distribution?for?emission?symbols?in?predicted?next?hidden?state?as?generated?by?HMM??[dat-1, dat-2, dat-3] Dialogue?act?left?context??[spt-1,spt-2, spt-3]? Speaker?label?left?context?[tktask(t-1), tktask(t-2), tktask(t-3)] Three?steps?of?subtask?history?(each?level?of?hierarchy?represented?as?a?separate?feature)??[ctask(t-1), ctask(t-2), ctask(t-3)]  
Three?steps?of?task?correctness?history?
pt Indicator?for?whether?the?target?utterance?was?immediately?preceded?by?a?task?event? 5.1 Overall Classification Task The overall dialogue act classification model was trained to classify each utterance with respect to the thirteen dialogue acts (Table 1). For this task, the feature selection algorithm selected 63 attributes including some syntax, dependency, pos, and word attributes as well as dialogue act, speaker, and task/subtask features. No hidden dialogue state features or task correctness attributes were selected. The overall classification accuracy was 62.8%. This accuracy constitutes a 369% improvement over baseline chance of 17% (the relative frequency of the most frequently occurring dialogue act, ACK). An alternate nontrivial baseline is a bigram model on true dialogue acts (including speaker tags); this model?s accuracy was 36.8%. The 
301
overall kappa for the full classifier was ?=.57. The confusion matrix for this model is depicted in Figure 3.        In addition to the classifier described above, we experimented with classifiers that used only the lexical and syntactic features of each utterance. This approach is of interest in part because it avoids the error propagation that can happen when a model relies on a series of its own previous classifications as features. The classifier that used only the set of lexical and syntactic features achieved a prediction accuracy of 60.2% and ?=.53 using 85 attributes.   
 5.2 Binary Dialogue Act Classification In tutoring, some student dialogue acts are particularly important to identify because of their implications for the tutor?s response or for the student model. For example, a student?s REQUEST FOR FEEDBACK requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base. UNCERTAIN FEEDBACK is another dialogue act of high importance because identifying it allows the tutor to respond in an affectively advantageous way (Forbes-Riley & Litman, 2009).  To explore which features are useful for classifying particular dialogue acts, we constructed binary dialogue act classifiers, one for each dialogue act, by preprocessing the dialogue act labels from the set of thirteen down to TRUE or FALSE depending on whether the label of the utterance matched the target dialogue act for that specialized classifier. Table 4 displays the features that were selected for each binary classifier, along with the percent accuracy and kappa for each model. Note that for some dialogue acts the chance baseline is very high, and therefore even a model with high prediction accuracy achieves a low kappa.         As depicted in Table 4, for several dialogue act models, the feature selection algorithm retained subtask and HMM features.   
Table 4. Binary DA classifiers  
DA? #?Features?Selected? %?Correct? Model???
ACK? 51? Lexical/syntax,?HMM,?DA?history?(preceding=S),?speaker?history?(preceding=Tutor)?? .933? .75?RF? 42? Lexical/syntax,?DA?history,?preceded?by?subtask? .905? .72?
EX? 57? Dependency,?pos,?word,?HMM,?DA?history?(preceding=EX),?subtask? .939? .45?
GR? 11? Syntax,?pos,?word,?DA?(previous=EMPTY),?speaker,?subtask?? .998? .97?UE? 21? Dependency,?pos,?word,?subtask? .991? .33?U? 63? Syntax,?dependency,?pos,?word,?HMM,?subtask? .979? .21?
NE? 44? Dependency,?pos,?word,?HMM,?DA?history?(2?ago=UNCERTAIN),?subtask? .987? 0?N? 83? Lexical/syntax,?DA?history,?subtask? .966? .76?PE? 90? Dependency,?pos,?word,?HMM,?subtask? .976? .10?
P? 110? Dependency,?pos,?word,?HMM,?DA?history?(previous=REQUEST?FEEDBACK)? .945? .58?Q? 43? Syntax,?dep,?pos,?word,?HMM,?subtask? .940? .60?S? 92? Syntax,?pos,?word,?HMM,?DA?history?(previous=EMPTY?or?Q)? .901? .57?
T? 29? Syntax,?pos,?word,?DA?history?(previous=POSITIVE)?(3?ago=POSITIVE)? .992? .92?    In an experiment to quantify the utility of these features, it was found that for many dialogue acts, a binary dialogue act classifier that was trained using only lexical and syntactic features achieved the same or better classification accuracy than the model that was given all features (Figure 4). For comparison, the modified baseline model used the last three true dialogue acts (with speaker tags); this model achieved better than chance for four dialogue acts and achieved nearly as well as the full model for GREETING (GR). The models that were given all possible features for selection outperformed the lexical/syntax-only model for seven of the thirteen dialogue acts (GREETING (GR), REQUEST FOR FEEDBACK (RF), POSITIVE FEEDBACK (P), POSITIVE ELABORATED FEEDBACK (PE), UNCERTAIN ELABORATED FEEDBACK (UE), NEGATIVE FEEDBACK (N), and EXTRA-DOMAIN (EX)); however, it should be noted that none of these differences in performance is statistically reliable at the p=0.05 level.   
Figure 3. Confusion matrix 
302
 Figure 4. Kappa for binary DA classifiers by features available for selection 6 Discussion We have presented a maximum likelihood classifier that assigns dialogue act labels to user utterances from a corpus of human-human tutorial dialogue given a set of lexical, syntactic, and structural features. Overall, this classifier achieved 62.8% accuracy in ten-fold cross-validation on the corpus. This performance is on par with other automatic dialogue act tagging models, both sequential and vector-based, in task-oriented domains that do not feature complex, user-driven parallel tasks. In a catalogue ordering domain with an integrated task and dialogue model, Bangalore et al (2009) report 75% classification accuracy for user utterances using a maximum entropy classifier, a 275% improvement over baseline. Poesio & Mikheev (1998) report 54% classification accuracy by utilizing conversational game structure and speaker changes in the Maptask corpus, an improvement of 170% over baseline. Recent work on Maptask reports a classification accuracy of 65.7% using local utterance (such as lexical/syntactic) features alone, with prosodic cues yielding further slight improvement (Sridhar et al, 2009). This classifier is analogous to our lexical/syntactic feature model, which achieved 60.2% accuracy. The results of these models demonstrate that, consistent with the findings in other task-oriented domains, lexical/syntactic features are highly useful for classifying student dialogue moves in this complex task-oriented domain. Models trained using those lexical/syntactic features 
performed almost universally better (with the exception of the binary classifier for GREETING) than models that were given the same left context of true dialogue act tags.  It was hypothesized that leveraging both the hidden dialogue state and hierarchical subtask features would improve the performance of the classifiers. There is some evidence that the subtask structure was helpful for the overall classifier; however, no HMM features were kept during feature selection for the overall model. Of the binary models, approximately half performed better than the overall model in terms of true positive rate; of those, three did so by including HMM or task/subtask features among the selected attributes to differentiate different tones of student feedback. However, this difference in performance was not statistically reliable. This finding suggests that, given lexical and syntactic features which are strong predictors of dialogue acts, the hidden dialogue state as captured by an an HMM may not contribute significantly to the dialogue act classification task. 7 Conclusion and Future Work Dialogue modeling for complex task-oriented domains poses significant challenges. An effective dialogue model allows systems to detect user dialogue acts so that they can respond in a manner that maximizes the chance of success. Experiments with the data-driven classifiers presented in this paper demonstrate that lexical/syntactic features can effectively classify student dialogue acts in the task-oriented tutoring domain. For POSITIVE, NEGATIVE, and UNCERTAIN ELABORATED student feedback acts, which play a key role in tutorial dialogue system, the addition of hidden dialogue state features (as learned by an HMM) and task/subtask features (annotated manually) improve classification accuracy, but not statistically reliably.    The overarching goal of this work is to create a data-driven tutorial dialogue system that learns its behavior from corpora of effective human tutoring. The dialogue act classification models reported here constitute an important step toward that goal, by integrating the dialogue stream with a parallel user-driven task event stream. The next generation of data-driven systems should leverage models that capture the rich interplay between dialogue and task. Future work will focus on data-driven approaches to task recognition and tutorial planning. Additionally, as dialogue system research addresses 
303
increasingly complex task-oriented domains, it becomes increasingly important to investigate unsupervised approaches for dialogue act classification and task recognition.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of the IUI, 1-8.  Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596.  Austin, J. L. (1962). How to do things with words. Oxford: Oxford University Press. Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  Berger, A. L., Pietra, V. J. D., & Pietra, S. A. D. (1996). A maximum entropy approach to natural language processing. Comp. Ling., 22(1), 71.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden markov models. Proceedings of NAACL-HLT, Short Papers, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging hidden dialogue state to select tutorial moves. Proceedings of the 5th NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Los Angeles, California.  Chotimongkol, A. (2008). Learning the structure of task-oriented conversations from the corpus of in-domain dialogs. (Unpublished Ph.D. Dissertation). Carnegie Mellon University School of Computer Science. Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.  De Marneffe, M. C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses 
from phrase structure parses. Proceedings of LREC, Genoa, Italy.   Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The WEKA data mining software: An update. SIGKDD Explorations, 11(1) Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Comm., 48(3-4), 354-373.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Loper, E., & Bird, S. (2004). NLTK: The natural language toolkit. Proceedings of the ACL Demonstration Session, Barcelona, Spain. 214-217.  Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137.  Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic modelling for multi-party spoken discourse. Proceedings of the ACL, Sydney, Australia. , 44(1) 17.  Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Sridhar, V. K. R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4), 407-422.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Comp. Ling., 26(3), 339-373.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.   
304
    
Time Stamp Dialogue Stream Task Stream  2008-04-11 18:23:45 Student:  so do i have to manipulate the array this time? [Q]   2008-04-11 18:23:53 Tutor:  this time, we need to do two things [S]    2008-04-11 18:24:02 Tutor:  first, we need to create a new array to hold the changed values [S]    2008-04-11 18:24:28     i 2008-04-11 18:24:28     n 2008-04-11 18:24:28     t 2008-04-11 18:24:28     \sp 1-a-i BUGGY 2008-04-11 18:24:35     \del  2008-04-11 18:24:36     \sp  2008-04-11 18:24:36     d 2008-04-11 18:24:36     o 2008-04-11 18:24:36     u 2008-04-11 18:24:36     b 2008-04-11 18:24:37     l 2008-04-11 18:24:37     e 2008-04-11 18:24:37     \sp 2008-04-11 18:24:39     [] 
1-a-i CORRECT 
2008-04-11 18:24:40     \sp  2008-04-11 18:24:42     n 2008-04-11 18:24:42     e 2008-04-11 18:24:42     w 2008-04-11 18:24:43     \sp 2008-04-11 18:24:44     \del 2008-04-11 18:24:45     T 2008-04-11 18:24:46     \del 2008-04-11 18:24:54     T 2008-04-11 18:24:54     i 2008-04-11 18:24:54     m 2008-04-11 18:24:54     e 2008-04-11 18:24:54     s 2008-04-11 18:24:55     3 2008-04-11 18:24:57     ; 
1-a-ii CORRECT 
2008-04-11 18:25:11 Student:  good? [RF]    2008-04-11 18:25:14 Tutor:  good so far, yes [PF]    2008-04-11 18:25:29 Student:  so now i have to change parts of the times array right? [Q]    2008-04-11 18:25:34 Tutor:  not quite [LF]    2008-04-11 18:25:57 Tutor:  So, when you create a new object, like a String for example, you'd say something like  String s = new String() [S]    2008-04-11 18:25:59 Tutor:  right? [AQ]    2008-04-11 18:26:06 Student:  right [P]    2008-04-11 18:26:14 Tutor:  arrays are similar [S]         
    
Appendix 
Excerpt 1. Parallel synchronous dialogue and task event streams with annotations. (Note tutor dialogue acts: AQ=ASSESSING QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK) 
305
