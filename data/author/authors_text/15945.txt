Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345?356,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Adaptor Grammars for Learning Non-Concatenative Morphology
Jan A. Botha and Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, UK
{jan.botha,phil.blunsom}@cs.ox.ac.uk
Abstract
This paper contributes an approach for
expressing non-concatenative morphological
phenomena, such as stem derivation in
Semitic languages, in terms of a mildly
context-sensitive grammar formalism. This
offers a convenient level of modelling ab-
straction while remaining computationally
tractable. The nonparametric Bayesian frame-
work of adaptor grammars is extended to this
richer grammar formalism to propose a prob-
abilistic model that can learn word segmenta-
tion and morpheme lexicons, including ones
with discontiguous strings as elements, from
unannotated data. Our experiments on He-
brew and three variants of Arabic data find
that the additional expressiveness to capture
roots and templates as atomic units improves
the quality of concatenative segmentation and
stem identification. We obtain 74% accuracy
in identifying triliteral Hebrew roots, while
performing morphological segmentation with
an F1-score of 78.1.
1 Introduction
Unsupervised learning of morphology is the task
of acquiring, from unannotated data, the intra-word
building blocks of a language and the rules by which
they combine to form words. This task is of interest
both as a gateway for studying language acquisition
in humans and as a way of producing morphological
analyses that are of practical use in a variety of nat-
ural language processing tasks, including machine
translation, parsing and information retrieval.
A particularly interesting version of the morphol-
ogy learning problem comes from languages that
use templatic morphology, such as Arabic, He-
brew and Amharic. These Semitic languages de-
rive verb and noun stems by interspersing abstract
root morphemes into templatic structures in a non-
concatenative way. For example, the Arabic root
k?t?b can combine with the template (i-a) to derive
the noun stem kitab (book). Established morpho-
logical analysers typically ignore this process and
simply view the derived stems as elementary units
(Buckwalter, 2002), or their account of it coincides
with a requirement for extensive linguistic knowl-
edge and hand-crafting of rules (Finkel and Stump,
2002; Schneider, 2010; Altantawy et al, 2010). The
former approach is bound to suffer from vocabu-
lary coverage issues, while the latter clearly does
not transfer easily across languages. The practical
appeal of unsupervised learning of templatic mor-
phology is that it can overcome these shortcomings.
Unsupervised learning of concatenative morphol-
ogy has received extensive attention, partly driven
by the MorphoChallenge (Kurimo et al, 2010) in re-
cent years, but that is not the case for root-templatic
morphology (Hammarstro?m and Borin, 2011).
In this paper we present a model-based method
that learns concatenative and root-templatic mor-
phology in a unified framework. We build on two
disparate strands of work from the literature: Firstly,
we apply simple Range Concatenating Grammars
(SRCGs) (Boullier, 2000) to parse contiguous and
discontiguous morphemes from an input string.
These grammars are mildly-context sensitive (Joshi,
1985), a superset of context-free grammars that
retains polynomial parsing time-complexity. Sec-
ondly, we generalise the nonparametric Bayesian
learning framework of adaptor grammars (Johnson
et al, 2007) to SRCGs.1 This should also be rel-
1Our formulation is in terms of SRCGs, which are equiv-
alent in power to linear context-free rewrite systems (Vijay-
Shanker et al, 1987) and multiple context-free grammars (Seki
et al, 1991), all of which are weaker than (non-simple) range
concatenating grammars (Boullier, 2000).
345
evant to other applications of probabilistic SRCGs,
e.g. in parsing (Maier, 2010), translation (Kaesham-
mer, 2013) and genetics (Kato et al, 2006).
In addition to unannotated data, our method re-
quires as input a minimal set of high-level grammar
rules that encode basic intuitions of the morphology.
This is where there would be room to become very
language specific. Our aim, however, is not to obtain
a best-published result in a particular language, but
rather to create a method that is applicable across
a variety of morphological processes. The specific
rules used in our empirical evaluation on Arabic and
Hebrew therefore contain hardly any explicit lin-
guistic knowledge about the languages and are ap-
plicable across the family of Semitic languages.
2 A powerful grammar for morphology
Concatenative morphology lends itself well to an
analysis in terms of finite-state transducers (FSTs)
(Koskenniemi, 1984). With some additional effort,
FSTs can also encode non-concatenative morphol-
ogy (Kiraz, 2000; Beesley and Karttunen, 2003;
Cohen-Sygal and Wintner, 2006; Gasser, 2009). De-
spite this seeming adequacy of regular languages to
describe morphology, we see two main shortcom-
ings that motivate moving further up the Chom-
sky hierarchy of formal languages: first is the is-
sue of learning. We are not aware of successful at-
tempts at inducing FST-based morphological analy-
sers in an unsupervised way, and believe the chal-
lenge lies in the fact that FSTs do not offer a conve-
nient way of expressing prior linguistic intuitions to
guide the learning process. Secondly, an FST com-
posed of multiple machines might capture morpho-
logical processes well and excel at analysis, but in-
terpretability of its internal operations are limited.
These shortcomings are overcome for concate-
native morphology by context-free adaptor gram-
mars, which allowed diverse segmentation models
to be formulated and investigated within a single
framework (Johnson et al, 2007; Johnson, 2008;
Sirts and Goldwater, 2013). In principle, that cov-
ers a wide range of phenomena (typical example
language in parentheses): affixal inflection (Czech)
and derivation (English), agglutinative derivation
(Turkish, Finnish), compounding (German). Our
agenda here is to extend that approach to include
non-concatenative processes such as root-templatic
derivation (Arabic), infixation (Tagalog) and cir-
cumfixation (Indonesian). In this pursuit, an ab-
straction that permits discontiguous constituents is
a highly useful modelling tool, but requires looking
beyond context-free grammars.
An idealised generative grammar that would cap-
ture all the aforementioned phenomena could look
like this:
Word? (Pre? Stem Suf?)+ (1)
e.g. English un+accept+able
Stem |Pre |Suf? Morph (2)
Stem? intercal (Root,Template) (3)
e.g. Arabic derivation k?t?b + i?a? kitab (book)
Stem? infix (Stem, Infix) (4)
e.g. Tagalog sulat (write)? sumulat (wrote)
Stem? circfix (Stem,Circumfix) (5)
e.g. Indonesian percaya (to trust)
? kepercayaan (belief)
where the symbols (excluding Word and Stem) im-
plicitly expand to the relevant terminal strings. The
bold-faced ?functions? combine the potentially dis-
contiguous yields of the argument symbols into sin-
gle contiguous strings, e.g. infix(s?ulat, um) pro-
duces stem sumulat.
Taken by themselves, the first two rules are sim-
ply a CFG that describes word formation as the
concatenation of stems and affixes, a formulation
that matches the underlying grammar of Morfessor
(Creutz and Lagus, 2007), a well-studied unsuper-
vised model.
The key aim of our extension is that we want the
grammar to capture a discontiguous string like k?t?b
as a single constituent in a parse tree. This leads to
well-understood problems in probabilistic grammars
(e.g. what is this rule?s probability?), but also corre-
sponds to the linguistic consideration that k?t?b is a
proper morpheme of the language (Prunet, 2006).
3 Simple range concatenating grammars
In this section we define SRCGs formally and
illustrate how they can be used to model non-
concatenative morphology. SRCGs define lan-
guages that are recognisable in polynomial time, yet
can capture discontiguous elements of a string un-
der a single category (Boullier, 2000). An SRCG-
346
rule operates on vectors of ranges in contrast to the
way a CFG-rule operates on single ranges (spans).
In other words, a non-terminal symbol in an SRCG
(CFG) derivation can dominate a subset (substring)
of terminals in an input string.
3.1 Formalism
An SRCG G is a tuple (N,T, V, P, S), with
finite sets of non-terminals (N ), termi-
nals (T ) and variables (V ), with a start sym-
bol S ? N . A rewrite rule p ? P of rank
r = ?(p) ? 0 has the form A(?1, . . . , ??(A)) ?
B1(?1,1, . . . , ?1,?(B1)) . . . Br(?r,1, . . . , ?r,?(Br)),
where each ?, ? ? (T ? V )?, and ?(A) is the
number of arguments a non-terminal A has, called
its arity. By definition, the start symbol has arity 1.
Any variable v ? V appearing in a given rule
must be used exactly once on each side of the
rule. Terminating rules are written with  as the
right-hand side and thus have rank 0.
A range is a pair of integers (i, j) denoting the
substring wi+1 . . . wj of a string w = w1 . . . wn.
A non-terminal becomes instantiated when its vari-
ables are bound to ranges through substitution. Vari-
ables within an argument imply concatenation and
therefore have to bind to adjacent ranges.
An instantiated non-terminal A? is said to de-
rive  if the consecutive application of a sequence
of instantiated rules rewrite it as . A string w is
within the language defined by a particular SRCG
iff the start symbol S, instantiated with the exhaus-
tive range (0, wn), derives .
An important distinction with regard to CFGs is
that, due to the instantiation mechanism, the order-
ing of non-terminals on the right-hand side of an
SRCG rule is irrelevant, i.e. A(ab) ? B(a)C(b)
and A(ab) ? C(b)B(a) are the same rule.2 Con-
sequently, the isomorphisms of any given SRCG
derivation tree all encode the same string, which is
uniquely defined through the instantiation process.
3.2 Application to morphological analysis
A fragment of the idealised grammar schema from
the previous section (?2) can be rephrased as an
SRCG by writing the rules in the newly introduced
2Certain ordering restrictions over the variables within an
argument need to hold for an SRCG to indeed be a simple RCG
(Boullier, 2000).
Word(wakitabi)
Suf(i)
i
Stm(kitab)
Template(i,a)Root(k,t,b)
Pre(wa)
aw k i t a b
Figure 1: Example derivation for wakitabi (and my
book) using the SRCG fragment from ?3.2. CFGs
cannot capture such crossing branches.
notation, and supplying a definition of the intercal
function as simply another rule of the grammar, with
instantiation for w = kitab shown below:
Word(abc)? Pre(a) Stem(b) Suf(c)
Stem(abcde)? Root(a, c, e) Template(b, d),
Stem(?0..1?, ?1..2?, ?2..3?, ?3..4?, ?4..5?)
? Root(?0..1?, ?2..3?, ?4..5?)
Template(?1..2?, ?3..4?)
Given an appropriate set of grammar rules (as we
present in ?5), we can parse an input string to ob-
tain a tree as shown in Figure 1. The overlapping
branches of the tree demonstrate that this grammar
captures something a CFG could not. From the parse
tree one can read off the word?s root morpheme and
the template used.
Although SRCGs specify mildly context-sensitive
grammars, each step in a derivation is context-free ?
a node?s expansion does not depend on other parts
of the tree. This property implies that a recogni-
tion/parsing algorithm can have a worst-case time
complexity that is polynomial in the input length n,
O(n(?+1)?) for arity ? and rank ?, which reduces
to O(n3?) for a binarised grammar. To capture the
maximal case of a root with k ? 1 characters and
k discontiguous templatic characters forming a stem
would require a grammar that has arity ? = k. For
Arabic, which has up to quadriliteral roots (k = 5),
the time complexity would be O(n15).3 This is a
daunting proposition for parsing, but we are careful
3The trade-off between arity and rank with respect to pars-
ing complexity has been characterised (Gildea, 2010), and the
appropriate refactoring may bring down the complexity for our
grammars too.
347
to set up our application of SRCGs in such a way
that this is not too big an obstacle:
Firstly, our grammars are defined over the char-
acters that make up a word, and not over words that
make up a sentence. As such, the input length n
would tend to be shorter than when parsing full sen-
tences from a corpus.
Secondly, we do type-based morphological analy-
sis, a view supported by evidence from Goldwater et
al. (2006), so each unique word in a dataset is only
ever parsed once with a given grammar. The set of
word types attested in the data sources of interest
here is fairly limited, typically in the tens of thou-
sands. For these reasons, our parsing and inference
tasks turn out to be tractable despite the high time
complexity.
4 Learning
4.1 Probabilistic SRCG
The probabilistic extension of SRCGs is similar to
the probabilistic extension of CFGs, and has been
used in other guises (Kato et al, 2006; Maier, 2010).
Each rule r ? P has an associated probability ?r
such that
?
r?PA
?r = 1. A random string in
the language of the grammar can then be obtained
through a generative procedure that begins with the
start symbol S and iteratively expands it until deriv-
ing : At each step for some current symbol A, a
rewrite rule r is sampled randomly from PA in ac-
cordance with the distribution over rules and used
to expand A. This procedure terminates when no
further expansions are possible. Of course, expan-
sions need to respect the range concatenating and or-
dering constraints imposed by the variables in rules.
The expansions imply a chain of variable bindings
going down the tree, and instantiation happens only
when rewriting into s but then propagates back up
the tree.
The probability P (w, t) of the resulting tree t and
terminal string w is the product
?
r ?r over the se-
quence of rewrite rules used. This generative proce-
dure is a conceptual device; in practice, one would
care about parsing some input string under this prob-
abilistic grammar.
4.2 PYSRCAG
A central property of the generative procedure un-
derlying probabilistic SRCGs is the fact that each
expansion happens independently, both of the other
expansions in the tree under construction and of any
other trees. To some extent, this flies in the face of
the reality of estimating a grammar from text, where
one would expect certain sub-trees to be used repeat-
edly across different input strings.
Adaptor grammars weaken this independence as-
sumption by allowing whole subtrees to be reused
during expansion. Informally, they act as a cache of
tree fragments whose tendency to be reused during
expansion is governed by the choice of adaptor func-
tion. Following earlier applications of adaptor gram-
mars (Johnson et al, 2007; Huang et al, 2011), we
employ the Pitman-Yor process (Pitman, 1995; Pit-
man and Yor, 1997) as adaptor function.
A Pitman-Yor Simple Range Concatenat-
ing Adaptor Grammar (PYSRCAG) is a tuple
G = (GS ,M,a, b,?), where GS is a probabilistic
SRCG as defined before and M ? N is a set
of adapted non-terminals. The vectors a and b,
indexed by the elements of M , are the discount
and concentration parameters for each adapted non-
terminal, with a ? [0, 1], b ? 0. ? are parameters to
Dirichlet priors on the rule probabilities ?.
PYSRCAG defines a generative process over a set
of trees T . Unadapted non-terminals A? ? N \M
are expanded as before (?4.1). For each adapted
non-terminal A ? M , a cache CA is maintained
for storing the terminating tree fragments expanded
from A earlier in the process, and we denote the
fragment corresponding to the i-th expansion of A
as zi. In other words, the sequence of indices zi
is the assignment of a sequence of expansions of
A to particular tree fragments. Given a cache CA
that has n previously generated trees comprising
m unique trees each used n1, . . . , nm times (where
n =
?
k nk), the tree fragment for the next expan-
sion of A, zn+1, is sampled conditional on the pre-
vious assignments z< according to
zn+1|z< ?
{
nk?a
n+b if zn+1 = k ? [1,m]
ma+b
n+b if zn+1 = m+ 1,
where a and b are those elements of a and b cor-
responding to A. The first case denotes the situa-
tion where a previously cached tree is reused for this
n + 1-th expansion of A; to be clear, this expands
A with a fully terminating tree fragment, meaning
that none of the nodes descending from A in the
348
tree being generated are subject to further expan-
sion. The second case by-passes the cache and ex-
pandsA according to the rules PA and rule probabil-
ities ?A of the underlying SRCG GS . Other caches
CB(B ? M) may come into play during those
expansions of the descendants of A; thus a PYS-
RCAG can define a hierarchical stochastic process.
Both cases eventually result in a terminating tree-
fragment for A, which is then added to the cache,
updating the counts n, nzn+1 and potentially m.
The adaptation does not affect the string language
of GS , but it maps the distribution over trees to one
that is distributed according to the PYP.
The invariance of SRCGs trees under isomor-
phism would make the probabilistic model deficient,
but we side-step this issue by requiring that grammar
rules are specified in a canonical way that ensures
a one-to-one correspondence between the order of
nodes in a tree and of terminals in the yield.
4.3 Inference under PYSRCAG
The inference procedure under our model is very
similar to that of CFG PY-adaptor grammars, so we
restate the central aspects here but refer the reader
to the original article by Johnson et al (2007) for
further details. First, one may integrate out the
adaptors to obtain a single distribution over the set
of trees generated from a particular non-terminal.
Thus, the joint probability of a particular sequence z
for the adapted non-terminal A with cached counts
(n1, . . . , nm) is
PY (z|a, b) =
?m
k=1 (a(k ? 1) + b)
?nk?1
j=1 (j ? a)
?n?1
i=0 (i+ b)
.
(6)
Taking all the adapted non-terminals into account,
the joint probability of a set of full trees T under the
grammar G is
P (T |a, b,?) =
?
A?M
B(?A + fA)
B(?A)
PY (z(T )|a, b),
(7)
where fA is a vector of the usage counts of rules
r ? PA across T , and B is the Euler beta function.
The posterior distribution over a set of strings
w is obtained by marginalising (7) over all trees
that have w as their yields. This is intractable to
compute directly, so instead we use MCMC tech-
niques to obtain samples from that posterior using a
component-wise Metropolis-Hastings sampler. The
sampler works by visiting each string w in turn and
drawing a new tree for it under a proposal grammar
GQ and randomly accepting that as the new analysis
for w according to the Metropolis-Hastings accept-
reject probability. As proposal grammar, we use the
analogous approximation of our G as Johnson et al
used for PCFGs, namely by taking a static snapshot
GQ of the adaptor grammar where additional rules
rewrite adapted non-terminals as the terminal strings
of their cached trees. Drawing a sample from the
proposal distribution is then a matter of drawing a
random tree from the parse chart of w under GQ.
Lastly, the adaptor hyperparameters a and b
are modelled by placing flat Beta(1, 1) and vague
Gamma(10, 0.1) priors on them, respectively, and
inferring their values using slice sampling (Johnson
and Goldwater, 2009).
5 Modelling root-templatic morphology
We start with a CFG-based adaptor grammar4 that
models words as a stem and any number of prefixes
and suffixes:
Word? Pre? Stem Suf? (8)
Pre | Stem | Suf? Char+ (9)
This fragment can be seen as building on the stem-
and-affix adaptor grammar presented in (Johnson et
al., 2007) for morphological analysis of English, of
which a later version also covers multiple affixes
(Sirts and Goldwater, 2013). In the particular case of
Arabic, multiple affixes are required to handle the at-
tachment of particles and proclitics onto base words.
To extend this to complex stems consisting of a
root with three radicals we have rules like the fol-
lowing:
Stem(abcdefg)? R3(b, d, e) T4(a, c, e, g) (10)
Stem(abcdef)? R3(a, c, e) T3(b, d, f) (11)
Stem(abcde)? R3(a, c, e) T2(b, d) (12)
Stem(abcd)? R3(a, c, d) T1(b) (13)
Stem(abc)? R3(a, b, c) (14)
4Adapted non-terminals are indicated by underlining and
we use the following abbreviations: X ? Y+ means one
or more instances of Y and encodes the rules X ? Ys and
Ys ? Ys Y | Y. Similarly, X ? Y? Z allows zero or more
instances of Y and encodes the rules X ? Z and X ? Y+ Z.
Further relabelling is added as necessary to avoid cycles among
adapted non-terminals.
349
The actual rules include certain permutations of
these, e.g. rule (13) has a variant R3(a, b, d)T1(c).
In unvocalised text, the standard written form of
Modern Standard Arabic (MSA), it may happen that
the stem and the root of a word form are one and the
same. So while rule (14) may look trivial, it ensures
that in such cases the radicals are still captured as de-
scendants of the non-terminal category R3, thereby
making their appearance in the cache.
A discontiguous non-terminal An is rewritten
through recursion on its arity down to 1, i.e.
An(v1, . . . , vn)? Al(v1, . . . , vn?1) Char(vn) with
base case A1(v) ? Char(v), where Char rewrites
all individual terminals as , vi are variables and
l = n?1.5 Note that although we provide the model
with two sets of discontiguous non-terminals R and
T, we do not specify their mapping onto the actual
terminal strings; no subdivision of the alphabet into
vowels and consonants is hard-wired.
6 Experiments
We evaluate our model on standard Arabic, Quranic
Arabic and Hebrew in terms of segmentation quality
and lexicon induction ability. These languages share
various properties, including morphology and lexi-
cal cognates, but are sufficiently different so as to
require manual intervention when transferring rule-
based morphological analysers across languages. A
key question in this evaluation is therefore whether
an appropriate instantiation of our model success-
fully generalises across related languages.
6.1 Data sets
Our models are unsupervised and therefore learn
from raw text, but their evaluation requires anno-
tated data as a gold-standard and these were derived6
as follows:
Arabic (MSA) We created the dataset BW by syn-
thesising 50k morphotactically correct word types
from the morpheme lexicons and consistency rules
supplied with the Buckwalter Arabic Morphological
5Including the arity as part of the non-terminal symbol
names forms part of our convention here to ensure that the
grammar contains no cycles, a situation which would compli-
cate inference under PYSRCAG.
6Our data preprocessing scripts are obtainable from
http://github.com/bothameister/pysrcag-data.
Types Stems Roots m/w c/w
BW 48428 24197 4717 2.3 6.4
BW
?
48428 30891 4707 2.3 10.7
QU
?
18808 12021 1270 1.9 9.9
HEB 5231 3164 492 2.1 6.7
Table 1: Corpus statistics, including average number
of morphemes (m/w) and characters (c/w) per word,
and total surface-realised roots of length 3 or 4.
Analyser (BAMA).7 This allowed control over the
word shapes, which is important to focus the evalu-
ation, while yielding reliable segmentation and root
annotations. BW has no vocalisation; we denote the
corresponding vocalised dataset as BW
?
.
Quranic Arabic We extracted the roughly 18k
word types from a morphologically analysed version
of the Quran (Dukes and Habash, 2010). As an ad-
ditional challenge, we left all given diacritics intact
for this dataset, QU
?
.
Hebrew We leveraged the Hebrew CHILDES
database as an annotated resource (Albert et al,
2013) and were able to extract 5k word types that
feature at least one affix to use as dataset HEB. The
corrected versions of words marked as non-standard
child language were used, diacritics were dropped,
and we conflated stressed and unstressed vowels to
overcome inconsistencies in the source data.
6.2 Models
We consider two classes of models. The first
is the strictly context-free adaptor grammar for
morphemes as sequences of characters using
rules (8)-(9), which we denote as Concat and
MConcat, where the latter allows multiple pre-
fixes/suffixes in a word. These serve as baselines for
the second class in which non-concatenative rules
are added. MTpl and Tpl denote the canonical ver-
7We used version 2.0, LDC2004L02, and sampled word
types having a single stem and at most one prefix, suffix or both,
according to the following random procedure: Sample a shape
(stem: 0.1, pre+stem: 0.25 stem+suf: 0.25, pre+stem+suf: 0.4).
Sample uniformly at random (with replacement) a stem from
the BAMA stem lexicon, and affix(es) from the ones consis-
tent with the chosen stem. The BAMA lexicons contain affixes
and their legitimate concatenations, so some of the generated
words would permit a linguistic segmentation into multiple pre-
fixes/suffixes. Nonetheless, we take as gold-standard segmenta-
tion precisely the items used by our procedure.
350
sions with stems as shown in the set of rules above,
and we experiment with a variant Tpl3Ch that al-
lows the non-terminal T1 to be rewritten as up to
three Char symbols, since the data indicate there are
cases where multiple characters intervene between
the radicals of a root.
These models exclude rule (10), which we include
only in the variant Tpl+T4. Lastly, TplR4 is the ex-
tension of Tpl+T4 to include a stem-forming rule
that uses R4.
As external baseline model we used Morfessor
(Creutz and Lagus, 2007), which performs decently
in morphological segmentation of a variety of lan-
guages, but only handles concatenation.
6.3 Method
The MCMC samplers converged within a few hun-
dred iterations and we collected 100 posterior sam-
ples after 900 iterations of burn-in. Collected sam-
ples, each of which is a set of parse trees of the input
word types, are used in two ways:
First, by averaging over the samples we can es-
timate the joint probability of a word type w and a
parse tree t under the adaptor grammar, conditional
on the data and the model?s hyperparameters. We
take the most probable parse of each word type and
evaluate the implied segmentation against the gold
standard segmentation. Likewise, we evaluate the
implied lexicon of stems, affixes and roots against
the corresponding reference sets. It should be em-
phasised that using this maximally probable analy-
sis is aimed at simplifying the evaluation set-up; one
could also extract multiple analyses of a word since
the model defines a distribution over them.
The second method abstracts away from individ-
ual word-types and instead averages over the union
of all samples to obtain an estimate of the probabil-
ity of a string s being generated by a certain category
(non-terminal) of the grammar. In this way we can
obtain a lexicon of the morphemes in each category,
ranked by their probability under the model.
6.4 Inducing Morpheme Lexicons
The quality of each induced lexicon is measured
with standard set-based precision and recall with re-
spect to the corresponding gold lexicon. The results
are summarised by balanced F-scores in Table 2.
The main result is that all our models capable of
forming complex stems obtain a marked improve-
ment in F-scores over the baseline concatenative
adaptor grammar, and the margin of improvement
grows along with the expressivity of the complex-
stem models tested. This applies across prefix, stem
and suffix categories and across our datasets, with
the exception of QU
?
, which we elaborate on in ?6.5.
Stem lexicons of Arabic were learnt with rel-
atively constant precision (?70%), but modelling
complex stems broadened the coverage by about
3000 stems over the concatenative model (against a
reference set of 24k stems). On vocalised Arabic,
the improvements for stems are along both dimen-
sions. In contrast, affix lexicons for both BW and
BW
?
are noisy and the models all generate greedily
to obtain near perfect recall but low precision.
On our Hebrew data, which comprises only 5k
words, the gains in lexicon quality from modelling
complex stems tend to be larger than on Arabic. This
is consistent with our intuition that an appropriate,
richer Bayesian prior helps overcome data sparsity.
Extracting a lexicon of roots is rendered challeng-
ing by the unsupervised nature of the model as the
labelling of grammar symbols is ultimately arbitrary.
Our simple approach was to regard a character tuple
parsed under category R3 as a root. This had mixed
success, as demonstrated by the outlier scores in Ta-
ble 2. In the one case where it was obvious that T3
had been been co-opted for the role, we report the
F-score obtained on the union of R3 and T3 strings.
Soft decisions The preceding set-based evaluation
imposes hard decisions about category membership.
But adaptor grammars are probabilistic by definition
and should thus also be evaluated in terms of prob-
abilistic ability. One method is to turn the model
predictions into a binary classifier of strings us-
ing Receiver-Operator-Characteristic (ROC) theory.
We plot the true positive rate versus the false pos-
itive rate for each prediction lexicon L? containing
strings that have probability greater than ? under the
model (for a grammar category of interest). A per-
fect classifier would rank all true positives (e.g. stem
strings) above false positives (e.g. non-stem strings),
corresponding to a curve in the upper left corner of
the ROC plot. A random guesser would trace a di-
agonal line. The area under the curves (AUC) is
the probability that the classifier would discriminate
correctly.
351
Vocalised Arabic (BW
?
) Unvocalised Arabic (BW) Hebrew (HEB)
Pre Stem Suf R3 Pre Stem Suf R3 Pre Stem Suf R3
Concat 15.0 20.2 25.4 - 32.8 44.1 40.3 - 18.7 20.9 29.2 -
Tpl 24.7 39.4 35.2 ?42.4 45.9 54.7 47.9 62.7 35.1 59.6 52.9 34.8
Tpl3Ch 28.4 36.0 36.5 5.2 50.3 55.1 48.5 62.4 38.6 61.5 56.6 7.1
Tpl+T4 29.0 44.8 41.0 3.9 46.2 54.2 47.7 62.3 32.5 59.6 53.0 36.4
TplR4 37.8 60.3 47.0 5.2 53.0 57.7 51.9 62.4 38.0 62.4 55.2 34.7
Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse
of each different dataset under each models. ?42.4 was obtained by taking the union of R3 and T3 items to
match the way the model used them (see ?6.4).
BW
?
BW QU
?
HEB
Morfessor 55.57 40.04 44.34 24.20
Concat 47.36 64.22 19.64 60.05
Tpl 60.42 71.91 22.53 77.26
Tpl3Ch 60.52 72.20 25.72 77.41
Tpl+T4 64.49 71.59 24.81 77.14
TplR4 74.54 73.66 - 78.14
Table 3: Segmentation quality in SBF1. The QU
?
results are for the corresponding M* models .
Our models with complex stem formation im-
prove over the baseline on the AUC metric too. We
include the ROC plots for Hebrew stem and root in-
duction in Figure 2, along with the roots the model
was most confident about (Table 4).
6.5 Morphological Analysis per Word Type
In this section we turn to the analyses our models
assign to each word type. Two aspects of interest are
the segmentation into sequential morphemes and the
identification of the root.
Our intercalating adaptor grammars consistently
obtain large gains in segmentation accuracy over the
baseline concatenative model, across all our datasets
(Table 3). We measure segmentation quality as seg-
ment border F1-score (SBF) (Sirts and Goldwater,
2013), which is the F-score over word-internal seg-
mentation points of the predicted analysis with re-
spect to the gold segmentation.
Of the two MSA datasets, the vocalised version
BW
?
presents a more difficult segmentation task as
its words are on average longer and feature 31k
unique contiguous morphemes, compared to the 24k
in BW for the same number of words. It should thus
benefit more from additional model expressivity, as
is reflected in the increase of 10 SBF when adding
the TplR4 rule to the other triliteral ones.
The best triliteral root identification accuracy (on
a per-word basis) was found for HEB (74%) and BW
(67%).8,9 Refer to Figure 3 for example analyses.
An interesting aspect of these results is that tem-
platic rules may aid segmentation quality without
necessarily giving perfect root identification. Mod-
elling stem substructure allows any regularities that
give rise to a higher data likelihood to be picked up.
The low performance on the Quran demands fur-
ther explanation. All our adaptor grammars severely
oversegmented this data, although the mistakes were
not uniformly distributed. Most of the performance
loss is on the 79% of words that have 1-2 mor-
phemes. On the remaining words (having 3-5 mor-
phemes), our models recover and approach the Mor-
fessor baseline (MConcat: 32.7 , MTpl3Ch: 38.6).
Preliminary experiments on BW had indicated
that adaptation of (single) affix categories is crucial
for good performance. Our multi-affixing models
used on QU
?
lacked a further level of adaptation for
composite affixes, which we suspect as a contribut-
ing factor to the lower performance on that dataset.
This remains to be confirmed in future experiments,
but would be consistent with other observations on
the role of hierarchical adaptation in adaptor gram-
mars (Sirts and Goldwater, 2013). The trend that
intercalated rules improve segmentation (compared
to the concatenative grammar) remains consistent
8When excluding cases where root equals stem, root identi-
fication on BW is 55%. Those cases are still not trivial, since
words without roots also exist.
9By way of comparison, Rodrigues and C?avar (2007)
presented an unsupervised statistics-based root identification
method that obtained precision ranging between 50-75%, the
higher requiring vocalised words.
352
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
False Positive Rate
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Tr
ue
 P
os
iti
ve
 R
at
e
TplR4 (0.84)
Tpl+T4 (0.83)
Concat (0.63)
random (0.5)
(a) Stems
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
False Positive Rate
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Tr
ue
 P
os
iti
ve
 R
at
e
TplR4 (0.77)
Tpl+T4 (0.77)
random (0.5)
(b) Triliteral roots
Figure 2: ROC curves for predicting the stem and root lexicons for the HEB dataset. The area under each
curve (AUC), as computed with the trapezium rule, is given in parentheses.
across datasets, despite the lower absolute perfor-
mance on QU
?
.
The performance of the Morfessor baseline was
quite mixed. Contrary to our expectations, it per-
forms best on the ?harder? BW
?
, worst on the ar-
guably simpler HEB and struggled less than the
adaptor grammars on QU
?
.
One factor here is that it learns according to
a grammar with multiple consecutive affixes and
stems, whereas all our experiments (except on QU
?
)
presupposed single affixes. This biases the evalua-
tion slightly in our favour, but works in Morfessor?s
favour on the QU
?
data which is annotated with mul-
tiple affixes.
7 Related work
The distinctive feature of our morphological model
is that it jointly addresses root identification and
morpheme segmentation, and our results demon-
strate the mutual benefit of this.
In contrast, earlier unsupervised approaches tend
to focus on these tasks in isolation.
In unsupervised Arabic segmentation, the para-
metric Bayesian model of (Lee et al, 2011) achieves
F1-scores in the high eighties by incorporating sen-
tential context and inferred syntactic categories,
both of which our model forgoes, although theirs has
no account of discontiguous root morphemes.
Root Example instances
1. spr X G s?ap?ar?.ti te.s?ap?r? ye.s?ap?r?.u
B sipur.im hixs?tap?xar? t
2. lbs X G l?ab?aS?.t li.l?b?oS? ti.l?b?eS?.i
B le hax l?b?iS? ti tx l?ab?S?.i
3. ptx X G p?at?ax?.ti ti.p?t?ex?.i
B li.p?t?oax? nixp?t?ax?.at
5. !al ? B ya.!al.u m?ax!?al?.a !?ac?l?xan it
Table 4: Top Hebrew roots hypothesised by Tpl+T4.
Numbers indicate position when ranked by model
probability. (G)ood and (B)ad instances from
the corpus are given with morpheme boundaries
marked: true positive (.), false negative ( ) and false
positive (x). Hypothesised root characters are bold-
faced, while accent (?) marks gold root characters.
Previous approaches to Arabic root identifica-
tion that sought to use little supervision typically
constrain the search space of candidate characters
within a word, leveraging pre-existing dictionar-
ies (Darwish, 2002; Boudlal et al, 2009) or rule
constraints (Elghamry, 2005; Rodrigues and C?avar,
2007; Daya et al, 2008).
In contrast to these approaches, our model re-
quires no dictionary, and while our grammar rules
effect some constraints on what could be a root, they
are specified in a convenient and flexible manner that
353
Word
Suf
k m
Stem
s t A r
Pre
w l >
wl >stAr km X
Pre(w l) ... Stem ... Suf(k m)
X2 s t ? r
R3 s ? t ? r
R1
r
R2 s ? t
R1
t
R1
s
T2 > ? A
T1
A
T1
>
(a) Concat & Tpl+T4, ?wl>stArkm? (BW)
Word
Stem
n o t i l l A
Pre
l i d a
li danotillA X
Pre(l i) ... Stem(danotill) ... Suf(A)
T4 o ? a ? i ? l
T1
l
T3 o ? a ? i
T1
i
T2 o ? a
T1
a
T1
o
R4 d ? n ? t ? l
R1
l
R3 d ? n ? t
R1
t
R2 d ? n
R1
n
R1
d
(b) Concat & TplR4, ?lidanotillA? (BW
?
)
Figure 3: Parse trees produced for words in the two standard Arabic datasets that were incorrectly segmented
by the baseline grammar. The templatic grammars correctly identified the triliteral and quadriliteral roots,
also fixing the segmentation of (a). In (b), the templatic grammar improved over the baseline by finding
the correct prefix but falsely posited a suffix. Unimportant subtrees are elided for space, while the yields of
discontiguous constituents are indicated next to their symbols, with dots marking gaps. Crossing branches
are not drawn but should be inferrable. Root characters are bold-faced in the reference analysisX. The non-
terminal X2 in (a) is part of a number of implementation-specific helper rules that ensure the appropriate
handling of partly contiguous roots.
makes experimentation with other phenomena easy.
Recent work by Fullwood and O?Donnell (2013)
goes some way toward jointly dealing with non-
concatenative and concatenative morphology in the
unsupervised setting, but their focus is limited to in-
flected stems and does not handle multiple consecu-
tive affixes. They analyse the Arabic verb stem (e.g.
kataba ?he wrote?) into a templatic bit-string denot-
ing root and non-root characters (e.g. r-r-r-) along
with a root morpheme (e.g. ktb) and a so-called
residue morpheme (e.g. aaa). Their nonparamet-
ric Bayesian model induces lexicons of these en-
tities and achieves very high performance on tem-
plates. The explicit formulation of templates allevi-
ates the labelling ambiguity that hampered our eval-
uation (?6.4), but we believe their method of anal-
ysis can be simulated in our framework using the
appropriate SRCG-rules.
Learning root-templatic morphology is loosely re-
lated to morphological paradigm induction (Clark,
2001; Dreyer and Eisner, 2011; Durrett and DeN-
ero, 2013). Our models do not represent templatic
paradigms explicitly, but it is interesting to note that
preliminary experiments with German indicate that
our adaptor grammars pick up on the past participle
forming circumfix in ab+ge+spiel+t (played back).
8 Conclusion and Outlook
We presented a new approach to modelling non-
concatenative phenomena in morphology using sim-
ple range concatenating grammars and extended
adaptor grammars to this formalism. Our experi-
ments show that this richer model improves morpho-
logical segmentation and morpheme lexicon induc-
tion on different languages in the Semitic family.
Various avenues for future work present them-
selves. Firstly, the lightly-supervised, meta-
grammar approach to adaptor grammars (Sirts and
Goldwater, 2013) can be extended to this more
powerful formalism to lessen the burden of defin-
ing the ?right? grammar rules by hand, and possi-
bly boost performance. Secondly, the discontigu-
ous constituents learnt with our framework can be
used as features in other downstream applications.
Especially in low-resource languages, the ability to
model non-concatenative phenomena (e.g. circum-
fixing, ablaut, etc.) can play an important role in re-
ducing data sparsity for tasks like word alignment
and language modelling. Finally, the PYSRCAG
presents another way of learning SRCGs in general,
which can thus be employed in other applications of
SRCGs, including syntactic parsing and translation.
Acknowledgements
We thank the anonymous reviewers for their valu-
able comments. Our PYSRCAG implementation
leveraged the adaptor grammar code released by
Mark Johnson, whom we thank, along with the in-
dividuals who contributed to the public data sources
that enabled the empirical elements of this paper.
354
References
Aviad Albert, Brian MacWhinney, Bracha Nir, and Shuly
Wintner. 2013. The Hebrew CHILDES corpus: tran-
scription and morphological analysis. Language Re-
sources and Evaluation, pages 1?33.
Mohamed Altantawy, Nizar Habash, Owen Rambow, and
Ibrahim Saleh. 2010. Morphological Analysis and
Generation of Arabic Nouns: A Morphemic Func-
tional Approach. In Proceedings of LREC, pages 851?
858.
Kenneth R Beesley and Lauri Karttunen. 2003. Fi-
nite state morphology, volume 18. CSLI publications
Stanford.
Abderrahim Boudlal, Rachid Belahbib, Abdelhak
Lakhouaja, Azzeddine Mazroui, Abdelouafi Meziane,
and Mohamed Bebah. 2009. A Markovian approach
for Arabic Root Extraction. The International Arab
Journal of Information Technology, 8(1):91?98.
Pierre Boullier. 2000. A cubic time extension of context-
free grammars. Grammars, 3(2-3):111?131.
Tim Buckwalter. 2002. Arabic Morphological Ana-
lyzer. Technical report, Linguistic Data Consortium,
Philedelphia.
Alexander Clark. 2001. Learning Morphology with Pair
Hidden Markov Models. In Proceedings of the ACL
Student Workshop, pages 55?60.
Yael Cohen-Sygal and Shuly Wintner. 2006. Finite-
state registered automata for non-concatenative mor-
phology. Computational Linguistics, 32(1):49?82.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1):1?34.
Kareem Darwish. 2002. Building a shallow Arabic
morphological analyzer in one day. In Proceedings
of the ACL Workshop on Computational Approaches
to Semitic Languages, pages 47?54. Association for
Computational Linguistics.
Ezra Daya, Dan Roth, and Shuly Wintner. 2008.
Identifying Semitic Roots: Machine Learning with
Linguistic Constraints. Computational Linguistics,
34(3):429?448.
Markus Dreyer and Jason Eisner. 2011. Discovering
Morphological Paradigms from Plain Text Using a
Dirichlet Process Mixture Model. In Proceedings of
EMNLP, pages 616?627, Edinburgh, Scotland.
Kais Dukes and Nizar Habash. 2010. Morphological An-
notation of Quranic Arabic. In Proceedings of LREC.
Greg Durrett and John DeNero. 2013. Supervised Learn-
ing of Complete Morphological Paradigms. In Pro-
ceedings of NAACL-HLT, pages 1185?1195, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Khaled Elghamry. 2005. A Constraint-based Algorithm
for the Identification of Arabic Roots. In Proceed-
ings of the Midwest Computational Linguistics Collo-
quium. Indiana University. Bloomington, IN.
Raphael Finkel and Gregory Stump. 2002. Generating
Hebrew verb morphology by default inheritance hier-
archies. In Proceedings of the ACL Workshop on Com-
putational Approaches to Semitic Languages. Associ-
ation for Computational Linguistics.
Michelle A. Fullwood and Timothy J. O?Donnell. 2013.
Learning non-concatenative morphology. In Proceed-
ings of the Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 21?27, Sofia, Bulgaria.
Association for Computational Linguistics.
Michael Gasser. 2009. Semitic morphological analysis
and generation using finite state transducers with fea-
ture structures. In Proceedings of EACL, pages 309?
317. Association for Computational Linguistics.
Daniel Gildea. 2010. Optimal Parsing Strategies for Lin-
ear Context-Free Rewriting Systems. In Proceedings
of NAACL, pages 769?776. Association for Computa-
tional Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating Between Types and Tokens
by Estimating Power-Law Generators. In Advances in
Neural Information Processing Systems, Volume 18.
Harald Hammarstro?m and Lars Borin. 2011. Unsuper-
vised Learning of Morphology. Computational Lin-
guistics, 37(2):309?350.
Yun Huang, Min Zhang, and Chew Lim Tan. 2011.
Nonparametric Bayesian Machine Transliteration with
Synchronous Adaptor Grammars. In Proceedings of
ACL (Short papers), pages 534?539.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: Experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL-HLT, pages 317?325.
Association for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A Framework for
Specifying Compositional Nonparametric Bayesian
Models. In Advances in Neural Information Process-
ing Systems, volume 19, page 641. MIT.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using Adaptor Grammars. In Proceedings
of ACL Special Interest Group on Computational Mor-
phology and Phonology (SigMorPhon), pages 20?27.
Association for Computational Linguistics.
Aravind K. Joshi. 1985. Tree adjoining grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In D.R. Dowty, L. Kart-
tunen, and A.M. Zwicky, editors, Natural Language
Parsing, chapter 6, pages 206?250. Cambridge Uni-
versity Press.
355
Miriam Kaeshammer. 2013. Synchronous Linear
Context-Free Rewriting Systems for Machine Trans-
lation. In Proceedings of the Workshop on Syntax, Se-
mantics and Structure in Statistical Translation, pages
68?77, Atlanta, Georgia. Association for Computa-
tional Linguistics.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic Multiple Context-Free Grammar for RNA
Pseudoknot Modeling. In Proceedings of the Inter-
national Workshop on Tree Adjoining Grammar and
Related Formalisms, pages 57?64.
George Anton Kiraz. 2000. Multitiered Nonlinear Mor-
phology Using Multitape Finite Automata: A Case
Study on Syriac and Arabic. Computational Linguis-
tics, 26(1):77?105, March.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
Proceedings of the 10th international conference on
Computational Linguistics, pages 178?181. Associa-
tion for Computational Linguistics.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2010.
Overview and Results of Morpho Challenge 2009. In
Multilingual Information Access Evaluation I. Text Re-
trieval Experiments, volume 6241 of Lecture Notes in
Computer Science, pages 578?597. Springer Berlin /
Heidelberg.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves morpho-
logical segmentation. In Proceedings of CoNLL.
Wolfgang Maier. 2010. Direct Parsing of Discon-
tinuous Constituents in German. In Proceedings of
the NAACL-HLT Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 58?66. Asso-
ciation for Computational Linguistics.
Jim Pitman and Marc Yor. 1997. The Two-Parameter
Poisson-Dirichlet Distribution Derived from a Stable
Subordinator. The Annals of Probability, 25(2):855?
900.
Jim Pitman. 1995. Exchangeable and partially exchange-
able random partitions. Probability Theory and Re-
lated Fields, 102:145?158.
Jean-Franc?ois Prunet. 2006. External Evidence and the
Semitic Root. Morphology, 16(1):41?67.
Paul Rodrigues and Damir C?avar. 2007. Learning Arabic
Morphology Using Statistical Constraint-Satisfaction
Models. In Elabbas Benmamoun, editor, Perspectives
on Arabic Linguistics: Proceedings of the 19th Ara-
bic Linguistics Symposium, pages 63?75, Urbana, IL,
USA. John Benjamins Publishing Company.
Nathan Schneider. 2010. Computational Cognitive
Morphosemantics: Modeling Morphological Compo-
sitionality in Hebrew Verbs with Embodied Construc-
tion Grammar. In Proceedings of the Annual Meeting
of the Berkeley Linguistics Society, Berkeley, CA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2):191?229.
Kairit Sirts and Sharon Goldwater. 2013. Minimally-
Supervised Morphological Segmentation using Adap-
tor Grammars. Transactions of the ACL.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In Proceedings of
ACL, pages 104?111.
356
Proceedings of the EACL 2012 Student Research Workshop, pages 64?73,
Avignon, France, 26 April 2012. c?2012 Association for Computational Linguistics
Hierarchical Bayesian Language Modelling
for the Linguistically Informed
Jan A. Botha
Department of Computer Science
University of Oxford, UK
jan.botha@cs.ox.ac.uk
Abstract
In this work I address the challenge of aug-
menting n-gram language models accord-
ing to prior linguistic intuitions. I argue
that the family of hierarchical Pitman-Yor
language models is an attractive vehicle
through which to address the problem, and
demonstrate the approach by proposing a
model for German compounds. In an em-
pirical evaluation, the model outperforms
the Kneser-Ney model in terms of perplex-
ity, and achieves preliminary improvements
in English-German translation.
1 Introduction
The importance of effective language models in
machine translation (MT) and automatic speech
recognition (ASR) is widely recognised. n-gram
models, in particular ones using Kneser-Ney
(KN) smoothing, have become the standard
workhorse for these tasks. These models are not
ideal for languages that have relatively free word
order and/or complex morphology. The ability to
encode additional linguistic intuitions into models
that already have certain attractive properties is an
important piece of the puzzle of improving ma-
chine translation quality for those languages. But
despite their widespread use, KN n-gram mod-
els are not easily extensible with additional model
components that target particular linguistic phe-
nomena.
I argue in this paper that the family of hierarchi-
cal Pitman-Yor language models (HPYLM) (Teh,
2006; Goldwater et al, 2006) are suitable for
investigations into more linguistically-informed
n-gram language models. Firstly, the flexibility
to specify arbitrary back-off distributions makes it
easy to incorporate multiple models into a larger
n-gram model. Secondly, the Pitman-Yor process
prior (Pitman and Yor, 1997) generates distribu-
tions that are well-suited to a variety of power-
law behaviours, as is often observed in language.
Catering for a variety of those is important since
the frequency distributions of, say, suffixes, could
be quite different from that of words. KN smooth-
ing is less flexibility in this regard. And thirdly,
the basic inference algorithms have been paral-
lelised (Huang and Renals, 2009), which should
in principle allow the approach to still scale to
large data sizes.
As a test bed, I consider compounding in Ger-
man, a common phenomenon that creates chal-
lenges for machine translation into German.
2 Background and Related Work
n-gram language models assign probabilities to
word sequences. Their key approximation is that
a word is assumed to be fully determined by n?1
words preceding it, which keeps the number of in-
dependent probabilities to estimate in a range that
is computationally attractive. This basic model
structure, largely devoid of syntactic insight, is
surprisingly effective at biasing MT and ASR sys-
tems toward more fluent output, given a suitable
choice of target language.
But the real challenge in constructing n-gram
models, as in many other probabilistic settings, is
how to do smoothing, since the vast majority of
linguistically plausible n-grams will occur rarely
or be absent altogether from a training corpus,
which often renders empirical model estimates
misleading. The general picture is that probability
mass must be shifted away from some events and
redistributed across others.
The method of Kneser and Ney (1995) and
64
its later modified version (Chen and Goodman,
1998) generally perform best at this smoothing,
and are based on the idea that the number of
distinct contexts a word appears in is an impor-
tant factor in determining the probability of that
word. Part of this smoothing involves discount-
ing the counts of n-grams in the training data;
the modified version uses different levels of dis-
counting depending on the frequency of the count.
These methods were designed with surface word
distributions, and are not necessarily suitable for
smoothing distributions of other kinds of surface
units.
Bilmes and Kirchhoff (2003) proposed a more
general framework for n-gram language mod-
elling. Their Factored Language Model (FLM)
views a word as a vector of features, such that a
particular feature value is generated conditional
on some history of preceding feature values. This
allowed the inclusion of n-gram models over se-
quences of elements like PoS tags and semantic
classes. In tandem, they proposed more compli-
cated back-off paths; for example, trigrams can
back-off to two underlying bigram distributions,
one dropping the left-most context word and the
other the right-most. With the right combina-
tion of features and back-off structure they got
good perplexity reductions, and obtained some
improvements in translation quality by applying
these ideas to the smoothing of the bilingual
phrase table (Yang and Kirchhoff, 2006).
My approach has some similarity to the FLM:
both decompose surface word forms into elements
that are generated from unrelated conditional dis-
tributions. They differ predominantly along two
dimensions: the types of decompositions and con-
ditioning possible, and my use of a particular
Bayesian prior for handling smoothing.
In addition to the HPYLM for n-gram lan-
guage modelling (Teh, 2006), models based on
the Pitman-Yor process prior have also been ap-
plied to good effect in word segmentation (Gold-
water et al, 2006; Mochihashi et al, 2009) and
speech recognition (Huang and Renals, 2007;
Neubig et al, 2010). The Graphical Pitman-Yor
process enables branching back-off paths, which
I briefly revisit in ?7, and have proved effective
in language model domain-adaptation (Wood and
Teh, 2009). Here, I extend this general line of
inquiry by considering how one might incorpo-
rate linguistically informed sub-models into the
HPYLM framework.
3 Compound Nouns
I focus on compound nouns in this work for two
reasons: Firstly, compounding is in general a very
productive process, and in some languages (in-
cluding German, Swedish and Dutch) they are
written as single orthographic units. This in-
creases data sparsity and creates significant chal-
lenges for NLP systems that use whitespace to
identify their elementary modelling units. A
proper account of compounds in terms of their
component words therefore holds the potential of
improving the performance of such systems.
Secondly, there is a clear linguistic intuition to
exploit: the morphosyntactic properties of these
compounds are often fully determined by the head
component within the compound. For example,
in ?Geburtstagskind? (birthday kid), it is ?Kind?
that establishes this compound noun as singular
neuter, which determine how it would need to
agree with verbs, articles and adjectives. In the
next section, I propose a model in the suggested
framework that encodes this intuition.
The basic structure of German compounds
comprises a head component, preceded by one or
more modifier components, with optional linker
elements between consecutive components (Gold-
smith and Reutter, 1998).
Examples
? The basic form is just the concatenation of two
nouns
Auto + Unfall = Autounfall (car crash)
? Linker elements are sometimes added be-
tween components
Ku?che + Tisch = Ku?chentisch (kitchen table)
? Components can undergo stemming during
composition
Schule + Hof = Schulhof (schoolyard)
? The process is potentially recursive
(Geburt + Tag) + Kind = Geburtstag + Kind
= Geburtstagskind (birthday kid)
The process is not limited to using nouns as
components, for example, the numeral in Zwei-
Euro-Mu?nze (two Euro coin) or the verb ?fahren?
(to drive) in Fahrzeug (vehicle). I will treat all
these cases the same.
65
3.1 Fluency amid sparsity
Consider the following example from the training
corpus used in the subsequent evaluations:
de: Die Neuinfektionen u?bersteigen weiterhin die
Behandlungsbemu?hungen.
en: New infections continue to outpace treatment ef-
forts.
The corpus contains numerous other compounds
ending in ?infektionen? (16) or ?bemu?hungen?
(117). A standard word-based n-gram model
discriminates among those alternatives using as
many independent parameters.
However, we could gauge the approximate syn-
tactic fluency of the sentence almost as well if we
ignore the compound modifiers. Collapsing all
the variants in this way reduces sparsity and yields
better n-gram probability estimates.
To account for the compound modifiers, a sim-
ple approach is to use a reverse n-gram language
model over compound components, without con-
ditioning on the sentential context. Such a model
essentially answers the question, ?Given that the
word ends in ?infektionen?, what modifier(s), if
any, are likely to precede it?? The vast majority of
nouns will never occur in that position, meaning
that the conditional distributions will be sharply
peaked.
mit der Draht?seil?bahn
Figure 1: Intuition for the proposed generative pro-
cess of a compound word: The context generates the
head component, which generates a modifier compo-
nent, which in turn generates another modifier. (Trans-
lation: ?with the cable car?)
3.2 Related Work on Compounds
In machine translation and speech recognition,
one approach has been to split compounds as a
preprocessing step and merge them back together
during postprocessing, while using otherwise un-
modified NLP systems. Frequency-based meth-
ods have been used for determining how aggres-
sively to split (Koehn and Knight, 2003), since
the maximal, linguistically correct segmentation
is not necessarily optimal for translation. This
gave rise to slight improvements in machine trans-
lation evaluations (Koehn et al, 2008), with fine-
tuning explored in (Stymne, 2009). Similar ideas
have also been employed for speech recognition
(Berton et al, 1996) and predictive-text input
(Baroni and Matiasek, 2002), where single-token
compounds also pose challenges.
4 Model Description
4.1 HPYLM
Formally speaking, an n-gram model is an
(n? 1)-th order Markov model that approxi-
mates the joint probability of a sequence of
words w as
P (w) ?
|w|?
i=1
P (wi|wi?n+1, . . . , wi?1),
for which I will occasionally abbreviate a con-
text [wi, . . . , wj ] as u. In the HPYLM, the condi-
tional distributions P (w|u) are smoothed by plac-
ing Pitman-Yor process priors (PYP) over them.
The PYP is defined through its base distribution,
and a strength (?) and discount (d) hyperparame-
ter that control its deviation away from its mean
(which equals the base distribution).
LetG[u,v] be the PYP-distributed trigram distri-
bution P (w|u, v). The hierarchy arises by using
as base distribution for the prior of G[u,v] another
PYP-distributedG[v], i.e. the distributionP (w|v).
The recursion bottoms out at the unigram distri-
bution G?, which is drawn from a PYP with base
distribution equal to the uniform distribution over
the vocabularyW . The hyperparameters are tied
across all priors with the same context length |u|,
and estimated during training.
G0 = Uniform(|W|)
G? ? PY (d0, ?0, G0)
...
Gpi(u) ? PY (d|pi(u)|, ?|pi(u)|, Gpi(pi(u)))
Gu ? PY (d|u|, ?|u|, Gpi(u))
w ? Gu,
where pi(u) truncates the context u by dropping
the left-most word in it.
4.2 HPYLM+c
Define a compound word w? as a sequence of
components [c1, . . . , ck], plus a sentinel symbol $
marking either the left or the right boundary of the
word, depending on the direction of the model. To
maintain generality over this choice of direction,
66
let ? be an index set over the positions, such that
c?1 always designates the head component.
Following the motivation in ?3.1, I set up the
model to generate the head component c?1 condi-
tioned on the word context u, while the remaining
components w? \ c?1 are generated by some model
F , independently of u.
To encode this, I modify the HPYLM in two
ways: 1) Replace the support with the reduced vo-
cabularyM, the set of unique components c ob-
tained when segmenting the items in W . 2) Add
an additional level of conditional distributionsHu
(with |u| = n? 1) where items fromM combine
to form the observed surface words:
Gu . . . (as before, except G0 =Uniform(|M|))
Hu ? PY (d|u|, ?|u|, Gu ? F )
w? ? Hu
So the base distribution for the prior of the word
n-gram distribution Hu is the product of a distri-
bution Gu over compound heads, given the same
context u, and another (n?-gram) language model
F over compound modifiers, conditioned on the
head component.
Choosing F to be a bigram model (n?=2) yields
the following procedure for generating a word:
c?1 ? Gu
for i = 2 to k
c?i ? F (?|c?i?1)
The linguistically motivated choice for condi-
tioning in F is ?ling = [k, k ? 1, . . . , 1] such that
c?1 is the true head component; $ is drawn from
F (?|c1) and marks the left word boundary.
In order to see if the correct linguistic intuition
has any bearing on the model?s extrinsic perfor-
mance, we will also consider the reverse, sup-
posing that the left-most component were actu-
ally more important in this task, and letting the
remaining components be generated left-to-right.
This is expressed by ?inv = [1, . . . , k], where $
this time marks the right word boundary and is
drawn from F (?|ck).
To test whether Kneser-Ney smoothing is in-
deed sometimes less appropriate, as conjectured
earlier, I will also compare the case where
F = FKN , a KN-smoothed model, with the case
where F = FHPY LM , another HPYLM.
Linker Elements In the preceding definition of
compound segmentation, the linker elements do
not form part of the vocabulary M. Regarding
linker elements as components in their own right
would sacrifice important contextual information
and disrupt the conditionals F (?|c?i?1). That is,
given Ku?che?n?tisch, we want P (Ku?che|Tisch) in
the model, but not P (Ku?che|n).
But linker elements need to be accounted
for somehow to have a well-defined generative
model. I follow the pragmatic option of merg-
ing any linkers onto the adjacent component ? for
?ling merging happens onto the preceding compo-
nent, while for ?inv it is onto the succeeding one.
This keeps the ?head? component c?1 in tact.
More involved strategies could be considered,
and it is worth noting that for German the pres-
ence and identity of linker elements between ci
and ci+1 are in fact governed by the preceding
component ci (Goldsmith and Reutter, 1998).
5 Training
For ease of exposition I describe inference with
reference to the trigram HPYLM+c model with
a bigram HPYLM for F , but the general case
should be clear.
The model is specified by the latent vari-
ables (G[?], G[v], G[u,v], H[u,v], F?, Fc), where
u, v ? W , c ?M, and hyperparameters ? =
{di, ?i} ? {d?j , ?
?
j} ? {d
??
2, ?
??
2}, where i = 0, 1, 2,
j = 0, 1, single primes designate the hyperpa-
rameters in FHPY LM and double primes those of
H[u,v]. We can construct a collapsed Gibbs sam-
pler by marginalising out these latent variables,
giving rise to a variant of the hierarchical Chinese
Restaurant Process in which it is straightforward
to do inference.
Chinese Restaurant Process A direct repre-
sentation of a random variable G drawn from a
PYP can be obtained from the so-called stick-
breaking construction. But the more indirect rep-
resentation by means of the Chinese Restaurant
Process (CRP) (Pitman, 2002) is more suitable
here since it relates to distributions over items
drawn from such a G. This fits the current set-
ting, where wordsw are being drawn from a PYP-
distributed G.
Imagine that a corpus is created in two phases:
Firstly, a sequence of blank tokens xi is instanti-
ated, and in a second phase lexical identities wi
are assigned to these tokens, giving rise to the
67
observed corpus. In the CRP metaphor , the se-
quence of tokens xi are equated with a sequence
of customers that enter a restaurant one-by-one to
be seated at one of an infinite number of tables.
When a customer sits at an unoccupied table k,
they order a dish ?k for the table, but customers
joining an occupied table have to dine on the dish
already served there. The dish ?i that each cus-
tomer eats is equated to the lexical identity wi of
the corresponding token, and the way in which ta-
bles and dishes are chosen give rise to the charac-
teristic properties of the CRP:
More formally, let x1, x2, . . . be draws fromG,
while t is the number of occupied tables, c the
number of customers in the restaurant, and ck the
number of customers at the k-th table. Condi-
tioned on preceding customers x1, . . . , xi?1 and
their arrangement, the i-th customer sits at table
k = k? according to the following probabilities:
Pr(k?| . . . ) ?
{
ck? ? d occupied table k?
? + dt unoccupied table t+ 1
Ordering a dish for a new table corresponds to
drawing a value ?k from the base distribution G0,
and it is perfectly acceptable to serve the same
kind of dish at multiple tables.
Some characteristic behaviour of the CRP can
be observed easily from this description: 1) As
more customers join a table, that table becomes
a more likely choice for future customers too.
2) Regardless of how many customers there are,
there is always a non-zero probability of joining
an unoccupied table, and this probability also de-
pends on the number of total tables.
The dish draws can be seen as backing off to
the underlying base distribution G0, an important
consideration in the context of the hierarchical
variant of the process explained shortly. Note that
the strength and discount parameters control the
extent to which new dishes are drawn, and thus
the extent of reliance on the base distribution.
The predictive probability of a word w given a
seating arrangement is given by
Pr(w| . . . ) ? cw ? dtw + (? + dt)G0(w)
In smoothing terminology, the first term can be
interpreted as applying a discount of dtw to the
observed count cw of w; the amount of dis-
count therefore depends on the prevalence of the
word (via tw). This is one significant way in
which the PYP/CRP gives more nuanced smooth-
ing than modified Kneser-Ney, which only uses
four different discount levels (Chen and Good-
man, 1998). Similarly, if the seating dynamics
are constrained such that each dish is only served
once (tw = 1 for any w), a single discount level
is affected, establishing direct correspondence to
original interpolated Kneser-Ney smoothing (Teh,
2006).
Hierarchical CRP When the prior of Gu has
a base distribution Gpi(u) that is itself PYP-
distributed, as in the HPYLM, the restaurant
metaphor changes slightly. In general, each node
in the hierarchy has an associated restaurant.
Whenever a new table is opened in some restau-
rantR, another customer is plucked out of thin air
and sent to join the parent restaurant pa(R). This
induces a consistency constraint over the hierar-
chy: the number of tables tw in restaurant R must
equal the number of customers cw in its parent
pa(R).
In the proposed HPYLM+c model using
FHPY LM , there is a further constraint of a simi-
lar nature: When a new table is opened and serves
dish ? = w? in the trigram restaurant for H[u,v],
a customer c?1 is sent to the corresponding bi-
gram restaurant for G[u,v], and customers c?2:k ,$
are sent to the restaurants for Fc? , for contexts
c? = c?1:k?1 . This latter requirement is novel here
compared to the hierarchical CRP used to realise
the original HPYLM.
Sampling Although the CRP allows us to re-
place the priors with seating arrangements S,
those seating arrangements are simply latent vari-
ables that need to be integrated out to get a true
predictive probability of a word:
p(w|D) =
?
S,?
p(w|S,?)p(S,?|D),
where D is the training data and, as before, ? are
the parameters. This integral can be approximated
by averaging over m posterior samples (S,?)
generated using Markov chain Monte Carlo meth-
ods. The simple form of the conditionals in the
CRP allows us to do a Gibbs update whereby the
table index k of a customer is resampled condi-
tioned on all the other variables. Sampling a new
seating arrangement S for the trigram HPYLM+c
thus corresponds to visiting each customer in the
restaurants for H[u,v], removing them while cas-
cading as necessary to observe the consistency
68
across the hierarchy, and seating them anew at
some table k?.
In the absence of any strong intuitions about ap-
propriate values for the hyperparameters, I place
vague priors over them and use slice sampling1
(Neal, 2003) to update their values during gener-
ation of the posterior samples:
d ? Beta(1, 1) ? ? Gamma(1, 1)
Lastly, I make the further approximation of
m = 1, i.e. predictive probabilities are informed
by a single posterior sample (S,?).
6 Experiments
The aim of the experiments reported here is to test
whether the richer account of compounds in the
proposed language models has positive effects on
the predictability of unseen text and the genera-
tion of better translations.
6.1 Methods
Data and Tools Standard data preprocessing
steps included normalising punctuation, tokenis-
ing and lowercasing all words. All data sets are
from the WMT11 shared-task.2. The full English-
German bitext was filtered to exclude sentences
longer than 50, resulting in 1.7 million parallel
sentences; word alignments were inferred from
this using the Berkeley Aligner (Liang et al,
2006) and used as basis from which to extract a
Hiero-style synchronous CFG (Chiang, 2007).
The weights of the log-linear translation mod-
els were tuned towards the BLEU metric on
development data using cdec?s (Dyer et al,
2010) implementation of MERT (Och, 2003).
For this, the set news-test2008 (2051 sen-
tences) was used, while final case-insensitive
BLEU scores are measured on the official test set
newstest2011 (3003 sentences).
All language models were trained on the target
side of the preprocessed bitext containing 38 mil-
lion tokens, and tested on all the German devel-
opment data (i.e. news-test2008,9,10).
Compound segmentation To construct a seg-
mentation dictionary, I used the 1-best segmenta-
tions from a supervised MaxEnt compound split-
ter (Dyer, 2009) run on all token types in bitext. In
addition, word-internal hyphens were also taken
1Mark Johnson?s implementation, http://www.cog.
brown.edu/?mj/Software.htm
2http://www.statmt.org/wmt11/
as segmentation points. Finally, linker elements
were merged onto components as discussed in
?4.2. Any token that is split into more than one
part by this procedure is regarded as a compound.
The effect of the individual steps is summarised
in Table 1.
# Types Example
None 350998 Geburtstagskind
pre-merge 201328 Geburtstag?kind
merge, ?ling 150980 Geburtstags?kind
merge, ?inv 162722 Geburtstag?skind
Table 1: Effect of segmentation on vocabulary size.
Metrics For intrinsic evaluation of language
models, perplexity is a common metric. Given a
trained model q, the perplexity over the words ?
in unseen test set T is exp
(
? 1|T |
?
? ln(q(?))
)
.
One convenience of this per-word perplexity is
that it can be compared consistently across dif-
ferent test sets regardless of their lengths; its neat
interpretation is another: a model that achieves a
perplexity of ? on a test set is on average ?-ways
confused about each word. Less confusion and
therefore lower test set perplexity is indicative of
a better model. This allows different models to be
compared relative to the same test set.
The exponent above can be regarded as an
approximation of the cross-entropy between the
model q and a hypothetical model p from which
both the training and test set were putatively gen-
erated. It is sometimes convenient to use this as
an alternative measure.
But a language model only really becomes use-
ful when it allows some extrinsic task to be exe-
cuted better. When that extrinsic task is machine
translation, the translation quality can be assessed
to see if one language model aids it more than an-
other. The obligatory metric for evaluating ma-
chine translation quality is BLEU (Papineni et al,
2001), a precision based metric that measures how
close the machine output is to a known correct
translation (the reference sentences in the test set).
Higher precision means the translation system is
getting more phrases right.
Better language model perplexities sometimes
lead to improvements in translation quality, but
it is not guaranteed. Moreover, even when real
translation improvements are obtained, they are
69
PPL c-Cross-ent.
mKN 441.32 0.1981
HPYLM 429.17 0.1994
FKN ?ling 432.95 0.2028
FKN ?inv 446.84 0.2125
FHPY LM ?ling 421.63 0.1987
FHPY LM ?inv 435.79 0.2079
Table 2: Monolingual evaluation results. The second
column shows perplexity measured all WMT11 Ger-
man development data (7065 sentences). At the word
level, all are trigram models, while F are bigram mod-
els using the specified segmentation scheme. The third
column has test cross-entropies measured only on the
6099 compounds in the test set (given their contexts ).
not guaranteed to be noticeable in the BLEU
score, especially when targeting an arguably nar-
row phenomenon like compounding.
BLEU
mKN 13.11
HPYLM 13.20
FHPY LM , ?ling 13.24
FHPY LM , ?inv 13.32
Table 3: Translation results, BLEU (1-ref), 3003 test
sentences. Trigram language models, no count prun-
ing, no ?unknown word? token.
P / R / F
mKN 22.0 / 17.3 / 19.4
HPYLM 21.0 / 17.8 / 19.3
FHPY LM , ?ling 23.6 / 17.3 / 19.9
FHPY LM , ?inv 24.1 / 16.5 / 19.6
Table 4: Precision, Recall and F-score of compound
translations, relative to reference set (72661 tokens, of
which 2649 are compounds).
6.2 Main Results
For the monolingual evaluation, I used an interpo-
lated, modified Kneser-Ney model (mKN) and an
HPYLM as baselines. It has been shown for other
languages that HPYLM tends to outperform mKN
(Okita and Way, 2010), but I am not aware of this
result being demonstrated on German before, as I
do in Table 2.
The main model of interest is HPYLM+c us-
ing the ?ling segmentation and a model FHPY LM
over modifiers; this model achieves the lowest
perplexity, 4.4% lower than the mKN baseline.
Next, note that using FKN to handle the modi-
fiers does worse than FHPY LM , confirming our
expectation that KN is less appropriate for that
task, although it still does better than the original
mKN baseline.
The models that use the linguistically im-
plausible segmentation scheme ?inv both fare
worse than their counterparts that use the sensible
scheme, but of all tested models only FKN & ?inv
fails to beat the mKN baseline. This suggests that
in some sense having any account whatsoever of
compound formation tends to have a beneficial ef-
fect on this test set ? the richer statistics due to a
smaller vocabulary could be sufficient to explain
this ? but to get the most out of it one needs the
superior smoothing over modifiers (provided by
FHPY LM ) and adherence to linguistic intuition
(via ?ling).
As for the translation experiments, the rela-
tive qualitative performance of the two baseline
language models carries over to the BLEU score
(HPYLM does 0.09 points better than KN), and is
further improved upon slightly by using two vari-
ants of HPYLM+c (Table 3).
6.3 Analysis
To get a better idea of how the extended mod-
els employ the increased expressiveness, I calcu-
lated the cross-entropy over only the compound
words in the monolingual test set (second column
of Table 2). Among the HPYLM+c variants, we
see that their performance on compounds only is
consistent with their performance (relative to each
other) on the whole corpus. This implies that
the differences in whole-corpus perplexities are at
least in part due to their different levels of adept-
ness at handling compounds, as opposed to some
fluke event.
It is, however, somewhat surprising to observe
that HPYLM+c do not achieve a lower com-
pound cross-entropy than the mKN baseline, as it
suggests that HPYLM+c?s perplexity reductions
compared to mKN arise in part from something
other than compound handling, which is their
whole point.
This discrepancy could be related to the fair-
ness of this direct comparison of models that ul-
70
timately model different sets of things: Accord-
ing to the generative process of HPYLM+c (?4),
there is no limit on the number of components in
a compound: in theory, an arbitrary number of
components c ? M can combine to form a word.
HPYLM+c is thus defined over a countably infi-
nite set of words, thereby reserving some prob-
ability mass for items that will never be realised
in any corpus, whereas the baseline models are
defined only over the finite set W . These direct
comparisons are thus lightly skewed in favour of
the baselines. This bolsters confidence in the per-
plexity reductions presented in the previous sec-
tion, but the skew may afflict compounds more
starkly, leading to the slight discrepancy observed
in the compound cross-entropies. What matters
more is the performance among the HPYLM+c
variants, since they are directly comparable.
To home in still further on the compound mod-
elling, I selected those compounds for which
HPYLM+c (FHPY LM ,?ling) does best/worst in
terms of the probabilities assigned, compared to
the mKN baseline (see Table 5). One pattern that
emerges is that the ?top? compounds mostly con-
sist of components that are likely to be quite com-
mon, and that this improves estimates both for n-
grams that are very rare (the singleton ?senkun-
gen der treibhausgasemmissionen? = decreases in
green house gas emissions) or relatively common
(158, ?der hauptstadt? = of the capital).
n-gram ? C
gesichts?punkten 0.064 335
700 milliarden us-?dollar 0.021 2
s. der treibhausgas?emissionen 0.018 1
r. der treibhausgas?emissionen 0.011 3
ministerium fu?r land?wirtschaft 0.009 11
bildungs?niveaus 0.009 14
newt ging?rich* -0.257 2
nouri al-?maliki* -0.257 3
klerikers moqtada al-?sadr* -0.258 1
nuri al-?maliki* -0.337 3
sankt peters?burg* -0.413 35
na?chtlichem flug?la?rm -0.454 2
Table 5: Compound n-grams in the test set for which
the absolute difference ? = PHPYLM+c?PmKN is great-
est. C is n-gram count in the training data. Asterisks
denote words that are not compounds, linguistically
speaking. Abbrevs: r. = reduktionen, s.= senkungen
On the other hand, the ?bottom? compounds
are mostly ones whose components will be un-
common; in fact, many of them are not truly com-
pounds but artefacts of the somewhat greedy seg-
mentation procedure I used. Alternative proce-
dures will be tested in future work.
Since the BLEU scores do not reveal much
about the new language models? effect on com-
pound translation, I also calculated compound-
specific accuracies, using precision, recall and
F-score (Table 4). Here, the precision for a
single sentence would be 100% if all the com-
pounds in the output sentence occur in the ref-
erence translation. Compared to the baselines,
the compound precision goes up noticeably under
the HPYLM+c models used in translation, with-
out sacrificing on recall. This suggests that these
models are helping to weed out incorrectly hy-
pothesised compounds.
6.4 Caveats
All results are based on single runs and are there-
fore not entirely robust. In particular, MERT
tuning of the translation model is known to in-
troduce significant variance in translation perfor-
mance across different runs, and the small differ-
ences in BLEU scores reported in Table 3 are very
likely to lie in that region.
Markov chain convergence also needs further
attention. In absence of complex latent struc-
ture (for the dishes), the chain should mix fairly
quickly, and as attested by Figure 2 it ?converges?
with respect to the test metric after about 20 sam-
ples, although the log posterior (not shown) had
not converged after 40. The use of a single poste-
rior sample could also be having a negative effect
on results.
7 Future Directions
The first goal will be to get more robust ex-
perimental results, and to scale up to 4-gram
models estimated on all the available monolin-
gual training data. If good performance can be
demonstrated under those conditions, this gen-
eral approach could pass as a viable alternative to
the current Kneser-Ney dominated state-of-the art
setup in MT.
Much of the power of the HPYLM+c model
has not been exploited in this evaluation, in par-
ticular its ability to score unseen compounds con-
sisting of known components. This feature was
71
0 10 20 30 40
Iteration
420
440
460
480
500
520
540
560
Pe
rpl
exi
ty
KN ?ling
HPYLM
HPYLM+c ?ling
mKN-baseline
Figure 2: Convergence of test set perplexities.
not active in these evaluations, mostly due to the
current phase of implementation. A second area
of focus is thus to modify the decoder to gen-
erate such unseen compounds in translation hy-
potheses. Given the current low compound recall
rates, this could greatly benefit translation quality.
An informal analysis of the reference translations
in the bilingual test set showed that 991 of the
1406 out-of-vocabulary compounds (out of 2692
OOVs in total) fall into this category of unseen-
but-recognisable compounds.
Ultimately the idea is to apply this modelling
approach to other linguistic phenomena as well.
In particular, the objective is to model instances
of concatenative morphology beyond compound-
ing, with the aim of improving translation into
morphologically rich languages. Complex agree-
ment patterns could be captured by condition-
ing functional morphemes in the target word on
morphemes in the n-gram context, or by stem-
ming context words during back-off. Such ad-
ditional back-off paths can be readily encoded in
the Graphical Pitman-Yor process (Wood and Teh,
2009).
These more complex models may require
longer to train. To this end, I intend to use the
single table per dish approximation (?5) to reduce
training to a single deterministic pass through the
data, conjecturing that this will have little effect
on extrinsic performance.
8 Summary
I have argued for further explorations into the
use of a family of hierarchical Bayesian models
for targeting linguistic phenomena that may not
be captured well by standard n-gram language
models. To ground this investigation, I focused
on German compounds and showed how these
models are an appropriate vehicle for encoding
prior linguistic intuitions about such compounds.
The proposed generative model beats the popu-
lar modified Kneser-Ney model in monolingual
evaluations, and preliminarily achieves small im-
provements in translation from English into Ger-
man. In this translation task, single-token Ger-
man compounds traditionally pose challenges to
translation systems, and preliminary results show
a small increase in the F-score accuracy of com-
pounds in the translation output. Finally, I have
outlined the intended steps for expanding this line
of inquiry into other related linguistic phenomena
and for adapting a translation system to get opti-
mal value out of such improved language models.
Acknowledgements
Thanks goes to my supervisor, Phil Blunsom, for
continued support and advice; to Chris Dyer for
suggesting the focus on German compounds and
supplying a freshly trained compound splitter; to
the Rhodes Trust for financial support; and to the
anonymous reviewers for their helpful feedback.
References
Marco Baroni and Johannes Matiasek. 2002. Pre-
dicting the components of German nominal com-
pounds. In ECAI, pages 470?474.
Andre Berton, Pablo Fetter, and Peter Regel-
Brietzmann. 1996. Compound Words in Large-
Vocabulary German Speech Recognition Systems.
In Proceedings of Fourth International Conference
on Spoken Language Processing. ICSLP ?96, vol-
ume 2, pages 1165?1168. IEEE.
Jeff A Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel back-
off. In Proceedings of NAACL-HLT (short papers),
pages 4?6, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stanley F Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical report.
David Chiang. 2007. Hierarchical Phrase-
Based Translation. Computational Linguistics,
33(2):201?228, June.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Se-
tiawan, Vladimir Eidelman, and Philip Resnik.
2010. cdec: A Decoder, Alignment, and Learning
framework for finite-state and context-free trans-
lation models. In Proceedings of the Association
72
for Computational Linguistics (Demonstration ses-
sion), pages 7?12, Uppsala, Sweden. Association
for Computational Linguistics.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of NAACL, pages 406?414. Association for
Computational Linguistics.
John Goldsmith and Tom Reutter. 1998. Automatic
Collection and Analysis of German Compounds. In
F. Busa F. et al, editor, The Computational Treat-
ment of Nominals, pages 61?69. Universite de Mon-
treal, Canada.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Interpolating Between Types and
Tokens by Estimating Power-Law Generators. In
Advances in Neural Information Processing Sys-
tems, Volume 18.
Songfang Huang and Steve Renals. 2007. Hierarchi-
cal Pitman-Yor Language Models For ASR in Meet-
ings. IEEE ASRU, pages 124?129.
Songfang Huang and Steve Renals. 2009. A paral-
lel training algorithm for hierarchical Pitman-Yor
process language models. In Proceedings of Inter-
speech, volume 9, pages 2695?2698.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modelling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and SIgnal Processing, pages
181?184.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL, pages 187?193. Association for Compu-
tational Linguistics.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better Machine Translation Qual-
ity for the German ? English Language Pairs. In
Third Workshop on Statistical Machine Translation,
number June, pages 139?142. Association for Com-
putational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word seg-
mentation with nested Pitman-Yor language mod-
eling. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 1 - ACL-IJCNLP
?09, pages 100?108, Suntec, Singapore. Associa-
tion for Computational Linguistics.
Radford M Neal. 2003. Slice Sampling. The Annals
of Statistics, 31(3):705?741.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a Language
Model from Continuous Speech. In Interspeech,
pages 1053?1056, Chiba, Japan.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, pages 160?167.
Tsuyoshi Okita and Andy Way. 2010. Hierarchical
Pitman-Yor Language Model for Machine Transla-
tion. Proceedings of the International Conference
on Asian Language Processing, pages 245?248.
Kishore Papineni, Salim Roukos, Todd Ward, Wei-
jing Zhu, Thomas J Watson, and Yorktown Heights.
2001. Bleu: A Method for Automatic Evaluation of
Machine Translation. Technical report, IBM.
J Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a sta-
ble subordinator. The Annals of Probability,
25:855?900.
J. Pitman. 2002. Combinatorial stochastic processes.
Technical report, Department of Statistics, Univer-
sity of California at Berkeley.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of German compounds. Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Student Research Workshop, pages 61?69.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the ACL, pages 985?992. Association for
Computational Linguistics.
Frank Wood and Yee Whye Teh. 2009. A Hierarchi-
cal Nonparametric Bayesian Approach to Statistical
Language Model Domain Adaptation. In Proceed-
ings of the 12th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS), pages
607?614, Clearwater Beach, Florida, USA.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
Backoff Models for Machine Translation of Highly
Inflected Languages. In Proceedings of the EACL,
pages 41?48.
73
