Proceedings of the Workshop on Linguistic Distances, pages 109?116,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Total rank distance and scaled total rank distance:
two alternative metrics in computational linguistics
Anca Dinu
University of Bucharest,
Faculty of Foreign Languages/
Edgar Quinet 17,
Bucharest, Romania
anca d dinu@yahoo.com
Liviu P. Dinu
University of Bucharest, Faculty of
Mathematics and Computer Science/
Academiei 14, 010014,
Bucharest, Romania
ldinu@funinf.cs.unibuc.ro
Abstract
In this paper we propose two metrics to be
used in various fields of computational lin-
guistics area. Our construction is based on
the supposition that in most of the natural
languages the most important information
is carried by the first part of the unit. We
introduce total rank distance and scaled to-
tal rank distance, we prove that they are
metrics and investigate their max and ex-
pected values. Finally, a short application
is presented: we investigate the similarity
of Romance languages by computing the
scaled total rank distance between the di-
gram rankings of each language.
1 Introduction
Decision taking processes are common and fre-
quent tasks for most of us in our daily life.
The ideal case would be that when the decisions
can be taken deterministically, based on some
clear, quantifiable and unambiguous parameters
and classifiers. However, there are many cases
when we decide based on subjective or sensor-
ial criteria (e.g. perceptions), but which prove to
function well. The domains in which decisions are
taken based on perceptions vary a lot: the quali-
tative evaluation of services, management, finan-
cial predictions, sociology, information/intelligent
systems, etc (Zadeh and Kacprzyk, 1999).
When people are asked to approximate the
height of some individual, they prefer to use terms
like: very tall, rather tall, tall enough, short, etc.
We can expect the same linguistic variable to have
a different metrical correspondence according to
the community to which the individual belongs
(i.e. an individual of 170 cm can be considered
short by the Australian soldiers and tall by the Es-
kimos). Similar situations also arise when people
are asked to hierarchically order a list of objects.
For example, we find it easy to make the top of
the best five novels that we read, since number one
is the novel that we like best and so on, rather than
to say that we liked in the proportion of 40% the
novel on the first position, 20 % the novel on the
second place and so on. The same thing is happen-
ing when we try to talk about the style of a certain
author: it is easier to say that the author x is closer
to y than z, then to quantify the distance between
their styles. In both cases we operate with a ?hid-
den variable? and a ?hidden metric?.
Especially when working with perceptions, but
not only, we face the situation to operate with
strings of objects where the essential information
is not given by the numerical value of some para-
meter of each object, but by the position the object
occupies in the strings (according to a natural hier-
archical order, in which on the first place we find
the most important element, on the second place
the next one and on the last position the least im-
portant element).
As in the case of perceptions calculus, in most
of the natural languages, the most important infor-
mation is also carried by the first part of the unit
(Marcus, 1974). Cf. M. Dinu (1997), it is advis-
able that the essential elements of a message to be
situated in the first part of the utterance, thus hav-
ing the best chances to be memorized1 (see Table
1).
Based on the remark that in most of the natural
1On the contrary, M. Dinu notices that at the other end, we
find the wooden language from the communist period, text
that was not meant to inform, but to confuse the receiver with
an incantation empty of content, and that used the reversed
process: to place the important information at the end of very
long phrases that started with irrelevant information
109
The length Memorized words (%)
of the phrase all first half second half
12 100 % 100 % 100 %
13 90 % 95 % 85 %
17 70 % 90% 50%
24 50 % 70 % 30 %
40 30 % 50 % 10 %
Table 1: The percentage of memorized words from
phrases
languages the most important information is car-
ried out by the first part of the unit, in this paper
we introduce two metrics: total rank distance and
scaled total rank distance.
Some preliminary and motivations are given in
Section 2. In Section 3 we introduce total rank dis-
tance; we prove that it is a metric (Section 3.1), we
investigate its max and expected values (Section
3.2) and its behavior regarding the median ranking
problem (Section 3.3). An extension for strings is
proposed in Section 4. Scaled total rank distance
is introduced in Section 4, where we prove that it
is a metric and we investigate its max and expected
values. In Section 6 a short application is pre-
sented: we investigate the similarity of Romance
languages by computing the scaled total rank dis-
tance between the digram rankings of each lan-
guage. Section 7 is reserved to conclusions, while
in Section 8 we give a mathematically addendum
where we present the proofs of the statements.
2 Rank distance
By analogy to computing with words, natural lan-
guage and genomics, we can say that if the differ-
ences between two strings are at the top (i.e., in
essential points), the distance has to have a bigger
value then when the differences are at the bottom
of the strings.
On the other hand, many of the similarity mea-
sures used today (edit distance, Hamming distance
etc.) do not take into account the natural tendency
of the objects to place the most important informa-
tion in the first part of the message.
This was the motivation we had in mind when
we proposed Rank distance (Dinu, 2003) as an al-
ternative similarity measure in computational lin-
guistics. This distance had already been suc-
cessfully used in computational linguistics, in
such problems as the similarity of Romance lan-
guages (Dinu and Dinu, 2005), or in bioinformat-
ics (in DNA sequence comparision problem, Dinu
and Sgarro).
2.1 Preliminaries and definitions
To measure the distance between two strings, we
use the following strategy: we scan (from left to
right) both strings and for each letter from the first
string we count the number of elements between
its position in first string and the position of its
first occurrence in the second string. We sum these
scores for all elements and obtain the rank dis-
tance. Clearly, the rank distance gives a score zero
only to letters which are in the same position in
both strings, as Hamming distance does (we recall
that Hamming distance is the number of positions
where two strings of the same length differ).
On the other hand, the reduced sensitivity of
the rank distance w.r.t. deletions and insertions
is of paramount importance, since it allows us to
make use of ad hoc extensions to arbitrary strings,
such as its low computational complexity is not
affected. This is not the case for the extensions
of the Hamming distance, mathematically optimal
but computationally heavy, which lead to the edit-
distance, or Levenshtein distance, and which are at
the base of the standard alignment principle. So,
rank distance sides with Hamming distance rather
than Levenshtein distance as far as computational
complexity is concerned: the fact that in the Ham-
ming and in the rank case the median string prob-
lem is tractable (Dinu and Manea), while in the
edit case it is is NP-hard (Higuera and Casacu-
berta, 2000), is a very significant indicator.
The rank distance is an ordinal distance tightly
related to the so-called Spearman?s footrule (Di-
aconis and Graham, 1977) 2, which has long been
used in non-parametric statistics. Unlike other or-
dinal distances, the Spearman?s footrule is linear
in n, and so very easy to compute. Its average
value is at two-thirds of the way to the maximum
value (both are quadratics in n); this is because,
in a way, the Spearman footrule becomes rather
?undiscriminating? for highly different orderings.
Rank distance has the same drawbacks and the
same advantages of Spearman?s foootrule. As for
?classical? ordinal distances for integers, with av-
erages values, maximal values, etc., the reader is
2Both Spearman?s footrules and binary Hamming dis-
tances are a special case of a well-known metric distance
called sometimes taxi distance, which is known to be equiv-
alent to the usual Euclidian distance. Computationally, taxi
distance is obviously linear.
110
referred to the basic work (Diaconis and Graham,
1977).
Let us go back to strings. Let us choose a fi-
nite alphabet, say {N,V,A,O} (Noun, Verb, Ad-
jective, Object) and two strings on that alphabet,
which for the moment will be constrained to be a
permutation of each other. E.g. take two strings
of length 6: NNV AOO and V OANON ; put
indexes for the occurrences of repeated letters in
increasing order to obtain N1N2V1A1O1O2 and
V1O1A1N1O2N2. Now, proceed as follows: in
the first sequence N1 is in position 1, while it is in
position 4 in the second sequence, and so the dif-
ference is 3; compute the difference in positions
for all letters and sum them. In this case the dif-
ferences are 3, 4, 2, 1, 3, 1 and so the distance is
14. Even if the computation of the rank distance
as based directly on its definition may appear to
be quadratic, in (Dinu and Sgarro) two algorithms
which take it back to linear complexity are exhibit.
In computational linguistics the rank distance
for strings without repetitions had been enough. In
a way, indexing converts a sequence with repeti-
tions into a sequence without repetitions, in which
the k occurrence of a letter a are replaced by sin-
gle occurrences of the k indexed letters a1, a2, . . .,
ak. Let u = x1x2 . . . xn and v = y1y2 . . . ym be
two strings of lengths n and m, respectively. For
an element xi ? u we define its order or rank by
ord(xi|u) = n+1?i: we stress that the rank of xi
is its position in the string, counted from the right
to the left, after indexing, so that for example the
second O in the string V OANON has rank 2.
Note that some (indexed) occurrences appear in
both strings, while some other are unmatched, i.e.
they appear only in one of the two strings. In de-
finition (1) the last two summations refer to these
unmatched occurrences. More precisely, the first
summation on x ? u ? v refers to occurrences x
which are common to both strings u and v, the sec-
ond summation on x ? u \ v refers to occurrences
x which appear in u but not in v, while the third
summation on x ? v \ u refers to occurrences x
which appear in v but not in u.
Definition 1 The rank distance between two
strings without repetitions u and v is given by:
?(u, v) = ?
x?u?v
|ord(x|u)? ord(x|v)|+
+ ?
x?u\v
ord(x|u) + ?
x?v\u
ord(x|v) (1)
Example 1 1. Let u = abcde and v = beaf be
two strings without repetitions. ?(u, v) =
|ord(a|u) ? ord(a|v)| + |ord(b|u) ?
ord(b|v)| + |ord(e|u) ? ord(e|v)| +
ord(c|u) + ord(d|u) + ord(f |v) =
3 + 0 + 2 + 3 + 2 + 1 = 11.
2. Let w1 = abbab and w2 = abbbac be two
strings with repetitions. Their corresponding
indexed strings will be: w1 = a1b1b2a2b3
and w2 = a1b1b2b3a2c1, respectively. So,
?(w1, w2) = ?(w1, w2) = 8.
Remark 1 The ad hoc nature of the rank distance
resides in the last two summations in (1), where
one compensates for unmatched letters, i.e. in-
dexed letters which appear only in one of the two
strings.
Deletions and insertions are less worrying in the
rank case rather than in the Hamming case: if one
incorrectly moves a symbol by, say, one position,
the Hamming distance loses any track of it, but
rank distance does not, and the mistake is quite
light. So, generalizations in the spirit of the edit
distance are unavoidable in the Hamming case,
even if they are computationally very demanding,
while in the rank case we may think of ad hoc
ways-out, which are computationally convenient.
3 Total Rank Distance
We remind that one of the goals of introducing
rank distance was to obtain a tool for measuring
the distance between two strings which is more
sensitive to the differences encountered in the be-
ginning of the strings than in the ending.
Rank distance satisfies in a good measure the
upper requirement (for example it penalizes more
heavily unmatched letters in the initial part of
strings), but some black points are yet remaining.
One of them is that rank distance is invariant to the
transpositions on a given length.
The following example is eloquent:
Example 2 1. Let a = (1, 2, 3, 4, 5), b =
(2, 1, 3, 4, 5), c = (1, 2, 4, 3, 5) and d =
(1, 2, 3, 5, 4) be four permutations. Rank dis-
tance between a and each of b, c or d is the
same, 2.
2. The same is happening with
a = (1, 2, 3, 4, 5, 6, 7, 8) and
b = (3, 2, 1, 4, 5, 6, 7, 8), c =
(1, 4, 3, 2, 5, 6, 7, 8), or d =
(1, 2, 3, 4, 5, 8, 7, 6) (here rank distance
is equal to 4).
111
In the following we will repair this inconve-
nient, by introducing the Total Rank Distance, a
measure which gives us a more comprehensive in-
formation (compared to rank distance) about the
two strings which we compare.
Since in many situations occurred in computa-
tional linguistics, the similarity for strings with-
out repetitions had been enough, in the following
we introduce first a metric between rankings3 and
then we generalize it to strings.
3.1 Total rank distance on permutations
Let A and B be two rankings over the same uni-
verse U , having the same length, n. Without loss
of generality, we suppose that U = {1, 2, . . . ,m}.
For each 1 ? i ? n we define the function ? by:
?(i) def= ?(Ai, Bi). (2)
where Ai and Bi are the partial rankings of length
i obtained from the initial rankings by deleting the
elements below position i (i.e. the top i rankings).
Definition 2 Let A and B be two rankings with
the same length over the same universe, U . The
Total Rank Distance between A and B is given by:
D(A,B) =
n?
i=1
?(i) =
n?
i=1
?(Ai, Bi).
Example 3 1. Let a, b, c and d be the four per-
mutations from Example 2, item 1. The total
rank distance between a and each of b, c, d
is: D(a, b) = 10, D(a, c) = 6, D(a, d) = 4.
2. The visible differences are also in the item 2
of the upper example if we apply total rank
distance: D(a, b) = 30, D(a, c) = 28,
D(a, d) = 10.
3A ranking is an ordered list of objects. Every ranking
can be considered as being produced by applying an order-
ing criterion to a given set of objects. More formally, let U
be a finite set of objects, called the universe of objects. We
assume, without loss of generality, that U = {1, 2, . . . , |U |}
(where by |U | we denote the cardinality of U ). A ranking
over U is an ordered list: ? = (x1 > x2 > . . . > xd),
where {x1, . . . , xd} ? U , and > is a strict ordering rela-
tion on {x1, . . . , xd}, (an ordering criterion. It is important
to point the fact that xi 6= xj if i 6= j. For a given object
i ? U present in ? , ?(i) represents the position (or rank) of i
in ? . If the ranking ? contains all the elements of U , than it is
called a full ranking. It is obvious that all full rankings repre-
sent all total orderings of U (the same as the permutations of
U ). However, there are situations when some objects cannot
be ranked by a given criterion: the ranking ? contains only
a subset of elements from the unverse U . Then, ? is called
partial ranking. We denote the set of elements in the list ?
with the same symbol as the list.
The following theorem states that our terminol-
ogy total rank distance is an adequate one:
Theorem 1 Total rank distance is a metric.
Proof:
It is easy to see that D(A,B) = D(B,A).
We prove that D(A,B) = 0 iff A = B. If
D(A,B) = 0, then ?(Ai, Bi) = 0 for each
1 ? i ? n (since ? is a metric, so a nonnega-
tive number), so ?(An, Bn) = ?(A,B) = 0, so
A = B.
For the triangle inequality we have: D(A,B)+
D(B,C) =
n?
i=1
?(Ai, Bi) +
n?
i=1
?(Bi, Ci)
=
n?
i=1
(?(Ai, Bi) + ?(Bi, Ci))
?
n?
i=1
?(Ai, Ci) = D(A,C). uunionsq
3.2 Expected and max values of the total
rank distance
Let Sn be the group of all permutations of length
n and let A, B be two permutations from Sn. We
investigate the max total rank distance between A
and B and the average total rank distance between
A and B.
Proposition 1 Under the upper hypothesis, the
expected value of the total rank distance between
A and B is:
E(D) = (n
2 ? 1)(n+ 2)
6 .
Proposition 2 Under the same hypothesis as in
the previous proposition, the max total rank dis-
tance between two permutations from Sn is:
max
A,B?Sn
D(A,B) = n
2(n+ 2)
4
and it is achieved when a permutation is the re-
verse of the other one.
3.3 On the aggregation problem via total
rank distance
Rank aggregation is the problem of combining
several ranked lists of objects in a robust way to
produce a single ranking of objects.
One of the most natural way to solve the aggre-
gation problem is to determine the median (some-
times called geometric median) of ranked lists via
a particular measure.
Given a multiset T of ranked lists, a median of
T is a list L such that
112
d(L, T ) = min
X
d(X,T ),
where d is a metric and X is a ranked list over
the universe of T .
Depending on the choice of measure d, the up-
per problem may contain many unpleasant sur-
prises. One of them is that computing the median
set is NP-complete for some usual measure (in-
cluding edit-distance or Kendal distance) even for
binary universe.
We will show in the following that the median
aggregation problem via Total rank distance can
be computed in polynomial time.
Theorem 2 Given a multiset T of full ranked lists
over the same universe, the median of T via total
rank distance can be computed in polynomial time,
namely proportional to the time to find a minimum
cost perfect matching in a bipartite graph.
Proof: Without loss of generality, we suppose
that the universe of lists is U = {1, 2, . . . , n}.
We define a weighted complete bipartite graph
G = (N,P,W ) as follows. The first set of nodes
N = {1, 2, . . . , n} denotes the set of elements to
be ranked in a full list. The second set of nodes
P = {1, 2, . . . , n} denotes the n available posi-
tions. The weight W (i, j) is the contribution, via
total rank distance, of node i to be ranked on place
j in a certain ranking.
We can give a close formula for computing the
weights W (i, j) and this ends the proof, because
we reduced the problem to the solving of the mini-
mum cost maximum matching problem on the up-
per bipartite graph ((Fukuda and Matsui, 1994),
(Fukuda and Matsui, 1992), (Dinu and Manea)).
uunionsq
4 An extension to strings of total rank
distance
We can extend total rank distance to strings.
Similar to the extensions of rank distance to
strings, we index each letter in a word with the
number of its previous occurrences.
First, we extent the total rank distance to rank-
ings with unequal lengths as it follows:
Definition 3 Let u and v be two rankings of length
|u| and |v|, respectively. We can assume that |u| <
|v|. The total rank distance between u and v is
defined by:
D(u, v) =
|u|?
i=1
?(vi, ui) +
|v|?
i=|u|+1
?(vi, u).
Theorem 3 The total rank distance between two
rankings with unequal lengths is a metric.
To extent the total rank distance to strings,
firstly we index both strings and than we apply
the upper definition to the newly obtained strings
(which are now rankings).
Example 4 Let u = aabca, v = aab and w =
bca be three strings. We obtained the following
results:
1. Rank distance: ?(u, v) =
?(a1a2b1c1a3, a1a2b1) = 9 and
?(u,w) = ?(a1a2b1c1a3, b1c2a1) = 9;
2. Total rank distance: D(u, v) =
D(a1a2b1c1a3, a1a2b1) = 13 and
D(u,w) = D(a1a2b1c1a3, b1c2a1) = 33.
What happens in item 1 is a consequence of a
general property of rank distance which states that
?(uv, u) = ?(uv, v), for any nonempty strings u
and v.
Total rank distance repairs this fact, as we can
see from item 2; we observe that the total rank
distance is more sensitive than rank distance to the
differences from the first part of strings.
5 Scaled Total Rank Distance
We use the same ideas from Total rank distance,
but we normalize each partial distance. To do this,
we divide each rank distance between two partial
rankings of length i by i(i+1), which is the max-
imal distance between two rankings of length i
(it corresponds to the case when the two rankings
have no common elements).
Definition 4 The Scaled Total Rank distance be-
tween two rankings A and B of length n is:
S(A,B) =
n?
i=1
?(Ai, Bi)
i(i+ 1) .
Theorem 4 Scaled total rank distance is a metric.
Proof: The proof is similar to the one from the
total rank distance. uunionsq
Remark 2 It is easy to see that S(A,B) ?
H(A,B), where H(A,B) is the Hamming dis-
tance.
113
Example 5 Let A = (a, b, c, d, e), B =
(b, a, c, d, e) and C = (a, b, d, e, c) be three per-
mutations. We have the following values for ?, D
and S, respectively:
1. Rank distance: ?(A,B) = 2, ?(A,C) = 4, so
?(A,B) < ?(A,C).
2. Total Rank Distance: D(A,B) = 2 + 2 + 2 +
2 + 2 = 10, D(A,C) = 0 + 0 + 2 + 4 + 4 = 10,
so D(A,B) = D(A,C).
3. Scaled Total Rank Distance: S(A,B) = 22+ 26+2
12 + 220 + 230 = 53 , S(A,C) = 02 + 06 + 212 + 420 +4
30 = 12 , so S(A,B) > S(A,C).
It is not hard to see that S(A,B) ? n, so we can
normalize scaled total rank distance by dividing it
to n.
We obtained the following two values for max
and average values of scaled total rank distance:
Proposition 3
1. If n ??, then max
A,B?Sn
1
nS(A,B) = 72 ? 4 ln 2.
2. The average value of scaled total rank distance
is: E(S) = 2(n?1)3 . When n ??, E(S)n ? 23 .
Remark 3 It is a nice exercise to show that 72 ?
4 ln 2 ? 1.
Proof: 72 ? 4 ln 2 ? 1 iff 1 ? 4(ln 4 ? 1).
But 4(ln 4 ? 1) > 4(ln 4 ? ln 3). From La-
grange Theorem, there is 3 < ? < 4 such that
ln 4 ? ln 3 = 1? , so 4(ln 4 ? ln 3) = 4? > 1, so
4(ln 4? 1) > 4(ln 4? ln 3) > 1. uunionsq
6 Application
We present here a short experiment regarding the
similarity of Romance languages. The work cor-
pus is formed by the representative vocabularies of
the following six Romance languages: Romanian,
Italian, Spanish, Catalan, French and Portuguese
languages (Sala, 1988). We extracted the digrams
from each vocabularies and then we constructed a
ranking of digrams for each language: on the first
position we put the most frequent digram of the
vocabulary, on the second position the next fre-
quent digram, and so on.
We apply the scaled total rank distance between
all pairs of such classifications and we obtain a se-
ries of results which are presented in Table 2.
Some remarks are immediate:
? If we analyze the Table 2, we observe
that every time Romanian finds itself at the
biggest distance from the other languages.
Table 2: Scaled total rank distances in Romance
languages
Ro It Sp Ca Po Fr
Ro 0 0.36 0.37 0.39 0.41 0.36
It 0.36 0 0.21 0.24 0.26 0.30
Sp 0.37 0.21 0 0.20 0.18 0.27
Ca 0.39 0.24 0.20 0 0.20 0.28
Po 0.41 0.26 0.18 0.20 0 0.30
Fr 0.36 0.30 0.27 0.28 0.30 0
This fact proves that the evolution of Ro-
manian in a distanced space from the Latin
nucleus has lead to bigger differences be-
tween Romanian and the rest of the Romance
languages, then the differences between any
other two Romance languages.
? The closest two languages are Portuguese
and Spanish.
? It is also remarkable that Catalan is equally
distanced from Portuguese and Spanish.
The upper remarks are in concordance with the
conclusions of (Dinu and Dinu, 2005) obtained
from the analise of the syllabic similarity of the
Romance languages, where the rank distance was
used to compare the rankings of syllables, based
on the frequency of syllables for each language.
During the time, different comparing methods
for natural languages were proposed. We mention
here the work of Hoppenbrouwers and Hoppen-
brouwers (2001). Their approach was the follow-
ing: using the letter frequency method for each
language variety the unigram frequencies of let-
ters are found on the basis of a corpus. The dis-
tance between two languages is equal to the sum
of the differences between the corresponding letter
frequencies. They verify that this approach cor-
rectly shows that the distance between Afrikaans
and Dutch is smaller than the distance between
Afrikaans and the Samoan language.
7 Conclusions
In this paper we provided some low-complexity
metrics to be used in various subfields of computa-
tional linguistics: total rank distance and scaled to-
tal rank distance. These metrics are inspired from
the natural tendency of objects to put the main in-
formation in the first part of the units. Our ana-
lyze was especially concentrated on the mathemat-
114
ical and computational properties of these metrics:
we showed that total rank distance and scaled to-
tal rank distance are metrics, computed their ex-
pected and max values on the permutations group
and showed that total rank distance can be used in
classification problem via a polynomial algorithm.
8 Mathematical addendum
This addendum may be skipped by readers who
are not interested in mathematical technicalities;
below some statements are sketched and other are
unproved, but then the proofs are quite straightfor-
ward.
Proposition 1:
Proof: It is not hard to see that D(A,Sn) =
D(B,Sn) for any two permutation A,B ? Sn.
So, the expected value can be computed by com-
puting first D(A,Sn) for a convenable permuta-
tion and then by dividing the upper sum to n!. If
we choose A = en (i.e. the identical permutation
of the group Sn), then the expected value is:
E(D) = 1n!
?
??Sn
D(en, ?).
The upper sum can be easily computed if we take
into account the fact that each number 1, 2, . . . , n
appears the same number of times (i.e. (n-1)!) on
the ranks 1, 2, . . . n. So, we obtain that the ex-
pected value is equal to:
E(D) = (n
2 ? 1)(n+ 2)
6 .
uunionsq
Proposition 2:
Proof: W.l.g. we can suppose that first permu-
tation is the identical one, i.e. en (otherwise we
will relabelled it). To compute the max value, the
following preliminary results must be proven (we
skipped the proofs).
We say that an integer from ? is low if its posi-
tion is ? n2 and it is high if its position is > n2 .
Let ? ? Sn be a permutation. We construct the
set ?? as following:
?? = {? ? Sn | ?x ? {1 . . . n}, x is low in ?
iff x is high in ? and viceversa}
Result 1 For each ? ? Sn and every two permu-
tation ?, pi in ?? we have: D(?, ?) = D(?, pi).
Result 2 For each ? ? Sn and every two permu-
tation ?, pi such that pi ? ?? and ? /? ??, we
have: D(?, ?) < D(?, pi).
To prove Result 2 we use the following Lemma:
Lemma 1 (Dinu, 2003) If a > b, then the func-
tion f(x) = |x ? b| ? |x ? a| is an increasing
one.
Result 3 Let ? ? Sn be a permutation. The max-
imum total rank distance is reached by the per-
mutation ? where ord(x|?) = n + 1 ? ord(x|?),
?x ? V (Pn). Under this conditions the maximum
total rank distance is:
max
A,B?Sn
D(A,B) = n
2(n+ 2)
4 (3)
In other words, we obtained a more general re-
sult:
Theorem 5 For a given permutation ?, the maxi-
mum rank distance is achieved by all permutations
from ?? and it is equal to (3).
uunionsq
Proposition 3:
Proof:
1. Similar to Proposition 2, given a permutation
? ? Sn, the max value is reached by its in-
vert. So, to give a close formula for the max
value it is enough to compute S(en, e?1n ). To
make easier our life, we can suppose that
n = 2k.
S(en, e?1n ) = k +
?k
i=1
2i2+(k?i)(k?i+1)
(k+i)(k+i+1) =
. . . = 4k ? 2k22k+1 ? 2(4k + 1)(
?k
i=1
1
k+i ?
k
2k+1);
When k ? ?, ?ki=1 1k+i ? ln 2, so
S(en,e?1n )
n = 72 ? 4 ln 2 uunionsq
2. To compute the expected value we use the
same motivation as in expected total rank dis-
tance. The rest is obvious.
Acknowledgements 1 We want to thank to re-
viewers for their comments and suggestions. Re-
search supported by CNR-NATO and MEdC-
ANCS.
References
P. Diaconis, R.L. Graham, 1977. Spearman footrule as
a Measure of Disarray, Journal of Royal Statistical
Society. Series B (Methodological), Vol. 39, No. 2,
262-268.
115
L. P. Dinu, 2003. On the classification and aggregation
of hierarchies with different constitutive elements,
Fundamenta Informaticae, 55(1), 39-50.
A. Dinu, L.P. Dinu, 2005. On the Syllabic Similari-
ties of Romance Languages. In Proc. CICLing 2005,
Lecture Notes in Computer Science, Volume 3406,
pp. 785-789.
L.P. Dinu, F. Manea. An efficient approach for the rank
aggregation problem. Theoretical Computer Science
(to appear).
L.P. Dinu, A. Sgarro. A low-complexity distance for
DNA strings, Fundamenta Informaticae (to appear).
M. Dinu, 1997. Comunicarea (in Romanian). Ed.
S?tiint?ifica?, Bucures?ti.
K. Fukuda, T. Matsui, 1992. Finding all minimum cost
perfect matchings in bipartite graphs, Networks, 22,
461-468.
K. Fukuda, T. Matsui, 1994. Finding all the perfect
matchings in bipartite graphs, Appl. Math. Lett.,
7(1), 15-18.
C. de la Higuera, F. Casacuberta, 2000. Topology of
strings: Median string is NP- complete, Theoretical
Computer Science, 230:39-48.
C. Hoppenbrouwers, G. Hoppenbrouwers, 2001. De
indeling van de Nederlandse streektalen. Dialecten
van 156 steden en dorpen geklasseerdvolgens de
FFM. Koninklijke Van Gorcum, Assen.
S. Marcus, 1974. Linguistic structures and generative
devices in molecular genetics. Cahiers Ling. Theor.
Appl., 11, 77-104.
M. Sala, (coord.) 1982. Vocabularul reprezentativ al
limbilor romanice, Bucures?ti.
L.A. Zadeh, J. Kacprzyk, 1999. Computing with words
in information/intelligent systems 1: Foundations, 2:
Application. Physica-Verlag, Heidelberg and New
York.
116
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 64?68,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Predicting Romanian Stress Assignment
Alina Maria Ciobanu
1,2
, Anca Dinu
1,3
, Liviu P. Dinu
1,2
1
Center for Computational Linguistics, University of Bucharest
2
Faculty of Mathematics and Computer Science, University of Bucharest
3
Faculty of Foreign Languages and Literatures, University of Bucharest
alina.ciobanu@my.fmi.unibuc.ro, anca_d_dinu@yahoo.com, ldinu@fmi.unibuc.ro
Abstract
We train and evaluate two models for Ro-
manian stress prediction: a baseline model
which employs the consonant-vowel struc-
ture of the words and a cascaded model
with averaged perceptron training con-
sisting of two sequential models ? one for
predicting syllable boundaries and another
one for predicting stress placement. We
show in this paper that Romanian stress is
predictable, though not deterministic, by
using data-driven machine learning tech-
niques.
1 Introduction
Romanian is a highly inflected language with a
rich morphology. As dictionaries usually fail to
cover the pronunciation aspects for all word forms
in languages with such a rich and irregular mor-
phology (Sef et al., 2002), we believe that a
data-driven approach is very suitable for syllabi-
cation and stress prediction for Romanian words.
Moreover, such a system proves extremely useful
for inferring syllabication and stress placement for
out-of-vocabulary words, for instance neologisms
or words which recently entered the language.
Even if they are closely related, Romanian
stress and syllabication were unevenly studied in
the computational linguistic literature, i.e., the
Romanian syllable received much more attention
than the Romanian stress (Dinu and Dinu, 2005;
Dinu, 2003; Dinu et al., 2013; Toma et al., 2009).
One possible explanation for the fact that Roma-
nian syllabication was more intensively studied
than Romanian stress is the immediate application
of syllabication to text editors which need reliable
hyphenation. Another explanation could be that
most linguists (most recently Dindelegan (2013))
insisted that Romanian stress is not predictable,
thus discouraging attempts to investigate any sys-
tematic patterns.
Romanian is indeed a challenging case study,
because of the obvious complexities of the data
with respect to stress assignment. At first sight, no
obvious patterns emerge for learning stress place-
ment (Dindelegan, 2013), other than as part of in-
dividual lexical items. The first author who chal-
lenges this view is Chitoran (2002), who argues in
favor of the predictability of the Romanian stress
system. She states that stress placement strongly
depends on the morphology of the language, more
precisely on the distribution of the lexical items
based on their part of speech (Chitoran, 1996).
Thus, considering this type of information, lexical
items can be clustered in a limited number of re-
gular subpatterns and the unpredictability of stress
placement is significantly reduced. A rule-based
method for lexical stress prediction on Romanian
was introduced by Oancea and Badulescu (2002).
Dou et al. (2009) address lexical stress predic-
tion as a sequence tagging problem, which proves
to be an accurate approach for this task. The
effectiveness of using conditional random fields
for orthographic syllabication is investigated by
Trogkanis and Elkan (2010), who employ them
for determining syllable boundaries and show that
they outperform previous methods. Bartlett et
al. (2008) use a discriminative tagger for auto-
matic orthographic syllabication and present seve-
ral approaches for assigning labels, including the
language-independent Numbered NB tag scheme,
which labels each letter with a value equal to the
distance between the letter and the last syllable
boundary. According to Damper et al. (1999), syl-
lable structure and stress pattern are very useful in
text-to-speech synthesis, as they provide valuable
knowledge regarding the pronunciation modeling.
Besides converting the letters to the corresponding
phonemes, information about syllable boundaries
and stress placement is also needed for the correct
synthesizing of a word in grapheme-to-phoneme
conversion (Demberg et al., 2007).
64
In this paper, we rely on the assumption that the
stress system of Romanian is predictable. We pro-
pose a system for automatic prediction of stress
placement and we investigate its performance by
accounting for several fine-grained characteristics
of Romanian words: part of speech, number of
syllables and consecutive vowels. We investigate
the consonant-vowel structure of the words (C/V
structure) and we detect a high number of stress
patterns. This calls for the need of machine learn-
ing techniques, in order to automatically learn
such a wide range of variational patterns.
2 Approach
We address the task of stress prediction for Roma-
nian words (out-of-context) as a sequence tagging
problem. In this paper, we account only for the pri-
mary stress, but this approach allows further deve-
lopment in order to account for secondary stress
as well. We propose a cascaded model consist-
ing of two sequential models trained separately,
the output of the first being used as input for the
second. We use averaged perceptron for parame-
ter estimation and three types of features which are
described in detail further in this section: n-grams
of characters, n-grams marking the C/V structure
of the word and binary positional indicators of the
current character with respect to the syllable struc-
ture of the word. We use one sequential model
to predict syllable boundaries and another one to
predict stress placement. Previous work on or-
thographic syllabication for Romanian (Dinu et
al., 2013) shows that, although a rule-based algo-
rithm models complex interactions between fea-
tures, its practicality is limited. The authors re-
port experiments on a Romanian dataset, where
the rule-based algorithm is outperformed by an
SVM classifier and a CRF system with character
n-gram features.
We use a simple tagging structure for mar-
king primary stress. The stressed vowel re-
ceives the positive tag 1, while all previous cha-
racters are tagged 0 and all subsequent ones
2. This structure helps enforce the uniqueness
of the positive tag. The main features used
are character n-grams up to n = W in a win-
dow of radius W around the current position.
For example, if W = 2, the feature template
consists of c[-2], c[-1], c[0], c[1], c[2],
c[-2:-1], c[-1:0], c[0:1], c[1:2]. If the
current letter is the fourth of the word dinosaur,
o, the feature values would be i, n, o, s, a, in, no,
os, sa. We use two additional types of features:
? features regarding the C/V structure of the
word: n-grams using, instead of characters,
markers for consonants (C) and vowels (V);
? binary indicators of the following positional
statements about the current character, re-
lated to the statistics reported in Table 1:
? exactly before/after a split;
? in the first/second/third/fourth syllable
of the word, counting from left to right;
? in the first/second/third/fourth syllable
of the word, counting from right to left
The syllabication prediction is performed with
another sequential model of length n? 1, where
each node corresponds to a position between two
characters. Based on experimenting and previ-
ous work, we adopted the Numbered NB labeling.
Each position is labeled with an integer denoting
the distance from the previous boundary. For ex-
ample, for the word diamond, the syllable (above)
and stress annotations (below) are as follows:
d i a m o n d
1 0 0 1 2 3
0 1 2 2 2 2 2
The features used for syllabication are based on
the same principle, but because the positions are
in-between characters, the window of radius W
has length 2W instead of 2W + 1. For this model
we used only character n-grams as features.
3 Data
We run our experiments for Romanian using the
RoSyllabiDict (Barbu, 2008) dictionary, which is
a dataset of annotated words comprising 525,528
inflected forms for approximately 65,000 lemmas.
This is, to our best knowledge, the largest experi-
ment conducted and reported for Romanian so far.
For each entry, the syllabication and the stressed
vowel (and, in case of ambiguities, also grammat-
ical information or type of syllabication) are pro-
vided. For example, the word copii (children) has
the following representation:
<form w="copii" obs="s."> co-p?i</form>
We investigate stress placement with regard to
the syllable structure and we provide in Table 1
the percentages of words having the stress placed
on different positions, counting syllables from the
beginning and from the end of the words as well.
For our experiments, we discard words which
do not have the stressed vowel marked, compound
65
Syllable %words
1
st
5.59
2
nd
18.91
3
rd
39.23
4
th
23.68
5
th
8.52
(a) counting syllables from
the beginning of the word
Syllable %words
1
st
28.16
2
nd
43.93
3
rd
24.14
4
th
3.08
5
th
0.24
(b) counting syllables from
the end of the word
Table 1: Stress placement for RoSyllabiDict
words having more than one stressed vowel and
ambiguous words (either regarding their part of
speech or type of syllabication).
We investigate the C/V structure of the words in
RoSyllabiDict using raw data, i.e., a, a?, ?, e, i, ?, o,
u are always considered vowels and the rest of the
letters in the Romanian alphabet are considered
consonants. Thus, we identify a very large number
of C/V structures, most of which are not determin-
istic with regard to stress assignment, having more
then one choice for placing the stress
1
.
4 Experiments and Results
In this section we present the main results drawn
from our research on Romanian stress assignment.
4.1 Experiments
We train and evaluate a cascaded model consist-
ing of two sequential models trained separately,
the output of the first being used as input to the
second. We split the dataset in two subsets: train
set (on which we perform cross-validation to se-
lect optimal parameters for our model) and test
set (with unseen words, on which we evaluate the
performance of our system). We use the same
train/test sets for the two sequential models, but
they are trained independently. The output of the
first model (used for predicting syllabication) is
used for determining feature values for the second
one (used for predicting stress placement) for the
test set. The second model is trained using gold
syllabication (provided in the dataset) and we re-
port results on the test set in both versions: us-
ing gold syllabication to determine feature values
1
For example, for CCV-CVC structure (1,390 occurrences
in our dataset) there are 2 associated stress patterns: CCV-
CVC (1,017 occurrences) and CCV-CVC (373 occurrences).
Words with 6 syllables cover the highest number of distinct
C/V structures (5,749). There are 31 C/V structures (rang-
ing from 4 to 7 syllables) reaching the maximum number of
distinct associated stress patterns (6).
and using predicted syllabication to determine fea-
ture values. The results with gold syllabication
are reported only for providing an upper bound for
learning and for comparison.
We use averaged perceptron training (Collins,
2002) from CRFsuite (Okazaki, 2007). For the
stress prediction model we optimize hyperparam-
eters using grid search to maximize the 3-fold
cross-validation F
1
score of class 1, which marks
the stressed vowels. We searched over {2,3,4}
for W and over {1,5,10,25,50} for the maximum
number of iterations. The values which optimize
the system are 4 for W and 50 for the maximum
number of iterations. We investigate, during grid
search, whether employing C/V markers and bi-
nary positional indicators improve our system?s
performance. It turns out that in most cases they
do. For the syllabication model, the optimal hy-
perparameters are 4 for the window radius and 50
for the maximum number of iterations. We evalu-
ate the cross-validation F
1
score of class 0, which
marks the position of a hyphen. The system ob-
tains 0.995 instance accuracy for predicting sylla-
ble boundaries.
We use a "majority class" type of baseline
which employs the C/V structures described in
Section 3 and assigns, for a word in the test set,
the stress pattern which is most common in the
training set for the C/V structure of the word, or
places the stress randomly on a vowel if the C/V
structure is not found in the training set
2
. The per-
formance of both models on RoSyllabiDict dataset
is reported in Table 2. We report word-level ac-
curacy, that is, we account for words for which
the stress pattern was correctly assigned. As ex-
pected, the cascaded model performs significantly
better than the baseline.
Model Accuracy
Baseline 0.637
Cascaded model (gold) 0.975
Cascaded model (predicted) 0.973
Table 2: Accuracy for stress prediction
Further, we perform an in-depth analysis of the
sequential model?s performance by accounting for
2
For example, the word copii (meaning children) has the
following C/V structure: CV-CVV. In our training set, there
are 659 words with this structure and the three stress patterns
which occur in the training set are as follows: CV-CVV (309
occurrences), CV-CVV (283 occurrences) and CV-CVV (67
occurrences). Therefore, the most common stress pattern CV-
CVV is correctly assigned, in this case, for the word copii.
66
several fine-grained characteristics of the words
in RoSyllabiDict. We divide words in categories
based on the following criteria:
? part of speech: verbs, nouns, adjectives
? number of syllables: 2-8, 9+
? number of consecutive vowels: with at least
2 consecutive vowels, without consecutive
vowels
Category Subcategory ] words
Accuracy
G P
POS
Verbs 167,193 0.995 0.991
Nouns 266,987 0.979 0.979
Adjectives 97,169 0.992 0.992
Syllables
2 syllables 34,810 0.921 0.920
3 syllables 111,330 0.944 0.941
4 syllables 154,341 0.966 0.964
5 syllables 120,288 0.981 0.969
6 syllables 54,918 0.985 0.985
7 syllables 17,852 0.981 0.989
8 syllables 5,278 0.992 0.984
9+ syllables 1,468 0.979 0.980
Vowels
With VV 134,895 0.972 0.972
Without VV 365,412 0.976 0.974
Table 3: Accuracy for cascaded model with
gold (G) and predicted (P) syllabication
We train and test the cascaded model indepen-
dently for each subcategory in the same manner as
we did for the entire dataset. We decided to use
cross-validation for parameter selection instead of
splitting the data in train/dev/test subsets in or-
der to have consistency across all models, because
some of these word categories do not comprise
enough words for splitting in three subsets (words
with more than 8 syllables, for example, have only
1,468 instances). The evaluation of the system?s
performance and the number of words in each cat-
egory are presented in Table 3.
4.2 Results Analysis
The overall accuracy is 0.975 for the cascaded
model with gold syllabication and 0.973 for the
cascaded model with predicted syllabication. The
former system outperforms the latter by only very
little. With regard to the part of speech, the high-
est accuracy when gold syllabication is used was
obtained for verbs (0.995), followed by adjectives
(0.992) and by nouns (0.979). When dividing the
dataset with respect to the words? part of speech,
the cascaded model with predicted syllabication
is outperformed only for verbs. With only a few
exceptions, the accuracy steadily increases with
the number of syllables. The peak is reached for
words with 6 syllables when using the gold syllab-
ication and for words with 7 syllables when using
the predicted syllabication. Although, intuitively,
the accuracy should be inversely proportional to
the number of syllables, because the number of
potential positions for stress placement increases,
there are numerous stress patterns for words with
6, 7 or more syllables, which never occur in the
dataset
3
. It is interesting to notice that stress pre-
diction accuracy is almost equal for words con-
taining two or more consecutive vowels and for
words without consecutive vowels. As expected,
when words are divided in categories based on
their characteristics the system is able to predict
stress placement with higher accuracy.
5 Conclusion and Future Work
In this paper we showed that Romanian stress
is predictable, though not deterministic, by using
data-driven machine learning techniques. Syllable
structure is important and helps the task of stress
prediction. The cascaded sequential model using
gold syllabication outperforms systems with pre-
dicted syllabication by only very little.
In our future work we intend to experiment with
other features as well, such as syllable n-grams
instead of character n-grams, for the sequential
model. We plan to conduct a thorough error analy-
sis and to investigate the words for which the sys-
tems did not correctly predict the position of the
stressed vowels. We intend to further investigate
the C/V structures identified in this paper and to
analyze the possibility to reduce the number of
patterns by considering details of word structure
(for example, instead of using raw data, to aug-
ment the model with annotations about which let-
ters are actually vowels) and to adapt the learning
model to finer-grained linguistic analysis.
Acknowledgements
The authors thank the anonymous reviewers for
their helpful comments. The contribution of the
authors to this paper is equal. Research supported
by a grant of ANRCS, CNCS UEFISCDI, project
number PN-II-ID-PCE-2011-3-0959.
3
For example, for the stress pattern CV-CV-CV-CV-CV-
CVCV, which matches 777 words in our dataset, the stress is
never placed on the first three syllables.
67
References
Ana-Maria Barbu. 2008. Romanian Lexical Data
Bases: Inflected and Syllabic Forms Dictionaries. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation, LREC 2008,
pages 1937?1941.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic Syllabification with Structured
SVMs for Letter-to-Phoneme Conversion. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, ACL-HLT 2008, pages 568?
576.
Ioana Chitoran. 1996. Prominence vs. rhythm: The
predictability of stress in Romanian. In Grammat-
ical theory and Romance languages, pages 47?58.
Karen Zagona.
Ioana Chitoran. 2002. The phonology of Romanian. A
constraint-based approach. Mouton de Gruyter.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10, EMNLP
2002, pages 1?8.
Robert I. Damper, Yannick Marchand, M. J. Adam-
son, and K. Gustafson. 1999. Evaluating the
pronunciation component of text-to-speech systems
for English: a performance comparison of differ-
ent approaches. Computer Speech & Language,
13(2):155?176.
Vera Demberg, Helmut Schmid, and Gregor M?hler.
2007. Phonological Constraints and Morphologi-
cal Preprocessing for Grapheme-to-Phoneme Con-
version. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2007, pages 96?103.
Gabriela Pan
?
a Dindelegan. 2013. The Grammar of
Romanian. Oxford University Press.
Liviu P. Dinu and Anca Dinu. 2005. A Parallel Ap-
proach to Syllabification. In Proceedings of the
6th International Conference on Computational Lin-
guistics and Intelligent Text Processing, CICLing
2005, pages 83?87.
Liviu P. Dinu, Vlad Niculae, and Octavia-Maria S
,
ulea.
2013. Romanian Syllabication Using Machine
Learning. In Proceedings of the 16th International
Conference on Text, Speech and Dialogue, TSD
2013, pages 450?456.
Liviu Petrisor Dinu. 2003. An Approach to Syllables
via some Extensions of Marcus Contextual Gram-
mars. Grammars, 6(1):1?12.
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn,
and Grzegorz Kondrak. 2009. A Ranking Approach
to Stress Prediction for Letter-to-Phoneme Conver-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th IJCNLP
of the AFNLP, ACL 2009, pages 118?126.
Eugeniu Oancea and Adriana Badulescu. 2002.
Stressed Syllable Determination for Romanian
Words within Speech Synthesis Applications. Inter-
national Journal of Speech Technology, 5(3):237?
246.
Naoaki Okazaki. 2007. CRFsuite: a fast implementa-
tion of Conditional Random Fields (CRFs).
Tomaz Sef, Maja Skrjanc, and Matjaz Gams. 2002.
Automatic Lexical Stress Assignment of Unknown
Words for Highly Inflected Slovenian Language. In
Proceedings of the 5th International Conference on
Text, Speech and Dialogue, TSD 2002, pages 165?
172.
S.-A. Toma, E. Oancea, and D. Munteanu. 2009.
Automatic rule-based syllabication for Romanian.
In Proceedings of the 5th Conference on Speech
Technology and Human-Computer Dialogue, SPeD
2009, pages 1?6.
Nikolaos Trogkanis and Charles Elkan. 2010. Con-
ditional Random Fields for Word Hyphenation. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2010,
pages 366?374.
68
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 102?106,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Temporal classification for historical Romanian texts
Alina Maria Ciobanu
Liviu P. Dinu
Octavia-Maria S, ulea
Faculty of Mathematics and Computer Science
Center for Computational Linguistics
University of Bucharest
alinamaria.ciobanu@yahoo.com
ldinu@fmi.unibuc.ro
mary.octavia@gmail.com
Anca Dinu
Faculty of Foreign Languages
University of Bucharest
anca d dinu@yahoo.com
Vlad Niculae
University of Wolverhampton
vlad@vene.ro
Abstract
In this paper we look at a task at border
of natural language processing, historical
linguistics and the study of language de-
velopment, namely that of identifying the
time when a text was written. We use
machine learning classification using lexi-
cal, word ending and dictionary-based fea-
tures, with linear support vector machines
and random forests. We find that lexical
features are the most helpful.
1 Introduction
Text dating, or determination of the time period
when it was written, proves to be a useful com-
ponent in NLP systems that can deal with such
diachronistically dynamic inputs (Moura?o et al,
2008). Besides this, the models that can perform
such classification can shine light on less than ob-
vious changes of certain features.
The knowledge captured in such systems can
prove useful in transferring modern language re-
sources and tools to historical domains (Meyer,
2011). Automatic translation systems between
and across language stages, as in the corpus in-
troduced by (Magaz, 2006), can benefit from the
identification of feature variation over time.
In this paper we study the problem of super-
vised temporal text classification across genres
and authors. The problem turns out to be solvable
to a very high degree of accuracy.
2 Related Work
The influence of the temporal effects in automatic
document classification is analyzed in (Moura?o et
al., 2008) and (Salles et al, 2010). The authors
state that a major challenge in building text clas-
sification models may be the change which occurs
in the characteristics of the documents and their
classes over time (Moura?o et al, 2008). There-
fore, in order to overcome the difficulties which
arise in automatic classification when dealing with
documents dating from different epochs, identify-
ing and accounting for document characteristics
changing over time (such as class frequency, rela-
tionships between terms and classes and the sim-
ilarity among classes over time (Moura?o et al,
2008)) is essential and can lead to a more accurate
discrimination between classes.
In (Dalli and Wilks, 2006) a method for clas-
sification of texts and documents based on their
predicted time of creation is successfully applied,
proving that accounting for word frequencies and
their variation over time is accurate. In (Kumar
et al, 2012) the authors argue as well for the ca-
pability of this method, of using words alone, to
determine the epoch in which a text was written or
the time period a document refers to.
The effectiveness of using models for individu-
als partitions in a timeline with the purpose of pre-
dicting probabilities over the timeline for new doc-
uments is investigated in (Kumar et al, 2011) and
(Kanhabua and N?rva?g, 2009). This approach,
based on the divergence between the language
model of the test document and those of the time-
line partitions, was successfully employed in pre-
dicting publication dates and in searching for web
pages and web documents.
In (de Jong et al, 2005) the authors raise the
problem of access to historical collections of doc-
uments, which may be difficult due to the differ-
ent historical and modern variants of the text, the
less standardized spelling, words ambiguities and
102
other language changes. Thus, the linking of cur-
rent word forms with their historical equivalents
and accurate dating of texts can help reduce the
temporal effects in this regard.
Recently, in (Mihalcea and Nastase, 2012), the
authors introduced the task of identifying changes
in word usage over time, disambiguating the epoch
at word-level.
3 Approach
3.1 Datasets used
In order to investigate the diachronic changes and
variations in the Romanian lexicon over time, we
used copora from five different stages in the evo-
lution of the Romanian language, from the 16th
to the 20th century. The 16th century represents
the beginning of the Romanian writing. In (Dim-
itrescu, 1994, p. 13) the author states that the mod-
ern Romanian vocabulary cannot be completely
understood without a thorough study of the texts
written in this period, which should be consid-
ered the source of the literary language used to-
day. In the 17th century, some of the most im-
portant cultural events which led to the develop-
ment of the Romanian language are the improve-
ment of the education system and the establish-
ing of several printing houses (Dimitrescu, 1994,
p. 75). According to (Lupu, 1999, p. 29), in
the 18th century a diversification of the philologi-
cal interests in Romania takes place, through writ-
ing the first Romanian-Latin bilingual lexicons,
the draft of the first monolingual dictionary, the
first Romanian grammar and the earliest transla-
tions from French. The transition to the Latin al-
phabet, which was a significant cultural achieve-
ment, is completed in the 19th century. The Cyril-
lic alphabet is maintained in Romanian writing
until around 1850, afterwards being gradually re-
placed with the Latin alphabet (Dimitrescu, 1994,
p. 270). The 19th century is marked by the conflict
(and eventually the compromise) between etymol-
ogism and phonetism in Romanian orthography.
In (Maiorescu, 1866) the author argues for apply-
ing the phonetic principle and several reforms are
enforced for this purpose. To represent this pe-
riod, we chose the journalism texts of the leading
Romanian poet Mihai Eminescu. He had a cru-
cial influence on the Romanian language and his
contribution to modern Romanian development is
highly appreciated. In the 20th century, some vari-
ations regarding the usage of diacritics in Roma-
nian orthography are noticed.
Century Corpus Nwordstype token
16
Codicele Todorescu 3,799 15,421
Codicele Martian 394 920
Coresi, Evanghelia cu ??nva?t?a?tura? 10,361 184,260
Coresi, Lucrul apostolesc 7,311 79,032
Coresi, Psaltirea slavo-roma?na? 4,897 36,172
Coresi, Ta?rgul evangheliilor 6,670 84,002
Coresi, Tetraevanghelul 3,876 36,988
Manuscrisul de la Ieud 1,414 4,362
Palia de la Ora?s?tie 6,596 62,162
Psaltirea Hurmuzaki 4,851 32,046
17
The Bible 15,437 179,639
Miron Costin, Letopiset?ul T?a?rii Moldovei 6,912 70,080
Miron Costin, De neamul moldovenilor 5,499 31,438
Grigore Ureche, Letopiset?ul T?a?rii Moldovei 5,958 55,128
Dosoftei, Viat?a si petreacerea sfint?ilor 23,111 331,363
Varlaam Motoc, Cazania 10,179 154,093
Varlaam Motoc, Ra?spunsul ??mpotriva 2,486 14,122
Catehismului calvinesc
18
Antim Ivireanul, Opere 11,519 123,221
Axinte Uricariul, Letopiset?ul T?a?rii 16,814 147,564
Roma?nesti s?i al T?a?rii Moldovei
Ioan Canta, Letopiset?ul T?a?rii Moldovei
Dimitrie Cantemir, Istoria ieroglifica? 13,972 130,310
Dimitrie Eustatievici Bras?oveanul, 5,859 45,621
Gramatica roma?neasca?
Ion Neculce, O sama? de cuvinte 9,665 137,151
19
Mihai Eminescu, Opere, v. IX 27,641 227,964
Mihai Eminescu, Opere, v. X 30,756 334,516
Mihai Eminescu, Opere, v. XI 27,316 304,526
Mihai Eminescu, Opere, v. XII 28,539 308,518
Mihai Eminescu, Opere, v. XIII 26,242 258,234
20
Eugen Barbu, Groapa 14,461 124,729
Mircea Cartarescu, Orbitor 35,486 306,541
Marin Preda, Cel mai iubit dintre pa?ma?nteni 28,503 388,278
Table 1: Romanian corpora: words
For preprocessing our corpora, we began by re-
moving words that are irrelevant for our investiga-
tion, such as numbers. We handled word bound-
aries and lower-cased all words. We computed,
for each text in our corpora, the number of words
(type and token). The results are listed in Table
1. For identifying words from our corpora in dic-
tionaries, we performed lemmatization. The in-
formation provided by the machine-readable dic-
tionary dexonline 1 regarding inflected forms al-
lowed us to identify lemmas (where no semantic
or part-of-speech ambiguities occurred) and to fur-
ther lookup the words in the dictionaries. In our
investigations based on dexonline we decided to
use the same approach as in (Mihalcea and Nas-
tase, 2012) and to account only for unambiguous
words. For example, the Romanian word ai is
morphologically ambiguous, as we identified two
corresponding lemmas: avea (verb, meaning to
have) and ai (noun, meaning garlic). The word
ama?nare is semantically ambiguous, having two
different associated lemmas, both nouns: ama?nar
(which means flint) and ama?na (which means to
postpone). We do not use the POS information di-
1http://dexonline.ro
103
rectly, but we use dictionary occurrence features
only for unambiguous words.
The database of dexonline aggregates informa-
tion from over 30 Romanian dictionaries from dif-
ferent periods, from 1929 to 2012, enabling us to
investigate the diachronic evolution of the Roma-
nian lexicon. We focused on four different sub-
features:
? words marked as obsolete in dexonline defi-
nitions (we searched for this tag in all dictio-
naries)
? words which occur in the dictionaries of ar-
chaisms (2 dictionaries)
? words which occur in the dictionaries pub-
lished before 1975 (7 dictionaries)
? words which occur in the dictionaries pub-
lished after 1975 (31 dictionaries)
As stated before, we used only unambiguous
words with respect to the part of speech, in order to
be able to uniquely identify lemmas and to extract
the relevant information. The aggregated counts
are presented in table 2.
Sub-feature 16 17 18 19 20
archaism type 1,590 2,539 2,114 1,907 2,140
token 5,652 84,804 56,807 120,257 62,035
obsolete type 5,652 8,087 7,876 9,201 8,465
token 172,367 259,367 199,899 466,489 279,654
< 1975 type 11,421 17,200 16,839 35,383 34,353
token 311,981 464,187 337,026 885,605 512,156
> 1975 type 12,028 18,948 18,945 42,855 41,643
token 323,114 480,857 356,869 943,708 541,258
Table 2: Romanian corpora: dexonline sub-
features
3.2 Classifiers and features
The texts in the corpus were split into chunks of
500 sentences in order to increase the number of
sample entries and have a more robust evaluation.
We evaluated all possible combinations of the four
feature sets available:
? lengths: average sentence length in words,
average word length in letters
? stopwords: frequency of the most common
50 words in all of the training set:
de s, i ??n a la cu au no o sa? ca? se pe
din s ca i lui am este fi l e dar pre ar
va? le al dupa? fost ??ntr ca?nd el daca?
ne n ei sau sunt
Century Precision Recall F1-score texts
16 1.00 1.00 1.00 16
17 1.00 0.88 0.94 17
18 0.88 1.00 0.93 14
19 1.00 1.00 1.00 23
20 1.00 1.00 1.00 21
average/ total 0.98 0.98 0.98 91
Table 4: Random Forest test scores using all fea-
tures and aggregating over 50 trees
? endings: frequency of all word suffixes of
length up to three, that occur at least 5 times
in the training set
? dictionary: proportion of words matching
the dexonline filters described above
The system was put together using the scikit-
learn machine learning library for Python (Pe-
dregosa et al, 2011), which provides an imple-
mentation of linear support vector machines based
on liblinear (Fan et al, 2008), an implementation
of random forests using an optimised version of
the CART algorithm.
4 Results
The hyperparameters (number of trees, in the ran-
dom forest case, and C, for the SVM) were op-
timized using 3 fold cross-validation for each of
the feature sets. For the best feature sets, denoted
with an asterisk in table 3, the test results and hy-
perparameter settings are presented in tables 4 and
5.
The results show that the nonlinear nature of
the random forest classifier is important when us-
ing feature sets so different in nature. However, a
linear SVM can perform comparably, using only
the most important features. The misclassifica-
tions that do occur are not between very distant
centuries.
5 Conclusions
We presented two classification systems, a linear
SVM one and a nonlinear random forest one, for
solving the temporal text classification problem on
Romanian texts. By far the most helpful features
turn out to be lexical, with dictionary-based histor-
ical information less helpful than expected. This is
probably due to inaccuracy and incompleteness of
104
lengths stopwords endings dictionary RF SVM
False False False False 25.38 25.38
False False False True 86.58 79.87
False False True False 98.51 95.16
False False True True 97.76 97.02
False True False False 98.51 96.27
False True False True 98.51 94.78
False True True False 98.88 *98.14
False True True True 98.51 97.77
True False False False 68.27 22.01
True False False True 92.92 23.13
True False True False 98.14 23.89
True False True True 98.50 23.14
True True False False 98.14 23.53
True True False True 98.51 25.00
True True True False 98.88 23.14
True True True True *99.25 22.75
Table 3: Cross-validation accuracies for different feature sets. The score presented is the best one over
all of the hyperparameter settings, averaged over the folds.
Century Precision Recall F1-score texts
16 1.00 1.00 1.00 16
17 1.00 1.00 1.00 17
18 1.00 0.93 0.96 14
19 1.00 1.00 1.00 23
20 0.95 1.00 0.98 21
average/ total 0.99 0.99 0.99 91
Table 5: Linear SVC test scores using only stop-
words and word endings for C = 104.
dictionary digitization, along with ambiguities that
might need to be dealt with better.
We plan to further investigate feature impor-
tances and feature selection for this task to ensure
that the classifiers do not actually fit authorship or
genre latent variables.
Acknowledgements
The authors thank the anonymous reviewers for
their helpful and constructive comments. The con-
tribution of the authors to this paper is equal. Re-
search supported by a grant of the Romanian Na-
tional Authority for Scientific Research, CNCS ?
UEFISCDI, project number PN-II-ID-PCE-2011-
3-0959.
References
Angelo Dalli and Yorick Wilks. 2006. Automatic dat-
ing of documents and temporal text classification.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, Sydney,, pages
17?-22.
Franciska de Jong, Henning Rode, and Djoerd Hiem-
stra. 2005. Temporal language models for the dis-
closure of historical text. In Humanities, computers
and cultural heritage: Proceedings of the XVIth In-
ternational Conference of the Association for His-
tory and Computing.
Florica Dimitrescu. 1994. Dinamica lexicului
roma?nesc - ieri s?i azi. Editura Logos. In Romanian.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874, June.
Nattiya Kanhabua and Kjetil N?rva?g. 2009. Using
temporal language models for document dating. In
ECML/PKDD (2), pages 738?741.
Abhimanu Kumar, Matthew Lease, and Jason
Baldridge. 2011. Supervised language modeling
for temporal resolution of texts. In CIKM, pages
2069?2072.
Abhimanu Kumar, Jason Baldridge, Matthew Lease,
and Joydeep Ghosh. 2012. Dating texts without ex-
plicit temporal cues. CoRR, abs/1211.2290.
Coman Lupu. 1999. Lexicografia roma?neasca? ??n pro-
cesul de occidentalizare latino-romanica? a limbii
roma?ne moderne. Editura Logos. In Romanian.
105
Judit Martinez Magaz. 2006. Tradi imt (xx-xxi):
Recent proposals for the alignment of a diachronic
parallel corpus. International Computer Archive of
Modern and Medieval English Journal, (30).
Titu Maiorescu. 1866. Despre scrierea limbei ruma?ne.
Edit?iunea s?i Imprimeria Societa?t?ei Junimea. In Ro-
manian.
Roland Meyer. 2011. New wine in old wineskins?
tagging old russian via annotation projection from
modern translations. Russian Linguistcs.
Rada Mihalcea and Vivi Nastase. 2012. Word epoch
disambiguation: Finding how words change over
time. In ACL (2), pages 259?263. The Association
for Computer Linguistics.
Fernando Moura?o, Leonardo Rocha, Renata Arau?jo,
Thierson Couto, Marcos Gonc?alves, and Wag-
ner Meira Jr. 2008. Understanding temporal aspects
in document classification. In WSDM ?08 Proceed-
ings of the 2008 International Conference on Web
Search and Data Mining, pages 159?170.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830, Oct.
Thiago Salles, Leonardo Rocha, Fernando Moura?o,
Gisele L. Pappa, Lucas Cunha, Marcos Gonc?alves,
and Wagner Meira Jr. 2010. Automatic document
classification temporally robust. Journal of Infor-
mation and Data Management, 1:199?211, June.
106
