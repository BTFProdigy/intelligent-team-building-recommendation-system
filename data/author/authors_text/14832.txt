R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 155 ? 164, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Chunking Using Conditional Random Fields  
in Korean Texts 
Yong-Hun Lee, Mi-Young Kim, and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering POSTECH and AITrc, 
San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, R. of Korea 
{yhlee95, colorful, jhlee}@postech.ac.kr 
Abstract. We present a method of chunking in Korean texts using conditional 
random fields (CRFs), a recently introduced probabilistic model for labeling 
and segmenting sequence of data. In agglutinative languages such as Korean 
and Japanese, a rule-based chunking method is predominantly used for its sim-
plicity and efficiency. A hybrid of a rule-based and machine learning method 
was also proposed to handle exceptional cases of the rules. In this paper, we 
present how CRFs can be applied to the task of chunking in Korean texts. Ex-
periments using the STEP 2000 dataset show that the proposed method signifi-
cantly improves the performance as well as outperforms previous systems. 
1   Introduction 
Text chunking is a process to identify non-recursive cores of various phrase types 
without conducting deep parsing of text [3]. Abney first proposed it as an intermedi-
ate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking 
using a machine learning method, many researchers have used various machine learn-
ing techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL-
2000 shared task with standard datasets and evaluation metrics, which is now a stan-
dard evaluation task for text chunking [3]. 
Most previous works with relatively high performance in English used machine 
learning methods for chunking [4,13]. Machine learning methods are mainly divided 
into the generative approach and conditional approach. The generative approach relies 
on generative probabilistic models that assign a joint probability p(X,Y) of paired 
input sequence and label sequence, X and Y respectively. It provides straightforward 
understanding of underlying distribution. However, this approach is intractable in 
most domains without strong independence assumptions that each input element is 
independent from the other elements in input sequence, and is also difficult to use 
multiple interacting features and long-range dependencies between input elements. 
The conditional approach views the chunking task as a sequence of classification 
problems, and defines a conditional probability p(Y|X) over label sequence given 
input sequence. A number of conditional models recently have been developed for 
use. They showed better performance than generative models as they can handle 
many arbitrary and overlapping features of input sequence [12]. 
A number of methods are applied to chunking in Korean texts. Unlike English, a 
rule-based chunking method [7,8] is predominantly used in Korean because of its 
well-developed function words, which contain information such as grammatical  
156 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
relation, case, tense, modal, etc. Chunking in Korean texts with only simple heuristic 
rules obtained through observation on the text shows a good performance similar to 
other machine learning methods [6]. Park et al proposed a hybrid of rule-based and 
machine learning method to handle exceptional cases of the rules, to improve the 
performance of chunking in Korean texts [5,6]. 
In this paper, we present how CRFs, a recently introduced probabilistic model for 
labeling and segmenting sequence of data [12], can be applied to the task of chunking 
in Korean texts. CRFs are undirected graphical models trained to maximize condi-
tional probabilities of label sequence given input sequence. It takes advantage of gen-
erative and conditional models. CRFs can include many correlated, overlapping fea-
tures, and they are trained discriminatively like conditional model. Since CRFs have 
single exponential model for the conditional probability of entire label sequence given 
input sequence, they also guarantee to obtain globally optimal label sequence. CRFs 
successfully have been applied in many NLP problems such as part-of-speech tagging 
[12], text chunking [13,15] and table extraction from government reports [19]. 
The rest of this paper is organized as follows. Section 2 gives a simple introduction 
to CRFs. Section 3 explains how CRFs is applied to the task of chunking in Korean 
texts. Finally, we present experimental results and draw conclusions.  
2   Conditional Random Fields 
Conditional Random Fields (CRFs) are conditional probabilistic sequence models 
first introduced by Lefferty et al[12]. CRFs are undirected graphical models, which 
can be used to define the joint probability distribution over label sequence given the 
entire input sequence to be labeled, rather than being directed graphical models such 
as Maximum Entropy Markov Models (MEMMs) [11].  It relaxes the strong inde-
pendence assumption of Hidden Markov Models (HMMs), as well as resolves the 
label bias problem exhibited by MEMMs and other non-generative directed graphical 
models such as discriminative Markov models [12]. 
2.1   Fundamentals of CRFs 
CRFs may be viewed as an undirected graphical model globally conditioned on input 
sequence [14]. Let X=x1 x2 x3 ?xn be an input sequence and Y=y1 y2 y3 ?yn a label se-
quence. In the chunking task, X is associated with a sequence of words and Y is asso-
ciated with a sequence of chunk types. If we assume that the structure of a graph 
forms a simple first-order chain, as illustrated in Figure 1, CRFs define the condi-
tional probability of a label sequence Y given an input sequence X by the Hammer-
sley-Clifford theorem [16] as follows: 
??
???
?
= ?? ?
i k
iikk iXyyfXZXYp ),,,(exp)(
1)|( 1?  (1) 
where Z(X) is a normalization factor; fk(yi-1, yi, X, i) is a feature function at positions i 
and i-1 in the label sequence; k?  is  a weight associated with feature kf . 
 Chunking Using Conditional Random Fields in Korean Texts 157 
 
Fig. 1. Graphical structure of chain-structured CRFs 
Equitation 1, the general form of a graph structure for modeling sequential data, 
can be expanded to Equation 2, 
??
???
?
+= ???? ?
i k
ikk
i k
iikk iXysiXyytXZ
XYp ),,(),,,(exp)(
1)|( 1 ??  (2) 
where tk(yi-1, yi, X, i) is a transition feature function of the entire input sequence and the 
labels at positions i and i-1 in the label sequence; sk(yi, X, i) is a state feature function 
of the label at position i and the observed input sequence; and k? and k? are parame-
ters to be estimated from training data. The parameters k? and k?  play similar roles to 
the transition and emission probabilities in HMMs [12]. Therefore, the most probable 
label sequence for input sequence X is Y* which maximizes a posterior probability. 
)|(maxarg* XYPY
Y
?=  (3) 
We can find Y* with dynamic programming using the Viterbi algorithm. 
2.2   Parameter Estimation for CRFs 
Assuming the training data {(X(n), Y(n))} are independently and identically distributed, 
the product of Equation 1 over all training sequences is a likelihood function of the 
parameter ? . Maximum likelihood training chooses parameter values such that the 
log-likelihood is maximized [10]. For CRFs, the log-likelihood )(?L  is given by 
? ??
?
??
???
?
?=
=
?
n
n
i k
nn
i
n
ikk
nn
n
XZiXyyf
XYPL
)(log),,,(
)|(log)(
)()()()(
1
)()(
?
? ?
 (4) 
It is not possible to analytically determine the parameter values that maximize the 
log-likelihood. Instead, maximum likelihood parameters must be identified using an 
iterative technique such as iterative scaling [12] or gradient-based methods [13,14].  
Lafferty et al proposed two iterative scaling algorithms to find parameters for 
CRFs. However, these methods converge into a global maximum very slowly. To 
158 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
overcome this problem of slow convergence, several researchers adopted modern 
optimization algorithms such as the conjugate-gradient method or the limited-memory 
BFGS(L-BFGS) method [17]. 
3   Chunking Using Conditional Random Fields in Korean Texts 
We now describe how CRFs are applied to the task of chunking in Korean texts. 
Firstly, we explore characteristics and chunk types of Korean. Then we explain the 
features for the model of chunking in Korean texts using CRFs. The ultimate goal of a 
chunker is to output appropriate chunk tags of a sequence of words with part-of-
speech tags.  
3.1   Characteristics of Korean 
Korean is an agglutinative language, in which a word unit (called an eojeol) is a com-
position of a content word and function word(s). Function words ? postpositions and 
endings ? give much information such as grammatical relation, case, tense, modal, 
etc. Well-developed function words in Korean help with chunking, especially NP and 
VP chunking. For example, when the part-of-speech of current word is one of deter-
miner, pronoun and noun, the following seven rules for NP chunking in Table 1 can 
find most NP chunks in text, with about 89% accuracy [6].  
Table 1. Rules for NP chunking in Korean texts 
No Previous eojeol Chunk tag of current word 
1 determiner I-NP 
2 pronoun I-NP 
3 noun I-NP 
4 noun + possessive postposition I-NP 
5 noun + relative postfix I-NP 
6 adjective + relative ending I-NP 
7 others B-NP 
For this reason, boundaries of chunks are easily found in Korean, compared to 
other languages such as English or Chinese. This is why a rule-based chunking 
method is predominantly used. However, with sophisticated rules, the rule-based 
chunking method has limitations when handling exceptional cases. Park et al pro-
posed a hybrid of the rule-based and the machine learning method to resolve this 
problem [5,6]. Many recent machine learning techniques can capture hidden charac-
teristics for classification. Despite its simplicity and efficiency, the rule-based method 
has recently been outdone by the machine learning method in various classification 
problems. 
3.2   Chunk Types of Korean 
Abney was the first to use the term ?chunk? to represent a non-recursive core of an 
intra-clausal constituent, extending from the beginning of constituent to its head. In 
 Chunking Using Conditional Random Fields in Korean Texts 159 
Korean, there are four basic phrases: noun phrase (NP), verb phrase (VP), adverb 
phrase (ADVP), and independent phrase (IP) [6]. As function words such as postposi-
tion or ending are well-developed, the number of chunk types is small compared to 
other languages such as English or Chinese. Table 2 lists the Korean chunk types, a 
simple explanation and examples of each chunk type. 
Table 2. The Korean chunk types 
No Category Explanation Example 
1 NP Noun Phrase [NP? ???? ???] [???]. ([the beautiful woman] [look]) 
2 VP Verb Phrase [???] [??] [VP???? ??]. ([the roof] [completely] [has fallen in]) 
3 ADVP Adverb Phrase [??] [ADVP ?? ??] [?? ??]. ([a bird] [very high] [is flying]) 
4 IP Independent Phrase [IP ?], [??] [??] [???]. ([wow] [this] [very] [is delicious]) 
Like the CoNLL-2000 dataset, we use three types of chunk border tags, indicating 
whether a word is outside a chunk (O), starts a chunk (B), or continues a chunk (I). 
Each chunk type XP has two border tags: B-XP and I-XP. XP should be one of NP, 
VP, ADVP and IP. There exist nine chunk tags in Korean. 
3.3   Feature Set of CRFs 
One advantage of CRFs is that they can use many arbitrary, overlapping features. So 
we take advantage of all context information of a current word. We use words, part-
of-speech tags of context and combinations of part-of-speech tags to determine the 
chunk tag of the current word,. The window size of context is 5; from left two words 
to right two words. Table 3 summarizes the feature set for chunking in Korean texts. 
Table 3. Feature set for the chunking in Korean texts 
Word POS tag Bi-gram of tags Tri-gram of tags 
wi-2= w 
wi-1= w 
wi= w 
wi+1= w 
wi+2= w 
ti-2= t 
ti-1= t 
ti= t 
ti+1= t 
ti+2= t 
ti-2= t?, ti-1= t 
ti-1= t?, ti= t 
ti= t?, ti+1= t 
ti+1= t?,ti+2= t 
ti-2= t?, ti-1= t?, ti= t 
ti-1= t?, ti= t?, ti+1= t 
ti= t?, ti+1= t?, ti+2= t 
4   Experiments 
In this section, we present experimental results of chunking using CRFs in Korean 
texts and compare the performance with previous systems of Park et al[6]. To make a 
fare comparison, we use the same dataset as Park et al[6]. 
160 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
4.1   Data Preparation 
For evaluation of our proposed method, we use the STEP 2000 Korean chunking 
dataset (STEP 2000 dataset)1, which is converted from the parsed KAIST Corpus [9]. 
Table 4. Simple statistics on the STEP 2000 dataset 
Information Value 
POS tags 52 
Words 321,328 
Sentences 12,092 
Chunk tags 9 
Chunks 112,658 
 
 
 
? 
npp B-NP his 
? 
jcm I-NP postposition: possessive 
? 
ncn I-NP book 
? 
jxt I-NP postposition: topic 
?? 
ncpa B-VP destructed 
? 
xsv I-VP be 
? 
ep I-VP pre-final ending : past 
? 
ef I-VP ending : declarative 
. sf O  
Fig. 2. An example of the STEP 2000 dataset 
The STEP 2000 dataset consists of 12,092 sentences. We divide this corpus into 
training data and test data. Training data has 10,883 sentences and test data has 
1,209 sentences, 90% and 10% respectively. Table 4 summarizes characteristics of 
the STEP 2000 dataset. Figure 2 shows an example sentence of the STEP 2000 data-
set and its format is equal to that of CoNLL-2000 dataset. Each line is composed of a 
word, its part-of-speech (POS) tag and a chunk tag. 
4.2   Evaluation Metric 
The standard evaluation metrics for chunking performance are precision, recall and F-
score (F
?=1) [3]. F-score is used for comparisons with other reported results. Each 
equation is defined as follows. 
                                                          
1
 STEP is an abbreviation of Software Technology Enhancement Program. We download this 
dataset from http://bi.snu.ac.kr/~sbpark/Step2000. If you want to know the part-of-speech tags 
used in the STEP 2000 dataset, you can reference KAIST tagset [9]. 
 Chunking Using Conditional Random Fields in Korean Texts 161 
outputinchunksof
chunkscorrectofprecision
#
#
=  (5) 
datatestinchunksof
chunkscorrectof
recall
#
#
=  (6) 
precisionrecall
precisionrecallF
+
??
=
=
2
1?  (7) 
4.3   Experimental Results 
Experiments were performed with C++ implementation of CRFs (FlexCRFs) on 
Linux with 2.4 GHz Pentium IV dual processors and 2.0Gbyte of main memory [18]. 
We use L-BFGS to train the parameters and use a Gaussian prior regularization in 
order to avoid overfitting [20]. 
Table 5. The performance of proposed method 
Chunk tag Precision Recall F-score 
NP 94.23 94.30 94.27 
VP 96.71 96.28 96.49 
ADVP 96.90 97.02 96.96 
IP 99.53 99.07 99.30 
All 95.42 95.31 95.36 
Total number of CRF features is 83,264. As shown in Table 5, the performances of 
most chunk type are 96~100%, very high performance. However, the performance of 
NP chunk type is lowest, 94.27% because the border of NP chunk type is very am-
biguous in case of consecutive nouns. Using more features such as previous chunk tag 
should be able to improve the performance of NP chunk type. 
Table 6. The experimental results of various chunking methods2 
 HMMs DT MBL Rule SVMs Hybrid CRFs 
Precision 73.75 92.29 91.41 91.28 93.63 94.47 95.42 
Recall 76.06 90.45 91.43 92.47 91.48 93.96 95.31 
F-score 74.89 91.36 91.38 91.87 92.54 94.21 95.36 
Park et al reported the performance of various chunking methods [6]. We add the 
experimental results of the chunking methods using HMMs-bigram and CRFs.  
In Table 6, F-score of chunking using CRFs in Korean texts is 97.19%, the highest 
                                                          
2
 Performances of all methods except HMMs and CRFs are cited from the experiment of Park 
et al[6]. They also use the STEP 2000 dataset and similar feature set. Therefore, the compari-
son of performance is reasonable. 
162 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
performance of all. It significantly outperforms all others, including machine learning 
methods, rule-based methods and hybrid methods. It is because CRFs have a global 
optimum solution hence overcoming the label bias problem. They also can use many 
arbitrary, overlapping features. 
Figure 3 shows the performance curve on the same test set in terms of the preci-
sion, recall and F-score with respect to the size of training data. In this figure, we can 
see that the performance slowly increases in proportion to the size of training data. 
 
Fig. 3. The performance curve respect to the size of training data 
In the experiment, we can see that CRFs can help improve the performance of 
chunking in Korean texts. CRFs have many promising properties except for the slow 
convergence speed compared to other models. In the next experiment, we have tried 
to analyze the importance of each feature and to make an additional experiment with 
various window sizes and any other useful features. 
5   Conclusion 
In this paper, we proposed a chunking method for Korean texts using CRFs. We ob-
served that the proposed method outperforms other approaches. Experiments on the 
STEP 2000 dataset showed that the proposed method yields an F-score of 95.36%. 
This performance is 2.82% higher than that of SVMs and 1.15% higher than that of 
the hybrid method. CRFs use a number of correlated features and overcome the label 
bias problem. We obtained a very high performance using only small features. Thus, 
if we use more features such as semantic information or collocation, we can obtain a 
better performance. 
From the experiment, we know that the proposed method using CRFs can signifi-
cantly improve the performance of chunking in Korean texts. CRFs are a good frame-
work for labeling an input sequence. In our future work, we will investigate how 
CRFs can be applied to other NLP problems: parsing, semantic analysis and spam 
filtering. Finally, we hope that this work can contribute to the body of research in  
this field. 
 Chunking Using Conditional Random Fields in Korean Texts 163 
Acknowledgements 
This work was supported by the KOSEF through the Advanced Information Technol-
ogy Research Center (AITrc) and by the BK21 Project. 
References 
1. S. Abney: Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny, editors, Principle-
based Parsing. Kluwer Academic Publishers (1991). 
2. L. A. Ramashaw and M. P. Marcus: Text chunking using transformation-based learning. 
Proceedings of the Thired ACL Workshop on Very Large Corpora (1995). 
3. E. F. Tjong Kim Sang and S. Buchholz: Introduction to the CoNLL-2000 shared task: 
Chunking. Proceedings of CoNLL-2000 (2000) 127-132. 
4. T. Kudo and Y. Matsumoto: Chunking with support vector machines. Proceedings of 
NAACL2001, ACL (2001). 
5. Park, S.-B. and Zhang, B.-T.: Combining a Rule-based Method and a k-NN for Chunking 
Korean Text. Proceedings of the 19th International Conference on Computer Processing of 
Oriental Languages (2001) 225-230. 
6. Park, S.-B. and Zhang, B.-T.: Text Chunking by Combining Hand-Crafted Rules and 
Memory-Based Learning. Proceedings of the 41st Annual Meeting of the Association for 
Computational Linguistics (2003) 497-504. 
7. H.-P. Shin: Maximally Efficient Syntactic Parsing with Minimal Resources. Proceedings 
of the Conference on Hangul and Korean Language Information Processing (1999)  
242-244. 
8. M.-Y. Kim, S.-J. Kang and J.-H. Lee: Dependency Parsing by Chunks. Proceedings of the 
27th KISS Spring Conference (1999) 327-329. 
9. J.-T. Yoon and K.-S. Choi: Study on KAIST Corpus, CS-TR-99-139, KAIST CS (1999). 
10. A. L. Berger, S. A. Della Pietra and V. J. Della Pietra: A maximum entropy approach to 
natural language processing. Computational Linguistics, 22(1) (1996) 39-71. 
11. Andrew McCallum, D. Freitag and F. Pereira: Maximum entropy Markov models for in-
formation extraction and segmentation. Proceedings of International Conference on Ma-
chine Learning , Stanford, California (2000) 591-598. 
12. John Lafferty, Andrew McCallum and Fernando Pereira: Conditional Random Fields: 
Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the 18th 
International Conference on Machine Learning (2001) 282-289. 
13. Fei Sha and Fernando Pereira: Shallow Parsing with Conditional Random Fields. Proceed-
ings of Human Language Technology-NAACL, Edmonton, Canada (2003). 
14. Hanna Wallach: Efficient Training of Conditional Random Fields. Thesis. Master of Sci-
ence School of Cognitive Science, Division of Informatics. University of Edinburgh 
(2002). 
15. Yongmei Tan, Tianshun Yao, Qing Chen and Jingbo Zhu: Applying Conditional Random 
Fields to Chinese Shallow Parsing. The 6th International Conference on Intelligent Text 
Processing and Computational Linguistics (CICLing-2005) . LNCS, Vol.3406, Springer, 
Mexico City, Mexico (2005) 167-176. 
16. J. Hammersley and P. Clifford. Markov fields on finite graphs and lattices. Unpublished 
manuscript (1971). 
 
164 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
17. D. C. Liu and J. Nocedal: On the limited memory bfgs method for large-scale optimiza-
tion. Mathematic Programming, 45 (1989) 503-528. 
18. Hieu Xuan Phan and Minh Le Nguyen: FlexCRFs: A Flexible Conditional Random Fields 
Toolkit. http:://www.jaist.ac.jp/~hieuxuan/flexcrfs/flexcrfs.html (2004). 
19. D. Pinto, A. McCallum, X. Wei and W. B. Croft: Table extraction using conditional ran-
dom fields. Proceedings of the ACM SIGIR (2003). 
20. S. F. Chen and R. Rosenfeld: A Gaussian prior for smoothing maximum entropy models. 
Technical Report CMU-CS-99-108, Carnegie Mellon University (1999). 
 
Ambiguity Packing in Constraint-based Parsing 
Practical Results 
Stephan Oepen 
Computat iona l  Linguistics 
Saar land University 
66041 Saarbriicken, Germany 
oe@coli, uni-sb, de 
John Carroll 
Cognitive and Computing Sciences 
University of Sussex 
Brighton BN1 9QH, UK 
j olmca?cogs, susx .  ac.  uk 
Abstract 
We describe a novel approach to 'packing' of local am- 
biguity in parsing with a wide-coverage HPSG gram- 
mar, and provide an empirical assessment of the in- 
teraction between various packing and parsing strate- 
gies. We present a linear-time, bidirectional subsump- 
tion test for typed feature structures and demonstrate 
that (a) subsumption- and equivalence-based packing is 
applicable to large HPSG grammars and (b) average parse 
complexity can be greatly reduced in bottom-up chart 
parsing with comprehensive HPSG implementations. 
1 Background 
The ambiguity inherent in natural anguage means 
that during parsing, some segments of the input 
string may end up being analysed as the same type 
of linguistic object in several different ways. Each 
of these different ways must be recorded, but subse- 
quent parsing steps must treat the set of analyses as 
a single entity, otherwise the computation becomes 
theoretically intractable. Earley's algorithm (Ear- 
ley, 1970), for example, avoids duplication of parse 
items by maintaining pointers to alternative deriva- 
tions in association with the item. This process 
has been termed 'local ambiguity packing' (Tomita, 
1985), and the structure built up by the parser, a 
'parse forest' (Billot &: Lang, 1989). Context free 
(CF) grammars represent linguistic objects in terms 
of atomic category symbols. The test for duplicate 
parse items--and thus being able to pack the sub- 
analyses associated with them--is equality of cate- 
gory symbols. In the final parse forest every differ- 
ent combination ofpacked nodes induces a distinct, 
valid parse tree. 
Most existing unification-based parsing systems 
either implicitly or explicitly contain a context-free 
core. For example, in the CLE (Alshawi, 1992) 
the (manually-assigned) functors of the Prolog terms 
forming the categories constitute a CF 'backbone'. 
In the Alvey Tools system (Carroll, 1993) each dis- 
tinct set of features i  automatically given a unique 
identifier and this is associated with every category 
containing those features. The packing technique 
has been shown to work well in practice in these 
and similar unification-augmented CF systems: the 
parser first tests for CF category equality, and then 
either (a) checks that the existing feature structure 
subsumes the newly derived one (Moore & Alshawi, 
1992), or (b) forms an efficiently processable disjunc- 
tion of the feature structures (Maxwell and Kaplan, 
1995). Extracting parses from the parse forest is 
similar to the CF case, except hat a global check for 
consistency of feature values between packed nodes 
or between feature structure disjuncts is required 
(this global validation is not required if the sub- 
sumption test is strengthened to feature structure 
equivalence). 
In contrast, there is essentially no CF compo- 
nent in systems which directly interpret HPSG gram- 
mars. Although HPSG feature structures are typed, 
an initial CF category equality test cannot be im- 
plemented straightforwardly in terms of the top- 
level types of feature structures since two compat- 
ible types need not be equal, but could stand in 
a subtype-supertype relationship. In addition, the 
feature structure subsumption test is potentially ex- 
pensive since feature structures are large, typically 
containing hundreds of nodes. It is therefore an open 
question whether parsing systems using grammars of 
this type can gain any advantage from local ambi- 
guity packing. 
The question is becoming increasingly impor- 
tant, though, as wide-coverage HPSG grammars are 
starting to be deployed in practical applications-- 
for example for 'deep' analysis in the VerbMo- 
bil speech-to-speech translation system (Wahlster, 
1997; Kiefer, Krieger, Carroll, & Malouf, 1999). 1 In 
this paper we answer the question by demonstrating 
that (a) subsumption- and equivalence-based f ature 
structure packing is applicable to large HPSG gram- 
mars, and (b) average complexity and time taken 
for the parsing task can be greatly reduced. In 
Section 2 we present a new, linear-time, bidirec- 
1A significant body of work on efficient processing with 
such grammars has been building up recently, with investi- 
gations into efficient feature structure operations, abstract- 
machine-based compilation, CF backbone computation, and 
finite-state approximation f HPSG derivations, amongst o h- 
ers (Flickinger, Oepen, Uszkoreit, & Tsujii, 2000). 
162 
tional subsumption test for typed feature structures, 
which we use in a bottom-up, chart-based parsing 
algorithm incorporating novel, efficient accounting 
mechanisms to guarantee minimal chart size (Sec- 
tion 3). We present a full-scale evaluation of the 
techniques on a large corpus (Section 4), and com- 
plete the picture with an empirically-based discus- 
sion of grammar estrictors and parsing strategies 
(Section 5). 
2 E f f i c ient  Subsumpt ion  and  
Equ iva lence  A lgor i thms 
Our feature structure subsumption algorithm 2 as- 
sumes totally well-typed structures (Carpenter, 
1992) and employs similar machinery to the 
quasi-destructive unification algorithm described by 
Tomabechi (1991). In particular, it uses temporary 
pointers in dag nodes, each pointer tagged with a 
generation counter, to keep track of intermediate 
results in processing; incrementing the generation 
counter invalidates all temporary pointers in a sin- 
gle operation. But whereas quasi-destructive unifi- 
cation makes two passes (determining whether the 
unification will be successful and then copying out 
the intermediate representation) the subsumption 
algorithm makes only one pass, checking reentran- 
cies and type-supertype r lationships at the same 
time. 3 The algorithm, shown in Figure 1, also si- 
multaneously tests if both feature structures ub- 
sume each other (i.e. they are equivalent), if either 
subsumes the other, or if there is no subsumption 
relation between them in either direction. 
The top-level entry point dag-subsumes-pO and
subsidiary function dag-subsumes-pO 0 each return 
two values, held in variables \]orwardp and back- 
wardp, both initially true, recording whether it is 
possible that the first dag subsumes the second 
and/or vice-versa, respectively. When one of these 
possibilities has been ruled out the appropriate vari- 
able is set to false; in the statement of the algorithm 
the two returned values are notated as a pair, i.e. 
(/orwardp, backwardp). If at any stage both vari- 
ables have become set to false the possibility of sub- 
sumption in both directions has been ruled out so 
the algorithm exits. 
The (recursive) subsidiary function dag-subsumes- 
pO 0 does most of the work, traversing the two input 
2Although independently-developed implementations of 
essentially the same algorithm can be found in the source code 
of The Attribute Logic Engine (ALE) version 3.2 (Carpenter 
& Penn, 1999) and the SICStus Prolog term utilities library 
(Penn, personal communication), we believe that there is no 
previous published escription of the algorithm. 
3Feature structure F subsumes feature structure G iff: 
(1) if path p is defined in F then p is also defined in G and 
the type of the value of p in F is a supertype or equal to the 
value in G, and (2) all paths that are reentrant in F are also 
reentrant in G. 
dags in step. First, it checks whether the current 
node in either dag is involved in a reentrancy that 
is not present in the other: for each node visited 
in one dag it adds a temporary pointer (held in the 
'copy' slot) to the corresponding node in the other 
dag. If a node is reached that already has a pointer 
then this is a point of reentrancy in the dag, and 
if the pointer is not identical to the other dag node 
then this reentrancy is not present in the other dag. 
In this case the possibility that the former dag sub- 
sumes the latter is ruled out. After the reentrancy 
check the type-supertype r lationship between the 
types at the current nodes in the two dags is deter- 
mined, and if one type is not equal to or a supertype 
of the other then subsumption cannot hold in that 
direction. Finally, after successfully checking the 
type-supertype r lationships, the function recurses 
into the arcs outgoing from each node that have the 
same label. Since we are assuming totally well-typed 
feature structures, it must be the case that either the 
sets of arc labels in the two dags are the same, or 
one is a strict superset of the other. Only arcs with 
the same labels need be processed; extra arcs need 
not since the type-supertype check at the two nodes 
will already have determined that the feature struc- 
ture containing the extra arcs must be subsumed by 
the other, and they merely serve to further specify 
it and cannot affect the final result. 
Our implementation f the algorithm contains ex- 
tra redundant but cheap optimizations which for rea- 
sons of clarity are not shown in figure 1; these in- 
clude tests that forwardp is true immediately before 
the first supertype check and that backwardp is true 
before the second. 4 
The use of temporary pointers means that the 
space complexity of the algorithm is linear in the 
sum of the sizes of the feature structures. However, 
in our implementation the 'copy' slot that the point- 
ers occupy is already present in each dag node (it is 
required for the final phase of unification to store 
new nodes representing equivalence classes), so in 
practice the subsumption test does not allocate any 
new storage. All pointer references take constant 
time since there are no chains of 'forwarded' point- 
ers (forwarding takes place only during the course of 
unification and no forwarded pointers are left after- 
wards). Assuming the supertype tests can be carried 
4There is scope for further optimisation of the algorithm in 
the case where dagl and dag2 are identical: full processing in- 
side the structure is not required (since all nodes inside it will 
be identical between the two dags and any strictly internal 
reentrancies will necessarily be the same), but we would still 
need to assign temporary pointers inside it so that any exter- 
nal reentrancies into the structure would be treated correctly. 
In our tests we have found that as far as constituents hat are 
candidates for local ambiguity packing are concerned there is 
in fact little equality of structures between them, so special 
equality processing does not justify the extra complication. 
163 
1 procedure dag-subsumes-p(dagl,dag2) --_
2 (forwardp, backwardp) <-- { establish context for non-local exit} 
3 catch with tag 'fail' dag-subsumes-pO(dagl, dag2, true, true); 
4 invalidate-temporary-pointers(); {reset emporary 'copy' pointers} 
5 return (forwardp, backwardp); 
6 end 
7 procedure dag-subsumes-pO(dagl,dag2,forwardp, backwardp) -
8 if (dagl.copy isempty) then dagl.copy <--- dag2; {check reentraneies} 
9 else if~dagl.copy ~ dag2) then forwardp <-- false; f i  
10 if  (dag2.copy is empty) then dag2.copy ~- dagl; 
11 else i f  (dag2.copy p dagl) then backwardp ~- false; fi 
12 if  (forwardp = false and backwardp = false) then 
13 throw (false, false) with tag 'fail'; {reentrancy check failed} 
14 fi 
15 if (not supertype-or-equal-p(dagl.type, dag2.type)) then forwardp +- false; fi {check types} 
16 if (not supertype-or-equal-p(dag2.type, dagl.type)) then backwardp <-- false; fl 
17 if (forwardp = false and backwardp = false) then 
18 throw (false, false) with tag 'fail'; {no subtype relations} 
19 fi 
20 for each arc in intersect(dagl.arcs, dag2.arcs) do {check shared arcs recursively} 
21 (forwardp, backwardp) <- 
22 dag-subsumes-pO(destination of arc for dagl, destination of arc for dag2, forwardp, backwardp); 
23 od  
24 return (forwardp, backwardp); {signal result to caller} 
25 end 
Figure 1: Bidirectional, linear-time feature structure subsumption (and equivalence) algorithm. 
out in constant time (e.g. by table lookup), and that 
the grammar allows us to put a small constant upper 
bound on the intersection ofoutgoing arcs from each 
node, the processing in the body of dag-subsumes- 
pO 0 takes unit time. The body may be executed up 
to N times where N is the number of nodes in the 
smaller of the two feature structures. So overall the 
algorithm has linear time complexity. In practice, 
our implementation (i the environment described in 
Section 4) performs of the order of 34,000 top-level 
feature structure subsumption tests per second. 
3 Ambiguity Packing in the Parser  
Moore and Alshawi (1992) and Carroll (1993) have 
investigated local ambiguity packing for unification 
grammars with CF backbones, using CF category 
equality and feature structure subsumption to test 
if a newly derived constituent can be packed. If a 
new constituent is equivalent to or subsumed by an 
existing constituent, then it can be packed into the 
existing one and will take no further part in pro- 
cessing. However, if the new constituent subsumes 
an existing one, the situation is not so straightfor- 
ward: either (a) no packing takes place and the new 
constituent forms a separate dge (Carroll, 1993), or 
(b) previous processing involving the old constituent 
is undone or invalidated, and it is packed into the 
new one (Moore & Alshawi, 1992; however, it is un- 
clear whether they achieve maximal compactness in 
practice: see Table 1). In the former case the parse 
forest produced will not be optimally compact; in 
the latter it will be, but maintaining chart consis- 
tency and parser correctness becomes a non-trivial 
problem. Packing of a new edge into an existing one 
we call proactive (or forward) packing; for the more 
complex situation involving a new edge subsuming 
an existing one we introduce the term retroactive (or 
backward) packing. 
Several issues arise when packing an old edge (old) 
into one that was newly derived (new) retroactively: 
(i) everything derived from old (called derivatives of 
old in the following) must be invalidated and ex- 
cluded from further processing (as new is known 
to generate more general derivatives); and (ii) all 
pending computation i volving old and its deriva- 
tives has to be blocked efficiently. Derivatives of 
old that are invalidated because of retroactive pack- 
ing may already contain packed analyses, however, 
which still represent valid ambiguity. These need to 
be repacked into corresponding derivatives of new 
when those become available. In turn, derivatives of 
old may have been packed already, such that they 
need not be available in the chart for subsequent sub- 
sumption tests. Therefore, the parser cannot simply 
delete verything derived from old when it is packed; 
instead, derivatives must be preserved (but blocked) 
164 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
procedure  block(edge, mark) - 
i f  (edge.frozen = false or mark = freeze) then edge.frozen +- mark; fi 
for each parent in edge.parents do block(parent, freeze); od 
end 
procedure packed-edge-p(new) - 
for each old in chart\[new.start\]\[new.end\] do 
(forwardp, backwardp) ~- dag-subsumes-p(old.dag, new.dag); 
i f  (forwardp = true and old.frozen = fa/se) then  
old.packed ~-- (new I old.packed); 
re turn  true; 
fi 
i f  (backwardp) then  
new.packed ~-- (new.packed @old.packed); 
old.packed +-- 0; 
{mark current edge} 
{ recursively freeze derivatives} 
{passive dges with same span} 
{ test category subsumption} 
{ equivalent or proactive packing} 
{pack 'new' into 'old'} 
{return to caller; signal success} 
{retroactive packing} 
{raise all packings into new host} 
if (old.frozen = false) then  new.packed e- (old I new.packed); fi {pack 'old' into 'new'} 
block(old, frost); {frost 'old' and freeze derivatives} 
delete(old, chart); {remove 'old' from the chart} 
fl 
od 
re turn  false; {signal failure to pack 'new' to caller} 
end 
Figure 2: Algorithm called on each newly derived edge to achieve maximal packing. 
until the derivations have been recomputed on the 
basis of new. 5 As new is equivalent to or more gen- 
eral than old it is guaranteed to derive at least the 
same set of edges; furthermore, the derivatives of 
new will again be equivalent to or more general than 
the corresponding edges derived from old. 
The procedure packed-edge-p(), sketched in Fig- 
ure 2, achieves pro- and retroactive packing with- 
out significant overhead in the parser; the algorithm 
can be integrated with arbitrary bottom-up (chart- 
based) parsing strategies. The interface assumes 
that the parser calls packed-edge-pO on each new 
edge new as it is derived; a return value of true indi- 
cates that new was packed proactively and requires 
no further processing. Conversely, a false return 
value from packed-edge-p 0 signals that new should 
subsequently undergo regular processing. The sec- 
ond part of the interface builds on notions we call 
frosting and freezing, meaning temporary and per- 
mament invalidation of edges, respectively. As a 
side-effect of calls to packed-edge-p(), a new edge 
can cause retroactive packing, resulting in the dele- 
5The situation is simpler in the CLE parser (Moore & Al- 
shawl, 1992) because constituents and dominance relations 
are separated in the chart. The CLE encoding, in fact, does not 
record the actual daughters used in building a phrase (e.g. as 
unique references or pointers, as we do), but instead preserves 
the category information (i.e. a description) of those daugh- 
ters. Hence, in extracting complete parses from the chart, 
the CLE has to perform (a limited) search with re-unification 
of categories; in this respect, the CLE parse forest still is an 
underspecified representation f the set of analyses, whereas 
our encoding (see below) facilitates unpacking without extra 
search. 
tion of one or more existing edges from the chart 
and blocking of derivatives. Whenever the parser 
accesses the chart (i.e. in trying to combine edges) 
or retrieves a task from the agenda, it is expected 
to ignore all edges and parser tasks involving such 
edges that have a non-null 'frozen' value. When an 
existing edge old is packed retroactively, it is frosted 
and ignored by the parser; as old now represents lo- 
cal ambiguity, it still has to be taken into account 
when the parse forest is unpacked. Derivatives of 
old, on the other hand, need to be invalidated in 
both further parsing and later unpacking, since they 
would otherwise give rise to spurious analyses; ac- 
cordingly, such derivatives are frozen permanently. 
Frosting and freezing is done in the subsidiary pro- 
cedure block () that walks up the parent link recur- 
sively, storing a mark into the 'frozen' slot of edges 
that distinguishes between temporary frosting (in 
the top-level call) and permanent freezing (in recur- 
sire calls). 
For a newly derived edge new, packed-edge-pO 
tests mutual subsumption against all passive edges 
that span the same portion of the input string. 
When forward subsumption (or equivalence) is de- 
tected and the existing edge old is not blocked, reg- 
ular proactive packing is performed (adding new to 
the packing list for old) and the procedure returns 
immediately. 6 In the case of backward subsump- 
6packing an edge el into another edge e2 logically means 
that e2 will henceforth serve as a representative for el and 
the derivation(s) that it encodes. In practice, el is removed 
from the chart and ignored in subsequent parser action and 
subsumption tests. Only in unpacking the parse forest will 
165 
20000 
17500 
15000 
12500 
10000 
7500 
5000 
2500 - 
0"  
No Chart Packing I
e passive edges \] 
? * .o . . .  . |  "'. ? 
, , i , , , = , i i = , , 
3 5 7 9 11 13 15 17 19 21 23 25 
String Length (in words) 
Figure 3: Effects of maximal ambiguity packing 
tion, analyses packed into old are raised into new 
(using the append operator '~' because new can at- 
tract multiple xisting edges in the loop); old itself is 
only packed into new when it is not blocked already. 
Finally, old is frosted, its derivatives are recursively 
frozen, and old is deleted from the chart. In contrast 
to proactive packing, the top-level loop in the pro- 
cedure continues so that new can pick up additional 
edges retroactively. However, once a backward sub- 
sumption is detected, it follows that no proactive 
packing can be achieved for new, as the chart can- 
not contain an edge that is more general than old. 
4 Empi r i ca l  Resu l t s  
We have carried out an evaluation of the algo- 
rithms presented above using the LinGO grammar 
(Flickinger & Sag, 1998), a publicly-available, multi- 
purpose, broad-coverage HPSG of English developed 
at CSLI Stanford. With roughly 8,000 types, an av- 
erage feature structure size of around 300 nodes, and 
64 lexical and grammar rules (fleshing out the inter- 
action of HPSG ID schemata, wellformedness prin- 
ciples, and LP constraints), LinGO is among the 
largest HPSG grammars available. We used the LKB 
system (Copestake, 1992, 1999) as an experimen- 
tation platform since it provides a parameterisable 
bottom-up chart parser and precise, fine-grained 
profiling facilities (Oepen & Flickinger, 1998). 7 All 
of our results were obtained in this environment, 
running on a 300 Mhz UltraSparc, and using a bal- 
anced test set of 2,100 sentences extracted from 
VerbMobil corpora of transcribed speech: input 
lengths from 1 to 20 words are represented with 100 
test items each; although sentences in the corpus 
range up to 36 words in length there are relatively 
few longer than 20 words. 
the category of el and its decomposition(s) in daughter edges 
(and corresponding subtrees) be used again, to multiply out 
and project local ambiguity. 
;'The LinGO grammar and LKB software are publicly avail- 
able at 'h t tp : / / l i ngo .  stanford,  edu/'. 
20000 
17500 
15000 
12500 
10000 
7500 
5000 
2500 
O~ 
Pro- and Retroactive Packing I
\] o passive edges \] 
A ,, .kit atAatmt'lLttvtd~l~lL 
, m ~  w w w w w w w w l v l ; w w ~  
"} A 1'3 1'5 1'7 2'1 2'3 25 
String Length (in words) 
on the total chart size (truncated above 25 words). 
Figure 3 compares total chart size (in all-paths 
mode) for the regular LKB parser and our variant 
with pro- and retroactive packing enabled. Factor- 
ing ambiguity reduces the number of passive dges 
by a factor of more than three on average, while for 
a number of cases the reduction is by a factor of 30 
and more. Compared to regular parsing, the rate of 
increase of passive chart items with respect o sen- 
tence length is greatly diminished. 
To quantify the degree of packing we achieve 
in practice, we re-ran the experiment reported by 
Moore and Alshawi (1992): counting the number of 
nodes required to represent all readings for a simple 
declarative sentence containing zero to six preposi- 
tional phrase (PP) modifiers. The results reported 
by Moore and Alshawi (1992) (using the CLE gram- 
mar of English) and those obtained using pro- and 
retroactive packing with the LinGO grammar are 
presented in Table 1. 8 Although the comparison 
involves different grammars we believe it to be in- 
structive, since (i) both grammars have comprehen- 
sive coverage, (ii) derive the same numbers of read- 
ings for all test sentences in this experiment, (iii) 
require (almost) the same number of nodes for the 
basic cases (zero and one PP), (iv) exhibit a similar 
size in nodes for one core PP (measured by the in- 
crement from n = 0 to n = 1), and (v) the syntactic 
simplicity of the test material hardly allows crosstalk 
SMoore and Alshawi (1992) use the terms 'node' and 
'record' interchangeably in their discussion of packing, where 
the CLE chart is comprised of separate con(stituent) and 
ana(lysis) entries for category and dominance information, 
respectively. It is unclear whether the counting of 'packed 
nodes' in Moore and Alshawi (1992) includes con records or 
not, since only maa records are required in parse tree recovery. 
In any case, both types of chart record need to be checked by 
subsumption as new entries are added to the chart. Con- 
versely, in our setup each edge represents not only the node 
category, but also pointers to the daughter(s) that gave rise 
to this edge, and moreover, where applicable, a list of packed 
edges that are subsumed by the category (but not necessarily 
by the daughters). For the LKB, the column 'result edges' in 
Table 1 refers to the total number of edges in the chart that 
contribute to at least one complete analysis. 
166 
Kim saw a cat (in the hotel) n 
CPU Time 
n readings parse unpack I plain 
msec msec msec 
0 1 
1 2 
2 5 
3 14 
4 42 
5 132 
6 429 
Moore & Alshawi 
packed nodes 
10 1.0 
21 2.1 
38 3.8 
62 6.2 
94 9.4 
135 13.5 
186 18.6 
Our Method 
result edges 
11 1.0 
23 2.1 
38 3.5 
56 5.1 
77 7.0 
101 9.2 
128 11.6 
210 
340 
460 
600 
870 
1,150 
1,460 
10 
40 
80 
200 
590 
1,860 
5,690 
180 
290 
530 
1,180 
2,990 
8,790 
28,160 
Table h Comparison of retroactive packing vs. the method used by Moore and Alshawi (1992); columns 
labeled '+' show the relative increase of packed nodes (result edges) normalised to the n -- 0 baseline. 
with other grammatical phenomena. Comparing rel- 
ative packing efficiency with increasing ambiguity 
(the columns labeled ' - '  in Table 1), our method ap- 
pears to produce a more compact representation f 
ambiguity than the CLE, and at the same time builds 
a more specific representation f the parse forest hat 
can be unpacked without search. To give an impres- 
sion of parser throughput, Table 1 includes timings 
for our parsing and unpacking (validation) phases, 
contrasted with the plain, non-packing LKB parser: 
as would be expected, parse time increases linearly 
in the number of edges, while unpacking costs re- 
flect the exponential increase in total numbers of 
analyses; the figures show that our packing scheme 
achieves a very significant speedup, even when un- 
packing time is included in the comparison. 
5 Choos ing  the  Grammar  Rest r i c to r  
and Pars ing  S t ra tegy  
In order for the subsumption relation to apply mean- 
ingfully to HPSG signs, two conditions must be met. 
Firstly, parse tree construction must not be dupli- 
cated in the feature structures (by means of the 
HPSG DTRS feature) but be left to the parser (i.e. 
recorded in the chart); this is achieved in a stan- 
dard way by feature structure restriction (Shieber, 
1985) applied to all passive dges. Secondly, the pro- 
cessing of constraints hat do not restrict he search 
space but build up new (often semantic) structure 
should be postponed, since they are likely to inter- 
fere with subsumption. For example, analyses that 
differ only with respect o PP attachment would 
have the same syntax, but differences in semantics 
may prevent hem being packed. This problem can 
be overcome by using restriction to (temporarily) re- 
move such (semantic) attributes from lexical entries 
and also from the rule set, before they are input 
to the parser in the initial parse forest construction 
phase. The second, unpacking phase of the parser e- 
verts to the unrestricted constraint set, so we can al- 
low overgeneration in the first phase and filter glob- 
ally inconsistent analyses during unpacking. Thus, 
the right choice of grammar restrictor can be viewed 
as an empirical rather than analytical problem. 
Table 2 summarizes packing efficiency and parser 
performance for three different restrictors (labeled 
no, partial, and full semantics, respectively); to 
gauge effects of input complexity, the table is fur- 
ther subdivided by sentence l ngth into two groups 
(of around 1,000 sentences each). Compared to reg- 
ular parsing, packing with the full semantics in place 
is not effective: the chart size is reduced slightly, but 
the extra cost for testing subsumption i creases total 
parse times by a factor of more than four. Eliminat- 
ing all semantics (i.e. the entire HPSG C0NT value), on 
the other hand, results in overgeneralisation: with 
less information i the feature structures we achieve 
the highest number of packings, but at the same 
time rules apply much more freely, resulting in a 
larger chart compared to parsing with a partial se- 
mantics; moreover, unpacking takes longer because 
the parse forest now contains inconsistent analyses. 
Restricting compositional semantics but preserving 
attributes that participate in selection and agree- 
ment results in minimal chart size and parsing time 
(shown in the partial semantics figures) for both di- 
visions of the test corpus. 
The majority of packings involve equivalent fea- 
ture structures which suggests that unpacking could 
be greatly simplified if the grammar restrictor was 
guaranteed to preserve the generative capacity of 
the grammar (in the first parsing phase); then, only 
packings involving actual subsumption would have 
to be validated in the unpacking phase. 9 Finally, 
9There is room for further investigation here: partly for 
theory-internal reasons, current development of the LinGO 
grammar isworking towards astricter separation of restrictive 
(selectional) and constructive (compositional) constraints in 
167 
1-10  
words 
I Passive Packed 
Parser Edges Trees 
no semantics 
partial semantics 
full semantics 
no packing 
116 
111 
149 
160 
0.9 
0.8 
2.8 
5.6 
no semantics 622 1.2 
> 10 partial semantics 575 1.0 
words full semantics 1693 33-9 
no packing 2075 99-9 
I Packings CPU Time (sec) 
= I -D? p se I unpack 
15"5 4"1 2"6 1"8 0"37 \] 0"05 
12"0 3"6 2"4 1"4 0"33 1 0"05 
2"1 0"4 0"2 0"1 0"60 0"04 
. . . .  0"44 
179"0 42"1 23"8 26"0 2"37 0"70 
134"9 35"0  20"6 18"9 1"97 0"63 
38"3 3"4 2"9 3"2 29"40 0"56 
. . . .  6"46 
Table 2: Contrasting various grammar estrictors on short (top) and medium-length (bottom) inputs; all 
numbers are averaged over 1,000 items per class; packings are, from left to right: equivalence ( ' - ' ) ,  pro- 
('-~') and retroactive ( 'r ' )  packings, and the number of edges that were frozen ('?'). 
we note that the number of retroactive packings is 
relatively small, and on average ach such packing 
leads to only one previously derived edge being in- 
validated. This, of course, is a function of the order 
in which edges are derived, i.e. the parsing strategy. 
All the results in Table 2 were obtained with a 
'right corner' strategy which aims to exhaust compu- 
tation for any suffix of the input string before mov- 
ing the input pointer to the left; this is achieved by 
start (where start means of a scoring function end - -W- 
and end are the vertices of the derivation that would 
result from the computation, and n is the total input 
length) that orders parser tasks in the agenda. How- 
ever, we have observed (Oepen & Callmeier, 2000) 
that HPSG-type, highly lexicalized grammars bene- 
fit greatly from a bidirectional, 'key'-driven, active 
parsing regime, since they often employ rules with 
underspecified arguments that are only instantiated 
by coreference with other daughters (where the 'key' 
daughter is the linguistic head in many but not all 
constructions). This requirement and the general 
non-predictability of categories derived for any to- 
ken substring (in particular with respect o unary 
rule applications), means that a particular parsing 
strategy may reduce retroactive packing but cannot 
avoid it in general. With pro- and retroactive pack- 
ing and the minimal accounting overhead, we find 
overall parser throughput to be very robust against 
variation in the parsing strategy. Lavie and Rosd 
(2000) present heuristics for ordering parser actions 
to achieve maximally compact parse forests--though 
only with respect to a CF category backbone---in the 
absence of retroactive packing; however, the tech- 
niques we have presented here allow local ambigu- 
ity packing and parser tuning--possibly including 
priority-driven best-first search--to be carried out 
mostly independently of each other. 
the grammar and underlying semantic theory. We expect hat 
our approach to packing will benefit from these developments. 
6 Conc lus ions  
We have presented novel algorithms for efficient sub- 
sumption checking and pro- and retroactive local 
ambiguity packing with large feature structures, and 
have provided strong empirical evidence that our 
approach can be applied beneficially to chart pars- 
ing with a large, broad-coverage HPSG of English. 
By comparison to previous work in unification-based 
parsing we have demonstrated that pro- and retroac- 
tive packing are well-suited to achieve optimal pack- 
ing; furthermore, xperimental results obtained with 
a publicly-available HPSG processing platform con- 
firm that ambiguity packing can greatly reduce av- 
erage parse complexity for this type of grammars. 
In related work, Miyao (1999) describes an ap- 
proach to packing in which alternative feature struc- 
tures are represented as packed, distributed isjunc- 
tions of feature structure fragments. Although the 
approach may have potential, the shifting of com- 
plex accounting into the unification algorithm is at 
variance with the findings of Kiefer et al (1999), 
who report large speed-ups from the elimination of 
disjunction processing during unification. Unfortu- 
nately, the reported evaluation measures and lack of 
discussion of parser control issues are insufficient to 
allow a precise comparison. 
We intend to develop the approach presented in 
this paper in several directions. Firstly, we will en- 
hance the unpacking phase to take advantage of the 
large number of equivalence packings we observe. 
This will significantly reduce the amount of work it 
needs to do. Secondly, many application contexts 
and subsequent layers of semantic processing will 
not require unfolding the entire parse forest; here, 
we need to define a selective, incremental unpack- 
ing procedure. Finally, applications like VerbMo- 
bil favour prioritized best-first rather than all-paths 
parsing. Using slightly more sophisticated account- 
ing in the agenda, we plan to investigate priority 
168 
propagation i a best-first variant of our parser. 
Acknowledgements  
We are grateful to Ulrich Callmeier, Ann Copestake, 
Dan Flickinger, and three anonymous reviewers for 
comments on a draft of the paper, to Bob Moore for 
a detailed explanation of the workings of the CLE 
parser, and to Gerald Penn for information about 
related implementations of the subsumption algo- 
rithm. The research was supported by the Deutsche 
Forschungsgemeinschaft as part of the Collaborative 
Research Division Resource-Adaptive Cognitive Pro- 
cesses, project B4 (PERFORM); and by a UK EPSRC 
Advanced Fellowship to the second author. 
Re ferences  
Alshawi, H. (Ed.). (1992). The Core Language En- 
gine. Cambridge, MA: MIT Press. 
Billot, S., & Lang, B. (1989). The structure of 
shared forests in ambiguous parsing. In Proceed- 
ings of the 27th Meeting of the Association for 
Computational Linguistics (pp. 143-151). Van- 
couver, BC. 
Carpenter, B. (1992). The logic of typed feature 
structures. Cambridge, UK: Cambridge Univer- 
sity Press. 
Carpenter, B., & Penn, G. (1999)? ALE. The At- 
tribute Logic Engine. User's guide version 3.2. 
(Universit~it Tfibingen: http://wwww, sfs .  nphi l  
? un i - tueb ingen,  de/~gpenn/ale, html)  
Carroll, J. (1993). Practical unification-based 
parsing of natural language (Technical Re- 
port # 314). Cambridge, UK: Computer 
Laboratory, Cambridge University. (Online 
at: f tp  : / / f tp .  el .  cam. ac. uk /papers / repor ts /  
TR314-j ac-pract ical-unif-pars ing. ps. gz) 
Copestake, A. (1992). The ACQUILEX LKB. Rep- 
resentation issues in semi-automatic a quisition of 
large lexicons. In Proceedings of the 3rd A CL Con- 
ference on Applied Natural Language Processing 
(pp. 88-96). Trento, Italy. 
Copestake, A. (1999). The (new) LKB sys- 
tem. User's guide. (CSLI, Stanford Uni- 
versity: http ://www-csli. stanford, edu/-~aac/ 
ikb. html) 
Earley, J. (1970). An efficient context-free parsing 
algorithm. Communications of the ACM, 13 (2), 
94 - 102. 
Flickinger, D., Oepen, S., Uszkoreit, H., & Tsu- 
jii, J. (Eds.). (2000). Journal of Natural Lan- 
guage Engineering. Special Issue on Efficient pro- 
cessing with HPSG: Methods, systems, evaluation. 
Cambridge, UK: Cambridge University Press. (in 
preparation) 
Flickinger, D. P., & Sag, I. A. (1998). Linguis- 
tic Grammars Online. A multi-purpose broad- 
coverage computational grammar of English. In 
CSLI Bulletin 1999 (pp. 64-68). Stanford, CA: 
CSLI Publications. 
Kiefer, B., Krieger, H.-U., Carroll, J., & Malouf, R. 
(1999). A bag of useful techniques for efficient and 
robust parsing. In Proceedings of the 37th Meeting 
of the Association for Computational Linguistics 
(pp. 473-480). College Park, MD. 
Lavie, A., & Ros~, C. (2000). Optimal ambiguity 
packing in context-free parsers with interleaved 
unification. In Proceedings of the 6th Interna- 
tional Workshop on Parsing Technologies (pp. 
147-158). Trento, Italy. 
Maxwell III, J. T., & Kaplan, R. M. (1995). A 
method for disjunctive constraint satisfaction. In 
M. Dalrymple, R. M. Kaplan, J. T. Maxwell III, 
& A. Zaenen (Eds.), Formal issues in Lexical- 
Functional Grammar (pp. 381-401). Stanford, 
CA: CSLI Publications. 
Miyao, Y. (1999). Packing of feature structures for 
efficient unification of disjunctive feature struc- 
tures. In Proceedings of the 37th Meeting of the 
Association for Computational Linguistics (pp. 
579-84). College Park, MD. 
Moore, R. C., & Alshawi, H. (1992). Syntactic 
and semantic processing. In H. Alshawi (Ed.), 
The Core Language Engine (pp. 129-148). Cam- 
bridge, MA: MIT Press. 
Oepen, S., & Callmeier, U. (2000). Measure for 
measure: Parser cross-fertilization. Towards in- 
creased component comparability and exchange. 
In Proceedings of the 6th International Workshop 
on Parsing Technologies (pp. 183-194). Trento, 
Italy. 
Oepen, S., & Flickinger, D. P. (1998). Towards ys- 
tematic grammar profiling. Test suite technology 
ten years after. Journal of Computer Speech and 
Language, 12 (4) (Special Issue on Evaluation), 
411-436. 
Shieber, S. M. (1985). Using restriction to extend 
parsing algorithms for complex feature-based for- 
malisms. In Proceedings of the 23rd Meeting of the 
Association for Computational Linguistics (pp. 
145-152). Chicago, IL. 
Tomabechi, H. (1991). Quasi-destructive graph uni- 
fication. In Proceedings of the 29th Meeting of the 
Association for Computational Linguistics (pp. 
315- 322). Berkeley, CA. 
Tomita, M. (1985). An efficient context-free parsing 
algorithm for natural anguages. In Proceedings of 
the 9th International Joint Conference on Artifi- 
cial Intelligence (pp. 756- 764). Los Angeles, CA. 
Wahlster, W. (1997). VerbMobil -- Erken- 
hung, Analyse, Transfer, Generierung und Syn- 
these yon Spontansprache (VerbMobil Report 
# 198). Saarbriicken, Germany: Deutsches 
Forschungszentrum fiir Kiinstliche Intelligenz 
GmbH. 
169 
High Precision Extraction of Grammatical Relations
John Carroll
Cognitive and Computing Sciences
University of Sussex
Falmer, Brighton
BN1 9QH, UK
Ted Briscoe
Computer Laboratory
University of Cambridge
JJ Thomson Avenue
Cambridge CB3 0FD, UK
Abstract
A parsing system returning analyses in the form of
sets of grammatical relations can obtain high pre-
cision if it hypothesises a particular relation only
when it is certain that the relation is correct. We
operationalise this technique?in a statistical parser
using a manually-developed wide-coverage gram-
mar of English?by only returning relations that
form part of all analyses licensed by the grammar.
We observe an increase in precision from 75% to
over 90% (at the cost of a reduction in recall) on a
test corpus of naturally-occurring text.
1 Introduction1
Head-dependent relationships (possibly labelled
with a relation type) have been advocated as a use-
ful level of representation for grammatical struc-
ture in a number of different large-scale language-
processing tasks. For instance, in recent work on
statistical treebank grammar parsing (e.g. Collins,
1999) high levels of accuracy have been reached
using lexicalised probabilistic models over head-
dependent tuples. Bouma, van Noord and Mal-
ouf (2001) create dependency treebanks semi-auto-
matically in order to induce dependency-based sta-
tistical models for parse selection. Lin (1998),
Srinivas (2000) and others have evaluated the ac-
curacy of both phrase structure-based and depen-
dency parsers by matching head-dependent rela-
tions against ?gold standard? relations, rather than
matching (labelled) phrase structure bracketings.
Research on unsupervised acquisition of lexical in-
formation from corpora, such as argument structure
of predicates (Briscoe and Carroll, 1997; McCarthy,
2000), word classes for disambiguation (Clark and
Weir, 2001), and collocations (Lin 1999), has used
grammatical relation/head/dependent tuples. Such
1A previous version of this paper was presented at
IWPT?01; this version contains new experiments and results.
tuples also constitute a convenient intermediate rep-
resentation in applications such as information ex-
traction (Palmer et al, 1993; Yeh, 2000), and docu-
ment retrieval on the Web (Grefenstette, 1997).
A variety of different approaches have been taken
for robust extraction of relation/head/dependent tu-
ples, or grammatical relations, from unrestricted
text. Dependency parsing is a natural technique
to use, and there has been some work in that area
on robust analysis and disambiguation (e.g. Laf-
ferty, Sleator and Temperley, 1992; Srinivas, 2000).
Finite-state approaches (e.g. Karlsson et al, 1995;
A??t-Mokhtar and Chanod, 1997; Grefenstette, 1998)
have used hand-coded transducers to recognise lin-
ear configurations of words and part of speech la-
bels associated with, for example, subject/object-
verb relationships. An intermediate step may be to
mark nominal, verbal etc. ?chunks? in the text and to
identify the head word of each of the chunks. Sta-
tistical finite-state approaches have also been used:
Brants, Skut and Krenn (1997) train a cascade of
Hidden Markov Models to tag words with their
grammatical functions. Approaches based on mem-
ory based learning have also used chunking as a
first stage, before assigning grammatical relation la-
bels to heads of chunks (Argamon, Dagan and Kry-
molowski, 1998; Buchholz, Veenstra and Daele-
mans, 1999). Blaheta and Charniak (2000) assume
a richer input representation consisting of labelled
trees produced by a treebank grammar parser, and
use the treebank again to train a further procedure
that assigns grammatical function tags to syntac-
tic constituents in the trees. Alternatively, a hand-
written grammar can be used that produces ?shal-
low? and perhaps partial phrase structure analyses
from which grammatical relations are extracted (e.g.
Carroll, Minnen and Briscoe, 1998; Lin, 1998).
Recently, Schmid and Rooth (2001) have de-
scribed an algorithm for computing expected gov-
ernor labels for terminal words in labelled headed
parse trees produced by a probabilistic context-free
grammar. A governor label (implicitly) encodes a
grammatical relation type (such as subject or ob-
ject) and a governing lexical head. The labels are
expected in the sense that each is weighted by the
sum of the probabilities of the trees giving rise to
it, and are computed efficiently by processing the
entire parse forest rather than individual trees. The
set of terminal/relation/governing-head tuples will
not typically constitute a globally coherent analy-
sis, but may be useful for interfacing to applications
that primarily accumulate fragments of grammati-
cal information from text (such as for instance in-
formation extraction, or systems that acquire lexical
data from corpora). The approach is not so suit-
able for applications that need to interpret complete
and consistent sentence structures (such as the anal-
ysis phase of transfer-based machine translation).
Schmid and Rooth have implemented the algorithm
for parsing with a lexicalised probabilistic context-
free grammar of English and applied it in an open
domain question answering system, but they do not
give any practical results or an evaluation.
In the paper we investigate empirically Schmid
and Rooth?s proposals, using a wide-coverage pars-
ing system applied to a test corpus of naturally-
occurring text, extend it with various thresholding
techniques, and observe the trade-off between pre-
cision and recall in grammatical relations returned.
Using the most conservative threshold results in a
parser that returns only grammatical relations that
form part of all analyses licensed by the grammar.
In this case, precision rises to over 90%, as com-
pared with a baseline of 75%.
2 The Analysis System
In this investigation we extend a statistical shallow
parsing system for English developed originally by
Carroll, Minnen and Briscoe (1998). Briefly, the
system works as follows: input text is labelled with
part-of-speech (PoS) tags by a tagger, and these
are parsed using a wide-coverage unification-based
?phrasal? grammar of English PoS tags and punctu-
ation. For disambiguation, the parser uses a prob-
abilistic LR model derived from parse tree struc-
tures in a treebank, augmented with a set of lexical
entries for verbs, acquired automatically from a 10
million word sample of the British National Corpus
(Leech, 1992), each entry containing subcategori-
sation frame information and an associated proba-
bility. The parser is therefore ?semi-lexicalised? in
that verbal argument structure is disambiguated lex-
ically, but the rest of the disambiguation is purely
structural.
The coverage of the grammar?the proportion of
sentences for which at least one complete spanning
analysis is found?is around 80% when applied to
the SUSANNE corpus (Sampson, 1995). In addition,
the system is able to perform parse failure recov-
ery, finding the highest scoring sequence of phrasal
fragments (following the approach of Kiefer et al,
1999), and the system has produced at least partial
analyses for over 98% of the sentences in the written
part of the British National Corpus.
The parsing system reads off grammatical rela-
tion tuples (GRs) from the constituent structure tree
that is returned from the disambiguation phase. In-
formation is used about which grammar rules in-
troduce subjects, complements, and modifiers, and
which daughter(s) is/are the head(s), and which the
dependents. In Carroll et al?s evaluation the system
achieves GR accuracy that is comparable to pub-
lished results for other systems: extraction of non-
clausal subject relations with 83% precision, com-
pared with Grefenstette?s (1998) figure of 80%; and
overall F-score2 of unlabelled head-dependent pairs
of 80%, as opposed to Lin?s (1998) 83%3 and Srini-
vas?s (2000) 84% (this with respect only to binary
relations, and omitting the analysis of control rela-
tionships). Blaheta and Charniak (2000) report an
F-score of 87% for assigning grammatical function
tags to constituents, but the task, and therefore the
scoring method, is rather different.
For the work reported in this paper we have ex-
tended Carroll et al?s basic system, implementing
a version of Schmid and Rooth?s expected gover-
nor technique (see section 1 above) but adapted for
unification-based grammar and GR-based analyses.
Each sentence is analysed as a set of weighted GRs
where the weight associated with each grammati-
cal relation is computed as the sum of the proba-
bilities of the parses that relation was derived from,
divided by the sum of the probabilities of all parses.
So, if we assume that Schmid and Rooth?s example
sentence Peter reads every paper on markup has 2
parses, one where on markup attaches to the preced-
ing noun having overall probability     and the
other where it has verbal attachment with probabil-
ity     , then some of the weighted GRs would be
2We use the F 	 measure defined as 
Automatic Identification of Infrequent Word Senses
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In this paper we show that an unsupervised method for
ranking word senses automatically can be used to iden-
tify infrequently occurring senses. We demonstrate this
using a ranking of noun senses derived from the BNC
and evaluating on the sense-tagged text available in both
SemCor and the SENSEVAL-2 English all-words task.
We show that the method does well at identifying senses
that do not occur in a corpus, and that those that are erro-
neously filtered but do occur typically have a lower fre-
quency than the other senses. This method should be
useful for word sense disambiguation systems, allowing
effort to be concentrated on more frequent senses; it may
also be useful for other tasks such as lexical acquisition.
Whilst the results on balanced corpora are promising, our
chief motivation for the method is for application to do-
main specific text. For text within a particular domain
many senses from a generic inventory will be rare, and
possibly redundant. Since a large domain specific cor-
pus of sense annotated data is not available, we evaluate
our method on domain-specific corpora and demonstrate
that sense types identified for removal are predominantly
senses from outside the domain.
1 Introduction
Much about the behaviour of words is most appro-
priately expressed in terms of word senses rather
than word forms. However, an NLP application
computing over word senses is faced with consid-
erable extra ambiguity. There are systems which
can perform word sense disambiguation (WSD) on
the words in input text, however there is room for
improvement since the best systems on the English
SENSEVAL-2 all-words task obtained at most 69%
for precision and recall. Whilst there are systems
that obtain higher precision (Magnini et al, 2001),
these typically suffer from a low recall. WSD per-
formance is affected by the degree of polysemy, but
even more so by the entropy of the frequency distri-
butions of the words? senses (Kilgarriff and Rosen-
zweig, 2000) since the distribution for many words
is highly skewed. Many of the senses in such an
inventory are rare and WSD and lexical acquisition
systems do best when they take this into account.
There are many ways that the skewed distribution
can be taken into account. One successful approach
is to back-off to the first (predominant) sense (Wilks
and Stevenson, 1998; Hoste et al, 2001). Another
possibility would be concentrate the selection pro-
cess to senses with higher frequency, and filter out
rare senses. This is implicitly done by systems
which rely on hand-tagged training corpora, since
rare senses often do not occur in the available data.
In this paper we use an unsupervised method to rank
word senses from an inventory according to preva-
lence (McCarthy et al, 2004a), and utilise the rank-
ing scores to identify senses which are rare. We use
WordNet for our inventory, since it is widely used
and freely available, but our method could in prin-
ciple be used with another MRD (we comment on
this in the conclusions). We report work with nouns
here, and leave evaluation on other PoS for the fu-
ture.
Our approach exploits automatically acquired
thesauruses which provide ?nearest neighbours? for
a given word entry. The neighbours are ordered
in terms of the distributional similarity that they
share with the target word. The neighbours relate
to different senses of the target word, so for exam-
ple the word competition in such a thesaurus pro-
vided by Lin 1 has neighbours tournament, event,
championship and then further down the ordered list
we see neighbours pertaining to a different sense
competitor,...market...price war. Pantel and Lin
(2002) demonstrate that it is possible to cluster the
neighbours into senses and relate these to WordNet
senses. In contrast, we use the distributional sim-
ilarity scores of the neighbours to rank the various
senses of the target word since we expect that the
quantity and similarity of the neighbours pertain-
ing to different senses will reflect the relative dom-
inance of the senses. This is because there will
1Available from
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
be more data for the more prevalent senses com-
pared to the less frequent senses. We use a measure
of semantic similarity from the WordNet Similarity
package to relate the senses of the target word to the
neighbours in the thesaurus.
The paper is structured as follows. The ranking
method is described elsewhere (McCarthy et al,
2004a), but we summarise in the following section
and describe how ranking scores can be used for fil-
tering word senses. Section 3 describes two exper-
iments using the BNC for acquisition of the sense
rankings with evaluation using the hand-tagged data
in i) SemCor and ii) the English SENSEVAL-2 all-
words task. We demonstrate that the majority of
senses identified by the method do not occur in these
gold-standards, and that for those that do, only a
small percentage of the sense tokens would be re-
moved in error by filtering these senses. In section 4
we use domain labels produced by (Magnini and
Cavaglia`, 2000) to demonstrate differences in the
senses filtered for a sample of words in two domain
specific corpora. We describe some related work in
section 5 and conclude in section 6.
2 Method
McCarthy et al (2004a) describe a method to pro-
duce a ranking over senses and find the predominant
sense of a word just using raw text. We summarise
the method below, and describe how we use it for
identifying candidate senses for filtering.
2.1 Ranking the Senses
In order to rank the senses of a target word (e.g.
plant) we use a thesaurus acquired from automati-
cally parsed text (section 2.2 below). This provides
the  nearest neighbours to each target word (e.g.
factory, refinery, tree etc...) along with the distribu-
tional similarity score between the target word and
its neighbour. We then use the WordNet similar-
ity package (Patwardhan and Pedersen, 2003) (see
section 2.3) to give us a semantic similarity mea-
sure (hereafter referred to as the WordNet similarity
measure) to weight the contribution that each neigh-
bour (e.g. factory) makes to the various senses of
the target word (e.g. flora, industrial, actor etc...).
We take each sense of the target word (  ) in turn
and obtain a score reflecting the prevalence which is
used for ranking. Let 
	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1073?1080
Manchester, August 2008
Automatic Seed Word Selection for Unsupervised Sentiment 
Classification of Chinese Text
Taras Zagibalov    John Carroll
University of Sussex
Department of Informatics
Brighton  BN1 9QH, UK
{T.Zagibalov,J.A.Carroll}@sussex.ac.uk
Abstract 
We describe and evaluate a new method 
of automatic seed word selection for un-
supervised  sentiment  classification  of 
product  reviews  in  Chinese.  The  whole 
method is unsupervised and does not re-
quire any annotated training data; it only 
requires information about commonly oc-
curring negations  and adverbials.  Unsu-
pervised  techniques  are  promising  for 
this task since they avoid problems of do-
main-dependency  typically  associated 
with supervised methods. The results ob-
tained  are  close  to  those  of  supervised 
classifiers and sometimes better, up to an 
F1 of 92%.
1 Introduction
Automatic classification of document  sentiment 
(and more generally extraction of opinion from 
text) has recently attracted a lot of interest. One 
of the main reasons for this is the importance of 
such  information  to  companies,  other 
organizations,  and  individuals.  Applications 
include  marketing  research  tools  that  help  a 
company see market or media reaction towards 
their  brands,  products  or  services,  or  search 
engines  that  help potential  purchasers  make  an 
informed choice of a product they want to buy. 
Sentiment  classification  research  has  drawn on 
and contributed to research in text classification, 
unsupervised  machine  learning,  and  cross-
domain adaptation.
This paper presents a new, automatic approach 
to automatic seed word selection as part of senti-
ment classification of product reviews written in 
Chinese,  which  addresses  the  problem  of  do-
 ? 2008. Licensed under the Creative Commons Attribu-
tion-Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved.
main-dependency of sentiment classification that 
has been observed in previous work. It may also 
facilitate  building  sentiment  classification  sys-
tems in other languages since the approach as-
sumes a very small amount of linguistic knowl-
edge: the only language-specific information re-
quired is a basic description of the most frequent 
negated adverbial constructions in the language.
The paper is structured as follows. Section 2 
surveys related work in sentiment classification, 
unsupervised  machine  learning  and  Chinese 
language  processing.  Section  3  motivates  our 
approach,  which  is  described  in  detail  in 
Section 4.  The  data  used  for  experiments  and 
baselines,  as well  as the results of  experiments 
are covered in Section 5. Section 6 discusses the 
lessons learned and proposes directions for future 
work.
2 Related Work
2.1 Sentiment Classification
Most work on sentiment classification has used 
approaches based on supervised machine learn-
ing.  For  example,  Pang  et  al. (2002)  collected 
movie reviews that had been annotated with re-
spect to sentiment by the authors of the reviews, 
and used this data to train supervised classifiers. 
A number of studies have investigated the impact 
on classification accuracy of different factors, in-
cluding choice of  feature set,  machine learning 
algorithm,  and pre-selection of the segments of 
text  to  be  classified.  For  example,  Dave  et  al. 
(2003) experiment with the use of linguistic, sta-
tistical and n-gram features and measures for fea-
ture  selection  and  weighting.  Pang  and  Lee 
(2004)  use  a  graph-based technique to  identify 
and  analyze  only subjective  parts  of  texts.  Yu 
and  Hatzivassiloglou  (2003)  use  semantically-
oriented  words  for  identification  of  polarity  at 
the sentence level. Most of this work assumes bi-
nary classification (positive and negative), some-
1073
times  with  the  addition  of  a  neutral  class  (in 
terms of polarity, representing lack of sentiment).
While  supervised  systems  generally  achieve 
reasonably high accuracy, they do so only on test 
data that is similar to the training data. To move 
to another domain one would have to collect an-
notated data in the new domain and retrain the 
classifier. Engstr?m (2004) reports decreased ac-
curacy in cross-domain classification since senti-
ment in different domains is often expressed in 
different ways. However, it is impossible in prac-
tice to have annotated data for all  possible do-
mains  of  interest.  Aue  and  Gamon  (2005)  at-
tempt  to  solve  the  problem of  the  absence  of 
large  amounts  of  labeled  data  by  customizing 
sentiment classifiers to new domains using train-
ing data from other domains. Blitzer et al (2007) 
investigate domain adaptation for sentiment clas-
sifiers using structural correspondence learning. 
Read  (2005)  also  observed  significant  differ-
ences between the accuracy of classification of 
reviews in the same domain but published in dif-
ferent time periods.
Recently, there has been a shift of interest to-
wards more fine-grained approaches to process-
ing of sentiment, in which opinion is extracted at 
the sentence level, sometimes including informa-
tion about different features of a product that are 
commented on and/or the opinion holder (Hu and 
Liu, 2004; Ku et al, 2006). But even in such ap-
proaches, McDonald et al (2007) note that infor-
mation about the overall sentiment orientation of 
a document  facilitates more  accurate extraction 
of more specific information from the text.
2.2 Unsupervised Approach
One way of tackling the problem of domain de-
pendency could be to use an approach that does 
not  rely  on  annotated  data.  Turney  (2002)  de-
scribes a method of sentiment classification  us-
ing two human-selected seed words (the words 
poor and  excellent)  in conjunction with a very 
large  text  corpus;  the  semantic  orientation  of 
phrases is computed as their association with the 
seed words (as measured by pointwise mutual in-
formation). The sentiment of a document is cal-
culated as the average semantic orientation of all 
such phrases.
Yarowsky  (1995)  describes  a  'semi-unsuper-
vised' approach to the problem of sense disam-
biguation  of  words,  also  using  a  set  of  initial 
seeds, in this case a few high quality sense anno-
tations. These annotations are used to start an it-
erative process of learning information about the 
contexts  in  which  senses  of  words  appear,  in 
each iteration labeling senses of previously unla-
beled  word  tokens  using  information  from the 
previous iteration.
2.3 Chinese Language Processing
A major issue in processing Chinese text is the 
fact that words are not delimited in the written 
language. In many cases, NLP researchers work-
ing  with  Chinese  use  an  initial  segmentation 
module  that  is  intended  to  break  a  text  into 
words.  Although  this  can  facilitate  the  use  of 
subsequent computational techniques, there is no 
a clear definition of what a 'word' is in the mod-
ern Chinese  language,  so the  use  of  such  seg-
menters is of dubious theoretical status; indeed, 
good  results  have  been  reported  from systems 
which do not assume such pre-processing (Foo 
and Li, 2004; Xu et al, 2004). 
2.4 Seed Word Selection
We are not aware of any sentiment analysis sys-
tem that uses unsupervised seed word selection. 
However, Pang et al (2002) showed that it is dif-
ficult  to  get  good coverage of  a  target  domain 
from manually selected words, and even simple 
corpus  frequency counts  may  produce  a  better 
list of features for supervised classification: hu-
man-created lists resulted in 64% accuracy on a 
movie  review  corpus,  while  a  list  of  frequent 
words scored 69%. Pang et al also observed that 
some  words  without  any  significant  emotional 
orientation were quite good features: for exam-
ple, the word ?still? turned out to be a good indi-
cator of positive reviews as it was often used in 
sentences  such  as  ?Still,  though,  it  was  worth 
seeing''.
3 Our Approach
Our main goal is to overcome the problem of do-
main-dependency  in  sentiment  classification. 
Unsupervised approaches seem promising in this 
regard, since they do not require annotated train-
ing data, just access to sufficient raw text in each 
domain. We base our approach on a previously 
described,  'almost-unsupervised'  system  that 
starts with only a single, human-selected seed ? 
(good) and uses an iterative method to extract a 
training sub-corpus (Zagibalov & Carroll, 2008). 
The approach does not use a word segmentation 
module;  in  this  paper  we use  the  term 'lexical 
item' to denote any sequence of Chinese charac-
ters that is treated by the system as a unit, what-
ever it is linguistically ? a morpheme, a word or 
a phrase.
1074
Our initial aim was to investigate ways of im-
proving the classifier by automatically finding a 
better seed, because Zagibalov & Carroll indicate 
that in different domains they could, by manual 
trial and error, find a seed other than  ? (good) 
which produced better results.
To find such a seed automatically,  we make 
two assumptions:
1. Attitude  is  often  expressed  through  the 
negation of vocabulary items with the op-
posite meaning; for example in Chinese it 
is  more  common  to  say  not  good than 
bad  (Tan,  2002). Zagibalov  &  Carroll's 
system uses this observation to find nega-
tive lexical items while nevertheless start-
ing only from a positive seed. This leads 
us  to  believe  that  it  is  possible  to  find 
candidate  seeds  themselves  by  looking 
for  sequences  of  characters  which  are 
used with negation. 
2. The polarity of a candidate seed needs to 
be determined. To do this we assume we 
can use the lexical item   ? (good) as a 
gold  standard  for  positive  lexical  items 
and  compare  the  pattern  of  contexts  a 
candidate seed occurs in to the pattern ex-
hibited by the gold standard.
Looking at product review corpora, we observed 
that  good is  always  more  often  used  without 
negation in positive texts, while in negative texts 
it  is  more  often  used  with  negation  (e.g.  not  
good). Also,  good occurs more often in positive 
texts than negative, and more frequently without 
negation than with it. We use the latter observa-
tion  as  the  basis  for  identifying  seed  lexical 
items,  finding those which occur with negation 
but more frequently occur without it.
As well as detecting negation1 we also use ad-
verbials2 to  avoid  hypothesizing  non-contentful 
seeds: the characters following the sequence of a 
negation and an adverbial are in general content-
ful units, as opposed to parts of words, function 
words, etc. In what follows we refer to such con-
structions as negated adverbial constructions.
1We use only six frequently occurring negations: ? (bu), ?
? (buhui), ?? (meiyou), ?? (baituo), ?? (mianqu), 
and ?? (bimian). We are trying to be as language-inde-
pendent as possible so we take a simplistic approach to de-
tecting negation.
2We use five frequently occurring adverbials: ? (hen), ?? 
(feichang), ? (tai), ? (zui), and ?? (bijiao). Similarly to 
negation, we deliberately take a simplistic approach.
4 Method 
We use a similar  sentiment  classifier and itera-
tive retraining technique to the almost-unsuper-
vised  system  of  Zagibalov  &  Carroll  (2008), 
summarized below in Sections 4.2 and 4.3. The 
main  new contributions  of  this  paper  are  tech-
niques for automatically finding the seeds from 
raw text in a particular domain (Section 4.1), and 
for detecting when the process should stop (Sec-
tion 4.4). This new system therefore differs from 
that of Zagibalov & Carroll (2008) in being com-
pletely unsupervised and not depending on arbi-
trary iteration limits. (The evaluation also differs 
since we focus in this paper on the effects of do-
main on sentiment classification accuracy).
4.1 Seed Lexical Item Identification
The first step is to identify suitable positive seeds 
for  the  given  corpus.  The  intuition  behind  the 
way this is done is outlined above in Section 3. 
The algorithm is as follows:
1. find all sequences of characters between 
non-character  symbols  (i.e.  punctuation 
marks,  digits  and  so  on)  that  contain 
negation  and  an  adverbial,  split  the  se-
quence at the negation, and store the char-
acter  sequence  that  follows  the  negated 
adverbial construction;
2. count the number of occurrences of each 
distinct  sequence that  follows a negated 
adverbial construction (X);
3. count the number of occurrences of each 
distinct sequence without the construction 
(Y);
4. find all sequences with Y ? X > 0.
4.2 Sentiment Classification
This  approach  to  Chinese  language  processing 
does not use pre-segmentation (in the sense dis-
cussed in Section 2.3) or grammatical analysis: 
the basic unit of processing is the 'lexical item', 
each of which is a sequence of one or more Chi-
nese characters excluding punctuation marks (so 
a lexical item may actually form part of a word, a 
whole word or a sequence of words), and 'zones', 
each of which is a sequence of characters delim-
ited by punctuation marks.
Each  zone  is  classified  as  either  positive  or 
negative based whether positive or negative vo-
cabulary  items  predominate.  As  there  are  two 
parts of the vocabulary (positive and negative), 
we  correspondingly  calculate  two  scores  (Si , 
1075
where  i is  either  positive or  negative)  using 
Equation (1), where Ld is the length in characters 
of a matching lexical item (raised to the power of 
two to increase the significance of longer items 
which capture more context),  Lphrase is the length 
of the current zone in characters, Sd is the current 
sentiment score of the matching lexical item (ini-
tially 1.0), and Nd is a negation check coefficient.
 
S i= Ld
2
L phrase S d N d
                   (1)
The negation check is a regular expression which 
determines  if  the lexical  item is  preceded by a 
negation within its enclosing zone. If a negation 
is found then Nd is set to ?1.
The sentiment  score of a zone is the sum of 
sentiment of all the items found in it.
To determine the sentiment orientation of the 
whole document, the classifier computes the dif-
ference between the number of positive and neg-
ative zones.  If the result is greater than zero the 
document is classified as positive, and vice ver-
sa.
4.3 Iterative Retraining
Iterative retraining is used to enlarge the initial 
seed  vocabulary into  a  comprehensive  vocabu-
lary  list  of  sentiment-bearing  lexical  items.  In 
each iteration, the current version of the classifier 
is run on the input corpus to classify each docu-
ment,  resulting in a training subcorpus of posi-
tive and a negative documents. The subcorpus is 
used to adjust the scores of existing positive and 
negative vocabulary items and to find new items 
to be included in the vocabulary. 
Each lexical item that occurs at least twice in 
the corpus is a candidate for inclusion in the vo-
cabulary list. After candidate items are found, the 
system  calculates  their  relative  frequencies  in 
both the positive and negative parts of the current 
training subcorpus.  The system also checks for 
negation while counting occurrences: if a lexical 
item is preceded by a negation, its count is re-
duced by one. 
For all candidate items we compare their rela-
tive frequencies in the positive and negative doc-
uments in the subcorpus using Equation (2).
difference= ?F p? F n??F p?Fn?/2
        (2)
If difference < 1, then the frequencies are similar 
and the item does not have enough distinguishing 
power,  so it  is  not  included in  the vocabulary. 
Otherwise  the  sentiment  score  of  the  item  is 
(re-)calculated  ?  according  to  Equation  (3)  for 
positive  items,  and  analogously  for  negative 
items.
F p?Fn         (3)
Finally, the adjusted vocabulary list with the new 
scores is ready for the next iteration3.
4.4 Iteration Control
To maximize the number of productive iterations 
while avoiding unnecessary processing and arbi-
trary  iteration  limits,  iterative  retraining  is 
stopped when there is no change to the classifica-
tion of any document over the previous two itera-
tions.
5 Experiments
5.1 Data
As our approach is unsupervised, we do not use 
an annotated training corpus, but run our iterative 
procedure on the raw data extracted from an an-
notated test corpus, and evaluate the final accura-
cy of the system with respect to the annotations 
in that corpus.
Our  test  corpus  is  derived  from product  re-
views harvested from the website IT1684. All the 
reviews  were  tagged by their  authors  as  either 
positive or negative overall.  Most reviews con-
sist of two or three distinct parts: positive opin-
ions, negative opinions, and comments ('other') ? 
although some reviews have only one part.  We 
removed  duplicate  reviews  automatically  using 
approximate matching, giving a corpus of 29531 
reviews of which 23122 are positive (78%) and 
6409 are  negative  (22%).  The  total  number  of 
different  products  in  the  corpus  is  10631,  the 
number of product categories is 255, and most of 
the reviewed products are either software prod-
ucts  or  consumer  electronics.  Unfortunately,  it 
appears  that  some  users  misuse  the  sentiment 
3An alternative approach might be to use point-wise mutual 
information instead of relative frequencies of newly found 
features in a subcorpus produced in the previous iteration. 
However, in preliminary experiments, SO-PMI did not pro-
duce good corpora from the first iteration. Also, it is not 
clear how to manage subsequent iterations since PMI would 
have to be calculated between thousands of new vocabulary 
items and every newly found sequence of characters, which 
would be computationally intractable.
4http://product.it168.com
1076
tagging facility on the website so quite a lot of 
reviews have incorrect tags. However, the parts 
of the reviews are much more reliably identified 
as being positive or negative so we used these as 
the items of the test corpus. In the experiments 
described below we use 10 subcorpora contain-
ing a total of 7982 reviews, distributed between 
product types as shown in Table 1. 
Corpus/product type Reviews
Monitors 683
Mobile phones 2317
Digital cameras 1705
MP3 players 779
Computer  parts  (CD-drives,  mother-
boards)
308
Video cameras and lenses 361
Networking (routers, network cards) 350
Office equipment (copiers,
multifunction devices, scanners)
611
Printers (laser, inkjet) 569
Computer peripherals (mice, keyboards, 
speakers)
457
Table 1. Product types and sizes of the test 
corpora.
We constructed five of the corpora by combin-
ing smaller ones of 100?250 reviews each (as in-
dicated  in  parentheses  in  Table  1)  in  order  to 
have reasonable amounts of data.
Each corpus has equal numbers of positive and 
negative reviews so we can derive upper bounds 
from the corpora (Section 5.2)  by applying su-
pervised  classifiers.  We  balance  the  corpora 
since (at least on this data) these classifiers per-
form less well with skewed class distributions5.
5.2 Baseline and Upper Bound
Since the  corpora  are  balanced with respect  to 
sentiment  orientation  the  na?ve  (unsupervised) 
baseline  is  50%.  We  also  produced  an  upper 
bound  using  Naive  Bayes  multinomial  (NBm) 
and Support Vector Machine (SVM)6 classifiers 
with the NTU Sentiment  Dictionary (Ku  et al, 
2006)  vocabulary items  as  the  feature  set.  The 
dictionary contains  2809 items  in  the  'positive' 
part  and  8273  items  in  the  'negative'.  We  ran 
5We have made this corpus publicly available at http://
www.informatics.sussex.ac.uk/users/tz21/coling08.zip
6We used WEKA 3.4.11 (http://www.cs.waikato.ac.nz/?ml/
weka )
both classifiers in 10-fold stratified cross-valida-
tion mode, resulting in the accuracies shown in 
Table 2. The macroaveraged accuracies across all 
10  corpora  are  82.78%  (NBm)  and  80.89% 
(SVM).
Corpus Nbm 
(%)
SVM 
(%)
Monitors 86.21 83.87
Mobile phones 86.52 84.49
Digital cameras 82.27 82.04
MP3 players 82.64 79.43
Computer parts 81.10 79.47
Video cameras and lenses 83.05 84.16
Networking 77.65 75.35
Office equipment 82.13 80.00
Printers 81.33 79.57
Computer peripherals 84.86 80.48
Table 2. Upper bound accuracies.
 We also tried adding the negations and adver-
bials specified in Section 3 to the feature set, and 
this resulted in slightly improved accuracies, of 
83.90% (Nbm) and 82.49% (SVM).
An alternative  approach would have been to 
automatically segment the reviews and then de-
rive a feature set of a manageable size by setting 
a threshold on word frequencies; however the ex-
tra processing means that this is a less valid up-
per bound.
Another possible comparison could be with a 
version of Turney's (2002) sentiment  classifica-
tion method applied to Chinese. However, the re-
sults  would  not  be  comparable  since  Turney's 
method would require the additional use of very 
large  text  corpus  and  the  manual  selection  of 
positive and negative seed words. 
5.3 Experiment 1
To be able to compare to the accuracy of the al-
most-unsupervised  approach  of  Zagibalov  & 
Carroll (2008), we ran our system using the seed 
 ? (good) for each corpus. The results are shown 
in Table 3. We compute precision, recall and F1 
measure rather than just accuracy, since our clas-
sifier can omit some reviews whereas the super-
vised classifiers attempt to classify all  reviews. 
The macroaveraged F1 measure is 80.55, which 
beats the na?ve baseline by over 30 percentage 
points, and approaches the two upper bounds.
1077
Corpus Iter P R F1
Monitors 12 86.62 86.24 86.43
Mobile phones 11 90.15 89.68 89.91
Digital cameras 13 81.33 80.23 80.78
MP3 players 13 86.10 85.10 85.60
Computer parts 10 69.10 67.53 68.31
Video cameras and 
lenses
10 82.81 81.44 82.12
Networking 11 69.28 68.29 68.78
Office equipment 12 81.83 80.36 81.09
Printers 12 81.04 79.61 80.32
Computer peripherals 10 82.20 81.84 82.02
Macroaverage 81.05 80.03 80.54
Table 3. Results with the single, manually
chosen seed ? (good) for each corpus.
5.4 Experiment 2
We then ran our full system, including the seed 
identifier. Appendix A shows that for most of the 
corpora the algorithm found different (highly do-
main-salient)  seeds.  Table  4  shows  the  results 
achieved.
Corpus Iter P R F1
Monitors 11 85.57 85.07 85.32
Mobile phones 10 92.63 92.19 92.41
Digital cameras 13 84.92 83.58 84.24
MP3 players 13 88.69 87.55 88.11
Computer parts 12 77.78 77.27 77.52
Video cameras and 
lenses
11 83.62 81.99 82.8
Networking 13 72.83 72.00 72.41
Office equipment 10 82.42 81.34 81.88
Printers 12 81.04 79.61 80.32
Computer peripherals 10 82.24 82.06 82.15
Macroaverage 83.17 82.27 82.72
Table 4. Results with the seeds automatically 
identified for each corpus. 
Across all 10 subcorpora, the improvement us-
ing  automatically  identified  seed  words  com-
pared with just using the seed good is significant 
(paired t-test, P<0.0001), and the F1 measure lies 
between the two upper bounds.
6 Conclusions and Future Work
The unsupervised approach to seed words selec-
tion for sentiment classification presented in this 
paper produces results which in most  cases are 
close to the results of supervised classifiers and 
to  the  previous  almost-unsupervised  approach: 
eight  out  of  ten  results  showed  improvement 
over the human selected seed word and three re-
sults  outperformed  the  supervised  approach, 
while three other results were less than 1% infe-
rior to the supervised ones.
How does  it  happen that  the  chosen seed is 
usually (in our  dataset  ? always)  positive?  We 
think that this happens due to the socially accept-
ed norm of behaviour: as a rule one needs to be 
friendly to communicate with others. This in turn 
defines  linguistic  means  of  expressing  ideas  ? 
they will be at least slightly positive overall. The 
higher  prevalence of positive  reviews has  been 
observed previously: for example, in our corpus 
before  we  balanced  it  almost  80% of  reviews 
were  positive;  Pang  et  al.  (2002)  constructed 
their move  review  corpus  from  an  original 
dataset  of  1301  positive  and  752  negative  re-
views (63% positive). Ghose et al (2007) quote 
typical  examples  of  highly  positive  language 
used in the online marketplace.  We can make a 
preliminary conclusion that a relatively high fre-
quency of positive  words  is  determined  by the 
usage of language that reflects the social  beha-
viour of people.
In future work we intend to explore these is-
sues of positivity of language use. We will also 
apply  our  approach  to  other  genres  containing 
some quantity of evaluative language (for exam-
ple newspaper articles), and see if it works equal-
ly well  for  languages  other  than Chinese.  It  is 
also likely we can use a smaller set of negation 
words and adverbials to produce the seed lists.
Acknowledgements
The first author is supported by the Ford Founda-
tion International Fellowships Program.
References
Aue, Anthony, and Michael Gamon. 2005. Customiz-
ing Sentiment Classifiers to New Domains: a Case 
Study. In Proceedings of the International Confer-
ence  RANLP-2005  Recent  Advances  in  Natural  
Language Processing.
1078
Blitzer,  John,  Mark  Dredze,  and  Fernando  Pereira. 
2007.  Biographies,  Bollywood,  Boom-boxes  and 
Blenders: Domain Adaptation for Sentiment Clas-
sification. In Proceedings of the 45th Annual Meet-
ing of  the Association of  Computational Linguis-
tics. 440?447.
Dave,  Kushal,  Steve Lawrence,  and David M. Pen-
nock.  2003.  Mining  the  Peanut  Gallery:  Opinion 
Extraction and Semantic Classification of Product 
Reviews.  In  Proceedings  of  the  Twelfth  Interna-
tional World Wide Web Conference. 519?528.
Engstr?m, Charlotte. 2004. Topic Dependence in Sen-
timent Classification. Unpublished MPhil Disserta-
tion. University of Cambridge.
Foo, Schubert, and Hui Li. 2004. Chinese Word Seg-
mentation and Its Effects on Information Retrieval. 
Information  Processing  and  Management,  40(1). 
161?190.
Ghose, Anindya, Panagiotis Ipeirotis, and Arun Sun-
dararajan. 2007. Opinion Mining using Economet-
rics: A Case Study on Reputation Systems. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics. 416?423.
Hu, Minqing, and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the 
10th ACM SIGKDD International Conference on  
Knowledge Discovery and Data Mining. 168?177.
Ku,  Lun-Wei,  Yu-Ting  Liang,  and  Hsin-Hsi  Chen. 
2006.  Opinion  Extraction,  Summarization  and 
Tracking in News and Blog Corpora. In  Proceed-
ings of the AAAI-2006 Spring Symposium on Com-
putational  Approaches  to  Analyzing  Weblogs. 
AAAI Technical Report.
McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike 
Wells,  and Jeff Reynar.  2007. Structured  Models 
for  Fine-to-Coarse  Sentiment  Analysis.  In  Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics. 432?439.
Pang, Bo, and Lillian Lee. 2004. A Sentimental Edu-
cation:  Sentiment  Analysis  Using  Subjectivity 
Summarization Based on Minimum Cuts. In  Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics. 271?278.
Pang,  Bo,  Lillian  Lee,  and  Shivakumar 
Vaithyanathan. 2002. Thumbs up? Sentiment Clas-
sification using Machine Learning Techniques.  In 
Proceedings of the 2002 Conference on Empirical  
Methods in Natural Language Processing. 79?86.
Read,  Jonathon.  2005.  Using  Emoticons  to  Reduce 
Dependency in Machine Learning Techniques for 
Sentiment  Classification.  In  Proceedings  of  the 
ACL Student Research Workshop at ACL-05. 43?
48.
Tan, Aoshuang.  2002.  Problemy skrytoj grammatiki. 
Sintaksis,  semantika  i  pragmatika  jazyka  izoliru-
ju??ego stroja na primere kitajskogo jazyka  [Prob-
lems of a hidden grammar. Syntax, semantics and 
pragmatics of a language of the isolating type, tak-
ing the Chinese language  as an example]. Jazyki  
Slavjanskoj Kultury.
Turney,  Peter  D.  2002.  Thumbs  Up  or  Thumbs 
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of  
the  40th  Annual  Meeting  of  the  Association  for  
Computational Linguistics. 417?424.
Xu, Jia, Richard Zens, and Hermann Ney. 2004. Do 
We Need Chinese Word Segmentation for Statisti-
cal  Machine  Translation?  In  Proceedings  of  the 
Third  SIGHAN  Workshop  on  Chinese  Language 
Learning. 122?128.
Yarowsky,  David.  1995.  Unsupervised  Word  Sense 
Disambiguation  Rivaling Supervised  Methods.  In 
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics. 189?196.
Yu, Hong, and Vasileios Hatzivassiloglou. 2003. To-
wards  Answering  Opinion  Questions:  Separating 
Facts from Opinions and Identifying the Polarity of 
Opinion  Sentences.  In  Proceedings  of  the  2003 
Conference on Empirical Methods in Natural Lan-
guage Processing. 129?136.
Zagibalov,  Taras,  and John Carroll.  2008. Unsuper-
vised  Classification of  Sentiment  and  Objectivity 
in Chinese Text. In Proceedings of the Third Inter-
national  Joint  Conference  on  Natural  Language 
Processing. 304?311.
1079
Appendix A. Seeds Automatically Identified for each Corpus.
Corpus Seed Corpus Seed
Monitors ? (good)
? (convenient; cheap)
?? (clear)
? (straight)
?? (comfortable)
? (fill, fulfill)
?? (sharp)
?? (comfortable)
? (cool)
Video 
cameras 
and lenses
?? (clear ?  of  sound  or  image)
?? (comfortable)
?? (practical)
?? (perfect)
? (cool)
Mobile 
phones
? (good)
?? (support)
? (convenient; cheap)
?? (comfortable)
?? (clear ?of sound or image)
? (sufficient)
?? (easy to use)
?? (comfortable)
??? (user friendly)
?? (smooth and easy)
?? (distinct) 
? (cool)
?? (has become better)
?? (durable)
??? (comfortable)
??? (satisfied)
?? (fit, suit)
??? (has become comfortable)
?? (applicable)
?? (handy)
?? (science, scientific)
Digital 
cameras 
? (good)
? (convenient; cheap)
?? (comfortable)
?? (clear?of sound or image)
?? (special)
? (cool)
?? (satisfied)
?? (durable)
?? (comfortable)
?? (perfect)
?? (straight)
?? (stable)
??? (has become comfortable)
?? (polite)
?? (detailed)
Networking ?? (stable) Printers ? (good)
MP3 players ? (good)
? (convenient; cheap)
?? (comfortable)
?? (practical)
?? (sensitive)
?? (comfortable)
? (cool)
??? (has become comfortable)
Computer 
peripherals 
? (good)
? (convenient;cheap)
?? (comfortable)
? (precise)
?? (comfortable)
?? (habitual)
?? (smooth and easy)
?? (stable)
Computer 
parts
? (good)
?? (stable)
Office 
equipment
? (good)
?? (comfortable)
?? (stable)
?? (practical)
1080
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 291?299,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Parsing Mildly Non-projective Dependency Structures?
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
David Weir and John Carroll
Department of Informatics
University of Sussex, United Kingdom
{davidw,johnca}@sussex.ac.uk
Abstract
We present parsing algorithms for vari-
ous mildly non-projective dependency for-
malisms. In particular, algorithms are pre-
sented for: all well-nested structures of
gap degree at most 1, with the same com-
plexity as the best existing parsers for con-
stituency formalisms of equivalent genera-
tive power; all well-nested structures with
gap degree bounded by any constant k;
and a new class of structures with gap de-
gree up to k that includes some ill-nested
structures. The third case includes all the
gap degree k structures in a number of de-
pendency treebanks.
1 Introduction
Dependency parsers analyse a sentence in terms
of a set of directed links (dependencies) express-
ing the head-modifier and head-complement rela-
tionships which form the basis of predicate argu-
ment structure. We take dependency structures to
be directed trees, where each node corresponds to
a word and the root of the tree marks the syn-
tactic head of the sentence. For reasons of effi-
ciency, many practical implementations of depen-
dency parsing are restricted to projective struc-
tures, in which the subtree rooted at each word
covers a contiguous substring of the sentence.
However, while free word order languages such
as Czech do not satisfy this constraint, parsing
without the projectivity constraint is computation-
ally complex. Although it is possible to parse
non-projective structures in quadratic time under a
model in which each dependency decision is inde-
pendent of all the others (McDonald et al, 2005),
?Partially supported by MEC and FEDER (HUM2007-
66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR,
INCITE08E1R104022ES, INCITE08ENA305025ES, IN-
CITE08PXIB302179PR, Rede Galega de Proc. da Linguaxe
e RI, Bolsas para Estad??as INCITE ? FSE cofinanced).
the problem is intractable in the absence of this as-
sumption (McDonald and Satta, 2007).
Nivre and Nilsson (2005) observe that most
non-projective dependency structures appearing
in practice are ?close? to being projective, since
they contain only a small proportion of non-
projective arcs. This has led to the study of
classes of dependency structures that lie be-
tween projective and unrestricted non-projective
structures (Kuhlmann and Nivre, 2006; Havelka,
2007). Kuhlmann (2007) investigates several such
classes, based on well-nestedness and gap degree
constraints (Bodirsky et al, 2005), relating them
to lexicalised constituency grammar formalisms.
Specifically, he shows that: linear context-free
rewriting systems (LCFRS) with fan-out k (Vijay-
Shanker et al, 1987; Satta, 1992) induce the set
of dependency structures with gap degree at most
k ? 1; coupled context-free grammars in which
the maximal rank of a nonterminal is k (Hotz and
Pitsch, 1996) induce the set of well-nested depen-
dency structures with gap degree at most k ? 1;
and LTAGs (Joshi and Schabes, 1997) induce the
set of well-nested dependency structures with gap
degree at most 1.
These results establish that there must be
polynomial-time dependency parsing algorithms
for well-nested structures with bounded gap de-
gree, since such parsers exist for their correspond-
ing lexicalised constituency-based formalisms.
However, since most of the non-projective struc-
tures in treebanks are well-nested and have a small
gap degree (Kuhlmann and Nivre, 2006), devel-
oping efficient dependency parsing strategies for
these sets of structures has considerable practical
interest, since we would be able to parse directly
with dependencies in a data-driven manner, rather
than indirectly by constructing intermediate con-
stituency grammars and extracting dependencies
from constituency parses.
We address this problem with the following
contributions: (1) we define a parsing algorithm
291
for well-nested dependency structures of gap de-
gree 1, and prove its correctness. The parser runs
in time O(n7), the same complexity as the best
existing algorithms for LTAG (Eisner and Satta,
2000), and can be optimised to O(n6) in the non-
lexicalised case; (2) we generalise the previous al-
gorithm to any well-nested dependency structure
with gap degree at most k in time O(n5+2k); (3)
we generalise the previous parsers to be able to
analyse not only well-nested structures, but also
ill-nested structures with gap degree at most k sat-
isfying certain constraints1, in time O(n4+3k); and
(4) we characterise the set of structures covered by
this parser, which we call mildly ill-nested struc-
tures, and show that it includes all the trees present
in a number of dependency treebanks.
2 Preliminaries
A dependency graph for a string w1 . . . wn is a
graph G = (V,E), where V = {w1, . . . , wn}
and E ? V ? V . We write the edge (wi, wj)
as wi ? wj , meaning that the word wi is a syn-
tactic dependent (or a child) of wj or, conversely,
that wj is the governor (parent) of wi. We write
wi ?? wj to denote that there exists a (possi-
bly empty) path from wi to wj . The projection
of a node wi, denoted bwic, is the set of reflexive-
transitive dependents of wi, that is: bwic = {wj ?
V | wj ?? wi}. An interval (with endpoints i and
j) is a set of the form [i, j] = {wk | i ? k ? j}.
A dependency graph is said to be a tree if it is:
(1) acyclic: wj ? bwic implies wi ? wj 6? E; and
(2) each node has exactly one parent, except for
one node which we call the root or head. A graph
verifying these conditions and having a vertex set
V ? {w1, . . . , wn} is a partial dependency tree.
Given a dependency tree T = (V,E) and a node
u ? V , the subtree induced by the node u is the
graph Tu = (buc, Eu) where Eu = {wi ? wj ?
E | wj ? buc}.
2.1 Properties of dependency trees
We now define the concepts of gap degree and
well-nestedness (Kuhlmann and Nivre, 2006). Let
T be a (possibly partial) dependency tree for
w1 . . . wn: We say that T is projective if bwic is
an interval for every word wi. Thus every node
in the dependency structure must dominate a con-
tiguous substring in the sentence. The gap degree
1Parsing unrestricted ill-nested structures, even when the
gap degree is bounded, is NP-complete: these structures are
equivalent to LCFRS for which the recognition problem is
NP-complete (Satta, 1992).
of a particular node wk in T is the minimum g ? N
such that bwkc can be written as the union of g+1
intervals; that is, the number of discontinuities in
bwkc. The gap degree of the dependency tree T is
the maximum among the gap degrees of its nodes.
Note that T has gap degree 0 if and only if T is
projective. The subtrees induced by nodes wp and
wq are interleaved if bwpc ? bwqc = ? and there
are nodes wi, wj ? bwpc and wk, wl ? bwqc such
that i < k < j < l. A dependency tree T is
well-nested if it does not contain two interleaved
subtrees. A tree that is not well-nested is said to
be ill-nested. Note that projective trees are always
well-nested, but well-nested trees are not always
projective.
2.2 Dependency parsing schemata
The framework of parsing schemata (Sikkel,
1997) provides a uniform way to describe, anal-
yse and compare parsing algorithms. Parsing
schemata were initially defined for constituency-
based grammatical formalisms, but Go?mez-
Rodr??guez et al (2008a) define a variant of the
framework for dependency-based parsers. We
use these dependency parsing schemata to de-
fine parsers and prove their correctness. Due to
space constraints, we only provide brief outlines
of the main concepts behind dependency parsing
schemata.
The parsing schema approach considers pars-
ing as deduction, generating intermediate results
called items. An initial set of items is obtained
from the input sentence, and the parsing process
involves deduction steps which produce new items
from existing ones. Each item contains informa-
tion about the sentence?s structure, and a success-
ful parsing process produces at least one final item
providing a full dependency analysis for the sen-
tence or guaranteeing its existence. In a depen-
dency parsing schema, items are defined as sets of
partial dependency trees2. To define a parser by
means of a schema, we must define an item set
and provide a set of deduction steps that operate
on it. Given an item set I, the set of final items
for strings of length n is the set of items in I that
contain a full dependency tree for some arbitrary
string of length n. A final item containing a de-
pendency tree for a particular string w1 . . . wn is
said to be a correct final item for that string. These
2The formalism allows items to contain forests, and the
dependency structures inside items are defined in a notation
with terminal and preterminal nodes, but these are not needed
here.
292
concepts can be used to prove the correctness of
a parser: for each input string, a parsing schema?s
deduction steps allow us to infer a set of items,
called valid items for that string. A schema is said
to be sound if all valid final items it produces for
any arbitrary string are correct for that string. A
schema is said to be complete if all correct final
items are valid. A correct parsing schema is one
which is both sound and complete.
In constituency-based parsing schemata, deduc-
tion steps usually have grammar rules as side con-
ditions. In the case of dependency parsers it is
also possible to use grammars (Eisner and Satta,
1999), but many algorithms use a data-driven ap-
proach instead, making individual decisions about
which dependencies to create by using probabilis-
tic models (Eisner, 1996) or classifiers (Yamada
and Matsumoto, 2003). To represent these algo-
rithms as deduction systems, we use the notion
of D-rules (Covington, 1990). D-rules take the
form a ? b, which says that word b can have a
as a dependent. Deduction steps in non-grammar-
based parsers can be tied to the D-rules associated
with the links they create. In this way, we ob-
tain a representation of the underlying logic of the
parser while abstracting away from control struc-
tures (the particular model used to create the de-
cisions associated with D-rules). Furthermore, the
choice points in the parsing process and the infor-
mation we can use to make decisions are made ex-
plicit in the steps linked to D-rules.
3 The WG1 parser
3.1 Parsing schema for WG1
We define WG1, a parser for well-nested depen-
dency structures of gap degree ? 1, as follows:
The item set is IWG1 = I1 ? I2, with
I1 = {[i, j, h, , ] | i, j, h ? N, 1 ? h ? n,
1 ? i ? j ? n, h 6= j, h 6= i? 1},
where each item of the form [i, j, h, , ] repre-
sents the set of all well-nested partial dependency
trees3 with gap degree at most 1, rooted at wh, and
such that bwhc = {wh} ? [i, j], and
I2 = {[i, j, h, l, r] | i, j, h, l, r ? N, 1 ? h ? n,
1 ? i < l ? r < j ? n, h 6= j, h 6= i? 1,
h 6= l ? 1, h 6= r}
3In this and subsequent schemata, we use D-rules to ex-
press parsing decisions, so partial dependency trees are as-
sumed to be taken from the set of trees licensed by a set of
D-rules.
where each item of the form [i, j, h, l, r] represents
the set of all well-nested partial dependency trees
rooted at wh such that bwhc = {wh} ? ([i, j] \
[l, r]), and all the nodes (except possibly h) have
gap degree at most 1. We call items of this form
gapped items, and the interval [l, r] the gap of
the item. Note that the constraints h 6= j, h 6=
i + 1, h 6= l ? 1, h 6= r are added to items to
avoid redundancy in the item set. Since the result
of the expression {wh} ? ([i, j] \ [l, r]) for a given
head can be the same for different sets of values of
i, j, l, r, we restrict these values so that we cannot
get two different items representing the same de-
pendency structures. Items ? violating these con-
straints always have an alternative representation
that does not violate them, that we can express
with a normalising function nm(?) as follows:
nm([i, j, j, l, r]) = [i, j ? 1, j, l, r] (if r ? j ? 1 or r = ),
or [i, l ? 1, j, , ] (if r = j ? 1).
nm([i, j, l ? 1, l, r]) = [i, j, l ? 1, l ? 1, r](if l > i + 1),
or [r + 1, j, l ? 1, , ] (if l = i + 1).
nm([i, j, i ? 1, l, r]) = [i ? 1, j, i ? 1, l, r].
nm([i, j, r, l, r]) = [i, j, r, l, r ? 1] (if l < r),
or [i, j, r, , ] (if l = r).
nm([i, j, h, l, r]) = [i, j, h, l, r] for all other items.
When defining the deduction steps for this and
other parsers, we assume that they always produce
normalised items. For clarity, we do not explicitly
write this in the deduction steps, writing ? instead
of nm(?) as antecedents and consequents of steps.
The set of initial items is defined as the set
H = {[h, h, h, , ] | h ? N, 1 ? h ? n},
where each item [h, h, h, , ] represents the set
containing the trivial partial dependency tree con-
sisting of a single node wh and no links. This
same set of hypotheses can be used for all the
parsers, so we do not make it explicit for subse-
quent schemata. Note that initial items are sepa-
rate from the item set IWG1 and not subject to its
constraints, so they do not require normalisation.
The set of final items for strings of length n in
WG1 is defined as the set
F = {[1, n, h, , ] | h ? N, 1 ? h ? n},
which is the set of items in IWG1 containing de-
pendency trees for the complete input string (from
position 1 to n), with their head at any word wh.
The deduction steps of the parser can be seen in
Figure 1A.
The WG1 parser proceeds bottom-up, by build-
ing dependency subtrees and joining them to form
larger subtrees, until it finds a complete depen-
dency tree for the input sentence. The logic of
293
A. WG1 parser:
Link Ungapped:
[h1, h1, h1, , ]
[i2, j2, h2, , ]
[i2, j2, h1, , ] wh2 ? wh1
such that wh2 ? [i2, j2] ? wh1 /? [i2, j2],
Link Gapped:
[h1, h1, h1, , ]
[i2, j2, h2, l2, r2]
[i2, j2, h1, l2, r2] wh2 ? wh1
such that wh2 ? [i2, j2] \ [l2, r2] ? wh1 /? [i2, j2] \ [l2, r2],
Combine Ungapped:
[i, j, h, , ] [j + 1, k, h, , ]
[i, k, h, , ]
Combine Opening Gap:
[i, j, h, , ] [k, l, h, , ]
[i, l, h, j + 1, k ? 1]
such that j < k ? 1,
Combine Keeping Gap Left:
[i, j, h, l, r] [j + 1, k, h, , ]
[i, k, h, l, r]
Combine Keeping Gap Right:
[i, j, h, , ] [j + 1, k, h, l, r]
[i, k, h, l, r]
Combine Closing Gap:
[i, j, h, l, r] [l, r, h, , ]
[i, j, h, , ]
Combine Shrinking Gap Left:
[i, j, h, l, r] [l, k, h, , ]
[i, j, h, k + 1, r]
Combine Shrinking Gap Right:
[i, j, h, l, r] [k, r, h, , ]
[i, j, h, l, k ? 1]
Combine Shrinking Gap Centre:
[i, j, h, l, r] [l, r, h, l2, r2]
[i, j, h, l2, r2]
B. WGK parser:
Link:
[h1, h1, h1, []]
[i2, j2, h2, [(l1, r1), . . . , (lg, rg)]]
[i2, j2, h1, [(l1, r1), . . . , (lg, rg)]]
wh2 ? wh1
such that wh2 ? [i2, j2] \
?g
p=1[lp, rp]
?wh1 /? [i2, j2] \
?g
p=1[lp, rp].
Combine Shrinking Gap Right:
[i, j, h, [(l1, r1), . . . , (lq?1, rq?1), (lq, r?), (ls, rs), . . . , (lg, rg)]]
[rq + 1, r?, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
Combine Opening Gap:
[i, lq ? 1, h, [(l1, r1), . . . , (lq?1, rq?1)]]
[rq + 1, m, h, [(lq+1, rq+1), . . . , (lg, rg)]]
[i, m, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k and lq ? rq ,
Combine Shrinking Gap Left:
[i, j, h, [(l1, r1), . . . , (lq, rq), (l?, rs), (ls+1, rs+1), . . . , (lg, rg)]]
[l?, ls ? 1, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
Combine Keeping Gaps:
[i, j, h, [(l1, r1), . . . , (lq, rq)]]
[j + 1, m, h, [(lq+1, rq+1), . . . , (lg, rg)]]
[i, m, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k,
Combine Shrinking Gap Centre:
[i, j, h, [(l1, r1), . . . , (lq, rq), (l?, r?), (ls, rs), . . . , (lg, rg)]]
[l?, r?, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
C. Additional steps to turn WG1 into MG1:
Combine Interleaving:
[i, j, h, l, r] [l, k, h, r + 1, j]
[i, k, h, , ]
Combine Interleaving Gap C:
[i, j, h, l, r] [l, k, h, m, j]
[i, k, h, m, r]
such that m < r + 1,
Combine Interleaving Gap L:
[i, j, h, l, r]
[l, k, h, r + 1, u]
[i, k, h, j + 1, u]
such that u > j,
Combine Interleaving Gap R:
[i, j, h, l, r]
[k, m, h, r + 1, j]
[i, m, h, l, k ? 1]
such that k > l.
D. General form of the MGk Combine step:
[ia1 , iap+1 ? 1, h, [(ia1+1, ia2 ? 1), . . . , (iap?1+1, iap ? 1)]]
[ib1 , ibq+1 ? 1, h, [(ib1+1, ib2 ? 1), . . . , (ibq?1+1, ibq ? 1)]]
[imin(a1,b1), imax(ap+1,bq+1) ? 1, h, [(ig1 , ig1+1 ? 1), . . . , (igr , igr+1 ? 1)]]
for each string of length n with a?s located at positions a1 . . . ap(1 ? a1 < . . . < ap ? n), b?s at positions b1 . . . bq(1 ? b1 <
. . . < bq ? n), and g?s at positions g1 . . . gr(2 ? g1 < . . . < gr ? n ? 1), such that 1 ? p ? k, 1 ? q ? k, 0 ? r ? k ? 1,
p + q + r = n, and the string does not contain more than one consecutive appearance of the same symbol.
Figure 1: Deduction steps for the parsers defined in the paper.
the parser can be understood by considering how
it infers the item corresponding to the subtree in-
duced by a particular node, given the items for the
subtrees induced by the direct dependents of that
node. Suppose that, in a complete dependency
analysis for a sentence w1 . . . wn, the word wh
has wd1 . . . wdp as direct dependents (i.e. we have
dependency links wd1 ? wh, . . . , wdp ? wh).
Then, the item corresponding to the subtree in-
duced by wh is obtained from the ones correspond-
ing to the subtrees induced by wd1 . . . wdp by: (1)
applying the Link Ungapped or Link Gapped step
to each of the items corresponding to the subtrees
induced by the direct dependents, and to the hy-
pothesis [h, h, h, , ]. This allows us to infer p
items representing the result of linking each of the
dependent subtrees to the new head wh; (2) ap-
plying the various Combine steps to join all of the
294
items obtained in the previous step into a single
item. The Combine steps perform a union oper-
ation between subtrees. Therefore, the result is a
dependency tree containing all the dependent sub-
trees, and with all of them linked to h: this is
the subtree induced by wh. This process is ap-
plied repeatedly to build larger subtrees, until, if
the parsing process is successful, a final item is
found containing a dependency tree for the com-
plete sentence.
3.2 Proving correctness
The parsing schemata formalism can be used to
prove the correctness of a parsing schema. To
prove that WG1 is correct, we need to prove
its soundness and completeness.4 Soundness is
proven by checking that valid items always con-
tain well-nested trees. Completeness is proven by
induction, taking initial items as the base case and
showing that an item containing a correct subtree
for a string can always be obtained from items
corresponding to smaller subtrees. In order to
prove this induction step, we use the concept of
order annotations (Kuhlmann, 2007; Kuhlmann
and Mo?hl, 2007), which are strings that lexicalise
the precedence relation between the nodes of a de-
pendency tree. Given a correct subtree, we divide
the proof into cases according to the order annota-
tion of its head and we find that, for every possible
form of this order annotation, we can find a se-
quence of Combine steps to infer the relevant item
from smaller correct items.
3.3 Computational complexity
The time complexity of WG1 is O(n7), as the
step Combine Shrinking Gap Centre works with 7
free string positions. This complexity with respect
to the length of the input is as expected for this
set of structures, since Kuhlmann (2007) shows
that they are equivalent to LTAG, and the best ex-
isting parsers for this formalism also perform in
O(n7) (Eisner and Satta, 2000). Note that the
Combine step which is the bottleneck only uses the
7 indexes, and not any other entities like D-rules,
so its O(n7) complexity does not have any addi-
tional factors due to grammar size or other vari-
ables. The space complexity of WG1 is O(n5)
for recognition, due to the 5 indexes in items, and
O(n7) for full parsing.
4Due to space constraints, correctness proofs for the
parsers are not given here. Full proofs are provided in the
extended version of this paper, see (Go?mez-Rodr??guez et al,
2008b).
It is possible to build a variant of this parser
with time complexity O(n6), as with parsers for
unlexicalised TAG, if we work with unlexicalised
D-rules specifying the possibility of dependencies
between pairs of categories instead of pairs of
words. In order to do this, we expand the item set
with unlexicalised items of the form [i, j, C, l, r],
where C is a category, apart from the existing
items [i, j, h, l, r]. Steps in the parser are dupli-
cated, to work both with lexicalised and unlex-
icalised items, except for the Link steps, which
always work with a lexicalised item and an un-
lexicalised hypothesis to produce an unlexicalised
item, and the Combine Shrinking Gap steps, which
can work only with unlexicalised items. Steps are
added to obtain lexicalised items from their unlex-
icalised equivalents by binding the head to partic-
ular string positions. Finally, we need certain vari-
ants of the Combine Shrinking Gap steps that take
2 unlexicalised antecedents and produce a lexi-
calised consequent; an example is the following:
Combine Shrinking Gap Centre L:
[i, j, C, l, r]
[l + 1, r, C, l2, r2]
[i, j, l, l2, r2]
such that cat(wl)=C
Although this version of the algorithm reduces
time complexity with respect to the length of the
input to O(n6), it also adds a factor related to the
number of categories, as well as constant factors
due to using more kinds of items and steps than
the original WG1 algorithm. This, together with
the advantages of lexicalised dependency parsing,
may mean that the original WG1 algorithm is more
practical than this version.
4 The WGk parser
The WG1 parsing schema can be generalised to
obtain a parser for all well-nested dependency
structures with gap degree bounded by a constant
k(k ? 1), which we call WGk parser. In order to
do this, we extend the item set so that it can contain
items with up to k gaps, and modify the deduction
steps to work with these multi-gapped items.
4.1 Parsing schema for WGk
The item set IWGk is the set of all
[i, j, h, [(l1, r1), . . . , (lg, rg)]] where i, j, h, g ? N
, 0 ? g ? k, 1 ? h ? n, 1 ? i ? j ? n , h 6= j,
h 6= i? 1; and for each p ? {1, 2, . . . , g}:
lp, rp ? N, i < lp ? rp < j, rp < lp+1 ? 1,
h 6= lp ? 1, h 6= rp.
An item [i, j, h, [(l1, r1), . . . , (lg, rg)]] repre-
sents the set of all well-nested partial dependency
295
trees rooted at wh such that bwhc = {wh}?([i, j]\
?g
p=1[lp, rp]), where each interval [lp, rp] is called
a gap. The constraints h 6= j, h 6= i + 1, h 6=
lp ? 1, h 6= rp are added to avoid redundancy, and
normalisation is defined as in WG1. The set of fi-
nal items is defined as the set F = {[1, n, h, []] |
h ? N, 1 ? h ? n}. Note that this set is the same
as in WG1, as these are the items that we denoted
[1, n, h, , ] in the previous parser.
The deduction steps can be seen in Figure 1B.
As expected, the WG1 parser corresponds to WGk
when we make k = 1. WGk works in the same
way as WG1, except for the fact that Combine
steps can create items with more than one gap5.
The correctness proof is also analogous to that of
WG1, but we must take into account that the set of
possible order annotations is larger when k > 1,
so more cases arise in the completeness proof.
4.2 Computational complexity
The WGk parser runs in time O(n5+2k): as in
the case of WG1, the deduction step with most
free variables is Combine Shrinking Gap Cen-
tre, and in this case it has 5 + 2k free indexes.
Again, this complexity result is in line with what
could be expected from previous research in con-
stituency parsing: Kuhlmann (2007) shows that
the set of well-nested dependency structures with
gap degree at most k is closely related to cou-
pled context-free grammars in which the maxi-
mal rank of a nonterminal is k + 1; and the con-
stituency parser defined by Hotz and Pitsch (1996)
for these grammars also adds an n2 factor for each
unit increment of k. Note that a small value of
k should be enough to cover the vast majority of
the non-projective sentences found in natural lan-
guage treebanks. For example, the Prague Depen-
dency Treebank contains no structures with gap
degree greater than 4. Therefore, a WG4 parser
would be able to analyse all the well-nested struc-
tures in this treebank, which represent 99.89% of
the total. Increasing k beyond 4 would not pro-
duce further improvements in coverage.
5 Parsing ill-nested structures
The WGk parser analyses dependency structures
with bounded gap degree as long as they are
well-nested. This covers the vast majority of
5In all the parsers in this paper, Combine steps may be
applied in different orders to produce the same result, causing
spurious ambiguity. In WG1 and WGk, this can be avoided
when implementing the schemata, by adding flags to items
so as to impose a particular order.
the structures that occur in natural-language tree-
banks (Kuhlmann and Nivre, 2006), but there is
still a significant minority of sentences that con-
tain ill-nested structures. Unfortunately, the gen-
eral problem of parsing ill-nested structures is NP-
complete, even when the gap degree is bounded:
this set of structures is closely related to LCFRS
with bounded fan-out and unbounded production
length, and parsing in this formalism has been
proven to be NP-complete (Satta, 1992). The
reason for this high complexity is the problem
of unrestricted crossing configurations, appearing
when dependency subtrees are allowed to inter-
leave in every possible way. However, just as
it has been noted that most non-projective struc-
tures appearing in practice are only ?slightly? non-
projective (Nivre and Nilsson, 2005), we charac-
terise a sense in which the structures appearing in
treebanks can be viewed as being only ?slightly?
ill-nested. In this section, we generalise the algo-
rithms WG1 and WGk to parse a proper superset
of the set of well-nested structures in polynomial
time; and give a characterisation of this new set
of structures, which includes all the structures in
several dependency treebanks.
5.1 The MG1 and MGk parsers
The WGk parser presented previously is based on
a bottom-up process, where Link steps are used to
link completed subtrees to a head, and Combine
steps are used to join subtrees governed by a com-
mon head to obtain a larger structure. As WGk is a
parser for well-nested structures of gap degree up
to k, its Combine steps correspond to all the ways
in which we can join two sets of sibling subtrees
meeting these constraints, and having a common
head, into another. Thus, this parser does not use
Combine steps that produce interleaved subtrees,
since these would generate items corresponding to
ill-nested structures.
We obtain a polynomial parser for a wider set of
structures of gap degree at most k, including some
ill-nested ones, by having Combine steps repre-
senting every way in which two sets of sibling sub-
trees of gap degree at most k with a common head
can be joined into another, including those produc-
ing interleaved subtrees, like the steps for gap de-
gree 1 shown in Figure 1C. Note that this does not
mean that we can build every possible ill-nested
structure: some structures with complex crossed
configurations have gap degree k, but cannot be
built by combining two structures of that gap de-
gree. More specifically, our algorithm will be able
296
to parse a dependency structure (well-nested or
not) if there exists a binarisation of that structure
that has gap degree at most k. The parser im-
plicitly works by finding such a binarisation, since
Combine steps are always applied to two items and
no intermediate item generated by them can ex-
ceed gap degree k (not counting the position of
the head in the projection).
More formally, let T be a dependency structure
for the string w1 . . . wn. A binarisation of T is
a dependency tree T ? over a set of nodes, each of
which may be unlabelled or labelled with a word
in {w1 . . . wn}, such that the following conditions
hold: (1) each node has at most two children, and
(2) wi ? wj in T if and only if wi ?? wj in
T ?. A dependency structure is mildly ill-nested
for gap degree k if it has at least one binarisation
of gap degree ? k. Otherwise, we say that it is
strongly ill-nested for gap degree k. It is easy
to prove that the set of mildly ill-nested structures
for gap degree k includes all well-nested structures
with gap degree up to k.
We define MG1, a parser for mildly ill-nested
structures for gap degree 1, as follows: (1) the
item set is the same as that of WG1, except that
items can now contain any mildly ill-nested struc-
tures for gap degree 1, instead of being restricted
to well-nested structures; and (2) deduction steps
are the same as in WG1, plus the additional steps
shown in Figure 1C. These extra Combine steps
allow the parser to combine interleaved subtrees
with simple crossing configurations. The MG1
parser still runs in O(n7), as these new steps do
not use more than 7 string positions.
The proof of correctness for this parser is sim-
ilar to that of WG1. Again, we use the concept
of order annotations. The set of mildly ill-nested
structures for gap degree k can be defined as those
that only contain annotations meeting certain con-
straints. The soundness proof involves showing
that Combine steps always generate items contain-
ing trees with such annotations. Completeness is
proven by induction, by showing that if a subtree
is mildly ill-nested for gap degree k, an item for
it can be obtained from items for smaller subtrees
by applying Combine and Link steps. In the cases
where Combine steps have to be applied, the order
in which they may be used to produce a subtree
can be obtained from its head?s order annotation.
To generalise this algorithm to mildly ill-nested
structures for gap degree k, we need to add a Com-
bine step for every possible way of joining two
structures of gap degree at most k into another.
This can be done systematically by considering a
set of strings over an alphabet of three symbols:
a and b to represent intervals of words in the pro-
jection of each of the structures, and g to repre-
sent intervals that are not in the projection of ei-
ther structure, and will correspond to gaps in the
joined structure. The legal combinations of struc-
tures for gap degree k will correspond to strings
where symbols a and b each appear at most k + 1
times, g appears at most k times and is not the first
or last symbol, and there is no more than one con-
secutive appearance of any symbol. Given a string
of this form, the corresponding Combine step is
given by the expression in Figure 1D. As a particu-
lar example, the Combine Interleaving Gap C step
in Figure 1C is obtained from the string abgab.
Thus, we define the parsing schema for MGk, a
parser for mildly ill-nested structures for gap de-
gree k, as the schema where (1) the item set is
like that of WGk, except that items can now con-
tain any mildly ill-nested structures for gap degree
k, instead of being restricted to well-nested struc-
tures; and (2) the set of deduction steps consists of
a Link step as the one in WGk, plus a set of Com-
bine steps obtained as expressed in Figure 1D.
As the string used to generate a Combine step
can have length at most 3k + 2, and the result-
ing step contains an index for each symbol of the
string plus two extra indexes, the MGk parser has
complexity O(n3k+4). Note that the item and de-
duction step sets of an MGk parser are always su-
persets of those of WGk. In particular, the steps
for WGk are those obtained from strings that do
not contain abab or baba as a scattered substring.
5.2 Mildly ill-nested dependency structures
The MGk algorithm defined in the previous sec-
tion can parse any mildly ill-nested structure for a
given gap degree k in polynomial time. We have
characterised the set of mildly ill-nested structures
for gap degree k as those having a binarisation of
gap degree ? k. Since a binarisation of a depen-
dency structure cannot have lower gap degree than
the original structure, this set only contains struc-
tures with gap degree at most k. Furthermore, by
the relation between MGk and WGk, we know that
it contains all the well-nested structures with gap
degree up to k.
Figure 2 shows an example of a structure that
has gap degree 1, but is strongly ill-nested for gap
degree 1. This is one of the smallest possible such
structures: by generating all the possible trees up
to 10 nodes (without counting a dummy root node
297
Language
Structures
Total
Nonprojective
Total
By gap degree By nestedness
Gap
degree 1
Gap
degree 2
Gap
degree 3
Gap
deg. > 3
Well-
Nested
Mildly
Ill-Nested
Strongly
Ill-Nested
Arabic 2995 205 189 13 2 1 204 1 0
Czech 87889 20353 19989 359 4 1 20257 96 0
Danish 5430 864 854 10 0 0 856 8 0
Dutch 13349 4865 4425 427 13 0 4850 15 0
Latin 3473 1743 1543 188 10 2 1552 191 0
Portuguese 9071 1718 1302 351 51 14 1711 7 0
Slovene 1998 555 443 81 21 10 550 5 0
Swedish 11042 1079 1048 19 7 5 1008 71 0
Turkish 5583 685 656 29 0 0 665 20 0
Table 1: Counts of dependency trees classified by gap degree, and mild and strong ill-nestedness (for their gap degree); appear-
ing in treebanks for Arabic (Hajic? et al, 2004), Czech (Hajic? et al, 2006), Danish (Kromann, 2003), Dutch (van der Beek et al,
2002), Latin (Bamman and Crane, 2006), Portuguese (Afonso et al, 2002), Slovene (Dz?eroski et al, 2006), Swedish (Nilsson
et al, 2005) and Turkish (Oflazer et al, 2003; Atalay et al, 2003).
Figure 2: One of the smallest strongly ill-nested structures.
This dependency structure has gap degree 1, but is only
mildly ill-nested for gap degree ? 2.
located at position 0), it can be shown that all the
structures of any gap degree k with length smaller
than 10 are well-nested or only mildly ill-nested
for that gap degree k.
Even if a structure T is strongly ill-nested for
a given gap degree, there is always some m ? N
such that T is mildly ill-nested for m (since every
dependency structure can be binarised, and binari-
sations have finite gap degree). For example, the
structure in Figure 2 is mildly ill-nested for gap de-
gree 2. Therefore, MGk parsers have the property
of being able to parse any possible dependency
structure as long as we make k large enough.
In practice, structures like the one in Figure 2
do not seem to appear in dependency treebanks.
We have analysed treebanks for nine different lan-
guages, obtaining the data presented in Table 1.
None of these treebanks contain structures that are
strongly ill-nested for their gap degree. There-
fore, in any of these treebanks, the MGk parser can
parse every sentence with gap degree at most k.
6 Conclusions and future work
We have defined a parsing algorithm for well-
nested dependency structures with bounded gap
degree. In terms of computational complexity,
this algorithm is comparable to the best parsers
for related constituency-based formalisms: when
the gap degree is at most 1, it runs in O(n7),
like the fastest known parsers for LTAG, and can
be made O(n6) if we use unlexicalised depen-
dencies. When the gap degree is greater than 1,
the time complexity goes up by a factor of n2
for each extra unit of gap degree, as in parsers
for coupled context-free grammars. Most of the
non-projective sentences appearing in treebanks
are well-nested and have a small gap degree, so
this algorithm directly parses the vast majority of
the non-projective constructions present in natural
languages, without requiring the construction of a
constituency grammar as an intermediate step.
Additionally, we have defined a set of struc-
tures for any gap degree k which we call mildly
ill-nested. This set includes ill-nested structures
verifying certain conditions, and can be parsed in
O(n3k+4) with a variant of the parser for well-
nested structures. The practical interest of mildly
ill-nested structures can be seen in the data ob-
tained from several dependency treebanks, show-
ing that all of the ill-nested structures in them are
mildly ill-nested for their corresponding gap de-
gree. Therefore, our O(n3k+4) parser can analyse
all the gap degree k structures in these treebanks.
The set of mildly ill-nested structures for gap
degree k is defined as the set of structures that have
a binarisation of gap degree at most k. This defini-
tion is directly related to the way the MGk parser
works, since it implicitly finds such a binarisation.
An interesting line of future work would be to find
an equivalent characterisation of mildly ill-nested
structures which is more grammar-oriented and
would provide a more linguistic insight into these
structures. Another research direction, which we
are currently working on, is exploring how vari-
ants of the MGk parser?s strategy can be applied
to the problem of binarising LCFRS (Go?mez-
Rodr??guez et al, 2009).
298
References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In Proc. of LREC 2002, pages
1968?1703, Las Palmas, Spain.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2002.
The annotation process in the Turkish treebank. In
Proc. of EACL Workshop on Linguistically Inter-
preted Corpora - LINC, Budapest, Hungary.
David Bamman and Gregory Crane. 2006. The design
and use of a Latin dependency treebank. In Proc. of
5th Workshop on Treebanks and Linguistic Theories
(TLT2006), pages 67?78.
Manuel Bodirsky, Marco Kuhlmann, and Mathias
Mo?hl. 2005. Well-nested drawings as models
of syntactic structure. Technical Report, Saar-
land University. Electronic version available at:
http://www.ps.uni-sb.de/Papers/.
Michael A. Covington. 1990. A dependency parser
for variable-word-order languages. Technical Re-
port AI-1990-01, Athens, GA.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k ?Zabokrtsky?, and Andreja ?Zele. 2006.
Towards a Slovene dependency treebank. In Proc.
of LREC 2006, pages 1388?1391, Genoa, Italy.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL-99, pages 457?
464, Morristown, NJ. ACL.
Jason Eisner and Giorgio Satta. 2000. A faster parsing
algorithm for lexicalized tree-adjoining grammars.
In Proc. of 5th Workshop on Tree-Adjoining Gram-
mars and Related Formalisms (TAG+5), pages 14?
19, Paris.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING-96, pages 340?345, Copenhagen.
Carlos Go?mez-Rodr??guez, John Carroll, and David
Weir. 2008a. A deductive approach to dependency
parsing. In Proc. of ACL?08:HLT, pages 968?976,
Columbus, Ohio. ACL.
Carlos Go?mez-Rodr??guez, David Weir, and John Car-
roll. 2008b. Parsing mildly non-projective depen-
dency structures. Technical Report CSRP 600, De-
partment of Informatics, University of Sussex.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proc. of NAACL?09:HLT (to appear).
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan ?Snaidauf,
and Emanuel Bes?ka. 2004. Prague Arabic depen-
dency treebank: Development in data and tools. In
Proc. of NEMLAR International Conference on Ara-
bic Language Resources and Tools, pages 110?117.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague depen-
dency treebank 2.0. CDROM CAT: LDC2006T01,
ISBN 1-58563-370-4.
Jir??? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proc. of ACL 2007, Prague,
Czech Republic. ACL.
Gu?nter Hotz and Gisela Pitsch. 1996. On pars-
ing coupled-context-free languages. Theor. Comput.
Sci., 161(1-2):205?233. Elsevier, Essex, UK.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Handbook of for-
mal languages, pages 69?124. Springer-Verlag,
Berlin/Heidelberg/NY.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In
Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT2003).
Marco Kuhlmann and Mathias Mo?hl. 2007. Mildly
context-sensitive dependency languages. In Proc. of
ACL 2007, Prague, Czech Republic. ACL.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proc.
of COLING/ACL main conference poster sessions,
pages 507?514, Morristown, NJ, USA. ACL.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Doctoral dissertation, Saar-
land University, Saarbru?cken, Germany.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In IWPT 2007: Proc. of the 10th Confer-
ence on Parsing Technologies. ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of
HLT/EMNLP 2005, pages 523?530, Morristown,
NJ, USA. ACL.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proc. of NODALIDA
2005 Special Session on Treebanks, pages 119?132.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proc. of ACL?05,
pages 99?106, Morristown, NJ, USA. ACL.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In A. Abeille, ed., Building and Exploit-
ing Syntactically-annotated Corpora. Kluwer, Dor-
drecht.
Giorgio Satta. 1992. Recognition of linear context-
free rewriting systems. In Proc. of ACL-92, pages
89?95, Morristown, NJ. ACL.
Klaas Sikkel. 1997. Parsing Schemata ? A Frame-
work for Specification and Analysis of Parsing Al-
gorithms. Springer-Verlag, Berlin/Heidelberg/NY.
L. van der Beek, G. Bouma, R. Malouf, and G. van
Noord. 2002. The Alpino dependency treebank.
In Computational Linguistics in the Netherlands
(CLIN), Twente University.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Proc.
of ACL-87, pages 104?111, Morristown, NJ. ACL.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of 8th International Workshop on
Parsing Technologies (IWPT 2003), pages 195?206.
299
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 419?426, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Domain-Specific Sense Distributions and Predominant Sense Acquisition
Rob Koeling & Diana McCarthy & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
robk,dianam,johnca  @sussex.ac.uk
Abstract
Distributions of the senses of words are
often highly skewed. This fact is exploited
by word sense disambiguation (WSD) sys-
tems which back off to the predominant
sense of a word when contextual clues are
not strong enough. The domain of a doc-
ument has a strong influence on the sense
distribution of words, but it is not feasi-
ble to produce large manually annotated
corpora for every domain of interest. In
this paper we describe the construction of
three sense annotated corpora in different
domains for a sample of English words.
We apply an existing method for acquir-
ing predominant sense information auto-
matically from raw text, and for our sam-
ple demonstrate that (1) acquiring such
information automatically from a mixed-
domain corpus is more accurate than de-
riving it from SemCor, and (2) acquiring
it automatically from text in the same do-
main as the target domain performs best
by a large margin. We also show that
for an all words WSD task this automatic
method is best focussed on words that are
salient to the domain, and on words with
a different acquired predominant sense in
that domain compared to that acquired
from a balanced corpus.
1 Introduction
From analysis of manually sense tagged corpora,
Kilgarriff (2004) has demonstrated that distributions
of the senses of words are often highly skewed. Most
researchers working on word sense disambiguation
(WSD) use manually sense tagged data such as Sem-
Cor (Miller et al, 1993) to train statistical classi-
fiers, but also use the information in SemCor on the
overall sense distribution for each word as a back-
off model. In WSD, the heuristic of just choosing the
most frequent sense of a word is very powerful, es-
pecially for words with highly skewed sense distri-
butions (Yarowsky and Florian, 2002). Indeed, only
5 out of the 26 systems in the recent SENSEVAL-3
English all words task (Snyder and Palmer, 2004)
outperformed the heuristic of choosing the most fre-
quent sense as derived from SemCor (which would
give 61.5% precision and recall1). Furthermore, sys-
tems that did outperform the first sense heuristic did
so only by a small margin (the top score being 65%
precision and recall).
Over a decade ago, Gale et al (1992) observed
the tendency for one sense of a word to prevail in a
given discourse. To take advantage of this, a method
for automatically determining the ?one sense? given
a discourse or document is required. Magnini et al
(2002) have shown that information about the do-
main of a document is very useful for WSD. This is
because many concepts are specific to particular do-
mains, and for many words their most likely mean-
ing in context is strongly correlated to the domain of
the document they appear in. Thus, since word sense
distributions are skewed and depend on the domain
at hand we would like to know for each domain of
application the most likely sense of a word.
However, there are no extant domain-specific
sense tagged corpora to derive such sense distribu-
tion information from. Producing them would be ex-
tremely costly, since a substantial corpus would have
to be annotated by hand for every domain of interest.
In response to this problem, McCarthy et al (2004)
proposed a method for automatically inducing the
1This figure is the mean of two different estimates (Sny-
der and Palmer, 2004), the difference being due to multiword
handling.
419
predominant sense of a word from raw text. They
carried out a limited test of their method on text in
two domains using subject field codes (Magnini and
Cavaglia`, 2000) to assess whether the acquired pre-
dominant sense information was broadly consistent
with the domain of the text it was acquired from.
But they did not evaluate their method on hand-
tagged domain-specific corpora since there was no
such data publicly available.
In this paper, we evaluate the method on domain
specific text by creating a sense-annotated gold stan-
dard2 for a sample of words. We used a lexical sam-
ple because the cost of hand tagging several corpora
for an all-words task would be prohibitive. We show
that the sense distributions of words in this lexical
sample differ depending on domain. We also show
that sense distributions are more skewed in domain-
specific text. Using McCarthy et al?s method, we
automatically acquire predominant sense informa-
tion for the lexical sample from the (raw) corpora,
and evaluate the accuracy of this and predominant
sense information derived from SemCor. We show
that in our domains and for these words, first sense
information automatically acquired from a general
corpus is more accurate than first senses derived
from SemCor. We also show that deriving first sense
information from text in the same domain as the tar-
get data performs best, particularly when focusing
on words which are salient to that domain.
The paper is structured as follows. In section 2
we summarise McCarthy et al?s predominant sense
method. We then (section 3) describe the new gold
standard corpora, and evaluate predominant sense
accuracy (section 4). We discuss the results with
a proposal for applying the method to an all-words
task, and an analysis of our results in terms of this
proposal before concluding with future directions.
2 Finding Predominant Senses
We use the method described in McCarthy et al
(2004) for finding predominant senses from raw
text. The method uses a thesaurus obtained from
the text by parsing, extracting grammatical relations
and then listing each word (  ) with its top  nearest
neighbours, where  is a constant. Like McCarthy
2This resource will be made publicly available for research
purposes in the near future.
et al (2004) we use 	 and obtain our thesaurus
using the distributional similarity metric described
by Lin (1998). We use WordNet (WN) as our sense
inventory. The senses of a word  are each assigned
a ranking score which sums over the distributional
similarity scores of the neighbours and weights each
neighbour?s score by a WN Similarity score (Pat-
wardhan and Pedersen, 2003) between the sense of
 and the sense of the neighbour that maximises the
WN Similarity score. This weight is normalised by
the sum of such WN similarity scores between all
senses of  and and the senses of the neighbour that
maximises this score. We use the WN Similarity jcn
score (Jiang and Conrath, 1997) since this gave rea-
sonable results for McCarthy et al and it is efficient
at run time given precompilation of frequency infor-
mation. The jcn measure needs word frequency in-
formation, which we obtained from the British Na-
tional Corpus (BNC) (Leech, 1992). The distribu-
tional thesaurus was constructed using subject, di-
rect object adjective modifier and noun modifier re-
lations.
3 Creating the Three Gold Standards
In our experiments, we compare for a sample
of nouns the sense rankings created from a bal-
anced corpus (the BNC) with rankings created from
domain-specific corpora (FINANCE and SPORTS)
extracted from the Reuters corpus (Rose et al,
2002). In more detail, the three corpora are:
BNC: The ?written? documents, amounting to 3209
documents (around 89.7M words), and covering a
wide range of topic domains.
FINANCE: 117734 FINANCE documents (around
32.5M words) topic codes: ECAT and MCAT
SPORTS: 35317 SPORTS documents (around 9.1M
words) topic code: GSPO
We computed thesauruses for each of these corpora
using the procedure outlined in section 2.
3.1 Word Selection
In our experiments we used FINANCE and SPORTS
domains. To ensure that a significant number of
the chosen words are relevant for these domains,
we did not choose the words for our experiments
completely randomly. The first selection criterion
we applied used the Subject Field Code (SFC) re-
420
source (Magnini and Cavaglia`, 2000), which assigns
domain labels to synsets in WN version 1.6. We se-
lected all the polysemous nouns in WN 1.6 that have
at least one synset labelled SPORT and one synset
labelled FINANCE. This reduced the set of words
to 38. However, some of these words were fairly
obscure, did not occur frequently enough in one of
the domain corpora or were simply too polysemous.
We narrowed down the set of words using the crite-
ria: (1) frequency in the BNC 
 1000, (2) at most
12 senses, and (3) at least 75 examples in each cor-
pus. Finally a couple of words were removed be-
cause the domain-specific sense was particularly ob-
scure3. The resulting set consists of 17 words4: club,
manager, record, right, bill, check, competition, con-
version, crew, delivery, division, fishing, reserve, re-
turn, score, receiver, running
We refer to this set of words as F&S cds. The first
four words occur in the BNC with high frequency ( 

10000 occurrences), the last two with low frequency
(  2000) and the rest are mid-frequency.
Three further sets of words were selected on the
basis of domain salience. We chose eight words that
are particularly salient in the Sport corpus (referred
to as S sal), eight in the Finance corpus (F sal), and
seven that had equal (not necessarily high) salience
in both, (eq sal). We computed salience as a ratio of
normalised document frequencies, using the formula

	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 547?554, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Word Sense Disambiguation Using Sense Examples Automatically Acquired
from a Second Language
Xinglong Wang
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh
EH8 9LW, UK
xwang@inf.ed.ac.uk
John Carroll
Department of Informatics
University of Sussex
Falmer, Brighton
BN1 9QH, UK
johnca@sussex.ac.uk
Abstract
We present a novel almost-unsupervised
approach to the task of Word Sense Dis-
ambiguation (WSD). We build sense ex-
amples automatically, using large quanti-
ties of Chinese text, and English-Chinese
and Chinese-English bilingual dictionar-
ies, taking advantage of the observation
that mappings between words and mean-
ings are often different in typologically
distant languages. We train a classifier on
the sense examples and test it on a gold
standard English WSD dataset. The eval-
uation gives results that exceed previous
state-of-the-art results for comparable sys-
tems. We also demonstrate that a little
manual effort can improve the quality of
sense examples, as measured by WSD ac-
curacy. The performance of the classifier
on WSD also improves as the number of
training sense examples increases.
1 Introduction
The results of the recent Senseval-3 competition
(Mihalcea et al, 2004) have shown that supervised
WSD methods can yield up to 72.9% accuracy1
on words for which manually sense-tagged data are
available. However, supervised methods suffer from
the so-called knowledge acquisition bottleneck: they
need large quantities of high quality annotated data
1This figure refers to the highest accuracy achieved in the
Senseval-3 English Lexical Sample task with fine-grained scor-
ing.
to produce reliable results. Unfortunately, very
few sense-tagged corpora are available and manual
sense-tagging is extremely costly and labour inten-
sive. One way to tackle this problem is trying to
automate the sense-tagging process. For example,
Agirre et al (2001) proposed a method for building
topic signatures automatically, where a topic signa-
ture is a set of words, each associated with some
weight, that tend to co-occur with a certain concept.
Their system queries an Internet search engine with
monosemous synonyms of words that have multiple
senses in WordNet (Miller et al, 1990), and then ex-
tracts topic signatures by processing text snippets re-
turned by the search engine. They trained a classifier
on the topic signatures and evaluated it on a WSD
task, but the results were disappointing.
In recent years, WSD approaches that exploit
differences between languages have shown great
promise. Several trends are taking place simulta-
neously under this multilingual paradigm. A clas-
sic one is to acquire sense examples using bilin-
gual parallel texts (Gale et al, 1992; Resnik and
Yarowsky, 1997; Diab and Resnik, 2002; Ng et al,
2003): given a word-aligned parallel corpus, the dif-
ferent translations in a target language serve as the
?sense tags? of an ambiguous word in the source
language. For example, Ng et al (2003) acquired
sense examples using English-Chinese parallel cor-
pora, which were manually or automatically aligned
at sentence level and then word-aligned using soft-
ware. A manual selection of target translations was
then performed, grouping together senses that share
the same translation in Chinese. Finally, the occur-
rences of the word on the English side of the parallel
547
texts were considered to have been disambiguated
and ?sense tagged? by the appropriate Chinese trans-
lations. A classifier was trained on the extracted
sense examples and then evaluated on the nouns in
Senseval-2 English Lexical Sample dataset. The re-
sults appear good numerically, but since the sense
groups are not in the gold standard, comparison with
other Senseval-2 results is difficult. As discussed by
Ng et al, there are several problems with relying on
bilingual parallel corpora for data collection. First,
parallel corpora, especially accurately aligned par-
allel corpora are rare, although attempts have been
made to mine them from the Web (Resnik, 1999).
Second, it is often not possible to distinguish all
senses of a word in the source language, by merely
relying on parallel corpora, especially when the cor-
pora are relatively small. This is a common problem
for bilingual approaches: useful data for some words
cannot be collected because different senses of poly-
semous words in one language often translate to the
same word in the other. Using parallel corpora can
aggravate this problem, because even if a word sense
in the source language has a unique translation in the
target language, the translation may not occur in the
parallel corpora at all, due to the limited size of this
resource.
To alleviate these problems, researchers seek
other bilingual resources such as bilingual dictio-
naries, together with monolingual resources that can
be obtained easily. Dagan and Itai (1994) proposed
an approach to WSD using monolingual corpora, a
bilingual lexicon and a parser for the source lan-
guage. One of the problems of this method is that for
many languages, accurate parsers do not exist. With
a small amount of classified data and a large amount
of unclassified data in both the source and the tar-
get languages, Li and Li (2004) proposed bilingual
bootstrapping. This repeatedly constructs classifiers
in the two languages in parallel and boosts the per-
formance of the classifiers by classifying data in
each of the languages and by exchanging informa-
tion regarding the classified data between two lan-
guages. With a certain amount of manual work, they
reported promising results, but evaluated on rela-
tively small datasets.
In previous work, we proposed to use Chinese
monolingual corpora and Chinese-English bilin-
gual dictionaries to acquire sense examples (Wang,
2004)2. We evaluated the sense examples using a
vector space WSD model on a small dataset con-
taining words with binary senses, with promising
results. This approach does not rely on scarce re-
sources such as aligned parallel corpora or accurate
parsers.
This paper describes further progress based on our
proposal: we automatically build larger-scale sense
examples and then train a Na??ve Bayes classifier on
them. We have evaluated our system on the English
Lexical Sample Dataset from Senseval-2 and the re-
sults show conclusively that such sense examples
can be used successfully in a full-scale fine-grained
WSD task. We tried to analyse whether more sense
examples acquired this way would improve WSD
accuracy and also whether a little human effort on
sense mapping could further improve WSD perfor-
mance.
The reminder of the paper is organised as fol-
lows. Section 2 outlines the acquisition algorithm
for sense examples. Section 3 describes details of
building this resource and demonstrates our appli-
cation of sense examples to WSD. We also present
results and analysis in this section. Finally, we con-
clude in Section 4 and talk about future work.
2 Acquisition of Sense Examples
Following our previous proposal (Wang, 2004), we
automatically acquire English sense examples using
large quantities of Chinese text and English-Chinese
and Chinese-English dictionaries. The Chinese lan-
guage was chosen because it is a distant language
from English and the more distant two languages
are, the more likely that senses are lexicalised differ-
ently (Resnik and Yarowsky, 1999). The underlying
assumption of this approach is that in general each
sense of an ambiguous English word corresponds to
a distinct translation in Chinese. As shown in Fig-
ure 1, firstly, the system translates senses of an En-
glish word into Chinese words, using an English-
Chinese dictionary, and then retrieves text snippets
from a large amount of Chinese text, with the Chi-
nese translations as queries. Then, the Chinese text
snippets are segmented and then translated back to
English word by word, using a Chinese-English dic-
2Sense examples were referred to as ?topic signatures? in
that paper.
548
English ambiguous word w
Sense 1 of w Sense 2 of w
Chinese translation of
sense 2
Chinese translation of
sense 1
English-Chinese
Lexicon
Chinese text snippet 1
Chinese text snippet 2
... ...
Chinese
Search
Engine
Chinese-
English
Lexicon
Chinese text snippet 1
Chinese text snippet 2
... ...
{English sense example 1
for sense 1 of w}
{English sense example 2
for sense 1 of w}
... ...
{English sense example 1
for sense 2 of w}
{English sense example 2
for sense 2 of w}
... ...
Chinese
Segementation
Figure 1. Process of automatic acquisition of sense examples.
For simplicity, assume w has two senses.
tionary. In this way, for each sense, a set of sense
examples is produced. As an example, suppose one
wants to retrieve sense examples for the financial
sense of interest. One first looks up the Chinese
translations of this sense in an English-Chinese dic-
tionary, and finds that |E is the right Chinese
translation corresponding to this particular sense.
Then, the next stage is to automatically build a col-
lection of Chinese text snippets by either searching
in a large Chinese corpus or on the Web, using |
E as query. Since Chinese is a language written
without spaces between words, one needs to use a
segmentor to mark word boundaries before translat-
ing the snippets word by word back to English. The
result is a collection of sense examples for the finan-
cial sense of interest, each containing a bag of words
that tend to co-occur with that particular sense. For
example, {interest rate, bank, annual, economy, ...}
might be one of the sense examples extracted for the
financial sense of interest. Note that words in a sense
example are unordered.
Since this method acquires training data for WSD
systems from raw monolingual Chinese text, it
avoids the problem of the shortage of English sense-
tagged corpora, and also of the shortage of aligned
bilingual corpora. Also, if existing corpora are
not big enough, one can always harvest more text
from the Web. However, like all methods based
on the cross-language translation assumption men-
tioned above, there are potential problems. For ex-
ample, it is possible that a Chinese translation of an
English sense is also ambiguous, and thus the con-
tents of text snippets retrieved may be regarding a
concept other than the one we want. In general,
when the assumption does not hold, one could use
the glosses defined in a dictionary as queries to re-
trieve text snippets, as comprehensive bilingual dic-
tionaries tend to include translations to all senses of
a word, where multiword translations are used when
one-to-one translation is not possible. Alternatively,
a human annotator could map the senses and trans-
lations by hand. As we will describe later in this
paper, we chose the latter way in our experiments.
3 Experiments and Results
We firstly describe in detail how we prepared the
sense examples and then describe a large scale WSD
evaluation on the English Senseval-2 Lexical Sam-
ple dataset (Kilgarriff, 2001). The results show that
our system trained with the sense examples achieved
significantly better accuracy than comparable sys-
tems. We also show that when a little manual effort
was invested in mapping the English word senses
to Chinese monosemous translations, WSD perfor-
mance improves accordingly. Based on further ex-
periments on a standard binary WSD dataset, we
also show that the technique scales up satisfacto-
rily so that more sense examples help achieve better
WSD accuracy.
3.1 Building Sense Examples
Following the approach described in Section 2,
we built sense examples for the 44 words in the
Senseval-2 dataset3. These 44 words have 223
senses in total to disambiguate. The first step was
translating English senses to Chinese. We used the
Yahoo! Student English-Chinese On-line Dictio-
nary4, as well as a more comprehensive electronic
dictionary. This is because the Yahoo! dictionary is
designed for English learners, and its sense granu-
larity is rather coarse-grained. It is good enough for
words with fewer or coarse-grained senses. How-
3These 44 words cover all nouns and adjectives in the
Senseval-2 dataset, but exclude verbs. We discuss this point
in section 3.2.
4See: http://cn.yahoo.com/dictionary.
549
ever, the Senseval-2 Lexical Sample task5 uses
WordNet 1.7 as gold standard, which has very fine
sense distinctions and translation granularity in the
Yahoo! dictionary does not conform to this standard.
PowerWord 20026 was chosen as a supplementary
dictionary because it integrates several comprehen-
sive English-Chinese dictionaries in a single appli-
cation. For each sense of an English word entry, both
Yahoo! and PowerWord 2002 dictionaries list not
only Chinese translations but also English glosses,
which provides a bridge between WordNet synsets
and Chinese translations in the dictionaries. In de-
tail, to automatically find a Chinese translation for
sense s of an English word w, our system looks up
w in both dictionaries and determines whether w has
the same or greater number of senses as in Word-
Net. If it does, in one of the bilingual dictionaries,
we locate the English gloss g which has the max-
imum number of overlapping words with the gloss
for s in the WordNet synset. The Chinese transla-
tion associated with g is then selected. Although
this simple method successfully identified Chinese
translations for 23 out of the 44 words (52%), trans-
lations for the remaining word senses remain un-
known because the sense distinctions are different
between our bilingual dictionaries and WordNet. In
fact, unless an English-Chinese bilingual WordNet
becomes available, this problem is inevitable. For
our experiments, we solved the problem by manu-
ally looking up dictionaries and identifying transla-
tions. For each one of the 44 words, PowerWord
2002 provides more Chinese translations than the
number of its synsets in WordNet 1.7. Thus the an-
notator simply selects the Chinese translations that
he considers a best match to the corresponding En-
glish senses. This task took an hour for an annotator
who speaks both languages fluently.
It is possible that the Chinese translations are also
ambiguous, which can make the topic of a collection
of text snippets deviate from what is expected. For
example, the oral sense of mouth can be translated as
? or?n in Chinese. However, the first translation
5The task has two variations: one to disambiguate fine-
grained senses and the other to coarse-grained ones. We evalu-
ated our sense examples on the former variation, which is obvi-
ously more difficult.
6A commercial electronic dictionary application. We used
the free on-line version at: http://cb.kingsoft.com.
(?) is a single-character word and is highly ambigu-
ous: by combining with other characters, its mean-
ing varies. For example,??means ?an exit? or ?to
export?. On the other hand, the second translation
(?n) is monosemous and should be used. To as-
sess the influence of such ?ambiguous translations?,
we carried out experiments involving more human
labour to verify the translations. The same annotator
manually eliminated those highly ambiguous Chi-
nese translations and then replaced them with less
ambiguous or ideally monosemous Chinese trans-
lations. This process changed roughly half of the
translations and took about five hours. We compared
the basic system with this manually improved one.
The results are presented in section 3.2.
Using translations as queries, the sense examples
were automatically extracted from the Chinese Gi-
gaword Corpus (CGC), distributed by the LDC7,
which contains 2.7GB newswire text, of which
900MB are sourced from Xinhua News Agency of
Beijing, and 1.8GB are drawn from Central News
from Taiwan. A small percentage of words have
different meanings in these two Chinese dialects,
and since the Chinese-English dictionary (LDC
Mandarin-English Translation Lexicon Version 3.0)
we use later is compiled with Mandarin usages in
mind, we mainly retrieve data from Xinhua News.
We set a threshold of 100, and only when the amount
of snippets retrieved from Xinhua News is smaller
than 100, do we turn to Central News to collect more
data. Specifically, for 48 out of the 223 (22%) Chi-
nese queries, the system retrieved less than 100 in-
stances from Xinhua News so it extracted more data
from Central News. In theory, if the training data is
still not enough, one could always turn to other text
resources, such as the Web.
To decide the optimal length of text snippets to
retrieve, we carried out pilot experiments with two
length settings: 250 (? 110 English words) and
400 (? 175 English words) Chinese characters, and
found that more context words helped improve WSD
performance (results not shown). Therefore, we re-
trieve text snippets with a length of 400 characters.
We then segmented all text snippets, using an ap-
plication ICTCLAS8. After the segmentor marked
7Available at: http://www.ldc.upenn.edu/Catalog/
8See: http://mtgroup.ict.ac.cn/?zhp/ICTCLAS
550
all word boundaries, the system automatically trans-
lated the text snippets word by word using the elec-
tronic LDC Mandarin-English Translation Lexicon
3.0. As expected, the lexicon does not cover all
Chinese words. We simply discarded those Chi-
nese words that do not have an entry in this lexi-
con. We also discarded those Chinese words with
multiword English translations. Since the discarded
words can be informative, one direction of our re-
search in the future is to find an up-to-date wide cov-
erage dictionary, and to see how much difference it
will make. Finally, we filtered the sense examples
with a stop-word list, to ensure only content words
were included.
We ended up with 223 sets of sense examples
for all senses of the 44 nouns and adjectives in the
test dataset. Each sense example contains a set of
words that were translated from a Chinese text snip-
pet, whose content should closely relate to the En-
glish word sense in question. Words in a sense ex-
ample are unordered, because in this work we only
used bag-of-words information. Except for the very
small amount of manual work described above to
map WordNet glosses to those in English-Chinese
dictionaries, the whole process is automatic.
3.2 WSD Experiments on Senseval-2 Lexical
Sample dataset
The Senseval-2 English Lexical Sample Dataset
consists of manually sense-tagged training and test
instances for nouns, adjectives and verbs. We only
tested our system on nouns and adjectives because
verbs often have finer sense distinctions, which
would mean more manual work would need to be
done when mapping WordNet synsets to English-
Chinese dictionary glosses. This would involve us in
a rather different kind of enterprise since we would
have moved from an almost-unsupervised to a more
supervised setup.
We did not use the training data supplied with the
dataset. Instead, we train a classifier on our auto-
matically built sense examples and test it on the test
data provided. In theory, any machine learning clas-
sifier can be applied. We chose the Na??ve Bayes al-
gorithm with kernel estimation9 (John and Langley,
1995) which outperformed a few other classifiers in
9We used the implementation in the Weka machine learning
package, available at: http://www.cs.waikato.ac.nz/?ml/weka.
art-n(5)
authority-n(7)
bar-n(13)
blind-a(3)
bum-n(4)
chair-n(4)
channel-n(7)
child-n(4)
church-n(3)
circuit-n(6)
colourless-a(2)
cool-a(6)
day-n(9)
detention-n(2)
dyke-n(2)
facility-n(5)
faithful-a(3)
fatigue-n(4)
feeling-n(6)
fine-a(9)
fit-a(3)
free-a(8)
graceful-a(2)
green-a(7)
grip-n(7)
hearth-n(3)
holiday-n(2)
lady-n(3)
local-a(3)
material-n(5)
mouth-n(8)
nation-n(3)
natural-a(10)
nature-n(5)
oblique-a(2)
post-n(8)
restraint-n(6)
sense-n(5)
simple-a(7)
solemn-a(2)
spade-n(3)
stress-n(5)
vital-a(4)
yew-n(2)
Basic
39.8
21.5
44.7
74.5
64.4
80.0
32.4
56.3
59.4
48.8
66.7
50.9
32.4
60.6
82.8
27.1
66.7
77.3
50.0
34.3
44.8
37.3
70.0
53.2
35.3
48.5
64.5
71.7
38.5
47.8
38.3
39.5
14.6
27.7
72.4
34.7
17.4
25.9
49.3
73.1
67.6
45.0
41.0
82.8
 Sys B
MW
59.6
23.7
52.0
75.0
62.2
82.9
36.5
56.3
59.4
69.8
69.4
50.9
33.1
84.8
86.2
28.8
66.7
77.3
50.0
32.9
44.8
48.2
73.3
58.5
37.3
51.5
75.0
77.8
43.6
49.3
41.7
39.5
34.0
31.9
73.3
45.6
19.6
46.3
50.7
76.9
70.6
45.0
46.2
89.7
Lesk(U)
16.3
30.4
2.0
32.7
53.3
56.5
21.9
56.2
45.3
5.9
54.3
9.6
0
43.8
57.1
46.6
26.1
44.2
2.0
5.7
3.4
7.3
72.4
10.6
17.6
81.2
29.0
50.9
31.6
44.9
31.7
18.9
6.8
41.3
72.4
6.3
28.9
24.5
12.1
24.0
60.6
2.6
0
17.9
Word
46.1 52.0 24.6Avg.
Basic
29.9
20.7
41.1
74.5
60.0
81.2
31.5
56.3
53.1
48.2
45.7
26.9
32.2
62.5
85.7
20.7
69.6
76.7
11.7
8.6
44.8
29.3
58.6
53.2
35.3
46.9
64.5
69.2
36.8
39.1
38.3
35.1
14.6
23.9
72.4
34.7
6.7
20.7
45.5
64.0
66.7
37.5
42.1
21.4
 Sys A
MW
51.0
22.8
48.3
74.5
60.0
82.6
35.6
56.3
56.3
68.2
45.7
26.9
32.9
84.4
85.7
22.4
69.6
74.4
11.7
11.4
44.8
37.8
58.6
58.5
37.3
50.0
74.2
73.6
42.1
44.9
41.7
35.1
32.0
26.1
72.4
45.6
6.7
43.4
45.5
76.0
63.6
37.5
42.1
25.0
40.7 46.0
RB
16.3
10.0
3.3
40.0
15.6
23.2
12.3
18.8
29.7
10.6
42.9
13.5
7.6
43.8
28.6
13.8
21.7
25.6
9.8
7.1
31.0
15.9
62.1
21.3
19.6
31.2
38.7
28.3
26.3
10.1
11.7
21.6
6.8
15.2
44.8
10.1
11.1
24.5
13.6
32.0
18.2
12.8
21.1
57.1
Baselines & A Senseval-2 Entry
MFB
41.8
39.1
38.4
78.2
68.9
76.8
13.7
54.7
56.2
27.1
65.7
46.2
60.0
62.5
53.6
48.3
78.3
76.7
56.9
42.9
58.6
35.4
79.3
75.5
35.3
71.9
77.4
64.2
55.3
20.3
36.7
78.4
27.2
45.7
69.0
31.6
28.9
24.5
51.5
96.0
63.6
48.7
92.1
78.6
18.1 50.5
UNED
50.0
34.8
27.8
74.5
11.1
81.2
17.8
43.8
62.5
55.3
31.4
46.2
20.0
78.1
35.7
25.9
78.3
86.0
60.8
44.3
48.3
35.4
79.3
78.7
21.6
65.6
54.8
58.5
34.2
53.6
48.3
70.3
44.7
23.9
27.6
41.8
17.8
30.2
51.5
96.0
54.5
20.5
94.7
71.4
46.4
Table 1. WSD accuracy on words in the English Senseval-2
Lexical Sample dataset. The left most column shows words,
their POS tags and how many senses they have. ?Sys A? and
?Sys B? are our systems, and ?MW? denotes a multi-word de-
tection module was used in conjunction with the ?Basic? sys-
tem. For comparison, it also shows two baselines: ?RB? is the
random baseline and ?MFB? is the most-frequent-sense base-
line. ?UNED? is one of the best unsupervised participants
in the Senseval-2 competition and ?Lesk(U)? is the highest
unsupervised-baseline set in the workshop. All accuracies are
expressed as percentages.
our pilot experiments on other datasets (results not
shown). The average length of a sense example is
35 words, which is much shorter than the length of
the text snippets, which was set to 400 Chinese char-
acters (? 175 English words). This is because func-
tion words and words that are not listed in the LDC
Mandarin-English lexicon were eliminated. We did
not apply any weighting to the features because per-
formance went down in our pilot experiments when
we applied a TF.IDF weighting scheme (results not
shown). We also limited the maximum number of
551
training sense examples to 6000, for efficiency pur-
poses. We attempted to tag every test data instance,
so our coverage (on nouns and adjectives) is 100%.
To assess the influence of ambiguous Chinese
translations, we prepared two sets of training data.
As described in section 3.1: sense examples in the
first set were prepared without taking ambiguity in
Chinese text into consideration, while those in the
second set were prepared with a little more human
effort involved trying to reduce ambiguity by us-
ing less ambiguous translations. We call the system
trained on the first set ?Sys A? and the one trained
on the second ?Sys B?.
In this lexical sample task, multiwords are ex-
pected to be picked out by participating WSD sys-
tems. For example, the answer art collection should
be supplied when this multiword occurs in a test
instance. It would be judged wrong if one tagged
the art in art collection as the artworks sense, even
though one could argue that this was also a cor-
rect answer. To deal with multiwords, we imple-
mented a very simple detection module, which tries
to match multiword entries in WordNet to the am-
biguous word and its left and right neighbours. For
example, if the module finds art collection is an en-
try in WordNet, it tags all occurrences of this multi-
word in the test data, regardless of the prediction by
the classifier.
The results are shown in Table 1. Our ?Sys B?
system, with and without the multiword detection
module, outperformed ?Sys A?, which shows that
sense examples acquired with less ambiguous Chi-
nese translations contain less noise and therefore
boost WSD performance. For comparison, the ta-
ble also shows various baseline performance figures
and a system that participated in Senseval-210. Con-
sidering that the manual work involved in our ap-
proach is negligible compared with manual sense-
tagging, we classify our systems as unsupervised
and we should aim to beat the random baseline.
This all four of our systems do easily. We also eas-
ily beat another unsupervised baseline ? the Lesk
(1986) baseline, which disambiguates words using
WordNet definitions. The MFB baseline is actu-
ally a ?supervised? baseline, since an unsupervised
10Accuracies for each word and averages were calculated
by us, based on the information on Senseval-2 Website. See:
http://www.sle.sharp.co.uk/senseval2/.
system does not have such prior knowledge before-
hand. McCarthy et al (2004) argue that this is a
very tough baseline for an unsupervised WSD sys-
tem to beat. Our ?Sys B? with multiword detection
exceeds it. ?Sys B? also exceeds the performance
of UNED (Ferna?ndez-Amoro?s et al, 2001), which
was the second-best ranked11 unsupervised systems
in the Senseval-2 competition.
There are a number of factors that can influence
WSD performance. The distribution of training data
for senses is one. In our experiments, we used all
sense examples that we built for a sense (with an
upper bound of 6000). However, the distribution of
senses in English text often does not match the dis-
tribution of their corresponding Chinese translations
in Chinese text. For example, suppose an English
word w has two senses: s1 and s2, where s1 rarely
occurs in English text, whereas sense s2 is used fre-
quently. Also suppose s1?s Chinese translation is
much more frequently used than s2?s translation in
Chinese text. Thus, the distribution of the two senses
in English is different from that of the translations in
Chinese. As a result, the numbers of sense exam-
ples we would acquire for the two senses would be
distributed as if they were in Chinese text. A clas-
sifier trained on this data would then tend to predict
unseen test instances in favour of the wrong distribu-
tion. The word nation, for example, has three senses,
of which the country sense is used more frequently
in English. However, in Chinese, the country sense
and the people sense are almost equally distributed,
which might be the reason for its WSD accuracy be-
ing lower with our systems than most of the other
words. A possible way to alleviate this problem is to
select training sense examples according to an esti-
mated distribution in natural English text, which can
be done by analysing available sense-tagged corpora
with help of smoothing techniques, or with the un-
supervised approach of (McCarthy et al, 2004).
Cultural differences can cause difficulty in retriev-
ing sufficient training data. For example, transla-
tions of senses of church and hearth appear only in-
frequently in Chinese text. Thus, it is hard to build
sense examples for these words. Another problem,
11One system performed better but their answers were not
on the official Senseval-2 website so that we could not do the
comparison. Also, that system did not attempt to disambiguate
as many words as UNED and us.
552
as mentioned above, is that translations of English
senses can be ambiguous in Chinese. For exam-
ple, Chinese translations of the words vital, natu-
ral, local etc. are also ambiguous to some extent,
and this might be a reason for their low perfor-
mance. One way to solve this, as we described, is
to manually check the translations. Another auto-
matic way is that, before retrieving text snippets, we
could segment or even parse the Chinese corpora,
which should reduce the level of ambiguity and lead
to better sense examples.
3.3 Further WSD Experiments
One of the strengths of our approach is that training
data come cheaply and relatively easily. However,
the sense examples are acquired automatically and
they inevitably contain a certain amount of noise,
which may cause problems for the classifier. To as-
sess the relationship between accuracy and the size
of training data, we carried out a series of experi-
ments, feeding the classifier with different numbers
of sense examples as training data.
For these experiments, we used another standard
WSD dataset, the TWA dataset. This is a manu-
ally sense-tagged corpus (Mihalcea, 2003), which
contains 2-way sense-tagged text instances, drawn
from the British National Corpus, for 6 nouns. We
first built sense examples for all the 12 senses using
the approach described above, then trained the same
Na??ve Bayes algorithm (NB) on different numbers
of sense examples.
In detail, for all of the 6 words, we did the fol-
lowing: given a word wi, we randomly selected n
sense examples for each of its senses si, from the
total amount of sense examples built for si. Then
the NB algorithm was trained on the 2 ? n exam-
ples and tested on wi?s test instances in TWA. We
recorded the accuracy and repeated this process 200
times and calculated the mean and variance of the
200 accuracies. Then we assigned another value to
n and iterated the above process until n took all the
predefined values. In our experiments, n was taken
from {50, 100, 150, 200, 400, 600, 800, 1000, 1200}
for words motion, plant and tank and from {50, 100,
150, 200, 250, 300, 350} for bass, crane and palm,
because there were less sense example data available
for the latter three words. Finally, we used the t-test
(p = 0.05) on pairwise sets of means and variances
to see if improvements were statistically significant.
 0.93
 0.91
 0.89
 0.87
 0.85
 0  50  100  150  200  250  300  350  400bass
 0.78
 0.76
 0.74
 0.72
 0.7
 0.68
 0  50  100  150  200  250  300  350  400crane
 0.82
 0.79
 0.76
 0.73
 0.7
 0.67
 0.64
 0  200  400  600  800  1000  1200  1400motion
 0.77
 0.76
 0.75
 0.74
 0.73
 0.72
 0  50  100  150  200  250  300  350  400palm
 0.76
 0.74
 0.72
 0.7
 0.68
 0.66
 0.64
 0  200  400  600  800  1000  1200  1400plant
 0.74
 0.72
 0.7
 0.68
 0.66
 0.64
 0  200  400  600  800  1000  1200  1400tank
Figure 2. Accuracy scores with increasing number of training
sense examples. Each bar is a standard deviation.
The results are shown in Figure 212. 34 out of 42
t-scores are greater than the t-test critical values, so
we are fairly confident that the more training sense
examples used, the more accurate the NB classifier
becomes on this disambiguation task.
4 Conclusions and Future Work
We have presented WSD systems that use sense ex-
amples as training data. Sense examples are ac-
quired automatically from large quantities of Chi-
nese text, with the help of Chinese-English and
English-Chinese dictionaries. We have tested our
WSD systems on the English Senseval-2 Lexical
Sample dataset, and our best system outperformed
comparable state-of-the-art unsupervised systems.
Also, we found that increasing the number of the
sense examples significantly improved WSD perfor-
mance. Since sense examples can be obtained very
cheaply from any large Chinese text collection, in-
12These experiments showed that our systems outperformed
the most-frequent-sense baseline and Mihalcea?s unsupervised
system (2003).
553
cluding the Web, our approach is a way to tackle the
knowledge acquisition bottleneck.
There are a number of future directions that we
could investigate. Firstly, instead of using a bilin-
gual dictionary to translate Chinese text snippets
back to English, we could use machine translation
software. Secondly, we could try this approach on
other language pairs, Japanese-English, for exam-
ple. This is also a possible solution to the problem
that ambiguity may be preserved between Chinese
and English. In other words, when a Chinese transla-
tion of an English sense is still ambiguous, we could
try to collect sense examples using translation in a
third language, Japanese, for instance. Thirdly, it
would be interesting to try to tackle the problem of
Chinese WSD using sense examples built using En-
glish, the reverse process to the one described in this
paper.
Acknowledgements
This research was funded by EU IST-2001-34460
project MEANING: Developing Multilingual Web-
Scale Language Technologies.
References
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching WordNet concepts with topic
signatures. In Proceedings of the NAACL workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations. Pittsburgh, USA.
Ido Dagan and Alon Itai. 1994. Word sense disam-
biguation using a second language monolingual cor-
pus. Computational Linguistics, 20(4):563?596.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the 40th Anniversary Meeting of the
Association for Computational Linguistics (ACL-02).
Philadelphia, USA.
David Ferna?ndez-Amoro?s, Julio Gonzalo, and Felisa
Verdejo. 2001. The UNED systems at Senseval-
2. In Poceedings of Second International Wordshop
on Evaluating Word Sense Disambiguation Systems
(Senseval-2). Toulouse, France.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Using bilingual materials to de-
velop word sense disambiguation methods. In Pro-
ceedings of the International Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion, pages 101?112.
George H. John and Pat Langley. 1995. Estimating con-
tinuous distributions in Bayesian classifiers. In Pro-
ceedings of the Eleventh Conference on Uncertainty in
Artificial Intelligence, pages 338?345.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the Second International
Workshop on Evaluating Word Sense Disambiguation
Systems (SENSEVAL-2). Toulouse, France.
Michael E. Lesk. 1986. Automated sense disambigua-
tion using machine-readable dictionaries: how to tell a
pinecone from an ice cream cone. In Proceedings of
the SIGDOC Conference.
Hang Li and Cong Li. 2004. Word translation dis-
ambiguation using bilingual bootstrapping. Compu-
tational Linguistics, 20(4):563?596.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics. Barcelona, Spain.
Rada Mihalcea, Timothy Chklovski, and Adam Killgar-
iff. 2004. The Senseval-3 English lexical sample task.
In Proceedings of the Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text (Senseval-3).
Rada Mihalcea. 2003. The role of non-ambiguous words
in natural language disambiguation. In Proceedings of
the Conference on Recent Advances in Natural Lan-
guage Processing, RANLP 2003. Borovetz, Bulgaria.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
Journal of Lexicography, 3(4):235?244.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What
and How?, pages 79?86.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2):113?133.
Philip Resnik. 1999. Mining the Web for bilingual text.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics.
Xinglong Wang. 2004. Automatic acquisition of En-
glish topic signatures based on a second language. In
Proceedings of the Student Research Workshop at ACL
2004. Barcelona, Spain.
554
Unsupervised Classification of Sentiment and Objectivity
in Chinese Text
TarasZagibalov John Carroll
University of Sussex
Department of Informatics
Brighton BN1 9QH, UK
{T.Zagibalov,J.A.Carroll}@sussex.ac.uk
Abstract
We address the problem of sentiment and
objectivity classification of product re-
views in Chinese. Our approach is distinct-
ive in that it treats both positive / negative
sentiment and subjectivity / objectivity not
as distinct classes but rather as a con-
tinuum; we argue that this is desirable from
the perspective of would-be customers who
read the reviews. We use novel unsuper-
vised techniques, including a one-word
'seed' vocabulary and iterative retraining
for sentiment processing, and a criterion of
'sentiment density' for determining the ex-
tent to which a document is opinionated.
The classifier achieves up to 87% F-meas-
ure for sentiment polarity detection.
1 Introduction
Automatic classification of sentiment has been a
focus of a number of recent research efforts (e.g.
(Turney, 2002; Pang et al, 2002; Dave at al.,
2003). An important potential application of such
work is in business intelligence: brands and com-
pany image are valuable property, so organizations
want to know how they are viewed by the media
(what the 'spin' is on news stories, and editorials),
business analysts (as expressed in stock market re-
ports), customers (for example on product review
sites) and their own employees. Another important
application is to help people find out others' views
about products they have purchased (e.g. consumer
electronics), services and entertainment (e.g.
movies), stocks and shares (from investor bulletin
boards), and so on. In the work reported in this pa-
per we focus on product reviews, with the intended
users of the processing being would-be customers.
Our approach is based on the insight that posi-
tive and negative sentiments are extreme points in
a continuum of sentiment, and that intermediate
points in this continuum are of potential interest.
For instance, in one scenario, someone might want
to get an idea of the types of things people are say-
ing about a particular product through reading a
sample of reviews covering the spectrum from
highly positive, through balanced, to highly nega-
tive. (We call a review balanced if it is an opinion-
ated text with an undecided or weak sentiment di-
rection). In another scenario, a would-be customer
might only be interested in reading balanced re-
views, since they often present more reasoned ar-
guments with fewer unsupported claims. Such a
person might therefore want to avoid reviews such
as Example (1) ? written by a Chinese purchaser of
a mobile phone (our English gloss).
(1)
?????????????????
?????????????????
??????????????????
??????????????????
?????????????????
????
The software is bad, some sent SMS are nev-
er received by the addressee; compatibility
is also bad, on some mobile phones the re-
ceived messages are in a scrambled encod-
ing! And sometimes the phone 'dies'! Photos
are horrible! It doesn't have a cyclic or pro-
304
grammable alarm-clock, you have to set it
every time, how cumbersome! The back cov-
er does not fit! The original software has
many holes!
In a third scenario, someone might decide they
would like only to read opinionated, weakly nega-
tive reviews such as Example (2), since these often
contain good argumentation while still identifying
the most salient bad aspects of a product.
(2)
?????????????????
?30KB??????????MP3???
??????????????????
?????????????????
?????????????????
????????????????
The response time of this mobile is very
long, MMS should be less than 30kb only to
be downloaded, also it doesn't support MP3
ring tones, (while) the built-in tunes are not
good, and from time to time it 'dies', but
when I was buying it I really liked it: very
original, very nicely matching red and white
colours, it has its individuality, also it's not
expensive, but when used it always causes
trouble, makes one's head ache
The review contains both positive and negative
sentiment covering different aspects of the product,
and the fact that it contains a balance of views
means that it is likely to be useful for a would-be
customer. Moving beyond review classification,
more advanced tasks such as automatic summa-
rization of reviews (e.g. Feiguina & LaPalme,
2007) might also benefit from techniques which
could distinguish more shades of sentiment than
just a binary positive / negative distinction.
A second dimension, orthogonal to positive /
negative, is opinionated / unopinionated (or equiv-
alently subjective / objective). When shopping for
a product, one might be interested in the physical
characteristics of the product or what features the
product has, rather than opinions about how well
these features work or about how well the product
as a whole functions. Thus, if one is looking for a
review that contains more factual information than
opinion, one might be interested in reviews like
Example (3).
(3)
?????????????????
???????5??800??500???
?????????????????
???WAP????????????
(My) overall feeling about this mobile is not
bad, it features: 5 alarm-clocks that switch
the phone on (off), phone book for 800 items
(500 people), lunar and solar calendars,
fast switching between time and date modes,
WAPnetworking, organizer,notebook and
so on.
This review is mostly neutral (unopinionated), but
contains information that could be useful to a
would-be customer which might not be in a prod-
uct specification document, e.g. fast switching be-
tween different operating modes. Similarly, would-
be customers might be interested in retrieving
completely unopinionated documents such as tech-
nical descriptions and user manuals. Again, as with
sentiment classification, we argue that opinionated
and unopinionated texts are not easily distinguish-
able separate sets, but form a continuum. In this
continuum, intermediate points are of interest as
well as the extremes.
A major obstacle for automatic classification of
sentiment and objectivity is lack of training data,
which limits the applicability of approaches based
on supervised machine learning. With the rapid
growth in textual data and the emergence of new
domains of knowledge it is virtually impossible to
maintain corpora of tagged data that cover all ? or
even most ? areas of interest. The cost of manual
tagging also adds to the problem. Reusing the same
corpus for training classifiers for new domains is
also not effective: several studies report decreased
accuracy in cross-domain classification (Engstr?m,
2004; Aue & Gamon, 2005) a similar problem has
also been observed in classification of documents
created over different time periods (Read, 2005).
In this paper we describe an unsupervised classi-
fication technique which is able to build its own
sentiment vocabulary starting from a very small
seed vocabulary, using iterative retraining to en-
large the vocabulary. In order to avoid problems of
domain dependence, the vocabulary is built using
text from the same source as the text which is to be
classified. In this paper we work with Chinese, but
using a very small seed vocabulary may mean that
this approach would in principle need very little
linguistic adjustment to be applied to a different
305
language. Written Chinese has some specific fea-
tures, one of which is the absence of explicitly
marked word boundaries, which makes word-based
processing problematic. In keeping with our unsu-
pervised, knowledge-poor approach, we do not use
any preliminary word segmentation tools or higher
level grammatical analysis.
The paper is structured as follows. Section 2 re-
views related work in sentiment classification and
more generally in unsupervised training of classi-
fiers. Section 3 describes our datasets, and Section
4 the techniques we use for unsupervised classifi-
cation and iterative retraining. Sections 5 and 6 de-
scribe a number of experiments into how well the
approaches work, and Section 7 concludes.
2 Related Work
2.1 Sentiment Classification
Most previous work on the problem of categoriz-
ing opinionated texts has focused on the binary
classification of positive and negative sentiment
(Turney, 2002; Pang et al, 2002; Dave at al.,
2003). However, Pang & Lee (2005) describe an
approach closer to ours in which they determine an
author's evaluation with respect to a multi-point
scale, similar to the 'five-star' sentiment scale
widely used on review sites. However, authors of
reviews are inconsistent in assigning fine-grained
ratings and quite often star systems are not consis-
tent between critics. This makes their approach
very author-dependent. The main differences are
that Pang and Lee use discrete classes (although
more than two), not a continuum as in our ap-
proach, and use supervised machine learning rather
than unsupervised techniques. A similar approach
was adopted by Hagedorn et al (2007), applied to
news stories: they defined five classes encoding
sentiment intensity and trained their classifier on a
manually tagged training corpus. They note that
world knowledge is necessary for accurate classifi-
cation in such open-ended domains.
There has also been previous work on determin-
ing whether a given text is factual or expresses
opinion (Yu& Hatzivassiloglu, 2003; Pang & Lee,
2004); again this work uses a binary distinction,
and supervised rather than unsupervised approach-
es.
Recent work on classification of terms with re-
spect to opinion (Esuli & Sebastiani, 2006) uses a
three-category system to characterize the opinion-
related properties of word meanings, assigning nu-
merical scores to Positive, Negative and Objective
categories. The visualization of these scores some-
what resembles our graphs in Section 5, although
we use two orthogonal scales rather than three cat-
egories; we are also concerned with classification
of documents rather than terms.
2.2 Unsupervised Classification
Abney (2002) compares two major kinds of unsu-
pervised approach to classification (co-training and
the Yarowsky algorithm). As we do not use multi-
ple classifiers our approach is quite far from co-
training. But it is close to the paradigm described
by Yarowsky (1995) and Turney (2002) as it also
employs self-training based on a relatively small
seed data set which is incrementally enlarged with
unlabelled samples. But our approach does not use
point-wise mutual information. Instead we use rel-
ative frequencies of newly found features in a
training subcorpus produced by the previous itera-
tion of the classifier. We also use the smallest pos-
sible seed vocabulary, containing just a single
word; however there are no restrictions regarding
the maximum number of items in the seed vocabu-
lary.
3 Data
3.1 Seed Vocabulary
Our approach starts out with a seed vocabulary
consisting of a single word, ? (good). This word
is tagged as a positive vocabulary item; initially
there are no negative items. The choice of word
was arbitrary, and other words with strongly posi-
tive or negative meaning would also be plausible
seeds. Indeed, ? might not be the best possible
seed, as it is relatively ambiguous: in some con-
texts it means to like or acts as the adverbial very,
and is often used as part of other words (although
usually contributing a positive meaning). But since
it is one of the most frequent units in the Chinese
language, it is likely to occur in a relatively large
number of reviews, which is important for the
rapid growth of the vocabulary list.
3.2 TestCorpus
Our test corpus is derived from product reviews
harvested from the website IT1681. All the reviews
were tagged by their authors as either positive or
negative overall. Most reviews consist of two or
three distinct parts: positive opinions, negative
opinions, and comments ('other') ? although some
1http://product.it168.com
306
reviews have only one part. We removed duplicate
reviews automatically using approximate match-
ing, giving a corpus of 29531 reviews of which
23122 are positive (78%) and 6409 are negative
(22%). The total number of different products in
the corpus is 10631, the number of product cate-
gories is 255, and most of the reviewed products
are either software products or consumer electron-
ics. Unfortunately, it appears that some users mis-
used the sentiment tagging facility on the website
so quite a lot of reviews have incorrect tags. How-
ever, the parts of the reviews are much more reli-
ably identified as being positive or negative so we
used these as the items of the test corpus. In the ex-
periments described in this paper we used 2317 re-
views of mobile phones of which 1158 are nega-
tive and 1159 are positive. Thus random choice
would have approximately 50% accuracy if all
items were tagged either as negative or positive2.
4 Method
4.1 Sentiment Classification
As discussed in Section 1, we do not carry out any
word segmentation or grammatical processing of
input documents. We use a very broad notion of
words (or phrases) in the Chinese language. The
basic units of processing are 'lexical items', each of
which is a sequence of one or more Chinese char-
acters excluding punctuation marks (which may
actually form part of a word, a whole word or a se-
quence of words), and `zones', each of which is a
sequence of characters delimited by punctuation
marks.
Each zone is classified as either positive or neg-
ative based whether positive or negative vocabu-
lary items predominate. In more detail, a simple
maximum match algorithm is used to find all lexi-
cal items (character sequences) in the zone that are
in the vocabulary list. As there are two parts of the
vocabulary (positive and negative), we correspond-
ingly calculate two scores using Equation (1)3,
S i=
Ld
L phrase
S d N d (1)
where Ld is the length in characters of a matching
lexical item, Lphrase is the length of the current zone
2This corpus is publicly available at http://www.informatics.
sussex.ac.uk/users/tz21/it168test.zip
3In the first iteration, when we have only one item in the vo-
cabulary, negative zones are found by means of the negation
check (so not + good = negative item).
in characters, Sd is the current sentiment score of
the matching lexical item (initially 1.0), and Nd is a
negation check coefficient. The negation check is a
regular expression which determines if the lexical
item is preceded by a negation within its enclosing
zone. If a negation is found then Nd is set to ?1.
The check looks for six frequently occurring nega-
tions:? (bu),?? (buhui),?? (meiyou),??
(baituo),?? (mianqu), and?? (bimian).
The sentiment score of a zone is the sum of sen-
timent scores of all the items found in it. In fact
there are two competing sentiment scores for every
zone: one positive (the sum of all scores of items
found in the positive part of the vocabulary list)
and one negative (the sum of the scores for the
items in the negative part). The sentiment direction
of a zone is determined from the maximum of the
absolute values of the two competing scores for the
zone.
This procedure is applied to all zones in a docu-
ment, classifying each zone as positive, negative,
or neither (in cases where there are no positive or
negative vocabulary items in the zone). To deter-
mine the sentiment direction of the whole docu-
ment, the classifier computes the difference be-
tween the number of positive and negative zones.
If the result is greater than zero the document is
classified as positive, and vice versa. If the result is
zero the document is balanced or neutral for senti-
ment.
4.2 Iterative Retraining
The task of iterative retraining is to enlarge the ini-
tial seed vocabulary (consisting of a single word as
discussed in Section 3.1) into a comprehensive vo-
cabulary list of sentiment-bearing lexical items. In
each iteration, the current version of the classifier
is run on the product review corpus to classify each
document, resulting in a training subcorpus of pos-
itive and a negative documents. The subcorpus is
used to adjust the scores of existing positive and
negative vocabulary items and to find new items to
be included in the vocabulary.
Each lexical item that occurs at least twice in the
corpus is a candidate for inclusion in the vocabu-
lary list. After candidate items are found, the sys-
tem calculates their relative frequencies in both the
positive and negative parts of the current training
subcorpus. The system also checks for negation
while counting occurrences: if a lexical item is pre-
ceded by a negation, its count is reduced by one.
This results in negative counts (and thus negative
relative frequencies and scores) for those items that
307
are usually used with negation; for example, ??
???(the quality is far too bad) is in the positive
part of the vocabulary with a score of ?1.70. This
means that the item was found in reviews classified
by the system as positive but it was preceded by a
negation. If during classification this item is found
in a document it will reduce the positive score for
that document (as it is in the positive part of the
vocabulary), unless the item is preceded by a nega-
tion. In this situation the score will be reversed
(multiplied by ?1), and the positive score will be
increased ? see Equation (1) above.
For all candidate items we compare their relative
frequencies in the positive and negative documents
in the subcorpus using Equation (2).
difference= ?F p? F n??F p?Fn?/2
(2)
If difference < 1, then the frequencies are similar
and the item does not have enough distinguishing
power, so it is not included in the vocabulary. Oth-
erwise the the sentiment score of the item is (re-)
calculated ? according to Equation (3) for positive
items, and analogously for negative items.
F p
F p?F n
(3)
Finally, the adjusted vocabulary list with the new
scores is ready for the next iteration.
4.3 Objectivity Classification
Given a sentiment classification for each zone in a
document, we compute sentiment density as the
proportion of opinionated zones with respect to the
total number of zones in the document. Sentiment
density measures the proportion of opinionated text
in a document, and thus the degree to which the
document as a whole is opinionated.
It should be noted that neither sentiment score
nor sentiment density are absolute values, but are
relative and only valid for comparing one docu-
ment with other. Thus, a sentiment density of 0.5
does not mean that the review is half opinionated,
half not. It means that the review is less opinionat-
ed than a review with density 0.9.
5 Experiments
We ran the system on the product review corpus
(Section 3.2) for 20 iterations. The results for bina-
ry sentiment classification are shown in Table 1.
We see increasing F-measure up to iteration 18, af-
ter which both precision and recall start to de-
screase; we therefore use the version of the classi-
fier as it stood after iteration 184. These figures are
only indicative of the classification accuracy of the
system. Accuracy might be lower for unseen text,
although since our approach is unsupervised we
could in principle perform further retraining itera-
tions on any sample of new text to tune the vocab-
ulary list to it.
We also computed a (strong) baseline, using as
the vocabulary list the NTU Sentiment Dictionary
(Ku et al, 2006)5 which is intended to contain only
sentiment-related words and phrases. We assigned
each positive and negative vocabulary item a score
of 1 or ?1 respectively. This setup achieved 87.77
precision and 77.09 recall on the product review
corpus.
In Section 1 we argued that sentiment and objec-
tivity should both be considered as continuums, not
Table 1. Results for binary sentiment classifica-
tion during iterative retraining.
4The size of the sentiment vocabulary after iteration 18 was
22530 (13462 positive and 9068 negative).
5Ku et al automatically generated the dictionary by enlarging
an initial manually created seed vocabulary by consulting two
thesauri, including tong2yi4ci2ci2lin2 and the Academia Sini-
ca Bilingual Ontological WordNet 3.
Iteration Precision Recall F-measure
1 77.62 28.43 41.62
2 76.15 73.81 74.96
3 81.15 80.07 80.61
4 83.54 82.79 83.16
5 84.66 83.78 84.22
6 85.51 84.77 85.14
7 86.59 85.76 86.17
8 86.78 86.11 86.44
9 87.15 86.32 86.74
10 87.01 86.37 86.69
11 86.9 86.15 86.53
12 87.05 86.41 86.73
13 86.87 86.19 86.53
14 87.35 86.67 87.01
15 87.13 86.45 86.79
16 87.14 86.5 86.82
17 86.8 86.24 86.52
18 87.57 86.89 87.22
19 87.23 86.67 86.95
20 87.18 86.54 86.86
308
binary distinctions. Section 4.1 describes how our
approach compares the number of positive and
negative zones for a document and treats the differ-
ence as a measure of the 'positivity' or 'negativity'
of a review. The document in Example (2), with 12
zones, is assigned a score of ?1 (the least negative
score possible): the review contains some positive
sentiment but the overall sentiment direction of the
review is negative. In contrast, Example (1) is
identified as a highly negative review, as would be
expected, with a score of ?8, from 11 zones.
Similarly, with regard to objectivity, the senti-
ment density of the text in Example (3) is 0.53,
which reflects its more factual character compared
to Example (1), which has a score of 0.91. We can
represent sentiment and objectivity on the follow-
ing scales:
Negative Balanced Positive
Unopinionated Neutral Opinionated
The scales are orthogonal, so we can combine
them into a single coordinate system:
Opinionated
Negative Positive
We would expect most product reviews to be
placed towards the top of the the coordinate system
(i.e. opinionated), and stretch from left to right.
Figure 1 plots the results of sentiment and objec-
tivity classification of the test corpus in this two di-
mensional coordinate system, where X represents
sentiment (with scores scaled with respect to the
number of zones so that ?100 is the most negative
possible and +100 the most positive), and Y repre-
sents sentiment density (0 being unopinionated and
1 being highly opinionated).
Most of the reviews are located in the upper part
of the coordinate system, indicating that they have
been classified as opinionated, with either positive
or negative sentiment direction. Looking at the
overall shape of the plot, more opinionated docu-
ments tend to have more explicit sentiment direc-
tion, while less opinionated texts stay closer to the
balanced / neutral region (around X = 0).
Figure 1. Reviews classified according to
sentiment (X axis) and degree of
opinionation (Y axis).
6 Discussion
As can be seen in Figure 1, the classifier managed
to map the reviews onto the coordinate system.
However, there are very few points in the neutral
region, that is, on the same X = 0 line as balanced
but with low sentiment density. By inspection, we
know that there are neutral reviews in our data set.
We therefore conducted a further experiment to in-
vestigate what the problem might be. We took
Wikipedia6 articles written in Chinese on mobile
telephony and related issues, as well as several ar-
ticles about the technology, the market and the his-
tory of mobile telecommunications, and split them
into small parts (about a paragraph long, to make
their size close to the size of the reviews) resulting
in a corpus of 115 documents, which we assume to
be mostly unopinionated. We processed these doc-
uments with the trained classifier and found that
they were mapped almost exactly where balanced
documents should be (see Figure 2).
Most of these documents have weak sentiment
direction (X = ?5 to +10), but are classified as rel-
atively opinionated (Y > 0.5). The former is to be
expected, whereas the latter is not. When investi-
gating the possible reasons for this behavior we no-
ticed that the classifier found not only feature de-
scriptions (like ???? nice touch) or expres-
sions which describe attitude (?? (one) like(s)),
but also product features (for example,?? MMS
or ?? TV) to be opinionated. This is because the
presence of some advanced features such as MMS
in mobile phones is often regarded as a positive by
6www.wikipedia.org
-40 -30 -20 -10 0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
309
Figure 2. Classification of a sample of articles
from Wikipedia.
Figure 3. Classification of a sample of articles
from Wikipedia, using the NTU Sentiment
Dictionary as the vocabulary list.
authors of reviews. In addition, the classifier found
words that were used in reviews to describe situa-
tions connected with a product and its features: for
example,?? (service) was often used in descrip-
tions of quite unpleasant situations when a user had
to turn to a manufacturer's post-sales service for re-
pair or replacement of a malfunctioning phone, and
?? (user) was often used to describe what one
can do with some advanced features. Thus the clas-
sifier was able to capture some product-specific as
well as market-specific sentiment markers, howev-
er, it was not able to distinguish the context these
generally objective words were used in. This re-
sulted in relatively high sentiment density of neu-
tral texts which contained these words but used in
other types of context.
To verify this hypothesis we applied the same
processing to our corpus derived from Wikipedia
articles, but using as the vocabulary list the NTU
Sentiment Dictionary. The results (Figure 3) show
that most of the neutral texts are now mapped to
the lower part of the opinionation scale (Y < 0.5),
as expected. Therefore, to successfully distinguish
between balanced reviews and neutral documents a
classifier should be able to detect when product
features are used as sentiment markers and when
they are not.
7 Conclusions and Future Work
We have described an approach to classification of
documents with respect to sentiment polarity and
objectivity, representing both as a continuum, and
mapping classified documents onto a coordinate
system that also represents the difference between
balanced and neutral text. We have presented a
novel, unsupervised, iterative retraining procedure
for deriving the classifier, starting from the most
minimal size seed vocabulary, in conjunction with
a simple negation check. We have verified that the
approach produces reasonable results. The ap-
proach is extremely minimal in terms of language
processing technology, giving it good possibilities
for porting to different genres, domains and lan-
guages.
We also found that the accuracy of the method
depends a lot on the seed word chosen. If the word
has a relatively low frequency or does not have a
definite sentiment-related meaning, the results may
be very poor. For example, an antonymous word to
? (good) in Chinese is ? (bad), but the latter is
not a frequent word: the Chinese prefer to say??
(not good). When this word was used as the seed
word, accuracy was little more than 15%. Al-
though the first iteration produced high precision
(82%), the size of the extracted subcorpus was
only 24 items, resulting in the system being unable
to produce a good classifier for the following itera-
tions. Every new iteration produced an even poorer
result as each new extracted corpus was of lower
accuracy.
On the other hand, it seems that a seed list con-
sisting of several low-frequency one-character
words can compensate each other and produce bet-
ter results by capturing a larger part of the corpus
(thus increasing recall). Nevertheless a single word
may also produce results even better than those for
multiword seed lists. For example, the two-charac-
ter word ?? (comfortable) as seed reached 91%
-40 -30 -20 -10 0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-40 -30 -20 -10 0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
310
accuracy with 90% recall. We can conclude that
our method relies on the quality of the seed word.
We therefore need to investigate ways of choosing
'lucky' seeds and avoiding 'unlucky' ones.
Future work should also focus on improving
classification accuracy: adding a little language-
specific knowledge to be able to detect some word
boundaries should help; we also plan to experiment
with more sophisticated methods of sentiment
score calculation. In addition, the notion of 'zone'
needs refining and language-specific adjustments
(for example, a 'reversed comma' should not be
considered to be a zone boundary marker, since
this punctuation mark is generally used for the enu-
meration of related objects).
More experiments are also necessary to deter-
mine how the approach works across domains, and
further investigation into methods for distinguish-
ing between balanced and neutral text.
Finally, we need to produce a new corpus that
would enable us to evaluate the performance of a
pre-trained version of the classifier that did not
have any prior access to the documents it was clas-
sifying: we need the reviews to be tagged not in a
binary way as they are now, but in a way that re-
flects the two continuums we use (sentiment and
objectivity).
Acknowledgements
The first author is supported by the Ford Founda-
tion International Fellowships Program.
References
Abney, Steven (2002) Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, Philadelphia, PA. 360?367.
Aue, Anthony & Michael Gamon (2005) Customizing
sentiment classifiers to new domains: a case study. In
Proceedings of RANLP-2005.
Dave, Kushal, Steve Lawrence & David M. Pennock
(2003) Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of the Twelfth International World Wide
WebConference. 519?528.
Engstr?m, Charlotte (2004) Topic dependence in senti-
ment classification. Unpublished MPhil dissertation,
University of Cambridge.
Esuli, Andrea & Fabrizio Sebastiani (2006) SENTI-
WORDNET: a publicly available lexical resource for
opinion mining. In Proceedings of LREC-06, the 5th
Conference on Language Resources and Evaluation,
Genoa, Italy.
Hagedorn, Bennett, Massimiliano Ciaramita & Jordi At-
serias (2007) World knowledge in broad-coverage in-
formation filtering. In Proceedings of the 30th ACM
SIGIR Conference on Research and Development in
Information Retrieval. 801?802.
Ku, Lun-Wei, Yu-Ting Liang & Hsin-Hsi Chen (2006)
Opinion extraction, summarization and tracking in
news and blog corpora. In Proceedings of the AAAI-
2006 Spring Symposium on Computational Ap-
proaches to Analyzing Weblogs, AAAI Technical Re-
port.
Feiguina, Olga & Guy Lapalme (2007) Query-based
summarization of customer reviews. In Proceedings
of the 20th Canadian Conference on Artificial Intelli-
gence, Montreal, Canada. 452?463.
Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan
(2002) Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA. 79?86.
Pang, Bo & Lillian Lee (2004) A sentimental education:
sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for Computation-
al Linguistics, Barcelona, Spain. 271?278.
Pang, Bo & Lillian Lee (2005) Seeing stars: exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the 43rd
Annual Meeting of the Association for Computation-
al Linguistics, Ann Arbor, MI. 115?124.
Read, Jonathon (2005) Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the Student
Research Workshop at ACL-05, Ann Arbor, MI.
Turney, Peter D. (2002) Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, Philadelphia, PA. 417?424.
Yarowsky, David (1995) Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, Cambridge, MA.
189?196.
Yu, Hong & Vasileios Hatzivassiloglou (2003) Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
Sapporo, Japan. 129?136.
311
c? 2003 Association for Computational Linguistics
Disambiguating Nouns, Verbs, and
Adjectives Using Automatically Acquired
Selectional Preferences
Diana McCarthy? John Carroll?
University of Sussex University of Sussex
Selectional preferences have been used by word sense disambiguation (WSD) systems as one
source of disambiguating information. We evaluate WSD using selectional preferences acquired
for English adjective?noun, subject, and direct object grammatical relationships with respect to
a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather
than individual word forms, so they can be used to disambiguate the co-occurring adjectives and
verbs, rather than just the nominal argument heads. We also investigate use of the one-sense-
per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word
within the current document in order to increase coverage. Although the preferences perform
well in comparison with other unsupervised WSD systems on the same corpus, the results show
that for many applications, further knowledge sources would be required to achieve an adequate
level of accuracy and coverage. In addition to quantifying performance, we analyze the results to
investigate the situations in which the selectional preferences achieve the best precision and in
which the one-sense-per-discourse heuristic increases performance.
1. Introduction
Although selectional preferences are a possible knowledge source in an automatic
word sense disambiguation (WDS) system, they are not a panacea. One problem is
coverage: Most previous work has focused on acquiring selectional preferences for
verbs and applying them to disambiguate nouns occurring at subject and direct ob-
ject slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson
2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion
of word tokens do not fall at these slots. There has been some work looking at other
slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Fed-
erici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of
coverage remains. Selectional preferences can be used for WSD in combination with
other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascer-
tain when they work well so that they can be utilized to their full advantage. This
article is aimed at quantifying the disambiguation performance of automatically ac-
quired selectional preferences in regard to nouns, verbs, and adjectives with respect to
a standard test corpus and evaluation setup (SENSEVAL-2) and to identify strengths
and weaknesses. Although there is clearly a limit to coverage using preferences alone,
because preferences are acquired only with respect to specific grammatical roles, we
show that when dealing with running text, rather than isolated examples, coverage can
be increased at little cost in accuracy by using the one-sense-per-discourse heuristic.
? Department of Informatics, University of Sussex, Brighton BN1 9QH, UK. E-mail: {dianam,
johnca}@sussex.ac.uk
640
Computational Linguistics Volume 29, Number 4
We acquire selectional preferences as probability distributions over the Word-
Net (Fellbaum 1998) noun hyponym hierarchy. The probability distributions are con-
ditioned on a verb or adjective class and a grammatical relationship. A noun is disam-
biguated by using the preferences to give probability estimates for each of its senses
in WordNet, that is, for WordNet synsets. Verbs and adjectives are disambiguated by
using the probability distributions and Bayes? rule to obtain an estimate of the proba-
bility of the adjective or verb class, given the noun and the grammatical relationship.
Previously, we evaluated noun and verb disambiguation on the English all-words task
in the SENSEVAL-2 exercise (Cotton et al 2001). We now present results also using
preferences for adjectives, again evaluated on the SENSEVAL-2 test corpus (but carried
out after the formal evaluation deadline). The results are encouraging, given that this
method does not rely for training on any hand-tagged data or frequency distributions
derived from such data. Although a modest amount of English sense-tagged data is
available, we nevertheless believe it is important to investigate methods that do not
require such data, because there will be languages or texts for which sense-tagged
data for a given word is not available or relevant.
2. Motivation
The goal of this article is to assess the WSD performance of selectional preference
models for adjectives, verbs, and nouns on the SENSEVAL-2 test corpus. There are two
applications for WSD that we have in mind and are directing our research. The first
application is text simplification, as outlined by Carroll, Minnen, Pearce et al (1999).
One subtask in this application involves substituting words with thier more frequent
synonyms, for example, substituting letter for missive. Our motivation for using WSD
is to filter out inappropriate senses of a word token, so that the substituting synonym
is appropriate given the context. For example, in the following sentence we would
like to use strategy, rather than dodge, as a substitute for scheme:
A recent government study singled out the scheme as an example to others.
We are also investigating the disambiguation of verb senses in running text before
subcategorization information for the verbs is acquired, in order to produce a sub-
categorization lexicon specific to sense (Preiss and Korhonen 2002). For example, if
subcategorization were acquired specific to sense, rather than verb form, then distinct
senses of fire could have different subcategorization entries:
fire(1) - sack: NP V NP
fire(2) - shoot: NP V NP, NP V
Selectional preferences could also then be acquired automatically from sense-tagged
data in an iterative approach (McCarthy 2001).
3. Methodology
We acquire selectional preferences from automatically preprocessed and parsed text
during a training phase. The parser is applied to the test data as well in the run-
time phase to identify grammatical relations among nouns, verbs, and adjectives. The
acquired selectional preferences are then applied to the noun-verb and noun-adjective
pairs in these grammatical constructions for disambiguation.
641
McCarthy and Carroll Disambiguating Using Selectional Preferences
0.0005 0.09
<time>
0.080.7
<entity> <measure>
drink suck sipmodel for verbclass
Sense Tagged output
<attribute>
Disambiguator
ParserPreprocessor
Acquisition
Preference
Selectional
a single glass with...
where you can drink
Training data
d04.s12.t09    drink%2:34:00::
d06.s13.t03    man%1:18:00::
instance ID   WordNet sense tag
ncmod  glass_NN1 single_JJ
dobj   drink_VV0    glass_NN1
ncsubj drink_VV0   you_PPY
Grammatical Relations
Test data
The men drink here ...
WordNet
Selectional Preferences
Figure 1
System overview. Solid lines indicate flow of data during training, and broken lines show that
at run time.
The overall structure of the system is illustrated in Figure 1. We describe the
individual components in sections 3.1?3.3 and 4.
3.1 Preprocessing
The preprocessor consists of three modules applied in sequence: a tokenizer, a part-
of-speech (POS) tagger, and a lemmatizer.
The tokenizer comprises a small set of manually developed finite-state rules for
identifying word and sentence boundaries. The tagger (Elworthy 1994) uses a bigram
hidden Markov model augmented with a statistical unknown word guesser. When
applied to the training data for selectional preference acquisition, it produces the sin-
gle highest-ranked POS tag for each word. In the run-time phase, it returns multiple
tag hypotheses, each with an associated forward-backward probability to reduce the
impact of tagging errors. The lemmatizer (Minnen, Carroll, and Pearce 2001) reduces
inflected verbs and nouns to their base forms. It uses a set of finite-state rules express-
ing morphological regularities and subregularities, together with a list of exceptions
for specific (irregular) word forms.
3.2 Parsing
The parser uses a wide-coverage unification-based shallow grammar of English POS
tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using
a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from
extra-grammaticality by returning partial parses. The output of the parser is a set of
grammatical relations (Carroll, Briscoe, and Sanfilippo 1998) specifying the syntactic
dependency between each head and its dependent(s), taken from the phrase structure
tree that is returned from the disambiguation phase.
For selectional preference acquisition we applied the analysis system to the 90
million words of the written portion of the British National Corpus (BNC); the parser
produced complete analyses for around 60% of the sentences and partial analyses
for over 95% of the remainder. Both in the acquisition phase and at run time, we
extract from the analyser output subject?verb, verb?direct object, and noun?adjective
642
Computational Linguistics Volume 29, Number 4
modifier dependencies.1 We did not use the SENSEVAL-2 Penn Treebank?style brack-
etings supplied for the test data.
3.3 Selectional Preference Acquisition
The preferences are acquired for grammatical relations (subject, direct object, and
adjective?noun) involving nouns and grammatically related adjectives or verbs. We
use WordNet synsets to define our sense inventory. Our method exploits the hyponym
links given for nouns (e.g., cheese is a hyponym of food), the troponym links for verbs 2
(e.g., limp is a troponym of walk), and the ?similar-to? relationship given for adjectives
(e.g., one sense of cheap is similar to flimsy).
The preference models are modifications of the tree cut models (TCMs) originally
proposed by Li and Abe (1995, 1998). The main differences between that work and
ours are that we acquire adjective as well as verb models, and also that our models
are with respect to verb and adjective classes, rather than forms. We acquire models
for classes because we are using the models for WSD, whereas Li and Abe used them
for structural disambiguation.
We define a TCM as follows. Let NC be the set of noun synsets (noun classes)
in WordNet: NC = {nc ? WordNet}, and NS be the set of noun senses 3 in Wordnet:
NS = {ns ? WordNet}. A TCM is a set of noun classes that partition NS disjointly.
We use ? to refer to such a set of classes in a TCM. A TCM is defined by ? and a
probability distribution:
?
nc??
p(nc) = 1 (8)
The probability distribution is conditioned by the grammatical context. In this
work, the probability distribution associated with a TCM is conditioned on a verb
class (vc) and either the subject or direct-object relation, or an adjective class (ac)
and the adjective?noun relation. Let VC be the set of verb synsets (verb classes) in
WordNet: VC = {vc ? WordNet}. Let AC be the set of adjective classes (which subsume
WordNet synsets; we elaborate further on this subsequently). Thus, the TCMs define
a probability distribution over NS that is conditioned on a verb class (vc) or adjective
class (ac) and a particular grammatical relation (gr):
?
nc??
p(nc|vc, gr) = 1 (9)
Acquisition of a TCM for a given vc and gr proceeds as follows. The data for
acquiring the preference are obtained from a subset of the tuples involving verbs
in the synset or troponym (subordinate) synsets. Not all verbs that are troponyms
or direct members of the synset are used in training. We take the noun argument
heads occurring with verbs that have no more than 10 senses in WordNet and a
frequency of 20 or more occurrences in the BNC data in the specified grammatical
relationship. The threshold of 10 senses removes some highly polysemous verbs having
many sense distinctions that are rather subtle. Verbs that have more than 10 senses
include very frequent verbs such as be and do that do not select strongly for their
1 In a previous evaluation of grammatical-relation accuracy with in-coverage text, the analyzer returned
subject?verb and verb?direct object dependencies with 84?88% recall and precision (Carroll, Minnen,
and Briscoe 1999).
2 In WordNet, verbs are organized by the troponymy relation, but this is represented with the same
hyponym pointer as is used in the noun hierarchy.
3 We refer to nouns attached to synsets as noun senses.
643
McCarthy and Carroll Disambiguating Using Selectional Preferences
0
20
40
60
80
100
0 50 100 150 200
%
 n
ot
 in
 W
or
dN
et
frequency rank
Figure 2
Verbs not in WordNet by BNC frequency.
arguments. The frequency threshold of 20 is intended to remove noisy data. We set
the threshold by examining a plot of BNC frequency and the percentage of verbs at
particular frequencies that are not listed in WordNet (Figure 2). Using 20 as a threshold
for the subject slot results in only 5% verbs that are not found in WordNet, whereas
73% of verbs with fewer than 20 BNC occurrences are not present in WordNet.4
The selectional-preference models for adjective?noun relations are conditioned on
an ac. Each ac comprises a group of adjective WordNet synsets linked by the ?similar-
to? relation. These groups are formed such that they partition all adjective synsets.
Thus AC = {ac ? WordNet adjective synsets linked by similar-to}. For example, Figure 3
shows the adjective classes that include the adjective fundamental and that are formed
in this way.5 For selectional-preference models conditioned on adjective classes, we
use only those adjectives that have 10 synsets or less in WordNet and have 20 or more
occurrences in the BNC.
The set of ncs in ? are selected from all the possibilities in the hyponym hierarchy
according to the minimum description length (MDL) principle (Rissanen 1978) as used
by Li and Abe (1995, 1998). MDL finds the best TCM by considering the cost (in bits)
of describing both the model and the argument head data encoded in the model. The
cost (or description length) for a TCM is calculated according to equation (10). The
number of parameters of the model is given by k, which is the number of ncs in ?
minus one. N is the sample of the argument head data. The cost of describing each
noun argument head (n) is calculated by the log of the probability estimate for that
noun:
description length = model description length + data description length
= k2 ? log |N| +?
?
n?N
log p(n) (10)
4 These threshold values are somewhat arbitrary, but it turns out that our results are not sensitive to the
exact values.
5 For the sake of brevity, not all adjectives are included in this diagram.
644
Computational Linguistics Volume 29, Number 4
basic
root radical
grassroots
underlying fundamental
rudimentary
primal elementary
basal base
of import important
primal key
central fundamental cardinal
crucial essential
valuable useful
historic
chief principal primary main
measurable
weighty grevious grave
significant important
monumental
profound fundamental
epochal
earthshaking
head
portentous prodigious
evidentiary evidential
noteworthy remarkable
large
Figure 3
Adjective classes that include fundamental.
The probability estimate for each n is obtained using the estimates for all the nss
that n has. Let Cn be the set of ncs that include n as a direct member: Cn = {nc ? NC|n ?
nc}. Let nc? be a hypernym of nc on ? (i.e. nc? ? {?|nc ? nc?}) and let nsnc? = {ns ? nc?}
(i.e., the set of nouns senses at and beneath nc? in the hyponym hierarchy). Then the
estimate p(n) is obtained using the estimates for the hypernym classes on ? for all the
Cn that n belongs to:
p(n) =
?
nc?Cn
p(nc?)
|nsnc? |
(11)
The probability at any particular nc? is divided by nsnc? to give the estimate for each
p(ns) under that nc?.
The probability estimates for the {nc ? ?} ( p(nc|vc, gr) or p(nc|ac, gr)) are obtained
from the tuples from the data of nouns co-occurring with verbs (or adjectives) belong-
ing to the conditioning vc (or ac) in the specified grammatical relationship (< n, v, gr >).
The frequency credit for a tuple is divided by |Cn| for any n, and by the number of
synsets of v, Cv (or Ca if the gr is adjective-noun):
freq(nc|vc, gr) =
?
v?vc
?
n?nc
freq(n|v, gr)
|Cn||Cv|
(12)
A hypernym nc? includes the frequency credit attributed to all its hyponyms ({nc ?
nc?}).
freq(nc?|vc, gr) =
?
nc?nc?
freq(nc|vc, gr) (13)
This ensures that the total frequency credit at any ? across the hyponym hierarchy
equals the credit for the conditioning vc. This will be the sum of the frequency credit
for all verbs that are direct members or troponyms of the vc, divided by the number
of other senses of each of these verbs:
freq(vc|gr) =
?
verb?vc
freq(verb|gr)
|Cv|
(14)
645
McCarthy and Carroll Disambiguating Using Selectional Preferences
group event
Root
human_action
TCM for 
TCM for
control monarchy
throne party
possession
strawmoney
handle
0.010.020.14
0.010.02 0.53
0.06
0.050.07
0.26
entity
seize assume usurp
seize clutch
Figure 4
TCMs for the direct-object slot of two verb classes that include the verb seize.
To ensure that the TCM covers all NS in WordNet, we modify Li and Abe?s original
scheme by creating hyponym leaf classes below all WordNet?s internal classes in the
hyponym hierarchy. Each leaf holds the ns previously held at the internal class.
Figure 4 shows portions of two TCMs. The TCMs are similar, as they both contain
the verb seize, but the TCM for the class that includes clutch has a higher probability
for the entity noun class compared to the class that also includes assume and usurp.
This example includes only top-level WordNet classes, although the TCM may use
more specific noun classes.
4. Disambiguation
Nouns, adjectives and verbs are disambiguated by finding the sense (nc, vc, or ac) with
the maximum probability estimate in the given context. The method disambiguates
nouns and verbs to the WordNet synset level and adjectives to a coarse-grained level
of WordNet synsets linked by the similar-to relation, as described previously.
4.1 Disambiguating Nouns
Nouns are disambiguated when they occur as subjects or direct objects and when
modified by adjectives. We obtain a probability estimate for each nc to which the target
noun belongs, using the distribution of the TCM associated with the co-occurring verb
or adjective and the grammatical relationship.
Li and Abe used TCMs for the task of structural disambiguation. To obtain proba-
bility estimates for noun senses occurring at classes beneath hypernyms on the cut, Li
and Abe used the probability estimate at the nc? on the cut divided by the number of
ns descendants, as we do when finding ? during training, so the probability estimate
is shared equally among all nouns in the nc?, as in equation (15).
p(ns ? nsnc?) =
p(nc?)
|nsnc? |
(15)
One problem with doing this is that in cases in which the TCM is quite high in the
hierarchy, for example, at the entity class, the probability of any ns?s occurring under
this nc? on the TCM will be the same and does not allow us to discriminate among
senses beneath this level.
For the WSD task, we compare the probability estimates at each nc ? Cn, so if
a noun belongs to several synsets, we compare the probability estimates, given the
context, of these synsets. We obtain estimates for each nc by using the probability of
the hypernym nc? on ?. Rather than assume that all synsets under a given nc? on ?
646
Computational Linguistics Volume 29, Number 4
have the same likelihood of occurrence, we multiply the probability estimate for the
hypernym nc? by the ratio of the prior frequency of the nc, that is, p(nc|gr), for which
we seek the estimate divided by the prior frequency of the hypernym nc? (p(nc?|gr)):
p(nc ? hyponyms of nc?|vc, gr) = p(nc?|vc, gr)? p(nc|gr)
p(nc?|gr) (16)
These prior estimates are taken from populating the noun hyponym hierarchy with the
prior frequency data for the gr irrespective of the co-occurring verbs. The probability
at the hypernym nc? will necessarily total the probability at all hyponyms, since the
frequency credit of hyponyms is propagated to hypernyms.
Thus, to disambiguate a noun occurring in a given relationship with a given verb,
the nc ? Cn that gives the largest estimate for p(nc|vc, gr) is taken, where the verb class
(vc) is that which maximizes this estimate from Cv. The TCM acquired for each vc of
the verb in the given gr provides an estimate for p(nc?|vc, gr), and the estimate for nc
is obtained as in equation (16).
For example, one target noun was letter, which occurred as the direct object of
sign in our parses of the SENSEVAL-2 data. The TCM that maximized the probability
estimate for p(nc|vc, direct object) is shown in Figure 5. The noun letter is disambiguated
by comparing the probability estimates on the TCM above the five senses of letter mul-
tiplied by the proportion of that probability mass attributed to that synset. Although
entity has a higher probability on the TCM, compared to matter, which is above the
correct sense of letter,6 the ratio of prior probabilities for the synset containing letter7
under entity is 0.001, whereas that for the synset under matter is 0.226. This gives a
probability of 0.009?0.226 = 0.002 for the noun class probability given the verb class
approval
root
entity
human
capitalist
interpretation
message
communication
abstraction
relation
human_act
symbol
0.16
0.012
0.0329
0.114
0.0060.009
draft
letter
matter
writing
explanation
letter
alphabetic_letter
letter
letter
letter
sign
Figure 5
TCM for the direct-object slot of the verb class including sign and ratify.
6 The gloss is ?a written message addressed to a person or organization; e.g. wrote an indignant letter to
the editor.?
7 The gloss is ?owner who lets another person use something (housing usually) for hire.?
647
McCarthy and Carroll Disambiguating Using Selectional Preferences
(with maximum probability) and grammatical context. This is the highest probability
for any of the synsets of letter, and so in this case the correct sense is selected.
4.2 Disambiguating Verbs and Adjectives
Verbs and adjectives are disambiguated using TCMs to give estimates for p(nc|vc, gr)
and p(nc|ac, gr), respectively. These are combined with prior estimates for p(nc|gr) and
p(vc|gr) (or p(ac|gr)) using Bayes? rule to give:
p(vc|nc, gr) = p(nc|vc, gr)? p(vc|gr)
p(nc|gr) (17)
and for adjective?noun relations:
p(ac|nc, adjnoun) = p(nc|ac, adjnoun)? p(ac|adjnoun)
p(nc|adjnoun) (18)
The prior distributions for p(nc|gr), p(vc|gr) and p(ac|adjnoun) are obtained dur-
ing the training phase. For the prior distribution over NC, the frequency credit of
each noun in the specified gr in the training data is divided by |Cn|. The frequency
credit attached to a hyponym is propagated to the superordinate hypernyms, and the
frequency of a hypernym (nc?) totals the frequency at its hyponyms:
freq(nc?|gr) =
?
nc?nc?
freq(nc|gr) (19)
The distribution over VC is obtained similarly using the troponym relation. For
the distribution over AC, the frequency credit for each adjective is divided by the
number of synsets to which the adjective belongs, and the credit for an ac is the sum
over all the synsets that are members by virtue of the similar-to WordNet link.
To disambiguate a verb occurring with a given noun, the vc from Cv that gives
the largest estimate for p(vc|nc, gr) is taken. The nc for the co-occurring noun is the
nc from Cn that maximizes this estimate. The estimate for p(nc|vc, gr) is taken as in
equation (16) but selecting the vc to maximize the estimate for p(vc|nc, gr) rather than
p(nc|vc, gr). An adjective is likewise disambiguated to the ac from all those to which the
adjective belongs, using the estimate for p(nc|ac, gr) and selecting the nc that maximizes
the p(ac|nc, gr) estimate.
4.3 Increasing Coverage: One Sense per Discourse
There is a significant limitation to the word tokens that can be disambiguated using
selectional preferences, in that they are restricted to those that occur in the specified
grammatical relations and in argument head position. Moreover, we have TCMs only
for adjective and verb classes in which there was at least one adjective or verb mem-
ber that met our criteria for training (having no more than a threshold of 10 senses in
WordNet and a frequency of 20 or more occurrences in the BNC data in the specified
grammatical relationship). We chose not to apply TCMs for disambiguation where we
did not have TCMs for one or more classes for the verb or adjective. To increase cov-
erage, we experimented with applying the one-sense-per-discourse (OSPD) heuristic
(Gale, Church, and Yarowsky 1992). With this heuristic, a sense tag for a given word
is propagated to other occurrences of the same word within the current document in
order to increase coverage. When applying the OSPD heuristic, we simply applied a
tag for a noun, verb, or adjective to all the other instances of the same word type with
the same part of speech in the discourse, provided that only one possible tag for that
word was supplied by the selectional preferences for that discourse.
648
Computational Linguistics Volume 29, Number 4
0
20
40
60
80
100
0 20 40 60 80 100
Re
ca
ll
Precision
sel-ospd-ana sel-ospd
sel
supervised
other unsupervised
first sense heuristic
sel
sel-ospd
sel-ospd-ana
Figure 6
SENSEVAL-2 English all-words task results.
5. Evaluation
We evaluated our system using the SENSEVAL-2 test corpus on the English all-
words task (Cotton et al, 2001). We entered a previous version of this system for
the SENSEVAL-2 exercise, in three variants, under the names ?sussex-sel? (selectional
preferences), ?sussex-sel-ospd? (with the OSPD heuristic), and ?sussex-sel-ospd-ana?
(with anaphora resolution). 8 For SENSEVAL-2 we used only the direct object and
subject slots, since we had not yet dealt with adjectives. In Figure 6 we show how our
system fared at the time of SENSEVAL-2 compared to other unsupervised systems.9
We have also plotted the results of the supervised systems and the precision and recall
achieved by using the most frequent sense (as listed in WordNet).10
In the work reported here, we attempted disambiguation for head nouns and verbs
in subject and direct object relationships, and for adjectives and nouns in adjective-
noun relationships. For each test instance, we applied subject preferences before direct
object preferences, and direct object preferences before adjective?noun preferences. We
also propagated sense tags to test instances not in these relationships by applying the
one-sense-per-discourse heuristic.
We did not use the SENSEVAL-2 coarse-grained classification, as this was not
available at the time when we were acquiring the selectional preferences. We therefore
8 The motivation for using anaphora resolution was increased coverage, but anaphora resolution turned
out not actually to improve performance.
9 We use unsupervised to refer to systems that do not use manually sense-tagged training data, such as
SemCor. Our systems, marked in the figure as sel, sel-ospd, and sel-ospd-ana are unsupervised.
10 We are indebted to Judita Preiss for the most-frequent-sense result. This was obtained using the
frequency data supplied with the WordNet 1.7 version prereleased for SENSEVAL-2.
649
McCarthy and Carroll Disambiguating Using Selectional Preferences
Table 1
Overall results.
With OSPD Without OSPD
Precision 51.1% Precision 52.3%
Recall 23.2% Recall 20.0%
Attempted 45.5% Attempted 38.3%
Table 2
Precision results by part of speech.
Precision (%) Baseline precision (%)
Nouns 58.5 51.7
Polysemous nouns 36.8 25.8
Verbs 40.9 29.7
Polysemous verbs 38.1 25.3
Adjectives 49.8 48.6
Polysemous adjectives 35.5 24.0
Nouns, verbs, and adjectives 51.1 44.9
Polysemous nouns, verbs, and adjectives 36.8 27.3
do not include in the following the coarse-grained results; they are just slightly better
than the fine-grained results, which seems to be typical of other systems.
Our latest overall results are shown in Table 1. In this table we show the results
both with and without the OSPD heuristic. The results for the English SENSEVAL-2
tasks were generally much lower than those for the original SENSEVAL competition.
At the time of the SENSEVAL-2 workshop, this was assumed to be due largely to the
use of WordNet as the inventory, as opposed to HECTOR (Atkins 1993), but Palmer,
Trang Dang, and Fellbaum (forthcoming) have subsequently shown that, at least for the
lexical sample tasks, this was due to a harder selection of words, with a higher average
level of polysemy. For three of the most polysemous verbs that overlapped between
the English lexical sample for SENSEVAL and SENSEVAL-2, the performance was
comparable. Table 2 shows our precision results including use of the OSPD heuristic,
broken down by part of speech. Although the precision for nouns is greater than that
for verbs, the difference is much less when we remove the trivial monosemous cases.
Nouns, verbs, and adjectives all outperform their random baseline for precision, and
the difference is more marked when monosemous instances are dropped.
Table 3 shows the precision results for polysemous words given the slot and the
disambiguation source. Overall, once at least one word token has been disambiguated
by the preferences, the OSPD heuristic seems to perform better than the selectional
preferences. We can see, however, that although this is certainly true for the nouns,
the difference for the adjectives (1.3%) is less marked, and the preferences outperform
OSPD for the verbs. It seems that verbs obey the OSPD principle much less than nouns.
Also, verbs are best disambiguated by their direct objects, whereas nouns appear to
be better disambiguated as subjects and when modified by adjectives.
6. Discussion
6.1 Selectional Preferences
The precision of our system compares well with that of other unsupervised systems on
the SENSEVAL-2 English all-words task, despite the fact that these other systems use a
650
Computational Linguistics Volume 29, Number 4
Table 3
Precision results for polysemous words by part of speech and slot or disambiguation source.
Subject (%) Dobj (%) Adjm (%) OSPD (%)
Polysemous nouns 33.7 26.8 31.0 49.0
Polysemous verbs 33.8 47.3 ? 29.8
Polysemous adjectives ? ? 35.1 36.4
Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8
number of different sources of information for disambiguation, rather than selectional
preferences in isolation. Light and Greiff (2002) summarize some earlier WSD results
for automatically acquired selectional preferences. These results were obtained for
three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a
training and test data set constructed by Resnik containing nouns occurring as direct
objects of 100 verbs that select strongly for their objects.
Both the test and training sets were extracted from the section of the Brown cor-
pus within the Penn Treebank and used the treebank parses. The test set comprised
the portion of this data within SemCor containing these 100 verbs, and the training
set comprised 800,000 words from the Penn Treebank parses of the Brown corpus not
within SemCor. All three systems obtained higher precision than the results we report
here, with Ciaramita and Johnson?s Bayesian belief networks achieving the best accu-
racy at 51.4%. These results are not comparable with ours, however, for three reasons.
First, our results for the direct-object slot are for all verbs in the English all-words task,
as opposed to just those selecting strongly for their direct objects. We would expect
that WSD results using selectional preferences would be better for the latter class of
verbs. Second, we do not use manually produced parses, but the output from our fully
automatic shallow parser. Third and finally, the baselines reported for Resnik?s test set
were higher than those for the all-words task. For Resnik?s test data, the random base-
line was 28.5%, whereas for the polysemous nouns in the direct-object relation on the
all-words task, it was 23.9%. The distribution of senses was also perhaps more skewed
for Resnik?s test set, since the first sense heuristic was 82.8% (Abney and Light 1999),
whereas it was 53.6% for the polysemous direct objects in the all-words task. Although
our results do show that the precision for the TCMs compares favorably with that of
other unsupervised systems on the English all-words task, it would be worthwhile to
compare other selectional preference models on the same data.
Although the accuracy of our system is encouraging given that it does not use
hand-tagged data, the results are below the level of state-of-the-art supervised systems.
Indeed, a system just assigning to each word its most frequent sense as listed in
WordNet (the ?first-sense heuristic?) would do better than our preference models
(and in fact better than the majority of the SENSEVAL-2 English all-words supervised
systems). The first-sense heuristic, however, assumes the existence of sense-tagged data
that are able to give a definitive first sense. We do not use any first-sense information.
Although a modest amount of sense-tagged data is available for English (Miller et al
1993, Ng and Lee 1996), for other languages with minimal sense-tagged resources, the
heuristic is not applicable. Moreover, for some words the predominant sense varies
depending on the domain and text type.
To quantify this, we carried out an analysis of the polysemous nouns, verbs,
and adjectives in SemCor occurring in more than one SemCor file and found that
a large proportion of words have a different first sense in different files and also
in different genres (see Table 4). For adjectives there seems to be a lot less ambi-
651
McCarthy and Carroll Disambiguating Using Selectional Preferences
Table 4
Percentages of words with a different predominant sense in SemCor, across files and genres.
File Genre
Nouns 70 66
Verbs 79 74
Adjectives 25 21
guity (this has also been noted by Krovetz [1998]; the data in SENSEVAL-2 bear
this out, with many adjectives occurring only in their first sense. For nouns and
verbs, for which the predominant sense is more likely to vary among texts, it would
be worthwhile to try to detect words for which using the predominant sense is
not a reliable strategy, for example, because the word shows ?bursty? topic-related
behavior.
We therefore examined our disambiguation results to see if there was any pattern
in the predicates or arguments that were easily disambiguated themselves or were
good disambiguators of the co-occurring word. No particular patterns were evident
in this respect, perhaps because of the small size of the test data. There were nouns
such as team (precision= 22 ) and cancer (
8
10 ) that did better than average, but whether
or not they did better than the first-sense heuristic depends of course on the sense
in which they are used. For example, all 10 occurrences of cancer are in the first
sense, so the first sense heuristic is impossible to beat in this case. For the test items
that are not in their first sense, we beat the first-sense heuristic, but on the other
hand, we failed to beat the random baseline. (The random baseline is 21.8% and
we obtained 21.4% for these items overall.) Our performance on these items is low
probably because they are lower-frequency senses for which there is less evidence
in the untagged training corpus (the BNC). We believe that selectional preferences
would perform best if they were acquired from similar training data to that for which
disambiguation is required. In the future, we plan to investigate our models for WSD
in specific domains, such as sport and finance. The senses and frequency distribution
of senses for a given domain will in general be quite different from those in a balanced
corpus.
There are individual words that are not used in the first sense on which our TCM
preferences do well, for example sound (precision = 22 ), but there are not enough data
to isolate predicates or arguments that are good disambiguators from those that are
not. We intend to investigate this issue further with the SENSEVAL-2 lexical sample
data, which contains more instances of a smaller number of words.
Performance of selectional preferences depends not just on the actual word being
disambiguated, but the cohesiveness of the tuple <pred, arg, gr>. We have therefore
investigated applying a threshold on the probability of the class (nc, vc, or ac) before
disambiguation. Figure 7 presents a graph of precision against threshold applied to the
probability estimate for the highest-scoring class. We show alongside this the random
baseline and the first-sense heuristic for these items. Selectional preferences appear to
do better on items for which the probability predicted by our model is higher, but the
first-sense heuristic does even better on these. The first sense heuristic, with respect to
SemCor, outperforms the selectional preferences when it is averaged over a given text.
That seems to be the case overall, but there will be some words and texts for which
the first sense from SemCor is not relevant, and use of a threshold on probability, and
perhaps a differential between probability of the top-ranked senses suggested by the
model, should increase precision.
652
Computational Linguistics Volume 29, Number 4
0
0.2
0.4
0.6
0.8
1
0 0.002 0.004 0.006 0.008 0.01
pr
ec
isi
on
threshold
"first sense"
"TCMs"
"random"
Figure 7
Thresholding the probability estimate for the highest-scoring class.
Table 5
Lemma/file combinations in SemCor with more than one sense evident.
Nouns 23%
Verbs 19%
Adjectives 1.6%
6.2 The OSPD Heuristic
In these experiments we applied the OSPD heuristic to increase coverage. One problem
in doing this when using a fine-grained classification like WordNet is that although
the OSPD heuristic works well for homonyms, it is less accurate for related senses
(Krovetz 1998), and this distinction is not made in WordNet. We did, however, find
that in SemCor, for the majority of polysemous11 lemma and file combinations, there
was only one sense exhibited (see Table 5). We refrained from using the OSPD in
situations in which there was conflicting evidence regarding the appropriate sense for
a word type occurring more than once in an individual file. In our experiments the
OSPD heuristic increased coverage by 7% and recall by 3%, at a cost of only a 1%
decrease in precision.
7. Conclusion
We quantified coverage and accuracy of sense disambiguation of verbs, adjectives,
and nouns in the SENSEVAL-2 English all-words test corpus, using automatically
acquired selectional preferences. We improved coverage and recall by applying the
one-sense-per-discourse heuristic. The results show that disambiguation models using
only selectional preferences can perform with accuracy well above the random base-
line, although accuracy would not be high enough for applications in the absence of
11 Krovetz just looked at ?actual ambiguity,? that is, words with more than one sense in SemCor. We
define polysemy as those words having more than one sense in WordNet, since we are using
SENSEVAL-2 data, and not SemCor.
653
McCarthy and Carroll Disambiguating Using Selectional Preferences
other knowledge sources (Stevenson and Wilks 2001). The results compare well with
those for other systems that do not use sense-tagged training data.
Selectional preferences work well for some word combinations and grammatical
relationships, but not well for others. We hope in future work to identify the situations
in which selectional preferences have high precision and to focus on these at the
expense of coverage, on the assumption that other knowledge sources can be used
where there is not strong evidence from the preferences. The first-sense heuristic, based
on sense-tagged data such as that available in SemCor, seems to beat unsupervised
models such as ours. For many words, however, the predominant sense varies across
domains, and so we contend that it is worth concentrating on detecting when the
first sense is not relevant, and where the selectional-preference models provide a high
probability for a secondary sense. In these cases evidence for a sense can be taken from
multiple occurrences of the word in the document, using the one-sense-per-discourse
heuristic.
Acknowledgments
This work was supported by UK EPSRC
project GR/N36493 ?Robust Accurate
Statistical Parsing (RASP)? and EU FW5
project IST-2001-34460 ?MEANING.? We are
grateful to Rob Koeling and three
anonymous reviewers for their helpful
comments on earlier drafts. We would also
like to thank David Weir and Mark
Mclauchlan for useful discussions.
References
Abney, Steven and Marc Light. 1999. Hiding
a semantic class hierarchy in a Markov
model. In Proceedings of the ACL Workshop
on Unsupervised Learning in Natural
Language Processing, pages 1?8.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the Fifth
Workshop on Computational Language
Learning (CoNLL-2001), pages 15?22.
Atkins, Sue. 1993. Tools for computer-aided
lexicography: The Hector project. In
Papers in Computational Lexicography:
COMPLEX 93, Budapest.
Briscoe, Ted and John Carroll. 1993.
Generalised probabilistic LR parsing of
natural language (corpora) with
unification-based grammars.
Computational Linguistics, 19(1):25?59.
Briscoe, Ted and John Carroll. 1995.
Developing and evaluating a probabilistic
LR parser of part-of-speech and
punctuation labels. In fourth
ACL/SIGPARSE International Workshop on
Parsing Technologies, pages 48?58, Prague,
Czech Republic.
Carroll, John, Ted Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and a new proposal. In Proceedings
of the International Conference on Language
Resources and Evaluation, pages 447?454.
Carroll, John, Guido Minnen, and Ted
Briscoe. 1999. Corpus annotation for
parser evaluation. In EACL-99
Post-conference Workshop on Linguistically
Interpreted Corpora, pages 35?41, Bergen,
Norway.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and
John Tait. 1999. Simplifying English text
for language impaired readers. In
Proceedings of the Ninth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 269?270,
Bergen, Norway.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference with
Bayesian networks. In Proceedings of the
18th International Conference of
Computational Linguistics (COLING-00),
pages 187?193.
Cotton, Scott, Phil Edmonds, Adam
Kilgarriff, and Martha Palmer. 2001.
SENSEVAL-2. Available at http://www.
sle.sharp.co.uk/senseval2/.
Elworthy, David. 1994. Does Baum-Welch
re-estimation help taggers? In Proceedings
of the fourth ACL Conference on Applied
Natural Language Processing, pages 53?58,
Stuttgart, Germany.
Federici, Stefano, Simonetta Montemagni,
and Vito Pirrelli. 1999. SENSE: An
analogy-based word sense
disambiguation system. Natural Language
Engineering, 5(2):207?218.
Fellbaum, Christiane, editor. 1998. WordNet,
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. A method for
disambiguating word senses in a large
corpus. Computers and the Humanities,
654
Computational Linguistics Volume 29, Number 4
26:415?439.
Krovetz, Robert. 1998. More than one sense
per discourse. In Proceedings of the
ACL-SIGLEX SENSEVAL Workshop.
Available at http://www.itri.bton.ac.
uk/events/senseval/
ARCHIVE/PROCEEDINGS/.
Li, Hang and Naoki Abe. 1995. Generalizing
case frames using a thesaurus and the
MDL principle. In Proceedings of the
International Conference on Recent Advances
in Natural Language Processing, pages
239?248, Bulgaria.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Light, Marc and Warren Greiff. 2002.
Statistical models for the induction and
use of selectional preferences. Cognitive
Science, 26(3):269?281.
McCarthy, Diana. 1997. Word sense
disambiguation for acquisition of
selectional preferences. In Proceedings of
the ACL/EACL 97 Workshop Automatic
Information Extraction and Building of Lexical
Semantic Resources for NLP Applications,
pages 52?61.
McCarthy, Diana. 2001. Lexical Acquisition at
the Syntax-Semantics Interface: Diathesis
Alternations, Subcategorization Frames and
Selectional Preferences. Ph.D. thesis,
University of Sussex.
Miller, George, A., Claudia Leacock, Randee
Tengi, and Ross T. Bunker. 1993. A
semantic concordance. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 303?308. Morgan
Kaufmann.
Minnen, Guido, John Carroll, and Darren
Pearce. 2001. Applied morphological
processing of English. Natural Language
Engineering, 7(3):207?223.
Ng, Hwee Tou and Hian Beng Lee. 1996.
Integrating multiple knowledge sources
to disambiguate word sense: An
exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 40?47.
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2003. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Forthcoming, Natural
Language Engineering.
Preiss, Judita and Anna Korhonen. 2002.
Improving subcategorization acquisition
with WSD. In Proceedings of the ACL
Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions,
Philadelphia, PA.
Resnik, Philip. 1997. Selectional preference
and sense disambiguation. In Proceedings
of the SIGLEX Workshop on Tagging Text with
Lexical Semantics: Why, What, and How?
pages 52?57, Washington, DC.
Ribas, Francesc. 1995. On learning more
appropriate selectional restrictions. In
Proceedings of the Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 112?118.
Rissanen, Jorma. 1978. Modelling by
shortest data description. Automatica,
14:465?471.
Stevenson, Mark and Yorick Wilks. 2001.
The interaction of knowledge sources in
word sense disambiguation. Computational
Linguistics, 17(3):321?349.
Proceedings of NAACL HLT 2009: Short Papers, pages 233?236,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Estimating and Exploiting the Entropy of Sense Distributions
Peng Jin
Institute of Computational Linguistics
Peking University
Beijing China
jandp@pku.edu.cn
Diana McCarthy, Rob Koeling and John Carroll
University of Sussex
Falmer, East Sussex
BN1 9QJ, UK
{dianam,robk,johnca}@sussex.ac.uk
Abstract
Word sense distributions are usually skewed.
Predicting the extent of the skew can help a
word sense disambiguation (WSD) system de-
termine whether to consider evidence from the
local context or apply the simple yet effec-
tive heuristic of using the first (most frequent)
sense. In this paper, we propose a method to
estimate the entropy of a sense distribution to
boost the precision of a first sense heuristic by
restricting its application to words with lower
entropy. We show on two standard datasets
that automatic prediction of entropy can in-
crease the performance of an automatic first
sense heuristic.
1 Introduction
Word sense distributions are typically skewed and
WSD systems do best when they exploit this ten-
dency. This is usually done by estimating the most
frequent sense (MFS) for each word from a training
corpus and using that sense as a back-off strategy for
a word when there is no convincing evidence from
the context. This is known as the MFS heuristic 1
and is very powerful since sense distributions are
usually skewed. The heuristic becomes particularly
hard to beat for words with highly skewed sense dis-
tributions (Yarowsky and Florian, 2002). Although
the MFS can be estimated from tagged corpora, there
are always cases where there is insufficient data, or
where the data is inappropriate, for example because
1It is also referred to as the first sense heuristic in the WSD
literature and in this paper.
it comes from a very different domain. This has mo-
tivated some recent work attempting to estimate the
distributions automatically (McCarthy et al, 2004;
Lapata and Keller, 2007). This paper examines the
case for determining the skew of a word sense distri-
bution by estimating entropy and then using this to
increase the precision of an unsupervised first sense
heuristic by restricting application to those words
where the system can automatically detect that it has
the most chance. We use a method based on that
proposed by McCarthy et al (2004) as this approach
does not require hand-labelled corpora. The method
could easily be adapted to other methods for predic-
ing predominant sense.
2 Method
Given a listing of senses from an inventory, the
method proposed by McCarthy et al (2004) pro-
vides a prevalence ranking score to produce a MFS
heuristic. We make a slight modification to Mc-
Carthy et al?s prevalence score and use it to es-
timate the probability distribution over the senses
of a word. We use the same resources as Mc-
Carthy et al (2004): a distributional similarity the-
saurus and a WordNet semantic similarity measure.
The thesaurus was produced using the metric de-
scribed by Lin (1998) with input from the gram-
matical relation data extracted using the 90 mil-
lion words of written English from the British Na-
tional Corpus (BNC) (Leech, 1992) using the RASP
parser (Briscoe and Carroll, 2002). The thesaurus
consists of entries for each word (w) with the top
50 ?nearest neighbours? to w, where the neighbours
are words ranked by the distributional similarity that
233
they share with w. The WordNet similarity score
is obtained with the jcn measure (Jiang and Con-
rath, 1997) using the WordNet Similarity Package
0.05 (Patwardhan and Pedersen, 2003) and WordNet
version 1.6. The jcn measure needs word frequency
information, which we obtained from the BNC.
2.1 Estimates of Predominance, Probability
and Entropy
Following McCarthy et al (2004), we calculate
prevalence of each sense of the word (w) using a
weighted sum of the distributional similarity scores
of the top 50 neighbours of w. The sense of w that
has the highest value is the automatically detected
MFS (predominant sense). The weights are deter-
mined by the WordNet similarity between the sense
in question and the neighbour. We make a modi-
fication to the original method by multiplying the
weight by the inverse rank of the neighbour from
the list of 50 neighbours. This modification magni-
fies the contribution to each sense depending on the
rank of the neighbour while still allowing a neigh-
bour to contribute to all senses that it relates too.
We verified the effect of this change compared to the
original ranking score by measuring cross-entropy. 2
Let Nw = n1,n2 . . .nk denote the ordered set of the
top k = 50 neighbours of w according to the distri-
butional similarity thesaurus, senses(w) is the set of
senses of w and dss(w,n j) is the distributional sim-
ilarity score of a word w and its jth neighbour. Let
wsi be a sense of w then wnss(wsi,n j) is the maxi-
mum WordNet similarity score between wsi and the
WordNet sense of the neighbour (n j) that maximises
this score. The prevalence score is calculated as fol-
lows with 1rankn j being our modification to McCarthyet al
Prevalence Score(wsi) = ?n j?Nw dss(w,n j)?
wnss(wsi,n j)
?wsi??senses(w)wnss(wsi? ,n j)
? 1rankn j
(1)
To turn this score into a probability estimate we sum
the scores over all senses of a word and the proba-
bility for a sense is the original score divided by this
sum:
2Our modified version of the score gave a lower cross-
entropy with SemCor compared to that in McCarthy et al The
result was highly significant with p < 0.01 on the t-test.
p?(wsi) = prevalence score(wsi)?ws j?w prevalence score(ws j)
(2)
To smooth the data, we evenly distribute 1/10 of the
smallest prevalence score to all senses with a unde-
fined prevalence score values. Entropy is measured
as:
H(senses(w)) =? ?
wsi?senses(w)
p(wsi)log(p(wsi))
using our estimate (p?) for the probability distribu-
tion p over the senses of w.
3 Experiments
We conducted two experiments to evaluate the ben-
efit of using our estimate of entropy to restrict appli-
cation of the MFS heuristic. The two experiments
are conducted on the polysemous nouns in SemCor
and the nouns in the SENSEVAL-2 English all words
task (we will refer to this as SE2-EAW).
3.1 SemCor
For this experiment we used all the polysemous
nouns in Semcor 1.6 (excluding multiwords and
proper nouns). We depart slightly from (McCarthy
et al, 2004) in including all polysemous nouns
whereas they limited the experiment to those with
a frequency in SemCor of 3 or more and where there
is one sense with a higher frequency than the others.
Table 1 shows the precision of finding the predomi-
nant sense using equation 1 with respect to different
entropy thresholds. At each threshold, the MFS in
Semcor provides the upper-bound (UB). The random
baseline (RBL) is computed by selecting one of the
senses of the target word randomly as the predomi-
nant sense. As we hypothesized, precision is higher
when the entropy of the sense distribution is lower,
which is an encouraging result given that the entropy
is automatically estimated. The performance of the
random baseline is higher at lower entropy which
shows that the task is easier and involves a lower de-
gree of polysemy of the target words. However, the
gains over the random baseline are greater at lower
entropy levels indicating that the merits of detect-
ing the skew of the distribution cannot all be due to
lower polysemy levels.
234
H precision #
(?) eq 1 RBL UB tokens
0.5 - - - 0
0.9 80.3 50.0 84.8 466
0.95 85.1 50.0 90.9 1360
1 68.5 50.0 87.4 9874
1.5 67.6 42.6 86.9 11287
2 58.0 36.7 79.5 25997
2.5 55.7 34.4 77.6 31599
3.0 50.2 30.6 73.4 41401
4.0 47.6 28.5 70.8 46987
5.0 (all) 47.3 27.3 70.5 47539
Table 1: First sense heuristic on SemCor
Freq ? P #tokens
1 45.9 1132
5 50.1 5765
10 50.7 10736
100 49.4 39543
1000(all) 47.3 47539
#senses ? P #tokens
2 67.2 10736
5 55.4 31181
8 50.1 41393
12 47.8 46041
30(all) 47.3 47539
Table 2: Precision (P) of equation 1 on SemCor with re-
spect to frequency and polysemy
We also conducted a frequency and polysemy
analysis shown in Table 2 to demonstrate that the
increase in precision is not all due to frequency or
polysemy. This is important, since both frequency
and polysemy level (assuming a predefined sense in-
ventory) could be obtained without the need for au-
tomatic estimation. As we can see, while precision
is higher for lower polysemy, the automatic estimate
of entropy can provide a greater increase in preci-
sion than polysemy, and frequency does not seem to
be strongly correlated with precision.
3.2 SENSEVAL-2 English All Words Dataset
The SE2-EAW task provides a hand-tagged test suite
of 5,000 words of running text from three articles
from the Penn Treebank II (Palmer et al, 2001).
Again, we examine whether precision of the MFS
H precision #
(?) eq 1 RBL SC UB tokens
0.5 - - - - 0
0.9 1 50.0 1 1 7
0.95 94.7 50.0 94.7 1 19
1 69.6 50.0 81.3 94.6 112
1.5 68.0 49.0 81.3 93.8 128
2 69.6 34.7 68.2 87.7 421
2.5 65.0 33.0 65.0 86.5 488
3.0 56.6 27.5 60.8 80.1 687
4.0 52.6 25.6 58.8 79.2 766
5.0 (all) 51.5 25.6 58.5 79.3 769
Table 3: First sense heuristic on SE2-EAW
heuristic can be increased by restricting application
depending on entropy. We use the same resources as
for the SemCor experiment. 3 Table 3 gives the re-
sults. The most frequent sense (MFS) from SE2-EAW
itself provides the upper-bound (UB). We also com-
pare performance with the Semcor MFS (SC). Per-
formance is close to the Semcor MFS while not re-
lying on any manual tagging. As before, precision
increases significantly for words with low estimated
entropy, and the gains over the random baseline are
higher compared to the gains including all words.
4 Related Work
There is promising related work on determining the
predominant sense for a MFS heuristic (Lapata and
Keller, 2007; Mohammad and Hirst, 2006) but our
work is the first to use the ranking score to estimate
entropy and apply it to determine the confidence in
the MFS heuristic. It is likely that these methods
would also have increased precision if the ranking
scores were used to estimate entropy. We leave such
investigations for further work.
Chan and Ng (2005) estimate word sense distri-
butions and demonstrate that sense distribution esti-
mation improves a supervised WSD classifier. They
use three sense distribution methods, including that
of McCarthy et al (2004). While the other two
methods outperform the McCarthy et al method,
3We also used a tool for mapping from WordNet 1.7 to
WordNet 1.6 (Daude? et al, 2000) to map the SE2-EAW noun
data (originally distributed with 1.7 sense numbers) to 1.6 sense
numbers.
235
they rely on parallel training data and are not appli-
cable on 9.6% of the test data for which there are
no training examples. Our method does not require
parallel training data.
Agirre and Mart??nez (2004) show that sense dis-
tribution estimation is very important for both super-
vised and unsupervised WSD. They acquire tagged
examples on a large scale by querying Google with
monosemous synonyms of the word senses in ques-
tion. They show that the method of McCarthy et
al. (2004) can be used to produce a better sampling
technique than relying on the bias from web data
or randomly selecting the same number of exam-
ples for each sense. Our work similarly shows that
the automatic MFS is an unsupervised alternative to
SemCor but our work does not focus on sampling
but on an estimation of confidence in an automatic
MFS heuristic.
5 Conclusions
We demonstrate that our variation of the McCarthy
et al (2004) method for finding a MFS heuristic can
be used for estimating the entropy of a sense dis-
tribution which can be exploited to boost precision.
Words which are estimated as having lower entropy
in general get higher precision. This suggests that
automatic estimation of entropy is a good criterion
for getting higher precision. This is in agreement
with Kilgarriff and Rosenzweig (2000) who demon-
strate that entropy is a good measure of the difficulty
of WSD tasks, though their measure of entropy was
taken from the gold-standard distribution itself.
As future work, we want to compare this approach
of estimating entropy with other methods for es-
timating sense distributions which do not require
hand-labelled data or parallel texts. Currently, we
disregard local context. We wish to couple the con-
fidence in the MFS with contextual evidence and in-
vestigate application on coarse-grained datasets.
Acknowledgements
This work was funded by the China Scholarship Council,
the National Grant Fundamental Research 973 Program
of China: Grant No. 2004CB318102, the UK EPSRC
project EP/C537262 ?Ranking Word Senses for Disam-
biguation?, and a UK Royal Society Dorothy Hodgkin
Fellowship to the second author.
References
E. Agirre and D. Mart??nez. 2004. Unsupervised wsd
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP-2004,
pages 25?32, Barcelona, Spain.
E. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings of
LREC-2002, pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Y.S. Chan and H.T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proceedings of
IJCAI 2005, pages 1010?1015, Edinburgh, Scotland.
J. Daude?, L. Padro?, and G. Rigau. 2000. Mapping word-
nets using structural information. In Proceedings of
the 38th Annual Meeting of the Association for Com-
putational Linguistics, Hong Kong.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Interna-
tional Conference on Research in Computational Lin-
guistics, Taiwan.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for english SENSEVAL. Computers and the
Humanities. Senseval Special Issue, 34(1?2):15?48.
M. Lapata and F. Keller. 2007. An information retrieval
approach to sense ranking. In Proceedings of NAACL-
2007, pages 348?355, Rochester.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING-ACL 98, Mon-
treal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of ACL-2004, pages 280?287, Barcelona,
Spain.
S. Mohammad and G. Hirst. 2006. Determining word
sense dominance using a thesauru s. In Proceedings of
EACL-2006, pages 121?128, Trento, Italy.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H. Trang Dang. 2001. English tasks: All-words and
verb lexical sample. In Proceedings of the SENSEVAL-
2 workshop, pages 21?24.
S. Patwardhan and T. Pedersen. 2003. The
wordnet::similarity package. http://wn-
similarity.sourceforge.net/.
D. Yarowsky and R. Florian. 2002. Evaluating sense
disambiguation performance across diverse parame-
ter spaces. Natural Language Engineering, 8(4):293?
310.
236
From RAGS to RICHES: exploiting the potential of a flexible generation
architecture  
Lynne Cahill  , John Carroll  , Roger Evans  , Daniel Paiva  ,
Richard Power

, Donia Scott  and Kees van Deemter 

ITRI, University of Brighton
Brighton, BN2 4GJ, UK
Firstname.Lastname@itri.bton.ac.uk
 School of Cognitive and Computing Sciences, University of Sussex
Brighton, BN1 9QH, UK
johnca@cogs.susx.ac.uk
Abstract
The RAGS proposals for generic speci-
fication of NLG systems includes a de-
tailed account of data representation,
but only an outline view of processing
aspects. In this paper we introduce a
modular processing architecture with a
concrete implementation which aims to
meet the RAGS goals of transparency
and reusability. We illustrate the model
with the RICHES system ? a generation
system built from simple linguistically-
motivated modules.
1 Introduction
As part of the RAGS (Reference Architecture for
Generation Systems) project, Mellish et al(2000)
introduces a framework for the representation of
data in NLG systems, the RAGS ?data model?.
This model offers a formally well-defined declar-
ative representation language, which supports the
complex and dynamic data requirements of gen-
eration systems, e.g. different levels of repre-
sentation (conceptual to syntax), mixed represen-
tations that cut across levels, partial and shared
structures and ?canned? representations. However

We would like to acknowledge the financial support of
the EPSRC (RAGS ? Reference Architecture for Generation
Systems: grant GR/L77102 to Donia Scott), as well as the
intellectual contribution of our partners at Edinburgh (Chris
Mellish and Mike Reape: grant GR/L77041 to Mellish) and
other colleagues at the ITRI, especially Nedjet Bouayad-
Agha. We would also like to acknowledge the contribution
of colleagues who worked on the RICHES system previ-
ously: Neil Tipper and Rodger Kibble. We are grateful to
our anonymous referees for their helpful comments.
RAGS, as described in that paper, says very little
about the functional structure of an NLG system,
or the issues arising from more complex process-
ing regimes (see for example Robin (1994), Inuie
et al, (1992) for further discussion).
NLG systems, especially end-to-end, applied
NLG systems, have many functionalities in com-
mon. Reiter (1994) proposed an analysis of such
systems in terms of a simple three stage pipeline.
More recently Cahill et al(1999) attempted to re-
peat the analysis, but found that while most sys-
tems did implement a pipeline, they did not im-
plement the same pipeline ? different functional-
ities occurred in different ways and different or-
ders in different systems. But this survey did
identify a number of core functionalities which
seem to occur during the execution of most sys-
tems. In order to accommodate this result, a ?pro-
cess model? was sketched which aimed to support
both pipelines and more complex control regimes
in a flexible but structured way (see (Cahill et al,
1999),(RAGS, 2000)). In this paper, we describe
our attempts to test these ideas in a simple NLG
application that is based on a concrete realisation
of such an architecture1 .
The RAGS data model aims to promote com-
parability and re-usability in the NLG research
community, as well as insight into the organisa-
tion and processing of linguistic data in NLG. The
present work has similar goals for the processing
aspects: to propose a general approach to organis-
ing whole NLG systems in a way which promotes
1More details about the RAGS project, the
RICHES implementation and the OASYS subsys-
tem can be found at the RAGS project web site:
http://www.itri.bton.ac.uk/projects/rags.
the same ideals. In addition, we aim to test the
claims that the RAGS data model approach sup-
ports the flexible processing of information in an
NLG setting.
2 The RAGS data model
The starting point for our work here is the RAGS
data model as presented in Mellish et al(2000).
This model distinguishes the following five levels
of data representation that underpin the genera-
tion process:
Rhetorical representations (RhetReps) define how propo-
sitions within a text are related. For example, the sen-
tence ?Blow your nose, so that it is clear? can be con-
sidered to consist of two propositions: BLOW YOUR
NOSE and YOUR NOSE IS CLEAR, connected by a re-
lation like MOTIVATION.
Document representations (DocReps) encode information
about the physical layout of a document, such as tex-
tual level (paragraph, orthographic sentence, etc.),
layout (indentation, bullet lists etc.) and their relative
positions.
Semantic representations (SemReps) specify information
about the meaning of individual propositions. For
each proposition, this includes the predicate and its
arguments, as well as links to underlying domain ob-
jects and scoping information.
Syntactic representations (SynReps) define ?abstract?
syntactic information such as lexical features (FORM,
ROOT etc.) and syntactic arguments and adjuncts
(SUBJECT, OBJECT etc.).
Quote representations These are used to represent literal
unanalysed content used by a generator, such as
canned text, pictures or tables.
The representations aim to cover the core com-
mon requirements of NLG systems, while avoid-
ing over-commitment on less clearly agreed is-
sues relating to conceptual representation on the
one hand and concrete syntax and document ren-
dering on the other. When one considers process-
ing aspects, however, the picture tends to be a lot
less tidy: typical modules in real NLG systems
often manipulate data at several levels at once,
building structures incrementally, and often work-
ing with ?mixed? structures, which include infor-
mation from more than one level. Furthermore
this characteristic remains even when one consid-
ers more purely functionally-motivated ?abstract?
NLG modules. For example, Referring Expres-
sion Generation, commonly viewed as a single
task, needs to have access to at least rhetorical and
document information as well as referencing and
adding to the syntactic information.
To accommodate this, the RAGS data model in-
cludes a more concrete representational proposal,
called the ?whiteboard? (Calder et al, 1999), in
which all the data levels can be represented in
a common framework consisting of networks of
typed ?objects? connected by typed ?arrows?. This
lingua franca allows NLG modules to manipulate
data flexibly and consistently. It also facilitates
modular design of NLG systems, and reusability
of modules and data sets. However, it does not in
itself say anything about how modules in such a
system might interact.
This paper describes a concrete realisation of
the RAGS object and arrows model, OASYS,
as applied to a simple but flexible NLG system
called RICHES. This is not the first such re-
alisation: Cahill et al, (2000) describes a par-
tial re-implementation of the ?Caption Generation
System? (Mittal et al, 1999) which includes an
objects and arrows ?whiteboard?. The OASYS
system includes more specific proposals for pro-
cessing and inter-module communication, and
RICHES demonstrates how this can be used to
support a modular architecture based on small
scale functionally-motivated units.
3 OASYS
OASYS (Objects and Arrows SYStem) is a soft-
ware library which provides:
  an implementation of the RAGS Object and
Arrows (O/A) data representation,
  support for representing the five-layer RAGS
data model in O/A terms,
  an event-driven active database server for
O/A representations.
Together these components provide a central core
for RAGS-style NLG applications, allowing sepa-
rate parts of NLG functionality to be specified in
independent modules, which communicate exclu-
sively via the OASYS server.
The O/A data representation is a simple
typed network representation language. An O/A
database consists of a collection of objects, each
of which has a unique identifier and a type, and
arrows, each of which has a unique identifier,
a type, and source and target objects. Such a
database can be viewed as a (possibly discon-
nected) directed network representation: the fig-
ures in section 5 give examples of such networks.
OASYS pre-defines object and arrow types re-
quired to support the RAGS data model. Two ar-
row types, el (element) and el(<integer>),
are used to build up basic network structures ?
el identifies its target as a member of the set rep-
resented by its source, el(3), identifies its tar-
get as the third element of the tuple represented
by its source. Arrow type realised by re-
lates structures at different levels of representa-
tion. for example, indicating that this SemRep
object is realised by this SynRep object. Arrow
type revised to provides for support for non-
destructive modification of a structure, mapping
from an object to another of the same type that
can be viewed as a revision of it. Arrow type
refers to allows an object at one level to indi-
rectly refer to an object at a different level. Object
types correspond to the types of the RAGS data
model, and are either atomic, tuples, sets or se-
quences. For example, document structures are
built out of DocRep (a 2-tuple), DocAttr (a set
of DocFeatAtoms ? feature-value pairs), DocRe-
pSeq (a sequence of DocReps or DocLeafs) and
DocLeafs.
The active database server supports multiple
independent O/A databases. Individual modules
of an application publish and retrieve objects and
arrows on databases, incrementally building the
?higher level?, data structures. Modules com-
municate by accessing a shared database. Flow
of control in the application is event-based: the
OASYS module has the central thread of execu-
tion, calls to OASYS generate ?events?, and mod-
ules are implemented as event handlers. A mod-
ule registers interest in particular kinds of events,
and when those events occur, the module?s hander
is called to deal with them, which typically will
involve inspecting the database and adding more
structure (which generates further events).
OASYS supports three kinds of events: pub-
lish events occur whenever an object or arrow is
published in a database, module lifecycle events
occur whenever a new module starts up or termi-
nates, and synthetic events ? arbitrary messages
passed between the modules, but not interpreted
by OASYS itself ? may be generated by mod-
ules at any time. An application starts up by ini-
tialising all its modules. This generates initialise
events, which at least one module must respond
to, generating further events which other modules
may respond to, and so on, until no new events
are generated, at which point OASYS generates
finalise events for all the modules and terminates
them.
This framework supports a wide range of archi-
tectural possibilities. Publish events can be used
to make a module wake up whenever data of a
particular sort becomes available for processing.
Lifecycle events provide, among other things, an
easy way to do pipelining: the second module in a
pipeline waits for the finalise event of the first and
then starts processing, the third waits similarly
for the second to finalise etc. Synthetic events
allow modules to tell each other more explicitly
that some data is ready for processing, in situa-
tion where simple publication of an object is not
enough.
RICHES includes examples of all three
regimes: the first three modules are pipelined us-
ing lifecycle events; LC and RE, FLO and REND
interact using synthetic events; while SF watches
the database specifically for publication events.
4 RICHES
The RICHES system is a simple generation sys-
tem that takes as input rhetorical plans and pro-
duces patient advice texts. The texts are intended
to resemble those found at the PharmWeb site
(http://www.pharmweb.net). These are
simple instructional texts telling patients how to
use certain types of medicines, such as nosedrops,
eye drops, suppositories etc.. An example text
from PharmWeb is shown in figure 1, alongside
the corresponding text produced by RICHES.
The main aim of RICHES is to demonstrate
the feasibility of a system based on both the RAGS
data model and the OASYS server model. The
modules collectively construct and access the data
representations in a shared blackboard space and
this allows the modules to be defined in terms of
their functional role, rather than say, the kind of
data they manipulate or their position in a pro-
cessing pipeline. Each of the modules in the sys-
 How to Use Nose Drops
1. Blow your nose gently, so that it is clear. 
  
2. Wash your hands. 
  
3. Unscrew the top of the bottle and draw some liquid into the dropper. 
  
4. Tilt your head back. 
  
5. Hold the dropper just above your nose and put the correct number of drops into your nostril. 
  
6. DO NOT let the dropper touch the inside of your nose. 
  
7. Keep your head tilted back for two to three minutes to help the drops run to the back of your nose. 
  
8. Replace the top on the bottle. 
 
KEEP ALL MEDICINES OUT OF THE REACH OF CHILDREN 
PharmWeb - Copyright?1994-2001. All rights reserved
  
  
Blow your nose so that it is clear. 
Wash your hands
Unscrew the top. Then draw the liquid into the dropper. 
Tilt your head back
Hold the dropper above your nose. Then put the drops into your nostril.
The dropper must not touch the inside.
Keep your head tilted back for two to three minutes so that the drops run to the back.
Replace the top on the bottle
Generated by RICHES version 1.0 (9/5/2001) on 9/5/2001 
?2001, ITRI, University of Brighton 
Figure 1: An example text from PharmWeb, together with the corresponding text generated by RICHES
tem is in itself very simple ? our primary interest
here is in the way they interact.
Figure 2 shows the structure of the system2.
The functionality of the individual modules is
briefly described below.
Rhetorical Oracle (RO) The input to the sys-
tem is a RhetRep of the document to be gen-
erated: a tree with internal nodes labelled with
(RST-style) rhetorical relations and RhetLeaves
referring to semantic proposition representations
(SemReps). RO simply accesses such a represen-
tation from a data file and initialises the OASYS
database.
Media Selection (MS) RICHES produces doc-
uments that may include pictures as well as text.
As soon as the RhetRep becomes available, this
module examines it and decides what can be il-
lustrated and what picture should illustrate it. Pic-
2The dashed lines indicate flow of information, solid ar-
rows indicate approximately flow of control between mod-
ules, double boxes indicate a completely reused module
(from another system), while a double box with a dashed
outer indicates a module partially reused. Ellipses indicate
information sources, as opposed to processing modules.
tures, annotated with their SemReps, are part of
the picture library, and Media Selection builds
small pieces of DocRep referencing the pictures.
Document Planner (DP) The Document Plan-
ner, based on the ICONOCLAST text planner
(Power, 2000) takes the input RhetRep and pro-
duces a document structure (DocRep). This
specifies aspects such as the text-level (e.g.,
paragraph, sentence) and the relative or-
dering of propositions in the DocRep. Its
leaves refer to SynReps corresponding to syntac-
tic phrases. This module is pipelined after MS,
to make sure that it takes account of any pictures
that have been included in the document.
Lexical Choice (LC) Lexical choice happens in
two stages. In the first stage, LC chooses the lex-
ical items for the predicate of each SynRep. This
fixes the basic syntactic structure of the proposi-
tion, and the valency mapping between semantic
and syntactic arguments. At this point the ba-
sic document structure is complete, and the LC
advises REND and SF that they can start pro-
cessing. LC then goes into a second phase, in-
TEXT
SENTENCE
RHETORICAL 
ORACLE
LEXICAL
FINALISER
RENDERER
LINGO
PICTURE
LIBRARY
SELECTION
MEDIUM FLO
LEXICON
CHOICE
OASYS
REFERRING
EXPRESSIONS
DOCUMENT
PLANNER
Figure 2: The structure of the RICHES system
terleaved with RE and FLO: for each sentence,
RE determines the referring expressions for each
noun phrase, LC then lexicalises them, and when
the sentence is complete FLO invokes LinGO to
realise them.
Referring Expressions (RE) The Referring
Expression module adapts the SynReps to add in-
formation about the form of a noun phrase. It de-
cides whether it should be a pronoun, a definite
noun phrase or an indefinite noun phrase.
Sentence Finaliser (SF) The Sentence Fi-
naliser carries out high level sentential organisa-
tion. LC and RE together build individual syntac-
tic phrases, but do not combine them into whole
sentences. SF uses rhetorical and document struc-
ture information to decide how to complete the
syntactic representations, for example, combin-
ing main and subordinate clauses. In addition, SF
decides whether a sentence should be imperative,
depending on who the reader of the document is
(an input parameter to the system).
Finalise Lexical Output (FLO) RICHES uses
an external sentence realiser component with its
own non-RAGS input specification. FLO provides
the interface to this realiser, extracting (mostly
syntactic) information from OASYS and convert-
ing it to the appropriate form for the realiser. Cur-
rently, FLO supports the LinGO realiser (Carroll
et al, 1999), but we are also looking at FLO mod-
ules for RealPro (Lavoie and Rambow, 1997) and
FUF/SURGE (Elhadad et al, 1997).
Renderer (REND) The Renderer is the module
that puts the concrete document together. Guided
by the document structure, it produces HTML for-
matting for the text and positions and references
the pictures. Individual sentences are produced
for it by LinGO, via the FLO interface. FLO actu-
ally processes sentences independently of REND,
so when REND makes a request, either the sen-
tence is there already, or the request is queued,
and serviced when it becomes available.
LinGO The LinGO realiser uses a wide-
coverage grammar of English in the LKB HPSG
framework, (Copestake and Flickinger, 2000).
The tactical generation component accepts in-
put in the Minimal Recursion Semantics formal-
ism and produces the target text using a chart-
driven algorithm with an optimised treatment of
modification (Carroll et al, 1999). No domain-
specific tuning of the grammar was required for
the RICHES system, only a few additions to the
lexicon were necessary.
5 An example: generation in RICHES
In this section we show how RICHES generates
the first sentence of the example text, Blow your
nose so that it is clear and the picture that accom-
panies the text.
The system starts with a rhetorical represen-
tation (RhetRep) provided by the RO (see Fig-
ure 3)3. The first active module to run is MS
3In the figures, labels indicate object types and the sub-
script numbers are identifiers provided by OASYS for each
which traverses the RhetRep looking at the se-
mantic propositions labelling the RhetRep leaves,
to see if any can be illustrated by pictures in the
picture library. Each picture in the library is en-
coded with a semantic representation. Matching
between propositions and pictures is based on the
algorithm presented in Van Deemter (1999) which
selects the most informative picture whose repre-
sentation contains nothing that is not contained in
the proposition. For each picture that will be in-
cluded, a leaf node of document representation is
created and a realised by arrow is added to it
from the semantic proposition object (see Figure
4).
  	


  



el(1) el(2)
  		
(motivation)
  	ffFinding Predominant Word Senses in Untagged Text
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The problem with using the
predominant, or first sense heuristic, aside from the
fact that it does not take surrounding context into
account, is that it assumes some quantity of hand-
tagged data. Whilst there are a few hand-tagged
corpora available for some languages, one would
expect the frequency distribution of the senses of
words, particularly topical words, to depend on the
genre and domain of the text under consideration.
We present work on the use of a thesaurus acquired
from raw textual corpora and the WordNet similar-
ity package to find predominant noun senses auto-
matically. The acquired predominant senses give a
precision of 64% on the nouns of the SENSEVAL-
2 English all-words task. This is a very promising
result given that our method does not require any
hand-tagged text, such as SemCor. Furthermore,
we demonstrate that our method discovers appropri-
ate predominant senses for words from two domain-
specific corpora.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account. This is shown by the results of
the English all-words task in SENSEVAL-2 (Cot-
ton et al, 1998) in figure 1 below, where the first
sense is that listed in WordNet for the PoS given
by the Penn TreeBank (Palmer et al, 2001). The
senses in WordNet are ordered according to the fre-
quency data in the manually tagged resource Sem-
Cor (Miller et al, 1993). Senses that have not oc-
curred in SemCor are ordered arbitrarily and af-
ter those senses of the word that have occurred.
The figure distinguishes systems which make use
of hand-tagged data (using HTD) such as SemCor,
from those that do not (without HTD). The high per-
formance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where ev-
idence from the context is not sufficient (Hoste et
al., 2001). Whilst a first sense heuristic based on a
sense-tagged corpus such as SemCor is clearly use-
ful, there is a strong case for obtaining a first, or pre-
dominant, sense from untagged corpus data so that
a WSD system can be tuned to the genre or domain
at hand.
SemCor comprises a relatively small sample of
250,000 words. There are words where the first
sense in WordNet is counter-intuitive, because of
the size of the corpus, and because where the fre-
quency data does not indicate a first sense, the or-
dering is arbitrary. For example the first sense of
tiger in WordNet is audacious person whereas one
might expect that carnivorous animal is a more
common usage. There are only a couple of instances
of tiger within SemCor. Another example is em-
bryo, which does not occur at all in SemCor and
the first sense is listed as rudimentary plant rather
than the anticipated fertilised egg meaning. We be-
lieve that an automatic means of finding a predomi-
nant sense would be useful for systems that use it as
a means of backing-off (Wilks and Stevenson, 1998;
Hoste et al, 2001) and for systems that use it in lex-
ical acquisition (McCarthy, 1997; Merlo and Ley-
bold, 2001; Korhonen, 2002) because of the limited
size of hand-tagged resources. More importantly,
when working within a specific domain one would
wish to tune the first sense heuristic to the domain at
hand. The first sense of star in SemCor is celestial
body, however, if one were disambiguating popular
news celebrity would be preferred.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
020
40
60
80
100
0 20 40 60 80 100
re
ca
ll

precision
First Sense
"using HTD" "without HTD" "First Sense"
Figure 1: The first sense heuristic compared with
the SENSEVAL-2 English all-words task results
are therefore investigating a method of automati-
cally ranking WordNet senses from raw text.
Many researchers are developing thesauruses
from automatically parsed data. In these each tar-
get word is entered with an ordered list of ?near-
est neighbours?. The neighbours are words ordered
in terms of the ?distributional similarity? that they
have with the target. Distributional similarity is
a measure indicating the degree that two words, a
word and its neighbour, occur in similar contexts.
From inspection, one can see that the ordered neigh-
bours of such a thesaurus relate to the different
senses of the target word. For example, the neigh-
bours of star in a dependency-based thesaurus pro-
vided by Lin 1 has the ordered list of neighbours:
superstar, player, teammate, actor early in the list,
but one can also see words that are related to another
sense of star e.g. galaxy, sun, world and planet fur-
ther down the list. We expect that the quantity and
similarity of the neighbours pertaining to different
senses will reflect the dominance of the sense to
which they pertain. This is because there will be
more relational data for the more prevalent senses
compared to the less frequent senses. In this pa-
per we describe and evaluate a method for ranking
senses of nouns to obtain the predominant sense of
a word using the neighbours from automatically ac-
quired thesauruses. The neighbours for a word in a
thesaurus are words themselves, rather than senses.
In order to associate the neighbours with senses we
make use of another notion of similarity, ?semantic
similarity?, which exists between senses, rather than
words. We experiment with several WordNet Sim-
ilarity measures (Patwardhan and Pedersen, 2003)
which aim to capture semantic relatedness within
1Available at
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
the WordNet hierarchy. We use WordNet as our
sense inventory for this work.
The paper is structured as follows. We discuss
our method in the following section. Sections 3 and
4 concern experiments using predominant senses
from the BNC evaluated against the data in SemCor
and the SENSEVAL-2 English all-words task respec-
tively. In section 5 we present results of the method
on two domain specific sections of the Reuters cor-
pus for a sample of words. We describe some re-
lated work in section 6 and conclude in section 7.
2 Method
In order to find the predominant sense of a target
word we use a thesaurus acquired from automati-
cally parsed text based on the method of Lin (1998).
This provides the  nearest neighbours to each tar-
get word, along with the distributional similarity
score between the target word and its neighbour. We
then use the WordNet similarity package (Patward-
han and Pedersen, 2003) to give us a semantic simi-
larity measure (hereafter referred to as the WordNet
similarity measure) to weight the contribution that
each neighbour makes to the various senses of the
target word.
To find the first sense of a word (  ) we
take each sense in turn and obtain a score re-
flecting the prevalence which is used for rank-
ing. Let   	


 be the ordered
set of the top scoring  neighbours of  from
the thesaurus with associated distributional similar-
ity scores 	ffProceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluating the Accuracy of an Unlexicalized
Statistical Parser on the PARC DepBank
Ted Briscoe
Computer Laboratory
University of Cambridge
John Carroll
School of Informatics
University of Sussex
Abstract
We evaluate the accuracy of an unlexi-
calized statistical parser, trained on 4K
treebanked sentences from balanced data
and tested on the PARC DepBank. We
demonstrate that a parser which is compet-
itive in accuracy (without sacrificing pro-
cessing speed) can be quickly tuned with-
out reliance on large in-domain manually-
constructed treebanks. This makes it more
practical to use statistical parsers in ap-
plications that need access to aspects of
predicate-argument structure. The com-
parison of systems using DepBank is not
straightforward, so we extend and validate
DepBank and highlight a number of repre-
sentation and scoring issues for relational
evaluation schemes.
1 Introduction
Considerable progress has been made in accu-
rate statistical parsing of realistic texts, yield-
ing rooted, hierarchical and/or relational repre-
sentations of full sentences. However, much
of this progress has been made with systems
based on large lexicalized probabilistic context-
free like (PCFG-like) models trained on the Wall
Street Journal (WSJ) subset of the Penn Tree-
Bank (PTB). Evaluation of these systems has been
mostly in terms of the PARSEVAL scheme using
tree similarity measures of (labelled) precision and
recall and crossing bracket rate applied to section
23 of the WSJ PTB. (See e.g. Collins (1999) for
detailed exposition of one such very fruitful line
of research.)
We evaluate the comparative accuracy of an un-
lexicalized statistical parser trained on a smaller
treebank and tested on a subset of section 23 of
the WSJ using a relational evaluation scheme. We
demonstrate that a parser which is competitive
in accuracy (without sacrificing processing speed)
can be quickly developed without reliance on large
in-domain manually-constructed treebanks. This
makes it more practical to use statistical parsers in
diverse applications needing access to aspects of
predicate-argument structure.
We define a lexicalized statistical parser as one
which utilizes probabilistic parameters concerning
lexical subcategorization and/or bilexical relations
over tree configurations. Current lexicalized sta-
tistical parsers developed, trained and tested on
PTB achieve a labelled F1-score ? the harmonic
mean of labelled precision and recall ? of around
90%. Klein and Manning (2003) argue that such
results represent about 4% absolute improvement
over a carefully constructed unlexicalized PCFG-
like model trained and tested in the same man-
ner.1 Gildea (2001) shows that WSJ-derived bilex-
ical parameters in Collins? (1999) Model 1 parser
contribute less than 1% to parse selection accu-
racy when test data is in the same domain, and
yield no improvement for test data selected from
the Brown Corpus. Bikel (2004) shows that, in
Collins? (1999)Model 2, bilexical parameters con-
tribute less than 0.5% to accuracy on in-domain
data while lexical subcategorization-like parame-
ters contribute just over 1%.
Several alternative relational evaluation
schemes have been developed (e.g. Carroll et al,
1998; Lin, 1998). However, until recently, no
WSJ data has been carefully annotated to support
relational evaluation. King et al (2003) describe
the PARC 700 Dependency Bank (hereinafter
DepBank), which consists of 700 WSJ sentences
randomly drawn from section 23. These sentences
have been annotated with syntactic features and
with bilexical head-dependent relations derived
from the F-structure representation of Lexical
Functional Grammar (LFG). DepBank facilitates
1Klein and Manning retained some functional tag infor-
mation from PTB, so it could be argued that their model re-
mains ?mildly? lexicalized since functional tags encode some
subcategorization information.
41
comparison of PCFG-like statistical parsers
developed from the PTB with other parsers whose
output is not designed to yield PTB-style trees,
using an evaluation which is closer to the protypi-
cal parsing task of recovering predicate-argument
structure.
Kaplan et al (2004) compare the accuracy and
speed of the PARC XLE Parser to Collins? Model
3 parser. They develop transformation rules for
both, designed to map native output to a subset of
the features and relations in DepBank. They com-
pare performance of a grammatically cut-down
and complete version of the XLE parser to the
publically available version of Collins? parser.
One fifth of DepBank is held out to optimize the
speed and accuracy of the three systems. They
conclude from the results of these experiments that
the cut-down XLE parser is two-thirds the speed
of Collins? Model 3 but 12% more accurate, while
the complete XLE system is 20% more accurate
but five times slower. F1-score percentages range
from the mid- to high-70s, suggesting that the re-
lational evaluation is harder than PARSEVAL.
Both Collins? Model 3 and the XLE Parser use
lexicalized models for parse selection trained on
the rest of the WSJ PTB. Therefore, although Ka-
plan et al demonstrate an improvement in accu-
racy at some cost to speed, there remain questions
concerning viability for applications, at some re-
move from the financial news domain, for which
substantial treebanks are not available. The parser
we deploy, like the XLE one, is based on a
manually-defined feature-based unification gram-
mar. However, the approach is somewhat differ-
ent, making maximal use of more generic struc-
tural rather than lexical information, both within
the grammar and the probabilistic parse selection
model. Here we compare the accuracy of our
parser with Kaplan et al?s results, by repeating
their experiment with our parser. This compari-
son is not straightforward, given both the system-
specific nature of some of the annotation in Dep-
Bank and the scoring reported. We, therefore, ex-
tend DepBank with a set of grammatical relations
derived from our own system output and highlight
how issues of representation and scoring can affect
results and their interpretation.
In ?2, we describe our development method-
ology and the resulting system in greater detail.
?3 describes the extended Depbank that we have
developed and motivates our additions. ?2.4 dis-
cusses how we trained and tuned our current sys-
tem and describes our limited use of information
derived from WSJ text. ?4 details the various ex-
periments undertaken with the extended DepBank
and gives detailed results. ?5 discusses these re-
sults and proposes further lines of research.
2 Unlexicalized Statistical Parsing
2.1 System Architecture
Both the XLE system and Collins? Model 3 pre-
process textual input before parsing. Similarly,
our baseline system consists of a pipeline of mod-
ules. First, text is tokenized using a deterministic
finite-state transducer. Second, tokens are part-of-
speech and punctuation (PoS) tagged using a 1st-
order Hidden Markov Model (HMM) utilizing a
lexicon of just over 50K words and an unknown
word handling module. Third, deterministic mor-
phological analysis is performed on each token-
tag pair with a finite-state transducer. Fourth, the
lattice of lemma-affix-tags is parsed using a gram-
mar over such tags. Finally, the n-best parses are
computed from the parse forest using a probabilis-
tic parse selection model conditioned on the struc-
tural parse context. The output of the parser can be
displayed as syntactic trees, and/or factored into a
sequence of bilexical grammatical relations (GRs)
between lexical heads and their dependents.
The full system can be extended in a variety of
ways ? for example, by pruning PoS tags but al-
lowing multiple tag possibilities per word as in-
put to the parser, by incorporating lexical subcate-
gorization into parse selection, by computing GR
weights based on the proportion and probability
of the n-best analyses yielding them, and so forth
? broadly trading accuracy and greater domain-
dependence against speed and reduced sensitivity
to domain-specific lexical behaviour (Briscoe and
Carroll, 2002; Carroll and Briscoe, 2002; Watson
et al, 2005; Watson, 2006). However, in this pa-
per we focus exclusively on the baseline unlexical-
ized system.
2.2 Grammar Development
The grammar is expressed in a feature-based, uni-
fication formalism. There are currently 676 phrase
structure rule schemata, 15 feature propagation
rules, 30 default feature value rules, 22 category
expansion rules and 41 feature types which to-
gether define 1124 compiled phrase structure rules
in which categories are represented as sets of fea-
42
tures, that is, attribute-value pairs, possibly with
variable values, possibly bound between mother
and one or more daughter categories. 142 of the
phrase structure schemata are manually identified
as peripheral rather than core rules of English
grammar. Categories are matched using fixed-
arity term unification at parse time.
The lexical categories of the grammar consist
of feature-based descriptions of the 149 PoS tags
and 13 punctuation tags (a subset of the CLAWS
tagset, see e.g. Sampson, 1995) which constitute
the preterminals of the grammar. The number
of distinct lexical categories associated with each
preterminal varies from 1 for some function words
through to around 35 as, for instance, tags for main
verbs are associated with a VSUBCAT attribute tak-
ing 33 possible values. The grammar is designed
to enumerate possible valencies for predicates by
including separate rules for each pattern of pos-
sible complementation in English. The distinc-
tion between arguments and adjuncts is expressed
by adjunction of adjuncts to maximal projections
(XP ? XP Adjunct) as opposed to government of
arguments (i.e. arguments are sisters within X1
projections; X1 ? X0 Arg1. . . ArgN).
Each phrase structure schema is associated with
one or more GR specifications which can be con-
ditioned on feature values instantiated at parse
time and which yield a rule-to-rule mapping from
local trees to GRs. The set of GRs associated with
a given derivation define a connected, directed
graph with individual nodes representing lemma-
affix-tags and arcs representing named grammati-
cal relations. The encoding of this mapping within
the grammar is similar to that of F-structure map-
ping in LFG. However, the connected graph is not
constructed and completeness and coherence con-
straints are not used to filter the phrase structure
derivation space.
The grammar finds at least one parse rooted in
the start category for 85% of the Susanne treebank,
a 140K word balanced subset of the Brown Cor-
pus, which we have used for development (Samp-
son, 1995). Much of the remaining data consists
of phrasal fragments marked as independent text
sentences, for example in dialogue. Grammati-
cal coverage includes the majority of construction
types of English, however the handling of some
unbounded dependency constructions, particularly
comparatives and equatives, is limited because of
the lack of fine-grained subcategorization infor-
mation in the PoS tags and by the need to balance
depth of analysis against the size of the deriva-
tion space. On the Susanne corpus, the geometric
mean of the number of analyses for a sentence of
length n is 1.31n. The microaveraged F1-score for
GR extraction on held-out data from Susanne is
76.5% (see section 4.2 for details of the evaluation
scheme).
The system has been used to analyse about 150
million words of English text drawn primarily
from the PTB, TREC, BNC, and Reuters RCV1
datasets in connection with a variety of projects.
The grammar and PoS tagger lexicon have been
incrementally improved by manually examining
cases of parse failure on these datasets. How-
ever, the effort invested amounts to a few days?
effort for each new dataset as opposed to the main
grammar development effort, centred on Susanne,
which has extended over some years and now
amounts to about 2 years? effort (see Briscoe, 2006
for further details).
2.3 Parser
To build the parsing module, the unification gram-
mar is automatically converted into an atomic-
categoried context free ?backbone?, and a non-
deterministic LALR(1) table is constructed from
this, which is used to drive the parser. The residue
of features not incorporated into the backbone
are unified on each rule application (reduce ac-
tion). In practice, the parser takes average time
roughly quadratic in the length of the input to cre-
ate a packed parse forest represented as a graph-
structured stack. The statistical disambiguation
phase is trained on Susanne treebank bracketings,
producing a probabilistic generalized LALR(1)
parser (e.g. Inui et al, 1997) which associates
probabilities with alternative actions in the LR ta-
ble.
The parser is passed as input the sequence of
most probable lemma-affix-tags found by the tag-
ger. During parsing, probabilities are assigned
to subanalyses based on the the LR table actions
that derived them. The n-best (i.e. most proba-
ble) parses are extracted by a dynamic program-
ming procedure over subanalyses (represented by
nodes in the parse forest). The search is effi-
cient since probabilities are associated with single
nodes in the parse forest and no weight function
over ancestor or sibling nodes is needed. Proba-
bilities capture structural context, since nodes in
43
the parse forest partially encode a configuration of
the graph-structured stack and lookahead symbol,
so that, unlike a standard PCFG, the model dis-
criminates between derivations which only differ
in the order of application of the same rules and
also conditions rule application on the PoS tag of
the lookahead token.
When there is no parse rooted in the start cat-
egory, the parser returns a connected sequence
of partial parses which covers the input based
on subanalysis probability and a preference for
longer and non-lexical subanalysis combinations
(e.g. Kiefer et al, 1999). In these cases, the GR
graph will not be fully connected.
2.4 Tuning and Training Method
The HMM tagger has been trained on 3M words
of balanced text drawn from the LOB, BNC and
Susanne corpora, which are available with hand-
corrected CLAWS tags. The parser has been
trained from 1.9K trees for sentences from Su-
sanne that were interactively parsed to manually
obtain the correct derivation, and also from 2.1K
further sentences with unlabelled bracketings de-
rived from the Susanne treebank. These brack-
etings guide the parser to one or possibly sev-
eral closely-matching derivations and these are
used to derive probabilities for the LR table us-
ing (weighted) Laplace estimation. Actions in the
table involving rules marked as peripheral are as-
signed a uniform low prior probability to ensure
that derivations involving such rules are consis-
tently lower ranked than those involving only core
rules.
To improve performance onWSJ text, we exam-
ined some parse failures from sections other than
section 23 to identify patterns of consistent fail-
ure. We then manually modified and extended the
grammar with a further 6 rules, mostly to handle
cases of indirect and direct quotation that are very
common in this dataset. This involved 3 days?
work. Once completed, the parser was retrained
on the original data. A subsequent limited inspec-
tion of top-ranked parses led us to disable 6 ex-
isting rules which applied too freely to the WSJ
text; these were designed to analyse auxiliary el-
lipsis which appears to be rare in this genre. We
also catalogued incorrect PoS tags fromWSJ parse
failures and manually modified the tagger lexicon
where appropriate. These modifications mostly
consisted of adjusting lexical probabilities of ex-
tant entries with highly-skewed distributions. We
also added some tags to extant entries for infre-
quent words. These modifications took a further
day. The tag transition probabilities were not rees-
timated. Thus, we have made no use of the PTB
itself and only limited use of WSJ text.
This method of grammar and lexicon devel-
opment incrementally improves the overall per-
formance of the system averaged across all the
datasets that it has been applied to. It is very
likely that retraining the PoS tagger on the WSJ
and retraining the parser using PTB would yield
a system which would perform more effectively
on DepBank. However, one of our goals is to
demonstrate that an unlexicalized parser trained
on a modest amount of annotated text from other
sources, coupled to a tagger also trained on
generic, balanced data, can perform competitively
with systems which have been (almost) entirely
developed and trained using PTB, whether or not
these systems deploy hand-crafted grammars or
ones derived automatically from treebanks.
3 Extending and Validating DepBank
DepBank was constructed by parsing the selected
section 23 WSJ sentences with the XLE system
and outputting syntactic features and bilexical re-
lations from the F-structure found by the parser.
These features and relations were subsequently
checked, corrected and extended interactively with
the aid of software tools (King et al, 2003).
The choice of relations and features is based
quite closely on LFG and, in fact, overlaps sub-
stantially with the GR output of our parser. Fig-
ure 1 illustrates some DepBank annotations used
in the experiment reported by Kaplan et al and
our hand-corrected GR output for the example
Ten of the nation?s governors meanwhile called
on the justices to reject efforts to limit abortions.
We have kept the GR representation simpler and
more readable by suppressing lemmatization, to-
ken numbering and PoS tags, but have left the
DepBank annotations unmodified.
The example illustrates some differences be-
tween the schemes. For instance, the subj and
ncsubj relations overlap as both annotations con-
tain such a relation between call(ed) and Ten), but
the GR annotation also includes this relation be-
tween limit and effort(s) and reject and justice(s),
while DepBank links these two verbs to a variable
pro. This reflects a difference of philosophy about
44
DepBank: obl(call?0, on?2)
stmt_type(call?0, declarative)
subj(call?0, ten?1)
tense(call?0, past)
number_type(ten?1, cardinal)
obl(ten?1, governor?35)
obj(on?2, justice?30)
obj(limit?7, abortion?15)
subj(limit?7, pro?21)
obj(reject?8, effort?10)
subj(reject?8, pro?27)
adegree(meanwhile?9, positive)
num(effort?10, pl)
xcomp(effort?10, limit?7)
GR: (ncsubj called Ten _)
(ncsubj reject justices _)
(ncsubj limit efforts _)
(iobj called on)
(xcomp to called reject)
(dobj reject efforts)
(xmod to efforts limit)
(dobj limit abortions)
(dobj on justices)
(det justices the)
(ta bal governors meanwhile)
(ncmod poss governors nation)
(iobj Ten of)
(dobj of governors)
(det nation the)
Figure 1: DepBank and GR annotations.
resolution of such ?understood? relations in differ-
ent constructions. Viewed as output appropriate to
specific applications, either approach is justifiable.
However, for evaluation, these DepBank relations
add little or no information not already specified
by the xcomp relations in which these verbs also
appear as dependents. On the other hand, Dep-
Bank includes an adjunct relation between mean-
while and call(ed), while the GR annotation treats
meanwhile as a text adjunct (ta) of governors, de-
limited by balanced commas, following Nunberg?s
(1990) text grammar but conveying less informa-
tion here.
There are also issues of incompatible tokeniza-
tion and lemmatization between the systems and
of differing syntactic annotation of similar infor-
mation, which lead to problems mapping between
our GR output and the current DepBank. Finally,
differences in the linguistic intuitions of the an-
notators and errors of commission or omission
on both sides can only be uncovered by manual
comparison of output (e.g. xmod vs. xcomp for
limit efforts above). Thus we reannotated the Dep-
Bank sentences with GRs using our current sys-
tem, and then corrected and extended this anno-
tation utilizing a software tool to highlight dif-
ferences between the extant annotations and our
own.2 This exercise, though time-consuming, un-
covered problems in both annotations, and yields
a doubly-annotated and potentially more valuable
resource in which annotation disagreements over
complex attachment decisions, for instance, can be
inspected.
The GR scheme includes one feature in Dep-
Bank (passive), several splits of relations in Dep-
Bank, such as adjunct, adds some of DepBank?s
featural information, such as subord form, as a
subtype slot of a relation (ccomp), merges Dep-
Bank?s oblique with iobj, and so forth. But it
does not explicitly include all the features of Dep-
Bank or even of the reduced set of semantically-
relevant features used in the experiments and eval-
uation reported in Kaplan et al. Most of these
features can be computed from the full GR repre-
sentation of bilexical relations between numbered
lemma-affix-tags output by the parser. For in-
stance, num features, such as the plurality of jus-
tices in the example, can be computed from the
full det GR (det justice+s NN2:4 the AT:3)
based on the CLAWS tag (NN2 indicating ?plu-
ral?) selected for output. The few features that can-
not be computed from GRs and CLAWS tags di-
rectly, such as stmt type, could be computed from
the derivation tree.
4 Experiments
4.1 Experimental Design
We selected the same 560 sentences as test data as
Kaplan et al, and all modifications that we made
to our system (see ?2.4) were made on the basis
of (very limited) information from other sections
of WSJ text.3 We have made no use of the further
140 held out sentences in DepBank. The results
we report below are derived by choosing the most
probable tag for each word returned by the PoS
tagger and by choosing the unweighted GR set re-
turned for the most probable parse with no lexical
information guiding parse ranking.
4.2 Results
Our parser produced rooted sentential analyses for
84% of the test items; actual coverage is higher
2The new version of DepBank along with evaluation
software is included in the current RASP distribution:
www.informatics.susx.ac.uk/research/nlp/rasp
3The PARC group kindly supplied us with the experimen-
tal data files they used to facilitate accurate reproduction of
this experiment.
45
Relation Precision Recall F1 P R F1 Relation
mod 75.4 71.2 73.3
ncmod 72.9 67.9 70.3
xmod 47.7 45.5 46.6
cmod 51.4 31.6 39.1
pmod 30.8 33.3 32.0
det 88.7 91.1 89.9
arg mod 71.9 67.9 69.9
arg 76.0 73.4 74.6
subj 80.1 66.6 72.7 73 73 73
ncsubj 80.5 66.8 73.0
xsubj 50.0 28.6 36.4
csubj 20.0 50.0 28.6
subj or dobj 82.1 74.9 78.4
comp 74.5 76.4 75.5
obj 78.4 77.9 78.1
dobj 83.4 81.4 82.4 75 75 75 obj
obj2 24.2 38.1 29.6 42 36 39 obj-theta
iobj 68.2 68.1 68.2 64 83 72 obl
clausal 63.5 71.6 67.3
xcomp 75.0 76.4 75.7 74 73 74
ccomp 51.2 65.6 57.5 78 64 70 comp
pcomp 69.6 66.7 68.1
aux 92.8 90.5 91.6
conj 71.7 71.0 71.4 68 62 65
ta 39.1 48.2 43.2
passive 93.6 70.6 80.5 80 83 82
adegree 89.2 72.4 79.9 81 72 76
coord form 92.3 85.7 88.9 92 93 93
num 92.2 89.8 91.0 86 87 86
number type 86.3 92.7 89.4 96 95 96
precoord form 100.0 16.7 28.6 100 50 67
pron form 92.1 91.9 92.0 88 89 89
prt form 71.1 58.7 64.3 72 65 68
subord form 60.7 48.1 53.6
macroaverage 69.0 63.4 66.1
microaverage 81.5 78.1 79.7 80 79 79
Table 1: Accuracy of our parser, and where
roughly comparable, the XLE as reported by King
et al
than this since some of the test sentences are el-
liptical or fragmentary, but in many cases are rec-
ognized as single complete constituents. Kaplan
et al report that the complete XLE system finds
rooted analyses for 79% of section 23 of the WSJ
but do not report coverage just for the test sen-
tences. The XLE parser uses several performance
optimizations which mean that processing of sub-
analyses in longer sentences can be curtailed or
preempted, so that it is not clear what proportion
of the remaining data is outside grammatical cov-
erage.
Table 1 shows accuracy results for each indi-
vidual relation and feature, starting with the GR
bilexical relations in the extended DepBank and
followed by most DepBank features reported by
Kaplan et al, and finally overall macro- and mi-
croaverages. The macroaverage is calculated by
taking the average of each measure for each indi-
vidual relation and feature; the microaverage mea-
sures are calculated from the counts for all rela-
tions and features.4 Indentation of GRs shows
degree of specificity of the relation. Thus, mod
scores are microaveraged over the counts for the
five fully specified modifier relations listed imme-
diately after it in Table 1. This allows comparison
of overall accuracy on modifiers with, for instance
overall accuracy on arguments. Figures in italics
to the right are discussed in the next section.
Kaplan et al?s microaveraged scores for
Collins? Model 3 and the cut-down and complete
versions of the XLE parser are given in Table 2,
along with the microaveraged scores for our parser
from Table 1. Our system?s accuracy results (eval-
uated on the reannotated DepBank) are better than
those for Collins and the cut-down XLE, and very
similar overall to the complete XLE (evaluated
on DepBank). Speed of processing is also very
competitive.5 These results demonstrate that a
statistical parser with roughly state-of-the-art ac-
curacy can be constructed without the need for
large in-domain treebanks. However, the perfor-
mance of the system, as measured by microrav-
eraged F1-score on GR extraction alone, has de-
clined by 2.7% over the held-out Susanne data,
so even the unlexicalized parser is by no means
domain-independent.
4.3 Evaluation Issues
The DepBank num feature on nouns is evalu-
ated by Kaplan et al on the grounds that it is
semantically-relevant for applications. There are
over 5K num features in DepBank so the overall
microaveraged scores for a system will be signifi-
cantly affected by accuracy on num. We expected
our system, which incorporates a tagger with good
empirical (97.1%) accuracy on the test data, to re-
cover this feature with 95% accuracy or better, as
it will correlate with tags NNx1 and NNx2 (where
?x? represents zero or more capitals in the CLAWS
4We did not compute the remaining DepBank features
stmt type, tense, prog or perf as these rely on information
that can only be extracted from the derivation tree rather than
the GR set.
5Processing time for our system was 61 seconds on one
2.2GHz Opteron CPU (comprising tokenization, tagging,
morphology, and parsing, including module startup over-
heads). Allowing for slightly different CPUs, this is 2.5?10
times faster than the Collins and XLE parsers, as reported by
Kaplan et al
46
System Eval corpus Precision Recall F1
Collins DepBank 78.3 71.2 74.6
Cut-down XLE DepBank 79.1 76.2 77.6
Complete XLE DepBank 79.4 79.8 79.6
Our system DepBank/GR 81.5 78.1 79.7
Table 2: Microaveraged overall scores from Kaplan et al and for our system.
tagset). However, DepBank treats the majority
of prenominal modifiers as adjectives rather than
nouns and, therefore, associates them with an ade-
gree rather than a num feature. The PoS tag se-
lected depends primarily on the relative lexical
probabilities of each tag for a given lexical item
recorded in the tagger lexicon. But, regardless
of this lexical decision, the correct GR is recov-
ered, and neither adegree(positive) or num(sg)
add anything semantically-relevant when the lex-
ical item is a nominal premodifier. A strategy
which only provided a num feature for nominal
heads would be both more semantically-relevant
and would also yield higher precision (95.2%).
However, recall (48.4%) then suffers against Dep-
Bank as noun premodifiers have a num feature.
Therefore, in the results presented in Table 1 we
have not counted cases where either DepBank or
our system assign a premodifier adegree(positive)
or num(sg).
There are similar issues with other DepBank
features and relations. For instance, the form of
a subordinator with clausal complements is anno-
tated as a relation between verb and subordina-
tor, while there is a separate comp relation be-
tween verb and complement head. The GR rep-
resentation adds the subordinator as a subtype of
ccomp recording essentially identical information
in a single relation. So evaluation scores based on
aggregated counts of correct decisions will be dou-
bled for a system which structures this informa-
tion as in DepBank. However, reproducing the ex-
act DepBank subord form relation from the GR
ccomp one is non-trivial because DepBank treats
modal auxiliaries as syntactic heads while the GR-
scheme treats the main verb as head in all ccomp
relations. We have not attempted to compensate
for any further such discrepancies other than the
one discussed in the previous paragraph. However,
we do believe that they collectively damage scores
for our system.
As King et al note, it is difficult to identify
such informational redundancies to avoid double-
counting and to eradicate all system specific bi-
ases. However, reporting precision, recall and F1-
scores for each relation and feature separately and
microaveraging these scores on the basis of a hi-
erarchy, as in our GR scheme, ameliorates many
of these problems and gives a better indication
of the strengths and weaknesses of a particular
parser, which may also be useful in a decision
about its usefulness for a specific application. Un-
fortunately, Kaplan et al do not report their re-
sults broken down by relation or feature so it is
not possible, for example, on the basis of the ar-
guments made above, to choose to compare the
performance of our system on ccomp to theirs for
comp, ignoring subord form. King et al do re-
port individual results for selected features and re-
lations from an evaluation of the complete XLE
parser on all 700 DepBank sentences with an al-
most identical overall microaveraged F1 score of
79.5%, suggesting that these results provide a rea-
sonably accurate idea of the XLE parser?s relative
performance on different features and relations.
Where we believe that the information captured
by a DepBank feature or relation is roughly com-
parable to that expressed by a GR in our extended
DepBank, we have included King et al?s scores
in the rightmost column in Table 1 for compari-
son purposes. Even if these features and relations
were drawn from the same experiment, however,
they would still not be exactly comparable. For in-
stance, as discussed in ?3 nearly half (just over 1K)
the DepBank subj relations include pro as one el-
ement, mostly double counting a corresponding
xcomp relation. On the other hand, our ta rela-
tion syntactically underspecifies many DepBank
adjunct relations. Nevertheless, it is possible to
see, for instance, that while both parsers perform
badly on second objects ours is worse, presumably
because of lack of lexical subcategorization infor-
mation.
47
5 Conclusions
We have demonstrated that an unlexicalized parser
with minimal manual modification for WSJ text ?
but no tuning of performance to optimize on this
dataset alne, and no use of PTB ? can achieve
accuracy competitive with parsers employing lex-
icalized statistical models trained on PTB.
We speculate that we achieve these results be-
cause our system is engineered to make minimal
use of lexical information both in the grammar and
in parse ranking, because the grammar has been
developed to constrain ambiguity despite this lack
of lexical information, and because we can com-
pute the full packed parse forest for all the test sen-
tences efficiently (without sacrificing speed of pro-
cessing with respect to other statistical parsers).
These advantages appear to effectively offset the
disadvantage of relying on a coarser, purely struc-
tural model for probabilistic parse selection. In fu-
ture work, we hope to improve the accuracy of the
system by adding lexical information to the statis-
tical parse selection component without exploiting
in-domain treebanks.
Clearly, more work is needed to enable more
accurate, informative, objective and wider com-
parison of extant parsers. More recent PTB-based
parsers show small improvements over Collins?
Model 3 using PARSEVAL, while Clark and Cur-
ran (2004) and Miyao and Tsujii (2005) report
84% and 86.7% F1-scores respectively for their
own relational evaluations on section 23 of WSJ.
However, it is impossible to meaningfully com-
pare these results to those reported here. The rean-
notated DepBank potentially supports evaluations
which score according to the degree of agreement
between this and the original annotation and/or de-
velopment of future consensual versions through
collaborative reannotation by the research com-
munity. We have also highlighted difficulties for
relational evaluation schemes and argued that pre-
senting individual scores for (classes of) relations
and features is both more informative and facili-
tates system comparisons.
6 References
Bikel, D.. 2004. Intricacies of Collins? parsing model, Com-
putational Linguistics, 30(4):479?512.
Briscoe, E.J.. 2006. An introduction to tag sequence gram-
mars and the RASP system parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC),
Las Palmas, Gran Canaria. 1499?1504.
Carroll, J. and E.J. Briscoe. 2002. High precision extraction
of grammatical relations. In Proceedings of the 19th Int.
Conf. on Computational Linguistics (COLING), Taipei,
Taiwan. 134?140.
Carroll, J., E. Briscoe and A. Sanfilippo. 1998. Parser evalu-
ation: a survey and a new proposal. In Proceedings of the
1st International Conference on Language Resources and
Evaluation, Granada, Spain. 447?454.
Clark, S. and J. Curran. 2004. The importance of supertag-
ging for wide-coverage CCG parsing. In Proceedings of
the 20th International Conference on Computational Lin-
guistics (COLING-04), Geneva, Switzerland. 282?288.
Collins, M.. 1999. Head-driven Statistical Models for Nat-
ural Language Parsing. PhD Dissertation, Computer and
Information Science, University of Pennsylvania.
Gildea, D.. 2001. Corpus variation and parser performance.
In Proceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP?01), Pittsburgh, PA.
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga.
1997. A new formalization of probabilistic GLR parsing.
In Proceedings of the 5th International Workshop on Pars-
ing Technologies (IWPT?97), Boston, MA. 123?134.
Kaplan, R., S. Riezler, T. H. King, J. Maxwell III, A. Vasser-
man and R. Crouch. 2004. Speed and accuracy in shal-
low and deep stochastic parsing. In Proceedings of the
HLT Conference and the 4th Annual Meeting of the North
American Chapter of the ACL (HLT-NAACL?04), Boston,
MA.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf. 1999.
A bag of useful techniques for efficient and robust pars-
ing. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, University of
Maryland. 473?480.
King, T. H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan. 2003. The PARC700 Dependency Bank. In Pro-
ceedings of the 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), Budapest, Hungary.
Klein, D. and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, Sapporo,
Japan. 423?430.
Lin, D.. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of the Workshop at LREC?98 on The Eval-
uation of Parsing Systems, Granada, Spain.
Manning, C. and H. Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press, Cambridge,
MA.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proceedings
of the 43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, MI. 83?90.
Nunberg, G.. 1990. The Linguistics of Punctuation. CSLI
Lecture Notes 18, Stanford, CA.
Sampson, G.. 1995. English for the Computer. Oxford Uni-
versity Press, Oxford, UK.
Watson, R.. 2006. Part-of-speech tagging models for parsing.
In Proceedings of the 9th Conference of Computational
Linguistics in the UK (CLUK?06), Open University, Mil-
ton Keynes.
Watson, R., J. Carroll and E.J. Briscoe. 2005. Efficient ex-
traction of grammatical relations. In Proceedings of the
9th Int. Workshop on Parsing Technologies (IWPT?05),
Vancouver, Ca..
48
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
           
The Second Release of the RASP System
Ted Briscoe? John Carroll? Rebecca Watson?
?Computer Laboratory, University of Cambridge, Cambridge CB3 OFD, UK
firstname.lastname@cl.cam.ac.uk
?Department of Informatics, University of Sussex, Brighton BN1 9QH, UK
J.A.Carroll@sussex.ac.uk
Abstract
We describe the new release of the RASP
(robust accurate statistical parsing) sys-
tem, designed for syntactic annotation of
free text. The new version includes a
revised and more semantically-motivated
output representation, an enhanced gram-
mar and part-of-speech tagger lexicon, and
a more flexible and semi-supervised train-
ing method for the structural parse ranking
model. We evaluate the released version
on the WSJ using a relational evaluation
scheme, and describe how the new release
allows users to enhance performance using
(in-domain) lexical information.
1 Introduction
The first public release of the RASP system
(Briscoe & Carroll, 2002) has been downloaded
by over 120 sites and used in diverse natural lan-
guage processing tasks, such as anaphora res-
olution, word sense disambiguation, identifying
rhetorical relations, resolving metonymy, detect-
ing compositionality in phrasal verbs, and diverse
applications, such as topic and sentiment classifi-
cation, text anonymisation, summarisation, infor-
mation extraction, and open domain question an-
swering. Briscoe & Carroll (2002) give further de-
tails about the first release. Briscoe (2006) pro-
vides references and more information about ex-
tant use of RASP and fully describes the modifi-
cations discussed more briefly here.
The new release, which is free for all non-
commercial use1, is designed to address several
weaknesses of the extant toolkit. Firstly, all mod-
ules have been incrementally improved to cover a
greater range of text types. Secondly, the part-of-
speech tagger lexicon has been semi-automatically
enhanced to better deal with rare or unseen be-
haviour of known words. Thirdly, better facil-
ities have been provided for user customisation.
1See http://www.informatics.susx.ac.uk/research/nlp/rasp/
for licence and download details.
?raw text
Tokeniser
?
PoS Tagger
?
Lemmatiser
?
Parser/Grammar
?
Parse Ranking Model
Figure 1: RASP Pipeline
Fourthly, the grammatical relations output has
been redesigned to better support further process-
ing. Finally, the training and tuning of the parse
ranking model has been made more flexible.
2 Components of the System
RASP is implemented as a series of modules writ-
ten in C and Common Lisp, which are pipelined,
working as a series of Unix-style filters. RASP
runs on Unix and is compatible with most C com-
pilers and Common Lisp implementations. The
public release includes Lisp and C executables for
common 32- and 64-bit architectures, shell scripts
for running and parameterising the system, docu-
mentation, and so forth. An overview of the sys-
tem is given in Figure 1.
2.1 Sentence Boundary Detection and
Tokenisation
The system is designed to take unannotated text or
transcribed (and punctuated) speech as input, and
not simply to run on pre-tokenised input such as
that typically found in corpora produced for NLP
purposes. Sentence boundary detection and to-
kenisation modules, implemented as a set of deter-
ministic finite-state rules in Flex (an open source
re-implementation of the original Unix Lex utility)
77
         
 PPPPP
QQQ
HHH
!!!!
HHH
QQQQ
!!!!!!!!      
(((((((

hhhhhhhhh
PPPPPSS

        
aaaa
XXXXXXX
hhhhhhhhhh
dependent
ta arg mod det aux conj
mod arg
ncmod xmod cmod pmod
subj or dobj
subj comp
ncsubj xsubj csubj obj pcomp clausal
dobj obj2 iobj xcomp ccomp
Figure 2: The GR hierarchy
and compiled into C, convert raw ASCII (or Uni-
code in UTF-8) data into a sequence of sentences
in which, for example punctuation tokens are sep-
arated from words by spaces, and so forth.
Since the first release this part of the system
has been incrementally improved to deal with a
greater variety of text types, and handle quo-
tation appropriately. Users are able to modify
the rules used and recompile the modules. All
RASP modules now accept XML mark up (with
certain hard-coded assumptions) so that data can
be pre-annotated?for example to identify named
entities?before being passed to the tokeniser, al-
lowing for more domain-dependent, potentially
multiword tokenisation and classification prior to
parsing if desired (e.g. Vlachos et al, 2006), as
well as, for example, handling of text with sen-
tence boundaries already determined.
2.2 PoS and Punctuation Tagging
The tokenised text is tagged with one of 150
part-of-speech (PoS) and punctuation labels (de-
rived from the CLAWS tagset). This is done
using a first-order (?bigram?) hidden markov
model (HMM) tagger implemented in C (Elwor-
thy, 1994) and trained on the manually-corrected
tagged versions of the Susanne, LOB and (sub-
set of) BNC corpora. The tagger has been aug-
mented with an unknown word model which per-
forms well under most circumstances. However,
known but rare words often caused problems as
tags for all realisations were rarely present. A se-
ries of manually developed rules has been semi-
automatically applied to the lexicon to amelio-
rate this problem by adding further tags with low
counts to rare words. The new tagger has an accu-
racy of just over 97% on the DepBank part of sec-
tion 23 of the Wall Street Journal, suggesting that
this modification has resulted in competitive per-
formance on out-of-domain newspaper text. The
tagger implements the Forward-Backward algo-
rithm as well as the Viterbi algorithm, so users can
opt for tag thresholding rather than forced-choice
tagging (giving >99% tag recall on DepBank, at
some cost to overall system speed). Recent exper-
iments suggest that this can lead to a small gain
in parse accuracy as well as coverage (Watson,
2006).
2.3 Morphological Analysis
The morphological analyser is also implemented
in Flex, with about 1400 finite-state rules in-
corporating a great deal of lexically exceptional
data. These rules are compiled into an efficient
C program encoding a deterministic finite state
transducer. The analyser takes a word form and
CLAWS tag and returns a lemma plus any inflec-
tional affixes. The type and token error rate of
the current system is less than 0.07% (Minnen,
Carroll and Pearce, 2001). The primary system-
internal value of morphological analysis is to en-
able later modules to use lexical information asso-
ciated with lemmas, and to facilitate further acqui-
sition of such information from lemmas in parses.
2.4 PoS and Punctuation Sequence Parsing
The manually-developed wide-coverage tag se-
quence grammar utilised in this version of the
parser consists of 689 unification-based phrase
structure rules (up from 400 in the first release).
The preterminals to this grammar are the PoS
and punctuation tags2. The terminals are featu-
ral descriptions of the preterminals, and the non-
terminals project information up the tree using
an X-bar scheme with 41 attributes with a maxi-
mum of 33 atomic values. Many of the original
2The relatively high level of detail in the tagset helps the
grammar writer to limit overgeneration and overacceptance.
78
     
rules have been replaced with multiple more spe-
cific variants to increase precision. In addition,
coverage has been extended in various ways, no-
tably to cover quotation and word order permuta-
tions associated with direct and indirect quotation,
as is common in newspaper text. All rules now
have a rule-to-rule declarative specification of the
grammatical relations they license (see ?2.6). Fi-
nally, around 20% of the rules have been manu-
ally identified as ?marked? in some way; this can
be exploited in customisation and in parse ranking.
Users can specify that certain rules should not be
used and so to some extent tune the parser to dif-
ferent genres without the need for retraining.
The current version of the grammar finds at least
one parse rooted in S for about 85% of the Susanne
corpus (used for grammar development), and most
of the remainder consists of phrasal fragments
marked as independent text sentences in passages
of dialogue. The coverage of our WSJ test data is
84%. In cases where there is no parse rooted in S,
the parser returns a connected sequence of partial
parses covering the input. The criteria are partial
parse probability and a preference for longer but
non-lexical combinations (Kiefer et al, 1999).
2.5 Generalised LR Parser
A non-deterministic LALR(1) table is constructed
automatically from a CF ?backbone? compiled
from the feature-based grammar. The parser
builds a packed parse forest using this table to
guide the actions it performs. Probabilities are as-
sociated with subanalyses in the forest via those
associated with specific actions in cells of the LR
table (Inui et al, 1997). The n-best (i.e. most
probable) parses can be efficiently extracted by
unpacking subanalyses, following pointers to con-
tained subanalyses and choosing alternatives in or-
der of probabilistic ranking. This process back-
tracks occasionally since unifications are required
during the unpacking process and they occasion-
ally fail (see Oepen and Carroll, 2000).
The probabilities of actions in the LR table
are computed using bootstrapping methods which
utilise an unlabelled bracketing of the Susanne
Treebank (Watson et al, 2006). This makes the
system more easily retrainable after changes in the
grammar and opens up the possibility of quicker
tuning to in-domain data. In addition, the struc-
tural ranking induced by the parser can be re-
ranked using (in-domain) lexical data which pro-
vides conditional probability distributions for the
SUBCATegorisation attributes of the major lexi-
cal categories. Some generic data is supplied for
common verbs, but this can be augmented by user
supplied, possibly domain specific files.
2.6 Grammatical Relations Output
The resulting set of ranked parses can be dis-
played, or passed on for further processing, in a
variety of formats which retain varying degrees of
information from the full derivations. We origi-
nally proposed transforming derivation trees into
a set of named grammatical relations (GRs), il-
lustrated as a subsumption hierarchy in Figure 2,
as a way of facilitating cross-system evaluation.
The revised GR scheme captures those aspects
of predicate-argument structure that the system is
able to recover and is the most stable and gram-
mar independent representation available. Revi-
sions include a treatment of coordination in which
the coordinator is the head in subsuming relations
to enable appropriate semantic inferences, and ad-
dition of a text adjunct (punctuation) relation to
the scheme.
Factoring rooted, directed graphs of GRs into a
set of bilexical dependencies makes it possible to
compute the transderivational support for a partic-
ular relation and thus compute a weighting which
takes account both of the probability of derivations
yielding a specific relation and of the proportion
of such derivations in the forest produced by the
parser. A weighted set of GRs from the parse for-
est is now computed efficiently using a variant of
the inside-outside algorithm (Watson et al, 2005).
3 Evaluation
The new system has been evaluated using our re-
annotation of the PARC dependency bank (Dep-
Bank; King et al, 2003)?consisting of 560 sen-
tences chosen randomly from section 23 of the
Wall Street Journal?with grammatical relations
compatible with our system. Briscoe and Carroll
(2006) discuss issues raised by this reannotation.
Relations take the following form: (relation
subtype head dependent initial) where relation
specifies the type of relationship between the head
and dependent. The remaining subtype and ini-
tial slots encode additional specifications of the re-
lation type for some relations and the initial or un-
derlying logical relation of the grammatical sub-
ject in constructions such as passive. We deter-
79
        
mine for each sentence the relations in the test set
which are correct at each level of the relational hi-
erarchy. A relation is correct if the head and de-
pendent slots are equal and if the other slots are
equal (if specified). If a relation is incorrect at
a given level in the hierarchy it may still match
for a subsuming relation (if the remaining slots all
match); for example, if a ncmod relation is mis-
labelled with xmod, it will be correct for all rela-
tions which subsume both ncmod and xmod, e.g.
mod. Similarly, the GR will be considered incor-
rect for xmod and all relations that subsume xmod
but not ncmod. Thus, the evaluation scheme cal-
culates unlabelled dependency accuracy at the de-
pendency (most general) level in the hierarchy.
The micro-averaged precision, recall and F1 score
are calculated from the counts for all relations in
the hierarchy. The macroaveraged scores are the
mean of the individual scores for each relation.
On the reannotated DepBank, the system
achieves a microaveraged F1 score of 76.3%
across all relations, using our new training method
(Watson et al, 2006). Briscoe and Carroll (2006)
show that the system has equivalent accuracy to
the PARC XLE parser when the morphosyntactic
features in the original DepBank gold standard are
taken into account. Figure 3 shows a breakdown
of the new system?s results by individual relation.
Acknowledgements
Development has been partially funded by
the EPSRC RASP project (GR/N36462 and
GR/N36493) and greatly facilitated by Anna Ko-
rhonen, Diana McCarthy, Judita Preiss and An-
dreas Vlachos. Much of the system rests on ear-
lier work on the ANLT or associated tools by Bran
Boguraev, David Elworthy, Claire Grover, Kevin
Humphries, Guido Minnen, and Larry Piano.
References
Briscoe, E.J. (2006) An Introduction to Tag Sequence Gram-
mars and the RASP System Parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll (2002) ?Robust accurate statisti-
cal annotation of general text?, Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC?02),
Las Palmas, Gran Canaria, pp. 1499?1504.
Briscoe, E.J. and J. Carroll (2006) ?Evaluating the Accu-
racy of an Unlexicalized Statistical Parser on the PARC
DepBank?, Proceedings of the COLING/ACL Conference,
Sydney, Australia.
Elworthy, D. (1994) ?Does Baum-Welch re-estimation help
taggers??, Proceedings of the 4th ACL Conference on Ap-
plied NLP, Stuttgart, Germany, pp. 53?58.
Relation Precision Recall F1 std GRs
dependent 79.76 77.49 78.61 10696
aux 93.33 91.00 92.15 400
conj 72.39 72.27 72.33 595
ta 42.61 51.37 46.58 292
det 87.73 90.48 89.09 1114
arg mod 79.18 75.47 77.28 8295
mod 74.43 67.78 70.95 3908
ncmod 75.72 69.94 72.72 3550
xmod 53.21 46.63 49.70 178
cmod 45.95 30.36 36.56 168
pmod 30.77 33.33 32.00 12
arg 77.42 76.45 76.94 4387
subj or dobj 82.36 74.51 78.24 3127
subj 78.55 66.91 72.27 1363
ncsubj 79.16 67.06 72.61 1354
xsubj 33.33 28.57 30.77 7
csubj 12.50 50.00 20.00 2
comp 75.89 79.53 77.67 3024
obj 79.49 79.42 79.46 2328
dobj 83.63 79.08 81.29 1764
obj2 23.08 30.00 26.09 20
iobj 70.77 76.10 73.34 544
clausal 60.98 74.40 67.02 672
xcomp 76.88 77.69 77.28 381
ccomp 46.44 69.42 55.55 291
pcomp 72.73 66.67 69.57 24
macroaverage 62.12 63.77 62.94
microaverage 77.66 74.98 76.29
Figure 3: Accuracy on DepBank
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga
(1997) ?A new formalization of probabilistic GLR pars-
ing?, Proceedings of the 5th International Workshop on
Parsing Technologies (IWPT?97), MIT, pp. 123?134.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf (1999) ?A
bag of useful techniques for efficient and robust parsing?,
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, University of Mary-
land, pp. 473?480.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan (2003) ?The PARC700 Dependency Bank?, Proceed-
ings of the 4th International Workshop on Linguistically
Interpreted Corpora (LINC-03), Budapest, Hungary.
Minnen, G., J. Carroll and D. Pearce (2001) ?Applied mor-
phological processing of English?, Natural Language En-
gineering, vol.7.3, 225?250.
Oepen, S. and J. Carroll (2000) ?Ambiguity packing in
constraint-based parsing ? practical results?, Proceedings
of the 1st Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle, WA,
pp. 162?169.
Watson, R. (2006) ?Part-of-speech tagging models for pars-
ing?, Proceedings of the 9th Annual CLUK Colloquium,
Open University, Milton Keynes, UK.
Watson, R., E.J. Briscoe and J. Carroll (2006) Semi-
supervised Training of a Statistical Parser from Unlabeled
Partially-bracketed Data, forthcoming.
Watson, R., J. Carroll and E.J. Briscoe (2005) ?Efficient ex-
traction of grammatical relations?, Proceedings of the 9th
Int. Workshop on Parsing Technologies (IWPT?05), Van-
couver, Canada.
80
Proceedings of ACL-08: HLT, pages 968?976,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Deductive Approach to Dependency Parsing?
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
John Carroll and David Weir
Department of Informatics
University of Sussex, United Kingdom
{johnca,davidw}@sussex.ac.uk
Abstract
We define a new formalism, based on Sikkel?s
parsing schemata for constituency parsers,
that can be used to describe, analyze and com-
pare dependency parsing algorithms. This
abstraction allows us to establish clear rela-
tions between several existing projective de-
pendency parsers and prove their correctness.
1 Introduction
Dependency parsing consists of finding the structure
of a sentence as expressed by a set of directed links
(dependencies) between words. This is an alterna-
tive to constituency parsing, which tries to find a di-
vision of the sentence into segments (constituents)
which are then broken up into smaller constituents.
Dependency structures directly show head-modifier
and head-complement relationships which form the
basis of predicate argument structure, but are not
represented explicitly in constituency trees, while
providing a representation in which no non-lexical
nodes have to be postulated by the parser. In addi-
tion to this, some dependency parsers are able to rep-
resent non-projective structures, which is an impor-
tant feature when parsing free word order languages
in which discontinuous constituents are common.
The formalism of parsing schemata (Sikkel, 1997)
is a useful tool for the study of constituency parsers
since it provides formal, high-level descriptions
of parsing algorithms that can be used to prove
their formal properties (such as correctness), es-
tablish relations between them, derive new parsers
from existing ones and obtain efficient implementa-
tions automatically (Go?mez-Rodr??guez et al, 2007).
The formalism was initially defined for context-free
grammars and later applied to other constituency-
based formalisms, such as tree-adjoining grammars
?Partially supported by Ministerio de Educacio?n y Ciencia
and FEDER (TIN2004-07246-C03, HUM2007-66607-C04),
Xunta de Galicia (PGIDIT07SIN005206PR, PGIDIT05PXIC-
10501PN, PGIDIT05PXIC30501PN, Rede Galega de Proc. da
Linguaxe e RI) and Programa de Becas FPU.
(Alonso et al, 1999). However, since parsing
schemata are defined as deduction systems over sets
of constituency trees, they cannot be used to de-
scribe dependency parsers.
In this paper, we define an analogous formalism
that can be used to define, analyze and compare de-
pendency parsers. We use this framework to provide
uniform, high-level descriptions for a wide range of
well-known algorithms described in the literature,
and we show how they formally relate to each other
and how we can use these relations and the formal-
ism itself to prove their correctness.
1.1 Parsing schemata
Parsing schemata (Sikkel, 1997) provide a formal,
simple and uniform way to describe, analyze and
compare different constituency-based parsers.
The notion of a parsing schema comes from con-
sidering parsing as a deduction process which gener-
ates intermediate results called items. An initial set
of items is directly obtained from the input sentence,
and the parsing process consists of the application of
inference rules (deduction steps) which produce new
items from existing ones. Each item contains a piece
of information about the sentence?s structure, and a
successful parsing process will produce at least one
final item containing a full parse tree for the sentence
or guaranteeing its existence.
Items in parsing schemata are formally defined
as sets of partial parse trees from a set denoted
Trees(G), which is the set of all the possible par-
tial parse trees that do not violate the constraints im-
posed by a grammar G. More formally, an item set
I is defined by Sikkel as a quotient set associated
with an equivalence relation on Trees(G).1
Valid parses for a string are represented by
items containing complete marked parse trees for
that string. Given a context-free grammar G =
1While Shieber et al (1995) also view parsers as deduction
systems, Sikkel formally defines items and related concepts,
providing the mathematical tools to reason about formal prop-
erties of parsers.
968
(N,?, P, S), a marked parse tree for a string
w1 . . . wn is any tree ? ? Trees(G)/root(?) =
S?yield(?) = w1 . . . wn2. An item containing such
a tree for some arbitrary string is called a final item.
An item containing such a tree for a particular string
w1 . . . wn is called a correct final item for that string.
For each input string, a parsing schema?s deduc-
tion steps allow us to infer a set of items, called valid
items for that string. A parsing schema is said to
be sound if all valid final items it produces for any
arbitrary string are correct for that string. A pars-
ing schema is said to be complete if all correct fi-
nal items are valid. A correct parsing schema is one
which is both sound and complete. A correct parsing
schema can be used to obtain a working implemen-
tation of a parser by using deductive engines such
as the ones described by Shieber et al (1995) and
Go?mez-Rodr??guez et al (2007) to obtain all valid fi-
nal items.
2 Dependency parsing schemata
Although parsing schemata were initially defined for
context-free parsers, they can be adapted to different
constituency-based grammar formalisms, by finding
a suitable definition of Trees(G) for each particular
formalism and a way to define deduction steps from
its rules. However, parsing schemata are not directly
applicable to dependency parsing, since their formal
framework is based on constituency trees.
In spite of this problem, many of the dependency
parsers described in the literature are constructive,
in the sense that they proceed by combining smaller
structures to form larger ones until they find a com-
plete parse for the input sentence. Therefore, it
is possible to define a variant of parsing schemata,
where these structures can be defined as items and
the strategies used for combining them can be ex-
pressed as inference rules. However, in order to de-
fine such a formalism we have to tackle some issues
specific to dependency parsers:
? Traditional parsing schemata are used to de-
fine grammar-based parsers, in which the parsing
process is guided by some set of rules which are
used to license deduction steps: for example, an
Earley Predictor step is tied to a particular gram-
mar rule, and can only be executed if such a rule
exists. Some dependency parsers are also grammar-
2wi is shorthand for the marked terminal (wi, i). These are
used by Sikkel (1997) to link terminal symbols to string posi-
tions so that an input sentence can be represented as a set of
trees which are used as initial items (hypotheses) for the de-
duction system. Thus, a sentence w1 . . . wn produces a set of
hypotheses {{w1(w1)}, . . . , {wn(wn)}}.
Figure 1: Representation of a dependency structure with
a tree. The arrows below the words correspond to its as-
sociated dependency graph.
based: for example, those described by Lombardo
and Lesmo (1996), Barbero et al (1998) and Ka-
hane et al (1998) are tied to the formalizations of de-
pendency grammar using context-free like rules de-
scribed by Hays (1964) and Gaifman (1965). How-
ever, many of the most widely used algorithms (Eis-
ner, 1996; Yamada and Matsumoto, 2003) do not use
a formal grammar at all. In these, decisions about
which dependencies to create are taken individually,
using probabilistic models (Eisner, 1996) or classi-
fiers (Yamada and Matsumoto, 2003). To represent
these algorithms as deduction systems, we use the
notion of D-rules (Covington, 1990). D-rules take
the form a ? b, which says that word b can have a
as a dependent. Deduction steps in non-grammar-
based parsers can be tied to the D-rules associated
with the links they create. In this way, we obtain
a representation of the semantics of these parsing
strategies that is independent of the particular model
used to take the decisions associated with each D-
rule.
? The fundamental structures in dependency pars-
ing are dependency graphs. Therefore, as items
for constituency parsers are defined as sets of par-
tial constituency trees, it is tempting to define items
for dependency parsers as sets of partial dependency
graphs. However, predictive grammar-based algo-
rithms such as those of Lombardo and Lesmo (1996)
and Kahane et al (1998) have operations which pos-
tulate rules and cannot be defined in terms of depen-
dency graphs, since they do not do any modifications
to the graph. In order to make the formalism general
enough to include these parsers, we define items in
terms of sets of partial dependency trees as shown in
Figure 1. Note that a dependency graph can always
be extracted from such a tree.
? Some of the most popular dependency parsing
algorithms, like that of Eisner (1996), work by con-
necting spans which can represent disconnected de-
pendency graphs. Such spans cannot be represented
by a single dependency tree. Therefore, our formal-
ism allows items to be sets of forests of partial de-
pendency trees, instead of sets of trees.
969
Taking these considerations into account, we de-
fine the concepts that we need to describe item sets
for dependency parsers:
Let ? be an alphabet of terminal symbols.
Partial dependency trees: We define the set of
partial dependency trees (D-trees) as the set of finite
trees where children of each node have a left-to-right
ordering, each node is labelled with an element of
??(??N), and the following conditions hold:
? All nodes labelled with marked terminals wi ?
(??N) are leaves,
? Nodes labelled with terminals w ? ? do not have
more than one daughter labelled with a marked
terminal, and if they have such a daughter node, it
is labelled wi for some i ? N,
? Left siblings of nodes labelled with a marked ter-
minal wk do not have any daughter labelled wj
with j ? k. Right siblings of nodes labelled with
a marked terminal wk do not have any daughter
labelled wj with j ? k.
We denote the root node of a partial dependency
tree t as root(t). If root(t) has a daughter node la-
belled with a marked terminal wh, we will say that
wh is the head of the tree t, denoted by head(t). If
all nodes labelled with terminals in t have a daughter
labelled with a marked terminal, t is grounded.
Relationship between trees and graphs: Let
t ? D-trees be a partial dependency tree; g(t), its
associated dependency graph, is a graph (V,E)
? V ={wi ? (??N) | wi is the label of a node in
t},
? E ={(wi, wj) ? (??N)2 | C,D are nodes in t
such that D is a daughter of C, wj the label of a
daughter of C, wi the label of a daughter of D}.
Projectivity: A partial dependency tree t ?
D-trees is projective iff yield(t) cannot be written
as . . . wi . . . wj . . . where i ? j.
It is easy to verify that the dependency graph
g(t) is projective with respect to the linear order of
marked terminals wi, according to the usual defi-
nition of projectivity found in the literature (Nivre,
2006), if and only if the tree t is projective.
Parse tree: A partial dependency tree t ?
D-trees is a parse tree for a given string w1 . . . wn
if its yield is a permutation of w1 . . . wn. If its yield
is exactly w1 . . . wn, we will say it is a projective
parse tree for the string.
Item set: Let ? ? D-trees be the set of de-
pendency trees which are acceptable according to a
given grammar G (which may be a grammar of D-
rules or of CFG-like rules, as explained above). We
define an item set for dependency parsing as a set
I ? ?, where ? is a partition of 2?.
Once we have this definition of an item set for
dependency parsing, the remaining definitions are
analogous to those in Sikkel?s theory of constituency
parsing (Sikkel, 1997), so we will not include them
here in full detail. A dependency parsing system is
a deduction system (I, H,D) where I is a depen-
dency item set as defined above, H is a set contain-
ing initial items or hypotheses, and D ? (2(H?I) ?
I) is a set of deduction steps defining an inference
relation `.
Final items in this formalism will be those con-
taining some forest F containing a parse tree for
some arbitrary string. An item containing such a tree
for a particular string w1 . . . wn will be called a cor-
rect final item for that string in the case of nonprojec-
tive parsers. When defining projective parsers, cor-
rect final items will be those containing projective
parse trees for w1 . . . wn. This distinction is relevant
because the concepts of soundness and correctness
of parsing schemata are based on correct final items
(cf. section 1.1), and we expect correct projective
parsers to produce only projective structures, while
nonprojective parsers should find all possible struc-
tures including nonprojective ones.
3 Some practical examples
3.1 Col96 (Collins, 96)
One of the most straightforward projective depen-
dency parsing strategies is the one described by
Collins (1996), directly based on the CYK pars-
ing algorithm. This parser works with dependency
trees which are linked to each other by creating
links between their heads. Its item set is defined as
ICol96 = {[i, j, h] | 1 ? i ? h ? j ? n}, where an
item [i, j, h] is defined as the set of forests containing
a single projective dependency tree t such that t is
grounded, yield(t) = wi . . . wj and head(t) = wh.
For an input string w1 . . . wn, the set of hypothe-
ses is H = {[i, i, i] | 0 ? i ? n + 1}, i.e., the set
of forests containing a single dependency tree of the
form wi(wi). This same set of hypotheses can be
used for all the parsers, so we will not make it ex-
plicit for subsequent schemata.3
The set of final items is {[1, n, h] | 1 ? h ? n}:
these items trivially represent parse trees for the in-
put sentence, where wh is the sentence?s head. The
deduction steps are shown in Figure 2.
3Note that the words w0 and wn+1 used in the definition do
not appear in the input: these are dummy terminals that we will
call beginning of sentence (BOS) and end of sentence (EOS)
marker, respectively; and will be needed by some parsers.
970
Col96 (Collins,96):
R-Link
[i, j, h1]
[j + 1, k, h2]
[i, k, h2]
wh1 ? wh2
L-Link
[i, j, h1]
[j + 1, k, h2]
[i, k, h1]
wh2 ? wh1
Eis96 (Eisner, 96):
Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1, F, F ]
R-Link [i, j, F, F ][i, j, T, F ] wi ? wj
L-Link [i, j, F, F ][i, j, F, T ] wj ? wi
CombineSpans
[i, j, b, c]
[j, k, not(c), d]
[i, k, b, d]
ES99 (Eisner and Satta, 99):
R-Link [i, j, i] [j + 1, k, k][i, k, k] wi ? wk
L-Link [i, j, i] [j + 1, k, k][i, k, i] wk ? wi
R-Combiner [i, j, i] [j, k, j][i, k, i]
L-Combiner [i, j, j] [j, k, k][i, k, k]
YM03 (Yamada and Matsumoto, 2003):
Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1]
R-Link
[i, j]
[j, k]
[i, k] wj ? wk L-Link
[i, j]
[j, k]
[i, k] wj ? wi
LL96 (Lombardo and Lesmo, 96):
Initter [(.S), 1, 0] ?(S)?P Predictor
[A(?.B?), i, j]
[B(.?), j + 1, j] B(?)?P
Scanner [A(?. ? ?), i, h? 1] [h, h, h][A(? ? .?), i, h] wh IS A
Completer [A(?.B?), i, j] [B(?.), j + 1, k][A(?B.?), i, k]
Figure 2: Deduction steps of the parsing schemata for some well-known dependency parsers.
As we can see, we use D-rules as side conditions
for deduction steps, since this parsing strategy is not
grammar-based. Conceptually, the schema we have
just defined describes a recogniser: given a set of D-
rules and an input string wi . . . wn, the sentence can
be parsed (projectively) under those D-rules if and
only if this deduction system can infer a correct final
item. However, when executing this schema with a
deductive engine, we can recover the parse forest by
following back pointers in the same way as is done
with constituency parsers (Billot and Lang, 1989).
Of course, boolean D-rules are of limited interest
in practice. However, this schema provides a formal-
ization of a parsing strategy which is independent
of the way linking decisions are taken in a partic-
ular implementation. In practice, statistical models
can be used to decide whether a step linking words
a and b (i.e., having a ? b as a side condition) is
executed or not, and probabilities can be attached to
items in order to assign different weights to different
analyses of the sentence. The same principle applies
to the rest of D-rule-based parsers described in this
paper.
3.2 Eis96 (Eisner, 96)
By counting the number of free variables used in
each deduction step of Collins? parser, we can con-
clude that it has a time complexity of O(n5). This
complexity arises from the fact that a parentless
word (head) may appear in any position in the par-
tial results generated by the parser; the complexity
can be reduced to O(n3) by ensuring that parentless
words can only appear at the first or last position
of an item. This is the principle behind the parser
defined by Eisner (1996), which is still in wide use
today (Corston-Oliver et al, 2006; McDonald et al,
2005a).
The item set for Eisner?s parsing schema is
IEis96 = {[i, j, T, F ] | 0 ? i ? j ? n} ?
{[i, j, F, T ] | 0 ? i ? j ? n} ? {[i, j, F, F ] |
0 ? i ? j ? n}, where each item [i, j, T, F ] is de-
fined as the item [i, j, j] ? ICol96, each item
[i, j, F, T ] is defined as the item [i, j, i] ? ICol96,
and each item [i, j, F, F ] is defined as the set
of forests of the form {t1, t2} such that t1 and
t2 are grounded, head(t1) = wi, head(t2) = wj ,
and ?k ? N(i ? k < j)/yield(t1) = wi . . . wk ?
yield(t2) = wk+1 . . . wj .
Note that the flags b, c in an item [i, j, b, c] indi-
cate whether the words in positions i and j, respec-
tively, have a parent in the item or not. Items with
one of the flags set to T represent dependency trees
where the word in position i or j is the head, while
items with both flags set to F represent pairs of trees
headed at positions i and j, and therefore correspond
to disconnected dependency graphs.
Deduction steps4 are shown in Figure 2. The
set of final items is {[0, n, F, T ]}. Note that these
items represent dependency trees rooted at the BOS
marker w0, which acts as a ?dummy head? for the
sentence. In order for the algorithm to parse sen-
tences correctly, we will need to define D-rules to
allow w0 to be linked to the real sentence head.
3.3 ES99 (Eisner and Satta, 99)
Eisner and Satta (1999) define an O(n3) parser for
split head automaton grammars that can be used
4Alternatively, we could consider items of the form [i, i +
1, F, F ] to be hypotheses for this parsing schema, so we would
not need an Initter step. However, we have chosen to use a stan-
dard set of hypotheses valid for all parsers because this allows
for more straightforward proofs of relations between schemata.
971
for dependency parsing. This algorithm is con-
ceptually simpler than Eis96, since it only uses
items representing single dependency trees, avoid-
ing items of the form [i, j, F, F ]. Its item set is
IES99 = {[i, j, i] | 0 ? i ? j ? n} ? {[i, j, j] |
0 ? i ? j ? n}, where items are defined as in
Collins? parsing schema.
Deduction steps are shown in Figure 2, and the set
of final items is {[0, n, 0]}. (Parse trees have w0 as
their head, as in the previous algorithm).
Note that, when described for head automaton
grammars as in Eisner and Satta (1999), this algo-
rithm seems more complex to understand and imple-
ment than the previous one, as it requires four differ-
ent kinds of items in order to keep track of the state
of the automata used by the grammars. However,
this abstract representation of its underlying seman-
tics as a dependency parsing schema shows that this
parsing strategy is in fact conceptually simpler for
dependency parsing.
3.4 YM03 (Yamada and Matsumoto, 2003)
Yamada and Matsumoto (2003) define a determinis-
tic, shift-reduce dependency parser guided by sup-
port vector machines, which achieves over 90% de-
pendency accuracy on section 23 of the Penn tree-
bank. Parsing schemata are not suitable for directly
describing deterministic parsers, since they work at
a high abstraction level where a set of operations
are defined without imposing order constraints on
them. However, many deterministic parsers can be
viewed as particular optimisations of more general,
nondeterministic algorithms. In this case, if we rep-
resent the actions of the parser as deduction steps
while abstracting from the deterministic implemen-
tation details, we obtain an interesting nondetermin-
istic parser.
Actions in Yamada and Matsumoto?s parser create
links between two target nodes, which act as heads
of neighbouring dependency trees. One of the ac-
tions creates a link where the left target node be-
comes a child of the right one, and the head of a
tree located directly to the left of the target nodes
becomes the new left target node. The other ac-
tion is symmetric, performing the same operation
with a right-to-left link. An O(n3) nondetermin-
istic parser generalising this behaviour can be de-
fined by using an item set IY M03 = {[i, j] |
0 ? i ? j ? n + 1}, where each item [i, j] is de-
fined as the item [i, j, F, F ] in IEis96; and the de-
duction steps are shown in Figure 2.
The set of final items is {[0, n + 1]}. In order for
this set to be well-defined, the grammar must have
no D-rules of the form wi ? wn+1, i.e., it must not
allow the EOS marker to govern any words. If this
is the case, it is trivial to see that every forest in an
item of the form [0, n + 1] must contain a parse tree
rooted at the BOS marker and with yield w0 . . . wn.
As can be seen from the schema, this algorithm
requires less bookkeeping than any other of the
parsers described here.
3.5 LL96 (Lombardo and Lesmo, 96) and
other Earley-based parsers
The algorithms in the above examples are based on
taking individual decisions about dependency links,
represented by D-rules. Other parsers, such as that
of Lombardo and Lesmo (1996), use grammars with
context-free like rules which encode the preferred
order of dependents for each given governor, as de-
fined by Gaifman (1965). For example, a rule of the
form N(Det ? PP ) is used to allow N to have Det
as left dependent and PP as right dependent.
The algorithm by Lombardo and Lesmo (1996)
is a version of Earley?s context-free grammar parser
(Earley, 1970) using Gaifman?s dependency gram-
mar, and can be written by using an item set
ILomLes = {[A(?.?), i, j] | A(??) ? P ?
1 ? i ? j ? n}, where each item [A(?.?), i, j] rep-
resents the set of partial dependency trees rooted at
A, where the direct children of A are ??, and the
subtrees rooted at ? have yield wi . . . wj . The de-
duction steps for the schema are shown in Figure 2,
and the final item set is {[(S.), 1, n]}.
As we can see, the schema for Lombardo and
Lesmo?s parser resembles the Earley-style parser in
Sikkel (1997), with some changes to adapt it to de-
pendency grammar (for example, the Scanner al-
ways moves the dot over the head symbol ?).
Analogously, other dependency parsing schemata
based on CFG-like rules can be obtained by mod-
ifying context-free grammar parsing schemata of
Sikkel (1997) in a similar way. The algorithm by
Barbero et al (1998) can be obtained from the left-
corner parser, and the one by Courtin and Genthial
(1998) is a variant of the head-corner parser.
3.6 Pseudo-projectivity
Pseudo-projective parsers can generate non-
projective analyses in polynomial time by using
a projective parsing strategy and postprocessing
the results to establish nonprojective links. For
example, the algorithm by Kahane et al (1998) uses
a projective parsing strategy like that of LL96, but
using the following initializer step instead of the
972
Initter and Predictor:5
Initter [A(?), i, i ? 1] A(?) ? P ? 1 ? i ? n
4 Relations between dependency parsers
The framework of parsing schemata can be used to
establish relationships between different parsing al-
gorithms and to obtain new algorithms from existing
ones, or derive formal properties of a parser (such as
soundness or correctness) from the properties of re-
lated algorithms.
Sikkel (1994) defines several kinds of relations
between schemata, which fall into two categories:
generalisation relations, which are used to obtain
more fine-grained versions of parsers, and filtering
relations, which can be seen as the reverse of gener-
alisation and are used to reduce the number of items
and/or steps needed for parsing. He gives a formal
definition of each kind of relation. Informally, a
parsing schema can be generalised from another via
the following transformations:
? Item refinement: We say that P1 ir?? P2 (P2 is an
item refinement of P1) if there is a mapping be-
tween items in both parsers such that single items
in P1 are broken into multiple items in P2 and in-
dividual deductions are preserved.
? Step refinement: We say that P1 sr?? P2 if the
item set of P1 is a subset of that of P2 and every
single deduction step in P1 can be emulated by a
sequence of inferences in P2.
On the other hand, a schema can be obtained from
another by filtering in the following ways:
? Static/dynamic filtering: P1
sf/df???? P2 if the item
set of P2 is a subset of that of P1 and P2 allows a
subset of the direct inferences in P16.
? Item contraction: The inverse of item refinement.
P1 ic?? P2 if P2 ir?? P1.
? Step contraction: The inverse of step refinement.
P1 sc?? P2 if P2 sr?? P1.
All the parsers described in section 3 can be re-
lated via generalisation and filtering, as shown in
Figure 3. For space reasons we cannot show formal
proofs of all the relations, but we sketch the proofs
for some of the more interesting cases:
5The initialization step as reported in Kahane?s paper is dif-
ferent from this one, as it directly consumes a nonterminal from
the input. However, using this step results in an incomplete
algorithm. The problem can be fixed either by using the step
shown here instead (bottom-up Earley strategy) or by adding an
additional step turning it into a bottom-up Left-Corner parser.
6Refer to Sikkel (1994) for the distinction between static and
dynamic filtering, which we will not use here.
4.1 YM03 sr?? Eis96
It is easy to see from the schema definitions that
IY M03 ? IEis96. In order to prove the relation
between these parsers, we need to verify that every
deduction step in YM03 can be emulated by a se-
quence of inferences in Eis96. In the case of the
Initter step this is trivial, since the Initters of both
parsers are equivalent. If we write the R-Link step in
the notation we have used for Eisner items, we have
R-Link [i, j, F, F ] [j, k, F, F ]
[i, k, F, F ] wj ? wk
This can be emulated in Eisner?s parser by an
R-Link step followed by a CombineSpans step:
[j, k, F, F ] ` [j, k, T, F ] (by R-Link),
[j, k, T, F ], [i, j, F, F ] ` [i, k, F, F ] (by CombineSpans).
Symmetrically, the L-Link step in YM03 can be
emulated by an L-Link followed by a CombineSpans
in Eis96.
4.2 ES99 sr?? Eis96
If we write the R-Link step in Eisner and Satta?s
parser in the notation for Eisner items, we have
R-Link [i, j, F, T ] [j + 1, k, T, F ][i, k, T, F ] wi ? wk
This inference can be emulated in Eisner?s parser
as follows:
` [j, j + 1, F, F ] (by Initter),
[i, j, F, T ], [j, j + 1, F, F ] ` [i, j + 1, F, F ] (CombineSpans),
[i, j + 1, F, F ], [j + 1, k, T, F ] ` [i, k, F, F ] (CombineSpans),
[i, k, F, F ] ` [i, k, T, F ] (by R-Link).
The proof corresponding to the L-Link step is sym-
metric. As for the R-Combiner and L-Combiner
steps in ES99, it is easy to see that they are partic-
ular cases of the CombineSpans step in Eis96, and
therefore can be emulated by a single application of
CombineSpans.
Note that, in practice, the relations in sections 4.1
and 4.2 mean that the ES99 and YM03 parsers are
superior to Eis96, since they generate fewer items
and need fewer steps to perform the same deduc-
tions. These two parsers also have the interesting
property that they use disjoint item sets (one uses
items representing trees while the other uses items
representing pairs of trees); and the union of these
disjoint sets is the item set used by Eis96. Also note
that the optimisation in YM03 comes from contract-
ing deductions in Eis96 so that linking operations
are immediately followed by combining operations;
while ES99 does the opposite, forcing combining
operations to be followed by linking operations.
4.3 Other relations
If we generalise the linking steps in ES99 so that the
head of each item can be in any position, we obtain a
973
Figure 3: Formal relations between several well-known dependency parsers. Arrows going upwards correspond to
generalisation relations, while those going downwards correspond to filtering. The specific subtype of relation is
shown in each arrow?s label, following the notation in Section 4.
correct O(n5) parser which can be filtered to Col96
just by eliminating the Combiner steps.
From Col96, we can obtain an O(n5) head-corner
parser based on CFG-like rules by an item refine-
ment in which each Collins item [i, j, h] is split into
a set of items [A(?.?.?), i, j, h]. Of course, the for-
mal refinement relation between these parsers only
holds if the D-rules used for Collins? parser corre-
spond to the CFG rules used for the head-corner
parser: for every D-rule B ? A there must be a
corresponding CFG-like rule A ? . . . B . . . in the
grammar used by the head-corner parser.
Although this parser uses three indices i, j, h, us-
ing CFG-like rules to guide linking decisions makes
the h indices unnecessary, so they can be removed.
This simplification is an item contraction which re-
sults in an O(n3) head-corner parser. From here,
we can follow the procedure in Sikkel (1994) to
relate this head-corner algorithm to parsers analo-
gous to other algorithms for context-free grammars.
In this way, we can refine the head-corner parser
to a variant of de Vreught and Honig?s algorithm
(Sikkel, 1997), and by successive filters we reach a
left-corner parser which is equivalent to the one de-
scribed by Barbero et al (1998), and a step contrac-
tion of the Earley-based dependency parser LL96.
The proofs for these relations are the same as those
described in Sikkel (1994), except that the depen-
dency variants of each algorithm are simpler (due
to the absence of epsilon rules and the fact that the
rules are lexicalised).
5 Proving correctness
Another useful feature of the parsing schemata
framework is that it provides a formal way to de-
fine the correctness of a parser (see last paragraph
of Section 1.1) which we can use to prove that our
parsers are correct. Furthermore, relations between
schemata can be used to derive the correctness of
a schema from that of related ones. In this sec-
tion, we will show how we can prove that the YM03
and ES99 algorithms are correct, and use that fact to
prove the correctness of Eis96.
5.1 ES99 is correct
In order to prove the correctness of a parser, we must
prove its soundness and completeness (see section
1.1). Soundness is generally trivial to verify, since
we only need to check that every individual deduc-
tion step in the parser infers a correct consequent
item when applied to correct antecedents (i.e., in this
case, that steps always generate non-empty items
that conform to the definition in 3.3). The difficulty
is proving completeness, for which we need to prove
that all correct final items are valid (i.e., can be in-
ferred by the schema). To show this, we will prove
the stronger result that all correct items are valid.
We will show this by strong induction on the
length of items, where the length of an item ? =
[i, k, h] is defined as length(?) = k ? i + 1. Cor-
rect items of length 1 are the hypotheses of the
schema (of the form [i, i, i]) which are trivially valid.
We will prove that, if all correct items of length m
are valid for all 1 ? m < l, then items of length l
are also valid.
Let [i, k, i] be an item of length l in IES99 (thus,
l = k? i+1). If this item is correct, then it contains
a grounded dependency tree t such that yield(t) =
wi . . . wk and head(t) = wi.
By construction, the root of t is labelled wi. Let
wj be the rightmost daughter of wi in t. Since t
is projective, we know that the yield of wj must be
of the form wl . . . wk, where i < l ? j ? k. If
l < j, then wl is the leftmost transitive dependent of
wj in t, and if k > j, then we know that wk is the
rightmost transitive dependent of wj in t.
Let tj be the subtree of t rooted at wj . Let t1 be
the tree obtained from removing tj from t. Let t2 be
974
the tree obtained by removing all the children to the
right of wj from tj , and t3 be the tree obtained by re-
moving all the children to the left of wj from tj . By
construction, t1 belongs to a correct item [i, l? 1, i],
t2 belongs to a correct item [l, j, j] and t3 belongs to
a correct item [j, k, j]. Since these three items have
a length strictly less than l, by the inductive hypoth-
esis, they are valid. This allows us to prove that the
item [i, k, i] is also valid, since it can be obtained
from these valid items by the following inferences:
[i, l ? 1, i], [l, j, j] ` [i, j, i] (by the L-Link step),
[i, j, i], [j, k, j] ` [i, k, i] (by the L-Combiner step).
This proves that all correct items of length l which
are of the form [i, k, i] are correct under the induc-
tive hypothesis. The same can be proved for items of
the form [i, k, k] by symmetric reasoning, thus prov-
ing that the ES99 parsing schema is correct.
5.2 YM03 is correct
In order to prove correctness of this parser, we fol-
low the same procedure as above. Soundness is
again trivial to verify. To prove completeness, we
use strong induction on the length of items, where
the length of an item [i, j] is defined as j ? i + 1.
The induction step is proven by considering any
correct item [i, k] of length l > 2 (l = 2 is the base
case here since items of length 2 are generated by
the Initter step) and proving that it can be inferred
from valid antecedents of length less than l, so it is
valid. To show this, we note that, if l > 2, either
wi has at least a right dependent or wk has at least a
left dependent in the item. Supposing that wi has a
right dependent, if t1 and t2 are the trees rooted at wi
and wk in a forest in [i, k], we call wj the rightmost
daughter of wi and consider the following trees:
v = the subtree of t1 rooted at wj , u1 = the tree ob-
tained by removing v from t1, u2 = the tree obtained
by removing all children to the right of wj from v,
u3 = the tree obtained by removing all children to
the left of wj from v.
We observe that the forest {u1, u2} belongs to the
correct item [i, j], while {u3, t2} belongs to the cor-
rect item [j, k]. From these two items, we can obtain
[i, k] by using the L-Link step. Symmetric reason-
ing can be applied if wi has no right dependents but
wk has at least a left dependent, and analogously to
the case of the previous parser, we conclude that the
YM03 parsing schema is correct.
5.3 Eis96 is correct
By using the previous proofs and the relationships
between schemata that we explained earlier, it is
easy to prove that Eis96 is correct: soundness is,
as always, straightforward, and completeness can be
proven by using the properties of other algorithms.
Since the set of final items in Eis96 and ES99 are
the same, and the former is a step refinement of the
latter, the completeness of ES99 directly implies the
completeness of Eis96.
Alternatively, we can use YM03 to prove the cor-
rectness of Eis96 if we redefine the set of final items
in the latter to be of the form [0, n+ 1, F, F ], which
are equally valid as final items since they always
contain parse trees. This idea can be applied to trans-
fer proofs of completeness across any refinement re-
lation.
6 Conclusions
We have defined a variant of Sikkel?s parsing
schemata formalism which allows us to represent
dependency parsing algorithms in a simple, declar-
ative way7. We have clarified relations between
parsers which were originally described very differ-
ently. For example, while Eisner presented his algo-
rithm as a dynamic programming algorithm which
combines spans into larger spans, Yamada and Mat-
sumoto?s works by sequentially executing parsing
actions that move a focus point in the input one po-
sition to the left or right, (possibly) creating a de-
pendency link. However, in the parsing schemata
for these algorithms we can see (and formally prove)
that they are related: one is a refinement of the other.
Parsing schemata are also a formal tool that can be
used to prove the correctness of parsing algorithms.
The relationships between dependency parsers can
be exploited to derive properties of a parser from
those of others, as we have seen in several examples.
Although the examples in this paper are cen-
tered in projective dependency parsing, the formal-
ism does not require projectivity and can be used to
represent nonprojective algorithms as well8. An in-
teresting line for future work is to use relationships
between schemata to find nonprojective parsers that
can be derived from existing projective counterparts.
7An alternative framework that formally describes some de-
pendency parsers is that of transition systems (McDonald and
Nivre, 2007). This model is based on parser configurations and
transitions, and has no clear relationship with the approach de-
scribed here.
8Note that spanning tree parsing algorithms based on edge-
factored models, such as the one by McDonald et al (2005b)
are not constructive in the sense outlined in Section 2, so the
approach described here does not directly apply to them. How-
ever, other nonprojective parsers such as (Attardi, 2006) follow
a constructive approach and can be analysed deductively.
975
References
Miguel A. Alonso, Eric de la Clergerie, David Cabrero,
and Manuel Vilares. 1999. Tabular algorithms for
TAG parsing. In Proc. of the Ninth Conference on Eu-
ropean chapter of the Association for Computational
Linguistics, pages 150?157, Bergen, Norway. ACL.
Giuseppe Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Proc. of
the Tenth Conference on Natural Language Learning
(CoNLL-X), pages 166?170, New York, USA. ACL.
Cristina Barbero, Leonardo Lesmo, Vincenzo Lombarlo,
and Paola Merlo. 1998. Integration of syntactic
and lexical information in a hierarchical dependency
grammar. In Proc. of the Workshop on Dependency
Grammars, pages 58?67, ACL-COLING, Montreal,
Canada.
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forest in ambiguous parsing. In Proc. of the
27th Annual Meeting of the Association for Computa-
tional Linguistics, pages 143?151, Vancouver, British
Columbia, Canada, June. ACL.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc. of
the 34th annual meeting on Association for Compu-
tational Linguistics, pages 184?191, Morristown, NJ,
USA. ACL.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using Bayes Point Machines. In Proc. of the main
conference on Human Language Technology Confer-
ence of the North American Chapter of the Association
of Computational Linguistics, pages 160?167, Morris-
town, NJ, USA. ACL.
Jacques Courtin and Damien Genthial. 1998. Parsing
with dependency relations and robust parsing. In Proc.
of the Workshop on Dependency Grammars, pages 88?
94, ACL-COLING, Montreal, Canada.
Michael A. Covington. 1990. A dependency parser for
variable-word-order languages. Technical Report AI-
1990-01, Athens, GA.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, pages 457?464, Mor-
ristown, NJ, USA. ACL.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of the
16th International Conference on Computational Lin-
guistics (COLING-96), pages 340?345, Copenhagen,
August.
Haim Gaifman. 1965. Dependency systems and phrase-
structure systems. Information and Control, 8:304?
337.
Carlos Go?mez-Rodr??guez, Jesu?s Vilares, and Miguel A.
Alonso. 2007. Compiling declarative specifications
of parsing algorithms. In Database and Expert Sys-
tems Applications, volume 4653 of Lecture Notes in
Computer Science, pages 529?538, Springer-Verlag.
David Hays. 1964. Dependency theory: a formalism and
some observations. Language, 40:511?525.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In COLING-ACL,
pages 646?652.
Vincenzo Lombardo and Leonardo Lesmo. 1996. An
Earley-type recognizer for dependency grammar. In
Proc. of the 16th conference on Computational linguis-
tics, pages 723?728, Morristown, NJ, USA. ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In ACL ?05: Proc. of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
91?98, Morristown, NJ, USA. ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov and Jan
Hajic?. 2005b. Non-projective dependency parsing us-
ing spanning tree algorithms. In HLT ?05: Proc. of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proc. of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 122?131.
Joakim Nivre. 2006. Inductive Dependency Parsing
(Text, Speech and Language Technology). Springer-
Verlag New York, Inc., Secaucus, NJ, USA.
Stuart M. Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. Journal of Logic Programming, 24:3?
36.
Klaas Sikkel. 1994. How to compare the structure of
parsing algorithms. In G. Pighizzini and P. San Pietro,
editors, Proc. of ASMICS Workshop on Parsing The-
ory. Milano, Italy, Oct 1994, pages 21?39.
Klaas Sikkel. 1997. Parsing Schemata ? A Framework
for Specification and Analysis of Parsing Algorithms.
Texts in Theoretical Computer Science ? An EATCS
Series. Springer-Verlag, Berlin/Heidelberg/New York.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proc. of 8th International Workshop on Parsing Tech-
nologies, pages 195?206.
976
Robust ,  App l ied  Morpho log ica l  Generat ion  . . . .  ......... ..... . _ 
Guido  Minnen John  Car ro l l  Dar ren  Pearce 
Cognitive and Comput ing Sciences 
University of Sussex 
Brighton BN1 9QH, UK 
{fir stname, lastname }@cogs. susx. ac. uk 
Abst ract  
In practical natural language generation sys- 
tems it is often advantageous to have a separate 
component that deals purely with morphologi- 
cal processing. We present such a component: a
fast and robust morphological generator for En- 
glish based on finite-state techniques that gen- 
erates a word form given a specification of the 
lemma, part-of-speech, and the type of inflec- 
tion required. We describe how this morpholog- 
ical generator is used in a prototype system for 
automatic simplification of English newspaper 
text, and discuss practical morphological and 
orthographic ssues we have encountered in gen- 
eration of unrestricted text within this applica- 
tion. 
1 In t roduct ion  
Most approaches to natural anguage generation 
(NLG) ignore morphological variation during 
word choice, postponing the computation of the 
actual word forms to be output to a final stage, 
sometimes termed 'linearisation'. The advan- 
tage of this setup is that the syntactic/lexical 
realisation component does not have to consider 
all possible word forms corresponding to each 
lemma (Shieber et al, 1990). In practice, it is 
advantageous to have morphological generation 
as a postprocessing component that is separate 
from the rest of the NLG system. A benefit 
is that since there are no competing claims on 
the representation framework from other types 
of linguistic and non-linguistic knowledge, the 
developer of the morphological generator is fl'ee 
to express morphological information in a per- 
spicuous and elegant manner. A further bene- 
fit is that localising morphological knowledge in 
a single component facilitates more systematic 
and reliable updating. From a software engi- 
neering perspective, modularisntion is likely to 
reduce system development costs and increase 
system reliability. As an individual module, 
the morphological generator will be more easily 
shareable between several different NLG appli- 
cations, and integrated into new ones. Finally, 
such a generator can be used on its own in other 
types of applications that do not contain a stan- 
dard NLG syntactic/lexical realisation compo- 
nent, such as text simplification (see Section 3). 
In this paper we describe a fast and robust 
generator for the inflectional morphology of En- 
glish that generates a word form given a speci- 
fication of a lemma, a part-of-speech (PoS) la- 
bel, and an inflectional type. The morphologi- 
cal generator was built using data from several 
large corpora and machine readable dictionar- 
ies. It does not contain an explicit lexicon or 
word-list, but instead comprises a set of mor- 
phological generalisations together with a list of 
exceptions for specific (irregular) word forms. 
This organisation into generalisations and ex- 
ceptions can save time and effort in system de- 
velopment since the addition of new vocabu- 
lary that has regular morphology does not re- 
quire any changes to the generator. In addition, 
the generalisation-exception architecture can be 
used to specify--and also override--preferences 
in cases where a lemma has more than one pos- 
sible surface word form given a particular inflec- 
tional type and PoS label. 
The generator is packaged up as a Unix 'fil- 
ter', making it easy to integrate into applica- 
tions. It is based on efficient finite-state tech- 
niques, and is implemented using the widely 
available Unix Flex utility (a reimplementation 
of the AT&T Unix Lex tool) (Levine et al, 
1992). The generator is freely available to the 
NLG research comnmnity (see Section 5 below). 
The paper is structured ms follows. Section 2 
describes the morphological generator and eval- 
201 
uates its accuracy. Section 3 outlines how the (1) {h}+"s+s_.N" 
generator is put ..to use in.a prototy.p~.system for.:.: ........... : .: ~-.:=..{a=e..tnxnfnp_~ord_:form (1, !~es"-) ).; } 
automatic simplification of text, and discusses 
a number of practical morphological nd ortho- 
graphic issues that we have encountered. Sec- 
tion 4 relates our work to that of others, and we 
conclude (Section 5) with directions for future 
work. 
2 Morpho log ica l  Generat ion  
2.1 The  Generator  
The morphological generator covers the produc- 
tive English affixes s for the plural form of nouns 
and the third person singular present tense of 
verbs, and ed for the past tense, en for the 
past participle, and ing for the present partici- 
ple forms of verbs. 1 The generator is imple- 
mented in Flex. 
The standard use of Flex is to construct 
'scanners', programs that recognise lexical pat- 
terns in text (Levine et al, 1992). A Flex 
description--the high-level description of a 
scanner that Flex takes as input--consists of a 
set of 'rules': pairs of regular expression pat- 
terns (which Flex compiles into deterministic 
finite-state automata (Aho et al, 1986)), and 
actions consisting of arbitrary C code. Flex cre- 
ates as output a C program which at run-time 
scans a text looking for occurrences of the reg- 
ular expressions. Whenever it finds one, it exe- 
cutes the corresponding C code. Flex is part of 
the Berkeley Unix distribution and as a result 
Flex programs are very portable. The standard 
version of Flex works with any ISO-8559 char- 
acter set; Unicode support is also available. 
The morphological generator expects to re- 
ceive as input a sequence of tokens of the form 
lemma+inflection_label, where lemma specifies 
tim lemma of the word form to be generated, 
inflection specifies the type of inflection (i.e. s, 
ed~ cn or ing), and label specifies the PoS of the 
word form. The PoS labels follow the same pat- 
tern as in the Lancaster CLAWS tag sets (Gar- 
side et al, 1987; Burnard, 1995)~ with noun tags 
starting with N, etc. The symbols + and _ are 
delimiters. 
An example of a morphological generator rule 
is given in (1). 
~\Ve do not currently cover comparative and superla- 
tive forms of adjectives or adverbs ince their productiv- 
ity is much less predictable. 
The left-hand side of the rule is a regular expres- 
sion. The braces signify exactly one occurrence 
of an element of the character set abbreviated 
by the symbol h; we assume here that h abbre- 
viates the upper and lower case letters of the al- 
phabet. The next symbol + specifies that there 
..... must. be a..sequence of one or.=more characters, 
each belonging to the character set abbreviated 
by h. Double quotes indicate literal character 
symbols. The right-hand side of the rule gives 
the C code to be executed when an input string 
matches the regular expression. When the Flex 
rule matches the input address+s_N, for exam- 
ple, the C function np_word_.form (defined else- 
where in the generator) is called to determine 
the word form corresponding to the input: the 
function deletes the inflection type and PoS la- 
bel specifications and the delimiters, removes 
the last character of the lemma, and finally at- 
taches the characters es; the word form gener- 
ated is thus addresses. 
Of course not all plural noun inflections are 
correctly generated by the rule in (1) since 
there are many irregularities and subregular- 
ities. These are dealt with using additional, 
more specific, rules. The order in which these 
rules are applied to the input follows exactly 
the order in which the rules appear in the Flex 
description. This makes for a very simple and 
perspicuous way to express generalizations and 
exceptions. For instance, the rule in (2) gener- 
ates the plural form of many English nouns that 
originate from Latin, such as stimulus. 
(2) 
{return(np_word_form(2, "i") ) ; } 
With the input stimulus+s_N, the output is 
stimuli rather than the incorrect *stimuluses 
that would follow from the application of the 
more general rule in (1). By ensuring that this 
rule precedes the rule in (1) in the description, 
nouns such as stimulus get the correct plural 
form inflection. Some other words in this class, 
though, do not have the Latinate plural form 
(e.g. *boni as a plural form of bonus); in these 
cases the generator contains rules specifying the 
correct forms as exceptions. 
202 
2.2 In f lect ional  P re ferences  
The rules constitutingthe g iaerator do not nec- 
essarily have to be mutually exclusive, so they 
can be used to capture the inflectional morphol- 
ogy of lemmata that have more than one pos- 
sible inflected form given a specific PoS label 
and inflectional type. An example of this is the 
multiple inflections of the noun cactus, which 
has not only the Latinate plural form cacti but 
also the English~ptura4.form.cactuses: , In addi- 
tion, inflections of some words differ according 
to dialect. For example, the past participle form 
of the verb to bear is borne in British English, 
whereas in American English the preferred word 
form is born. 
In cases where there is more than one possi- 
ble inflection for a particular input lemma, the 
order of the rules in the Flex description de- 
termines the inflectional preference. For exam- 
ple, with the noun cactus, the fact that the rule 
in (2) precedes the one in (1) causes the gener- 
ator to output the word form cacti rather than 
cactuses even though both rules are applicable. 2 
It is important o note, though, that the gen- 
erator will always choose between multiple in- 
flections: there is no way for it to output all 
possible word forms for a particular input. 3 
2.3 Consonant  Doub l ing  
An important issue concerning morphological 
generation that is closely related to that of 
inflectional preference is consonant doubling. 
This phenomenon, occurring mainly in British 
English, involves the doubling of a consonant 
at the end of a lemma when the lemma is in- 
flected. For example, the past tense/participle 
inflection of the verb to travel is travelled in 
British English, where the final consonant of the 
lemma is doubled before the suffix is attached. 
In American English the past tense/participle 
inflection of the verb to travel is usually spelt 
traveled. Consonant doubling is triggered on 
the basis of both orthographic and phonologi- 
cal information: when a word ends in one vowel 
-"Rule choice based on ordering in the description can 
in fact be overridden by arranging for the second or sub- 
sequent match to cover a larger part of the input so that 
the longest match heuristic applies (Levine et al, 1992). 
But note that the rules in (t) and (2) will always match 
the same input span. 
3Flex does not allow the use of rules that have iden- 
tical left-hand side regular expressions. 
followed by one consonant and the last part of 
..-: =the, word is stressedyin-general:.the ~eonsona, t 
is doubled (Procter, 1995). However there are 
exceptions to this, and in any case the input to 
the morphological generator does not contain 
information about stress. 
Consider the Flex rule in (3), where the sym- 
bols C and V abbreviate the character sets con- 
sisting of (upper and lower case) consonants and 
.vowels,. respectively. 
(3) {A}*{C}{V}"t+ed_V ?' 
{return(cb_wordf orm(O, '%", "ed" ) ) ; } 
Given the input submit+ed_ V this rule correctly 
generates ubmitted. However, the verb to ex- 
hibit does not undergo consonant doubling so 
this rule will generate, incorrectly, the word 
form exhibitted. 
In order to ensure that the correct inflection 
of a verb is generated, the morphological gener- 
ator uses a list of (around 1,100) lemmata that 
allow consonant doubling, extracted automati- 
cally from the British National Corpus (BNC; 
Burnard, 1995). The list is checked before in- 
flecting verbs. Given the fact that there are 
many more verbs that do not allow consonant 
doubling, listing the verbs that do is the most 
economic solution. An added benefit is that if a 
lemma does allow consonant doubling but is not 
included in the list then the word form gener- 
ated will still be correct with respect o Ameri- 
can English. 
2.4 Der iv ing the  Generator  
The morphological generator comprises a set of 
of approximately 1,650 rules expressing mor- 
phological regularities, ubregularities, and ex- 
ceptions for specific words; also around 350 lines 
of C/Flex code for program initialisation and 
defining the functions called by the rule actions. 
The rule set is in fact obtained by automati- 
cally reversing a morphological analyser. This 
is a much enhanced version of the analyser orig- 
inally developed for tile GATE system (Cun- 
ningham et al, 1996). Minnen and Carroll (Un- 
der review) describe in detail how the reversal is 
performed. The generator executable occupies 
around 700Kb on disc. 
The analyser--and therefore the generator-- 
includes exception lists derived from WordNet 
(version 1.5: Miller et al, 1993). In addi- 
tion. we have incorporated ata acquired semi- 
203 
automatically from the following corpora and 
machine readable,dictionaries: the..LOB. cor- 
pus (Garside et al, 1987), the Penn Tree- 
bank (Marcus et al, 1993), the SUSANNE cor- 
pus (Sampson, 1995), the Spoken English Cor- 
pus (Taylor and Knowles, 1988), the Oxford 
Psycholinguistic Database (Quinlan, 1992), and 
the "Computer-Usable" version of the Oxford 
Advanced Learner's Dictionary of Current En- 
glish (OALDCE; Mitton,  1.9.92). 
2.5 Evaluation 
Minnen and Carroll (Under review) report an 
evaluation of the accuracy of the morphologi- 
cal generator with respect o the CELEX lexi- 
cal database (version 2.5; Baayen et al, 1993). 
This threw up a small number of errors which 
we have now fixed. We have rerun the CELEX- 
based evaluation: against the past tense, past 
and present participle, and third person singu- 
lar present ense inflections of verbs, and all plu- 
ral nouns. After excluding multi-word entries 
(phrasal verbs, etc.) we were left with 38,882 
out of the original 160,595 word forms. For each 
of these word forms we fed the corresponding 
input (derived automatically from the lemma- 
tisation and inflection specification provided by 
CELEX) to the generator. 
We compared the generator output with the 
original CELEX word forms, producing a list 
of mistakes apparently made by the generator, 
which we then checked by hand. In a number 
of cases either the CELEX lemmatisation was 
wrong in that it disagreed with the relevant en- 
try in the Cambridge International Dictionary 
of English (Procter, 1995), or the output of the 
generator was correct even though it was not 
identical to the word form given in CELEX. 
We did not count these cases as mistakes. We 
also found that CELEX is inconsistent with re- 
spect to consonant doubling. For example, it 
includes the word form pettifogged, 4 whereas 
it omits many consonant doubled words that 
are much more common (according to counts 
from the BNC). For example, the BNC con- 
tains around 850 occurrences of the word form 
programming tagged as a verb, but this form 
is not present in CELEX. The form programing 
does occur in CELEX, but does not in the BNC. 
4A rare word, meaning to be overly concerned with 
small, unimportant details. 
We did not count these cases as mistakes either. 
:Of~he :r~m~i.ning: 359'.mist~kes(:346:~c0neern6d 
word forms that do not occur at all in the 100M 
words of the BNC. We categorised these as irrel- 
evant for practical applications and so discarded 
them. Thus the type accuracy of the morpho- 
logical analyser with respect o the CELEX lex- 
ical database is 99.97%. The token accuracy is 
99.98% with respect o the 14,825,661 relevant 
.tokens .i.mthe BNC .(i.e. ,at.rate ,of two errors per 
ten thousand words). 
We tested the processing speed of the gener- 
ator on a Sun Ultra 10 workstation. In order 
to discount program startup times (which are 
anyway only of the order of 0.05 seconds) we 
used input files of 400K and 800K tokens and 
recorded the difference in timings; we took the 
averages of 10 runs. Despite its wide coverage 
the morphological generator is very fast: it gen- 
erates at a rate of more than 80,000 words per 
second. 5 
3 The  Generator  in an  App l ied  
System 
3.1 Text  S impl i f icat ion 
The morphological generator forms part of a 
prototype system for automatic simplification 
of English newspaper text (Carroll et al, 1999). 
The goal is to help people with aphasia (a lan- 
guage impairment ypically occurring as a re- 
sult of a stroke or head injury) to better un- 
derstand English newspaper text. The system 
comprises two main components: an analysis 
module which downloads the source newspaper 
texts from the web and computes yntactic anal- 
yses for the sentences in them, and a simpli- 
fication module which operates on the output 
of the analyser to improve the comprehensit)il- 
ity of the text. Syntactic simplification (Can- 
ning and Tait, 1999) operates on the syntax 
trees produced in the analysis phase, for exam- 
ple converting sentences in the passive vdice to 
active, and splitting long sentences at appropri- 
ate points. A subsequent lexical simplification 
stage (Devlin and Tait, 1998) replaces difficult 
or rare content words with simpler synonyms. 
The analysis component contains a morpho- 
logical analyser, and it is the base forms of 
sit is likely that a modest increase in speed could be 
obtained by specifying optimisation levels in Flex and 
gcc that are higher than the defaults. 
204 
words that are passed through the system; this with a list of exceptions (e.g. heir, unanimous) 
? eases the task of.the texic~l.simplification t odo ,: =,eollecCed:using.the:pronunciation information in 
ule. The final processing stage in the system 
is therefore morphological generation, using the 
generator described in the previous ection. 
3.2 Appl ied Morpho log ica l  Generat ion  
We are currently testing the components of the 
simplification system on a corpus of 1000 news 
the OALDCE, supplemented by-further cases 
(e.g. unidimensional) found in the BNC. In the 
case of abbreviations or acronyms (recognised 
by the occurrence ofnon-word-initial capital et- 
ters and trailing full-stops) we key off the pro- 
nunciation of the first letter considered in isola- 
tion. 
stories downloaded from .the :Sunde!T!and Echo ....... Simi!arlyi .the orthography .of .the .genit.ive 
(a local newspaper in North-East England). In marker cannot be determined without taking 
our testing we have found that newly encoun- 
tered vocabulary only rarely necessitates any 
modification to the generator (or rather the 
analyser) source; if the word has regular mor- 
phology then it is handled by the rules express- 
ing generalisations. Also, a side-effect of the fact 
that the generator is derived from the analyser 
is that the two modules have exactly the same 
coverage and are guaranteed to stay in step with 
each other. This is important in the context of 
an applied system. The accuracy of the gener- 
ator is quite sufficient for this application; our 
experience is that typographical mistakes in the 
original newspaper text are much more common 
than errors in morphological processing. 
3.3 Or thograph ic  Postprocess ing  
Some orthographic phenomena span more than 
one word. These cannot be dealt with in mor- 
phological generation since this works strictly a 
word at a time. We have therefore implemented 
a final orthographic postpmcessing stage. Con- 
sider the sentence: 6 
(4) *Brian Cookman is the attraction at 
the K ing 's  Arms on Saturday night 
and he will be back on Sunday night 
for a acoustic jam session. 
This is incorrect orthographically because the 
determiner in the final noun phrase should be 
an, as in an acoustic jam session. In fact an 
nmst be used if the following word starts with 
a vowel sound, and a otherwise. We achieve 
this, again using a filter implemented in Flex, 
with a set of general rules keying off the next 
word's first letter (having skipped any inter- 
vening sentence-internal punctuation), together 
6This sentence is taken from the story "The demise 
of Sunder land's  Vaux Breweries is giving local musicians 
a case of the blues" publ ished in the Sunderland Ech, o 
on 26 August 1999. 
context into account, since it depends on the 
identity of the last letter of the preceding word. 
In the sentence in (4) we need only eliminate 
the space before the genitive marking, obtain- 
ing King's Arms. But, following the newspaper 
style guide, if the preceding word ends in s or z 
we have to 'reduce' the marker as in, for exam- 
ple, Stacey Edwards' skilful fingers. 
The generation of contractions presents more 
of a problem. For example, changing he will 
to he'll would make (4) more idiomatic. But 
there are cases where this type of contraction is
not permissible. Since these cases seem to be 
dependent on syntactic ontext (see Section 4 
below), and we have syntactic structure from 
the analysis phase, we are in a good position 
to make the correct choice. However, we have 
not yet tackled this issue and currently take the 
conservative approach of not contracting in any 
circumstances. 
4 Re la ted  Work  
We are following a well-established line of re- 
search into the use of finite-state techniques for 
lexical and shallow syntactic NLP tasks (e.g. 
Karttunen et al (1996)). Lexical transduc- 
ers have been used extensively for morphological 
analysis, and in theory a finite-state transducer 
implementing an analyser can be reversed to 
produce a generator. However, we are not aware 
of published research on finite-state morpho- 
logical generators (1) establishing whether in 
practice they perform with similar efficiency to 
morphological analysers, (2) quantifying their 
type/token accuracy with respect to an inde- 
pendent, extensive 'gold standard', and (3) in- 
dicating how easily they can be integrated 
into larger systems. Furthermore, although a 
number of finite-state compilation toolkits (e.g. 
t(arttunen (1994)) are publicly available or can 
205 
be licensed for research use, associated large- length trailing strings and concatenating suf- 
.scale l.inguis tic .,descriptions=-~ar,,,exa,mple=~n,-:.:~...~.. fixes ........ All ~mo~phologicaUy,..subreguta,r-~ :forms. 
glish morphological lexicons--are usually com- 
mercial products and are therefore not freely 
available to the NLG research community. 
The work reported here is-also related to 
work on lexicon representation and morpho- 
logical processing using the DATR representa- 
tion language (Cahill, 1993; Evans and Gazdar, 
must be entered explicitly in the lexicon, as well 
as irregular ones. The situation is similar in 
FUF/SURGE, morphological generation in the 
SURGE grammar (Elhadad and Robin, 1996) 
being performed by procedures which inspect 
lemma endings, strip off trailing strings when 
appropriate, and concatenate suffixes. 
.1996). 
cal and more of an engineering perspective, fo- 
cusing on morphological generation i  the con- 
text of wide-coverage practical NLG applica- 
tions. There are also parallels to research in 
the two-level morphology framework (Kosken- 
niemi, 1983), although in contrast o our ap- 
proach this framework has required exhaustive 
lexica and hand-crafted morphological (unifi- 
cation) grammars in addition to orthographic 
descriptions (van Noord, 1991; Ritchie et al, 
1992). The SRI Core Language Engine (A1- 
shawi, 1992) uses a set of declarative segmen- 
tation rules which are similar in content o our 
rules and are used in reverse to generate word 
forms. The system, however, is not freely avail- 
able, again requires an exhaustive stem lexicon, 
and the rules are not compiled into an efficiently 
executable finite-state machine but are only in- 
terpreted. 
The work that is perhaps the most similar 
in spirit to ours is that of the LADL group, in 
their compilation of large lexicons of inflected 
word forms into finite-state transducers (Mohri, 
1996). The resulting analysers run at a com- 
parable speed to our generator and the (com- 
pacted) executables are of similar size. How- 
ever, a full form lexicon is unwieldy and incon- 
venient o update: and a system derived from it 
cannot cope gracefully with unknown words be- 
cause it does not contain generalisations about 
regular or subregular morphological behaviour. 
The morphological components of current 
widely-used NLG systems tend to consist of 
hard-wired procedural code that is tightly 
bound to the workings of the rest of the system. 
For instance, the Nigel grammar (Matthiessen, 
1984) contains Lisp code that classifies verb, 
noun and adjective endings, and these classes 
are picked up by further code inside the t<PML 
system (Bateman, 2000) itself which performs 
inflectional generation by stripping off variable 
However,.. we,.~adopt .less ..of .a~.theoreti~ .... -..,.,.Jn~ eLtr~ent~.,NI,G~-.systerns,~or.#hographic 4nfor- 
mation is distributed throughout he lexicon 
and is applied via the grammar or by hard-wired 
code. This makes orthographic processing dif- 
ficult to decouple from the rest of the system, 
compromising maintainability and ease of reuse. 
For example, in SURGE, markers for alan us- 
age can be added to lexical entries for nouns to 
indicate that their initial sound is consonant- 
or vowel-like, and is contrary to what their or- 
thography would suggest. (This is only a partial 
solution since adjectives, adverbs--and more 
rarely other parts of speech--can follow the in- 
definite article and thus need the same treat- 
ment). The appropriate indefinite article is in- 
serted by procedures associated with the gram- 
mar. In DRAFTER-2 (Power et al, 1998), an 
alan feature can be associated with any lex- 
ical entry, and its value is propagated up to 
the NP level through leftmost rule daughters in 
the grammar (Power, personal communication). 
Both of these systems interleave orthographic 
processing with other processes in realisation. 
In addition, neither has a mechanism for stat- 
ing exceptions for whole subclasses of words, for 
example those starting us followed by a vowel-- 
such as use and usua l - -wh ich  must be preceded 
by a. KPML appears not to perform this type 
of processing at all. 
We are not aware of any literature describing 
(practical) NLG systems that generate contrac- 
tions. However, interesting linguistic research in 
this direction is reported by Pullmn and Zwicky 
(In preparation),. This work investigates tile un- 
derlying syntactic structure of sentences that 
block auxiliary reductions, for example those 
with VP ellipsis as in (5). 
(5) *She's usually home wh, en he's. 
206 
5 Conc lus ions  provided to us by the University of Sheffield 
We have described a generatorf0r English in:: ' G.A~E ~projoet-,...:(3hris -Brew,,.Dale- Gerdem:an.n~.. 
flectional morphology. The main features of the Adam Kilgarriff and Ehud Reiter have sug- 
generator are: 
wide coverage and high accuracy It in- 
corporates data from several large corpora 
and machine readable dictionaries. An 
evaluation has shown the error rate to be 
very low. 
robustness The generator does not contain 
an explicit lexicon or word-list, but instead 
comprises a set of morphological generali- 
sations together with a list of exceptions for 
specific (irregular) words. Unknown words 
are very often handled correctly by the gen- 
eralisations. 
maintainabi l i ty and ease of use The or- 
ganisation into generalisations and excep- 
tions can save development time since ad- 
dition of new vocabulary that has regular 
morphology does not require any changes 
to be made. The generator is packaged up 
as a Unix filter, making it easy to integrate 
into applications. 
speed and portabi l i ty The generator is 
based on efficient finite-state techniques, 
and implemented using the widely available 
Unix Flex utility. 
freely available The morphological gener- 
ator and the orthographic postproces- 
sor are fi'eely available to the NLG re- 
search community. See <http://www.cogs. 
susx.ac.uk/lab/nlp/carroll/morph.html>. 
In future work we intend to investigate the 
use of phonological information in machine 
readable dictionaries for a more principled so- 
lution to the consonant doubling problem. We 
also plan to further increase the flexibility of 
the generator by including an option that al- 
lows the user to choose whether it has a prei~r- 
ence for generating British or American English 
spelling. 
Acknowledgements  
This work was fimded by UK EPSRC project 
GR/L53175 'PSET: Pra(:tical Simplification of 
English Text', and by all EPSRC Advanced Fel- 
lowship to tim second author. The original ver- 
sion of t.lw morl)hol~gi('al nalyscr was kindly 
gested improvements to the analyser/generator. 
Thanks also to the anonymous reviewers for in: 
sightful comments. 
References  
Alfred Aho, Ravi Sethi, and Jeffrey Ullman. 
..... ..~ 1986..=-?:ompilers,: ~Principles,~Techniques and
Tools. Addison-Wesley. 
Hiyan Alshawi, editor. 1992. The Core Lan- 
guage Engine. MIT Press, Cambridge, MA. 
Harald Baayen, Richard Piepenbrock, and Hed- 
derik van Rijn. 1993. The CELEX Lexi- 
cal Database (CD-ROM). Linguistic Data 
Consortium, University of Pennsylvania, 
Philadelphia, PA, USA. 
John Bateman. 2000. KPML (Version 3.1) 
March 2000. University of Bremen, Germany, 
< http://www.fbl0.uni-bremen.de/anglistik/ 
langpro/kpml/README.html>. 
Lou Burnard. 1995. Users reference guide for 
the British National Corpus. Technical re- 
port, Oxford University Computing Services. 
Lynne Cahill. 1993. Morphonology in the lex- 
icon. In Proceedings of the 6th Conference 
of the European Chapter of the Association 
for Computational Linguistics, pages 87-96, 
Utrecht, The Netherlands. 
Yvonne Canning and John Tait. 1999. Syntac- 
tic simplification of newspaper text for apha- 
sic readers. In Proceedings ofthe ACM SIGIR 
Workshop on Customised Information Deliv- 
ery, Berkeley, CA, USA. 
John Carroll, Guido Minnen, Darren Pearce, 
Yvonne Canning, Siobhan Devlin, and John 
Tait. 1999. Simplifying English text for lan- 
guage impaired readers. In Pwceedings of the 
9th Conference of th, e European Chapter of 
the Association for Computational Ling.uis- 
tics (EACL), Bergen, Norway. 
Hamish Cunningham, Yorick Wilks, and Robert 
Gaizauskas. 1996. GATE--a GenerM Archi- 
tecture for Text Engineering. In Proceed- 
ings of the 16th Conference on Computational 
Linguistics, Copenhagen, Denmark. 
Siobhan Devlin and John Tait. 1998. The use 
of a psychotinguistic database in the simpli- 
fication of text for aphasic readers. In (Ner- 
bonne. 1998). 
207 
Michael Elhadad and Jacques Robin. 1996. An 
overview of SU-KGE:..A ~eusable~,.eomprehen- 
sive syntactic realization component. Tech- 
nical Report 96-03, Dept of Mathematics and 
Computer Science, Ben Gurion University, Is- 
rael. 
Roger Evans and Gerald Gazdar. 1996. DATR: 
a language for lexical knowledge representa- 
tion. Computational Linguistics, 22. 
Roger Garside, Ge.off;ey.. _Leech, and...Geoffrey 
Sampson. 1987. The computational nalysis 
of English: a corpus-based approach. Long- 
man, London. 
Lauri Karttunen, Jean-Pierre Chanod, Gregory 
Grefenstette, and Anne Schiller. 1996. Regu- 
lar expressions for language ngineering. Nat- 
ural Language Engineering, 2(4):305-329. 
Lauri Karttunen. 1994. Constructing lexical 
transducers. In Proceedings of the 14th Con- 
ference on Computational Linguistics, pages 
406-411, Kyoto, Japan. 
Kimmo Koskenniemi. 1983. Two-level model 
for morphological analysis. In 8th Interna- 
tional Joint Conference on Artificial Intelli- 
gence, pages 683-685, Karlsruhe, Germany. 
John Levine, Tony Mason, and Doug Brown. 
1992. Lex ~4 Yacc. O'Reilly and Associates, 
second edition. 
Mitch Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Build- 
ing a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313-330. 
Christian Matthiessen. 1984. Systemic Gram- 
mar in computation: The Nigel case. In Pro- 
ceedings of the 1st Conference of the European 
Chapter of the Association for Computational 
Linguistics, pages 155-164, Pisa, Italy. 
George Miller, Richard Beckwith, Christiane 
Fellbaum, Derek Gross, Katherine Miller, and 
Randee Tengi. 1993. Five Papers on Word- 
Net. Princeton University, Princeton, N.J. 
Guido Minnen and John Carroll. Under review. 
Past and robust morphological processing tools 
for practical NLP applications. 
Roger Mitton. 1992. A description of a 
computer-usable dictionary file based on 
the Oxford Advanced Learner's Dictio- 
nary of Current English. Availat)le at 
< ftp: / / ota.ox.ac.uk / pub /ota/ pub lic / d icts / 710 /
text710.doe: >.
Mehryar Mohri. 1996. On some applications of 
.......~ :fmittee-:sta, t e -automata,.-.t heeory.,.~tox.:natu.ea, l~lam.- 
guage processing. Natural Language Engi- 
neering, 2(1):61-80. 
John Nerbonne, editor. 1998. Linguistic 
Databases. Lecture Notes. CSLI Publica- 
tions, Stanford, USA. 
Richard Power, Donia Scott, and Roger Evans. 
1998. What You See Is What You Meant: di- 
rect knowledge diting~with natu~aLlanguage 
feedback. In Proceedings of the 13th Bien- 
nial European Conference on Artificial Intel- 
ligence (ECAI 98), Brighton, UK. 
Paul Procter. 1995. Cambridge International 
Dictionary of English. Cambridge University 
Press. 
Geoffrey Pullum and Arnold Zwicky. In prepa- 
ration. Licensing of prosodic features by 
syntactic rules: the key to auxiliary reduc- 
tion. First version presented to the Annual 
Meeting of the Linguistic Society of America, 
Chicago, Illinois, January 1997. Available at 
< http://www.lsadc.org/web2/99modabform.ht 
Philip Quinlan. 1992. The Oxford Psycholin- 
guistic Database. Oxford University Press. 
Graeme Ritchie, Graham Russell, Alan Black, 
and Stephen Pulman. 1992. Computational 
morphology: practical mechanisms for the 
English lexicon. MIT Press. 
Geoffrey Sampson. 1995. English for the com- 
puter. Oxford University Press. 
Stuart Shieber, Gertjan van Noord, Robert 
Moore, and Fernando Pereira. 1990. Seman- 
tic head-driven generation. Computational 
Linguistics, 16(1):7-17. 
Lita Taylor and Gerry Knowles. 1988. Man- 
ual of information to accompany the SEC 
Corpus: the machine-readable corpus of spo- 
ken English. Manuscript, Urfiversity of Lan- 
caster. UK. 
Gertjan van Noord. 1991. Morphology in 
MiMo2. Manuscript, University of Utrecht, 
The Netherlands. 
208 
Using an open-source unification-based system for CL/NLP teaching
Ann Copestake
Computer Laboratory
University of Cambridge
Cambridge, UK
aac@cl.cam.ac.uk
John Carroll
Cognitive and Computing Sciences
University of Sussex
Falmer, Brighton, UK
johnca@cogs.susx.ac.uk
Dan Flickinger
CSLI, Stanford University and
YY Software
Ventura Hall,
Stanford, USA
danf@csli.stanford.edu
Robert Malouf
Alfa Informatica,
University of Groningen,
Postbus 716, 9700 AS Groningen,
The Netherlands
malouf@let.rug.nl
Stephan Oepen
YY Software and
CSLI, Stanford University
110 Pioneer Way
Mountain View, USA
oe@yy.com
Abstract
We demonstrate the open-source LKB
system which has been used to teach the
fundamentals of constraint-based gram-
mar development to several groups of
students.
1 Overview of the LKB system
The LKB system is a grammar development
environment that is distributed as part of the
open source LinGO tools (http://www-
csli.stanford.edu/?aac/lkb.html
and http://lingo.stanford.edu, see
also Copestake and Flickinger, 2000). It is an
open-source grammar development environment
implemented in Common Lisp, distributed not
only as source but also as a standalone application
that can be run on Linux, Solaris and Windows
(see the website for specific requirements). It
will also run under Macintosh Common Lisp,
but for this a license is required. The LKB in-
cludes a parser, generator, support for large-scale
inheritance hierarchies (including the use of
defaults), various tools for manipulating semantic
representations, a rich set of graphical tools
for analyzing and debugging grammars, and
extensive on-line documentation. Grammars of
all sizes have been written using the LKB, for
several languages, mostly within the linguistic
frameworks of Categorial Grammar and Head-
Driven Phrase Structure Grammar. The LKB
system was initially developed in 1991, but has
gone through multiple versions since then. It
is in active use by a considerable number of
researchers worldwide. An introductory book
on implementing grammars in typed feature
structure formalisms using the LKB is near
completion (Copestake, in preparation).
2 Demo outline
Although the LKB has been successfully used for
large-scale grammar development, this demon-
stration will concentrate on its use with relatively
small scale teaching grammars, of a type which
can be developed by students in practical exer-
cises. We will show an English grammar frag-
ment which is linked to a textbook on formal syn-
tax (Sag and Wasow, 1999) to illustrate how the
system may be used in conjunction with more tra-
ditional materials in a relatively linguistically ori-
ented course. We will demonstrate the tools for
analyzing parses and for debugging and also dis-
cuss the way that parse selection mechanisms can
be incorporated in the system. If time permits, we
will show how semantic analyses produced with a
somewhat more complex grammar can be linked
up to a theorem prover and also exploited in se-
mantic transfer for Machine Translation. Exer-
cises where the grammar is part of a larger system
are generally appropriate for advanced courses or
for NLP application courses.
The screen dump in the figure is from a session
working with a grammar fragment for Esperanto.
This shares its basic types and rules with the
English textbook grammar fragment mentioned
above. The windows shown are:
1. The LKB Top interaction window: main
  
 
 
 
Figure 1: Screen dump of the LKB system
menus plus feedback and error messages
2. Type hierarchy window (fragment): the
more general types are on the left. Nodes in
the hierarchy have menus that provide more
information about the types, such as their as-
sociated constraints.
3. Type constraint for the type intrans-verb:
again nodes are clickable for further infor-
mation.
4. Parse tree for La knabo dormas (the boy
sleeps): a larger display for parse trees is
also available, but this scale is useful for
summary information. Menus associated
with trees allow for display of associated se-
mantic information if any is included in the
grammar and for generation. Here the dis-
play shows inflectional rules as well as nor-
mal syntactic rules: hence the VP node un-
der dormas, which corresponds to the stem.
5. In the middle is an emacs window displaying
the source file for the lexicon associated with
this grammar.1 It shows the entry for the lex-
1(We generally use emacs as an editor when teaching,
eme dorm, which, like most lexical entries in
this grammar, just specifies a spelling and a
type (here intrans-verb).
6. Part of the parse chart corresponding to the
tree is shown in the bottom window: nodes
which have knabo as a descendant are high-
lighted. Again, these nodes are active: one
very useful facility associated with them is a
unification checker which allows the gram-
mar writer to establish why a rule did not
apply to a phrase or phrases.
3 Use of the LKB in teaching
Teaching uses of the LKB have included under-
graduate and graduate courses on formal syntax
and on computational linguistics at several sites,
grammar engineering courses at two ESSLLI
summer schools, and numerous student projects
at undergraduate, masters and doctoral levels. An
advantage of the LKB is that students learn to use
a system which is sufficiently heavy duty for more
advanced work, up to the scale at least of research
although this causes some overhead, especially for students
who are only used to word processing programs.
prototypes. This provides them with a good plat-
form on which to build for further research. Feed-
back from the courses we have taught has mostly
been very positive, but we have found a ratio of
six students to one instructor (or teaching assis-
tant) to be the maximum that is workable. One
major reason is that debugging students? gram-
mars and teaching debugging techniques is time-
consuming.
When teaching an introductory course with the
LKB, we start the students off with a very sim-
ple grammar, which they are asked to expand
in specific ways. We introduce various addi-
tional techniques and formal devices (such as in-
flectional and lexical rules, defaults, difference
lists and gaps) gradually during a course. Mate-
rial from our ESSLLI courses, including starting
grammars, exercises and solutions is distributed
via the website. Several other small grammars
developed by students are also distributed as part
of the LKB system and we would welcome fur-
ther contributions. We are hoping to facilitate this
by making it easier for people outside the LinGO
group to add and modify grammars.
Several graduate students have used versions
of the LKB system as part of their thesis work,
for diverse projects including machine transla-
tion and grammar learning. It has been used
in the development of several large grammars,
especially the LinGO English Resource Gram-
mar (ERG), which is itself open-source. Re-
search applications for the ERG include spoken
language machine translation in Verbmobil, gen-
eration for a speech prosthesis, and automated
email response, under development for commer-
cial use. The LKB/ERG combination can be used
by researchers who require a grammar which pro-
vides a detailed semantic analysis and reason-
ably broad coverage, for instance for experiments
on dialogue. The LKB has also been used as
a grammar preprocessor to facilitate experiments
on efficiency using the ERG with other systems
(Flickinger et al 2000).
4 Comparison with other work
There is a long history of the use of fea-
ture structure based systems in teaching, dat-
ing back at least to PATR (Shieber, 1986:
see http://www.ling.gu.se/?li/). The
Alvey Natural Language Tools (Briscoe et al
1987) have been used for teaching at several uni-
versities: Briscoe and Grover developed an ex-
tensive set of teaching examples and exercises,
which is however unpublished. Versions of the
SRI Core Language Engine (Alshawi, 1992) and
of the XTAG grammar (XTAG group, 1995) and
parser have also been used for teaching. Besides
the LKB, typed feature structure environments
have been used at many universities, though un-
like the systems cited above, most have only been
used with small grammars and may not scale
up. Hands on courses using various systems have
been run at many recent summer schools includ-
ing ESSLLI 99 (using the Xerox XLE, see Butt
et al 1999) and ESSLLI 97 and the 1999 LSA
summer school (both using ConTroll, see Hin-
richs and Meurers, 1999). Very little seems to
have been formally published describing expe-
riences in teaching with grammar development
environments, though Bouma (1999) describes
material for teaching a computational linguistics
course that includes exercises using the Hdrug
unification-based enviroment to extend a gram-
mar.
Despite this rich variety of tools, we believe
that the LKB system has a combination of fea-
tures which make it distinctive and give it a useful
niche in teaching. The most important points are
that its availability as open source, combined with
scale and efficiency, allow advanced projects to be
supported as well as introductory courses. As far
as we are aware, it is the only system freely avail-
able with a broad coverage grammar that sup-
ports semantic interpretation and generation. Es-
pecially for more linguistically oriented courses,
the link to the Sag and Wasow textbook is also
important. Similar grammars could be developed
for other systems, but would be less directly com-
parable to the textbook since this assumes a de-
fault formalism which so far is only implemented
in the LKB.
On the other hand, the LKB is not a suitable ba-
sis for a course that involves the students learning
to implement a unifier, parser and so on. The sys-
tem is quite complex (about 120 files and 40,000
lines of Lisp code) and though the vast majority
of this is concerned with non-core functionality,
such as the graphical interfaces, it is still some-
what daunting. This seems an inevitable trade-
off of having a system powerful enough for real
applications (see Bouma (1999) for related dis-
cussion). It is questionable whether the LKB is
entirely satisfactory as a student?s first computa-
tional grammar system, although we have used it
with students who have no prior experience of this
sort: ideally we would suggest starting off with
brief exercises with a pure context-free grammar
to explain the concepts of well-formedness, re-
cursion and so on. We also wouldn?t necessar-
ily advocate using the LKB as a core component
of a first course on formal syntax for linguistic
students, since the specifics of dealing with an
implementation may interfere with understanding
of basic concepts, though it is suitable as a sup-
plement to an initial course or as the basis for a
slightly more advanced course.
We think there is considerable potential for
building materials for courses that allow students
to work with realistic but transparent applications
using the LKB and a large grammar as a compo-
nent. Developing such materials is clearly nec-
essary in order to give students useful practical
experience. It is however very time-consuming,
and most probably will have to be undertaken as
part of a cooperative, open-source development
involving people from several different institu-
tions.
Acknowledgements
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. The current versions of the English
grammars associated with the Sag and Wasow
textbook were largely developed by Christopher
Callison-Burch while he was an undergraduate at
Stanford.
References
Alshawi, Hiyan (ed). [1992] The Core Language
Engine, MIT Press, Cambridge, MA.
Bouma, Gosse. [1999] ?A modern computa-
tional linguistics course using Dutch.? In Frank
van Eynde and Ineke Schuurman, editors, CLIN
1998, Papers from the Ninth CLIN Meeting, Am-
sterdam. Rodopi Press.
Briscoe, Ted, Claire Grover, Bran Boguraev
and John Carroll. [1987] ?A formalism and en-
vironment for the development of a large gram-
mar of English?, Proceedings of the 10th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-87), Milan, Italy, 703?708.
Butt, Miriam, Anette Frank and Jonas Kuhn.
[1999] ?Development of large scale LFG gram-
mars ? Linguistics, Engineering and Resources?,
http://www.xrce.xerox.com/people/
frank/esslli99-hp/index.html
Copestake, Ann. [in preparation] Implementing
typed feature structure grammars, CSLI Publica-
tions, Stanford.
Copestake, Ann and Dan Flickinger. [2000]
?An open-source grammar development environ-
ment and broad-coverage English grammar us-
ing HPSG?, Second conference on Language Re-
sources and Evaluation (LREC-2000), Athens,
Greece.
Flickinger, Daniel, Stephan Oepen, Hans
Uszkoreit and Jun?ichi Tsujii. [2000] Journal of
Natural Language Engineering. Special Issue on
Efficient Processing with HPSG: Methods, Sys-
tems, Evaluation, 6(1).
Hinrichs, Erhard and Detmar Meurers [1999]
?Grammar Development in Constraint-Based
Formalisms?,
http://www.ling.ohio-state.edu/
?dm/lehre/lsa99/material.html,
see also http://www.sfs.nphil.uni-
tuebingen.de/controll/
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
The XTAG Research Group [1995]. ?A Lex-
icalized Tree Adjoining Grammar for English?
IRCS Report 95-03, University of Pennsylvania?
MEANING: a Roadmap to Knowledge Technologies 
 
German Rigau. TALP Research Center. UPC. Barcelona. rigau@lsi.upc.es 
Bernardo Magnini. ITC-IRST. Povo-Trento. magnini@itc.it 
Eneko Agirre. IXA group. EHU. Donostia. eneko@si.ehu.es  
Piek Vossen. Irion Technologies. Delft. Piek.Vossen@irion.nl  
John Carroll. COGS. U. Sussex. Brighton. johnca@cogs.susx.ac.uk 
 
Abstract  
Knowledge Technologies need to extract 
knowledge from existing texts, which 
calls for advanced Human Language 
Technologies (HLT). Progress is being 
made in Natural Language Processing but 
there is still a long way towards Natural 
Language Understanding. An important 
step towards this goal is the development 
of technologies and resources that deal 
with concepts rather than words. The 
MEANING project argues that we need to 
solve two complementary and 
intermediate tasks to enable the next 
generation of intelligent open domain 
HLT application systems: Word Sense 
Disambiguation and large-scale 
enrichment of Lexical Knowledge Bases. 
Innovations in this area will lead to HLT 
with deeper understanding of texts, and 
immediate progress in real applications of 
Knowledge Technologies. 
Introduction 
The field of Information Society Technologies 
(IST) is one of the main thematic priorities of 
the European Commission for the 6th Framework 
programme. In this field, Knowledge 
Technologies (KT) aim to provide meaning to 
the petabytes of information content our 
societies will generate in the near future. 
Information and knowledge management 
systems need to evolve accordingly, to enable 
the next generation of intelligent open domain 
Human Language Technologies (HLT) that will 
deal with the growing potential of the 
knowledge-rich and multilingual society. 
In order to develop a trustable semantic web 
infrastructure and a multilingual ontology 
framework to support knowledge management a 
wide range of techniques are required to 
progressively automate the knowledge lifecycle. 
In particular, this involves extracting high-level 
meaning from the large collections of content 
data and its representation and management in a 
common knowledge base. 
Even now, building large and rich knowledge 
bases takes a great deal of expensive manual 
effort; this has severely hampered Knowledge-
Technologies and HLT application development. 
For example, dozens of person-years have been 
invest into the development of wordnets1 for 
various languages, but the data in these 
resources is still not sufficiently rich to support 
advanced concept-based HLT applications 
directly. Furthermore, resources produced by 
introspection usually fail to register what really 
occurs in texts. Applications will not scale up to 
working in the open domain without more 
detailed and rich general-purpose, which should 
perhaps include domain-specific linguistic 
knowledge.  
The MEANING project identifies two 
complementary intermediate tasks which we 
think are crucial in order to enable the next 
generation of intelligent open domain HLT 
application systems: Word Sense 
Disambiguation (WSD) and large-scale 
enrichment of Lexical Knowledge Bases.  
                                                     
1 A wordnet is a conceptually structured knowledge 
base of word senses. The English WordNet (Miller 
90, Fellbaum 98) has been developed at Princeton 
University over the past 14 years. EuroWordNet 
(Vossen 1998) is a multilingual database with 
wordnets for several European languages (Dutch, 
Italian, Spanish, German, French, Czech and 
Estonian). Balkanet is building wordnets for the 
Balkan languages following the EuroWordNet 
design. 
The advance in these two areas will allow for 
large-scale extractions of shallow meaning from 
texts, in the form of relations among concepts. 
WSD provides the technology to convert 
relations between words into relations between 
concepts. Rich and large-scale Lexical 
Knowledge Bases will have be the repositories 
of extracted relations and other linguistic 
knowledge.  
However, progress is difficult due to the 
following interdependence: 
? In order to achieve accurate WSD, we need 
far more linguistic and semantic knowledge 
than is available in current lexical 
knowledge bases (e.g. current wordnets).  
? In order to enrich Lexical Knowledge Bases 
we need to acquire information from 
corpora, which have been accurately tagged 
with word senses.  
Providing innovative technology to solve this 
problem will be one of the main challenges to 
access KTs.  
Following this introduction section 1 presents 
the major research goals in HLT. Section 2 
presents the MEANING roadmap. Finally, 
section 4 draws the conclusions. 
1 Major research goals in HLT 
In order to extend the state-of-the-art in human 
language technologies (HLT) future research 
must devise: (1) innovative processes and tools 
for automatic acquisition of lexical knowledge 
from large-scale document collections; (2) novel 
techniques for accurately selecting the sense of 
open-class words in a large number of 
languages; (3) ways to enrich existing 
multilingual linguistic knowledge resources with 
new kinds of lexical information by 
automatically mapping information across 
languages. We present each one in turn. 
1.1 Dealing with knowledge acquisition 
The acquisition of linguistic knowledge from 
corpora has been a very successful line of 
research. Research in the acquisition of 
subcategorization information, selectional 
preferences, in thematic role assignments and 
diathesis alternations (Agirre and Mart?nez 
2001, 2002, McCarthy and Korhonen, 1998; 
Korhonen et al, 2000; McCarthy 2001), domain 
information (Magnini and Cavagli? 2000), topic 
signatures (Agirre et al 2001b), lexico-semantic 
relations between words (Agirre et al 2002) etc. 
has obtained encouraging results. The 
acquisition process usually involves large bodies 
of text, which have been previously processed 
with shallow language processors.  
Much of the use of the acquired knowledge 
has been hampered by the fact that the texts are 
not sense-disambiguated, and therefore, only 
knowledge for words can be acquired, that is, 
subcategorization for words, selectional 
preferences for words, etc. It is a well 
established fact that much of the linguistic 
behavior of words can be better explained if it is 
keyed to word senses.  
For instance, the subcategorization frames of 
verbs are highly dependent of the sense of the 
verb. Some senses of a given verb allow for a 
particular combination of complements, while 
others do not (McCarthy, 2001). The same is 
applicable to selectional preferences; traditional 
approaches that learn selectional preferences for 
a verb, tend to mix e.g. all subjects for differents 
senses, even if verbs can have different 
selectional preferences for each word sense 
(Agirre & Martinez, 2002). 
Having texts automatically sense-tagged with 
high accuracy will produce significantly better 
acquired knowledge at a sense level, including 
subcategorization frequencies, domain 
information, topic signatures, selectional 
preferences, specific lexico-semantic relations, 
thematic role assignments and diathesis 
alternations. It will also facilitate the 
investigation on automatic methods for dealing 
with new senses not present in current wordnets 
and clustering of word senses. Furthermore, 
linguistic information keyed to word senses that 
are linked to interlingual concepts (as proposed 
in the EuroWordNet model), can be easily 
integrated in a multilingual Lexical Knowledge 
Base (cf. section 2.3) 
2.2 Dealing with WSD 
Word Sense Disambiguation (WSD) is the task 
of assigning the appropriate meaning (sense) to a 
given word in a text or discourse. Ide and 
Veronis (1998) argue that word sense ambiguity 
is a central problem for many established HLT 
applications (for example Machine Translation, 
Information Extraction and Information 
Retrieval). This is also the case for associated 
sub-tasks (i.e. reference resolution and parsing). 
For this reason many international research 
groups are working on WSD, using a wide range 
of approaches. However, no large-scale broad-
coverage accurate WSD system has been built 
up to date2. With current state-of-the-art 
accuracy in the range 60-70%, WSD is one of 
the most important open problems in Natural 
Language Processing. 
A promising current line of research uses 
semantically annotated corpora to train Machine 
Learning (ML) algorithms to decide which word 
sense to choose in which contexts. The words in 
these annotated corpora are tagged manually 
with semantic classes taken from a particular 
lexical semantic resource (most commonly 
WordNet). Many standard ML techniques have 
been tried, such as Bayesian learning, Exemplar 
based learning, Decision Lists, and recently 
margin-based classifiers like Boosting and 
Support Vector Machines (Escudero et al, 
2000a, 2000b, 2000c, 2000d, 2001; Mart?nez 
and Agirre, 2000). These approaches are termed 
"supervised" because they learn from previously 
sense annotated data and therefore they require a 
large amount of human intervention to annotate 
the training data. 
Supervised WSD systems are data hungry. 
They suffer from the "knowledge acquisition 
bottleneck", it takes them mere seconds to digest 
all of the processed corpus contained in training 
materials that take months to annotate manually. 
So, although Machine Learning classifiers are 
undeniably effective, they are not feasible until 
we can obtain reliable unsupervised training 
data. Ng (1997) estimates that the manual 
annotation effort necessary to build a broad 
coverage word-sense annotated English corpus 
is about 16 person-years; and this effort would 
have to be replicated for each different language. 
Unfortunately, many people think that Ng?s 
estimate might fell short, as the annotated corpus 
thus produced is not guaranteed to enable high 
accuracy WSD.  
Some recent work is focusing on reducing 
the acquisition cost and the need for supervision 
                                                     
2 See the conclusions of the SENSEVAL-2 
competition: http://www.sle.sharp.co.uk/senseval2/ 
in corpus-based methods for WSD. Leacock et 
al. (1998) and Mihalcea and Moldovan (1999) 
automatically generate arbitrarily large corpora 
for unsupervised WSD training, using the 
synonyms or definitions of word senses 
provided in WordNet to formulate search engine 
queries over the Web. In another line of 
research, (Yarowsky, 1995) and (Blum and 
Mitchell, 1998) have shown that it is possible to 
reduce the need for supervision with the help of 
large amounts of unannotated data. Applying 
these ideas, (Agirre and Mart?nez, 2000) has 
developed knowledge-based prototypes for 
obtaining accurate examples from the web for 
specific WordNet synsets, as well as, large 
quantities of unannotated examples. 
But in order to make significant advances in 
WSD system accuracy, systems need to be able 
to use types of lexical knowledge that are not 
currently available in wide-coverage lexical 
knowledge bases: for example subcategorisation 
frequencies for predicates (particularly verbs) 
rely on word senses, selectional preferences of 
predicates for classes of arguments, amongst 
others (Carroll and McCarthy, 2000; McCarthy 
et al, 2001; Agirre and Mart?nez, 2002;).  
2.3 Dealing with multilingualism  
Language diversity is at the same time a 
valuable cultural heritage worth preserving, and 
an obstacle to achieving a more cohesive social 
and economic development. This situation has 
been further stressed as a major challenge in IST 
research lines. Improving language 
communication capabilities is a prerequisite for 
increasing industrial competitiveness, this way 
leading to a sound growth in key economic 
sectors.  
However, this obstacle can be helpful 
because all languages realize the meaning in 
different ways. We can benefit from this fact 
using a novel multilingual mapping process that 
exploits the EuroWordNet architecture. In 
EuroWordNet local wordnets are linked via an 
Inter-Lingual-Index (ILI) allowing the 
connection from words in one language to 
translation equivalent words in any of the other 
languages. In that way, technological advances 
in one language can help the other.  
For instance, for Basque, being an 
agglutinative language with very rich 
morphological-syntactic information, it is 
possible to extract semantic relations that would 
be more difficult to capture in other languages. 
Below we can see an example of the relation 
betwewen silversmith and silver, extracted from 
the Basque words zilargile ? zilar respectively. 
This relation has been disambiguated into the 
?maker_of? lexico-semantic relation (Agirre & 
Lersundi, 2000).  
On the contrary, Basque is not largely present 
in the web as the others. Using this approach it is 
possible to balance both gaps.  
Although the technology to provide 
compatibility across wordnets exits (Daud? et al 
1999, 2000, 2001), new research is needed for 
porting and uploading the various types of 
knowledge across languages, and new ways to 
test the validity of the ported knowledge in the 
target languages.  
3. The MEANING Roadmap 
The improvements mentioned above have been 
explored separately with relative success. In 
fact, no research group in isolation has tried to 
combine all this aforementioned factors. We 
designed the MEANING project3 convinced that 
only a combination of all relevant knowledge 
and resources will be able to produce significant 
advances in this crucial research area.  
MEANING will treat the web as a (huge) 
corpus to learn information from, since even the 
largest conventional corpora available (e.g. the 
Reuters corpus, the British National Corpus) are 
not large enough to be able to acquire reliable 
information in sufficient detail about language 
behaviour. Moreover, most languages do not 
have large or diverse enough corpora available. 
MEANING proposes an innovative 
bootstrapping process to deal with the inter-
dependency between WSD and knowledge 
acquisition: 
1. Train accurate WSD systems and apply 
them to very large corpora by coupling 
knowledge-based techniques on the existing 
EuroWordNet (e.g. to populate it with 
domain labels, to induce automatically 
                                                     
                                                     3 Started in March 2002, MEANING IST-2001-
34460 "Developing Multilingual Web-scale 
Language Technologies" is a three years research 
project funded by the EC. 
training examples) with ML techniques that 
combine very large amounts of labeled and 
unlabeled data. When ready, use also the 
knowledge acquired in 2. 
2. Use the obtained accurate WSD data in 
conjunction with shallow parsing techniques 
and domain tagging to extract new linguistic 
knowledge to incorporate into 
EuroWordNet. 
This method will be able to break this 
interdependency in a series of cycles thanks to 
the fact that the WSD system will be based on 
all domain information, sophisticated linguistic 
knowledge, large numbers of automatically 
tagged examples from the web, and a 
combination of annotated and unannotated data. 
The first WSD system will have weaker 
linguistic knowledge, but the sole combination 
of the rest of the factors will produce significant 
performance gains. Besides, some of the 
required linguistic knowledge can be acquired 
from unnanotated data, and can therefore be 
acquired without using any WSD system. Once 
acceptable WSD is available, the acquired 
knowledge will be of a higher quality, and will 
allow for better WSD performance. 
Multilingualism will be also helpful for 
MEANING. The idiosyncratic way the meaning 
is realised in a particular language will be 
captured and ported to the rest of languages 
involved in the project4 using EuroWordNet as a 
Multilingual Central Repository in three 
consecutive phases (see figure 1). 
For instance, selectional preferences acquired 
for verb senses based on the English corpora, 
can be uploaded into the Multilingual Central 
Repository. As the selectional prefenrece 
relation is keyed to concepts in the repository, 
this knowledge can be ported to the other 
languages. Of course, the ported knowledge 
needs to be checked in order to evaluate the 
validity of this approach.  
Below, we can see the selectional preference 
for the first sense of know from (Agirre & 
martinez, 2002). The first sense of know is 
univocally linked to <know, cognize,
cognise>, which in EuroWordNet is linked to 
4 MEANING will work with three major European 
languages (English, Spanish and Italian) and two 
minority languages (Catalan and Basque).  
w
S
a
B
s
0
0
0
0
0
4
W
s
m
p
m
c
e
Multilingual Central Repository 
EANING is going to constitute 
wledge resource for a number of 
sses that need large amounts of 
to be effective tools (e.g. web 
P tools and software of the next 
l benefit from the MEANING 
Multilingual
Central Repository
Italian
EWN
Basque
EWN
Spanish
EWN
English
EWN
Basque
Web Corpus
Italian
Web Corpus
English
Web Corpus
Catalan
EWN
Spanish
Web Corpus
Catalan
Web Corpus
ACQ
ACQACQ
ACQ
UPLOADUPLOAD
UPLOADUPLOAD
PORT
PORT
PORT
PORT
WSD
WSD
WSD
WSD
 access applications are based on 
NG will open the way for access 
gual web based on concepts, 
lications with capabilities that 
ceed those currently available. 
ill facilitate development of 
pen domain Internet applications 
tion/Answering, Cross Lingual 
etrieval, Summarisation, Text 
Event Tracking, Information 
achine Translation, etc.). 
EANING will supply a common 
cture to Internet documents, thus 
owledge management of web 
ommon conceptual structure is a ord senses conocer_1 and saber_1 in 
panish, con?ixer_1 and saber_1 in Catalan 
nd antzeman_1, jakin_2 and ezagutu_1 in 
asque.  
ense 1: know, cognize -- (be
cognizant or aware of a fact or a
specific piece of information;
possess knowledge or information
about;
,1128 <communication> 
,0615 <measure quantity amount quantum> 
,0535 <attribute> 
,0389 <object physical_object> 
,0307 <cognition knowledge> 
 Conclusions 
here the acquisition of knowledge  from large-
cale document collections will be  one of the 
ajor challenge for the next generation of text 
rocessing applications, MEANING emphasises 
ultilingual  content-based access to web 
ontent. Moreover, it can provide a keystone 
nabling technologies for the semantic web. In 
particular, the 
produced by M
the natural kno
semantic proce
linguistic data 
ontologies). NL
generation wil
outcomes.  
Figure 1: MEANING data flow. 
Current web
words; MEANI
to the multilin
providing app
significantly ex
MEANING w
concept-based o
(such as Ques
Information R
Categorisation, 
Extraction, M
Furthermore, M
conceptual stru
facilitating kn
content. This c
decisive enabling technology for allowing the 
semantic web. 
Acknowledgements 
The MEANING project is funded by the 
European Commission (IST-2001-34460). 
References 
Agirre E. and Lersundi M. Extracci?n de relaciones 
l?xico-sem?nticas a partir de palabras derivadas 
usando patrones de definici?n. Proceedings of the 
Annual SEPLN meeting. Spain, 2000. 
Agirre E., Lersundi M. and Mart?nez D. A 
Multilingual Approach to Disambiguate 
Prepositions and Case Suffixes. Proceeding of the 
Workshop ?Word Sense Disambiguation: Recent 
Successes and Future Directions? organized by 
ACL 2002.  
Agirre E. and Mart?nez D. Exploring automatic word 
sense disambiguation with decision lists and the 
Web.  Proceedings of the Workshop ?Semantic 
Annotation And Intelligent Annotation? organized 
by COLING 2000. Luxembourg. 2000.  
Agirre E. and Martinez D. Learning class-to-class 
selectional preferences. Proceedings of the 
Workshop "Computational Natural Language 
Learning" (CoNLL-2001). In conjunction with 
ACL'2001/EACL'2001. Toulouse. 2001. 
Agirre E., Ansa O., Mart?nez D. and Hovy E. 
Enriching WordNet concepts with topic signatures. 
Proceedings of the NAACL workshop on WordNet 
and Other lexical Resources: Applications, 
Extensions and Customizations. Pittsburg. 2001. 
Agirre E. and Martinez D. Integrating selectional 
preferences in WordNet. Proceedings of the first 
International WordNet Conference. Mysore, India, 
2002. 
Blum A. and Mitchel T. Combining labelled and 
unlabeled data with co-training. In Proceedings of 
the 11th Annual Conference on Computational 
Learning Theory. 1998. 
Carroll, J. and McCarthy, D. Word sense 
disambiguation using automatically acquired 
verbal preferences. Computers and the Humanities. 
Senseval Special Issue, Vol. 34, No 1-2. 2000. 
Daud? J., Padr? L. and Rigau G., Mapping 
Multilingual Hierarchies using Relaxation 
Labelling, Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora (EMNLP/VLC'99). Maryland, 
1999.  
Daud? J., Padr? L. and Rigau G., Mapping WordNets 
Using Structural Information , 38th Anual Meeting 
of the ACL. Hong Kong, 2000.  
Daud? J., Padr? L. and Rigau G., A Complete WN1.5 
to WN1.6 Mapping, Proceedings of NAACL 
Workshop "WordNet and Other Lexical Resources: 
Applications, Extensions and Customizations". 
Pittsburg, PA, 2001. 
Escudero G., M?rquez L. and Rigau G., Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 11th European Conference on 
Machine Learning. Barcelona. 2000.  
Escudero G., M?rquez L. and Rigau G., Naive Bayes 
and Exemplar-Based approaches to Word Sense 
Disambiguation Revisited.  Proceedings of the 14th 
European Conference on Artificial Intelligence, 
Berlin. 2000.  
Escudero G., M?rquez L. and Rigau G., A 
Comparison between Supervised Learning 
Algorithms for Word Sense Disambiguation. 
Proceedings of Fourth Computational Natural 
Language Learning Workshop. Lisbon. 2000.  
Escudero G., M?rquez L. and Rigau G., An Empirical 
Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems. Proceedings 
of Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora. Hong Kong. 2000. 
Escudero G., M?rquez L. and Rigau G., Using 
LazyBoosting for Word Sense Disambiguation. 
Proceedings of 2nd International Workshop 
?Evaluating Word Sense Disambiguation 
Systems?, SENSEVAL-2. Toulouse. 2001. 
Fellbaum C. editor. WordNet An Electronic Lexical 
Database. The MIT Press. 1998. 
Ide, N. and V?ronis, J. Introduction to the special 
issue on word sense disambiguation: The state of 
the art. Computational Linguistics, 24 (1), 1998. 
Korhonen A., Gorrell, G. and McCarthy D. Statistical 
Filtering and Subcategorization Frame 
Acquisition. In Proceedings of the Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora. 
Hong Kong. 2000. 
Leacock, C. Chodorow, M. and Miller, G.A. Using 
Corpus Statistics and WordNet Relations for Sense 
Identication, Computational Linguistics, 24(1), 
1998. 
Magnini B. and Cavagli? G., Integrating subject field 
codes into WordNet. In Proceedings of the 2nd 
International Conference on Language Resources 
and Evaluation, Athens. 2000. 
Mart?nez D. and Agirre E. One Sense per Collocation 
and Genre/Topic Variations. Proceedings of the 
Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora. Hong Kong, 2000. 
McCarthy, D. and Korhonen, A. Detecting verbal 
participation in diathesis alternations. Proceedings 
of the 17th International Conference on 
Computational Linguistics and 36th Annual 
Meeting of the Association for Computational 
Linguistics COLING-ACL'98. Montreal. 1998.  
McCarthy D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, 
Subcategorization Frames and Selectional 
Preferences. Ph.D. thesis, University of Sussex. 
2001. 
McCarthy D., Carroll J. and Preiss J. Disambiguating 
noun and verb senses using automatically acquired 
selectional preferences. Proceedings of the 
SENSEVAL-2 Workshop at ACL/EACL'01, 
Toulouse. 2001. 
Mihalcea R. and Moldovan D. An automatic method 
for generating sense tagged corpora. In 
Proceedings of American Association for Artificial 
Intelligence. 1999. 
Miller G. Five papers on WordNet, Special Issue of 
International Journal of Lexicogrphy 3(4). 1990. 
Ng. H. T. Getting Serious about Word Sense 
Disambiguation. In Proceedings of Workshop 
?Tagging Text with Lexical Semantics: Why, what 
and how??, Washington, 1997. 
Vossen P. EuroWordNet: A Multilingual Database 
with Lexical Semantic Networks, Kluwer Academic 
Publishers, Dordrecht. 1998. 
Yarowsky D., Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 1995. 
 	

	
Using Automatically Acquired Predominant Senses for Word Sense
Disambiguation
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The first (or predominant)
sense heuristic assumes the availability of hand-
tagged data. Whilst there are hand-tagged corpora
available for some languages, these are relatively
small in size and many word forms either do not
occur, or occur infrequently. In this paper we in-
vestigate the performance of an unsupervised first
sense heuristic where predominant senses are ac-
quired automatically from raw text. We evaluate on
both the SENSEVAL-2 and SENSEVAL-3 English all-
words data. For accurate WSD the first sense heuris-
tic should be used only as a back-off, where the evi-
dence from the context is not strong enough. In this
paper however, we examine the performance of the
automatically acquired first sense in isolation since
it turned out that the first sense taken from SemCor
outperformed many systems in SENSEVAL-2.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account (McCarthy et al, 2004). The high
performance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where evi-
dence from the context is not sufficient (Hoste et al,
2001).
The first sense heuristic is a powerful one. Us-
ing the first sense listed in SemCor on the SENSE-
VAL-2 English all-words data we obtained the re-
sults given in table 1, (where the PoS was given by
the gold-standard data in the SENSEVAL-2 data it-
self). 1 Recall is lower than precision because there
are many words which do not occur in SemCor. Use
1We did not include items which were tagged ?U?
(unassignable) by the human annotators.
PoS precision recall baseline
Noun 70 60 45
Verb 48 44 22
Adjective 71 59 44
Adverb 83 79 59
All PoS 67 59 41
Table 1: The SemCor first sense on the SENSEVAL-
2 English all-words data
of the first sense listed in WordNet gives 65% pre-
cision and recall for all PoS on these items. The
fourth column on table 1 gives the random base-
line which reflects the polysemy of the data. Ta-
ble 2 shows results obtained when we use the most
common sense for an item and PoS using the fre-
quency in the SENSEVAL-2 English all-words data
itself. Recall is lower than precision since we only
use the heuristic on lemmas which have occurred
more than once and where there is one sense which
has a greater frequency than the others, apart from
trivial monosemous cases. 2 Precision is higher in
table 2 than in table 1 reflecting the difference be-
tween an a priori first sense determined by Sem-
Cor, and an upper bound on the performance of this
heuristic for this data. This upper bound is quite
high because of the very skewed sense distributions
in the test data itself. The upper bound for a docu-
ment, or document collection, will depend on how
homogenous the content of that document collec-
tion is, and the skew of the word sense distributions
therein. Indeed, the bias towards one sense for a
given word in a given document or discourse was
observed by Gale et al (1992).
Whilst a first sense heuristic based on a sense-
tagged corpus such as SemCor is clearly useful,
there is a case for obtaining a first, or predomi-
nant, sense from untagged corpus data so that a WSD
2If we include polysemous items that have only occurred
once in the data we obtain a precision of 92% and a recall of
85% over all PoS.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
PoS precision recall baseline
Noun 95 73 45
Verb 79 43 22
Adjective 88 59 44
Adverb 91 72 59
All PoS 90 63 41
Table 2: The SENSEVAL-2 first sense on the SEN-
SEVAL-2 English all-words data
system can be tuned to a given genre or domain
(McCarthy et al, 2004) and also because there will
be words that occur with insufficient frequency in
the hand-tagged resources available. SemCor com-
prises a relatively small sample of 250,000 words.
There are words where the first sense in WordNet is
counter-intuitive, because this is a small sample, and
because where the frequency data does not indicate
a first sense, the ordering is arbitrary. For exam-
ple the first sense of tiger in WordNet is audacious
person whereas one might expect that carnivorous
animal is a more common usage.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
are investigating a method of automatically ranking
WordNet senses from raw text, with no reliance on
manually sense-tagged data such as that in SemCor.
The paper is structured as follows. We discuss
our method in the following section. Section 3 de-
scribes an experiment using predominant senses ac-
quired from the BNC evaluated on the SENSEVAL-2
English all-words task. In section 4 we present our
results on the SENSEVAL-3 English all-words task.
We discuss related work in section 5 and conclude
in section 6.
2 Method
The method is described in (McCarthy et al, 2004),
which we summarise here. We acquire thesauruses
for nouns, verbs, adjectives and adverbs based on
the method proposed by Lin (1998) using grammat-
ical relations output from the RASP parser (Briscoe
and Carroll, 2002). The grammatical contexts used
are listed in table 3, but there is scope for extending
or restricting the contexts for a given PoS.
We use the thesauruses for ranking the senses of
the target words. Each target word (  ) e.g. plant
in the thesaurus is associated with a list of nearest
PoS grammatical contexts
Noun verb in direct object or subject relation
adjective or noun modifier
Verb noun as direct object or subject
Adjective modified noun, modifing adverb
Adverb modified adjective or verb
Table 3: Grammatical contexts used for acquiring
the thesauruses
neighbours ( 	
 ) with distributional similarity
scores (  ) e.g. factory 0.28, refinery 0.17,
tree 0.14 etc... 3 Distributional similarity is a mea-
sure indicating the degree that two words, a word
and its neighbour, occur in similar contexts. The
neighbours reflect the various senses of the word
( Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 160?170,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficient extraction of grammatical relations
Rebecca Watson, John Carroll
 
and Ted Briscoe
Computer Laboratory, University of Cambridge, Cambridge, CB3 OFD, UK
firstname.lastname@cl.cam.ac.uk
 
Department of Informatics, University of Sussex, Brighton BN1 9QH, UK
J.A.Carroll@sussex.ac.uk
Abstract
We present a novel approach for applying
the Inside-Outside Algorithm to a packed
parse forest produced by a unification-
based parser. The approach allows a node
in the forest to be assigned multiple inside
and outside probabilities, enabling a set of
?weighted GRs? to be computed directly
from the forest. The approach improves
on previous work which either loses effi-
ciency by unpacking the parse forest be-
fore extracting weighted GRs, or places
extra constraints on which nodes can be
packed, leading to less compact forests.
Our experiments demonstrate substantial
increases in parser accuracy and through-
put for weighted GR output.
1 Introduction
RASP is a robust statistical analysis system for
English developed by Briscoe and Carroll (2002).
It contains a syntactic parser which can output
analyses in a number of formats, including (n-
best) syntactic trees, robust minimal recursion se-
mantics (Copestake, 2003), grammatical relations
(GRs), and weighted GRs. The weighted GRs for
a sentence comprise the set of grammatical relations
in all parses licensed for that sentence, each GR is
weighted based on the probabilities of the parses
in which it occurs. This weight is normalised to
fall within the range  0,1  where  indicates that
all parses contain the GR. Therefore, high precision
GR sets can be determined by thresholding on the
GR weight (Carroll and Briscoe, 2002). Carroll and
Briscoe compute weighted GRs by first unpacking
all parses or the n-best subset from the parse forest.
Hence, this approach is either (a) inefficient (and for
some examples impracticable) if a large number of
parses are licensed by the grammar, or (b) inaccu-
rate if the number of parses unpacked is less than
the number licensed by the grammar.
In this paper, we show how to obviate the need
to trade off efficiency and accuracy by extracting
weighted GRs directly from the parse forest us-
ing a dynamic programming approach based on the
Inside-Outside algorithm (IOA) (Baker, 1979; Lari
and Young, 1990). This approach enables efficient
calculation of weighted GRs over all parses and sub-
stantially improves the throughput and memory us-
age of the parser. Since the parser is unification-
based, we also modify the parsing algorithm so that
local ambiguity packing is based on feature structure
equivalence rather than subsumption.
Similar dynamic programming techniques that
are variants of the IOA have been applied for re-
lated tasks, such as parse selection (Johnson, 2001;
Schmid and Rooth, 2001; Geman and Johnson,
2002; Miyao and Tsujii, 2002; Kaplan et al, 2004;
Taskar et al, 2004). The approach we take is similar
to Schmid and Rooth?s (2001) adaptation of the al-
gorithm, where ?expected governors? (similar to our
?GR specifications?) are determined for each tree,
and alternative nodes in the parse forest have the
same lexical head. Initially, they create a packed
parse forest and during a second pass the parse forest
nodes are split if multiple lexical heads occur. The
IOA is applied over this split data structure. Simi-
larly, Clark and Curran (2004) alter their packing al-
gorithm so that nodes in the packed chart have the
same semantic head and ?unfilled? GRs. Our ap-
160
proach is novel in that while calculating inside prob-
abilities we allow any node in the parse forest to
have multiple semantic heads.
Clark and Curran (2004) apply Miyao and Tsu-
jii?s (2002) dynamic programming approach to de-
termine weighted GRs. They outline an alterna-
tive parse selection method based on the resulting
weighted GRs: select the (consistent) GR set with
the highest average weighted GR score. We apply
this parse selection approach and achieve 3.01% rel-
ative reduction in error. Further, the GR set output
by this approach is a consistent set whereas the high
precision GR sets outlined in (Carroll and Briscoe,
2002) are neither consistent nor coherent.
The remainder of this paper is organised as fol-
lows: Section 2 gives details of the RASP sys-
tem that are relevant to this work. Section 3 de-
scribes our test suite and experimental environment.
Changes required to the current parse forest cre-
ation algorithm are discussed in Section 4, while
Section 5 outlines our dynamic programming ap-
proach for extracting weighted GRs (EWG). Sec-
tion 6 presents experimental results showing (a) im-
proved efficiency achieved by EWG, (b) increased
upper bounds of precision and recall achieved us-
ing EWG, and (c) increased accuracy achieved by
a parse selection algorithm that would otherwise be
too inefficient to consider. Finally, Section 7 out-
lines our conclusions and future lines of research.
2 The RASP System
RASP is based on a pipelined modular architec-
ture in which text is pre-processed by a series of
components including sentence boundary detection,
tokenisation, part of speech tagging, named entity
recognition and morphological analysis, before be-
ing passed to a statistical parser1 . A brief overview
of relevant aspects of syntactic processing in RASP
is given below; for full details of system compo-
nents, see Briscoe and Carroll (1995; 2002; 2005)2.
1Processing times given in this paper do not include these
pre-processing stages, since they take negligible time compared
with parsing.
2RASP is freely available for research use; visit
http://www.informatics.susx.ac.uk/research/nlp/rasp/
2.1 The Grammar
Briscoe and Carroll (2005) describe the (manually-
written) feature-based unification grammar and the
rule-to-rule mapping from local trees to GRs. The
mapping specifies for each grammar rule the seman-
tic head(s) of the rule (henceforth, head), and one or
more GRs that should be output (optionally depend-
ing on feature values instantiated at parse time). For
example, Figure 1 shows a grammar rule analysing a
verb phrase followed by a prepositional phrase mod-
ifier. The rule identifies the first daughter (1) as the
semantic head, and specifies that one of five possi-
ble GRs is to be output, depending on the value of
the PSUBCAT syntactic feature; so, for example, if the
feature has the value NP, then the relation is ncmod
(non-clausal modifier), with slots filled by the se-
mantic heads of the first and second daughters (the 1
and 2 arguments).
Before parsing, a context free backbone is derived
automatically from the grammar, and an LALR(1)
parse table is computed from this backbone (Carroll,
1993, describes the procedure in detail). Probabili-
ties are associated with actions in the parse table,
by training on around 4K sentences from the Su-
sanne corpus (Sampson, 1995), each sentence hav-
ing been semi-automatically converted from a tree-
bank bracketing to a tree conforming to the unifica-
tion grammar (Briscoe and Carroll, 1995).
2.2 The Parse Forest
When parsing, the LALR table action probabilities
are used to assign a score to each newly derived
(sub-)analysis. Additionally, on each reduce ac-
tion (i.e. complete application of a rule), the rule?s
daughters are unified with the sequence of sub-
analyses being consumed. If unification fails then
the reduce action is aborted. Local ambiguity pack-
ing (packing, henceforth) is performed on the ba-
sis of feature structure subsumption. Thus, the
parser builds and returns a compact structure that ef-
ficiently represents all parses licensed by the gram-
mar: the parse forest. Since unification often fails
it is not possible to apply beam or best first search
strategies during construction of the parse forest;
statistically high scoring paths often end up in unifi-
cation failure. Hence, the parse forest represents all
parses licensed by the grammar.
161
V1/vp_pp : V1[MOD +] --> H1 P2[ADJ -, WH -] :
1 :
2 = [PSUBCAT NP], (ncmod _ 1 2) :
2 = [PSUBCAT NONE], (ncmod prt 1 2) :
2 = [PSUBCAT (VP, VPINF, VPING, VPPRT, AP)], (xmod _ 1 2) :
2 = [PSUBCAT (SFIN, SINF, SING)], (cmod _ 1 2) :
2 = [PSUBCAT PP], (pmod 1 2).
Figure 1: Example grammar rule, showing the rule name and syntactic specification (on the first line),
identification of daughter 1 as the semantic head (second line), and possible GR outputs depending on the
parse-time value of the PSUBCAT feature of daughter 2 (subsequent lines).
Figure 2 shows a simplified parse forest contain-
ing three parses generated for the following pre-
processed text3:
I PPIS1 see+ed VVD the AT man NN1
in II the AT park NN1
The GR specifications shown are instantiated based
on the values of syntactic features at daughter nodes,
as discussed in Section 2.1 above. For example, the
V1/vp pp sub-analysis (towards the left hand side of
the Figure) contains the instantiated GR specifica-
tion   1, (ncmod 1 2)  since its second daughter has
the value NP for its PSUBCAT feature.
Henceforth, we will use the term ?node? to refer to
data structures in our parse forest corresponding to a
rule instantiation: a sub-analysis resulting from ap-
plication of a reduce action. Back pointers are stored
in nodes, indicating which daughters were used to
create the sub-analysis. These pointers provide a
means to traverse the parse forest during subsequent
processing stages. A ?packed node? is a node rep-
resenting a sub-analysis that is subsumed by, and
hence packed into, another node. Packing is consid-
ered for nodes only if they are produced in the same
LR state and represent sub-analyses with the same
word span. A parse forest can have a number of root
nodes, each one dominating analyses spanning the
whole sentence with the specified top category.
2.3 Parser Output
From the parse forest, RASP unpacks the ?n-best?4
syntactic trees using a depth-first beam search (Car-
roll, 1993). There are a number of types of analysis
3The part of speech tagger uses a subset of the Lancaster
CLAWS2 tagset ? http://www.comp.lancs.ac.uk/computing/research/
ucrel/claws2tags.html
4This number  is specified by the user, and represents the
maximal number of parses to be unpacked.
output available, including syntactic tree, grammati-
cal relations (GRs) and robust minimal recursion se-
mantics (RMRS). Each of these is computed from
the n-best trees.
Another output possibility is weighted GRs (Car-
roll and Briscoe, 2002); this is the unique set of GRs
from the n-best GRs, each GR weighted according
to the sum of the probabilities of the parses in which
it occurs. Therefore, a number of processing stages
determine this output: unpacking the n-best syntac-
tic trees, determining the corresponding n-best GR
sets and finding the unique set of GRs and corre-
sponding weights.
The GRs for each parse are computed from the
set of GR specifications at each node, passing the
(semantic) head of each sub-analysis up to the next
higher level in the parse tree (beginning from word
nodes). GR specifications for nodes (which, if re-
quired, have been instantiated based on the features
of daughter nodes) are referred to as ?unfilled? un-
til the slots containing numbers are ?filled? with the
corresponding heads of daughter nodes. For exam-
ple, the grammar rule named NP/det n has the un-
filled GR specification   2, (det 2 1)  . Therefore, if
an NP/det n local tree has two daughters with heads
the and cat respectively, the resulting filled GR spec-
ification will be   cat, (det cat the)  , i.e. the head of
the local tree is cat and the GR output is (det cat the).
Figure 3 illustrates the n-best GRs and the
corresponding (non-normalised and normalised)
weighted GRs for the sentence I saw the man in
the park. The corresponding parse forest for this
example is shown in Figure 2. Weights on the
GRs are normalised probabilities representing the
weighted proportion of parses in which the GR
occurs. This weighting is in practice calculated
as the sum of parse probabilities for parses con-
162
T/
tx
t-
sc
1/
-
S/
np
_v
p
I_
PP
IS
1
V1
/v
_n
p_
pp
V1
/v
p_
pp
V1
/v
_n
p
se
e+
ed
_V
VD
th
e_
AT
ma
n_
NN
1
in
_I
I
th
e_
AT
pa
rk
_N
N1
N1
/n
N1
/n
NP
/d
et
_n
NP
/d
et
_n
PP
/p
1
PP
/p
1
PP
/p
1
P1
/p
_n
p
NP
/d
et
_n
N1
/n
1_
pp
1
P1
/p
_n
p
P1
/p
_n
p
in
_I
I
V1
/v
_n
p
in
_I
I
-0.
02
63
28
93
2
-4.
05
27
59
6
-0.
47
56
08
94
-3.
19
57
16
-2.
78
88
75
-3.
23
82
97
2
-4.
61
76
63
2e
-4
-0.
00
67
16
03
06
-1.
35
22
34
5e
-4
-0.
13
60
40
73 -
0.0
01
26
28
43
3
-1.
35
22
34
5e
-4
-1.
08
08
25
7
-0.
39
11
12
9
-7.
49
44
67
7
-3.
77
18
31
3
-3.
83
19
09
2
-3.
59
08
41
8
-0.
00
64
18
27
42
-3.
83
19
09
2
-3.
59
08
41
8
-1.
59
90
18
3
-0.
32
94
57
7
-3.
83
19
09
2
-3.
59
08
41
8
-2.
75
51
12
4
-3.
77
18
31
3
<1
>
<2
,(d
et 
2 1
)>
<1
,(d
ob
j 1
 2)
>
<1
,(d
ob
j 1
 2)
>
<1
,(d
ob
j 1
 2)
>
<1
>
<2
,(d
et 
2 1
)>
<2
,(d
et 
2 1
)>
<1
>
<1
,(n
cm
od
 _ 
1 2
)>
<1
>
<1
>
<1
,(io
bj 
1 3
)>
 
<2
,(n
csu
bj 
2 1
)> 
<1
,(d
ob
j 1
 2)
>
<1
,(n
cm
od
 _ 
1 2
)>
<1
,(d
ob
j 1
 2)
>
*p
ac
ke
d*
*p
ac
ke
d*
Figure 2: Simplified parse forest for I saw the man in the park. Each element in the directed acyclic graph
represents a node in the parse forest and is shown with the sub-analysis? rule name, reduce probability
(or shift probability at word nodes) and (instantiated) GR specifications. Two nodes are packed into the
V1/v np pp node, so there will be three alternative parses for the sentence. Nodes with multiple in-going
pointers on their left are shared. All thin lines indicate pointers from left to right, i.e. from mother to daughter
nodes.
163
taining the specific GR, normalised by the sum
of all parse probabilities. For example, the GR
(iobj see+ed in) is in one parse with probability
 

 	 , the non-normalised score. The sum of
all parse probabilities is        
  . Therefore,
the normalised probability (and final weight) of the
GR is  
 Proceedings of the Linguistic Annotation Workshop, pages 93?100,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating Expressions of Appraisal in English
Jonathon Read, David Hope and John Carroll
Department of Informatics
University of Sussex
United Kingdom
{j.l.read,drh21,j.a.carroll}@sussex.ac.uk
Abstract
The Appraisal framework is a theory of the
language of evaluation, developed within the
tradition of systemic functional linguistics.
The framework describes a taxonomy of the
types of language used to convey evaluation
and position oneself with respect to the eval-
uations of other people. Accurate automatic
recognition of these types of language can
inform an analysis of document sentiment.
This paper describes the preparation of test
data for algorithms for automatic Appraisal
analysis. The difficulty of the task is as-
sessed by way of an inter-annotator agree-
ment study, based on measures analogous to
those used in the MUC-7 evaluation.
1 Introduction
The Appraisal framework (Martin and White, 2005)
describes a taxonomy of the language employed in
communicating evaluation, explaining how users of
English convey attitude (emotion, judgement of peo-
ple and appreciation of objects), engagement (as-
sessment of the evaluations of other people) and
how writers may modify the strength of their atti-
tude/engagement. Accurate automatic analysis of
these aspects of language will augment existing re-
search in the fields of sentiment (Pang et al, 2002)
and subjectivity analysis (Wiebe et al, 2004), but as-
sessing the usefulness of analysis algorithms lever-
aging the Appraisal framework will require test data.
At present there are no machine-readable
Appraisal-annotated texts publicly available. Real-
world instances of Appraisal in use are limited
to example extracts that demonstrate the theory,
coming from a wide variety of genres as disparate
as news reporting (White, 2002; Martin, 2004) and
poetry (Martin and White, 2005). These examples,
while useful in demonstrating the various aspects
of Appraisal, can only be employed in a qualitative
analysis and would bring about inconsistencies
if analysed collectively ? one can expect the
writing style to depend upon the genre, resulting in
significantly different syntactic constructions and
lexical choices.
We therefore need to examine Appraisal across
documents in the same genre and investigate pat-
terns within that particular register. This paper dis-
cusses the methodology of an Appraisal annotation
study and an analysis of the inter-annotator agree-
ment exhibited by two human judges. The output
of this study has the additional benefit of bringing
a set of machine-readable annotations of Appraisal
into the public domain for further research.
This paper is structured as follows. The next sec-
tion offers an overview of the Appraisal framework.
Section 3 discusses the methodology adopted for
the annotation study. Section 4 discusses the mea-
sures employed to assess inter-annotator agreement
and reports the results of these measures. Section
5 offers an analysis of cases of systematic disagree-
ment. Other computational work utilising the Ap-
praisal framework is reviewed in Section 6. Section
7 summarises the paper and outlines future work.
2 The linguistic framework of Appraisal
The Appraisal framework (Martin and White, 2005)
is a development of work in Systemic Functional
93
appraisal
attitude
engagement
graduation
affect
judgement
appreciation
inclination
happiness
security
satisfaction
esteem
sanction
normality
capacity
tenacity
veracity
propriety
reaction
composition
valuation
impact
quality
balance
complexity
contract
expand
disclaim
proclaim
deny
counter
pronounce
endorse
concur
affirm
concede
entertain
attribute acknowledge
distance
force
focus
quantification
intensification
number
mass
extent
proximity (space)
proximity (time)
distribution (space)
distribution (time)
degree
vigour
Figure 1: The Appraisal framework.
Linguistics (Halliday, 1994) and is concerned with
interpersonal meaning in text?the negotiation of
social relationships by communicating emotion,
judgement and appreciation. The taxonomy de-
scribed by the Appraisal framework is depicted in
Figure 1.
Appraisal consists of three subsystems that oper-
ate in parallel: attitude looks at how one expresses
private state (Quirk et al, 1985) (one?s emotion and
opinions); engagement considers the positioning of
oneself with respect to the opinions of others and
graduation investigates how the use of language
functions to amplify or diminish the attitude and en-
gagement conveyed by a text.
2.1 Attitude: emotion, ethics and aesthetics
The Attitude sub-system describes three areas of pri-
vate state: emotion, ethics and aesthetics. An atti-
tude is further qualified by its polarity (positive or
negative). Affect identifies feelings?author?s emo-
tions as represented by their text. Judgement deals
with authors? attitude towards the behaviour of peo-
ple; how authors applaud or reproach the actions
of others. Appreciation considers the evaluation of
things?both man-made and natural phenomena.
2.2 Engagement: appraisals of appraisals
Through engagement, Martin and White (2005) deal
with the linguistic constructions by which authors
construe their point of view and the resources used
to adopt stances towards the opinions of other peo-
ple. The theory of engagement follows Stubbs
(1996) in that it assumes that all utterances convey
point of view and Bakhtin (1981) in supposing that
all utterances occur in a miscellany of other utter-
ances on the same motif, and that they carry both
implicit and explicit responses to one another. In
other words, all text is inherently dialogistic as it en-
codes authors? reactions to their experiences (includ-
ing previous interaction with other writers). Engage-
ment can be both retrospective (that is, an author will
acknowledge and agree or disagree with the stances
of others who have previously appraised a subject),
and prospective (one may anticipate the responses of
an intended audience and include counter-responses
in the original text).
2.3 Graduation: strength of evaluations
Martin and White (2005) consider the resources by
which writers alter the strength of their evaluation
as a system of graduation. Graduation is a general
property of both attitude and engagement. In atti-
tude it enables authors to convey greater or lesser
degrees of positivity or negativity, while graduation
of engagements scales authors? conviction in their
utterance.
Graduation is divided into two subsystems. Force
alters appraisal propositions in terms of its inten-
94
sity, quantity or temporality, or by means of spatial
metaphor. Focus considers the resolution of seman-
tic categories, for example:
They play real jazz.
They play jazz, sort of.
In real terms a musician either plays jazz or they
do not, but these examples demonstrate how authors
blur the lines of semantic sets and how binary rela-
tionships can be turned into scalar ones.
3 Annotation methodology
The corpus used in this study consists of unedited
book reviews. Book reviews are good candidates for
this study as, while they are likely to contain similar
language by virtue of being from the same genre of
writing, we can also expect examples of Appraisal?s
many classes (for example, the emotion attributed
to the characters in reviews of novels, judgements
of authors? competence and character, appreciation
of the qualities of books and engagement with the
propositions put forth by the authors under review).
The articles were taken from the web sites of
four British newspapers (The Guardian, The Inde-
pendent, The Telegraph and The Times) on two dif-
ferent dates?31 July 2006 and 11 September 2006.
Each review is attributed to a unique author. The
corpus is comprised of 38 documents, containing a
total of 36,997 tokens in 1,245 sentences.
Two human annotators, d and j, participated in
this study, assigning tags independently. The anno-
tators were well-versed in the Appraisal framework,
having studied the latest literature. The judges were
asked to annotate appraisal-bearing terms with the
appraisal type presumed to be intended by the au-
thor of the text. They were asked to highlight each
example of appraisal and specify the type of atti-
tude, engagement or graduation present. They also
assigned a polarity (positive or negative) to attitudi-
nal items and a scaling (up or down) to graduating
items, employing a custom-developed software tool
to annotate the documents.
Four alternative annotation strategies were con-
sidered. One approach is to allow only a single token
per annotation. However, this is too simplistic for
an Appraisal annotation study?a unit of Appraisal
is frequently larger than a single token. Consider the
following examples:
(1)
The design was deceptively?VERACITY simple?
COMPLEXITY. (?)
(2)
The design was deceptively simple?COMPLEXITY.
Example 1 demonstrates that a single-token ap-
proach is inappropriate as it ascribes a judgement
of someone?s honesty, whereas Example 2 indicates
the correct analysis?the sentence is an apprecia-
tion of the simplicity of the ?design?. This example
shows how it is necessary to annotate larger units of
appraisal-bearing language.
Including more tokens, however, increases the
complexity of the annotation task, and reduces the
likelihood of agreement between the judges, as the
annotated tokens of one judge may be a subset of,
or overlap with, those of another. We therefore ex-
perimented with tagging entire sentences in order to
constrain the annotators? range of choices. This re-
sulted in its own problems as there is often more than
one appraisal in a sentence, for example:
(3)
The design was deceptively simple?COMPLEXITY
and belied his ingenuity?CAPACITY.
An alternative approach is to permit annotators
to tag an arbitrary number of contiguous tokens.
Arbitrary-length tagging is disadvantageous as the
judges will frequently tag units of differing length,
but this can be compensated for by relaxing the rules
for agreement?for example, by allowing intersect-
ing annotations to match successfully (Wiebe et al,
2005). Bruce and Wiebe (1999) employ another
approach, creating units from every non-compound
sentence and each conjunct of every compound sen-
tence. This side-steps the problem of ambiguity in
appraisal unit length, but will still fail to capture both
appraisals demonstrated in the second conjunct of
Example 4.
(4)
The design was deceptively simple?COMPLEXITY
and belied his remarkable?NORMALITY
ingenuity?CAPACITY.
Ultimately in this study, we permitted judges to
annotate any number of tokens in order to allow
for multiple Appraisal units of differing sizes within
sentences. Annotation was carried out over two
rounds, punctuated by an intermediary analysis of
95
d j d j d j
Inclination 1.26 3.50 Balance 2.64 1.84 Distance 0.69 0.59
Happiness 2.80 2.32 Complexity 2.52 2.74 Number 0.82 2.63
Security 4.31 2.22 Valuation 6.08 9.29 Mass 0.22 1.63
Satisfaction 1.67 2.32 Deny 3.05 3.67 Proximity (Space) 0.09 0.14
Normality 8.00 4.44 Counter 4.79 3.78 Proximity (Time) 0.03 0.55
Capacity 11.46 9.63 Pronounce 3.84 1.21 Distribution (Space) 0.41 1.39
Tenacity 3.72 4.44 Endorse 2.05 1.49 Distribution (Time) 0.82 2.56
Veracity 3.15 2.01 Affirm 0.54 1.14 Degree 4.38 5.72
Propriety 13.32 12.61 Concede 0.38 0.03 Vigour 0.60 0.45
Impact 6.11 4.23 Entertain 2.27 2.43 Focus 3.02 2.29
Quality 2.55 3.40 Acknowledge 2.42 3.33
Table 1: The distribution of the Appraisal types selected by each annotator (%).
d j
Documents 115.74 77.21
Sentences 3.65 2.43
Words 0.12 0.08
Table 2: The density of annotations relative to the
number of documents, sentences and words.
agreement and disagreement between the two anno-
tators. The judges discussed examples of the most
common types of disagreement in an attempt to ac-
quire a common understanding for the second round,
but annotations from the first round were left unal-
tered.
Following the methodology described above, d
made 3,176 annotations whilst j made 2,886 anno-
tations. The distribution of the Appraisal types as-
cribed is shown in Table 1, while Table 2 details the
density of annotations in documents, sentences and
words.
4 Measuring inter-annotator agreement
The study of inter-annotator agreement begins by
considering the level of agreement exhibited by the
annotators in deciding which tokens are representa-
tive of Appraisal, irrespective of the type. As dis-
cussed, this is problematic as judges are liable to
choose different length token spans when marking
up what is essentially the same appraisal, as demon-
strated by Example 5.
(5)
[d] It is tempting to point to the bombs in Lon-
don and elsewhere, to the hideous mess?QUALITY
in Iraq, to recent victories of the Islamists, to
the violent and polarised rhetoric?PROPRIETY and
answer yes.
[j] It is tempting to point to the bombs in
London and elsewhere, to the hideous?QUALITY
mess?BALANCE in Iraq, to recent victories of Is-
lamists, to the violent?PROPRIETY and polarised?
PROPRIETY rhetoric and answer yes.
Wiebe et al (2005), who faced this problem when
annotating expressions of opinion under their own
framework, accept that it is necessary to consider the
validity of all judges? interpretations and therefore
consider intersecting annotations (such as ?hideous?
and ?hideous mess?) to be matches. The same relax-
ation of constraints is employed in this study.
Tasks with a known number of annotative units
can be analysed with measures of agreement such as
Cohen?s ? Coefficient (1960), but the judges? free-
dom in this task prohibits meaningful application of
this measure. For example, consider howword sense
annotators are obliged to choose from a limited fixed
set of senses for each token, whereas judges anno-
tating Appraisal are free to select one of thirty-two
classes for any contiguous substring of any length
within each document; there are 16
(
n2 ? n
)
pos-
sible choices in a document of n tokens (approxi-
mately 6.5 ? 108 possibilities in this corpus).
A wide range of evaluation metrics have been em-
ployed by the Message Understanding Conferences
(MUCs). The MUC-7 tasks included extraction of
named entities, equivalence classes, attributes, facts
and events (Chinchor, 1998). The participating sys-
tems were evaluated using a variety of related mea-
sures, defined in Table 3. These tasks are similar to
Appraisal annotation in that the units are formed of
an arbitrary number of contiguous tokens.
In this study the agreement exhibited by an an-
notator a is evaluated as a pair-wise comparison
against the other annotator b. Annotator b provides
96
COR Number correct
INC Number incorrect
MIS Number missing
SPU Number spurious
POS Number possible = COR + INC + MIS
ACT Number actual = COR + INC + SPU
FSC F-score = (2 ? REC ? PRE)
/ (REC + PRE)
REC Precision = COR/POS
PRE Recall = COR/ACT
SUB Substitution = INC/ (COR + INC)
ERR Error per response = (INC + SPU + MIS)
/ (COR + INC + SPU + MIS)
UND Under-generation = MIS/POS
OVG Over-generation = SPU/ACT
Table 3: MUC-7 score definitions (Chinchor 1998).
FSC REC PRE ERR UND OVG
d 0.682 0.706 0.660 0.482 0.294 0.340
j 0.715 0.667 0.770 0.444 0.333 0.230
x? 0.698 0.686 0.711 0.462 0.312 0.274
Table 4: MUC-7 test scores, evaluating the agree-
ment in text anchors selected by the annotators. x?
denotes the average value, calculated using the har-
monic mean.
a presumed gold standard for the purposes of evalu-
ating agreement. Note, however, that in this case it
does not necessarily follow that REC (a w.r.t. b) =
PRE (b w.r.t. a). Consider that a may tend to make
one-word annotations whilst b prefers to annotate
phrases; the set of a?s annotations will contain mul-
tiple matches for some of the phrases annotated by b
(refer to Example 5, for instance). The ?number cor-
rect? will differ for each annotator in the pair under
evaluation.
Table 4 lists the values for the MUC-7 measures
applied to the text spans selected by the annota-
tors. Annotator d is inclined to identify text as Ap-
praisal more frequently than annotator j. This re-
sults in higher recall for d, but with lower preci-
sion. Naturally, the opposite observation can be
made about annotator j. Both annotators exhibit a
high error rate at 48.2% and 44.4% for d and j re-
spectively. The substitution rate is not listed as there
are no classes to substitute when considering only
text anchor agreement. The second round of anno-
tation achieved slightly higher agreement (the mean
F-score increased by 0.033).
FSC REC PRE SUB ERR
0 0.698 0.686 0.711 0.000 0.462
1 0.635 0.624 0.647 0.090 0.511
2 0.528 0.518 0.538 0.244 0.594
3 0.448 0.441 0.457 0.357 0.655
4 0.396 0.388 0.403 0.433 0.696
5 0.395 0.388 0.403 0.433 0.696
Table 5: Harmonic means of the MUC-7 test scores
evaluating the agreement in text anchors and Ap-
praisal classes selected by the annotators, at each
level of hierarchical abstraction.
Having considered the annotators? agreement
with respect to text anchors, we go on to analyse
the agreement exhibited by the annotators with re-
spect to the types of Appraisal assigned to the text
anchors. The Appraisal framework is a hierarchi-
cal system?a tree with leaves corresponding to the
annotation types chosen by the judges. When in-
vestigating agreement in Appraisal type, the follow-
ing measures include not just the leaf nodes but also
their parent types, collapsing the nodes into increas-
ingly abstract representations. For example happi-
ness is a kind of affect, which is a kind of attitude,
which is a kind of appraisal. These relationships are
depicted in full in Figure 2. Note that in the follow-
ing measurements of inter-annotator agreement leaf
nodes are included in subsequent levels (for exam-
ple, focus is a leaf node at level 2, but is also consid-
ered to be a member of levels 3, 4 and 5).
Table 5 shows the harmonic means of the MUC-
7 measures of the annotators? agreement at each of
the levels depicted in Figure 2. As one might ex-
pect, the agreement steadily drops as the classes be-
come more concrete?classes become more specific
and more numerous so the complexity of the task
increases.
Table 5 also lists the average rate of substitutions
as the annotation task?s complexity increases, show-
ing that the annotators were able to fairly easily
distinguish between instances of the three subsys-
tems of Appraisal (Attitude, Engagement and Grad-
uation) as the substitution rate at level 1 is low (only
9%). As the number of possible classes increases an-
notators are more likely to confuse appraisal types,
with disagreement occurring on approximately 44%
of annotations at level 5. The second round of an-
notations resulted in slightly improved agreement at
97
Level 0: .698
Level 1: .635
Level 2: .528
Level 3: .448
Level 4: .396
Level 5: .395
appraisal
attitude: .701
engagement: .507
graduation: .479
affect: .519
judgement: .586
appreciation: .567
contract: .502
expand: .445
force: .420
focus: .287
inclination: .249
happiness: .448
security: .335
satisfaction: .374
esteem: .489
sanction: .575
reaction: .510
composition: .432
valuation: .299
disclaim: .555
proclaim: .336
entertain: .459
attribute: .427
quantification: .233
intensification: .513
normality: .289
capacity: .431
tenacity: .395
veracity: .519
propriety: .540
impact: .462
quality: .336
balance: .300
complexity: .314
deny: .451
counter: .603
pronounce: .195
endorse: .331
concur: .297
acknowledge: .390
distance: .415
number: .191
mass: .104
extent: .242
degree: .510
vigour: .117
affirm: .325
concede: .000
proximity (space): .000
proximity (time): .000
distribution (space): .110
distribution (time): .352
Figure 2: The Appraisal framework with hierarchical levels highlighted. Appraisal classes and levels are
accompanied by the harmonic mean of the F-scores of the annotators for that class/level.
each level of abstraction (the mean F-score increased
by 0.051 at the most abstract level).
Of course, some Appraisal classes are easier to
identify than others. Figure 2 summarises the agree-
ment for each node in the Appraisal hierarchy with
the harmonic mean of the F-scores of the annotators
for each class. Typically, the attitude annotations are
easiest to identify, whereas the other subsystems of
engagement and graduation tend to be more difficult.
The Proximity children of Extent exhibited no
agreement whatsoever. This seems to have arisen
from the differences in the judges? interpretations of
proximity. In the case of Proximity (Space), for ex-
ample, one judge annotated words that function to
modify the spatial distance of other concepts (e.g.
near), whereas the other selected words placing con-
cepts at a specific location (e.g. homegrown, local).
This confusion between modifying words and spe-
98
cific locations also accounts for the low agreement
in the Distribution (Space) type.
The measures show that it is also difficult to
achieve a consensus on what qualifies as engage-
ments of the Pronounce type. Both annotators select
expressions that assert the irrefutability of a propo-
sition (e.g. certainly or in fact or it has to be said).
Judge d, however, tends to perceive pronouncement
as occurring wherever the author makes an assertion
(e.g. this is or there will be). Judge j seems to re-
quire that the assertion carry a degree of emphasis to
include a term in the Pronounce class.
The low agreement of the Mass graduations can
also be explained in this way, as both d and j se-
lect strong expressions relating to size (e.g. massive
or scant). Annotator j found additional but weaker
terms like largely or slightly.
The Pronounce and Mass classes provide typical
examples of the disagreement exhibited by the an-
notators. It is not that the judges have wildly differ-
ent understandings of the system, but rather they dis-
agree in the bounds of a class?one annotator may
require a greater degree of strength of a term to war-
rant its inclusion in a class.
Contingency tables (not depicted due to space
constraints) reveal some interesting tendencies for
confusion between the two annotators. Approxi-
mately 33% of d?s annotations of Proximity (Space)
were ascribed as Capacity by j. The high percent-
age is due to the rarity of annotations of Proxim-
ity (Space), but the confusion comes from differing
units of Appraisal, as shown in Example 6.
(6)
[d] But at key points in this story, one gets
the feeling that the essential factors are op-
erating just outside?PROXIMITY (SPACE)
James?s field of vision?CAPACITY.
[j] But at key points in this story, one gets the
feeling that the essential factors are operating just
outside James?s field of vision?CAPACITY.
Another interesting case of frequent confusion is
the pair of Satisfaction and Propriety. Though not
closely related in the Attitude subsystem, j chooses
Propriety for 21% of d?s annotations of Satisfaction.
The confusion is typified by Example 7, where it is
apparent that there is disagreement in terms of who
is being appraised.
(7)
[d] Like him, Vermeer ? or so he chose to be-
lieve ? was an artist neglected?SATISFACTION and
wronged?SATISFACTION by critics and who had
died an almost unknown.
[j] Like him, Vermeer ? or so he chose to believe
? was an artist neglected and wronged?PROPRIETY
by critics and who had died an almost unknown.
Annotator d believes that the author is communi-
cating the artist?s dissatisfaction with the way he is
treated by critics, whereas j believes that the critics
are being reproached for their treatment of the artist.
This highlights a problem with the coding scheme,
which simplifies the task by assuming only one type
of Appraisal is conveyed by each unit.
5 Related work
Taboada and Grieve (2004) initiated computational
experimentation with the Appraisal framework, as-
signing adjectives into one of the three broad atti-
tude classes. The authors apply SO-PMI-IR (Turney,
2002) to extract and determine the polarity of adjec-
tives. They then use a variant of SO-PMI-IR to de-
termine a ?potential? value for affect, judgement and
appreciation, calculating the mutual information be-
tween the adjective and three pronoun-copular pairs:
I was (affect); he was (judgement) and it was (ap-
preciation). While the pairs seem compelling mark-
ers of the respective attitude types, they incorrectly
assume that appraisals of affect are limited to the
first person whilst judgements are made only of the
third person. We can expect a high degree of overlap
between the sets of documents retrieved by queries
formed using these pairs (e.g. I was a happy ?X?;
he was a happy ?X?; It was a happy ?X?).
Whitelaw et al (2005) use the Appraisal frame-
work to specify frames of sentiment. These ?Ap-
praisal Groups? are derived from aspects of Attitude
and Graduation:
Attitude: affect | judgement | appreciation
Orientation positive | negative
Force: low | neutral | high
Focus: low | neutral | high
Polarity: marked | unmarked
Their process begins with a semi-automatically con-
structed lexicon of these Appraisal groups, built us-
ing example terms from Martin and White (2005) as
seeds into WordNet synsets. The frames supplement
bag of words-based machine learning techniques for
99
sentiment analysis and they achieve minor improve-
ments over unigram features.
6 Summary
This paper has discussed the methodology of an ex-
ercise annotating book reviews according to the Ap-
praisal framework, a functional linguistic theory of
evaluation in English. The agreement exhibited by
two human judges was measured by analogy with
the evaluation employed for the MUC-7 shared tasks
(Chinchor, 1998).
The agreement varied greatly depending on the
level of abstraction in the Appraisal hierarchy
(a mean F-score of 0.698 at the most abstract
level through to 0.395 at the most concrete level).
The agreement also depended on the type being
annotated?there was more agreement evident for
types of attitude compared to types of engagement
or graduation.
The exercise is the first step in an ongoing study
of approaches for the automatic analysis of expres-
sions of Appraisal. The primary output of this work
is a corpus of book reviews independently annotated
with Appraisal types by two coders. Agreement was
in general low, but if one assumes that the intersec-
tion of both sets of annotations contains reliable ex-
amples, this leaves 2,223 usable annotations.
Future work will employ these annotations to
evaluate algorithms for the analysis of Appraisal,
and investigate the usefulness of the Appraisal
framework when in the computational analysis of
document sentiment and subjectivity.
Acknowledgments
We would like to thank Bill Keller for advice when
designing the annotation methodology. The work of
the first author is supported by a UK EPSRC stu-
dentship.
References
M. M. Bakhtin. 1981. The Dialogic Imagination. Uni-
versity of Texas Press, Austin. Translated by C. Emer-
son & M. Holquist.
Rebecca Bruce and Janyce Wiebe. 1999. Recognizing
subjectivity: a case study in manual tagging. Natural
Language Engineering, 5(1):1?16.
N. Chinchor. 1998. MUC-7 test scores introduction.
In Proceedings of the Seventh Message Understanding
Conference.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measures,
20:37?46.
M. A. K. Halliday. 1994. An Introduction to Functional
Grammar. Edward Arnold, London.
J. R. Martin and P. R. R. White. 2005. Language of Eval-
uation: Appraisal in English. Palgrave Macmillan.
J. R. Martin. 2004. Mourning: how we get algned. Dis-
course & Society, 15(2-3):321?344.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA, USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman.
M. Stubbs. 1996. Towards a modal grammar of English:
a matter of prolonged fieldwork. In Text and Corpus
Analysis. Blackwell, Oxford.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal automatically. In Spring Symposium on Ex-
ploring Attitude and Affect in Text. American Associa-
tion for Artificial Intelligence, Stanford. AAAI Tech-
nical Report SS-04-07.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, Philadelphia, PA, USA.
P. R. R.White. 2002. Appraisal? the language of evalu-
ation and stance. In Jef Verschueren, Jan-Ola O?stman,
Jan Blommaert, and Chris Bulcaen, editors, Handbook
of Pragmatics, pages 1?27. John Benjamins, Amster-
dam.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the 14th ACM international confer-
ence on Information and knowledge management.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational linguistics, 30(3):277?308.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
100
Proceedings of the 10th Conference on Parsing Technologies, pages 23?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Semi-supervised Training of a Statistical Parser
from Unlabeled Partially-bracketed Data
Rebecca Watson and Ted Briscoe
Computer Laboratory
University of Cambridge, UK
FirstName.LastName@cl.cam.ac.uk
John Carroll
Department of Informatics
University of Sussex, UK
J.A.Carroll@sussex.ac.uk
Abstract
We compare the accuracy of a statisti-
cal parse ranking model trained from a
fully-annotated portion of the Susanne
treebank with one trained from unla-
beled partially-bracketed sentences de-
rived from this treebank and from the
Penn Treebank. We demonstrate that
confidence-based semi-supervised tech-
niques similar to self-training outperform
expectation maximization when both are
constrained by partial bracketing. Both
methods based on partially-bracketed
training data outperform the fully su-
pervised technique, and both can, in
principle, be applied to any statistical
parser whose output is consistent with
such partial-bracketing. We also explore
tuning the model to a different domain
and the effect of in-domain data in the
semi-supervised training processes.
1 Introduction
Extant statistical parsers require extensive and
detailed treebanks, as many of their lexical and
structural parameters are estimated in a fully-
supervised fashion from treebank derivations.
Collins (1999) is a detailed exposition of one
such ongoing line of research which utilizes the
Wall Street Journal (WSJ) sections of the Penn
Treebank (PTB). However, there are disadvan-
tages to this approach. Firstly, treebanks are ex-
pensive to create manually. Secondly, the richer
the annotation required, the harder it is to adapt
the treebank to train parsers which make differ-
ent assumptions about the structure of syntac-
tic analyses. For example, Hockenmeier (2003)
trains a statistical parser based on Combinatory
Categorial Grammar (CCG) on the WSJ PTB,
but first maps the treebank to CCG derivations
semi-automatically. Thirdly, many (lexical) pa-
rameter estimates do not generalize well be-
tween domains. For instance, Gildea (2001) re-
ports that WSJ-derived bilexical parameters in
Collins? (1999) Model 1 parser contribute about
1% to parse selection accuracy when test data
is in the same domain, but yield no improve-
ment for test data selected from the Brown Cor-
pus. Tadayoshi et al (2005) adapt a statistical
parser trained on the WSJ PTB to the biomed-
ical domain by retraining on the Genia Corpus,
augmented with manually corrected derivations
in the same format. To make statistical parsing
more viable for a range of applications, we need
to make more effective and flexible use of extant
training data and minimize the cost of annota-
tion for new data created to tune a system to a
new domain.
Unsupervised methods for training parsers
have been relatively unsuccessful to date, in-
cluding expectation maximization (EM) such as
the inside-outside algorithm (IOA) over PCFGs
(Baker, 1979; Prescher, 2001). However, Pereira
and Schabes (1992) adapted the IOA to apply
over semi-supervised data (unlabeled bracket-
ings) extracted from the PTB. They constrain
the training data (parses) considered within the
IOA to those consistent with the constituent
boundaries defined by the bracketing. One ad-
vantage of this approach is that, although less
information is derived from the treebank, it gen-
23
eralizes better to parsers which make different
representational assumptions, and it is easier,
as Pereira and Schabes did, to map unlabeled
bracketings to a format more consistent with
the target grammar. Another is that the cost
of annotation with unlabeled brackets should be
lower than that of developing a representation-
ally richer treebank. More recently, both Riezler
et al (2002) and Clark and Curran (2004) have
successfully trained maximum entropy parsing
models utilizing all derivations in the model con-
sistent with the annotation of the WSJ PTB,
weighting counts by the normalized probability
of the associated derivation. In this paper, we
extend this line of investigation by utilizing only
unlabeled and partial bracketing.
We compare the performance of a statisti-
cal parsing model trained from a detailed tree-
bank with that of the same model trained with
semi-supervised techniques that require only un-
labeled partially-bracketed data. We contrast
an IOA-based EM method for training a PGLR
parser (Inui et al, 1997), similar to the method
applied by Pereira and Schabes to PCFGs, to a
range of confidence-based semi-supervised meth-
ods described below. The IOA is a generaliza-
tion of the Baum-Welch or Forward-Backward
algorithm, another instance of EM, which can be
used to train Hidden Markov Models (HMMs).
Elworthy (1994) and Merialdo (1994) demon-
strated that Baum-Welch does not necessarily
improve the performance of an HMM part-of-
speech tagger when deployed in an unsuper-
vised or semi-supervised setting. These some-
what negative results, in contrast to those of
Pereira and Schabes (1992), suggest that EM
techniques require fairly determinate training
data to yield useful models. Another motiva-
tion to explore alternative non-iterative meth-
ods is that the derivation space over partially-
bracketed data can remain large (>1K) while
the confidence-based methods we explore have a
total processing overhead equivalent to one iter-
ation of an IOA-based EM algorithm.
As we utilize an initial model to annotate ad-
ditional training data, our methods are closely
related to self-training methods described in the
literature (e.g. McClosky et al 2006, Bacchi-
ani et al 2006). However these methods have
been applied to fully-annotated training data
to create the initial model, which is then used
to annotate further training data derived from
unannotated text. Instead, we train entirely
from partially-bracketed data, starting from the
small proportion of ?unambiguous? data whereby
a single parse is consistent with the annota-
tion. Therefore, our methods are better de-
scribed as semi-supervised and the main focus
of this work is the flexible re-use of existing
treebanks to train a wider variety of statistical
parsing models. While many statistical parsers
extract a context-free grammar in parallel with
a statistical parse selection model, we demon-
strate that existing treebanks can be utilized to
train parsers that deploy grammars that make
other representational assumptions. As a result,
our methods can be applied by a range of parsers
to minimize the manual effort required to train
a parser or adapt to a new domain.
?2 gives details of the parsing system that are
relevant to this work. ?3 and ?4 describe our
data and evaluation schemes, respectively. ?5
describes our semi-supervised training methods.
?6 explores the problem of tuning a parser to a
new domain. Finally, ?7 gives conclusions and
future work.
2 The Parsing System
Sentences are automatically preprocessed in a
series of modular pipelined steps, including to-
kenization, part of speech (POS) tagging, and
morphological analysis, before being passed to
the statistical parser. The parser utilizes a man-
ually written feature-based unification grammar
over POS tag sequences.
2.1 The Parse Selection Model
A context-free ?backbone? is automatically de-
rived from the unification grammar1 and a gen-
eralized or non-deterministic LALR(1) table is
1This backbone is determined by compiling out the
values of prespecified attributes. For example, if we com-
pile out the attribute PLURAL which has 2 possible val-
ues (plural or not) we will create 2 CFG rules for each
rule with categories that contain PLURAL. Therefore,
no information is lost during this process.
24
constructed from this backbone (Tomita, 1987).
The residue of features not incorporated into
the backbone are unified on each reduce action
and if unification fails the associated derivation
paths also fail. The parser creates a packed
parse forest represented as a graph-structured
stack.2 The parse selection model ranks com-
plete derivations in the parse forest by com-
puting the product of the probabilities of the
(shift/reduce) parse actions (given LR state and
lookahead item) which created each derivation
(Inui et al, 1997).
Estimating action probabilities, consists of
a) recording an action history for the correct
derivation in the parse forest (for each sen-
tence in a treebank), b) computing the fre-
quency of each action over all action histories
and c) normalizing these frequencies to deter-
mine probability distributions over conflicting
(i.e. shift/reduce or reduce/reduce) actions.
Inui et al (1997) describe the probability
model utilized in the system where a transition
is represented by the probability of moving from
one stack state, ?i?1, (an instance of the graph
structured stack) to another, ?i. They estimate
this probability using the stack-top state si?1,
next input symbol li and next action ai. This
probability is conditioned on the type of state
si?1. Ss and Sr are mutually exclusive sets
of states which represent those states reached
after shift or reduce actions, respectively. The
probability of an action is estimated as:
P (li, ai, ?i|?i?1) ?
{
P (li, ai|si?1) si?1 ? Ss
P (ai|si?1, li) si?1 ? Sr
}
Therefore, normalization is performed over all
lookaheads for a state or over each lookahead
for the state depending on whether the state is
a member of Ss or Sr, respectively (hereafter
the I function). In addition, Laplace estimation
can be used to ensure that all actions in the
2The parse forest is an instance of a feature forest as
defined by Miyao and Tsujii (2002). We will use the term
?node? herein to refer to an element in a derivation tree
or in the parse forest that corresponds to a (sub-)analysis
whose label is the mother?s label in the corresponding CF
?backbone? rule.
table are assigned a non-zero probability (the
IL function).
3 Training Data
The treebanks we use in this work are in one of
two possible formats. In either case, a treebank
T consists of a set of sentences. Each sentence
t is a pair (s,M), where s is the automatically
preprocessed set of POS tagged tokens (see ?2)
and M is either a fully annotated derivation, A,
or an unlabeled bracketing U . This bracketing
may be partial in the sense that it may be com-
patible with more than one derivation produced
by a given parser. Although occasionally the
bracketing is itself complete but alternative non-
terminal labeling causes indeterminacy, most of-
ten the ?flatter? bracketing available from ex-
tant treebanks is compatible with several alter-
native ?deeper? mostly binary-branching deriva-
tions output by a parser.
3.1 Derivation Consistency
Given t = (s,A), there will exist a single deriva-
tion in the parse forest that is compatible (cor-
rect). In this case, equality between the deriva-
tion tree and the treebank annotation A iden-
tifies the correct derivation. Following Pereira
and Schabes (1992) given t = (s, U), a node?s
span in the parse forest is valid if it does not
overlap with any span outlined in U , and hence,
a derivation is correct if the span of every node
in the derivation is valid in U . That is, if no
crossing brackets are present in the derivation.
Thus, given t = (s, U), there will often be more
than one derivation compatible with the partial
bracketing.
Given the correct nodes in the parse forest
or in derivations, we can then extract the cor-
responding action histories and estimate action
probabilities as described in ?2.1. In this way,
partial bracketing is used to constrain the set of
derivations considered in training to those that
are compatible with this bracketing.
3.2 The Susanne Treebank and
Baseline Training Data
The Susanne Treebank (Sampson, 1995) is uti-
lized to create fully annotated training data.
25
This treebank contains detailed syntactic deriva-
tions represented as trees, but the node label-
ing is incompatible with the system grammar.
We extracted sentences from Susanne and auto-
matically preprocessed them. A few multiwords
are retokenized, and the sentences are retagged
using the POS tagger, and the bracketing de-
terministically modified to more closely match
that of the grammar, resulting in a bracketed
corpus of 6674 sentences. We will refer to this
bracketed treebank as S, henceforth.
A fully-annotated and system compatible
treebank of 3543 sentences from S was also
created. We will refer to this annotated tree-
bank, used for fully supervised training, as B.
The system parser was applied to construct
a parse forest of analyses which are compati-
ble with the bracketing. For 1258 sentences,
the grammar writer interactively selected cor-
rect (sub)analyses within this set until a sin-
gle analysis remained. The remaining 2285 sen-
tences were automatically parsed and all consis-
tent derivations were returned. Since B contains
more than one possible derivation for roughly
two thirds of the data the 1258 sentences (paired
with a single tree) were repeated twice so that
counts from these trees were weighted more
highly. The level of reweighting was determined
experimentally using some held out data from
S. The baseline supervised model against which
we compare in this work is defined by the func-
tion IL(B) as described in ?2.1. The costs of
deriving the fully-annotated treebank are high
as interactive manual disambiguation takes an
average of ten minutes per sentence, even given
the partial bracketing derived from Susanne.
3.3 The WSJ PTB Training Data
The Wall Street Journal (WSJ) sections of the
Penn Treebank (PTB) are employed as both
training and test data by many researchers in
the field of statistical parsing. The annotated
corpus implicitly defines a grammar by provid-
ing a labeled bracketing over words annotated
with POS tags. We extracted the unlabeled
bracketing from the de facto standard training
sections (2-21 inclusive).3 We will refer to the
resulting corpus as W and the combination (con-
catenation) of the partially-bracketed corpora S
and W as SW .
3.4 The DepBank Test Data
King et al (2003) describe the development
of the PARC 700 Dependency Bank, a gold-
standard set of relational dependencies for 700
sentences (from the PTB) drawn at random
from section 23 of the WSJ (the de facto stan-
dard test set for statistical parsing). In all the
evaluations reported in this paper we test our
parser over a gold-standard set of relational de-
pendencies compatible with our parser output
derived (Briscoe and Carroll, 2006) from the
PARC 700 Dependency Bank (DepBank, hence-
forth).
The Susanne Corpus is a (balanced) subset of
the Brown Corpus which consists of 15 broad
categories of American English texts. All but
one category (reportage text) is drawn from dif-
ferent domains than the WSJ. We therefore, fol-
lowing Gildea (2001) and others, consider S, and
also the baseline training data, B, as out-of-
domain training data.
4 The Evaluation Scheme
The parser?s output is evaluated using a rela-
tional dependency evaluation scheme (Carroll,
et al, 1998; Lin, 1998) with standard measures:
precision, recall and F1. Relations are organized
into a hierarchy with the root node specifying an
unlabeled dependency. The microaveraged pre-
cision, recall and F1 scores are calculated from
the counts for all relations in the hierarchy which
subsume the parser output. The microaveraged
F1 score for the baseline system using this eval-
uation scheme is 75.61%, which ? over similar
sets of relational dependencies ? is broadly com-
parable to recent evaluation results published by
King and collaborators with their state-of-the-
art parsing system (Briscoe et al, 2006).
3The pipeline is the same as that used for creating S
though we do not automatically map the bracketing to
be more consistent with the system grammar, instead,
we simply removed unary brackets.
26
4.1 Wilcoxon Signed Ranks Test
The Wilcoxon Signed Ranks (Wilcoxon, hence-
forth) test is a non-parametric test for statistical
significance that is appropriate when there is one
data sample and several measures. For example,
to compare the accuracy of two parsers over the
same data set. As the number of samples (sen-
tences) is large we use the normal approximation
for z. Siegel and Castellan (1988) describe and
motivate this test. We use a 0.05 level of sig-
nificance, and provide z-value probabilities for
significant results reported below. These results
are computed over microaveraged F1 scores for
each sentence in DepBank.
5 Training from Unlabeled
Bracketings
We parsed all the bracketed training data us-
ing the baseline model to obtain up to 1K top-
ranked derivations and found that a significant
proportion of the sentences of the potential set
available for training had only a single deriva-
tion compatible with their unlabeled bracket-
ing. We refer to these sets as the unambiguous
training data (?) and will refer to the remaining
sentences (for which more than one derivation
was compatible with their unlabeled bracketing)
as the ambiguous training data (?). The avail-
ability of significant quantities of unambiguous
training data that can be found automatically
suggests that we may be able to dispense with
the costly reannotation step required to gener-
ate the fully supervised training corpus, B.
Table 1 illustrates the split of the corpora into
mutually exclusive sets ?, ?, ?no match? and
?timeout?. The latter two sets are not utilized
during training and refer to sentences for which
all parses were inconsistent with the bracketing
and those for which no parses were found due
to time and memory limitations (self-imposed)
on the system.4 As our grammar is different
from that implicit in the WSJ PTB there is a
high proportion of sentences where no parses
were consistent with the unmodified PTB brack-
4As there are time and memory restrictions during
parsing, the SW results are not equal to the sum of those
from S and W analysis.
Corpus | ? | | ? | No Match Timeout
S 1097 4138 1322 191
W 6334 15152 15749 1094
SW 7409 19248 16946 1475
Table 1: Corpus split for S, W and SW .
eting. However, a preliminary investigation of
no matches didn?t yield any clear patterns of
inconsistency that we could quickly ameliorate
by simple modifications of the PTB bracketing.
We leave for the future a more extensive investi-
gation of these cases which, in principle, would
allow us to make more use of this training data.
An alternative approach that we have also ex-
plored is to utilize a similar bootstrapping ap-
proach with data partially-annotated for gram-
matical relations (Watson and Briscoe, 2007).
5.1 Confidence-Based Approaches
We use ? to build an initial model. We then
utilize this initial model to derive derivations
(compatible with the unlabeled partial brack-
eting) for ? from which we select additional
training data. We employ two types of selection
methods. First, we select the top-ranked deriva-
tion only and weight actions which resulted in
this derivation equally with those of the initial
model (C1). This method is similar to ?Viterbi
training? of HMMs though we do not weight
the corresponding actions using the top parse?s
probability. Secondly, we select more than one
derivation, placing an appropriate weight on
the corresponding action histories based on the
initial model?s confidence in the derivation. We
consider three such models, in which we weight
transitions corresponding to each derivation
ranked r with probability p in the set of size n
either using 1n , 1r or p itself to weight counts.5
For example, given a treebank T with sentences
t = (s, U), function P to return the set of
parses consistent with U given t and function A
that returns the set of actions given a parse p,
then the frequency count of action ak in Cr is
5In ?2.1 we calculate action probabilities based on fre-
quency counts where we perform a weighted sum over
action histories and each history has a weight of 1. We
extend this scheme to include weights that differ between
action histories corresponding to each derivation.
27
determined as follows:
| ak |=
?|T |
i=1
?|P (ti)|
j=1,ak?A(pij)
1
j
These methods all perform normalization over
the resulting action histories using the training
function IL and will be referred to as Cn, Cr
and Cp, respectively. Cn is a ?uniform? model
which weights counts only by degree of ambi-
guity and makes no use of ranking information.
Cr weights counts by derivation rank, and Cp
is simpler than and different to one iteration of
EM as outside probabilities are not utilized. All
of the semi-supervised functions described here
take two arguments: an initial model and the
data to train over, respectively.
Models derived from unambiguous training
data, ?, alone are relatively accurate, achiev-
ing indistinguishable performance to that of the
baseline system given either W or SW as train-
ing data. We utilize these models as initial mod-
els and train over different corpora with each of
the confidence-based models. Table 2 gives re-
sults for all models. Results statistically signifi-
cant compared to the baseline system are shown
in bold print (better) or italic print (worse).
These methods show promise, often yielding sys-
tems whose performance is significantly better
than the baseline system. Method Cr achieved
the best performance in this experiment and re-
mained consistently better in those reported be-
low. Throughout the different approaches a do-
main effect can be seen, models utilizing just S
are worse, although the best performing models
benefit from the use of both S and W as training
data (i.e. SW ).
5.2 EM
Our EM model differs from that of Pereira and
Schabes as a PGLR parser adds context over
a PCFG so that a single rule can be applied
in several different states containing reduce ac-
tions. Therefore, the summation and normaliza-
tion performed for a CFG rule within IOA is in-
stead applied within such contexts. We can ap-
ply I (our PGLR normalization function with-
out Laplace smoothing) to perform the required
steps if we output the action history with the
Model Prec Rec F1 P (z)?
Baseline 77.05 74.22 75.61 -
IL(?(S)) 76.02 73.40 74.69 0.0294
C1(IL(?(S)), ?(S)) 77.05 74.22 75.61 0.4960
Cn(IL(?(S)), ?(S)) 77.51 74.80 76.13 0.0655
Cr(IL(?(S)), ?(S)) 77.73 74.98 76.33 0.0154
Cp(IL(?(S)), ?(S)) 76.45 73.91 75.16 0.2090
IL(?(W )) 77.01 74.31 75.64 0.1038
C1(IL(?(W )), ?(W )) 76.90 74.23 75.55 0.2546
Cn(IL(?(W )), ?(W )) 77.85 75.07 76.43 0.0017
Cr(IL(?(W )), ?(W )) 77.88 75.04 76.43 0.0011
Cp(IL(?(W )), ?(W )) 77.40 74.75 76.05 0.1335
IL(?(SW )) 77.09 74.35 75.70 0.1003
C1(IL(?(SW )), ?(SW )) 76.86 74.21 75.51 0.2483
Cn(IL(?(SW )), ?(SW )) 77.88 75.05 76.44 0.0048
Cr(IL(?(SW )), ?(SW )) 78.01 75.13 76.54 0.0007
Cp(IL(?(SW )), ?(SW )) 77.54 74.95 76.23 0.0618
Table 2: Performance of all models on DepBank.
?represents the statistical significance of the sys-
tem against the baseline model.
corresponding normalized inside-outside weight
for each node (Watson et al, 2005).
We perform EM starting from two initial mod-
els; either a uniform probability model, IL(), or
from models derived from unambiguous train-
ing data, ?. Figure 1 shows the cross entropy
decreasing monotonically from iteration 2 (as
guaranteed by the EM method) for different cor-
pora and initial models. Some models show an
initial increase in cross-entropy from iteration 1
to iteration 2, because the models are initial-
ized from a subset of the data which is used to
perform maximisation. Cross-entropy increases,
by definition, as we incorporate ambiguous data
with more than one consistent derivation.
Performance over DepBank can be seen in
Figures 2, 3, and 4 for each dataset S, W and
SW, respectively. Comparing the Cr and EM
lines in each of Figures 2, 3, and 4, it is evident
that Cr outperforms EM across all datasets, re-
gardless of the initial model applied. In most
cases, these results are significant, even when
we manually select the best model (iteration)
for EM.
The graphs of EM performance from itera-
tion 1 illustrate the same ?classical? and ?initial?
patterns observed by Elworthy (1994). When
EM is initialized from a relatively poor model,
such as that built from S (Figure 2), a ?classical?
28
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
0 2 4 6 8 10 12 14 16
H(C,G)
Iteration Number
EM(IL(), S)
r
r
r r r r r r r r r r r r r
r
EM(IL(?(S)), S)
c
c
c c c c c c c c c c c c c
c
EM(IL(), W )
4
4 4 4 4 4 4 4 4 4 4 4 4 4 4
4
EM(IL(?(W )),W )
?
? ? ? ? ? ? ? ? ? ? ? ? ? ?
?
EM(IL(), SW )
2
2 2 2 2 2 2 2 2 2 2 2 2 2 2
2
EM(IL(?(SW )), SW )
3
3 3 3 3 3 3 3 3 3 3 3 3 3 3
3
Figure 1: Cross Entropy Convergence for vari-
ous training data and models, with EM.
pattern emerges with relatively steady improve-
ment from iteration 1 until performance asymp-
totes. However, when the starting point is better
(Figures 3 and 4), the ?initial? pattern emerges
in which the best performance is reached after a
single iteration.
6 Tuning to a New Domain
When building NLP applications we would want
to be able to tune a parser to a new domain
with minimal manual effort. To obtain training
data in a new domain, annotating a corpus with
partial-bracketing information is much cheaper
than full annotation. To investigate whether
such data would be of value, we considered W
to be the corpus over which we were tuning and
applied the best performing model trained over
S, Cr(IL(?(S)), ?(S)), as our initial model. Fig-
ure 5 illustrates the performance of Cr compared
to EM.
Tuning using Cr was not significantly differ-
ent from the model built directly from the entire
data set with Cr, achieving 76.57% as opposed
to 76.54% F1 (see Table 2). By contrast, EM
performs better given all the data from the be-
ginning rather than tuning to the new domain.
74
74.5
75
75.5
76
76.5
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(S)), ?(S))
EM(IL(), S)
r
r r
r
r
r r r r r r r
r
r r
r
EM(IL(?(S)), S)
b
b b
b
b b
b
b
b b b b b b b
b
b
Figure 2: Performance over S for Cr and EM.
75
75.2
75.4
75.6
75.8
76
76.2
76.4
76.6
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(W )), ?(W ))
EM(IL(), W )
r
r
r
r r r r
r r r r
r
r r r
r
EM(IL(?(W )),W )
b
b
b
b
b b b b b b
b b b b
b b
b
Figure 3: Performance over W for Cr and EM.
29
75
75.2
75.4
75.6
75.8
76
76.2
76.4
76.6
76.8
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(SW )), ?(SW ))
EM(IL(), SW )
r r
r
r r r r r r r r r r
r r
r
EM(IL(?(SW )), SW )
b
b
b
b
b b b b b b b b b b b b
b
Figure 4: Performance over SW for Cr and EM.
Cr generally outperforms EM, though it is worth
noting the behavior of EM given only the tun-
ing data (W ) rather than the data from both do-
mains (SW ). In this case, the graph illustrates a
combination of Elworthy?s ?initial? and ?classical?
patterns. The steep drop in performance (down
to 69.93% F1) after the first iteration is proba-
bly due to loss of information from S. However,
this run also eventually converges to similar per-
formance, suggesting that the information in S
is effectively disregarded as it forms only a small
portion of SW , and that these runs effectively
converge to a local maximum over W .
Bacchiani et al (2006), working in a similar
framework, explore weighting the contribution
(frequency counts) of the in-domain and out-of-
domain training datasets and demonstrate that
this can have beneficial effects. Furthermore,
they also tried unsupervised tuning to the in-
domain corpus by weighting parses for it by
their normalized probability. This method is
similar to our Cp method. However, when we
tried unsupervised tuning using the WSJ and
an initial model built from S in conjunction with
our confidence-based methods, performance de-
graded significantly.
74
74.5
75
75.5
76
76.5
77
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(SW )), ?(SW ))
Cr(Cr(IL(?(S)), ?(S)), W )
EM(IL(?(SW )), SW )
b
b
b
b
b b b b b b b b b b b b
b
EM(Cr(IL(?(S)), ?(S)), W )
r
r
r
r
r r
r
r r r r r
r r r
r
EM(Cr(IL(?(S)), ?(S)), SW )
c
c
c
c c c c c
c c c c c c c c
c
Figure 5: Tuning over the WSJ PTB (W ) from
Susanne Corpus (S).
7 Conclusions
We have presented several semi-supervised
confidence-based training methods which have
significantly improved performance over an ex-
tant (more supervised) method, while also re-
ducing the manual effort required to create
training or tuning data. We have shown
that given a medium-sized unlabeled partially
bracketed corpus, the confidence-based models
achieve superior results to those achieved with
EM applied to the same PGLR parse selection
model. Indeed, a bracketed corpus provides flex-
ibility as existing treebanks can be utilized de-
spite the incompatibility between the system
grammar and the underlying grammar of the
treebank. Mapping an incompatible annotated
treebank to a compatible partially-bracketed
corpus is relatively easy compared to mapping
to a compatible fully-annotated corpus.
An immediate benefit of this work is that
(re)training parsers with incrementally-modified
grammars based on different linguistic frame-
works should be much more straightforward ?
see, for example Oepen et al (2002) for a good
discussion of the problem. Furthermore, it sug-
gests that it may be possible to usefully tune
30
a parser to a new domain with less annotation
effort.
Our findings support those of Elworthy (1994)
and Merialdo (1994) for POS tagging and sug-
gest that EM is not always the most suit-
able semi-supervised training method (espe-
cially when some in-domain training data is
available). The confidence-based methods were
successful because the level of noise introduced
did not outweigh the benefit of incorporating
all derivations compatible with the bracketing
in which the derivations contained a high pro-
portion of correct constituents. These findings
may not hold if the level of bracketing available
does not adequately constrain the parses consid-
ered ? see Hwa (1999) for a related investigation
with EM.
In future work we intend to further investigate
the problem of tuning to a new domain, given
that minimal manual effort is a major prior-
ity. We hope to develop methods which required
no manual annotation, for example, high preci-
sion automatic partial bracketing using phrase
chunking and/or named entity recognition tech-
niques might yield enough information to sup-
port the training methods developed here.
Finally, further experiments on weighting the
contribution of each dataset might be beneficial.
For instance, Bacchiani et al (2006) demon-
strate imrpovements in parsing accuracy with
unsupervised adaptation from unannotated data
and explore the effect of different weighting of
counts derived from the supervised and unsu-
pervised data.
Acknowledgements
The first author is funded by the Overseas Re-
search Students Awards Scheme, and the Poyn-
ton Scholarship awarded by the Cambridge Aus-
tralia Trust in collaboration with the Cam-
bridge Commonwealth Trust. Development of
the RASP system was and is supported by the
EPSRC (grants GR/N36462, GR/N36493 and
GR/T19919).
References
Bacchiani, M., Riley, M., Roark, B. and R.
Sproat (2006) ?MAP adaptation of stochas-
tic grammars?, Computer Speech and Lan-
guage, vol.20.1, pp.41?68.
Baker, J. K. (1979) ?Trainable grammars for
speech recognition? in Klatt, D. and Wolf,
J. (eds.), Speech Communications Papers for
the 97th Meeting of the Acoustical Society of
America, MIT, Cambridge, Massachusetts,
pp. 557?550.
Briscoe, E.J., J. Carroll and R. Watson (2006)
?The Second Release of the RASP System?,
Proceedings of ACL-Coling?06, Sydney, Aus-
tralia.
Carroll, J., Briscoe, T. and Sanfilippo, A. (1998)
?Parser evaluation: a survey and a new
proposal?, Proceedings of LREC, Granada,
pp. 447?454.
Clark, S. and J. Curran (2004) ?Parsing the WSJ
Using CCG and Log-Linear Models?, Pro-
ceedings of 42nd Meeting of the Association
for Computational Linguistics, Barcelona,
pp. 103?110.
Collins, M. (1999) Head-driven Statistical Mod-
els for Natural Language Parsing, PhD Dis-
sertation, University of Pennsylvania.
Elworthy, D. (1994) ?Does Baum-Welch Re-
estimation Help Taggers??, Proceedings of
ANLP, Stuttgart, Germany, pp. 53?58.
Gildea, D. (2001) ?Corpus variation and parser
performance?, Proceedings of EMNLP, Pitts-
burgh, PA.
Hockenmaier, J. (2003) Data and models for sta-
tistical parsing with Combinatory Categorial
Grammar, PhD Dissertation, The Univer-
sity of Edinburgh.
Hwa, R. (1999) ?Supervised grammar induction
using training data with limited constituent
information?, Proceedings of ACL, College
Park, Maryland, pp. 73?79.
Inui, K., V. Sornlertlamvanich, H. Tanaka and
T. Tokunaga (1997) ?A new formalization
of probabilistic GLR parsing?, Proceedings
31
of IWPT, MIT, Cambridge, Massachusetts,
pp. 123?134.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple
and R. Kaplan (2003) ?The PARC700 De-
pendency Bank?, Proceedings of LINC, Bu-
dapest.
Lin, D. (1998) ?Dependency-based evaluation
of MINIPAR?, Proceedings of Workshop at
LREC?98 on The Evaluation of Parsing Sys-
tems, Granada, Spain.
McClosky, D., Charniak, E. and M. Johnson
(2006) ?Effective self-training for parsing?,
Proceedings of HLT-NAACL, New York.
Merialdo, B. (1994) ?Tagging English Text with
a Probabilistic Model?, Computational Lin-
guistics, vol.20.2, pp.155?171.
Miyao, Y. and J. Tsujii (2002) ?Maximum En-
tropy Estimation for Feature Forests?, Pro-
ceedings of HLT, San Diego, California.
Oepen, S., K. Toutanova, S. Shieber, C. Man-
ning, D. Flickinger, and T. Brants (2002)
?The LinGO Redwoods Treebank: Motiva-
tion and preliminary applications?, Proceed-
ings of COLING, Taipei, Taiwan.
Pereira, F and Y. Schabes (1992) ?Inside-
Outside Reestimation From Partially
Bracketed Corpora?, Proceedings of ACL,
Delaware.
Prescher, D. (2001) ?Inside-outside estimation
meets dynamic EM?, Proceedings of 7th
Int. Workshop on Parsing Technologies
(IWPT01), Beijing, China.
Riezler, S., T. King, R. Kaplan, R. Crouch,
J. Maxwell III and M. Johnson (2002)
?Parsing the Wall Street Journal using a
Lexical-Functional Grammar and Discrimi-
native Estimation Techniques?, Proceedings
of 40th Annual Meeting of the Association
for Computational Linguistics, Philadelphia,
pp. 271?278.
Sampson, G. (1995) English for the Computer,
Oxford University Press, Oxford, UK.
Siegel S. and N. J. Castellan (1988) Nonpara-
metric Statistics for the Behavioural Sci-
ences, 2nd edition, McGraw-Hill.
Tadayoshi, H., Y. Miyao and J. Tsujii (2005)
?Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain?,
Proceedings of IJCNLP, Jeju Island, Korea.
Tomita, M. (1987) ?An Efficient Augmented
Context-Free Parsing Algorithm?, Computa-
tional Linguistics, vol.13(1?2), pp.31?46.
Watson, R. and E.J. Briscoe (2007) ?Adapting
the RASP system for the CoNLL07 domain-
adaptation task?, Proceedings of EMNLP-
CoNLL-07, Prague.
Watson, R., J. Carroll and E.J. Briscoe (2005)
?Efficient extraction of grammatical rela-
tions?, Proceedings of 9th Int. Workshop on
Parsing Technologies (IWPT?05), Vancou-
ver, Ca..
32
Proceedings of the 10th Conference on Parsing Technologies, pages 48?59,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Efficiency in Unification-Based N -Best Parsing
Yi Zhang?, Stephan Oepen?, and John Carroll?
?Saarland University, Department of Computational Linguistics, and DFKI GmbH (Germany)
?University of Oslo, Department of Informatics (Norway)
?University of Sussex, Department of Informatics (UK)
Abstract
We extend a recently proposed algorithm for
n-best unpacking of parse forests to deal ef-
ficiently with (a) Maximum Entropy (ME)
parse selection models containing important
classes of non-local features, and (b) forests
produced by unification grammars contain-
ing significant proportions of globally incon-
sistent analyses. The new algorithm empir-
ically exhibits a linear relationship between
processing time and the number of analyses
unpacked at all degrees of ME feature non-
locality; in addition, compared with agenda-
driven best-first parsing and exhaustive pars-
ing with post-hoc parse selection it leads to
improved parsing speed, coverage, and ac-
curacy.?
1 Background?Motivation
Technology for natural language analysis using lin-
guistically precise grammars has matured to a level
of coverage and efficiency that enables parsing of
large amounts of running text. Research groups
working within grammatical frameworks like CCG
(Clark & Curran, 2004), LFG (Riezler et al, 2002),
and HPSG (Malouf & van Noord, 2004; Oepen,
Flickinger, Toutanova, & Manning, 2004; Miyao,
Ninomiya, & Tsujii, 2005) have successfully in-
tegrated broad-coverage computational grammars
with sophisticated statistical parse selection models.
The former delineate the space of possible analy-
ses, while the latter provide a probability distribu-
?The first author warmly acknowledges the guidance of his
PhD advisors, Valia Kordoni and Hans Uszkoreit. We are grate-
ful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger,
and Erik Velldal for many discussions and their support. We
thank Ron Kaplan, Martin Kay, and Bob Moore for provid-
ing insightful information about related approaches, notably the
XLE and CLE parsers.
tion over competing hypotheses. Parse selection ap-
proaches for these frameworks often use discrimi-
native Maximum Entropy (ME) models, where the
probability of each parse tree, given an input string,
is estimated on the basis of select properties (called
features) of the tree (Abney, 1997; Johnson, Ge-
man, Canon, Chi, & Riezler, 1999). Such features,
in principle, are not restricted in their domain of
locality, and enable the parse selection process to
take into account properties that extend beyond lo-
cal contexts (i.e. sub-trees of depth one).
There is a trade-off in this set-up between the ac-
curacy of the parse selection model, on the one hand,
and the efficiency of the search for the best solu-
tion(s), on the other hand. Extending the context size
of ME features, within the bounds of available train-
ing data, enables increased parse selection accuracy.
However, the interplay of the core parsing algo-
rithm and the probabilistic ranking of alternate (sub-
)hypotheses becomes considerably more complex
and costly when the feature size exceeds the domain
of locality (of depth-one trees) that is characteristic
of phrase structure grammar-based formalisms. One
current line of research focuses on finding the best
balance between parsing efficiency and parse selec-
tion techniques of increasing complexity, aiming to
identify the most probable solution(s) with minimal
effort.
This paper explores a range of techniques, com-
bining a broad-coverage, high-efficiency HPSG
parser with a series of parse selection models with
varying context size of features. We sketch three
general scenarios for the integration: (a) a baseline
sequential configuration, where all results are enu-
merated first, and subsequently ranked; (b) an in-
terleaved but approximative solution, performing a
greedy search for an n-best list of results; and (c) a
two-phase approach, where a complete packed for-
48
est is created and combined with a specialized graph
search procedure to selectively enumerate results in
(globally) correct rank order. Although conceptu-
ally simple, the second technique has not previously
been evaluated for HPSG parsing (to the best of our
knowledge). The last of these techniques, which we
call selective unpacking, was first proposed by Car-
roll & Oepen (2005) in the context of chart-based
generation. However, they only provide an account
of the algorithm for local ME properties and assert
that the technique should generalize to larger con-
texts straightforwardly. This paper describes this
generalization of selective unpacking, in its appli-
cation to parsing, and demonstrates that the move
from features that resemble a context-free domain
of locality to features of, in principle, arbitrary con-
text size can indeed be based on the same algorithm,
but the required extensions are non-trivial.
The structure of the paper is as follows. Sec-
tion 2 summarizes our formalism, grammars used,
parse selection approach, and training and test data.
Section 3 discusses the range of possibilities for
structuring the process of statistical, grammar-based
parsing, and Sections 4 to 6 describe our approach
to efficient n-best parsing. We present experimental
results in Section 7, compare our approach to previ-
ous ones (Section 8), and finally conclude.
2 Overall Set-up
While couched in the HPSG framework, the tech-
niques explored here are applicable to the larger
class of unification-based grammar formalisms. We
make use of the DELPH-IN1 reference formalism,
as implemented by a variety of systems, including
the LKB (Copestake, 2002) and PET (Callmeier,
2002). For the experiments discussed here, we
adapted the open-source PET parsing engine in
conjunction with two publicly available grammars,
the English Resource Grammar (ERG; Flickinger,
2000) and the DFKI German Grammar (GG; Mu?ller
& Kasper, 2000, Crysmann, 2005). Our parse se-
lection models were trained and evaluated on HPSG
treebanks that are distributed with these grammars.
The following paragraphs summarize relevant prop-
erties of the structures manipulated by the parser,
1Deep Linguistic Processing with HPSG, an open-
source repository of grammars and processing tools; see
?http://www.delph-in.net/?.
subjh
hspec
det the le
the
sing noun
n intr le
dog
third sg fin verb
v unerg le
barks
Figure 1: Sample HPSG derivation tree for the sentence the
dog barks. Phrasal nodes are labeled with identifiers of gram-
mar rules, and (pre-terminal) lexical nodes with class names for
types of lexical entries.
followed by relevant background on parse selection.
Figure 1 shows an example ERG derivation tree.
Internal tree nodes are labeled with identifiers of
grammar rules, and leaves with lexical entries. The
derivation tree provides complete information about
the actual HPSG analysis, in the sense that it can be
viewed as a recipe for computing it. Lexical entries
and grammar rules alike are ultimately just feature
structures, complex and highly-structured linguistic
categories. When unified together in the configura-
tion depicted by the derivation tree, the resulting fea-
ture structure yields an HPSG sign, a detailed repre-
sentation of the syntactic and semantic properties of
the input string. Just as the full derivation denotes a
feature structure, so do its sub-trees, and for gram-
mars like the ERG and GG each such structure will
contain hundreds of feature ? value pairs.
Because of the lexicalized nature of HPSG (and
similar frameworks) our parsers search for well-
formed derivations in a pure bottom-up fashion.
Other than that, there are no hard-wired assumptions
about the order of computation, i.e. the specific pars-
ing strategy. Our basic set-up closely mimics that of
Oepen & Carroll (2002), where edges indexed by
sub-string positions in a chart represent the nodes of
the tree, recording both a feature structure (as its cat-
egory label) and the identity of the underlying lexi-
cal entry or rule in the grammar. Multiple edges de-
rived for identical sub-strings can be ?packed? into a
single chart entry in case their feature structures are
compatible, i.e. stand in an equivalence or subsump-
tion relation. By virtue of having each edge keep
back-pointers to its daughter edges?the immediate
sub-nodes in the tree whose combination resulted in
49
the mother edge?the parse forest provides a com-
plete and explicit encoding of all possible results in a
maximally compact form.2 A simple unpacking pro-
cedure is obtained from the cross-multiplication of
all local combinatorics, which is directly amenable
to dynamic programming.
Figure 2 shows a hypothetical forest (on the left),
where sets of edges exhibiting local ambiguity have
been packed into a single ?representative? edge, viz.
the one in each set with one or more incoming dom-
inance arcs. Confirming the findings of Oepen &
Carroll (2002), in our experiments packing under
feature structure subsumption is much more effec-
tive than packing under mere equivalence, i.e. for
each pair of edges (over identical sub-strings) that
stand in a subsumption relation, a technique that
Oepen & Carroll (2002) termed retro-active pack-
ing ensures that the more general of the two edges
remains in the chart. When packing under subsump-
tion, however, some of the cross-product of local
ambiguities in the forest may not be globally con-
sistent. Assume for example that, in Figure 2, edges
6 and 8 subsume 7 and 9 , respectively; combining
7 and 9 into the same tree during unpacking can in
principle fail. Thus, unpacking effectively needs to
deterministically replay unifications, but this extra
expense in our experience is negligible when com-
pared to the decreased cost of constructing the for-
est under subsumption. In Section 3 we argue that
this very property, in addition to increasing parsing
efficiency, interacts beneficially with parse selection
and on-demand enumeration of results in rank order.
Following (Johnson et al, 1999), a conditional
ME model of the probabilities of trees {t1 . . . tn}
for a string s, and assuming a set of feature
functions {f1 . . . fm} with corresponding weights
{?1 . . . ?m}, is defined as:
p(ti|s) =
exp?j ?jfj(ti)
?n
k=1 exp
?
j ?jfj(tk)
(1)
2This property of parse forests is not a prerequisite of the
chart parsing framework. The basic CKY procedure (Kasami,
1965), for example, as well as many unification-based adapta-
tions (e.g. the Core Language Engine; Moore & Alshawi, 1992)
merely record the local category of each edge, which is suffi-
cient for the recognition task and simplifies the search. How-
ever, reading out complete trees from the chart, then, amounts
to a limited form of search, going back to the rules of the gram-
mar itself to (re-)discover decomposition relations among chart
entries.
Type Sample Features
1 ?0 subjh hspec third sg fin verb?
1 ?1 ? subjh hspec third sg fin verb?
1 ?0 hspec det the le sing noun?
1 ?1 subjh hspec det the le sing noun?
1 ?2 ? subjh hspec det the le sing noun?
2 ?0 subjh third sg fin verb?
2 ?0 subjh hspce?
2 ?1 subjh hspec det the le?
2 ?1 subjh hspec sing noun?
3 ?1 n intr le dog?
3 ?2 det the le n intr le dog?
3 ?3  det the le n intr le dog?
Table 1: Examples of structural features extracted from the
derivation tree in Figure 1. The Type column indicates the
template corresponding to each sample feature; the integer that
starts each feature indicates the degree of grandparenting (in the
case of type 1 and 2 features) or n-gram size (type 3 features).
The symbols ? and  denote the root of the tree and left pe-
riphery of the yield, respectively.
Feature functions fj can test for arbitrary structural
properties of analyses ti, and their value typically is
the number of times a specific property is present
in ti. Toutanova, Manning, Flickinger, & Oepen
(2005) propose an inventory of features that per-
form well in HPSG parse selection; currently we re-
strict ourselves to the best-performing of these, of
the form illustrated in Table 1, comprising depth-
one sub-trees (or portions of these) with grammar-
internal identifiers as node labels, plus optionally
a chain of one or more dominating nodes (i.e. lev-
els of grandparents). If a grandparents chain is
present then the feature is non-local. For expository
purposes, Table 1 includes another feature type, n-
grams over leaf nodes of the derivation; in Section 5
below we speculate about the incorporation of these
(and similar) features in our algorithm.
3 Interleaving Parsing and Ranking
At an abstract level, given a grammar and an associ-
ated ME parse selection model, there are three basic
ways of combining them in order to find the single
?best? or small set of n-best results.
The first way is a na??ve sequential set-up, in which
the parser first enumerates the full set of analyses,
computes a score for each using the model, and re-
turns the highest-ranking n results. For carefully
50
1 ?
?
2 3
?
|
?
4 3
?
2 ?
?
5 6
?
|
?
5 7
?
4 ?
?
8 6
?
|
?
8 7
?
|
?
9 6
?
|
?
9 7
?
6 ?
?
10
?
|
?
11
?
Figure 2: Sample forest and sub-node decompositions: ovals in the forest (on the left) indicate packing of edges under subsump-
tion, i.e. edges 4 , 7 , 9 , and 11 are not in the chart proper. During unpacking, there will be multiple ways of instantiating a
chart edge, each obtained from cross-multiplying alternate daughter sequences locally. The elements of this cross-product we call
decomposition, and they are pivotal points both for stochastic scoring and dynamic programming in selective unpacking. The table
on the right shows all non-leaf decompositions for our example packed forest: given two ways of decomposing 6 , there will be
three candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees.
crafted grammars and inputs of average complexity
the approach can perform reasonably well.
Another mode of operation is to organize the
parser?s search according to an agenda (i.e. priority
queue) that assigns numeric scores to parsing moves
(Erbach, 1991). Each such move is an application of
the fundamental rule of chart parsing, combining an
active and a passive edge, and the scores represent
the expected ?figure of merit? (Caraballo & Char-
niak, 1998) of the resulting structure. Assuming a
parse selection model of the type sketched in Sec-
tion 2, we can determine the agenda priority for a
parsing move according to the (unnormalized) ME
score of the derivation (sub-)tree that would result
from its successful execution. Note that, unlike in
probabilistic context-free grammars (PCFGs), ME
scores of partial trees do not necessarily decrease as
the tree size increases; instead, the distribution of
feature weights is in the range (??,+?), centered
around 0, where negative weights intuitively corre-
spond to dis-preferred properties.
This lack of monotonicity in the scores associated
with sub-trees, on the one hand, is beneficial, in that
performing a greedy best-first search becomes prac-
tical: in contrast, with PCFGs and their monoton-
ically decreasing probabilities on larger sub-trees,
once the parser finds the first full tree the chart nec-
essarily has been instantiated almost completely. On
the other hand, the same property prohibits the appli-
cation of exact best-first techniques like A? search,
because there is no reliable future cost estimate; in
this respect, our set-up differs fundamentally from
that of Klein & Manning (2003) and related PCFG
parsing work. Using the unnormalized sum of ME
weights on a partial solution as its agenda score, ef-
fectively, means that sub-trees with low scores ?sink?
to the bottom of the agenda; highly-ranked partial
constituents, in turn, instigate the immediate cre-
ation of larger structures, and ideally the bottom-up
agenda-driven search will greedily steer the parser
towards full analyses with high scores. Given its
heuristic nature, this procedure cannot guarantee
that its n-best list of results corresponds to the glob-
ally correct rank order, but it may in practice come
reasonably close to it. While conceptually simple,
greedy best-first search does not combine easily with
ambiguity packing in the chart: (a) at least when
packing under subsumption, it is not obvious how
to accurately compute the agenda score of packed
nodes, and (b) to the extent that the greedy search
avoids exploration of dis-preferred local ambigu-
ity, the need for packing should be greatly reduced.
Unfortunately, in scoring bottom-up parsing moves,
ME features involving grandparenting are not ap-
plicable, leading to a second potential source of re-
duced parse selection accuracy. In Section 7 below,
we provide an empirical evaluation of both the na??ve
sequential and greedy best-first approaches.
4 Selective Unpacking
Carroll & Oepen (2005) observe that, at least for
grammars like the ERG, the construction of the
parse forest can be very efficient (with observed
polynomial complexity), especially when packing
edges under subsumption. Their selective unpacking
procedure, originally proposed for the forest created
by a chart generator, aims to unpack the n-best set
51
1 procedure selectively-unpack-edge(edge , n) ?
2 results? ??; i? 0;
3 do
4 hypothesis? hypothesize-edge(edge , i); i? i + 1;
5 if (new? instantiate-hypothesis(hypothesis)) then
6 n? n ? 1; results? results ? ?new?;
7 while (hypothesis and n ? 1)
8 return results;
9 procedure hypothesize-edge(edge , i) ?
10 if (edge.hypotheses[i]) return edge.hypotheses[i];
11 if (i = 0) then
12 for each (decomposition in decompose-edge(edge)) do
13 daughters? ? ?; indices? ? ?
14 for each (edge in decomposition.rhs) do
15 daughters? daughters ? ?hypothesize-edge(edge, 0)?;
16 indices? indices ? ?0?;
17 new-hypothesis(edge, decomposition, daughters, indices);
18 if (hypothesis? edge.agenda.pop()) then
19 for each (indices in advance-indices(hypothesis.indices)) do
20 if (indices ? hypothesis.decomposition.indices) then continue
21 daughters? ? ?;
22 for each (edge in hypothesis.decomposition.rhs) each (i in indices) do
23 daughter? hypothesize-edge(edge, i);
24 if (not daughter) then daughters? ??; break
25 daughters? daughters ? ?daughter?;
26 if (daughters) then new-hypothesis(edge, hypothesis.decomposition, daughters, indices)
27 edge.hypotheses[i]? hypothesis;
28 return hypothesis;
29 procedure new-hypothesis(edge , decomposition , daughters , indices) ?
30 hypothesis? new hypothesis(decomposition, daughters, indices);
31 edge.agenda.insert(score-hypothesis(hypothesis), hypothesis);
32 decomposition.indices? decomposition.indices? {indices};
Figure 3: Selective unpacking procedure, enumerating the n best realizations for a top-level result edge from a packed forest. An
auxiliary function decompose-edge() performs local cross-multiplication as shown in the examples in Figure 2. Another utility
function not shown in pseudo-code is advance-indices(), a ?driver? routine searching for alternate instantiations of daughter edges,
e.g. advance-indices(?0 2 1?)? {?1 2 1? ?0 3 1? ?0 2 2?}. Finally, instantiate-hypothesis() is the function that actually builds
result trees, replaying the unifications of constructions from the grammar (as identified by chart edges) with the feature structures
of daughter constituents.
of full trees from the forest, guaranteeing the glob-
ally correct rank order according to the probability
distribution, with a minimal amount of search. The
basic algorithm is a specialized graph search through
the forest, with local contexts of optimization corre-
sponding to packed nodes.
Each such node represents local combinatorics,
and two key notions in the selective unpacking pro-
cedure are the concepts of (a) decomposing an edge
locally into candidate ways of instantiating it, and
of (b) nested contexts of local search for ranked
hypotheses (i.e. uninstantiated edges) about candi-
date subtrees. See Figure 2 for examples of the de-
composition of edges. Given one decomposition?
i.e. a vector of candidate daughters for a particu-
lar rule?there can be multiple ways of instanti-
ating each daughter: a parallel index vector ~I =
?i0 . . . in? serves to keep track of ?vertical? search
among daughter hypotheses, where each index ij
denotes the i-th best instantiation (hypothesis) of
the daughter at position j. If we restrict ME fea-
tures to a depth of one (i.e. without grandparent-
ing), then given the additive nature of ME scores
on complete derivations, it can be guaranteed that
hypothesized trees including an edge e as an im-
mediate daughter must use the best instantiation of
e in their own best instantiation. Assuming a bi-
nary rule, the corresponding hypothesis would use
daughter indices of ?0 0?. The second-best instan-
tiation, in turn, can be obtained from moving to the
second-best hypothesis for one of the elements in the
(right-hand side of the) decomposition, e.g. indices
52
?0 1? or ?1 0? in the binary example. Hypotheses are
associated with ME scores and ordered within each
nested context by means of a local priority queue
(stored in the original representative edge, for con-
venience). Therefore, nested local optimizations re-
sult in a top-down, breadth-first, exact n-best search
through the packed forest, while avoiding exhaustive
cross-multiplication of packed nodes.
Figure 3 shows the unchanged pseudo-code of
Carroll & Oepen (2005). The main function
hypothesize-edge() controls both the ?horizontal? and
?vertical? search, initializing the set of decompo-
sitions and pushing initial hypotheses onto the lo-
cal agenda when called on an edge for the first
time (lines 11 ? 17). For each call, the procedure
retrieves the current next-best hypothesis from the
agenda (line 18), generates new hypotheses by ad-
vancing daughter indices (while skipping over con-
figurations seen earlier) and calling itself recursively
for each new index (lines 19 ? 26), and, finally, ar-
ranging for the resulting hypothesis to be cached for
later invocations on the same edge and i values (line
27). Note that unification (in instantiate-hypothesis())
is only invoked on complete, top-level hypotheses,
as our structural ME features can actually be eval-
uated prior to building each full feature structure.
However, as Carroll & Oepen (2005) suggest, the
procedure could be adapted to perform instantiation
of sub-hypotheses within each local search, should
additional features require it. For better efficiency,
the instantiate-hypothesis() routine applies dynamic
programming (i.e. memoization) to intermediate re-
sults.
5 Generalizing the Algorithm
Carroll & Oepen (2005) offer no solution for selec-
tive unpacking with larger context ME features. Yet,
both Toutanova et al (2005) and our own experi-
ments (described in Section 7 below) suggest that
properties of larger contexts and especially grand-
parenting can greatly improve parse selection ac-
curacy. The following paragraphs outline how to
generalize the basic selective unpacking procedure,
while retaining its key properties: exact n-best enu-
meration with minimal search. Our generalization of
the algorithm distinguishes between ?upward? con-
texts, with grandparenting with dominating nodes as
a representative feature type, and ?downward? exten-
sions, which we discuss for the example of lexical
n-gram features.
A na??ve approach to selective unpacking with
grandparenting might be extending the cross-
multiplication of local ambiguity to trees of more
than depth one. However, with multiple levels of
grandparenting this approach would greatly increase
the combinatorics to be explored, and it would pose
the puzzle of overlapping local contexts of opti-
mization. Choices made among the alternates for
one packed node would interact with other ambi-
guity contexts in their internal nodes, rather than
merely at the leaves of their decompositions. How-
ever, it is sufficient to keep the depth of decompo-
sitions to minimal sub-trees and rather contextual-
ize each decomposition as a whole. Assuming our
sample forest and set of decompositions from Fig-
ure 2, let ?1 4 ? : 6 ??10 ? denote the decomposi-
tion of node 6 in the context of 4 and 1 as its
immediate parents. When descending through the
forest, hypothesize-edge() can, without significant ex-
tra cost, maintain a vector ~P = ?pn . . . p0? of par-
ents of the current node, for n-level grandparenting.
For each packed node, the bookkeeping elements of
the graph search procedure need to be contextual-
ized on ~P , viz. (a) the edge-local priority queue,
(b) the record of index vectors hypothesized already,
and (c) the cache of previous instantiations. Assum-
ing each is stored in an associative array, then all
references to edge.agenda in the original procedure
can be replaced by edge.agenda[~P], and likewise for
other slots. With these extensions in place, the orig-
inal control structure of nested, on-demand creation
of hypotheses and dynamic programming of partial
results can be retained, and for each packed node
with multiple parents ( 6 in our sample forest) there
will be parallel, contextualized partitions of opti-
mization. Thus, extra combinatorics introduced in
this generalized procedure are confined to only such
nodes, which (intuitively at least) appears to estab-
lish the lower bound of added search needed?while
keeping the algorithm non-approximative. Section 7
provides empirical data on the degradation of the
procedure in growing levels of grandparenting and
the number of n-best results to be extracted from the
forest.
Finally, we turn to enlarged feature contexts that
53
capture information from nodes below the elements
of a local decomposition. Consider the example
of feature type 3 in Table 1, n-grams (of vari-
ous size) over properties of the yield of the parse
tree. For now we only consider lexical bi-grams.
For an edge e dominating a sub-string of n words
?wi . . . wi+n?1? there will be n? 1 bi-grams inter-
nal to e, and two bi-grams that interact with wi?1
and wi+n?which will be determined by the left-
and right-adjacent edges to e in a complete tree. The
internal bi-grams are unproblematic, and we can as-
sume that ME weights corresponding to these fea-
tures have been included in the sum of weights as-
sociated to e. Seeing that e may occur in multiple
trees, with different sister edges, the selective un-
packing procedure has to take this variation into ac-
count when evaluating local contexts of optimiza-
tion.
Let xey denote an edge e, with x and y as the
lexical types of its leftmost and rightmost daugh-
ters, respectively. Returning to our sample forest,
assume lexicalizations ? 10? and ? 11 ? (each span-
ning only one word), with ? 6= ?. Obviously, when
decomposing 4 as ?8 6 ?, its ME score, in turn, will
depend on the choice made in the expansion of 6 :
the sequences
?
? 8? ? 6?
?
and
?
? 8? ? 6 ?
?
will dif-
fer in (at least) the scores associated with the bi-
grams ???? vs. ????. Accordingly, when evalu-
ating candidate decompositions of 4 , the number of
hypotheses that need to be considered is doubled;
as an immediate consequence, there can be up to
eight distinct lexicalized variants for the decompo-
sition 1 ??4 3 ? further up in the tree. It may look
as if combinatorics will cross-multiply throughout
the tree?in the worst case returning us to an ex-
ponential number of hypotheses?but this is fortu-
nately not the case: regarding the external bi-grams
of 1 , node 6 no longer participates in its left- or
rightmost periphery, so variation internal to 6 is not
a multiplicative factor at this level. This is essen-
tially the observation of Langkilde (2000), and her
bottom-up factoring of n-gram computation is eas-
ily incorporated into our top-down selective unpack-
ing control structure. At the point where hypothesize-
edge() invokes itself recursively (line 23 in Figure 3),
its return value is now a set of lexicalized alternates,
and hypothesis creation (in line 26) can take into ac-
count the local cross-product of all such alternation.
Including additional properties from non-local sub-
trees (for example higher-order n-grams and head
lexicalization) is a straightforward extension of this
scheme, replacing our per-edge left- and rightmost
periphery symbols with a generalized vector of ex-
ternally relevant, internal properties. In addition
to traditional (head) lexicalization as we have just
discussed it, such extended ?downward? properties
on decompositions?percolated from daughters to
mothers and cross-multiplied as appropriate?could
include metrics of constituent weight too, for exam-
ple to enable the ME model to prefer ?balanced? co-
ordination structures.
However, given that Toutanova et al (2005) ob-
tain only marginally improved parse selection accu-
racy from the inclusion of n-gram (and other lexical)
ME features, we have left the implementation of lex-
icalization and empirical evaluation for future work.
6 Failure Caching and Propagation
As we pointed out at the end of Section 4, during
the unpacking phase, unification is only replayed in
instantiate-hypothesis() on the top-level hypotheses. It
is only at this step that inconsistencies in the local
combinatorics are discovered. However, such a dis-
covery can be used to improve the unpacking rou-
tine by (a) avoiding further unification on hypothe-
ses that have already failed to instantiate, (b) avoid-
ing creating new hypotheses based on failed sub-
hypotheses. This requires some changes to the rou-
tines instantiate-hypothesis() and hypothesize-edge(), as
well as an extra boolean marker for each hypothesis.
The extended instantiate-hypothesis() starts by
checking whether the hypothesis is already marked
as failed. If it is not so marked, the routine recur-
sively instantiates all sub-hypotheses. Any failure
will again lead to instant return. Otherwise, unifica-
tion is used to create a new edge from the outcome of
the sub-hypothesis instantiations. If this unification
fails, the current hypothesis is marked. Moreover,
all its ancestor hypotheses are also marked (by re-
cursively following the pointers to the direct parent
hypotheses) as they are also guaranteed to fail.
Correspondingly, hypothesize-edge() needs to
check the instantiation failure marker to avoid re-
turning hypotheses that are guaranteed to fail. If
a hypothesis coming out of the agenda is already
54
marked as failed, it will be used to create new hy-
potheses (with advance-indices()), but dropped af-
terward. Subsequent hypotheses will be popped
from the agenda until either a hypothesis that is not
marked as failed is returned, or the agenda is empty.
Moreover, hypothesize-edge() also needs to avoid
creating new hypotheses based on failed sub-
hypotheses. When a failed sub-hypothesis is found,
the creation of the new hypothesis is skipped. But
the index vector ~I may not be simply discarded.
Otherwise hypotheses based on advance-indices(~I)
will not be reachable in the search. On the other
hand, simply adding every advance-indices(~I) on to
the pending creation list is not efficient either in the
case where multiple sub-hypotheses fail.
To solve the problem, we compute a failure vec-
tor ~F = ?f0 . . . fn?, where fj is 1 when the sub-
hypothesis at position j is known as failed, and 0
otherwise. If a sub-hypothesis at position j is failed
then all the index vectors having value ij at posi-
tion j must also fail. By putting the result of ~I + ~F
on the pending creation list, we can safely skip the
failed rows of sub-hypotheses, while not losing the
reachability of the others. As an example, suppose
we have a ternary index vector ?3 1 2? for which a
new hypothesis is to be created. By checking the in-
stantiation failure marker of the sub-hypotheses, we
find that the first and the third sub-hypotheses are al-
ready marked. The failure recording vector will then
be ?1 0 1?. By putting ?4 1 3? = ?3 1 2? + ?1 0 1?
on to the pending hypothesis creation list, the failed
sub-hypotheses are skipped.
We evaluate the effects of instantiation failure
caching and propagation below in Section 7.
7 Empirical Results
To evaluate the performance of the selective unpack-
ing algorithm, we carried out a series of empirical
evaluations with the ERG and GG, in combination
with a modified version of the PET parser. When
running the ERG we used as our test set the JH4
section of the LOGON treebank3, which contains
1603 items with an average sentence length of 14.6
words. The remaining LOGON treebank (of around
3The treebank is comprised of several booklets of
edited, instructional texts on backcountry activities in Nor-
way. The data is available from the LOGON web site at
?http://www.emmtee.net?.
Configuration GP Coverage Time (s)
greedy best-first 0 91.6% 3889
exhaustive unpacking 0 84.5% 4673
selective unpacking
0 94.3% 2245
1 94.3% 2529
2 94.3% 3964
3 94.2% 3199
4 94.2% 3502
Table 2: Coverage on the ERG for different configurations, with
fixed resource consumption limits (of 100k passive edges or 300
seconds). In all cases, up to ten ?best? results were searched,
and Coverage shows the percentage of inputs that succeed to
parse within the available resource. Time shows the end-to-end
processing time for each batch.
5 15 25 35
String Length (Number of Input Tokens)
0
1
2
3
4
5
6
(s)
(generated by [incr tsdb()] at 23-mar-2007 (12:44 h))?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? gready best-first
? exhaustive unpacking
? selective unpacking
? forest creation
Figure 4: Parsing times for different configurations using the
ERG, in all three cases searching for up to ten results, without
the use of grandparenting.
8,000 items) was used in training the various ME
parse disambiguation models. For the experiment
with GG, we designated a 2825-item portion of the
DFKI Verbmobil treebank4 for our tests, and trained
ME models on the remaining 10,000 utterances. At
only 7.4 words, the average sentence length is much
shorter in the Verbmobil data.
We ran seven different configurations of the parser
with different search strategies and (un-)packing
mechanisms:
? Agenda driven greedy n-best parsing using the
ME score without grandparenting features; no
local ambiguity packing;
? Local ambiguity packing with exhaustive un-
packing, without grandparenting features;
4The data in this treebank is taken from transcribed appoint-
ment scheduling dialogues; see ?http://gg.dfki.de/?
for further information on GG and its treebank.
55
1 10 20 30 40 50 60 70 80 90 100
Maximum Number of Trees to Unpack (n)
0.00
0.02
0.04
0.06
0.08
0.10
(s)
? ? ? ? ?
? ?
?? ? ? ?
? ? ?
?
? ? ? ?
? ? ?
?? ? ?
? ? ?
?
?? ?
? ?
? ? ?
?
GP=0
GP=1
GP=2
GP=3
GP=4
Figure 5: Mean times for selective unpacking of all test items
for n-best parsing with the ERG, for varying n and grandpar-
enting (GP) levels
? Local ambiguity packing and selective unpack-
ing for n-best parsing, with 0 through 4 levels
of grandparenting (GP) features.
As a side-effect of differences in efficiency, some
configurations could not complete parsing all sen-
tences given reasonable memory constraints (which
we set at a limit of 100k passive edges or 300 sec-
onds processing time per item). The overall cover-
age and processing time of different configurations
on JH4 are given in Table 2.
The correlation between processing time and cov-
erage is interesting. However, it makes the efficiency
comparison difficult as parser behavior is not clearly
defined when the memory limit is exceeded. To cir-
cumvent this problem, in the following experiments
we average only over those 1362 utterances from
JH4 that complete parsing within the resource limit
in all seven configurations. Nevertheless, it must
be noted that this restriction potentially reduces effi-
ciency differences between configurations, as some
of the more challenging inputs (which typically lead
to the largest differences) are excluded.
Figure 4 compares the processing time of differ-
ent configurations. The difference is much more
significant for longer sentences (i.e. with more than
15 words). If the parser unpacks exhaustively, the
time for unpacking grows with sentence length at a
quickly increasing rate. In such cases, the efficiency
gain with ambiguity packing in the parsing phase
is mostly lost in the unpacking phase. The graph
shows that greedy best-first parsing without packing
outperforms exhaustive unpacking for sentences of
Configuration Exact Match Top Ten
random choice 11.34 43.06
no grandparenting 52.52 68.38
greedy best-first 51.79 69.48
grandparenting[1] 56.83 85.33
grandparenting[2] 56.55 84.14
grandparenting[3] 56.37 84.14
grandparenting[4] 56.28 84.51
Table 3: Parse selection accuracy for various levels of grandpar-
enting. The exact match column shows the percentage of cases
in which the correct tree, according to the treebank, was ranked
highest by the model; conversely, the top ten column indicates
how often the correct tree was among the ten top-ranking re-
sults.
less than 25 words. With sentences longer than 25
words, the packing mechanism helps the parser to
overtake greedy best-first parsing, although the ex-
haustive unpacking time also grows fast.
With the selective unpacking algorithm presented
in the previous sections, unpacking time is reduced,
and grows only slowly as sentence length increases.
Unpacking up to ten results, when contrasted with
the timings for forest creation (i.e. the first parsing
phase) in Figure 4, adds a near-negligible extra cost
to the total time required for both phases. Moreover,
Figure 5 shows that with selective unpacking, as n
is increased, unpacking time grows roughly linearly
for all levels of grandparenting (albeit always with
an initial delay in unpacking the first result).
Table 4 summarizes a number of internal parser
measurements using the ERG with different pack-
ing/unpacking settings. Besides the difference in
processing time, we also see a significant difference
in ?space? between exhaustive and selective un-
packing. Also, the difference in ?unifications? and
?copies? indicates that with our selective unpacking
algorithm, these expensive operations on typed fea-
ture structures are significantly reduced.
In return for increased processing time (and
marginal loss in coverage) when using grandparent-
ing features, Table 3 shows some large improve-
ments in parse selection accuracy (although the pic-
ture is less clear-cut at higher-order levels of grand-
parenting5). A balance point between efficiency
5The models were trained using the open-source TADM pack-
age (Malouf, 2002), using default hyper-parameters for all con-
figurations, viz. a convergence threshold of 10?8, variance of
the prior of 10?4, and frequency cut-off of 5. It is likely that
56
Configuration GP Unifications Copies Space Unpack Total(#) (#) (kbyte) (s) (s)
? 15
greedy best-first 0 1845 527 2328 ? 0.12
words
exhaustive unpacking 0 2287 795 8907 0.01 0.12
selective unpacking
0 1912 589 8109 0.00 0.12
1 1913 589 8109 0.01 0.12
2 1914 589 8109 0.01 0.12
3 1914 589 8110 0.01 0.12
4 1914 589 8110 0.02 0.13
> 15
greedy best-first 0 25233 5602 24646 ? 1.66
words
exhaustive unpacking 0 39095 15685 80832 0.85 1.95
selective unpacking
0 17489 4422 33326 0.03 1.17
1 17493 4421 33318 0.05 1.21
2 17493 4421 33318 0.09 1.25
3 17495 4422 33321 0.13 1.27
4 17495 4422 33320 0.21 1.34
Table 4: Contrasting the efficiency of various (un-)packing settings in use with ERG on short (top) and medium-length (bottom)
inputs; in each configuration, up to ten trees are extracted. Unification and Copies is the count of top-level FS operations, where
only successful unifications require a subsequent copy (when creating a new edge). Unpack and Total are unpacking and total parse
time, respectively.
and accuracy can be made according to application
needs.
Finally, we compare the processing time of the
selective unpacking algorithm with and without in-
stantiation failure caching and propagation (as de-
scribed in Section 4 above). The empirical results
for GG are summarized in Table 5, showing clearly
that the technique reduced unnecessary hypotheses
and instantiation failures. The design philosophy of
the ERG and GG differ. During the first, forest cre-
ation phase, GG suppresses a number of features (in
the HPSG sense, not the ME sense) that can actually
constrain the combinatorics of edges. This move
makes the packed forest more compact, but it im-
plies that unification failures will be more frequent
during unpacking. In a sense, GG thus moves part
of the search for globally consistent derivations into
the second phase, and it is possible for the forest to
contain ?result? trees that ultimately turn out to be
incoherent. Dynamic programming of instantiation
failures makes this approach tractable, while retain-
ing the general breadth-first characteristic of the se-
lective unpacking regime.
further optimization of hyper-parameters for individual config-
urations would moderately improve model performance, espe-
cially for higher-order grandparenting levels with large numbers
of features.
8 Discussion
The approach to n-best parsing described in this pa-
per takes as its point of departure recent work of Car-
roll & Oepen (2005), which describes an efficient al-
gorithm for unpacking n-best trees from a forest pro-
duced by a chart-based sentence generator and con-
taining local ME properties with associated weights.
In an almost contemporaneous study, but in the con-
text of parsing with treebank grammars, Huang &
Chiang (2005) develop a series of increasingly effi-
cient algorithms for unpacking n-best results from
a weighted hypergraph representing a parse forest.
The algorithm of Carroll & Oepen (2005) and the
final one of Huang & Chiang (2005) are essentially
equivalent, and turn out to be reformulations of an
approach originally described by Jime?nez & Marzal
(2000) (although expressed there only for grammars
in Chomsky Normal Form).
In this paper we have considered ME properties
that extend beyond immediate dominance relations,
extending up to 4 levels of grandparenting. Pre-
vious work has either assumed properties that are
restricted to the minimal parse fragments (i.e. sub-
trees of depth one) that make up the packed repre-
sentation (Geman & Johnson, 2002), or has taken a
more relaxed approach by allowing non-local prop-
57
Configuration Unifications Copies Hypotheses Space Unpack Total(#) (#) (#) (kbyte) (ms) (ms)
greedy best-first 5980 1447 ? 9202 ? 400
selective, no caching 5535 1523 1245 27188 70 410
selective, with cache 4915 1522 382 27176 10 350
Table 5: Efficiency effects of the instantiation failure caching and propagation with GG, without grandparenting. All statistics are
averages over the 1941 items that complete within the resource bounds in all three configurations. Unification, Copies, Unpack,
and Total have the same interpretation as in Table 4, and Hypotheses is the average count of hypothesized sub-trees.
erties but without addressing the problem of how to
efficiently extract the top-ranked trees from a packed
forest (Miyao & Tsujii, 2002).
Probably the work closest in spirit to our approach
is that of Malouf & van Noord (2004), who use an
HPSG grammar comparable to the ERG and GG,
non-local ME features, and a two-phase parse for-
est creation and unpacking approach. However, their
unpacking phase uses a beam search to find a good
(single) candidate for the best parse; in contrast?
for ME models containing the types of non-local
features that are most important for accurate parse
selection?we avoid an approximative search and ef-
ficiently identify exactly the n-best parses.
When parsing with context free grammars, a (sin-
gle) parse can be retrieved from a parse forest in
time linear in the length of the input string (Bil-
lot & Lang, 1989). However, as discussed in Sec-
tion 2, when parsing with a unification-based gram-
mar and packing under feature structure subsump-
tion, the cross-product of some local ambiguities
may not be globally consistent. This means that ad-
ditional unifications are required at unpacking time.
In principle, when parsing with a pathological gram-
mar with a high rate of failure, extracting a single
consistent parse from the forest could take exponen-
tial time (see Lang (1994) for a discussion of this is-
sue with respect to Indexed Grammars). In the case
of GG, a high rate of unification failure in unpacking
is dramatically reduced by our instantiation failure
caching and propagation mechanism.
9 Conclusions and Future Work
We have described and evaluated an algorithm for
efficiently computing the n-best analyses from a
parse forest produced by a unification grammar, with
respect to a Maximum Entropy (ME) model con-
taining two classes of non-local features. The al-
gorithm is efficient in that it empirically exhibits a
linear relationship between processing time and the
number of analyses unpacked, at all degrees of ME
feature non-locality. It improves over previous work
in providing the only exact procedure for retrieving
n-best analyses from a packed forest that can deal
with features with extended domains of locality and
with forests created under subsumption. Our algo-
rithm applies dynamic programming to intermediate
results and local failures in unpacking alike.
The experiments compared the new algorithm
with baseline systems representing other possible
approaches to parsing with ME models: (a) a single
phase of agenda-driven parsing with on-line prun-
ing based on intermediate ME scores, and (b) two-
phase parsing with exhaustive unpacking and post-
hoc ranking of complete trees. The new approach
showed better speed, coverage, and accuracy than
the baselines.
Although we have dealt with the non-local ME
features that in previous work have been found to be
the most important for parse selection (i.e. grand-
parenting and n-grams), this does not exhaust the
full range of features that could possibly be useful.
For example, it may be the case that accurately re-
solving some kinds of ambiguities can only be done
with reference to particular parts?or combinations
of parts?of the HPSG feature structures represent-
ing the analysis of a complete constituent. To deal
with such cases we are currently designing an exten-
sion to the algorithms described here which would
add a ?controlled? beam search, in which the size of
the beam was limited by the interval of score adjust-
ments for ME features that could only be evaluated
once the full linguistic structure became available.
This approach would involve a constrained amount
of extra search, but would still produce the exact n-
best trees.
58
References
Abney, S. P. (1997). Stochastic attribute-value grammars. Com-
putational Linguistics, 23, 597 ? 618.
Billot, S., & Lang, B. (1989). The structure of shared forests
in ambiguous parsing. In Proceedings of the 27th Meeting
of the Association for Computational Linguistics (pp. 143 ?
151). Vancouver, BC.
Callmeier, U. (2002). Preprocessing and encoding techniques
in PET. In S. Oepen, D. Flickinger, J. Tsujii, & H. Uszkor-
eit (Eds.), Collaborative language engineering. A case study
in efficient grammar-based processing. Stanford, CA: CSLI
Publications.
Caraballo, S. A., & Charniak, E. (1998). New figures of merit
for best-first probabilistic chart parsing. Computational Lin-
guistics, 24(2), 275 ? 298.
Carroll, J., & Oepen, S. (2005). High-efficiency realization for
a wide-coverage unification grammar. In R. Dale & K. F.
Wong (Eds.), Proceedings of the 2nd International Joint
Conference on Natural Language Processing (Vol. 3651, pp.
165 ? 176). Jeju, Korea: Springer.
Clark, S., & Curran, J. R. (2004). Parsing the WSJ using CCG
and log-linear models. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics (pp. 104 ?
111). Barcelona, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Crysmann, B. (2005). Relative clause extraposition in German.
An efficient and portable implementation. Research on Lan-
guage and Computation, 3(1), 61 ? 82.
Erbach, G. (1991). A flexible parser for a linguistic develop-
ment environment. In O. Herzog & C.-R. Rollinger (Eds.),
Text understanding in LILOG (pp. 74 ? 87). Berlin, Ger-
many: Springer.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1),
15 ? 28.
Geman, S., & Johnson, M. (2002). Dynamic programming for
parsing and estimation of stochastic unification-based gram-
mars. In Proceedings of the 40th Meeting of the Association
for Computational Linguistics. Philadelphia, PA.
Huang, L., & Chiang, D. (2005). Better k-best parsing. In
Proceedings of the 9th International Workshop on Parsing
Technologies (pp. 53 ? 64). Vancouver, Canada.
Jime?nez, V. M., & Marzal, A. (2000). Computation of the
n best parse trees for weighted and stochastic context-free
grammars. In Proceedings of the Joint International Work-
shops on Advances in Pattern Recognition (pp. 183 ? 192).
London, UK: Springer-Verlag.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Association
for Computational Linguistics (pp. 535 ? 541). College Park,
MD.
Kasami, T. (1965). An efficient recognition and syntax al-
gorithm for context-free languages (Technical Report # 65-
758). Bedford, MA: Air Force Cambrige Research Labora-
tory.
Klein, D., & Manning, C. D. (2003). A* parsing. Fast exact
Viterbi parse selection. In Proceedings of the 4th Confer-
ence of the North American Chapter of the ACL. Edmonton,
Canada.
Lang, B. (1994). Recognition can be harder than parsing. Com-
putational Intelligence, 10(4), 486 ? 494.
Langkilde, I. (2000). Forest-based statistical sentence gener-
ation. In Proceedings of the 1st Conference of the North
American Chapter of the ACL. Seattle, WA.
Malouf, R. (2002). A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of the
6th Conference on Natural Language Learning. Taipei, Tai-
wan.
Malouf, R., & van Noord, G. (2004). Wide coverage parsing
with stochastic attribute value grammars. In Proceedings of
the IJCNLP workshop Beyond Shallow Analysis. Hainan,
China.
Miyao, Y., Ninomiya, T., & Tsujii, J. (2005). Corpus-oriented
grammar development for acquiring a Head-Driven Phrase
Structure Grammar from the Penn Treebank. In K.-Y. Su,
J. Tsujii, J.-H. Lee, & O. Y. Kwong (Eds.), Natural language
processing (Vol. 3248, pp. 684 ? 693). Hainan Island, China.
Miyao, Y., & Tsujii, J. (2002). Maximum entropy estimation
for feature forests. In Proceedings of the Human Language
Technology Conference. San Diego, CA.
Moore, R. C., & Alshawi, H. (1992). Syntactic and semantic
processing. In H. Alshawi (Ed.), The Core Language Engine
(pp. 129 ? 148). Cambridge, MA: MIT Press.
Mu?ller, S., & Kasper, W. (2000). HPSG analysis of German.
In W. Wahlster (Ed.), Verbmobil. Foundations of speech-to-
speech translation (Artificial Intelligence ed., pp. 238 ? 253).
Berlin, Germany: Springer.
Oepen, S., & Carroll, J. (2002). Efficient parsing for
unification-based grammars. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collaborative language en-
gineering. A case study in efficient grammar-based process-
ing (pp. 195 ? 225). Stanford, CA: CSLI Publications.
Oepen, S., Flickinger, D., Toutanova, K., & Manning, C. D.
(2004). LinGO Redwoods. A rich and dynamic treebank for
HPSG. Journal of Research on Language and Computation,
2(4), 575 ? 596.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell III,
J. T., & Johnson, M. (2002). Parsing the Wall Street Journal
using a Lexical-Functional Grammar and discriminative es-
timation techniques. In Proceedings of the 40th Meeting of
the Association for Computational Linguistics. Philadelphia,
PA.
Toutanova, K., Manning, C. D., Flickinger, D., & Oepen, S.
(2005). Stochastic HPSG parse selection using the Red-
woods corpus. Journal of Research on Language and Com-
putation, 3(1), 83 ? 105.
59
Unsupervised Acquisition of Predominant
Word Senses
Diana McCarthy
University of Sussex
Rob Koeling
University of Sussex
Julie Weeds
University of Sussex
John Carroll?
University of Sussex
There has been a great deal of recent research into word sense disambiguation, particularly
since the inception of the Senseval evaluation exercises. Because a word often has more than
one meaning, resolving word sense ambiguity could benefit applications that need some level
of semantic interpretation of language input. A major problem is that the accuracy of word
sense disambiguation systems is strongly dependent on the quantity of manually sense-tagged
data available, and even the best systems, when tagging every word token in a document,
perform little better than a simple heuristic that guesses the first, or predominant, sense of a
word in all contexts. The success of this heuristic is due to the skewed nature of word sense
distributions. Data for the heuristic can come from either dictionaries or a sample of sense-
tagged data. However, there is a limited supply of the latter, and the sense distributions and
predominant sense of a word can depend on the domain or source of a document. (The first
sense of ?star? for example would be different in the popular press and scientific journals).
In this article, we expand on a previously proposed method for determining the predominant
sense of a word automatically from raw text. We look at a number of different data sources and
parameterizations of the method, using evaluation results and error analyses to identify where
the method performs well and also where it does not. In particular, we find that the method
does not work as well for verbs and adverbs as nouns and adjectives, but produces more accurate
predominant sense information than the widely used SemCor corpus for nouns with low coverage
in that corpus. We further show that the method is able to adapt successfully to domains when
using domain specific corpora as input and where the input can either be hand-labeled for domain
or automatically classified.
? Department of Informatics, Brighton BN1 9QH, UK. E-mail: {dianam,robk,juliewe,johnca}@sussex.ac.uk.
Submission received: 16 November 2005; revised submission received: 12 July 2006; accepted for publication
16 February 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
1. Introduction
In word sense disambiguation, the ?first sense? heuristic (choosing the first, or predom-
inant sense of a word) is used by most state-of-the-art systems as a back-off method
when information from the context is not sufficient to make a more informed choice.
In this article, we present an in-depth study of a method for automatically acquiring
predominant senses for words from raw text (McCarthy et al 2004a).
The method uses distributionally similar words listed as ?nearest neighbors?
in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the
observation that the more prevalent a sense of a word, the more neighbors will relate
to that sense, and the higher their distributional similarity scores will be. The senses
of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because
this is widely used, is publicly available, and has plenty of gold-standard evaluation
data available (Miller et al 1993; Cotton et al 2001; Preiss and Yarowsky 2001; Mihalcea
and Edmonds 2004). The distributional strength of the neighbors is associated with the
senses of a word using a measure of semantic similarity which relies on the relationships
between word senses, such as hyponyms (available in an inventory such as WordNet)
or overlap in the definitions of word senses (available in most dictionaries), or both.
In this article we provide a detailed discussion and quantitative analysis of the
motivation behind the first sense heuristic, and a full description of our method. We
extend previously reported work in a number of different directions:
 We evaluate the method on all parts of speech (PoS) on SemCor (Miller
et al 1993). Previous experiments (McCarthy et al 2004c) evaluated only
nouns on SemCor, or all PoS but only on the Senseval-2 (Cotton et al 2001)
and Senseval-3 (Mihalcea and Edmonds 2004) data. The evaluation on all
PoS is much more extensive because the SemCor corpus is composed of
220,000 words in contrast to the 6 documents in the Senseval-2 and -3
English all words data (10,000 words).
 We compare two WordNet similarity measures in our evaluation on
nouns, and also contrast performance using two publicly available
thesauruses, both produced from the same NEWSWIRE corpus, but one
derived using a proximity-based approach and the other using
dependency relations from a parser. It turns out that the results from the
proximity-based thesaurus are comparable to those from the dependency-
based thesaurus; this is encouraging for applying the method to languages
without sophisticated analysis tools.
 We manually analyze a sample of errors from the SemCor evaluation. A
small number of errors can be traced back to inherent shortcomings of our
method, but the main source of error is due to noise from related senses.
This is a common problem for all WSD systems (Ide and Wilks 2006) but
one which is only recently starting to be addressed by the WSD
community (Navigli, Litkowski, and Hargraves 2007).
 One motivation for an automatic method for acquiring predominant
senses is that there will always be words for which there are insufficient
data available in manually sense-tagged resources. We compare the
performance of our automatic method with the first sense heuristic
derived from SemCor on nouns in the Senseval-2 data. We find that the
554
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
automatic method outperforms the one obtained from manual annotations
in SemCor for nouns with fewer than five occurrences in SemCor.
 Aside from the lack of coverage of manually annotated data, there is a
need for first sense heuristics to be specific to domain. We explore the
potential for applying the method with domain-specific text for all PoS in
an experiment using a gold-standard domain-specific resource (Magnini
and Cavaglia` 2000) which we have used previously only with nouns. We
show that although there is a little mileage to be had from domain-specific
first sense heuristics for verbs, nouns benefit greatly from domain-specific
training.
 In previous work (Koeling, McCarthy, and Carroll 2005) we produced
manually sense-annotated domain-specific test corpora for a lexical
sample, and demonstrated that predominant senses acquired (from
hand-classified corpora) in the same domain as the test data outperformed
the SemCor first sense. We further this exploration by contrasting with
results from training on automatically categorized text from the English
Gigaword Corpus and show that the results are comparable to those using
hand-classified domain data.
The article is organized as follows. In the next section we motivate the use of pre-
dominant sense information in WSD systems and the need for acquiring this information
automatically. In Section 3 we give an overview of related work in WSD, focusing on the
acquisition of prior sense distributions and domain-specific sense information. Section 4
describes our acquisition method. Section 5 describes the experimental setup for the
work reported in this article. Section 6 describes four experiments. The first evaluates
the first sense heuristic using predominant sense information acquired for all PoS on
SemCor; for nouns we compare two semantic similarity methods and three different
types of distributional thesaurus. We also report an error analysis for all PoS of our
method. The second experiment compares the performance of the automatic method
to the manually produced data in SemCor, on nouns in the Senseval-2 data, looking
particularly at nouns which have a low frequency in SemCor. The third uses corpora in
restricted domains and the subject field code gold standard of Magnini and Cavaglia`
(2000) to investigate the potential for domain-specific rankings for different PoS. The
fourth compares results when we train and test on domain-specific corpora, where
the training data is (1) manually categorized for domain and from the same corpus
as the test data, and (2) where the training data is harvested automatically from another
corpus which is categorized automatically. Finally, we conclude (Section 7) and discuss
directions for future work (Section 8).
2. Motivation
The problem of disambiguating the meanings of words in text has received much
attention recently, particularly since the inception of the Senseval evaluation exercises
(Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004).
One of the standard Senseval tasks (the ?all words? task) is to tag each open class word
with one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum
1998). The most accurate word sense disambiguation (WSD) systems use supervised
machine learning approaches (Stevenson and Wilks 2001), trained on text which has
been sense tagged by hand. However, the performance of these systems is strongly
555
Computational Linguistics Volume 33, Number 4
dependent on the quantity of training data available (Yarowsky and Florian 2002),
and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998). The
largest all words sense tagged corpus is SemCor, which is 220,000 words taken from
103 passages, each of about 2,000 words, from the Brown corpus (Francis and Kuc?era
1979) and the complete text of a 19th-century American novel, The Red Badge of Courage,
which totals 45,600 words (Landes, Leacock, and Tengi 1998). Approximately half of the
words in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) and
these have been linked to WordNet senses by human taggers using a software interface.
The shortage of training data due to the high costs of tagging texts has motivated
research into unsupervised methods for WSD. But in the English all-words tasks in
Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make
use of hand-tagged data (in some form or other) performed substantially worse than
those that did. Table 1 summarizes the situation. It gives the precision and recall of
the best1 two supervised (S) and unsupervised (U)2 systems for the English all words
and English lexical sample for Senseval-23 and -3, along with the first sense baseline
(FS) reported by the task organizers.4 This is a simple application of the ?first sense?
heuristic?that is, using the most common sense of a word for every instance of it in the
test corpus, regardless of context. Although contextual WSD is of course preferable, the
baseline is a very powerful one and unsupervised systems find it surprisingly hard to
beat (indeed, some of the systems that report themselves as unsupervised actually make
some use of a manually obtained first-sense heuristic). Considering both precision and
recall, only 5 of 26 systems in the Senseval-3 English all-words task beat the first sense
heuristic as derived from SemCor (61.5%5), and then by only a few percentage points
(the top system scoring 65% precision and recall) despite using hand-tagged training
data available from SemCor and previous Senseval data sets, large sets of contextual
features, and sophisticated machine learning algorithms.
The performance of WSD systems, at least for all-words tasks, seems to have
plateaued at a level just above the first sense heuristic (Snyder and Palmer 2004). This is
due to the shortage of training data and the often fine granularity of sense distinctions.
Ide and Wilks (2006) argue that it is best to concentrate effort on distinctions which
are useful for applications and where systems can be confident of high precision. In
cases where systems are less confident, but word senses, rather than words, are needed,
the first sense heuristic is a powerful back-off strategy. This strategy is dependent on
information provided in dictionaries. Two dictionaries that have been used by English
WSD systems are the Longman Dictionary of Contemporary English (LDOCE) (Procter
1 We rank the systems by the recall scores, because this is the accuracy over the entire test set regardless of
how many items were attempted.
2 Note that the classification of systems as unsupervised is not straightforward. Systems reported as
unsupervised in the Senseval proceedings sometimes make use of some manual annotations. For
example, the top scoring system that reported itself unsupervised in the Senseval-3 lexical sample task
used manually sense-tagged training data for constructing glosses.
3 The verb lexical sample was done as a separate exercise for Senseval-2, and for brevity we have not
included the results from this task.
4 The all-words task organizers used the first sense as listed in WordNet. This is based on the SemCor first
sense because WordNet senses are ordered according to the frequency data in SemCor. However, where
senses are not found in WordNet, the ordering is arbitrarily determined as a function of the ?grind?
program (see http://wordnet.princeton.edu/man/grind.1WN.htm). The lexical sample task organizers
state that they use the ?most frequent sense? but do not stipulate if this is taken from WordNet, or
directly from SemCor.
5 This figure is the arithmetic mean of two published estimates (Snyder and Palmer 2004), the difference
being due to the treatment of multiwords.
556
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 1
The best two performing systems of each type (according to fine-grained recall) in Senseval-2
and -3.
All words Lexical sample
Precision (%) Recall (%) Precision (%) Recall (%)
Senseval-2 S 69.0 69.0 64.2 64.2
Senseval-2 S 63.6 63.6 63.8 63.8
Senseval-2 U 45.1 45.1 40.2 40.1
Senseval-2 U 36.0 36.0 58.1 31.9
FS baseline 57.0 57.0 47.6 47.6
Senseval-3 S 65.1 65.1 72.9 72.9
Senseval-3 S 65.1 64.2 72.6 72.6
Senseval-3 U 58.3 58.2 66.1 65.7
Senseval-3 U 55.7 54.6 56.3 56.3
FS baseline 61.5 61.5 55.2 55.2
1978) and WordNet (Fellbaum 1998). These both provide a ranking of senses accord-
ing to their predominance. The sense ordering in LDOCE is based on lexicographer
intuition, whereas in WordNet the senses are ordered according to their frequency in
SemCor (Miller et al 1993).
There are two major problems with deriving a first sense heuristic from these types
of resources. The first is that the predominant sense of a word varies according to
the source of the document (McCarthy and Carroll 2003) and with the domain. For
example, the first sense of star as derived from SemCor is celestial body, but if one were
disambiguating popular news stories then celebrity would be more likely. Domain,
topic, and genre are important in WSD (Martinez and Agirre 2000; Magnini et al 2002)
and the sense-frequency distributions of words depend on all of these factors. Any
dictionary will provide only a single sense ranking, whether this is derived from sense-
tagged data as in WordNet, lexicographer intuition as in LDOCE, or inspection of corpus
data as in the Oxford Advanced Learner?s Dictionary (Hornby 1989). A fixed order of
senses may not reflect the data that an NLP system is dealing with.
The second problem with obtaining predominant sense information applies to the
use of hand-tagged resources, such as SemCor. Such resources are relatively small due
to the cost of manual tagging (Kilgarriff 1998). Many words will simply not be covered,
or occur only a few times. For many words in WordNet the ordering of word senses is
based on a very small number of occurrences in SemCor. For example, the first sense
of tiger is an audacious person whereas most people would assume the carnivorous
animal sense is more prevalent. This is because the two senses each occur exactly once
in SemCor, and when there is no frequency information to break the tie the WordNet
sense ordering is assigned arbitrarily. There are many fairly common words (such as
the noun crane) which do not occur at all in SemCor. Table 2 gives the number and
percentage of words6 in WordNet and the BNC which do not occur in SemCor. As one
would expect from Zipf?s law, a substantial number of words do not occur in SemCor,
even when we do not consider multiwords. Many of these words are extremely rare, but
6 Here and elsewhere in this article we give figures only for words without embedded spaces, that is, not
multiwords.
557
Computational Linguistics Volume 33, Number 4
Table 2
Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor.
WordNet types BNC types
PoS No. % No. %
noun 43,781 81.9 360,535 97.5
verb 4,741 56.4 25,292 87.6
adjective 14,991 72.3 95,908 95.4
adverb 2,405 64.4 10,223 89.2
Table 3
Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no
data in SemCor (0 columns), or with very little data (? 1 and ? 5 occurrences). Note that there
are no annotations for adverbs in the Senseval-3 documents.
Senseval-2 Senseval-3
0 ? 1 ? 5 0 ? 1 ? 5
PoS No. % No. % No. % No. % No. % No. %
noun 12 3.2 28 7.4 49 12.9 13 3.1 26 6.3 69 16.7
verb 7 2.1 11 3.4 28 8.6 3 0.9 10 2.9 36 10.4
adjective 9 4.2 16 7.4 50 23.1 8 4.7 15 8.9 33 19.5
adverb 1 0.9 1 0.9 2 1.8 ? ? ? ? ? ?
in any given document it is likely that there will be at least some words without SemCor
data. Table 3 quantifies this, for the Senseval-2 and -3 all-words tasks test data, showing
the percentage of polysemous word types with no frequency information in SemCor, the
percentage with zero or one occurrences, and the percentage with up to five occurrences.
(For example, the table indicates that 12.9% of nouns in the Senseval-2 data, and 16.7%
in Senseval-3, have five or fewer occurrences in SemCor.) Thus, although SemCor may
cover many frequently occurring word types in a given document, there are likely to be
a substantial proportion for which there is very little or no information available.
Tables 4 and 5 present an analysis of the actual ambiguity of polysemous words
within the six documents making up the Senseval-2 and -3 all-words test data. They
show the extent to which these words are used in a predominant sense, within a
document, and the extent to which this is the same as that given by SemCor. The two
tables share a common format: columns 2?5 give percentages over all ?document/word
type? combinations. The second column shows the percentage of the ?document/word
type? combinations where the word is used in the document in only one of its senses.
The fourth column shows the same percentage but for ?document/word type? combi-
nations where the word is used in more than one sense in the document. The third and
fifth columns give the percentage of the words in the preceding columns (second and
fourth, respectively) where the first sense for the word in the document is the same as in
SemCor (FS = SC FS). For the third column, this is the only sense that this word appears
in within the document. (Note that for any row, columns 2 and 4 account for all possibil-
ities so will always add up to 100.) The sixth column gives the mean degree of polysemy,
according to WordNet, for the set of words that these figures are calculated for.
558
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 4
Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than
once in a document (adverb data is only from Senseval-2).
1 sense > 1 sense
PoS % FS = SC FS % % FS = SC FS % Mean polysemy
noun 72.2 52.2 27.8 7.3 5.9
verb 45.6 25.1 54.4 16.9 12.7
adjective 62.9 40.5 37.1 10.3 4.8
adverb 64.7 50.0 35.3 17.6 4.7
The figures in Table 4 are for words occurring more than once in a given Senseval
test document. The tendency for words to be used in only one sense in any given
document7 is strongest for nouns, although adverbs and adjectives also tend towards
one sense. Verbs are on average much more polysemous than the other parts of speech
yet still 45.6% of polysemous verbs which occur more than once are used in only a single
sense. However, because verbs are in general more polysemous, it makes it less likely
that if a verb occurs in only one sense in a document then it will be the one indicated by
SemCor.
The figures in Table 5 are for all words in the Senseval documents (not just those oc-
curring more than once), showing the accuracy of a SemCor-derived first-sense heuristic
for words with a frequency below a specified threshold (column 1) in SemCor. The table
shows that although having a first sense from SemCor is certainly useful, when looking
at figures for all the words in the Senseval documents a good proportion have first
senses other than the one indicated by SemCor. Furthermore, the lower the frequency
in SemCor the more likely that the first sense indicated by SemCor is wrong. (However,
the situation is slightly different for adverbs because there are not many with low
frequency in SemCor and they are on average not very polysemous, so for them a first
sense derived from a resource like SemCor?where one exists?is possibly sufficient.)
These results show that although SemCor is a useful resource, there will always be
words for which its coverage is inadequate. In addition, few languages have extensive
hand-tagged resources or sense orderings produced by lexicographers. Moreover, gen-
eral resources containing word sense information are not likely to be appropriate when
processing language for a wider variety of domains, topics, and genres. What is needed
is a means to find predominant senses automatically.
3. Related Work
Most research in WSD to date has concentrated on using contextual features, typically
neighboring words, to help infer the correct sense of a target word. In contrast, our
work is aimed at discovering the predominant sense of a word from raw text because
7 The tendency for words to be used in only one sense in a given discourse is weaker for fine-grained
distinctions (Krovetz 1998) compared to coarse-grained distinctions (Gale, Church, and Yarowsky 1992).
Nevertheless, even with a fine-grained inventory the first sense heuristic is certainly powerful, as shown
in Table 1.
559
Computational Linguistics Volume 33, Number 4
Table 5
Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data,
broken down by their frequencies of occurrence in SemCor (adverb data is only from
Senseval-2).
1 sense > 1 sense
Frequency % FS = SC FS % % FS = SC FS % Mean polysemy
noun
? 1 (54) 96.3 24.1 3.7 0.0 2.8
? 5 (118) 96.6 43.2 3.4 0.0 3.2
? 10 (191) 96.9 48.7 3.1 0.0 3.3
all (792) 88.8 51.6 11.2 2.5 5.5
verb
? 1 (21) 100.0 33.3 0.0 0.0 2.4
? 5 (64) 98.4 35.9 1.6 1.6 3.2
? 10 (110) 98.2 38.2 1.8 1.8 3.5
all (671) 82.6 39.3 17.4 5.1 9.0
adjective
? 1 (31) 93.5 19.4 6.5 0.0 2.5
? 5 (83) 95.2 34.9 4.8 1.2 2.7
? 10 (120) 90.8 40.8 9.2 1.7 2.8
all (385) 82.6 46.2 17.4 3.6 5.1
adverb
? 1 (1) 0.0 0.0 100.0 0.0 2.0
? 5 (2) 50.0 50.0 50.0 0.0 2.0
? 10 (8) 87.5 62.5 12.5 0.0 2.3
all (111) 82.9 62.2 17.1 5.4 4.0
the first sense heuristic is so powerful, and because manually sense-tagged data is not
always available.
Lapata and Brew (2004) highlighted the importance of a good prior in WSD. They
used syntactic evidence to find a prior distribution for Levin (1993) verb classes, and
incorporated this in a WSD system. Lapata and Brew obtained their priors for verb
classes directly from subcategorization evidence in a parsed corpus, whereas we use
parsed data to find distributionally similar words (nearest neighbors) to the target
word which reflect the different senses of the word and have associated distributional
similarity scores which can be used for ranking the senses according to prevalence.
We would, however, agree that subcategorization evidence should be very useful for
disambiguating verbs, and would hope to combine such evidence with our ranking
models for context-based WSD.
A major benefit of our work is that this method permits us to produce predominant
senses for any desired domain and text type. Buitelaar and Sacaleanu (2001) explored
ranking and selection of synsets in GermaNet for specific domains using the words
in a given synset, and those related by hyponymy, and a term relevance measure
taken from information retrieval. Buitelaar and Sacaleanu evaluated their method on
identifying domain-specific concepts using human judgments on 100 items. We evaluate
560
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
our method using publicly available resources for balanced text, and, for domain-
specific investigations, resources we have developed ourselves (Koeling, McCarthy,
and Carroll 2005). Magnini and Cavaglia` (2000) associated WordNet word senses with
particular domains, and this has proved useful for high precision WSD (Magnini et
al. 2001); indeed, we have used their domain labels (or subject field codes, SFCs) for
evaluation (Section 6.3). Identification of these SFCs for word senses was semi-automatic
and required a considerable amount of hand-labeling. Our approach requires only raw
text from the given domain and because of this it can easily be applied to a new domain
or sense inventory, as long as there is enough appropriate text.
There are other approaches aimed at gleaning domain-specific information from
raw data. Gliozzo, Giuliano, and Strapparava (2005) induced domain models from raw
data using unsupervised latent semantic models and then fed this into a supervised
WSD model and evaluated on Senseval-3 lexical sample data in four languages. Chan
and Ng (2005) obtained probability distributions to feed into their supervised WSD mod-
els. They used multilingual parallel corpus data to provide probability estimates for a
subset of 22 nouns from the lexical sample task. They then fed this into a supervised WSD
model and verified that the estimates for prior distributions improved performance for
supervised WSD. We intend eventually to use our prevalence scores to feed into un-
supervised WSD models. Although unsupervised models seem to be beaten whenever
there is training data to be had, we anticipate that unsupervised models with improved
priors from the ranking might outperform supervised systems in situations where there
is little training data available. Whereas this article is about finding predominant senses
for back-off in a WSD system, the method could be applied to finding a prior distribution
over all word senses of each target word. It is our intention that the back-off models pro-
duced by our prevalence ranking, either as predominant senses or prior distributions
over word senses, could be combined with contextual information for WSD.
Mohammad and Hirst (2006) describe an approach to acquiring predominant senses
from corpora which makes use of the category information in the Macquarie Thesaurus.
Evaluation is performed on an artificially constructed test set from unambiguous words
in the same category as the 27 test words (nouns, verbs, and adjectives). The senses of
the words are the categories of the thesaurus and the experiment uses only two senses
of each word, the two most predominant ones. The predominance of the two senses is
altered systematically. The results are encouraging because a much smaller amount of
corpus data is needed compared to our approach. However, their method has only been
applied to an artificially constructed test set, rather than a publicly available corpus, and
has yet to be applied in a domain-specific setting, which is the chief motivation of our
work.
The work of Pantel and Lin (2002) is probably the most closely related study
that predates ours, although their ultimate goal is different. Pantel and Lin devised
a method called CBC (clustering by committee) where the 10 nearest neighbors of
a word in a distributional thesaurus are clustered to identify the various senses of
the word. Pantel and Lin use a measure of semantic similarity (Lin 1997) to evaluate
the discovered classes with respect to WordNet as a gold standard. The CBC method
obtained a precision of 61% (the percentage of senses discovered that did exist in
WordNet) and a recall of 51% (the percentage of senses discovered from the union of
those discovered with different clustering algorithms that they tried).8
8 The calculation of recall was over the union of senses discovered automatically, rather than over the
senses in WordNet, because senses in WordNet may be unattested in the data.
561
Computational Linguistics Volume 33, Number 4
Pantel and Lin?s approach is related to ours in that, in their sense discovery pro-
cedure, predominant senses have more of a chance of being found than other senses,
although their algorithm is specifically tailored to look for senses regardless of fre-
quency. To do this the algorithm removes neighbors of the target word once they
are assigned to a cluster so that less frequent senses can be discovered. Our method,
described in detail in Section 4, associates the nearest neighbors to the senses of the
target in a predefined inventory (we use WordNet). We rank the senses using a measure
which sums over the distributional similarity of neighbors weighted by the strength of
the association between the neighbors and the sense. This is done on the assumption
that more prevalent senses will have strong associations with more nearest neighbors
because they have occurred in more contexts in the corpus used for producing the
thesaurus. Both the number and the distributional similarity of the neighbors are used
in our prevalence ranking measure. Pantel and Lin process the possible clusters in order
of their average distributional similarity and number of neighbors but do not take the
number of neighbors into account in the scores given for the clusters. The measures
that Pantel and Lin associate with their clusters are determined by the cohesiveness
of the cluster with the target word because their aim is one of sense discovery. Their
measure is the similarity between the cluster and the target word and does not retain
the distributional similarity of the neighbors within the cluster. It is quite possible that
there is a low frequency sense of a target word with synonyms that form a nice cohesive
group.
Although the number of neighbors assigned to a cluster may correlate with our
ranking score, intuition suggests that a combination of the quantity and distributional
similarity of neighbors to the target word sense is best for determining the relative
predominance of senses. In Section 6 we test this hypothesis using a simplified version
of our method which only uses the number of neighbors, and assigns each to one
sense. Comparisons with the CBC algorithm as it stands would be difficult because
in order to evaluate acquisition of predominance information we have used publicly
available gold-standard sense-tagged corpora, and these have WordNet senses. CBC
will not always find WordNet senses. For example, using the on-line demonstration of
CBC,9 several common senses from nouns from the Senseval-2 lexical sample are not
discovered, including the upright object sense of post, the block of something sense
of bar, the daytime sense of day and the meaning of the word sense of the word sense.
Automatic acquisition of sense inventories is an important endeavor, and we hope to
look at ways of combining our method for detecting predominance with automatically
induced inventories such as those produced by CBC. Evaluation of induced inventories
should be done in the context of an application, because the senses will be keyed to the
acquisition corpus and not to WordNet.
Induction of senses allows coverage of senses appearing in the data that are not
present in a predefined inventory. Although we could adapt our method for use with
an automatically induced inventory, our method which uses WordNet might also be
combined with one that can automatically find new senses from text and then relate
these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with
unknown nouns.
9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with the
option to include all corpora (TREC-2002, TREC-9, and COSMOS).
562
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
4. Method
In our method, the predominant sense for a target word is determined from a preva-
lence ranking of the possible senses for that word. The senses come from a predefined
inventory (which might be a dictionary or WordNet-like resource). The ranking is
derived using a distributional thesaurus automatically produced from a large corpus,
and a semantic similarity measure defined over the sense inventory. The distributional
thesaurus contains a set of words that are ?nearest neighbors? to the target word with
respect to similarity of the way in which they are distributed. (Distributional similarity
is based on the hypothesis of Harris, 1968, that words which occur in similar contexts
have related meanings.) The thesaurus assigns a distributional similarity score to each
neighbor word, indicating its closeness to the target word. For example, the nearest10
neighbors of sandwich might be:
salad, pizza, bread, soup...
and the nearest neighbors of the polysemous noun star11 might be:
actor, footballer, planet, circle...
These neighbors reflect the various senses of the word, which for star might be:
 a celebrity
 a celestial body
 a shape
 a sign of the zodiac12
We assume that the number and distributional similarity scores of neighbors pertaining
to a given sense of a target word will reflect the prevalence of that sense in the corpus
from which the thesaurus was derived. This is because the more prevalent senses of the
word will appear more frequently and in more contexts than other, less prevalent senses.
The neighbors of the target word relate to its senses, but are themselves word forms
rather than senses. The senses of the target word are predefined in a sense inventory
and we use a semantic similarity score defined over the sense inventory to relate the
neighbors to the various senses of the target word. The two semantic similarity scores
that we use in this article are implemented in the WordNet similarity package. One uses
the overlap in definitions of word senses, based on Lesk (1986), and the other uses a
combination of corpus statistics and the WordNet hyponym hierarchy, based on Jiang
and Conrath (1997). We describe these fully in Section 4.2. We now describe intuitively
10 In this and other examples we restrict ourselves to four neighbors for brevity.
11 In this example we assume that the sense inventory assigns four senses to star, but the inventory could
assign fewer or more depending on its level of granularity and level of detail.
12 Note that this zodiac or horoscope sense of star usually occurs as part of the multiword star sign (e.g.,
your star sign secrets revealed) or in plural form (your stars today?free online).
563
Computational Linguistics Volume 33, Number 4
the measure for ranking the senses according to predominance, and then give a more
formal definition.
The measure uses the sum total of the distributional similarity scores of the k nearest
neighbors. This total is divided between the senses of the target word by apportioning
the distributional similarity of each neighbor to the senses. The contribution of each
neighbor is measured in terms of its distributional similarity score so that ?nearer?
neighbors count for more. The distributional similarity score of each neighbor is divided
between the various senses rather than attributing the neighbor to only one sense. This
is done because neighbors can relate to more than one sense due to relationships such
as systematic polysemy. For example, in the thesaurus we describe subsequently in
Section 4.1 acquired from the BNC, chicken has neighbors duck and goose which relate to
both the meat and animal senses. We apportion the contribution of a neighbor to each
of the word senses according to a weight which is the normalized semantic similarity
score between the sense and the neighbor. We normalize the semantic similarity scores
because some of the semantic similarity scores that we use, described in Section 4.2,
can get disproportionately large. Because we normalize the semantic similarity scores,
the sum of the ranking scores for a word equals the sum of the distributional similarity
scores. To summarize, we rank the senses of the target word, such as star, by apportion-
ing the distributional similarity scores of the top k neighbors between the senses. Each
distributional similarity score (dss) is weighted by a normalized semantic similarity
score (sss) between the sense and the neighbor. This process is illustrated in Figure 1.
More formally, to find the predominant sense of a word (w) we take each sense
in turn and obtain a prevalence score. Let Nw = {n1, n2...nk} be the ordered set of the
top scoring k neighbors of w from the distributional thesaurus with associated scores
{dss(w, n1), dss(w, n2), ...dss(w, nk)}. Let senses(w) be the set of senses of w in the sense
inventory. For each sense of w (si ? senses(w)) we obtain a prevalence score by summing
over the dss(w, nj) of each neighbor (nj ? Nw) multiplied by a weight. This weight is the
sss between the target sense (si) and nj divided by the sum of all sss scores for senses(w)
Figure 1
The prevalence ranking process for the noun star.
564
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
and nj. sss is the maximum WordNet similarity score (sss?) between si and the senses of
nj (sx ? senses(nj)).13 Each sense si ? senses(w) is therefore assigned a score as follows:
Prevalence Score(w, si) =
?
nj?Nw
dss(w, nj) ?
sss(si, nj)
?
si??senses(w) sss(si? , nj)
(1)
where
sss(si, nj) = max
sx?senses(nj )
sss?(si, sx) (2)
We describe dss and sss? in Sections 4.1 and 4.2. Note that the dss for a given neighbor
is shared between the different senses of w depending on the weight given by the
normalized sss.
4.1 The Distributional Similarity Score
Measures of distributional similarity take into account the shared contexts of the two
words. Several measures of distributional similarity have been described in the litera-
ture. In our experiments, dss is computed using Lin?s similarity measure (Lin 1998a).
We set the number of nearest neighbors to equal 50.14 We use three different sources of
data for our first two experiments, resulting in three distributional thesauruses. These
are described in the next section. We use domain-specific data for our third and fourth
experiments. The data sources for these are described in Sections 6.3 and 6.4.
A word, w, is described by a set of features, f , each with an associated frequency,
where each feature is a pair ?r, x? consisting of a grammatical relation name and the
other word in the relation. We computed distributional similarity scores for every pair of
words of the same PoS where each word?s total feature frequency was at least 10. A the-
saurus entry of size k for a target word w is then defined as the k most similar words to w.
A large number of distributional similarity measures have been proposed in the
literature (see Weeds 2003 for a review) and comparing them is outside the scope of this
work. However, the study of Weeds and Weir (2005) provides interesting insights into
what makes a ?good? distributional similarity measure in the contexts of semantic simi-
larity prediction and language modeling. In particular, weighting features by pointwise
mutual information (Church and Hanks 1989) appears to be beneficial. The pointwise
mutual information (I(w, f )) between a word and a feature is calculated as
I(w, f ) = log
P( f |w)
P( f )
(3)
Intuitively, this means that the occurrence of a less-common feature is more important
in describing a word than a more-common feature. For example, the verb eat is more
selective and tells us more about the meaning of its arguments than the verb be.
13 We use sss for the semantic similarity between a WordNet sense and another word, the neighbor. We use
sss? for the semantic similarity between two WordNet senses, si and a sense of the neighbor (sx).
14 From previous work (McCarthy et al 2004b), the value of k has a minimal effect on finding the
predominant sense; however, we will continue experimentation with this in the future for using our
ranking score for estimating probability distributions of senses, because a sufficiently large value of k will
be needed to include neighbors for rarer senses.
565
Computational Linguistics Volume 33, Number 4
We chose to use the distributional similarity score described by Lin (1998a) because
it is an unparameterized measure which uses pointwise mutual information to weight
features and which has been shown (Weeds 2003) to be highly competitive in making
predictions of semantic similarity. This measure is based on Lin?s information-theoretic
similarity theorem (Lin 1997):
The similarity between A and B is measured by the ratio between the amount of
information needed to state the commonality of A and B and the information needed to
fully describe what A and B are.
In our application, if T(w) is the set of features f such that I(w, f ) is positive, then the
similarity between two words, w and n, is
dss(w, n) =
?
f?T(w)?T(n)
(
I(w, f ) + I(n, f )
)
?
f?T(w) I(w, f ) +
?
f?T(n) I(n, f )
(4)
However, due to this choice of dss and the openness of the domain, we restrict ourselves
to only considering words with a total feature frequency of at least 10. Weeds et al (2005)
do show that distributional similarity can be computed for lower frequency words but
this is using a highly specialized corpus of 400,000 words from the biomedical domain.
Further, it has been shown (Weeds et al 2005; Weeds and Weir 2005) that performance
of Lin?s distributional similarity score decreases more significantly than other measures
for low frequency nouns. We leave the investigation of other distributional similarity
scores and the application to smaller corpora as areas for further study.
4.2 The Semantic Similarity Scores
WordNet is widely used for research in WSD because it is publicly available and there
are a number of associated sense-tagged corpora (Miller et al 1993; Cotton et al 2001;
Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) available for testing purposes.
Several semantic similarity scores have been proposed that leverage the structure of
WordNet; for sss? we experiment with two of these, as implemented in the WordNet
Similarity Package (Patwardhan and Pedersen 2003).
The WordNet Similarity Package implements a range of similarity scores. McCarthy
et al (2004b) experimented with six of these for the sss? used in the prevalence score,
Equation (2). In the experiments reported here we use the two scores that performed
best in that previous work. We briefly summarize them here; Patwardhan, Banerjee,
and Pedersen (2003) give a more detailed discussion. The scores measure the similarity
between two WordNet senses (s1 and s2).
lesk This measure (Banerjee and Pedersen 2002) maximizes the number of overlap-
ping words in the gloss, or definition, of the senses. It uses the glosses of semanti-
cally related (according to WordNet) senses too. We use the default version of the
measure in the package with no normalizing for gloss length, and the default set
of relations:
lesk(s1, s2) = |{W1 ? definition(s1)}| ? |{W2 ? definition(s2)}| (5)
where definitions(s) is the gloss definition of sense s concatenated with the gloss
definitions of the senses related to s where the relationships are defined by the de-
566
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
fault set of relations in the relations.dat file supplied with the WordNet Similarity
package. W ? definition(s) is the set of words from the concatenated definitions.
jcn This measure (Jiang and Conrath 1997) uses corpus data to populate classes
(synsets) in the WordNet hierarchy with frequency counts. Each synset is incre-
mented with the frequency counts (from the corpus) of all words belonging to
that synset, directly or via the hyponymy relation. The frequency data is used to
calculate the ?information content? (IC; Resnik 1995) of a class as follows:
IC(s) = ?log(p(s)) (6)
Jiang and Conrath specify a distance measure:
Djcn(s1, s2) = IC(s1) + IC(s2) ? 2 ? IC(s3) (7)
where the third class (s3) is the most informative, or most specific, superordinate
synset of the two senses s1 and s2. This is converted to a similarity measure
in the WordNet Similarity package by taking the reciprocal as in Equation (8)
(which follows). For this reason, the jcn values can get very large indeed when
the distances are negligible, for example where the neighbor has a sense which is
a synonym. This is a motivation for our normalizing the sss in Equation (1).
jcn(s1, s2) = 1/Djcn(s1, s2) (8)
The IC data required for the jcn measure can be acquired automatically from raw
text. We used raw data from the BNC to create the IC files. There are various parameters
that can be set in the WordNet Similarity Package when creating these files; we used
the RESNIK method of counting frequencies in WordNet (Resnik 1995), the stop words
provided with the package, and no smoothing.
The lesk score is applicable to all parts of speech, whereas the jcn is applicable
only to nouns and verbs because it relies on IC counts which are obtained using the
hyponym links and these only exist for nouns and verbs.15 However, we did not use
jcn for verbs because in previous experiments (McCarthy et al 2004c) the lesk measure
outperformed jcn because the structure of the hyponym hierarchy is very shallow for
verbs and the measure is therefore considerably less informative for verbs than it is for
nouns.
4.3 An Example
We illustrate the application of our measure with an example. For star, if we set16 k = 4
and have the dss for the previously given neighbors as in the first row of Table 6, and
15 For verbs these pointers actually encode troponymy, which is a particular kind of entailment relation,
rather than hyponymy.
16 In this example, as before, we set k to 4 for the sake of brevity.
567
Computational Linguistics Volume 33, Number 4
Table 6
Example dss and sss scores for star and its neighbors.
Neighbors of star (dss)
Senses actor (0.22) footballer (0.12) planet (0.08) circle (0.03)
celebrity 0.42 0.53 0.02 0.01
celestial body 0.01 0.01 0.68 0.10
shape 0.0 0.0 0.02 0.78
zodiac 0.03 0.03 0.21 0.01
Total 0.46 0.57 0.93 0.90
the sss between the senses and the neighbors as in the remaining rows, the prevalence
score for celebrity would be:
= 0.22 ? 0.420.46 + 0.12 ? 0.530.57 + 0.08 ? 0.020.93 + 0.03 ? 0.010.90
= 0.2009 + 0.1116 + 0.0017 + 0.0003
= 0.3145
The prevalence score for each of the senses would be:
prevalence score(celebrity) = 0.3145
prevalence score(celestial body) = 0.0687
prevalence score(shape) = 0.0277
prevalence score(zodiac) = 0.0390
so the method would select celebrity as the predominant sense.
5. Experimental Setup
5.1 The Distributional Thesauruses
The three thesauruses used in our first two experiments were all created automatically
from raw corpus data, based either on grammatical relations between words computed
by syntactic parsers or alternatively on word proximity relations.
We created the first thesaurus, which we call BNC, from grammatical relation output
produced by the RASP system (Briscoe and Carroll 2002) applied to the 90M words of
the ?written? portion of the British National Corpus (Leech 1992), for all polysemous
nouns, verbs, adjectives, and adverbs in WordNet. For each word we considered co-
occurring words in the grammatical contexts listed in Table 7.
In the first two experiments, we also use two further automatically computed
distributional thesauruses, produced by Dekang Lin from 125M words of text from the
Wall Street Journal, San Jose Mercury News, and AP Newswire, using the same similarity
measure. The thesauruses are publicly available.17 One was constructed based on word
17 The thesauruses are available for download from http://www.cs.ualberta.ca/~lindek/
downloads.htm.
568
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 7
Grammatical contexts used for acquiring the BNC thesaurus.
PoS Grammatical contexts
noun verb in direct object or subject relation, adjective or noun modifier
verb noun as direct object or subject
adjective modified noun, modifying adverb
adverb modified adjective or verb
Table 8
Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6.
PoS Thesaurus types NISC NITH
noun BNC 7,090 2,436 115
noun DEP 6,583 2,176 217
noun PROX 6,582 2,176 217
verb BNC 2,958 553 45
adjective BNC 3,659 1,208 123
adverb BNC 505 132 38
similarities computed from syntactic dependencies produced by MINIPAR (Lin 1998b),
and the other was constructed based on textual proximity relationships between words.
We refer below to the original corpus as NEWSWIRE, and these two thesauruses as DEP
and PROX, respectively. We restricted our experiments to the nouns in these thesauruses.
Table 8 contains details of the numbers of polysemous (according to WordNet 1.6)
words contained in these thesauruses, the number of words in SemCor that were not
found in these thesauruses (NITH) and the number of words in the thesauruses that
were not in SemCor (NISC).
For the experiments described in Sections 6.3 and 6.4 we use exactly the same
method as that proposed for the BNC thesaurus, however the data source is different
and is described in those sections.
5.2 The Sense Inventory
We use WordNet version 1.6 as the sense inventory for our first three experiments, and
1.7.1 for our last experiment.18
For sss? we use the WordNet Similarity Package version 0.05 (Patwardhan and
Pedersen 2003).
18 We use 1.6 which is a rather old version of WordNet so that we can directly evaluate on the SemCor data
released with this version; we also use it to enable comparison with the results of McCarthy et al (2004a).
We use WordNet 1.7.1 for the fourth experiment, because this is the version that was used for annotating
the test data in that experiment. We plan to move to more recent versions of WordNet and experiment
with other sense inventories in the future.
569
Computational Linguistics Volume 33, Number 4
6. Experiments
In this section we describe four experiments using our method for acquiring predomi-
nant sense information.
The first experiment evaluates automatically acquired predominant senses for all
parts of speech, using SemCor as the test corpus. This extends previous work which
had only evaluated all PoS on Senseval-2 (Cotton et al 2001) and Senseval-3 (Mihalcea
and Edmonds 2004) data. The SemCor corpus is composed of 220,000 words, in contrast
to the 6 documents in the Senseval-2 and -3 English all-words data (10,000 words). We
examine the effects of using the two different semantic similarity scores that performed
well in previous work: jcn is quick to compute but lesk has the advantage that it is
applicable to all PoS and can be implemented for any dictionary with sense defini-
tions. We compare three thesauruses: one is derived from the BNC and two from the
NEWSWIRE corpus. The two from the NEWSWIRE corpus examine the requirement for
a parser by contrasting results obtained when the thesaurus is built using parsed data
compared to a proximity approach. We contrast the results of the BNC thesaurus with
a simplified version of the prevalence score which uses the number of the k neighbors
closest to a sense for ranking without using the dss and without sharing the credit for
a neighbor between senses. We also perform an error analysis on a random sample
of words for which a predominant sense was found that differed from that given by
SemCor, identifying and giving an indication of the frequencies of the main sources of
error.
The second experiment is on nouns in the Senseval-2 all-words data, again using
predominant senses acquired using each of the three distributional thesauruses, but
in this experiment we explore the benefits of an automatic first sense heuristic when
there is inadequate data in available resources. Although McCarthy et al (2004c) show
that on Senseval-2 and Senseval-3 test data a first sense heuristic derived from SemCor
outperforms the automatic method, we look at whether the method?s performance is
relatively stronger on words for which there is little data in SemCor. This is important
because, as we have shown in Table 5, low frequency words are used often in senses
other than the sense that is ranked first according to SemCor.
In addition to the issue of lack of coverage of manually annotated resources, sense
frequency will depend on the domain of the data. In the third experiment, we revisit
some previous work on noun senses and domain (McCarthy et al 2004a) using corpora
of news text about sports and finance. Using distributional thesauruses computed from
these corpora and a gold standard domain labeling of word senses we look at the
potential for computing domain-specific predominant senses for parts of speech other
than nouns.
Continuing the line of research on automatic acquisition of domain-specific pre-
dominant senses, the fourth experiment compares results when we train and test on
domain-specific corpora, where the training data is (1) manually categorized for domain
and from the same corpus as the gold-standard test data, and (2) where the training data
is harvested automatically from another corpus which is categorized automatically.
6.1 Experiment 1: All Parts of Speech
In this experiment, we evaluate the accuracy of automatically acquired predominant
senses for all open class parts of speech, taking SemCor as the gold standard. For nouns
we use the semantic similarity measures lesk and jcn, and for other parts of speech, lesk.
We use the three distributional thesauruses BNC, DEP, and PROX.
570
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The gold standard is derived from the Brown Corpus files publicly released as part
of SemCor, rather than the processed data provided in the cntlist file in the WordNet
distribution. The released SemCor files contain only the tagged data from the Brown
Corpus and do not include data from The Red Badge of Courage. We use the released data
rather than that in cntlist because this includes the actual tagged examples which are
marked for genre by the Brown files. We envisage the possibility of further experiments
with these genre markers. We only evaluate on instances where a single, unique sense
is supplied by the annotators. So, for example, we ignore instances like the following
with multiple wnsn values:
<wf cmd=done pos=NN lemma=tooth wnsn=3;1 lexsn=1:05:02::;1:08:00::>tooth</wf>
We also only evaluate on polysemous words (according to WordNet) having one sense
in SemCor which is more frequent than any other, and for which both SemCor and our
thesauruses have at least a minimal amount of data. Specifically, a word must occur
three or more times in SemCor; it must also occur in ten or more grammatical relations
in the parsed version of the BNC and have neighbors in the distributional thesaurus, or
be present in Dekang Lin?s thesaurus.19
We evaluate on nouns, verbs, adjectives, and adverbs separately, computing a num-
ber of accuracy measures, both type-based and token-based. PSacc is calculated over
word types in SemCor which have one sense which occurs more than any other. It is the
accuracy of identifying the predominant sense in SemCor. If the automatic ranking has
a tie for the top ranked sense then we score that word as incorrect.20 So we have
PSacc =
|correcttyp|
|typesmf |
? 100 (9)
where typesmf are the types in SemCor such that one sense is more frequent than
any other, the word has occurred at least three times in SemCor and has an entry
in the thesaurus. |correcttyp| is the number of these where the automatically acquired
predominant sense matches the first sense in SemCor.
PSaccBL is the predominant sense random baseline, obtained as follows:
PSaccBL =
?
w?typesmf
1
|senses(w)|
|typesmf |
? 100 (10)
WSDsc is a token-based measure. It is the WSD accuracy that would be obtained
by using the first sense heuristic with the automatically acquired predominant sense
information, in cases where there was a unique automatic top ranked sense:
WSDsc =
|correcttok|
|SCtokensafs|
? 100 (11)
19 Although we do not evaluate words for which there were no neighbors in the thesaurus, we could extend
the thesaurus to include some of these by widening the range of grammatical relations covered and
compensating for some systematic PoS tagging errors.
20 If we exclude these words with joint top ranking from the automatic method (precision rather than recall)
then we obtain marginally higher accuracy for the jcn measure but no difference for lesk.
571
Computational Linguistics Volume 33, Number 4
where |correcttok| is the number of tokens disambiguated correctly out of the tokens in
SemCor having an automatically acquired first sense (SCtokensafs).
SC FS is the WSD accuracy of the SemCor first sense heuristic on the same set of
tokens (SCtokensafs), which is the upper bound because the information it uses is derived
from the test data itself. RBL is the random baseline for the WSD task, calculated by
splitting the credit for each token to be tagged in the test data evenly between all of the
word?s senses.
RBL =
?
w?SCtokensafs
1
|senses(w)|
|SCtokensafs|
? 100 (12)
The results are shown in Table 9. We examined differences between the semantic
similarity measures (lesk and jcn), the BNC and DEP thesauruses, and the DEP and
PROX thesauruses using the ?2 test of significance with one degree of freedom (Siegel
and Castellan 1988). None of the differences between the different combinations of
similarity measures and thesauruses for the type-based measure PSacc are significant.
The differences between lesk and jcn are significant for the token-based measure WSDsc
for both the BNC and PROX thesauruses (both p < .001), however not when comparing
lesk and jcn for the DEP thesaurus. Although lesk is more accurate than jcn, at least on
the WSD task, jcn is much faster because of the precompilation of IC in the WordNet
similarity package; however, lesk has the additional benefit of being applicable to other
parts of speech. The method gives particularly good results for adjectives, given that
they have a similar random baseline to nouns. It does not do so well for adverbs and
verbs, but still performs well above the random baseline which is low for verbs due
to their high degree of polysemy. Given that the first sense heuristic from SemCor is
particularly strong for adverbs, it is disappointing that the automatic method does not
perform as well as it does on adjectives. One possible reason for this might be that
adverbs are often less strongly associated to the verbs that they modify than adjectives
are to the nouns that they modify, so the distributional thesaurus information is less
reliable. Another reason may be that less data are available for adverbs, both in the
thesaurus and also in WordNet.
Table 9
Evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 54.5 32.3 53,468 48.7 68.6 24.7
noun lesk DEP 2,437 56.3 32.1 52,158 49.2 68.4 24.6
noun lesk PROX 2,437 55.9 32.1 52,158 49.0 68.4 24.6
noun jcn BNC 2,555 54.0 32.3 53,429 46.1 68.6 24.7
noun jcn DEP 2,436 56.4 32.1 52,122 48.8 68.4 24.6
noun jcn PROX 2,436 55.9 32.1 52,117 47.7 68.4 24.6
verb lesk BNC 1,149 45.6 27.1 31,182 36.1 57.1 17.1
adjective lesk BNC 1,154 60.4 32.8 18,216 56.8 73.8 24.9
adverb lesk BNC 230 52.2 39.9 8,810 43.2 76.1 33.0
572
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Comparing the results for the DEP and the PROX thesauruses, we see that although
there is no significant difference in PSacc (with either lesk or jcn), there is for WSDsc
when using jcn (p < .001), but not when comparing the lesk values for these thesauruses.
Even though the differences between jcn DEP and jcn PROX are significant, the absolute
differences are nevertheless relatively small; this bodes well for applying the automatic
predominant sense method to languages less well resourced than English, because
the PROX thesaurus was produced without using a parser. The differences in results
between jcn BNC and jcn DEP for nouns are statistically significant (p < .001).21 The
better accuracy with DEP may be because the NEWSWIRE corpus is larger than the
BNC. We intend to investigate the effects of corpus size in the future. The differences in
results between lesk BNC and lesk DEP for nouns are not significant.
6.1.1 Results Using Simplified Prevalence Score. A simple variation of our method is just to
associate each neighbor with just one sense and use the number of neighbors associated
with a sense for the prevalence score. This gives a modified version of Equation (1)
where each sense si ? senses(w) is assigned a score as follows:
Simplified Prevalence Score(w, si) = |{nj ? Nw} : arg max
sk?senses(w)
(sss(sk, nj)) = si| (13)
where
sss(sk, nj) = max
sx?senses(nj )
sss?(sk, sx) (14)
For the example in Table 6, celebrity would get the top score of 2 (due to it having
the highest sss for actor and footballer), celestial body would get a score of 1 (due to its
sss with planet), shape would get 1 (due to circle), and zodiac would obtain a Simplified
Prevalence Score of 0 because it does not have the highest sss for any of the neighbors.
As the results from Table 10 show, we do not get such good results with this score.
This supports our intuition that a combination of both the number of neighbors and
their distributional similarity scores is important for determining predominance. The
rest of the article gives results and analysis for our original prevalence score as given in
Equation (1).
6.1.2 Error Analysis. We took a random sample of 80 words that occurred more than five
times in SemCor, 20 words for each PoS, from those where the automatically identified
predominant sense was different from the SemCor first sense when using the lesk sss
and BNC thesaurus and our ranking score as defined in Equation (1) (i.e., the data
represented by the first result line and the last three result lines of Table 9). Herein, we
call the automatically identified sense AUTO FS, and the SemCor sense SemCor FS. We
21 The coverage of the SemCor data by the DEP and PROX thesauruses is slightly lower than that of the
BNC-derived thesaurus due to mismatches in spelling and capitalization and also probably because the
NEWSWIRE corpus is narrower in genre and domain than the BNC.
573
Computational Linguistics Volume 33, Number 4
Table 10
Simplified prevalence score, evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 52.9 32.3 53,175 47.2 68.6 24.7
noun jcn BNC 2,555 50.1 32.3 52,033 46.7 69.2 24.8
verb lesk BNC 1,149 45.1 27.1 30,364 36.7 58.0 17.4
adjective lesk BNC 1,154 58.3 32.8 18,136 56.0 73.7 24.8
adverb lesk BNC 230 50.0 39.9 8,802 42.2 76.1 33.0
manually inspected the data for each of the words to find the source of the problem.
We did not have the (substantial) resources that would be required to sense tag all
occurrences of these words in the BNC to see what their actual first senses were. Instead,
we examined the parses, grammatical relations, and sense definitions for the words to
see why the AUTO FS was ranked above the SemCor FS. We found the following main
types of error:22
corpora The difference appears to be due to genuine divergence between the BNC
and SemCor. For this error type we looked at the BNC parses to see if the acquired
predominant sense was clearly due to differences in the corpus data. There may
be other errors that should have been assigned this category, but without access
to sense tagged BNC data we could not be sure of this, so we used this category
conservatively. An example of this error is the adjective solid which has the good
quality first sense in the Brown files in SemCor, but the firm sense according to
our BNC automatic ranking.
related The automatic predominant sense is closely related to the SemCor first sense.
Although many word senses are related to some extent, the category was picked
where a close relationship seemed to be the main cause of the error. An example
is the noun straw which has two senses in WordNet 1.6, fibre used for hats and
fodder and plant material. The SemCor FS was the former whereas our AUTO FS
was the latter.
competing Two or more related senses are ranked highly but they are overtaken by
an unrelated sense. For example, the ranking and scores for the noun transmission
are:
WordNet sense Description Prevalence score
5 gears 1.79
2 communication 1.20
1 act of sending a message 1.19
3 fraction of radiant energy 0.48
4 infection 0.15
22 There were a few other, less numerous types of error, for example systematic PoS mis-tagging of particles
(such as down) as adverbs.
574
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The act of sending a message sense is overtaken by the gears sense because the
credit from shared distributional neighbors is split between it and the communi-
cation sense.
neighbors There are not many neighbors related to the sense. There can be various
reasons for this, such as the sense having restricted contexts of occurrence or only
a small number of near synonyms existing for the sense. An example of this is
the adjective live where the SemCor FS unrecorded sense seems to occur in the
BNC corpus more than the alive sense; there are plenty of grammatical relations
pertaining to this sense, but there are few distributional neighbors near in meaning
to unrecorded.23
spurious similarity The WordNet similarity scores were misled by spurious relation-
ships to neighbors; this can occur in dense areas such as the ?physical object?
region of the noun hyponym hierarchy. An example of this is the verb tap which
has neighbors push and press which are related to the AUTO FS (solicit) as well as
the SemCor FS (strike lightly).
The results of the error analysis are shown in Table 11. The analysis shows that
differences between the training (BNC) and testing (SemCor) corpora are not a major
source of error. Although SemCor itself (the released files from the Brown corpus
comprising only 200,000 words) is not large enough to build a thesaurus with entries
for a reasonable portion of the words, we did build a thesaurus from the entire Brown
corpus (1 million words) to see the effect of corpus data. The results are compared
to those from the BNC in Table 12 on the set of words which had thesaurus entries
in the Brown data (to make the results more comparable, because the corpora are of
such different sizes). We also show the average results for 10 random selections of a
1 million word random sample of the BNC. To do this we randomly selected 190 th of the
tuples.24 The differences in the WSDsc for the BNC 190 sample and the Brown corpus are
significant (p < .01 on the ?2), but the differences in PSacc are not significant. Although
the entire BNC produced better results than the Brown data, this is undoubtedly due to
the difference in size of the corpus. Taking a comparably sized sample, the results are
slightly better from Brown which is the corpus from which SemCor is taken.
For nouns, it was apparent that in two cases less-prevalent senses were receiving a
higher ranking simply because the credit for some neighbors associated with another
meaning was split between related senses (error type competing). This was not ob-
served for other parts of speech, possibly because the AUTO FS was rarely unrelated to
the SemCor FS.
There were some problems arising from spurious similarity. One possible source
of such problems is due to the ambiguity of the neighbor; in the future we will look
at reducing this source of error by removing neighbors which have a value for sx in
Equation (2) which is not the same as that preferred by the other senses of the target
word (w). For adverbs, all the cases that were categorized as spurious similarity were
also noted to be related to the SemCor FS, though they were not categorized as related
as this was not considered the primary cause of the error.
The analysis was hardest for verbs. Verbs are on average highly polysemous, and
often have senses that are related. Furthermore, the structure of the WordNet verb tro-
ponym hierarchy is very shallow compared to the noun hyponymy hierarchy, so there
23 The closest neighbors to the adjective live are adult, forthcoming, lively, solo, excellent, stuffed, living, dead,
and australian weekly.
24 The variance for the 190 sample for PSacc was 0.46 and for WSDsc it was 0.49.
575
Computational Linguistics Volume 33, Number 4
Table 11
Results of the error analysis for the sample of 80 words.
PoS
noun verb adjective adverb All PoS
corpora 1 2 1 1 5
related 8 12 13 8 41
competing 2 0 0 0 2
neighbors 4 3 2 2 11
spurious similarity 5 3 4 9 21
Table 12
SemCor results for Nouns using jcn.
Thesaurus PSacc% WSDsc %
full BNC 53.8 44.9
1
90 BNC 46.6 40.8
Brown 47.2 41.7
are more possibilities for spurious similarities from overlap of glosses. So, although we
tried to identify the main problem source, for verbs the problems usually arose from a
combination of factors and the relatedness of the senses was usually one of these.
Relatedness of senses and fine-grained distinctions are major sources of error. There
have been various attempts to group WordNet senses both manually and automati-
cally (Agirre and Lopez de Lacalle 2003; McCarthy 2006; Palmer, Dang, and Fellbaum
2007). Indeed, McCarthy demonstrated that distributional and semantic similarity can
be used for relating word senses and that such methods increase accuracy of first
sense heuristics, including the automatic method proposed here. WSD is improved with
coarser-grained inventories but ultimately, performance depends on the application.
For example, the noun bar has 11 senses in WordNet 1.6. These include the pub sense
as well as the counter sense and these are related to a certain extent. One might want
to group them when acquiring predominant senses, but there may be situations where
they should be distinguished. For example, if one were to ask a robot to ?go to the bar?
one would hope it could use the context to go get the drinks rather than replying that it
is already there! Even in cases where fine-grained distinctions are ultimately required, it
may be helpful to have a coarse-grained prior and then use contextual features to tease
apart subtle sense distinctions.
From our error analysis, the problem of semantically isolated senses (identified as
neighbors) was not a major source of error, but still causes some problems. One possible
remedy might be to identify these cases by looking for neighbors which relate strongly
to a sense which none of the other neighbors relate to and weighting the contribution
from these neighbors more. This may however give rise to further errors because of the
noise introduced by focusing on individual neighbors. We will explore such directions
in future work.
576
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
In this experiment we did not assign any credit for near misses. In many cases
of error the SemCor FS nonetheless received a high prevalence score. In the future
we hope to use the score for probability estimation, and combine this with contextual
information for WSD as in related work by Lapata and Brew (2004) and Chan and Ng
(2005).
6.2 Experiment 2: Frequency and the SemCor First Sense Heuristic
In the previous section we described an evaluation of the accuracy of automatically
acquired predominant sense information. We carried out the evaluation with respect to
SemCor in order to have as much test data as possible. To obtain reasonably reliable
gold-standard first-sense data and first-sense heuristic upper bounds, we limited the
evaluation to words occurring at least three times in SemCor. Clearly this scenario is
unrealistic. For many words, and particularly for nouns, there is very little or no data in
SemCor; Table 2 shows that 81.9% of nouns (excluding multiwords) listed in WordNet
do not occur at all in SemCor. Thus, even for English, which has substantial manually
sense-tagged resources, coverage is severely limited for many words.
For a more realistic comparison of automatic and manual heuristics, we therefore
now change to a different test corpus, the Senseval-2 English all-words task data set. We
focus on nouns and evaluate using all words regardless of their frequencies in SemCor.
We examine the effect of frequency in SemCor on performance of a SemCor-derived
heuristic in comparison to results from our automatic method on the same words. Our
hypothesis is that although automatically acquired predominant sense information may
not outperform first-sense data obtained from a hand-tagged resource over all words in
a text, the information may well be more accurate for low frequency items.
We use a mapping between different WordNet versions25 (Daude?, Padro?, and Rigau
2000) to obtain the Senseval-2 all words noun data (originally distributed with 1.7 sense
numbers) with 1.6 sense numbers. As well as examining the performance of our method
in contrast to the SemCor heuristic, we calculate an upper bound for this using the
first sense heuristic from the Senseval-2 all-words data itself. This is obtained for nouns
with two or more occurrences in the Senseval-2 data and where one sense occurs more
than any of the others. We calculate type, precision, and recall, using this Senseval-2
first-sense as the gold standard. The recall measure is the same as PSacc described
previously, except that we include items which do not have entries in the thesaurus,
scoring them incorrect. Precision only includes items where there is a sense ranked
higher than any other for that word with the prevalence score, that is, it does not include
items with a joint automatic ranking. We also calculate token precision and recall (WSD).
These measures relate to WSDsc, but again, recall includes words not in the thesaurus
which are scored incorrect, and precision does not include items with a joint automatic
ranking. We also separately compute WSD precision for words not in SemCor (NISC).
The results are shown in Table 13.26
The automatically acquired predominant sense results (the first six lines of results
in the table) are approaching the SemCor-derived results (third line from the bottom of
the table). The NISC results are particularly encouraging, but with the caveat that there
are only 17 such words in the data. The precision for these items is higher than the
25 This mapping is available at http://www.lsi.upc.es/~nlp/tools/mapping.html.
26 Note that these figures are lower than those of McCarthy et al (2004a) in a similar experiment because
the evaluation here is only on polysemous words.
577
Computational Linguistics Volume 33, Number 4
Table 13
Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words
task data.
Type WSD/token
Settings Precision (%) Recall (%) Precision (%) Recall (%) Precision NISC (%)
lesk BNC 56.3 53.7 54.6 53.4 58.3
lesk DEP 52.0 47.2 52.6 48.7 58.3
lesk PROX 52.0 47.2 52.3 48.5 58.3
jcn BNC 52.4 50.0 51.8 50.6 66.7
jcn DEP 52.0 47.2 58.0 53.7 83.3
jcn PROX 53.1 48.1 57.3 53.1 83.3
SemCor 64.8 63.0 58.5 57.3 0.0
Senseval-2 ? ? 90.8 60.1 100.0
RBL 26.5 26.5 26.0 26.0 50.0
overall figure. This is because the nouns involved are less frequent so tend to be less
polysemous and consequently have a higher random baseline. There are a few nouns
that are not in the automatic ranking, but this is due to the fact that neighbors were
not collected for these nouns in the thesaurus because of tagging or parser errors or
the particular set of grammatical relations used. It should be possible to extend the
range of grammatical relations, or use proximity-based relations, so that neighbors can
be obtained in these cases. It would also be possible to assign some credit in the case of
joint top ranked senses to increase coverage.
Looking at Table 13 in more detail, it seems to be the case that although the BNC
thesaurus does well in identifying the first sense of a word (the type results), the PROX
and DEP thesauruses from the NEWSWIRE corpus return better WSD results when
used with the jcn measure. This is possibly because jcn works well for more frequent
items due to its incorporation of frequency information, and the NEWSWIRE corpus
has more data for frequent words, although coverage is not as good as the BNC as
seen by the bigger differences in precision and recall and the figures in Table 8. The
lower coverage may be due to the narrower domain and genre of the NEWSWIRE
corpus, though spelling and capitalization differences probably also account for some
differences.
Table 14 shows results on the Senseval-2 nouns for the best similarity measure
and thesaurus combinations in Table 13 for nouns at or below various frequencies
in SemCor. (The differences between the DEP and PROX thesauruses are negligible at
frequencies of 10 or below, so for those we report only the results for DEP.) As we
anticipated, for low frequency words the automatic methods do give more accurate
predominant sense information than SemCor. The low number of test items at frequency
five or less means that results for jcn with the BNC thesaurus are not significantly better
when compared with SemCor (p = .05); however the lesk WSD results are significantly
better (p < .01 for the ? 1 threshold and p < .05 for the ? 5 threshold). On the whole, we
see that the automatic method, using either jcn or lesk and any of the three thesauruses,
tend to give better results than SemCor on nouns which have low coverage in SemCor.
Figures 2 and 3 show the precision for type and token (WSD) evaluation where the
items have a frequency at or below given thresholds in SemCor. Although the manually
578
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 14
Senseval-2 results, polysemous nouns only, broken down by their frequencies of occurrence in
SemCor.
Type WSD/token
No. of occurrences in Precision Recall Precision Recall
SemCor (no. of words) Settings (%) (%) (%) (%)
jcn BNC 100.0 33.3 66.7 47.1
lesk BNC 100.0 33.3 58.3 41.2
0 (17) jcn DEP 100.0 33.3 83.3 58.8
lesk DEP 100.0 33.3 58.3 41.2
SemCor 0.0 0.0 0.0 0.0
Senseval-2 ? ? 100.0 52.9
RBL 38.9 38.9 46.1 46.1
jcn BNC 66.7 44.4 54.1 45.5
lesk BNC 83.3 55.6 67.6 56.8
jcn DEP 50.0 22.2 51.7 34.1
? 1 (44) lesk DEP 75.0 33.3 69.0 45.5
SemCor 50.0 33.3 33.3 20.5
Senseval-2 ? ? 93.3 63.6
RBL 40.7 40.7 42.8 42.8
jcn BNC 80.0 61.5 63.0 57.5
lesk BNC 90.0 69.2 71.2 65.0
? 5 (80) jcn DEP 71.4 38.5 56.7 42.5
lesk DEP 85.7 46.2 70.0 52.5
SemCor 60.0 46.2 54.0 42.5
Senseval-2 ? ? 95.9 58.8
RBL 38.1 38.1 39.1 39.1
jcn BNC 75.0 63.2 59.3 55.8
lesk BNC 68.8 57.9 62.8 59.2
? 10 (120) jcn DEP 66.7 42.1 56.8 45.0
lesk DEP 58.3 36.8 58.9 46.7
SemCor 68.8 57.9 57.3 49.2
Senseval-2 ? ? 96.8 50.8
RBL 37.5 37.5 38.0 38.0
jcn BNC 76.0 67.9 66.7 64.8
lesk BNC 64.0 57.1 71.6 69.6
? 15 (250) jcn DEP 60.0 42.9 68.8 55.6
lesk DEP 55.0 39.3 67.3 54.4
jcn PROX 70.0 50.0 72.3 58.4
lesk PROX 55.0 39.3 66.8 54.0
SemCor 64.0 57.1 66.5 62.0
Senseval-2 ? ? 98.8 68.4
RBL 32.9 32.9 30.4 30.4
jcn BNC 52.4 50.0 51.8 50.6
lesk BNC 56.3 53.7 54.6 53.4
all (786) jcn DEP 52.0 47.2 58.0 53.7
lesk DEP 52.0 47.2 52.6 48.7
jcn PROX 53.1 48.1 57.3 53.1
lesk PROX 52.0 47.2 52.3 48.5
SemCor 64.8 63.0 58.5 57.3
Senseval-2 ? ? 90.8 60.1
RBL 26.5 26.5 26.0 26.0
579
Computational Linguistics Volume 33, Number 4
Figure 2
?TYPE? precision on finding the predominant sense for the Senseval-2 English all-words test
data for nouns having a frequency less than or equal to various thresholds.
produced SemCor first-sense heuristic outperforms the automatic methods over all the
test items (see the ?all? results in Table 14), when items are below a frequency threshold
of five the automatic methods give better results. Indeed, as the threshold is moved
up to 20 and even 30, more nouns are covered, and the automatic methods are still
comparable and in some cases competitive with the SemCor heuristic.
6.3 Experiment 3: The Influence of Domain
In this experiment, we investigate the potential of the automatic ranking method for
computing predominant senses with respect to particular domains. We have previ-
ously demonstrated that the method produces intuitive domain-specific models for
nouns (McCarthy et al 2004a), and that these can be more accurate than first senses de-
rived from SemCor for words salient to a domain (Koeling, McCarthy, and Carroll 2005).
Here we investigate the behavior for other parts of speech, using a similar experimental
setup to that of McCarthy et al That work used the subject field codes (SFC) (Magnini
and Cavaglia` 2000)27 as a gold standard. In SFC the Princeton English WordNet is
augmented with some domain labels. Every synset in WordNet?s sense inventory is
annotated with at least one domain label, selected from a set of about 200 labels. These
labels are organized in a tree structure. Each synset of WordNet 1.6 is labeled with one
or more labels. The label factotum is assigned if any other is inadequate. The first level
consists of five main categories (e.g., doctrines and social science) and factotum.doctrines
27 More recently referred to as WordNet Domains (WN-DOMAINS).
580
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Figure 3
WSD precision on the Senseval-2 English all-words test data for nouns having a frequency less
than or equal to various thresholds.
has subcategories such as art, religion, and psychology. Some subcategories are further
divided in subcategories (e.g., dance, music, and theatre are subcategories of art).
McCarthy et al (2004a) used two domain-specific corpora for input to the method
for finding predominant senses. The corpora were obtained from the Reuters Corpus,
Volume 1 (RCV1; Rose, Stevenson, and Whitehead 2002) using the Reuters topic codes.
The two domain-specific corpora were:
SPORTS (Reuters topic code GSPO), 9.1 million words
FINANCE (Reuters topic codes ECAT and MCAT), 32.5 million words
In that work we produced sense rankings for a set of 38 nouns which have at
least one synset with an economy SFC label and one with a sport SFC label. We then
demonstrated that there were more sport labels assigned to the predominant senses
acquired from the SPORTS corpus and more economy labels assigned to those from the
FINANCE corpus. The predominant senses from both domains had a similarly high
percentage of factotum (domain-independent) labels. We reproduce the results here (in
Figure 4) for ease of reference, and for comparison with other results presented in this
section. The y-axis in this figure shows the percentage of the predominant sense labels
for these 38 nouns that have the SFC label indicated by the x-axis.
We envisaged running the same experiment with verbs, adjectives, and adverbs,
although we suspected that these would show less domain-specific tendencies and
there would be fewer candidate words to work with. The SFC labels for all senses of
polysemous words (excluding multiwords) in the various parts of speech are shown in
Table 15. We see from the distribution of factotum labels across the parts of speech that
nouns are certainly the PoS most likely to be influenced by domain.
To produce results like Figure 4 for each PoS, we needed words having at least one
synset with a sport label and one with an economy label. There were 20 such verbs but
581
Computational Linguistics Volume 33, Number 4
Figure 4
Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the
SPORTS and FINANCE corpora.
only two adjectives and no adverbs meeting this condition. We therefore performed
the experiment only with verbs. To do this we used the SPORTS and FINANCE corpora
as before, computing thesauruses for verbs using the grammatical relations specified
in Table 7. The results for the distribution of domain labels of the predominant senses
Table 15
Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech.
Domain % Domain %
noun biology 29.3 verb factotum 67.0
factotum 20.7 psychology 3.5
art 6.2 sport 2.9
sport 3.1 art 2.5
medicine 3.1 biology 2.5
other 37.6 other 21.6
adjective factotum 67.8 adverb factotum 81.4
biology 6.5 psychology 7.5
art 3.2 art 1.8
psychology 2.7 physics 1.1
physics 1.9 economy 1.1
other 17.9 other 7.1
582
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
acquired from the SPORTS and FINANCE corpora are shown in Figure 5. We see the same
tendency for sport labels for predominant senses from the SPORTS corpus and economy
labels for the predominant senses from the FINANCE corpus, but the relationship is
less marked compared with nouns because of the high proportions of factotum senses
in both corpora for verbs. We believe that acquisition of domain-specific predominant
senses should be focused on those words which show domain-specific tendencies. We
hope to put more work into automatic detection of these tendencies using indicators
such as domain salience and words that have different sense rankings in a given domain
compared to the BNC (as discussed by Koeling, McCarthy, and Carroll 2005).
6.4 Experiment 4: Domain-Specific Predominant Sense Acquisition
In the final set of experiments we evaluate the acquired predominant senses for domain-
specific corpora. The first of the two experiments was reported by Koeling, McCarthy,
and Carroll (2005), but we extend it by the second experiment reported subsequently.
Because there are no publicly available domain-specific manually sense-tagged corpora,
we created our own gold standard. The two chosen domains (SPORTS and FINANCE) and
the domain-neutral corpus (BNC) are the same as we used in the previous experiment.
We selected 40 words and we sampled (randomly) sentences containing these words
from the three corpora and asked annotators to choose the correct sense for the target
words. The set consists of 17 words which have at least one sense assigned an economy
domain label and at least one sense assigned a sports label: club, manager, record, right,
bill, check, competition, conversion, crew, delivery, division, fishing, reserve, return, score,
receiver, running; eight words that are particular salient in the SPORTS domain: fan,
star, transfer, striker, goal, title, tie, coach; eight words that are particular salient in the
Figure 5
Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the
SPORTS and FINANCE corpora.
583
Computational Linguistics Volume 33, Number 4
Table 16
WSD using predominant senses, training, and testing on all domain combinations
(hand-classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 39.1 49.9 24.0
SPORTS 25.7 19.7 43.7
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
FINANCE domain: package, chip, bond, market, strike, bank, share, target; and seven words
that are equally salient in both domains: will, phase, half, top, performance, level, country.
Koeling, McCarthy, and Carroll (2005) give further details of the construction of the gold
standard.
In the first experiment, we train on a corpus of documents with manually assigned
domain labels (i.e., sub-corpora of the Reuters corpus, see Section 6.3), and we test on
data from the same source. In a second experiment we build a text classifier, use the
text classifier to obtain SPORTS and FINANCE corpora (using general newswire text from
the English Gigaword Corpus; Graff 2003) and test on the gold-standard data from the
Reuters corpus. The second experiment eliminates issues about dependencies between
training and test data and will shed light on the question of how robust the acquired
predominant sense method is with respect to noise in the input data. At the same time,
the second experiment paves the way towards creating predominant sense inventories
for any conceivable domain.
6.4.1 Experiment Using Hand-Labeled Data. In this section we focus on the predominant
sense evaluation of the experiments described by Koeling, McCarthy, and Carroll (2005).
After running the predominant sense finding algorithms on the raw text of the two do-
main corpora (SPORTS and FINANCE) and the domain-neutral corpus (BNC), we evaluate
the accuracy of performing WSD on the sample of 40 words purely with the first sense
heuristic using all nine combinations of training and test corpora. The results (as given
in Table 16) are compared with a random baseline (?Random BL?)28 and the accuracy
using the first sense heuristic from SemCor (?SemCor FS?).29
The results in Table 16 show that the best results are obtained when the predominant
senses are acquired using the appropriate domain (i.e., test and training data from the
same domain). Moreover, when trained on the domain-relevant corpora, the random
baseline as well as the baseline provided by SemCor are comfortably beaten. It can be
observed from these results that apparently the BNC is more similar to the FINANCE
corpus than it is to the SPORTS corpus. The results for the SPORTS domain lag behind the
results for the FINANCE domain by almost 6 percentage points. This could be because
28 The random baseline is
?
i?tokens
1
#senses(i) .
29 The precision is given alongside in brackets because a predominant sense for the word striker is not
supplied by SemCor. The automatic method proposes a predominant sense in every case.
584
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 17
WSD using predominant senses, training, and testing on all domain combinations (automatically
classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 38.2 44.0 29.0
SPORTS 27.0 23.4 45.0
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
of the smaller amount of training data available (32M words versus 9M words), but it
could also be an artifact of this particular selection of words.
6.4.2 Experiment Using Automatically Classified Data. Although the previous experiment
shows that it is possible to acquire domain-specific predominant senses successfully, the
usefulness of doing this will be far greater if there is no need to classify corpora with
respect to domain by hand. There is no such thing as a standard domain specification
because the definition of a domain depends on user and application. It would be
advantageous if we could automatically obtain a user-/application-specific corpus from
which to acquire predominant senses.
In this section we describe an experiment where we build a text classifier using
WordNet as a sense inventory and the SFC domain extension (see Section 6.3). We
extracted bags of domain-specific words from WordNet for all the defined domains by
collecting all the word senses (synsets) and corresponding glosses associated with each
domain label. These bags of words are the fingerprints for the domains and we used
them to train a Support Vector Machine (SVM) text classifier using TwentyOne.30
The classifier distinguishes between 48 classes (the first and second levels of the
SFC hierarchy). When a document is evaluated by the classifier, it returns a list of
all the classes (domains) it recognizes and an associated confidence score reflecting the
certainty that the document belongs to that particular domain. We classified 10 months?
worth of data from the English Gigaword Corpus using this classifier and assigned each
document to the corpus belonging to the highest scoring class of the classifier?s output.
The level of confidence was ignored at this stage.
This resulted in a SPORTS corpus comprising about 11M words and a FINANCE
corpus of about 27M words. The predominant sense finding algorithm was run on the
raw text of these two corpora and we followed exactly the same evaluation strategy as
in the previous section. The results are summarized in Table 17 and are very similar
to those based on hand-labeled corpora. Again, the best results are obtained when test
and training data are derived from the same domain. The FINANCE?FINANCE result
is slightly worse, but is still well above both Random and the SemCor baseline. The
SPORTS?SPORTS result has slightly improved over the result reported in the previous
30 TwentyOne Classifier is an Irion Technologies product: www.irion.ml/products/english/
products classify.html.
585
Computational Linguistics Volume 33, Number 4
section. The reason for these differences may well be because the FINANCE corpus used
for this experiment is smaller and the SPORTS corpus is slightly larger than those used in
the hand-labeled experiment.
Automatically classifying documents inherently introduces noise in the training
corpora. This experiment to test the robustness of our method for finding predominant
senses suggests that it deals well with the noise. Further experiments that take the
confidence levels of the classifier into account will allow us to create corpora with less
noise and will allow us to find the right balance between corpus size and corpus quality.
7. Conclusions
In this article we have argued that information on the predominant sense of words is
important, and that it is desirable to be able to infer this automatically from unlabeled
text. We presented a number of evaluations investigating various facets of a previously
proposed method for automatically acquiring this information (McCarthy et al 2004a).
The evaluations extend ones in previous publications in a number of ways: they use
larger, balanced test data sets, and they compare alternative semantic similarity scores
and distributional thesauruses derived from different corpora and based on different
kinds of relations. We also looked in detail at areas where the method performs well
and also where it does not, and carried out a manual error analysis to identify the types
of mistakes it makes.
Our main results are:
 The predominant sense acquisition method produces promising results
overall for all open class parts of speech, when evaluated on SemCor, a
large balanced corpus.
 The highest accuracies are for nouns and adjectives; overall accuracy for
verbs is lower, but they have the lowest random baseline; adverbs have the
lowest average polysemy but gains over the random baseline are lower
than for other PoS.
 Using a thesaurus computed from proximity-based relations produces
almost as good results as using an otherwise identical one computed from
syntactic dependency-based relations.
 Lesk?s semantic similarity score (Banerjee and Pedersen 2002, lesk)
produces particularly good results for nouns which have low corpus
frequencies; Jiang and Conrath?s (1997, jcn) score does well on higher
frequency words.31
 For low frequency nouns in SemCor, the method, using any combination
of automatically acquired thesaurus and semantic similarity score that we
tried, produces more accurate predominant sense information than
SemCor. In particular, for nouns with a frequency of five or less (12.9% of
the polysemous nouns in the Senseval-2 data) it outperforms the SemCor
first sense heuristic. As the threshold is increased, the SemCor first sense
31 The lesk score has wider applicability than jcn since it can be applied to all parts of speech. It can also be
used with any sense inventory which has textual definitions for its senses even if the inventory does not
contain WordNet-like semantic relations.
586
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
heuristic becomes more competitive, but some of the automatic methods
are still outperforming it for nouns occurring 20 or fewer times in SemCor.
 Nouns show a stronger tendency for domain-specific meanings than other
parts of speech, but predominant senses for verbs acquired automatically
with respect to domain-specific corpora also correlate with the appropriate
domain labeling for those senses.
 Predominant senses acquired using domain-specific corpora outperform
those from SemCor in a WSD task, for a selection of nouns, using corpora
consisting of either hand-classified or automatically-classified documents.
8. Further Work
We are continuing to work on automatic ranking of word senses for WSD. Our next step
will be to use the numeric values of sense prevalence scores to compare the skews in
the distributions of word senses across different corpora and see if this enables us to
detect automatically words for which a domain- or genre-specific ranking is warranted.
Looking at skews should also help in predicting words for which contextual WSD is
likely to be particularly powerful, for example when more than one sense is scored
as being highly prevalent. In such situations we will combine our method with an
approach to unsupervised context-based WSD which uses the collocates of the distri-
butional neighbors associated with each of the senses as contextual features.
Our error analysis shows that many errors in identifying predominant senses are
caused by the sense distinctions in WordNet being particularly fine-grained. We have
recently (Koeling and McCarthy 2007) evaluated our method on the coarse-grained
English all words task at SemEval (Navigli, Litkowski, and Hargraves 2007). We will fol-
low work on finding relationships between WordNet senses to induce coarser-grained
classes (McCarthy 2006), and on automatic induction of senses (Pantel and Lin 2002)
and adapt our method to acquire prevalence rankings for these. The granularity of the
inventory will depend on the application and we plan to apply rankings over such
inventories for WSD within the context of a task, such as lexical substitution (McCarthy
and Navigli 2007).
To date we have only applied our methods to English. We plan to apply our
approach to other languages for which sense tagged resources of the size of SemCor are
not available. Given the good results with Lin?s proximity based thesaurus we believe
our method should work even for languages which do not have high quality parsers
available.
Acknowledgments
This work was supported by the UK EPSRC
project EP/C537262 ?Ranking Word Senses
for Disambiguation: Models and
Applications,? and a UK Royal Society
Dorothy Hodgkin Fellowship to the first
author. We are grateful to Dekang Lin for
making his thesaurus data publicly available
and to Siddharth Patwardhan and Ted
Pedersen for the WordNet Similarity
Package. We thank the anonymous reviewers
for the many helpful comments and
suggestions they made.
References
Agirre, Eneko and Oier Lopez de Lacalle.
2003. Clustering WordNet word senses. In
Recent Advances in Natural Language
Processing, pages 121?130, Borovets,
Bulgaria.
Banerjee, Satanjeev and Ted Pedersen. 2002.
An adapted Lesk algorithm for word sense
disambiguation using WordNet. In
Proceedings of the Third International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-02),
pages 136?145, Mexico City.
587
Computational Linguistics Volume 33, Number 4
Briscoe, Edward and John Carroll. 2002.
Robust accurate statistical annotation of
general text. In Proceedings of the Third
International Conference on Language
Resources and Evaluation (LREC),
pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Buitelaar, Paul and Bogdan Sacaleanu.
2001. Ranking and selecting synsets by
domain relevance. In Proceedings of
WordNet and Other Lexical Resources:
Applications, Extensions and Customizations,
NAACL 2001 Workshop, pages 119?124,
Pittsburgh, PA.
Chan, Yee Seng and Hwee Tou Ng. 2005.
Word sense disambiguation with
distribution estimation. In Proceedings of
the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005),
pages 1010?1015, Edinburgh, UK.
Church, Kenneth W. and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In
Proceedings of the 27th Annual Conference of
the Association for Computational Linguistics
(ACL-89), pages 76?82, Vancouver, British
Columbia, Canada.
Ciaramita, Massimiliano and Mark Johnson.
2003. Supersense tagging of unknown
nouns in WordNet. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
pages 168?175, Sapporo, Japan.
Cotton, Scott, Phil Edmonds, Adam
Kilgarriff, and Martha Palmer. 2001.
Senseval-2. http://www.sle.sharp.
co.uk/senseval2.
Curran, James. 2005. Supersense tagging
of unknown nouns using semantic
similarity. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages
26?33, Ann Arbor, MI.
Daude?, Jordi, Lluis Padro?, and German
Rigau. 2000. Mapping WordNets using
structural information. In Proceedings of the
38th Annual Meeting of the Association for
Computational Linguistics, pages 504?511,
Hong Kong.
Fellbaum, Christiane, editor. 1998. WordNet,
An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
Francis, W. Nelson and Henry Kuc?era, 1979.
Manual of Information to Accompany a
Standard Corpus of Present-Day Edited
American English, for Use with Digital
Computers. Department of Linguistics,
Brown University, Providence, RI. Revised
and amplified ed.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. One sense per discourse.
In Proceedings of the 4th DARPA Speech and
Natural Language Workshop, pages 233?237,
Harriman, NY.
Gliozzo, Alfio, Claudio Giuliano, and
Carlo Strapparava. 2005. Domain kernels
for word sense disambiguation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 403?410, Ann Arbor, MI.
Graff, David. 2003. English Gigaword.
Linguistic Data Consortium, Philadelphia,
PA.
Harris, Zellig S. 1968. Mathematical Structures
of Languages. Wiley, New York, NY.
Hornby, A. S. 1989. Oxford Advanced Learner?s
Dictionary of Current English. Oxford
University Press, Oxford, UK.
Ide, Nancy and Yorick Wilks. 2006. Making
sense about sense. In Eneko Agirre and
Phil Edmonds, editors, Word Sense
Disambiguation, Algorithms and Applications.
Springer, Dordrecht, The Netherlands,
pages 47?73.
Jiang, Jay and David Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In 10th
International Conference on Research in
Computational Linguistics, pages 19?33,
Taiwan.
Kilgarriff, Adam. 1998. Gold standard
datasets for evaluating word sense
disambiguation programs. Computer
Speech and Language, 12(3):453?472.
Kilgarriff, Adam and Martha Palmer, editors.
2000. Senseval: Special Issue of the Journal
Computers and the Humanities, volume
34(1?2). Kluwer, Dordrecht, The
Netherlands.
Koeling, Rob and Diana McCarthy. 2007.
Sussx: WSD using automatically acquired
predominant senses. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 314?317,
Prague, Czech Republic.
Koeling, Rob, Diana McCarthy, and John
Carroll. 2005. Domain-specific sense
distributions and predominant sense
acquisition. In Proceedings of the Human
Language Technology Conference and
EMNLP, pages 419?426, Vancouver, British
Columbia, Canada.
Krovetz, Robert. 1998. More than one sense
per discourse. In Proceedings of the
ACL-SIGLEX Senseval Workshop.
http://www.itri.bton.ac.uk/events/
senseval/ARCHIVE/PROCEEDINGS/.
Landes, Shari, Claudia Leacock, and
Randee I. Tengi, editors. 1998. Building
588
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Semantic Concordances. The MIT Press,
Cambridge, MA.
Lapata, Mirella and Chris Brew. 2004. Verb
class disambiguation using informative
priors. Computational Linguistics,
30(1):45?75.
Leech, Geoffrey. 1992. 100 million words of
English: The British National Corpus.
Language Research, 28(1):1?13.
Lesk, Michael. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of the
ACM SIGDOC Conference, pages 24?26,
Toronto, Canada.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago and
London.
Lin, Dekang. 1997. Using syntactic
dependency as local context to resolve
word sense ambiguity. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association
for Computational Linguistics (ACL-97),
pages 64?71, Madrid, Spain.
Lin, Dekang. 1998a. Automatic retrieval and
clustering of similar words. In Proceedings
of COLING-ACL?98, pages 768?774,
Montreal, Canada.
Lin, Dekang. 1998b. Dependency-based
evaluation of MINIPAR. In Proceedings of
the Workshop on the Evaluation of Parsing
Systems, pages 48?56, Granada, Spain.
http://www.cs.ualberta.ca/~lindek/
minipar.htm.
Magnini, Bernardo and Gabriela Cavaglia`.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413?1418, Athens, Greece.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzuli, and Alfio Gliozzo. 2001.
Using domain information for word sense
disambiguation. In Proceedings of the
Senseval-2 Workshop, pages 111?114,
Toulouse, France.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Martinez, David and Eneko Agirre. 2000.
One sense per collocation and genre/topic
variations. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora, pages 207?215, Hong Kong.
McCarthy, Diana. 2006. Relating WordNet
senses for word sense disambiguation. In
Proceedings of the EACL 06 Workshop:
Making Sense of Sense: Bringing
Psycholinguistics and Computational
Linguistics Together, pages 17?24, Trento,
Italy.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004a. Finding
predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004b. Ranking WordNet
senses automatically. CSRP 569,
Department of Informatics, University of
Sussex, January.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004c. Using
automatically acquired predominant
senses for word sense disambiguation. In
Proceedings of the ACL Senseval-3 Workshop,
pages 151?154, Barcelona, Spain.
McCarthy, Diana and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical
substitution task. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 48?53,
Prague, Czech Republic.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings Senseval-3 3rd
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Barcelona, Spain.
Miller, George A., Claudia Leacock, Randee
Tengi, and Ross T. Bunker. 1993. A
semantic concordance. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 303?308, San Francisco,
CA.
Mohammad, Saif and Graeme Hirst.
2006. Determining word sense dominance
using a thesaurus. In Proceedings of
the 11th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL-2006), pages 121?128,
Trento, Italy.
Navigli, Roberto, Ken Litkowski, and
Orin Hargraves. 2007. SemEval-2007
task 7: Coarse-grained English all-words
task. In Proceedings of ACL/SIGLEX
SemEval-2007, pages 30?35, Prague,
Czech Republic.
589
Computational Linguistics Volume 33, Number 4
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13(02):137?163.
Pantel, Patrick and Dekang Lin.
2002. Discovering word senses from
text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and
Data Mining, pages 613?619, Edmonton,
Alberta, Canada.
Patwardhan, Siddharth, Satanjeev Banerjee,
and Ted Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing 2003), pages 241?257, Mexico
City, Mexico.
Patwardhan, Siddharth and Ted Pedersen.
2003. The CPAN WordNet::Similarity
Package. http://search.cpan.org/~sid/
WordNet-Similarity-0.05/.
Preiss, Judita and David Yarowsky, editors.
2001. Proceedings of Senseval-2 Second
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Toulouse, France.
Procter, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman Group Ltd., Harlow, UK.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity
in a taxonomy. In 14th International
Joint Conference on Artificial Intelligence,
pages 448?453, Montreal, Canada.
Rose, Tony G., Mary Stevenson, and Miles
Whitehead. 2002. The Reuters Corpus
volume 1?From yesterday?s news to
tomorrow?s language resources. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 827?833, Las Palmas,
Canary Islands, Spain.
Siegel, Sidney and N. John Castellan.
1988. Non-Parametric Statistics for the
Behavioral Sciences. McGraw-Hill,
New York, NY.
Snyder, Benjamin and Martha Palmer.
2004. The English all-words task.
In Proceedings of the ACL Senseval-3
Workshop, pages 41?43, Barcelona,
Spain.
Stevenson, Mark and Yorick Wilks. 2001.
The interaction of knowledge sources for
word sense disambiguation. Computational
Linguistics, 27(3):321?350.
Weeds, Julie. 2003. Measures and
Applications of Lexical Distributional
Similarity. Ph.D. thesis, Department of
Informatics, University of Sussex,
Brighton, UK.
Weeds, Julie, James Dowdall, Gerold
Schneider, Bill Keller, and David Weir.
2005. Using distributional similarity to
organise biomedical terminology.
Terminology, 11(1):107?141.
Weeds, Julie and David Weir. 2005.
Co-occurrence Retrieval: A flexible
framework for lexical distributional
similarity. Computational Linguistics,
31(4):439?476.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation
performance across diverse parameter
spaces. Natural Language Engineering,
8(4):293?310.
590
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 132?141,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus
for Cross-Domain Sentiment Classification
Danushka Bollegala
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@
iba.t.u-tokyo.ac.jp
David Weir
School of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
d.j.weir@
sussex.ac.uk
John Carroll
School of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
j.a.carroll@
sussex.ac.uk
Abstract
We describe a sentiment classification method
that is applicable when we do not have any la-
beled data for a target domain but have some
labeled data for multiple other domains, des-
ignated as the source domains. We automat-
ically create a sentiment sensitive thesaurus
using both labeled and unlabeled data from
multiple source domains to find the associa-
tion between words that express similar senti-
ments in different domains. The created the-
saurus is then used to expand feature vectors
to train a binary classifier. Unlike previous
cross-domain sentiment classification meth-
ods, our method can efficiently learn from
multiple source domains. Our method signif-
icantly outperforms numerous baselines and
returns results that are better than or com-
parable to previous cross-domain sentiment
classification methods on a benchmark dataset
containing Amazon user reviews for different
types of products.
1 Introduction
Users express opinions about products or services
they consume in blog posts, shopping sites, or re-
view sites. It is useful for both consumers as well
as for producers to know what general public think
about a particular product or service. Automatic
document level sentiment classification (Pang et al,
2002; Turney, 2002) is the task of classifying a given
review with respect to the sentiment expressed by
the author of the review. For example, a sentiment
classifier might classify a user review about a movie
as positive or negative depending on the sentiment
expressed in the review. Sentiment classification
has been applied in numerous tasks such as opinion
mining (Pang and Lee, 2008), opinion summariza-
tion (Lu et al, 2009), contextual advertising (Fan
and Chang, 2010), and market analysis (Hu and Liu,
2004).
Supervised learning algorithms that require la-
beled data have been successfully used to build sen-
timent classifiers for a specific domain (Pang et al,
2002). However, sentiment is expressed differently
in different domains, and it is costly to annotate
data for each new domain in which we would like
to apply a sentiment classifier. For example, in the
domain of reviews about electronics products, the
words ?durable? and ?light? are used to express pos-
itive sentiment, whereas ?expensive? and ?short bat-
tery life? often indicate negative sentiment. On the
other hand, if we consider the books domain the
words ?exciting? and ?thriller? express positive sen-
timent, whereas the words ?boring? and ?lengthy?
usually express negative sentiment. A classifier
trained on one domain might not perform well on
a different domain because it would fail to learn the
sentiment of the unseen words.
Work in cross-domain sentiment classification
(Blitzer et al, 2007) focuses on the challenge of
training a classifier from one or more domains
(source domains) and applying the trained classi-
fier in a different domain (target domain). A cross-
domain sentiment classification system must over-
come two main challenges. First, it must identify
which source domain features are related to which
target domain features. Second, it requires a learn-
ing framework to incorporate the information re-
132
garding the relatedness of source and target domain
features. Following previous work, we define cross-
domain sentiment classification as the problem of
learning a binary classifier (i.e. positive or negative
sentiment) given a small set of labeled data for the
source domain, and unlabeled data for both source
and target domains. In particular, no labeled data is
provided for the target domain.
In this paper, we describe a cross-domain senti-
ment classification method using an automatically
created sentiment sensitive thesaurus. We use la-
beled data from multiple source domains and unla-
beled data from source and target domains to rep-
resent the distribution of features. We represent a
lexical element (i.e. a unigram or a bigram of word
lemma) in a review using a feature vector. Next, for
each lexical element we measure its relatedness to
other lexical elements and group related lexical ele-
ments to create a thesaurus. The thesaurus captures
the relatedness among lexical elements that appear
in source and target domains based on the contexts
in which the lexical elements appear (their distribu-
tional context). A distinctive aspect of our approach
is that, in addition to the usual co-occurrence fea-
tures typically used in characterizing a word?s dis-
tributional context, we make use, where possible, of
the sentiment label of a document: i.e. sentiment la-
bels form part of our context features. This is what
makes the distributional thesaurus sensitive to senti-
ment. Unlabeled data is cheaper to collect compared
to labeled data and is often available in large quan-
tities. The use of unlabeled data enables us to ac-
curately estimate the distribution of words in source
and target domains. Our method can learn from a
large amount of unlabeled data to leverage a robust
cross-domain sentiment classifier.
We model the cross-domain sentiment classifica-
tion problem as one of feature expansion, where we
append additional related features to feature vectors
that represent source and target domain reviews in
order to reduce the mismatch of features between the
two domains. Methods that use related features have
been successfully used in numerous tasks such as
query expansion (Fang, 2008), and document classi-
fication (Shen et al, 2009). However, feature expan-
sion techniques have not previously been applied to
the task of cross-domain sentiment classification.
In our method, we use the automatically created
thesaurus to expand feature vectors in a binary clas-
sifier at train and test times by introducing related
lexical elements from the thesaurus. We use L1 reg-
ularized logistic regression as the classification al-
gorithm. (However, the method is agnostic to the
properties of the classifier and can be used to expand
feature vectors for any binary classifier). L1 regular-
ization enables us to select a small subset of features
for the classifier. Unlike previous work which at-
tempts to learn a cross-domain classifier using a sin-
gle source domain, we leverage data from multiple
source domains to learn a robust classifier that gen-
eralizes across multiple domains. Our contributions
can be summarized as follows.
? We describe a fully automatic method to create
a thesaurus that is sensitive to the sentiment of
words expressed in different domains.
? We describe a method to use the created the-
saurus to expand feature vectors at train and test
times in a binary classifier.
2 A Motivating Example
To explain the problem of cross-domain sentiment
classification, consider the reviews shown in Ta-
ble 1 for the domains books and kitchen appliances.
Table 1 shows two positive and one negative re-
view from each domain. We have emphasized in
boldface the words that express the sentiment of
the authors of the reviews. We see that the words
excellent, broad, high quality, interesting, and
well researched are used to express positive senti-
ment in the books domain, whereas the word disap-
pointed indicates negative sentiment. On the other
hand, in the kitchen appliances domain the words
thrilled, high quality, professional, energy sav-
ing, lean, and delicious express positive sentiment,
whereas the words rust and disappointed express
negative sentiment. Although high quality would
express positive sentiment in both domains, and dis-
appointed negative sentiment, it is unlikely that we
would encounter well researched in kitchen appli-
ances reviews, or rust or delicious in book reviews.
Therefore, a model that is trained only using book
reviews might not have any weights learnt for deli-
cious or rust, which would make it difficult for this
model to accurately classify reviews of kitchen ap-
pliances.
133
books kitchen appliances
+ Excellent and broad survey of the development of
civilization with all the punch of high quality fiction.
I was so thrilled when I unpack my processor. It is
so high quality and professional in both looks and
performance.
+ This is an interesting and well researched book. Energy saving grill. My husband loves the burgers
that I make from this grill. They are lean and deli-
cious.
- Whenever a new book by Philippa Gregory comes
out, I buy it hoping to have the same experience, and
lately have been sorely disappointed.
These knives are already showing spots of rust de-
spite washing by hand and drying. Very disap-
pointed.
Table 1: Positive (+) and negative (-) sentiment reviews in two different domains.
sentence Excellent and broad survey of
the development of civilization.
POS tags Excellent/JJ and/CC broad/JJ
survey/NN1 of/IO the/AT
development/NN1 of/IO civi-
lization/NN1
lexical elements
(unigrams)
excellent, broad, survey, devel-
opment, civilization
lexical elements
(bigrams)
excellent+broad, broad+survey,
survey+development, develop-
ment+civilization
sentiment fea-
tures (lemma)
excellent*P, broad*P, sur-
vey*P, excellent+broad*P,
broad+survey*P
sentiment fea-
tures (POS)
JJ*P, NN1*P, JJ+NN1*P
Table 2: Generating lexical elements and sentiment fea-
tures from a positive review sentence.
3 Sentiment Sensitive Thesaurus
One solution to the feature mismatch problem out-
lined above is to use a thesaurus that groups differ-
ent words that express the same sentiment. For ex-
ample, if we know that both excellent and delicious
are positive sentiment words, then we can use this
knowledge to expand a feature vector that contains
the word delicious using the word excellent, thereby
reducing the mismatch between features in a test in-
stance and a trained model. Below we describe a
method to construct a sentiment sensitive thesaurus
for feature expansion.
Given a labeled or an unlabeled review, we first
split the review into individual sentences. We carry
out part-of-speech (POS) tagging and lemmatiza-
tion on each review sentence using the RASP sys-
tem (Briscoe et al, 2006). Lemmatization reduces
the data sparseness and has been shown to be effec-
tive in text classification tasks (Joachims, 1998). We
then apply a simple word filter based on POS tags to
select content words (nouns, verbs, adjectives, and
adverbs). In particular, previous work has identified
adjectives as good indicators of sentiment (Hatzi-
vassiloglou and McKeown, 1997; Wiebe, 2000).
Following previous work in cross-domain sentiment
classification, we model a review as a bag of words.
We select unigrams and bigrams from each sentence.
For the remainder of this paper, we will refer to un-
igrams and bigrams collectively as lexical elements.
Previous work on sentiment classification has shown
that both unigrams and bigrams are useful for train-
ing a sentiment classifier (Blitzer et al, 2007). We
note that it is possible to create lexical elements both
from source domain labeled reviews as well as from
unlabeled reviews in source and target domains.
Next, we represent each lexical element u using a
set of features as follows. First, we select other lex-
ical elements that co-occur with u in a review sen-
tence as features. Second, from each source domain
labeled review sentence in which u occurs, we cre-
ate sentiment features by appending the label of the
review to each lexical element we generate from that
review. For example, consider the sentence selected
from a positive review of a book shown in Table 2.
In Table 2, we use the notation ?*P? to indicate posi-
tive sentiment features and ?*N? to indicate negative
sentiment features. The example sentence shown in
Table 2 is selected from a positively labeled review,
and generates positive sentiment features as shown
in Table 2. In addition to word-level sentiment fea-
tures, we replace words with their POS tags to create
134
POS-level sentiment features. POS tags generalize
the word-level sentiment features, thereby reducing
feature sparseness.
Let us denote the value of a feature w in the fea-
ture vector u representing a lexical element u by
f(u, w). The vector u can be seen as a compact rep-
resentation of the distribution of a lexical element u
over the set of features that co-occur with u in the re-
views. From the construction of the feature vector u
described in the previous paragraph, it follows that
w can be either a sentiment feature or another lexical
element that co-occurs with u in some review sen-
tence. The distributional hypothesis (Harris, 1954)
states that words that have similar distributions are
semantically similar. We compute f(u, w) as the
pointwise mutual information between a lexical ele-
ment u and a feature w as follows:
f(u, w) = log
(
c(u,w)
N
?n
i=1 c(i,w)
N ?
?m
j=1 c(u,j)
N
)
(1)
Here, c(u,w) denotes the number of review sen-
tences in which a lexical element u and a feature
w co-occur, n and m respectively denote the total
number of lexical elements and the total number of
features, and N =
?n
i=1
?m
j=1 c(i, j). Pointwise
mutual information is known to be biased towards
infrequent elements and features. We follow the dis-
counting approach of Pantel & Ravichandran (2004)
to overcome this bias.
Next, for two lexical elements u and v (repre-
sented by feature vectors u and v, respectively), we
compute the relatedness ?(v, u) of the feature v to
the feature u as follows,
?(v, u) =
?
w?{x|f(v,x)>0} f(u, w)
?
w?{x|f(u,x)>0} f(u, w)
. (2)
Here, we use the set notation {x|f(v, x) > 0} to
denote the set of features that co-occur with v. Re-
latedness of a lexical element u to another lexical
element v is the fraction of feature weights in the
feature vector for the element u that also co-occur
with the features in the feature vector for the ele-
ment v. If there are no features that co-occur with
both u and v, then the relatedness reaches its min-
imum value of 0. On the other hand if all features
that co-occur with u also co-occur with v, then the
relatedness , ?(v, u), reaches its maximum value of
1. Note that relatedness is an asymmetric measure
by the definition given in Equation 2, and the relat-
edness ?(v, u) of an element v to another element u
is not necessarily equal to ?(u, v), the relatedness of
u to v.
We use the relatedness measure defined in Equa-
tion 2 to construct a sentiment sensitive thesaurus in
which, for each lexical element u we list lexical el-
ements v that co-occur with u (i.e. f(u, v) > 0) in
descending order of relatedness values ?(v, u). In
the remainder of the paper, we use the term base en-
try to refer to a lexical element u for which its related
lexical elements v (referred to as the neighbors of u)
are listed in the thesaurus. Note that relatedness val-
ues computed according to Equation 2 are sensitive
to sentiment labels assigned to reviews in the source
domain, because co-occurrences are computed over
both lexical and sentiment elements extracted from
reviews. In other words, the relatedness of an ele-
ment u to another element v depends upon the sen-
timent labels assigned to the reviews that generate u
and v. This is an important fact that differentiates
our sentiment-sensitive thesaurus from other distri-
butional thesauri which do not consider sentiment
information.
Moreover, we only need to retain lexical elements
in the sentiment sensitive thesaurus because when
predicting the sentiment label for target reviews (at
test time) we cannot generate sentiment elements
from those (unlabeled) reviews, therefore we are
not required to find expansion candidates for senti-
ment elements. However, we emphasize the fact that
the relatedness values between the lexical elements
listed in the sentiment-sensitive thesaurus are com-
puted using co-occurrences with both lexical and
sentiment features, and therefore the expansion can-
didates selected for the lexical elements in the tar-
get domain reviews are sensitive to sentiment labels
assigned to reviews in the source domain. Using
a sparse matrix format and approximate similarity
matching techniques (Sarawagi and Kirpal, 2004),
we can efficiently create a thesaurus from a large set
of reviews.
4 Feature Expansion
Our feature expansion phase augments a feature vec-
tor with additional related features selected from the
135
sentiment-sensitive thesaurus created in Section 3 to
overcome the feature mismatch problem. First, fol-
lowing the bag-of-words model, we model a review
d using the set {w1, . . . , wN}, where the elements
wi are either unigrams or bigrams that appear in the
review d. We then represent a review d by a real-
valued term-frequency vector d ? RN , where the
value of the j-th element dj is set to the total number
of occurrences of the unigram or bigram wj in the
review d. To find the suitable candidates to expand a
vector d for the review d, we define a ranking score
score(ui,d) for each base entry in the thesaurus as
follows:
score(ui,d) =
?N
j=1 dj?(wj , ui)
?N
l=1 dl
(3)
According to this definition, given a review d, a base
entry ui will have a high ranking score if there are
many words wj in the review d that are also listed
as neighbors for the base entry ui in the sentiment-
sensitive thesaurus. Moreover, we weight the re-
latedness scores for each word wj by its normal-
ized term-frequency to emphasize the salient uni-
grams and bigrams in a review. Recall that related-
ness is defined as an asymmetric measure in Equa-
tion 2, and we use ?(wj , ui) in the computation of
score(ui,d) in Equation 3. This is particularly im-
portant because we would like to score base entries
ui considering all the unigrams and bigrams that ap-
pear in a review d, instead of considering each uni-
gram or bigram individually.
To expand a vector, d, for a review d, we first
rank the base entries, ui using the ranking score
in Equation 3 and select the top k ranked base en-
tries. Let us denote the r-th ranked (1 ? r ? k)
base entry for a review d by vrd. We then extend the
original set of unigrams and bigrams {w1, . . . , wN}
by the base entries v1d, . . . , v
k
d to create a new vec-
tor d? ? R(N+k) with dimensions corresponding to
w1, . . . , wN , v1d, . . . , v
k
d for a review d. The values
of the extended vector d? are set as follows. The
values of the first N dimensions that correspond to
unigrams and bigrams wi that occur in the review d
are set to di, their frequency in d. The subsequent k
dimensions that correspond to the top ranked based
entries for the review d are weighted according to
their ranking score. Specifically, we set the value of
the r-th ranked base entry vrd to 1/r. Alternatively,
one could use the ranking score, score(vrd, d), itself
as the value of the appended base entries. However,
both relatedness scores as well as normalized term-
frequencies can be small in practice, which leads to
very small absolute ranking scores. By using the
inverse rank, we only take into account the rela-
tive ranking of base entries and ignore their absolute
scores.
Note that the score of a base entry depends on a
review d. Therefore, we select different base en-
tries as additional features for expanding different
reviews. Furthermore, we do not expand each wi
individually when expanding a vector d for a re-
view. Instead, we consider all unigrams and bi-
grams in d when selecting the base entries for ex-
pansion. One can think of the feature expansion pro-
cess as a lower dimensional latent mapping of fea-
tures onto the space spanned by the base entries in
the sentiment-sensitive thesaurus. The asymmetric
property of the relatedness (Equation 2) implicitly
prefers common words that co-occur with numerous
other words as expansion candidates. Such words
act as domain independent pivots and enable us to
transfer the information regarding sentiment from
one domain to another.
Using the extended vectors d? to represent re-
views, we train a binary classifier from the source
domain labeled reviews to predict positive and neg-
ative sentiment in reviews. We differentiate the ap-
pended base entries vrd from wi that existed in the
original vector d (prior to expansion) by assigning
different feature identifiers to the appended base en-
tries. For example, a unigram excellent in a feature
vector is differentiated from the base entry excellent
by assigning the feature id, ?BASE=excellent? to the
latter. This enables us to learn different weights for
base entries depending on whether they are useful
for expanding a feature vector. We use L1 regu-
larized logistic regression as the classification algo-
rithm (Ng, 2004), which produces a sparse model in
which most irrelevant features are assigned a zero
weight. This enables us to select useful features for
classification in a systematic way without having to
preselect features using heuristic approaches. The
regularization parameter is set to its default value
of 1 for all the experiments described in this paper.
136
5 Experiments
5.1 Dataset
To evaluate our method we use the cross-domain
sentiment classification dataset prepared by Blitzer
et al (2007). This dataset consists of Amazon prod-
uct reviews for four different product types: books
(B), DVDs (D), electronics (E) and kitchen appli-
ances (K). There are 1000 positive and 1000 neg-
ative labeled reviews for each domain. Moreover,
the dataset contains some unlabeled reviews (on av-
erage 17, 547) for each domain. This benchmark
dataset has been used in much previous work on
cross-domain sentiment classification and by eval-
uating on it we can directly compare our method
against existing approaches.
Following previous work, we randomly select 800
positive and 800 negative labeled reviews from each
domain as training instances (i.e. 1600?4 = 6400);
the remainder is used for testing (i.e. 400 ? 4 =
1600). In our experiments, we select each domain in
turn as the target domain, with one or more other do-
mains as sources. Note that when we combine more
than one source domain we limit the total number
of source domain labeled reviews to 1600, balanced
between the domains. For example, if we combine
two source domains, then we select 400 positive and
400 negative labeled reviews from each domain giv-
ing (400 + 400) ? 2 = 1600. This enables us to
perform a fair evaluation when combining multiple
source domains. The evaluation metric is classifica-
tion accuracy on a target domain, computed as the
percentage of correctly classified target domain re-
views out of the total number of reviews in the target
domain.
5.2 Effect of Feature Expansion
To study the effect of feature expansion at train time
compared to test time, we used Amazon reviews for
two further domains, music and video, which were
also collected by Blitzer et al (2007) but are not
part of the benchmark dataset. Each validation do-
main has 1000 positive and 1000 negative labeled
reviews, and 15000 unlabeled reviews. Using the
validation domains as targets, we vary the number
of top k ranked base entries (Equation 3) used for
feature expansion during training (Traink) and test-
ing (Testk), and measure the average classification
0 200 400 600 800 10000
200
400
600
800
1000  
Traink 
Test k
0.776
0.778
0.78
0.782
0.784
0.786
Figure 1: Feature expansion at train vs. test times.
B D K B+D B+K D+K B+D+K50
55
60
65
70
75
80
85
Source Domains
Accu
racy 
on el
ectro
nics d
omai
n
Figure 2: Effect of using multiple source domains.
accuracy. Figure 1 illustrates the results using a heat
map, where dark colors indicate low accuracy val-
ues and light colors indicate high accuracy values.
We see that expanding features only at test time (the
left-most column) does not work well because we
have not learned proper weights for the additional
features. Similarly, expanding features only at train
time (the bottom-most row) also does not perform
well because the expanded features are not used dur-
ing testing. The maximum classification accuracy is
obtained when Testk = 400 and Traink = 800, and
we use these values for the remainder of the experi-
ments described in the paper.
5.3 Combining Multiple Sources
Figure 2 shows the effect of combining multiple
source domains to build a sentiment classifier for
the electronics domain. We see that the kitchen do-
main is the single best source domain when adapt-
ing to the electronics target domain. This behavior
137
0 200 400 600 80040
4550
5560
6570
7580
85
Positive/Negative instances
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 3: Effect of source domain labeled data.
0 0.2 0.4 0.6 0.8 150
55
60
65
70
Source unlabeled dataset size
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 4: Effect of source domain unlabeled data.
is explained by the fact that in general kitchen appli-
ances and electronic items have similar aspects. But
a more interesting observation is that the accuracy
that we obtain when we use two source domains is
always greater than the accuracy if we use those do-
mains individually. The highest accuracy is achieved
when we use all three source domains. Although
not shown here for space limitations, we observed
similar trends with other domains in the benchmark
dataset.
To investigate the impact of the quantity of source
domain labeled data on our method, we vary the
amount of data from zero to 800 reviews, with equal
amounts of positive and negative labeled data. Fig-
ure 3 shows the accuracy with the DVD domain as
the target. Note that source domain labeled data is
used both to create the sentiment sensitive thesaurus
as well as to train the sentiment classifier. When
there are multiple source domains we limit and bal-
ance the number of labeled instances as outlined in
Section 5.1. The amount of unlabeled data is held
constant, so that any change in classification accu-
0 0.2 0.4 0.6 0.8 150
55
60
65
70
Target unlabeled dataset size
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 5: Effect of target domain unlabeled data.
racy is directly attributable to the source domain la-
beled instances. Because this is a binary classifica-
tion task (i.e. positive vs. negative sentiment), a ran-
dom classifier that does not utilize any labeled data
would report a 50% classification accuracy. From
Figure 3, we see that when we increase the amount
of source domain labeled data the accuracy increases
quickly. In fact, by selecting only 400 (i.e. 50% of
the total 800) labeled instances per class, we achieve
the maximum performance in most of the cases.
To study the effect of source and target domain
unlabeled data on the performance of our method,
we create sentiment sensitive thesauri using differ-
ent proportions of unlabeled data. The amount of
labeled data is held constant and is balanced across
multiple domains as outlined in Section 5.1, so any
changes in classification accuracy can be directly at-
tributed to the contribution of unlabeled data. Figure
4 shows classification accuracy on the DVD target
domain when we vary the proportion of source do-
main unlabeled data (target domain?s unlabeled data
is fixed).
Likewise, Figure 5 shows the classification ac-
curacy on the DVD target domain when we vary
the proportion of the target domain?s unlabeled data
(source domains? unlabeled data is fixed). From Fig-
ures 4 and 5, we see that irrespective of the amount
being used, there is a clear performance gain when
we use unlabeled data from multiple source domains
compared to using a single source domain. How-
ever, we could not observe a clear gain in perfor-
mance when we increase the amount of the unla-
beled data used to create the sentiment sensitive the-
saurus.
138
Method K D E B
No Thesaurus 72.61 68.97 70.53 62.72
SCL 80.83 74.56 78.43 72.76
SCL-MI 82.06 76.30 78.93 74.56
SFA 81.48 76.31 75.30 77.73
LSA 79.00 73.50 77.66 70.83
FALSA 80.83 76.33 77.33 73.33
NSS 77.50 73.50 75.50 71.46
Proposed 85.18 78.77 83.63 76.32
Within-Domain 87.70 82.40 84.40 80.40
Table 3: Cross-domain sentiment classification accuracy.
5.4 Cross-Domain Sentiment Classification
Table 3 compares our method against a number of
baselines and previous cross-domain sentiment clas-
sification techniques using the benchmark dataset.
For all previous techniques we give the results re-
ported in the original papers. The No Thesaurus
baseline simulates the effect of not performing any
feature expansion. We simply train a binary clas-
sifier using unigrams and bigrams as features from
the labeled reviews in the source domains and ap-
ply the trained classifier on the target domain. This
can be considered to be a lower bound that does
not perform domain adaptation. SCL is the struc-
tural correspondence learning technique of Blitzer
et al (2006). In SCL-MI, features are selected us-
ing the mutual information between a feature (uni-
gram or bigram) and a domain label. After selecting
salient features, the SCL algorithm is used to train a
binary classifier. SFA is the spectral feature align-
ment technique of Pan et al (2010). Both the LSA
and FALSA techniques are based on latent semantic
analysis (Pan et al, 2010). For the Within-Domain
baseline, we train a binary classifier using the la-
beled data from the target domain. This upper base-
line represents the classification accuracy we could
hope to obtain if we were to have labeled data for the
target domain. Note that this is not a cross-domain
classification setting. To evaluate the benefit of us-
ing sentiment features on our method, we give a NSS
(non-sentiment sensitive) baseline in which we cre-
ate a thesaurus without using any sentiment features.
Proposed is our method.
From Table 3, we see that our proposed method
returns the best cross-domain sentiment classifica-
tion accuracy (shown in boldface) for the three do-
mains kitchen appliances, DVDs, and electronics.
For the books domain, the best results are returned
by SFA. The books domain has the lowest number
of unlabeled reviews (around 5000) in the dataset.
Because our method relies upon the availability of
unlabeled data for the construction of a sentiment
sensitive thesaurus, we believe that this accounts for
our lack of performance on the books domain. How-
ever, given that it is much cheaper to obtain unla-
beled than labeled data for a target domain, there is
strong potential for improving the performance of
our method in this domain. The analysis of vari-
ance (ANOVA) and Tukey?s honestly significant dif-
ferences (HSD) tests on the classification accuracies
for the four domains show that our method is sta-
tistically significantly better than both the No The-
saurus and NSS baselines, at confidence level 0.05.
We therefore conclude that using the sentiment sen-
sitive thesaurus for feature expansion is useful for
cross-domain sentiment classification. The results
returned by our method are comparable to state-of-
the-art techniques such as SCL-MI and SFA. In par-
ticular, the differences between those techniques and
our method are not statistically significant.
6 Related Work
Compared to single-domain sentiment classifica-
tion, which has been studied extensively in previous
work (Pang and Lee, 2008; Turney, 2002), cross-
domain sentiment classification has only recently re-
ceived attention in response to advances in the area
of domain adaptation. Aue and Gammon (2005) re-
port a number of empirical tests into domain adap-
tation of sentiment classifiers using an ensemble of
classifiers. However, most of these tests were un-
able to outperform a simple baseline classifier that
is trained using all labeled data for all domains.
Blitzer et al (2007) apply the structural corre-
spondence learning (SCL) algorithm to train a cross-
domain sentiment classifier. They first chooses a set
of pivot features using pointwise mutual informa-
tion between a feature and a domain label. Next,
linear predictors are learnt to predict the occur-
rences of those pivots. Finally, they use singular
value decomposition (SVD) to construct a lower-
dimensional feature space in which a binary classi-
139
fier is trained. The selection of pivots is vital to the
performance of SCL and heuristically selected pivot
features might not guarantee the best performance
on target domains. In contrast, our method uses all
features when creating the thesaurus and selects a
subset of features during training using L1 regular-
ization. Moreover, we do not require SVD, which
has cubic time complexity so can be computation-
ally expensive for large datasets.
Pan et al (2010) use structural feature alignment
(SFA) to find an alignment between domain spe-
cific and domain independent features. The mu-
tual information of a feature with domain labels is
used to classify domain specific and domain inde-
pendent features. Next, spectral clustering is per-
formed on a bipartite graph that represents the re-
lationship between the two sets of features. Fi-
nally, the top eigenvectors are selected to construct
a lower-dimensional projection. However, not all
words can be cleanly classified into domain spe-
cific or domain independent, and this process is con-
ducted prior to training a classifier. In contrast, our
method lets a particular lexical entry to be listed as
a neighour for multiple base entries. Moreover, we
expand each feature vector individually and do not
require any clustering. Furthermore, unlike SCL and
SFA, which consider a single source domain, our
method can efficiently adapt from multiple source
domains.
7 Conclusions
We have described and evaluated a method to
construct a sentiment-sensitive thesaurus to bridge
the gap between source and target domains in
cross-domain sentiment classification using multi-
ple source domains. Experimental results using a
benchmark dataset for cross-domain sentiment clas-
sification show that our proposed method can im-
prove classification accuracy in a sentiment classi-
fier. In future, we intend to apply the proposed
method to other domain adaptation tasks.
Acknowledgements
This research was conducted while the first author
was a visiting research fellow at Sussex university
under the postdoctoral fellowship of the Japan Soci-
ety for the Promotion of Science (JSPS).
References
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
Technical report, Microsoft Research.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP 2006.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
ACL 2007, pages 440?447.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In COL-
ING/ACL 2006 Interactive Presentation Sessions.
Teng-Kai Fan and Chia-Hui Chang. 2010. Sentiment-
oriented contextual advertising. Knowledge and Infor-
mation Systems, 23(3):321?344.
Hui Fang. 2008. A re-examination of query expansion
using lexical resources. In ACL 2008, pages 139?147.
Z. Harris. 1954. Distributional structure. Word, 10:146?
162.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In ACL 1997, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD 2004, pages 168?177.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In ECML 1998, pages 137?142.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009.
Rated aspect summarization of short comments. In
WWW 2009, pages 131?140.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In ICML 2004.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
WWW 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In EMNLP 2002, pages 79?
86.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In NAACL-
HLT?04, pages 321 ? 328.
Sunita Sarawagi and Alok Kirpal. 2004. Efficient set
joins on similarity predicates. In SIGMOD ?04, pages
743?754.
140
Dou Shen, Jianmin Wu, Bin Cao, Jian-Tao Sun, Qiang
Yang, Zheng Chen, and Ying Li. 2009. Exploit-
ing term relationship to boost text classification. In
CIKM?09, pages 1637 ? 1640.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL 2002, pages 417?424.
Janyce M. Wiebe. 2000. Learning subjective adjective
from corpora. In AAAI 2000, pages 735?740.
141
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 613?623,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning to Predict Distributions of Words Across Domains
Danushka Bollegala
Department of Computer Science
University of Liverpool
Liverpool,
L69 3BX, UK
danushka.bollegala@
liverpool.ac.uk
David Weir
Department of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
d.j.weir@
sussex.ac.uk
John Carroll
Department of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
j.a.carroll@
sussex.ac.uk
Abstract
Although the distributional hypothesis has
been applied successfully in many natural
language processing tasks, systems using
distributional information have been lim-
ited to a single domain because the dis-
tribution of a word can vary between do-
mains as the word?s predominant mean-
ing changes. However, if it were pos-
sible to predict how the distribution of
a word changes from one domain to an-
other, the predictions could be used to
adapt a system trained in one domain to
work in another. We propose an unsuper-
vised method to predict the distribution of
a word in one domain, given its distribu-
tion in another domain. We evaluate our
method on two tasks: cross-domain part-
of-speech tagging and cross-domain sen-
timent classification. In both tasks, our
method significantly outperforms compet-
itive baselines and returns results that are
statistically comparable to current state-
of-the-art methods, while requiring no
task-specific customisations.
1 Introduction
The Distributional Hypothesis, summarised by the
memorable line of Firth (1957) ? You shall know
a word by the company it keeps ? has inspired a
diverse range of research in natural language pro-
cessing. In such work, a word is represented by
the distribution of other words that co-occur with
it. Distributional representations of words have
been successfully used in many language process-
ing tasks such as entity set expansion (Pantel et al,
2009), part-of-speech (POS) tagging and chunk-
ing (Huang and Yates, 2009), ontology learning
(Curran, 2005), computing semantic textual sim-
ilarity (Besanc?on et al, 1999), and lexical infer-
ence (Kotlerman et al, 2012).
However, the distribution of a word often varies
from one domain
1
to another. For example, in
the domain of portable computer reviews the word
lightweight is often associated with positive sen-
timent bearing words such as sleek or compact,
whereas in the movie review domain the same
word is often associated with negative sentiment-
bearing words such as superficial or formulaic.
Consequently, the distributional representations of
the word lightweight will differ considerably be-
tween the two domains. In this paper, given the
distribution w
S
of a word w in the source domain
S, we propose an unsupervised method for pre-
dicting its distributionw
T
in a different target do-
main T .
The ability to predict how the distribution of a
word varies from one domain to another is vital
for numerous adaptation tasks. For example, un-
supervised cross-domain sentiment classification
(Blitzer et al, 2007; Aue and Gamon, 2005) in-
volves using sentiment-labeled user reviews from
the source domain, and unlabeled reviews from
both the source and the target domains to learn
a sentiment classifier for the target domain. Do-
main adaptation (DA) of sentiment classification
becomes extremely challenging when the distribu-
tions of words in the source and the target domains
are very different, because the features learnt from
the source domain labeled reviews might not ap-
pear in the target domain reviews that must be
classified. By predicting the distribution of a word
across different domains, we can find source do-
main features that are similar to the features in
target domain reviews, thereby reducing the mis-
match of features between the two domains.
We propose a two-step unsupervised approach
to predict the distribution of a word across do-
mains. First, we create two lower dimensional la-
1
In this paper, we use the term domain to refer to a col-
lection of documents about a particular topic, for example
reviews of a particular kind of product.
613
tent feature spaces separately for the source and
the target domains using Singular Value Decom-
position (SVD). Second, we learn a mapping from
the source domain latent feature space to the tar-
get domain latent feature space using Partial Least
Square Regression (PLSR). The SVD smoothing
in the first step both reduces the data sparseness in
distributional representations of individual words,
as well as the dimensionality of the feature space,
thereby enabling us to efficiently and accurately
learn a prediction model using PLSR in the sec-
ond step. Our proposed cross-domain word dis-
tribution prediction method is unsupervised in the
sense that it does not require any labeled data in
either of the two steps.
Using two popular multi-domain datasets, we
evaluate the proposed method in two prediction
tasks: (a) predicting the POS of a word in a tar-
get domain, and (b) predicting the sentiment of a
review in a target domain. Without requiring any
task specific customisations, systems based on our
distribution prediction method significantly out-
perform competitive baselines in both tasks. Be-
cause our proposed distribution prediction method
is unsupervised and task independent, it is poten-
tially useful for a wide range of DA tasks such en-
tity extraction (Guo et al, 2009) or dependency
parsing (McClosky et al, 2010). Our contribu-
tions are summarised as follows:
? Given the distribution w
S
of a word w in a
source domain S, we propose a method for
learning its distribution w
T
in a target do-
main T .
? Using the learnt distribution prediction
model, we propose a method to learn a cross-
domain POS tagger.
? Using the learnt distribution prediction
model, we propose a method to learn a cross-
domain sentiment classifier.
To our knowledge, ours is the first successful at-
tempt to learn a model that predicts the distribu-
tion of a word across different domains.
2 Related Work
Learning semantic representations for words us-
ing documents from a single domain has received
much attention lately (Vincent et al, 2010; Socher
et al, 2013; Baroni and Lenci, 2010). As we have
already discussed, the semantics of a word varies
across different domains, and such variations are
not captured by models that only learn a single se-
mantic representation for a word using documents
from a single domain.
The POS of a word is influenced both by its
context (contextual bias), and the domain of the
document in which it appears (lexical bias). For
example, the word signal is predominately used
as a noun in MEDLINE, whereas it appears pre-
dominantly as an adjective in the Wall Street Jour-
nal (WSJ) (Blitzer et al, 2006). Consequently, a
tagger trained on WSJ would incorrectly tag sig-
nal in MEDLINE. Blitzer et al (2006) append
the source domain labeled data with predicted piv-
ots (i.e. words that appear in both the source and
target domains) to adapt a POS tagger to a tar-
get domain. Choi and Palmer (2012) propose
a cross-domain POS tagging method by training
two separate models: a generalised model and a
domain-specific model. At tagging time, a sen-
tence is tagged by the model that is most similar
to that sentence. Huang and Yates (2009) train a
Conditional Random Field (CRF) tagger with fea-
tures retrieved from a smoothing model trained us-
ing both source and target domain unlabeled data.
Adding latent states to the smoothing model fur-
ther improves the POS tagging accuracy (Huang
and Yates, 2012). Schnabel and Sch?utze (2013)
propose a training set filtering method where they
eliminate shorter words from the training data
based on the intuition that longer words are more
likely to be examples of productive linguistic pro-
cesses than shorter words.
The sentiment of a word can vary from one do-
main to another. In Structural Correspondence
Learning (SCL) (Blitzer et al, 2006; Blitzer et
al., 2007), a set of pivots are chosen using point-
wise mutual information. Linear predictors are
then learnt to predict the occurrence of those piv-
ots, and SVD is used to construct a lower dimen-
sional representation in which a binary classifier
is trained. Spectral Feature Alignment (SFA) (Pan
et al, 2010) also uses pivots to compute an align-
ment between domain specific and domain inde-
pendent features. Spectral clustering is performed
on a bipartite graph representing domain specific
and domain independent features to find a lower-
dimensional projection between the two sets of
features. The cross-domain sentiment-sensitive
thesaurus (SST) (Bollegala et al, 2011) groups
together words that express similar sentiments in
614
different domains. The created thesaurus is used to
expand feature vectors during train and test stages
in a binary classifier. However, unlike our method,
SCL, SFA, or SST do not learn a prediction model
between word distributions across domains.
Prior knowledge of the sentiment of words, such
as sentiment lexicons, has been incorporated into
cross-domain sentiment classification. He et al
(2011) propose a joint sentiment-topic model that
imposes a sentiment-prior depending on the oc-
currence of a word in a sentiment lexicon. Pono-
mareva and Thelwall (2012) represent source and
target domain reviews as nodes in a graph and ap-
ply a label propagation algorithm to predict the
sentiment labels for target domain reviews from
the sentiment labels in source domain reviews. A
sentiment lexicon is used to create features for a
document. Although incorporation of prior senti-
ment knowledge is a promising technique to im-
prove accuracy in cross-domain sentiment classi-
fication, it is complementary to our task of distri-
bution prediction across domains.
The unsupervised DA setting that we consider
does not assume the availability of labeled data for
the target domain. However, if a small amount of
labeled data is available for the target domain, it
can be used to further improve the performance of
DA tasks (Xiao et al, 2013; Daum?e III, 2007).
3 Distribution Prediction
3.1 In-domain Feature Vector Construction
Before we tackle the problem of learning a model
to predict the distribution of a word across do-
mains, we must first compute the distribution of
a word from a single domain. For this purpose, we
represent a word w using unigrams and bigrams
that co-occur with w in a sentence as follows.
Given a document H, such as a user-review of
a product, we split H into sentences, and lemma-
tize each word in a sentence using the RASP sys-
tem (Briscoe et al, 2006). Using a standard stop
word list, we filter out frequent non-content un-
igrams and select the remainder as unigram fea-
tures to represent a sentence. Next, we generate
bigrams of word lemmas and remove any bigrams
that consists only of stop words. Bigram features
capture negations more accurately than unigrams,
and have been found to be useful for sentiment
classification tasks. Table 1 shows the unigram
and bigram features we extract for a sentence us-
ing this procedure. Using data from a single do-
sentence This is an interesting and well researched book
unigrams this, is, an, interesting, and, well, researched,
(surface) book
unigrams this, be, an, interest, and, well, research, book
(lemma)
unigrams interest, well, research, book
(features)
bigrams this+be, be+an, an+interest, interest+and,
(lemma) and+well, well+research, research+book
bigrams an+interest, interest+and, and+well,
(features) well+research, research+book
Table 1: Extracting unigram and bigram features.
main, we construct a feature co-occurrence ma-
trix A in which columns correspond to unigram
features and rows correspond to either unigram or
bigram features. The value of the element a
ij
in
the co-occurrence matrix A is set to the number of
sentences in which the i-th and j-th features co-
occur.
Typically, the number of unique bigrams is
much larger than that of unigrams. Moreover, co-
occurrences of bigrams are rare compared to co-
occurrences of unigrams, and co-occurrences in-
volving a unigram and a bigram. Consequently,
in matrix A, we consider co-occurrences only be-
tween unigrams vs. unigrams, and bigrams vs.
unigrams. We consider each row in A as repre-
senting the distribution of a feature (i.e. unigrams
or bigrams) in a particular domain over the uni-
gram features extracted from that domain (repre-
sented by the columns of A). We apply Positive
Pointwise Mutual Information (PPMI) to the co-
occurrence matrix A. This is a variation of the
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990), in which all PMI values that are less
than zero are replaced with zero (Lin, 1998; Bul-
linaria and Levy, 2007). Let F be the matrix that
results when PPMI is applied to A. Matrix F has
the same number of rows, n
r
, and columns, n
c
, as
the raw co-occurrence matrix A.
Note that in addition to the above-mentioned
representation, there are many other ways to rep-
resent the distribution of a word in a particular do-
main (Turney and Pantel, 2010). For example,
one can limit the definition of co-occurrence to
words that are linked by some dependency relation
(Pado and Lapata, 2007), or extend the window
of co-occurrence to the entire document (Baroni
and Lenci, 2010). Since the method we propose
in Section 3.2 to predict the distribution of a word
across domains does not depend on the particular
615
feature representation method, any of these alter-
native methods could be used.
To reduce the dimensionality of the feature
space, and create dense representations for words,
we perform SVD on F. We use the left singu-
lar vectors corresponding to the k largest singular
values to compute a rank k approximation
?
F, of
F. We perform truncated SVD using SVDLIBC
2
.
Each row in
?
F is considered as representing a word
in a lower k (n
c
) dimensional feature space cor-
responding to a particular domain. Distribution
prediction in this lower dimensional feature space
is preferrable to prediction over the original fea-
ture space because there are reductions in overfit-
ting, feature sparseness, and the learning time. We
created two matrices,
?
F
S
and
?
F
T
from the source
and target domains, respectively, using the above
mentioned procedure.
3.2 Cross-Domain Feature Vector Prediction
We propose a method to learn a model that can
predict the distribution w
T
of a word w in the
target domain T , given its distribution w
S
in
the source domain S. We denote the set of
features that occur in both domains by W =
{w
(1)
, . . . , w
(n)
}. In the literature, such features
are often referred to as pivots, and they have been
shown to be useful for DA, allowing the weights
learnt to be transferred from one domain to an-
other. Various criteria have been proposed for se-
lecting a small set of pivots for DA, such as the
mutual information of a word with the two do-
mains (Blitzer et al, 2007). However, we do not
impose any further restrictions on the set of pivots
W other than that they occur in both domains.
For each word w
(i)
? W , we denote the cor-
responding rows in
?
F
S
and
?
F
T
by column vec-
tors w
(i)
S
and w
(i)
T
. Note that the dimensional-
ity of w
(i)
S
and w
(i)
T
need not be equal, and we
may select different numbers of singular vectors
to approximate
?
F
S
and
?
F
T
. We model distribu-
tion prediction as a multivariate regression prob-
lem where, given a set {(w
(i)
S
,w
(i)
T
)}
n
i=1
consist-
ing of pairs of feature vectors selected from each
domain for the pivots in W , we learn a mapping
from the inputs (w
(i)
S
) to the outputs (w
(i)
T
).
We use Partial Least Squares Regression
(PLSR) (Wold, 1985) to learn a regression model
using pairs of vectors. PLSR has been applied in
2
http://tedlab.mit.edu/
?
dr/SVDLIBC/
Algorithm 1 Learning a prediction model.
Input: X, Y, L.
Output: Prediction matrix M.
1: Randomly select ?
l
from columns in Y
l
.
2: v
l
= X
l
>?
l
/
?
?
?
?
X
l
>?
l
?
?
?
?
3: ?
l
= X
l
v
l
4: q
l
= Y
l
>?
l
/
?
?
?
?
Y
l
>?
l
?
?
?
?
5: ?
l
= Y
l
q
l
6: If ?
l
is unchanged go to Line 7; otherwise go to Line 2
7: c
l
= ?
l
>?
l
/
?
?
?
??
l
>?
l
?
?
?
?
8: p
l
= X
l
>?
l
/?
l
>?
l
9: X
l+1
= X
l
? ?
l
p
l
>
and Y
l+1
= Y
l
? c
l
?
l
q
l
>
.
10: Stop if l = L; otherwise l = l + 1 and return to Line 1.
11: Let C = diag(c
1
, . . . , c
L
), and V = [v
1
. . .v
L
]
12: M = V(P
>
V)
?1
CQ
>
13: return M
Chemometrics (Geladi and Kowalski, 1986), pro-
ducing stable prediction models even when the
number of samples is considerably smaller than
the dimensionality of the feature space. In particu-
lar, PLSR fits a smaller number of latent variables
(10? 100 in practice) such that the correlation be-
tween the feature vectors for pivots in the two do-
mains are maximised in this latent space.
Let X and Y denote matrices formed by ar-
ranging respectively the vectors w
(i)
S
s and w
(i)
T
in
rows. PLSR decomposes X and Y into a series of
products between rank 1 matrices as follows:
X ?
L
?
l=1
?
l
p
l
>
= ?P
>
(1)
Y ?
L
?
l=1
?
l
q
l
>
= ?Q
>
. (2)
Here, ?
l
, ?
l
, p
l
, and q
l
are column vectors, and
the summation is taken over the rank 1 matrices
that result from the outer product of those vectors.
The matrices, ?, ?, P, and Q are constructed re-
spectively by arranging ?
l
, ?
l
, p
l
, and q
l
vectors
as columns.
Our method for learning a distribution predic-
tion model is shown in Algorithm 1. It is based on
the two block NIPALS routine (Wold, 1975; Rosi-
pal and Kramer, 2006) and iteratively discovers L
pairs of vectors (?
l
,?
l
) such that the covariances,
Cov(?
l
,?
l
), are maximised under the constraint
||p
l
|| = ||q
l
|| = 1. Finally, the prediction matrix,
M is computed using ?
l
,?
l
,p
l
, q
l
. The predicted
distribution
?
w
T
of a word w in T is given by
?
w
T
= Mw
S
. (3)
616
Our distribution prediction learning method is un-
supervised in the sense that it does not require
manually labeled data for a particular task from
any of the domains. This is an important point,
and means that the distribution prediction method
is independent of the task to which it may subse-
quently be applied. As we go on to show in Sec-
tion 6, this enables us to use the same distribution
prediction method for both POS tagging and sen-
timent classification.
4 Domain Adaptation
The main reason that a model trained only on the
source domain labeled data performs poorly in
the target domain is the feature mismatch ? few
features in target domain test instances appear in
source domain training instances. To overcome
this problem, we use the proposed distribution pre-
diction method to find those related features in the
source domain that correspond to the features ap-
pearing in the target domain test instances.
We consider two DA tasks: (a) cross-domain
POS tagging (Section 4.1), and (b) cross-domain
sentiment classification (Section 4.2). Note that
our proposed distribution prediction method can
be applied to numerous other NLP tasks that in-
volve sequence labelling and document classifica-
tion.
4.1 Cross-Domain POS Tagging
We represent each word using a set of features
such as capitalisation (whether the first letter of the
word is capitalised), numeric (whether the word
contains digits), prefixes up to four letters, and
suffixes up to four letters (Miller et al, 2011).
Next, for each word w in a source domain labeled
(i.e. manually POS tagged) sentence, we select its
neighbours u
(i)
in the source domain as additional
features. Specifically, we measure the similarity,
sim(u
(i)
S
,w
S
), between the source domain distri-
butions of u
(i)
and w, and select the top r simi-
lar neighbours u
(i)
for each word w as additional
features for w. We refer to such features as dis-
tributional features in this work. The value of a
neighbour u
(i)
selected as a distributional feature
is set to its similarity score sim(u
(i)
S
,w
S
). Next,
we train a CRF model using all features (i.e. cap-
italisation, numeric, prefixes, suffixes, and distri-
butional features) on source domain labeled sen-
tences.
We train a PLSR model, M, that predicts the
target domain distribution Mu
(i)
S
of a word u
(i)
in
the source domain labeled sentences, given its dis-
tribution, u
(i)
S
. At test time, for each word w that
appears in a target domain test sentence, we mea-
sure the similarity, sim(Mu
(i)
S
,w
T
), and select
the most similar r words u
(i)
in the source domain
labeled sentences as the distributional features for
w, with their values set to sim(Mu
(i)
S
,w
T
). Fi-
nally, the trained CRF model is applied to a target
domain test sentence.
Note that distributional features are always se-
lected from the source domain during both train
and test times, thereby increasing the number of
overlapping features between the trained model
and test sentences. To make the inference tractable
and efficient, we use a first-order Markov factori-
sation, in which we consider all pairwise combi-
nations between the features for the current word
and its immediate predecessor.
4.2 Cross-Domain Sentiment Classification
Unlike in POS tagging, where we must individ-
ually tag each word in a target domain test sen-
tence, in sentiment classification we must classify
the sentiment for the entire review. We modify the
DA method presented in Section 4.1 to satisfy this
requirement as follows.
Let us assume that we are given a set
{(x
(i)
S
, y
(i)
)}
n
i=1
of n labeled reviews x
(i)
S
for the
source domain S. For simplicity, let us consider
binary sentiment classification where each review
x
(i)
is labeled either as positive (i.e. y
(i)
= 1) or
negative (i.e. y
(i)
= ?1). Our cross-domain bi-
nary sentiment classification method can be easily
extended to the multi-class setting as well. First,
we lemmatise each word in a source domain la-
beled review x
(i)
S
, and extract both unigrams and
bigrams as features to represent x
(i)
S
by a binary-
valued feature vector. Next, we train a binary clas-
sification model, ?, using those feature vectors.
Any binary classification algorithm can be used
to learn ?. In our experiments, we used L2 reg-
ularised logistic regression.
Next, we train a PLSR model, M, as described
in Section 3.2 using unlabeled reviews in the
source and target domains. At test time, we rep-
resent a test target review H using a binary-valued
feature vector h of unigrams and bigrams of lem-
mas of the words in H, as we did for source do-
main labeled train reviews. Next, for each feature
w
(j)
extracted from H, we measure the similarity,
617
sim(Mu
(i)
S
,w
(j)
T
), between the target domain dis-
tribution of w
(j)
, and each feature (unigram or bi-
gram) u
(i)
in the source domain labeled reviews.
We score each source domain feature u
(i)
for its
relatedness to H using the formula:
score(u
(i)
,H) =
1
|H|
|H|
?
j=1
sim(Mu
(i)
S
,w
(j)
T
) (4)
where |H| denotes the total number of features ex-
tracted from the test review H. We select the top
scoring r features u
(i)
as distributional features for
H, and append those to h. The corresponding val-
ues of those distributional features are set to the
scores given by Equation 4. Finally, we classify
h using the trained binary classifier ?. Note that
given a test review, we find the distributional fea-
tures that are similar to all the words in the test re-
view from the source domain. In particular, we do
not find distributional features independently for
each word in the test review. This enables us to
find distributional features that are consistent with
all the features in a test review.
4.3 Model Choices
For both POS tagging and sentiment classifica-
tion, we experimented with several alternative
approaches for feature weighting, representation,
and similarity measures using development data,
which we randomly selected from the training in-
stances from the datasets described in Section 5.
For feature weighting for sentiment classifica-
tion, we considered using the number of occur-
rences of a feature in a review and tf-idf weight-
ing (Salton and Buckley, 1983). For representa-
tion, we considered distributional features u
(i)
in
descending order of their scores given by Equa-
tion 4, and then taking the inverse-rank as the val-
ues for the distributional features (Bollegala et al,
2011). However, none of these alternatives re-
sulted in performance gains. With respect to simi-
larity measures, we experimented with cosine sim-
ilarity and the similarity measure proposed by Lin
(1998); cosine similarity performed consistently
well over all the experimental settings. The feature
representation was held fixed during these similar-
ity measure comparisons.
For POS tagging, we measured the effect of
varying r, the number of distributional features,
using a development dataset. We observed that
setting r larger than 10 did not result in signifi-
cant improvements in tagging accuracy, but only
increased the train time due to the larger feature
space. Consequently, we set r = 10 in POS tag-
ging. For sentiment analysis, we used all features
in the source domain labeled reviews as distri-
butional features, weighted by their scores given
by Equation 4, taking the inverse-rank. In both
tasks, we parallelised similarity computations us-
ing BLAS
3
level-3 routines to speed up the com-
putations. The source code of our implementation
is publicly available
4
.
5 Datasets
To evaluate DA for POS tagging, following Blitzer
et al (2006), we use sections 2 ? 21 from Wall
Street Journal (WSJ) as the source domain labeled
data. An additional 100, 000 WSJ sentences from
the 1988 release of the WSJ corpus are used as the
source domain unlabeled data. Following Schn-
abel and Sch?utze (2013), we use the POS labeled
sentences in the SACNL dataset (Petrov and Mc-
Donald, 2012) for the five target domains: QA fo-
rums, Emails, Newsgroups, Reviews, and Blogs.
Each target domain contains around 1000 POS
labeled test sentences and around 100, 000 unla-
beled sentences.
To evaluate DA for sentiment classification,
we use the Amazon product reviews collected by
Blitzer et al (2007) for four different product cat-
egories: books (B), DVDs (D), electronic items
(E), and kitchen appliances (K). There are 1000
positive and 1000 negative sentiment labeled re-
views for each domain. Moreover, each domain
has on average 17, 547 unlabeled reviews. We use
the standard split of 800 positive and 800 negative
labeled reviews from each domain as training data,
and the remainder for testing.
6 Experiments and Results
For each domain D in the SANCL (POS tag-
ging) and Amazon review (sentiment classifica-
tion) datasets, we create a PPMI weighted co-
occurrence matrix F
D
. On average, F
D
created
for a target domain in the SANCL dataset con-
tains 104, 598 rows and 65, 528 columns, whereas
those numbers in the Amazon dataset are 27, 397
and 35, 200 respectively. In cross-domain senti-
ment classification, we measure the binary senti-
ment classification accuracy for the target domain
3
http://www.openblas.net/
4
http://www.csc.liv.ac.uk/
?
danushka/
software.html
618
test reviews for each pair of domains (12 pairs in
total for 4 domains). On average, we have 40, 176
pivots for a pair of domains in the Amazon dataset.
In cross-domain POS tagging, WSJ is always
the source domain, whereas the five domains in
SANCL dataset are considered as the target do-
mains. For this setting we have 9822 pivots on
average. The number of singular vectors k se-
lected in SVD, and the number of PLSR dimen-
sions L are set respectively to 1000 and 50 for the
remainder of the experiments described in the pa-
per. Later we study the effect of those two param-
eters on the performance of the proposed method.
The L-BFGS (Liu and Nocedal, 1989) method is
used to train the CRF and logistic regression mod-
els.
6.1 POS Tagging Results
Table 2 shows the token-level POS tagging accu-
racy for unseen words (i.e. words that appear in the
target domain test sentences but not in the source
domain labeled train sentences). By limiting the
evaluation to unseen words instead of all words,
we can evaluate the gain in POS tagging accuracy
solely due to DA. The NA (no-adapt) baseline sim-
ulates the effect of not performing any DA. Specif-
ically, in POS tagging, a CRF trained on source
domain labeled sentences is applied to target do-
main test sentences, whereas in sentiment classi-
fication, a logistic regression classifier trained us-
ing source domain labeled reviews is applied to the
target domain test reviews. The S
pred
baseline di-
rectly uses the source domain distributions for the
words instead of projecting them to the target do-
main. This is equivalent to setting the prediction
matrix M to the unit matrix. The T
pred
baseline
uses the target domain distribution w
T
for a word
w instead of Mw
S
. If w does not appear in the
target domain, then w
T
is set to the zero vector.
The S
pred
and T
pred
baselines simulate the two al-
ternatives of using source and target domain dis-
tributions instead of learning a PLSR model. The
DA method proposed in Section 4.1 is shown as
the Proposed method. Filter denotes the train-
ing set filtering method proposed by Schnabel and
Sch?utze (2013) for the DA of POS taggers.
From Table 2, we see that the Proposed method
achieves the best performance in all five domains,
followed by the T
pred
baseline. Recall that the
T
pred
baseline cannot find source domain words
that do not appear in the target domain as distri-
Target NA S
pred
T
pred
Filter Proposed
QA 67.34 68.18 68.75 57.08 69.28
?
Emails 65.62 66.62 67.07 65.61 67.09
Newsgroups 75.71 75.09 75.57 70.37 75.85
?
Reviews 56.36 54.60 56.68 47.91 56.93
?
Blogs 76.64 54.78 76.90 74.56 76.97
?
Table 2: POS tagging accuracies on SANCL.
butional features for the words in the target do-
main test reviews. Therefore, when the overlap be-
tween the vocabularies used in the source and the
target domains is small, T
pred
cannot reduce the
mismatch between the feature spaces. Poor perfor-
mance of the S
pred
baseline shows that the distri-
butions of a word in the source and target domains
are different to the extent that the distributional
features found using source domain distributions
are inadequate. The two baselines S
pred
and T
pred
collectively motivate our proposal to learn a distri-
bution prediction model from the source domain
to the target. The improvements of Proposed over
the previously proposed Filter are statistically sig-
nificant in all domains except the Emails domain
(denoted by ? in Table 2 according to the Bino-
mial exact test at 95% confidence). However, the
differences between the T
pred
and Proposed meth-
ods are not statistically significant.
6.2 Sentiment Classification Results
In Figure 1, we compare the Proposed cross-
domain sentiment classification method (Section
4.2) against several baselines and the current state-
of-the-art methods. The baselines NA, S
pred
, and
T
pred
are defined similarly as in Section 6.1. SST
is the Sentiment Sensitive Thesaurus proposed by
Bollegala et al (2011). SST creates a single distri-
bution for a word using both source and target do-
main reviews, instead of two separate distributions
as done by the Proposed method. SCL denotes
the Structural Correspondence Learning method
proposed by Blitzer et al (2006). SFA denotes
the Spectral Feature Alignment method proposed
by Pan et al (2010). SFA and SCL represent the
current state-of-the-art methods for cross-domain
sentiment classification. All methods are evalu-
ated under the same settings, including train/test
split, feature spaces, pivots, and classification al-
gorithms so that any differences in performance
can be directly attributable to their domain adapt-
ability. For each domain, the accuracy obtained
by a classifier trained using labeled data from that
619
E?>B D?>B K?>B5055
6065
7075
8085
Accura
cy
B?>E D?>E K?>E50
60
70
80
90
Accura
cy
B?>D E?>D K?>D5055
6065
7075
8085
Accura
cy
 
 
NA SFA SST SCL Spred Tpred ProposedB?>K E?>K D?>K
50
60
70
80
90
Accura
cy
Figure 1: Cross-Domain sentiment classification.
domain is indicated by a solid horizontal line in
each sub-figure. This upper baseline represents
the classification accuracy we could hope to obtain
if we were to have labeled data for the target do-
main. Clopper-Pearson 95% binomial confidence
intervals are superimposed on each vertical bar.
From Figure 1 we see that the Proposed method
reports the best results in 8 out of the 12 domain
pairs, whereas SCL, SFA, and S
pred
report the
best results in other cases. Except for the D-E set-
ting in which Proposed method significantly out-
performs both SFA and SCL, the performance of
the Proposed method is not statistically signifi-
cantly different to that of SFA or SCL.
The selection of pivots is vital to the perfor-
mance of SFA. However, unlike SFA, which re-
quires us to carefully select a small subset of pivots
(ca. less than 500) using some heuristic approach,
our Proposed method does not require any pivot
selection. Moreover, SFA projects source domain
reviews to a lower-dimensional latent space, in
which a binary sentiment classifier is subsequently
trained. At test time SFA projects a target review
into this lower-dimensional latent space and ap-
plies the trained classifier. In contrast, our Pro-
posed method predicts the distribution of a word
in the target domain, given its distribution in the
source domain, thereby explicitly translating the
source domain reviews to the target. This property
enables us to apply the proposed distribution pre-
diction method to tasks other than sentiment anal-
ysis such as POS tagging where we must identify
distributional features for individual words.
10 100 200 300 400 500 600 700 8000.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
PLSR dimensions
Accu
racy
 
 
E??>BD??>B
Figure 2: The effect of PLSR dimensions.
Unlike our distribution prediction method,
which is unsupervised, SST requires labeled data
for the source domain to learn a feature mapping
between a source and a target domain in the form
of a thesaurus. However, from Figure 1 we see
that in 10 out of the 12 domain-pairs the Proposed
method returns higher accuracies than SST.
To evaluate the overall effect of the number of
singular vectors k used in the SVD step, and the
number of PLSR components L used in Algorithm
1, we conduct two experiments. To evaluate the ef-
fect of the PLSR dimensions, we fixed k = 1000
and measured the cross-domain sentiment classi-
fication accuracy over a range of L values. As
shown in Figure 2, accuracy remains stable across
a wide range of PLSR dimensions. Because the
time complexity of Algorithm 1 increases linearly
with L, it is desirable that we select smaller L val-
620
1000 1500 2000 2500 3000
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
SVD dimensions
Accu
racy
 
 E??>BD??>B
Figure 3: The effect of SVD dimensions.
Measure Distributional features
sim(u
S
, w
S
) thin (0.1733), digestible (0.1728),
small+print (0.1722)
sim(u
T
, w
T
) travel+companion (0.6018), snap-in
(0.6010), touchpad (0.6016)
sim(u
S
, w
T
) segregation (0.1538), participation
(0.1512), depression+era (0.1508)
sim(Mu
S
, w
T
) small (0.2794), compact (0.2641),
sturdy (0.2561)
Table 3: Top 3 distributional features u ? S for
the word lightweight (w).
ues in practice.
To evaluate the effect of the SVD dimensions,
we fixed L = 100 and measured the cross-domain
sentiment classification accuracy for different k
values as shown in Figure 3. We see an overall
decrease in classification accuracy when k is in-
creased. Because the dimensionality of the source
and target domain feature spaces is equal to k, the
complexity of the least square regression problem
increases with k. Therefore, larger k values result
in overfitting to the train data and classification ac-
curacy is reduced on the target test data.
As an example of the distribution prediction
method, in Table 3 we show the top 3 similar
distributional features u in the books (source) do-
main, predicted for the electronics (target) domain
word w = lightweight, by different similarity
measures. Bigrams are indicted by a + sign and
the similarity scores of the distributional features
are shown within brackets.
Using the source domain distributions for both
u and w (i.e. sim(u
S
, w
S
)) produces distribu-
tional features that are specific to the books do-
main, or to the dominant adjectival sense of hav-
ing no importance or influence. On the other
hand, using target domain distributions for u and
w (i.e. sim(u
T
, w
T
)) returns distributional fea-
tures of the dominant nominal sense of lower in
weight frequently associated with electronic de-
vices. Simply using source domain distributions
u
S
(i.e. sim(u
S
, w
T
)) returns totally unrelated dis-
tributional features. This shows that word distribu-
tions in source and target domains are very differ-
ent and some adaptation is required prior to com-
puting distributional features.
Interestingly, we see that by using the dis-
tributions predicted by the proposed method
(i.e. sim(Mu
S
, w
T
)) we overcome this problem
and find relevant distributional features from the
source domain. Although for illustrative purposes
we used the word lightweight, which occurs in
both the source and the target domains, our pro-
posed method does not require the source domain
distribution w
S
for a word w in a target domain
document. Therefore, it can find distributional fea-
tures even for words occurring only in the target
domain, thereby reducing the feature mismatch
between the two domains.
7 Conclusion
We proposed a method to predict the distribution
of a word across domains. We first create a distri-
butional representation for a word using the data
from a single domain, and then learn a Partial
Least Square Regression (PLSR) model to pre-
dict the distribution of a word in a target domain
given its distribution in a source domain. We eval-
uated the proposed method in two domain adapta-
tion tasks: cross-domain POS tagging and cross-
domain sentiment classification. Our experiments
show that without requiring any task-specific cus-
tomisations to our distribution prediction method,
it outperforms competitive baselines and achieves
comparable results to the current state-of-the-art
domain adaptation methods.
References
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case
study. Technical report, Microsoft Research.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673 ? 721.
Romaric Besanc?on, Martin Rajman, and Jean-C?edric
Chappelier. 1999. Textual similarities based on a
621
distributional approach. In Proc. of DEXA, pages
180 ? 184.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120 ?
128.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proc. of ACL, pages 440 ? 447.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a senti-
ment sensitive thesaurus for cross-domain sentiment
classification. In Proc. of ACL/HLT, pages 132 ?
141.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proc. of
COLING/ACL Interactive Presentation Sessions.
John A. Bullinaria and Jospeh P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510 ? 526.
Jinho D. Choi and Martha Palmer. 2012. Fast and
robust part-of-speech tagging using dynamic model
selection. In Proc. of ACL Short Papers, volume 2,
pages 363 ? 367.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22 ? 29,
March.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 26 ? 33.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proc. of ACL, pages 256 ? 263.
John R. Firth. 1957. A synopsis of linguistic theory
1930-55. Studies in Linguistic Analysis, pages 1 ?
32.
Paul Geladi and Bruce R. Kowalski. 1986. Partial
least-squares regression: a tutorial. Analytica Chim-
ica Acta, 185(0):1 ? 17.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named en-
tity recognition. In Proc. of NAACL, pages 281 ?
289.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proc. of
ACL/HLT, pages 123 ? 131.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL-IJCNLP?09, pages 495
? 503.
Fei Huang and Alexander Yates. 2012. Biased repre-
sentation learning for domain adaptation. In Proc.
of EMNLP/CoNLL, pages 1313 ? 1323.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2012. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359 ? 389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL, pages 768 ? 774.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503 ? 528.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proc. of NAACL/HLT, pages 28 ? 36.
John E. Miller, Manabu Torii, and K. Vijay-Shanker.
2011. Building domain-specific taggers without an-
notated (domain) data. In Proc. of EMNLP/CoNLL,
pages 1103 ? 1111.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161 ?
199.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proc. of WWW, pages 751 ? 760.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proc. of EMNLP, pages 938 ? 947.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes of
the 1st SANCL Workshop.
Natalia Ponomareva and Mike Thelwall. 2012. Do
neighbours help? an exploration of graph-based al-
gorithms for cross-domain sentiment classification.
In Proc. of EMNLP, pages 655 ? 665.
Roman Rosipal and Nicole Kramer. 2006. Overview
and recent advances in partial least squares. In
C. Saunders et al, editor, SLSFS?05, volume 3940 of
LNCS, pages 34 ? 51, Berlin Heidelberg. Springer-
Verlag.
G. Salton and C. Buckley. 1983. Introduction to
Modern Information Retreival. McGraw-Hill Book
Company.
Tobias Schnabel and Hinrich Sch?utze. 2013. Towards
robust cross-domain domain adaptation for part-of-
speech tagging. In Proc. of IJCNLP, pages 198 ?
206.
622
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proc. of EMNLP, pages 1631 ? 1642.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Aritificial Intelligence Research,
37:141 ? 188.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antonie Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. Journal of Machine Learning
Research, 11:3371 ? 3408.
Herman Wold. 1975. Path models with latent vari-
ables: the NIPALS approach. In H. M. Blalock
et al, editor, Quantitative socialogy: international
perspective on mathematical and statistical model-
ing, pages 307 ? 357. Academic.
Herman Wold. 1985. Partial least squares. In Samel
Kotz and Norman L. Johnson, editors, Encyclopedia
of the Statistical Sciences, pages 581 ? 591. Wiley.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In
Proc. of EMNLP, pages 152 ? 162.
623
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 77?82,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Chunking Clinical Text Containing Non-Canonical Language
Aleksandar Savkov
Department of Informatics
University of Sussex
Brighton, UK
a.savkov@sussex.ac.uk
John Carroll
Department of Informatics
University of Sussex
Brighton, UK
j.a.carroll@sussex.ac.uk
Jackie Cassell
Primary Care and Public Health
Brighton and Sussex Medical School
Brighton, UK
j.cassell@bsms.ac.uk
Abstract
Free text notes typed by primary care
physicians during patient consultations
typically contain highly non-canonical
language. Shallow syntactic analysis of
free text notes can help to reveal valu-
able information for the study of disease
and treatment. We present an exploratory
study into chunking such text using off-
the-shelf language processing tools and
pre-trained statistical models. We evalu-
ate chunking accuracy with respect to part-
of-speech tagging quality, choice of chunk
representation, and breadth of context fea-
tures. Our results indicate that narrow con-
text feature windows give the best results,
but that chunk representation and minor
differences in tagging quality do not have
a significant impact on chunking accuracy.
1 Introduction
Clinical text contains rich, detailed information of
great potential use to scientists and health service
researchers. However, peculiarities of language
use make the text difficult to process, and the pres-
ence of sensitive information makes it hard to ob-
tain adequate quantities for developing processing
systems. The short term goal of most research
in the area is to achieve a reliable language pro-
cessing foundation that can support more complex
tasks such as named entity recognition (NER) to a
sufficiently reliable level.
Chunking is the task of identifying non-
recursive phrases in text (Abney, 1991). It is a
type of shallow parsing that is a less challeng-
ing task than dependency or constituency parsing.
This makes it likely to give more reliable results on
clinical text, since there is a very limited amount of
annotated (or even raw) text of this kind available
for system development. Even though chunking
does not provide as much syntactic information as
full parsing, it is an excellent method for identify-
ing base noun phrases (NP), which is a key issue
in symptom and disease identification. Identify-
ing symptoms and diseases is at the heart of har-
nessing the potential of clinical data for medical
research purposes.
There are few resources that enable researchers
to adapt general domain techniques to clinical text.
Using the Harvey Corpus
1
? a chunk annotated
clinical text language resource ? we present an ex-
ploratory study into adapting general domain tools
and models to apply to free text notes typed by UK
primary care physicians.
2 Related Work
The Mayo Clinic Corpus (Pakhomov et al., 2004)
is a key resource that has been widely used as
a gold standard in part-of-speech (POS) tagging
of clinical text. Based on that corpus and the
Penn TreeBank (Marcus et al., 1993), Coden et al.
(2005) present an analysis of the effects of domain
data on the performance of POS tagging mod-
els, demonstrating significant improvements with
models trained entirely on domain data. Savova
et al. (2010) use this corpus for the development
of cTAKES, Mayo Clinic?s processing pipeline for
clinical text.
Fan et al. (2011) show that using more diverse
clinical data can lead to more accurate POS tag-
ging. They report that models trained on clinical
text datasets from two different institutions per-
form on each of the datasets better than both mod-
els trained only on the same or the other dataset.
Fan et al. (2013) present guidelines for syntac-
tic parsing of clinical text and a clinical Treebank
annotated according to them. The guidelines are
designed to help the annotators handle the non-
canonical language that is typical of clinical text.
1
An article describing the corpus is currently under re-
view.
77
3 Data
The Harvey Corpus is a chunk-annotated corpus
consisting of pairs of manually anonymised UK
primary care physician (General Practitioner, or
GP) notes and associated Read codes (Bentley et
al., 1996). Each Read code has a short textual
gloss. The purpose of the codes is to make it easy
to extract structured data from clinical records.
The reason we include the codes in the corpus is
that GPs often use their glosses as the beginning of
their note. Two typical examples (without chunk
annotation for clarity) are shown below.
Birth details | | Normal deliviery Girl
Weight - 3. 960kg Apgar score @ 1min
- 9 Apgar score @ 5min - 9 Vit K given
Paed check NAD HC - 34. 9cm Hip test
performed
(1)
Chest pain | | musculoskel pain last w/e,
nil to find, ecg by paramedic no change,
reassured, rev sos
(2)
The corpus comprises 890 pairs of Read codes
and notes, each annotated by medical experts us-
ing a chunk annotation scheme that includes non-
recursive noun phrases (NPs), main verb groups
(MVs), and a common annotation for adjectival
and adverbial phrases (APs). Example (3) be-
low illustrates the annotation. The majority of
the records (750) were double blind annotated by
medical experts, after which the resulting annota-
tion was adjudicated by a third medical expert an-
notator.
[Chest pain]
NP
| | [musculoskel pain]
NP
[last w/e]
NP
, [nil]
AP
to [find]
MV
, [ecg]
NP
by [paramedic]
NP
[no change]
NP
,
[reassured]
MV
, [rev]
MV
[sos]
AP
(3)
Inter-annotator agreement was 0.86 f-score, tak-
ing one annotator to be the gold standard and the
other the candidate. We calculate the f-score ac-
cording to the MUC-7 (Chinchor, 1998) specifica-
tion, with the standard f-score formula. The calcu-
lation is kept symmetric with regard to the choice
of gold standard annotator by limiting the counting
of incorrect categories to one per tag, and equat-
ing the missing and spurious categories. For ex-
ample, three words annotated as one three-token
chunk by annotator A and three one-token chunks
by annotator B will have one incorrect and two
missing/spurious elements.
The rest of the records are a by-product of the
training process. Ninety records were triple anno-
tated by three different medical experts with the
help of a computational linguist, and fifty records
were double annotated by a medical expert ? alone
and together with a computational linguist.
It is important to note that the text in the corpus
is not representative of all types of GP notes. It is
focused on text that represents the dominant part
of day-to-day notes, rather than standard edited
text such as copies of letters to specialists and
other medical practitioners.
Even though the corpus data is very rich in in-
formation, its non-canonical language means that
it is very different from other clinical corpora
such as the Mayo Clinic Corpus (Pakhomov et al.,
2004) and poses different challenges for process-
ing. The GP notes in the Harvey Corpus can be
regarded as groups of medical ?tweets? meant to
be used mainly by the author. Sentence segmenta-
tion in the classical sense of the term is often im-
possible, because there are no sentences. Instead
there are short bursts of phrases concatenated to-
gether often without any indication of their bound-
aries. The average length of a note is roughly 30
tokens including the Read code. This is in con-
trast to notes in other clinical text datasets, which
range from 100 to 400 tokens on average (Fan et
al., 2011; Pakhomov et al., 2004). As well as typ-
ical clinical text characteristics such as domain-
specific acronyms, slang, and abbreviations, punc-
tuation and casing are often misleading (if present
at all), and some common classes of words (e.g.
auxiliary verbs) are almost completely absent.
4 Chunking
State-of-the-art text chunking accuracy reaches an
f-score of 95% (Sun et al., 2008). However, this
is for standard, edited text, and relies on accurate
POS tagging in a pre-processing step. However,
the characteristics of GP-written free text make ac-
curate part of speech (POS) tagging and chunking
difficult. Major problems are caused by unknown
tokens and ambiguities due to omitted words or
phrases.
We evaluate two standard chunking tools, Yam-
Cha (Kudo and Matsumoto, 2003) and CRF++
2
,
selected based on their support for trainable con-
text features. The tools were applied to the Har-
2
http://crfpp.googlecode.com/svn/
trunk/doc/index.html
78
POS YamCha IOB YamCha BEISO CRF++ IOB CRF++ BEISO
ARK
IRC
75.35 76.63 ?1.04 76.87 ?2.91 75.87 ?1.64 76.23 ?1.99
ARK
Twitter
? 76.72 ?2.11 77.53 ?1.65 76.63 ?2.36 77.23 ?1.06
ARK
Ritter
75.70 76.59 ?2.01 76.72 ?2.11 76.63 ?1.05 77.17 ?1.77
cTAKES 82.42 75.32 ?2.52 75.85 ?2.02 75.43 ?1.79 75.53 ?1.90
GENIA 80.63 71.70 ?2.27* 74.86 ?1.41 74.16 ?2.03* 74.19 ?1.72
RASP ? 74.24 ?1.84 75.10 ?1.31 75.63 ?2.33 75.76 ?2.18
Stanford 80.68 76.40 ?1.69 76.36 ?2.92 75.95 ?1.25 75.94 ?1.91
SVMTool 76.40 74.32 ?2.57 74.30 ?2.71 74.66 ?1.77 74.68 ?2.28
Wapiti 73.39 74.74 ?2.29 74.78 ?1.33 73.59 ?2.62 73.83 ?2.31
baseline ? 69.66 ?1.89* 69.76 ?1.24 67.05 ?1.15* 68.65 ?1.41
Table 1: Chunking results using YamCha and CRF++ on data automatically POS tagged using nine
different models; the baseline is with no tagging. The IOB and BEISO columns compare the impact
of two chunk representation strategies. The POS column indicates the part-of-speech tagging accuracy
for a subset of the corpus. Asterisks indicate pairs of significantly different YamCha and CRF++ results
(t-test with 0.05 p-value).
vey Corpus with automatically generated POS an-
notation. Given the small amount of data and
the challenges presented above, we expected that
our results would be lower than those reported by
Savova et al. (2010). The aim of these experi-
ments is to find the best performance obtainable
with standard chunking tools, which we will build
on in further stages of our research.
We conducted pairs of experiments, one with
each chunking tool, divided into three groups: the
first investigates the effects of choice of POS tag-
ger for training data annotation (Section 4.1); the
second compares two chunk representations (Sec-
tion 4.2); and the third searches for the optimal
context features (Section 4.3). All feature tuning
experiments were conducted on a development set
and tested using 10-fold cross-validation on the
rest of the data. We used 10% of the whole data
for the development set and 90% of the remain-
ing data for a training sample during development.
This guarantees the development model is trained
on the same amount of data as the testing model.
4.1 Part-of-Speech Tagging
We evaluated and compared the results yielded
by the two chunkers, having applied each of
seven off-the-shelf POS taggers. Of these tag-
gers, cTAKES (Savova et al., 2010) and GENIA
(Tsuruoka et al., 2005) are the only ones trained
on data that resembles ours, which suggests that
they should have the best chance of performing
well. We also selected a number of other taggers
while trying to diversify their algorithms and train-
ing data as much as possible: the POS tagger part
of the Stanford NLP package (Toutanova et al.,
2003) because it is one of the most successfully
applied in the field; the RASP tagger (Briscoe
et al., 2006) because of its British National Cor-
pus (Clear, 1993) training data; the ARK tagger
(Owoputi et al., 2013) because of the terseness of
the tweet language; and the SVMTool (Gim?enez
and M`arquez, 2004) and Wapiti (Lavergne et al.,
2010) because they use SVM and CRF algorithms.
Our baseline model uses no part of speech infor-
mation.
Using the Penn TreeBank tagset (Marcus et al.,
1993), we manually annotated a subset of the cor-
pus of comparable size to the development set. Us-
ing this dataset we estimated the tagging accuracy
for all models that support that tagset (omitting
RASP and ARK Twitter since they use different
tagsets). In this evaluation, cTAKES is the best
performing model, followed closely by the Stan-
ford POS tagger and GENIA.
The results in Table 1 show that the differ-
ences between chunking models trained on differ-
ent POS annotations are small and mostly not sta-
tistically significant from each other. However, all
the results are significantly better than the base-
line, apart from those based on the GENIA tagger
output.
4.2 Chunk Representation
The dominant chunk representation standard in-
side, outside, begin (IOB) introduced by Ramshaw
and Marcus (1995) and established with the
79
CoNLL-2000 shared task (Sang and Buchholz,
2000) takes a minimalistic approach to the rep-
resentation problem in order to keep the number
of labels low. Note that for chunking representa-
tions the total number of labels is the product of
the chunk types and the set of representation types
plus the outside tag, meaning that for IOB with
our set of three chunk types (NP, MV, AP) there
are seven labels.
Alternative chunk representations, such as be-
gin, end, inside, single, outside (BEISO)
3
as used
by Kudo and Matsumoto (2001), offer more fine-
grained tagsets, presumably at a performance cost.
That cost is unnecessary unless there is something
to be gained from a more fine-grained tagset at de-
coding time, because the two representations are
deterministically inter-convertible. For instance,
an end tag could be useful for better recognising
boundaries between chunks of the same type. The
BEISO tagset model looks for the boundary be-
fore and after crossing it, while an IOB model
only looks after. This should give only a small
gain with standard edited text because the chunk
type distribution is fairly well balanced and punc-
tuation divides ambiguous cases such as lists of
compound nouns. However, the Harvey Corpus
is NP-heavy and contains many sequences of NP
chunks that do not have any punctuation to mark
their boundaries.
We evaluated the two chunk representations in
combination with each POS tagger. Table 1 shows
that the differences between the results for the
two representations are small and never statisti-
cally significant. We also evaluated the two chunk
representations with different amounts of training
data. The resulting learning curves (Figure 1) are
almost identical.
4.3 Context Features
We approached the feature tuning task by first ex-
ploring the smaller feature space of YamCha and
then using the trends there to constrain the fea-
tures of CRF++. YamCha has three groups of fea-
tures responsible for tokens, POS tags and dynam-
ically generated (i.e. preceding) chunk tags. For
all experiments we determined the best feature set
by exhaustively testing all context feature combi-
nations within a predefined range. We used the
same context window for the token and tag fea-
tures in order to reduce the search space. Given
3
Also sometimes abbreviated IOBSE
Feature Set CV Dev
W
-1
-W
1
, T
-1
-T
1
, C
-1
77.28 ?1.9 75.28
W
-1
-W
1
, T
-1
-T
1
, C
-2
-C
-1
77.27 ?2.6 74.70
W
-1
-W
2
, T
-1
-T
2
, C
-1
76.86 ?1.5 74.08
W
-2
-W
1
, T
-2
-T
1
, C
-2
76.46 ?1.3 74.00
W
-1
-W
1
, T
-1
-T
1
, C
-2
76.89 ?2.1 73.92
W
-2
-W
1
, T
-2
-T
1
, C
-3
-C
-1
76.52 ?0.9 73.91
W
-1
-W
1
, T
-1
-T
1
, C
-3
-C
-1
77.02 ?2.0 73.90
W
-2
-W
2
, T
-2
-T
2
, C
-1
77.03 ?1.9 73.86
W
-1
-W
1
, T
-1
-T
1
, C
-3
77.15 ?1.5 73.63
W
-3
-W
1
, T
-3
-T
1
, C
-2
-C
-1
75.71 ?1.9 73.63
Table 2: Development set and 10-fold cross-
validation results for the top ten feature sets of
YamCha models trained on ARK
Twitter
POS an-
notation. Token features are represented with
W, POS features with T, and dynamically gener-
ated chunk features with C. None of the cross-
validation results are significantly different from
each other (t-test with 0.05 p-value).
the terseness of the text we expected that wider
context windows of more than three tokens would
not be beneficial to the model, and therefore did
not consider them. Our experiments using Yam-
Cha confirmed this hypothesis and showed a con-
sistent trend among all experiments in favouring a
window of -1 to +1 for tokens and slightly wider
for chunk tags (see Table 2).
CRF++ provides a more powerful feature con-
figuration allowing for unary and pairwise
4
fea-
tures of output tags. The unary features allow the
construction of token or POS tag bigrams and tri-
grams in addition to the standard context windows.
The feature tuning search space with so many pa-
rameters is enormous, which required us to use our
findings from the YamCha experiments to trim it
down and make it computationally feasible. First,
we decreased the search window of all features by
one in each direction from -3:3 to -2:2. Second, we
used the top scoring POS model from the first ex-
perimental runs to constrain the features even fur-
ther by selecting only the top one hundred for the
rest of the models.
We could not identify the same uniform trend in
the top feature sets as we could with YamCha. Our
results ranged from very small context windows
to the maximum size of our search space. How-
4
The unary and pairwise features of output tags are re-
ferred to as unigram and bigram features of output tags on
the CRF++ web page. Although this is correct, it can also
be confused with unigrams and bigrams of tokens, which are
expressed as unary (unigram) output tag features.
80
55 
60 
65 
70 
75 
80 
20 80 140 200 260 320 380 440 500 560 620 680 740 800 
IOB BEISO 
Figure 1: Chunking results for YamCha IOB and
BEISO models with increasing amounts of train-
ing data.
ever, we noticed that BEISO feature sets tend to
be smaller than the IOB ones. We also found that
the pairwise features normally improve the results.
5 Discussion and Future Work
We were surprised that the experiments did not
show a clear correlation between POS tagging ac-
curacy and chunking accuracy. On the other hand,
the chunking results using POS tagged data are
significantly better than the baseline, except when
using the GENIA tagger output. The small dif-
ferences between training sets of similar POS ac-
curacy could be explained due to the non-uniform
impact of the wrong POS tag on the chunking pro-
cess. Some mistakes such as labelling a noun as
a verb in the middle of a NP chunk are almost
sure to propagate and cause further chunking er-
rors, whereas others may have minimal or no ef-
fect, for example labelling a singular noun as a
proper noun. An error analysis of verb tags and
noun tags (Table 3) shows that the ARK models
tend to make more mistakes that keep the anno-
tation within the same tag group compared to the
GENIA model (see column pairs 1 and 3, and 2
and 4). This is a possible explanation for the lower
accuracy of the chunking model trained on data
tagged by GENIA.
Our experiments showed that the models using
the two chunk representations did not perform sig-
nificantly differently from each other. We also
showed that this conclusion is likely to hold if
Model N
group
V
group
Nouns Verbs
ARK
IRC
67.17 78.26 88.26 85.99
ARK
Twitter
- - 86.97 88.71
ARK
Ritter
68.57 77.29 90.64 85.02
cTAKES 83.93 62.80 93.85 69.08
GENIA 81.56 61.83 92.03 71.01
RASP - - 84.59 83.58
Stanford 80.30 73.42 91.89 83.09
SVMTool 69.97 70.04 90.08 80.19
Wapiti 65.64 66.66 87.84 74.87
Table 3: Detailed view of the POS model re-
sults focusing on the noun and verb tag groups.
The leftmost two columns of figures show accura-
cies over tags in the respective groups; the right-
most two columns show the accuracies of the same
groups if all tags in a group are replaced with a
group tag, e.g. V for verbs
5
.
more training data were available.
There are a number of ways we could improve
chunking accuracy besides increasing the amount
of training data. Although our results do not show
a clear trend, Fan et al. (2011) demonstrate that the
domain of part-of-speech training data has a sig-
nificant impact on tagging accuracy, which could
potentially impact chunking results if it decreases
the number of errors that propagate during chunk-
ing. An important problem in that area is dealing
with present and past participles, which are almost
sure to cause error propagation if mislabelled (as
nouns or adjectives, respectively). Participles are
more ambiguous in terse contexts lacking auxil-
iary verbs, which are natural disambiguation indi-
cators. Another direction in processing that could
contribute to better chunking is better token and
sentence segmentation. Finally, unknown words,
which may potentially have the largest impact on
chunking accuracy, could be dealt with using a
generic solution such as feature expansion based
on distributional similarity.
References
S. Abney. 1991. Parsing by chunks. In Robert C.
Berwick, Steven P. Abney, and Carol Tenny, editors,
Principle-Based Parsing: Computation and Psy-
cholinguistics, pages 257?278. Kluwer, Dordrecht.
T. Bentley, C. Price, and P. Brown. 1996. Structural
and lexical features of successive versions of the
5
Note that these results are different from what would be
yielded by a classifier trained on data subjected to the same
tag substitution.
81
read codes. In Proceedings of the Annual Confer-
ence of The Primary Health Care Specialist Group
of the British Computer Society, pages 91?103.
T. Briscoe, J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings of
the COLING/ACL on Interactive Presentation Ses-
sions, COLING-ACL?06, pages 77?80, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
N. Chinchor. 1998. Appendix B: Test scores. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7), Fairfax, VA, April.
J. Clear. 1993. The British National Corpus. In
George P. Landow and Paul Delany, editors, The
Digital Word, pages 163?187. MIT Press, Cam-
bridge, MA, USA.
A. Coden, S. Pakhomov, R. Ando, P. Duffy, and
C. Chute. 2005. Domain-specific language mod-
els and lexicons for tagging. Journal of Biomedical
Informatics, 38:422?430.
J.-W. Fan, R. Prasad, R.M. Yabut, R.M. Loomis, D.S.
Zisook, J.E. Mattison, and Y. Huang. 2011. Part-of-
speech tagging for clinical text: Wall or bridge be-
tween institutions? In American Medical Informat-
ics Association Annual Symposium, 1, pages 382?
391. American Medical Informatics Association.
J.-W. Fan, E. Yang, M. Jiang, R. Prasad, R. Loomis,
D. Zisook, J. Denny, H. Xu, and Y. Huang. 2013.
Research and applications: Syntactic parsing of clin-
ical text: guideline and corpus development with
handling ill-formed sentences. JAMIA, 20(6):1168?
1177.
J. Gim?enez and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector
machines. In Proceedings of the 4th LREC, Lisbon,
Portugal.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of the Second
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
Technologies, NAACL?01, pages 1?8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 24?31, Morristown, NJ,
USA. Association for Computational Linguistics.
T. Lavergne, O. Capp?e, and F. Yvon. 2010. Practi-
cal very large scale CRFs. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 504?513. Association for
Computational Linguistics, July.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel,
N. Schneider, and N. Smith. 2013. Improved
part-of-speech tagging for online conversational text
with word clusters. In Proceedings of NAACL-HLT,
pages 380?390.
S. Pakhomov, A. Coden, and C. Chute. 2004. Creat-
ing a test corpus of clinical notes manually tagged
for part-of-speech information. In Proceedings of
the International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and Its Applica-
tions, JNLPBA?04, pages 62?65, Stroudsburg, PA,
USA. Association for Computational Linguistics.
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94.
E. Sang and S. Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proceed-
ings of the 2nd Workshop on Learning Language
in Logic and the 4th Conference on Computational
Natural Language Learning, pages 13?14.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507?513.
X. Sun, L.-P. Morency, D. Okanoharay, and J. Tsujii.
2008. Modeling latent-dynamic in shallow parsing:
A latent conditional model with improved inference.
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING?08),
Manchester, UK, August.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of
the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics on Human Language Technology - Volume 1,
NAACL?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. Devel-
oping a robust part-of-speech tagger for biomedical
text. In Proceedings of the 10th Panhellenic Con-
ference on Advances in Informatics, PCI?05, pages
382?392, Berlin, Heidelberg. Springer-Verlag.
82
