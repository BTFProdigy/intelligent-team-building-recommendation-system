Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 785?789,
Dublin, Ireland, August 23-24, 2014.
UoW: NLP Techniques Developed at the University of Wolverhampton for
Semantic Similarity and Textual Entailment
Rohit Gupta, Hanna B
?
echara, Ismail El Maarouf and Constantin Or
?
asan
Research Group in Computational Linguistics,
Research Institute of Information and Language Processing,
University of Wolverhampton, UK
{R.Gupta, Hanna.Bechara, I.El-Maarouf, C.Orasan}@wlv.ac.uk
Abstract
This paper presents the system submit-
ted by University of Wolverhampton for
SemEval-2014 task 1. We proposed a ma-
chine learning approach which is based
on features extracted using Typed Depen-
dencies, Paraphrasing, Machine Transla-
tion evaluation metrics, Quality Estima-
tion metrics and Corpus Pattern Analysis.
Our system performed satisfactorily and
obtained 0.711 Pearson correlation for the
semantic relatedness task and 78.52% ac-
curacy for the textual entailment task.
1 Introduction
The SemEval task 1 (Marelli et al., 2014a) in-
volves two subtasks: predicting the degree of re-
latedness between two sentences and detecting the
entailment relation holding between them. The
task uses SICK dataset (Marelli et al., 2014b),
consisting of 10000 pairs, each annotated with re-
latedness in meaning and entailment relationship
holding between them. Similarity measures be-
tween sentences are required in a wide variety of
NLP applications. In applications like Informa-
tion Retrieval (IR), measuring similarity is a vi-
tal step in order to determine the best result for
a related query. Other applications such as Para-
phrasing and Translation Memory (TM) rely on
similarity measures to weight results. However,
computing semantic similarity between sentences
is a complex and difficult task, due to the fact that
the same meaning can be expressed in a variety of
ways. For this reason it is necessary to have more
than a surface-form comparison.
We present a method based on machine learning
which exploits available NLP technology. Our ap-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
proach relies on features inspired by deep seman-
tics (such as parsing and paraphrasing), machine
translation quality estimation, machine translation
evaluation and Corpus Pattern Analysis (CPA
1
).
We use the same features to measure both se-
mantic relatedness and textual entailment. Our hy-
pothesis is that each feature covers a particular as-
pect of implicit similarity and entailment informa-
tion contained within the pair of sentences. Train-
ing is performed in a regression framework for se-
mantic relatedness and in a classification frame-
work for textual entailment.
The remainder of the paper is structured as fol-
lows. In Section 2, we review the work related
to our study and the existing NLP technologies
used to measure sentence similarity. In Sections 3
and 4, we describe our approach and the similarity
measures we used. In Section 5, we present the re-
sults and an analysis of our runs based on the test
and training data provided by the SemEval-2014
task. Finally, our work is summed up in Section 6
with perspectives for future work we would like to
explore.
2 Related Work
The areas of semantic relatedness and entailment
have received extensive interest from the research
community in the last decade. Earlier work in
relatedness (Banerjee and Pedersen, 2003; Li et
al., 2006) exploited WordNet in various ways to
extract the semantic relatedness. Banerjee and
Pedersen (2003) presented a measure using ex-
tended gloss overlap. This measure takes two
WordNet synsets as input and uses the overlap
of their WordNet glosses to compute their degree
of semantic relatedness. Li et al. (2006) pre-
sented a semantic similarity metric based on the
semantic similarity of words in a sentence. Re-
cently, Wang and Cer (2012) presented an ap-
1
http://pdev.org.uk
785
proach that uses probabilistic edit-distance to mea-
sure semantic similarity. The approach uses prob-
abilistic finite state and pushdown automata to
model weighted edit-distance where state transi-
tions correspond to edit-operations. In some as-
pects, our work is similar to B?ar et al. (2012),
who presented an approach which combines var-
ious text similarity measures using a log-linear re-
gression model.
Entailment has been modelled using various ap-
proaches. The main approaches are based on
logic inferencing (Moldovan et al., 2003), ma-
chine learning (Hickl et al., 2006; Castillo, 2010)
and tree edit-distance (Kouylekov and Magnini,
2005). Most of the recent approaches employ var-
ious syntactic or tree edit models (Heilman and
Smith, 2010; Mai et al., 2011; Rios and Gelbukh,
2012; Alabbas and Ramsay, 2013). Recently, Al-
abbas and Ramsay (2013) presented a modified
tree edit distance approach, which extends tree
edit distance to the level of subtrees. The ap-
proach extends Zhang-Shasha?s algorithm (Zhang
and Shasha, 1989).
3 Features
Our system uses the same 31 features for both sub-
tasks. This section explains them and the code
which implements most of them can be found on
GitHub
2
.
3.1 Language Technology Features
We used existing language processing tools to ex-
tract features. Stanford CoreNLP
3
toolkit provides
lemma, parts of speech (POS), named entities, de-
pendencies relations of words in each sentence.
We calculated Jaccard similarity on surface
form, lemma, dependencies relations, POS and
named entities to get the feature values. The Jac-
card similarity computes sentence similarity by di-
viding the overlap of words on the total number of
words of both sentences.
Sim(s1, s2) =
|s1 ? s2|
|s1 ? s2|
(1)
where in equation (1), Sim(s1, s2) is the Jaccard
similarity between sets of words s1 and s2.
We used the same toolkit to identify corefer-
ence relations and determine clusters of corefer-
ential entities. The coreference feature value was
2
https://github.com/rohitguptacs/wlvsimilarity
3
http://nlp.stanford.edu/software/corenlp.shtml
calculated using clusters of coreferential entities.
The intuition is that sentences containing corefer-
ential entities should have some semantic related-
ness. In order to extract clusters of coreferential
entities, the pair of sentences was treated as a doc-
ument. The coreference feature value using these
clusters was calculated as follows:
V alue
coref
=
CC
TC
(2)
where CC is the number of clusters formed by the
participation of entities (at least one entity from
each sentence of the pair) in both sentences and
TC is the total number of clusters.
We calculated two separate feature values for
dependency relations: the first feature concate-
nated the words involved in a dependency relation
and the second used grammatical relation tags. For
example, for the sentence pair ?the kids are play-
ing outdoors? and ?the students are playing out-
doors? the Jaccard similarity is calculated based
on concatenated words ?kids::the, playing::kids,
playing::are, ROOT::playing, playing::outdoors?
and ?students::the, playing::students, playing::are,
ROOT::playing, playing::outdoors? to get the
value for the first feature and ?det, nsubj, aux, root,
dobj? and ?det, nsubj, aux, root, dobj? to get the
value for the second feature.
These language technology features try to cap-
ture the token based similarity and grammatical
similarity between a pair of sentences.
3.2 Paraphrasing Features
We used the PPDB paraphrase database (Ganitke-
vitch et al., 2013) to get the paraphrases. We used
lexical and phrasal paraphrases of ?L? size. For
each sentence of the pair, we created two sets of
bags of n-grams (1 ? n ? length of the sentence).
We extended each set with paraphrases for each n-
gram available from paraphrase database. We then
calculated the Jaccard similarity (see Section 3.1)
between these extended bag of n-grams to get the
feature value. This feature capture the cases where
one sentence is a paraphrase of the other.
3.3 Negation Feature
Our system does not attempt to model similar-
ity with negation, but since negation is an impor-
tant feature for contradiction in textual entailment,
we designed a non-similarity feature. The system
checks for the presence of a negation word such as
?no?, ?never? and ?not? in the pair of sentences and
786
returns ?1? (?0? otherwise) if both or none of the
sentences contain any of these words.
3.4 Machine Translation Quality Estimation
Features
Seventeen of the features consist of Machine
Translation Quality Estimation (QE) features,
based on the work of (Specia et al., 2009) and used
as a baseline in recent QE tasks (such as (Callison-
Burch et al., 2012)). We extracted these features
by treating the first set of sentences as the Machine
Translation (MT) ?source?, and the second set of
sentences as the MT ?target?. In Machine Trans-
lation, these features are used to access the quality
of MT ?target?. The QE features include shallow
surface features such as the number of punctua-
tion marks, the average length of words, the num-
ber of words. Furthermore, these features include
n-gram frequencies and language model probabil-
ities. A full list of the QE features is provided in
the documentation of the QE system
4
(Specia et
al., 2009).
QE features relate to well-formedness and syn-
tax, and are not usually used to compute seman-
tic relatedness between sentences. We have used
them in the hope that the surface features at least
will show us some structural similarity between
sentences.
3.5 Machine Translation Evaluation Features
Additionally, we used BLEU (Papineni et al.,
2002), a very popular machine translation evalu-
ation metric, as a feature. BLEU is based on n-
gram counts. It is meant to capture the similarity
between translated text and references for machine
translation evaluation. The BLEU score over sur-
face, lemma and POS was calculated to get three
feature values. In a pair of sentences, one side was
treated as a translation and another as a reference.
We applied it at the sentence level to capture the
similarity between two sentences.
3.6 Corpus Pattern Analysis Features
Corpus Pattern Analysis (CPA) (Hanks, 2013) is
a procedure in corpus linguistics that associates
word meaning with word use by means of seman-
tic patterns. CPA is a new technique for map-
ping meaning onto words in text. It is currently
being used to build a ?Pattern Dictionary of En-
glish Verbs?(PDEV
5
). It is based on the Theory of
4
https://github.com/lspecia/quest
5
http://pdev.org.uk
Norms and Exploitations (Hanks, 2013).
There are two features extracted from PDEV.
They both make use of a derived resource called
the CPA network (Bradbury and El Maarouf,
2013). The CPA network links verbs according
to similar semantic patterns (e.g. both ?pour? and
?trickle? share an intransitive use where the subject
is ?liquid?).
The first feature value compares the main verbs
in both sentences. When both verbs share a pat-
tern, the system returns a value of ?1? (otherwise
?0?). The second feature extends the CPA network
to compute the probability of a PDEV pattern,
given a word. This probability is computed over
the portion of the British National Corpus which is
manually tagged with PDEV patterns. The prob-
ability of a pattern given each word of a sentence
of the dataset is obtained by the product of those
probabilities. The feature value is the (normalised)
number of common patterns from the three most
probable patterns in each sentence. These features
try to capture similarity based on semantic pat-
terns.
4 Predicting Through Machine Learning
4.1 Model Description
We used a support vector machine in order to build
a regression model to predict semantic relatedness
and a classification model to predict textual entail-
ment. For the actual implementation we used Lib-
SVM
6
(Chang and Lin, 2011).
We used a regression model for the related-
ness task that estimates a continuous score be-
tween 1 and 5 for each sentence. For the entail-
ment task, we trained a classification model which
assigns one of three different labels (ENTAIL-
MENT, CONTRADICTION, NEUTRAL) to each
sentence pair. We trained both systems on the
4500 sentence training set, augmented with the
500 sentence trial data. The values of C and ?
have been optimised through a grid-search which
uses a 5-fold cross-validation method.
The RBF kernel proved to be the best for both
tasks.
5 Results and Analysis
We submitted 4 runs of our system (Run-1 to Run-
4). Run-1 was submitted as primary run. Run-2,
Run-3 and Run-4 systems were identical except
6
http://www.csie.ntu.edu.tw/ cjlin/libsvm/
787
Run-1 Run-2 Run-3 Run-4
C 8 8 2 2
? 0.0441 0.0441 0.125 0.125
Pearson 0.7111 0.7166 0.6968 0.6975
Table 1: Results: Relatedness.
for some parameter differences for SVM train-
ing and the replacement of the values which were
outside the boundaries (1-5). If relatedness val-
ues predicted by the system were less than 1 or
greater than 5, these values were replaced by 1
and 5 respectively for Run-1, Run-2 and Run-4
and 1.5 and 4.5 respectively for Run-3. Our pri-
mary run also used one extra feature for related-
ness, which was obtained by considering entail-
ment judgement as a feature. Our hypothesis was
that entailment judgement may help in measur-
ing relatedness. In the actual test this feature was
not helpful and we obtained Pearson correlation of
0.711 for the primary run, compared to 0.716 for
Run-2. The details of runs are given in Table 1 and
2.
After training both models, we ran a feature
selection algorithm to determine which features
yielded the highest accuracy, and therefore had the
highest impact on our system. Perhaps unsurpris-
ingly, the QE features were not very useful in pre-
dicting semantic similarity or entailment. How-
ever, despite their focus on fluency rather than se-
mantic correctness, the QE features still managed
to contribute to some improvements in the textual
entailment task (increasing accuracy by 1%), and
the semantic relatedness task (0.027 increase in
Pearson correlation).
In the entailment (classification) task, the
strongest feature proved to be the negation fea-
ture with 70% accuracy (on the training set) when
training on this feature only. This suggests that
some measure of negation is crucial in determin-
ing whether a sentence contradicts or entails an-
other sentence. Other strong features were lemma,
paraphrasing and dependencies.
In the relatedness (regression) task, the lemma,
surface, paraphrasing, dependencies, PDEV fea-
tures were the strongest contributors to accuracy.
Run-1 Run-2 Run-3 Run-4
C 16 16 8 8
? 0.0625 0.0625 0.5 0.5
Accuracy 78.526 78.526 78.343 78.343
Table 2: Results: Entailment.
6 Conclusion and Future Work
We have presented an efficient approach to calcu-
late semantic relatedness and textual entailment.
One noticeable point of our approach is that we
have used the same features for both tasks and
our system performed well in each of these tasks.
Therefore, our system captures reasonably good
models to compute semantic relatedness and tex-
tual entailment.
In the future we would like to explore more fea-
tures and particularly those based on tree edit dis-
tance, WordNet and PDEV. Our intuition suggests
that tree edit distance seems to be more helpful for
entailment, whereas WordNet and PDEV seem to
be more helpful for similarity measurement. Ad-
ditionally, we would like to combine our tech-
niques for measuring relatedness and entailment
with MT evaluation techniques. We would fur-
ther like to apply these techniques cross-lingually,
moving into other areas like machine translation
evaluation and quality estimation.
Acknowledgement
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union?s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement no. 317471 and partly supported
by an AHRC grant ?Disambiguating Verbs by Col-
location project, AH/J005940/1, 2012-2015?.
References
Maytham Alabbas and Allan Ramsay. 2013. Natural
language inference for Arabic using extended tree
edit distance with subtrees. Journal of Artificial In-
telligence Research, 48:1?22.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In IJCAI, volume 3, pages 805?810.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In First Joint Conference on
788
Lexical and Computational Semantics, Association
for Computational Linguistics, pages 435?440.
Jane Bradbury and Isma??l El Maarouf. 2013. An
empirical classification of verbs based on Semantic
Types: the case of the ?poison? verbs. In Proceed-
ings of the Joint Symposium on Semantic Process-
ing. Textual Inference and Structures in Corpora,
pages 70?74.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia, editors.
2012. Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation. Association for Com-
putational Linguistics, Montr?eal, Canada, June.
Julio J. Castillo. 2010. Recognizing textual en-
tailment: experiments with machine learning al-
gorithms and RTE corpora. Special issue: Natu-
ral Language Processings and its Applications, Re-
search in Computing Science, 46:155?164.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Juri Ganitkevitch, Van Durme Benjamin, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758?764, Atlanta, Georgia.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In The 2010 An-
nual Conference of the North American Chapter of
the ACL, number June, pages 1011?1019.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Rec-
ognizing textual entailment with LCC?s GROUND-
HOG system. In Proceedings of the Second PAS-
CAL Challenges Workshop.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In Proceedings of the First Challenge
Workshop Recognising Textual Entailment, pages
17?20.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O?shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138?1150.
Zhewei Mai, Y Zhang, and Donghong Ji. 2011. Rec-
ognizing text entailment via syntactic tree match-
ing. In Proceedings of NTCIR-9 Workshop Meeting,
pages 361?364, Tokyo, Japan.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014).
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. COGEX : A Logic Prover
for Question Answering. In Proceedings of HLT-
NAACL, number June, pages 87?93.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311?318.
Miguel Rios and Alexander Gelbukh. 2012. Recog-
nizing Textual Entailment with a Semantic Edit Dis-
tance Metric. In 11th Mexican International Confer-
ence on Artificial Intelligence, pages 15?20. IEEE.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In 13th Conference of the European Asso-
ciation for Machine Translation, pages 28?37.
Mengqiu Wang and Daniel Cer. 2012. Stanford: prob-
abilistic edit distance metrics for STS. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 648?654.
Kaizhong Zhang and Dennis Shasha. 1989. Simple
Fast Algorithms for the Editing Distance between
Trees and Related Problems. SIAM Journal on Com-
puting, 18(6):1245?1262.
789
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131?140,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
An evaluation of syntactic simplification rules for people with autism
Richard Evans, Constantin Or
?
asan and Iustin Dornescu
Research Institute in Information and Language Processing
University of Wolverhampton
United Kingdom
{R.J.Evans, C.Orasan, I.Dornescu2}@wlv.ac.uk
Abstract
Syntactically complex sentences consti-
tute an obstacle for some people with
Autistic Spectrum Disorders. This pa-
per evaluates a set of simplification rules
specifically designed for tackling complex
and compound sentences. In total, 127 dif-
ferent rules were developed for the rewrit-
ing of complex sentences and 56 for the
rewriting of compound sentences. The
evaluation assessed the accuracy of these
rules individually and revealed that fully
automatic conversion of these sentences
into a more accessible form is not very re-
liable.
1 Introduction
People with Autistic Spectrum Disorders (ASD)
show a diverse range of reading abilities: on
the one hand, 5%-10% of users have the capac-
ity to read words from an early age without the
need for formal learning (hyperlexia), on the other
hand many users demonstrate weak comprehen-
sion of what has been read (Volkmar and Wiesner,
2009). They may have difficulty inferring contex-
tual information or may have trouble understand-
ing mental verbs or emotional language, as well
as long sentences with complex syntactic structure
(Tager-Flusberg, 1981; Kover et al., 2012). To ad-
dress these difficulties, the FIRST project
1
is de-
veloping a tool which makes texts more accessible
for people with ASD. In order to get a better un-
derstanding of the needs of these readers, a thor-
ough analysis was carried out to derive a list of
high priority obstacles to reading comprehension.
Some of these obstacles are related to syntactic
complexity and constitute the focus of this paper.
Even though the research in the FIRST project fo-
cuses on people with ASD, many of the obstacles
1
http://first-asd.eu
identified in the project can pose difficulties for a
wide range of readers such as language learners
and people with other language disorders.
This paper presents and evaluates a set of rules
used for simplifying English complex and com-
pound sentences. These rules were developed as
part of a syntactic simplification system which was
initially developed for users with ASD, but which
can also be used for other tasks that require syn-
tactic simplification of sentences. In our research,
we consider that syntactic complexity is usually
indicated by the occurrence of certain markers or
signs of syntactic complexity, referred to hereafter
as signs, such as punctuation ([,] and [;]), con-
junctions ([and], [but], and [or]), complementis-
ers ([that]) or wh-words ([what], [when], [where],
[which], [while], [who]). These signs may have
a range of syntactic linking and bounding func-
tions which need to be automatically identified,
and which we analysed in more detail in (Evans
and Orasan, 2013).
Our syntactic simplification process operates in
two steps. In the first, signs of syntactic complex-
ity are automatically classified and in the second,
manually crafted rules are applied to simplify the
relevant sentences. Section 3 presents more details
about the method. Evaluation of automatic simpli-
fication is a difficult issue. Given that the purpose
of this paper is to gain a better understanding of
the performance of the rules used for simplifying
compound sentences and complex sentences, Sec-
tion 4 presents the methodology developed for this
evaluation and discusses the results obtained. The
paper finishes with conclusions.
2 Background information
Despite some findings to the contrary (Arya et al.,
2011), automatic syntactic simplification has been
motivated by numerous neurolinguistic and psy-
cholinguistic studies. Brain imaging studies indi-
cate that processing syntactically complex struc-
131
tures requires more neurological activity than pro-
cessing simple structures (Just et al., 1996). A
study undertaken by Levy et al. (2012) showed
that people with aphasia are better able to un-
derstand syntactically simple reversible sentences
than syntactically complex ones.
Further motivation is brought by research in
NLP, which demonstrates that performance levels
in information extraction (Agarwal and Boggess,
1992; Rindflesch et al., 2000; Evans, 2011),
syntactic parsing (Tomita, 1985; McDonald and
Nivre, 2011), and, to some extent, machine trans-
lation (Gerber and Hovy, 1998) are somewhat de-
termined by the length and syntactic complexity of
the sentences being processed.
Numerous rule-based methods for syntactic
simplification have been developed (Siddharthan,
2006) and used to facilitate NLP tasks such as
biomedical information extraction (Agarwal and
Boggess, 1992; Rindflesch et al., 2000; Evans,
2011). In these approaches, rules are triggered
by pattern-matching applied to the output of text
analysis tool such as partial parsers and POS tag-
gers. Chandrasekar and Srinivas (1997) presented
an automatic method to learn syntactic simplifi-
cation rules for use in such systems. Unfortu-
nately, that approach is only capable of learning
a restricted range of rules and requires access to
expensive annotated resources.
With regard to applications improving text ac-
cessibility for human readers, Max (2000) de-
scribed the use of syntactic simplification for
aphasic readers. In work on the PSET project,
Canning (2002) implemented a system which ex-
ploits a syntactic parser in order to rewrite com-
pound sentences as sequences of simple sentences
and to convert passive sentences into active ones
for readers with aphasia. The success of these sys-
tems is tied to the performance levels of the syn-
tactic parsers that they employ.
More recently, the availability of resources such
as Simple Wikipedia has enabled text simplifi-
cation to be included in the paradigm of statis-
tical machine translation (Yatskar et al., 2010;
Coster and Kauchak, 2011). In this context,
translation models are learned by aligning sen-
tences in Wikipedia with their corresponding ver-
sions in Simple Wikipedia. Manifesting Basic En-
glish (Ogden, 1932), the extent to which Simple
Wikipedia is accessible to people with autism has
not yet been fully assessed.
The field of text summarisation includes numer-
ous approaches that can be regarded as examples
of syntactic simplification. For example, Cohn and
Lapata (2009) present a tree-to-tree transduction
method that is used to filter non-essential infor-
mation from syntactically parsed sentences. This
compression process often reduces the syntactic
complexity of those sentences. An advantage of
this approach is that it can identify elements for
deletion even when such elements are not indi-
cated by explicit signs of syntactic complexity.
The difficulty is that they rely on high levels of ac-
curacy and granularity of automatic syntactic anal-
ysis. As noted earlier, it has been observed that the
accuracy of parsers is inversely proportional to the
length and complexity of the sentences being anal-
ysed (Tomita, 1985; McDonald and Nivre, 2011).
The approach to syntactic simplification de-
scribed in the current paper is a two step pro-
cess involving detection and tagging of the bound-
ing and linking functions of various signs of syn-
tactic complexity followed by a rule-based sen-
tence rewriting step. Relevant to the first step, Van
Delden and Gomez (2002) developed a machine
learning method to determine the syntactic roles
of commas. Meier et al. (2012) describe German
language resources in which the linking functions
of commas and semicolons are annotated. The an-
notated resources exploited by the machine learn-
ing method presented in Section 3.2.1 of the cur-
rent paper are presented in (Evans and Orasan,
2013). From a linguistic perspective, Nunberg et
al. (2002) provide a grammatical analysis of punc-
tuation in English.
The work described in this paper was under-
taken in a project aiming to improve the accessibil-
ity of text for people with autism. It was motivated
at least in part by the work of O?Connor and Klein
(2004), which describes strategies to facilitate the
reading comprehension of people with ASD.
The proposed method is intended to reduce
complexity caused by both complex and com-
pound sentences and differs from those described
earlier in this section. Sentence compression
methods are not suitable for the types of rewrit-
ing required in simplifying compound sentences.
Parsers are more likely to have lower accuracy
when processing these sentences, and therefore the
proposed method does not use information about
the syntactic structure of sentences in the process.
Our method is presented in the next section.
132
3 The syntactic simplifier
In our research, we regard coordination and sub-
ordination as key elements of syntactic complex-
ity. A thorough study of the potential obstacles to
the reading comprehension of people with autism
highlighted particular types of syntactic complex-
ity, many of which are linked to coordination
and subordination. Section 3.1 briefly presents
the main obstacles linked to syntactic complexity
identified by the study. It should be mentioned that
most of the obstacles are problematic not only for
autistic people and other types of reader can also
benefit from their removal. The obstacles identi-
fied constituted the basis for developing the sim-
plification approach briefly described in Section
3.2.
3.1 User requirements
Consultations with 94 subjects meeting the strict
DSM-IV criteria for ASD and with IQ > 70 led to
the derivation of user preferences and high priority
user requirements related to structural processing.
A comprehensive explanation of the findings can
be found in (Martos et al., 2013). This section dis-
cusses briefly the two types of information of rel-
evance to the processing of sentence complexity
obtained in our study.
First, in terms of the demand for access to texts
of particular genres/domains, it was found that
young people (aged 12-16) seek access to doc-
uments in informative (arts/leisure) domains and
they have less interest in periodicals and newspa-
pers or imaginative texts. Adults (aged 16+) seek
access to informative and scientific texts (includ-
ing newspapers), imaginative text, and the lan-
guage of social networking and communication.
In an attempt to accommodate the interests of both
young people and adults, we developed a cor-
pus which contains newspaper articles, texts about
health, and literary texts.
Second, the specific morpho-syntactic phenom-
ena that pose obstacles to reading comprehension
that are relevant to this paper are:
1. Compound sentences, which should be split
into sentences containing a single clause.
2. Complex sentences: in which relative clauses
should either be:
(a) converted into adjectival pre-modifiers
or
(b) deleted from complex sentences and
used to generate copular constructions
linking the NP in the matrix clause with
the predication of the relative clause
In addition, the analysis revealed other types
of obstacles such as explicative clauses, which
should be deleted, and uncommon conjunctions
(including conjuncts) which should be replaced
by more common ones. Conditional clauses that
follow the main clause and non-initial adverbial
clauses should be pre-posed, and passive sen-
tences should be converted in the active form. Var-
ious formatting issues such as page breaks that oc-
cur within paragraphs and end-of-line hyphenation
are also problematic and should be avoided.
Section 3.2 describes the method developed to
address the obstacles caused by compound and
complex sentences.
3.2 The approach
Processing of obstacles to reading comprehension
in this research has focused on detection and re-
duction of syntactic complexity caused by the oc-
currence in text of compound sentences (1) and
complex sentences (2).
(1) Elaine Trego never bonded with 16-month-old
Jacob [and] he was often seen with bruises, a
murder trial was told.
(2) The two other patients, who are far more
fragile than me, would have been killed by
the move.
In (1), the underlined phrases are the conjoins
of a coordinate constituent. In (2), the underlined
phrase is a subordinate constituent of the larger,
superordinate phrase the two other patients, who
are far more fragile than me.
The overall syntactic simplification pipeline
consists of the following steps:
Step 1. Tagging of signs of syntactic complexity
with information about their syntactic linking
or bounding functions
Step 2. The complexity of sentences tagged in
step 1 is assessed and used to trigger the ap-
plication of two iterative simplification pro-
cesses, which are applied exhaustively and
sequentially to each input sentence:
133
a. Decomposition of compound sentences
(the simplification function converts one
input string into two output strings)
b. Decomposition of complex sentences
(the simplification function converts one
input string into two output strings)
Step 3. Personalised transformation of sentences
according to user preference profiles which
list obstacles to be tackled and the threshold
complexity levels that specify whether sim-
plification is necessary.
Steps 1 and 2 are applied iteratively ensuring
that an input sentence can be exhaustively simpli-
fied by decomposition of the input string into pairs
of progressively simpler sentences. No further
simplification is applied to a sentence when the
system is unable to detect any signs of syntactic
complexity within it. This paper reports on steps 1
and 2. The personalisation step, which takes into
consideration the needs of individual users, is not
discussed.
3.2.1 Identification of signs of complexity
Signs of syntactic complexity typically indicate
constituent boundaries, e.g. punctuation marks,
conjunctions, and complementisers. To facilitate
information extraction, a rule-based approach to
simplify coordinated conjoins was proposed by
Evans (2011), which relies on classifying signs
based on their linking functions.
In more recent work, an extended annotation
scheme was proposed in (Evans and Orasan, 2013)
which enables the encoding of links and bound-
aries between a wider range of syntactic con-
stituents and covers more syntactic phenomena.
A corpus covering three text categories (news ar-
ticles, literature, and patient healthcare informa-
tion leaflets), was annotated using this extended
scheme.
2
Most sign labels contain three types of infor-
mation: boundary type, syntactic projection level,
and grammatical category of the constituent(s).
Some labels cover signs which bound interjec-
tions, tag questions, and reported speech and a
class denoting false signs of syntactic complex-
ity, such as use of the word that as a specifier or
anaphor. The class labels are a combination of the
following acronyms:
2
http://clg.wlv.ac.uk/resources/
SignsOfSyntacticComplexity/
1. {C|SS|ES}, the generic function as a coor-
dinator (C), the left boundary of a subordi-
nate constituent (SS), or the right boundary
of a subordinate constituent (ES).
2. {P |L|I|M |E}, the syntactic projection level
of the constituent(s): prefix (P), lexical (L),
intermediate (I), maximal (M), or extended/-
clausal (E).
3. {A|Adv|N |P |Q|V }, the grammatical cate-
gory of the constituent(s): adjectival (A), ad-
verbial (Adv), nominal (N), prepositional (P),
quantificational (Q), and verbal (V).
4. {1|2}, used to further differentiate sub-
classes on the basis of some other label-
specific criterion.
The scheme uses a total of 42 labels to distin-
guish between different syntactic functions of the
bounded constituents. Although signs are marked
by a small set of tokens (words and punctuation),
the high number of labels and their skewed dis-
tribution make signs highly ambiguous. In addi-
tion, each sign is only assigned exactly one label,
i.e. that of the dominant constituent in the case of
nesting, further increasing ambiguity. These char-
acteristics make automatic classification of signs
challenging.
The automatic classification of signs of syntac-
tic complexity is achieved using a machine learn-
ing approach described in more detail in Dornescu
et al. (2013). After experimenting with several
methods of representing the training data and with
several classifiers, the best results were obtained
by using the BIO model to train a CRF tagger. The
features used were the signs? surrounding con-
text (a window of 10 tokens and their POS tags)
together with information about the distance to
other signs signs in the same sentence and their
types. The method achieved an overall accuracy
of 82.50% (using 10 fold cross-validation) on the
manually annotated corpus.
3.2.2 Rule-based approach to simplification
of compound sentences and complex
sentences
The simplification method exploits two iterative
processes that are applied in sequence to input
text that has been tokenised with respect to sen-
tences, words, punctuation, and signs of syntac-
tic complexity. The word tokens in the input text
134
Rule ID CEV-12
Sentence type Compound (coordination)
Match pattern A that [B] sign
CEV
[C] .
Transform pattern A that [B]. A that [C].
Ex: input [Investigations showed]
A
that [the glass came from a car?s side window]
B
and
CEV
[thousands of batches had been tampered with on five separate weekends]
C
.
Ex: output [Investigations showed]
A
that [the glass came from a car?s side window]
B
.
[Investigations showed]
A
that [thousands of batches had been tampered with on five
separate weekends]
C
.
Rule ID CEV-26
Sentence type Compound (coordination)
Match pattern A v
CC
B: ?[C] sign
CEV
[D]?.
Transform pattern A v B: ?[C]?. A v B: ?[D]?.
Ex: input [He]
A
added[]
B
: ?[If I were with Devon and Cornwall police I?d be very interested in
the result of this case]
C
and
CEV
[I certainly expect them to renew their interest]
D
.?
Ex: output [He]
A
added[]
B
: ?[If I were with Devon and Cornwall police I?d be very interested in
the result of this case]
C
.?
[He]
A
added[]
B
: ?[I certainly expect them to renew their interest]
D
.?
Table 1: Patterns used to identify conjoined clauses.
have also been labelled with their parts of speech
and the signs have been labelled with their gram-
matical linking and bounding functions. The pat-
terns rely mainly on nine sign labels which delimit
clauses (*EV)
3
, noun phrases (*MN) and adjecti-
val phrases (*MA). These sign labels can signal
either coordinated conjoins (C*) or the start (SS*)
or end (ES*) of a constituent.
The first iterative process exploits patterns in-
tended to identify the conjoins of compound sen-
tences. The elements common to these patterns
are signs tagged as linking clauses in coordination
(label CEV). The second process exploits patterns
intended to identify relative clauses in complex
sentences. The elements common to these patterns
are signs tagged as being left boundaries of subor-
dinate clauses (label SSEV).
The identification of conjoint clauses depends
on accurate tagging of words with information
about their parts of speech and signs with informa-
tion about their general roles in indicating the left
or right boundaries of subordinate constituents.
The identification of subordinate clauses requires
more detailed information. In addition to the in-
formation required to identify clause conjoins, in-
formation about the specific functions of signs is
required. The simplification process is thus highly
dependent on the performance of the automatic
sign tagger.
Table 1 displays two patterns for identifying
conjoined clauses and Table 2 displays two pat-
terns for identifying subordinate clauses. In the
3
In these example the * character is used to indicate any
sequence of characters, representing the bounding or linking
function of the sign.
tables, upper case letters denote contiguous se-
quences of text,
4
the underbar denotes signs of
class CEV (in row Compound) and SSEV (in row
Complex). Verbs with clause complements are
denoted by v
CC
, while words of part of speech
X are denoted by w
X
. The symbol s is used
to denote additional signs of syntactic complex-
ity while v denotes words with verbal POS tags.
Words explicitly appearing in the input text are
italicised. Elements of the patterns representing
clause conjoins and subordinate clauses appear in
square brackets.
Each pattern is associated with a sentence
rewriting rule. A rule is applied on each itera-
tion of the algorithm. Sentences containing signs
which correspond to conjoint clauses are con-
verted into two strings which are identical to the
original save that, in one, the conjoint clause is
replaced by a single conjoin identified in the con-
joint while in the other, the identified conjoin is
omitted. Sentences containing signs which indi-
cate subordinate clauses are converted into two
new strings. One is identical to the original save
that the relative clause is deleted. The second is
automatically generated, and consists of the NP in
the matrix clause modified by the relative clause, a
conjugated copula, and the predication of the rela-
tive clause. Tables 1 and 2 give examples of trans-
formation rules for the given patterns. In total,
127 different rules were developed for the rewrit-
ing of complex sentences and 56 for the rewriting
of compound sentences.
4
Note that these sequences of text may contain additional
signs tagged CEV or SSEV.
135
Rule ID SSEV-61
Sentence type Complex (subordination)
Match pattern A s B [sign
SSEV
C v D].
Transform pattern A s B. That C v D.
Ex: input [During the two-week trial, the jury heard how Thomas became a frequent visitor to
Roberts?s shop in the summer of 1997]
A
, [after meeting him through a friend]
B
[who
[lived near the shop,]
C
[described as a ?child magnet? by one officer]
D
.
Ex: output [During the two-week trial, the jury heard how Thomas became a frequent visitor to
Roberts?s shop in the summer of 1997]
A
, [after meeting him through a friend]
B
.
That friend [lived near the shop,]
C
[described as a ?child magnet? by one officer]
D
.
Rule ID SSEV-72
Sentence type Complex (subordination)
Match pattern [A w
IN
w
DT
* n {n|of}* sign
SSEV
] w
V BD
B {.|?|!}
Transform pattern N/A
Pattern SSEV-72 is used to prevent rewriting of complex sentences when the subordinate
clause is the argument of a clause complement verb. The result of this rule is to strip the
tag from the triggering sign of syntactic complexity
Ex: input [Eamon Reidy, 32,]
A
fled [across fields in Windsor Great Park after the crash[, the court
heard.]
Table 2: Patterns used to identify subordinate clauses.
4 Evaluation
The detection and classification of signs of syntac-
tic complexity can be evaluated via standard meth-
ods in LT based on comparing classifications made
by the system with classifications made by linguis-
tic experts. This evaluation is reported in (Dor-
nescu et al., 2013). Unfortunately, the evaluation
of the actual simplification process is difficult, as
there are no well established methods for measur-
ing its accuracy. Potential methodologies for eval-
uation include comparison of system output with
human simplification of a given text, analysis of
the post-editing effort required to convert an au-
tomatically simplified text into a suitable form for
end users, comparisons using experimental meth-
ods such as eye tracking and extrinsic evaluation
via NLP applications such as information extrac-
tion, all of which have weaknesses in terms of ad-
equacy and expense.
Due to the challenges posed by these previously
established methods, we decided that before we
employ them and evaluate the output of the sys-
tem as a whole, we focus first on the evaluation
of the accuracy of the two rule sets employed by
the syntactic processor. The evaluation method is
based on comparing sets of simplified sentences
derived from an original sentence by linguistic ex-
perts with sets derived by the method described in
Section 3.
4.1 The gold standard
Two gold standards were developed to support
evaluation of the two rule sets. Texts from the gen-
res of health, literature, and news were processed
by different versions of the syntactic simplifier. In
one case, the only rules activated in the syntac-
tic simplifier were those concerned with rewriting
compound sentences. In the second case, the only
rules activated were those concerned with rewrit-
ing complex sentences. The output of the two ver-
sions was corrected by a linguistic expert to ensure
that each generated sentence was grammatically
well-formed and consistent in meaning with the
original sentence. Sentences for which even man-
ual rewriting led to the generation of grammati-
cally well-formed sentences that were not consis-
tent in meaning with the originals were removed
from the test data. After filtering, the test data
contained nearly 1,500 sentences for use in eval-
uating rules to simplify of compound sentences,
and nearly 1,100 sentences in the set used in eval-
uating rules to simplify complex sentences. The
break down per genre/domain is given in Tables
3a and 3b.
The subset of sentences included in the gold
standard contained manually annotated informa-
tion about the signs of syntactic complexity. This
was done to enable reporting of the evaluation re-
sults in two modes: one in which the system con-
sults an oracle for classification of signs of syntac-
tic complexity and one in which the system con-
sults the output of the automatic sign tagger.
4.2 Evaluation results
Evaluation results are reported in terms of accu-
racy of the simplification process and the change
in readability of the generated sentences. Com-
putation of accuracy is based on the mean Leven-
136
Text category
News Health Literature
#Compound sentences 698 325 418
Accuracy Oracle 0.758 0.612 0.246
Classifier 0.314 0.443 0.115
?Flesch Oracle 11.1 8.2 15.3
Classifier 9.9 10.2 13.6
?Avg. Oracle -12.58 -9.86 -16.69
Sent. Len. Classifier -13.08 -12.30 -16.79
(a) Evaluation of simplification of compound sentences
Text category
News Health Literature
#Complex sentences 369 335 379
Accuracy Oracle 0.452 0.292 0.475
Classifier 0.433 0.227 0.259
?Flesch Oracle 2.5 0.8 2.3
Classifier 2.3 0.9 2.3
?Avg. Oracle -2.96 -0.90 -2.80
Sent. Len. Classifier -2.80 -0.99 -2.11
(b) Evaluation of simplification of complex sentences
Table 3: Evaluation results for the two syntactic phenomena on three text genres
shtein similarity
5
between the sentences generated
by the system and the most similar simplified sen-
tences verified by the linguistic expert. Once the
most similar sentence in the key has been found,
that element is no longer considered for the rest of
the simplified sentences in the system?s response
to the original. In this evaluation, sentences are
considered to be converted correctly if their LS >
0.95. The reason for setting such a high threshold
for the Levenshtein ratio is because the evaluation
method should only reward system responses that
match the gold standard almost perfectly save for a
few characters which could be caused by typos or
variations in the use of punctuation and spaces. A
sentence is considered successfully simplified, and
implicitly all the rules used in the process are con-
sidered correctly applied, when all the sentences
produced by the system are converted correctly ac-
cording to the gold standard. This evaluation ap-
proach may be considered too inflexible as it does
not take into consideration the fact that a sentence
can be simplified in several ways. However, the
purpose here is to evaluate the way in which sen-
tences are simplified using specific rules.
In order to calculate the readability of the gen-
erated sentences we initially used the Flesch score
(Flesch, 1949). However, our system changes the
text only by rewriting sentences into sequences of
simpler sentences and does not make any changes
at the lexical level. For this reason, any changes
observed in the Flesch score are due to changes
in the average sentence length. Therefore, for our
experiments we report both ?Flesch score and
?average sentence length.
The evaluation results are reported separately
for the three domains. In addition, the results are
calculated when the classes of the signs are de-
5
Defined as 1 minus the ratio of Levenshtein distance be-
tween the two sentences to the length in characters of the
longest of the two sentences being compared.
rived from the manually annotated data (Oracle)
and from use of the automatic classifier (Classi-
fier).
Table 3a presents the accuracy of the rules im-
plemented to convert compound sentences into a
more accessible form. The row #Compound sen-
tences displays the number of sentences in the test
data that contain signs of conjoint clauses (signs
of class CEV). The results obtained are not unex-
pected. In all cases the accuracy of the simplifi-
cation rules is higher when the labels of signs are
assigned by the oracle. With the exception of the
health domain, the same pattern is observed when
?Flesch is considered. The highest accuracy is
obtained on the news texts, then the health do-
main, and finally the literature domain. However,
despite significantly lower accuracy on the litera-
ture domain, the readability of the sentences from
the literature domain benefits most from the auto-
matic simplification. This can be noticed both in
the improved Flesch scores and reduced sentence
length.
Table 3b presents the accuracy of the rules
which simplify complex sentences. In this table,
#Complex sentences denotes the number of sen-
tences in the test data that contain relative clauses.
The rest of the measures are calculated in the same
way as in Table 3a. Inspection of the table shows
that, for the news and health domains, the accu-
racy of these simplification rules is significantly
lower than the simplification rules used for com-
pound sentences. Surprisingly, the rules work bet-
ter for the literature domain than for the others.
The improvement in the readability of texts from
the health domain is negligible, which can be ex-
plained by the poor performance of the simplifica-
tion rules on this domain.
137
4.3 Error analysis
In order to have a better understanding of the per-
formance of the system, the performance of the
individual rules was also recorded. Tables 4 and 5
contain the most error prone trigger patterns for
conjoined and subordinate clauses respectively.
The statistics were derived from rules applied to
texts of all three categories of texts and the signs
of syntactic complexity were classified using an
oracle, in order to isolate the influence of the rules
in the system output. In this context, the accu-
racy with which the syntactic processor converts
sentences containing conjoint clauses into a more
accessible form is 0.577. The accuracy of this task
with regard to subordinate clauses is 0.411.
The most error-prone trigger patterns for con-
joined clauses are listed in Table 4, together with
information on the conjoin that they are intended
to detect (left or right), their error rate, and the
number of number of errors made. The same in-
formation is presented for the rules converting sen-
tences containing subordinate clauses in Table 5,
but in this case the patterns capture the subordina-
tion relations. In the patterns, words with partic-
ular parts of speech are denoted by the symbol w
with the relevant Penn Treebank tag appended as a
subscript. Verbs with clause complements are de-
noted v
CC
. Signs of syntactic complexity are de-
noted by the symbol s with the abbreviation of the
functional class appended as a subscript. Specific
words are printed in italics. In the patterns, the
clause coordinator is denoted ? ? and upper case
letters are used to denote stretches of contiguous
text.
Rules CEV-25a and SSEV-78a are applied when
the input sentence triggers none of the other imple-
mented patterns. Errors of this type quantify the
number of sentences containing conjoint or subor-
dinate clauses that cannot be converted into a more
accessible form by rules included in the structural
complexity processor. Both rules have quite high
error rates, but these errors can only be addressed
via the addition of new rules or the adjustment of
already implemented rules.
SSEV-36a is a pattern used to prevent process-
ing of sentences that contain verbs with clause
complements. This pattern was introduced be-
cause using the sentence rewriting algorithm pro-
posed here to process sentences containing these
subordinate clauses would generate ungrammati-
cal output.
Table 5 contains only 4 items because for the
rest of the patterns the number of errors was less
than 3. A large number of these rules had an error
rate of 1 which motivated their deactivation. Un-
fortunately this did not lead to improved accuracy
of the overall conversion process.
5 Conclusions and future work
Error analysis revealed that fully automatic con-
version compound and complex sentences into a
more accessible form is quite unreliable, partic-
ularly for texts of the literature category. It was
noted that conversion of complex sentences into a
more accessible form is more difficult than con-
version of compound sentences. However, sub-
ordinate clauses are significantly more prevalent
than conjoint clauses in the training and testing
data collected so far.
The evaluation of the rule sets used in the con-
version of compound and complex sentences into
a more accessible form motivates further specific
development of the rule sets. This process in-
cludes deletion of rules that do not meet particu-
lar thresholds for accuracy and the development of
new rules to address cases where input sentences
fail to trigger any conversion rules (signalled by
activation of redundant rules CEV-25a and SSEV-
78a).
The results are disappointing given that the
syntactic simplification module presented in this
paper is expected to be integrated in a system
that makes texts more accessible for people with
autism. However, this simplification module will
be included in a post-editing environment for peo-
ple with ASD. In this setting, it may still prove
useful, despite its low accuracy.
Acknowledgments
The research described in this paper was par-
tially funded by the European Commission un-
der the Seventh (FP7-2007-2013) Framework Pro-
gramme for Research and Technological Devel-
opment (FP7- ICT-2011.5.5 FIRST 287607). We
gratefully acknowledge the contributions of all the
members of the FIRST consortium for their feed-
back and comments during the development of the
methods, and to Laura Hasler for her help with the
evaluation.
138
ID Conjoin Trigger pattern Error rate #Errors
CEV-24b B A B 0.131 59
CEV-24a A A B 0.119 54
CEV-12b A that C A that B C 0.595 25
CEV-25a NA NA 0.956 22
CEV-26a A v
CCV
B : ?C? A v
CC
B : ?C D? 0.213 16
CEV-26b A v
CCV
B : ?D? A v
CC
B : ?C D? 0.203 14
Table 4: Error rates for rules converting sentences with conjoint clauses
ID Matrix clause / subordinate clause Trigger pattern Error rate #Errors
SSEV-78a NA NA 0.517 45
SSEV-72a A , C w
{verb}
D A s B C w
{verb}
D 0.333 4
SSEV-36a NA A told w
{noun|PRP|DT|IN}
* B 0.117 4
SSEV-13b w
VBN
w
IN
(w
{DT|PRP$|noun|CD}
|-|,)* w
{noun}
B
A w
VBN
w
IN
{w
{DT|PRP$|noun|CD}
|-|,}* w
{noun}
B
1 3
Table 5: Error rates for rules converting sentences with subordinate clauses
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th annual meeting for Computa-
tional Linguistics, pages 15?21, Newark, Delaware.
Association for Computational Linguistics.
D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson.
2011. The effects of syntactic and lexical com-
plexity on the comprehension of elementary science
texts. International Electronic Journal of Elemen-
tary Education, 4 (1):107?125.
Y. Canning. 2002. Syntactic Simplification of Text.
Ph.d. thesis, University of Sunderland.
R Chandrasekar and B Srinivas. 1997. Automatic in-
duction of rules for text simplification. Knowledge-
Based Systems, 10:183?190.
T. Cohn and M. Lapata. 2009. Sentence Compression
as Tree Transduction. Journal of Artificial Intelli-
gence Research, 20(34):637?74.
W. Coster and D. Kauchak. 2011. Simple english
wikipedia: A new text simplification task. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2011),
pages 665?669, Portland, Oregon, June. Association
of Computational Linguistics.
Iustin Dornescu, Richard Evans, and Constantin
Or?asan. 2013. A Tagging Approach to Identify
Complex Constituents for Text Simplification. In
Proceedings of Recent Advances in Natural Lan-
guage Processing, pages 221 ? 229, Hissar, Bul-
garia.
Richard Evans and Constantin Orasan. 2013. Annotat-
ing signs of syntactic complexity to support sentence
simplification. In I. Habernal and V. Matousek, edi-
tors, Text, Speech and Dialogue. Proceedings of the
16th International Conference TSD 2013, pages 92?
104. Springer, Plzen, Czech Republic.
R. Evans. 2011. Comparing methods for the syn-
tactic simplification of sentences in information ex-
traction. Literary and Linguistic Computing, 26
(4):371?388.
R. Flesch. 1949. The art of readable writing. Harper,
New York.
Laurie Gerber and Eduard H. Hovy. 1998. Improving
translation quality by manipulating sentence length.
In David Farwell, Laurie Gerber, and Eduard H.
Hovy, editors, AMTA, volume 1529 of Lecture Notes
in Computer Science, pages 448?460. Springer.
M. A. Just, P. A. Carpenter, and K. R. Thulborn. 1996.
Brain activation modulated by sentence comprehen-
sion. Science, 274:114?116.
S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J.
Hagerman, and L. Abbeduto. 2012. Syntactic com-
prehension in boys with autism spectrum disorders:
Evidence from specific constructions. In Proceed-
ings of the 2012 International Meeting for Autism
Research, Athens, Greece. International Society for
Autism Research.
J. Levy, E. Hoover, G. Waters, S. Kiran, D. Caplan,
A. Berardino, and C. Sandberg. 2012. Effects of
syntactic complexity, semantic reversibility, and ex-
plicitness on discourse comprehension in persons
with aphasia and in healthy controls. American
Journal of Speech?Language Pathology, 21(2):154
? 165.
Wolfgang Maier, Sandra K?ubler, Erhard Hinrichs, and
Julia Kriwanek. 2012. Annotating coordination in
the penn treebank. In Proceedings of the Sixth Lin-
guistic Annotation Workshop, pages 166?174, Jeju,
Republic of Korea, July. Association for Computa-
tional Linguistics.
Juan Martos, Sandra Freire, Ana Gonzlez, David Gil,
Richard Evans, Vesna Jordanova, Arlinda Cerga,
Antoneta Shishkova, and Constantin Orasan. 2013.
User preferences: Updated report. Technical report,
139
The FIRST Consortium, Available at http://first-
asd.eu/D2.2.
A. Max. 2000. Syntactic simplification - an applica-
tion to text for aphasic readers. Mphil in computer
speech and language processing, University of Cam-
bridge, Wolfson College.
Ryan T. McDonald and Joakim Nivre. 2011. Analyz-
ing and integrating dependency parsers. Computa-
tional Linguistics, 37(1):197?230.
Geoffrey Nunberg, Ted Briscoe, and Rodney Huddle-
ston. 2002. Punctuation. chapter 20 In Huddleston,
Rodney and Geoffrey K. Pullum (eds) The Cam-
bridge Grammar of the English Language, pages
1724?1764. Cambridge University Press.
I. M. O?Connor and P. D. Klein. 2004. Exploration
of strategies for facilitating the reading comprehen-
sion of high-functioning students with autism spec-
trum disorders. Journal of Autism and Developmen-
tal Disorders, 34:2:115?127.
C. K. Ogden. 1932. Basic English: a general intro-
duction with rules and grammar. K. Paul, Trench,
Trubner & Co., Ltd., London.
Thomas C. Rindflesch, Jayant V. Rajan, and Lawrence
Hunter. 2000. Extracting molecular binding rela-
tionships from biomedical text. In Proceedings of
the sixth conference on Applied natural language
processing, pages 188?195, Seattle, Washington.
Association of Computational Linguistics.
A. Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Compu-
tation, 4:1:77?109.
Helen Tager-Flusberg. 1981. Sentence comprehen-
sion in autistic children. Applied Psycholinguistics,
2:1:5?24.
Masaru Tomita. 1985. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Kluwer Academic Publishers, Norwell, MA, USA.
Sebastian van Delden and Fernando Gomez. 2002.
Combining finite state automata and a greedy learn-
ing algorithm to determine the syntactic roles of
commas. In Proceedings of the 14th IEEE Inter-
national Conference on Tools with Artificial Intel-
ligence, ICTAI ?02, pages 293?, Washington, DC,
USA. IEEE Computer Society.
F.R. Volkmar and L. Wiesner. 2009. A Practical Guide
to Autism. Wiley, Hoboken, NJ.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and
L. Lee. 2010. For the sake of simplicity: Unsu-
pervised extraction of lexical simplifications from
wikipedia. In Proceedings of Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 365?
368, Los Angeles, California, June. Association of
Computational Linguistics.
140
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 1?10,
Dublin, Ireland, August 24th 2014.
Relative clause extraction for syntactic simplification
Iustin Dornescu, Richard Evans, Constantin Or
?
asan
Research Group in Computational Linguistics
University of Wolverhampton
United Kingdom
{i.dornescu2,r.j.evans,c.orasan}@wlv.ac.uk
Abstract
This paper investigates non-destructive simplification, a type of syntactic text simplification
which focuses on extracting embedded clauses from structurally complex sentences and rephras-
ing them without affecting their original meaning. This process reduces the average sentence
length and complexity to make text simpler. Although relevant for human readers with low read-
ing skills or language disabilities, the process has direct applications in NLP. In this paper we
analyse the extraction of relative clauses through a tagging approach. A dataset covering three
genres was manually annotated and used to develop and compare several approaches for auto-
matically detecting appositions and non-restrictive relative clauses. The best results are obtained
by a ML model developed using crfsuite, followed by a rule based method.
1 Introduction
Text simplification (TS) is the process of reducing the complexity of a text while preserving its meaning
(Chandrasekar et al., 1996; Siddharthan, 2002a; Siddharthan, 2006). There are two main types of sim-
plification: syntactic and lexical. The focus of syntactic simplification is to take long and structurally
complicated sentences and rewrite them as sequences of sentences which are shorter and structurally
simpler. Lexical simplification focuses on replacing words which could make reading texts difficult with
more common terms and expressions. The focus of this paper is on syntactic simplification and more
specifically on how to identify noun post-modifying clauses from complex sentences.
The occurrence of embedded clauses due to subordination and coordination increases the structural
complexity of sentences, especially in long sentences where such phenomena are more prevalent. Sim-
ple sentences are usually much easier to understand by humans and can be more reliably processed by
Natural Language Processing (NLP) tools. Psycholinguistic and neurolinguistic imaging studies show
that syntactically complex sentences require more effort to process than syntactically simple ones (Just
et al., 1996; Levy et al., 2012). For this reason, complex sentences can cause problems to people with
language disabilities. At the same time, previous work indicates that syntactic simplification can im-
prove the reliability of NLP applications such as information extraction (Agarwal and Boggess, 1992;
Rindflesch et al., 2000; Evans, 2011), and machine translation (Gerber and Hovy, 1998). In the field
of syntactic parsing, studies show that parsing accuracy is lower for longer sentences (Tomita, 1985;
McDonald and Nivre, 2011). Therefore, the impact of this paper can be two-fold: on the one hand, it
can help increase the accuracy of automatic language processing, and on the other hand, it can be used
to make text more accessible to people with reading difficulties.
The research presented in this paper was carried out in the context of FIRST
1
, an EU funded project
which develops tools to make texts more accessible to people with Autism Spectrum Disorders (ASD). In
order to have a proper understanding of the obstacles which pose difficulties to people with ASD, a survey
of the literature on reading comprehension and questionnaires completed by people with ASD were
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://first-asd.eu
1
Figure 1: The online annotation using brat
conducted (Martos et al., 2013). The research confirmed that among other types of syntactic complexity,
subordinated clauses should be processed by a syntactic simplifier to make the text easier to read.
In this paper, we tackle non-destructive simplification, a form of syntactic simplification in which
a clause-based approach is employed to rephrase text in such a way that the meaning of the original
text is preserved as much as possible. This is specifically linked to certain types of syntactic structures
which can be extracted from the matrix clause without affecting meaning. subordinates, with These
types include appositions and non-restrictive relative clauses (Siddharthan, 2002b). This paper presents
a method specifically developed for identifying appositions and non-restrictive relative clauses which
can be removed from a text without losing essential information.
This paper is structured as follows: Section 2 presents the dataset used to carry out the experiments
presented in this paper, including the annotation guidelines and inter-annotator agreement. The machine
learning method developed to detect relative clauses is presented in Section 3 and the evaluation results
in Section 4. In Section 5, conclusions are drawn.
2 Dataset
To carry out the research presented in this paper, a corpus was annotated. This section presents the
annotation guidelines used in the process and discusses issues encountered during the annotation. The
annotation was performed using the BRAT
2
tool (Stenetorp et al., 2012). The guidelines were given
to the annotators and were explained in a group discussion were several examples were also analysed.
Subsequently annotators were given a small set of sentences to trial individually and their questions and
feedback led to a revised set of guidelines. Once this training phase was complete, the actual annotation
was carried out. The corpus was split randomly each part being annotated by at least two annotators.
2.1 Text genres
The corpus consists of sentences extracted from texts collected in the FIRST project and covers three
genres: newswire, healthcare and literature, with some additional sentences from the Penn Treebank
(Marcus et al., 1993). The corpus was developed to assist TS for people with ASD (Evans and Or?asan,
2013), following the notion that structurally complex constituents are explicitly indicated by signs of
complexity such as conjunctions, complementisers and punctuation marks. Evans and Or?asan (2013)
developed an annotation scheme and manually labeled these signs.
2
brat rapid annotation tool http://brat.nlplab.org/about.html
2
Figure 1 shows the interface of the annotation tool. For each annotated span, annotators were asked
to fill in three attributes: a) the type (relative, nominal, adjectival, verbal, prepositional), b) whether it is
restrictive (no, yes, unknown) and c) the annotator?s confidence (low, medium, high). The amount of data
in the corpus is listed for each genre in Table 1, i.e. number of sentences and tokens. On average, around
half of sentences contain an annotated span, but they occur more frequently in newswire and healthcare
than in literature.
Table 1: Corpora used and the total number of annotated spans
Genre (Corpus) Sentences Tokens Spans Span tokens Sent. len. Span len.
healthcare 1214 27379 958 6094 22.55 6.36
news (METER1) 1038 28367 732 5592 27.33 7.64
news (METER2) 1377 37515 1165 9203 27.24 7.90
literature 1946 48620 431 3834 24.98 8.90
News (Penn T.B.) 1733 39740 625 5652 22.93 9.04
Overall 7308 181621 3911 30375 24.85 7.77
2.2 Annotation guidelines
The annotation task involved tagging contiguous sequences of words that comprise post-modifiers of
nouns. These are syntactic constituents which follow the head noun in a complex noun phrase (NP), pro-
viding additional information about it. We are interested in those post-modifiers which provide additional
information but are not part of the parent clause and can be extracted from the sentence without chang-
ing its core meaning. These constituents can be either phrases or clauses and are typically bounded by
punctuation marks (such as commas, dashes, parentheses) or by functional words (prepositions, relative
pronouns, etc.).
The noun post-modifiers of interest are typically clauses or phrases rather than individual words, so
not every noun modifier should be marked. Typically they follow the noun phrase whose head they are
providing details about and cover several type of subordinated structures (appositions, relative clauses,
etc.) In the annotation, the most important aspect is to detect correctly the extent of the annotation (e.g.
include surrounding commas). The type is marked as an attribute and evaluated separately. Another
attribute indicates whether or not the modifier is a restrictive relative clause.
2.2.1 Restrictive and non-restrictive relative clauses
Restrictive modifiers serve to restrict or limit the set of possible referents of a phrase. In (1a), the subject
is restricted to one particular set of chips in a discourse model that may contain many different sets of
chips. In (1b), no such restriction is in effect. In this discourse model, all of the chips are made of gallium
arsenide and are fragile. Sentences containing restrictive noun post-modifiers require a different method
for conversion into a more accessible form than sentences containing non-restrictive noun post-modifiers.
(1) a. The chips made of gallium arsenide are very fragile (restrictive)
b. The chips, which are made of gallium arsenide, are very fragile (non-restrictive)
Deletion of the noun post-modifier from (1a) produces a sentence that is inconsistent in meaning with
the original. All chips, not just the set made of gallium arsenide, are then described as very fragile.
When converting sentences with restrictive post-modifiers, it is necessary to generate two sentences: one
to put the set of chips made of gallium arsenide into focus and to distinguish this set from the other sets
that exist in the discourse model and the other to assert the fact that this set of chips is very fragile. By
contrast, deletion of the post-modifier in (1b) produces a sentence that is still consistent in meaning with
the original.
In a particular context, it can be quite clear to understand if a specific entity is meant or whether a
restricted category of entities is referred to. Normally the sentence is read with two different intonations
3
to indicate the two different meanings (which is why commas usually mark non-restrictive clauses). The
presence, or absence of commas should be used to differentiate ambiguous cases in which not enough
context is available to decide which is the intended meaning.
(2) a. They visited two companies today: one in Manchester and one in Liverpool. The company
[which is located] in Manchester was remarkable. (restrictive)
b. They visited a company and a school. The company, [which is located] in Manchester, was
remarkable. (non-restrictive)
Restrictive relative clauses are also called integrated, defining or identifying relative clauses. Similarly,
non-restrictive relative clauses are called supplementary, appositive, non-defining or non-identifying rel-
ative clauses.
2.2.2 Types of noun post-modifier
Depending on their syntactic function, there are five types of noun post-modifier:
1. Relative clauses (usually marked by relative pronouns who(m), which, that). These finite clauses
are constituents of subclausal elements (noun phrases) within a superordinate clause. They differ from
other types of clause such as adverbial clauses, because they are not direct elements of the superordinate
clause.
3
They have only an indirect link to the main clause.
(3) A Bristol hospital that retained the hearts of 300 children who died in complex operations behaved
in a ?cavalier? fashion towards the parents, an inquiry was told yesterday.
(4) The florist [who was] sent the flowers was pleased.
2. Nominal-appositives (which are themselves NPs). Apposition is a relation holding between two
NPs (the appositives) in which one serves to define the other. The second NP commonly has a defining
role with regard to the first.
(5) Catherine Hawkins, regional general manager for the National Health Service in the South-west
until 1992, appeared before the inquiry yesterday without a solicitor - one should have been pro-
vided by the department.
(6) My wife, a nurse by training, has helped the accident victim.
(7) Goldwater, the junior senator from Arizona, received the Republican nomination in 1964.
3. Non-finite clauses (VP, typically start with an -ing participle or -ed participle verb). These clauses
have a non-finite verb and are non-restrictive.
4
These post-modifiers can be regarded as examples of
post-modification by a reduced relative clause. To illustrate, in example (8), the non-finite clause is a
reduction of the relative clause who was sitting across from the defendant.
(8) Assistant Chief Constable Robin Searle, sitting across from the defendant, said that the police had
suspected his involvement since 1997.
(9) Lord Melchett led a dawn raid on a farm in Norfolk, causing 17,400 of damage to a genetically
modified crop and disrupting a research programme, a court was told yesterday.
4. Prepositional phrase post-modification (PP, typically starting with a preposition). Similar to
non-finite clauses, post-modification by PPs, can be regarded as post-modification by ?reduced? relative
clauses. For example, the PP in example (10) can be considered a reduction of the relative clause who is
of Chelmsford, Essex.
3
In syntax, elements are the fundamental units of a clause: subject, verb, object, complement, or adverbial.
4
They occur in a different tone unit, typically bounded by commas, from the noun head that they modify. They do not
restrict or limit the set of possible referents of the complex noun phrase that they modify.
4
(10) Boe, of Chelmsford, Essex, admitted six fraud charges and asked for 35 similar offences to be taken
into consideration.
5. Adjectival post-modification (AP, including attributes such as height or age). Similar to non-
finite clauses, post-modification by adjectival phrases, can be regarded as post-modification by ?reduced?
relative clauses. For example, the adjectival phrase in example (11) below can be considered a reduction
of the relative clause who is 58 [years old].
(11) Stanley Cameron, 58 [years old], was convicted in August of 16 counts including vessel homicide
and driving under the influence in the November 1997 crash in Fort Lauderdale, Florida.
(12) Student Richard, 5ft 1Oins tall, has now left home.
2.3 Annotation insights
The corpus was split into chunks of roughly 100-150 sentences and each was annotated by 2 to 5 an-
notators. Sentences were randomly selected from the corpus based on length and the presence of signs
of complexity (conjunctions, commas, parentheses). Where annotated spans did not match, a reviewer
made a final adjudication.
The agreement on detecting the span of post-modifiers was relatively low, on average, the pairwise
F1 score was 54.90%. This is mainly because of the way annotators interpreted the instructions. For
example, some annotators systematically marked all parenthetical expressions whereas others never did
this. The most frequent error was omission of relevant post-modifiers; annotators typically reached
higher precision than recall. This suggests a systematic disagreement which affects files annotated by
few annotators. A way to address this problem is by aggregating all annotated spans for each document
and asking annotators to confirm which of them are indeed post-modifiers. This is being carried out in a
separate study, where a voting scheme is used to mitigate the recall problem.
Looking only at the cases where two annotators marked the same span as a post-modifier, we can
investigate the level of agreement reached on the individual attributes: type and restrictiveness. Anno-
tators reached high pairwise agreement (kappa=0.78) when marking the type of the post-modifier. This
suggests that the beginning of the post-modifiers has reliable markers which could be used to automat-
ically predict type. The pairwise agreement for restrictiveness is lower (kappa=0.51), but still good,
considering that the two values are not equally distributed (70% of post-modifiers are non restrictive).
Possible causes of this are: lack of context (sentences were extracted from their source documents), lack
of domain knowledge (where the post-modifers are not about entities, but specialised terminology, such
as symptoms, procedures, strategies).
Although agreement on the two attributes can be improved, the biggest challenge is to ensure that all
post-modifiers are annotated, i.e. to address situations when only one annotator marks a span. One com-
mon cause of disagreement concerns noun modifiers within the same NP, such as prepositional phrases.
For example:
(13) The $2.5 billion Byron 1 plant near Rockford was completed in 1985.
While this span modifies a noun, it is part of the NP itself, and it is arguably too short to be relevant
for rephrasing the sentence in a TS system; it is more likely a candidate for deletion as is the case with
sentence compression systems.
Another frequent issue concerns nested modifiers, where annotators usually marked only one of the
possible constituents. A related issue is how to deal with nested and overlapping spans, not only from
the point of view of the guidelines, but also in the way the annotation is used in practice.
(14) The new plant, located in Chinchon, about 60 miles from Seoul, will help meet increased demand.
An interesting debate concerns ambiguous constituents which can have several interpretations. In the
previous sentence, the second constituent about 60 miles from Seoul can be considered an apposition
modifying the proper noun Chinchon, or a prepositional phrase modifying the verb located; both entail a
5
similar meaning to a human reader. This example illustrates a situation which frequently occurs in natural
language text: for stylistic or editorial reasons writers omit words which are implied by the context. The
effect is that the syntactic structure becomes ambiguous, but the information communicated to the reader
is nevertheless unaffected. This issue also suggests that distinguishing the type of a post-modifier (i.e.
relative, nominal, adjectival, verbal, prepositional) only reflects its form and less so its role. For example,
it is easy to rephrase most post-modifiers as relative clauses, e.g.:
(15) a. Nominal-appositives: My wife, who is a nurse by training, has helped the accident victim.
b. Verbal-appositives: Lord Melchett led a dawn raid on a farm in Norfolk, which caused causing
17,400 of damage... , a court was told yesterday.
c. Prepositional-appositives: Boe, who lives in of Chelmsford, Essex, admitted six fraud charges
and asked for 35 similar offences to be taken into consideration.
d. Adjectival-appositives: Student Richard, who is 5ft 1Oins tall, has now left home.
During the annotation process there were several issues raised by the annotators, both seeking clarifi-
cations of the guidelines as well as identifying new situations in the corpus. A conclusion of the feedback
we gathered is that the most important decision is whether a post-modifier is restrictive or not, as this
will lead to different strategies for rephrasing the content in order to preserve the meaning as much as
possible. The type can be deduced based on the first tokens in the post-modifier.
3 Detection of relative clauses
In this paper we follow the sign complexity scheme introduced by Evans and Or?asan (2013), where
punctuation marks and functional words are considered explicit markers of coordinated and subordinated
constituents, the two syntactic mechanisms leading to structurally complex sentences.
The signs of syntactic complexity comprise conjunctions ([and], [but], [or]), a complementiser
([that]), wh-words ([what], [when], [where], [which], [while], [who]), punctuation marks ([,], [;],
[:]), and 30 compound signs consisting of one of these lexical items immediately preceded by a punctu-
ation mark (e.g. [, and]). These signs are automatically tagged with a label indicating type of constituent
they delimit, such as finite clauses (EV) or strict appositives (MN), and the position of the sign, such as
start/left boundary (SS*) or end/right boundary (ES*). For example, the label ESMA indicates end of an
adjectival phrase. An automatic tagger for signs of syntactic complexity was developed using a sequence
tagging approach (Dornescu et al., 2013) and is used in this work to select complex sentences from the
corpus and to provide linguistic information to the proposed approach.
The two baselines used are rule based systems for detecting post modifiers. System RC1 uses a set of
rules to detect appositives which are delimited by punctuation marks and do not contain any verbs. Such
expressions are typically nominal appositives or parenthetical expressions e.g.
(16) a. The chief financial officer, Gregory Barnum, announced the merger in an interview.
b. Oxygen can be given with a face mask or through little tubes (nasal cannulae or ?nasal specs?)
that sit just under your nostrils.
c. The business depends heavily on the creativity of its chief designer, Seymour Cray.
3.1 Rule-based approach
The second baseline used as a reference, DAPR (Detection of Adnominal Post-modifiers by Rules),
is a component of a text simplification system for people with autistic spectrum disorders (Evans et
al., 2014). Although the system can also rephrase complex sentences, in this paper we only used the
appositive constituents detected in a sentence by DAPR.
It employs several hand-crafted linguistic rules which detect the extent of appositions based on the
presence of signs of syntactic complexity, in this case punctuation marks, relative pronouns, etc. DAPR
6
exploits rules and patterns to convert sentences containing noun post-modifiers such as finite clauses
(EV), strict appositives (MN), adjective phrases (MA), prepositional phrases (MP), and non-restrictive
non-finite clauses (MV) into a more accessible form.
The conversion procedure is implemented as an iterative process. When a pattern matches the input
sentence, the detected post-modifier is deleted and the resulting sentence is then processed. The priority
of each pattern determines the order in which they are matched when processing sentences which contain
multiple left boundaries of relevant constituents (i.e. signs of complexity tagged with certain labels). The
patterns are implemented to match the first (leftmost) sign of syntactic complexity in the sentence.
The rules used to convert sentences containing noun post-modifiers exploit patterns to identify both
the noun post-modifier and the preceding part of the matrix NP, which can be used to re-phrase the post-
modifier as a coherent, stand-alone sentence. Table 3 provides examples of patterns and the strings that
they match for each class of signs serving as the left boundary of a noun post-modifier. The patterns are
expressed using terms described in Table 2.
Table 2: Terms used in the patterns
Element Description
w
v
Verbal words, including ?ed verbs tagged as adjectives
w
n
Nominal words
w
a
Adjectival words with POS tags JJ, JJS, and VBN
w
nmod
Nominal modifiers: adjectives, nouns
w
nspec
Nomimal specifiers: determiners, numbers, possessive pronouns
w
POS
Word with part-of-speech tag POS (from the Penn Treebank tagset (Santorini, 1990)
utilised by the part of speech tagger distributed with the LT TTT2 package)
CLASS Sign of syntactic complexity of functional class CLASS (Evans and Or?asan, 2013).
? Quotation marks
B-F Sequences of zero or more characters
Table 3: Rules used to used to detect noun post-modifiers
Type Rule Trigger pattern & Example
SSEV 61 w
IN
w
DT
* w
n
{wn|of}* SSEV w
VBD
C sb ?*
But he was chased for a mile-and-a-half by a passer-by who gave police a description of the Citroen
driver.
7 w
{n|DT}
* w
n
SSEV B ESCCV
Some staff at the factory, which employed 800 people, said they noticed cuts on his fingers.
SSMA 81 w
NNP
* w
NNP
SSMA w
{RB|CD}
* w
CD
ESMA w
VBD
Matthew?s pregnant mum Collette Jackson, 24, collapsed sobbing after the pair were sentenced.
83 w
NNP
* w
NNP
SSMA w
CD
ESMA
The court heard that Khattab, 25, a trainee pharmacist, confused double strength chloroform water
with concentrated chloroform.
SSMN 6 w
{NNP|NNPS}
* w
{n|a}
* w
n
SSMN B ESMN
Mr Justice Forbes told the pharmacists that both Mr Young and his girlfriend, Collette Jackson, 24, of
Runcorn, Cheshire, had been devastated by the premature loss of their son.
3 w
{DT|PRP$}
{w
{n|a}
|of}* w
n
SSMN B ESMN
Police became aware that a car, a VW Golf, was arriving in Nottingham from London.
SSMP 4 w
{NNP|NNPS}
* w
{NNP|NNPS}
SSMP ?* w
IN
B ESMP
Justin Rushbrooke, for the Times, said: ?We say libel it is, but it?s a very, very long way from being a
grave libel.
1 w
{NNP|NNPS}
* w
{NNP|NNPS}
{is|are|was|were} w
CD
SSMP w
IN
B ESMA
In the same case Stephen Warner, 33, of Nottingham, was jailed for five years for possession of heroin
with intent to supply.
SSMV 12 w
PRP
w
RB
* w
VBD
B SSMV w
RB
* w
VBG
C {sb|}
He attended anti-drugs meetings with Nottinghamshire police, sitting across from Assistant Chief
Constable Robin Searle.
2 w
{NNP|NNPS}
* w
{NNP|NNPS}
SSMV w
{VBG|VBN}
B ESMV
Andrew Easteal, prosecuting, said police had suspected Francis might be involved in drugs and had
begun to investigate him early last year.
The underlined examples in Table 3 mark only the noun post-modifier. The patterns also identify the
7
preceding part of the matrix NP (in square brackets in the example below). The rules include substitutions
of indefinite articles by demonstratives or definite articles. Following the method applied to sentences
containing noun post-modifiers, rule SSEV-63 would convert:
(17) But he was chased for a mile-and-a-half by a passer-by who gave police a description of the driver.
Into the more accessible sequence of sentences:
(18) a. But he was chased for a mile-and-a-half by [a passer-by].
b. [That passer-by] gave police a description of the driver.
3.2 ML-based approach
As many types of appositive modifiers are simple in structure, we also follow a tagging approach for
the task of detecting noun post-modifiers. We employ the common IOB2 format where the beginning of
each noun post-modifier is tagged as B-PM and tokens inside it are tagged as I-PM. All other tokens are
tagged as other: O. This is similar to a named entity recognition or to a chunking task where only one
type of entity/chunk is detected. For comparison we compare the performance of the approach with a
rule based method for detecting appositive post-modifiers.
The corpus was used to build two supervised tagging models based on Conditional Random Fields
(Lafferty et al., 2001): CRF++
5
and crfsuite
6
. Four feature sets were used. Model A contains standard
features used in chunking, such as word form, lemma and part of speech (POS) tag. Model B adds the
predictions of baseline system RC1 as an additional feature: using the IOB2 models, tokens have one
of three values: B-RC1, I-RC1 or O-RC1. Similarly, model C adds the predictions of baseline system
DAPR also using the IOB2 approach. This allows us to test whether the baseline systems are robust
enough to be employed as input to the sequence tagging models. Model D adds information about the
tokens of the sentence which are signs of syntactic complexity. These are produced automatically.
4 Results and analysis
Results reported by conlleval
7
, the standard tool for evaluating tagging, are presented in Table 4. Al-
though the two baselines, RC1 and DAPR, out-perform the CRF++ models, the best overall performance
is achieved by the crfsuite models.
The rules employed by the RC1 baseline can be misled by sentences containing enumerations, nu-
merical expressions and direct speech due to false positive matches. Although few and addressing the
simplest post-modifiers, the rules perform well.
The more complex baseline, DAPR, appears to be more conservative (it makes the fewest predictions
overall), which suggests it covers fewer types of appositions than covered by our dataset. Compared to
the previous baseline, DAPR detects more complex appositions and relative clauses with better precision,
but with reduced recall.
Although the CRF models also use as features the predictions made by the two baseline models, due
to the level of noise, the improvement is small, between 1 and 2 points. Adding information about the
tagged signs of syntactic complexity actually has a negative impact on both models, suggesting that the
signs are less relevant for this type of syntactic constituent. A large difference in performance is noted
between the two CRF tools: whereas CRF++ is outperformed by both baselines, crfsuite achieves much
better performance despite using the same input features.
To gain better insights into the performance of the best model, Table 5 presents label-wise results.
Given that the average length of a post-modifier is 7, inside tokens (I-PM) are 7 times more prevalent
than beginning tokens (B-PM). Despite this, the model achieves similar performance for both (F1 score
just below 0.60). The two tables also bring evidence suggesting that detecting the end token of a post-
modifier is challenging: although the start is correctly detected for 48.89% of appositives, only 39.94%
5
http://crfpp.googlecode.com/svn/trunk/doc/index.html
6
http://www.chokkan.org/software/crfsuite/
7
http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
8
Table 4: Results reported by conlleval on the test set (90076 tokens, 2098 annotated post-modifiers)
#predicted #correct
phrases phrases accuracy precision recall F1
RC1 baseline 1287 371 81.01 28.83 17.68 21.92
DAPR baseline 535 163 81.25 30.47 7.77 12.38
CRF++ A:word & POS 3372 289 85.48 8.57 13.78 10.57
+B:RC1 predictions 3381 315 85.66 9.32 15.01 11.50
+C:DAPR predictions 3586 319 85.63 8.90 15.20 11.22
+D:tagged signs 3680 319 85.60 8.67 15.20 11.04
crfsuite A:word & POS 1391 790 87.54 56.79 37.65 45.29
+B:RC1 predictions 1437 825 87.55 57.41 39.32 46.68
+C:DAPR predictions 1470 838 87.56 57.01 39.94 46.97
+D:tagged signs 1481 838 87.56 56.58 39.94 46.83
are a perfect match. This suggests that more work is necessary to improve the ability to detect post-
modifiers but also to better determine their correct extent. The second part is critical to the perceived
performance of the TS system, as incorrect detection usually leads to incorrect text being generated for
users, whereas a loss in recall may be transparent.
Table 5: Label-wise performance for the best model (crfsuite C)
label #match #model #ref precision recall F1
O 70452 77884 73955 90.46 95.26 92.80
B-PM 1014 1469 2074 69.03 48.89 57.24
I-PM 7406 10723 14047 69.07 52.72 59.80
Macro-average 76.18 65.63 69.95
5 Conclusions
The paper presents a new resource for syntactic text simplification, a corpus annotated with relative
clauses and appositions which can be used to develop and evaluate non-destructive simplification sys-
tems. These systems extract certain types of syntactic constituents and embedded clauses and rephrase
them as stand-alone sentences to generate less structurally complex text while preserving the meaning
intact. A supervised tagging model for automatic detection of appositions was built using the corpus and
will be included in a text simplification system.
Acknowledgements
The research described in this paper was partially funded by the European Commission under the Sev-
enth (FP7-2007-2013) Framework Programme for Research and Technological Development (FP7-ICT-
2011.5.5 FIRST 287607). We gratefully acknowledge the contributions of all the members of the FIRST
consortium for their feedback and comments, and to the annotators for their useful insights.
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but useful approach to conjunct identification. In Proceedings
of the 30th Annual Meeting on Association for Computational Linguistics, ACL ?92, pages 15?21, Stroudsburg,
PA, USA. Association for Computational Linguistics.
R. Chandrasekar, Christine Doran, and B. Srinivas. 1996. Motivation and Methods for Text Simplification. In
Proceedings of the 16th conference on Computational linguistics - Volume 2, pages 1041?1044.
9
Iustin Dornescu, Richard Evans, and Constantin Or?asan. 2013. A Tagging Approach to Identify Complex Con-
stituents for Text Simplification. In Galia Angelova, Kalina Bontcheva, and RuslanMitkov, editors, Proceedings
of Recent Advances in Natural Language Processing, RANLP?13, pages 221 ? 229, Hissar, Bulgaria. RANLP
2011 Organising Committee / ACL.
Richard Evans and Constantin Or?asan. 2013. Annotating signs of syntactic complexity to support sentence sim-
plification. In Ivan Habernal and Vclav Matou?sek, editors, Text, Speech, and Dialogue, volume 8082 of Lecture
Notes in Computer Science, pages 92?104. Springer Berlin Heidelberg.
Richard Evans, Constantin Or?asan, and Iustin Dornescu. 2014. An evaluation of syntactic simplification rules
for people with autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for
Target Reader Populations (PITR), pages 131?140, Gothenburg, Sweden, April. Association for Computational
Linguistics.
Richard J. Evans. 2011. Comparing methods for the syntactic simplification of sentences in information extraction.
LLC, 26(4):371?388.
Laurie Gerber and Eduard H. Hovy. 1998. Improving translation quality by manipulating sentence length. In
David Farwell, Laurie Gerber, and Eduard H. Hovy, editors, AMTA, volume 1529 of Lecture Notes in Computer
Science, pages 448?460. Springer.
M. A. Just, P. A. Carpenter, and K. R. Thulborn. 1996. Brain activation modulated by sentence comprehension.
Science, 274:114?116.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, pages 282?289.
J. Levy, E. Hoover, G. Waters, S. Kiran, D. Caplan, A. Berardino, and C. Sandberg. 2012. Effects of syntactic
complexity, semantic reversibility, and explicitness on discourse comprehension in persons with aphasia and in
healthy controls. American Journal of Speech?Language Pathology, 21(2):154 ? 165.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The penn treebank. Computational Linguistics, 19(2):313?330.
Juan Martos, Sandra Freire, Ana Gonzlez, David Gil, Richard Evans, Vesna Jordanova, Arlinda Cerga, Antoneta
Shishkova, and Constantin Orasan. 2013. User preferences: Updated report. Technical report, The FIRST
Consortium.
Ryan McDonald and Joakim Nivre. 2011. Analyzing and integrating dependency parsers. Comput. Linguist.,
37(1):197?230, March.
Thomas C. Rindflesch, Jayant V. Rajan, and Lawrence Hunter. 2000. Extracting molecular binding relationships
from biomedical text. In ANLP, pages 188?195.
Beatrice Santorini. 1990. Part-Of-Speech tagging guidelines for the Penn Treebank project (3rd revision, 2nd
printing). Technical report, Department of Linguistics, University of Pennsylvania, Philadelphia, PA, USA.
A. Siddharthan. 2002a. An architecture for a text simplification system. Language Engineering Conference, 2002.
Proceedings.
Advaith Siddharthan. 2002b. Resolving Attachment and Clause Boundary Ambiguities for Simplifying Relative
Clause Constructs. Association for Computational Linguistics Student Research Workshop, pages 60?65.
Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation,
4:77?109.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c, Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Brat: A web-based tool for nlp-assisted text annotation. In Proceedings of the Demonstrations at the 13th
Conference of the European Chapter of the Association for Computational Linguistics, EACL ?12, pages 102?
107, Stroudsburg, PA, USA. Association for Computational Linguistics.
Masaru Tomita. 1985. Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems. Kluwer
Academic Publishers, Norwell, MA, USA.
10
