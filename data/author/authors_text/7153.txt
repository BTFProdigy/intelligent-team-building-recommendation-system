Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 345?352,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Collaborative Framework for Collecting Thai Unknown Words from
the Web
Choochart Haruechaiyasak, Chatchawal Sangkeettrakarn, Pornpimon Palingoon
Sarawoot Kongyoung and Chaianun Damrongrat
Information Research and Development Division (RDI)
National Electronics and Computer Technology Center (NECTEC)
Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand
rdi5@nnet.nectec.or.th
Abstract
We propose a collaborative framework for
collecting Thai unknown words found on
Web pages over the Internet. Our main
goal is to design and construct a Web-
based system which allows a group of in-
terested users to participate in construct-
ing a Thai unknown-word open dictionary.
The proposed framework provides sup-
porting algorithms and tools for automati-
cally identifying and extracting unknown
words from Web pages of given URLs.
The system yields the result of unknown-
word candidates which are presented to
the users for verification. The approved
unknown words could be combined with
the set of existing words in the lexicon
to improve the performance of many NLP
tasks such as word segmentation, infor-
mation retrieval and machine translation.
Our framework includes word segmenta-
tion and morphological analysis modules
for handling the non-segmenting charac-
teristic of Thai written language. To take
advantage of large available text resource
on the Web, our unknown-word boundary
identification approach is based on the sta-
tistical string pattern-matching algorithm.
Keywords: Unknown words, open dictio-
nary, word segmentation, morphological
analysis, word-boundary detection.
1 Introduction
The advent of the Internet and the increasing pop-
ularity of the Web have altered many aspects of
natural language usage. As more people turn to the
Internet as a new communicating channel, the tex-
tual information has increased tremendously and
is also widely accessible. More importantly, the
available information is varied largely in terms of
topic difference and multi-language characteristic.
It is not uncommon to find a Web page written in
Thai lies adjacent to aWeb page written in English
via a hyperlink, or a Web page containing both
Thai and English languages. In order to perform
well in this versatile environment, an NLP system
must be adaptive enough to handle the variation in
language usage. One of the problems which re-
quires special attention is unknown words.
As with most other languages, unknown words
also play an extremely important role in Thai-
language NLP. Unknown words are viewed as one
of the problematic sources of degrading the per-
formance of traditional NLP applications such as
MT (Machine Translation), IR (Information Re-
trieval) and TTS (Text-To-Speech). Reduction in
the amount of unknown words or being able to
correctly identify unknown words in these sys-
tems would help increase the overall system per-
formance.
The problem of unknown words in Thai lan-
guage is perhaps more severe than in English or
other latin-based languages. As a result of the
information technology revolution, Thai people
have become more familiar with other foreign lan-
guages especially English. It is not uncommon to
hear a few English words over a course of con-
versation between two Thai people. The foreign
words along with other Thai named entities are
among the new words which are continuously cre-
ated and widely circulated. To write a foreign
word, the transliterated form of Thai alphabets is
often used. The Royal Institute of Thailand is the
official organization in Thailand who has respon-
345
sibility and authority in defining and approving the
use of new words. The process of defining a new
word is manual and time-consuming as each word
must be approved by a working group of linguists.
Therefore, this traditional approach of construct-
ing the lexicon is not a suitable solution, especially
for systems running on the Web environment.
Due to the inefficiency of using linguists in
defining new lexicon, there must be a way to au-
tomatically or at least semi-automatically collect
new unknown words. In this paper, we propose
a collaborative framework for collecting unknown
words from Web pages over the Internet. Our
main purpose is to design and construct a system
which automatically identifies and extracts un-
known words found on Web pages of given URLs.
The compiled list of unknown-word candidates is
to be verified by a group of participants. The ap-
proved unknown words are then added to the ex-
isting lexicon along with the other related infor-
mation such as meaning and POS (part of speech).
This paper focuses on the underlying algorithms
for supporting the process of identifying and ex-
tracting unknown words. The overall process is
composed of two steps: unknown-word detection
and unknown-word boundary identification. The
first step is to detect the locations of unknown-
word occurrences from a given text. Since Thai
language belongs to the class of non-segmenting
language group in which words are written contin-
uously without using any explicit delimiting char-
acter, detection of unknown words could be ac-
complished mainly by using a word-segmentation
algorithm with a morphological analysis. By us-
ing a dictionary-based word-segmentation algo-
rithm, locations of words which are not previ-
ously included in the dictionary will be easily de-
tected. These unknown words belong to the class
of explicit unknown words and often represent the
transliteration of foreign words.
The other class of unknown words is hidden
unknown words. This class includes new words
which are created through the combination of
some existing words in the lexicon. The hidden
unknown words are usually named entities such
as a person?s name and an organization?s name.
The hidden unknown words could be identified us-
ing the approaches such as n-gram generation and
phrase chunking. The scope of this paper focuses
only on the extraction of the explicit unknown
words. However, the design of our framework also
includes the extraction of hidden unknown words.
We will continue to explore this issue in our future
works.
Once the location of an unknown word is de-
tected, the second step involves the identification
of its boundary. Since we use the Web as our
main resource, we could take advantage of its large
availability of textual contents. We are interested
in collecting unknown words which occur more
than once throughout the corpus. Unknown words
which occur only once in the large corpus are not
considered as being significant. These words may
be unusual words which are not widely accepted,
or could be misspelling words. Using this assump-
tion, our approach for identifying the unknown-
word boundary is based on a statistical pattern-
matching algorithm. The basic idea is that the
same unknown word which occurs more than once
would likely to appear in different surrounding
contexts. Therefore, a group of characters which
form the unknown word could be extracted by an-
alyzing the string matching patterns.
To evaluate the effectiveness of our proposed
framework, experiments using a real data set col-
lected from the Web are performed. The experi-
ments are designed to test each of the two main
steps of the framework. Variation of morphologi-
cal analysis are tested for the unknown-word de-
tection. The detection rate of unknown words
were found to be as high as approximately 96%.
Three variations of string pattern-matching tech-
niques were tested for unknown-word boundary
identification. The identification accuracy was
found to be as high as approximately 36%. The
relatively low accuracy is not the major concern
since the unknown-word candidates are to be ver-
ified and corrected by users before they are ac-
tually added to the dictionary. The system is
implemented via the Web-browser environment
which provides user-friendly interface for verifi-
cation process.
The rest of this paper is organized as fol-
lows. The next section presents and discusses
related works previously done in the unknown-
word problem. Section 3 provides an overview
of unknown-word problem in the relation to the
word-segmentation process. Section 4 presents the
proposed framework with underlying algorithms
in details. Experiments are performed in Section
5 with results and discussion. The conclusion is
given in Section 6.
346
2 Previous Works
The research and study in unknown-word prob-
lem have been extensively done over the past
decades. Unknown words are viewed as prob-
lematic source in the NLP systems. Techniques
in identifying and extracting unknown words are
somewhat language-dependent. However, these
techniques could be classified into two major cat-
egories, one for segmenting languages and an-
other for non-segmenting languages. Segment-
ing languages, such as latin-based languages, use
delimiting characters to separate written words.
Therefore, once the unknown words are detected,
their boundaries could be identified relatively eas-
ily when compared to those for non-segmenting
languages.
Some examples of techniques involving
segmenting languages are listed as follows.
Toole (2000) used multiple decision trees to
identify names and misspellings in English texts.
Features used in constructing the decision trees
are, for example, POS (Part-Of-Speech), word
length, edit distance and character sequence
frequency. Similarly, a decision-tree approach
was used to solve the POS disambiguation
and unknown word guessing in (Orphanos and
Christodoulakis, 1999). The research in the
unknown-word problem for segmenting lan-
guages is also closely related to the extraction of
named entities. The difference of these techniques
to those in non-segmenting languages is that
the approach needs to parse the written text in
word-level as opposed to character-level.
The research in unknown-word problem for
non-segmenting languages is highly active for
Chinese and Japanese. Many approaches have
been proposed and experimented with. Asahara
and Matsumoto (2004) proposed a technique of
SVM-based chunking to identify unknown words
from Japanese texts. Their approach used a sta-
tistical morphological analyzer to segment texts
into segments. The SVM was trained by using
POS tags to identify the unknown-word bound-
ary. Chen and Ma (2002) proposed a practical
unknown word extraction system by considering
both morphological and statistical rule sets for
word segmentation. Chang and Su (1997) pro-
posed an unsupervised iterative method for ex-
tracting unknown lexicons from Chinese text cor-
pus. Their idea is to include the potential unknown
words to the augmented dictionary in order to im-
prove the word segmentation process. Their pro-
posed approach also includes both contextual con-
straints and the joint character association metric
to filter the unlikely unknown words. Other ap-
proaches to identify unknown words include sta-
tistical or corpus-based (Chen and Bai, 1998), and
the use of heuristic knowledge (Nie et al , 1995)
and contextual information (Khoo and Loh, 2002).
Some extensions to unknown-word identification
have been done. An example include the determi-
nation of POS for unknown words (Nakagawa et
al. , 2001).
The research in unknown words for Thai lan-
guage has not been widely done as in other lan-
guages. Kawtrakul et al (1997) used the combina-
tion of a statistical model and a set of context sen-
sitive rules to detect unknown words. Our frame-
work has a different goal from previous works. We
consider unknown-word problem as collaborative
task among a group of interested users. As more
textual content is provided to the system, new un-
known words could be extracted with more accu-
racy. Thus, our framework can be viewed as col-
laborative and statistical or corpus-based.
3 Unknown-Word Problem in Word
Segmentation Algorithms
Similar to Chinese, Japanese and Korea, Thai lan-
guage belongs to the class of non-segmenting lan-
guages in which words are written continuously
without using any explicit delimiting character.
To handle non-segmenting languages, the first re-
quired step is to perform word segmentation. Most
word segmentation algorithms use a lexicon or
dictionary to parse texts at the character-level. A
typical word segmentation algorithm yields three
types of results: known words, ambiguous seg-
ments, and unknown segments. Known words are
existing words in the lexicon. Ambiguous seg-
ments are caused by the overlapping of two known
words. Unknown segments are the combination of
characters which are not defined in the lexicon.
In this paper, we are interested in extracting
the unknown words with high precision and re-
call results. Three types of unknown words are
hidden, explicit and mixed (Kawtrakul et al ,
1997). Hidden unknown words are composed by
different words existing in the lexicon. To illus-
trate the idea, let us consider an unknown word
ABCD where A, B, C, and D represents individ-
ual characters. Suppose that AB and CD both ex-
347
ist in a dictionary, then ABCD is considered as
a hidden unknown word. The explicit unknown
words are newly created words by using differ-
ent characters. Let us again consider an unknown
word ABCD. Suppose that there is no substring
of ABCD (i.e., AB, BC, CD, ABC, BCD) exists in
the dictionary, then ABCD is considered as explicit
unknown words. The mixed unknown words are
composed of both existing words in a dictionary
and non-existing substrings. From the example of
unknown string ABCD, if there is at least one sub-
string of ABCD (i.e., AB, BC, CD, ABC, BCD) ex-
ists in the dictionary, then ABCD is considered as
a mixed unknown word.
It can be immediately seen that the detection of
the hidden unknown words are not trivial since the
parser would mistakenly assume that all the frag-
ments of the words are valid, i.e., previously de-
fined in the dictionary. In this paper, we limit our-
self to the extraction of the explicit and mixed un-
known words. This type of unknown words usu-
ally represent the transliteration of foreign words.
Detection of these unknown words could be ac-
complished mainly by using a word-segmentation
algorithm with a morphological analysis. By using
a dictionary-based word-segmentation algorithm,
locations of words which are not previously de-
fined in the lexicon could be easily detected.
4 The Proposed Framework
The overall framework is shown in Figure 1.
Two major components are information agent and
unknown-word analyzer. The details of each com-
ponent are given as follows.
? Information agent: This module is com-
posed of a Web crawler and an HTML parser.
It is responsible for collecting HTML sources
from the given URLs and extracting the tex-
tual data from the pages. Our framework is
designed to support multi-user and collabora-
tive environment. The advantage of this de-
sign approach is that unknown words could
be collected and verified more efficiently.
More importantly, it allows users to select the
Web pages which suit their interests.
? Unknown-word analyzer: This module is
composed of many components for analyzing
and extracting unknown words. Word seg-
mentation module receives text strings from
the information agent and segments them
into a list of words. N-gram generation
module is responsible for generating hidden
unknown-word candidates. Morphological
analysis module is used to form initial ex-
plicit unknown-word segments. String pat-
tern matching unit performs unknown-word
boundary identification task. It takes the
intermediate unknown segments and iden-
tifies their boundaries by analyzing string
matching patterns The results are processed
unknown-word candidates which are pre-
sented to linguists for final post-processing
and verification. New unknown words are
combined with the dictionary to iteratively
improve the performance of the word seg-
mentation module. Details of each compo-
nent are given in the following subsections.
4.1 Unknown-Word Detection
As previously mentioned in Section 3, applying
a word-segmentation algorithm on a text string
yields three different segmented outputs: known,
ambiguous, and unknown segments. Since our
goal is to simply detect the unknown segments
without solving or analyzing other related issues
in word segmentation, using the longest-matching
word segmentation algorithm previously proposed
by Poowarawan (1986) is sufficient. An exam-
ple to illustrate the word-segmentation process is
given as follows.
Let the following string denotes a
text string written in Thai language:
{a1a2...aib1b2...bjc1c2...ck}. Suppose that
{a1a2...ai} and {c1c2...ck} are known words
from the dictionary, and {b1b2...bj} be an un-
known word. For the explicit unknown-word
case, applying the word-segmentation algo-
rithm would yield the following segments:
{a1a2...ai}{b1}{b2}...{bj}{c1c2...ck}. It can be
observed that the detected unknown positions for
a single unknown word are individual characters
in the unknown word itself. Based on the initial
statistical analysis of a Thai lexicon, it was found
that the averaged number of characters in a word
is equal to 7. This characteristic is quite different
from other non-segmenting languages such as
Chinese and Japanese in which a word could
be a character or a combination of only a few
characters. Therefore, to reduce the complexity
in unknown-word boundary identification task,
the unknown segments could be merged to
form multiple-character segments. For exam-
348
    
      	 
        
      
  
   
	 
    
  
  
    
  Proceedings of the 8th Workshop on Asian Language Resources, pages 64?71,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Constructing Thai Opinion Mining Resource:
A Case Study on Hotel Reviews
Choochart Haruechaiyasak, Alisa Kongthon,
Pornpimon Palingoon and Chatchawal Sangkeettrakarn
Human Language Technology Laboratory (HLT)
National Electronics and Computer Technology Center (NECTEC)
choochart.har@nectec.or.th, alisa.kon@nectec.or.th,
pornpimon.pal@nectec.or.th, chatchawal.san@nectec.or.th
Abstract
Opinion mining and sentiment analy-
sis has recently gained increasing atten-
tion among the NLP community. Opin-
ion mining is considered a domain-
dependent task. Constructing lexicons
for different domains is labor intensive.
In this paper, we propose a framework
for constructing Thai language resource
for feature-based opinion mining. The
feature-based opinion mining essentially
relies on the use of two main lexicons,
features and polar words. Our approach
for extracting features and polar words
from opinionated texts is based on syn-
tactic pattern analysis. The evaluation
is performed with a case study on ho-
tel reviews. The proposed method has
shown to be very effective in most cases.
However, in some cases, the extraction
is not quite straightforward. The rea-
sons are due to, firstly, the use of conver-
sational language in written opinionated
texts and, secondly, the language seman-
tic. We provide discussion with possible
solutions on pattern extraction for some
of the challenging cases.
1 Introduction
With the popularity of Web 2.0 or social net-
working websites, the amount of user-generated
contents has increased exponentially. One in-
teresting type of these user-generated contents
is texts which are written with some opinions
and/or sentiments. An in-depth analysis of these
opinionated texts could reveal potentially use-
ful information regarding the preferences of peo-
ple towards many different topics including news
events, social issues and commercial products.
Opinion mining and sentiment analysis is such
task for analyzing and summarizing what people
think about a certain topic.
Due to its potential and useful applications,
opinion mining has gained a lot of interest in text
mining and NLP communities (Ding et al, 2008;
Jin et al, 2009). Much work in this area focused
on evaluating reviews as being positive or nega-
tive either at the document level (Turney, 2002;
Pang et al, 2002; Dave et al, 2003; Beineke
et al, 2004) or sentence level (Kim and Hovy,
2004; Wiebe and Riloff, 2005; Wilson et al,
2009; Yu and Hatzivassiloglou, 2003). For in-
stance, given some reviews of a product, the sys-
tem classifies them into positive or negative re-
views. No specific details or features are identi-
fied about what customers like or dislike. To ob-
tain such details, a feature-based opinion mining
approach has been proposed (Hu and Liu, 2004;
Popescu and Etzioni, 2005). This approach typi-
cally consists of two following steps.
1. Identifying and extracting features of an ob-
ject, topic or event from each sentence upon
which the reviewers expressed their opin-
ion.
2. Determining whether the opinions regard-
ing the features are positive or negative.
The feature-based opinion mining could pro-
vide users with some insightful information re-
lated to opinions on a particular topic. For exam-
ple, for hotel reviews, the feature-based opinion
64
mining allows users to view positive or negative
opinions on hotel-related features such as price,
service, breakfast, room, facilities and activities.
Breaking down opinions into feature level is very
essential for decision making. Different cus-
tomers could have different preferences when se-
lecting hotels to stay for vacation. For example,
some might prefer hotels which provide full fa-
cilities, however, some might prefer to have good
room service.
The main drawback of the feature-based opin-
ion mining is the preparation of different lex-
icons including features and polar words. To
make things worse, these lexicons, especially the
features, are domain-dependent. For a partic-
ular domain, a set of features and polar words
must be prepared. The process for language
resource construction is generally labor inten-
sive and time consuming. Some previous works
have proposed different approaches for automat-
ically constructing the lexicons for the feature-
based opinion mining (Qiu et al, 2009; Riloff
and Wiebe, 2003; Sarmento et al, 2009). Most
approaches applied some machine learning al-
gorithms for learning the rules from the corpus.
The rules are used for extracting new features
and polar words from untagged corpus. Reviews
of different approaches are given in the related
work section.
In this paper, we propose a framework for
constructing Thai language resource for the
feature-based opinion mining. Our approach
is based on syntactic pattern analysis of two
lexicon types: domain-dependent and domain-
independent. The domain-dependent lexicons
include features, sub-features and polar words.
The domain-independent lexicons are particles,
negative words, degree words, auxiliary verbs,
prepositions and stop words. Using these lexi-
cons, we could construct a set of syntactic rules
based on the frequently occurred patterns. The
rule set can be used for extracting more unseen
sub-features and polar words from untagged cor-
pus.
We evaluated the proposed framework on the
domain of hotel reviews. The experimental re-
sults showed that our proposed method is very
effective in most cases, especially for extracting
polar words. However, in some cases, the extrac-
tion is not quite straightforward due to the use of
conversational language, idioms and hidden se-
mantic. We provide some discussion on the chal-
lenging cases and suggest some solutions as the
future work.
The remainder of this paper is organized as
follows. In next section, we review some related
works on different approaches for constructing
language resources for opinion mining and sen-
timent analysis. In Section 3, we present the pro-
posed framework for constructing Thai opinion
mining resource by using the dual pattern extrac-
tion method. In Section 4, we apply the proposed
framework with a case study of hotel reviews.
The performance evaluation is given with the ex-
periment results. Some difficult cases are dis-
cussed along with some possible solutions. Sec-
tion 5 concludes the paper with the future work.
2 Related work
The problem of developing subjectivity lexicons
for training and testing sentiment classifiers has
recently attracted some attention. The Multi-
perspective Question Answering (MPQA) opin-
ion corpus is a well-known resource for senti-
ment analysis in English (Wiebe et al, 2005).
It is a collection of news articles from a vari-
ety of news sources manually annotated at word
and phrase levels for opinions and other private
states (i.e., beliefs, emotions, sentiments, spec-
ulations, etc.). The annotation in this work also
took into account the context, which is essential
for resolving possible ambiguities and accurately
determining polarity.
Although most of the reference corpora has
been focused on English language, work on other
languages is growing as well. Kanayama et
al. (2006) proposed an unsupervised method to
detect sentiment words in Japanese. In this work,
they used clause level context coherency to iden-
tify candidate sentiment words from sentences
that appear successively with sentences contain-
ing seed sentiment words. Their assumption is
that unless the context is changed with adver-
sative expressions, sentences appearing together
65
in that context tend to have the same polari-
ties. Hence, if one of them contains sentiments
words, the other successive sentences are likely
to contain sentiment words as well. Ku and
Chen (2007) proposed the bag-of-characters ap-
proach to determine sentiment words in Chinese.
This approach calculates the observation proba-
bilities of characters from a set of seed sentiment
words first, then dynamically expands the set and
adjusts their probabilities. Later in 2009, Ku
et al (2009), extended their bag-of-characters
approach by including morphological structures
and syntactic structures between sentence seg-
ment. Their experiments showed better perfor-
mance of word polarity detection and opinion
sentence extraction.
Some other methods to automatically gener-
ate resources for subjectivity analysis for a for-
eign language have leveraged the resources and
tools available for English. For example, Be-
nea et al (2008) applied machine translation and
standard Naive Bayes and SVM for subjectiv-
ity classification for Romanian. Their exper-
iments showed promising results for applying
automatic translation to construct resources and
tools for opinion mining in a foreign language.
Wan (2009) also leveraged an available English
corpus for Chinese sentiment classification by
using the co-training approach to make full use
of both English and Chinese features in a uni-
fied framework. Jijkoun and Hofmann (2009)
also described a method for creating a Dutch
subjectivity lexicon based on an English lexi-
con. They applied a PageRank-like algorithm
that bootstraps a subjectivity lexicon from the
translation of the English lexicon and rank the
words in the thesaurus by polarity using the net-
work of lexical relations (e.g., synonymy, hy-
ponymy) in Wordnet.
3 The proposed framework
The performance of the feature-based opinion
mining relies on the design and completeness
of related lexicons. Our lexicon design dis-
tinguishes lexicons into two types, domain-
dependent and domain-independent. The design
of domain-dependent lexicons is based on the
feature-based opinion mining framework pro-
posed by Liu et al (2005). The framework starts
by setting the domain scope such as digital cam-
era. The next step is to design a set of features
associated with the given domain. For the do-
main of digital camera, features could be, for in-
stance, ?price?, ?screen size? and ?picture qual-
ity?. Features could contain sub-features. For
example, the picture quality could have the sub-
features as ?macro mode?, ?portrait mode? and
?night mode?. Preparing multiple feature levels
could be time-consuming, therefore, we limit the
features into two levels: main features and sub-
features.
Another domain-dependent lexicon is po-
lar words. Polar words are sentiment words
which represent either positive or negative views
on features. Although some polar words are
domain-independent and have explicit meanings
such as ?excellent?, ?beautiful?, ?expensive?
and ?terrible?. Some polar words are domain-
dependent and have implicit meanings depend-
ing on the contexts. For example, the word
?large? is generally considered positive for the
screen size feature of digital camera domain.
However, for the dimension feature of mobile
phone domain, the word ?large? could be con-
sidered as negative.
On the other hand, the domain-independent
lexicons are regular words which provide differ-
ent parts of speech (POS) and functions in the
sentence. For opinion mining task, we design
six different domain-independent lexicons as fol-
lows (some examples are shown in Table 1).
? Particles (PAR): In Thai language, these
words refer to the sentence endings which
are normally used to add politeness of the
speakers (Cooke, 1992).
? Negative words (NEG): Like English,
these words are used to invert the opinion
polarity. Examples are ?not?, ?unlikely?
and ?never?.
? Degree words (DEG): These words are
used as an intensifier to the polar words.
Examples are ?large?, ?very?, ?enormous?.
66
? Auxiliary verbs (AUX): These words are
used to modify verbs. Examples are
?should?, ?must? and ?then?.
? Prepositions (PRE): Like English, Thai
prepositions are used to mark the relations
between two words.
? Stop words (STO): These words are used
for grammaticalization. Thai language is
considered an isolating language, to form a
noun the words ?karn? and ?kwam? are nor-
mally placed in front of a verb or a noun,
respectively. Therefore, these words could
be neglected when analyzing opinionated
texts.
Table 1: Domain-independent lexicons
Although some of the above lexicons are sim-
ilar to English, however, some words are placed
in different position in a sentence. For example,
in Thai, a degree word is usually placed after a
polar word. For example, ?very good? would be
written as ?good very? in Thai.
Figure 1 shows all processes and work flow
under the proposed framework. The process
starts with a corpus which is tagged based on
two lexicon types. From the tagged corpus,
we construct patterns and lexicons. The pat-
tern construction is performed by collecting text
segments which contain both features and polar
words. All patterns are sorted by the frequency
of occurrence. The lexicon construction is per-
formed by simply collecting words which are al-
ready tagged with the lexicon types. The lex-
icons are used for performing the feature-based
opinion mining task such as classifying and sum-
marizing the reviews as positive and negative
based on different features. The completeness of
lexicons is very important for the feature-based
opinion mining. To collect more lexicons, pat-
terns are used in the dual pattern extraction pro-
cess to extract more features and polar words
from the untagged corpus.
Figure 1: The proposed opinion resource con-
struction framework based on the dual pattern
extraction.
4 A case study of hotel reviews
To evaluate the proposed framework, we per-
form some experiments with a case study of
hotel reviews. In Thailand, tourism is ranked
as one of the top industries. From the statis-
tics provided by the Office of Tourism Develop-
ment1, the number of international tourists vis-
iting Thailand in 2009 is approximately 14 mil-
1The Office of Tourism Development,
http://www.tourism.go.th
67
lions. The number of registered hotels in all re-
gions of Thailand is approximately 5,000. Pro-
viding an opinion mining system on hotel re-
views could be very useful for tourists to make
decision on hotel choice when planning a trip.
4.1 Corpus preparation
We collected customer reviews from the Agoda
website2. The total number of reviews in the cor-
pus is 8,436. Each review contains the name of
the hotel as the title and comments in free-form
text format. We designed a set of 13 main fea-
tures: service, cleanliness, hotel condition, loca-
tion, food, breakfast, room, facilities, price, com-
fort, quality, activities and security. The set of
main features is designed based on the features
obtained from the Agoda website. Some addi-
tional features, such as activities and security, are
added to provide users with more dimensions.
In this paper, we focus on two main fea-
tures: breakfast and service. Table 2 shows the
domain-dependent lexicons related to the break-
fast feature. For breakfast main feature (FEA),
we include all synonyms which could be used to
describe breakfast in Thai. These include En-
glish terms with their synonyms, transliterated
terms and abbreviations.
The breakfast sub-features (FEA*) are spe-
cific concepts of breakfast. Examples include
?menu?, ?taste?, ?service? and ?coffee?. It can
be observed that some of the sub-features could
also act as a main feature. For example, the
sub-feature ?service? of breakfast is also used
as the main feature ?service?. Providing sub-
feature level could help revealing more insight-
ful dimension for the users. However, designing
multiple feature levels could be time-consuming,
therefore, we limit the features into two levels,
i.e., main feature and sub-feature. The polar
words (POL) are also shown in the table. We
denote the positive and negative polar words by
placing [+] and [-] after each word. It can be
observed that some polar words are dependent
on sub-features. For example, the polar word
?long line? can only be used for the sub-feature
?restaurant?.
2Agoda website: http://www.agoda.com
Table 2: Domain-dependent lexicons for the
breakfast feature.
Table 3 shows the domain-dependent lexicons
related to the service feature. The main fea-
tures include synonyms, transliterated and En-
glish terms which describe the concept service.
The service sub-features are, for example, ?re-
ception?, ?security guard?, ?maid?, ?waiter? and
?concierge?. Unlike the breakfast feature, the
polar words for the service feature are quite gen-
eral and could mostly be applied for all sub-
features. Another observation is that some of the
polar words are based on Thai idiom. For ex-
ample, the phrase ?having rigid hands? in Thai
means ?impolite?. In Thai culture, people show
politeness by doing the ?wai? gesture.
4.2 Experiments and results
Using the tagged corpus and the extracted lexi-
cons, we construct the most frequently occurred
patterns. For two main features, breakfast and
service, the numbers of tagged reviews for each
feature are 301 and 831, respectively. We ran-
domly split the corpus into 80% as training set
and 20% as test set. We only consider the pat-
terns which contain both features (either main
features or sub-features) and polar words. For
the breakfast feature, the total number of ex-
tracted patterns is 86. For the service feature, the
total number of extracted patterns is 192. Table
4 and 5 show some examples of most frequently
68
Table 3: Domain-dependent lexicons for the ser-
vice feature.
occurred patterns extracted from the corpus. The
symbols of the tag set are as shown in Table 1
and 2 with the tag <OTH> denoting any other
words.
From the tables, two patterns which occur fre-
quently for both features are <FEA><POL>
and <FEA*><POL>. These two patterns are
very simple and show that the opinionated texts
in Thai are mostly very simple. Users just simply
write a word describing the feature followed by
a polar word (either positive or negative) with-
out using any verb in between. Some examples
for the pattern <FEA*><POL> are <coffee
cup><dirty> and <employee><friendly>. In
English, a verb ?to be? (is/am/are) is usually re-
quired between <FEA*> and <POL>.
Using the extracted patterns, we perform the
dual pattern extraction process to collect the
sub-features and polar words from the test data
set. Table 6 shows the evaluation results of
sub-features and polar words extraction for both
breakfast and service features. It can be observed
that the set of patterns could extract polar words
(POL) with higher accuracy than sub-features
(FEA*). This could be due to the patterns used to
Table 4: Top-ranked breakfast patterns with ex-
amples
describe the polar words are straightforward and
not complicated. This is especially true for the
case of breakfast feature in which the accuracy
is approximately 95%.
Table 5: Top-ranked service patterns with exam-
ples
4.3 Discussion
Table 7 and 8 show some examples of challeng-
ing cases for breakfast and service features, re-
spectively. The polar words shown in both tables
are very difficult to extract since the patterns can
69
Feature Accuracy (%)FEA* POL
Breakfast 80.00 95.74
Service 82.56 89.29
Table 6: Evaluation results of features and polar
words extraction.
not be easily captured. The difficulties are due to
many reasons including the language semantic
and the need of world knowledge. For example,
in case #5 of service feature, the whole phrase
can be interpreted as ?attentive?. It is difficult
for the system to generate the pattern based on
this phrase. Another example is case #4 of both
tables, the customers express their opinions by
comparing to other hotels. To analyze the senti-
ment correctly would require the knowledge of a
particular hotel or hotels in specific locations.
Table 7: Examples of difficult cases of breakfast
feature
5 Conclusion and future work
We proposed a framework for constructing Thai
opinion mining resource with a case study on
hotel reviews. Two sets of lexicons, domain-
dependent and domain-independent, are de-
signed to support the pattern extraction process.
The proposed method first constructs a set of pat-
terns from a tagged corpus. The extracted pat-
terns are then used to automatically extract and
collect more sub-features and polars words from
an untagged corpus. The performance evaluation
Table 8: Examples of difficult cases of service
feature
was done with a collection of hotel reviews ob-
tained from a hotel reservation website. From
the experimental results, polar words could be
extracted more easily than sub-features. This
is due to the polar words often appear in spe-
cific positions with repeated contexts in the opin-
ionated texts. In some cases, extraction of sub-
features and polar words are not straightforward
due to the difficulties in generalizing patterns.
For example, some subjectivity requires com-
plete phrases to describe the polarity. In some
cases, the sub-features are not explicitly shown
in the sentence. For future work, we plan to com-
plete the construction of the corpus by consider-
ing the rest of main features. Another plan is to
include the semantic analysis into the pattern ex-
traction process. For example, the phrase ?forget
something? could imply negative polarity for the
service feature.
References
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. Proc. of the
2008 empirical methods in natural language pro-
cessing, 127?135.
Beineke, Philip, Trevor Hastie and Shivakumar
Vaithyanathan. 2004. The sentimental factor: im-
proving review classification via human-provided
information. Proc. of the 42nd Annual Meeting on
Association for Computational Linguistics, 263?
270.
70
Cooke, J.R. 1992. Thai sentence particles: putting
the puzzle together. Proc. of the The Third Inter-
national Symposium on Language and Linguistics,
1105?1119.
Dave, Kushal, Steve Lawrence and David M. Pen-
nock. 2003. Mining the peanut gallery: opinion
extraction and semantic classification of product
reviews. Proc. of the 12th international confer-
ence on World Wide Web, 519?528.
Ding, Xiaowen, Bing Liu and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
Proc. of the int. conf. on web search and web data
mining, 231?240.
Hu, Minqing and Bing Liu. 2004. Mining and sum-
marizing customer reviews. Proc. of the 10th ACM
SIGKDD international conference on Knowledge
discovery and data mining, 168?177.
Jin, Wei, Hung Hay Ho and Rohini K. Srihari. 2009.
OpinionMiner: a novel machine learning system
for web opinion mining and extraction. Proc. of
the 15th ACM SIGKDD, 1195?1204.
Kim, Soo-Min and Eduard Hovy. 2004. Determining
the sentiment of opinions. Proc. of the 20th inter-
national conference on Computational Linguistics,
1367?1373.
Qiu, Guang, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon
through double propagation. Proc. of the 21st In-
ternational Joint Conferences on Artificial Intelli-
gence, 1199?1204.
Jijkoun, Valentin and Katja Hofmann. 2009. Gener-
ating a non-English subjectivity lexicon: relations
that matter. Proc. of the 12th Conference of the
European Chapter of the Association for Compu-
tational Linguistics, 398?405.
Kanayama, Hiroshi and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. Proc. of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, 355?363.
Ku, Lun-Wei and Hsin-Hsi Chen. 2007 Mining opin-
ions from the Web: Beyond relevance retrieval.
Journal of American Society for Information Sci-
ence and Technology, 58(12):1838?1850.
Ku, Lun-Wei, Ting-Hao Huang and Hsin-Hsi Chen.
2009. Using morphological and syntactic struc-
tures for Chinese opinion analysis. Proc. of the
2009 empirical methods in natural language pro-
cessing, 1260?1269.
Liu, Bing, Minqing Hu and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the Web. Proc. of the 14th World Wide
Web, 342?351.
Pang, Bo, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. Proc. of the ACL-
02 conf. on empirical methods in natural language
processing, 79?86.
Popescu, Ana-Maria and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. Proc. of the conf. on human language tech-
nology and empirical methods in natural language
processing, 339?346.
Riloff, Ellen and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. Proc.
of the 2003 conference on empirical methods in
natural language processing, 105?112.
Sarmento, Lu??s, Paula Carvalho, Ma?rio J. Silva, and
Euge?nio de Oliveira. 2009. Automatic creation
of a reference corpus for political opinion min-
ing in user-generated content. Proc. of the 1st
CIKM workshop on topic-sentiment analysis for
mass opinion, 29?36.
Turney, Peter D. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised clas-
sification of reviews. Proc. of the 40th ACL, 417?
424.
Wan, Xiaojun. 2009. Co-training for cross-lingual
sentiment classification. Proc. of the joint conf. of
ACL and IJCNLP, 235?243.
Wiebe, Janyce and Ellen Riloff. 2005. Creating
subjective and objective sentence classifiers from
unannotated texts. Proc. of Conference on Intelli-
gent Text Processing and Computational Linguis-
tics, 486?497.
Wiebe, Janyce, Theresa Wilson and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2-3):165?210.
Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Comput. Linguist., 35(3):399?433.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating
facts from opinions and identifying the polarity of
opinion sentences. Proc. of the Conference on Em-
pirical Methods in Natural Language Processing,
129?136.
71
