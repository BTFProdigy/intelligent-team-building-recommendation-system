Mining WordNet for Fuzzy Sentiment:
Sentiment Tag Extraction from WordNet Glosses
Alina Andreevskaia and Sabine Bergler
Concordia University
Montreal, Quebec, Canada
{andreev, bergler}@encs.concordia.ca
Abstract
Many of the tasks required for semantic
tagging of phrases and texts rely on a list
of words annotated with some semantic
features. We present a method for ex-
tracting sentiment-bearing adjectives from
WordNet using the Sentiment Tag Extrac-
tion Program (STEP). We did 58 STEP
runs on unique non-intersecting seed lists
drawn from manually annotated list of
positive and negative adjectives and evalu-
ated the results against other manually an-
notated lists. The 58 runs were then col-
lapsed into a single set of 7, 813 unique
words. For each word we computed a
Net Overlap Score by subtracting the total
number of runs assigning this word a neg-
ative sentiment from the total of the runs
that consider it positive. We demonstrate
that Net Overlap Score can be used as a
measure of the words degree of member-
ship in the fuzzy category of sentiment:
the core adjectives, which had the high-
est Net Overlap scores, were identified
most accurately both by STEP and by hu-
man annotators, while the words on the
periphery of the category had the lowest
scores and were associated with low rates
of inter-annotator agreement.
1 Introduction
Many of the tasks required for effective seman-
tic tagging of phrases and texts rely on a list of
words annotated with some lexical semantic fea-
tures. Traditional approaches to the development
of such lists are based on the implicit assumption
of classical truth-conditional theories of meaning
representation, which regard all members of a cat-
egory as equal: no element is more of a mem-
ber than any other (Edmonds, 1999). In this pa-
per, we challenge the applicability of this assump-
tion to the semantic category of sentiment, which
consists of positive, negative and neutral subcate-
gories, and present a dictionary-based Sentiment
Tag Extraction Program (STEP) that we use to
generate a fuzzy set of English sentiment-bearing
words for the use in sentiment tagging systems 1.
The proposed approach based on the fuzzy logic
(Zadeh, 1987) is used here to assign fuzzy sen-
timent tags to all words in WordNet (Fellbaum,
1998), that is it assigns sentiment tags and a degree
of centrality of the annotated words to the senti-
ment category. This assignment is based on Word-
Net glosses. The implications of this approach for
NLP and linguistic research are discussed.
2 The Category of Sentiment as a Fuzzy
Set
Some semantic categories have clear membership
(e.g., lexical fields (Lehrer, 1974) of color, body
parts or professions), while others are much more
difficult to define. This prompted the development
of approaches that regard the transition frommem-
bership to non-membership in a semantic category
as gradual rather than abrupt (Zadeh, 1987; Rosch,
1978). In this paper we approach the category of
sentiment as one of such fuzzy categories where
some words ? such as good, bad ? are very cen-
tral, prototypical members, while other, less cen-
tral words may be interpreted differently by differ-
ent people. Thus, as annotators proceed from the
core of the category to its periphery, word mem-
1Sentiment tagging is defined here as assigning positive,
negative and neutral labels to words according to the senti-
ment they express.
209
bership in this category becomes more ambiguous,
and hence, lower inter-annotator agreement can be
expected for more peripheral words. Under the
classical truth-conditional approach the disagree-
ment between annotators is invariably viewed as a
sign of poor reliability of coding and is eliminated
by ?training? annotators to code difficult and am-
biguous cases in some standard way. While this
procedure leads to high levels of inter-annotator
agreement on a list created by a coordinated team
of researchers, the naturally occurring differences
in the interpretation of words located on the pe-
riphery of the category can clearly be seen when
annotations by two independent teams are com-
pared. The Table 1 presents the comparison of GI-
H4 (General Inquirer Harvard IV-4 list, (Stone et
al., 1966)) 2 and HM (from (Hatzivassiloglou and
McKeown, 1997) study) lists of words manually
annotated with sentiment tags by two different re-
search teams.
GI-H4 HM
List composition nouns, verbs,
adj., adv.
adj. only
Total list size 8, 211 1, 336
Total adjectives 1, 904 1, 336
Tags assigned Positiv, Nega-
tiv or no tag
Positive
or Nega-
tive
Adj. with 1, 268 1, 336
non-neutral tags
Intersection 774 (55% 774 (58%
(% intersection) of GI-H4 adj) of HM)
Agreement on tags 78.7%
Table 1: Agreement between GI-H4 and HM an-
notations on sentiment tags.
The approach to sentiment as a category with
fuzzy boundaries suggests that the 21.3% dis-
agreement between the two manually annotated
lists reflects a natural variability in human an-
notators? judgment and that this variability is re-
lated to the degree of centrality and/or relative im-
portance of certain words to the category of sen-
timent. The attempts to address this difference
2The General Inquirer (GI) list used in this study was
manually cleaned to remove duplicate entries for words with
same part of speech and sentiment. Only the Harvard IV-4
list component of the whole GI was used in this study, since
other lists included in GI lack the sentiment annotation. Un-
less otherwise specified, we used the full GI-H4 list including
the Neutral words that were not assigned Positiv or Negativ
annotations.
in importance of various sentiment markers have
crystallized in two main approaches: automatic
assignment of weights based on some statistical
criterion ((Hatzivassiloglou and McKeown, 1997;
Turney and Littman, 2002; Kim and Hovy, 2004),
and others) or manual annotation (Subasic and
Huettner, 2001). The statistical approaches usu-
ally employ some quantitative criterion (e.g., mag-
nitude of pointwise mutual information in (Turney
and Littman, 2002), ?goodness-for-fit? measure in
(Hatzivassiloglou and McKeown, 1997), probabil-
ity of word?s sentiment given the sentiment if its
synonyms in (Kim and Hovy, 2004), etc.) to de-
fine the strength of the sentiment expressed by a
word or to establish a threshold for the member-
ship in the crisp sets 3 of positive, negative and
neutral words. Both approaches have their limi-
tations: the first approach produces coarse results
and requires large amounts of data to be reliable,
while the second approach is prohibitively expen-
sive in terms of annotator time and runs the risk of
introducing a substantial subjective bias in anno-
tations.
In this paper we seek to develop an approach
for semantic annotation of a fuzzy lexical cate-
gory and apply it to sentiment annotation of all
WordNet words. The sections that follow (1) de-
scribe the proposed approach used to extract sen-
timent information from WordNet entries using
STEP (Semantic Tag Extraction Program) algo-
rithm, (2) discuss the overall performance of STEP
on WordNet glosses, (3) outline the method for
defining centrality of a word to the sentiment cate-
gory, and (4) compare the results of both automatic
(STEP) and manual (HM) sentiment annotations
to the manually-annotated GI-H4 list, which was
used as a gold standard in this experiment. The
comparisons are performed separately for each of
the subsets of GI-H4 that are characterized by a
different distance from the core of the lexical cat-
egory of sentiment.
3 Sentiment Tag Extraction from
WordNet Entries
Word lists for sentiment tagging applications can
be compiled using different methods. Automatic
methods of sentiment annotation at the word level
can be grouped into two major categories: (1)
corpus-based approaches and (2) dictionary-based
3We use the term crisp set to refer to traditional, non-
fuzzy sets
210
approaches. The first group includes methods
that rely on syntactic or co-occurrence patterns
of words in large texts to determine their senti-
ment (e.g., (Turney and Littman, 2002; Hatzivas-
siloglou and McKeown, 1997; Yu and Hatzivas-
siloglou, 2003; Grefenstette et al, 2004) and oth-
ers). The majority of dictionary-based approaches
use WordNet information, especially, synsets and
hierarchies, to acquire sentiment-marked words
(Hu and Liu, 2004; Valitutti et al, 2004; Kim
and Hovy, 2004) or to measure the similarity
between candidate words and sentiment-bearing
words such as good and bad (Kamps et al, 2004).
In this paper, we propose an approach to senti-
ment annotation of WordNet entries that was im-
plemented and tested in the Semantic Tag Extrac-
tion Program (STEP). This approach relies both
on lexical relations (synonymy, antonymy and hy-
ponymy) provided in WordNet and on the Word-
Net glosses. It builds upon the properties of dic-
tionary entries as a special kind of structured text:
such lexicographical texts are built to establish se-
mantic equivalence between the left-hand and the
right-hand parts of the dictionary entry, and there-
fore are designed to match as close as possible the
components of meaning of the word. They have
relatively standard style, grammar and syntactic
structures, which removes a substantial source of
noise common to other types of text, and finally,
they have extensive coverage spanning the entire
lexicon of a natural language.
The STEP algorithm starts with a small set of
seed words of known sentiment value (positive
or negative). This list is augmented during the
first pass by adding synonyms, antonyms and hy-
ponyms of the seed words supplied in WordNet.
This step brings on average a 5-fold increase in
the size of the original list with the accuracy of the
resulting list comparable to manual annotations
(78%, similar to HM vs. GI-H4 accuracy). At the
second pass, the system goes through all WordNet
glosses and identifies the entries that contain in
their definitions the sentiment-bearing words from
the extended seed list and adds these head words
(or rather, lexemes) to the corresponding category
? positive, negative or neutral (the remainder). A
third, clean-up pass is then performed to partially
disambiguate the identified WordNet glosses with
Brill?s part-of-speech tagger (Brill, 1995), which
performs with up to 95% accuracy, and eliminates
errors introduced into the list by part-of-speech
ambiguity of some words acquired in pass 1 and
from the seed list. At this step, we also filter out
all those words that have been assigned contradict-
ing, positive and negative, sentiment values within
the same run.
The performance of STEP was evaluated using
GI-H4 as a gold standard, while the HM list was
used as a source of seed words fed into the sys-
tem. We evaluated the performance of our sys-
tem against the complete list of 1904 adjectives in
GI-H4 that included not only the words that were
marked as Positiv, Negativ, but also those that were
not considered sentiment-laden by GI-H4 annota-
tors, and hence were by default considered neutral
in our evaluation. For the purposes of the evalua-
tion we have partitioned the entire HM list into 58
non-intersecting seed lists of adjectives. The re-
sults of the 58 runs on these non-intersecting seed
lists are presented in Table 2. The Table 2 shows
that the performance of the system exhibits sub-
stantial variability depending on the composition
of the seed list, with accuracy ranging from 47.6%
to 87.5% percent (Mean = 71.2%, Standard Devi-
ation (St.Dev) = 11.0%).
Average Average
run size % correct
# of adj StDev % StDev
PASS 1 103 29 78.0% 10.5%
(WN Relations)
PASS 2 630 377 64.5% 10.8%
(WN Glosses)
PASS 3 435 291 71.2% 11.0%
(POS clean-up)
Table 2: Performance statistics on STEP runs.
The significant variability in accuracy of the
runs (Standard Deviation over 10%) is attributable
to the variability in the properties of the seed list
words in these runs. The HM list includes some
sentiment-marked words where not all meanings
are laden with sentiment, but also the words where
some meanings are neutral and even the words
where such neutral meanings are much more fre-
quent than the sentiment-laden ones. The runs
where seed lists included such ambiguous adjec-
tives were labeling a lot of neutral words as sen-
timent marked since such seed words were more
likely to be found in the WordNet glosses in their
more frequent neutral meaning. For example, run
# 53 had in its seed list two ambiguous adjectives
1
dim and plush, which are neutral in most of the
contexts. This resulted in only 52.6% accuracy
(18.6% below the average). Run # 48, on the
other hand, by a sheer chance, had only unam-
biguous sentiment-bearing words in its seed list,
and, thus, performed with a fairly high accuracy
(87.5%, 16.3% above the average).
In order to generate a comprehensive list cov-
ering the entire set of WordNet adjectives, the 58
runs were then collapsed into a single set of unique
words. Since many of the clearly sentiment-laden
adjectives that form the core of the category of
sentiment were identified by STEP in multiple
runs and had, therefore, multiple duplicates in the
list that were counted as one entry in the com-
bined list, the collapsing procedure resulted in
a lower-accuracy (66.5% - when GI-H4 neutrals
were included) but much larger list of English ad-
jectives marked as positive (n = 3, 908) or neg-
ative (n = 3, 905). The remainder of WordNet?s
22, 141 adjectives was not found in any STEP run
and hence was deemed neutral (n = 14, 328).
Overall, the system?s 66.5% accuracy on the
collapsed runs is comparable to the accuracy re-
ported in the literature for other systems run on
large corpora (Turney and Littman, 2002; Hatzi-
vassiloglou and McKeown, 1997). In order to
make a meaningful comparison with the results
reported in (Turney and Littman, 2002), we also
did an evaluation of STEP results on positives and
negatives only (i.e., the neutral adjectives from GI-
H4 list were excluded) and compared our labels to
the remaining 1266 GI-H4 adjectives. The accu-
racy on this subset was 73.4%, which is compara-
ble to the numbers reported by Turney and Littman
(2002) for experimental runs on 3, 596 sentiment-
marked GI words from different parts of speech
using a 2x109 corpus to compute point-wise mu-
tual information between the GI words and 14
manually selected positive and negative paradigm
words (76.06%).
The analysis of STEP system performance
vs. GI-H4 and of the disagreements between man-
ually annotated HM and GI-H4 showed that
the greatest challenge with sentiment tagging of
words lies at the boundary between sentiment-
marked (positive or negative) and sentiment-
neutral words. The 7% performance gain (from
66.5% to 73.4%) associated with the removal of
neutrals from the evaluation set emphasizes the
importance of neutral words as a major source of
sentiment extraction system errors 4. Moreover,
the boundary between sentiment-bearing (positive
or negative) and neutral words in GI-H4 accounts
for 93% of disagreements between the labels as-
signed to adjectives in GI-H4 and HM by two in-
dependent teams of human annotators. The view
taken here is that the vast majority of such inter-
annotator disagreements are not really errors but
a reflection of the natural ambiguity of the words
that are located on the periphery of the sentiment
category.
4 Establishing the degree of word?s
centrality to the semantic category
The approach to sentiment category as a fuzzy
set ascribes the category of sentiment some spe-
cific structural properties. First, as opposed to the
words located on the periphery, more central ele-
ments of the set usually have stronger and more
numerous semantic relations with other category
members 5. Second, the membership of these cen-
tral words in the category is less ambiguous than
the membership of more peripheral words. Thus,
we can estimate the centrality of a word in a given
category in two ways:
1. Through the density of the word?s relation-
ships with other words ? by enumerating its
semantic ties to other words within the field,
and calculating membership scores based on
the number of these ties; and
2. Through the degree of word membership am-
biguity ? by assessing the inter-annotator
agreement on the word membership in this
category.
Lexicographical entries in the dictionaries, such
as WordNet, seek to establish semantic equiva-
lence between the word and its definition and pro-
vide a rich source of human-annotated relation-
ships between the words. By using a bootstrap-
ping system, such as STEP, that follows the links
between the words in WordNet to find similar
words, we can identify the paths connecting mem-
bers of a given semantic category in the dictionary.
With multiple bootstrapping runs on different seed
4It is consistent with the observation by Kim and Hovy
(2004) who noticed that, when positives and neutrals were
collapsed into the same category opposed to negatives, the
agreement between human annotators rose by 12%.
5The operationalizations of centrality derived from the
number of connections between elements can be found in so-
cial network theory (Burt, 1980)
212
lists, we can then produce a measure of the den-
sity of such ties. The ambiguity measure de-
rived from inter-annotator disagreement can then
be used to validate the results obtained from the
density-based method of determining centrality.
In order to produce a centrality measure, we
conducted multiple runs with non-intersecting
seed lists drawn from HM. The lists of words
fetched by STEP on different runs partially over-
lapped, suggesting that the words identified by the
system many times as bearing positive or negative
sentiment are more central to the respective cate-
gories. The number of times the word has been
fetched by STEP runs is reflected in the Gross
Overlap Measure produced by the system. In
some cases, there was a disagreement between dif-
ferent runs on the sentiment assigned to the word.
Such disagreements were addressed by comput-
ing the Net Overlap Scores for each of the found
words: the total number of runs assigning the word
a negative sentiment was subtracted from the to-
tal of the runs that consider it positive. Thus, the
greater the number of runs fetching the word (i.e.,
Gross Overlap) and the greater the agreement be-
tween these runs on the assigned sentiment, the
higher the Net Overlap Score of this word.
The Net Overlap scores obtained for each iden-
tified word were then used to stratify these words
into groups that reflect positive or negative dis-
tance of these words from the zero score. The zero
score was assigned to (a) the WordNet adjectives
that were not identified by STEP as bearing posi-
tive or negative sentiment 6 and to (b) the words
with equal number of positive and negative hits
on several STEP runs. The performance measures
for each of the groups were then computed to al-
low the comparison of STEP and human annotator
performance on the words from the core and from
the periphery of the sentiment category. Thus, for
each of the Net Overlap Score groups, both auto-
matic (STEP) and manual (HM) sentiment annota-
tions were compared to human-annotated GI-H4,
which was used as a gold standard in this experi-
ment.
On 58 runs, the system has identified 3, 908
English adjectives as positive, 3, 905 as nega-
tive, while the remainder (14, 428) of WordNet?s
22, 141 adjectives was deemed neutral. Of these
14, 328 adjectives that STEP runs deemed neutral,
6The seed lists fed into STEP contained positive or neg-
ative, but no neutral words, since HM, which was used as a
source for these seed lists, does not include any neutrals.
Figure 1: Accuracy of word sentiment tagging.
884 were also found in GI-H4 and/or HM lists,
which allowed us to evaluate STEP performance
and HM-GI agreement on the subset of neutrals as
well. The graph in Figure 1 shows the distribution
of adjectives by Net Overlap scores and the aver-
age accuracy/agreement rate for each group.
Figure 1 shows that the greater the Net Over-
lap Score, and hence, the greater the distance of
the word from the neutral subcategory (i.e., from
zero), the more accurate are STEP results and the
greater is the agreement between two teams of hu-
man annotators (HM and GI-H4). On average,
for all categories, including neutrals, the accuracy
of STEP vs. GI-H4 was 66.5%, human-annotated
HM had 78.7% accuracy vs. GI-H4. For the words
with Net Overlap of ?7 and greater, both STEP
and HM had accuracy around 90%. The accu-
racy declined dramatically as Net Overlap scores
approached zero (= Neutrals). In this category,
human-annotated HM showed only 20% agree-
ment with GI-H4, while STEP, which deemed
these words neutral, rather than positive or neg-
ative, performed with 57% accuracy.
These results suggest that the two measures of
word centrality, Net Overlap Score based on mul-
tiple STEP runs and the inter-annotator agreement
(HM vs. GI-H4), are directly related 7. Thus, the
Net Overlap Score can serve as a useful tool in
the identification of core and peripheral members
of a fuzzy lexical category, as well as in predic-
7In our sample, the coefficient of correlation between the
two was 0.68. The Absolute Net Overlap Score on the sub-
groups 0 to 10 was used in calculation of the coefficient of
correlation.
213
tion of inter-annotator agreement and system per-
formance on a subgroup of words characterized by
a given Net Overlap Score value.
In order to make the Net Overlap Score measure
usable in sentiment tagging of texts and phrases,
the absolute values of this score should be nor-
malized and mapped onto a standard [0, 1] inter-
val. Since the values of the Net Overlap Score
may vary depending on the number of runs used in
the experiment, such mapping eliminates the vari-
ability in the score values introduced with changes
in the number of runs performed. In order to ac-
complish this normalization, we used the value of
the Net Overlap Score as a parameter in the stan-
dard fuzzy membership S-function (Zadeh, 1975;
Zadeh, 1987). This function maps the absolute
values of the Net Overlap Score onto the interval
from 0 to 1, where 0 corresponds to the absence of
membership in the category of sentiment (in our
case, these will be the neutral words) and 1 reflects
the highest degree of membership in this category.
The function can be defined as follows:
S(u;?, ?, ?) =
?
?
?
?
?
?
?
0 for u ? ?
2(u??
???
)2 for? ? u ? ?
1? 2(u??
???
)2 for ? ? u ? ?
1 for u ? ?
where u is the Net Overlap Score for the word
and ?, ?, ? are the three adjustable parameters: ?
is set to 1, ? is set to 15 and ?, which represents a
crossover point, is defined as ? = (? + ?)/2 = 8.
Defined this way, the S-function assigns highest
degree of membership (=1) to words that have the
the Net Overlap Score u ? 15. The accuracy vs.
GI-H4 on this subset is 100%. The accuracy goes
down as the degree of membership decreases and
reaches 59% for values with the lowest degrees of
membership.
5 Discussion and conclusions
This paper contributes to the development of NLP
and semantic tagging systems in several respects.
? The structure of the semantic category of
sentiment. The analysis of the category
of sentiment of English adjectives presented
here suggests that this category is structured
as a fuzzy set: the distance from the core
of the category, as measured by Net Over-
lap scores derived from multiple STEP runs,
is shown to affect both the level of inter-
annotator agreement and the system perfor-
mance vs. human-annotated gold standard.
? The list of sentiment-bearing adjectives. The
list produced and cross-validated by multiple
STEP runs contains 7, 814 positive and neg-
ative English adjectives, with an average ac-
curacy of 66.5%, while the human-annotated
list HM performed at 78.7% accuracy vs.
the gold standard (GI-H4) 8. The remaining
14, 328 adjectives were not identified as sen-
timent marked and therefore were considered
neutral.
The stratification of adjectives by their Net
Overlap Score can serve as an indicator
of their degree of membership in the cate-
gory of (positive/negative) sentiment. Since
low degrees of membership are associated
with greater ambiguity and inter-annotator
disagreement, the Net Overlap Score value
can provide researchers with a set of vol-
ume/accuracy trade-offs. For example, by
including only the adjectives with the Net
Overlap Score of 4 and more, the researcher
can obtain a list of 1, 828 positive and nega-
tive adjectives with accuracy of 81% vs. GI-
H4, or 3, 124 adjectives with 75% accuracy
if the threshold is set at 3. The normalization
of the Net Overlap Score values for the use in
phrase and text-level sentiment tagging sys-
tems was achieved using the fuzzy member-
ship function that we proposed here for the
category of sentiment of English adjectives.
Future work in the direction laid out by this
study will concentrate on two aspects of sys-
tem development. First further incremental
improvements to the precision of the STEP
algorithm will be made to increase the ac-
curacy of sentiment annotation through the
use of adjective-noun combinatorial patterns
within glosses. Second, the resulting list of
adjectives annotated with sentiment and with
the degree of word membership in the cate-
gory (as measured by the Net Overlap Score)
will be used in sentiment tagging of phrases
and texts. This will enable us to compute the
degree of importance of sentiment markers
found in phrases and texts. The availability
8GI-H4 contains 1268 and HM list has 1336 positive and
negative adjectives. The accuracy figures reported here in-
clude the errors produced at the boundary with neutrals.
214
of the information on the degree of central-
ity of words to the category of sentiment may
improve the performance of sentiment deter-
mination systems built to identify the senti-
ment of entire phrases or texts.
? System evaluation considerations. The con-
tribution of this paper to the development
of methodology of system evaluation is two-
fold. First, this research emphasizes the im-
portance of multiple runs on different seed
lists for a more accurate evaluation of senti-
ment tag extraction system performance. We
have shown how significantly the system re-
sults vary, depending on the composition of
the seed list.
Second, due to the high cost of manual an-
notation and other practical considerations,
most bootstrapping and other NLP systems
are evaluated on relatively small manually
annotated gold standards developed for a
given semantic category. The implied as-
sumption is that such a gold standard repre-
sents a random sample drawn from the pop-
ulation of all category members and hence,
system performance observed on this gold
standard can be projected to the whole se-
mantic category. Such extrapolation is not
justified if the category is structured as a lex-
ical field with fuzzy boundaries: in this case
the precision of both machine and human an-
notation is expected to fall when more pe-
ripheral members of the category are pro-
cessed. In this paper, the sentiment-bearing
words identified by the system were stratified
based on their Net Overlap Score and eval-
uated in terms of accuracy of sentiment an-
notation within each stratum. These strata,
derived from Net Overlap scores, reflect the
degree of centrality of a given word to the
semantic category, and, thus, provide greater
assurance that system performance on other
words with the same Net Overlap Score will
be similar to the performance observed on the
intersection of system results with the gold
standard.
? The role of the inter-annotator disagree-
ment. The results of the study presented in
this paper call for reconsideration of the role
of inter-annotator disagreement in the devel-
opment of lists of words manually annotated
with semantic tags. It has been shown here
that the inter-annotator agreement tends to
fall as we proceed from the core of a fuzzy
semantic category to its periphery. There-
fore, the disagreement between the annota-
tors does not necessarily reflect a quality
problem in human annotation, but rather a
structural property of the semantic category.
This suggests that inter-annotator disagree-
ment rates can serve as an important source
of empirical information about the structural
properties of the semantic category and can
help define and validate fuzzy sets of seman-
tic category members for a number of NLP
tasks and applications.
References
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
R.S. Burt. 1980. Models of network structure. Annual
Review of Sociology, 6:79?141.
Philip Edmonds. 1999. Semantic representations of
near-synonyms for automatic lexical choice. Ph.D.
thesis, University of Toronto.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Gregory Grefenstette, Yan Qu, David A. Evans, and
James G. Shanahan. 2004. Validating the Cover-
age of Lexical Resources for Affect Analysis and
Automatically Classifying New Words along Se-
mantic Axes. In Yan Qu, James Shanahan, and
Janyce Wiebe, editors, Exploring Attitude and Af-
fect in Text: Theories and Applications, AAAI-2004
Spring Symposium Series, pages 71?78.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In 35th ACL, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD-04, pages 168?
177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. UsingWordNet to measure
semantic orientation of adjectives. In LREC 2004,
volume IV, pages 1115?1118.
Soo-Min Kim and Edward Hovy. 2004. Determining
the sentiment of opinions. In COLING-2004, pages
1367?1373, Geneva, Switzerland.
215
Adrienne Lehrer. 1974. Semantic Fields and Lexi-
cal Structure. North Holland, Amsterdam and New
York.
Eleanor Rosch. 1978. Principles of Categorization. In
Eleanor Rosch and Barbara B. Lloyd, editors, Cog-
nition and Categorization, pages 28?49. Lawrence
Erlbaum Associates, Hillsdale, New Jersey.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M.
Ogilvie. 1966. The General Inquirer: a computer
approach to content analysis. M.I.T. studies in com-
parative politics. M.I.T. Press, Cambridge, MA.
Pero Subasic and Alison Huettner. 2001. Affect Anal-
ysis of Text Using Fuzzy Typing. IEEE-FS, 9:483?
496.
Peter Turney and Michael Littman. 2002. Un-
supervised learning of semantic orientation from
a hundred-billion-word corpus. Technical Report
ERC-1094 (NRC 44929), National Research Coun-
cil of Canada.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing Affective Lexical Re-
sources. PsychNology Journal, 2(1):61?83.
Hong Yu and Vassileios Hatzivassiloglou. 2003. To-
wards Answering Opinion Questions: Separating
Facts from Opinions and Identifying the Polarity of
Opinion Sentences. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-
03).
Lotfy A. Zadeh. 1975. Calculus of Fuzzy Restric-
tions. In L.A. Zadeh, K.-S. Fu, K. Tanaka, and
M. Shimura, editors, Fuzzy Sets and their Applica-
tions to cognitive and decision processes, pages 1?
40. Academic Press Inc., New-York.
Lotfy A. Zadeh. 1987. PRUF ? a Meaning Rep-
resentation Language for Natural Languages. In
R.R. Yager, S. Ovchinnikov, R.M. Tong, and H.T.
Nguyen, editors, Fuzzy Sets and Applications: Se-
lected Papers by L.A. Zadeh, pages 499?568. John
Wiley & Sons.
216
Proceedings of ACL-08: HLT, pages 290?298,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
When Specialists and Generalists Work Together: Overcoming Domain
Dependence in Sentiment Tagging
Alina Andreevskaia
Concordia University
Montreal, Quebec
andreev@cs.concordia.ca
Sabine Bergler
Concordia University
Montreal, Canada
bergler@cs.concordia.ca
Abstract
This study presents a novel approach to the
problem of system portability across differ-
ent domains: a sentiment annotation system
that integrates a corpus-based classifier trained
on a small set of annotated in-domain data
and a lexicon-based system trained on Word-
Net. The paper explores the challenges of sys-
tem portability across domains and text gen-
res (movie reviews, news, blogs, and product
reviews), highlights the factors affecting sys-
tem performance on out-of-domain and small-
set in-domain data, and presents a new sys-
tem consisting of the ensemble of two classi-
fiers with precision-based vote weighting, that
provides significant gains in accuracy and re-
call over the corpus-based classifier and the
lexicon-based system taken individually.
1 Introduction
One of the emerging directions in NLP is the de-
velopment of machine learning methods that per-
form well not only on the domain on which they
were trained, but also on other domains, for which
training data is not available or is not sufficient to
ensure adequate machine learning. Many applica-
tions require reliable processing of heterogeneous
corpora, such as the World Wide Web, where the
diversity of genres and domains present in the Inter-
net limits the feasibility of in-domain training. In
this paper, sentiment annotation is defined as the
assignment of positive, negative or neutral senti-
ment values to texts, sentences, and other linguistic
units. Recent experiments assessing system porta-
bility across different domains, conducted by Aue
and Gamon (2005), demonstrated that sentiment an-
notation classifiers trained in one domain do not per-
form well on other domains. A number of methods
has been proposed in order to overcome this system
portability limitation by using out-of-domain data,
unlabelled in-domain corpora or a combination of
in-domain and out-of-domain examples (Aue and
Gamon, 2005; Bai et al, 2005; Drezde et al, 2007;
Tan et al, 2007).
In this paper, we present a novel approach to the
problem of system portability across different do-
mains by developing a sentiment annotation sys-
tem that integrates a corpus-based classifier with
a lexicon-based system trained on WordNet. By
adopting this approach, we sought to develop a
system that relies on both general and domain-
specific knowledge, as humans do when analyzing
a text. The information contained in lexicographi-
cal sources, such as WordNet, reflects a lay person?s
general knowledge about the world, while domain-
specific knowledge can be acquired through classi-
fier training on a small set of in-domain data.
The first part of this paper reviews the extant lit-
erature on domain adaptation in sentiment analy-
sis and highlights promising directions for research.
The second part establishes a baseline for system
evaluation by drawing comparisons of system per-
formance across four different domains/genres -
movie reviews, news, blogs, and product reviews.
The final, third part of the paper presents our sys-
tem, composed of an ensemble of two classifiers ?
one trained on WordNet glosses and synsets and the
other trained on a small in-domain training set.
290
2 Domain Adaptation in Sentiment
Research
Most text-level sentiment classifiers use standard
machine learning techniques to learn and select fea-
tures from labeled corpora. Such approaches work
well in situations where large labeled corpora are
available for training and validation (e.g., movie re-
views), but they do not perform well when training
data is scarce or when it comes from a different do-
main (Aue and Gamon, 2005; Read, 2005), topic
(Read, 2005) or time period (Read, 2005). There are
two alternatives to supervised machine learning that
can be used to get around this problem: on the one
hand, general lists of sentiment clues/features can be
acquired from domain-independent sources such as
dictionaries or the Internet, on the other hand, unsu-
pervised and weakly-supervised approaches can be
used to take advantage of a small number of anno-
tated in-domain examples and/or of unlabelled in-
domain data.
The first approach, using general word lists au-
tomatically acquired from the Internet or from dic-
tionaries, outperforms corpus-based classifiers when
such classifiers use out-of-domain training data or
when the training corpus is not sufficiently large to
accumulate the necessary feature frequency infor-
mation. But such general word lists were shown to
perform worse than statistical models built on suf-
ficiently large in-domain training sets of movie re-
views (Pang et al, 2002). On other domains, such
as product reviews, the performance of systems that
use general word lists is comparable to the perfor-
mance of supervised machine learning approaches
(Gamon and Aue, 2005).
The recognition of major performance deficien-
cies of supervised machine learning methods with
insufficient or out-of-domain training brought about
an increased interest in unsupervised and weakly-
supervised approaches to feature learning. For in-
stance, Aue and Gamon (2005) proposed training
on a samll number of labeled examples and large
quantities of unlabelled in-domain data. This sys-
tem performed well even when compared to sys-
tems trained on a large set of in-domain examples:
on feedback messages from a web survey on knowl-
edge bases, Aue and Gamon report 73.86% accu-
racy using unlabelled data compared to 77.34% for
in-domain and 72.39% for the best out-of-domain
training on a large training set.
Drezde et al (2007) applied structural corre-
spondence learning (Drezde et al, 2007) to the task
of domain adaptation for sentiment classification of
product reviews. They showed that, depending on
the domain, a small number (e.g., 50) of labeled
examples allows to adapt the model learned on an-
other corpus to a new domain. However, they note
that the success of such adaptation and the num-
ber of necessary in-domain examples depends on
the similarity between the original domain and the
new one. Similarly, Tan et al (2007) suggested to
combine out-of-domain labeled examples with unla-
belled ones from the target domain in order to solve
the domain-transfer problem. They applied an out-
of-domain-trained SVM classifier to label examples
from the target domain and then retrained the classi-
fier using these new examples. In order to maximize
the utility of the examples from the target domain,
these examples were selected using Similarity Rank-
ing and Relative Similarity Ranking algorithms (Tan
et al, 2007). Depending on the similarity between
domains, this method brought up to 15% gain com-
pared to the baseline SVM.
Overall, the development of semi-supervised ap-
proaches to sentiment tagging is a promising direc-
tion of the research in this area but so far, based
on reported results, the performance of such meth-
ods is inferior to the supervised approaches with in-
domain training and to the methods that use general
word lists. It also strongly depends on the similarity
between the domains as has been shown by (Drezde
et al, 2007; Tan et al, 2007).
3 Factors Affecting System Performance
The comparison of system performance across dif-
ferent domains involves a number of factors that can
significantly affect system performance ? from train-
ing set size to level of analysis (sentence or entire
document), document domain/genre and many other
factors. In this section we present a series of experi-
ments conducted to assess the effects of different ex-
ternal factors (i.e., factors unrelated to the merits of
the system itself) on system performance in order to
establish the baseline for performance comparisons
across different domains/genres.
291
3.1 Level of Analysis
Research on sentiment annotation is usually con-
ducted at the text (Aue and Gamon, 2005; Pang et
al., 2002; Pang and Lee, 2004; Riloff et al, 2006;
Turney, 2002; Turney and Littman, 2003) or at the
sentence levels (Gamon and Aue, 2005; Hu and Liu,
2004; Kim and Hovy, 2005; Riloff et al, 2006). It
should be noted that each of these levels presents dif-
ferent challenges for sentiment annotation. For ex-
ample, it has been observed that texts often contain
multiple opinions on different topics (Turney, 2002;
Wiebe et al, 2001), which makes assignment of the
overall sentiment to the whole document problem-
atic. On the other hand, each individual sentence
contains a limited number of sentiment clues, which
often negatively affects the accuracy and recall if
that single sentiment clue encountered in the sen-
tence was not learned by the system.
Since the comparison of sentiment annotation
system performance on texts and on sentences
has not been attempted to date, we also sought
to close this gap in the literature by conducting
the first set of our comparative experiments on
data sets of 2,002 movie review texts and 10,662
movie review snippets (5331 with positive and
5331 with negative sentiment) provided by Bo Pang
(http://www.cs.cornell.edu/People/pabo/movie-
review-data/).
3.2 Domain Effects
The second set of our experiments explores system
performance on different domains at sentence level.
For this we used four different data sets of sentences
annotated with sentiment tags:
? A set of movie review snippets (further: movie)
from (Pang and Lee, 2005). This dataset of
10,662 snippets was collected automatically
from www.rottentomatoes.com website. All
sentences in reviews marked ?rotten? were con-
sidered negative and snippets from ?fresh? re-
views were deemed positive. In order to make
the results obtained on this dataset comparable
to other domains, a randomly selected subset of
1066 snippets was used in the experiments.
? A balanced corpus of 800 manually annotated
sentences extracted from 83 newspaper texts
(further, news). The full set of sentences
was annotated by one judge. 200 sentences
from this corpus (100 positive and 100 neg-
ative) were also randomly selected from the
corpus for an inter-annotator agreement study
and were manually annotated by two indepen-
dent annotators. The pairwise agreement be-
tween annotators was calculated as the percent
of same tags divided by the number of sen-
tences with this tag in the gold standard. The
pair-wise agreement between the three anno-
tators ranged from 92.5 to 95.9% (?=0.74 and
0.75 respectively) on positive vs. negative tags.
? A set of sentences taken from personal
weblogs (further, blogs) posted on Live-
Journal (http://www.livejournal.com) and on
http://www.cyberjournalist.com. This corpus
is composed of 800 sentences (400 sentences
with positive and 400 sentences with negative
sentiment). In order to establish the inter-
annotator agreement, two independent judges
were asked to annotate 200 sentences from this
corpus. The agreement between the two an-
notators on positive vs. negative tags reached
99% (?=0.97).
? A set of 1200 product review (PR) sentences
extracted from the annotated corpus made
available by Bing Liu (Hu and Liu, 2004)
(http://www.cs.uic.edu/ liub/FBS/FBS.html).
The data set sizes are summarized in Table 1.
Movies News Blogs PR
Text level 2002 texts n/a n/a n/a
Sentence level 10662 800 800 1200
snippets sent. sent. sent.
Table 1: Datasets
3.3 Establishing a Baseline for a Corpus-based
System (CBS)
Supervised statistical methods have been very suc-
cessful in sentiment tagging of texts: on movie re-
view texts they reach accuracies of 85-90% (Aue
and Gamon, 2005; Pang and Lee, 2004). These
methods perform particularly well when a large vol-
ume of labeled data from the same domain as the
292
test set is available for training (Aue and Gamon,
2005). For this reason, most of the research on senti-
ment tagging using statistical classifiers was limited
to product and movie reviews, where review authors
usually indicate their sentiment in a form of a stan-
dardized score that accompanies the texts of their re-
views.
The lack of sufficient data for training appears to
be the main reason for the virtual absence of exper-
iments with statistical classifiers in sentiment tag-
ging at the sentence level. To our knowledge, the
only work that describes the application of statis-
tical classifiers (SVM) to sentence-level sentiment
classification is (Gamon and Aue, 2005)1. The av-
erage performance of the system on ternary clas-
sification (positive, negative, and neutral) was be-
tween 0.50 and 0.52 for both average precision and
recall. The results reported by (Riloff et al, 2006)
for binary classification of sentences in a related
domain of subjectivity tagging (i.e., the separation
of sentiment-laden from neutral sentences) suggest
that statistical classifiers can perform well on this
task: the authors have reached 74.9% accuracy on
the MPQA corpus (Riloff et al, 2006).
In order to explore the performance of dif-
ferent approaches in sentiment annotation at the
text and sentence levels, we used a basic Na??ve
Bayes classifier. It has been shown that both
Na??ve Bayes and SVMs perform with similar ac-
curacy on different sentiment tagging tasks (Pang
and Lee, 2004). These observations were con-
firmed with our own experiments with SVMs and
Na??ve Bayes (Table 3). We used the Weka pack-
age (http://www.cs.waikato.ac.nz/ml/weka/) with
default settings.
In the sections that follow, we describe a set
of comparative experiments with SVMs and Na??ve
Bayes classifiers (1) on texts and sentences and (2)
on four different domains (movie reviews, news,
blogs, and product reviews). System runs with un-
igrams, bigrams, and trigrams as features and with
different training set sizes are presented.
1Recently, a similar task has been addressed by the Affective
Text Task at SemEval-1 where even shorter units ? headlines
? were classified into positive, negative and neutral categories
using a variety of techniques (Strapparava and Mihalcea, 2007).
4 Experiments
4.1 System Performance on Texts vs. Sentences
The experiments comparing in-domain trained sys-
tem performance on texts vs. sentences were con-
ducted on 2,002 movie review texts and on 10,662
movie review snippets. The results with 10-fold
cross-validation are reported in Table 22.
Trained on Texts Trained on Sent.
Tested on Tested on Tested on Tested on
Texts Sent. Texts Sent.
1gram 81.1 69.0 66.8 77.4
2gram 83.7 68.6 71.2 73.9
3gram 82.5 64.1 70.0 65.4
Table 2: Accuracy of Na??ve Bayes on movie reviews.
Consistent with findings in the literature (Cui et
al., 2006; Dave et al, 2003; Gamon and Aue, 2005),
on the large corpus of movie review texts, the in-
domain-trained system based solely on unigrams
had lower accuracy than the similar system trained
on bigrams. But the trigrams fared slightly worse
than bigrams. On sentences, however, we have ob-
served an inverse pattern: unigrams performed bet-
ter than bigrams and trigrams. These results high-
light a special property of sentence-level annota-
tion: greater sensitivity to sparseness of the model:
On texts, classifier error on one particular sentiment
marker is often compensated by a number of cor-
rectly identified other sentiment clues. Since sen-
tences usually contain a much smaller number of
sentiment clues than texts, sentence-level annota-
tion more readily yields errors when a single sen-
timent clue is incorrectly identified or missed by
the system. Due to lower frequency of higher-order
n-grams (as opposed to unigrams), higher-order n-
gram language models are more sparse, which in-
creases the probability of missing a particular sen-
timent marker in a sentence (Table 33). Very large
2All results are statistically significant at ? = 0.01 with two
exceptions: the difference between trigrams and bigrams for the
system trained and tested on texts is statistically significant at
alpha=0.1 and for the system trained on sentences and tested on
texts is not statistically significant at ? = 0.01.
3The results for movie reviews are lower than those reported
in Table 2 since the dataset is 10 times smaller, which results
in less accurate classification. The statistical significance of the
293
training sets are required to overcome this higher n-
gram sparseness in sentence-level annotation.
Dataset Movie News Blogs PRs
Dataset size 1066 800 800 1200
unigrams
SVM 68.5 61.5 63.85 76.9
NB 60.2 59.5 60.5 74.25
nb features 5410 4544 3615 2832
bigrams
SVM 59.9 63.2 61.5 75.9
NB 57.0 58.4 59.5 67.8
nb features 16286 14633 15182 12951
trigrams
SVM 54.3 55.4 52.7 64.4
NB 53.3 57.0 56.0 69.7
nb features 20837 18738 19847 19132
Table 3: Accuracy of unigram, bigram and trigram mod-
els across domains.
4.2 System Performance on Different Domains
In the second set of experiments we sought to com-
pare system results on sentences using in-domain
and out-of-domain training. Table 4 shows that in-
domain training, as expected, consistently yields su-
perior accuracy than out-of-domain training across
all four datasets: movie reviews (Movies), news,
blogs, and product reviews (PRs). The numbers for
in-domain trained runs are highlighted in bold.
Test Data
Training Data Movies News Blogs PRs
Movies 68.5 55.2 53.2 60.7
News 55.0 61.5 56.25 57.4
Blogs 53.7 49.9 63.85 58.8
PRs 55.8 55.9 56.25 76.9
Table 4: Accuracy of SVM with unigram model
results depends on the genre and size of the n-gram: on prod-
uct reviews, all results are statistically significant at ? = 0.025
level; on movie reviews, the difference between Nav?e Bayes
and SVM is statistically significant at ? = 0.01 but the signif-
icance diminishes as the size of the n-gram increases; on news,
only bi-grams produce a statistically significant (? = 0.01) dif-
ference between the two machine learning methods, while on
blogs the difference between SVMs and Nav?e Bayes is most
pronounced when unigrams are used (? = 0.025).
It is interesting to note that on sentences, regard-
less of the domain used in system training and re-
gardless of the domain used in system testing, un-
igrams tend to perform better than higher-order n-
grams. This observation suggests that, given the
constraints on the size of the available training sets,
unigram-based systems may be better suited for
sentence-level sentiment annotation.
5 Lexicon-Based Approach
The search for a base-learner that can produce great-
est synergies with a classifier trained on small-set
in-domain data has turned our attention to lexicon-
based systems. Since the benefits from combining
classifiers that always make similar decisions is min-
imal, the two (or more) base-learners should com-
plement each other (Alpaydin, 2004). Since a sys-
tem based on a fairly different learning approach
is more likely to produce a different decision un-
der a given set of circumstances, the diversity of
approaches integrated in the ensemble of classifiers
was expected to have a beneficial effect on the over-
all system performance.
A lexicon-based approach capitalizes on the
fact that dictionaries, such as WordNet (Fell-
baum, 1998), contain a comprehensive and domain-
independent set of sentiment clues that exist in
general English. A system trained on such gen-
eral data, therefore, should be less sensitive to do-
main changes. This robustness, however is expected
to come at some cost, since some domain-specific
sentiment clues may not be covered in the dictio-
nary. Our hypothesis was, therefore, that a lexicon-
based system will perform worse than an in-domain
trained classifier but possibly better than a classifier
trained on out-of domain data.
One of the limitations of general lexicons and
dictionaries, such as WordNet (Fellbaum, 1998), as
training sets for sentiment tagging systems is that
they contain only definitions of individual words
and, hence, only unigrams could be effectively
learned from dictionary entries. Since the struc-
ture of WordNet glosses is fairly different from
that of other types of corpora, we developed a sys-
tem that used the list of human-annotated adjec-
tives from (Hatzivassiloglou and McKeown, 1997)
as a seed list and then learned additional unigrams
294
from WordNet synsets and glosses with up to 88%
accuracy, when evaluated against General Inquirer
(Stone et al, 1966) (GI) on the intersection of our
automatically acquired list with GI. In order to ex-
pand the list coverage for our experiments at the text
and sentence levels, we then augmented the list by
adding to it all the words annotated with ?Positiv?
or ?Negativ? tags in GI, that were not picked up by
the system. The resulting list of features contained
11,000 unigrams with the degree of membership in
the category of positive or negative sentiment as-
signed to each of them.
In order to assign the membership score to each
word, we did 58 system runs on unique non-
intersecting seed lists drawn from manually anno-
tated list of positive and negative adjectives from
(Hatzivassiloglou and McKeown, 1997). The 58
runs were then collapsed into a single set of 7,813
unique words. For each word we computed a score
by subtracting the total number of runs assigning
this word a negative sentiment from the total of the
runs that consider it positive. The resulting measure,
termed Net Overlap Score (NOS), reflected the num-
ber of ties linking a given word with other sentiment-
laden words in WordNet, and hence, could be used
as a measure of the words? centrality in the fuzzy
category of sentiment. The NOSs were then normal-
ized into the interval from -1 to +1 using a sigmoid
fuzzy membership function (Zadeh, 1975)4. Only
words with fuzzy membership degree not equal to
zero were retained in the list. The resulting list
contained 10,809 sentiment-bearing words of differ-
ent parts of speech. The sentiment determination at
the sentence and text level was then done by sum-
ming up the scores of all identified positive unigrams
(NOS>0) and all negative unigrams (NOS<0) (An-
dreevskaia and Bergler, 2006).
5.1 Establishing a Baseline for the
Lexicon-Based System (LBS)
The baseline performance of the Lexicon-Based
System (LBS) described above is presented in Ta-
ble 5, along with the performance results of the in-
domain- and out-of-domain-trained SVM classifier.
Table 5 confirms the predicted pattern: the
LBS performs with lower accuracy than in-domain-
4With coefficients: ?=1, ?=15.
Movies News Blogs PRs
LBS 57.5 62.3 63.3 59.3
SVM in-dom. 68.5 61.5 63.85 76.9
SVM out-of-dom. 55.8 55.9 56.25 60.7
Table 5: System accuracy on best runs on sentences
trained corpus-based classifiers, and with similar
or better accuracy than the corpus-based classifiers
trained on out-of-domain data. Thus, the lexicon-
based approach is characterized by a bounded but
stable performance when the system is ported across
domains. These performance characteristics of
corpus-based and lexicon-based approaches prompt
further investigation into the possibility to combine
the portability of dictionary-trained systems with the
accuracy of in-domain trained systems.
6 Integrating the Corpus-based and
Dictionary-based Approaches
The strategy of integration of two or more sys-
tems in a single ensemble of classifiers has been
actively used on different tasks within NLP. In sen-
timent tagging and related areas, Aue and Gamon
(2005) demonstrated that combining classifiers can
be a valuable tool in domain adaptation for senti-
ment analysis. In the ensemble of classifiers, they
used a combination of nine SVM-based classifiers
deployed to learn unigrams, bigrams, and trigrams
on three different domains, while the fourth domain
was used as an evaluation set. Using then an SVM
meta-classifier trained on a small number of target
domain examples to combine the nine base clas-
sifiers, they obtained a statistically significant im-
provement on out-of-domain texts from book re-
views, knowledge-base feedback, and product sup-
port services survey data. No improvement occurred
on movie reviews.
Pang and Lee (2004) applied two different clas-
sifiers to perform sentiment annotation in two se-
quential steps: the first classifier separated subjec-
tive (sentiment-laden) texts from objective (neutral)
ones and then they used the second classifier to clas-
sify the subjective texts into positive and negative.
Das and Chen (2004) used five classifiers to deter-
mine market sentiment on Yahoo! postings. Simple
majority vote was applied to make decisions within
295
the ensemble of classifiers and achieved accuracy of
62% on ternary in-domain classification.
In this study we describe a system that attempts to
combine the portability of a dictionary-trained sys-
tem (LBS) with the accuracy of an in-domain trained
corpus-based system (CBS). The selection of these
two classifiers for this system, thus, was theory-
based. The section that follows describes the classi-
fier integration and presents the performance results
of the system consisting of an ensemble CBS and
LBS classifier and a precision-based vote weighting
procedure.
6.1 The Classifier Integration Procedure and
System Evaluation
The comparative analysis of the corpus-based and
lexicon-based systems described above revealed that
the errors produced by CBS and LBS were to a
great extent complementary (i.e., where one classi-
fier makes an error, the other tends to give the cor-
rect answer). This provided further justification to
the integration of corpus-based and lexicon-based
approaches in a single system.
Table 6 below illustrates the complementarity of
the performance CBS and LBS classifiers on the
positive and negative categories. In this experiment,
the corpus-based classifier was trained on 400 an-
notated product review sentences5. The two systems
were then evaluated on a test set of another 400 prod-
uct review sentences. The results reported in Table 6
are statistically significant at ? = 0.01.
CBS LBS
Precision positives 89.3% 69.3%
Precision negatives 55.5% 81.5%
Pos/Neg Precision 58.0% 72.1%
Table 6: Base-learners? precision and recall on product
reviews on test data.
Table 6 shows that the corpus-based system has a
very good precision on those sentences that it classi-
fies as positive but makes a lot of errors on those sen-
tences that it deems negative. At the same time, the
lexicon-based system has low precision on positives
5The small training set explains relatively low overall per-
formance of the CBS system.
and high precision on negatives6. Such complemen-
tary distribution of errors produced by the two sys-
tems was observed on different data sets from differ-
ent domains, which suggests that the observed dis-
tribution pattern reflects the properties of each of
the classifiers, rather than the specifics of the do-
main/genre.
In order to take advantage of the observed com-
plementarity of the two systems, the following pro-
cedure was used. First, a small set of in-domain
data was used to train the CBS system. Then both
CBS and LBS systems were run separately on the
same training set, and for each classifier, the preci-
sion measures were calculated separately for those
sentences that the classifier considered positive and
those it considered negative. The chance-level per-
formance (50%) was then subtracted from the pre-
cision figures to ensure that the final weights reflect
by how much the classifier?s precision exceeds the
chance level. The resulting chance-adjusted preci-
sion numbers of the two classifiers were then nor-
malized, so that the weights of CBS and LBS clas-
sifiers sum up to 100% on positive and to 100% on
negative sentences. These weights were then used
to adjust the contribution of each classifier to the de-
cision of the ensemble system. The choice of the
weight applied to the classifier decision, thus, varied
depending on whether the classifier scored a given
sentence as positive or as negative. The resulting
system was then tested on a separate test set of sen-
tences7. The small-set training and evaluation exper-
iments with the system were performed on different
domains using 3-fold validation.
The experiments conducted with the Ensemble
system were designed to explore system perfor-
mance under conditions of limited availability of an-
notated data for classifier training. For this reason,
the numbers reported for the corpus-based classifier
do not reflect the full potential of machine learn-
ing approaches when sufficient in-domain training
data is available. Table 7 presents the results of
these experiments by domain/genre. The results
6These results are consistent with an observation in
(Kennedy and Inkpen, 2006), where a lexicon-based system
performed with a better precision on negative than on positive
texts.
7The size of the test set varied in different experiments due
to the availability of annotated data for a particular domain.
296
are statistically significant at ? = 0.01, except the
runs on movie reviews where the difference between
the LBS and Ensemble classifiers was significant at
? = 0.05.
LBS CBS Ensemble
News Acc 67.8 53.2 73.3
F 0.82 0.71 0.85
Movies Acc 54.5 53.5 62.1
F 0.73 0.72 0.77
Blogs Acc 61.2 51.1 70.9
F 0.78 0.69 0.83
PRs Acc 59.5 58.9 78.0
F 0.77 0.75 0.88
Average Acc 60.7 54.2 71.1
F 0.77 0.72 0.83
Table 7: Performance of the ensemble classifier
Table 7 shows that the combination of two classi-
fiers into an ensemble using the weighting technique
described above leads to consistent improvement in
system performance across all domains/genres. In
the ensemble system, the average gain in accuracy
across the four domains was 16.9% relative to CBS
and 10.3% relative to LBS. Moreover, the gain in
accuracy and precision was not offset by decreases
in recall: the net gain in recall was 7.4% relative to
CBS and 13.5% vs. LBS. The ensemble system on
average reached 99.1% recall. The F-measure has
increased from 0.77 and 0.72 for LBS and CBS clas-
sifiers respectively to 0.83 for the whole ensemble
system.
7 Discussion
The development of domain-independent sentiment
determination systems poses a substantial challenge
for researchers in NLP and artificial intelligence.
The results presented in this study suggest that the
integration of two fairly different classifier learning
approaches in a single ensemble of classifiers can
yield substantial gains in system performance on all
measures. The most substantial gains occurred in
recall, accuracy, and F-measure.
This study permits to highlight a set of factors
that enable substantial performance gains with the
ensemble of classifiers approach. Such gains are
most likely when (1) the errors made by the clas-
sifiers are complementary, i.e., where one classifier
makes an error, the other tends to give the correct
answer, (2) the classifier errors are not fully random
and occur more often in a certain segment (or cate-
gory) of classifier results, and (3) there is a way for
a system to identify that low-precision segment and
reduce the weights of that classifier?s results on that
segment accordingly. The two classifiers used in this
study ? corpus-based and lexicon-based ? provided
an interesting illustration of potential performance
gains associated with these three conditions. The
use of precision of classifier results on the positives
and negatives proved to be an effective technique for
classifier vote weighting within the ensemble.
8 Conclusion
This study contributes to the research on sentiment
tagging, domain adaptation, and the development of
ensembles of classifiers (1) by proposing a novel ap-
proach for sentiment determination at sentence level
and delineating the conditions under which great-
est synergies among combined classifiers can be
achieved, (2) by describing a precision-based tech-
nique for assigning differential weights to classifier
results on different categories identified by the clas-
sifier (i.e., categories of positive vs. negative sen-
tences), and (3) by proposing a new method for sen-
timent annotation in situations where the annotated
in-domain data is scarce and insufficient to ensure
adequate performance of the corpus-based classifier,
which still remains the preferred choice when large
volumes of annotated data are available for system
training.
Among the most promising directions for future
research in the direction laid out in this paper is the
deployment of more advanced classifiers and fea-
ture selection techniques that can further enhance
the performance of the ensemble of classifiers. The
precision-based vote weighting technique may prove
to be effective also in situations, where more than
two classifiers are integrated into a single system.
We expect that these more advanced ensemble-of-
classifiers systems would inherit the benefits of mul-
tiple complementary approaches to sentiment anno-
tation and will be able to achieve better and more
stable accuracy on in-domain, as well as on out-of-
domain data.
297
References
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. The MIT Press, Cambridge, MA.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for a fuzzy sentiment: Sentiment tag extrac-
tion from WordNet glosses. In Proceedings the 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, Trento, IT.
Anthony Aue and Michael Gamon. 2005. Customizing
sentiment classifiers to new domains: a case study. In
Proccedings of the International Conference on Recent
Advances in Natural Language Processing, Borovets,
BG.
Xue Bai, Rema Padman, and Edoardo Airoldi. 2005. On
learning parsimonious models for extracting consumer
opinions. In Proceedings of the 38th Annual Hawaii
International Conference on System Sciences, Wash-
ington, DC.
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Com-
parative experiments on sentiment classification for
online product reviews. In Proceedings of the 21st
International Conference on Artificial Intelligence,
Boston, MA.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the Peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW03, Budapest, HU.
Mark Drezde, John Blitzer, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, Prague, CZ.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the ACL-05 Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, Ann Arbor, US.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the the 40th Annual Meeting
of the Association of Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD-04, pages 168?177.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment Classification of Movie Reviews Using Con-
textual Valence Shifters. Computational Intelligence,
22(2):110?125.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detec-
tion of opinion bearing words and sentences. In Pro-
ceedings of the Second International Joint Conference
on Natural Language Processing, Companion Volume,
Jeju Island, KR.
Bo Pang and Lilian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the 43nd
Meeting of the Association for Computational Linguis-
tics, Ann Arbor, US.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Conference on Empiri-
cal Methods in Natural Language Processing.
Jonathon Read. 2005. Using emoticons to reduce depen-
dency in machine learning techniques for sentiment
classification. In Proceedings of the ACL-2005 Stu-
dent Research Workshop, Ann Arbor, MI.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, Sydney, AUS.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The General Inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics. M.I.T. Press, Cambridge, MA.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proceedings of the
4th International Workshop on Semantic Evaluations,
Prague, CZ.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Zueqi
Cheng. 2007. A Novel Scheme for Domain-transfer
Problem in the context of Sentiment Analysis. In Pro-
ceedings of CIKM 2007.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21:315?346.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association of Computational Linguis-
tics.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
Evaluative and Speculative Language. In Proceedings
of the 2nd ACL SIGDial Workshop on Discourse and
Dialogue, Aalberg, DK.
Lotfy A. Zadeh. 1975. Calculus of Fuzzy Restrictions.
In L.A. Zadeh et al, editor, Fuzzy Sets and their Ap-
plications to cognitive and decision processes, pages
1?40. Academic Press Inc., New-York.
298
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 117?120,
Prague, June 2007. c?2007 Association for Computational Linguistics
CLaC and CLaC-NB: Knowledge-based and corpus-based approaches
to sentiment tagging
Alina Andreevskaia
Concordia University
1455 de Maisonneuve Blvd.
Montreal, Canada
andreev@cs.concordia.ca
Sabine Bergler
Concordia University
1455 de Maisonneuve Blvd.
Montreal, Canada
bergler@cs.concordia.ca
Abstract
For the Affective Text task at Semeval-
1/Senseval-4, the CLaC team compared a
knowledge-based, domain-independent ap-
proach and a standard, statistical machine
learning approach to ternary sentiment an-
notation of news headlines. In this paper
we describe the two systems submitted to
the competition and evaluate their results.
We show that the knowledge-based unsu-
pervised method achieves high accuracy and
precision but low recall, while supervised
statistical approach trained on small amount
of in-domain data provides relatively high
recall at the cost of low precision.
1 Introduction
Sentiment tagging of short text spans ? sentences,
headlines, or clauses ? poses considerable chal-
lenges for automatic systems due to the scarcity of
sentiment clues in these units: sometimes, the deci-
sion about the text span sentiment has to be based
on just a single sentiment clue and the cost of every
error is high. This is particularly true for headlines,
which are typically very short. Therefore, an ideal
system for sentiment tagging of headlines has to use
a large set of features with dependable sentiment an-
notations and to be able to reliably deduce the senti-
ment of the headline from the sentiment of its com-
ponents.
The valence labeling subtask of the Affective Text
task requires ternary ? positive vs. negative vs.
neutral ? classification of headlines. While such
categorization at the sentence level remains rela-
tively unexplored1 , the two related sentence-level,
binary classification tasks ? positive vs. negative
and subjective vs. objective ? have attracted con-
siderable attention in the recent years (Hu and Liu,
2004; Kim and Hovy, 2005; Riloff et al, 2006; Tur-
ney and Littman, 2003; Yu and Hatzivassiloglou,
2003). Unsupervised knowledge-based methods are
the preferred approach to classification of sentences
into positive and negative, mostly due to the lack of
adequate amounts of labeled training data (Gamon
and Aue, 2005). These approaches rely on presence
and scores of sentiment-bearing words that have
been acquired from dictionaries (Kim and Hovy,
2005) or corpora (Yu and Hatzivassiloglou, 2003).
Their accuracy on news sentences is between 65 and
68%.
Sentence-level subjectivity detection, where train-
ing data is easier to obtain than for positive vs. neg-
ative classification, has been successfully performed
using supervised statistical methods alone (Pang and
Lee, 2004) or in combination with a knowledge-
based approach (Riloff et al, 2006).
Since the extant literature does not provide clear
evidence for the choice between supervised machine
learning methods and unsupervised knowledge-
based approaches for the task of ternary sentiment
classification of sentences or headlines, we devel-
oped two systems for the Affective Text task at
SemEval-2007. The first system (CLaC) relies on
the knowledge-rich approach that takes into consid-
1To our knowledge, the only work that attempted such clas-
sification at the sentence level is (Gamon and Aue, 2005) that
classified product reviews.
117
eration multiple clues, such as a list of sentiment-
bearing unigrams and valence shifters, and makes
use of sentence structure in order to combine these
clues into an overall sentiment of the headline. The
second system (CLaC-NB) explores the potential of
a statistical method trained on a small amount of
manually labeled news headlines and sentences.
2 CLaC System: Syntax-Aware
Dictionary-Based Approach
The CLaC system relies on a knowledge-based,
domain-independent, unsupervised approach to
headline sentiment detection and scoring. The
system uses three main knowledge inputs: a list
of sentiment-bearing unigrams, a list of valence
shifters (Polanyi and Zaenen, 2006), and a set of
rules that define the scope and results of com-
bination of sentiment-bearing words with valence
shifters.
2.1 List of sentiment-bearing words
The unigrams used for sentence/headline classifica-
tion were learned from WordNet (Fellbaum, 1998)
dictionary entries using the STEP system described
in (Andreevskaia and Bergler, 2006b). In order to
take advantage of the special properties of WordNet
glosses and relations, we developed a system that
used the human-annotated adjectives from (Hatzi-
vassiloglou and McKeown, 1997) as a seed list and
learned additional unigrams from WordNet synsets
and glosses. The STEP algorithm starts with a
small set of manually annotated seed words that
is expanded using synonymy and antonymy rela-
tions in WordNet. Then the system searches all
WordNet glosses and selects the synsets that contain
sentiment-bearing words from the expanded seed
list in their glosses. In order to eliminate errors
produced by part-of-speech ambiguity of some of
the seed words, the glosses are processed by Brill?s
part-of-speech tagger (Brill, 1995) and only the seed
words with matching part-of-speech tags are consid-
ered. Headwords with sentiment-bearing seed words
in their definitions are then added to the positive or
negative categories depending on the seed-word sen-
timent. Finally, words that were assigned contra-
dicting ? positive and negative ? sentiment within
the same run were eliminated. The average accu-
racy of 60 runs with non-intersecting seed lists when
compared to General Inquirer (Stone et al, 1966)
was 74%. In order to improve the list coverage,
the words annotated as ?Positiv? or ?Negativ? in the
General Inquirer that were not picked up by STEP
were added to the final list.
Since sentiment-bearing words in English have
different degree of centrality to the category of sen-
timent, we have constructed a measure of word cen-
trality to the category of positive or negative sen-
timent described in our earlier work (Andreevskaia
and Bergler, 2006a). The measure, termed Net Over-
lap Score (NOS), is based on the number of ties that
connect a given word to other words in the category.
The number of such ties is reflected in the num-
ber of times each word was retrieved from Word-
Net by multiple independent STEP runs with non-
intersecting seed lists. This approach allowed us
to assign NOSs to each unigram captured by mul-
tiple STEP runs. Only words with fuzzy member-
ship score not equal to zero were retained in the
list. The resulting list contained 10,809 sentiment-
bearing words of different parts of speech.
2.2 Valence Shifters
The brevity of the headlines compared to typical
news sentences2 requires that the system is able to
make a correct decision based on very few sentiment
clues. Due to the scarcity of sentiment clues, the ad-
ditional factors, such as presence of valence shifters,
have a greater impact on the system performance on
headlines than on sentences or texts, where impact
of a single error can often be compensated by a num-
ber of other, correctly identified sentiment clues. For
this reason, we complemented the system based on
fuzzy score counts with the capability to discern and
take into account some relevant elements of syntac-
tic structure of sentences. We added to the system
two components in order to enable this capability:
(1) valence shifter handling rules and (2) parse tree
analysis.
Valence shifters can be defined as words that mod-
ify the sentiment expressed by a sentiment-bearing
word (Polanyi and Zaenen, 2006). The list of va-
lence shifters used in our experiments was a com-
2An average length of a sentence in a news corpus is over 20
words, while the average length of headlines in the test corpus
was only 7 words.
118
bination of (1) a list of common English nega-
tions, (2) a subset of the list of automatically ob-
tained words with increase/decrease semantics, and
(3) words picked up in manual annotation conducted
for other research projects by two trained linguists.
The full list consists of 490 words and expressions.
Each entry in the list of valence shifters has an action
and scope associated with it. The action and scope
tags are used by special handling rules that enable
our system to identify such words and phrases in the
text and take them into account in sentence senti-
ment determination. In order to correctly determine
the scope of valence shifters in a sentence, we intro-
duced into the system the analysis of the parse trees
produced by MiniPar (Lin, 1998).
As a result of this processing, every headline re-
ceived a score according to the combined fuzzy NOS
of its constituents. We then mapped this score,
which ranged between -1.2 and 0.99, into the
[-100, 100] scale as required by the competition or-
ganizers.
3 CLaC-NB System: Na??ve Bayes
Supervised statistical methods have been very suc-
cessful in sentiment tagging of texts and in subjec-
tivity detection at sentence level: on movie review
texts they reach an accuracy of 85-90% (Aue and
Gamon, 2005; Pang and Lee, 2004) and up to 92%
accuracy on classifying movie review snippets into
subjective and objective using both Nave Bayes and
SVM (Pang and Lee, 2004). These methods per-
form particularly well when a large volume of la-
beled data from the same domain as the test set is
available for training (Aue and Gamon, 2005). The
lack of sufficient data for training appears to be the
main reason for the virtual absence of experiments
with statistical classifiers in sentiment tagging at the
sentence level.
In order to explore the potential of statistical ap-
proaches on sentiment classification of headlines,
we implemented a basic Na??ve Bayes classifier with
smoothing using Lidstone?s law of succession (with
?=0.1). No feature selection was performed.
The development set for the Affective Text task
consisted of only 250 headlines, which is not suf-
ficient for training of a statistical classifier. In or-
der to increase the size of the training corpus, we
augmented it with a balanced set of 900 manually
annotated news sentences on a variety of topics ex-
tracted from the Canadian NewsStand database3 and
200 headlines from different domains collected from
Google News in January 20074.
The probabilities assigned by the classifier were
mapped to [-100, 100] as follows: all negative head-
lines received a score of -100, all positive headlines
+100, and neutral headlines 0.
4 Results and Discussion
Table 1 shows the results of the two CLaC systems
for valence labeling subtask of Affective Text task
compared to all participating systems average. The
best subtask scores are highlighted in bold.
System Pearson Acc. Prec. Rec. F1
correl.
CLaC 47.7 55.1 61.4 9.2 16
CLaC-NB 25.4 31.2 31.2 66.4 42
Task average 33.2 44.7 44.85 29.6 23.7
Table 1: System results
The comparison between the two CLaC systems
clearly demonstrates the relative advantages of the
two approaches. The knowledge-based unsuper-
vised system performed well above average on three
main measures: the Pearson correlation between
fine-grained sentiment assigned by CLaC system
and the human annotation; the accuracy for ternary
classification; and the precision of binary (positive
vs. negative) classification. These results demon-
strate that an accurately annotated list of sentiment-
bearing words combined with sophisticated valence
shifter handling produces acceptably accurate senti-
ment labels even for such difficult data as news head-
lines. This system, however, was not able to provide
good recall.
On the contrary, supervised machine learning has
very good recall, but low accuracy relative to the
results of the unsupervised knowledge-based ap-
proach. This shortcoming could be in part reduced
if more uniformly labeled headlines were available
3http://www.il.proquest.com/products pq/
descriptions/Canadian newsstand.shtml
4The interannotator agreement for this data, as measured by
Kappa, was 0.74.
119
for training. However, we can hardly expect large
amounts of such manually annotated data to be
handy in real-life situations.
5 Conclusions
The two CLaC systems that we submitted to the
Affective Text task have tested the applicability of
two main sentiment tagging approaches to news
headlines annotation. The results of the two sys-
tems indicate that the knowledge-based unsuper-
vised approach that relies on an automatically ac-
quired list of sentiment-bearing unigrams and takes
into account the combinatorial properties of valence
shifters, can produce high quality sentiment annota-
tions, but may miss many sentiment-laden headlines.
On the other hand, supervised machine learning has
good recall even with a relatively small training set,
but its precision and accuracy are low. In our future
work we will explore the potential of combining the
two approaches in a single system in order to im-
prove both recall and precision of sentiment annota-
tion.
References
Alina Andreevskaia and Sabine Bergler. 2006a. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings EACL-06,
the 11rd Conference of the European Chapter of the
Association for Computational Linguistics, Trento, IT.
Alina Andreevskaia and Sabine Bergler. 2006b. Seman-
tic tag extraction from wordnet glosses. In Proceed-
ings of LREC-06, the 5th Conference on Language Re-
sources and Evaluation, Genova, IT.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
In RANLP-05, the International Conference on Recent
Advances in Natural Language Processing, Borovets,
Bulgaria.
Eric Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part-of-Speech Tagging. Computational Lin-
guistics, 21(4).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the ACL-05 Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, Ann Arbor, MI.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. In Proceedings of ACL-97, 35nd Meeting of
the Association for Computational Linguistics, pages
174?181, Madrid, Spain. ACL.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Tenth ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing (KDD-04), pages 168?177.
Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Companion Volume to the Proceedings of IJCNLP-05,
the Second International Joint Conference on Natural
Language Processing, pages 61?66, Jeju Island, KR.
Dekang Lin. 1998. Dependency-based Evaluation
of MINIPAR. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, pages 768?774,
Granada, Spain.
Bo Pang and Lilian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL-04,
42nd Meeting of the Association for Computational
Linguistics, pages 271?278.
Livia Polanyi and Annie Zaenen. 2006. Contextual Va-
lence Shifters. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theory and Application. Springer Verlag.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis. In
Proceedings of EMNLP-06, the Conference on Empir-
ical Methods in Natural Language Processing, pages
440?448, Sydney, AUS.
P. J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The General Inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics. M.I.T. Press, Cambridge, MA.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21:315?346.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Michael Collins and Mark Steedman, ed-
itors, Proceedings of EMNLP-03, 8th Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Sapporo, Japan.
120
