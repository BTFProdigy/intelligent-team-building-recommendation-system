Querying Temporal Databases Using Controlled Natural 
Language* 
Ran i  Nelken  N iss i ln  Francez 
Computer  Science Department  
The Technion 
Haifa 32000, Israel 
Abst rac t  
Recent years have shown a surge in interest in 
temporal database systems, which allow users to 
store time-dependent intbrmation. We present 
a novel controlled natural anguage interface to 
temporal databases, based on translating nat- 
ural language questions into SQL/Telnporal, a 
temporal database query language. The syn- 
tactic analysis is done using the Type-Logical 
Grammar framework, highlighting its utility not; 
only as a theoretical fralnework but also as a 
practical tool. The semantic analysis is done 
using a novel theory of the semantics of tempo- 
ral questions, focusing on the role of temporal 
preposition phrases rather than the more tradi- 
tional focus on tense and aspect. Our transla- 
tion method is considerably simpler than pre- 
vious attempts in this direction. We present a 
prototype software implementation. 
1 I n t roduct ion  
Traditionally, database management systems 
were designed to store snapshot information, 
valid at a particular moment of time (state). 
Itowever, many applications require handling 
dynamic time-dependent information, pertain- 
ing not only to the present, but also to the 
past and future. Adding temporal support 
to databases has proved to be a surprisingly 
* This work was carried out as part of the research 
project "Semantics of Natural Language Temporal Ques- 
tions and Interfaces to Temporal Database Systems" 
sponsored by the Fund for interdisciplinary researdl, ad- 
ministered by the Israeli Academy of Science. We thank 
Michael BShlen, Bob Carpenter and Andreas Steiner for 
each allowing us to incorporate their software within our 
own. We also thank Yoad Winter and the anonymous 
referees for helpful comments on a previous version of 
this paper. The work of the second author was partially 
supported by the fund for the promotion of l'eseardl in 
the Tcchniou. 
thorny issue (Tansel et el., 1993). A re- 
cent drive to consolidate research efforts has 
led to the design of a consensus temporal 
data model and associated temporal database 
query language, SQL/Temporal, an extension 
of the popular Structured Query Language 
(SQL) (Snodgrass, 2000). 1 SQL/?J)enlpora\] rep- 
resents a significant improvement over standard 
SQL in allowing programmers to express tem- 
poral queries (Snodgrass, 2000). Since tempo- 
ral database (TI)B) implementations are still 
in their infancy (Boehlen, 1995), there is lit- 
tle practical experience with SQL/Temporal; let 
alone experience of non-expert users. However, 
it; is our belief that such users are bound to find 
the expression of complex temporal queries in 
SQL/Temporal to be extremely difficult. 
In an attempt o counter this problem, we 
present a translation method from controlled 
natural anguage (NL) to SQL/Temporal. Fol- 
lowing the standard pipeline architecture of 
such methods, translation is done via an inter- 
mediate meaning representation language illus- 
trated in Figure 1. NL questions are first parsed 
using a grammar in the Type Logical Gram- 
mar (TLG) framework (Carpenter, 1998; Mor- 
rill, 1.998). Simultaneously with parsing, the NL 
question is translated into a formula of a for- 
mal language called Lane, (Toman, 11.996), based 
on the interval operators of (Allen, 1983). The 
translation is based on an independently moti- 
vated novel semantics of sentences modified by 
temporal Preposition Ph, rases (PPs) (Pratt and 
Francez 1997; 2000, Nelken and Francez 1999). 
The constructed LAno. formula is then trans- 
tThrough continued design, SQL/Temporal has 
evolved from predecessor versions named TSQL2 (Snod- 
grass, 1.995) and ATSQL2 (Boehlen et el., 1996). It is 
expected to be incorporated within the new version of 
SQL named SQL3. 
1076 
la,ted into an SQl,/Teml)oral query, which is 
subsequently submitted to a. l)rototyl)e TI)B im- 
plementation for eva.luation. Finally, the TI)B's 
answer is 1)resented to the user. 
Figure 1: The tr~nslation pipeline 
We have implemented this method as a pro- 
totype software tool, called Q IYEI~TY, (allnost) 
an acronym lbr "Querying with 15'nglish, of l~cla- 
rio'hal Temporal Databases". Parsing and trans- 
la.tion to I,Alle,, is done using the TLG Theo- 
rem Prover (C~u:penter, 1999). The trmlslation 
from \['allen to SQL/Temporal is done using an 
adaptation of a tempera.1 logic (Ti,) to Aq'SQL2 
translator of (Boehlen et al, 1996). The result- 
lag query is submitted to a prototype TI)B im- 
plementation, calle(l Timel)\]} (Steiner, 1{)97). 
T'he different modules are COul)led into an inte- 
grated system iml)lemented in Sicstus 1)rolog on 
a. UNIX platform with a WW\?-l)ased gral)hi-- 
caJ fi'ont-end. We discuss some of the directions 
required in order to turn the system fi:om a. re- 
search prototype to a working tool. 
2 Related  work  
There is voluminous litera.ture on the design 
of Nil, interfaces to general (non-tenq)oral) 
databa.ses (see (lYrrault and Grosz, \]988; 
Col)estake and Jones, 1990; Androutsopoulos et
al ,  1995) for surveys) and by now their main 
a,:lva.ntages and disadvantages are well under- 
stood. Much less work has been devoted to 
the design of Nil, interfa.ces to TDBs (C'lifford, 
1990; Hinrichs, 1988) or other computer sys- 
t(.'nls involving a temporal dimension (Crouch 
and l)ulnm.n, 71993). ()f lYa, rticular relevance 
is (Androutsopoulos, 1996), who presents a lin- 
guistically motivated translation method fi'om 
NL queries into TSQL2 using a.n ltl)SG (Pol- 
la, rd and Sag, 1.994) grammar and a TL as an 
intermediate representation language. Our al)- 
1)l:oach shares many characte.ristics with (An- 
d routsopoulos, 1996), but there also important 
difl'erences, which we point out throughout the 
paper. 
\?e I)egin our presentation of the. translation 
method with a brief overview of the T\])I}, as its 
structure determines many of the deign choices 
ta.lw.n in devising the translation method. 
3 The  TDB 
A TDB is a two-sorted tirst-oMer structure. 
The domain consists of a Data Do'main, D, and 
a Temporal Domain of intervals, 7'/, detined 
as follows. Let Tp 1)e a. set (of time points) 
with a discrete linear order without endl)oints, 
<._ .7'1 is defined as the set of pairs : 7) = 
Z,)la _< b c Tu{-oo ,  oo}}. a rdational 
database schema is a set, of single-sorted pred- 
icate symbols (H,1,..., l~k). Given a relational 
database schelna p, a Tl)l} schema p' is the sol; 
3/  of two-sorted predicate sylnbols (l?11,..., \]~), 
where the sort of /?~ is D aritv(l~i) x TI. A 
TI)13 instance of schema f is a set of relations 
R\[ C l) arity(l{i) X 7), where each R~ is finite. 
For instance, assume a database schema p 
consisting era  single binary predicate symbol 
'work, storing lbr each elnployee the department 
ill which she is employed. The Tl)l} schema 
p' consists of the relation wo'rk', called a valid- 
ti'mc state table, which adds a. temporal argu- 
nlent 1;o the original relation, caJled the valid- 
time of the table. '\['he temporal argument can 
be used to store the history (and 1)erhaps even 
future plans) of departments in which eml)loyees 
are employed. Following a. suggestion of (An- 
droutsopoulos, 1996), we also include relations 
mal)l)ing names of ca.lendricat i ems to tempo- 
ral intervals in p'. For instance, we store a. rela- 
tion year  / ma.l)l)ing the year 2000 to the interval 
\[71. \].2(100-3 J. 12.2000\] (which in turn is n lappe(t 
to an element of 7)). 
We now describe the translation process. 
4 The  t rans la t ion  process  
The translation 1)recess a.ccepts input NL ques- 
tions in a controlled subset of Nil,. Restrict- 
ing inl)ut language in this way enables e\[l'ective 
processing of a sufficiently rich fragment while 
avoiding many of the well-know problems of un- 
restricted NI,. We use a formal grammar in the 
TLG framework. Our grammar is specially de- 
signed for use with ~ particular TDB schema. 
Future work will allow easier configuration of 
the grammar with respect o the schema. 
Our grammar is based on work on all inde- 
pendently motivated theory of the semantics of 
telnt)orality. Most of the research in this tield 
(see (Steedman, 1997) for a survey) has rocllsed 
on the issues of tense a.nd aspect. \~e ha.ndle 
tense, but purposefully not aspect, which plays 
1077 
an import,~nt role in (Androutsol)oulos , 1996). 
Aspect, which is used to retlect speal~ers' tempo- 
ral viewpoint with respect o reported situations 
is an imi)ortant facet of NL temporality. How- 
ever, its relevance to TDBs is questionable, as it 
is unlikely that a realistic TDI3 would actually 
encode such subjective viewpoints. Moreover, 
handling aspect requires postulating a more 
complex data model. For instance, (Androut- 
sopoulos, 1996) augments tile TDB model with 
event-like "occurrence klentifiers", and adds a.n 
additional argument to temt)oral relations indi- 
cating whether a given event has cuhninated or 
not. While such devices may perhaps be linguis- 
tically justified, it is unclear whether the TDI3 
community would adopt such augmentations of 
the model. 
Instead, following (Pratt and Vrancez, 2000; 
Nelken and Francez, 1999) our focus is on sen- 
tences modified by temporal PPs. These PPs 
are analyzed as variants of standard general- 
ized quantifiers (Barwise and Cooper, 1981), 
in which qnantification is over: time. Using 
this framework, we handle questions that re- 
fer explicitly to the temporal (timension (e.g. 
When/during which year . . . )  as well as ques- 
tions in which temporality is hnplied by the 
TI)B context (e.g. Did Mary work in marketing?, 
Which employees worked in marketing?). We ban- 
(lie both clausal a.nd phrasal teml)ora\] Pl)s (e.g. 
after John worked in R&D, during every year). J\n 
important strength of this semantic theory is 
that it; allows for arbitrary iteration of PPs (e.g. 
one month during every year until 1992). In ad- 
dition, our grammar also handles qua.ntific~tion 
over individuals (e.g. some employee), coordina- 
tion and negation. 
input questions are parsed using a lexicalized 
type-logical grammar. Lexical items are ass(> 
elated with a syntactic ategory and a higher- 
order lambda-term representing its semantics. 
Taking advantage of 'I'LG's elegantly tight cou- 
pling of syntax and semantics, parsing and con- 
struction of a semantic representation i the 
form of an LAl le n Ibrmula proceed silnultane- 
onsly, in a bottom-up fashion. \?e have found 
using TLG to be advantageous over a feature- 
structure based formalism (such as IIPSG as 
in (A ndroutsopoulos, 1996)), since formula con- 
struction is an integral part of the l)arslng and 
does not require complex ad-hoc manipulations 
of feature structures. 
Using a particular grammar helps reduce 
some of the ambiguity inherent in unrestricted 
NL. For instance, whereas in general a preposi- 
tion such as at is ambiguous between a tempo- 
ral and a locative interpretation, the choice of 
the coml)lement NP relative to a given schema- 
induced grammar deterministically fixes the in- 
terpretation. As another example, whereas it- 
erating several temporal PPs (e.g. during some 
month every year) opens up exponential seep- 
ing possibilities, some choices are eliminated by 
world knowledge, which is encoded in the gram- 
mar (e.g. every year must have higher scope than 
some month since months are included in years 
and not vice-versa). In cases of remaining ambi- 
guity, Cite user is presented with all Cite distinct 
possibilities. Future work will allow the user: to 
lnalce informed choices between different possi- 
ble readings, e.g. by presenting him with NL 
l)ara.phrases of the alternatives. 
\?e translate NI, questions into \],All(',,,- i f ' ire 
main reason for: not translating directly to 
SQl,/Temporal is that the \]atter is not closed 
\['or sub-formulae, i.e. a sub-formula of a well- 
formed query is not necessarily well-formed. 
Since LAne, "is closed for" sub-formulae, composi- 
tionally constructing formulae while parsing in 
a bottom-u 1)fashion becomes much easier. 
I, An~, is defined as follows (Toman, 1996). 
I,et p be the database schema (1~, . . . ,  I~t,.). l,et: 
L ::=/~,~(x, I)ILA LI~LI~x.LI~I.LIx = yllc~J 
where x ,y  are variables over D, x is a vector 
of such variables, l, J are varlet)los or constants 
over T\], and (r is one of the operators: precedes, 
meets, overlaps, equals, contains. LAlien is de- 
fined as tile set of' \[brmulae p E L that con- 
Lain at most one free variable over TI. The 
answer to a formula 90 relative to a TDB D is 
{x, l l> ~ :(x,  :)}. 
To illustrate, consider the NL question: 
During which years did Mary work in marketing? 
'File I ,Alle n representation for" it is constructed in
a bottom-up nla, nnel'. The meaning representa- 
tion of the main clause Mary worked in marketing 
is constructed as: 
A J  _ /) 
In this formula, I denotes a Reichenbachian- 
like reference time, J denotes a time interval 
1078 
( I , r ing which Mary  worked in nia,rl(eA:\]nlg, which 
is loca,l;od in the l)a,sl; (l;he conl;ribution ol7 the 
t, onse) a, nd is hicluded wil;hin 1. 
The me~miug o\[" the \['ull (lll(~,~lJon \]8 COil- 
structed 1)y a,l)t)lying th(; Inca, sing o\[' the \]lll;or- 
rclga,1;ive ton~l)oi:a,1 1)1 ) during which year i;o the 
lllea,ning o\[' Lho c.\[a, uso. \? i l ; \ ] io . t  going \]lifO (lc- 
ta, ils, l;ho r('~s, lt is: 
A./ Cpa,,~tAJ C\ ] )  
The effect of applying the 1)1 ~ is that  1;lie wu'i- 
able / is now 1)ol;h free ~ul(l rest;rioted to 1)e {\]lo 
t ime of a, yea, r. The a,liswer 1;o l:li(; I 'orin.la, is 
~lie so{ o\[" ilitoi:vaJ,~ \] 1;ha,t a, re. ye4~,i',% a,n(I (lur-- 
\]ilg whi(:h 1;\]ioro is a,n iill;orva,l ,\] COliLaiil0d ill 
tlle~ l)asl;~ ( l . r ing whi(:h ~l/\]il.rry work(,(l ill llla,rl,:ol,- 
ing. VVo allow \]l;ol:a,1;od I)l)s 1;o a,l)l)iy ili a, siniila,r 
Ill a,lillor. 
Not  ( ;vory I~Allc n 1'O1'I111111"/, (:orrosl)Oi~(l~ 1;o ;/,11 
eva hlal)lo ,qQl~/'thiilpora\] query, in 1)arlJc, ula,r~ 
fornuila,e niigJll; \]ia,vo a,li hl:finil;e a lis\vor. \]"orinu- 
la,e tha,t a,r(; safe \[rOlll this a, nd \]'dato(l l)rol) - 
loins a, re t(;ruied domai'n,-i~zd<lJ<'~M?'n,~ ((Ml(lur 
a,n(l Tol)or ~ \ ]99 \ ] ;  AI) i t(q)o,t  (;t a l., 19!)5). I)o- 
nla,in in ( le t )en( lonce is a,n un(l(wida,1)lu sonlanl ; i ( :  
l ) I:O\[)Ol:i;y. 11 ow(;v(;r;  w(; i ln pose cerl,ai li ~yl l ta( : l , ic  
r('M,l:ict;iOliS on g(;nera,lx;d l'ornlulao l,ha,t oitSlli'O 
it,. '\]'heso l:(~slw\]c!,\]OllS :-tISO nim l)lil 7, l\[IO l,i:ansla,- 
1;ion task l'roin I,A,,~I, to ,gQl;/Toml)or~d. This 
tra, nslalsioli is I)ased Oil a nlo(lili('a,tion of the 
Lra, llshl,l;or \[1"()111 \[irM;-or(l(H" TI ,  over  t\]lll(~-I)O\]\]ll,,~ 
1;o A'I 'SQI,? of (l/oehlen el; a l., 71996). 
Tho syiita,ctica,l\]y restrh:te(l version o1' l,Alle. 
we use has l;ho uni(luO a,(Iva, nl;a,ge o1" I)ohig very 
clos(; both 1;o the langllag(; IlSO(I \]11 (N(2lkCll 
a, nd IPra,ncez, 1999) Oil tli(~ on(; hand a li(\] i;o 
SQl , / ' tbml)orM 011 tho, oLh('r. The S(HlllqJll;\](:g 
o\[' NI, Lonlpora\] exl)rossions is o\['ten expressed 
using (;xl)licit i:el'(;r(;llCO Lo intorwds. IAk(;wis(;, 
SQL/'lhml)ol:a,1 has Alhm-sl,ylo Ol)orators over 
inl;erva,ls. Androul;sot)oulos (\]996) uses a, CtlS- 
tomized 'FI, :.t,s a,ll hilx;rniodia,1;e~ \].q, l l~lla,g0, in 
whi?\]i t(;ntl)ora,1 i:(;la,tions a,t:e encod(;(l USillg 
L(;nil)ol:a,l opora?ors ra,t;hor 1;ii++i~ oxplicii~ ro\['or- 
once Lo hiterva,\]s. We \]ia,v(; \['ound using a, syl'i- 
l;actica,lly i:osl, ri(:l;ed vel:8iOli of' I,All<;n 1;o t)e a,(i- 
v,anl;ageous, a.s il; a cl;ua,lly sire plifie;~ the tra n,~la- 
tion. 
( \ ]oni ; in , ing oil r i)reviou,~ oxa,lllt)le> 1;lie rostllt- 
i l lg  I~Alle. rOl:l l l l l la, iS StllJS0.(tllonl;iy IA'P.,llSla,l,(,(l 
into tile \['ollowing S(~\]JTcnl i)oral (lu(;ry: 
NONSEQUENCED VALIDTINE 
SELECT DISTINCT aO.cl AS cl 
FRON work  t AS al,year I AS aO 
WHERE VALIDTIME(aO) con%ains 
VALIDTINE(al) 
AND al.cl = ~mary' 
AND al.c2 = ~marketing' 
AND PERIOD(TIMESTANP'beginning', 
TIMESTAMP'now') contains VALIDTIME(al) 
The (ltl(;ry :a,sks for the first a,rglunon{ of tlio 
rela, t ion \]nsi41,1lCO yea l  J such tiia, t the refection 
\]IIM,a,IlCO work  # in(:lu(los a, 1;ul)lo consisting of 
'Mary '~ 'market ing '  a, nd a, va,lid time. which 
is l;oml)orally iucl.dc'(l in the wind l;ime of" the 
yea, r, as well as in the \]nl;ol:va,l sl;:4,1:l;ing a,1; the 
~\])Ogi l l l l i l ig ~O\[' (;\]1110 i/,ll(\] (ql(l\]n~ :llOW : -  V\]Z. l, he~ 
past. The 'l'l)l~ rt;,~polMs 1)y returning a table 
conl,aini,<e; exactly 1;ho re(ltleslx;d year names. 
5 Conc lus ion  
rl~\]lO, a,ddith)n of the. tOtal)oral (limonsion to 
(la, ta,1)asc ,~ysl,cnl,<-; i,crca,ses lJie\]r power 1)ul, also 
thoir COml)lexity. ' lb increa,se the usabil i ty of 
T1)171.% we, pi'esout a, prollotyl)e coutrol led NI~ \]ll- 
terl'a,ce, to a, TI)l:f. Our soma~nl;ic focus is ou t,h(~ 
.su of l,(;ntpora,1 gun(;ra, lized qua,lH;ificrs, based 
ou (I)ra,IL a,u(I l"ra,ncez~ 20()())~ ra,ther tha, n t(ms(; 
a,m.l aSl)e(:l,. As a, rB:.ed 1)y ((.k)l)(~sl:a,l,:(~ a, ud 
Joiles, 1990), \]la,lMlin~ <lUa,utificath)ll i~ one o1" 
the areas in wli\]ch Nil, interfaces ha,vo a, pot(m- 
tia,l a,(Iva, ni,a,ge over l)otll \['orma\] la, ilgtia,ge.s mid 
~ra, l)hica,1 llsor iuterf'aces. 
In (-onq)a,rison with previous work, w(; ~-we 
able 1,o co\]lsidera, bly silnl)li\[3, tim tra,usla, Cion 
melJlo(l, l,'irst, , s ing  'FLG~ ra, thor t;lia.u a, 
f'o.~-l,|AlrO-s|;l'llC(,lll'(~ ibrma,lisin provides ~ much 
s\]nll)le.r method for ('oi).,sl;rucl;ilig s(mla, nl,i(: rol)re- 
sonl;a,tion~. ,g(~(:on(1, using LAIlcl, :+'4 a,ll iul;(;rnl(;- 
dia,te m(;a,,in<e; r(q)r(;s(mta, tion la,nB;ua,B;e yields a, 
lu,(:h lnor(; stra,iglH;\['orwazd trnllsla,tion tha, n us- 
in<e; a restr icted T I .  
0.o  re.st, l)e:~r in mi.d~ tha,t our iml)IOmo.t~- 
tion in at IJm prototypo stago. Turuing it into ;~ 
l)ractica,l tool would roq. iro considora, blo work, 
as i8 true o1:' most  COml)a, ra, ble systems.  Futuro 
work includos increa,sod NI, cov(;ra,ge, adding a, 
(lisa, nd)il4,a, t ion mod. lo ,  ha,ndling nonl imd a, ud 
tonipol:a,I a na, i)hora, , allowing uiull,il)l(;-senl;once~ 
(lu(;rlo~, a, nd ~onora,l;\]on of" N\]~ a.ii,~\voi;s \['i'onl the 
resull;s i)resont;(;d I)y tli(; T \ ] ) I} .  
1079 
References  
S. Abiteboul, R. Hull, and V. Vianu. 1995. 
Foundations of Databascs. Addison-Wesley. 
J .F. Allen. 1983. Maintaining knowledge about 
temporal intervals. CACM, 26(11):832 843, 
nOV.  
I. Androutsopoulos, G. l). Ritchie, and 
P. Thanisch. 1995. Natural language inter- 
faces to databases- an introduction. Natural 
language Engineering, 1(1) :29-81. 
I. Androutsopoulos. 1996. A principled Frame- 
work for Constructing Natural Language In- 
terfaces to Temporal Databases. Ph.D. thesis, 
University of Edinburgh. 
J. Barwise and R. Cooper. 1981. Generalized 
quantifiers and natural anguage. Linguistics 
and Philosophy, 4:159-219. 
Michael It. Boehlen, Jan Chomicki, Richard T. 
Snodgrass, and David ~\]bma.n. 1996. Query- 
ing TSQL2 databases with temporal ogic. 
In lb'oceedings of the 5th h~ternational Con,- 
ference on Extending Database Technology 
(EDBT), Avignon, France. 
M. \[l. Boehlen. 1995. Temporal database 
system implementations. Unpublished 
manuscript, Department of Ma.thema.tics and 
Computer Science, Aalborg University. 
B. Carpenter. 1998. Lectures on Type-Logical 
Semantics. MIT Press. 
B. Carpenter. 1999. Type-logical gramn~ar the- 
orem prover, http://www.colloquial.com/tlg. 
J. Clifford. 1990. Fornzal ,5'emantics and Prag- 
rustics for Natural Language Querying. Cam- 
bridge University Press, Cambridge. C~m~- 
bridge Tracts in Theoretical Computer Sci- 
ence 8. 
A. Copestake and K. Sparck Jones. 1990. Nat- 
ural language interfaces to databases. The 
Knowledge Engineering Review, 5(4):225- 
249. 
R. S. Crouch and S. G. Puhmm. 1993. Time 
and modality in a natural anguage interface 
to a planning system. Artificial Intclligenee 
63, 63:265-304. 
A. Van Gelder and R.W. Topor. 1991. Safety 
and translation of relational calculus queries. 
ACM Transaction on Database Systems, 
1.6(2) :235-278, June. 
E. W. Hinrichs. 1988. Tense, quantifiers, and 
contexts. Computational Linguistics, 14:3 
\].~:? 
G. Morrill. 11998. Type Logical Grammar: Cat- 
egorial Logic of ,S'ign, s. Kluwer, Dordrecht. 
R. Nelken and N. Francez. 1999. A seman- 
tics tbr temporal questions. In Geert-Jan M. 
Kruijf and Richard T. Oehrle, editors, Pro- 
ceedings of Formal Grammar i999, pages 
1.3:1 142. 
C. R. Perrault and B. J. Grosz. 1988. Natural 
language interfaces. In H. E. Shrobe, editor, 
Exploring Art,ificial Intelligence, pages 133- 
172. Morgan Kaufmann Publishers Inc., San 
Mateo, California. 
C. Pollard and i. A. Sag. 1994. Ilead Driven 
Phrase StructuT~ Grammar. University of 
Chicago Press, Chicago. 
1. Pratt and N. Francez. 1997. On the se- 
mantics of temporal prepositions and preposi- 
tion phrases. Technical Report UMCS-97-4- 
2, University of Manchester, Department of' 
Computer Science. 
I. Pratt and N. Francez. 2000. Temporal prepo- 
sitions and temporal generalized qnantifiers. 
To appear in Linguistics and Philosophy. 
R. T. Snodgrass, editor. 1995. The TSQL2 
TEmporal query language. Klnwer, Norwell. 
MA. 
R. T. Snodgrass. 2000. Developin, g Time- 
Oriented Database Applicatio'n,s in SQL. 
Morgan Kauflnani~ Publishers, San Fran- 
cisco, CA. 
M. Steedman. 1997. Temporality. In J. Van 
Benthem and A. Ter Meulen, editors, Hand- 
boo~: of loflic and language, pages 895 938. El- 
sevier. 
A. Steiner. 1997. A Generalization Approach to 
Temporal \])ata Models and their hnplemen~ 
ration. F'h.l). thesis, Department lnformatik, 
ETIt Zurich. 
A. Tansel, J. Cliflbrd, S. Gadia, S. Jajodia, 
A. Segev, and R. Snodgrass (eds.). 1993. 
Temporal Databases: Theory, l)esign~ and 
Implementation. Database Systems and Ap- 
plications Series. Benjamin/Cummings, Red- 
wood City, CA. 
1). Toman. 1996. Point vs. Interval-based 
Query Languages for Temporal Databases. In 
Proceedings of the A CM SIGA CT-,S'IGMOD- 
,5'IGAI~,'I ~ POD,5', pages 58-67, Montreal, 
Canada, June. 
1080 
Towards Robust Context-Sensitive Sentence Alignment for Monolingual
Corpora
Rani Nelken and Stuart M. Shieber
Division of Engineering and Applied Sciences
Harvard University
33 Oxford St.
Cambridge, MA 02138
 
nelken,shieber  @deas.harvard.edu
Abstract
Aligning sentences belonging to compa-
rable monolingual corpora has been sug-
gested as a first step towards training
text rewriting algorithms, for tasks such
as summarization or paraphrasing. We
present here a new monolingual sen-
tence alignment algorithm, combining a
sentence-based TF*IDF score, turned into
a probability distribution using logistic re-
gression, with a global alignment dynamic
programming algorithm. Our approach
provides a simpler and more robust solu-
tion achieving a substantial improvement
in accuracy over existing systems.
1 Introduction
Sentence-aligned bilingual corpora are a crucial
resource for training statistical machine trans-
lation systems. Several authors have sug-
gested that large-scale aligned monolingual cor-
pora could be similarly used to advance the perfor-
mance of monolingual text-to-text rewriting sys-
tems, for tasks including summarization (Knight
and Marcu, 2000; Jing, 2002) and paraphras-
ing (Barzilay and Elhadad, 2003; Quirk et al,
2004). Unlike bilingual corpora, such as the Cana-
dian Hansard corpus, which are relatively rare, it is
now fairly easy to amass corpora of related mono-
lingual documents. For instance, with the ad-
vent of news aggregator services such as ?Google
News?, one can readily collect multiple news sto-
ries covering the same news item (Dolan et al,
2004). Utilizing such a resource requires align-
ing related documents at a finer level of resolu-
tion, identifying which sentences from one docu-
ment align with which sentences from the other.
Previous work has shown that aligning related
monolingual documents is quite different from
the well-studied multi-lingual alignment task.
Whereas documents in a bilingual corpus are typ-
ically very closely aligned, monolingual corpora
exhibit a much looser level of alignment, with
similar content expressed using widely divergent
wording, grammatical form, and sentence order.
Consequently, many of the simple surface-based
methods that have proven to be so successful in
bilingual sentence alignment, such as correlation
of sentence length, linearity of alignment, and a
predominance of one-to-one sentence mapping,
are much less likely to be effective for monolin-
gual sentence alignment.
Barzilay and Elhadad (2003) suggested that
these disadvantages could be at least partially off-
set by the recurrence of the same lexical items in
document pairs. Indeed, they showed that a sim-
ple cosine word-overlap score is a good baseline
for the task, outperforming much more sophisti-
cated methods. They also observed that context is
a powerful factor in determining alignment. They
illustrated this on a corpus of Encyclopedia Bri-
tannica entries describing world cities, where each
entry comes in two flavors, the comprehensive en-
cyclopedia entry, and a shorter and simpler ele-
mentary version. Barzilay and Elhadad used con-
text in two different forms. First, using inter-
document context, they took advantage of com-
monalities in the topical structure of the encyclo-
pedia entries to identify paragraphs that are likely
to be about the same topic. They then took ad-
vantage of intra-document context by using dy-
namic programming to locally align sequences of
sentences belonging to paragraphs about the same
topic, yielding improved accuracy on the corpus.
While powerful, such commonalities in document
structure appear to be a special feature of the
Britannica corpus, and therefore cannot be relied
upon for other corpora.
In this paper we present a novel algorithm for
sentence alignment in monolingual corpora. At
the core of the algorithm is a classical similar-
161
ity score based on differentially weighting words
according to their Term Frequency-Inverse Doc-
ument Frequency (TF*IDF) (Spa?rck-Jones, 1972;
Salton and Buckley, 1988). We treat sentences as
documents, and the collection of sentences in the
two documents being compared as the document
collection, and use this score to estimate the prob-
ability that two sentences are aligned using logis-
tic regression. Surprisingly, this approach by it-
self yields competitive accuracy, yielding the same
level of accuracy as Barzilay and Elhadad?s algo-
rithm, and higher than all previous approaches on
the Britannica corpus. Such matching, however,
is still noisy. We further improve accuracy by us-
ing a global alignment dynamic programming al-
gorithm, which prunes many spurious matches.
Our approach validates Barzilay and Elhadad?s
observation regarding the utility of incorporating
context. In fact, we are able to extract more infor-
mation out of the intra-document context. First, by
using TF*IDF at the level of sentences, we weigh
words in a sentence with respect to other sentences
of the document. Second, global alignment takes
advantage of (noisy) linear order of sentences. We
make no use of inter-document context, and in par-
ticular make no assumptions about common topi-
cal structure that are unique to the Britannica cor-
pus, thus ensuring the scalability of the approach.
Indeed, we successfully apply our algorithm to
a very different corpus, the three Synoptic gospels
of the New Testament: Matthew, Mark, and Luke.
Putting aside any religious or theological signifi-
cance of these texts, they offer an excellent data
source for studying alignment, since they contain
many parallels, which have been conveniently an-
notated by bible scholars (Aland, 1985). Our algo-
rithm achieves a significant improvement over the
baseline for this corpus as well, demonstrating the
general applicability of our approach.
2 Related work
Several authors have tackled the monolingual sen-
tence correspondence problem. SimFinder (Hatzi-
vassiloglou et al, 1999; Hatzivassiloglou et al,
2001) examined 43 different features that could
potentially help determine the similarity of two
short text units (sentences or paragraphs). Of
these, they automatically selected 11 features, in-
cluding word overlap, synonymy as determined
by WordNet (Fellbaum, 1998), matching proper
nouns and noun phrases, and sharing semantic
classes of verbs (Levin, 1993).
The Decomposition method (Jing, 2002) re-
lies on the observation that document summaries
are often constructed by extracting sentence frag-
ments from the document. It attempts to identify
such extracts, using a Hidden Markov Model of
the process of extracting words. The HMM uses
features of word identity and document position,
in which transition probabilities are based on lo-
cality assumptions. For instance, after a word is
extracted, an adjacent word or one that belongs to
a nearby sentence is more likely to be extracted
than one that is further away.
Barzilay and Elhadad (2003) apply a 4-step al-
gorithm:
1. Cluster the paragraphs of the training docu-
ments into topic-specific clusters, based on
word overlap. For instance, paragraphs in
the Britannica city entries describing climate
might cluster together.
2. Learn mapping rules between paragraphs of
the full and elementary versions, taking the
word-overlap and the clusters as features.
3. Given a new pair of texts, identify sentence
pairs with high overlap, and take these to be
aligned. Then, classify paragraphs accord-
ing to the clusters learned in Step 1, and use
the mapping rules of Step 2 to match pairs of
paragraphs between the documents.
4. Finally, take advantage of the paragraph clus-
tering and mapping, by locally aligning only
sentences belonging to mapped paragraph
pairs.
Dolan et al (2004) used Web-aggregated news
stories to learn both sentence-level and word-level
alignments. Having collected a large corpus of
clusters of related news stories from Google and
MSN news aggregator services, they first seek re-
lated sentences, using two methods. First, using
a high Levenshtein distance score they identify
139K sentence pairs of which about 16.7% are es-
timated to be unrelated (using human evaluation of
a sample). Second, assuming that the first two sen-
tences of related news stories should be matched,
provided they have a high enough word-overlap,
yields 214K sentence pairs of which about 40%
are estimated to be unrelated. No recall estimates
162
are provided; however, with the release of the an-
notated Microsoft Research Paraphrase Corpus,1
it is apparent that Dolan et al are seeking much
more tightly related pairs of sentences than Barzi-
lay and Elhadad, ones that are virtually semanti-
cally equivalent. In subsequent work, the same au-
thors (Quirk et al, 2004) used such matched sen-
tence pairs to train Giza++ (Och and Ney, 2003)
on word-level alignment.
The recent PASCAL ?Recognizing Textual En-
tailment? (RTE) challenge (Dagan et al, 2005) fo-
cused on the problem of determining whether one
sentence entails another. Beyond the difference in
the definition of the required relation between sen-
tences, the RTE challenge focuses on isolated sen-
tence pairs, as opposed to sentences within a doc-
ument context. The task was judged to be quite
difficult, with many of the systems achieving rela-
tively low accuracy.
3 Data
The Britannica corpus, collected and annotated
by Barzilay and Elhadad (2003), consists of 103
pairs of comprehensive and elementary encyclope-
dia entries describing major world cities. Twenty
of these document pairs were annotated by human
judges, who were asked to mark sentence pairs
that contain at least one clause expressing the same
information, and further split into a training and
testing set.
As a rough indication of the diversity of the
dataset and the difference of the task from bilin-
gual alignment, we define the alignment diver-
sity measure (ADM) for two texts, T1   T2, to be:
2  matches

T1  T2 

T1
 
T2
 , where matches is the number of
matching sentence pairs. Intuitively, for closely
aligned document pairs, as prevalent in bilingual
alignment, one would expect an ADM value close
to 1. The average ADM value for the training doc-
ument pairs of the Britannica corpus is 0  26.
For the gospels, we use the King James ver-
sion, available electronically from the Sacred Text
Archive.2 The gospels? lengths span from 678
verses (Mark) to 1151 verses (Luke), where we
treat verses as sentences. For training and eval-
uation purposes, we use the list of parallels given
by Aland (1985).3 We use the pair Matthew-Mark
1http://research.microsoft.com/
research/downloads/
2http://www.sacred-texts.com
3The parallels are available online from http://www.
bible-researcher.com/parallels.html.
for training and the two pairs: Matthew-Luke and
Mark-Luke for testing. Whereas for the Britannica
corpus parallels were marked at the resolution of
sentences, Aland?s annotation presents parallels as
matched sequences of verses, known as pericopes.
For instance, Matthew:4.1-11 matches Mark:1.12-
13. We write v 	 p to indicate that verse v belongs
to pericope p.4
4 Algorithm
We now describe the algorithm, starting with the
TF*IDF similarity score, followed by our use of
logistic regression, and the global alignment.
4.1 From word overlap to TF*IDF
Barzilay and Elhadad (2003) use a cosine mea-
sure of word-overlap as a baseline for the task.
As can be expected, word overlap is a relatively
effective indicator of sentence similarity and re-
latedness (Marcu, 1999). Unfortunately, plain
word-overlap assigns all words equal importance,
not even distinguishing between function and con-
tent words. Thus, once the overlap threshold is
decreased to improve recall, precision degrades
rapidly. For instance, if a pair of sentences has
one or two words in common, this is inconclusive
evidence of their similarity or difference.
One way to address this problem is to differ-
entially weight words using the TF*IDF scoring
scheme, which has become standard in Informa-
tion Retrieval (Salton and Buckley, 1988). IDF
was also used for the similar task of directional en-
tailment by Monz and de Rijke (2001). To apply
this scheme for the task at hand we diverge from
the standard IDF definition by viewing each sen-
tence as a document, and the pair of documents as
a combined collection of N single-sentence docu-
ments. For a term t in sentence s, we define TFs 
 t 
to be a binary indicator of whether t occurs in s,5
and DF


t  to be the number of sentences in which
t occurs. The TF*IDF weight is:
ws 
 t  de f TFs 
 t  log

N
DF


t 
.
4The annotation of matched pericopes induces a partial
segmentation of each gospel into paragraph-like segments.
Since this segmentation is part of the gold annotation, we do
not use it in our algorithm.
5Using a binary indicator rather than the more typical
number of occurrences yielded better accuracy on the Bri-
tannica training set. This is probably due to the ?documents?
being only of sentence length.
163
 1
 0.8
 0.6
 0.5
 0.4
 0.276
 0.2
 0
 1 0.8 0.6 0.4 0.2 0
pr
ob
ab
ilit
y
similarity
pr
ob
ab
ilit
y
pr
ob
ab
ilit
y
Figure 1: Logistic Regression for Britannica train-
ing data
We use these scores as the basis of a standard
cosine similarity measure,
sim


s1   s2 
s1   s2

s1
 
s2


?t ws1

t
 
ws2

t


?t w2s1

t

?t w2s2

t

.
We normalize terms by using Porter stem-
ming (Porter, 1980). For the Britannica corpus, we
also normalized British/American spelling differ-
ences using a small manually-constructed lexicon.
4.2 Logistic regression
TF*IDF scores provide a numeric measure of sen-
tence similarity. To use them for choosing sen-
tence pairs, we proceeded to learn a probability of
two sentences being matched, given their TF*IDF
similarity score, pr


match  1  sim  . We expect
this probability to follow a sigmoid-shaped curve.
While it is always monotonically increasing, the
rate of ascent changes; for very low or very high
values it is not as steep as for middle values. This
reflects the intuition that while we always prefer a
higher scoring pair over a lower scoring pair, this
preference is more pronounced in the middle range
than in the extremities.
Indeed, Figure 1 shows a graph of this distri-
bution on the training part of the Britannica cor-
pus, where point


x
 
y  represents the fraction y of
correctly matched sentences of similarity x. Over-
layed on top of the points is a logistic regression
model of this distribution, defined as the function
p 
ea

bx
1  ea

bx ,
where a and b are parameters. We used
Weka (Witten and Frank, 1999) to automatically
learn the parameters of the distribution on the
training data. These are set to a  7  89 and
b  27  56 for the Britannica corpus.
1 2 3 4
a b c
pg2
pg1
Figure 2: Reciprocal best hit example. Arrows in-
dicate the best hit for each verse. The pairs con-
sidered correct are  2
 
b  and  4
 
c  .
Logistic regression scales the similarity scores
monotonically but non-linearly. In particular, it
changes the density of points at different score
levels. In addition, we can use this distribution
to choose a threshold, th, for when a similarity
score is indicative of a match. Optimizing the
F-measure on the training data using Weka, we
choose a threshold value of th  0  276. Note
that since the logistic regression transformation is
monotonic, the existence of a threshold on proba-
bilities implies the existence of a threshold on the
original sim scores. Moreover, such a threshold
might be obtained by means other than logistic re-
gression. The scaling, however, will become cru-
cial once we do additional calculations with these
probabilities in Section 4.4.
Applying logistic regression to the gospels is
complicated by the fact that we only have a cor-
rect alignment at the resolution of pericopes, and
not individual verses. Verse pairs that do not be-
long to a matched pericope pair can be safely con-
sidered unaligned, but for a matched pericope pair,
pg1   pg2 , we do not know which verse is matched
with which. We solve this by searching for the
reciprocal best hit, a method often used to find
orthologous genes in related species (Mushegian
and Koonin, 1996). For each verse in each peri-
cope, we find the top matching verse in the other
pericope. We take as correct all and only pairs
of verses x
 
y, such that x is y?s best match and y
is x?s best match. An example is shown in Fig-
ure 2. Taking these pairs as matched yields an
ADM value of 0  34 for the training pair of doc-
uments.
We used the reciprocally best-matched pairs of
the training portion of the gospels to find logistic
regression parameters


a 	 9  60
 
b  25  00  , and
164
a threshold,


th  0  250  . Note that we rely on this
matching only for training, but not for evaluation
(see Section 5.2).
4.3 Method 1: TF*IDF
As a simple method for choosing sentence pairs,
we just select all sentence pairs with pr


match  
th. We use the following additional heuristics:
 We unconditionally match the first sentence
of one document with the first sentence of
the other document. As noted by Quirk et al
(2004), these are very likely to be matched,
as verified on our training set as well.
 We allow many-to-one matching of sen-
tences, but limit them to at most 2-to-1 sen-
tences in both directions (by allowing only
the top two matches per sentence to be cho-
sen), since such multiple matchings often
arise due to splitting a sentence into two, or
conversely, merging two sentences into one.
4.4 Method 2: TF*IDF + Global alignment
Matching sentence pairs according to TF*IDF ig-
nores sentence ordering completely. For bilingual
texts, Gale and Church (1991) demonstrated the
extraordinary effectiveness of a global alignment
dynamic programming algorithm, where the basic
similarity score was based on the difference in sen-
tence lengths, measured in characters. Such meth-
ods fail to work in the monolingual case. Gale
and Church?s algorithm (using the implementation
of Danielsson and Ridings (1997)) yields 2% pre-
cision at 2.85% recall on the Britannica corpus.
Moore?s algorithm (2002), which augments sen-
tence length alignment with IBM Model 1 align-
ment, reports zero matching sentence pairs (re-
gardless of threshold).
Nevertheless, we expect sentence ordering can
provide important clues for monolingual align-
ment, bearing in mind two main differences from
the bilingual case. First, as can be expected by the
ADM value, there are many gaps in the alignment.
Second, there can be large segments that diverge
from the linear order predicted by a global align-
ment, as illustrated by the oval in Figure 3 (Figure
2, (Barzilay and Elhadad, 2003)).
To model these features of the data, we use a
variant of Needleman-Wunsch alignment (1970).
We compute the optimal alignment between sen-
tences 1   i of the comprehensive text and sentences
1   j of the elementary version by
 0
 50
 100
 150
 200
 250
 0  5  10  15  20  25  30
Se
nt
en
ce
s 
in
 c
om
pr
eh
en
siv
e 
ve
rs
io
n
Sentences in elementary version
Manual alignment
Se
nt
en
ce
s 
in
 c
om
pr
eh
en
siv
e 
ve
rs
io
n
Figure 3: Gold alignment for a text from the Bri-
tannica corpus.
s


i
 
j   max


s


i  1
 
j  1   pr


match


i
 
j  
s


i  1
 
j   pr


match


i
 
j  
s


i
 
j  1   pr


match


i
 
j  
Note that the dynamic programming sums match
probabilities, rather than the original sim scores,
making crucial use of the calibration induced by
the logistic regression. Starting from the first pair
of sentences, we find the best path through the ma-
trix indexed by i and j, using dynamic program-
ming. Unlike the standard algorithm, we assign no
penalty to off-diagonal matches, allowing many-
to-one matches as illustrated schematically in Fig-
ure 4. This is because for the loose alignment ex-
hibited by the data, being off-diagonal is not in-
dicative of a bad match. Instead, we prune the
complete path generated by the dynamic program-
ming using two methods. First, as in Section 4.3,
we limit many-to-one matches to 2-to-1, by al-
lowing just the two best matches per sentence to
be included. Second, we eliminate sentence pairs
with very low match probabilities


pr


match 
0  005  , a value learned on the training data. Fi-
nally, to deal with the divergences from the lin-
ear order, we add the top n pairs with very high
match probability, above a higher threshold, th 	 .
Optimizing on the training data, we set n  5 and
th 	  0  65 for both corpora.
Note that although Barzilay and Elhadad also
used an alignment algorithm, they restricted it
only to sentences judged to belong to topically re-
lated paragraphs. As noted above, this restriction
relies on a special feature of the corpus, the fact
that encyclopedia entries follow a relatively regu-
lar structure of paragraphs. By not relying on such
165



Figure 4: Global alignment
corpus-specific features, our approach gains in ro-
bustness.
5 Evaluation
5.1 Britannica corpus
Precision/recall curves for both methods, aggre-
gated over all the documents of the testing por-
tion of the Britannica corpus are given in Fig-
ure 5. To obtain different precision/recall points,
we vary the threshold above which a sentence pair
is deemed matched. Of course, when practically
applying the algorithm, we have to pick a partic-
ular threshold, as we have done by choosing th.
Precision/recall values at this threshold are also in-
dicated in the figure.6
 1
 0.9
 0.8
 0.7
 0.6
 0.7 0.6 0.558 0.5 0.4 0.3
Pr
ec
is
io
n
Recall
TF*IDF + Align
Pr
ec
is
io
n
TF*IDF
Pr
ec
is
io
n
Precision @ 55.8 Recall
Pr
ec
is
io
n
Pr
ec
is
io
n
Precision/Recall @ th
Figure 5: Precision/Recall curves for the Britan-
nica corpus
Comparative results with previous algorithms
are given in Table 1, in which the results for Barzi-
lay and Elhadad?s algorithm and previous ones are
taken from Barzilay and Elhadad (2003). The pa-
per reports the precision at 55.8% recall, since
the Decomposition method (Jing, 2002) only pro-
duced results at this level of recall, as some of the
method?s parameters were hard-coded.
Interestingly, the TF*IDF method is highly
competitive in determining sentence similarity.
6Decreasing the threshold to 0.0 does not yield all pairs,
since we only consider pairs with similarity strictly greater
than 0.0, and restrict many-to-one matches to 2-to-1.
Algorithm Precision
SimFinder 24%
Word Overlap 57.9%
Decomposition 64.3%
Barzilay & Elhadad 76.9%
TF*IDF 77.0%
TF*IDF + Align 83.1%
Table 1: Precision at 55.8% Recall
Despite its simplicity, it achieves the same perfor-
mance as Barzilay and Elhadad?s algorithm,7 and
is better than all previous ones. Significant im-
provement is achieved by adding the global align-
ment.
Clearly, the method is inherently limited in that
it can only match sentences with some lexical
overlap. For instance, the following sentence pair
that should have been matched was missed:
 Population soared, reaching 756,000 by
1903, and urban services underwent exten-
sive modification.
 At the beginning of the 20th century, Warsaw
had about 700,000 residents.
Matching ?1903? with ?the beginning of the
20th century? goes beyond the scope of any
method relying predominantly on word identity.
The hope is, however, that such mappings could
be learned by amassing a large corpus of accu-
rately sentence-aligned documents, and then ap-
plying a word-alignment algorithm, as proposed
by Quirk et al (2004). Incidentally, examining
sentence pairs with high TF*IDF similarity scores,
there are some striking cases that appear to have
been missed by the human judges. Of course, we
faithfully and conservatively relied on the human
annotation in the evaluation, ignoring such cases.
5.2 Gospels
For evaluating our algorithm?s accuracy on the
gospels, we again have to contend with the fact
that the correct alignments are given at the resolu-
tion of pericopes, not verses. We cannot rely on
the reciprocal best hit method we used for train-
ing, since it relies on the TF*IDF similarity scores,
which we are attempting to evaluate. We therefore
devise an alternative evaluation criterion, counting
7We discount the minor difference as insignifi cant.
166
a pair of verses as correctly aligned if they belong
to a matched pericope in the gold annotation.
Let Gold


g1   g2  be the set of matched pericope
pairs for gospels g1   g2, according to Aland (1985).
For each pair of matched verses, vg1   vg2 , we count
the pair as a true positive if and only if there is
a pericope pair  pg1   pg2  	 Gold 
 g1   g2  such that
vgi 	 pgi   i  1   2. Otherwise, it is a false positive.
Precision is defined as usual (P  t p  


t p  f p  ).
For recall, we note that not all the verses of a
matched pericope should be matched, especially
when one pericope has substantially more verses
than the other. In general, we may expect the num-
ber of verses to be matched to be the minimum of
 pg1  and  pg2  . We thus define recall as:
R  t p  

?

pg1  pg2  Gold

g1  g2 
min


 pg1     pg2  	 .
The results are given in Figure 6, including the
word-overlap baseline, TF*IDF ranking with lo-
gistic regression, and the added global alignment.
Once again, TF*IDF yields a substantial improve-
ment over the baseline, and results are further im-
proved by adding the global alignment.
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.6 0.5 0.4 0.3 0.2 0.1 0
Pr
ec
is
io
n
Recall
TF*IDF + Align
Pr
ec
is
io
n
TF*IDF
Pr
ec
is
io
n
Overlap
Figure 6: Precision/Recall curves for the gospels
6 Conclusions and future work
For monolingual alignment to achieve its full po-
tential for text rewriting, huge amounts of text
would need to be accurately aligned. Since mono-
lingual corpora are so noisy, simple but effective
methods as described in this paper will be required
to ensure scalability.
We have presented a novel algorithm for align-
ing the sentences of monolingual corpora of com-
parable documents. Our algorithm not only yields
substantially improved accuracy, but is also sim-
pler and more robust than previous approaches.
The efficacy of TF*IDF ranking is remarkable in
the face of previous results. In particular, TF*IDF
was not chosen by the feature selection algorithm
of Hatzivassiloglou et al (2001), who directly ex-
perimented and rejected TF*IDF measures as be-
ing less effective in determining similarity. We be-
lieve this striking difference can be attributed to
the source of the weights. Recall that our TF*IDF
weights treat each sentence as a separate docu-
ment for the purpose of weighting. TF*IDF scores
used in previous work are likely to have been ob-
tained either by aggregation over the full docu-
ment corpus, or by comparison with an external
general collection, which is bound to yield lower
discriminative power. To illustrate this, consider
two words, such as the name of a city, and the
name of a building in that city. Viewed globally,
both words are likely to belong to the long tail
of the Zipf distribution, having almost indistin-
guishable logarithmic IDF. However, in the ency-
clopedia entry describing the city, the city?s name
is likely to appear in many sentences, while the
building name may appear only in the single sen-
tence that refers to it, and thus the latter should
be scored higher. Conversely, a word that is rela-
tively frequent in general usage, e.g., ?river? might
be highly discriminative between sentences.
We further improve on the TF*IDF results by
using a global alignment algorithm. We expect
that more sophisticated sequence alignment tech-
niques, as studied for biological sequence anal-
ysis, might yield improved results, in particular
for comparing loosely matched document pairs in-
volving non-linear text transformations such as in-
versions and translocations. Such methods could
still modularly rely on the TF*IDF scoring.
We reiterate Barzilay and Elhadad?s conclusion
about the effectiveness of using the document con-
text for the alignment of text. In fact, we are
able to take better advantage of the intra-document
context, while not relying on any assumptions
about inter-document context that might be spe-
cific to one particular corpus. Identifying scalable
principles for the use of inter-document context
poses a challenging topic for future research.
We have restricted our attention here to pre-
annotated corpora, allowing better comparison
with previous work, and sidestepping the labor-
intensive task of human annotation. Having es-
167
tablished a simple and robust document alignment
method, we leave its application to much larger-
scale document sets for future work.
Acknowledgments
We thank Regina Barzilay and Noemie Elhadad
for providing access to the annotated Britannica
corpus, and for discussion. This work was sup-
ported in part by National Science Foundation
grant BCS-0236592.
References
Kurt Aland, editor. 1985. Synopsis Quattuor Evange-
liorum. American Bible Society, 13th edition, De-
cember.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entail-
ment challenge. In Proceedings of the PASCAL
Challenges Workshop on Recognising Textual En-
tailment, pages 1?8, April.
Pernilla Danielsson and Daniel Ridings. 1997. Prac-
tical presentation of a vanilla aligner. Research re-
ports from the Department of Swedish, Goeteborg
University GU-ISS-97-2, Sprakdata, February.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th International Con-
ference on Computational Linguistics (COLING-
2004), Geneva, Switzerland, August.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
William A. Gale and Kenneth W. Church. 1991. A
program for aligning sentences in bilingual corpora.
In Meeting of the Association for Computational
Linguistics, pages 177?184.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combi-
nations via machine learning. In Proceedings of the
1999 Joint SIGDAT conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 203?212, College Park, Maryland.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
SIMFINDER: A flexible clustering tool for sum-
marization. In Proceedings of the Workshop on
Automatic Summarization, pages 41?49. Associa-
tion for Computational Linguistics, 2001.
Hongyan Jing. 2002. Using hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?543.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization ? step one: Sentence compres-
sion. In Proceedings of the American Association
for Artificial Intelligence conference (AAAI).
Beth Levin. 1993. English Verb Classes And Alterna-
tions: A Preliminary Investigation. The University
of Chicago Press.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ?99: Proceedings of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, August 15-19,
1999, Berkeley, CA, USA, pages 137?144. ACM.
Christof Monz and Maarten de Rijke. 2001. Light-
weight subsumption checking for computational
semantics. In Patrick Blackburn and Michael
Kohlhase, editors, Proceedings of the 3rd Workshop
on Inference in Computational Semantics (ICoS-3),
pages 59?72.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Stephen D.
Richardson, editor, AMTA, volume 2499 of Lec-
ture Notes in Computer Science, pages 135?144.
Springer.
Arcady R. Mushegian and Eugene V. Koonin. 1996.
A minimal gene set for cellular life derived by com-
parison of complete bacterial genomes. Proceedings
of the National Academies of Science, 93:10268?
10273, September.
S.B. Needleman and C.D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. J. Mol.
Biol., 48:443?453.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Martin F. Porter. 1980. An algorithm for suffi x strip-
ping. Program, 14(3):130?137.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 142?149, Barcelona Spain, July.
Gerard Salton and Chris Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Karen Spa?rck-Jones. 1972. Exhaustivity and speci-
fi city. Journal of Documentation, 28(1):11?21.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
168
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 137?140,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mining Wikipedia Revision Histories for Improving Sentence Compression
Elif Yamangil Rani Nelken
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138, USA
{elif,nelken}@eecs.harvard.edu
Abstract
A well-recognized limitation of research on
supervised sentence compression is the dearth
of available training data. We propose a new
and bountiful resource for such training data,
which we obtain by mining the revision his-
tory of Wikipedia for sentence compressions
and expansions. Using only a fraction of the
available Wikipedia data, we have collected
a training corpus of over 380,000 sentence
pairs, two orders of magnitude larger than the
standardly used Ziff-Davis corpus. Using this
newfound data, we propose a novel lexical-
ized noisy channel model for sentence com-
pression, achieving improved results in gram-
maticality and compression rate criteria with a
slight decrease in importance.
1 Introduction
With the increasing success of machine translation
(MT) in recent years, several researchers have sug-
gested transferring similar methods for monolingual
text rewriting tasks. In particular, Knight and Marcu
(2000) (KM) applied a channel model to the task of
sentence compression ? dropping words from an in-
dividual sentence while retaining its important in-
formation, and without sacrificing its grammatical-
ity. Compressed sentences can be useful either on
their own, e.g., for subtitles, or as part of a larger
summarization or MT system. A well-recognized
problem of this approach, however, is data spar-
sity. While bilingual parallel corpora are abundantly
available, monolingual parallel corpora, and espe-
cially collections of sentence compressions are van-
ishingly rare. Indeed, most work on sentence com-
pression has used the Ziff-Davis corpus (Knight and
Marcu, 2000), which consists of a mere 1067 sen-
tence pairs. While data sparsity is a common prob-
lem of many NLP tasks, it is much more severe for
sentence compression, leading Turner and Charniak
(2005) to question the applicability of the channel
model for this task altogether.
Our contribution in this paper is twofold. First,
we solve the data sparsity issue by showing that
abundant sentence compressions can be extracted
from Wikipedia?s revision history. Second, we use
this data to validate the channel model approach
for text compression, and improve upon it by cre-
ating a novel fully lexicalized compression model.
Our model improves grammaticality and compres-
sion rate with only a slight decrease in importance.
2 Data: Wikipedia revision histories as a
source of sentence compressions
Many researchers are increasingly turning to
Wikipedia as a large-scale data source for training
NLP systems. The vast majority of this work uses
only the most recent version of the articles. In fact,
Wikipedia conveniently provides not only the lat-
est version, but the entire revision history of each
of its articles, as dramatically visualized by Vie?gas
et al (2004). Through Wikipedia?s collaborative
editing process, articles are iteratively amended and
refined by multiple Web users. Users can usually
change any aspect of the document?s structure and
content, but for our purposes here, we focus only on
sentence-level edits that add or drop words.
We have downloaded the July snapshot of the
137
English Wikipedia, consisting of 1.4 million arti-
cles, and mined a subset of them for such compres-
sions/expansions. We make the simplifying assump-
tion that all such edits also retain the core mean-
ing of the sentence, and are therefore valid training
data for our purposes. This assumption is of course
patently na??ve, as there are many cases in which such
revisions reverse sentence meaning, add or drop es-
sential information, are part of a flame war, etc.
Classifying these edits is an interesting task which
we relegate to future work.1
From about one-third of the snapshot, we ex-
tracted over 380,000 sentence pairs, which is 2 or-
ders of magnitude more than the Ziff-Davis corpus.2
Wikipedia currently has 2.3 million articles and is
constantly expanding. We can therefore expect an
increase of another order of magnitude. We thus can
afford to be extremely selective of the sentence pairs
we use. To handle a dataset of such size (hundreds of
GBs), we split it into smaller chunks, and distribute
all the processing.
More technically, for each article, we first extract
all revisions, and split each revision into a list of its
sentences. We run an edit-distance comparison be-
tween each such pair, treating each sentence as an
atomic ?letter?. We look for all replacements of one
sentence by another and check whether one sentence
is a compression of the other.3 We then run Collins?
parser (1997), using just the sentence pairs where
parsing succeeds with a negative log likelihood be-
low 200.
3 Noisy channel model
We follow KM in modeling the problem using a gen-
erative noisy channel model, but use the new-found
training data to lexicalize the model. Sentences start
their life in short form, s, are ranked by a source
language model, p(s), and then probabilistically ex-
panded to form the long sentence, p(l|s). During
decoding, given a long sentence, we seek the most
likely short sentence that could have generated it.
1For instance, compressions are more likely to signal op-
tional information than expansions; the lexical items added are
likely to be indicative of the type of edit, etc.
2The sentence pair corpus is available by contacting the
authors.
3We ignore word reorderings or replacements that are be-
yond word addition or deletion.
Using Bayes? rule, this is equivalent to seeking the
short sentence s that maximizes p(s) ? p(l|s).
3.1 Lexicalized channel model
KM?s original model was purely syntax-based.
Daume et al (2002) used a lexicalized PCFG to
rerank the compressions, showing that the addition
of lexical information helps eliminate improbable
compressions. Here, we propose to enhance lexical-
ization by including lexical information within the
channel model, allowing us to better model which
compressions are likely and which are not. A min-
imal example pair illustrating the utility of lexical-
ization is the following.
(1) Hillary barely won the primaries.
(2) Hillary almost won the primaries.
The validity of dropping the adverbial here clearly
depends on the lexical value of the adverb. It is more
acceptable to drop the adverb in Sentence 1, since
dropping it in Sentence 2 reverses the meaning. We
learn probabilities of the form:
p( S[won]
NP[Hillary] ADVP[almost] VP[won]
| S[won]
NP[Hillary] VP[won]
)
Our model has the power of making compression de-
cisions based on lexical dependencies between the
compressed and retained parts of the parse tree.
Note that Daume et al?s reranking model cannot
achieve this type of distinction, since it is based on
reranking the compressed version, at which point the
adverb is no longer available.
Since we are interested not only in learning how
to compress, but also when to compress, we also in-
clude in this procedure unchanged CFG rule pairs
that are attested in the corpus. Thus, different ways
of expanding a CFG rule compete with each other as
well as the possibility of not doing any expansion.
3.2 Smoothing
In order to smooth our estimates we use Witten-Bell
discounting (1991) with 6 levels of back-off. This
method enables us to tune the confidence parameter
associated with an estimate inversely proportionally
with the diversity of the context of the estimate. The
different levels are illustrated in Table 1. Level 1,
138
the most specific level, is fully lexicalized. Transi-
tioning to levels 2 to 4, we lose the lexical informa-
tion about the subtrees that are not dropped, the head
child bearing subtree, and the dropped subtrees, re-
spectively. At level 4, we end up with the non-
lexicalized estimates that are equivalent to KM?s
model. In subsequent back off levels, we abstract
away from the CFG rules. In particular, level 5 es-
timates the probability of dropping subtrees in the
context of a certain parent and head child, and level
6 estimates the probability of the same outcome in
the coarser context of a parent only.
3.3 Source model
In addition to the lexicalized channel model, we also
use a lexicalized probabilistic syntax-based source
model, which we train from the parser?s output on
the short sentences of each pair.
3.4 Decoding
We implemented the forest-based statistical sen-
tence generation method of Langkilde (2000). KM
tailored this method to sentence compression, com-
pactly encoding all compressions of a sentence in
a forest structure. The forest ranking algorithm
which extracts compressed parse trees, optimized
the model scores as well as an additional bigram
score. Since our model is lexicalized, the bigram
scores become less relevant, which was confirmed
by experimentation during development. Therefore
in our implementation we exclude the bigram scores
and other related aspects of the algorithm such as
pruning of bigram-suboptimal phrases.
4 Evaluation
We evaluated our system using the same method as
KM, using the same 32 sentences taken from the
Ziff-Davis corpus. We solicited judgments of im-
portance (the value of the retained information), and
grammaticality for our compression, the KM results,
and human compressions from 8 judges, on a scale
of 1 (worst) to 5 (best). Mean and standard deviation
are shown in Table 2. Our model improves gram-
maticality and compression rate criteria with only a
slight decrease in importance. Here are some illus-
trative examples, with the deleted material shown in
brackets:
(3) The chemical etching process [used for glare
protection] is effective and will help if your
office has the fluorescent-light overkill [that
?s typical in offices].
(4) Prices range from $5,000 [for a microvax
2000] to $179,000 [for the vax 8000 or
higher series].
We suspect that the decrease in importance stems
from our indiscriminative usage of compressions
and expansions to train our system. We hypothesize
that in Wikipedia, expansions often add more useful
information, as opposed to compressions which are
more likely to drop superfluous or erroneous infor-
mation.4 Further work is required to classify sen-
tence modifications.
Since one of our model?s back-off levels simulates
KM?s model, we plan to perform an additional com-
parative evaluation of both models trained on the
same data.
5 Discussion and future work
Turner and Charniak (2005) question the viability
of a noisy channel model for the sentence compres-
sion task. Briefly put, in the typically sparse data
setting, there is no way to distinguish between the
probability of a sentence as a short sentence and its
probability as a regular sentence of English. Fur-
thermore, the channel model is likely to prefer to
leave sentences intact, since that is the most preva-
lent pattern in the training data. Thus, they argue,
the channel model is not really compressing, and it
is only by virtue of the length penalty that anything
gets shortened at all. Our hope here is that by using
a far richer source of short sentences, as well as a
huge source of compressions, we can overcome this
problem. The noisy channel model posits a virtual
competition on each word of coming either from the
source model (in which case it is retained in the com-
pression) or from the channel model (in which case
it is dropped). By having access to a large data set
for the first time, we hope to be able to learn which
parts of the sentence are more likely to come from
4For instance, here is an expansion seen in the data, where
the added information (italicized) is important: ?In 1952 and
1953 he was stationed in Sendai, Japan during the Korean War
and was shot.? It would be undesirable to drop this added
phrase.
139
Back-off level expanded short
1 S[won]? NP[Hillary] ADVP[almost] VP[won] S[won]? NP[Hillary] VP[won]
2 S[won]? NP ADVP[almost] VP[won] S[won]? NP VP[won]
3 S? NP ADVP[almost] VP S? NP VP
4 S? NP ADVP VP S? NP VP
5 parent = S, head-child = VP, child = ADVP parent = S, head-child = VP
6 parent = S, child = ADVP parent = S
Table 1: Back off levels
KM Our model Humans
Compression 72.91% 67.38% 53.33%
Grammaticality 4.02?1.03 4.31?0.78 4.78?0.17
Importance 3.86?1.09 3.65?1.07 3.90?0.58
Table 2: Evaluation results
which of the two parts of the model. Further work is
required in order to clarify this point.
Naturally, discriminative models such as McDon-
ald (2006) are also likely to improve by using the
added data. We leave the exploration of this topic
for future work.
Finally, we believe that the Wikipedia revision
history offers a wonderful resource for many addi-
tional NLP tasks, which we have begun exploring.
Acknowledgments
This work was partially supported by a Google re-
search award, ?Mining Wikipedia?s Revision His-
tory?. We thank Stuart Shieber for his comments on
an early draft of this paper, Kevin Knight and Daniel
Marcu for sharing the Ziff-Davis dataset with us, and
the volunteers for rating sentences. Yamangil thanks
Michael Collins for his feedback on the project idea.
References
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Philip R. Cohen and
WolfgangWahlster, editors, Proceedings of the Thirty-
Fifth Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 16?23, Somerset, New Jersey. As-
sociation for Computational Linguistics.
H. Daume, Kevin Knight, I Langkilde-Geary, Daniel
Marcu, and K Yamada. 2002. The importance of lexi-
calized syntax models for natural language generation
tasks. Proceedings of the Second International Confer-
ence on Natural Language Generation. Arden House,
NJ, July 1-3.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National Con-
ference on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 703?710. AAAI Press / The MIT Press.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the first conference on
North American chapter of the Association for Com-
putational Linguistics, pages 170?177, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Ryan T. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL 2006, 11st Conference of the EuropeanChap-
ter of the Association for Computational Linguistics,
April 3-7, 2006, Trento, Italy, pages 297?304.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
290?297,Morristown, NJ, USA. Association for Com-
putational Linguistics.
Fernanda B. Vie?gas, Martin Wattenberg, and Kushal
Dave. 2004. Studying cooperation and conflict be-
tween authors with istory flow visualizations. In Eliza-
beth Dykstra-Erickson andManfred Tscheligi, editors,
CHI, pages 575?582. ACM.
I.Witten and T. Bell. 1991. The zero-frequencyproblem:
Estimating the probabilities of novel events in adaptive
text compression. IEEE Transactions on Information
Theory, 37(4).
140
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 79?86,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
		
ff