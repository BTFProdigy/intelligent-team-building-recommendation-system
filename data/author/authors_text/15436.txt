Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 43?51,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Asynchronous fixed-grid scanning with dynamic codes
Russ Beckley and Brian Roark
Center for Spoken Language Understanding, Oregon Health & Science University
{beckleyr,roark}@cslu.ogi.edu
Abstract
In this paper, we examine several methods
for including dynamic, contextually-sensitive
binary codes within indirect selection typing
methods using a grid with fixed symbol posi-
tions. Using Huffman codes derived from a
character n-gram model, we investigate both
synchronous (fixed latency highlighting) and
asynchronous (self-paced using long versus
short press) scanning. Additionally, we look
at methods that allow for scanning past a tar-
get and returning to it versus methods that re-
move unselected items from consideration. Fi-
nally, we investigate a novel method for dis-
playing the binary codes for each symbol to
the user, rather than using cell highlighting, as
the means for identifying the required input
sequence for the target symbol. We demon-
strate that dynamic coding methods for fixed
position grids can be tailored for very diverse
user requirements.
1 Introduction
For many years, a key focus in Augmentative and
Alternative Communication (AAC) has been provid-
ing text processing capabilities to those for whom
direct selection of symbols on a keyboard (virtual or
otherwise) is not a viable option. In lieu of direct
selection, a binary (yes/no) response can be given
through any number of switches, including buttons
or pads that are pressed with hand, head, or foot,
eyeblink detectors, or other switches that can lever-
age whatever reliable movement is available. These
indirect selection methods typically involve system-
atically scanning through options and eliciting the
binary yes/no response at each step of scanning. For
example, row/column scanning is a very common
approach for indirect selection. Auto row/column
scanning on a square grid, such as that shown in Fig-
ure 1, will highlight each row in turn for some fixed
duration (dwell time); if the binary switch is trig-
gered before the dwell time expires, the row is se-
lected; otherwise the next row is highlighted. Once
a row is selected, cells in this row are then individu-
ally highlighted in turn, until one is selected, which
identifies the intended character.
This sort of indirect selection method amounts to
assigning a binary code to every symbol in the grid.
If triggering the switch (e.g., pressing a button or
blinking) is taken as a ?yes? or 1, then its absence is
taken as a ?no? or 0. In such a way, every letter in the
grid has a binary code based on the scanning strat-
egy. For example, in Figure 1, the letter ?n? is in the
third row and fourth column; if row scanning starts
at the top, it takes two ?no?s and a ?yes? to select the
correct row; and then three ?no?s and a ?yes? to select
the correct column. This translates to a binary code
of ?0010001?.
In the preceding example, the codes for all sym-
bols are determined by their position in the alpha-
ordered grid. However, faster input can be achieved
by assigning shorter codes to likely symbols. For ex-
ample, imagine a user has just typed ?perso? and is
ready to type the next letter. In this context, the let-
ter ?n? is quite likely in English, hence if a very short
code is assigned to that letter (e.g., ?01?), then the
user requires only two actions (a ?no? and a ?yes?)
to produce the letter, rather than the 7 actions re-
Figure 1: Spelling grid in rough alpha order.
43
quired by the row/column code given above. There
are methods for assigning codes that minimize the
expected code length for a given probability model
(Huffman, 1952). The quality of the probability
model used for deriving codes can make a large dif-
ference in the code length and hence in the efficiency
of the input method. When the model can accurately
assign probabilities to symbols, the shortest binary
codes can be assigned to the likeliest symbols, which
thus require the fewest inputs (either yes or no) from
the user. The best probabilistic models will take into
account what has already been typed to assign prob-
ability to each symbol. The probabilities are contex-
tually dependent, and therefore so are the optimal
binary code assignments. This was illustrated in the
?person? example provided earlier. To provide an-
other example, the probability of the letter ?u? is not
particularly high overall in English (less than 0.02),
but if the previously typed symbol is ?q?, its proba-
bility is very high. Thus, in many contexts, there are
other letters that should get the shortest code, but
in that particular context, following ?q?, ?u? is very
likely, hence it should receive the shortest code.
Common scanning methods, however, present a
problem when trying to leverage contextually sen-
sitive language models for efficient scanning. In
particular, methods of scanning that rely on high-
lighting contiguous regions ? such as widely used
row/column scanning ? define their codes in terms
of location in the grid, e.g., upper left-hand cor-
ner requires fewer keystrokes to select than lower
right-hand corner using row/column scanning. To
improve the coding in such an approach requires
moving characters to short-code regions of the grid.
In other words, with row/column scanning meth-
ods, the symbol needing the shortest code must
move into the upper left-hand corner of the grid.
Yet the cognitive overhead of dealing with frequent
grid reorganization is typically thought to outweigh
any speedup that is achieved through more efficient
coding (Baletsa et al, 1976; Lesher et al, 1998).
If one assumes a fixed grid, i.e., no dynamic re-
organization of the symbols, then row/column scan-
ning can gain efficiency by placing frequent char-
acters in the upper left-hand corner, but cannot use
contextually informed models. This is akin to Morse
code, which assigns fixed codes to symbols based on
overall frequency, without considering context.
Figure 2: Scanning of non-contiguous sets of cells
Roark et al (2010) presented a new approach
which dropped the requirement of contiguous high-
lighting, thus allowing the use of variable codes on a
fixed grid. For example, consider the grid in Figure
2, where two symbols in different rows and columns
are jointly highlighted. This approach, which we
will term ?Huffman scanning?, allowed the binary
codes to be optimized using Huffman coding meth-
ods (see Section 2.2) with respect to contextually
sensitive language models without dynamic reorga-
nization of the grid. The method resulted in typing
speedups over conventional row/column scanning.
One downside to the variable scanning that results
from Huffman scanning is that users cannot antici-
pate their target symbol?s binary code in any given
context. In row/column scanning, the binary code
of each symbol is immediately obvious from its lo-
cation in the grid, hence users can anticipate when
they will need to trigger the switch. In Huffman
scanning, users must continuously monitor and react
when their target cells light up. The time required to
allow for this motor reaction means that scan rates
are typically slower than in row/column scanning;
and stress levels ? due to the demands of immediate
response to highlighting ? higher.
Huffman scanning is not the only way to allow
variable coding on a fixed grid. In this paper, we in-
vestigate alternatives to Huffman scanning that also
allow for efficient coding on a fixed grid. The three
alternative methods that we investigate are asyn-
chronous methods, i.e., all of the scanning is self-
paced; there is no scan rate that must be matched by
the user. Rather than ?yes? being a button press and
?no? a timeout, these approaches, like Morse code,
differentiate between short and long presses1. There
are several benefits of this sort of asynchronous ap-
1Alternatively, two switches can be used.
44
proach: individuals who struggle with the timing re-
quirements of auto, step or directed scanning can
proceed without having to synchronize their move-
ments to the interface; individuals can interrupt their
communication ? e.g., for side talk ? for an arbitrary
amount of time and come back to it in exactly the
same state; and it reduces the stress of constantly
monitoring the scanning sequence and reacting to it
within the time limits of the interface.
The last of our alternative methods is a novel ap-
proach that displays the code for each symbol at
once as a series of dots and dashes underneath the
symbol ? as used in Morse code ? rather than us-
ing cell highlighting to prompt the user as in the
other conditions. Unlike Morse code, these codes
are derived using Huffman coding based on n-gram
language models, thus change with every context.
Since they are displayed for the user, no code mem-
orization is required. This novel interface differs
from Huffman scanning in several ways, so we also
present intermediate methods that differ in only one
or another dimension, so that we can assess the im-
pact of each characteristic.
Our results show that displaying entire codes at
once for asynchronous scanning was a popular and
effective method for indirect selection, despite the
fact that it shared certain dis-preferred characteris-
tics with the least popular of our methods. This
points the way to future work investigating methods
to combine the preferred characteristics from our set
of alternatives into a yet more effective interface.
2 Background and Related Work
2.1 Indirect selection
Some of the key issues influencing the work in this
paper have already been mentioned above, such as
the tradeoffs between fixed versus dynamic grids.
For a full presentation of the range of indirect selec-
tion methods commonly in use, we refer the read-
ers to Beukelman and Mirenda (1998). But in this
section we will highlight several key distinctions of
particular relevance to this work.
As mentioned in the previous section, indirect se-
lection strategies allow users to select target sym-
bols through a sequence of simpler operations, typi-
cally a yes/no indication. This is achieved by scan-
ning through options displayed in the user inter-
face. Beukelman and Mirenda (1998) mention cir-
cular scanning (around a circular interface), linear
scanning (one at a time), and group-item scanning
(e.g., row/column scanning to find the desired cell).
Another variable in scanning is the speed of scan-
ning ? e.g., how long does the highlighting linger
on the options before advancing. Finally, there are
differences in selection control strategy. Beukel-
man and Mirenda (1998) mention automatic scan-
ning, where highlighted options are selected by ac-
tivating a switch, and advance automatically if the
switch is not activated within the specified dwell
time; step scanning, where highlighted options are
selected when the switch is not activated within the
specified dwell time, and advance only if the switch
is activated; and directed scanning, where the high-
lighting moves while the switch is activated and se-
lection occurs when the switch is released. In all of
these methods, synchrony with the scan rate of the
interface is paramount.
Speech and language pathologists working with
AAC users must assess the specific capabilities of
the individual to determine their best interface op-
tion. For example, an individual who has difficulty
precisely timing short duration switch activation but
can hold a switch more easily might do better with
directed scanning.
Morse code, with its dots and dashes, is also an in-
direct selection method that has been used in AAC,
but it is far less common than the above mentioned
approaches due to the overhead of memorizing the
codes. Once learned, however, this approach can
be an effective communication strategy, as discussed
with specific examples in Beukelman and Mirenda
(1998). Often the codes are entered with switches
that allow for easy entry of both dots and dashes,
e.g., using two switches, one for dot and one for
dash. In this study, we have one condition that
is similar to Morse code in using dots and dashes,
but without requiring code memorization2. The in-
terface used for the experiments identifies dots and
dashes with short and long keypresses.
2Thanks to a reviewer for pointing out that DynaVox Series
5 displays dynamically-assigned codes for non-letter buttons in
their Morse code interface, much as we do for the entire symbol
set. In contrast to our approach, their codes are not assigned
using probabilistic models, rather to contrast with the standard
Morse codes, which are used for the letters. Further, the cursor
that we use to identify position within the code (see Section 3.5)
is not used in the Dynavox interface.
45
2.2 Binary codes
In indirect selection, the series of actions required to
select a given character is determined by the binary
code. As mentioned in Section 1, row/column scan-
ning assigns binary codes based on location within
the grid. Ordering the symbols so that frequent
characters are located in the upper left-hand cor-
ner of the grid will provide those frequent charac-
ters with short codes with a row/column scanning
approach, though not the minimal possible binary
codes. Given a probability distribution over sym-
bols, there are known algorithms for building a bi-
nary code that has the minimum expected bits ac-
cording to the distribution, i.e., codes will be op-
timally short (Huffman, 1952). The quality of the
codes, however, depends on the quality of the prob-
ability model, i.e., whether the model fits the actual
distribution in that context.
Roark et al (2010) presented a scanning approach
for a fixed grid that used Huffman codes derived
from n-gram language models (see Section 2.3).
The approach leveraged better probability models to
achieve shorter code lengths, and achieved an over-
all speedup over row/column scanning for the 10
subjects in the trial, despite the method being closely
tied to reaction time. The method requires monitor-
ing of the target cell in the grid and reaction when it
is highlighted, since the pattern of highlighting is not
predictable from symbol position in the grid, unlike
row/column scanning.
2.3 Language modeling
Language models assign probabilities to strings in
the language being modeled, which has broad utility
for many tasks in speech and language processing.
The most common language modeling approach is
the n-gram model, which estimates probabilities of
strings as the product of the conditional probability
of each symbol given previous symbols in the string,
under a Markov assumption. That is, for a string
S = s1 . . . sn of n symbols, a k+1-gram model is
defined as
P(S) = P(s1)
n?
i=2
P(si | s1 . . . si?1)
? P(s1)
n?
i=2
P(si | si?k . . . si?1)
where the approximation is made by imposing the
Markov assumption. Note that the probability of the
first symbol s1 is typically conditioned on the fact
that it is first in the string. Each of the conditional
probabilities in such a model is a multinomial dis-
tribution over the symbols in a vocabulary ?, and
the models are typically regularized (or smoothed)
to avoid assigning zero probability to strings in ??.
See Chen and Goodman (1998) for an excellent
overview of modeling and regularization methods.
For the current application, the conditional prob-
ability P(si | si?k . . . si?1) can be used to as-
sign probabilities to all possible next symbols, and
these probabilities can be used to assign Huff-
man codes. For example, if the user has typed
?the perso? and is preparing to type the next letter,
we estimate P( n | t h e p e r s o ) as well as
P( m | t h e p e r s o ) and every other possi-
ble next symbol, from a large corpus. Note that
smoothing methods mentioned above ensure that ev-
ery symbol receives non-zero probability mass. Also
note that the space character (represented above as
? ?) is a symbol in the model, hence the models take
into account context across word boundaries. Given
these estimated probabilities, known algorithms for
assigning Huffman codes are used to assign short
codes to the most likely next symbols, in a way that
minimizes expected code length.
3 Methods
Since this paper aims to compare new methods with
Huffman scanning presented in Roark et al (2010),
we follow that paper in many key respects, including
training data, test protocol, and evaluation measures.
For all trials we use a 6?6 grid, as shown in Fig-
ures 1 and 2, which includes the 26 characters in the
English alphabet, 8 punctuation characters (comma,
period, double quote, single quote, dash, dollar sign,
colon and semi-colon), a white space delimiter (de-
noted with underscore) and a delete symbol (de-
noted with ?). Unlike Roark et al (2010), our
grid is in rough alphabetic order rather than in fre-
quency order. In that paper, they compared Huffman
scanning with row/column scanning, which would
have been put at a disadvantage with alphabetic or-
der, since frequent characters would have received
longer codes than they do in a frequency ordered
grid. In this paper, however, all of the approaches
46
are using Huffman codes and scanning of possibly
non-contiguous subsets of characters, so the code
efficiency does not depend on location in the grid.
Thus for ease of visual scanning, we chose in this
study to use alphabetic ordering.
3.1 Language models and binary codes
We follow Roark et al (2010) and build character-
based smoothed 8-gram language models from a
normalized 42M character subset of the English gi-
gaword corpus and the CMU pronunciation dictio-
nary. This latter lexicon is used to increase coverage
of words that are unobserved in the corpus, and is in-
cluded in training as one observation per word in the
lexicon. Smoothing is performed with a generalized
version of Witten-Bell smoothing (Witten and Bell,
1991) as presented in Carpenter (2005). Text nor-
malization and smoothing parameterizations were as
presented in Roark et al (2010). Probability of the
delete symbol ? was taken to be 0.05 in all trials
(the same as the probability of an error, see Sec-
tion 3.2), and all other probabilities derived from the
trained n-gram language model.
3.2 Huffman scanning
Our first scanning condition replicates the Huffman
scanning from Roark et al (2010), with two differ-
ences. First, as stated above, we use an alphabetic
ordering of the grid as shown in Figure 2, in place
of their frequency ordered grid. Second, rather than
calibrating the scan rate of each individual, we fixed
the scan rate at 600 ms across all subjects.
One key aspect of their method is dealing with
errors of omission and commission, i.e., what hap-
pens when a subject misses their target symbol. In
standard row/column scanning, rows are highlighted
starting from the top of the grid, incrementing down-
wards one row at a time. If no row has been selected
after iterating through all rows, the scanning begins
again at the top. In such a way, if the subject mistak-
enly neglects to select their intended row, they can
just wait until it is highlighted again. Similarly, if the
wrong row is selected, there is usually a mechanism
whereby the columns are scanned for some number
of iterations, at which point row scanning resumes.
The upshot of this is that users can make an error and
still manage to select their intended symbol after the
scanning system returns to it.
Roark et al (2010) present a method for allow-
ing the same kind of robustness to error in Huff-
man scanning, by recomputing the Huffman code
after every bit. If the probability that the bit was
correct is p, then the probability that it was incor-
rect is 1?p. In Huffman scanning, a subset is high-
lighted and the user indicates yes or no ? yes, the
target symbol is in the set; or no, the target symbol
is not in the set. If the answer is ?yes? and the set
includes exactly one symbol, it is typed. Otherwise,
for all symbols in the selected set (highlighted sym-
bols if ?yes?; non-highlighted if ?no?), their proba-
bilities are multiplied by p (the probability of being
correct), while the probabilities of the other set of
symbols are multiplied by 1?p. The probabilities
are then re-normalized and a new Huffman code is
generated, the first bit of which drives which sym-
bols are highlighted at the next step. In such a way,
even if the target symbol is in the highlighted set
when it is not selected (or vice versa), it is not elim-
inated from consideration; rather its probability is
diminished (by multiplying by 1?p, which in this
paper is set to 0.05) and scanning continues. Even-
tually the symbol will be highlighted again, much
as is the case in row/column scanning. We also use
this method within the Huffman scanning condition
reported in this paper.
3.3 Asynchronous scanning
Our second condition replaces the scan rate of 600
ms from the Huffman scanning approach outlined
in Section 3.2 with an asynchronous approach that
does not rely upon a scan rate. The grid and scan-
ning method remain identical, but instead of switch
versus no switch, we use short switch (rapid release)
versus long switch (slower release). This is similar
to the dot/dash distinction in Morse code. For this
paper, we used a threshold of 200 ms to distinguish
a short versus a long switch, i.e., if the button press
is released within 200 ms it is short; otherwise long.
Since Huffman scanning already has switch activa-
tion as ?yes?, this could be thought of as having the
long press replace no-press in the interface.
With this change, the scanning does not automat-
ically advance to the next set, but waits indefinitely
for the user to enter the next bit of the code. The
same method for dealing with errors as with Huff-
man scanning is employed in this condition, i.e., re-
47
Figure 3: Scanning of non-contiguous sets of cells, with
symbols that have been eliminated from consideration
deemphasized (a, b, c, e, o, t)
computing the Huffman code after every bit and tak-
ing into account the probability of the bit being in
error. One might see this as a self-paced version of
Huffman scanning.
One benefit of this approach is that it does not re-
quire the user to synchronize their movements to a
particular scan rate of the interface. One potential
downside for some users is that it does require more
active keypresses than auto scanning. In auto scan-
ning, only the ?1? bits of the code require switch ac-
tivation; the ?0? bits are produced passively by wait-
ing for the dwell time to expire. In contrast, all bits
in the asynchronous approaches require one of two
kinds of switch activation.
3.4 Not returning to non-selected symbols
Our third condition is just like the second except
it does not recompute the Huffman codes after ev-
ery bit, changing the way in which user errors are
handled. At the start of the string or immediately
after a letter has been typed, the Huffman codes
are calculated in exactly the same way as the pre-
vious two conditions, based on the n-gram language
model given the history of what has been typed so
far. However, after each bit is entered for the cur-
rent symbol, rather than multiplying by p and 1?p as
detailed in Section 3.2, symbols that have not been
selected are eliminated from consideration and will
not be highlighted again, i.e., will not be returned to
for subsequent selection. For example, in Figure 3
we see that there is a set of highlighted characters,
but also a set of characters that have been eliminated
from consideration and are deemphasized in the in-
terface to indicate that they can no longer be selected
(specifically: a, b, c, e, o and t). Those are symbols
that were not selected in previous steps of the scan-
ning, and are no longer available to be typed in this
position. If the user makes a mistake in the input,
eliminating the actual target symbol, the only way
to fix it is to type another symbol, delete it, and re-
type the intended symbol.
This condition is included in the study because
recalculation of codes after every bit becomes prob-
lematic when the codes are explicitly displayed (the
next condition). By including these results, we can
tease apart the impact of not recalculating codes af-
ter every bit versus the impact of displaying codes in
the next condition. Later, in the discussion, we will
return to this characteristic of the interface and dis-
cuss some alternatives that may allow for different
error recovery strategies.
This change to the interface has a couple of impli-
cations. First, the optimal codes are slightly shorter
than with the previous Huffman scanning methods,
since no probability mass is reserved for errors. In
other words, the perfect user that never makes a mis-
take would be able to type somewhat faster with
this method, which is not surprising, since reserv-
ing probability for returning to something that was
rejected is of no utility if no mistakes are ever made.
The experimental results presented later in the pa-
per will show explicitly how much shorter the codes
are for our particular test set. Second, it is possi-
ble to type a symbol without ever actively selecting
it, if all other symbols in the grid have been elimi-
nated. For example, if there are two symbols left and
the system highlights one symbol, which is rejected,
then the other symbol is typed. This contrasts with
the previous methods that only type when a single
character set is actively selected.
3.5 Displaying codes
Our final condition also does not recompute codes
after every bit, but in addition does away with high-
lighting of cells as the mechanism for scanning, and
instead displays dots and dashes directly beneath
each letter in the fixed grid. For example, Figure
4 shows the dots and dashes required for each let-
ter directly below that letter in the grid, and Figure
5 shows a portion of that grid magnified for easier
detailed viewing. Each code includes the dots and
dashes required to input that symbol, plus a cursor
?|? that indicates how much of the code has already
48
Figure 4: Scanning of non-contiguous sets of cells, dis-
playing dots and dashes rather than highlighting
Figure 5: A magnification of part of the above grid
been entered. For example, to type the letter ?s? us-
ing the code in Figure 5 , one must input: long, short,
short, long, short.
Since these codes are displayed, there is no mem-
orization required to input the target symbol. Like
row/column scanning, once the target symbol has
been found in the grid, the input sequence is known
in entirety by the user, which can facilitate planning
of sequences of actions rather than simply reacting
to updates in the interface. The cursor helps the user
know where they are in the code, which can be help-
ful for long codes. Figure 6 shows a magnification
of the interface when there are only two options re-
maining ? a dot selects ?l? and a dash selects ?u?.
4 Experiments
We recruited 10 native English speaking subjects be-
tween the ages of 26 and 50 years, who are not users
Figure 6: Cursor shows how much of code has been en-
tered
of scanning interfaces for typing and have typical
motor function. Following Roark et al (2010), we
use the phrase set from MacKenzie and Soukoreff
(2003) to measure typing performance, and the same
five strings from that set were used as evaluation
strings in this study as in Roark et al (2010). Prac-
tice strings were randomly selected from the rest of
the phrase set. Subjects used an Ablenet Jellybean R?
button as the binary switch. The error rate parameter
was fixed at 5% error rate.
The task in all conditions was to type the pre-
sented phrase exactly as it is presented. Symbols
that are typed in error ? as shown in Figure 7 ? must
be repaired by selecting the delete symbol (?) to
delete the incorrect symbol, followed by the correct
symbol. The reported times and bits take into ac-
count the extra work required to repair errors.
We tested subjects under four conditions. All four
conditions made use of 8-gram character language
models and Huffman coding, as described in Sec-
tion 3.1, and an alpha-ordered grid. The first condi-
tion is a replication of the Huffman scanning condi-
tion from Roark et al (2010), with the difference in
scan rate (600ms versus mean 475ms in their paper)
and the grid layout. This is an auto scan approach,
where the highlighting advances at the end of the
dwell time, as described in Section 3.2. The second
condition is asynchronous scanning, i.e., replacing
the dwell time with a long button press as described
in Section 3.3, but otherwise identical to condition 1.
The third condition was also asynchronous, but did
not recompute the binary code after every bit, so that
there is no return to characters eliminated from con-
sideration, as described in Section 3.4, but otherwise
identical to condition 2. Finally, the fourth condition
Figure 7: After an incorrect symbol is typed, it must be
deleted and the correct symbol typed in its place
49
Speed (cpm) Bits per character Error rate Long code rate
Scanning condition mean (std) mean (std) opt. mean (std) mean (std)
1.Huffman Roark et al (2010) 23.4 (3.7) 4.3 (1.1) 2.6 4.1 (2.2) 19.3 (14.2)
synchronous This paper 25.5 (3.2) 3.3 (0.4) 2.6 1.8 (1.1) 7.3 (4.1)
2. Huffman asynchronous 20.0 (3.7) 3.1 (0.2) 2.6 3.1 (2.5) 3.8 (1.2)
3. Huffman asynch, no return 17.2 (3.2) 3.1 (0.3) 2.4 7.7 (2.7) 0 (0)
4. Huffman asynch, display codes 18.7 (3.9) 3.0 (0.3) 2.4 6.9 (2.5) 0 (0)
Table 1: Typing results for 10 users on 5 test strings (total 31 words, 145 characters) under 4 conditions.
displays the codes for each character as described in
Section 3.5, without highlighting, but is otherwise
identical to condition 3.
Subjects were given a brief demo of the four con-
ditions by an author, then proceeded to a practice
phase. Practice phrases were given in each of the
four conditions, until subjects reached sufficient pro-
ficiency in the method to type a phrase with fewer
than 10% errors. After the practice phases in all four
conditions were completed, the test phases com-
menced. The ordering of the conditions in the test
phase was random. Subjects again practiced in a
condition until they typed a phrase with fewer than
10% errors, and then were presented with the five
test strings in that condition. After completion of
the test phase for a condition, they were prompted to
fill out a short survey about the condition.
Table 1 presents means and standard deviations
across our subjects for characters per minute, bits
per character, error rate and what Roark et al (2010)
termed ?long code rate?, i.e., percentage of sym-
bols that were correctly selected after being scanned
past. For condition 1, we also present the result for
the same condition reported in Roark et al (2010).
Comparing the first two rows of that table, we can
see that our subjects typed slightly faster than those
reported in Roark et al (2010) in condition 1, with
fewer bits per character, mainly due to lower error
rates and less scanning past targets. This can be at-
tributed to either the slower scanning speed or the al-
phabetic ordering of the grid (or both). In any case,
even with the slower scan rate, the overall speed is
faster in this condition than what was reported in that
paper.
The other three conditions are novel to this paper.
Moving from synchronous to asynchronous (with
long press) but leaving everything else the same
Survey Huffman Huffman No Display
Question synch asynch return codes
Fatigued 2.1 3.2 3.4 2.5
Stressed 1.9 2.2 2.9 2.0
Liked it 3.8 3.0 2.3 3.5
Frustrated 1.9 2.8 4.0 2.4
Table 2: Mean Likert scores to survey questions (5 = a
lot; 1 = not at all)
(condition 2) leads to slower typing speed but fewer
bits per character. The error rate is higher than in
the synchronous condition 1, but there is less scan-
ning past the target symbol. In discussion with sub-
jects, the higher error rate might be attributed to los-
ing track of which button press (short or long) goes
with highlighting, or also to intended short presses
being registered by the system as long.
The final two conditions allow no return to char-
acters once they have been scanned past, hence the
?long code rates? go to zero, and the error rates in-
crease. Note that the optimal bits per character are
slightly better than in the other trials, as mentioned
in Section 3.4, yet the subject bits per character stay
mostly the same as with condition 2. Typing speed
is slower in these two conditions, though slightly
higher when the codes are displayed versus the use
of highlighting.
In Table 2 we present the mean Likert scores from
the survey. The four statements that subjects as-
sessed were:
1. I was fatigued by the end of the trial
2. I was stressed by the end of the trial
3. I liked this trial
4. I was frustrated by this trial
The scores were: 1 (not at all); 2 (a little); 3 (not
sure); 4 (somewhat) and 5 (a lot).
50
The results in Table 2 show high frustration and
stress with condition 3, and much lower fatigue,
stress and frustration (hence higher ?liking?) for con-
dition 4, where the codes are displayed. Overall,
there seemed to be a preference for Huffman syn-
chronous, followed by displaying the codes.
5 Discussion
There are several take-away lessons from this ex-
periment. First, the frustration and slowdown that
result from the increased error rates in condition 3
make this a dispreferred solution, even though dis-
allowing returning to symbols that have been ruled
out in scanning reduced the bits per character (opti-
mal and in practice). Yet in order to display a stable
code in condition 4 (which was popular), recalcula-
tion of codes after every bit (as is done in the first
two conditions) is not an option. To make condi-
tion 4 more effective, some effective means for al-
lowing scanning to return to symbols that have been
scanned past must be devised.
Second, asynchronous scanning does seem to be
a viable alternative to auto scanning, which may be
of utility for certain AAC users. Such an approach
may be well suited to individuals using two switches
for asynchronous row/column scanning. Other users
may find the increased level of switch activation re-
quired for scanning in these conditions too demand-
ing. One statistic not shown in Table 1 is number
of keypresses required. In condition 1, some of the
?bits? required to type the character are produced by
not pressing the button. In the other three conditions,
all ?bits? result from either a short or long press, so
the button is pressed for every bit. In condition 1,
the mean number of key presses per character was
1.5, which is approximately half of the total button
presses required per character in the other methods.
Future directions include investigations into
methods that combine some of the strengths of the
various approaches. In particular, we are interested
in methods that allow for the direct display of codes
for either synchronous or asynchronous scanning,
but which also allow for scanning past and return to
target characters that were mistakenly not selected.
The benefit of displaying codes ? allowing for an-
ticipation and planning in scanning ? are quite high,
and this paper has not exhausted the exploration of
such approaches. Among the alternatives being con-
sidered are: requiring all codes to have a short press
(confirmation) bit as the last bit of the code; having
a ?reset? symbol or gesture; and recalculating codes
after some number of bits, greater than one. Each
of these methods would somewhat increase the op-
timal bits per character, but may result in superior
user performance. Finally, we intend to include ac-
tive AAC users in subsequent studies of these meth-
ods.
References
G. Baletsa, R. Foulds, and W. Crochetiere. 1976. Design
parameters of an intelligent communication device. In
Proceedings of the 29th Annual Conference on Engi-
neering in Medicine and Biology, page 371.
D. Beukelman and P. Mirenda. 1998. Augmentative and
Alternative Communication: Management of Severe
Communication Disorders in Children and Adults.
Paul H. Brookes, Baltimore, MD, second edition.
B. Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software, pages 86?99.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report, TR-10-98, Harvard University.
D.A. Huffman. 1952. A method for the construction of
minimum redundancy codes. In Proceedings of the
IRE, volume 40(9), pages 1098?1101.
G.W. Lesher, B.J. Moulton, and D.J. Higginbotham.
1998. Techniques for augmenting scanning commu-
nication. Augmentative and Alternative Communica-
tion, 14:81?101.
I.S. MacKenzie and R.W. Soukoreff. 2003. Phrase sets
for evaluating text entry techniques. In Proceedings of
the ACM Conference on Human Factors in Computing
Systems (CHI), pages 754?755.
B. Roark, J. de Villiers, C. Gibbons, and M. Fried-Oken.
2010. Scanning methods and language modeling for
binary switch typing. In Proceedings of the NAACL-
HLT Workshop on Speech and Language Processing
for Assistive Technologies (SLPAT), pages 28?36.
I.H. Witten and T.C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4):1085?1094.
51
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 56?64,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Robust kaomoji detection in Twitter
Steven Bedrick, Russell Beckley, Brian Roark, Richard Sproat
Center for Spoken Language Understanding, Oregon Health & Science University
Portland, Oregon, USA
Abstract
In this paper, we look at the problem of robust
detection of a very productive class of Asian
style emoticons, known as facemarks or kao-
moji. We demonstrate the frequency and pro-
ductivity of these sequences in social media
such as Twitter. Previous approaches to detec-
tion and analysis of kaomoji have placed lim-
its on the range of phenomena that could be
detected with their method, and have looked
at largely monolingual evaluation sets (e.g.,
Japanese blogs). We find that these emoticons
occur broadly in many languages, hence our
approach is language agnostic. Rather than
relying on regular expressions over a prede-
fined set of likely tokens, we build weighted
context-free grammars that reward graphical
affinity and symmetry within whatever sym-
bols are used to construct the emoticon.
1 Introduction
Informal text genres, such as email, SMS or social
media messages, lack some of the modes used in
spoken language to communicate affect ? prosody
or laughter, for example. Affect can be provided
within such genres through the use of text format-
ting (e.g., capitalization for emphasis) or through the
use of extra-linguistic sequences such as the widely
used smiling, winking ;) emoticon. These sorts of
vertical face representations via ASCII punctuation
sequences are widely used in European languages,
but in Asian informal text genres another class of
emoticons is popular, involving a broader symbol set
and with a horizontal facial orientation. These go by
the name of facemarks or kaomoji. Figure 1 presents
|?_?|
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
57603166048497664
57596757185544192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
57745944376119296 (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
[o_-]
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
6031 6048497664
5967 7185 4192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
74594 376119  (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
\(?v?)/
57606710235369473: interesting use of u338 (diagonal overlay; in 
the "combining diacritical marks" section)
57807873274683393: RT @adamsbaldwin: THIS! --> | RT @MelissaTweets 
"By being afraid to go at ?bama politically, people display a soft 
racism." ~ #Repres ... (note peace sign in the "O" in Obama)
57577928942305280: Use of "ake" for "a que" in Spanish
57577937330913280: Example of tricky-to-tokenize tweet (irregular 
spacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,y 
es completamente cierta.Ellas son geniales. #LEGOO :) <3
57651140510224384: great English abbreviations and use of Unicode: 
Hehe? OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby: 
@enahoanagha Sha? M sad:-(
57581074657718272: You dnt never answer yo phone!
57583914226683904: IHate When Ppl Have Attitudes Wit Mee For No 
Reason. <---- repetition and also "With"->"Wit", "People"->"Ppl"
57850097320460289: Good example of shortening: @China_DollCris u 
lucky smh I'm going str8 thru !!
57610065699549184 <--- using Cyrillic characters to write in 
Spanish/portugese...
57577987641573376: hashtags used as parts of speech; also note "2" 
instead of "to"
57592260870668288: awareness of spelling issues
Fun smileys:
(?_?'!)
57746992926949376 (breve with combining lower line, makes a nice 
tear effect)
?
?
-?
( !!!)
"? ??#
"(?
?
?
? ? ? ? ??
?
?
)#
(?_?)
57592260870668288
57689354826547200: ?(?????)? as well as (!!! )))))))))?
57678625805320192
57675270324367360 (???!???)
57617464455991296
57603166048497664
57596757185544192 " ?[? ? ??]#, (????)
57584430105108480 (??"??)
57745965351837696 (*?m?*)
57745944376119296 (?
?
?
?
-?
?
?
?)
57745659142483968 (????`) 
57825858454433792 <-- uses a fun swirly Tamil letter: (?!?! `)
Figure 1: Some representative kaomoji emoticons
several examples of these sequences, including both
relatively common kaomoji as well as more exotic
and complex creations.
This class of emoticon is far more varied and pro-
ductive than the sideways European style emoticons,
and even lists of on the order of ten thousand emoti-
cons will fail to cover all instances in even a mod-
est sized sample of text. This relative productiv-
ity is due to several factors, including the horizon-
tal orientation, which allows for more flexibility in
configuring features both within the face and sur-
rounding the face (e.g., arms) than the vertical ori-
entation. Another important factor underlying kao-
moji productivity is historical in nature. kaomoji
were developed and popularized in Japan and other
Asian countries whose scripts have always required
multibyte character encodings, and whose users of
electronic communication systems have significant
experience working with characters beyond those
found in the standard ASCII set.
Linguistic symbols from various scripts can be
appropriated into the kaomoji for their resemblence
to facial features, such as a winking eye, and au-
thors of kaomoji sometimes use advanced Unicode
techniques to decorate glyphs with elaborate com-
binations of diacritic marks. For example, the kao-
56
moji in the top righthand corner of Figure 1, includes
an Arabic letter, and Thai vowel diacritics. Accu-
rate detection of these tokens ? and other common
sequences of extra-linguistic symbol sequences ? is
important for normalization of social media text for
downstream applications.
At the most basic level, the complex and unpre-
dictable combinations of characters found within
many kaomoji (often including punctuation and
whitespace, as well as irregularly-used Unicode
combining characters) can seriously confound sen-
tence and word segmentation algorithms that at-
tempt to operate on kaomoji-rich text; since segmen-
tation is typically the first step in any text process-
ing pipeline, issues here can cause a wide variety
of problems downstream. Accurately removing or
normalizing such sequences before attempting seg-
mentation can ensure that existing NLP tools are
able to effectively work with and analyze kaomoji-
including text.
At a higher level, the inclusion of a particular
kaomoji in a text represents a conscious decision
on the part of the text?s author, and fully interpret-
ing the text necessarily involves a degree of inter-
pretation of the kaomoji that they chose to include.
European-style emoticons form a relatively closed
set and are often fairly straightforward to interpret
(both in terms of computational, as well as human,
effort); kaomoji, on the other hand, are far more di-
verse, and interpretation is rarely simple.
In this paper, we present preliminary work on
defining robust models for detecting kaomoji in so-
cial media text. Prior work on detecting and classi-
fying these extra-linguistic sequences has relied on
the presence of fixed attested patterns (see discus-
sion in Section 2) for detection, and regular expres-
sions for segmentation. While such approaches can
capture the most common kaomoji and simple vari-
ants of them, the productive and creative nature of
the phenomenon results in a non-negligible out-of-
vocabulary problem. In this paper, we approach the
problem by examining a broader class of possible
sequences (see Section 4.2) for symmetry using a
robust probabilistic context-free grammar with rule
probabilities proportional to the symmetry or affin-
ity of matched terminal items in the rule. Our PCFG
is robust in the sense that every candidate sequence
is guaranteed to have a valid parse. We use the re-
sulting Viterbi best parse to provide a score to the
candidate sequence ? reranking our high recall list
to achieve, via thresholds, high precision. In addi-
tion, we investigate unsupervised model adaptation,
by incorporating Viterbi-best parses from a small set
of attested kaomoji scraped from websites; and in-
ducing grammars with a larger non-terminal set cor-
responding to regions of the face.
We present bootstrapping experiments for deriv-
ing highly functional, language independent models
for detecting kaomoji in text, on multilingual Twit-
ter data. Our approach can be used as part of a
stand-alone detection model, or as input into semi-
automatic kaomoji lexicon development. Before de-
scribing our approach, we will first present prior
work on this class of emoticon.
2 Prior Work
Nakamura et al (2003) presented a natural language
dialogue system that learned a model for generat-
ing kaomoji face marks within Japanese chat. They
trained a neural net to produce parts of the emoti-
con ? mouth, eyes, arms and ?optional things? as
observed in real world data. They relied on a hand-
constructed inventory of observed parts within each
of the above classes, and stitched together predicted
parts into a complete kaomoji using simple tem-
plates.
Tanaka et al (2005) presented a finite-state
chunking approach for detecting kaomoji in
Japanese on-line bulletin boards using SVMs with
simple features derived from a 7 character window.
Training was performed on kaomoji dictionaries
found online. They achieved precision and recall in
the mid-80s on their test set, which was a significant
recall improvement (17% absolute) and modest
precision improvement (1.5%) over exact match
within the dictionaries. They note certain kinds of
errors, e.g., ?(Thu)? which demonstrate that their
chunking models are (unsurprisingly) not capturing
the typical symmetry of kaomoji. In addition, they
perform classification of the kaomoji into 6 rough
categories (happy, sad, angry, etc.), achieving high
performance (90% accuracy) using a string kernel
within an SVM classifier.
Ptaszynski et al (2010) present work on a large
database of kaomoji, which makes use of an analy-
57
sis of the gestures conveyed by the emoticons and
their relation to a theory of non-verbal expressions.
They created an extensive (approximately 10,000
entry) lexicon with 10 emotion classes, and used
this database as the basis of both emoticon extrac-
tion from text and emotion classification. To detect
an emoticon in text, their system (named ?CAO?)
looked for three symbols in a row from a vocabulary
of the 455 most frequent symbols in their database.
Their approach led to a 2.4% false negative rate
when evaluated on 1,000 sentences extracted from
Japanese blogs. Once detected, the system extracts
the emoticon from the string using a gradual relax-
ation from exact match to approximate match, with
various regular expressions depending on specific
partial match criteria. A similar deterministic al-
gorithm based on sequenced relaxation from exact
match was used to assign affect to the emoticon.
Our work focuses on the emoticon detection
stage, and differs from the above systems in a num-
ber of ways. First, while kaomoji were popularized
in Asia, and are most prevalent in Asian languages,
they not only found in messages in those languages.
In Twitter, which is massively multilingual, we find
kaomoji with some frequency in many languages,
including European languages such as English and
Portuguese, Semitic languages and a range of Asian
languages. Our intent is to have a language inde-
pendent algorithm that looks for such sequences in
any message. Further, while we make use of online
dictionaries as development data, we appreciate the
productivity of the phenomenon and do not want to
restrict the emoticons that we detect to those con-
sisting of pre-observed characters. Hence we focus
instead on characteristics of kaomoji that have been
ignored in the above models: the frequent symmetry
of the strings. We make use of context-free mod-
els, built in such a way as to guarantee a parse for
any candidate sequence, which permits exploration
of a much broader space of potential candidates than
the prior approaches, using very general models and
limited assumptions about the key components of
the emoticons.
3 Data
Our starting resources consisted of a large, multi-
lingual corpus of Twitter data as well as a smaller
collection of kaomoji scraped from Internet sources.
Our Twitter corpus consists of approximately 80
million messages collected using Twitter?s ?Stream-
ing API? over a 50-day period from June through
August 2011. The corpus is extremely linguistically
diverse; human review of a small sample identified
messages written in >30 languages. The messages
themselves exhibit a wide variety of phenomena, in-
cluding substantial use of different types of Inter-
net slang and written dialect, as well as numerous
forms of non-linguistic content such as emoticons
and ?ASCII art.?
We took a two-pronged approach to developing
a set of ?gold-standard? kaomoji. Our first ap-
proach involved manually ?scraping? real-world ex-
amples from the Internet. Using a series of hand-
written scripts, we harvested 9,193 examples from
several human-curated Internet websites devoted to
collecting and exhibiting kaomoji. Many of these
consisted of several discrete sub-units, typically in-
cluding at least one ?face? element along with a
small amount of additional content. For example,
consider the following kaomoji, which appeared in
this exact form eight times in our Twitter corpus:
?(*???)??+.???????+.??
(???*)? 9
?(!"?" ? ?)??????#? 8
?(??????)??? 9
?(???`;)???? 52
??????? 5
?_(?_? ) 1
?/T?T)/??????? 4
???????????????????????? 1
?????? 1
????? 426
?????? 1
?(?????)/????? 1
?????????????? 4
??(?????)(??_ _)???? 10
????(???)??????? 1
????(T-T)?(^^ )???? 2
???Uo???oU??? 1
?????+.(???)(???)?+.?!! 1
. Note that, in this case, the
?face? is followed by a small amount of hiragana,
and that the message concludes with a dingbat in the
form of a ?heart? symbol.1
Of these 9,193 scraped examples, we observed
?3,700 to appear at least once in our corpus of
Twitter messages, and ?2,500 more than twice.
The most common kaomoji occurred with frequen-
cies in the low hundreds of thousands, although the
frequency with which individual kaomoji appeared
roughly followed a power-law distribution, meaning
that there were a small number that occurred with
great frequency and a much larger number that only
appeared rarely.
From this scraped corpus, we attempted to iden-
tify a subset that consisted solely of ?faces? to serve
as a high-precision training set. After observing
that nearly all of the faces involved a small number
of characters bracketed one of a small set of natu-
ral grouping characters (parentheses, ?curly braces,?
1Note as well that this kaomoji includes not only a wide va-
riety of symbols, but that some of those symbols are themselves
modified using combining diacritic marks. This is a common
practice in modern kaomoji, and one that complicates analysis.
58
etc.), we extracted approximately 6,000 substrings
matching a very simple regular expression pattern.
This approach missed many kaomoji, and of the
examples that it did detect, many were incom-
plete (in that they were missing any extra-bracketed
content? arms, ears, whiskers, etc.) However, the
contents of this ?just faces? sub-corpus offered de-
cent coverage of many of the core kaomoji phenom-
ena in a relatively noise-free manner. As such, we
found it to be useful as ?seed? data for the grammar
adaptation described in section 4.4.
In addition to our ?scraped? kaomoji corpus, we
constructed a smaller corpus of examples drawn di-
rectly from our Twitter corpus. The kaomoji phe-
nomenon is complex enough that capturing it in its
totality is difficult. However, it is possible to capture
a subset of kaomoji by looking for regions of per-
fect lexical symmetry. This approach will capture
many of the more regularly-formed and simple kao-
moji (for example, ?(-_-)?), although it will miss
many valid kaomoji. Using this approach, we iden-
tied 3,580 symmetrical candidate sequences; most
of these were indeed kaomoji, although there were
several false positives (for example, symmetrical se-
quences of repeated periods, question marks, etc.).
Using simple regular expressions, we were able to
remove 289 such false positives.
Interestingly, there was very little overlap be-
tween the corpus scraped from the Web and the sym-
metry corpus. A total of 39 kaomoji appeared in ex-
actly the same form in both sets. We noted, however,
that the kaomoji harvested from the Web tended to
be longer and more elaborate than those identified
from our Twitter corpus using the symmetry heuris-
tic (Mann-Whitney U, p < 0.001), and as previously
discussed, the Web kaomoji often contained one or
more face elements. Thus we expanded our defi-
nition of overlap, and counted sequences from the
symmetrical corpus that were substrings of scraped
kaomoji. Using this criterion, we identified 177 pos-
sibly intersecting kaomoji. The fact that so few indi-
vidual examples occurred in both corpora illustrates
the extremely productive nature of the phenomenon.
4 Methods
4.1 Graphical similarity
The use of particular characters in kaomoji is ul-
timately based on their graphical appearance. For
Figure 2: Ten example character pairs with imperfect
(but very high) symmetry identified by our algorithm.
Columns are: score, hex code point 1, hex code point
2, glyph 1, glyph 2.
example, good face delimiters frequently include
mated brackets or parentheses, since these elements
naturally look as if they delimit material. Further-
more, there are many characters which are not tech-
nically ?paired,? but look roughly more-or-less sym-
metrical. For example, the Arabic-Indic digits!???"and
!???" are commonly used as bracketing delimiters, for
example: !???". These characters can serve both as
?arms? as well as ?ears.?
Besides bracketing, symmetry plays an additional
role in kaomoji construction. Glyphs that make good
?eyes? are often round; ?noses? are often symmet-
ric about their central axis. Therefore a measure of
graphical similarity between characters is desirable.
To that end, we developed a very simple measure
of similarity. From online sources, we downloaded
a sample glyph for each code point in the Unicode
Basic Multilingual Plane, and extracted a bitmap for
each. In comparing two glyphs we first scale them
to have the same aspect ratio if necessary, and we
then compute the proportion of shared pixels be-
tween them, with a perfect match being 1 and the
worst match being 0. We can thus compute whether
two glyphs look similar; whether one glyph is a good
mirror image of the other (by comparing glyph A
with the mirror image of glyph B); and whether a
glyph is (vertically) symmetric (by computing the
similarity of the glyph and its vertical mirror image).
The method, while clearly simple-minded,
nonetheless produces plausible results, as seen in
Figure 2, which shows the best 10 candidates for
mirror image character pairs. We also calculate
the same score without flipping the image verti-
cally, which is also used to score possible symbol
matches, as detailed in Section 4.3.
59
4.2 Candidate extraction
We perform candidate kaomoji extraction via a very
simple hidden Markov model, which segments all
strings of Unicode graphemes into contiguous re-
gions that are either primarily linguistic (mainly
language encoding symbols2) or primarily non-
linguistic (mainly punctuation, or other symbols).
Our candidate emoticons, then, are this extensive
list of mainly non-linguistic symbol sequences. This
is a high recall approach, returning most sequences
that contain valid emoticons, but quite low precision,
since it includes many other sequences as well (ex-
tended runs of punctuation, etc.).
The simple HMM consists of 2 states: call them
A (mainly linguistic) and @ (mainly non-linguistic).
Since there are two emitted symbol classes (linguis-
tic L and non-linguistic N ), each HMM state must
have two emission probabilities, one for its domi-
nant symbol class (L in A and N in @) and one
for the other symbol class. Non-linguistic symbols
occur quite often in linguistic sequences, as punc-
tuation for example. However, sequences of, say,
3 or more in a row are not particularly frequent.
Similarly, linguistic symbols occur often in kaomoji,
though not often in sequences of, say, 3 or more.
Hence, to segment into contiguous sequences of a
certain number in a row, the probability of transition
from state A to state @ or vice versa must be signif-
icantly lower than the probability of emitting one or
two N from A states or L from @ states. We thus
have an 8 parameter HMM (four transition and four
emission probabilities) that was coarsely parameter-
ized to have the above properties, and used it to ex-
tract candidate non-linguistic sequences for evalua-
tion by our PCFG model.
Note that this approach does have the limitation
that it will trim off some linguistic symbols that oc-
cur on the periphery of an emoticon. Future versions
of this part of the system will address this issue by
extending the HMM. For this paper, we made use of
a slightly modified version of this simple HMM for
candidate extraction. The modifications involved the
addition of a special input state for whitespace and
full-stop punctuation, which helped prevent certain
very common classes of false-positive.
2Defined as a character having the Unicode ?letter? charac-
ter property.
rule score rule score
X? a X b S(a,b) X? a b S(a,b)
X? a X  X? X a 
X? a ? X? X X ?
Table 1: Rule schemata for producing PCFG
4.3 Baseline grammar induction
We perform a separate PCFG induction for ev-
ery candidate emoticon sequence, based on a small
set of rule templates methods for assigning rule
weights. By inducing small, example-specific
PCFGs, we ensure that every example has a valid
parse, without growing the grammar to the point that
the grammar constant would seriously impact parser
efficiency.
Table 1 shows the rule schemata that we used for
this paper. The resulting PCFG would have a single
non-terminal (X) and the variables a and b would be
instantiated with terminal items taken from the can-
didate sequence. Each instantiated rule receives a
probability proportional to the assigned score. For
the rules that ?pair? symbols a and b, a score is as-
signed in two ways, call them S1(a, b) and S2(a, b)
(they will be defined in a moment). Then S(a, b) =
max(S1(a, b) and S2(a, b)). If S(a, b) < ?, for some
threshold ?,3 then no rule is generated. S1 is the
graphical similarity of the first symbol with the verti-
cal mirror image of the second symbol, calculated as
presented in Section 4.1. This will give a high score
for things like balanced parentheses. S2 is the graph-
ical similarity of the first symbol with the second
symbol (not vertically flipped), which gives high
scores to the same or similar symbols. This permits
matches for, say, eyes that are not symmetric due to
an orientation of the face, e.g.,
!???"
(!#!). The other pa-
rameters (, ? and ?) are included to allow for, but
penalize, unmatched symbols in the sequence.
All possible rules for a given sequence are instan-
tiated using these templates, by placing each symbol
in the a slot with all subsequent symbols in the b slot
and scoring, as well as creating all rules with just a
alone for that symbol. For example, if we are given
the kaomoji (o o;) specific rules would be created
if the similarity scores were above threshold. For the
second symbol ?o?, the algorithm would evaluate the
3For this paper, ? was chosen to be 0.7.
60
similarity between ?o? and each of the four symbols
to its right , o, ; and ).
The resulting PCFG is normalized by summing
the score for each rule and normalizing by the score.
The grammar is then transformed to a weakly equiv-
alent CNF by binarizing the ternary rules and in-
troducing preterminal non-terminals. This grammar
is then provided to the parser4, which returns the
Viterbi best parse of the candidate emoticon along
with its probability. The score is then converted to
an approximate perplexity by dividing the negative
log probability by the number of unique symbols in
the sequence and taking the exponential.
4.4 Grammar enhancement and adaptation
The baseline grammar induction approach outlined
in the previous section can be improved in a cou-
ple of ways, without sacrificing the robustness of the
approach. One way is through grammar adaptation
based on automatic parses of attested kaomoji. The
other is by increasing the number of non-terminals
in the grammar, according to a prior understanding
of their typical (canonical) structure. We shall dis-
cuss each in turn.
Given a small corpus of attested emoticons (in our
case, the ?just faces? sub-corpus described in sec-
tion 3), we can apply the parser above to those ex-
amples, and extract the Viterbi best parses into an
automatically created treebank. From that treebank,
we extract counts of rule productions and use these
rule counts to inform our grammar estimation. The
benefit of this approach is that we will obtain addi-
tional probability mass for frequently observed con-
structions in that corpus, thus preferring commonly
associated pairs within the grammar. Of course, the
corpus only has a small fraction of the possible sym-
bols that we hope to cover in our robust approach, so
we want to incorporate this information in a way that
does not limit the kinds of sequences we can parse.
We can accomplish this by using simple Maxi-
mum a Posteriori (MAP) adaptation of the grammar
(Bacchiani et al, 2006). In this scenario, we will
first use our baseline method of grammar induction,
using the schemata shown in Table 1. The scores
derived in that process then serve as prior counts
4We used the BUBS parser (Bodenstab et al, 2011).
http://code.google.com/p/bubs-parser/
for the rules in the grammar, ensuring that all of
these rules continue to receive probability mass. We
then add in the counts for each of the rules from the
treebank. Many of the rules may have been unob-
served in the corpus, in which case they receive no
additional counts; observed rules, however, will re-
ceive extra weight proportional to their frequency in
that corpus. Note that these additional weights can
be scaled according to a given parameter. After in-
corporating these additional counts, the grammar is
normalized and parsing is performed as before. Of
course, this process can be iterated ? a new auto-
matic treebank can be produced based on an adapted
grammar, and so on.
In addition to grammar adaptation, we can en-
rich our grammars by increasing the non-terminal
sets. To do this, we created a nested hierarchy
of ?regions? of the emoticons, with constraints re-
lated to the canonical composition of the faces,
e.g., eyes are inside of faces, noses/mouths between
eyes, etc. These non-terminals replace our generic
non-terminal X in the rule schemata. For the cur-
rent paper, we included the following five ?region?
non-terminals: ITEM, OUT, FACE, EYES, NM. The
non-terminal ITEM is intended as a top-most non-
terminal to allow multiple emoticons in a single se-
quence, via an ITEM ? ITEM ITEM production.
None of the others non-terminals have repeating pro-
ductions of that sort ? so this replaces the X? X X
production from Table 1.
Every production (other than ITEM ? ITEM
ITEM) has zero or one non-terminals on the right-
hand side. In our new schemata, non-terminals on
the left-hand side can only have non-terminals on the
right-hand side at the same or lower levels. This en-
forces the nesting constraint, i.e., that eyes are inside
of the face. Levels can be omitted however ? e.g.,
eyes but no explicit face delimiter ? hence we can
?skip? a level using unary projections, e.g., FACE?
EYES. Those will come with a ?skip level? weight.
Categories can also rewrite to the same level (with a
?stay level? weight) or rewrite to the next level af-
ter emitting symbols (with a ?move to next level?
weight).
To encode a preference to move to the next level
rather than to stay at the same level, we assign a
weight of 1 to moving to the next level and a weight
of 0.5 to staying at the same level. The ?skip?
61
rule score
ITEM ? ITEM ITEM ?
ITEM ? OUT ?
OUT ? a OUT b S(a,b) + 0.5
OUT ? a OUT  + 0.5
OUT ? OUT a  + 0.5
OUT ? a FACE b S(a,b) + 1
OUT ? a FACE  +1
OUT ? FACE a  +1
OUT ? FACE 0.5
FACE ? a FACE b S(a,b) + 0.5
FACE ? a FACE  + 0.5
FACE ? FACE a  + 0.5
FACE ? a EYES b S(a,b) + 1
FACE ? a EYES  +1
FACE ? EYES a  +1
FACE ? EYES 0.1
EYES ? a EYES b S(a,b) + 0.5
EYES ? a EYES  + 0.5
EYES ? EYES a  + 0.5
EYES ? a NM b S(a,b) + 1
EYES ? a NM  +1
EYES ? NM a  +1
EYES ? NM 0.1
EYES ? a b S(a,b) + 1
NM ? a NM 
NM ? NM a 
NM ? a ?
Table 2: Rule schemata for expanded non-terminal set
weights depend on the level, e.g., skipping OUT
should be cheap (weight of 0.5), while skipping
the others more expensive (weight of 0.1). These
weights are like counts, and are added to the similar-
ity counts when deriving the probability of the rule.
Finally, there is a rule in the schemata in Table 1 with
a pair of symbols and no middle non-terminal. This
is most appropriate for eyes, hence will only be gen-
erated at that level. Similarly, the single symbol on
the right-hand side is for the NM (nose/mouth) re-
gion. Table 2 presents our expanded rule schemata.
Note that the grammar generated with this ex-
panded set of non-terminals is robust, just as the ear-
lier grammar is, in that every sequence is guaranteed
to have a parse. Further, it can be adapted using the
same methods presented earlier in this section.
5 Experimental Results
Using the candidate extraction methodology de-
scribed in section 4.2, we extracted 1.6 million dis-
tinct candidates from our corpus of 80 million Twit-
ter messages (candidates often appeared in multi-
ple messages). These candidates included genuine
emoticons, as well as extended strings of punc-
tuation and other ?noisy? chunks of text. Gen-
uine kaomoji were often picked up with some
amount of leading or trailing punctuation, for exam-
ple: ?..\(??`)/?; other times, kaomoji beginning
with linguistic characters were truncated: (^?*)?.
We provided these candidates to our parser un-
der four different conditions, each one producing
1.5 million parse trees: the single non-terminal ap-
proach described in section 4.3 or the enhanced mul-
tiple non-terminal approach described in section 4.4,
both with and without training via the Maximum A
Posteriori approach described in section 4.4.
Using the weighted-inside-score method de-
scribed in section 4.3, we produced a ranked list
of candidate emoticons from each condition?s out-
put. ?Well-scoring? candidates were ones for which
the parser was able to construct a low-cost parse.
We evaluated our approach in two ways. The first
way examined precision? how many of the best-
scoring candidate sequences actually contained kao-
moji? Manually reviewing all 1.6 million candidates
was not feasible, so we evaluated this aspect of our
system?s performance on a small subset of its out-
put. Computational considerations forced us to pro-
cess our large corpus in parallel, meaning that our set
of 1.6 million candidate kaomoji was already parti-
tioned into 160 sets of?10,000 candidates each. We
manually reviewed the top 1,000 sorted results from
one of these partitions, and flagged any entries that
did not contain or consist of a face-like kaomoji. The
results of each condition are presented in table 3.
The second evaluation approach we will exam-
ine looks at how our method compares with the
trigram-based approach described by (Yamada et al,
2007) (as described by (Ptaszynski et al, 2010)).
We trained both smoothed and unsmoothed lan-
guage models 5 on the ?just faces? sub-corpus used
for the A Posteriori grammar enhancement, and
computed perplexity measurements for the same
set ?10,000 candidates used previously. Table 3
presents these results; clearly, a smoothed trigram
model can achieve good results. The unsmoothed
model at first glance seems to have performed very
well; note, however, that only approximately 600
(out of nearly 10,000) candidates were ?matched?
by the unsmoothed model (i.e., they did not contain
any OOV symbols and therefore had finite perplex-
ity scores), yielding a very small but high-precision
set of emoticons.
Looking at precision, the model-based ap-
proaches outperformed our grammar approach. It
5Using the OpenGrm ngram language modeling toolkit.
62
Condition P@1000 MAP
Single Nonterm, Untrained 0.662 0.605
Single Nonterm, Trained 0.80 0.945
Multiple Nonterm, Untrained 0.795 0.932
Multiple Nonterm, Trained 0.885 0.875
Unsmoothed 3-gram 0.888 0.985
Smoothed 3-gram 0.905 0.956
Mixed, Single Nonterm, Untrained 0.662 0.902
Mixed, Single Nonterm, Trained 0.804 0.984
Mixed, Multiple Nonterm, Untrained 0.789 0.932
Mixed, Multiple Nonterm, Trained 0.878 0.977
Table 3: Experimental Results.
should be noted, however, that the trigram approach
was much less tolerant of certain non-standard for-
mulations involving novel characters or irregular
formulations ((?!?)?(?o?)?(??o??)
and
(?!?)?(?o?)?(??o??) are examples of kao-moji that our grammar-based approach ranked more
highly than did the trigram approach). The two
approaches also had different failure profiles. The
grammar approach?s false positives tended to be
symmetrical sequences of punctuation, whereas the
language models? were more variable. Were we to
review a larger selection of candidates, we believe
that the structure-capturing nature of the grammar
approach would enable it to outperform the more
simplistic approach.
We also attempted a hybrid ?mixed? approach in
which we used the language models to re-rank the
top 1,000 ?best? candidates from our parser?s output.
This generally resulted in improved performance,
and for some conditions the improvement was sub-
stantial. Future work will explore this approach in
greater detail and over larger amounts of data.
6 Discussion
We describe an almost entirely unsupervised ap-
proach to detecting kaomoji in irregular, real-world
text. In its baseline state, our system is able to ac-
curately identify a large number of examples using
a very simple set of templates, and can distinguish
kaomoji from other non-linguistic content (punctu-
ation, etc.). Using minimal supervision, we were
able to effect a dramatic increase in our system?s
performance. Visual comparison of the ?untrained?
results with the ?trained? results was instructive.
The untrained systems? results were very heavily in-
fluenced by their template rules? strong preference
for visual symmetry. Many instances of symmet-
rical punctuation sequences (e.g., ..?..) ended
up being ranked more highly than even fairly sim-
ple kaomoji, and in the absence of other informa-
tion, the length of the input strings also played a too-
important role in their rankings.
The MAP-ehanced systems? results, on the other
hand, retained their strong preference for symme-
try, but were also influenced by the patterns and
characters present in their training data. For ex-
ample, two of the top-ranked ?false positives? from
the enhanced system were the sequences >,< and
= =, both of which (while symmetrical) also con-
tain characters often seen in kaomoji. By using more
structurally diverse training data, we expect further
improvements in this area. Also, our system cur-
rently relies on a very small number of relatively
simplistic grammar templates; expanding these to
encode additional structure may also help.
Due to our current scoring mechanism, our parser
is biased against certain categories of kaomoji. Par-
ticularly poorly-scored are complex creations such
as (((| ???? ??|? ???=???| ???? ??|?))). In this example, the large number
of combining characters and lack of obvious nest-
ing therein confounded our templates and produced
expensive parse trees. Future work will involve im-
proved handling of such cases, either by modified
parsing schemes or additional templates.
One other area of future work is to match par-
ticular kaomoji, or fragments of kaomoji (e.g. par-
ticular eyes), to particular affective states, or other
features of the text. Some motifs are already well
known: for example, there is wide use of TT, or the
similar-looking Korean hangeul vowel yu, to repre-
sent crying eyes. We propose to do this initially by
computing the association between particular kao-
moji and words in the text. Such associations may
yield more than just information on the likely af-
fect associated with a kaomoji. So, for example,
using pointwise mutual information as a measure
of association, we found that in our Twitter corpus,
(*?_?*) seems to be highly associated with tweets
about Korean pop music, *-* with Brazilian post-
ings, and with Indonesian postings. Such
associations presumably reflect cultural preferences,
and could prove useful in identifying the provenance
of a message even if more conventional linguistic
techniques fail.
63
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Adaptive beam-width prediction for ef-
ficient cyk parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 440?449.
Junpei Nakamura, Takeshi Ikeda, Nobuo Inui, and
Yoshiyuki Kotani. 2003. Learning face mark for nat-
ural language dialogue system. In Proc. Conf. IEEE
Int?l Conf. Natural Language Processing and Knowl-
edge Eng, pages 180?185.
Michal Ptaszynski, Jacek Maciejewski, Pawel Dybala,
Rafal Rzepka, and Kenji Araki. 2010. Cao: A fully
automatic emoticon analysis system based on theory
of kinesics. IEEE Transactions on Affective Comput-
ing, 1:46?59.
Yuki Tanaka, Hiroya Takamura, and Manabu Okumura.
2005. Extraction and classification of facemarks with
kernel methods. In Proc. 10th Int?l Conf. Intelligent
User Interfaces.
T. Yamada, S. Tsuchiya, S. Kuroiwa, and F. Ren. 2007.
Classification of facemarks using n-gram. In Inter-
national Conference on Natural Language Processing
and Knowledge Engineering, pages 322?327.
64
