Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1396?1404,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Device-Dependent Readability for Improved Text Understanding
A-Yeong Kim Hyun-Je Song Seong-Bae Park Sang-Jo Lee
School of Computer Science and Engineering
Kyungpook National University
Daegu, 702-701, Korea
{aykim,hjsong,sbpark}@sejong.knu.ac.kr, sjlee@knu.ac.kr
Abstract
Readability is used to provide users with high-
quality service in text recommendation or text
visualization. With the increasing use of hand-
held devices, reading device is regarded as
an important factor for readability. There-
fore, this paper investigates the relationship
between readability and reading devices such
as a smart phone, a tablet, and paper. We sug-
gest readability factors that are strongly related
with the readability of a specific device by
showing the correlations between various fac-
tors in each device and human-rated readabil-
ity. Our experimental results show that each
device has its own readability characteristics,
and thus different weights should be imposed
on readability factors according to the device
type. In order to prove the usefulness of the
results, we apply the device-dependent read-
ability to news article recommendation.
1 Introduction
Readability is a function that maps a given text into a
readability score by considering ?how easily the text is
read and understood? (Richards et al., 1992; Zamanian
and Heydari, 2012). Normally, the readability score is
formulated as a combination of various factors. These
factors reflect the easiness and understanding of the
text and include text presentation format, font size, av-
erage ratio of annotated images, and sentence length
(Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012;
?
Oquist, 2006). Therefore, readability can be used to
provide satisfiable services in text recommendation or
text visualization.
The study on readability has begun in the education
field to measure the level of a text. With the success
of using readability in education (Franc?ois and Fairon,
2012; Heilman et al., 2008; Ma et al., 2012), read-
ability has been used in a range of domains recently.
For example, in document retrieval, readability is used
to provide documents to non-expert users so that they
can read the retrieved documents easily (Jameel et al.,
2012; Yan et al., 2006). In text mining, readability has
been employed to analyze the characteristics of text.
Especially, Hillbom showed the differences in readabil-
ity between broadsheet newspapers and tabloids that
share a similar political stance (Hillbom, 2009).
There is one important issue of readability that has
not been studied in natural language processing. It is a
reading device. That is, previous studies focused only
on text printed on paper. However, with the increasing
use of hand-held devices, people in these days use var-
ious reading devices such as a tablet and a smart phone
as well as a paper. Readability score can be different
according to the device type, because each device has
its own idiosyncrasy. For example, assume that a sys-
tem recommends the same news article to both user A
who reads it in her smart phone and user B who reads
it on paper. Although both users read the same article,
user A might believe that her article is more difficult to
read than user B because of the screen size of her smart
phone.
This paper explores the relationship between reading
devices and readability. For this purpose, we first inves-
tigate whether readability changes according to device
type or not. Then, we analyze which readability fac-
tors are affected by reading devices. To see the rela-
tionship between readability factors and devices, var-
ious well-known readability factors are computed for
news articles collected from an Internet portal. At the
same time, the readability of each article is also man-
ually rated. When the readability is rated manually, it
is done three times for different reading devices of a
smart phone, a tablet, and paper. The factors that af-
fect the readability actually in each device are found
out through the correlations between the factors and the
manually-labeled readability. Some factors are impor-
tant to the readability of smart phone, but insignificant
to that of paper. Therefore, we discover the importance
of each readability factor for each device by analyzing
the correlations.
The usefulness of the device-dependent readability
is proven by applying it to news article recommenda-
tion. That is, different importance weights for read-
ability factors are considered according to device type
when recommending news articles. Our experimental
results show that the performance of news article rec-
ommendation gets best when the device used for read-
ing news articles is identical to the device used for mea-
suring readability. Therefore, it is essential to consider
different importance weights according to device type
1396
in news article recommendation. It also proves that
the proposed device-dependent readability reflects the
characteristics of reading devices well.
The rest of this paper is organized as follows. We
first review related studies on readability. Next, we
introduce various readability factors and propose the
device-dependent readability. Then, the news article
recommendation using the device-dependent readabil-
ity is explained. This recommendation is prepared to
prove the usefulness of the device-dependent readabil-
ity. In the experiments, we present the experimental
results on the relationship between reading devices and
readability. We also describe the experiments on news
recommendation using the device-dependent readabil-
ity and present their results. Finally, we summarize our
research.
2 Related work
The history of readability studies began in the 1800s.
Early studies focused on the frequency of easy words,
sentence length, and word length (Huld?en, 2004).
Flesch designed a formula to calculate ?reading ease?
using only the average word length and sentence length
(Flesch, 1948). He adjusted the relative importance
between word length and sentence length using 100
words selected randomly from a corpus. This formula
is called the Flesch-Kincaid formula, and is generally
used in measuring the readability of a textbook (Kin-
caid et al., 1975). Dale and Chall (1949) defined a list
of 3,000 easy words. Then, they used the average sen-
tence length and the percentage of words not included
in the list. These studies simply used superficial fac-
tors, and thus do not reflect syntactic factors.
Recent studies on readability use various factors in-
cluding syntactic ones, and combine them to produce
a highly predictive model of readability. Franc?ois and
Faircon (2012) proposed a readability formula with 46
textual factors for French as a foreign language. The
factors represent lexical, syntactic, and semantic char-
acteristics of sentences, and the specificities of French.
They are extracted from 28 French Foreign Language
(FFL) textbooks written for adults learning FFL. On the
other hand, Pitler and Nenkova (2008) showed the rela-
tion between readability factors and readability. They
used human ratings from the Wall Street Journal cor-
pus, and computed the correlations between the read-
ability factors and the average human ratings. Accord-
ing to their results, the average number of verb phrases
in a sentence, the number of words in an article, the
likelihood of the vocabulary, and the likelihood of the
discourse relations are highly correlated with human
ratings. However, these studies did not consider the
reading devices, but focused on how well a text is writ-
ten. Since the readability can be differentiated accord-
ing to reading device, a reading device should be con-
sidered when computing the readability of a given text.
To the best of our knowledge, there are few studies
on the readability on mobile devices that do not con-
sider language-related aspects. Most studies on mobile
devices focused on the development of new text format
and layout to help users read documents easily.
?
Oquist
(2006) proposed a new text presentation format called
the dynamic Rapid Serial Visual Presentation. Accord-
ing to his experimental results, this format helps to re-
duce eye movements. On the other hand, Hasegawa
et al. (2008) evaluated the readability of documents
on mobile devices with regard to screen and font size.
They reported that the readability is improved when the
characters are vertically enlarged. Readability on mo-
bile devices is not reflected only by the visualization
factors, but also by textual factors. Therefore, this pa-
per explores the readability factors that reflect the lexi-
cal and grammatical complexity of text and are affected
by reading devices.
3 Readability Factors
Table 1 lists the readability factors used in this paper.
Basically, they are based on the factors proposed by
Pitler and Nenkova (2008). However, some factors are
excluded and some new factors are added. This is be-
cause some of their factors are computationally infeasi-
ble and language-dependent. As a result, we have thir-
teen readability factors. These readability factors are
divided into four types: superficial, lexical, syntactic
factors, and lexical cohesion.
3.1 Superficial Factors
Superficial factors were used in most early readability
studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et
al., 1975), and reflect the construction of a text. We in-
vestigate four factors: text length (TL), sentence length
(SL), average number of words per sentence (WS), and
average number of characters per word (CW). Since
longer text is perceived as ?harder-to-read? than short
one, these factors are all reciprocally related with read-
ability.
The first two factors are related to length. TL counts
the number of characters in a text, whereas SL com-
putes the number of sentences. When a writer attempts
to write many topics in a text, she tends to use many
kinds of words simultaneously. As a result, the text be-
comes longer and more complex. Such long length of
text disturbs a reader?s comprehension of the text, and
then it is more difficult for the reader to read the text
(Heilman et al., 2008).
WS counts the average number of words per sen-
tence, and CW reflects the average number of characters
per word. When they are large, the sentence is diffi-
cult to read, which leads to difficulties in understanding
the text. Especially, CW reflects compound nouns and
technical words. For instance, compound nouns in Ko-
rean are usually long, because there is no spacing be-
tween words in a compound noun. For example, let us
consider a compound noun, ?Daehanmingukjungboo,?
which means the Korean government. Actually this
compound noun consists of two independent nouns.
1397
Type of Factors Abbr. Description
Superficial factors
TL The number of characters in a text
SL The number of sentences in a text
WS Average number of words per sentence
CW Average number of characters per word
Lexical factor LL Article likelihood estimated by language model
Syntactic factors
PTD Average parse tree depths per sentence
NP Average number of noun phrases per sentence
VP Average number of verb phrases per sentence
SBAR Average number of subordinate clauses per sentence
Lexical cohesion
COS Average cosine similarity between pairs of adjacent sentences
WO Average word overlap between pairs of adjacent sentences
NPO Average word overlap over noun and pronoun only
PRP Average number of pronouns per sentence
Table 1: Description of readability factors
One is ?Daehanminguk? meaning Korea and the other
is ?Jungboo? meaning a government. The two are con-
catenated to form a compound noun and become a long
single word. In addition, many difficult words such as
domain-specific terms tend to be long. Such lengthy
words make it difficult to read a text.
3.2 Lexical Factor
Lexical factor determines whether a given text con-
sists of frequent words. Texts that express a new trend
in various fields often use many newly coined words.
Such neologisms make it difficult to read and under-
stand a text. Therefore, an easily-understandable text
is composed of widely-used words rather than unusual
words.
In order to compute the use of frequent words in a
text, a unigram language model is used as in the work
of Pitler and Nenkova (2008). In this model, the log
likelihood of text t is computed by
?
w?t
C(w) ? logP (w|B). (1)
where P (w|B) is the probability of a word w according
to a background corpus B, and C(w) is the number of
times that w appears in t.
This factor examines the familiarity of the words
used in the text. The more frequently a word appears
in the background corpus, the more familiar it is re-
garded. The frequency of a word w is then reflected
into P (w|B) computed from the independent back-
ground corpus B. Therefore, the factor LL is positively
related with readability.
3.3 Syntactic Factors
Syntactic factors reflect sentence complexity directly
that affects human processing of a sentence. We con-
sider the average parse tree depth per sentence (PTD),
the average number of noun phrases per sentence (NP),
the average number of verb phrases per sentence (VP),
and the average number of subordinate clauses per sen-
tence (SBAR) as syntactic factors. These four factors
were defined by Schwarm and Ostendorf (2005).
A reader regards a text as difficult when the sen-
tences in the text have large parse tree depths or many
subordinate clauses. Thus, PTD and SBAR are related
negatively with readability. On the other hand, the re-
lationship of NP and VP to readability are not one way.
The large number of noun phrases in a text requires
a reader to remember more items (Barzilay and Lap-
ata, 2008; Pitler and Nenkova, 2008). However, it also
makes the text more interesting. The texts written for
adults actually contain more entities than those writ-
ten for children (Barzilay and Lapata, 2008). The same
is true for VP. The large number of verb phrases in a
sentence makes the sentence more complex. However,
people feel that a text is more easier to comprehend
when related clauses are grouped together (Bailin and
Grafstein, 2001).
3.4 Lexical Cohesion
Lexical cohesion denotes how the sentences in a text
are semantically connected. People usually bring con-
tinuous sentences into their mind at the same time, and
interpret them as a single unit (Okazaki et al., 2005). In
other words, a reader prefers text whose sentences are
smoothly connected to text whose sentences are inde-
pendent of one another. Therefore, sentence continuity
plays a primary role in understanding an entire text.
In the classic study of cohesion, various uses of
cohesive elements such as pronouns, definite articles,
and topic continuity have been discussed (Halliday and
Hasan, 1976). This paper uses the average cosine sim-
ilarity (COS), word overlap (WO), word overlap over
just nouns and pronouns (NPO) between pairs of adja-
cent sentences, and the average number of pronouns per
sentence (PRP). COS, WO, and NPO are superficial mea-
sures of topic continuity, whereas PRP is an indicative
feature of sentence continuity. High values for these
factors imply that the sentences in the text are related
somehow. Therefore, these factors are believed to be
related positively with readability.
1398
3.5 Measurement of Readability
When a reading device d is given, the readability of
text t, represented as R(t|d), is formulated as a com-
bination of readability factors with their corresponding
weight in the device. We assume that w
i|d
, the weight
of a readability factor f
i
, is dependent on the reading
device d. Following the previous work of Pitler and
Nenkova (2008), we also assume that each readabil-
ity factor affects readability independently. Therefore,
readability is calculated as a weighted linear sum of all
readability factors. That is, R(t|d) is computed by
R(t|d) =
?
i?{1,2,...,M}
w
i|d
? f
i
(t) (2)
where M is the number of readability factors.
Each weight w
i|d
is determined from a set of news
articles T . We collected a large number of news arti-
cles from an Internet news portal. The readability of
each article was manually labeled. This is done three
times, since we have three different devices of a smart
phone, a tablet, and paper. Since human rating of each
article t ? T is available for each device, w
i|d
?s can
be estimated by linear regression. These weights are
different according to the devices.
4 News Article Recommendation by
Device-Dependent Readability
The fact that the weights w
i|d
in Equation (2) are differ-
ent for each device d implies that the readability mea-
surement should be different depending on the device
type. In order to see the usefulness of this device-
dependent readability, we apply it to news article rec-
ommendation. News article recommendation aims to
provide a user with news articles that interest the user.
Thus, it selects a few articles that meet user preference
from a gigantic amount of news events. Various meth-
ods have reported notable results in news article rec-
ommendation (Das et al., 2007; Li et al., 2010; Liu et
al., 2010). In addition, with the recent interest in hand-
held devices, the demand for news recommendation on
hand-held devices is increasing. However, there has
been, at least as far as we know, no study on the read-
ability of hand-held devices.
Device-dependent readability is reflected into news
article recommendation through a re-ranking frame-
work. Figure 1 depicts the overall process of suggest-
ing news articles for a specific device with the device-
dependent readability. The point of this figure is to
measure how appropriate a news article is for a spe-
cific reading device. For this, a news recommendation
system first chooses a set of news articles from a news
repository based on its own criterion. Then, we re-rank
them by the device-dependent readability to obtain the
final set of ranked news articles for the device.
Formally, a news article recommendation ranks a set
of articles, A = {a
1
, a
2
, ..., a
m
}, where a
i
represents
the i-th article. The order between ranks a
1
 a
2

Min Max Average
Article length 68 610 346.5
# of sentences 1 14 6.24
# of words per sentence 8 33 16.93
# of words per article 17 178 99.34
Table 2: Statistics of the news article data
...  a
m
should be satisfied by the criterion of the
recommendation system. That is, assuming that the
system has a score function score(a
i
), score(a
i
) >
score(a
j
) has to be met if a
i
 a
j
. Then, the top
k(k ? m) articles of A by the score function are sug-
gested as appropriate news articles. After that, the se-
lected articles are re-ranked by another criterion, the
device-dependent readability. That is, the final rank of
an article within the selected set is determined by an-
other function, rerank. Since this function has to re-
flect the device-dependent readability, it takes two pa-
rameters. One is an article, and the other is a device
type. The re-rank function is modeled as
rerank(a, d) = R(a|d)
=
?
i?{1,2,...,M}
w
i|d
? f
i
(a). (3)
As a result, the readability-based re-ranking module
suggests the news articles based on how easily the ar-
ticles are read on a specific reading device. Note that
even the same article would be ranked differently ac-
cording to the device type because the article is re-
ranked by the device-dependent readability. At last, the
top k
?
(k
?
? k) re-ranked articles among them are sug-
gested as final news articles.
5 Experiments
5.1 Experiments on Readability Factors
5.1.1 Experiment Settings
For the experiments of analyzing relationship between
readability factors and readability, we collected a Ko-
rean news corpus from Naver News
1
. This corpus con-
tains news articles from June 10, 2013 to June 25,
2013. We selected 74 articles randomly from the cor-
pus which were used for readability formula and show-
ing the relationships between readability factors. All
selected articles belong to one of three categories: ?Pol-
itics?, ?Entertainment?, and ?Sports?. A set of these 74
news articles becomes T , and is used to compute the
weights in Equation (2). Table 2 describes a simple
statistics of the selected news articles. The shortest ar-
ticle consists of 68 characters, whereas the longest one
has 610 characters. The average length of article is
346.5. The shortest article is written in one sentence,
and the longest has 14 sentences. One article has ap-
proximately 6.24 sentences on average. In addition, the
1
A Korean news portal of which web address is
http://news.naver.com.
1399
News
Repository
Device
dependent
re-ranking
News
Articles
News
Recommendation
System
Readability
Figure 1: Overall process of re-ranking news articles based on device-dependent readability
number of words per sentence ranges from 8 to 33, and
the average is 16.93. The minimum number of words
in an article is 17, and the maximum number of words
is 178. An article is composed of 99.34 words on aver-
age.
In order to compute the lexical factor LL by Equa-
tion (1), a background corpus B is required. Since this
corpus should be independent from the news articles
explained above, the Naver News is adopted again to
generate B. For the background corpus B, we col-
lected news articles from January 1, 2013 to September
6, 2013, but excluded the articles from June 10 to June
25, because they are already used. This corpus consists
of 298,729 articles with 3,264,104 distinct words.
The readability score for each article was manually
labeled by three undergraduate students. To investigate
the relationship between reading devices and readabil-
ity, each article was read using three different reading
devices. The Galaxy Note 1 with a 5-inch screen is
used as the smart phone, Galaxy Tab 10.1 with a 10.1-
inch screen is used as the tablet, and A4-size paper
is used for the paper. That is, the human annotators
read and rated 74 articles per device. The order of the
devices where the annotators evaluated readability is
smart phone, tablet, and paper. This order was main-
tained for all the experiments. All aspects but content
texts were under control. For instance, font = ?Gothic,
12 pt? (this is most commonly used font and size that
most Korean web pages and textbooks use), font color
= ?black?, alignment = ?both? were used for all three
devices. In addition, the non-content aspects were ex-
actly same for devices because the annotators of read-
ability and the recommended articles shared the read-
ing devices. Although these aspects affect readability
and many previous studies already proved it, it is not
our concern. We only attempt to capture how read-
Reading device Min Max Average
Smart phone 1.67 5 3.423 ? 0.741
Tablet 1.33 5 3.531 ? 0.837
Paper 2 5 3.360 ? 0.594
Table 3: Readability scores given by human annotators
ability is affected by the content in different types of
devices.
Human annotators can remember the content of
news articles when they read articles with three de-
vices. The human annotators were asked to read and
evaluate many articles within a relatively short period.
Therefore, before the main experiments, we performed
a pilot experiment on the memory effects of previously
read articles and verified it empirically. We hired three
undergraduate students who were not involved in our
main experiments. The students read the same 250 ar-
ticles four times, and these also come from Naver News
corpus which are not included the previous 74 articles.
After their first reading, they read the articles again in
3, 7, and 14 days later. After 3 days, two students re-
membered the articles somewhat, but one student re-
membered them vaguely. Since they almost forgot the
articles after 7 days, we placed 7 days interval between
devices.
The readability score of an article was rated by the
annotators using the questions in the work of Pitler and
Nenkova (2008). We use only two of the questions,
while they used four questions for the annotators. Their
questions are intended to measure the extent of how
well a text is written, how it fits together, how easy
it is to understand, and how interesting it is. We can
consider ?well-written? and ?fit-together? as a syntac-
tic perspective, whereas ?easy to understand? and ?in-
teresting? belong to a content perspective. For such a
1400
Smart phone Tablet Paper
Factor Value Factor Value Factor Value
SL -0.394 SL -0.370 NP 0.298
TL -0.293 WS 0.321 WS 0.278
WS 0.288 LL 0.253 LL 0.268
LL 0.249 NP 0.240 VP 0.244
Table 4: Pearson correlation coefficients of important
readability factors
reason, four questions can be summarized in two ques-
tions. The two questions used are
? How well-written is this article?
? How interesting is this article?
For these two questions, each annotator assigns a score
between 1 and 5 to each article. Here, 1 point means
that the article is worst and 5 point implies that it is
best. A readability score of one human annotator is
composed with the average of two questions (well-
written, interesting). We used the average of three hu-
man annotators? readability scores in our experiments.
Table 3 shows the readability scores of the articles for
each device. According to this table, the readability
score ranges from 1.67 to 5 for the smart phone, 1.33
to 5 for the tablet, and ranges from 2 to 5 for the paper.
The average readability is 3.423 for the smart phone,
3.531 for the tablet, and 3.360 for the paper. To see
the inter-judge agreement among annotators, the Kappa
coefficient (Fleiss, 1971) is used. The Kappa values
for the ?smart phone?, ?tablet?, and ?paper? are 0.342,
0.333, and 0.361, respectively. All these values corre-
spond to fair agreement.
5.1.2 Experimental Results
In order to see the importance of each factor in a spe-
cific device, we adopt the Pearson correlation coeffi-
cients between readability factors and reading devices.
Table 4 lists the four most important factors in each
device and their Pearson correlation coefficients. Espe-
cially, p-value is smaller than 0.05 for all factors in this
table.
For the smart phone, SL, the number of sentences in
a text, is the most important readability factor. Its cor-
relation with the smart phone is -0.394. TL, the number
of characters, is the second important factor and has a
negative correlation of -0.293. These results imply that
readers are negatively sensitive to the length of an arti-
cle because of the small display size of a smart phone.
That is, in the smart phone, longer articles are recog-
nized as difficult to read compared to shorter ones. The
number of words per sentence, WS, is the third impor-
tant factor with correlation of 0.288. The log-likelihood
of an article, LL, is also positively related with the read-
ability, which proves that widely-used words make it
easy to understand an article. The top three factors are
superficial with regard to text length. Therefore, the su-
perficial factors are more important than other types of
factors for the smart phone.
SL is the most critical readability factor even for the
tablet. It affects readability with high correlation of -
0.370. The second important factor is WS with correla-
tion of 0.321. Both of these factors are superfical. The
third important factor, LL, is positively related with
readability as expected. The fourth factor that affects
readability is the number of noun phrases, NP. It is nat-
ural for NP to be positively related with the readability.
Finally, for the paper, NP is most strongly related to
readability with correlation of 0.298. The second im-
portant factor is WS, whose correlation is 0.278. LL is
the third important factor and shows a positive relation-
ship. Note that WS and LL are important readability
factors for all devices. The next important readabil-
ity factor for the paper is the average number of verb
phrases (VP). The articles with many noun phrases and
verb phrases are perceived as easier-to-read for the pa-
per. Note that the importance of superficial factors is
limited for the paper. We expected that WS is negatively
related, but, it is positively related with readability for
all three devices. The reason for this could be that the
annotators thought the articles with higher WS are more
interesting.
The important factors for the smart phone are differ-
ent from those for the paper. On the other hand, the
tablet shares many factors with both the smart phone
and the paper. Because the screen size of a tablet is
similar to the size of an A4 paper, the tablet and the pa-
per share readability factors. However, length-related
factors play a more important role than syntactic fac-
tors in the smart phone because a smart phone has a
smaller screen.
5.2 Experiments on News Recommendation
5.2.1 Experiment Settings
Experiments for news article recommendation were
performed to see the effectiveness of device-dependent
readability. The process of news recommendation with
device-dependent readability is as follows. For a spe-
cific device,
1. Select top-k news articles from a news repository
by the criterion of the recommendation system.
2. Re-rank the k articles by the readability of the de-
vice using Equation (3).
3. Select top-k
?
news articles by the new rank.
4. Human annotators read and rate the k
?
articles
with the device.
5. Compare the ranks of k
?
articles by device-
dependent readability with those by human rat-
ings.
Since we have three types of devices, this process is
performed three times with a different device.
The news articles from September 10, 2013 to
September 12, 2013 collected from Naver News were
1401
Min Max Average
Article length 277 6,077 990.68
# of sentences 4 199 22.85
# of words per sentence 4 100 15.73
# of words per article 71 2,034 301.61
Table 5: Statistics of news data for recommendation
Reading device Min Max Average
Smart phone 1 5 3.513 ? 0.962
Tablet 1 5 3.344 ? 0.852
Paper 1 5 3.250 ? 0.907
Table 6: Scores of news articles by human annotators
in news recommendation
used as the news repository. The number of times that
a news article was actually read by its anonymous read-
ers at the portal site is used as the criterion for the rec-
ommendation system. Since this criterion is provided
on a daily basis and news articles were collected for
three days, the process explained above is performed
three times. The top twenty articles were selected by
the criterion every day. That is, k = 20. Table 5 shows
the statistics of the total 60 articles. The shortest arti-
cle consists of 277 characters, and the longest article
has 6,077 characters. On average, an article is writ-
ten with 990.68 characters. The minimum number of
sentences in an article is 4, and the maximum number
of sentences is 199. An article is composed of 22.85
sentences on average. The average number of words in
a sentence is 15.73, whereas a sentence length ranges
from 4 to 100 words. The shortest article has 71 words,
and the longest article has 2,034 words. One article has
approximately 301.61 words on average.
Three human annotators labeled the scores of the
news articles manually. The annotators were the same
persons who labeled the readability scores. Similar to
the previous experiments, 7 days intervals was placed
among devices to reduce the memory effect. The same
two questions used in the previous section were used
again for this experiment. The annotators assigned a
score between 1 and 5 to every article for each ques-
tion. The final score of an article was obtained by aver-
aging six scores (two questions from three annotators).
Table 6 summarizes the scores of the articles by the
human annotators. As shown in this table, the article
scores vary for all reading devices. The average scores
for smart phone, tablet, and paper are 3.513, 3.344,
and 3.250 respectively. The Kappa value for the ?smart
phone? is 0.402, and that for both the ?tablet? and the
?paper? is 0.393. Thus, the value of ?smart phone? falls
into moderate agreement, whereas those of the ?tablet?
and ?paper? correspond to fair agreement. The perfor-
mance of the news article recommendation is evaluated
with the Normalized Discounted Cumulative Gain at
top P (NDCG@P ) (J?arvelin and Kek?al?ainen, 2002).
Figure 2: NDCG@k
?
scores with various k
?
for the
smart phone.
Figure 3: NDCG@k
?
scores with various k
?
for the
tablet.
5.2.2 Experimental Results
For the a baseline criterion, we use the news article
recommendation system in Naver, which recommends
news article by the number of article hits. Figures 2 to 4
show the NDCG@k
?
scores with 1 ? k
?
? 10 for the
three devices. Each graph in these figures compares the
performance of various devices when the readability
for a specific device is used. That is, Figure 2 depicts
the NDCG@k
?
scores for the recommended news arti-
cles when the articles are shown in the smart phone, the
tablet, and the paper respectively. In computing their
NDCG@k
?
scores, the news articles are re-ranked by
readability for the smart phone. Therefore, in this fig-
ure we expect that the NDCG@k
?
score for using the
smart phone is higher than those for using the tablet and
paper. In the same way, Figure 3 and Figure 4 compare
the NDCG@k
?
scores when the readabilities for the
tablet and paper are used.
In all three graphs, the best news recommendation
performance is achieved when the device used to read
1402
Figure 4: NDCG@k
?
scores with various k
?
for the
paper.
news articles is the same as the device used for read-
ability. In Figure 2, the use of the smart phone outper-
forms those of other devices when k
?
? 6. This proves
that the quality of highly ranked news articles is much
better for the smart phone than for other devices, when
the readability for smart phone is used.
Figure 3 shows the NDCG@k
?
scores for using var-
ious devices when the news articles are re-ranked by
readability for the tablet. In this figure, the use of
the tablet as a reading device is better than using the
smart phone or the paper. The performance difference
is largest at k
?
= 3. The difference becomes smaller
as k
?
increases up to 10, but the performance of tablet
is still higher than those of others. In Figure 2 and 3,
when k
?
= 1, the baseline outperforms other devices.
We believe this happens because the baseline chooses
news articles by user-hit. Therefore, many articles rec-
ommended by the baseline are interesting because peo-
ple tend to click more often when an article is inter-
esting. As noted, readability reflects users? interests,
which leads to high performance of the baseline. The
performance of paper is best in Figure 4, since the ar-
ticles are re-ranked by the readability for paper. Paper
outperforms all other devices for all k
?
s. Note that the
performances of the baseline are always lowest regard-
less of reading device.
From all results above, we can infer that the use of
device-dependent readability is helpful to news article
recommendation. This is because the readability fac-
tors that affect the readers of news articles are different
according to the reading device. Therefore, it is im-
portant to reflect the characteristics of a reading device
when recommending news articles.
6 Conclusion
In this paper, we have proposed a device-dependent
readability. Since a reading device is one of the most
important features of readability, different weights have
been assigned to the readability factors according to de-
vice type. We have shown that the important readabil-
ity factors are distinct according to the reading device
by investigating the correlation between the readability
factors and the reading device. Through the correlation,
we found that tablet shares many important factors with
both smart phone and paper.
The experiments on the news articles collected from
an Internet portal proved that readability is actually af-
fected by the reading device. In addition, the validity of
the device-dependent readability was shown by apply-
ing it to the news article recommendation. The news
articles were first ranked by the criterion of the recom-
mendation system. Then, they were re-ranked by the
device-dependent readability. Our experiments showed
that the recommendation performance of the re-ranked
articles gets best when the device used for readability is
the same as the reading device. These two types of ex-
periments proved the importance and effectiveness of
the device-dependent readability.
Acknowledgments
This work was supported by the IT R&D program of
MSIP/KEIT (10044494, WiseKB: Big data based self-
evolving knowledge base and reasoning platform) and
the Industrial Strategic Technology Development Pro-
gram (10035348, Development of a Cognitive Planning
and Learning Model for Mobile Platforms) funded by
the Ministry of Knowledge Economy(MKE, Korea).
References
Alan Bailin and Ann Grafstein. 2001. The linguistic
assumptions underlying readability formulae: A cri-
tique. Language & Communication, 21(3):285?301.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Edgar Dale and Jeanne Chall. 1949. The concept of
readability. Elementary English, 26(1):19?26.
Abhinandan Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th International Conference on
World Wide Web, pages 271?280.
Joseph Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin,
76(5):378?382.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of Applied Psychology, 32(3):221?233.
Thomas Franc?ois and C?edrick Fairon. 2012. An
AI readability formula for French as a foreign lan-
guage. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 466?477.
1403
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Group Ltd.
Satoshi Hasegawa, Kazuhiro Fujikake, Masako Omori,
and Masaru Miyao. 2008. Readability of charac-
ters on mobile phone liquid crystal displays. In-
ternational Journal of Occupational Safety and Er-
gonomics (JOSE), 14(3):293?304.
Michael Heilman, Kevyn Collins-Thompson, and
Maxine Eskenazi. 2008. An analysis of statistical
models and features for reading difficulty prediction.
In Proceedings of the Third Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 71?79.
Kristina Hillbom. 2009. Newspaper Readability: a
Broadsheet vs. a Tabloid. Ph.D. thesis, University of
G?avle.
M?ans Huld?en. 2004. Linguistic complexity in
two major american newspapers and the associated
press newswire, 1900?2000. Master?s thesis,
?
Abo
Akademi University.
Shoaib Jameel, Wai Lam, and Xiaojun Qian. 2012.
Ranking text documents based on conceptual dif-
ficulty using term embedding and sequential dis-
course cohesion. In Proceedings of the The 2012
IEEE/WIC/ACM International Joint Conferences on
Web Intelligence and Intelligent Agent Technology-
Volume 01, pages 145?152.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cu-
mulated gain-based evaluation of IR techniques.
ACM Transactions on Information Systems (TOIS),
20(4):422?446.
J. Peter Kincaid, Robert Fishburne Jr., Richard Rogers,
and Brad Chissom. 1975. Derivation of new read-
ability formulas (automated readability index, fog
count and flesch reading ease formula) for navy en-
listed personnel. Technical report, DTIC Document.
Harry Kitson. 1927. The mind of the buyer. MacMil-
lan Company.
Lihong Li, Wei Chu, John Langford, and Robert E.
Schapire. 2010. A contextual-bandit approach to
personalized news article recommendation. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 661?670.
Jiahui Liu, Peter Dolan, and Elin R. Pedersen. 2010.
Personalized news recommendation based on click
behavior. In Proceedings of the 15th International
Conference on Intelligent User Interfaces, pages 31?
40.
Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 548?552.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru
Ishizuka. 2005. Improving chronological ordering
of sentences extracted from multiple newspaper ar-
ticles. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4(3):321?339.
Gustav
?
Oquist. 2006. Evaluating readability on mo-
bile devices. Ph.D. thesis, Uppsala University.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
186?195.
Jack Richards, John Platt, Heidi Platt, and Christophe
Candlin. 1992. Longman Dictionary of Language
Teaching and Applied Linguistics, volume 78. Long-
man London.
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 523?530.
Xin Yan, Dawei Song, and Xue Li. 2006. Concept-
based document readability in domain specific infor-
mation retrieval. In Proceedings of the 15th ACM In-
ternational Conference on Information and Knowl-
edge Management, pages 540?549.
Mostafa Zamanian and Pooneh Heydari. 2012. Read-
ability of texts: State of the art. Theory and Practice
in Language Studies, 2(1):43?53.
1404
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1025?1034,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Cost Sensitive Part-of-Speech Tagging:
Differentiating Serious Errors from Minor Errors
Hyun-Je Song1 Jeong-Woo Son1 Tae-Gil Noh2 Seong-Bae Park1,3 Sang-Jo Lee1
1School of Computer Sci. & Eng. 2Computational Linguistics 3NLP Lab.
Kyungpook Nat?l Univ. Heidelberg University Dept. of Computer Science
Daegu, Korea Heidelberg, Germany University of Illinois at Chicago
{hjsong,jwson,tgnoh}@sejong.knu.ac.kr sbpark@uic.edu sjlee@knu.ac.kr
Abstract
All types of part-of-speech (POS) tagging er-
rors have been equally treated by existing tag-
gers. However, the errors are not equally im-
portant, since some errors affect the perfor-
mance of subsequent natural language pro-
cessing (NLP) tasks seriously while others do
not. This paper aims to minimize these serious
errors while retaining the overall performance
of POS tagging. Two gradient loss functions
are proposed to reflect the different types of er-
rors. They are designed to assign a larger cost
to serious errors and a smaller one to minor
errors. Through a set of POS tagging exper-
iments, it is shown that the classifier trained
with the proposed loss functions reduces se-
rious errors compared to state-of-the-art POS
taggers. In addition, the experimental result
on text chunking shows that fewer serious er-
rors help to improve the performance of sub-
sequent NLP tasks.
1 Introduction
Part-of-speech (POS) tagging is needed as a pre-
processor for various natural language processing
(NLP) tasks such as parsing, named entity recogni-
tion (NER), and text chunking. Since POS tagging is
normally performed in the early step of NLP tasks,
the errors in POS tagging are critical in that they
affect subsequent steps and often lower the overall
performance of NLP tasks.
Previous studies on POS tagging have shown
high performance with machine learning techniques
(Ratnaparkhi, 1996; Brants, 2000; Lafferty et al,
2001). Among the types of machine learning ap-
proaches, supervised machine learning techniques
were commonly used in early studies on POS tag-
ging. With the characteristics of a language (Rat-
naparkhi, 1996; Kudo et al, 2004) and informa-
tive features for POS tagging (Toutanova and Man-
ning, 2000), the state-of-the-art supervised POS tag-
ging achieves over 97% of accuracy (Shen et al,
2007; Manning, 2011). This performance is gen-
erally regarded as the maximum performance that
can be achieved by supervised machine learning
techniques. There have also been many studies on
POS tagging with semi-supervised (Subramanya et
al., 2010; S?gaard, 2011) or unsupervised machine
learning methods (Berg-Kirkpatrick et al, 2010;
Das and Petrov, 2011) recently. However, there still
exists room to improve supervised POS tagging in
terms of error differentiation.
It should be noted that not all errors are equally
important in POS tagging. Let us consider the parse
trees in Figure 1 as an example. In Figure 1(a),
the word ?plans? is mistagged as a noun where it
should be a verb. This error results in a wrong parse
tree that is severely different from the correct tree
shown in Figure 1(b). The verb phrase of the verb
?plans? in Figure 1(b) is discarded in Figure 1(a)
and the whole sentence is analyzed as a single noun
phrase. Figure 1(c) and (d) show another tagging er-
ror and its effect. In Figure 1(c), a noun is tagged as
a NNS (plural noun) where its correct tag is NN (sin-
gular or mass noun). However, the error in Figure
1(c) affects only locally the noun phrase to which
?physics? belongs. As a result, the general structure
of the parse tree in Figure 1(c) is nearly the same as
1025
SVP
VP
NP
The treasury 
to
raise 150 billion in cash.
DT NNP
TO
VB CD CD IN NN
S
plans
NNS
(a) A parse tree with a serious error.
S
VPNP
The   treasury 
DT NNP
S
VP
VPto
raise 150 billion in cash.
TO
VB CD CD IN NN
plans
VBZ
(b) The correct parse tree of the sentence?The treasury
plans . . .?.
S
NP VP
We
PRP
altered
VBN
NP
NP PP
the chemistry and physics
DT
of the atmosphere
NN CC NNS INDT NN
(c) A parse tree with a minor error.
S
NP VP
We
PRP
altered
VBN
NP
NP PP
the chemistry and physics
DT
of the atmosphere
NN CC NN INDT NN
(d) The correct parse tree of the sentence ?We altered
. . .?.
Figure 1: An example of POS tagging errors
the correct one in Figure 1(d). That is, a sentence
analyzed with this type of error would yield a cor-
rect or near-correct result in many NLP tasks such
as machine translation and text chunking.
The goal of this paper is to differentiate the seri-
ous POS tagging errors from the minor errors. POS
tagging is generally regarded as a classification task,
and zero-one loss is commonly used in learning clas-
sifiers (Altun et al, 2003). Since zero-one loss con-
siders all errors equally, it can not distinguish error
types. Therefore, a new loss is required to incorpo-
rate different error types into the learning machines.
This paper proposes two gradient loss functions to
reflect differences among POS tagging errors. The
functions assign relatively small cost to minor er-
rors, while larger cost is given to serious errors.
They are applied to learning multiclass support vec-
tor machines (Tsochantaridis et al, 2004) which is
trained to minimize the serious errors. Overall accu-
racy of this SVM is not improved against the state-
of-the-art POS tagger, but the serious errors are sig-
nificantly reduced with the proposed method. The
effect of the fewer serious errors is shown by apply-
ing it to the well-known NLP task of text chunking.
Experimental results show that the proposed method
achieves a higher F1-score compared to other POS
taggers.
The rest of the paper is organized as follows. Sec-
tion 2 reviews the related studies on POS tagging. In
Section 3, serious and minor errors are defined, and
it is shown that both errors are observable in a gen-
eral corpus. Section 4 proposes two new loss func-
tions for discriminating the error types in POS tag-
ging. Experimental results are presented in Section
5. Finally, Section 6 draws some conclusions.
2 Related Work
The POS tagging problem has generally been solved
by machine learning methods for sequential label-
1026
Tag category POS tags
Substantive NN, NNS, NNP, NNPS, CD, PRP, PRP$
Predicate VB, VBD, VBG, VBN, VBP, VBZ, MD, JJ, JJR, JJS
Adverbial RB, RBR, RBS, RP, UH, EX, WP, WP$, WRB, CC, IN, TO
Determiner DT, PDT, WDT
Etc FW, SYM, POS, LS
Table 1: Tag categories and POS tags in Penn Tree Bank tag set
ing. In early studies, rich linguistic features and su-
pervised machine learning techniques are applied by
using annotated corpora like the Wall Street Journal
corpus (Marcus et al, 1994). For instance, Ratna-
parkhi (1996) used a maximum entropy model for
POS tagging. In this study, the features for rarely
appearing words in a corpus are expanded to im-
prove the overall performance. Following this direc-
tion, various studies have been proposed to extend
informative features for POS tagging (Toutanova
and Manning, 2000; Toutanova et al, 2003; Man-
ning, 2011). In addition, various supervised meth-
ods such as HMMs and CRFs are widely applied to
POS tagging. Lafferty et al (2001) adopted CRFs
to predict POS tags. The methods based on CRFs
not only have all the advantages of the maximum
entropy markov models but also resolve the well-
known problem of label bias. Kudo et al (2004)
modified CRFs for non-segmented languages like
Japanese which have the problem of word boundary
ambiguity.
As a result of these efforts, the performance of
state-of-the-art supervised POS tagging shows over
97% of accuracy (Toutanova et al, 2003; Gime?nez
and Ma`rquez, 2004; Tsuruoka and Tsujii, 2005;
Shen et al, 2007; Manning, 2011). Due to the high
accuracy of supervised approaches for POS tagging,
it has been deemed that there is no room to im-
prove the performance on POS tagging in supervised
manner. Thus, recent studies on POS tagging focus
on semi-supervised (Spoustova? et al, 2009; Sub-
ramanya et al, 2010; S?gaard, 2011) or unsuper-
vised approaches (Haghighi and Klein, 2006; Gold-
water and Griffiths, 2007; Johnson, 2007; Graca et
al., 2009; Berg-Kirkpatrick et al, 2010; Das and
Petrov, 2011). Most previous studies on POS tag-
ging have focused on how to extract more linguistic
features or how to adopt supervised or unsupervised
approaches based on a single evaluation measure,
accuracy. However, with a different viewpoint for
errors on POS tagging, there is still some room to
improve the performance of POS tagging for subse-
quent NLP tasks, even though the overall accuracy
can not be much improved.
In ordinary studies on POS tagging, costs of er-
rors are equally assigned. However, with respect
to the performance of NLP tasks relying on the re-
sult of POS tagging, errors should be treated differ-
ently. In the machine learning community, cost sen-
sitive learning has been studied to differentiate costs
among errors. By adopting different misclassifica-
tion costs for each type of errors, a classifier is op-
timized to achieve the lowest expected cost (Elkan,
2001; Cai and Hofmann, 2004; Zhou and Liu, 2006).
3 Error Analysis of Existing POS Tagger
The effects of POS tagging errors to subsequent
NLP tasks vary according to their type. Some errors
are serious, while others are not. In this paper, the
seriousness of tagging errors is determined by cat-
egorical structures of POS tags. Table 1 shows the
Penn tree bank POS tags and their categories. There
are five categories in this table: substantive, pred-
icate, adverbial, determiner, and etc. Serious tag-
ging errors are defined as misclassifications among
the categories, while minor errors are defined as mis-
classifications within a category. This definition fol-
lows the fact that POS tags in the same category
form similar syntax structures in a sentence (Zhao
and Marcus, 2009). That is, inter-category errors are
treated as serious errors, while intra-category errors
are treated as minor errors.
Table 2 shows the distribution of inter-category
and intra-category errors observed in section 22?
24 of the WSJ corpus (Marcus et al, 1994) that is
tagged by the Stanford Log-linear Part-Of-Speech
1027
Predicted category
Substantive Predicate Adverbial Determiner Etc
Substantive 614 479 32 10 15
Predicate 585 743 107 2 14
True category Adverbial 41 156 500 42 2
Determiner 13 7 47 24 0
Etc 23 11 3 1 0
Table 2: The distribution of tagging errors on WSJ corpus by Stanford Part-Of-Speech Tagger.
Tagger (Manning, 2011) (trained with WSJ sections
00?18). In this table, bold numbers denote inter-
category errors while all other numbers show intra-
category errors. The number of total errors is 3,471
out of 129,654 words. Among them, 1,881 errors
(54.19%) are intra-category, while 1,590 of the er-
rors (45.81%) are inter-category. If we can reduce
these inter-category errors under the cost of mini-
mally increasing intra-category errors, the tagging
results would improve in quality.
Generally in POS tagging, all tagging errors are
regarded equally in importance. However, inter-
category and intra-category errors should be distin-
guished. Since a machine learning method is opti-
mized by a loss function, inter-category errors can
be efficiently reduced if a loss function is designed
to handle both types of errors with different cost. We
propose two loss functions for POS tagging and they
are applied to multiclass Support Vector Machines.
4 Learning SVMs with Class Similarity
POS tagging has been solved as a sequential labeling
problem which assumes dependency among words.
However, by adopting sequential features such as
POS tags of previous words, the dependency can be
partially resolved. If it is assumed that words are
independent of one another, POS tagging can be re-
garded as a multiclass classification problem. One
of the best solutions for this problem is by using an
SVM.
4.1 Training SVMs with Loss Function
Assume that a training data set D =
{(x1, y1), (x2, y2), . . . , (xl, yl)} is given where
xi ? Rd is an instance vector and yi ? {+1,?1}
is its class label. SVM finds an optimal hyperplane
satisfying
xi ? w + b ? +1 for yi = +1,
xi ? w + b ? ?1 for yi = ?1,
where w and b are parameters to be estimated from
training data D. To estimate the parameters, SVMs
minimizes a hinge loss defined as
?i = Lhinge(yi, w ? xi + b)
= max{0, 1 ? yi ? (w ? xi + b)}.
With regularizer ||w||2 to control model complexity,
the optimization problem of SVMs is defined as
min
w,?
1
2
||w||2 + C
l
?
i=1
?i,
subject to
yi(xi ? w + b) ? 1? ?i, and ?i ? 0 ?i,
where C is a user parameter to penalize errors.
Crammer et al (2002) expanded the binary-class
SVM for multiclass classifications. In multiclass
SVMs, by considering all classes the optimization
of SVM is generalized as
min
w,?
1
2
?
k?K
||wk||2 + C
l
?
i=1
?i,
with constraints
(wyi ? ?(xi, yi))? (wk ? ?(xi, k)) ? 1? ?i,
?i ? 0 ?i, ?k ? K \ yi,
where ?(xi, yi) is a combined feature representation
of xi and yi, and K is the set of classes.
1028
POS
SUBSTANTIVE
PREDICATE ADVERBIAL
OTHERS
NOUN
PRONOUN
DETERMINER
DT
PDT
NNS
NN NNP
NNPS
CD
PRP PRP$
VERB
VBD
VB
VBG
VBN
VBP
VBZ
MD
ADJECT
JJR
JJ JJS
SYM
FW POS
LS
ADVERB
WH- CONJUNCTION
RBR
RB RBS
RP
UH
EX
WP
WP$
WRB
IN
CC TO
WDT
Figure 2: A tree structure of POS tags.
Since both binary and multiclass SVMs adopt a
hinge loss, the errors between classes have the same
cost. To assign different cost to different errors,
Tsochantaridis et al (2004) proposed an efficient
way to adopt arbitrary loss function, L(yi, yj) which
returns zero if yi = yj , otherwise L(yi, yj) > 0.
Then, the hinge loss ?i is re-scaled with the inverse
of the additional loss between two classes. By scal-
ing slack variables with the inverse loss, margin vi-
olation with high loss L(yi, yj) is more severely re-
stricted than that with low loss. Thus, the optimiza-
tion problem with L(yi, yj) is given as
min
w,?
1
2
?
k?K
||wk||2 + C
l
?
i=1
?i, (1)
with constraints
(wyi ? ?(xi, yi))? (wk ? ?(xi, k)) ? 1?
?i
L(yi, k)
,
?i ? 0 ?i, ?k ? K \ yi,
With the Lagrange multiplier ?, the optimization
problem in Equation (1) is easily converted to the
following dual quadratic problem.
min
?
1
2
l
?
i,j
?
ki?K\yi
?
kj?K\yj
?i,ki?j,kj ?
J(xi, yi, ki)J(xj , yj, kj)?
l
?
i
?
ki?K\yi
?i,ki ,
with constraints
? ? 0 and
?
ki?K\yi
?i,ki
L(yi, ki)
? C, ?i = 1, ? ? ? , l,
where J(xi, yi, ki) is defined as
J(xi, yi, ki) = ?(xi, yi)? ?(xi, ki).
4.2 Loss Functions for POS tagging
To design a loss function for POS tagging, this paper
adopts categorical structures of POS tags. The sim-
plest way to reflect the structure of POS tags shown
in Table 1 is to assign larger cost to inter-category
errors than to intra-category errors. Thus, the loss
function with the categorical structure in Table 1 is
defined as
Lc(yi, yj) =
?
?
?
?
?
?
?
0 if yi = yj ,
? if yi 6= yj but they belong
to the same POS category,
1 otherwise,
(2)
where 0 < ? < 1 is a constant to reduce the value of
Lc(yi, yj) when yi and yj are similar. As shown in
this equation, inter-category errors have larger cost
than intra-category errors. This loss Lc(yi, yj) is
named as category loss.
The loss function Lc(yi, yj) is designed to reflect
the categories in Table 1. However, the structure
of POS tags can be represented as a more complex
structure. Let us consider the category, predicate.
1029
?
Class NN Class NNS
Class VB
(a) Multiclass SVMs with hinge loss
Class NN Class NNS
Class VB
?
L(NN, VB)
?
L(NN, NNS)
(b) Multiclass SVMs with the proposed loss
function
Figure 3: Effect of the proposed loss function in multiclass SVMs
This category has ten POS tags, and can be further
categorized into two sub-categories: verb and ad-
ject. Figure 2 represents a categorical structure of
POS tags as a tree with five categories of POS tags
and their seven sub-categories.
To express the tree structure of Figure 2 as a loss,
another loss function Lt(yi, yj) is defined as
Lt(yi, yj) =
1
2
[Dist(Pi,j , yi) +Dist(Pi,j, yj)]? ?, (3)
where Pi,j denotes the nearest common parent of
both yi and yj , and the function Dist(Pi,j, yi) re-
turns the number of steps from Pi,j to yi. The user
parameter ? is a scaling factor of a unit loss for a
single step. This loss Lt(yi, yj) returns large value
if the distance between yi and yj is far in the tree
structure, and it is named as tree loss.
As shown in Equation (1), two proposed loss
functions adjust margin violation between classes.
They basically assign less value for intra-category
errors than inter-category errors. Thus, a classi-
fier is optimized to strictly keep inter-category er-
rors within a smaller boundary. Figure 3 shows a
simple example. In this figure, there are three POS
tags and two categories. NN (singular or mass noun)
and NNS (plural noun) belong to the same cate-
gory, while VB (verb, base form) is in another cat-
egory. Figure 3(a) shows the decision boundary of
NN based on hinge loss. As shown in this figure, a
single ? is applied for the margin violation among
all classes. Figure 3(b) also presents the decision
boundary of NN, but it is determined with the pro-
posed loss function. In this figure, the margin vio-
lation is applied differently to inter-category (NN to
VB) and intra-category (NN to NNS) errors. It re-
sults in reducing errors between NN and VB even if
the errors between NN and NNS could be slightly
increased.
5 Experiments
5.1 Experimental Setting
Experiments are performed with a well-known stan-
dard data set, the Wall Street Journal (WSJ) corpus.
The data is divided into training, development and
test sets as in (Toutanova et al, 2003; Tsuruoka and
Tsujii, 2005; Shen et al, 2007). Table 3 shows some
simple statistics of these data sets. As shown in
this table, training data contains 38,219 sentences
with 912,344 words. In the development data set,
there are 5,527 sentences with about 131,768 words,
those in the test set are 5,462 sentences and 129,654
words. The development data set is used only to se-
lect ? in Equation (2) and ? in Equation (3).
Table 4 shows the feature set for our experiments.
In this table, wi and ti denote the lexicon and POS
tag for the i-th word in a sentence respectively. We
use almost the same feature set as used in (Tsuruoka
and Tsujii, 2005) including word features, tag fea-
1030
Training Develop Test
Section 0?18 19?21 22?24
# of sentences 38,219 5,527 5,462
# of terms 912,344 131,768 129,654
Table 3: Simple statistics of experimental data
Feature Name Description
Word features wi?2, wi?1, wi, wi+1, wi+2wi?1 ? wi, wi ? wi+1
Tag features
ti?2, ti?1, ti+1, ti+2
ti?2 ? ti?1, ti+1 ? ti+2
ti?2 ? ti?1 ? ti+1, ti?1 ? ti+1 ? ti+2
ti?2 ? ti?1 ? ti+1 ? ti+2
Tag/Word
combination
ti?2?wi, ti?1 ?wi, ti+1?wi, ti+2?wi
ti?1 ? ti+1 ? wi
Prefix features prefixes of wi (up to length 9)
Suffix features suffixes of wi (up to length 9)
Lexical features
whether wi contains capitals
whether wi has a number
whether wi has a hyphen
whether wi is all capital
whether wi starts with capital and
locates at the middle of sentence
Table 4: Feature template for experiments
tures, word/tag combination features, prefix and suf-
fix features as well as lexical features. The POS tags
for words are obtained from a two-pass approach
proposed by Nakagawa et al (2001).
In the experiments, two multiclass SVMs with the
proposed loss functions are used. One is CL-MSVM
with category loss and the other is TL-MSVM with
tree loss. A linear kernel is used for both SVMs.
5.2 Experimental Results
CL-MSVM with ? = 0.4 shows the best overall per-
formance on the development data where its error
rate is as low as 2.71%. ? = 0.4 implies that the
cost of intra-category errors is set to 40% of that of
inter-category errors. The error rate of TL-MSVM
is 2.69% when ? is 0.6. ? = 0.4 and ? = 0.6 are set
in the all experiments below.
Table 5 gives the comparison with the previous
work and proposed methods on the test data. As can
be seen from this table, the best performing algo-
rithms achieve near 2.67% error rate (Shen et al,
2007; Manning, 2011). CL-MSVM and TL-MSVM
Error
(%)
# of Intra
error
# of Inter
error
(Gime?nez and Ma`rquez,
2004) 2.84
1,995
(54.11%)
1,692
(45.89%)
(Tsuruoka and Tsujii,
2005) 2.85 - -
(Shen et al, 2007) 2.67 1,856(53.52%)
1,612
(46.48%)
(Manning, 2011) 2.68 1,881(54.19%)
1,590
(45.81%)
CL-MSVM (? = 0.4) 2.69 1,916(55.01%)
1,567
(44.99%)
TL-MSVM (? = 0.6) 2.68 1,904(54.74%)
1,574
(45.26%)
Table 5: Comparison with the previous works
achieve an error rate of 2.69% and 2.68% respec-
tively. Although overall error rates of CL-MSVM
and TL-MSVM are not improved compared to the
previous state-of-the-art methods, they show reason-
able performance.
For inter-category error, CL-MSVM achieves the
best performance. The number of inter-category er-
ror is 1,567, which shows 23 errors reduction com-
pared to previous best inter-category result by (Man-
ning, 2011). TL-MSVM also makes 16 less inter-
category errors than Manning?s tagger. When com-
pared with Shen?s tagger, both CL-MSVM and TL-
MSVM make far less inter-category errors even if
their overall performance is slightly lower than that
of Shen?s tagger. However, the intra-category er-
ror rate of the proposed methods has some slight
increases. The purpose of proposed methods is to
minimize inter-category errors but preserving over-
all performance. From these results, it can be found
that the proposed methods which are trained with the
proposed loss functions do differentiate serious and
minor POS tagging errors.
5.3 Chunking Experiments
The task of chunking is to identify the non-recursive
cores for various types of phrases. In chunking, the
POS information is one of the most crucial aspects in
identifying chunks. Especially inter-category POS
errors seriously affect the performance of chunking
because they are more likely to mislead the chunk
compared to intra-category errors.
Here, chunking experiments are performed with
1031
POS tagger Accuracy (%) Precision Recall F1-score
(Shen et al, 2007) 96.08 94.03 93.75 93.89
(Manning, 2011) 96.08 94 93.8 93.9
CL-MSVM (? = 0.4) 96.13 94.1 93.9 94.00
TL-MSVM (? = 0.6) 96.12 94.1 93.9 94.00
Table 6: The experimental results for chunking
a data set provided for the CoNLL-2000 shared
task. The training data contains 8,936 sentences
with 211,727 words obtained from sections 15?18
of the WSJ. The test data consists of 2,012 sentences
and 47,377 words in section 20 of the WSJ. In order
to represent chunks, an IOB model is used, where
every word is tagged with a chunk label extended
with B (the beginning of a chunk), I (inside a chunk),
and O (outside a chunk). First, the POS informa-
tion in test data are replaced to the result of our POS
tagger. Then it is evaluated using trained chunking
model. Since CRFs (Conditional Random Fields)
has been shown near state-of-the-art performance in
text chunking (Fei Sha and Fernando Pereira, 2003;
Sun et al, 2008), we use CRF++, an open source
CRF implementation by Kudo (2005), with default
feature template and parameter settings of the pack-
age. For simplicity in the experiments, the values
of ? in Equation (2) and ? in Equation (3) are set
to be 0.4 and 0.6 respectively which are same as the
previous section.
Table 6 gives the experimental results of text
chunking according to the kinds of POS taggers in-
cluding two previous works, CL-MSVM, and TL-
MSVM. Shen?s tagger and Manning?s tagger show
nearly the same performance. They achieve an ac-
curacy of 96.08% and around 93.9 F1-score. On the
other hand, CL-MSVM achieves 96.13% accuracy
and 94.00 F1-score. The accuracy and F1-score of
TL-MSVM are 96.12% and 94.00. Both CL-MSVM
and TL-MSVM show slightly better performances
than other POS taggers. As shown in Table 5, both
CL-MSVM and TL-MSVM achieve lower accura-
cies than other methods, while their inter-category
errors are less than that of other experimental meth-
ods. Thus, the improvement of CL-MSVM and TL-
MSVM implies that, for the subsequent natural lan-
guage processing, a POS tagger should considers
different cost of tagging errors.
6 Conclusion
In this paper, we have shown that supervised POS
tagging can be improved by discriminating inter-
category errors from intra-category ones. An inter-
category error occurs by mislabeling a word with
a totally different tag, while an intra-category error
is caused by a similar POS tag. Therefore, inter-
category errors affect the performances of subse-
quent NLP tasks far more than intra-category errors.
This implies that different costs should be consid-
ered in training POS tagger according to error types.
As a solution to this problem, we have proposed
two gradient loss functions which reflect different
costs for two error types. The cost of an error type is
set according to (i) categorical difference or (ii) dis-
tance in the tree structure of POS tags. Our POS
experiment has shown that if these loss functions
are applied to multiclass SVMs, they could signif-
icantly reduce inter-category errors. Through the
text chunking experiment, it is shown that the multi-
class SVMs trained with the proposed loss functions
which generate fewer inter-category errors achieve
higher performance than existing POS taggers.
We have shown that cost sensitive learning can be
applied to POS tagging only with multiclass SVMs.
However, the proposed loss functions are general
enough to be applied to other existing POS taggers.
Most supervised machine learning techniques are
optimized on their loss functions. Therefore, the
performance of POS taggers based on supervised
machine learning techniques can be improved by ap-
plying the proposed loss functions to learn their clas-
sifiers.
Acknowledgments
This research was supported by the Converg-
ing Research Center Program funded by the
Ministry of Education, Science and Technology
(2011K000659).
References
Yasemin Altun, Mark Johnson, and Thomas Hofmann.
2003. Investigating Loss Functions and Optimiza-
tion Methods for Discriminative Learning of Label Se-
quences. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing. pp.
145?152.
1032
Talyor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Un-
supervised Learning with Features. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics. pp. 582?590.
Thorsten Brants. 2000. TnT-A Statistical Part-of-Speech
Tagger. In Proceedings of the Sixth Applied Natural
Language Processing Conference. pp. 224?231.
Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal Document Categorization with Support Vector Ma-
chines. In Proceedings of the Thirteenth ACM Inter-
national Conference on Information and Knowledge
Management. pp. 78?87.
Koby Crammer, Yoram Singer. 2002. On the Algorith-
mic Implementation of Multiclass Kernel-based Vec-
tor Machines. Journal of Machine Learning Research,
Vol. 2. pp. 265?292.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics. pp.
600?609.
Charles Elkan. 2001. The Foundations of Cost-Sensitive
Learning. In Proceedings of the Seventeenth Interna-
tional Joint Conference on Artificial Intelligence. pp.
973?978.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation.
pp. 43?46.
Sharon Goldwater and Thomas T. Griffiths. 2007. A
fully Bayesian Approach to Unsupervised Part-of-
Speech Tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics. pp. 744?751.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs Parameter Sparsity in La-
tent Variable Models. In Advances in Neural Informa-
tion Processing Systems 22. pp. 664?672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
Learning for Sequence Models. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics. pp. 320?327.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of the 2007 Joint Meet-
ing of the Conference on Empirical Methods in Natu-
ral Language Processing and the Conference on Com-
putational Natural Language Learning. pp. 296?305.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 230?237.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning. pp. 282?289.
Christopher D. Manning. 2011. Part-of-Speech Tagging
from 97% to 100%: Is It Time for Some Linguistics?.
In Proceedings of the 12th International Conference
on Intelligent Text Processing and Computational Lin-
guistics. pp. 171?189.
Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto.
2001. Unknown Word Guessing and Part-of-Speech
Tagging Using Support Vector Machines. In Proceed-
ings of the Sixth Natural Language Processing Pacific
Rim Symposium. pp. 325?331.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 133?142.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. In Proceedings of
the Human Language Technology and North American
Chapter of the Association for Computational Linguis-
tics. pp. 213?220.
Libin Shen, Giorgio Satta, and Aravind K. Joshi 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. pp.
760?767.
Anders S?gaard 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In Proceed-
ings of the 49th Annual Meeting of the Association of
Computational Linguistics. pp. 48?52.
Drahom??ra ?johanka? Spoustova`, Jan Hajic?, Jan Raab,
and Miroslav Spousta 2009. Semi-supervised training
for the averaged perceptron POS tagger. In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics. pp. 763?771.
Amarnag Subramanya, Slav Petrov and Fernando Pereira
2010. Efficient Graph-Based Semi-Supervised Learn-
ing of Structured Tagging Models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 167?176.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara
and Jun?ichi Tsujii 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics.
pp. 841?848.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
1033
In Proceedings of the Human Language Technology
and North American Chapter of the Association for
Computational Linguistics. pp. 252?259.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing. pp. 63?70.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemi Altun. 2004. Support Vec-
tor Learning for Interdependent and Structured Output
Spaces. In Proceedings of the 21st International Con-
ference on Machine Learning. pp. 104?111.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy for
Tagging Sequence Data. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing. pp. 467?474.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, Vol. 19, No.2 . pp. 313?330.
Qiuye Zhao and Mitch Marcus. 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 688?697.
Zhi-Hua Zhou and Xu-Ying Liu 2006. On Multi-Class
Cost-Sensitive Learning. In Proceedings of the AAAI
Conference on Artificial Intelligence. pp. 567?572.
1034
