Proceedings of NAACL HLT 2009: Short Papers, pages 29?32,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Large-scale Computation of Distributional Similarities for Queries
Enrique Alfonseca
Google Research
Zurich, Switzerland
ealfonseca@google.com
Keith Hall
Google Research
Zurich, Switzerland
kbhall@google.com
Silvana Hartmann
University of Stuttgart
Stuttgart, Germany
silvana.hartmann@ims.uni-stuttgart.de
Abstract
We present a large-scale, data-driven approach
to computing distributional similarity scores
for queries. We contrast this to recent web-
based techniques which either require the off-
line computation of complete phrase vectors,
or an expensive on-line interaction with a
search engine interface. Independent of the
computational advantages of our approach, we
show empirically that our technique is more
effective at ranking query alternatives that the
computationally more expensive technique of
using the results from a web search engine.
1 Introduction
Measuring the semantic similarity between queries
or, more generally, between pairs of very short texts,
is increasingly receiving attention due to its many
applications. An accurate metric of query simi-
larities is useful for query expansion, to improve
recall in Information Retrieval systems; for query
suggestion, to propose to the user related queries
that might help reach the desired information more
quickly; and for sponsored search, where advertisers
bid for keywords that may be different but semanti-
cally equivalent to user queries.
In this paper, we study the problem of measuring
similarity between queries using corpus-based unsu-
pervised methods. Given a query q, we would like
to rank all other queries according to their similarity
to q. The proposed approach compares favorably to
a state-of-the-art unsupervised system.
2 Related work
Distributional similarity methods model the similar-
ity or relatedness of words using a metric defined
over the set of contexts in which the words appear
(Firth, 1957). One of the most common representa-
tions for contexts is the vector space model (Salton
et al, 1975). This is the basic idea of approaches
such as (Grefenstette, 1992; Bordag, 2008; Lin,
1998; Riloff and Shepherd, 1997), with some varia-
tions; e.g., whether syntactic information is used ex-
plicitly, or which weight function is applied. Most of
the existing work has focused on similarity between
single words or syntactically-correct multiword ex-
pressions. In this work, we adapt these techniques
to calculate similarity metrics between pairs of com-
plete queries, which may or may not be syntactically
correct.
Other approaches for query similarity use sta-
tistical translation models (Riezler et al, 2008),
analysing search engine logs (Jones et al, 2006),
looking for different anchor texts pointing to the
same pages (Kraft and Zien, 2004), or replacing
query words with other words that have the high-
est pointwise mutual information (Terra and Clarke,
2004).
Sahami and Helman (Sahami and Heilman, 2006)
define a web kernel function for semantic similarity
based on the snippets of the search results returned
by the queries. The algorithm used is the following:
(a) Issue a query x to a search engine and collect
the set of n snippets returned by the search engine;
(b) Compute the tf?idf vector vi for each document
snippet di; (c) Truncate each vector to include its m
29
highest weighted terms; (d) Construct the centroid
of the L2-normalized vectors vi; (e) Calculate the
similarity of two queries as the dot product of their
L2-normalized vectors, i.e. as the cosine of both
vectors.
This work was followed up by Yih and Meek (Yih
and Meek, 2007), who combine the web kernel with
other simple metrics of similarity between word vec-
tors (Dice Coefficient, Jaccard Coefficient, Overlap,
Cosine, KL Divergence) in a machine learning sys-
tem to provide a ranking of similar queries.
3 Proposed method
Using a search engine to collect snippets (Sahami
and Heilman, 2006; Yih and Meek, 2007; Yih and
Meek, 2008) takes advantage of all the optimizations
performed by the retrieval engine (spelling correc-
tion, relevance scores, etc.), but it has several disad-
vantages: first, it is not repeatable, as the code un-
derlying search engines is in a constant state of flux;
secondly, it is usually very expensive to issue a large
number of search requests; sometimes the APIs pro-
vided limit the number of requests. In this section,
we describe a method which overcomes these draw-
backs. The distributional methods we propose for
calculating similarities between words and multi-
word expressions profit from the use of a large Web-
based corpus.
The contextual vectors for a query can be col-
lected by identifying the contexts in which the query
appears. Queries such as [buy a book] and [buy
some books] are supposed to appear close to simi-
lar context words in a bag-of-words model, and they
should have a high similarity. However, there are
two reasons why this would yield poor results:
First, as the length of the queries grows, the prob-
ability of finding exact queries in the corpus shrinks
quickly. As an example, when issuing the queries
[Lindsay Lohan pets] and [Britney Spears pets] to
Google enclosed in double quotes, we obtain only
6 and 760 results, respectively. These are too few
occurrences in order to collect meaningful statistics
about the contexts of the queries.
Secondly, many user queries are simply a concate-
nation of keywords with weak or no underlying syn-
tax. Therefore, even if they are popular queries, they
may not appear as such in well-formed text found
in web documents. For example, queries like [hol-
lywood dvd cheap], enclosed in double quotes, re-
trieve less than 10 results. Longer queries, such as
[hotel cheap new york fares], are still meaningful,
but do not appear frequently in web documents.
In order to use of distributional similarities in the
query setting, we propose the following method.
Given a query of interest p = [w1, w2, ..., wn]:
1. For each word wi collect all words that appear
close to wi in the web corpus (i.e., a bag-fo-
words models). Empirically we have chosen
all the words whose distance to wi is less or
equal to 3. This gives us a vector of context
words and frequencies for each of the words in
the query, ~vi = (fi1, fi2, ..., fi|V |), where |V | is
the size of the corpus vocabulary.
2. Represent the query p with a vector of words,
and the weight associated to each word is the
geometric mean of the frequencies for the word
in the original vectors:
~qv =
0
B
@
0
@
|n|Y
i=1
fi1
1
A
1
n
,
0
@
|n|Y
i=1
fi2
1
A
1
n
, ...,
0
@
|n|Y
i=1
fi|V |
1
A
1
n
1
C
A
3. Apply the ?2 test as a weighting function test to
measure whether the query and the contextual
feature are conditionally independent.
4. Given two queries, use the cosine between their
vectors to calculate their similarity.
The motivations for this approach are: the geo-
metric mean is a way to approximate a boolean AND
operation between the vectors, while at the same
time keeping track of the magnitude of the frequen-
cies. Therefore, if two queries only differ on a very
general word, e.g. [books] and either [buy books]
or [some books], the vector associated to the general
words (buy or some in the example) will have non-
zero values for most of the contextual features, be-
cause they are not topically constrained; and the vec-
tors for the queries will have similar sets of features
with non-zero values. Equally relevant, terms that
are closely related will appear in the proximity of a
similar set of words and will have similar vectors.
For example, if the two queries are Sir Arthur Co-
nan Doyle books and Sir Arthur Conan Doyle nov-
els, given that the vectors for books and novels are
expected to have similar features, these two queries
30
Contextual word acid fast bacteria Query
acidogenicity 11 6 4 6.41506
auramin 2 5 2 2.71441
bacillae 3 10 4 4.93242
carbolfuchsin 1 28 2 8.24257
dehydrogena 5 3 3 3.55689
diphtheroid 5 9 92 16.05709
fuchsine 42 3 4 7.95811
glycosilation 3 2 3 2.62074
Table 1: Example of context words for the query [acid fast bacteria].
will receive a high similarity score.
On the other hand, this combination also helps in
reducing word ambiguity. Consider the query bank
account; the bag-of-words vector for bank will con-
tain words related to the various senses of the word,
but when combining it to account only the terms that
belong to the financial domain and are shared be-
tween the two vectors will be included in the final
query vector.
Finally, we note that the geometric mean provides
a clean way to encode the pair-wise similarities of
the individual words of the phrase. One can inter-
pret the cosine similarity metric as the magnitude of
the vector constructed by the scalar product of the
individual vectors. Our approach scales this up by
taking the scalar product of the vectors for all words
in the phrase and then scaling them by the number of
words (i.e., the geometric mean). Instead of comput-
ing the magnitude of this vector, we use it to com-
pute similarities for the entire phrase.
As an example of the proposed procedure, Table 1
shows a random sample of the contextual features
collected for the words in the query [acid fast bac-
teria], and how the query?s vector is generated by
using the geometric mean of the frequencies of the
features in the vectors for the query words.
4 Experiments and results
4.1 Experimental settings
To collect the contextual features for words and
phrases, we have used a corpus of hundreds of mil-
lions of documents crawled from the Web in August
2008. An HTML parser is used to extract text and
non-English documents are discarded. After pro-
cess, the remaining corpus contains hundreds of bil-
lions of words.
As a source of keywords, we have used the top
0 1 2 3 4
0 280 95 14 1 0
1 108 86 65 4 0
2 11 47 83 16 0
3 1 2 17 45 2
4 0 0 1 1 2
Table 2: Confusion matrix for the pairs in the goldstandard. Rows
represent first rater scores, and columns second rater scores.
one and a half million English queries sent to the
Google search engine after being fully anonymized.
We have calculated the pairwise similarity between
all queries, which would potentially return 2.25 tril-
lion similarity scores, but in practice returns a much
smaller number as many pairs have non-overlapping
contexts.
As a baseline, we have used a new implementa-
tion of the Web Kernel similarity (Sahami and Heil-
man, 2006). The parameters are set the same as re-
ported in the paper with the exception of the snip-
pet size; in their study, the size was limited to 1,000
characters and in our system, the normal snippet re-
turned by Google is used (around 160 characters).
In order to evaluate our system, we prepared a
goldstandard set of query similarities. We have ran-
domly sampled 65 queries from our full dataset, and
obtained the top 20 suggestions from both the Sa-
hami system and the distributional similarities sys-
tem. Two human raters have rated the original query
and the union of the sets of suggestions, using the
same 5-point Likert scale that Sahami used. Table 2
shows the confusion matrix of scores between the
two raters. Most of the disagreements are between
the scores 0 and 1, which means that probably it was
not clear enough whether the queries were unrelated
or only slightly related. It is also noteworthy that
in this case, very few rewritten queries were clas-
sified as being better than the original, which also
suggests to us that probably we could remove the
topmost score from the classifications scale.
We have evaluated inter-judge agreement in the
following two ways: first, using the weighted Kappa
score, which has a value of 0.7111. Second, by
grouping the pairs judged as irrelevant or slightly
relevant (scores 0 and 1) as a class containing nega-
tive examples, and the pairs judged as very relevant,
equal or better (scores 2 through 4) as a class con-
taining positive examples. Using this two-class clas-
31
Method Prec@1 Prec@3 Prec@5 mAP AUC
Web Kernel 0.39 0.35 0.32 0.49 0.22
Unigrams 0.47 0.53 0.47 0.57 0.26
N-grams 0.70 0.57 0.52 0.71 0.54
Table 3: Results. mAP is mean average precision, and AUC is the
area under the precision/recall curve.
sification, Cohen?s Kappa score becomes 0.6171.
Both scores indicates substantial agreement amongst
the raters.
The data set thus collected is a ranked list of sug-
gestions for each query1, and can be used to evaluate
any other suggestion-ranking system.
4.2 Experiments and results
As an evolution of the distributional similarities
approach, we also implemented a second version
where the queries are chunked into phrases. The
motivation for the second version is that, in some
queries, like [new york cheap hotel], it makes sense
to handle new york as a single phrase with a sin-
gle associated context vector collected from the web
corpus. The list of valid n-grams is collected by
combining several metrics, e.g. whether Wikipedia
contains an entry with that name, or whether they
appear quoted in query logs. The queries are then
chunked greedily always preferring the longer n-
gram from our list.
Table 3 shows the results of trying both systems
on the same set of queries. The original system is
the one called Unigrams, and the one that chunks
the queries is the one called N-grams. The distri-
butional similarity approaches outperform the web-
based kernel on all the metrics, and chunking queries
shows a good improvement over using unigrams.
5 Conclusions
This paper extends the vector-space model of dis-
tributional similarities to query-to-query similarities
by combining different vectors using the geometric
mean. We show that using n-grams to chunk the
queries improves the results significantly. This out-
performs the web-based kernel method, a state-of-
the-art unsupervised query-to-query similarity tech-
nique, which is particularly relevant as the corpus-
based method does not benefit automatically from
1We plan to make it available to the research community.
search engine features.
References
S. Bordag. 2008. A Comparison of Co-occurrence and
Similarity Measures as Simulations of Context. Lec-
ture Notes in Computer Science, 4919:52.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis, pages 1?32.
G. Grefenstette. 1992. Use of syntactic context to pro-
duce term association lists for text retrieval. In Pro-
ceedings of the 15th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 89?97. ACM New York, NY,
USA.
R. Jones, B. Rey, O. Madani, andW. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15th
international conference on World Wide Web, pages
387?396. ACM New York, NY, USA.
Reiner Kraft and Jason Zien. 2004. Mining anchor text
for query refinement. In WWW ?04: Proceedings of
the 13th international conference on World Wide Web,
pages 666?674, New York, NY, USA. ACM.
D. Lin. 1998. Extracting Collocations from Text Cor-
pora. In First Workshop on Computational Terminol-
ogy, pages 57?63.
Stefan Riezler, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Improved
Query Expansion. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING?08).
E. Riloff and J. Shepherd. 1997. A corpus-based ap-
proach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124. As-
sociation for Computational Linguistics.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international con-
ference on World Wide Web, pages 377?386.
G. Salton, A. Wong, and CS Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613?620.
Egidio Terra and Charles L.A. Clarke. 2004. Scoring
missing terms in information retrieval tasks. In CIKM
?04: Proceedings of the thirteenth ACM international
conference on Information and knowledge manage-
ment, pages 50?58, New York, NY, USA. ACM.
W. Yih and C. Meek. 2007. Improving Similarity Mea-
sures for Short Segments of Text. In Proceedings of
the Natural Conference on Artificial Intelligence, vol-
ume 2, page 1489. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999.
W. Yih and C. Meek. 2008. Consistent Phrase Relevance
Measures. Data Mining and Audience Intelligence for
Advertising (ADKDD 2008), page 37.
32
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 580?590,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
UBY ? A Large-Scale Unified Lexical-Semantic Resource
Based on LMF
Iryna Gurevych??, Judith Eckle-Kohler?, Silvana Hartmann?, Michael Matuschek?,
Christian M. Meyer? and Christian Wirth?
? Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
? Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science
Technische Universita?t Darmstadt
http://www.ukp.tu-darmstadt.de
Abstract
We present UBY, a large-scale lexical-
semantic resource combining a wide range
of information from expert-constructed
and collaboratively constructed resources
for English and German. It currently
contains nine resources in two lan-
guages: English WordNet, Wiktionary,
Wikipedia, FrameNet and VerbNet,
German Wikipedia, Wiktionary and
GermaNet, and multilingual OmegaWiki
modeled according to the LMF standard.
For FrameNet, VerbNet and all collabora-
tively constructed resources, this is done
for the first time. Our LMF model captures
lexical information at a fine-grained level
by employing a large number of Data
Categories from ISOCat and is designed
to be directly extensible by new languages
and resources. All resources in UBY can
be accessed with an easy to use publicly
available API.
1 Introduction
Lexical-semantic resources (LSRs) are the foun-
dation of many NLP tasks such as word sense
disambiguation, semantic role labeling, question
answering and information extraction. They are
needed on a large scale in different languages.
The growing demand for resources is met nei-
ther by the largest single expert-constructed re-
sources (ECRs), such as WordNet and FrameNet,
whose coverage is limited, nor by collaboratively
constructed resources (CCRs), such as Wikipedia
and Wiktionary, which encode lexical-semantic
knowledge in a less systematic form than ECRs,
because they are lacking expert supervision.
Previously, there have been several indepen-
dent efforts of combining existing LSRs to en-
hance their coverage w.r.t. their breadth and depth,
i.e. (i) the number of lexical items, and (ii) the
types of lexical-semantic information contained
(Shi and Mihalcea, 2005; Johansson and Nugues,
2007; Navigli and Ponzetto, 2010b; Meyer and
Gurevych, 2011). As these efforts often targeted
particular applications, they focused on aligning
selected, specialized information types. To our
knowledge, no single work focused on modeling
a wide range of ECRs and CCRs in multiple lan-
guages and a large variety of information types in
a standardized format. Frequently, the presented
model is not easily scalable to accommodate an
open set of LSRs in multiple languages and the in-
formation mined automatically from corpora. The
previous work also lacked the aspects of lexicon
format standardization and API access. We be-
lieve that easy access to information in LSRs is
crucial in terms of their acceptance and broad ap-
plicability in NLP.
In this paper, we propose a solution to this. We
define a standardized format for modeling LSRs.
This is a prerequisite for resource interoperabil-
ity and the smooth integration of resources. We
employ the ISO standard Lexical Markup Frame-
work (LMF: ISO 24613:2008), a metamodel for
LSRs (Francopoulo et al 2006), and Data Cate-
gories (DCs) selected from ISOCat.1 One of the
main challenges of our work is to develop a model
that is standard-compliant, yet able to express the
information contained in diverse LSRs, and that in
the long term supports the integration of the vari-
ous resources.
The main contributions of this paper can be
1http://www.isocat.org/
580
summarized as follows: (1) We present an LMF-
based model for large-scale multilingual LSRs
called UBY-LMF. We model the lexical-semantic
information down to a fine-grained level of in-
formation (e.g. syntactic frames) and employ
standardized definitions of linguistic information
types from ISOCat. (2) We present UBY, a large-
scale LSR implementing the UBY-LMF model.
UBY currently contains nine resources in two
languages: English WordNet (WN, Fellbaum
(1998), Wiktionary2 (WKT-en), Wikipedia3 (WP-
en), FrameNet (FN, Baker et al(1998)), and
VerbNet (VN, Kipper et al(2008)); German Wik-
tionary (WKT-de), Wikipedia (WP-de), and Ger-
maNet (GN, Kunze and Lemnitzer (2002)), and
the English and German entries of OmegaWiki4
(OW), referred to as OW-en and OW-de. OW,
a novel CCR, is inherently multilingual ? its ba-
sic structure are multilingual synsets, which are a
valuable addition to our multilingual UBY. Essen-
tial to UBY are the nine pairwise sense alignments
between resources, which we provide to enable
resource interoperability on the sense level, e.g.
by providing access to the often complementary
information for a sense in different resources. (3)
We present a Java-API which offers unified access
to the information contained in UBY.
We will make the UBY-LMF model, the re-
source UBY and the API freely available to the
research community.5 This will make it easy for
the NLP community to utilize UBY in a variety of
tasks in the future.
2 Related Work
The work presented in this paper concerns
standardization of LSRs, large-scale integration
thereof at the representational level, and the uni-
fied access to lexical-semantic information in the
integrated resources.
Standardization of resources. Previous work
includes models for representing lexical informa-
tion relative to ontologies (Buitelaar et al 2009;
McCrae et al 2011), and standardized single
wordnets (English, German and Italian wordnets)
in the ISO standard LMF (Soria et al 2009; Hen-
rich and Hinrichs, 2010; Toral et al 2010).
2http://www.wiktionary.org/
3http://www.wikipedia.org/
4http://www.omegawiki.org/
5http://www.ukp.tu-darmstadt.de/data/uby
McCrae et al(2011) propose LEMON, a con-
ceptual model for lexicalizing ontologies as an
extension of the LexInfo model (Buitelaar et al
2009). LEMON provides an LMF-implementation
in the Web Ontology Language (OWL), which
is similar to UBY-LMF, as it also uses DCs
from ISOCat, but diverges further from the stan-
dard (e.g. by removing structural elements such
as the predicative representation class). While
we focus on modeling lexical-semantic informa-
tion comprehensively and at a fine-grained level,
the goal of LEMON is to support the linking be-
tween ontologies and lexicons. This goal entails
a task-targeted application: domain-specific lex-
icons are extracted from ontology specifications
and merged with existing LSRs on demand. As a
consequence, there is no available large-scale in-
stance of the LEMON model.
Soria et al(2009) define WordNet-LMF, an
LMF model for representing wordnets used in
the KYOTO project, and Henrich and Hinrichs
(2010) do this for GN, the German wordnet.
These models are similar, but they still present
different implementations of the LMF meta-
model, which hampers interoperability between
the resources. We build upon this work, but ex-
tend it significantly: UBY goes beyond model-
ing a single ECR and represents a large number
of both ECRs and CCRs with very heterogeneous
content in the same format. Also, UBY-LMF
features deeper modeling of lexical-semantic in-
formation. Henrich and Hinrichs (2010), for
instance, do not explicitly model the argument
structure of subcategorization frames, since each
frame is represented as a string. In UBY-LMF,
we represent them at a fine-grained level neces-
sary for the transparent modeling of the syntax-
semantics interface.
Large-scale integration of resources. Most
previous research efforts on the integration of re-
sources targeted at world knowledge rather than
lexical-semantic knowledge. Well known exam-
ples are YAGO (Suchanek et al 2007), or DBPe-
dia (Bizer et al 2009).
Atserias et al(2004) present the Meaning Mul-
tilingual Central Repository (MCR). MCR inte-
grates five local wordnets based on the Interlin-
gual Index of EuroWordNet (Vossen, 1998). The
overall goal of the work is to improve word sense
disambiguation. This work is similar to ours, as it
581
aims at a large-scale multilingual resource and in-
cludes several resources. It is however restricted
to a single type of resource (wordnets) and fea-
tures a single type of lexical information (seman-
tic relations) specified upon synsets. Similarly,
de Melo and Weikum (2009) create a multilin-
gual wordnet by integrating wordnets, bilingual
dictionaries and information from parallel cor-
pora. None of these resources integrate lexical-
semantic information, such as syntactic subcate-
gorization or semantic roles.
McFate and Forbus (2011) present NULEX,
a syntactic lexicon automatically compiled from
WN, WKT-en and VN. As their goal is to cre-
ate an open-license resource to enhance syntactic
parsing, they enrich verbs and nouns in WN with
inflection information from WKT-en and syntac-
tic frames from VN. Thus, they only use a small
part of the lexical information present in WKT-en.
Padro? et al(2011) present their work on lex-
icon merging within the Panacea Project. One
goal of Panacea is to create a lexical resource de-
velopment platform that supports large-scale lex-
ical acquisition and can be used to combine exist-
ing lexicons with automatically acquired ones. To
this end, Padro? et al(2011) explore the automatic
integration of subcategorization lexicons. Their
current work only covers Spanish, and though
they mention the LMF standard as a potential data
model, they do not make use of it.
Shi and Mihalcea (2005) integrate FN, VN and
WN, and Palmer (2009) presents a combination of
Propbank, VN and FN in a resource called SEM-
LINK in order to enhance semantic role labeling.
Similar to our work, multiple resources are in-
tegrated, but their work is restricted to a single
language and does not cover CCRs, whose pop-
ularity and importance has grown tremendously
over the past years. In fact, with the excep-
tion of NULEX, CCRs have only been consid-
ered in the sense alignment of individual resource
pairs (Navigli and Ponzetto, 2010a; Meyer and
Gurevych, 2011).
API access for resources. An important factor
to the success of a large, integrated resource is a
single public API, which facilitates the access to
the information contained in the resource. The
most important LSRs so far can be accessed us-
ing various APIs, for instance the Java WordNet
API,6 or the Java-based Wikipedia API.7
With a stronger focus of the NLP community
on sharing data and reproducing experimental re-
sults these tools are becoming important as never
before. Therefore, a major design objective of
UBY is a single API. This is similar in spirit to the
motivation of Pradhan et al(2007), who present
integrated access to corpus annotations as a main
goal of their work on standardizing and integrat-
ing corpus annotations in the OntoNotes project.
To summarize, related work focuses either on
the standardization of single resources (or a single
type of resource), which leads to several slightly
different formats constrained to these resources,
or on the integration of several resources in an
idiosyncratic format. CCRs have not been con-
sidered at all in previous work on resource stan-
dardization, and the level of detail of the model-
ing is insufficient to fully accommodate different
types of lexical-semantic information. API ac-
cess is rarely provided. This makes it hard for
the community to exploit their results on a large
scale. Thus, it diminishes the impact that these
projects might achieve upon NLP beyond their
original specific purpose, if their results were rep-
resented in a unified resource and could easily be
accessed by the community through a single pub-
lic API.
3 UBY ? Data model
LMF defines a metamodel of LSRs in the Uni-
fied Modeling Language (UML). It provides a
number of UML packages and classes for model-
ing many different types of resources, e.g. word-
nets and multilingual lexicons. The design of
a standard-compliant lexicon model in LMF in-
volves two steps: in the first step, the structure
of the lexicon model has to be defined by choos-
ing a combination of the LMF core package and
zero to many extensions (i.e. UML packages). In
the second step, these UML classes are enriched
by attributes. To contribute to semantic interop-
erability, it is essential for the lexicon model that
the attributes and their values refer to Data Cat-
egories (DCs) taken from a reference repository.
DCs are standardized specifications of the terms
that are used for attributes and their values, or in
other words, the linguistic vocabulary occurring
6http://sourceforge.net/projects/jwordnet/
7http://code.google.com/p/jwpl/
582
in a lexicon model. Consider, for instance, the
term lexeme that is defined differently in WN and
FN: in FN, a lexeme refers to a word form, not
including the sense aspect. In WN, on the con-
trary, a lexeme is an abstract pairing of mean-
ing and form. According to LMF, the DCs are
to be selected from ISOCat, the implementation
of the ISO 12620 Data Category Registry (DCR,
Broeder et al(2010)), resulting in a Data Cate-
gory Selection (DCS).
Design of UBY-LMF. We have designed UBY-
LMF8 as a model of the union of various hetero-
geneous resources, namely WN, GN, FN, and VN
on the one hand and CCRs on the other hand.
Two design principles guided our development
of UBY-LMF: first, to preserve the information
available in the original resources and to uni-
formly represent it in UBY-LMF. Second, to be
able to extend UBY in the future by further lan-
guages, resources, and types of linguistic infor-
mation, in particular, alignments between differ-
ent LSRs.
Wordnets, FN and VN are largely complemen-
tary regarding the information types they provide,
see, e.g. Baker and Fellbaum (2009). Accord-
ingly, they use different organizational units to
represent this information. Wordnets, such as
WN and GN, primarily contain information on
lexical-semantic relations, such as synonymy, and
use synsets (groups of lexemes that are synony-
mous) as organizational units. FN focuses on
groups of lexemes that evoke the same prototypi-
cal situation (so-called semantic frames, Fillmore
(1982)) involving semantic roles (so-called frame
elements). VN, a large-scale verb lexicon, is or-
ganized in Levin-style verb classes (Levin, 1993)
(groups of verbs that share the same syntactic al-
ternations and semantic roles) and provides rich
subcategorization frames including semantic roles
and a specification of semantic predicates.
UBY-LMF employs several direct subclasses
of Lexicon in order to account for the various or-
ganization types found in the different LSRs con-
sidered. While the LexicalEntry class reflects
the traditional headword-based lexicon organiza-
tion, Synset represents synsets from wordnets,
SemanticPredicate models FN semantic
frames, and SubcategorizationFrameSet
corresponds to VN alternation classes.
8See www.ukp.tu-darmstadt.de/data/uby
SubcategorizationFrame is com-
posed of syntactic arguments, while
SemanticPredicate is composed of se-
mantic arguments. The linking between syntactic
and semantic arguments is represented by the
SynSemCorrespondence class.
The SenseAxis class is very important in
UBY-LMF, as it connects the different source
LSRs. Its role is twofold: first, it links the cor-
responding word senses from different languages,
e.g. English and German. Second, it represents
monolingual sense alignments, i.e. sense align-
ments between different lexicons in the same lan-
guage. The latter is a novel interpretation of
SenseAxis introduced by UBY-LMF.
The organization of lexical-semantic knowl-
edge found in WP, WKT, and OW can be mod-
eled with the classes in UBY-LMF as well. WP
primarily provides encyclopedic information on
nouns. It mainly consists of article pages which
are modeled as Senses in UBY-LMF.
WKT is in many ways similar to tradi-
tional dictionaries, because it enumerates senses
under a given headword on an entry page.
Thus, WKT entry pages can be represented by
LexicalEntries and WKT senses by Senses.
OW is different from WKT and WP, as it is or-
ganized in multilingual synsets. To model OW
in UBY-LMF, we split the synsets per language
and included them as monolingual Synsets in
the corresponding Lexicon (e.g., OW-en or OW-
de). The original multilingual information is pre-
served by adding a SenseAxis between corre-
sponding synsets in OW-en and OW-de.
The LMF standard itself contains only few lin-
guistic terms and does neither specify attributes
nor their values. Therefore, an important task in
developing UBY-LMF has been the specification
of attributes and their values along with the proper
attachment of attributes to LMF classes. In partic-
ular, this task involved selecting DCs from ISO-
Cat and, if necessary, adding new DCs to ISOCat.
Extensions in UBY-LMF. Although UBY-
LMF is largely compliant with LMF, the task of
building a homogeneous lexicon model for many
highly heterogeneous LSRs led us to extend LMF
in several ways: we added two new classes and
several new relationships between classes.
First, we were facing a huge variety of lexical-
semantic labels for many different dimensions of
583
semantic classification. Examples of such dimen-
sions include ontological type (e.g. selectional re-
strictions in VN and FN), domain (e.g. Biology in
WN), style and register (e.g. labels in WKT, OW),
or sentiment (e.g. sentiment of lexical units in
FN). Since we aim at an extensible LMF-model,
capable of representing further dimensions of se-
mantic classification, we did not squeeze the in-
formation on semantic classes present in the con-
sidered LSRs into existing LMF classes. Instead,
we addressed this issue by introducing a more
general class, SemanticLabel, which is an op-
tional subclass of Sense, SemanticPredicate,
and SemanticArgument. This new class has
three attributes, encoding the name of the label,
its type (e.g. ontological, register, sentiment), and
a numeric quantification (e.g. sentiment strength).
Second, we attached the subclass Frequency
to most of the classes in UBY-LMF, in order to
encode frequency information. This is of partic-
ular importance when using the resource in ma-
chine learning applications. This extension of the
standard has already been made in WordNet-LMF
(Soria et al 2009). Currently, the Frequency
class is used to keep corpus frequencies for lex-
ical units in FN, but we plan to use it for en-
riching many other classes with frequency in-
formation in future work, such as Senses or
SubcategorizationFrames.
Third, the representation of FN in LMF re-
quired adding two new relationships between
LMF classes: we added a relationship between
SemanticArgument and Definition, in or-
der to represent the definitions available for frame
elements in FN. In addition, we added a re-
lationship between the Context class and the
MonoLingualExternalRef, to represent the
links to annotated corpus sentences in FN.
Finally, WKT turned out to be hard to tackle,
because it contains a special kind of ambiguity in
the semantic relations and translation links listed
for senses: the targets of both relations and trans-
lation links are ambiguous, as they refer to lem-
mas (word forms), rather than to senses (Meyer
and Gurevych, 2010). These ambiguous rela-
tion targets could not directly be represented in
LMF, since sense and translation relations are
defined between senses. To resolve this, we
added a relationship between SenseRelation
and FormRepresentation, in order to encode
the ambiguous WKT relation target as a word
form. Disambiguating the WKT relation targets
to infer the target sense is left to future work.
A related issue occurred, when we mapped WN
to LMF. WN encodes morphologically related
forms as sense relations. UBY-LMF represents
these related forms not only as sense relations (as
in WordNet-LMF), but also at the morphologi-
cal level using the RelatedForm class from the
LMF Morphology extension. In LMF, however,
the RelatedForm class for morphologically re-
lated lexemes is not associated with the corre-
sponding sense in any way. Discarding the WN
information on the senses involved in a particular
morphological relation would lead to information
loss in some cases. Consider as an example the
WN verb buy (purchase) which is derivationally
related to the noun buy, while on the other hand
buy (accept as true, e.g. I can?t buy this story) is
not derivationally related to the noun buy. We ad-
dressed this issue by adding a sense attribute to
the RelatedForm class. Thus, in extension of
LMF, UBY-LMF allows sense relations to refer to
a form relation target and morphological relations
to refer to a sense relation target.
Data Categories in UBY-LMF. We encoun-
tered large differences in the availability of DCs
in ISOCat for the morpho-syntactic, lexical-
syntactic, and lexical-semantic parts of UBY-
LMF. Many DCs were missing in ISOCat and we
had to enter them ourselves. While this was feasi-
ble at the morpho-syntactic and lexical-syntactic
level, due to a large body of standardization re-
sults available, it was much harder at the lexical-
semantic level where standardization is still on-
going. At the lexical-semantic level, UBY-LMF
currently allows string values for a number of at-
tribute values, e.g. for semantic roles. We can eas-
ily integrate the results of the ongoing standard-
ization efforts into UBY-LMF in the future.
4 UBY ? Population with information
4.1 Representing LSRs in UBY-LMF
UBY-LMF is represented by a DTD (as suggested
by the standard) which can be used to automat-
ically convert any given resource into the corre-
sponding XML format.9 This conversion requires
a detailed analysis of the resource to be converted,
followed by the definition of a mapping of the
9Therefore, UBY-LMF can be considered as a serializa-
tion of LMF.
584
concepts and terms used in the original resource
to the UBY-LMF model. There are two major
tasks involved in the development of an automatic
conversion routine: first, the basic organizational
unit in the source LSR has to be identified and
mapped, e.g. synset in WN or semantic frame in
FN, and second, it has to be determined, how a
(LMF) sense is defined in the source LSR.
A notable aspect of converting resources into
UBY-LMF is the harmonization of linguistic ter-
minology used in the LSRs. For instance, a
WN Word and a GN Lexical Unit are mapped to
Sense in UBY-LMF.
We developed reusable conversion routines for
the future import of updated versions of the source
LSRs into UBY, provided the structure of the
source LSR remains stable. These conversion
routines extract lexical data from the source LSRs
by calling their native APIs (rather than process-
ing the underlying XML data). Thus, all lexical
information which can be accessed via the APIs
is converted into UBY-LMF.
Converting the LSRs introduced in the previ-
ous section yielded an instantiation of UBY-LMF
named UBY. The LexicalResource instance
UBY currently comprises 10 Lexicon instances,
one each for OW-de and OW-en, and one lexicon
each for the remaining eight LSRs.
4.2 Adding Sense Alignments
Besides the uniform and standardized representa-
tion of the single LSRs, one major asset of UBY
is the semantic interoperability of resources at the
sense level. In the following, we (i) describe how
we converted already existing sense alignments of
resources into LMF, and (ii) present a framework
to infer alignments automatically for any pair of
resources.
Existing Alignments. Previous work on sense
alignment yielded several alignments, such as
WN?WP-en (Niemann and Gurevych, 2011),
WN?WKT-en (Meyer and Gurevych, 2011) and
VN?FN (Palmer, 2009).
We converted these alignments into UBY-LMF
by creating a SenseAxis instance for each pair of
aligned senses. This involved mapping the sense
IDs from the proprietary alignment files to the
corresponding sense IDs in UBY.
In addition, we integrated the sense alignments
already present in OW and WP. Some OW en-
tries provide links to the corresponding WP page.
Also, the German and English language editions
of WP and OW are connected by inter-language
links between articles (Senses in UBY). We can
expect that these links have high quality, as they
were entered manually by users and are subject
to community control. Therefore, we straightfor-
wardly imported them into UBY.
Alignment Framework. Automatically creat-
ing new alignments is difficult because of word
ambiguities, different granularities of senses,
or language specific conceptualizations (Navigli,
2006). To support this task for a large number
of resources across languages, we have designed
a flexible alignment framework based on the
state-of-the-art method of Niemann and Gurevych
(2011). The framework is generic in order to al-
low alignments between different kinds of entities
as found in different resources, e.g. WN synsets,
FN frames or WP articles. The only requirement
is that the individual entities are distinguishable
by a unique identifier in each resource.
The alignment consists of the following steps:
First, we extract the alignment candidates for a
given resource pair, e.g. WN sense candidates for
a WKT-en entry. Second, we create a gold stan-
dard by manually annotating a subset of candi-
date pairs as ?valid? or ?non-valid?. Then, we
extract the sense representations (e.g. lemmatized
bag-of-words based on glosses) to compute the
similarity of word senses (e.g. by cosine similar-
ity). The gold standard with corresponding sim-
ilarity values is fed into Weka (Hall et al 2009)
to train a machine learning classifier, and in the
final step this classifier is used to automatically
classify the candidate sense pairs as (non-)valid
alignment. Our framework also allows us to train
on a combination of different similarity measures.
Using our framework, we were able to re-
produce the results reported by Niemann and
Gurevych (2011) and Meyer and Gurevych
(2011) based on the publicly available evaluation
datasets10 and the configuration details reported
in the corresponding papers.
Cross-Lingual Alignment. In order to align
word senses across languages, we extended the
monolingual sense alignment described above to
the cross-lingual setting. Our approach utilizes
10http://www.ukp.tu-darmstadt.de/data/sense-alignment/
585
Moses,11 trained on the Europarl corpus. The
lemma of one of the two senses to be aligned
as well as its representations (e.g. the gloss) is
translated into the language of the other resource,
yielding a monolingual setting. E.g., the WN
synset {vessel, watercraft} with its gloss ?a craft
designed for water transportation? is translated
into {Schiff, Wasserfahrzeug} and ?Ein Fahrzeug
fu?r Wassertransport?, and then the candidate ex-
traction and all downstream steps can take place
in German. An inherent problem with this ap-
proach is that incorrect translations also lead to
invalid alignment candidates. However, these are
most probably filtered out by the machine learn-
ing classifier as the calculated similarity between
the sense representations (e.g. glosses) should be
low if the candidates do not match.
We evaluated our approach by creating a cross-
lingual alignment between WN and OW-de, i.e.
the concepts in OW with a German lexicaliza-
tion.12 To our knowledge, this is the first study on
aligning OW with another LSR. OW is especially
interesting for this task due to its multilingual con-
cepts, as described by Matuschek and Gurevych
(2011). The created gold standard could, for in-
stance, be re-used to evaluate alignments for other
languages in OW.
To compute the similarity of word senses, we
followed the approach by Niemann and Gurevych
(2011) while covering both translation directions.
We used the cosine similarity for comparing the
German OW glosses with the German translations
of WN glosses and cosine and personalized page
rank (PPR) similarity for comparison of the Ger-
man OW glosses translated into English with the
original English WN glosses. Note that PPR sim-
ilarity is not available for German as it is based
on WN. Thereby, we filtered out the OW con-
cepts without a German gloss which left us with
11,806 unique candidate pairs. We randomly se-
lected 500 WN synsets for analysis yielding 703
candidate pairs. These were manually annotated
as being (non-)alignments. For the subsequent
machine learning task we used a simple threshold-
based classifier and ten-fold cross validation.
Table 1 summarizes the results of different sys-
tem configurations. We observe that translation
11http://www.statmt.org/moses/
12OmegaWiki consists of interlinked language-
independent concepts to which lexicalizations in several
languages are attached.
Translation Similarity
direction measure P R F1
EN > DE Cosine (Cos) 0.666 0.575 0.594
DE > EN Cos 0.674 0.658 0.665
DE > EN PPR 0.721 0.712 0.716
DE > EN PPR + Cos 0.723 0.712 0.717
Table 1: Cross-lingual alignment results
into English works significantly better than into
German. Also, the more elaborate similarity mea-
sure PPR yields better results than cosine similar-
ity, while the best result is achieved by a combina-
tion of both. Niemann and Gurevych (2011) make
a similar observation for the monolingual setting.
Our F-measure of 0.717 in the best configuration
lies between the results of Meyer and Gurevych
(2011) (0.66) and Niemann and Gurevych (2011)
(0.78), and thus verifies the validity of the ma-
chine translation approach. Therefore, the best
alignment was subsequently integrated into UBY.
5 Evaluating UBY
We performed an intrinsic evaluation of UBY by
computing a number of resource statistics. Our
evaluation covers two aspects: first, it addresses
the question if our automatic conversion routines
work correctly. Second, it provides indicators for
assessing UBY in terms of the gain in coverage
compared to the single LSRs.
Correctness of conversion. Since we aim to
preserve the maximal amount of information from
the original LSRs, we should be able to replace
any of the original LSRs and APIs by UBY and
the UBY-API without losing information. As
the conversion is largely performed automatically,
systematic errors and information loss could be
introduced by a faulty conversion routine. In or-
der to detect such errors and to prove the correct-
ness of the automatic conversion and the result-
ing representation, we have compared the orig-
inal resource statistics of the classes and infor-
mation types in the source LSRs to the cor-
responding classes in their UBY counterparts.
For instance, the number of lexical relations in
WordNet has been compared to the number of
SenseRelations in the UBY WordNet lexi-
con.13
13For detailed analysis results see the UBY website.
586
Lexical Sense
Lexicon Entry Sense Relation
FN 9,704 11,942 ?
GN 83,091 93,407 329,213
OW-de 30,967 34,691 60,054
OW-en 51,715 57,921 85,952
WP-de 790,430 838,428 571,286
WP-en 2,712,117 2,921,455 3,364,083
WKT-de 85,575 72,752 434,358
WKT-en 335,749 421,848 716,595
WN 156,584 206,978 8,559
VN 3,962 31,891 ?
UBY 4,259,894 4,691,313 5,300,941
Table 2: UBY resource statistics (selected classes).
Lexicon pair Languages SenseAxis
WN?WP-en EN?EN 50,351
WN?WKT-en EN?EN 99,662
WN?VN EN?EN 40,716
FN?VN EN?EN 17,529
WP-en?OW-en EN?EN 3,960
WP-de?OW-de DE?DE 1,097
WN?OW-de EN?DE 23,024
WP-en?WP-de EN?DE 463,311
OW-en?OW-de EN?DE 58,785
UBY All 758,435
Table 3: UBY alignment statistics.
Gain in coverage. UBY offers an increased
coverage compared to the single LSRs as reflected
in the resource statistics. Tables 2 and 3 show the
statistics on central classes in UBY. As UBY is
organized in several Lexicons, the number of
UBY lexical entries is the sum of the lexical en-
tries in all 10 Lexicons. Thus, UBY contains
more than 4.2 million lexical entries, 4.6 million
senses, 5.3 million semantic relations between
senses and more than 750,000 alignments. These
statistics represent the total numbers of lexical en-
tries, senses and sense relations in UBY without
filtering of identical (i.e. corresponding) lexical
entries, senses and relations. Listing the num-
ber of unique senses would require a full align-
ment between all integrated resources, which is
currently not available.
We can, however, show that UBY contains over
3.08 million unique lemma-POS combinations for
English and over 860,000 for German, over 3.94
million in total, see Table 4. Therefore, we as-
sessed the coverage on lemma level. Table 4 also
shows the number of lemmas with entries in one
or more than one lexicon, additionally split by
POS and language. Lemmas occurring only once
in UBY increase the coverage at lemma level. For
lemmas with parallel entries in several UBY lex-
icons, new information becomes available in the
form of additional sense definitions and comple-
mentary information types attached to lemmas.
Finally, the increase in coverage at sense level
can be estimated for senses that are aligned across
at least two UBY-lexicons. We gain access to
all available, partly complementary information
types attached to these aligned senses, e.g. seman-
tic relations, subcategorization frames, encyclo-
pedic or multilingual information. The number
of pairwise sense alignments provided by UBY is
given in Table 3. In addition, we computed how
many senses simultaneously take part in at least
two pairwise sense alignments. For English, this
applies to 31,786 senses, for which information
from 3 UBY lexicons is available.
EN Lexicons noun verb adjective
5 1 699 -
4 1,630 1,888 430
3 8,439 1,948 2,271
2 53,856 4,727 12,290
1 2,900,652 50,209 41,731
? (unique EN) 3,080,771
DE Lexicons noun verb adjective
4 1,546 - -
3 10,374 372 342
2 26,813 3,174 2,643
1 803,770 6,108 7,737
? (unique DE) 862,879
Table 4: Number of lemmas (split by POS and lan-
guage) with entries in i UBY lexicons, i = 1, . . . , 5.
6 Using UBY
UBY API. For convenient access to UBY, we
implemented a Java-API which is built around
the Hibernate14 framework. Hibernate allows to
easily store the XML data which results from
converting resources into Uby-LMF into a corre-
sponding SQL database.
Our main design principle was to keep the ac-
cess to the resource as simple as possible, despite
the rich and complex structure of UBY. Another
14http://www.hibernate.org/
587
important design aspect was to ensure that the
functionality of the individual, resource-specific
APIs or user interfaces is mirrored in the UBY
API. This enables porting legacy applications to
our new resource. To facilitate the transition to
UBY, we plan to provide reference tables which
list the corresponding UBY-API operations for the
most important operations in the WN API, some
of which are shown in Table 5.
WN function UBY function
Dictionary UBY
getIndexWord(pos,
lemma)
getLexicalEntries(
pos, lemma)
IndexWord LexicalEntry
getLemma() getLemmaForm()
Synset Synset
getGloss() getDefinitionText()
getWords() getSenses()
Pointer SynsetRelation
getType() getRelName()
Word Sense
getPointers() getSenseRelations()
Table 5: Some equivalent operations in WN API and
UBY API.
While it is possible to limit access to single re-
sources by a parameter and thus mimic the behav-
ior of the legacy APIs (e.g. only retrieve Synsets
and their relations from WN), the true power of
UBY API becomes visible when no such con-
straints are applied. In this case, all imported re-
sources are queried to get one combined result,
while retaining the source of the respective in-
formation. On top of this, the information about
existing sense alignments across resources can be
accessed via SenseAxis relations, so that the re-
turned combined result covers not only the lexi-
cal, but also the sense level.
Community issues. One of the most important
reasons for UBY is creating an easy-to-use pow-
erful LSR to advance NLP research and develop-
ment. Therefore, community building around the
resource is one of our major concerns. To this end,
we will offer free downloads of the lexical data
and software presented in this paper under open li-
censes, namely: The UBY-LMF DTD, mappings
and conversion tools for existing resources and
sense alignments, the Java API, and, as far as li-
censing allows,15 already converted resources. If
resources cannot be made available for download,
the conversion tools will still allow users with ac-
cess to these resources to import them into UBY
easily. In this way, it will be possible for users to
build their ?custom UBY? containing selected re-
sources. As the underlying resources are subject
to continuous change, updates of the correspond-
ing components will be made available on a regu-
lar basis.
7 Conclusions
We presented UBY, a large-scale, standardized
LSR containing nine widely used resources in two
languages: English WN, WKT-en, WP-en, FN
and VN, German WP-de, WKT-de, and GN, and
OW in English and German. As all resources
are modeled in UBY-LMF, UBY enables struc-
tural interoperability across resources and lan-
guages down to a fine-grained level of informa-
tion. For FN, VN and all of the CCRs in En-
glish and German, this is done for the first time.
Besides, by integrating sense alignments we also
enable the lexical-semantic interoperability of re-
sources. We presented a unified framework for
aligning any LSRs pairwise and reported on ex-
periments which align OW-de and WN. We will
release the UBY-LMF model, the resource and the
UBY-API at the time of publication.16 Due to the
added value and the large scale of UBY, as well as
its ease of use, we believe UBY will boost the per-
formance of NLP making use of lexical-semantic
knowledge.
Acknowledgments
This work has been supported by the Emmy
Noether Program of the German Research Foun-
dation (DFG) under grant No. GU 798/3-1 and
by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant
No. I/82806. We thank Richard Eckart de
Castilho, Yevgen Chebotar, Zijad Maksuti and Tri
Duc Nghiem for their contributions to this project.
References
Jordi Atserias, Lu??s Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
15Only GermaNet is subject to a restricted license and can-
not be redistributed in UBY format.
16http://www.ukp.tu-darmstadt.de/data/uby
588
Vossen. 2004. The Meaning Multilingual Central
Repository. In Proceedings of the second interna-
tional WordNet Conference (GWC 2004), pages 23?
30, Brno, Czech Republic.
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as complementary resources for
annotation. In Proceedings of the Third Linguis-
tic Annotation Workshop, ACL-IJCNLP ?09, pages
125?129, Suntec, Singapore.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th
International Conference on Computational Lin-
guistics (COLING-ACL?98, pages 86?90, Montreal,
Canada.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia A Crys-
tallization Point for the Web of Data. Journal of
Web Semantics: Science, Services and Agents on the
World Wide Web, (7):154?165.
Daan Broeder, Marc Kemps-Snijders, Dieter Van Uyt-
vanck, Menzo Windhouwer, Peter Withers, Peter
Wittenburg, and Claus Zinn. 2010. A Data Cat-
egory Registry- and Component-based Metadata
Framework. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), pages 43?47, Valletta, Malta.
Paul Buitelaar, Philipp Cimiano, Peter Haase, and
Michael Sintek. 2009. Towards Linguistically
Grounded Ontologies. In Lora Aroyo, Paolo
Traverso, Fabio Ciravegna, Philipp Cimiano, Tom
Heath, Eero Hyvo?nen, Riichiro Mizoguchi, Eyal
Oren, Marta Sabou, and Elena Simperl, editors, The
Semantic Web: Research and Applications, pages
111?125, Berlin/Heidelberg, Germany. Springer.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined ev-
idence. In Proceedings of the 18th ACM conference
on Information and knowledge management (CIKM
?09), CIKM ?09, pages 513?522, New York, NY,
USA. ACM.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Charles J. Fillmore. 1982. Frame Semantics. In The
Linguistic Society of Korea, editor, Linguistics in
the Morning Calm, pages 111?137. Hanshin Pub-
lishing Company, Seoul, Korea.
Gil Francopoulo, Nuria Bel, Monte George, Nico-
letta Calzolari, Monica Monachini, Mandy Pet, and
Claudia Soria. 2006. Lexical Markup Framework
(LMF). In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC), pages 233?236, Genoa, Italy.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Verena Henrich and Erhard Hinrichs. 2010. Standard-
izing wordnets in the ISO standard LMF: Wordnet-
LMF for GermaNet. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (COLING), pages 456?464, Beijing, China.
Richard Johansson and Pierre Nugues. 2007. Us-
ing WordNet to extend FrameNet coverage. In
Proceedings of the Workshop on Building Frame-
semantic Resources for Scandinavian and Baltic
Languages, at NODALIDA, pages 27?30, Tartu, Es-
tonia.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. Language Resources and Evalu-
ation, 42:21?40.
Claudia Kunze and Lothar Lemnitzer. 2002. Ger-
maNet ? representation, visualization, application.
In Proceedings of the Third International Con-
ference on Language Resources and Evaluation
(LREC), pages 1485?1491, Las Palmas, Canary Is-
lands, Spain.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press, Chicago,
IL, USA.
Michael Matuschek and Iryna Gurevych. 2011.
Where the journey is headed: Collaboratively con-
structed multilingual Wiki-based resources. In
SFB 538: Mehrsprachigkeit, editor, Hamburger Ar-
beiten zur Mehrsprachigkeit, Hamburg, Germany.
John McCrae, Dennis Spohr, and Philipp Cimiano.
2011. Linking Lexical Resources and Ontologies
on the Semantic Web with Lemon. In The Seman-
tic Web: Research and Applications, volume 6643
of Lecture Notes in Computer Science, pages 245?
259. Springer, Berlin/Heidelberg, Germany.
Clifton J. McFate and Kenneth D. Forbus. 2011.
NULEX: an open-license broad coverage lexicon.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 363?367, Portland, OR, USA.
Christian M. Meyer and Iryna Gurevych. 2010. Worth
its Weight in Gold or Yet Another Resource ?
A Comparative Study of Wiktionary, OpenThe-
saurus and GermaNet. In Alexander Gelbukh, ed-
itor, Computational Linguistics and Intelligent Text
Processing: 11th International Conference, volume
6008 of Lecture Notes in Computer Science, pages
38?49. Berlin/Heidelberg: Springer, Ias?i, Romania.
Christian M. Meyer and Iryna Gurevych. 2011. What
Psycholinguists Know About Chemistry: Align-
ing Wiktionary and WordNet for Increased Domain
589
Coverage. In Proceedings of the 5th International
Joint Conference on Natural Language Processing
(IJCNLP), pages 883?892, Chiang Mai, Thailand.
Roberto Navigli and Simone Paolo Ponzetto. 2010a.
BabelNet: Building a Very Large Multilingual Se-
mantic Network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 216?225, Uppsala, Sweden, July.
Roberto Navigli and Simone Paolo Ponzetto. 2010b.
Knowledge-rich Word Sense Disambiguation Ri-
valing Supervised Systems. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1522?1531, Uppsala,
Sweden.
Roberto Navigli. 2006. Meaningful Clustering of
Senses Helps Boost Word Sense Disambiguation
Performance. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages
105?112, Sydney, Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
People?s Web meets Linguistic Knowledge: Auto-
matic Sense Alignment of Wikipedia and WordNet.
In Proceedings of the 9th International Conference
on Computational Semantics (IWCS), pages 205?
214, Oxford, UK.
Muntsa Padro?, Nu?ria Bel, and Silvia Necsulescu.
2011. Towards the Automatic Merging of Lexical
Resources: Automatic Mapping. In Proceedings of
the International Conference on Recent Advances
in Natural Language Processing (RANLP), pages
296?301, Hissar, Bulgaria.
Martha Palmer. 2009. Semlink: Linking PropBank,
VerbNet and FrameNet. In Proceedings of the Gen-
erative Lexicon Conference (GenLex-09), pages 9?
15, Pisa, Italy.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. In Proceedings of
the International Conference on Semantic Comput-
ing, pages 517?526, Washington, DC, USA.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings
of the Sixth International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing), pages 100?111, Mexico City, Mexico.
Claudia Soria, Monica Monachini, and Piek Vossen.
2009. Wordnet-LMF: fleshing out a standardized
format for Wordnet interoperability. In Proceed-
ings of the 2009 International Workshop on Inter-
cultural Collaboration, pages 139?146, Palo Alto,
CA, USA.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, pages 697?706, Banff,
Canada.
Antonio Toral, Stefania Bracale, Monica Monachini,
and Claudia Soria. 2010. Rejuvenating the Italian
WordNet: Upgrading, Standarising, Extending. In
Proceedings of the 5th Global WordNet Conference
(GWC), Bombay, India.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht, Nether-
lands.
590
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1363?1373,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using
Wiktionary as Interlingual Connection
Silvana Hartmann? and Iryna Gurevych??
? Ubiquitous Knowledge Processing Lap (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
? Ubiquitous Knowledge Processing Lap (UKP-DIPF)
German Institute for Educational Research and Educational Information
www.ukp.tu-darmstadt.de
Abstract
We present a new bilingual FrameNet lex-
icon for English and German. It is cre-
ated through a simple, but powerful ap-
proach to construct a FrameNet in any
language using Wiktionary as an inter-
lingual representation. Our approach is
based on a sense alignment of FrameNet
and Wiktionary, and subsequent transla-
tion disambiguation into the target lan-
guage. We perform a detailed evaluation
of the created resource and a discussion of
Wiktionary as an interlingual connection
for the cross-language transfer of lexical-
semantic resources. The created resource
is publicly available at http://www.
ukp.tu-darmstadt.de/fnwkde/.
1 Introduction
FrameNet is a valuable resource for natural lan-
guage processing (NLP): semantic role labeling
(SRL) systems based on FrameNet provide se-
mantic analysis for NLP applications, such as
question answering (Narayanan and Harabagiu,
2004; Shi and Mihalcea, 2005) and information
extraction (Mohit and Narayanan, 2003). How-
ever, their wide deployment has been prohibited
by the poor coverage and limited availability of a
similar resource in many languages.
Expert-built lexical-semantic resources are ex-
pensive to create. Previous cross-lingual trans-
fer of FrameNet used corpus-based approaches, or
resource alignment with multilingual expert-built
resources, such as EuroWordNet. The latter in-
directly also suffers from the high cost and con-
strained coverage of expert-built resources.
Recently, collaboratively created resources have
been investigated for the multilingual extension of
resources in NLP, beginning with Wikipedia (Nav-
igli and Ponzetto, 2010). They rely on the so-
called ?Wisdom of the Crowds?, contributions by
a large number of volunteers, which results in a
continuously updated high-quality resource avail-
able in hundreds of languages. Due to the ency-
clopedic nature of Wikipedia, previous work fo-
cused on encyclopedic information for Wikipedia
entries, i.e., almost exclusively on nouns.
This is not enough for resources like FrameNet.
Such resources need lexical-semantic information
on various POS. For FrameNet, information on
the predicates associated with a semantic frame ?
mostly verbs, nouns, and adjectives ? is crucial,
for instance gloss or syntactic subcategorization.
A solution for the problem of multilingual ex-
tension of lexical semantic resources is to use
Wiktionary, a collaboratively created dictionary,
as connection between languages. It provides
high-quality lexical information on all POS, for in-
stance glosses, sense relations, syntactic subcate-
gorization, etc. Like Wikipedia, it is continuously
extended and contains translations to hundreds of
languages, including low-resource ones. To our
knowledge, Wiktionary has not been evaluated as
an interlingual index for the cross-lingual exten-
sion of lexical-semantic resources.
In this paper, we present a novel method for
the creation of bilingual FrameNet lexicons based
on an alignment to Wiktionary. We demonstrate
our method on the language pair English-German
and present the resulting resources, a lemma-based
multilingual and a sense-disambiguated German-
English FrameNet lexicon.
The understanding of lexical-semantic re-
sources and their combinations, e.g., how align-
ment algorithms can be adapted to individual re-
source pairs and different POS, is essential for
their effective use in NLP and a prerequisite for
later in-task evaluation and application. To en-
hance this understanding for the presented re-
source pair, we perform a detailed analysis of
the created resource and compare it to existing
FrameNet resources for German.
1363
The contributions of our work are the following:
(1) We create a novel sense alignment between
FrameNet and the English Wiktionary. It results
in a multilingual FrameNet FNWKxx, which links
FrameNet senses to lemmas in 280 languages. (2)
We create a sense-disambiguated English-German
FrameNet lexicon FNWKde based on FNWKxx
and translation disambiguation on the German
Wiktionary.1 (3) We analyze the two resources
and outline further steps for creating a multilin-
gual FrameNet.
This is a major step towards the vision of this
paper: a simple, but powerful approach to partially
construct a FrameNet in any language using Wik-
tionary as an interlingual representation.
2 Resource Overview
FrameNet (Baker et al, 1998) is an expert-built
lexical-semantic resource incorporating the theory
of frame-semantics (Fillmore, 1976). It groups
word senses in frames that represent particular sit-
uations. Thus, the verb complete and the noun
completion belong to the Activity finish frame. The
participants of these situations, typically realized
as syntactic arguments, are the semantic roles of
the frame, for instance the Agent performing an ac-
tivity, or the Activity itself. FrameNet release 1.5
contains 1,015 frames, and 11,942 word senses.
Corpus texts annotated with frames and their roles
have been used to train automatic SRL systems.
Wiktionary is a collaboratively created dictio-
nary available in over 500 language editions. It is
continuously extended and revised by a commu-
nity of volunteer editors. The English language
edition contains over 500,000 word senses.2
Wiktionary is organized like a traditional dic-
tionary in lexical entries and word senses. For the
word senses, definitions and example sentences, as
well as other lexical information, such as register
(e.g., colloquial), phonetic transcription, inflec-
tion may be available, including language-specific
types of information. Senses also provide trans-
lations to other languages. These are connected
to lexical entries in the respective language edi-
tions via hyperlinks. This allows us to use Wik-
tionary as an interlingual connection between mul-
tiple languages.
1The xx in FNWKxx stands for all the languages in the
resource. After translation disambiguation in a specific lan-
guage, xx is replaced by the corresponding language code.
2as of May 2013, see http://en.wiktionary.
org/wiki/Wiktionary:Statistics.
Figure 1: Method overview.
The quality of Wiktionary has been confirmed
by Meyer and Gurevych (2012b) who also give an
overview on the usage of Wiktionary in NLP ap-
plications such as speech synthesis.
3 Method Overview
Our method consists of two steps visualized in
Fig. 1. In the first step, we create a novel sense
alignment between FrameNet and the English
Wiktionary following Niemann and Gurevych
(2011). Thus, the FrameNet sense of to complete
with frame Activity finish is assigned to the sense
of to complete in Wiktionary meaning to finish.
This step establishes Wiktionary as an interlin-
gual index between FrameNet senses and lemmas
in many languages, and builds the foundation for
the bilingual FrameNet extension.
It results in a basic multilingual FrameNet lexi-
con FNWKxx with translations to lemmas in 283
languages. An example: by aligning the FrameNet
sense of the verb complete with gloss to finish
with the corresponding English Wiktionary sense,
we collect 39 translations to 22 languages, e.g., the
German fertigmachen and the Spanish terminar.
The second step is the disambiguation of the
translated lemmas with respect to the target lan-
guage Wiktionary in order to retrieve the lin-
guistic information of the corresponding word
sense in the target language Wiktionary (Meyer
and Gurevych, 2012a). We evaluate this step
for English and German and create the bilingual
FrameNet lexicon FNWKde. For the example
sense of complete, we extract lexical information
for the word sense of its German translation fer-
tigmachen, for instance a German gloss, an ex-
ample sentence, register information (colloquial),
and synonyms, e.g., beenden. As a side-benefit of
our method, we also extend the English FrameNet
by the linguistic information in Wiktionary.
1364
4 Related Work
4.1 Creating FrameNets in New Languages
There are two main lines of research in bootstrap-
ping a FrameNet for languages other than English.
The first, corpus-based approach is to automat-
ically extract word senses in the target language
based on parallel corpora and frame annotations
in the source language. In this vein, Pado? and
Lapata (2005) propose a cross-lingual FrameNet
extension to German and French; Johansson and
Nugues (2005) and Johansson and Nugues (2006)
do this for Spanish and Swedish, and Basili et al
(2009) for Italian.
Pado? and Lapata (2005) observe that their ap-
proach suffers from polysemy errors, because lem-
mas in the source language need to be disam-
biguated with respect to all the frames they evoke.
To alleviate this problem, they use a disambigua-
tion approach based on the most frequent frame;
Basili et al (2009) use distributional methods for
frame disambiguation. Our approach is based on
sense alignments and therefore explicitly aims to
avoid such errors.
The second line of work is resource-based:
FrameNet is aligned to multilingual resources in
order to extract senses in the target language. Us-
ing monolingual resources, this approach has also
been employed to extend FrameNet coverage for
English (Shi and Mihalcea, 2005; Johansson and
Nugues, 2007; Ferrandez et al, 2010).
De Cao et al (2008) map FrameNet frames
to WordNet synsets based on the embedding of
FrameNet lemmas in WordNet. They use Multi-
WordNet, an English-Italian wordnet, to induce an
Italian FrameNet lexicon with 15,000 entries.
To create MapNet, Tonelli and Pianta (2009)
align FrameNet senses with WordNet synsets by
exploiting the textual similarity of their glosses.
The similarity measure is based on stem overlap of
the candidates? glosses expanded by WordNet do-
mains, the WordNet synset, and the set of senses
for a FrameNet frame. In Tonelli and Pighin
(2009), they use these features to train an SVM-
classifier to identify valid alignments and report
an F1-score of 0.66 on a manually annotated gold
standard. They report 4,265 new English senses
and 6,429 new Italian senses, which were derived
via MultiWordNet.
ExtendedWordFramenet (Laparra and Rigau,
2009; Laparra and Rigau, 2010) is also based
on the alignment of FrameNet senses to Word-
Net synsets. The goal is the multilingual cover-
age extension of FrameNet, which is achieved by
linking WordNet to wordnets in other languages
(Spanish, Italian, Basque, and Catalan) in the Mul-
tilingual Central Repository. For each language,
they add more then 10,000 senses to FrameNet.
They rely on a knowledge-based word sense dis-
ambiguation algorithm to establish the alignment
and report F1=0.75 on a gold standard based on
Tonelli and Pighin (2009).
Tonelli and Giuliano (2009) align FrameNet
senses to Wikipedia entries with the goal to ex-
tract word senses and example sentences in Ital-
ian. Based on Wikipedia, this alignment is re-
stricted to nouns. Subsequent work on Wikipedia
and FrameNet follows a different path and tries to
enhance the modeling of selectional preferences
for FrameNet predicates (Tonelli et al, 2012).
Finally, there have been suggestions to com-
bine the corpus-based and the resource-based ap-
proaches: Borin et al (2012) do this for Finnish
and Swedish. They prove the feasibility of
their approach by creating a preliminary Finnish
FrameNet with 2,694 senses.
Mouton et al (2010) directly exploit the trans-
lations in the English and French Wiktionary edi-
tions to extend the French FrameNet. They match
the FrameNet senses to Wiktionary lexical en-
tries, thus encountering the problem of polysemy
in the target language. To solve this, they de-
fine a set of filters that control how target lemmas
are distributed over frames, increasing precision at
the expense of recall (P=0.74, R=0.3, F1=0.42).
While their approach is in theory applicable to
other languages, our approach goes beyond this
by laying the ground for simultaneous FrameNet
extension in multiple languages via FNWKxx.
4.2 Wiktionary Sense Alignments
Collaboratively created resources have become
popular for sense alignments for NLP, start-
ing with the alignment between WordNet and
Wikipedia (Ruiz-Casado et al, 2005; Ponzetto
and Navigli, 2009). Wiktionary has been subject
to few alignment efforts: de Melo and Weikum
(2009) integrate information from Wiktionary into
Universal WordNet. Meyer and Gurevych (2011)
map WordNet synsets to Wiktionary senses and
show their complementary domain coverage.
1365
5 FrameNet ? Wiktionary Alignment
5.1 Alignment Technique
We follow the state-of-the-art sense alignment
technique introduced by Niemann and Gurevych
(2011). They align senses in WordNet to
Wikipedia entries in a supervised setting using se-
mantic similarity measures.
One reason to use their method was that it al-
lows zero alignments or one-to-many alignments.
This is crucial for obtaining a high-quality align-
ment of heterogeneous resources, such as the pre-
sented one, because their sense granularity and
coverage can diverge a lot.
The alignment algorithm consists of two steps.
In the candidate extraction step, we iterate over all
FrameNet senses and match them with all senses
from Wiktionary which have the same lemma and
thus are likely to describe the same sense.
This step yields a set of candidate sense pairs
Call. In the classification step, a similarity score
between the textual information associated with
the senses in a candidate pair (e.g., their gloss) is
computed and a threshold-based classifier decides
for each pair on valid alignments.
Niemann and Gurevych (2011) combine two
different types of similarity (i) cosine similarity
on bag-of-words vectors (COS) and (ii) a person-
alized PageRank-based similarity measure (PPR).
The PPR measure (Agirre and Soroa, 2009) maps
the glosses of the two senses to a semantic vec-
tor space spanned up by WordNet synsets and then
compares them using the chi-square measure.
The semantic vectors ppr are computed us-
ing the personalized PageRank algorithm on the
WordNet graph. They determine the important
nodes in the graph as the nodes that a random
walker following the edges visits most frequently:
ppr = cMppr + (1? c)vppr, (1)
where M is a transition probability matrix be-
tween the n WordNet synsets, c is a damping fac-
tor, and vppr is a vector of size n representing
the probability of jumping to the node i associated
with each vi. For personalized PageRank, vppr is
initialized in a particular way: the initial weight is
distributed equally over the m vector components
(i.e., synsets) associated with a word in the sense
gloss, other components receive a 0 value.
For each similarity measure, Niemann and
Gurevych (2011) determine a threshold (tppr and
tcos) independently on a manually annotated gold
standard. The final alignment decision is the con-
junction of two decision functions:
a(ss, st) =
PPR(ss, st) > tppr& COS(ss, st) > tcos.
(2)
We differ from Niemann and Gurevych (2011) in
that we use a joint training setup which determines
tppr and tcos to optimize classification performance
directly (as proposed in Gurevych et al (2012)):
(tppr, tcos) = argmax(tppr,tcos)F1(a), (3)
where F1 is the maximized evaluation score and a
is the decision function in equation (2).
5.2 Candidate Extraction
To compile the candidate set, we paired senses
from both resources with identical lemma-POS
combinations. FrameNet senses are defined by a
lemma, a gloss, and a frame. Wiktionary senses
are defined by a lemma and a gloss. For the
FrameNet sense Activity finish of the verb com-
plete, we find two candidate senses in Wiktionary
(to finish and to make whole). There are on av-
erage 3.7 candidates per FrameNet sense. The full
candidate setCall contains over 44,000 sense pairs
and covers 97% of the 11,942 FrameNet senses.
5.3 Gold Standard Creation
For the gold standard, we sampled 2,900 candidate
pairs from Call. The properties of the gold stan-
dard mirror the properties of Call: the sampling
preserved the distribution of POS in Call (around
40% verbs and nouns, and 12% adjectives) and
the average numbers of candidates per FrameNet
sense. This ensures that highly polysemous words
as well as words with few senses are selected.
Two human raters annotated the sense pairs
based on their glosses. The annotation task con-
sisted in a two-class annotation: Do the presented
senses have same meaning - (YES/NO). The raters
received detailed guidelines and were trained on
around 100 sense pairs drawn from the sample.
We computed Cohen?s ? to measure the inter-
rater agreement between the two raters. It is
?=0.72 on the full set, which is considered accept-
able according to Artstein and Poesio (2008). An
additional expert annotator disambiguated ties.
For comparison: Meyer and Gurevych (2011)
report ?=0.74 for their WordNet ? Wiktionary
gold standard, and Niemann and Gurevych (2011)
1366
adj noun verb all
? .8 .77 .65 .72
Table 1: Inter-rater agreement.
?=0.87 for their WordNet ? Wikipedia gold stan-
dard. These gold standards only consist of nouns,
which appear to be an easier annotation task than
verb senses. This is supported by our analysis of
the agreement by POS (see Table 1): the agree-
ment on nouns and adjectives lies between the two
agreement scores previously reported on nouns.
Thus our annotation is of similar quality. Only
the agreement on verbs is slightly below the ac-
ceptability threshold of 0.67 (Artstein and Poesio,
2008). The verb senses are very fine-grained and
thus present a difficult alignment task. Therefore,
we had an expert annotator correct the verbal part
of the gold standard set. After removing the train-
ing set for the raters, the final gold standard con-
tains 2,789 sense pairs. 28% of these are aligned.
5.4 Alignment Experiments
We determined the best setting for the alignment
of FrameNet and Wiktionary in a ten-fold cross-
validation on the gold standard.
Besides the parameters for the computation of
the PPR vectors (we used the publicly available
UKB tool by Agirre and Soroa (2009)), the main
parameter in the experiments is the textual in-
formation that is used to represent the senses.
For FrameNet senses, we used the lemma-pos,
sense gloss, example sentences, frame name and
frame definition as textual features; for Wiktionary
senses, we considered lemma-pos, sense gloss, ex-
ample sentences, hyponyms and synonyms.
We computed the similarity scores on tok-
enized, lemmatized and stop-word-filtered texts.
First, we evaluated models for COS and PPR
independently based on various combinations of
the textual features listed above. We then used
the parameter setting of the best-performing sin-
gle models to train the model that jointly optimizes
the thresholds for PPR and COS (see eqn. (5)). In
Table 2, we report on the results of the best single
models and the best joint model.
For the evaluation, we compute precision P, re-
call R and F1 on the positive class (aligned=true),
e.g., precision P is the number of pairs correctly
aligned divided by all aligned pairs.
We achieved the highest precision and F1-score
Evaluation verb noun adj all
P Random-1 BL 0.503 0.559 0.661 0.557
WKT-1 BL 0.620 0.664 0.725 0.66
BEST COS 0.639 0.778 0.706 0.703
BEST PPR 0.66 0.754 0.729 0.713
BEST JOINT 0.677 0.766 0.742 0.728
R Random-1 BL 0.471 0.546 0.683 0.540
WKT-1 BL 0.581 0.65 0.75 0.64
BEST COS 0.658 0.758 0.754 0.715
BEST PPR 0.666 0.724 0.754 0.699
BEST JOINT 0.683 0.783 0.83 0.75
F1 Random-1 BL 0.487 0.552 0.672 0.549
WKT-1 BL 0.60 0.657 0.737 0.65
BEST COS 0.648 0.768 0.729 0.709
BEST PPR 0.663 0.739 0.741 0.706
BEST JOINT 0.68 0.775 0.784 0.739
UBound 0.735 0.834 0.864 0.797
Table 2: Alignment performance by POS.
for COS using all available features, but excluding
FrameNet example sentences because they intro-
duce too much noise. Adding the frame name and
frame definition to the often short glosses provides
a richer sense representation for the COS measure.
The best-performing PPR configuration uses
sense gloss and lemma-pos. For the joint model,
we employed the best single PPR configuration,
and a COS configuration that uses sense gloss ex-
tended by Wiktionary hypernyms, synonyms and
FrameNet frame name and frame definition, to
achieve the highest score, an F1-score of 0.739.
5.5 Gold Standard Evaluation
We compared the performance of our alignment
on the gold standard to a baseline which randomly
selects one target sense from the candidate set of
each source sense (Random-1). We also consider
the more competitive Wiktionary first sense base-
line (WKT-1). It is guided by the heuristic that
more frequent senses are listed first in Wiktionary
(Meyer and Gurevych, 2010). It is a stronger base-
line with an F1-score of 0.65 (see Table 2).
To derive the upper bound for the alignment per-
formance (UBound), we computed the F1 score
from the average pairwise F1-score of the annota-
tors according to Hripcsak and Rothschild (2005).
As the evaluation set mirrors the POS distri-
bution in FrameNet and is sufficiently large, un-
like earlier alignments, we can analyze the per-
formance by POS. The BEST JOINT model per-
forms well on nouns, slightly better on adjectives,
and worse on verbs, see Table 2. For the baselines
and the UBound the same applies, with the dif-
ference that adjectives receive even better results
1367
in comparison. This fits in with the perceived de-
gree of difficulty according to the observed poly-
semy for the POS: for verbs we have many candi-
date sets with two or more candidates, i.e., we ob-
serve higher polysemy, while for nouns and even
stronger for adjectives, many small candidate sets
occur, which stand for an easier alignment de-
cision. This is in line with the reported higher
complexity of lexical resources with respect to
verbs and greater difficulty in alignments and word
sense disambiguation (Laparra and Rigau, 2010).
The performance of BEST JOINT on all POS
is F1=0.73, which is significantly higher than the
WKT-1 baseline (p<0.05 according to McNe-
mar?s test). The performance on nouns (F1=0.775)
is on par with the results reported by Niemann and
Gurevych (2011) for nouns (F1=0.78).
5.6 Error Analysis
The confusion matrix from the evaluation of BEST
JOINT on the gold standard shows 214 false pos-
itives and 191 false negatives. The false nega-
tives suffer from low overlap between the glosses,
which are often quite short (contend - assert),
sometimes circular (sinful - relating to sin). Align-
ing senses with such glosses is difficult for a sys-
tem based on semantic similarity. In about 50% of
the analyzed pairs, highly similar words are used
in the gloss, that we should be able to detect with
second-order representations, for instance by ex-
panding short definitions with the definitions of
the contained words, or via derivational similarity.
A number of false positives occur because the
gold standard was developed in a very fine-grained
manner: distinctions such as causative vs. inchoa-
tive (enlarge: become large vs. enlarge: make
large) were explicitly stressed in the definitions
and thus annotated as different senses by the anno-
tators. This was motivated by the fact that this dis-
tinction is relevant for many frames in FrameNet.
The first meaning of enlarge belongs to the frame
Expansion, the second to Cause expansion. Our
similarity based approach cannot capture such dif-
ferences well.
6 Intermediate Resource FNWKxx
6.1 Statistics
We applied the best system setup to the full can-
didate set of over 44,000 candidates to create the
intermediate resource FNWKxx. The alignment
consists of 12,094 sense pairs. It covers 82% of
fine-grained P coarse-grained P
All POS 0.67 0.78
By POS verb noun adj verb noun adj0.53 0.73 0.80 0.73 0.82 0.85
Table 3: Post-hoc evaluation (precision P).
the senses in FrameNet and 86% of the frames. It
connects more than 9,800 unique FrameNet senses
with more than 10,000 unique Wiktionary senses,
which shows that both non-alignments and multi-
ple alignments occur for some source senses.
6.2 Post-hoc Evaluation
Our cross-validation approach entails the danger
of over-fitting. In order to verify the quality of
the alignment, we performed a detailed post-hoc
analysis on a sample of 270 aligned sense pairs
randomly drawn from the set of aligned senses.
Because sense granularity was an issue in the
error analysis, we considered two alignment deci-
sions: (a) fine-grained alignment: the two glosses
describe the same sense; (b) coarse-grained align-
ment. The causative/inchoative distinction is,
among others, ignored.
The evaluation results are listed in Table 3. The
precision for the fine-grained (a) is lower than the
allover precision on the gold standard. The evalua-
tion by POS shows that the result for nouns and ad-
jectives is equal or superior to the evaluation result
on the gold standard, while it is worse for verbs.
This shows that over-fitting, if at all, is only a risk
for the verb senses.
The allover precision for (b) exceeds the pre-
cision on the gold standard. Particularly verbs
receive much better results. This shows that
a coarse-grained alignment may suffice for the
FrameNet extension.
This evaluation confirms the quality of the
sense alignment, in particular with respect to the
FrameNet extension. But it also elicits the ques-
tion whether a coarse-grained alignment would
suffice. We will discuss this question below.
6.3 Resource Analysis
For each of the aligned senses in the 12,094
aligned sense pairs, we extracted glosses from
Wiktionary. Because FrameNet glosses are often
very brief, the additional glosses will benefit algo-
rithms such as frame detection for SRL. We also
added 4,352 new example sentences from Wik-
1368
tionary to FrameNet.
We can extract 2,151 new lemma-POS for
FrameNet frames from the synonyms of the
aligned senses in Wiktionary. We also ex-
tract other related lemma-POS, for instance 487
antonyms, 126 hyponyms, and 19 hypernyms.
This step establishes Wiktionary as an interlin-
gual connection between FrameNet and a large
number of languages, including low-resource
ones: via Wiktionary, we connect FrameNet
senses to translations in 283 languages, e.g., we
translate the sense of the verb complete associ-
ated with the frame Activity Finish to the German
colloquial fertigmachen, the Spanish terminar, the
Turkish tamamlamak, and 19 other languages.
For 36 languages, we can extract more than
1,000 translations each, among them low-resource
languages such as Telugu, Swahili, or Kurdish.
The languages with most translations are: Finnish
(9,333), Russian (7,790), and German (6,871).
The number of Finnish translations is more than
three times larger than the preliminary Finnish
FrameNet by Borin et al (2012). Likewise, we
get three times the number of German lemma-POS
than provided by the SALSA corpus.
7 Translation Disambiguation
7.1 Disambiguation Method
FNWKxx initially does not provide lexical-
semantic information for the German translations:
the translations link to a lemma in the German
Wiktionary, not a target sense. In order to inte-
grate the information attached to a German Wik-
tionary sense, e.g., the gloss, into our resource, the
lemmas need to be disambiguated.
We use the sense-disambiguated Wiktionary re-
sulting from a recently published approach for
the disambiguation of relations and translations in
Wiktionary (Meyer and Gurevych, 2012a) to cre-
ate our new bilingual (German-English) FrameNet
lexicon FNWKde.
Their approach combines information on the
source sense and all potential target senses in order
to determine the best target sense in a rule-based
disambiguation strategy. The information is en-
coded as binary features, which are ordered in a
back-off hierarchy: if the first feature applies, the
target sense is selected, otherwise the second fea-
ture is considered, and so forth.
The most important features are: definition
overlap between source and automatically trans-
SALSA2 P&L05 FNWKde
Type Corpus Corpus Lexicon
Creation Manual Automatic Automatic
Frames(+p) 266(907) 468 755
Senses 1,813 9,851 5,897
Examples 24,184 1,672,551 6,933
Glosses - - 5,897
Table 4: Frame-semantic resources for German.
lated target definition; occurrence of the source
lemma in the target definition; shared linguistic
information (e.g., same register); inverse transla-
tion relations (i.e., the source lemma occurs on the
translation list of the target sense); relation over-
lap; Lesk measure between original and translated
glosses in source and target language; and finally,
backing off to the first target sense.
For the gold standard evaluation of the approach
we refer to Meyer and Gurevych (2012a): their
system obtained an F1-score of 0.67 for the task of
disambiguating translations from English to Ger-
man, and an F1-score of 0.79 for the disambigua-
tion of English sense relations. We use the latter to
identify target senses of synonyms in FNWKxx.
8 Resource FNWKde
8.1 Statistics
Table 4 gives an overview of FNWKde. It con-
tains 5,897 pairs of German Wiktionary senses
and FrameNet senses, i.e., 86% of the translations
could be disambiguated. Each sense has a gloss,
and there are 6,933 example sentences.
Based on the relation disambiguation and in-
ference of new relations by Meyer and Gurevych
(2012a), we can also disambiguate synonyms in
the English Wiktionary. This leads to a further ex-
tension of the English FrameNet summarized in
Table 5. The number of Wiktionary senses aligned
to FrameNet senses is increased by 50%.
We also provide results for other sense relations,
e.g., antonyms. We will discuss whether and how
they can be integrated as FrameNet senses in our
resource below.
8.2 Post-hoc Evaluation
Because the errors of two subsequently applied au-
tomatic methods can multiply, we provide a post-
hoc evaluation of the results.
To evaluate the quality of the German FrameNet
lexicon, we collected the FrameNet senses for a
list of 15 frames that were sampled by Pado? and
1369
# English senses # English senses
Relation per FrameNet sense per frame
SYNONYM 17,713 13,288
HYPONYM 4,818 3,347
HYPERNYM 6,369 3,961
ANTONYM 9,626 6,737
Table 5: Statistics after relation disambiguation.
Lapata (2005) according to three frequency bands
on a large corpus. There are 115 senses associated
with these frames in our resource. In a manual
evaluation of these 115 senses, we find that 67%
were assigned correctly to their frames. This is
higher than expected, considering the errors from
the applied methods add up.
Further analysis revealed that both resource cre-
ation steps contribute equally to the 39 errors. For
17 of the evaluated sense pairs, redundancy con-
firms their quality: they were obtained indepen-
dently by two or three alignment-and-translation
paths and do not contain alignment errors.
8.3 Comparison
We compare FNWKde to two German frame-
semantic resources, the manually annotated
SALSA corpus (Burchardt et al, 2006) and a
resource from Pado? and Lapata (2005), hence-
forth P&L05. Note that both resources are frame-
annotated corpora, while FNWKde is a FrameNet-
like lexicon and contains information complemen-
tary to the corpora. The different properties of the
resources are contrasted in Table 4.
The automatically developed resources, includ-
ing FNWKde, provide a larger number of senses
than SALSA. The annotated corpora contain a
large number of examples, but they do not pro-
vide any glosses, which are useful for frame detec-
tion in SRL, nor do they contain any other lexical-
semantic information.
FNWKde covers a larger number of FrameNet
frames than the other two resources. 266 of the
907 frames in SALSA are connected to original
FrameNet frames, the others are newly-developed
proto-frames p (shown in parentheses in Table 4).
Table 6 describes the proportion of the over-
lapping frames and senses3 to the respective re-
sources. The numbers on frame overlap show
that our resource covers the frames in the other
3Note that the senses in SALSA and P&L05 are defined
by frame, lemma, and POS. In Table 6, FNWKde senses
with identical frame, lemma, and POS, but different gloss are
therefore conflated to one sense.
Resource r % of r % of FNWKde
Frame SALSA 2 89% 31%
P&L05 90% 55%
Sense SALSA 2 15% 5%
P&L05 10% 19%
Table 6: Overlap of FNWKde with resource r.
resources well (89% and 90% coverage respec-
tively), and that it adds frames not covered in the
other resources: P&L05 only covers 55% of the
frames in FNWKde. The sense overlap shows
that the resources have senses in common, which
confirms the quality of the automatically devel-
oped resources, but they also complement each
other. FNWKde, for instance, adds 3,041 senses
to P&L05.
9 Discussion: a Multilingual FrameNet
FNWKxx builds an excellent starting point to cre-
ate FrameNet lexicons in various languages: the
translation counts, for instance 6,871 for German,
compare favorably to FrameNet 1.5, which con-
tains 9,700 English lemma-POS.
To create those FrameNet lexicons, the transla-
tion disambiguation approach used for FNWKde
(step 2 in Fig. 1) needs to be adapted to other lan-
guages. The approach is in theory applicable to
any language, but there are some obstacles: first,
it relies on the availability of the target sense in
the target language Wiktionary. For many of the
top 30 languages in FNWKxx, the Wiktionary edi-
tions seem sufficiently large to provide targets for
translation disambiguation,4 and they are contin-
uously extended. Second, our approach requires
access to the target language Wiktionary, but the
data format across Wiktionary language editions
is not standardized. Third, the approach requires
machine translation into the target language. For
languages, where such a tool is not available, we
could default to the first-sense-heuristic, or en-
courage the Wiktionary community to link the
translations to their target Wiktionary senses in-
spired by Sajous et al (2010).
Another issue that applies to all automatic
(and also manual) approaches of cross-lingual
FrameNet extension is the restricted cross-
language applicability of frames. Boas (2005)
reports that, while many frames are largely
4see overview table at http://www.ukp.
tu-darmstadt.de/fnwkde/.
1370
language-independent, other frames receive
culture-specific or language-specific interpreta-
tions, for example calendars or holidays. Also,
fine-grained sense and frame distinctions may be
more relevant in one language than in another
language. Such granularity differences also led
to the addition of proto-frames in SALSA 2 (Re-
hbein et al, 2012). Therefore, manual correction
or extension of a multilingual FrameNet based on
FNWKde may be desired for specific applications.
In this case, the automatically created FrameNets
in other languages are good starting points that
can be quickly and efficiently compiled.
The quality of the multilingual FNWKxx de-
pends on i) the translations in the interlingual con-
nection Wiktionary, which are manually created,
controlled by the community, and therefore reli-
able, and ii) on the FrameNet?Wiktionary align-
ment. Therefore, we evaluated our sense align-
ment method in detail. The alignment reaches
state-of-the-art results, and the analysis shows that
the method is particularly fit for a coarse-grained
alignment. We however find lower performance
for verbs in a fine-grained setting. We argue
that an improved alignment algorithm, for instance
taking subcategorization information into account,
can identify the fine-grained distinctions.
The post-hoc analysis raised the question of
FrameNet frame granularity. Do separate frames
exist for causative/inchoative alternations (as Be-
ing dry and Cause to be dry for to dry), or do they
belong to the same frame (Make noise for to creak
and to creak something)? For the coarse-grained
frames, fine-grained decisions can be merged in a
second classification step. Alternatively, we could
map Wiktionary senses directly to frames, and in-
clude features that cover the granularity distinc-
tions, e.g., whether the existing senses of a frame
show the semantic alternation.
We could use the same approach to assign
senses to a frame which are derived via sense
relations other than synonymy, i.e., for linking
antonyms or hyponyms to a frame. Some frames
do cover antonymous predicates, others do not.
Based on Wiktionary, our approach suffers less
from the disadvantages of previous resource-based
work, i.e., the constraints of expert-built resources
and the lack of lexical information in Wikipedia.
Unlike corpus-based approaches for cross-lingual
FrameNet extension, our approach does not pro-
vide frame-semantic annotations for the example
sentences. Our advantage is that we create a
FrameNet lexicon with lexical-semantic informa-
tion in the target language. Example annotations
can be additionally obtained via cross-lingual an-
notation projection (Pado? and Lapata, 2009), and
the lexical information in FNWKde can be used to
guide this process.
10 Conclusion
The resource-coverage bottleneck for frame-
semantic resources is particularly severe for less
well-resourced languages. We present a simple,
but effective approach to solve this problem using
the English Wiktionary as an interlingual repre-
sentation and subsequent translation disambigua-
tion in the target language. We validate our ap-
proach on the language pair English-German and
discuss the options and requirements for creating
FrameNets in further languages.
As part of this work, we created the first sense
alignment between FrameNet and the English
Wiktionary. The resulting resource FNWKxx con-
nects FrameNet senses to over 280 languages. The
bilingual English-German FrameNet lexicon FN-
WKde competes with manually created resources,
as shown by a comparison to the SALSA corpus.
We make both resources publicly available in
the standardized format UBY-LMF (Eckle-Kohler
et al, 2012), which supports automatic processing
of the resources via the UBY Java API, see
http://www.ukp.tu-darmstadt.de/
fnwkde/.
We also extended FrameNet by several thou-
sand new English senses from Wiktionary which
are provided as part of FNWKde. In our future
work, we will evaluate the benefits of the extracted
information to SRL.
Acknowledgments
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806
and by the German Research Foundation under
grant No. GU 798/3-1 and grant No. GU 798/9-1.
We thank Christian Meyer and Judith-Eckle
Kohler for insightful discussions and comments,
and Christian Wirth for contributions in the early
stage of this project. We also thank the anonymous
reviewers for their helpful remarks.
1371
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?41, Athens, Greece.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Pro-
ceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguis-
tics (COLING-ACL?98), pages 86?90, Montreal,
Canada.
Roberto Basili, Diego Cao, Danilo Croce, Bonaventura
Coppola, and Alessandro Moschitti. 2009. Cross-
language frame semantics transfer in bilingual cor-
pora. In Alexander Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing, volume
5449 of Lecture Notes in Computer Science, pages
332?345. Springer Berlin Heidelberg.
Hans C. Boas. 2005. Semantic Frames as Interlingual
Representations for Multilingual Lexical Databases.
International Journal of Lexicography, 18(4):445?
478.
Lars Borin, Markus Forsberg, Richard Johansson, Kris-
tiina Muhonen, Tanja Purtonen, and Kaarlo Voion-
maa. 2012. Transferring frames: Utilization of
linked lexical resources. In Proceedings of the
NAACL-HLT Workshop on the Induction of Linguis-
tic Structure, pages 8?15, Montre?al, Canada.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The SALSA corpus: a German corpus re-
source for lexical semantics. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 969?974, Genoa,
Italy.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining word sense
and usage for modeling frame semantics. In Pro-
ceedings of the 2008 Conference on Semantics in
Text Processing, STEP ?08, pages 85?101, Strouds-
burg, PA, USA.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management (CIKM
2009), pages 513?522, New York, NY, USA.
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hart-
mann, Michael Matuschek, and Christian M. Meyer.
2012. UBY-LMF - A Uniform Model for Standard-
izing Heterogeneous Lexical-Semantic Resources in
ISO-LMF. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC?12), pages 275?282, Istanbul, Turkey.
Oscar Ferrandez, Michael Ellsworth, Rafael Munoz,
and Collin F. Baker. 2010. Aligning FrameNet
and WordNet based on Semantic Neighborhoods.
In Proceedings of the Seventh International Con-
ference on Language Resources and Evaluation
(LREC?10), pages 310?314, Valletta, Malta.
Charles J. Fillmore. 1976. Frame Semantics and the
Nature of Language. In Annuals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, volume 280,
pages 20?32. New York Academy of Sciences, New
York, NY, USA.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby - A Large-Scale Uni-
fied Lexical-Semantic Resource Based on LMF. In
Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2012), pages 580?590, Avignon,
France.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-Measure, and Reliability in Infor-
mation Retrieval. Journal of the American Medical
Informatics Association, 12(3):296?298.
Richard Johansson and Pierre Nugues. 2005. Us-
ing Parallel Corpora for Automatic Transfer of
FrameNet Annotation. In Proceedings of the 1st
ROMANCE FrameNet Workshop, Cluj-Napoca, Ro-
mania.
Richard Johansson and Pierre Nugues. 2006. A
framenet-based semantic role labeler for swedish. In
Proceedings of the COLING/ACL 2006 Main Con-
ference Poster Sessions, pages 436?443, Sydney,
Australia, July.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic
Resources for Scandinavian and Baltic Languages,
at NODALIDA, pages 27?30, Tartu, Estonia.
Egoitz Laparra and German Rigau. 2009. Integrating
WordNet and FrameNet using a Knowledge-based
Word Sense Disambiguation Algorithm. In Pro-
ceedings of the International Conference RANLP-
2009, pages 208?213, Borovets, Bulgaria.
Egoitz Laparra and German Rigau. 2010. eXtended
WordFrameNet. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC?10), pages 1214?1419, Valletta,
Malta.
Christian M. Meyer and Iryna Gurevych. 2010.
How Web Communities Analyze Human Language:
Word Senses in Wiktionary. In Proceedings of
the Second Web Science Conference, Raleigh, NC,
USA.
1372
Christian M. Meyer and Iryna Gurevych. 2011. What
Psycholinguists Know About Chemistry: Align-
ing Wiktionary and WordNet for Increased Domain
Coverage. In Proceedings of the 5th International
Joint Conference on Natural Language Processing,
pages 883?892, Chiang Mai, Thailand.
Christian M. Meyer and Iryna Gurevych. 2012a. To
Exhibit is not to Loiter: A Multilingual, Sense-
Disambiguated Wiktionary for Measuring Verb Sim-
ilarity. In Proceedings of COLING 2012, pages
1763?1780, Mumbai, India.
Christian M. Meyer and Iryna Gurevych. 2012b. Wik-
tionary: A new rival for expert-built lexicons? Ex-
ploring the possibilities of collaborative lexicogra-
phy. In Sylviane Granger and Magali Paquot, edi-
tors, Electronic Lexicography, pages 259?291. Ox-
ford University Press, Oxford.
Behrang Mohit and Srini Narayanan. 2003. Semantic
Extraction with Wide-Coverage Lexical Resources.
In Proceedings of HLT-NAACL 2003: Companion
Volume, pages 64?66, Edmonton, Canada.
Claire Mouton, Gae?l de Chalendar, and Beno??t Richert.
2010. FrameNet Translation Using Bilingual Dic-
tionaries with Evaluation on the English-French
Pair. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), pages 20?27, Valletta, Malta.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering Based on Semantic Structures. In
Proceedings of the 20th international conference
on Computational Linguistics - COLING ?04, pages
693?701, Geneva, Switzerland.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 216?225, Uppsala, Sweden.
Elisabeth (geb. Wolf) Niemann and Iryna Gurevych.
2011. The People?s Web meets Linguistic Knowl-
edge: Automatic Sense Alignment of Wikipedia and
WordNet. In Proceedings of the International Con-
ference on Computational Semantics (IWCS), pages
205?214, Singapore.
Sebastian Pado? and Mirella Lapata. 2005. Cross-
lingual bootstrapping of semantic lexicons: the case
of FrameNet. In Proceedings of the 20th national
conference on Artificial intelligence - Volume 3,
AAAI?05, pages 1087?1092, Pittsburgh, PA, USA.
Sebastian Pado? and Mirella Lapata. 2009. Cross-
lingual Annotation Projection for Semantic Roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-Scale Taxonomy Mapping for Restructuring
and Integrating Wikipedia. In Proceedings of the
21st International Joint Conference on AI, pages
2083?2088, Pasadena, CA, USA.
Ines Rehbein, Joseph Ruppenhofer, Caroline
Sporleder, and Manfred Pinkal. 2012. Adding
nominal spice to SALSA - frame-semantic annota-
tion of German nouns and verbs. In Proceedings
of the 11th Conference on Natural Language
Processing (KONVENS?12), pages 89?97, Vienna,
Austria.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic Assignment of
Wikipedia Encyclopedic Entries to WordNet
Synsets. In Advances in Web Intelligence, volume
3528 of Lecture Notes in Computer Science, pages
380?386. Springer, Berlin Heidelberg.
Franck Sajous, Emmanuel Navarro, Bruno Gaume,
Laurent Pre?vot, and Yannick Chudy. 2010. Semi-
automatic endogenous enrichment of collaboratively
constructed lexical resources: piggybacking onto
wiktionary. In Proceedings of the 7th interna-
tional conference on Advances in natural language
processing, IceTAL?10, pages 332?344. Springer,
Berlin, Heidelberg.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Computational
Linguistics and Intelligent Text Processing, pages
100?111. Springer, Berlin Heidelberg.
Sara Tonelli and Claudio Giuliano. 2009. Wikipedia as
frame information repository. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 276?285, Singapore.
Sara Tonelli and Emanuele Pianta. 2009. A novel ap-
proach to mapping FrameNet lexical units to Word-
Net synsets. In IWCS-8 ?09: Proceedings of the
Eighth International Conference on Computational
Semantics, pages 342?345, Tilburg, The Nether-
lands.
Sara Tonelli and Daniele Pighin. 2009. New Features
for FrameNet - WordNet Mapping. In Proceedings
of the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2009), pages 219?
227, Boulder, CO, USA.
Sara Tonelli, Volha Bryl, Claudio Giuliano, and Lu-
ciano Serafini. 2012. Investigating the seman-
tics of frame elements. In Knowledge Engineer-
ing and Knowledge Management, volume 7603 of
Lecture Notes in Computer Science, pages 130?143.
Springer Berlin Heidelberg.
1373
