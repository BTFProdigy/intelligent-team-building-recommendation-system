Advances in Children?s Speech Recognition  
within an Interactive Literacy Tutor 
 
Andreas Hagen, Bryan Pellom, Sarel Van Vuuren, and Ronald Cole 
Center for Spoken Language Research 
University of Colorado at Boulder 
http://cslr.colorado.edu 
 
 
 
 
Abstract1 
In this paper we present recent advances in 
acoustic and language modeling that improve 
recognition performance when children read 
out loud within digital books. First we extend 
previous work by incorporating cross-
utterance word history information and dy-
namic n-gram language modeling. By addi-
tionally incorporating Vocal Tract Length 
Normalization (VTLN), Speaker-Adaptive 
Training (SAT) and iterative unsupervised 
structural maximum a posteriori linear regres-
sion (SMAPLR) adaptation we demonstrate a 
54% reduction in word error rate.  Next, we 
show how data from children?s read-aloud 
sessions can be utilized to improve accuracy 
in a spontaneous story summarization task.  
An error reduction of 15% over previous pub-
lished results is shown.  Finally we describe a 
novel real-time implementation of our re-
search system that incorporates time-adaptive 
acoustic and language modeling. 
1 Introduction 
Pioneering research by MIT and CMU as well as more 
recent work by the IBM Watch-me-Read Project have 
demonstrated that speech recognition can play an effec-
tive role in systems designed to improve children?s 
reading abilities (Mostow et al, 1994; Zue et al, 1996). 
In CMU?s Project LISTEN, for example, the tutor oper-
ates by prompting children to read individual sentences 
out loud.  The tutor listens to the child using speech 
recognition and extracts features that can be used to 
detect oral reading miscues (Mostow et al, 2002; Tam 
et al 2003).   Upon detecting reading miscues, the tutor 
provides appropriate feedback to the child.  Recent re-
                                                        
1
 This work was supported in part by grants from the National Science 
Foundation's Information Technology Research (ITR) Program and 
the Interagency Educational Research Initiative (IERI) under grants 
NSF/ITR: REC-0115419, NSF/IERI: EIA-0121201, NSF/ITR: IIS-
0086107, NSF/IERI: 1R01HD-44276.01, NSF: INT-0206207; and the 
Coleman Institute for Cognitive Disabilities. The views expressed in 
this paper do not necessarily represent the views of the NSF. 
sults show that such automated reading tutors can im-
prove student achievement (Mostow et al 2003). Pro-
viding real time feedback by highlighting words as the 
are read out loud is the basis of at least one commercial 
product today (http://www.soliloquy.com).  
Cole et al (2003) and Wise et al (in press) describe 
a new scientifically-based literacy program, Founda-
tions to Fluency, in which a virtual tutor?a lifelike 3D 
computer model?interacts with children in multimodal 
learning tasks to teach them to read. A key component 
of this program is the Interactive Book, which combines 
real-time speech recognition, facial animation, and natu-
ral language understanding capabilities to teach children 
to read and comprehend text.  Interactive Books are 
designed to improve student achievement by helping 
students to learn to read fluently, to acquire new knowl-
edge through deep understanding of what they read, to 
make connections to other knowledge, and to express 
their ideas concisely through spoken or written summa-
ries. Transcribed spoken summaries can be graded 
automatically to provide feedback to the student about 
their comprehension.  
During reading out loud activities in Interactive 
Books, the goal is to design a computer interface and 
speech recognizer that combine to teach the student to 
read fluently and naturally.  Here, speech recognition is 
used to track a child?s position within the text during 
read-aloud sessions in addition to providing timing and 
confidence information which can be used for reading 
assessment. The speech recognizer must follow the stu-
dents verbal behaviors accurately and quickly, so the 
cursor (or highlighted word) appears at the right place 
and right time when the student is reading fluently, and 
pauses when the student hesitates to sound out a word. 
The recognizer must also score mispronounced words 
accurately so that the student can revisit these words 
and receive feedback about their pronunciation after 
completing a paragraph or page (since highlighting hy-
pothesized mispronounced words when reading out loud 
may disrupt fluent reading behavior).   
In this paper we focus on the problem of speech rec-
ognition to track and provide feedback during reading 
out loud and to transcribe spoken summaries of text. 
Specifically, we describe several new methods for in-
corporating language modeling knowledge into the read 
aloud task.  In addition, through use of speaker adapta-
tion, we also demonstrate the potential for significant 
gains in recognition accuracy.  Finally, we leverage 
improvements in speech recognition for read aloud 
tracking to improve performance for spoken story sum-
marization.  Work reported here extends previous work 
in several important ways: by integrating the research 
advances into a real time system, and by including time-
adaptive language modeling and time-adaptive acoustic 
modeling of the child?s voice into the system. 
The paper is organized as follows. Sect. 2 describes 
our baseline speech recognition system and reading 
tracking method. Sect. 3 presents our rationale for using 
word-error-rate as a measure of performance.  Sect. 4 
describes the read aloud and story summarization cor-
pora used in this work. Sect. 5 describes and evaluates 
proposed improvements in a read aloud speech recogni-
tion task. Sect. 6 describes how these improvements 
translate to improved recognition of story summaries 
produced by a child. Sect. 7 details our real-time system 
implementation. 
2 Baseline System 
For this work we use the SONIC speech recognition 
system (Pellom, 2001; Pellom and Hacioglu, 2003).  
The recognizer implements an efficient time-
synchronous, beam-pruned Viterbi token-passing search 
through a static re-entrant lexical prefix tree while 
utilizing continuous density mixture Gaussian HMMs.  
For children?s speech, the recognizer has been trained 
on 46 hours of data from children in grades K through 9 
extracted from the CU Read and Prompted speech 
corpus (Hagen et al, 2003) and the OGI Kids? speech 
corpus (Shobaki et al, 2000).  Further, the baseline 
system utilizes PMVDR cepstral coefficients (Yapanel 
and Hansen, 2003) for improved noise robustness. 
During read-aloud operation, the speech recognizer 
models the story text using statistical n-gram language 
models.  This approach gives the recognizer flexibility 
to insert/delete/substitute words based on acoustics and 
to provide accurate confidence information from the 
word-lattice.  The recognizer receives packets of audio 
and automatically detects voice activity.  When the 
child speaks, the partial hypotheses are sent to a reading 
tracking module.  The reading tracking module deter-
mines the current reading location by aligning each par-
tial hypothesis with the book text using a Dynamic 
Programming search.  In order to allow for skipping of 
words or even skipping to a different place within the 
text, the search finds words that when strung together 
minimize a weighted cost function of adjacent word-
proximity and distance from the reader's last active 
reading location. The Dynamic Programming search 
additionally incorporates constraints to account for 
boundary effects at the ends of each partial phrase. 
3 Evaluation Methodology 
There are many different ways in which speech recogni-
tion can be used to serve children. In computer-based 
literacy tutors, speech recognition can be used to meas-
ure children's ability to read fluently and pronounce 
words while reading out loud, to engage in spoken dia-
logues with an animated agent to assess and train com-
prehension, or to transcribe spoken summaries of stories 
that can be graded automatically.  Because of the variety 
of ways of using speech recognition systems, it is criti-
cally important to establish common metrics that are 
used by the research community so that progress can be 
measured both within and across systems. 
For this reason, we argue that word error rate calcu-
lations using the widely accepted NIST scoring software 
provides the most widely accepted, easy to use and 
highly valid metric.  In this scoring procedure, word 
error rate is computed strictly by comparing the speech 
recognizer output against a known human transcription 
(or the text in a book).  Of course, authors are free to 
define and report other measures, such as detection/false 
alarm curves for useful events such as reading miscues.  
However, such analyses should always supplement re-
ports of word error rates using a single standardized 
measure. Adopting this strategy enables fair and bal-
anced comparisons within and across systems for any 
speech data given a known word-level transcription. 
4 Experimental Data 
For all experiments in this paper we use speech data and 
associated transcriptions from 106 children (grade 3: 17 
speakers, grade 4: 28 speakers, and grade 5: 61 speak-
ers) who were asked to read one of ten stories and to 
provide a spoken story summary.  The 16 kHz audio 
data contains an average of 1054 words (min 532 
words; max 1926 words) with an average of 413 unique 
words per story.  The resulting summaries spoken by 
children contain an average of 168 words. 
5 Improved Read-Aloud Recognition 
Baseline: Our baseline read-aloud system utilizes a 
trigram language model constructed from a normalized 
version of the story text. Text normalization consists 
primarily of punctuation removal and determination of 
sentence-like units.  For example,  
 
It was the first day of summer vacation.  Sue and Billy were 
eating breakfast.  ?What can we do today?? Billy asked. 
 
is normalized as: 
 
<s> IT WAS THE FIRST DAY OF SUMMERVACATION</s> 
<s> SUE AND BILLY WERE EATING BREAKFAST</s> 
<s> WHAT CAN WE DO TODAY </s> 
<s> BILLY ASKED </s> 
 
The resulting text is used to estimate a back-off trigram 
language model. We stress that only the story text is 
used to construct the language model. Details on the 
story texts are provided in Hagen et al (2003). Note that 
the sentence markers (<s> and </s>) are used to repre-
sent positions of expected speaker pause.  This baseline 
system is shown in Table 1(A) to produce a 17.4% word 
error rate. 
Improved Sentence Context Modeling: It is impor-
tant in the context of this research to note that children 
do not pause between each estimated sentence bound-
ary.  Instead, many children read fluently across phrases 
and sentences, where more experienced readers would 
pause. For this reason, we improved upon our baseline 
system by estimating language model parameters using 
a combined text material that is generated both with and 
without the contextual sentence markers (<s> and </s>).  
Results of this modification are shown in Table 1(B) 
and show a reduction in error from 17.4% to 13.5%. 
Improved Word History Modeling:  Most speech 
recognition systems operate on the utterance as a pri-
mary unit of recognition.  Word history information 
typically is not maintained across segmented utterances.  
However, in our text example, the words ?do today? 
should provide useful information to the recognizer that 
?Billy asked? may follow.  We therefore modify the 
recognizer to incorporate knowledge of previous utter-
ance word history. During token-passing search, the 
initial word-history tokens are modified to account for 
the fact that the incoming sentence may be either the 
beginning of a new sentence or a direct extension of the 
previous utterance?s word-end history.  Incorporating 
this constraint lowers the word error rate from 13.5% to 
12.7% as shown in Table 1(C). 
Dynamic n-gram Language Modeling:  During story 
reading we can anticipate words that are likely to be 
spoken next based upon the words in the text that are 
currently being read aloud.  To account for this knowl-
edge, we estimate a series of position-sensitive n-gram 
language models by partitioning the story into overlap-
ping regions containing at most 150 words (i.e., each 
region is centered on 50 words of text with 50 words 
before and 50 words after).  For each partition, we con-
struct an n-gram language model by using the entire 
normalized story text in addition to a 10x weighting of 
text within the partition.  Each position-sensitive lan-
guage model therefore contains the entire story vocabu-
lary.  We also compute a general language model 
estimated solely from the entire story text (similar to 
Table 1(C)).   At run-time, the recognizer implements a 
word-history buffer containing the most recent 15 rec-
ognized words.  After decoding each utterance, the 
probability of the text within the word history buffer is 
computed using each of the position-sensitive language 
models.  The language model with the highest probabil-
ity is selected for the first-pass decoding of the subse-
quent utterance.  This modification decreases the word 
error rate from 12.7% to 10.7% (Table 1(D)). 
Vocal Tract Normalization and Acoustic Adaptation:  
We further extend on our baseline system by incorporat-
ing the Vocal Tract Length Normalization (VTLN) 
method described in Welling et al (1999).  Based on 
results shown in Table 1(E), we see that VTLN provides 
only a marginal gain (0.1% absolute).  Our final set of 
acoustic models for the read aloud task are both VTLN 
normalized and estimated using Speaker Adaptive 
Training (SAT).  The SAT models are determined by 
estimating a single linear feature space transform for 
each training speaker (Gales, 1997).  The means and 
variances of the VTLN/SAT models are then iteratively 
adapted using the SMAPLR algorithm (Siohan, 2002) to 
yield a final recognition error rate of 8.0% absolute (Ta-
ble 1(G)).  By combining all of these techniques, we 
achieved a 54% reduction in word error rate relative to 
the baseline system.    
 
Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline: single n-gram 
language model 17.7% 17.4% 
(B) (A) + Begin/End Sentence 
Context Modeling 14.0% 13.5% 
(C) (B) + between utterance 
word history modeling 13.0% 12.7% 
(D) (C) + dynamic  
n-gram language model 11.0% 10.7% 
(E) (D) + VTLN 10.9% 10.6% 
(F) (E) + VTLN/SAT + 
SMAPLR (iteration 1) 8.2% 8.2% 
(G) (E) + VTLN/SAT + 
SMAPLR (iteration 2) 8.0% 8.0% 
Table 1: Recognition of children?s read out-loud data. 
6 Improved Story Summary Recognition 
One of the unique and powerful features of our interac-
tive books is the notion of assessing and training com-
prehension by providing feedback to the student about a 
typed summary of text that the student has just read 
(Cole et al, 2003). Verbal input is especially important 
for younger children who often can not type well. Util-
izing summaries from the children?s speech corpus, 
Hagen et al (2003) showed that an error rate of 42.6% 
could be achieved.  The previous work, however, did 
not consider utilizing the read story material to provide 
improved initial acoustic models for the summarization 
task.  In Table 2 we demonstrate several findings using 
a language model trained on story text and example 
summaries produced by children (leaving out data from 
the child under test).  Without any adaptation the error 
rate is 47.1%.  However, utilizing adapted models from 
the read stories (see Table 1(G)) provides an initial per-
formance gain of nearly 10% absolute.  Further use 
SMAPLR adaptation reduces the error rate to 36.1%. 
 Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline / no adaptation 47.0% 47.1% 
(B) Read-aloud adapted models 
(VTLN/SAT) 37.2% 38.0% 
(C) (B) + SMAPLR  
adaptation iteration #1 36.0% 36.6% 
(D) (C) + SMAPLR 
adaptation iteration #2 35.1% 36.1% 
Table 2:  Recognition of spontaneous story summaries 
7 Practical Real-Time Implementation 
The research systems described in Sect. 5 and 6 do not 
operate in real-time since multiple adaptation passes 
over the data are required.  To address this issue, we 
have implemented a real-time system that operates on 
small pipelined audio segments (250ms on average).  
When evaluated on the read-aloud task (Sect. 5), the 
initial baseline system achieves an error rate of 19.5%.  
This system has a real-time factor of 0.56 on a 2.4 GHz 
Intel Pentium 4 PC with 512MB of RAM. When inte-
grated, the proposed methods show the error rate can be 
reduced from 19.5% to 12.7% (compare with 10.7% 
error research system in Table 1(D)).  The revised sys-
tem which incorporates dynamic language modeling 
operates 35% faster than the single language model 
method while also reducing the variance in real-time 
factor for each processed chunk of audio.  Further gains 
are possible by incorporating adaptation in an incre-
mental manner.  For example, in Table 3(C) a real-time 
system that incorporates incremental unsupervised 
maximum likelihood linear regression (MLLR) adapta-
tion of the Gaussian means is shown.  This final real-
time system simultaneously adapts both language and 
acoustic model parameters during system use. The sys-
tem is now being refined for deployment in classrooms 
within the CLT project. We were able to further im-
prove the system after the submission deadline. The 
current WER on the story read aloud task improved to 
7.6%; while a WER of 32.2% was achieved on the 
summary recognition task. The improvements are due to 
the inclusion of a breath model and the additional use of 
audio data from 103 second graders for more accurate 
acoustic modeling. 
 
PMVDR Front-End System Description WER (%) RTF 
(A) Baseline: single LM 19.5% 0.56  (  2=0.11) 
(B) Proposed System 12.7% 0.36 (  2=0.06) 
(C) (B) + Incremental 
MLLR adaptation 11.5% 
0.80 
(  2=0.33) 
Table 3:  Evaluation of real-time read out-loud system. 
References 
 
V. Zue, S. Seneff, J. Polifroni, H. Meng, J. Glass 
(1996). ?Multilingual Human-Computer Interactions: 
From Information Acess to Language Learning,? 
ICSLP-96, Philadelphia, PA 
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane 
(1994). "A Prototype Reading Coach that Listens", 
AAAI-94, Seattle, WA, pp. 785-792.  
Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee (2003). 
?Training a Confidence Measure for a Reading Tutor 
that Listens?. Eurospeech, Geneva, Switzerland, 
3161-3164. 
J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin 
(2002). ?Predicting oral reading miscues? ICSLP-02, 
Denver, Colorado. 
J. Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo, 
S. Eitelman, C. Huang, B. Junker, M. B. Sklar, and 
B. Tobin (2003). ?Evaluation of an automated Read-
ing Tutor that listens:  Comparison to human tutoring 
and classroom instruction?. Journal of Educational 
Computing Research, 29(1), 61-117 
R. Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma, 
J. Movellan, S. Schwartz, D. Wade-Stein, W. Ward, 
J. Yan (2003). ?Perceptive Animated Interfaces: First 
Steps Toward a New Paradigm for Human Computer 
Interaction,? Proceedings of the IEEE, Vol. 91, No. 
9, pp. 1391-1405 
A. Hagen, B. Pellom, and R. Cole (2003). "Children?s 
Speech Recognition with Application to Interactive 
Books and Tutors", ASRU-2003, St. Thomas, USA 
B. Pellom (2001). "SONIC: The University of Colorado 
Continuous Speech Recognizer", Technical Report 
TR-CSLR-2001-01, University of Colorado. 
B. Pellom and K. Hacioglu (2003). "Recent Improve-
ments in the CU Sonic ASR System for Noisy 
Speech: The SPINE Task", ICASSP-2003, Hong 
Kong, China. 
U. Yapanel, J. H.L. Hansen (2003). "A New Perspective    
on Feature Extraction for Robust In-vehicle Speech  
Recognition" Eurospeech, Geneva, Switzerland. 
K. Shobaki, J.-P. Hosom, and R. Cole (2000). "The OGI 
Kids' Speech Corpus and Recognizers", Proc. 
ICSLP-2000, Beijing, China. 
L. Welling, S. Kanthak, and H. Ney. (1999) "Improved 
Methods for Vocal Tract Length Normalization", 
ICASSP, Phoenix, Arizona. 
M. Gales (1997). Maximum Likelihood Linear Trans-
formations for HMM-Based Speech Recognition", 
Tech. Report, CUED/F-INFENG/TR291, Cambridge 
University. 
O. Siohan, T. Myrvoll, and C.-H. Lee (2002) "Structural 
Maximum a Posteriori Linear Regression for Fast 
HMM Adaptation", Computer, Speech and Lan-
guage, 16, pp. 5-24.  
DISCUSS: A dialogue move taxonomy layered over semantic
representations
Lee Becker1 Wayne H. Ward1,2 Sarel van Vuuren1 Martha Palmer1
{lee.becker, martha.palmer, sarel.vanvuuren}@colorado.edu,
wward@bltek.com
1University of Colorado at Boulder, 2Boulder Language Technologies
Abstract
In this paper we describe DISCUSS, a dialogue move taxonomy layered over semantic represen-
tations. We designed this scheme to enable development of computational models of tutorial dia-
logues and to provide an intermediate representation suitable for question and tutorial act generation.
As such, DISCUSS captures semantic and pragmatic elements across four dimensions: Dialogue Act,
Rhetorical Form, Predicate Type, Semantic Roles. Together these dimensions provide a summary of
an utterance?s propositional content and how it may change the underlying information state of the
conversation. This taxonomy builds on previous work in both general dialogue act taxonomies as
well as work in tutorial act and tutorial question categorization. The types and values found within
our taxonomy are based on preliminary observations and on-going annotation from our corpus of
multimodal tutorial dialogues for elementary school science education.
1 Introduction
Past successes with conversational Intelligent Tutoring Systems (ITS) (Graesser et al, 2001), have helped
to demonstrate the efficacy of computer-led, tutorial dialogue. However, ITS will not reach their full
potential until they can overcome current limitations in spoken dialogue technologies. Producing systems
capable of leading open-ended, Socratic-style tutorials will likely require more sophisticated models to
automate analysis and generation of dialogue. A well defined tutorial dialogue annotation scheme can
serve as a stepping stone towards these goals. Such a scheme should account for differences in tutoring
style and question scaffolding techniques and should capture the subtle distinctions between different
question types. To do this, requires a representation that connects a turn?s communicative and rhetorical
functions to its underlying semantic content.
While efforts such as DAMSL (Core and Allen, 1997) and DIT++ (Bunt, 2009) have helped to make
dialogue act annotation more uniform and applicable to a wider audience, and while tutoring-specific
initiatives (Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008) have helped to bring dialogue
acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences
in tutorial questioning styles exhibited in our corpus of Socratic-style tutorial dialogues. Conversely,
question type categories (Graesser and Person, 1994; Nielsen et al, 2008) have been designed with
education in mind, but they largely ignore how the student and tutor may work together to construct
meaning. The DISCOUNT scheme?s (Pilkington, 1999) combination of dialogue acts and rhetorical
functions enabled it to better capture tutoring moves, but its omission of shallow semantics prevents it
from capturing how content influences behavior.
Our long-term goals of automatic dialogue characterization, tutorial move prediction and question
generation led us to design our own dialogue representation called DISCUSS (Dialogue Scheme for
Unifying Speech and Semantics). Design of this dialogue move taxonomy was based on preliminary
observations from our corpus of tutorial dialogues, and was influenced by the aforementioned research.
We hope that undertaking this ambitious endeavor to capture not only a turn?s pragmatic interpretation,
310
but also its rhetorical and semantic functions will enable us to better model the complexity of open-ended,
tutorial dialogue.
The remainder of the this paper is organized as follows. In the next section we describe our tutorial
dialogue setting and our data. Section 3 discusses the organization of the DISCUSS annotation scheme.
Section 4 briefly explains the current status of our annotation. Lastly section 5 outlines our future plans
and conclusions.
2 Tutorial Dialogue Setting and Data
My Science Tutor (MyST) (Ward et al, 2010) is a conversational virtual tutor designed to improve
science learning and understanding for students in grades 3-5. Students using MyST investigate and
discuss science through natural spoken dialogues and multimedia interactions with a virtual tutor named
Marni. The MyST dialogue design and tutoring style is based on a pedagogy called Questioning the
Author (QtA) (Beck et al, 1996), wherein the teacher facilitates discovery by challenging students with
open-ended questions and by directly keying in on ideas expressed in the student?s language.
To gather data for MyST system coverage and dialogue analysis, we ran Wizard-of-Oz (WoZ) exper-
iments that allowed a human tutor to be inserted into the interaction loop. Project tutors trained in QtA
served as Wizards and were responsible for accepting and overriding system actions. Over the past three
years we have accumulated over five-hundred, 15-minute WoZ sessions across four modules Magnetism
and Electricity, Measurement, Variables, and Water, each with 16 lessons. Student speech from these
sessions was professionally transcribed at the word level.
3 The DISCUSS Annotation Scheme
The Dialogue Scheme for Unifying Speech and Semantics (DISCUSS) is a multifaceted dialogue move
taxonomy intended to capture both the pragmatic and semantic interpretations of an utterance. A DIS-
CUSS move is a tuple composed of values from four dimensions: Dialogue Act, Rhetorical Form, Pred-
icate Type, and Semantic Roles. Together these dimensions convey the communicative action, surface
form, and meaning of an utterance independent of the original utterance text.
We designed DISCUSS to serve as an intermediate representation that will enable future work in
dialogue session characterization, dialogue strategy optimization, and automatic question generation. To
facilitate these goals, we have endeavored to create a taxonomy that is both descriptive and curriculum-
independent while allowing for expansion as necessary. A complete listing of all the DISCUSS moves
and dimensions can be found in our forthcoming technical report.
In the following subsection we will describe the different DISCUSS move categories. Descriptions
of the Semantic Role and Predicate Type are found in the subsection about semantic dimensions, while
discussion about the dialogue act and rhetorical form has been placed in the pragmatic dimensions
subsection. Throughout the rest of this paper we denote DISCUSS tuples using the following notation:
Dialogue Act/Rhetorical Form/Predicate Type ?Semantic Role?.
3.1 Move Categories
DISCUSS moves are dictated by the dialogue act dimension and may belong to one of three broad cate-
gories: Dialogue Control, Information Exchange, and Attention Management. Dialogue Control moves
are largely concerned with maintaining and enabling the flow of information. This includes dialogue
acts such as Acknowledge, Open, Close, Repeat, and RequestRepeat. The Information Exchange moves
relay content (often lesson-specific) between speakers using moves such as Assert, Ask, Answer, Mark,
Revoice. For tutorial dialogue the bulk of student-tutor interactions reside in this category. Lastly, At-
tention Management moves indicate how a speaker exercises initiative over other speakers or topics.
Dialogue acts found in the attention category are Focus, Defer, Elicit, and Direct.
311
3.2 Semantic Dimensions
The semantic dimensions define the objects, events, properties and relations contained within an utter-
ance. The semantic roles at the lowest level of the DISCUSS hierarchy directly capture the propositional
entities. Predicate Types summarize the interactions between all of the semantic roles found within an
utterance.
Semantic Roles: The MyST system models a lesson?s key concepts as propositions which are real-
ized as semantic frames. For MyST natural language understanding, these frames serve as the top-level
nodes for a manually written semantic grammar used by the Phoenix parser (Ward, 1994). Two example
concepts/frames and Phoenix parses are shown below. Although these semantic frames form the basis
of MyST dialogues, for DISCUSS annotation we sought a more domain-independent representation that
would generalize across a wide range of subjects. We began with VerbNet (Schuler, 2005) for defining
our set of semantic roles because of its intuitive balance between descriptiveness and portability. While
we used a majority of the labels as is, we found that the definition of some roles needed to be modified
or extended to properly cover our set of concepts. For example, many concepts that express proportion-
ality relationships can not be easily represented using predicate argument structure, and are more easily
decomposed into cause and effect roles. We also added the catch-all keyword label to reflect terms that
may relate to the proposition, but are not part of the core representation.
For our annotation project, rather than manually tagging all of the utterances with VerbNet labels, we
created a mapping layer between the Phoenix frame roles and the VerbNet roles. The table below shows
two frames along with their role mappings. We envision that in future projects, the hand-tuned semantic
grammars could be replaced with a statistically trained semantic role labeler.
Frame: BatteryFunction Frame: MagnetsAttract
Description: The DCell is the source of elec-
tricity.
Description: Magnets attract to certain ob-
jects.
?Instrument?: [Battery] ?Instrument?: [Magnet]
?Predicate?: [Source] ?Predicate?: [Attract]
?Theme?: [Electricity] ?Theme?: [Object]
Predicate Type: Simply knowing an utterance?s propositional content is insufficient for inferring
what was stated. Consider the two exchanges shown in the table below. The mixture of semantic roles
in both students? responses are identical. Additionally, we can not differentiate between the exchanges
based solely on dialogue act or rhetorical form. We need additional information to know the first scenario
seeks to elicit discussion about observations while the second scenario focuses on procedures. One can
also imagine such information would be useful for identifying communication breakdowns. For example,
responding with a description of a procedure to a request about a process may indicate that the student
did not understand the question or that the student is unwilling or unable to address the question.
T12: Tell me about what?s going on here in this picture.
Ask/Describe/Observation
S13: The wires connect the battery and the light bulb and then then light bulb lights up.
Answer/Describe/Observation
?Instrument?.wires ?Predicate?.connect ?Theme1?.battery ?Theme2?.light bulb ?Effect?.bulb
lights up
T7: Tell me about how you got the bulb to light up.
Ask/Describe/Procedure
S8: To make the light go we connected the wires to the battery and the bulb.
Answer/Describe/Procedure
?Effect?.light go ?Predicate?.connected ?Instrument?.wires ?Theme1?.battery ?Theme2?.bulb
To address this need, we created the Predicate Type based partly on the rhetorical predicates used in
the DISCOUNT (Pilkington, 1999) scheme. While DISCOUNT included discourse relations in the set
of predicate types, we restrict predicate types to those that encapsulate or summarize the collection of
semantic roles in an utterance. Example predicate types include procedure, observation and purpose. A
complete list of predicate types can be found in our forthcoming technical report.
312
3.3 Pragmatic Dimensions
The pragmatic dimensions are composed of the dialogue act dimension and the rhetorical form dimen-
sion. The dialogue act expresses the communicative function of a move and is the most general dimen-
sion in DISCUSS. The rhetorical form expresses attributes of the utterance?s surface realization and can
be thought of as refining the intent of the coarser dialogue act.
Dialogue Act: The dialogue act dimension is the top-level dimension in DISCUSS with the values
of all other dimensions depending on the value of this dimension. Like with the majority of dialogue
act taxonomies, DISCUSS dialogue acts have a grounding in speech act theory with a focus on what
action the utterance performs. While most of the dialogue acts in the Dialogue Control and Informa-
tion Exchange move categories have direct corollaries to those found in other taxonomies like DIT++ or
DAMSL, we needed to supplement them with two frequently used Questioning the Author discussion
moves: marking and revoicing. In marking, the tutor highlights parts of the student?s language to em-
phasize important points and to steer the conversation towards key concepts. Revoicing serves a similar
purpose, but instead of highlighting, the tutor rephrases student speech to clarify ideas they may have
been struggling with. Examples of these acts are shown below.
S5: that when you stick a magnet to a rusty nail and then you stick it to a paper clip it sticks
Answer/Describe/Process
T6: I think I heard you say something about magnets sticking or attracting. Tell me more about that.
Mark/None/None, Ask/Elaborate/Process
S33: well when you scrub the the paperclip to the magnet the paperclip is starting to be a magnet
Answer/Describe/Process
T34: very good, so if the magnet gets close to the paperclip it picks it up
Feedback/Positive/None, Revoice/None/None
Dialogue acts in the Attention Management move category also reflect many of the actions regularly
seen in tutorial dialogue. Focus and Defer acts are often used to move to or away from lesson-specific
topics. In our corpus Direct is typically used to give instructions related to the multimedia (e.g. ?Click
on the box? or ?Look at this animation.?).
Rhetorical Form: The DISCUSS Rhetorical Form dimension provides another mechanism for dif-
ferentiating between utterances with identical semantic content. While the dialogue act dimension is
useful for providing an utterance?s pragmatic interpretation and for determining what sequences are li-
censed, by itself it provides no indication of how a speaker is advancing the topic under discussion.
Additional information is needed to create an utterance?s surface form. Consider the two transactions
in the table below. The semantic parses in both scenarios would be identical, however the tutor?s ques-
tions and the resulting student response serve very different functions. In the first, the tutor is asking
for a description and in the second, identification. Selection of the DISCUSS rhetorical forms found in
the Information Exchange move category were inspired by the sixteen top-level tags used in Rhetori-
cal Structure Theory (RST) (Mann and Thompson, 1988). While RST uses a rhetorical relation to link
clauses and to show the development of an argument, DISCUSS uses the rhetorical form to refine the
dialogue act. A sequence of dialogue acts paired with rhetorical forms can show progressions in the
dialogue and tutoring process such as a shift from open-ended to directed questioning.
T1: Can you tell which one is the battery? T1: Can you describe what is going on with the battery?
Ask/Describe/Visual Ask/Identify/None
S2: The battery is putting out electricity. S2: The battery is the one putting out the electricity.
Answer/Describe/Process Answer/Identify/None
4 Annotation Status
We are still in the early stages of this ambitious annotation project. We currently have approximately
60 transcripts singly-annotated with DISCUSS moves. Each of these transcripts represents roughly 15
minutes of conversation and 50 turns on average. The DISCUSS taxonomy is a work in progress. Though
313
we have created the tags for each dimension based on a wide body of prior research and on preliminary
studies of our transcripts, we expect that future analysis of our annotation reliability and consistency will
likely lead us to add, modify, and combine tags. We anticipate that DISCUSS?s multidimensional nature
will likely raise issues for inter-annotator reliability, and the ability to add multiple tags per turn will
further complicate the process of evaluating agreement.
5 Future Work and Conclusions
We plan to use our corpus of DISCUSS annotated tutorial dialogues to build dialogue models for a variety
of applications including assessment of tutorial quality and dialogue move prediction. This annotation
will allow us to investigate what features of tutorial dialogue correlate with increased learning gains and
what types of questions encourage greater student interaction. Data-driven dialogue characterization will
also allow us to explore how tutorial tactics vary across domains and tutors. We envision this work as an
important first step towards automatic question generation.
In this paper we introduced the DISCUSS dialogue move taxonomy. This scheme overlays dialogue
act and rhetorical annotation over semantic representations. We believe this combination of pragmatic
interpretations and semantic representations provide an intermediate representation rich enough to an-
alyze the interactions in a complex task-oriented domain like tutorial dialogue. Furthermore, we think
DISCUSS moves can succinctly summarize the actions of a speaker?s turn, while still providing suffi-
cient information for natural language generation of dialogue moves.
Acknowledgments This work was supported by grants from the NSF (DRL-0733322, DRL-0733323) and the IES (R3053070434).
Any findings, recommendations, or conclusions are those of the author and do not necessarily represent the views of NSF or
IES.
References
Beck, I. L., M. G. McKeown, J. Worthy, C. A. Sandora, and L. Kucan (1996). Questioning the author: A year-long classroom
implementation to engage students with text. The Elementary School Journal 96(4), 387?416.
Buckley, M. and M. Wolska (2008). A classification of dialogue actions in tutorial dialogue. In Proc. COLING, pp. 73?80.
ACL.
Bunt, H. (2009). The dit++ taxonomy for functional dialogue markup. In Proc. EDAML 2009.
Core, M. and J. Allen (1997). Coding dialogs with the damsl annotation scheme. In AAAI Fall Symposium on Comm. Action in
Humans and Machines, pp. 28?35.
Graesser, A., X. Hu, S. Susarla, D. Harter, N. Person, M. Louwerse, B. Olde, and the Tutoring Research Group (2001).
Autotutor: An intelligent tutor and conversational tutoring scaffold. In Proc. AIED?01, pp. 47?49.
Graesser, A. and N. Person (1994). Question asking during tutoring. American Educational Research Journal 31, 104?137.
Mann, W. C. and S. A. Thompson (1988). Rhetorical structure theory: Toward a functional theory of text organization. Text 8(3),
243?281.
Nielsen, R. D., J. Buckingham, G. Knoll, B. Marsh, and L. Palen (2008, September). A taxonomy of questions for question
generation. In Proc. WS on the Question Generation STEC.
Pilkington, R. M. (1999). Analysing educational discourse: The discount scheme. Technical Report 99/2, Computer Based
Learning Unit, University of Leeds.
Schuler, K. K. (2005). VerbNet: A broad-coverage, comprehensive verb lexicon. Ph. D. thesis, University of Pennsylvania.
Tsovaltzi, D. and E. Karagjosova (2004). A view on dialogue move taxonomies for tutorial dialogues. In Proc. SIGDial, pp.
35?38. ACL.
Ward, W. (1994). Extracting information from spontaneous speech. In Proc. ICSLP.
Ward, W., R. Cole, D. Bolanos, C. Buchenroth-Martin, E. Svirsky, S. Van Vuuren, T. Weston, J. Zheng, and L. Becker (2010).
My science tutor: A conversational multi-media virtual tutor for elementary school science. ACM TSLP: Special Issue on
Speech and Language Processing of Children?s Speech for Child-machine Interaction Applications.
314
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 1?11,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Question Ranking and Selection in Tutorial Dialogues
Lee Beckera and Martha Palmer 2a and Sarel van Vuuren 3a and Wayne Ward 4a,b
aThe Center for Computational Language and Education Research (CLEAR)
University of Colorado Boulder
bBoulder Language Technologies
{lee.becker,martha.palmer,sarel.vanvuuren}@colorado.edu
wward@bltek.com
Abstract
A key challenge for dialogue-based intelligent
tutoring systems lies in selecting follow-up
questions that are not only context relevant
but also encourage self-expression and stimu-
late learning. This paper presents an approach
to ranking candidate questions for a given di-
alogue context and introduces an evaluation
framework for this task. We learn to rank us-
ing judgments collected from expert human
tutors, and we show that adding features de-
rived from a rich, multi-layer dialogue act
representation improves system performance
over baseline lexical and syntactic features to
a level in agreement with the judges. The ex-
perimental results highlight the important fac-
tors in modeling the questioning process. This
work provides a framework for future work
in automatic question generation and it rep-
resents a step toward the larger goal of di-
rectly learning tutorial dialogue policies di-
rectly from human examples.
1 Introduction
Socratic tutoring styles place an emphasis on elicit-
ing information from the learner to help them build
their own connections to the material. The role of a
tutor in a Socratic dialogue is to scaffold the material
and present questions that ultimately lead the student
to an ?A-ha!? moment. Numerous studies have il-
lustrated the effectiveness of Socratic-style tutoring
(VanLehn et al, 2007; Rose et al, 2001; Collins and
Stevens, 1982); consequently recreating the behav-
ior on a computer has long been a goal of research
in Intelligent Tutoring Systems (ITS). Recent suc-
cesses have shown the efficacy of conversational ITS
(Graesser et al, 2005; Litman and Silliman, 2004;
Ward et al, 2011b), however these systems are still
not as effective as human tutors, and much improve-
ment is needed before they can truly claim to be So-
cratic. Furthermore, development and tuning of tu-
torial dialogue behavior requires significant human
effort.
While our overarching goal is to improve ITS
by automatically learning tutorial dialogue strategies
directly from expert tutor behavior, we focus on the
crucial subtask of selecting follow-up questions. Al-
though asking questions is only a subset of the over-
all tutoring process, it is still a complex process that
requires understanding of the dialogue state, the stu-
dent?s ability, and the learning goals.
This work frames question selection as a task of
scoring and ranking candidate questions for a spe-
cific point in the tutorial dialogue. Since dialogue
is a dynamic process with multiple correct possibil-
ities, we do not restrict ourselves only to the moves
and questions found in a corpus of transcripts. In-
stead we posit ?What if we had a fully automatic
question generation system?? and subsequently use
candidate questions hand-authored for each dialogue
context. To explore the mechanisms involved in
ranking follow-up questions against one other, we
pair these questions with judgments of quality from
expert human tutors and extract surface form and
dialogue-based features to train machine learning
classification models to rank the appropriateness of
questions for specific points in a dialogue.
Our results show promise with our best question
1
ranking models exhibiting performance on par with
expert human tutors. Furthermore these experiments
demonstrate the utility and importance of rich dia-
logue move annotation for modeling decision mak-
ing in conversation and tutoring.
2 Background and Related Works
Learning tutorial dialogue policies from corpora is
a growing area of research in natural language pro-
cessing and intelligent tutoring systems. Past studies
have made use of hidden Markov models (Boyer et
al., 2009a) and reinforcement learning (Chi et al,
2010; Chi et al, 2009; Chi et al, 2008) to discover
tutoring strategies. However, these approaches are
typically optimized to maximize learning gains, and
are not necessarily focused on replicating human tu-
tor behavior. Other work has explored specific fac-
tors in questioning such as when to ask ?why? ques-
tions (Rose et al, 2003), provide hints (Tsovaltzi
and Matheson, 2001), or insert discourse markers
(Kim et al, 2000).
There is also an expanding body of work that ap-
plies ranking algorithms toward the task of ques-
tion generation (QG) using approaches such as over-
generation-and-ranking (Heilman and Smith, 2010),
language model ranking (Yao, 2010), and heuristics-
based ranking (Agarwal and Mannem, 2011). While
the focus of these efforts centers on issues of gram-
maticality, fluency, and content selection for auto-
matic creation of standalone questions, we move to
the higher level task of choosing context appropri-
ate questions. Our work merges aspects of these
QG approaches with the sentence planning tradi-
tion from natural language generation (Walker et al,
2001; Rambow et al, 2001). In sentence planning
the goal is to select lexico-structural resources that
encode communicative action. Rather than select-
ing representations, we use them directly as part of
the feature space for learning functions to rank the
questions? actual surface form realization. To our
knowledge there has been no research in ranking the
quality and suitability of questions within a tutorial
dialogue context.
Because questioning tactics depend heavily on the
curriculum and choice of pedagogy, we ground our
investigations within the context of the My Science
Tutor (MyST) intelligent tutoring system (Ward et
al., 2011b), a conversational virtual tutor designed
to improve science learning and understanding for
students in grades 3-5 (ages 8-11). Students using
MyST investigate and discuss science through nat-
ural spoken dialogues and multimedia interactions
with a virtual tutor named Marni. The MyST dia-
logue design and tutoring style is based on a ped-
agogy called Questioning the Author (QtA) (Beck
et al, 1996) which emphasizes open-ended ques-
tions and keying in on student language to promote
self-explanation of concepts, and its curriculum is
based on the Full Option Science System (FOSS) 1
a proven system for inquiry based learning.
3 Data Collection
3.1 MyST Logfiles and Transcripts
For these experiments, we use MyST transcripts col-
lected in a Wizard-of-Oz (WoZ) condition with a hu-
man tutor inserted into the interaction loop. Project
tutors trained in both QtA and in the tutorial sub-
ject matter served as the wizards. During a ses-
sion tutors were responsible for accepting, overrid-
ing, and/or authoring system actions. Tutor wizards
were also responsible for setting the current dialogue
frame to indicate which of the learning goals was
currently in focus. Students talked to MyST via mi-
crophone while MyST communicates using Text-to-
Speech (TTS) in the WoZ setting. A typical MyST
session revolves around a single FOSS lesson and
lasts approximately 15 minutes. To obtain a dia-
logue transcript, tutor moves are taken directly from
the system logfile, while student speech is manu-
ally transcribed from audio. In addition to the di-
alogue text, MyST records additional information
such as timestamps and the current dialogue frame
(i.e. learning goal). In total we make use of tran-
scripts from 122 WoZ dialogues covering 10 units
on magnetism and electricity and 2 in measurement
and standards.
3.2 Dialogue Annotation
Lesson-independent analysis of dialogue requires
a level of abstraction that reduces a dialogue to
its underlying actions and intentions. To address
this need we use the Dialogue Schema Unifying
Speech and Semantics (DISCUSS) (Becker et al,
1http://www.fossweb.com
2
2011), a multidimensional dialogue move taxon-
omy that captures both the pragmatic and seman-
tic interpretation of an utterance. Instead of us-
ing one label, a DISCUSS move is a tuple com-
posed of three dimensions: Dialogue Act, Rhetor-
ical Form, Predicate Type. Together these labels
account for the action, function, and content of an
utterance. This scheme draws from past work in
task-oriented dialogue acts (Bunt, 2009; Core and
Allen, 1997), tutorial act taxonomies (Pilkington,
1999; Tsovaltzi and Karagjosova, 2004; Buckley
and Wolska, 2008; Boyer et al, 2009b) discourse
relations (Mann and Thompson, 1986) and question
taxonomies (Graesser and Person, 1994; Nielsen et
al., 2008).
Dialogue Act (22 tags): The dialogue act dimen-
sion is the top-level dimension in DISCUSS, and its
values govern the possible values for the other di-
mensions. Though the DISCUSS dialogue act layer
seeks to replicate the learnings from other well-
established taxonomies like DIT++ (Bunt, 2009) or
DAMSL (Core and Allen, 1997) wherever possible,
the QtA style of pedagogy driving our tutoring ses-
sions dictated the addition of two tutorial specific
acts: marking and revoicing. A mark act highlights
key words from the student?s speech to draw atten-
tion to a particular term or concept. Like with mark-
ing, revoicing keys in on student language, but in-
stead of highlighting specific words, a revoice act
will summarize or refine the student?s language to
bring clarity to a concept.
Rhetorical Form (22 tags): Although the dia-
logue act is useful for identifying the speaker?s in-
tent, it gives no indication of how the speaker is ad-
vancing the conversation. The rhetorical form re-
fines the dialogue act by providing a link to its sur-
face form realization. Consider the questions ?What
is the battery doing?? and ?Which one is the bat-
tery??. They would both be labeled with Ask dia-
logue acts, but they elicit two very different kinds
of responses. The former, which elicits some form
of description, would be labeled with a Describe
rhetorical form, while the latter is seeking to Iden-
tify an object. Similarly an Assert act from a tutor
could be coupled with a Describe rhetorical form to
introduce new information or with a Recap to recon-
vey a major point.
Predicate Type (19 tags): Beyond knowing the
Reliability Metric DA RF PT
Cohen?s Kappa 0.75 0.72 0.63
Exact Agreement 0.80 0.66 0.56
Partial Agreement 0.89 0.77 0.68
Table 1: Inter-annotator agreement for DISCUSS types
(DA=Dialogue Act, RF=Rhetorical Form, PT=Predicate
Type)
propositional content of an utterance, it is useful to
know how the entities and predicates in a response
relate to one another. A student may mention several
keywords that are semantically similar to the learn-
ing goals, but it is important for a tutor to recognize
whether the student?s language provides a deeper de-
scription of some phenomena or if it is simply a su-
perficial observation. The Predicate Type aims to
categorize the semantic relationships a student may
talk about; whether it is a Procedure, a Function, a
Causal Relation, or some other predicate type.
3.2.1 Annotation
All transcripts used in this experiment have been
annotated with DISCUSS labels at the turn level. A
reliability study using 15% of the transcripts was
conducted to assess inter-rater agreement of DIS-
CUSS tagging. This consisted of 18 doubly anno-
tated transcripts comprised of 828 dialogue utter-
ances.
To assess inter-rater reliability we use Cohen?s
Kappa (?) (Carletta, 1996). Because DISCUSS per-
mits multiple labels per instance, we compute a ?
value for each label and provide a mean for each
DISCUSS dimension. To get an additional sense of
agreement, we use two other metrics: exact agree-
ment and partial agreement. For each of these met-
rics, we treat each annotators? annotations as a per
class bag-of-labels. For exact agreement, each an-
notators? set of labels must match exactly to receive
credit. Partial agreement is defined as the number
of intersecting labels divided by the total number
of unique labels. Together these statistics help to
bound the reliability of the DISCUSS annotation.
Table 1 lists all three metrics broken down by DIS-
CUSS dimension. The ? values show fair agreement
for the dialogue act and rhetorical form dimensions,
whereas the predicate type shows more moderate
agreement. This difference reflects the relative diffi-
3
culty in labeling each dimension, and the agreement
as a whole illustrates the open-endedness of the task.
3.3 Question Authoring
While the long-term plan for this work is to inte-
grate fully automatic question generation into a tu-
toring system, for this study we opted to use manu-
ally authored questions. This allows us to remain
focused on learning to identify context appropri-
ate questions rather than confounding our experi-
ments with issues of question grammaticality and
well-formedness. Even though using multiple au-
thors would provide greater diversity of questions,
to avoid repeated effort and to maintain consistency
in authoring we trained a single question author
in both the FOSS material and MyST QtA tech-
niques. Although he was free to author any ques-
tion he found appropriate, our guidelines primar-
ily emphasized authoring by making permutations
aligned with DISCUSS dimensions while also per-
mitting the author to incorporate changes in word-
ing, learning-goal content, and tutoring tactics. For
example, we taught him to consider how QtA moves
such as Revoicing, Marking, or Recapping could al-
ter otherwise similar questions. To minimize the risk
of rater bias, we explicitly told our author to avoid
using positive feedback expressions such as ?Good
job!? or ?Great!?. Table 2 illustrates how the com-
binations of DISCUSS labels, QtA tactics, and dia-
logue context drives the question generation process.
To simulate the conditions available to both the
human WoZ and computer MyST tutors, the author
was presented with the entire dialogue history pre-
ceding the decision point, the current dialogue frame
(learning goal), and any visuals that may be on-
screen. Question authoring contexts were manually
selected to capture points where students provided
responses to tutor questions. This eliminated the
need to account for other dialogue behavior such as
greetings, closings, or meta-behavior, and allowed
us to focus on follow-up style questions. Because
these question authoring contexts came from actual
tutorial dialogues, we also extracted the original turn
provided by the tutor, and we filtered out turns that
did not contain questions related to the lesson con-
tent. Our corpus has 205 question authoring contexts
comprised of 1025 manually authored questions and
131 questions extracted from the original transcript
yielding 1156 questions in total.
3.4 Ratings Collection
To rate questions, we enlisted the help of four tu-
tors who had previously served as project tutors and
wizards. The raters were presented with much of
the same information used during question author-
ing. The interface included the entire dialogue his-
tory preceding the question decision point and a list
of up to 6 candidate questions (5 manually authored,
1 taken from the original transcript if applicable). To
give a more complete tutoring context, raters also
had access to the lessons? learning goals and the in-
teractive visuals used by MyST.
Previous studies in rating questions (Becker et al,
2009) have found poor inter-rater agreement when
rating questions in isolation. To decrease the task?s
difficulty we instead ask raters to simultaneously
score all candidate questions. Because we did not
want to bias raters, we did not specify specific cri-
teria for question quality. Instead we instructed the
raters to consider the question?s role in assisting stu-
dent understanding of the learning goals and to think
about factors such as tutorial pacing, context appro-
priateness, and content. Scores were collected us-
ing an ordinal 10-point scale ranging from 1 (low-
est/worst) to 10 (highest/best).
Each set of questions was rated by at least three
tutors, and rater assignments were selected to ensure
raters never score questions from sessions they tu-
tored themselves. In total we collected ratings for
1156 question representing a total of 205 question
contexts distributed across 30 transcripts.
3.4.1 Rater Agreement
Because these judgments are subjective, a key
challenge in this work centers on understanding to
what degree the tutors agree with one another. Since
our goal is to rank questions and not to score ques-
tions, we convert each tutors scores for a given con-
text into a rank-ordered list. To compute inter-
rater agreement in ranking, we use Kendall?s-Tau
(? ) rank correlation coefficient. This measure is a
non-parametric statistic that quantifies the similarity
in orderings of data, and it is closely tied to AUC,
the area under the receiver operating characteristics
(ROC) curve. Though Kendall?s-? can vary from -1
to 1, its value is highly task dependent, and it is typ-
4
. . .
T: Tell me more about what is happening with the electricity in a complete circuit.
S: Well the battery sends all the electricity in a circuit to the motor so the motor starts to go.
Candidate Question Frame Element DISCUSS
Q1 Roll over the switch and then in your own
words, tell me again what a complete or
closed circuit is all about.
Same Same Direct/Task/Visual
Ask/Describe/Configuration
Q2 How is this circuit setup? Is it open or closed? Same Same Ask/Select/Configuration
Q3 To summarize, a closed circuit allows the
electricity to flow and the motor to spin. Now
in this circuit, we have a new component. The
switch. What is the switch all about?
Diff Diff Assert/Recap/Proposition
Direct/Task/Visual
Ask/Describe/Function
Q4 You said something about the motor spinning
in a complete circuit. Tell me more about that.
Same Same Revoice/None/None
Ask/Elaborate/CausalRelation
Table 2: Example dialogue context snippet and a collection of candidate questions. The frame, element, and DISCUSS
columns show how the questions vary from one another.
ically lower when the range of possible choices is
narrow as it is in this task. To get a single score we
average ? values across all sets of questions (con-
texts) and all pairs of raters. The mean value for all
pairs of raters and contexts is ? = 0.1478. The inter-
rater statistics are shown in table 3. While inter-rater
agreement is fairly modest, we do see lots of vari-
ation between different pairs of tutors. Addition-
ally, we found that a pair of raters agreed on the top
rated question 33% of the time. This suggests that
despite their common training and experience, the
raters may be using different criteria in rating.
To assess the tutors? internal consistency, we had
each tutor re-rate 60 sets of questions approximately
two months after their first trial, and we computed
self-agreement Kendall?s-? values using the method
above. These statistics are listed in the bottom row
of table 3. In contrast with the inter-rater agreement,
self-agreement is much more consistent giving fur-
ther evidence for a difference in criteria. Together
self and inter-rater agreement help bound expected
system performance in ranking.
4 Automatic Ranking
Because we are more interested in learning to pre-
dict which questions are more suitable for a given
tutoring scenario than we are in assigning specific
scores to questions, we approach the task of ques-
tion selection as a ranking task. To create a gold-
rater A rater B rater C rater D
rater A X 0.2590 0.1418 0.0075
rater B 0.2590 X 0.1217 0.2370
rater C 0.1418 0.1217 X 0.0540
rater D 0.0075 0.2370 0.0540 X
mean 0.1361 0.2059 0.1058 0.0995
self 0.4802 0.4022 0.2327 0.3531
Table 3: Inter-rater rank agreement (Kendall?s-? ). The
bottom row is the self-agreement for contexts they rated
in two separate trials.
standard for training and evaluation we first need to
convert the collective ratings for a set of questions
into a rank-ordered list. While the most straight-
forward way to make this conversion is to average
the ratings for each item, this approach assumes all
raters operate on the same scale. Furthermore, a sin-
gle score does not account for how a question re-
lates to other candidate questions. Instead we create
a single rank-order by tabulating pairwise wins for
all pairs of questions qi, qj , (i 6= j) within a given
dialogue context C. If rating(qi) > rating(qj),
questions qi receives a win. This is summed across
all raters for the context. The question(s) with the
most wins has rank 1. Questions with an equal num-
ber of wins are considered tied and are given the av-
erage ranking of their ordinal positions. For exam-
ple if two questions are tied for second place, they
5
are each assigned a ranking of 2.5.
Using this rank-ordering we then train a pairwise
classifier to learn a preferences function (Cohen et
al., 1998) that determines if one question has a bet-
ter rank than another. For each question qi within a
contextC, we construct a vector of features ?i. For a
pair of questions qi and qj , we then create a new vec-
tor using the difference of features: ?(qi, qj , C) =
?i ? ?j . For training, if rank(qi) < rank(qj), the
classification is positive otherwise it is negative. To
account for the possibility of ties, and to make the
difference measure appear symmetric, we train both
combinations (qi, qj) and (qj , qi). During decoding,
we run the trained classifier on all pairs and tabulate
wins using the approach described above.
For our experiments we train pairwise classi-
fiers using Mallet?s Maximum Entropy (McCallum,
2002) and SVMLight?s Support Vector Machines
models (Joachims, 1999). We also use SVMRank
(Joachims, 1999), which performs the same max-
imum margin separation as SVMLight, but uses
Kendall?s-? as a loss function to optimize for rank
ordering. We run SVMRank with a linear kernel
and model parameters of c = 2.0 and  = 0.0156.
For MaxEnt, we use Mallet?s default model param-
eters. Training and evaluation are carried out us-
ing 10-fold cross validation (3 transcripts per fold,
approximately 7 dialogue contexts per transcript).
Folds are partitioned by FOSS unit, to ensure train-
ing and evaluation are on different lessons. To ex-
plore the impact of DISCUSS representations on this
question ranking task, we train and evaluate models
by incrementally adding additional information ex-
tracted from the DISCUSS annotation.
4.1 Features
When designing features for this task, we wanted to
capture the factors that may play a role in the tutor?s
decision making process during question selection.
When rating, scorers may consider factors such as
the question?s surface form, lesson relevance, con-
textual relevance. The subsections below detail the
motivations and intuitions behind these factors.
4.1.1 Surface Form Features
When presented with a list of questions, a rater
likely bases the decision on his or her initial reaction
to the questions? wording. In some cases, wording
may supercede any other decisions regarding edu-
cational value or dialogue cohesiveness. Question
verbosity is captured by the number of words in the
question feature. Analysis of rater comments also
suggested that preferences are often tied to the ques-
tion?s form and structure. A rough measure of form
comes from the Wh-word features to mark the pres-
ence of the following question words: who, what,
why, where, when, which, and how. Additionally we
use the bag-of-part-of-speech-tags (POS) features to
provide another aspect of the question?s structure.
4.1.2 Lexical Similarity Features
Past work (Ward et al, 2011a) has shown that en-
trainment, the process of automatic alignment be-
tween dialogue partners, is a useful predictor of
learning and is a key factor in facilitating a success-
ful conversation. For question selection, we hypoth-
esize that successful tutors ask questions that dis-
play some degree of semantic entrainment with stu-
dent utterances. In MyST-based tutoring, dialogue
actions are driven by the goal of eliciting student re-
sponses that address the learning goals for the les-
son. Consequently, choosing an appropriate ques-
tion may depend on how closely student responses
align with the learning goals. To model both en-
trainment and lexical similarity we extract features
for unigram and bigram overlap of words, word-
lemmas, and part-of-speech tags between the pairs
below.
? The candidate question and the student?s last
utterance
? The candidate question and the last tutor?s ut-
terance
? The candidate question and the text of the cur-
rent learning goal
? The candidate question and the text of the other
learning goals
Example learning goals for a lesson on circuits are
provided in table 4. The current learning goal is sim-
ply the learning goal in focus at the point of question
asking according to the MyST logfile. Other learn-
ing goals are all other goals for the lesson. Using
the example from the table, if goal 2 is the current
learning goal, then goals 1 and 3 are the other goals.
6
Goal 1: Wires carry electricity and can connect
components
Goal 2: Bulb receives electricity and transforms
electricity into heat
Goal 3: A circuit provides a pathway for energy
to flow
Table 4: Example learning goals
4.1.3 DISCUSS Features
The lexical and surface form features provide
some cues about the content of the question, but
they do not account for the action or intent in tutor-
ing. The DISCUSS annotation allows us to bridge
between the question?s semantics and pragmatically
and focus on what differentiates one question from
another. Basic DISCUSS features include bags of
Dialogue Acts (DA), Rhetorical Forms (RF), and
Predicate types (PT) found in the question?s DIS-
CUSS annotation. We capture the question?s dia-
logue cohesiveness with binary features indicating
whether or not the question?s RF and PT match those
found in the previous student and tutor turns.
4.1.4 Contextualized DISCUSS Features
In tutoring, follow-up questions are licensed by
the questions that precede them. For example a tutor
may be less likely to ask how an object functions un-
til after the object has first been identified by the stu-
dent. Along a different dimension, a tutor?s line of
questioning may change to match a student?s under-
standing of the material. Struggling students may re-
quire additional opportunities to explain themselves,
while advanced students may benefit more from a
more rapid pace of instruction.
We model the conditional relevance of moves
by computing dialogue act transition probabilities
from our corpus of DISCUSS annotated tutorial di-
alogues. Although DISCUSS allows multiple tags
per dialogue turn, we simplify probability calcula-
tions by treating each DISCUSS tuple as a separate
event, and tallying all pairs of turn-turn labels. A
DISCUSS tuple consists of a Dialogue Act (DA),
Rhetorical Form (RF), and Predicate Type (PT),
and we use different subsets of the tuple to com-
pute the transition probabilities listed in equations 1-
3. All probabilities are computed using Laplace-
smoothing. When extracting features, we sum the
log of the probabilities for each DISCUSS label
present in the question.
MyST models dialogue as a sequence of seman-
tic frames which correspond to specific learning
goals. For natural language understanding, MyST
uses Phoenix semantic grammars (Ward, 1994) to
identify which elements within these frames have
been filled. To account for student progress in ques-
tion asking, we compute the conditional probabil-
ity of a DISCUSS label given the percentage of el-
ements filled in the current dialogue frame (equa-
tion 4). This progress percentage is discretized into
bins of 0-25%, 25-50%, 50-75%, and 75-100%.
p(DA,RF, PTquestion|DA,RF, PTstud. turn) (1)
p(DA,RFquestion|DA,RFstudent turn) (2)
p(PTquestion|PTstudent turn) (3)
p(DA,RF, PTques.|% elements filled) (4)
4.2 Evaluation
To evaluate our systems? performance in ranking,
we use two measures commonly used in information
retrieval: the Mean Kendall?s-? measure described
in section 3.4.1 and Mean Reciprocal Rank (MRR).
MRR is the average of the multiplicative inverse of
the rank of the highest ranking question across all
contexts. To account for ties we use the Tau-b vari-
ant of Kendall?s-? , and for MRR we compute re-
ciprocal rank by averaging the system rankings for
all of the questions tied for first. To obtain a gold-
standard ranking for comparison, we combine indi-
vidual raters? ratings using the approached described
in section 4.
5 Results and Discussion
We trained several models to investigate how differ-
ent feature classes influence overall performance in
ranking. The results for these experiments are listed
in Table 5. Because we found comparable perfor-
mance between MaxEnt and SVMLight, we only
report results for MaxEnt and SVMRank models.
In addition to MRR and Kendall?s-? , we list the
number of concordances and discordances in pair-
wise classification to give the reader another sense
of the accuracy associated with rank agreement.
Random Baseline: On average, assigning ran-
dom ranks will yield mean ?=0 and MRR=0.408.
7
Model Features Mean Num. Num. Pairwise MRR
Kendall?s-? Concord. Discord. Accuracy
MaxEnt CONTEXT+DA+PT+MATCH+POS- 0.211 1560 974 0.616 0.516
SVMRank CONTEXT+DA+PT+MATCH+POS- 0.190 1725 1154 0.599 0.555
MaxEnt CONTEXT+DA+RF+PT+MATCH+POS- 0.185 1529 1014 0.601 0.512
MaxEnt DA+RF+PT+MATCH+POS- 0.179 1510 1009 0.599 0.503
MaxEnt DA+RF+PT+MATCH+ 0.163 1506 1044 0.591 0.485
MaxEnt DA+RF+PT+ 0.147 1500 1075 0.583 0.480
MaxEnt DA+RF+ 0.130 1458 1082 0.574 0.476
MaxEnt DA+ 0.120 1417 1076 0.568 0.458
SVMRank Baseline 0.108 1601 1278 0.556 0.473
MaxEnt Baseline 0.105 1410 1115 0.558 0.448
Table 5: System scores by feature set and and machine learning model. Presence or absence of specific features is
denoted with a ?+? or ?-? otherwise the label refers to a set of features. The Baseline features consist of the Surface Form
and Lexical Similarity features described in sections 4.1.1 and 4.1.2. POS are the bag-of-POS surface form features.
DA, RF, and PT refer to the DISCUSS presence features for the Dialogue Act, Rhetorical Form, and Predicate Type
dimensions described in section 4.1.3. MATCH refers specifically to the RF and PT match features. CONTEXT
refers to the Contextualized DISCUSS features described in section 4.1.4. The best scores for each column appear in
boldface.
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.00
10
20
30
40
50
Freq
uen
cy
?mean=0.211
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.0Kendall's Tau(?) Range
0
10
20
30
40
50
Freq
uen
cy
?mean=0.105
Figure 1: Distribution of per-context Kendall?s-? values
for the top-scoring system (top), and the baseline system
(bottom).
Baseline System: Our baseline system used all
of the surface form and lexical similarity features
described above. This set of features achieves the
highest rank agreement (? = 0.105) using max-
imum entropy and the highest MRR (0.473) with
SVMRank . This improvement over the random
baseline suggests there is a correlation between a
question?s ranking and its surface form.
DISCUSS System: Table 5 shows system per-
formance steadily improves as additional DISCUSS
features are included in the model. When us-
1 2 3 4 5 6 70.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=1.80
1 2 3 4 5 6 7Mean System Rank
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=2.11
Figure 2: Distribution of per-context system ranks for the
highest rated question for the top-scoring system (top),
and the baseline system (bottom). These ranks are the
inverse of the reciprocal rank used to calculate MRR.
ing DISCUSS features, removing the part-of-speech
features gives an additional bump in performance
suggesting that there is an overlap in information
between DISCUSS representations and POS tags.
Finally, adding contextualized DISCUSS features
pushes our ranking models to their highest level
of agreement with ? = 0.211 using MaxEnt and
MRR=0.555 using SVMRank . Inspection of the
MRR values shows that without taking into account
the possibility of ties the baseline system selects
8
the top-ranked question in 44/205 (21.4%) contexts.
While the system with the best MRR score, correctly
chooses the top-ranked question in 71/205 (34.6%)
contexts ? a rate comparable to how often a pair of
raters agreed on the number-one item (33.4%).
Application of the Wilcoxon signed-rank test
shows the DISCUSS system exhibits statistically
significant improvement over the baseline system in
its distribution of Kendall?s-? values (n = 205, z =
7350, p < 0.001) and distribution of reciprocal
ranks (n = 205, z = 3739, p < 0.001). Figures 1
and 2 give visual confirmation of this improvement,
and highlight the overall reduction in negative ? val-
ues as well as the greater-than-50% increase in like-
lihood of selecting the best question first.
To get another perspective on system perfor-
mance, we evaluated our human raters on the gold-
standard rankings from the subset of questions used
for assessing internal agreement. This yielded a
mean ? between 0.2589 and 0.3619. If we remove
ratings so that the gold-standard does not include the
rater under evaluation, tutor performance drops to
a range of 0.1523 to 0.2432, which is roughly cen-
tered around the agreement exhibited by our best-
performing system.
Looking at the impact of learning algorithms
we see that SVMRank tends to perform better on
MRR while the pairwise maximum entropy mod-
els yield higher ? ?s. One possible explanation for
this discrepancy may stem from the ranking algo-
rithms? different treatment of ties. The pairwise
model permits ties, whereas the scores produced by
SVMRank produce a strict order. Without ties, it is
difficult to exactly match the raters? orderings which
had numerous ties, which can in turn produce an
overall higher number of concordances and discor-
dances than the pairwise classification model.
6 Conclusions and Future Work
We have introduced a framework for learning and
evaluating models for ranking and selecting ques-
tions for a given point in a tutorial dialogue. Fur-
thermore these experiments show that it is feasible
to learn this behavior by coupling predefined ques-
tions with ratings from trained tutors. Supplement-
ing our baseline surface form and lexical similarity
features with additional features extracted from the
dialogue context and DISCUSS dialogue act anno-
tation improves system performance in ranking to a
level on par with expert human tutors. These results
illustrate how question asking depends not only on
the form of the question but also on the underlying
dialogue action, function and content.
In the near future we plan to train models on indi-
vidual tutors to investigate which factors drive in-
dividual preferences in question asking. We also
plan to characterize system performance using auto-
matically labeled DISCUSS annotation. Lastly, we
feel these results provide a natural starting point to
explore automatic generation of questions from the
DISCUSS dialogue move representation.
Acknowledgments
This work was supported by grants from the
NSF (DRL-0733322, DRL-0733323), the IES
(R3053070434) and the DARPA GALE program
(Contract No. HR0011-06-C-0022, a supplement
for VerbNet attached to the subcontract from the
BBN-AGILE Team). Any findings, recommenda-
tions, or conclusions are those of the author and do
not necessarily represent the views of NSF, IES, or
DARPA.
References
Manish Agarwal and Prashanth Mannem. 2011. Auto-
matic gap-fill question generation from text books au-
tomatic gap-fill question generation from text books
automatic gap-fill questions from text books. In Pro-
ceedings of the Sixth Workshop on Innovative Use of
NLP for Building Educational Applications.
I. L. Beck, M. G. McKeown, J. Worthy, C. A. San-
dora, and L. Kucan. 1996. Questioning the au-
thor: A year-long classroom implementation to engage
students with text. The Elementary School Journal,
96(4):387?416.
L. Becker, R. D. Nielsen, and W. Ward. 2009. What a
pilot study says about running a question generation
challenge. In Proceedings of the Second Workshop on
Question Generation, Brighton, England, July.
L. Becker, W. Ward, S. van Vuuren, and M. Palmer. 2011.
Discuss: A dialogue move taxonomy layered over se-
mantic representations. In In Proceedings of the In-
ternational Conference on Computational Semantics
(IWCS) 2011, Oxford, England, January 12-14.
K.E. Boyer, E.Y. Ha, M. Wallis, R. Phillips, M.A. Vouk,
and J.C. Lester. 2009a. Discovering tutorial dialogue
9
strategies with hidden markov models. In Proceed-
ings of the 14th International Conference on Artificial
Intelligence in Education (AIED ?09), pages 141?148,
Brighton, U.K.
K.E. Boyer, W.J. Lahti, R. Phillips, M. D. Wallis, M. A.
Vouk, and J. C. Lester. 2009b. An empirically derived
question taxonomy for task-oriented tutorial dialogue.
In Proceedings of the Second Workshop on Question
Generation, pages 9?16, Brighton, U.K.
M. Buckley and M. Wolska. 2008. A classification of
dialogue actions in tutorial dialogue. In Proceedings
of COLING 2008, pages 73?80. ACL.
H. C. Bunt. 2009. The DIT++ taxonomy for functional
dialogue markup. In Proc. EDAML 2009.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):pp. 249?254.
M. Chi, P. Jordan, K. VanLehn, and M. Hall. 2008. Re-
inforcement learning-based feature selection for devel-
oping pedagogically effective tutorial dialogue tactics.
In Ryan S. Baker, Tiffany Barnes, and Joseph Becker,
editors, Proceedings of the 1st International Confer-
ence on Educational Data Mining, pages pp258?265.
M. Chi, P. W. Jordan, K. VanLehn, and D. J. Litman.
2009. To elicit or to tell: Does it matter? In Artifi-
cial Intelligence in Education, pages 197?204.
M. Chi, K. VanLehn, and D. Litman. 2010. Do micro-
level tutorial decisions matter: Applying reinforce-
ment learning to induce do micro-level tutorial deci-
sions matter. In Vincent Aleven, Judy Kay, and Jack
Mostow, editors, Preceedings of the 10th Internation
Confernce on Intelligent Tutoring Systems (ITS 2010).
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1998. Learning to order things. In Advances
in Neural Information Processing Systems 10 (NIPS
1998).
A. Collins and A. Stevens. 1982. Goals and methods for
inquiry teachers. Advances in Instructional Psychol-
ogy, 2.
M. G. Core and J.F. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In AAAI Fall Symposium,
pages 28?35.
A.C. Graesser and N.K. Person. 1994. Question ask-
ing during tutoring. American Educational Research
Journal, 31:104?137.
A.C. Graesser, P. Chipman, B.C Haynes, and A. Olney.
2005. Autotutor: An intelligent tutoring system with
mixed-initiative dialogue. IEEE Transactions in Edu-
cation, 48:612?618.
M. Heilman and N. A. Smith. 2010. Good question! sta-
tistical ranking for question generation. In Proceed-
ings of NAACL/HLT 2010.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
J.H. Kim, M. Glass, R. Freedman, and M.W. Evens.
2000. Learning the use of discourse markers in tuto-
rial dialogue learning the use of discourse markers in
tutorial dialogue. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society.
D. Litman and S. Silliman. 2004. Itspoke: An intel-
ligent tutoring spoken dialogue system. In Compan-
ion Proceedings of the Human Language Technology
Conference: 4th Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
W.C. Mann and S.A Thompson. 1986. Rhetorical struc-
ture theory: Description and construction of text struc-
tures. In In Proceedings of the Third International
Workshop on Text Generation, August.
A. K. McCallum, 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
R. D. Nielsen, J. Buckingham, G. Knoll, B. Marsh, and
L. Palen. 2008. A taxonomy of questions for ques-
tion generation. In Proceedings of the Workshop on
the Question Generation Shared Task and Evaluation
Challenge, September.
R.M. Pilkington. 1999. Analysing educational dis-
course: The discount scheme. Technical Report 99/2,
Computer Based Learning Unit, University of Leeds.
Owen Rambow, Monica Rogati, and Marilyn A. Walker.
2001. Evaluating a trainable sentence planner for a
spoken dialogue system evaluating a trainable sen-
tence planner for a spoken dialogue system. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL 2001).
C.P. Rose, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. A comparative evalu-
ation of socratic versus didactic tutoring. In Proceed-
ings of Cognitive Sciences Society.
C.P. Rose, D. Bhembe, S. Siler, R. Srivastava, and
K. VanLehn. 2003. The role of why questions in ef-
fective human tutoring. In Proceedings of Artificial
Intelligence in Education (AIED 2003).
D. Tsovaltzi and E. Karagjosova. 2004. A view on dia-
logue move taxonomies for tutorial dialogues. In Pro-
ceedings of SIGDIAL 2004, pages 35?38. ACL.
D. Tsovaltzi and C. Matheson. 2001. Formalising hint-
ing in tutorial dialogues. In In EDILOG: 6th workshop
on the semantics and pragmatics of dialogue, pages
185?192.
K. VanLehn, A.C. Graesser, G.T. Jackson, P. Jordan,
A. Olney, and C.P. Rose. 2007. When are tutorial
dialogues more effective than reading? Cognitive Sci-
ence, 31(1):3?62.
10
Marilyn A. Walker, Owen Rambow, and Monica Rogati.
2001. SPOT: A trainable sentence planner. In Pro-
ceedings of the North American Meeting of the Asso-
ciation for Computational Linguistics (NAACL).
A. Ward, D. Litman, and M. Eskenazi. 2011a. Predict-
ing change in student motivation by measuring cohe-
sion between predicting change in student motivation
by measuring cohesion between tutor and student. In
Proceedings of the Sixth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
136?141.
W. Ward, R. Cole, D. Bolan?os, C. Buchenroth-Martin,
E. Svirsky, S. van Vuuren, T. Weston, J. Zheng, and
L. Becker. 2011b. My science tutor: A conversa-
tional multi-media virtual tutor for elementary school
science. ACM Transactions on Speech and Language
Processing (TSLP), 7(4), August.
W. Ward. 1994. Extracting information from sponta-
neous speech. In Proceedings of the International
Conference on Speech and Language Processing (IC-
SLP).
Xuchen Yao. 2010. Question generation with minimal
recursion semantics. Master?s thesis, Saarland Uni-
versity.
11
