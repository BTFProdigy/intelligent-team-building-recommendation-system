Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 391?398,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Spontaneous Speech Understanding for Robust Multi-Modal
Human-Robot Communication
Sonja Hu?wel, Britta Wrede
Faculty of Technology, Applied Computer Science
Bielefeld University, 33594 Bielefeld, Germany
shuewel,bwrede@techfak.uni-bielefeld.de
Abstract
This paper presents a speech understand-
ing component for enabling robust situated
human-robot communication. The aim is
to gain semantic interpretations of utter-
ances that serve as a basis for multi-modal
dialog management also in cases where
the recognized word-stream is not gram-
matically correct. For the understand-
ing process, we designed semantic pro-
cessable units, which are adapted to the
domain of situated communication. Our
framework supports the specific character-
istics of spontaneous speech used in com-
bination with gestures in a real world sce-
nario. It also provides information about
the dialog acts. Finally, we present a pro-
cessing mechanism using these concept
structures to generate the most likely se-
mantic interpretation of the utterances and
to evaluate the interpretation with respect
to semantic coherence.
1 Introduction
Over the past years interest in mobile robot ap-
plications has increased. One aim is to allow for
intuitive interaction with a personal robot which is
based on the idea that people want to communi-
cate in a natural way (Breazeal et al, 2004)(Daut-
enhahn, 2004). Although often people use speech
as the main modality, they tend to revert to addi-
tional modalities such as gestures and mimics in
face-to-face situations. Also, they refer to objects
1This work has been supported by the European Union
within the ?Cognitive Robot Companion? (COGNIRON)
project (FP6-IST-002020) and by the German Research
Foundation within the Graduate Program ?Task Oriented
Communication?.
in the physical environment. Furthermore, speech,
gestures and information of the environment are
used in combination in instructions for the robot.
When participants perceive a shared environment
and act in it we call this communication ?situated?
(Milde et al, 1997). In addition to these features
that are characteristic for situated communication,
situated dialog systems have to deal with several
problems caused by spontaneous speech phenom-
ena like ellipses, indirect speech acts or incom-
plete sentences. Large pauses or breaks occur in-
side an utterance and people tend to correct them-
selves. Utterances often do not follow a standard
grammar as written text.
Service robots have not only to be able to cope
with this special kind of communication but they
also have to cope with noise that is produced by
their own actuators or the environment. Speech
recognition in such scenarios is a complex and dif-
ficult task, leading to severe degradations of the
recognition performance. The goal of this paper
is to present a framework for human-robot inter-
action (HRI) that enables robust interpretation of
utterances under the specific conditions in HRI.
2 Related Work
Some of the most explored speech processing
systems are telephone-based information systems.
Their design rather differs from that of situated
HRI. They are uni-modal so that every information
has to be gathered from speech. However, speech
input is different as users utter longer phrases
which are generally grammatically correct. These
systems are often based on a large corpus and can
therefore be well trained to perform satisfactory
speech recognition results. A prominent example
for this is the telephone based weather forecast in-
formation service JUPITER (Zue et al, 2000).
391
Over the past years interest increased in mo-
bile robot applications where the challenges are
even more complex. While many of these prob-
lems (person tracking, attention, path finding) are
already in the focus of research, robust speech un-
derstanding has not yet been extensively explored
in the context of HRI. Moreover, interpretation
of situated dialogs in combination with additional
knowledge sources is rarely considered. Recent
projects with related scope are the mobile robots
CARL (Lopes et al, 2005) and ALBERT (Ro-
galla et al, 2002), and the robotic chandelier Elvis
(Juster and Roy, 2004). The main task of the robot
CARL is robust language understanding in con-
text of knowledge acquisition and management.
It combines deep and shallow parsing to achieve
robustness. ALBERT is designed to understand
speech commands in combination with gestures
and object detection with the task to handle dishes.
The home lighting robot Elvis gets instructions
about lighting preferences of a user via speech and
gestural input. The robot itself has a fixed position
but the user may walk around in the entire room.
It uses keyword spotting to analyze the semantic
content of speech. As speech recognition in such
robot scenarios is a complex and difficult task, in
these systems the speech understanding analysis
is constrained to a small set of commands and not
oriented towards spontaneous speech. However,
deep speech understanding is necessary for more
complex human robot interaction.
There is only little research in semantic speech
analysis of spontaneous speech. A widely used ap-
proach of interpreting sentences is the idea of case
grammar (Bruce, 1975). Each verb has a set of
named slots, that can be filled by other slots, typ-
ically nouns. Syntactic case information of words
inside a sentence marks the semantic roles and
thus, the corresponding slots can be filled. An-
other approach of processing spontaneous speech
by using semantic information for the Air Travel
Information Service (ATIS) task is implemented
in the Phoenix system (Ward, 1994). Slots in
frames represent the basic semantic entities known
to the system. A parser using semantic gram-
mars maps input onto these frame representations.
The idea of our approach is similar to that of the
Phoenix system, in that we also use semantic en-
tities for extracting information. Much effort has
been made in the field of parsing strategies com-
bined with semantic information. These systems
support preferably task oriented dialog systems,
e.g., the ATIS task as in (Popescu et al, 2004)
and (Milward, 2000), or virtual world scenarios
(Gorniak and Roy, 2005), which do not have to
deal with uncertain visual input. The aim of the
FrameNet project (Fillmore and Baker, 2001) is to
create a lexicon resource for English, where every
entry receives a semantic frame description.
In contrast to other presented approaches we fo-
cus on deep semantic analysis of situated sponta-
neous speech.Written language applications have
the advantage to be trainable on large corpora,
which is not the case for situated speech based ap-
plications. And furthermore, interpretation of sit-
uated speech depends on environmental informa-
tion. Utterances in this context are normally less
complex, still our approach is based on a lexicon
that allows a broad variety of utterances. It also
takes speech recognition problems into account
by ignoring non-consistent word hypotheses and
scoring interpretations according to their semantic
completeness. By adding pragmatic information,
natural dialog processing is facilitated.
3 Situated Dialog Corpus
With our robot BIRON we want to improve so-
cial and functional behavior by enabling the sys-
tem to carry out a more sophisticated dialog for
handling instructions. One scenario is a home-tour
where a user is supposed to show the robot around
the home. Another scenario is a plant-watering
task, where the robot is instructed to water differ-
ent plants. There is only little research on multi-
modal HRI with speech-based robots. A study
how users interact with mobile office robots is re-
ported in (Hu?ttenrauch et al, 2003). However, in
this evaluation, the integration of different modal-
ities was not analyzed explicitly. But even though
the subjects were not allowed to use speech and
gestures in combination, the results support that
people tended to communicate in a multi-modal
way, nevertheless.
To receive more detailed information about the
instructions that users are likely to give to an as-
sistant in home or office we simulated this sce-
nario and recorded 14 dialogs from German native
speakers. Their task was to instruct the robot to
water plants. Since our focus in this stage of the
development of our system lies on the situatedness
of the conversation, the robot was simply replaced
by a human pretending to be a robot. The subjects
392
were asked to act as if it would be a robot. As pro-
posed in (Lauriar et al, 2001), a preliminary user
study is necessary to reduce the number of repair
dialogs between user and system, such as queries.
The corpus provides data necessary for the design
of the dialog components for multi-modal interac-
tion. We also determined the lexicon and obtained
the SSUs that describe the scene and tasks for the
robot.
The recorded dialogs feature the specific na-
ture of dialog situations in multi-modal commu-
nication situations. The analysis of the corpus is
presented in more detail in (Hu?wel and Kummert,
2004). It confirms that spontaneously spoken ut-
terances seldom respect the standard grammar and
structure of written sentences. People tend to use
short phrases or single words. Large pauses of-
ten occur during an utterance or the utterance is
incomplete. More interestingly, the multi-modal
data shows that 13 out of 14 persons used pointing
gestures in the dialogs to refer to objects. Such ut-
terances cannot be interpreted without additional
information of the scene. For example, an utter-
ance such as ?this one? is used with a pointing
gesture to an object in the environment. We re-
alize, of course, that for more realistic behavior
towards a robot a real experiment has to be per-
formed. However this time- and resource-efficient
procedure allowed us to build a system capable of
facilitating situated communication with a robot.
The implemented system has been evaluated with
a real robot (see section 7). In the prior version we
used German as language, now the dialog system
has adapted to English.
4 The Robot Assistant BIRON
The aim of our project is to enable intuitive inter-
action between a human and a mobile robot. The
basis for this project is the robot system BIRON
(et. al, 2004). The robot is able to visually track
persons and to detect and localize sound sources.
Generation
Language
Recognition
Gesture
Object
Recognition
Object 
Attention
System SceneModel
lexicon
 + SSU
database
fusion
engine
Understanding
Speech
Robot
Control
Manager
DialogSpeechRecognition
history
Figure 1: Overview of the BIRON dialog system
architecture
The robot expresses its focus of attention by turn-
ing the camera into the direction of the person
currently speaking. From the orientation of the
person?s head it is deduced whether the speaker
addresses the robot or not. The main modality
of the robot system is speech but the system can
also detect gestures and objects. Figure 1 gives
an overview of the architecture of BIRON?s multi-
modal interaction system. For the communica-
tion between these modules we use an XML based
communication framework (Fritsch et al, 2005).
In the following we will briefly outline the inter-
acting modules of the entire dialog system with
the speech understanding component.
Speech recognition: If the user addresses
BIRON by looking in its direction and starting to
speak, the speech recognition system starts to an-
alyze the speech data. This means that once the
attention system has detected that the user is prob-
ably addressing the robot it will route the speech
signal to the speech recognizer. The end of the
utterance is detected by a voice activation detec-
tor. Since both components can produce errors the
speech signal sent to the recognizer may contain
wrong or truncated parts of speech. The speech
recognition itself is performed with an incremen-
tal speaker-independent system (Wachsmuth et al,
1998), based on Hidden Markov Models. It com-
bines statistical and declarative language models
to compute the most likely word chain.
Dialog manager: The dialog management
serves as the interface between speech analysis
and the robot control system. It also generates an-
swers for the user. Thus, the speech analysis sys-
tem transforms utterances with respect to gestural
and scene information, such as pointing gestures
or objects in the environment, into instructions for
the robot. The dialog manager in our application is
agent-based and enables a multi-modal, mixed ini-
393
tiative interaction style (Li et al, 2005). It is based
on semantic entities which reflect the information
the user uttered as well as discourse information
based on speech-acts. The dialog system classifies
this input into different categories as e.g., instruc-
tion, query or social interaction. For this purpose
we use discourse segments proposed by Grosz and
Sidner (Grosz and Sidner, 1986) to describe the
kind of utterances during the interaction. Then the
dialog manager can react appropriately if it knows
whether the user asked a question or instructed
the robot. As gesture and object detection in our
scenario is not very reliable and time-consuming,
the system needs verbal hints of scene information
such as pointing gestures or object descriptions to
gather information of the gesture detection and ob-
ject attention system.
5 Situated Concept Representations
Based on the situated conversational data, we de-
signed ?situated semantic units? (SSUs) which are
suitable for fast and automatic speech understand-
ing. These SSUs basically establish a network of
strong (mandatory) and weak (optional) relations
of sematic concepts which represent world and
discourse knowledge. They also provide ontolog-
ical information and additional structures for the
integration of other modalities. Our structures are
inspired by the idea of frames which provide se-
mantic relations between parts of sentences (Fill-
more, 1976).
Till now, about 1300 lexical entries are stored
in our database that are related to 150 SSUs. Both
types are represented in form of XML structures.
The lexicon and the concept database are based on
our experimental data of situated communication
(see section 3) and also on data of a home-tour
scenario with a real robot. This data has been an-
notated by hand with the aim to provide an ap-
propriate foundation for human-robot interaction.
It is also planned to integrate more tasks for the
robot as, e.g., courier service. This can be done by
only adding new lexical entries and correspond-
ing SSUs without spending much time in reorga-
nization. Each lexical entry in our database con-
tains a semantic association to the related SSUs.
Therefore, equivalent lexical entries are provided
for homonyms as they are associated to different
concepts.
In figure 2 the SSU Showing has an open link
to the SSUs Actor and Object. Missing links to
Instruction
Object
   Actor
top   
opt?frames   
   Time
mand?frames
   Person_involved
SSU  Showing
Figure 2: Schematic SSU ?Showing? for utter-
ances like ?I show you my poster tomorrow?.
strongly connected SSUs are interpreted as miss-
ing information and are thus indicators for the di-
alog management system to initiate a clarification
question or to look for information already stored
in the scene model (see fig. 1). The SSUs also
have connections to optional arguments, but they
are less important for the entire understanding pro-
cess.
The SSUs also include ontological information,
so that the relations between SSUs can be de-
scribed as general as possible. For example, the
SSU Building subpart is a sub-category of Object.
In our scenario this is important as for example the
unit Building subpart related to the concept?wall?
has a fixed position and can be used as navigation-
support in contrast to other objects. The top-
category is stored in the entry top, a special item
of the SSU. By the use of ontological information,
SSUs also differentiate between task and commu-
nication related information and thereby support
the strategy of the dialog manager to decouple task
from communication structure. This is important
in order to make the dialog system independent
of the task and enable scalable interaction capa-
bilities. For example the SSU Showing belongs to
the discourse type Instruction. Other types impor-
tant for our domain are Socialization, Description,
Conrmation, Negation, Correction, and Query.
Further types may be included, if necessary.
In our domain, missing information in an utter-
ance can often be acquired from the scene. For
example the utterance ?look at this? and a point-
ing gesture to a table will be merged to the mean-
ing ?look at the table?. To resolve this meaning,
we use hints of co-verbal gestures in the utter-
ance. Words as ?this one? or ?here? are linked
to the SSU Potential gesture, indicating a relation
between speech and gesture. The timestamp of the
utterance enables temporal alignment of speech
and gesture. Since gesture recognition is expen-
sive in computing time and often not well-defined,
such linguistic hints can reduce these costs dra-
394
matically.
The utterance ?that? can also represent an
anaphora, and is analyzed in both ways, as
anaphora and as gesture hint. Only if there is no
gesture, the dialog manager will decide that the
word probably was used in an anaphoric manner.
Since we focus on spontaneous speech, we can-
not rely on the grammar, and therefore the se-
mantic units serve as the connections between the
words in an utterance. If there are open connec-
tions interpretable as missing information, it can
be inferred what is missing and be integrated by
the contextual knowledge. This structure makes
it easy to merge the constituents of an utterance
solely by semantic relations without additional
knowledge of the syntactic properties. By this,
we lose information that might be necessary in
several cases for disambiguation of complex ut-
terances. However, spontaneous speech is hard
to parse especially since speech recognition errors
often occur on syntactically relevant morphemes.
We therefore neglect the cases which tend to occur
very rarely in HRI scenarios.
6 Semantic Processing
In order to generate a semantic interpretation of
an utterance, we use a special mechanism, which
unifies words of an utterance into a single struc-
ture. The system also considers the ontological in-
formation of the SSUs to generate the most likely
interpretation of the utterance. For this purpose,
the mechanism first associates lexical entries of
all words in the utterance with the corresponding
SSUs. Then the system tries to link all SSUs to-
gether into one connected uniform. Some SSUs
provide open links to other SSUs, which can be
filled by semantic related SSUs. The SSU Be-
side for example provides an open link to Object.
This SSU can be linked to all Object entities and
to all subtypes of Object. Thus, an utterance as
?next to the door? can be linked together to form
a single structure (see fig. 3). The SSUs which
possess open links are central for this mechanism,
they represent roots for parts of utterances. How-
ever, these units can be connected by other roots,
likewise to generate a tree representing semantic
relations inside an utterance.
The fusion mechanism computes in its best case
in linear time and in worst case in square time.
A scoring function underlies the mechanism: the
more words can be combined, the better is the rat-
ontological link
strong reference
lexical mapping
Building_subpart
"next to   the door" 
Beside
Object
Figure 3: Simplied parse tree example .
ing. The system finally chooses the structure with
the highest score. Thus, it is possible to handle se-
mantic variations of an utterance in parallel, such
as homonyms. Additionally, the rating is help-
ful to decide whether the speech recognition result
is reliable or not. In this case, the dialog man-
ager can ask the user for clarification. In the next
version we will use a more elaborate evaluation
technique to yield better results such as rating the
amount of concept-relations and missing relations,
distinguish between important and optional rela-
tions, and prefer relations to words nearby.
A converter forwards the result of the mech-
anism as an XML-structure to the dialog man-
ager. A segment of the result for the dialog man-
ager is presented in Figure 4. With the category-
descriptions the dialog-module can react fast on
the user?s utterance without any further calcula-
tion. It uses them to create inquiries to the user
or to send a command to the robot control system,
such as ?look for a gesture?, ?look for a blue ob-
ject?, or ?follow person?. If the interpreted utter-
ance does not fit to any category it gets the value
fragment. These utterances are currently inter-
preted in the same way as partial understandings
and the dialog manager asks the user to provide
more meaningful information.
Figure 1 illustrates the entire architecture of the
speech understanding system and its interfaces to
other modules. The SSUs and the lexicon are
stored in an external XML-databases. As the
speech understanding module starts, it first reads
these databases and converts them into internal
data-structures stored in a fast accessible hash ta-
ble. As soon as the module receives results from
speech recognition, it starts to merge. The mech-
anism also uses a history, where former parts of
utterances are stored and which are also integrated
in the fusing mechanism. The speech understand-
ing system then converts the best scored result into
a semantic XML-structure (see Figure 4) for the
dialog manager.
395
<metaInfo>
<time>1125573609635</time>
<status>full</status>
</metaInfo>
<semanticInfo>
<u>what can you do</u>
<category>query</category>
<content>
<unit = Question_action>
<name>what</name>
<unit = Action>
<name>do</name>
<unit = Ability>
<name>can</name>
<unit = Proxy>
<name>you</name>
...
<u>this is a green cup</u>
<category>description</category>
<content>
<unit = Existence>
<name>is</name>
<unit = Object_kitchen>
<name>cup</name>
<unit = Potential_gesture>
<name>this</name>
</unit>
<unit = Color>
<name>green</name>
</unit>
...
Figure 4: Two segments of the speech understand-
ing results for the utterances ?what can you do?
and ?this is a green cup?.
6.1 Situated Speech Processing
Our approach has various advantages dealing with
spontaneous speech. Double uttered words as in
the utterance ?look - look here? are ignored in our
approach. The system still can interprete the ut-
terance, then only one word is linked to the other
words. Corrections inside an utterance as ?the left
em right cube? are handled similar. The system
generates two interpretations of the utterance, the
one containing left the other right. The system
chooses the last one, since we assume that cor-
rections occur later in time and therefore more
to the right. The system deals with pauses in-
side utterances by integrating former parts of ut-
terances stored in the history. The mechanism also
processes incomplete or syntactic incorrect utter-
ances. To prevent sending wrong interpretations to
the dialog-manager the scoring function rates the
quality of the interpretation as described above. In
our system we also use scene information to eval-
uate the entire correctness so that we do not only
have to rely on the speech input. In case of doubt
the dialog-manager requests to the user.
For future work it is planned to integrate addi-
tional information sources, e.g., inquiries of the
dialog manager to the user. The module will also
User1: Robot look - do you see?
This - is a cow. Funny.
Do you like it? ...
User2: Look here robot - a cup.
Look here a - a keyboard.
Let?s try that one. ...
User3: Can you walk in this room?
Sorry, can you repeat your answer?
How fast can you move? ...
Figure 5: Excerptions of the utterances during the
experiment setting.
store these information in the history which will be
used for anaphora resolution and can also be used
to verify the output of the speech recognition.
7 Evaluation
For the evaluation of the entire robot system
BIRON we recruited 14 naive user between 12
and 37 years with the goal to test the intuitive-
ness and the robustness of all system modules as
well as its performance. Therefore, in the first of
two runs the users were asked to familiarize them-
selves with the robot without any further informa-
tion of the system. In the second run the users
were given more information about technical de-
tails of BIRON (such as its limited vocabulary).
We observed similar effects as described in section
2. In average, one utterance contained 3.23 words
indicating that the users are more likely to utter
short phrases. They also tend to pause in the mid-
dle of an utterance and they often uttered so called
meta-comments such as ?that?s fine?. In figure 5
some excerptions of the dialogs during the experi-
ment settings are presented.
Thus, not surprisingly the speech recognition
error rate in the first run was 60% which decreased
in the second run to 42%, with an average of 52%.
High error rate seems to be a general problem in
settings with spontaneous speech as other systems
also observed this problem (see also (Gorniak and
Roy, 2005)). But even in such a restricted exper-
iment setting, speech understanding will have to
deal with speech recognition error which can never
be avoided.
In order to address the two questions of (1)
how well our approach of automatic speech un-
derstanding (ASU) can deal with automatic speech
recognition (ASR) errors and (2) how its perfor-
mance compares to syntactic analysis, we per-
formed two analyses. In order to answer ques-
tion (1) we compared the results from the semantic
analysis based on the real speech recognition re-
396
sults with an accuracy of 52% with those based on
the really uttered words as transcribed manually,
thus simulating a recognition rate of 100%. In to-
tal, the semantic speech processing received 1642
utterances from the speech recognition system.
From these utterances 418 utterances were ran-
domly chosen for manual transcription and syntac-
tic analysis. All 1642 utterances were processed
and performed on a standard PC with an average
processing time of 20ms, which fully fulfills the
requirements of real-time applications. As shown
in Table 1 39% of the results were rated as com-
plete or partial misunderstandings and 61% as cor-
rect utterances with full semantic meaning. Only
4% of the utterances which were correctly recog-
nized were misinterpreted or refused by the speech
understanding system. Most errors occurred due
to missing words in the lexicon.
Thus, the performance of the speech under-
standing system (ASU) decreases to the same
degree as that of the speech recognition system
(ASR): with a 50% ASR recognition rate the num-
ber of non-interpretable utterances is doubled in-
dicating a linear relationship between ASR and
ASU.
For the second question we performed a manual
classification of the utterances into syntactically
correct (and thus parseable by a standard pars-
ing algorithm) and not-correct. Utterances fol-
lowing the English standard grammar (e.g. im-
perative, descriptive, interrogative) or containing
a single word or an NP, as to be expected in an-
swers, were classified as correct. Incomplete ut-
terances or utterances with a non-standard struc-
ture (as occurred often in the baby-talk style ut-
terances) were rated as not-correct. In detail, 58
utterances were either truncated at the end or be-
ginning due to errors of the attention system, re-
sulting in utterances such as ?where is?, ?can you
find?, or ?is a cube?. These utterances also include
instances where users interrupted themselves. In
51 utterances we found words missing in our lex-
icon database. 314 utterances where syntactically
correct, whereas in 28 of these utterances a lexicon
entry is missing in the system and therefore would
ASR=100% ASR=52%
ASU not or part. interpret. 15% 39%
ASU fully interpretable 84% 61%
Table 1: Semantic Processing results based on dif-
ferent word recognition accuracies.
lead to a failure of the parsing mechanism. 104 ut-
terances have been classified as syntactically not-
correct.
In contrast, the result from our mechanism per-
formed significantly better. Our system was able
to interprete 352 utterances and generate a full se-
mantic interpretation, whereas 66 utterances could
only be partially interpreted or were marked as
not interpretable. 21 interpretations of the utter-
ances were semantically incorrect (labeled from
the system wrongly as correct) or were not as-
signed to the correct speech act, e.g., ?okay? was
assigned to no speech act (fragment) instead to
conrmation. Missing lexicon entries often lead
to partial interpretations (20 times) or sometimes
to complete misinterpretations (8 times). But still
in many cases the system was able to interprete the
utterance correctly (23 times). For example ?can
you go for a walk with me? was interpreted as ?can
you go with me? only ignoring the unknown ?for
a walk?.The utterance ?can you come closer? was
interpreted as a partial understanding ?can you
come? (ignoring the unknown word ?closer?). The
results are summarized in Table 2.
As can be seen the semantic error rate with 15%
non-interpretable utterances is just half of the syn-
tactic correctness with 31%. This indicates that
the semantic analysis can recover about half of the
information that would not be recoverable from
syntactic analysis.
ASU Synt. cor.
not or part. interpret. 15% not-correct 31%
fully interpret. 84% correct 68%
Table 2: Comparison of semantic processing result
with syntactic correctness based on a 100% word
recognition rate.
8 Conclusion and Outlook
In this paper we have presented a new approach
of robust speech understanding for mobile robot
assistants. It takes into account the special char-
acteristics of situated communication and also the
difficulty for the speech recognition to process ut-
terances correctly. We use special concept struc-
tures for situated communication combined with
an automatic fusion mechanism to generate se-
mantic structures which are necessary for the di-
alog manager of the robot system in order to re-
spond adequately.
This mechanism combined with the use of our
397
SSUs has several benefits. First, speech is in-
terpreted even if speech recognition does not al-
ways guarantee correct results and speech input is
not always grammatically correct. Secondly, the
speech understanding component incorporates in-
formation about gestures and references to the en-
vironment. Furthermore, the mechanism itself is
domain-independent. Both, concepts and lexicon
can be exchanged in context of a different domain.
This semantic analysis already produces elab-
orated interpretations of utterances in a fast way
and furthermore, helps to improve robustness of
the entire speech processing system. Nevertheless,
we can improve the system. In our next phase we
will use a more elaborate scoring function tech-
nique and use the correlations of mandatory and
optional links to other concepts to perform a better
evaluation and also to help the dialog manager to
find clues for missing information both in speech
and scene. We will also use the evaluation results
to improve the SSUs to get better results for the
semantic interpretation.
References
C. Breazeal, A. Brooks, J. Gray, G. Hoffman, C. Kidd,
H. Lee, J. Lieberman, A. Lockerd, and D. Mulanda.
2004. Humanoid robots as cooperative partners for
people. Int. Journal of Humanoid Robots.
B. Bruce. 1975. Case systems for natural language.
Artificial Intelligence, 6:327?360.
K. Dautenhahn. 2004. Robots we like to live with?! -
a developmental perspective on a personalized, life-
long robot companion. In Proc. Int. Workshop on
Robot and Human Interactive Communication (RO-
MAN).
A. Haasch et. al. 2004. BIRON ? The Bielefeld Robot
Companion. In E. Prassler, G. Lawitzky, P. Fior-
ini, and M. Ha?gele, editors, Proc. Int. Workshop on
Advances in Service Robotics, pages 27?32. Fraun-
hofer IRB Verlag.
C. J. Fillmore and C. F. Baker. 2001. Frame seman-
tics for text understanding. In Proc. of WordNet and
Other Lexical Recources Workshop. NACCL.
C. J. Fillmore. 1976. Frame semantics and the nature
of language. In Annals of the New York Academy of
Sciences: Conf. on the Origin and Development of
Language and Speech, volume 280, pages 20?32.
J. Fritsch, M. Kleinehagenbrock, A. Haasch, S. Wrede,
and G. Sagerer. 2005. A flexible infrastructure for
the development of a robot companion with exten-
sible HRI-capabilities. In Proc. IEEE Int. Conf. on
Robotics and Automation, pages 3419?3425.
P. Gorniak and D. Roy. 2005. Probabilistic Ground-
ing of Situated Speech using Plan Recognition and
Reference Resolution. In ICMI. ACM Press.
B. J. Grosz and C. L. Sidner. 1986. Attention, inten-
tion, and the structure of discourse. Computational
Linguistics, 12(3):175?204.
H. Hu?ttenrauch, A. Green, K. Severinson-Eklundh,
L. Oestreicher, and M. Norman. 2003. Involving
users in the design of a mobile office robot. IEEE
Transactions on Systems, Man and Cybernetics, Part
C.
S. Hu?wel and F. Kummert. 2004. Interpretation of
situated human-robot dialogues. In Proc. of the 7th
Annual CLUK, pages 120?125.
J. Juster and D. Roy. 2004. Elvis: Situated Speech and
Gesture Understanding for a Robotic Chandelier. In
Proc. Int. Conf. Multimodal Interfaces.
S. Lauriar, G. Bugmann, T. Kyriacou, J. Bos, and
E. Klein. 2001. Personal robot training via natu-
ral language instructions. IEEE Intelligent Systems,
16:3, pages 38?45.
S. Li, A. Haasch, B. Wrede, J. Fritsch, and G. Sagerer.
2005. Human-style interaction with a robot for co-
operative learning of scene objects. In Proc. Int.
Conf. on Multimodal Interfaces.
L. Seabra Lopes, A. Teixeira, M. Quindere, and M. Ro-
drigues. 2005. From robust spoken language under-
standing to knowledge aquisition and management.
In EUROSPEECH 2005.
J. T. Milde, K. Peters, and S. Strippgen. 1997. Situated
communication with robots. In First Int. Workshop
on Human-Computer-Conversation.
D. Milward. 2000. Distributing representation for ro-
bust interpretation of dialogue utterances. In ACL.
A.-M. Popescu, A. Armanasu, O. Etzioni, D. Ko, and
A. Yates. 2004. Modern natural language interfaces
to databases: Composing statistical parsing with se-
mantic tractability. In Proc. of COLING.
O. Rogalla, M. Ehrenmann, R. Zo?llner, R. Becher, and
R. Dillmann. 2002. Using gesture and speech con-
trol for commanding a robot assistant. In Proc. of
the 11th IEEE Int. Workshop on Robot and Human
interactive Communication, pages 454?459. RO-
MAN.
S. Wachsmuth, G. A. Fink, and G. Sagerer. 1998. Inte-
gration of parsing and incremental speech recogni-
tion. In EUSIPCO, volume 1, pages 371?375.
W. Ward. 1994. Extracting Information From Sponta-
neous Speech. In ICRA, pages 83?86. IEEE Press.
V. Zue, S. Seneff, J. Glass, J. Polifronti, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER:
A telephone-based conversational interface for
weather information. IEEE Transactions on Speech
and Audio Processing, pages 100?112, January.
398
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 153?160,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A computational model of multi-modal grounding
for human robot interaction
Shuyin Li, Britta Wrede, and Gerhard Sagerer
Applied Computer Science, Faculty of Technology
Bielefeld University, 33594 Bielefeld, Germany
shuyinli, bwrede, sagerer@techfak.uni-bielefeld.de
Abstract
Dialog systems for mobile robots operat-
ing in the real world should enable mixed-
initiative dialog style, handle multi-modal
information involved in the communica-
tion and be relatively independent of the
domain knowledge. Most dialog systems
developed for mobile robots today, how-
ever, are often system-oriented and have
limited capabilities. We present an agent-
based dialog model that are specially de-
signed for human-robot interaction and
provide evidence for its efficiency with our
implemented system.
1 Introduction
Natural language is the most intuitive way to com-
municate for human beings (Allen et al, 2001). It
is, therefore, very important to enable dialog capa-
bility for personal service robots that should help
people in their everyday life. However, the inter-
action with a robot as a mobile, autonomous de-
vice is different than with many other computer
controlled devices which affects the dialog model-
ing. Here we want to first clarify the most essen-
tial requirements for dialog management systems
for human-robot interaction (HRI) and then out-
line state-of-the-art dialog modeling approaches to
position ourselves.
The first requirement results from the situated-
ness (Brooks, 1986) of HRI. A mobile robot is
situated ?here and now? and cohabits the same
physical world as the user. Environmental changes
can have massive influence on the task execution.
For example, a robot should fetch a cup from the
kitchen but the door is locked. Under this cir-
cumstance the dialog system must support mixed-
initiative dialog style to receive user commands on
the one side and to report on the perceived envi-
ronmental changes on the other side. Otherwise
the robot had to break up the task execution and
there is no way for the user to find out the reason.
The second challenge for HRI dialog manage-
ment is the embodiment of a robot which changes
the way of interaction. Empirical studies show that
the visual access to the interlocutor?s body affects
the conversation in the way that non-verbal behav-
iors are used as communicative signals (Nakano et
al., 2003). For example, to refer to a cup that is
visible to both dialog partners, the speaker tends
to say ?this cup? while pointing to it. The same
strategy is considerably ineffective during a phone
call. This example shows, an HRI dialog system
must account for multi-modal communication.
The third, probably the unique challenge for
HRI dialog management is the implication of the
learning ability of such a robot. Since a personal
service robot is intended to help human in their
individual household it is impossible to hard-code
all the knowledge it will need into the system, e.g.,
where the cup is and what should be served for
lunch. Thus, it is essential for such a robot to
be able to learn new knowledge and tasks. This
ability, however, has the implication for the dia-
log system that it can not rely on comprehensive,
hard-coded knowledge to do dialog planning. In-
stead, it must be designed in a way that it has a
loose relationship with the domain knowledge.
Many dialog modeling approaches already ex-
ist. McTear (2002) classified them into three main
types: finite state-based, frame-based, and agent-
based. In the first two approaches the dialog struc-
ture is closely coupled with pre-defined task steps
and can therefore only handle well-structured
tasks for which one-side led dialog styles are suf-
ficient. In the agent-based approach, the com-
153
munication is viewed as a collaboration between
two intelligent agents. Different approaches in-
spired by psychology and linguistics are in use
within this category. For example, within the
TRAINS/TRIPS project several complex dialog
systems for collaborative problem solving have
been developed (Allen et al, 2001). Here the dia-
log system is viewed as a conversational agent that
performs communicative acts. During a conver-
sation, the dialog system selects the communica-
tive goal based on its current belief about the do-
main and general conversational obligations. Such
systems make use of communication and domain
model to enable mixed-initiative dialog style and
to handle more complex tasks. In the HRI field,
due to the complexity of the overall systems, usu-
ally the finite-state-based strategy is employed
(Matsui et al, 1999; Bischoff and Graefe, 2002;
Aoyama and Shimomura, 2005). As to the is-
sue of multi-modality, one strand of the research
concerns the fusion and representation of multi-
modal information such as (Pfleger et al, 2003)
and the other strand focuses on the generalisation
of human-like conversational behaviors for virtual
agents. In this strand, Cassell (2000) proposes a
general architecture for multi-modal conversation
and Traum (2002) extends his information-state
based dialog model by adding more conversational
layers to account for multi-modality.
In this paper we present an agent-based dialog
model for HRI. As described in section 2, the two
main contributions of this model are the new mod-
eling approach of Clark?s grounding mechanism
and the extension of this model to handle multi-
modal grounding. In section 3 we outline the ca-
pabilities of the implemented system and present
some quantitative evaluation results.
2 Dialog Model
We view a dialog as a collaboration between two
agents. Agents are subject to common conversa-
tional rules and participate in a conversation by
issuing multi-modal contributions (e.g., by say-
ing something or displaying a facial expression).
In subsection 2.1 we show how we handle con-
versational tasks by modeling the conversational
rules based on grounding and in subsection 2.2 we
present how we model individual contributions to
tackle the issue of multi-modality. In subsection
2.3 we put these two things together to complete
the model description. In this section, we also put
concrete examples from the robot domain to clar-
ify the relatively abstract model.
2.1 Grounding
One of the most influential theories on the collab-
orative nature of dialog is the common ground the-
ory of Clark (1992). In his opinion, agents need
to coordinate their mental states based on their
mutual understanding about the current tasks, in-
tentions, and goals during a conversation. Clark
termed this process as grounding and proposed a
contribution model. In this model, ?contributions?
from conversational agents are considered to be
the basic component of a conversation. Each con-
tribution has two phases: a Presentation phase and
an Acceptance phase. In the Presentation phase the
speaker presents an utterance to the listener, in the
Acceptance phase the listener issues an evidence
of understanding to the speaker. The speaker can
only be sure that the utterance she presented previ-
ously has become a part of their common ground
if this evidence is available.
Although this well established theory provides
comprehensive insight into human conversation
two issues in this theory remain critical when be-
ing applied to model dialog. The first one is the re-
cursivity of Acceptance. Clark claimed, since ev-
erything said by one agent needs to be understood
by her interlocutor, each Acceptance should also
play the role of Presentation which needs to be ac-
cepted, too. The contributions are thus to be or-
ganized as a graph. However, this implies that the
grounding process may never really end (Traum,
1994). The second critical issue is taking con-
tributions as the most basic grounding units. In
Clark?s view, the basic grounding unit, i.e., the unit
of conversation at which grounding takes place,
is the contribution. To provide Acceptance for a
contribution agents may need to issue clarification
questions or repair. But when modeling a dialog,
especially a task-oriented dialog, it is hard to map
one single contribution from one agent to a domain
task since tasks are always cooperately done by
the two agents (Cahn and Brennan, 1999). Traum
(1994) addressed the first issue by introducing a
finite-state based grounding mechanism and Cahn
and Brennan (1999) used ?exchanges?? as the ba-
sic grounding unit to tackle the second critical is-
sue. We combine the advantages of their work and
present a grounding mechanism based on an aug-
mented push-down automaton as described below.
154
Basic grounding unit: As Cahn and Brennan
we take exchange as the most basic grounding
unit. An exchange is a pair of contributions ini-
tiated by the two conversational agents. They rep-
resent the idea of adjacency pairs (Schegloff and
Sacks, 1973). The first contribution of the ex-
change is the Presentation and the second contri-
bution is the Acceptance, e.g., if one asks a ques-
tion and the other answers it, then the question is
the Presentation and the answer is the Acceptance.
In our model, a contribution only represents one
speech act. For example, if an agent says ?Hello,
my name is Tom, what is your name?? this ut-
terances is segmented into three Presentations (a
greeting, a statement, and a question) although
they occur in one turn. These three Presentations
initiate three exchanges and each of them needs to
be accepted by the interlocutor.
Changing status of grounding units: Also as
proposed by Cahn and Brennan, an exchange has
two states: not (yet) grounded and grounded. An
exchange is grounded if the Acceptance of the
Presentation is available. Note, the Acceptance
can be an implicit one, e.g, in form of ?contin-
ued attention? in Clark?s term. Taking the exam-
ple above, the other agent would reply ?Hello, my
name is Jane.? without explicitely commenting
Tom?s name, yet the three exchanges that Tom ini-
tiated were all accepted.
Organization of grounding units: In accor-
dance with Traum we do not think that the Pre-
sentation of one exchange should play the role
of the Acceptance of its previous exchange. In-
stead, we organize exchanges in a stack. The stack
represents the whole ungrounded discourse: un-
grounded exchanges are pushed onto it and the
grounded ones are popped out of it. One major
question of this representation is: What has the
grounding status of individual exchange to do with
the grounding status of the whole stack? Jane?s
Acceptance of Tom?s greeting has no apparent re-
lation to the remaining two still ungrounded ex-
changes initiated by Tom. But in the center em-
bedding example in Fig. 1, the Acceptance of B1
(utterance A2) contributes to the Acceptance of
A1 (utterance B2). These examples show that the
grounding status of the whole discourse depends
on (1) the grounding status of the individual ex-
changes and (2) the relationship between these ex-
changes, the grounding relation. These relations
are introduced by the Presentation of each ex-
change because they start an exchange. We identi-
fied 4 types of grounding relations: Default, Sup-
port, Correct, and Delete. In the following we
look at these relations in more detail and refer to
exchanges with relation x to its immediately pre-
ceding exchange (IPE) as ?x exchange?, e.g., Sup-
port exchange:
Default: The current Presentation introduces a
new account that is independent of the previous
exchange in terms of grounding, e.g., what Tom
said to Jane constructs three Presentations that ini-
tiate three default exchanges. Such exchanges can
be grounded independently of each other.
Support: If an agent can not provide Accep-
tance for the given Presentation she will initiate
a new exchange to support the grounding process
of the ungrounded exchange. A typical exam-
ple of such an exchange is a clarification ques-
tion like ?I beg your pardon??. If a Support ex-
change is grounded its initiator will try to ground
the IPE again with the newly collected information
through the supporting exchange.
Correct: Some exchanges are created to correct
the content of the IPE, e.g., in case that the lis-
tener misunderstood the speaker and the speaker
corrects it. Similar to Support, after such an ex-
change is grounded its IPE is updated with new
information and has to be grounded again.
Delete: Agents can give up their effort to build a
common ground with her interlocutor, e.g., by say-
ing ?Forget it.?. If the interlocutor agrees, such ex-
changes have the effect that all the ungrounded ex-
changes from the initial Default exchange up to the
current state are no longer relevant and the agents
do not need to ground them any more.
Note, once an exchange is grounded it is imme-
diately removed from the stack so that its IPE be-
comes the IPE of the next exchange. This model
is described as an augmented push-down automa-
ton (Fig. 2). It is augmented in so far that transi-
tions can trigger actions and a variable number of
exchanges can be popped or pushed in one step.
There are five states in this APDA and they rep-
resent the fact what kind of ungrounded exchange
is on the top of the stack. Along the arrows that
connect the states the input (denoted as I), the re-
sulting stack operation (denoted as S) and the pos-
sible action that is triggered (denoted as A) are
given. The input of this automaton includes Pre-
sentation (e.g., ?defaultP? stands for ?Default Pre-
sentation?) and Acceptance.
155
A1: What do you think about Mr. Watton?
B1: Mr. Watton? our music teacher?
A2: Yes. (accept B1)
B2: Well, he is OK. (accept A1)
Figure 1: An example of center embedding
Top
SupportTop DeleteTop
DefaultTop
Start
Correct
I:acc;
 S:po
p(ex); 
A:upda
te(IPE
)
I:supp
ortP; 
S:pus
h(ex)I:corre
ctP; S
:push
(ex)
I:acc;
 S:po
p(ex); 
A:corre
ct(IPE)
I:supportP; S:push(ex)
I:acc; S:pop(all)deleteP; S:I[supportP|correctP|
I:acc
; S:p
op(ex
)
I:def
aultP
; S:p
ush(e
x)
S:pop(ex)I:acc
S:push(ex)I:defaultP
I:defaultP; S:pop(all)&push(ex)I:acc; S:pop(ex); A:correct(IPE) I:acc; S:pop(ex)
IcorrectP; S:push(ex)
I:acc; S:pop(all)
I:deleteP; S:push(ex)
I:supportP; S:push(ex)
I:acc; S:pop(ex); A:update(IPE)
I:acc; S:pop(ex)
I:defaultP; S:pop(all)&push(ex)
S:pop(ex)I:acc S:push(ex)I:supportP
I:deleteP; S:push(ex)I:acc; S:pop(ex)
I:deleteP; S:push(ex)
I:acc; S:pop(ex)
I:correctP; S:push(ex)
Figure 2: Augmented push-down automaton for
grounding (ex: exchange)
As long as there is an ungrounded exchange
at the top of the stack, the addressee will try to
ground it by providing Acceptance, unless its va-
lidity is deleted. For the reason of space, we only
explain the APDA with the center embedding ex-
ample in Fig. 1. Contribution A1 introduces a
question into the discourse which initiates a De-
fault exchange, say Ex1. This exchange is pushed
onto the stack. Instead of providing Acceptance
to A1, contribution B1 initiates a new exchange,
say Ex2, with grounding relation Support to Ex1
and is pushed onto the stack. Then contribution
A2 acknowledges B1 so that Ex2 is grounded and
popped out of the stack. The top element of the
stack is now the ungrounded Ex1. Since Ex2 sup-
ported Ex1, the Ex1 is updated with the infor-
mation contained in Ex2 (The music teacher was
meant) and B2 then successfully grounds this up-
dated Ex1.
In our model, every exchange can be individu-
ally grounded and contributes to the grounding of
the whole ungrounded discourse by acting on the
IPE according to their grounding relations. This
way we can organize the discourse in a sequence
without losing the local grounding flexibility. For
an implemented system, this means that both the
user and the system can easily take initiative or
issue clarification questions. To implement this
model, however, two points are crucial. The first
one is the recognition of the user?s contribution
type: for every user contribution, the dialog sys-
tem needs to decide whether it is a Presentation or
an Acceptance. If it is a Presentation, the system
needs further to decide whether it initiates a new
account, corrects or supports the current one, or
deletes it. This issue of intention recognition is a
classical challenge for dialog systems. We present
our solution in section 3. The second point is that
the dialog system needs to know when to create an
exchange of certain grounding relation by generat-
ing an appropriate Presentation and when to create
an Acceptance. For that we need to first look at the
structure of individual contributions more closely
in the next subsection.
2.2 The structure of agents? contributions
To represent the structure of the individual contri-
butions we take into account the whole language
generation process which enables us to come up
with a powerful solution as described below.
The layers of a contribution: What we can
observe in a conversation are only exchanges of
agents? contributions in verbal or non-verbal form.
But in fact the contributions are the end-product
of a complex cognitive process: language produc-
tion. Levelt (1989) identified three phases of lan-
guage production: conceptualization, formulation,
and articulation. The production of an utterance
starts from the conception of a communicative in-
tention and the semantic organization in the con-
ceptualization phase before the utterance can be
formulated and articulated in the next two phases.
Intentions can arise from the previous discourse or
from other motivations such as needs for help or
information. This finding motivates us to set up a
two-layered structure of contributions. One layer
is the so-called intention layer where communi-
cation intentions are conceived. For a robot the
communication intentions come from the analysis
of the previous discourse or from the robot control
system. The other layer is the conversation layer.
The communication intentions are formulated and
articulated here1. These two layers represent the
intention conception and the language generation
process, respectively. We term this two-layered
structure of contribution interaction unit (IU).
The issue of multi-modality: Face-to-face
conversations are multi-modal. Speech and body
language (e.g., gesture) can happen simultane-
ously. McNeill (1992) stated that gesture and
speech arise from the same semantic source, the
1Since most robot systems use speech synthesizer to gen-
erate acoustic output which replaces the articulation process,
only formulation is performed on this layer.
156
so-called ?idea unit? and are co-expressive. Since
semantic representation is created out of commu-
nicative intentions (Levelt, 1989) we assume the
communication intentions are the modality inde-
pendent base that governs the multi-modal lan-
guage production. We, therefore, extend our struc-
ture above by introducing two generators on the
conversation layer: one verbal and one non-verbal
generator that represent the verbal and non-verbal
language generation mechanism based on the
communication intentions created on the intention
layer. The relationship between these two genera-
tors is variable. For example, Iverson et al (1999)
identified three types of informational relationship
- Conversation Layer -
verbalgenerator non-verbalgenerator
intention conception
- Intention Layer -
Figure 3: IU
between speech and gesture:
reinforcement (gesture rein-
forces the message conveyed
in speech, e.g., emphatic ges-
ture), disambiguation (ges-
ture serves as the precise ref-
erent of the speech, e.g., deic-
tic gesture accompanying the
utterance ?this cup?), and adding-information
(e.g., saying ?The ball is so big.? and shaping
the size with hands). In our work, when process-
ing users? multi-modal contributions we focus on
the disambiguation relation; when creating multi-
modal contributions for the robot we are also inter-
ested in other informational relations 2. The struc-
ture of an IU is illustrated in Fig. 3.
Operation flow within an interaction unit:
During a conversation an agent either initiates
an account or replies to the interlocutor?s ac-
count. The communication intentions can thus be
self-motivated or other-motivated. For a robot,
self-motivated intentions can be triggered by the
robot control system, e.g., observed environmen-
tal changes. In this case, an IU is created with
its intention layer importing the message from the
robot control system and exporting an intention.
This intention is transfered to the conversation
layer which then formulates a verbal message with
the verbal generator and/or constructs a body lan-
guage expression with the non-verbal generator.
Other-motivated intentions can be triggered by the
needs of the on-going conversation, e.g., the need
to answer a question, or be triggered by robot?s ex-
ecution results of the tasks specified previously by
the user. The operation flow is similar to that of
2This policy has a practical reason: it is much more diffi-
cult in computer science to correctly recognize and interpret
human motion than to simulate it.
the self-motivation apart from the fact that, in case
of intentions motivated by conversational needs,
the intention layer of the IU does not import any
robot control system message but creates an inten-
tion directly. Note, the IUs that are initiated by the
robot and by the user have identical structure. But
in case of user initiated IUs we do not make any
assumption of their underlying intention building
process and the intention layer of their IUs are thus
always empty.
With the IUs, we can integrate the non-verbal
behavior systematically into the communication
process and model multi-modal dialog. Although
it is not the focus of our work, our model can also
handle purely non-verbal contributions, since the
verbal generator does not always need to be acti-
vated if the non-verbal generator already provides
enough information about the speaker?s intention.
Possible scenarios are: the user looks tired (pre-
sentation) and the robot offers ?I can do that for
you.? (acceptance) or the user says something
(presentation) and robot nods (acceptance).
2.3 Putting things together
Till now we have discussed our concept of using
a grounding mechanism to organize contributions
and of representing individual contributions as IU.
Now it is time to look at the still open point at the
end of the section 2.1: when to create an IU as
Presentation and when an IU as Acceptance.
Self-motivated intentions usually trigger the
creation of an IU as Presentation with Default re-
lation to its IPE. For example, if the robot needs
to report something to the user it can create a De-
fault exchange by generating an IU as its Presen-
tation. The user is then expected to signal her Ac-
ceptance. Other-motivated intentions can, accord-
ing to the context, result in either Presentation or
Acceptance. To make the correct decision we de-
veloped criteria based on the joint intention theory
of Levesque et al (1990) which predicts that dur-
ing a collaboration the partners are committed to
a joint goal that they will always try to conform
till they reach the goal or give up. Note, this does
not mean that one will always agree with her inter-
locutor, but they will behave in the way that they
think is the best to achieve the goal. This theory
can be applied to human-robot dialog in a twofold
sense: Firstly, a dialog can be generally seen as
a collaboration as Clark proposed. Secondly, the
human-robot dialog is mostly task-oriented, i.e.,
157
the human and the robot work towards the same
goal. With this theory in mind we describe how
we process other-motivated contributions below.
The precondition of language production based
on other-motivated intentions is language percep-
tion. Before reacting, i.e., before creating her own
IU, an agent first needs to understand the inten-
tion conveyed by her interlocutor?s IU by study-
ing its conversation layer. Since we focus on dis-
ambiguation function of non-verbal behavior we
assume that agents first study the generated ver-
bal information, if the intention can not be fully
recognized here, one will further study the infor-
mation provided by the non-verbal generator (e.g.,
a gesture) and fuse the verbal and non-verbal in-
formation. If the intention recognition is still un-
successful, the agent can not provide Acceptance
for the given IU. If she is still committed to the
dialog she will issue a clarification question, i.e.,
she generates an IU as Presentation that initiates
a Support exchange to the current ungrounded ex-
change. If the intention of her interlocutor is suc-
cessfully recognized the language perception pro-
cess ends and the agent tries to create her own IU.
As described in subsection 2.2 the creation of the
IU starts from the creation of an intention on the
intention layer. In case of a robot, the dialog sys-
tem accesses the robot control system and awaits
its reaction to the conveyed information (e.g., a
user instruction). Usually, a robot is designated
to do something for the user, i.e., the robot is com-
mitted to the goal proposed by the user, so we de-
fine the robot can only provide acceptance if the
task is successfully executed. In this case, the robot
completes the current IU with the filled intention
layer by generating an confirmation on its conver-
sation layer. Afterwards, this grounded exchange
can be popped from the stack. If the robot can not
execute the task for some reasons, then the current
exchange can not be grounded and the robot will
take the current IU with the filled intention layer
as another Presentation that initiates a Support or
Correct exchange to the current ungrounded ex-
change, similar as the case in Fig. 1. The conversa-
tion layer of this IU can thus formulate something
like ?Sorry, I can?t do that because...? and present
a sorrowful face. This new Support or Correct ex-
change is pushed onto the stack. Figure 4 illus-
trates this process as a UML activity diagram.
In our model we only do general conversational
planning instead of domain specific task planning.
study non?verbal info on the CL
intention recognized?
intention recognized?
intention conformsthe joint goal?
complete IU as Acceptance create IU as Presentation
push Exchange with the interlocutor?s IU as presentationn
yes no
yes
no
no
yes
study verbal info on the interlocutor?s CL
Support or Correct relationground exchange n
pop exchange n
create exchange n+1 with
push exchange n+1
(access robot control system)create one?s own IL
Figure 4: Handling other-motivated contribution
(CL: Conversation layer; IL: Intention Layer)
What the dialog system needs to know from the
robot control system is what processing results it
can produce. The association of these results with
robot intentions in terms of whether they start a
new account, support or correct one, or delete it,
can be configured externally and thus easily up-
dated or replaced. Based on this configuration IUs
are generated that operate according to the ground-
ing mechanism as described in section 2.1.
3 Implementation
This dialog model was implemented for our robot
BIRON, a personal robot with learning abilities.
It can detect and follow persons, focus on objects
(according to human deictic gestures) and store
collected information into a memory. Our imple-
mentation scenario is the so-called home tour: a
user shows a new robot her home to prepare it for
future tasks. The robot should be able to learn and
remember features of objects that the user men-
tions and it ?sees?, e.g., name, color, images etc.
Besides, our system was also successfully ported
to a humanoid robot BARTHOC for studies of
emotional and social factors of HRI (see. Fig. 5).
Figure 5: Robots BIRON and BARTHOC
The dialog manager is linked to a speech under-
standing system which transforms parts of speech
158
from a speech recognizer into a speech-act-based
form. To recognize the user?s intention, the dia-
log system classifies this input into 10 categories
of three groups according to heuristics, e.g., in-
struction, description, and query initiate new tasks
and thus a new Default exchange; deletion and
correction initiate Delete and Correct exchanges
that are related to early exchanges; and confirma-
tion, negation, etc. can only be responses and
are, therefore, viewed as user?s Acceptance of ex-
changes that the robot initiates. The main part of
the dialog system is the Dialog Manager that car-
ries out grounding analysis and stack operations.
The Robot Control Manager receives messages
from the robot control system and calls the Dia-
log Manager to do relevant operations. This dialog
system enables multi-modal, mixed-initiative dia-
log style and can handle complex repair behaviors.
With the example dialog in Fig. 6, which is tran-
scripted from a real interaction session between a
user and BIRON, we discuss the two most impor-
tant features of our system.
Ex3
R1: Hello human!
Who are you?
U1: Hello! 
something I can remember it.
U2: OK, um, look, this is a mug.
U3: This is a mug, umm, here.
R4: Sorry, can you use some other words?Ex5
Ex2 R2: My name is BIRON, if you show me
R5: Sorry, I can?t find it. Are you pointing
to it with your finger?
U5: Yes!
R6: I know it is sometimes difficult with me, 
but don?t feel discouraged!
R7: OK, I?ve found it, it is really nice!
Ex6
R3: I beg your pardon?
U4: This is a mug!
U6: (laugh)
Ex7
Ex4
Ex1 (looking friendly)
(looking friendly)
(looking puzzled)
(looking sad)
(looking sad)
(looking ashamed)
(looking happily)
Figure 6: A dialog example with the extrovert
BIRON. (U: user, R: robot, Ex: Exchange)
Taking Initiative and robot personality: Ini-
tiatives that a dialog system can take often depends
on its back-end application. Since BIRON does
not have a task planner which would be ideal to
demonstrate this ability we implemented an extro-
vert personality for it (additionally to its basic per-
sonality) that takes communication-related initia-
tives. The basic BIRON behaves in a rather pas-
sive way and only says something when addressed
by the user. In contrast, the extrovert BIRON
greets persons actively (R1 in Table 6) and re-
marks on its own performance (R6). When the
robot control system detects a person the dialog
system initiates a Default exchange to greet her.
BIRON can also measure its own performance by
counting the number of Support exchanges it has
initiated for the current topic. Since the Support
exchanges are only created if BIRON can not pro-
vide Acceptance to the user?s Presentation (be-
cause it does not understand the user or it can
not execute a task), the amount of the Support ex-
changes thus has direct correlation to the robot?s
overall performance. On the other hand, the more
Default exchanges there are, the better is the per-
formance because the agents can proceed to an-
other topic only if the current one is grounded (or
deleted). Based on this performance indication
BIRON does remarks to motivate users.
Resolving multi-modal object references: It
happens quite frequently in the home tour scenario
that the user points to some objects and says ?This
is a z?. BIRON needs to associate its symbolic
name (and eventually other features) mentioned by
the user with the image of the object. The reso-
lution of such multi-modal object references (U4-
R7 in Table 6) is solved as following: the Dialog
Manager creates an IU for the user-initiated utter-
ance (e.g., ?this is a cup?) and studies the verbal
and non-verbal generator on its conversation layer.
In the verbal generator, what the pronoun ?this?
refers to is unclear, but it indicates that the user
might be using a gesture. Therefore, the Dialog
Manager further studies the non-verbal generator.
The responsible robot vision module is activated
here to search for a gesture and to identify the ob-
ject cup. If the cup is found in the scene, this mod-
ule assigns an ID to the image and stores it in the
memory. After the Dialog Manager receives this
ID, the processing of the conversation layer of the
user IU ends, the Dialog Manager proceeds to cre-
ate its own IU to react to the user?s IU. Problems
with the object identification indicate failure of the
intention recognition process on the user conversa-
tion layer. In this case, the Dialog Manager creates
a Support exchange to ask the user which object
she refers to and retries it if she does not oppose
(R5-R7). This process and the associated multi-
modality fusion and representation are described
in (Li et al, 2005) in detail.
The evaluation of dialog systems for human
robot interaction is still an open issue. A robot
system is usually a complex system including a
159
large number of modules that claim plenty of pro-
cessing time or are subject to environmental con-
ditions. For the dialog system, this means that the
correct interpretation and transaction of user utter-
ances is by no means a guaranty for a prompt re-
sponse or successful task execution. Thus, the per-
formance of the dialog system can not be directly
measured with the performance of the overall sys-
tem like most desktop dialog applications. We are
still working at evaluation metrics for HRI dialog
systems (Green et al, 2006). But the efficiency
of our system is already visible in the small ef-
fort associated with the porting of this system to
another robot platform and in the pilot user study
with BIRON. In this study, each of the 14 users in-
teracted with BIRON twice. In the total 28 runs
the dialog system generated 903 exchanges for
the 813 user utterances. Among these exchanges,
34% initiated clarification questions. This result
correlated with the evaluation result of our speech
understanding system which fully understood 65%
of all the user utterances. 18.6% of the exchanges
were Support exchanges created due to execution
failure of the robot control system which corre-
sponds to the performance of the robot control sys-
tem. The average processing time of the dialog
system was 11 msec.
4 Conclusion
In this paper we presented an agent-based dialog
model for HRI. The implemented system enables
multi-modal, mixed-initiative dialog style and is
relatively domain independent. The real-time test-
ing of the system proves its efficiency. We will
work out detailed evaluation metrics for our sys-
tem to be able to draw more general conclusion
about the strength and weakness of our model.
References
J. Allen, D. K. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2001. Towards conversational
human-computer interaction. AI Magazine, 22(4).
K. Aoyama and H. Shimomura. 2005. Real world speech
interaction with a humanoid robot on a layered robot be-
havior control architecture. In Proc. Int. Conf. on Robotics
and Automation.
R. Bischoff and V. Graefe. 2002. Dependable multimodal
communication and interaction with robotic assistants. In
Proc. Int. Workshop on Robot-Human Interactive Commu-
nication (ROMAN).
R. A. Brooks. 1986. A robust layered control system for a
mobile robot. IEEE Journal of Robotics and Automation,
2(1):14?23.
J. E. Cahn and S. E. Brennan. 1999. A psychological model
of grounding and repair in dialog. In Proc. Fall 1999 AAAI
Symposium on Psychological Models of Communication
in Collaborative Systems.
J. Cassell, T. Bickmore, L. Campbell, and H. Vilhjalmsson.
2000. Human conversation as a system framework: De-
signing embodied conversational agents. In J. Cassell,
J. Sullivan, S. Prevost, and E. Churchill, editors, Embod-
ied conversational agents. MIT Press.
H. H. Clark, editor. 1992. Arenas of Language Use. Univer-
sity of Chicago Press.
A. Green, K. Severinson-Eklundh, B. Wrede, and S. Li.
2006. Integrating miscommunication analysis in natural
language interface design for a service robot. In Proc. Int.
Conf. on Intelligent Robots and Systems. submitted.
J. M. Iverson, O. Capirci, E. Longobardi, and M. C. Caselli.
1999. Gesturing in mother-child interactions. Cognitive
Develpment, 14(1):57?75.
W. Levelt. 1989. Speaking: From intention to articulation.
Cambridge, MA: MIT Press.
H. J. Levesque, P. R. Cohen, and J. H. T. Nunnes. 1990. On
acting together. In Proc. Nat. Conf. on Artificial Intelli-
gence (AAAI).
S. Li, A. Haasch, B. Wrede, J. Fritsch, and G. Sagerer.
2005. Human-style interaction with a robot for coopera-
tive learning of scene objects. In Proc. Int. Conf. on Mul-
timodal Interfaces.
T. Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, T. Kurita,
I. Hara, and N. Otsu. 1999. Integrated natural spoken
dialogue system of jijo-2 mobile robot for office services,.
In Proc. AAAI Nat. Conf. and Innovative Applications of
Artificial Intelligence Conf.
D. McNeill. 1992. Hand and Mind: What Gesture Reveal
about Thought. University of Chicago Press.
M. F. McTear. 2002. Spoken dialogue technology: enabling
the conversational interface. ACM Computing Surveys,
34(1).
Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 2003.
Towards a model of face-to-face grounding. In Proc. An-
nual Meeting of the Association for Computational Lin-
guistics.
N. Pfleger, J. Alexandersson, and T. Becker. 2003. A ro-
bust and generic discourse model for multimodal dialogue.
In Proc. 3rd Workshop on Knowledge and Reasoning in
Practical Dialogue Systems.
E. A. Schegloff and H. Sacks. 1973. Opening up closings.
Semiotica, pages 289?327.
D. Traum and J. Rickel. 2002. Embodied agents for multi-
party dialogue in immersive virtual world. In Proc. 1st Int.
Conf on Autonomous Agents and Multi-agent Systems.
D. Traum. 1994. A Computational Theory of Grounding in
Natural Language Conversation. Ph.D. thesis, University
of Rochester.
160
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 229?232,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Pamini: A framework for assembling mixed-initiative human-robot
interaction from generic interaction patterns
Julia Peltason and Britta Wrede
Applied Informatics, Faculty of Technology
Bielefeld University, Germany
jpeltaso, bwrede@techfak.uni-bielefeld.de
Abstract
Dialog modeling in robotics suffers from
lack of generalizability, due to the fact
that the dialog is heavily influenced by
the tasks the robot is able to perform.
We introduce interleaving interaction pat-
terns together with a general protocol for
task communication which enables us to
systematically specify the relationship be-
tween dialog structure and task structure.
We argue that this approach meets the re-
quirements of advanced dialog modeling
on robots and at the same time exhibits a
better scalability than existing concepts.
1 Introduction
The need for interaction modeling on robots is
widely acknowledged, not only for instructing
them but also for enabling them to learn from hu-
mans within interaction. Yet, today?s robotic sys-
tems often do not have a dedicated dialog system
but employ simple command-matching techniques
(e.g. (Bo?hme et al, 2003)). Other systems rely
on finite-state based dialog managers (e.g. (Bauer
et al, 2009)) that couple dialog and task man-
agement which hampers maintainability and reuse
and does not scale well for more complex interac-
tion scenarios.
On the other hand, concepts for reusable dialog
frameworks have been established within the spo-
ken dialog community for traditional information-
seeking domains where the system first collects
the required parameters, then presents the desired
information to the user, e.g. an accommodation
or travel information (e.g. (Bohus and Rudnicky,
2009)). However, these concepts are not directly
transferable to robotics, due to the situated nature
of human-robot interaction. A robot perceives the
world and acts on it ? and it is supposed to com-
municate about that. For instance, a robot manip-
Figure 1: The Curious Robot set-up.
ulating objects needs to be able to verbalize its in-
tended actions and give feedback about its internal
state, but it also needs to react on feedback com-
ments or corrections uttered by the human tutor. In
order to equip a robot with such fine-grained com-
munication skills, tight integration of action exe-
cution into interaction is required. In simple terms,
while the dialog frameworks mentioned can be re-
ferred to as information-oriented, we need rather
action-oriented approaches for robotics.
R: What is that? ?points?
H: This is a lemon.
R: A melon. OK.
H: No, I said ?lemon?!
R: A lemon. OK.
R: How should I grasp the lemon?
H: That way. ?demonstrates grip?
R: Alright, with the power grasp.
R: I am going to grasp it ?starts grasping?
R: I start grasping now.
H: Robot, what objects do you know?
R: Lemon.
R: OK, I finished grasping. ?stops grasping?
Table 1: A typical interaction with the Curious Robot.
2 The Pamini framework
This paper presents the Pamini (Pattern-based
mixed-initiative) human-robot interaction frame-
work. Pamini proposes a new approach for dialog
229
modeling on robots that includes (1) a task-state
protocol providing a fine-grained interface to the
complex domain processing of the robotic system
and (2) the concept of generic interaction patterns
that support rapid prototyping of human-robot in-
teractions and can be combined in a flexible way.
Previous versions of the Pamini framework
have been applied in several mixed-initiative
learning scenarios. For example, in the Home-
Tour scenario a mobile robot builds up a spatial
model of its environment and gradually improves
its model by attempting to obtain information from
the human (Peltason et al, 2009). In the Curious
Robot scenario shown in figure 1, an anthropomor-
phic robot learns to label and grasp objects, apply-
ing a proactive dialog strategy that provides guid-
ance for untrained users (Lu?tkebohle et al, 2009).
A dialog excerpt is shown in table 1.
2.1 The task state protocol
A dialog system for robotics needs to coordinate
with a number of components, e.g. for perceptual
analysis, motor control or components generating
nonverbal feedback. To realize this, we use the
concept of tasks that can be performed by com-
ponents. Tasks are described by an execution state
and a task specification containing the information
required for execution. A protocol specifies task
states relevant for coordination and possible tran-
sitions between them as shown in figure 2. Task
updates, i.e. updates of the task state and possi-
bly the task specification, cause event notifications
which are delivered to the participating compo-
nents whereupon they take an appropriate action.
update requested
running
cancel requested
initiated
CANCELLED
DONEfailed
accepted
result_availablerejected cancel_failedcancel
accept, rejectupdate
Figure 2: The task life-cycle. A task gets initiated, ac-
cepted, may be cancelled or updated, may deliver intermedi-
ate results and finally is completed. Alternatively, it can be
rejected by the handling component or execution may fail.
Tight integration with action execution A
robot performing e.g. a grasp action supervised
by the human requires multi-step communication
between the dialog system and the arm control as
illustrated in figure 3. Generally, with the accepted
state, the proposed protocol enables the dialog sys-
tem to provide feedback during slow-going actions
indicating the internal system state. Further, with
the update and result available states, it supports
the modification of the task specification during
execution and thus gives the robot the ability to
react to comments, corrections and commands on-
the-fly.
Arm ControlSpeech recognition Text-to-Speech Dialog
accept Grasp3: 
cancel_failed7: 
complete Grasp9: 
receive (Grasp the apple)1: 
receive (Stop)5: 
initiate Grasp2: say(I am going to grasp the apple.)
4: 
cancel Grasp6: 
say(I can not stop)8: 
say(I finished)10: 
Figure 3: The component communication for a grasp ac-
tion requested by the human. As the dialog manager (DLG)
receives the grasp command, it initiates a grasp task which
is accepted by the arm control. The DLG is notified about
the task state update and acknowledges task execution. As
the human commands cancelling, the DLG sets the task state
cancel. Since the arm control fails to cancel the task, it sets
the task state cancel failed which the DLG reacts on by ex-
pressing an excuse. Finally the task is completed, and the
DLG acknowledges successful task execution.
Mixed-initiative interaction The Pamini dialog
manager offers dialog tasks for other components,
e.g. greeting the human, informing the human
about anything or conversely requesting informa-
tion from the human. While human initiative is re-
alized whenever input from a speech understand-
ing component is received, robot initiative occurs
when a system component requests a dialog task to
be executed. Situation permitting, the dialog man-
ager will accept the dialog task, go into interaction
with the human, and finally complete the dialog
task. Thus, it can react to the system?s and the hu-
man?s initiative using the same task-state protocol
Learning within interaction The task state pro-
tocol supports robotic learning within interaction
by establishing mechanisms for information trans-
fer from the dialog system to the robotic sub-
system. Once information is available from the
human, Pamini augments the task specification
230
with the new information and sets the task state
result available. Since this transition may be
taken multiple times, given information can be
corrected. Also, mixed-initiative enables active
learning, where the learner provokes a situation
providing new information instead of waiting un-
til such situation eventually presents itself.
2.2 Interaction patterns
In an interaction, dialog acts are not unrelated
events, but form coherent sequences. For exam-
ple, a question is usually followed by an answer,
and a request is typically either accepted or re-
jected. Influenced by the concepts of adjacency
pairs (Schegloff and Sacks, 1973), conversation
policies (Winograd, 1986) and software design
patterns, we propose the concept of interaction
patterns that describe recurring dialog structures
on a high level. Interaction patterns can be formal-
ized as transducer augmented with internal state
actions, consisting of
? a set of human dialog acts H and a set of robot dialog
acts R, e.g. H.request or R.assert;
? a set of incoming task events T , e.g. accepted or failed;
? a set of states S representing the interaction state;
? a set of actions A the dialog manager performs, e.g.
initiating or updating a task or reset interaction;
? an input alphabet ? ? (H ? T );
? an output alphabet ? ? R;
? a transition function T : S ? ?? ?? S ?A? ? ??.
By admitting task events as input and internal
actions that perform task initiation and update,
the dialog level is linked with the domain level.
The patterns have been implemented as statecharts
(Harel, 1987), an extended form of finite state ma-
chines, which provides both an executable model
and an understandable graphical representation as
shown in figure 5. For instance, the cancellable
state nameaction, when enteredH.dialog-act / state nameH.dialog-act / R.dialog-act state nametask event / R.dialog-act //
Figure 5: Interaction patterns are represented as transducer
that takes as input human dialog acts and task events and pro-
duces robot dialog acts as output.
action request pattern shown in figure 4 describes
an action request initiated by the human that can
be cancelled during execution. The normal course
of events is that the human requests the action to
be executed, the dialog manager initiates the do-
main task, the responsible system component ac-
cepts execution so that the dialog manager will
assert execution. Finally, the task is completed
and the robot acknowledges. In contrast, the cor-
rectable information request pattern is initiated by
the human. Here, on receiving the respective di-
alog task request, the dialog manager will ask for
the desired information and accept the dialog task.
Once the human provides the answer, the robot
will repeat it as implicit confirmation that can be
corrected if necessary. Table 2 lists all patterns that
have been identified so far.
Initiated by user Initiated by robot
Cancellable action request Self-initiated cancellable action
Simple action request Self-initiated simple action
Information request Correctable information request
Interaction opening Simple information request
Interaction closing Clarification
Interaction restart
System reset
Table 2: Available interaction patterns.
Pattern configuration The patterns themselves
do not determine what kind of task is to be ex-
ecuted or what kind of information to obtain ex-
actly. These specifics are defined by the configu-
ration associated with each pattern, and a concrete
scenario is realized by configuring a set of patterns
using a domain-specific language and registering
them with the dialog manager.
In detail, it needs to be specified for the human?s
dialog acts what kind of (possibly multimodal) in-
put is interpreted as a given dialog act which is
done by formulating conditions over the input. For
the robot?s dialog acts, their surface form needs to
be specified. Up to now, speech output and point-
ing gestures are implemented as output modalities
and can be combined. Moreover, also the task
communication needs to be configured, i.e. the
task specification itself as well as possible task
specification updates. In addition, the developer
can define context variables and use them to pa-
rameterize the robot?s dialog acts and in task spec-
ification updates. This is how e.g. for the robot?s
information request the answer is transferred from
the human to the responsible system component.
Interleaving patterns during interaction Dur-
ing interaction, the registered patterns are em-
ployed in a flexible way by admitting patterns to
be interrupted by other patterns and possibly re-
sumed later which leads to interleaving patterns.
By default, simpler patterns are permitted to be
nested within temporally extended patterns. For
example, it seems reasonable to permit monitoring
questions uttered by the human to be embedded in
the robot?s slow-going grasp execution as shown
231
initiateinitiate-system-task(ShortTerm)stae nemH.d. asserted mimHeloHgm-ogcce/HeR.d.ktgmmeaH
refused 
mimHeloHgm-oaevecHeR.d.ktaenme cancel_requestedupdate-system-task-state(abort)
stcgce.d.
failed mimHeloHgm-ogeR.d.ktg/Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 341?343,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Engagement-based Multi-party Dialog with a Humanoid Robot
David Klotz and Johannes Wienke and Julia Peltason and Britta Wrede and Sebastian Wrede
Applied Informatics Group
Bielefeld University
{dklotz, jwienke, jpeltaso, bwrede, swrede}@techfak.uni-bielefeld.de
Vasil Khalidov and Jean-Marc Odobez
IDIAP Research Institute
{vasil.khalidov, odobez}@idiap.ch
Abstract
When a robot is situated in an environment
containing multiple possible interaction part-
ners, it has to make decisions about when to
engage specific users and how to detect and
react appropriately to actions of the users that
might signal the intention to interact.
In this demonstration we present the integra-
tion of an engagement model in an existing di-
alog system based on interaction patterns. As
a sample scenario, this enables the humanoid
robot Nao to play a quiz game with multiple
participants.
1 Introduction
Giving robotic systems the ability to join in conver-
sation with one or multiple users poses many new
challenges for the development of appropriate dia-
log systems and models. When a dialog system is
situated in the real, physical world and used in more
open settings, more effort needs to be spent on estab-
lishing and maintaining clear communication chan-
nels between the system and its users. E.g. the sys-
tem first needs to detect that there are potential users
with whom interacting would be possible, it needs to
decide if a detected person wants to interact with the
system at all and it needs to make decisions when
and how it should try to start an interaction with that
person.
Bohus and Horvitz (2009) have developed a
model for representing the current relation of a user
with such a system (their engagement state) and de-
termining if they want to be involved in an interac-
tion with the system (using explicit engagement ac-
tions and the more abstract engagement intention).
Each user can be engaged in specific interactions
(denoting different ?basic unit[s] of sustained, in-
teractive problem-solving?) and there can be multi-
ple such interactions, each with potentially different
users.
This demonstration shows how an engagement
model inspired by these ideas was integrated into
an existing dialog system and how it helps in real-
izing interactive scenarios with a robot that incorpo-
rate cues for the dialog from the system?s environ-
ment. Section 3 gives more details about this model
and how it is used by the dialog.
2 Scenario
As a scenario for this demonstration we chose a sim-
ple quiz game involving the robot Nao as a host play-
ing with one or multiple human users. At first, the
robot waits until one of the human interaction part-
ners approaches. When the person opens the interac-
tion (i.e. by greeting the robot), the system responds
with an appropriate greeting. While the person con-
tinues to show the intention to interact with the robot
(determined by the process described in section 3.1),
the robot will ask questions randomly chosen from
a predefined set and will try to judge if the person
answered them correctly.
When another person enters the robot?s field of
view, the system also tries to determine if they have
the intention to interact with it. If that is the case, the
system suspends the current interaction with the first
person and actively tries to engage the second per-
son, encouraging him or her to join the ongoing quiz
game. The prospective new player can then choose
341
Figure 1: Two persons interacting with the developed system.
to join or decline the request.
As long as one of the engaged participants shows
the intention to interact, the robot continues to ask
questions which all participants can try to answer.
The quiz game is stopped either by an explicit re-
quest of one the users or after all participants have
left the scene.
This scenario serves as a good testbed for the in-
tegration of different cues for the engagement model
and how that model affects the actions taken by the
dialog system. The right-hand side of figure 1 shows
two people interacting with the robot during the quiz
game.
3 System Overview
Figure 2 shows an overview of the different com-
ponents involved in the demonstrated system. This
includes components for the perception (e.g. access-
ing images from the robot?s camera and audio from
its microphones), for generating actions (e.g. using
the robot?s text-to-speech system), the dialog system
itself and a memory system for connecting these di-
verse components.
The dialog system used for this demonstration
is called PaMini, which is short for ?Pattern-based
Mixed-Initiative human-robot Interaction? and is
described in more detail by Peltason and Wrede
(2010). This dialog system was modified in Klotz
(2010) with a model of engagement based on the
ideas presented by Bohus and Horvitz (2009). In
our adaptation of this model, there are extension
points for integrating different sources of informa-
tion about the user?s engagement intentions and ac-
tions, described in the following section.
3.1 Determining the User?s Actions &
Intention
For determining the user?s actions (e.g. if the user
explicitly wants to start an interaction with the sys-
tem), this demonstration uses a set of possible utter-
ances which are simply matched against the results
of a speech recognition module.
To get an estimation of the user?s intention to in-
teract, the image from the robot?s camera is first used
to detect the faces of users and to estimate their cur-
rent visual focus of attention. A module based on
a framework by Ba and Odobez (2009) is used to
determine probabilities that the user is looking at
each of a pre-defined list of possible focus targets,
including the robot itself and other users visible in
the scene. The upper left of figure 1 shows a visu-
alization of this module?s output. Nao denotes the
robot as the focus target with the highest probabil-
ity, while the designation UN is short for the ?unfo-
cused? target.
This list of probabilities is then stored in a mem-
342
Figure 2: Components of the developed system.
ory system developed by Wienke and Wrede (2011).
The memory system provides temporal query capa-
bilities which are finally used to guess a user?s cur-
rent intention of interacting with the robot based on
the history of the probabilities that the robot was the
user?s current visual focus of attention target. This
result is also stored in the memory system together
will all other information known about a user.
3.2 Engagement Cues for the Dialog
The dialog system receives the information about the
user?s state and intention from the memory system
and uses it in several rules for controlling its own en-
gagement actions. The intention is e.g. used to deter-
mine if there is a new user that should be persuaded
to join the quiz game described in section 2 and if
any of the users still shows interest so that a new
question should be asked. The general state of the
detected users is also used e.g. to observe when the
users leave the robot?s field of view for a longer pe-
riod of time which causes the dialog system to close
its current interaction.
4 Conclusion
We have shown how an existing dialog system that
was enhanced using an explicit model of engage-
ment can be used to realize interactive scenarios
with a robot that is situated in the physical world.
An estimation of the user?s current visual focus of
attention is used to gauge their intention to engage
the robot in conversation.
A video recording of two people interacting with
the developed system is available online at http:
//youtu.be/pWZLVF2Xa8g
Acknowledgments
This work was done in the HUMAVIPS project,
funded by the European Commission Seventh
Framework Programme, Theme Cognitive Systems
and Robotics, Grant agreement no. 247525.
References
S. Ba and J.-M. Odobez. 2009. Recognizing Visual Fo-
cus of Attention from Head Pose in Natural Meetings.
IEEE Trans. on System, Man and Cybernetics: part B,
Cybernetics, 39:16?34.
Dan Bohus and Eric Horvitz. 2009. Models for multi-
party engagement in open-world dialog. In Proceed-
ings of the SIGDIAL 2009 Conference, pages 225?234,
London, UK. Association for Computational Linguis-
tics.
David Klotz. 2010. Modeling engagement in a multi-
party human-robot dialog. Master?s thesis, Bielefeld
University.
Julia Peltason and Britta Wrede. 2010. Modeling
human-robot interaction based on generic interaction
patterns. In AAAI Fall Symposium: Dialog with
Robots, Arlington, VA, USA.
Johannes Wienke and Sebastian Wrede. 2011. A spatio-
temporal working memory for multi-level data fusion.
In Proc. of IEEE/RSJ International Conference on In-
telligent Robots and Systems. submitted.
343
