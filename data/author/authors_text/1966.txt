Exploiting Paraphrases in a Question Answering System
Fabio Rinaldi, James Dowdall,
Kaarel Kaljurand, Michael Hess
Institute of Computational Linguistics,
University of Zu?rich
Winterthurerstrasse 190
CH-8057 Zu?rich, Switzerland
{rinaldi,dowdall,kalju,hess}
@ifi.unizh.ch
Diego Molla?
Centre for Language Technology,
Macquarie University,
Sydney NSW 2109, Australia
{diego}@ics.mq.edu.au
Abstract
We present a Question Answering system
for technical domains which makes an in-
telligent use of paraphrases to increase the
likelihood of finding the answer to the user?s
question. The system implements a simple
and efficient logic representation of ques-
tions and answers that maps paraphrases
to the same underlying semantic represen-
tation. Further, paraphrases of technical
terminology are dealt with by a separate
process that detects surface variants.
1 Introduction
The problem of paraphrases conceals a number of
different linguistic problems, which in our opinion
need to be treated in separate ways. In fact, para-
phrases can happen at various levels in language. Us-
ing the examples provided in the call for papers for
this workshop, we would like to attempt a simple
classification, without any pretense of being exhaus-
tive:
1. Lexical synonymy.
Example: article, paper, publication
2. Morpho-syntactic variants.
a) Oswald killed Kennedy. / Kennedy was killed
by Oswald.
b) Edison invented the light bulb. / Edison?s
invention of the light bulb.
while (a) is purely syntactical (active vs pas-
sive), (b) involves a nominalisation.
3. PP-attachment.
a plant in Alabama / the Alabama plant
4. Comparatives vs superlatives.
be better than anybody else / be the best
5. Subordinate clauses vs separate sentences linked
by anaphoric pronouns.
The tree healed its wounds by growing new bark.
/ The tree healed its wounds. It grew new bark.
6. Inference.
The stapler costs $10. / The price of the stapler
is $10.
Where is Thimphu located? / Thimphu is capi-
tal of what country?
Of course combinations of the different types are
possible, e.g. Oswald killed Kennedy / Kennedy
was assassinated by Oswald is a combination of (1)
and (2).
Different types of knowledge and different linguis-
tic resources are needed to deal with each of the
above types. While type (1) can be dealt with us-
ing a resource such as WordNet (Fellbaum, 1998),
type (2) needs effective parsing and mapping of syn-
tactic structures into a common deeper structure,
possibly using a repository of nominalisations like
NOMLEX (Meyers et al, 1998). More complex
approaches are needed for the other types, up to
type (6) where generic world knowledge is required,
for instance to know that being a capital of a country
implies being located in that country. 1 Such world
knowledge could be expressed in the form of axioms,
like the following:
(X costs Y) iff (the price of X is Y)
In this paper we focus on the role of paraphrases
in a Question Answering (QA) system targeted at
1Note that the reverse is not true, and therefore this
is not a perfect paraphrase.
technical manuals. Technical documentation is char-
acterised by vast amounts of domain-specific termi-
nology, which needs to be exploited for providing in-
telligent access to the information contained in the
manuals (Rinaldi et al, 2002b). The approach taken
by QA systems is to allow a user to ask a query (for-
mulated in natural language) and have the system
search a background collection of documents in order
to locate an answer. The field of Question Answer-
ing has flourished in recent years2, in part, due to
the QA track of the TREC competitions (Voorhees
and Harman, 2001). These competitions evaluate
systems over a common data set alowing develop-
ers to benchmark performance in relation to other
competitors.
It is a common assumption that technical termi-
nology is subject to strict controls and cannot vary
within a given editing process. However this assump-
tion proves all too often to be incorrect. Unless edi-
tors are making use of a terminology control system
that forces them to use a specific version of a term,
they will naturally tend to use various paraphrases
to refer to the intended domain concept. Besides in
a query a user could use an arbitrary paraphrases of
the target term, which might happen to be one of
those used in the manual itself or might happen to
be a novel one.
We describe some potential solutions to this prob-
lem, taking our Question Answering system as an ex-
ample. We show which benefits our approach based
on paraphrases bring to the system. So far two dif-
ferent domains have been targeted by the system.
An initial application aims at answering questions
about the Unix man pages (Molla? et al, 2000a; Molla?
et al, 2000b). A more complex application targets
the Aircraft Maintenance Manual (AMM) of the Air-
bus A320 (Rinaldi et al, 2002b). Recently we have
started new work, using the Linux HOWTOs as a
new target domain.
In dealing with these domains we have identified
two major obstacles for a QA system, which we can
summarise as follows:
? The Parsing Problem
? The Paraphrase Problem
The Parsing Problem consists in the increased
difficulty of parsing text in a technical domain due to
domain-specific sublanguage. Various types of multi
word expressions characterise these domains, in par-
ticular referring to specific concepts like tools, parts
or procedures. These multi word expressions might
2Although early work in AI already touched upon the
topic, e.g. (Woods, 1977).
include lexical items which are either unknown to
a generic lexicon (e.g. coax cable) or have a spe-
cific meaning unique to this domain. Abbreviations
and acronyms are another common source of incon-
sistencies. In such cases the parser might either
fail to identify the compound as a phrase and con-
sequently fail to parse the sentence including such
items. Alternatively the parser might attempt to
?guess? their lexical category (in the set of open class
categories), leading to an exponential growth of the
number of possible syntactic parses. Not only the in-
ternal structure of the compound can be multi-way
ambiguous, even the boundaries of the compounds
might be difficult to detect and the parsers might
try odd combinations of the tokens belonging to the
compounds with neighbouring tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who can-
not be expected to be completely familiar with the
domain terminology. Even experienced users, who
know very well the domain, might not remember the
exact wording of a compound and use a paraphrase
to refer to the underlying domain concept. Besides
even in the manual itself, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identified as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphrases might be cre-
ated by the users each time they query the system.
In the rest of this paper we describe first our Ques-
tion Answering System (in Section 2) and briefly
show how we solved the first of the two problems
described above. Then, in Section 3 we show in de-
tail how the system is capable of coping with the
Paraphrase Problem. Finally in Section 4 we discuss
some related work.
2 A Question Answering System for
Technical Domains
Over the past few years our research group has devel-
oped an Answer Extraction system (ExtrAns) that
works by transforming documents and queries into a
semantic representation called Minimal Logical Form
(MLF) (Molla? et al, 2000a) and derives the answers
by logical proof from the documents. A full linguis-
tic (syntactic and semantic) analysis, complete with
lexical alternations (synonyms and hyponyms) is per-
formed. While documents are processed in an off-line
stage, the query is processed on-line.
Two real world applications have so far been im-
plemented with the same underlying technology. The
original ExtrAns system (Molla? et al, 2000b) is used
///// a.d electrical coax cable.n4 connects.v062 the.d external antenna.n1 to.o the.d ANT connection.n1 /////
-Wd
ff Dsu ff Ss
-
MVp
-Os
ff Ds
-Js
ff Ds
RW
Figure 1: An Example of LG Output
to extract answers to arbitrary user queries over the
Unix documentation files (?man pages?). A set of
500+ unedited man pages has been used for this ap-
plication. An on-line demo of ExtrAns can be found
at the project web page.3
 Knowledge 
Base
Document
Linguistic
Analysis
Term
processing
Figure 2: Off-line
Processing of Docu-
ments
More recently we tackled
a different domain, the Air-
plane Maintenance Manu-
als (AMM) of the Air-
bus A320 (Rinaldi et al,
2002b), which offered the
additional challenges of an
SGML-based format and a
much larger size (120MB).4
Despite being developed
initially for a specific do-
main, ExtrAns has demon-
strated a high level of do-
main independence.
As we work on relatively
small volumes of data we
can afford to process (in
an off-line stage) all the
documents in our collection
rather than just a few se-
lected paragraphs (see Fig-
ure 2). Clearly in some sit-
uations (e.g. processing in-
coming news) such an ap-
proach might not be fea-
sible and paragraph index-
ing techniques would need
to be used. Our current ap-
proach is particularly tar-
geted to small and medium sized collections.
In an initial phase all multi-word expressions
from the domain are collected and structured in
an external resource, which we will refer to as the
TermBase (Rinaldi et al, 2003; Dowdall et al, 2003).
The document sentences (and user queries) are syn-
tactically processed with the Link Grammar (LG)
parser (Sleator and Temperley, 1993) which uses a
3http://www.ifi.unizh.ch/cl/extrans/
4Still considerably smaller than the size of the docu-
ment collections used for TREC
grammar with a wide coverage of English and has
a robust treatment of ungrammatical sentences and
unknown words. The multi-word terms from the the-
saurus are identified and passed to the parser as sin-
gle tokens. This prevents (futile) analysis of the in-
ternal structure of terms (see Figure 1), simplifying
parsing by 46%. This solves the first of the problems
that we have identified in the introduction (?The
Parsing Problem?).
In later stages of processing, a corpus-based ap-
proach (Brill and Resnik, 1994) is used to deal with
ambiguities that cannot be solved with syntactic in-
formation only, in particular attachments of preposi-
tional phrases, gerunds and infinitive constructions.
ExtrAns adopts an anaphora resolution algorithm
(Molla? et al, 2003) that is based on Lappin and Le-
ass? approach (Lappin and Leass, 1994). The original
algorithm, which was applied to the syntactic struc-
tures generated by McCord?s Slot Grammar (Mc-
Cord et al, 1992), has been ported to the output of
Link Grammar. So far the resolution is restricted to
sentence-internal pronouns but the same algorithm
can be applied to sentence-external pronouns too.
A lexicon of nominalisations based on NOMLEX
(Meyers et al, 1998) is used for the most important
cases. The main problem here is that the semantic
relationship between the base words (mostly, but not
exclusively, verbs) and the derived words (mostly,
but not exclusively, nouns) is not sufficiently sys-
tematic to allow a derivation lexicon to be compiled
automatically. Only in relatively rare cases is the
relationship as simple as with to edit <a text> ?
editor of <a text> / <text> editor, as the effort
that went into building resources such as NOMLEX
also shows.
User queries are processed on-line and converted
into MLFs (possibly expanded by synonyms) and
proved by refutation over the document knowledge
base (see Figure 3). Pointers to the original text at-
tached to the retrieved logical forms allow the system
to identify and highlight those words in the retrieved
sentence that contribute most to that particular an-
swer. When the user clicks on one of the answers
provided, the corresponding document will be dis-
played with the relevant passages highlighted.
 Knowledge 
Base
ANSWERSQuery
Document
Linguistic
Analysis
Paraphrase
Identification
Figure 3: On-line Processing of Queries
The meaning of the documents and of the queries
produced by ExtrAns is expressed by means of Mini-
mal Logical Forms (MLFs). The MLFs are designed
so that they can be found for any sentence (using
robust approaches to treat very complex or ungram-
matical sentences), and they are optimized for NLP
tasks that involve the semantic comparison of sen-
tences, such as Answer Extraction.
The expressivity of the MLFs is minimal in the
sense that the main syntactic dependencies between
the words are used to express verb-argument rela-
tions, and modifier and adjunct relations. However,
complex quantification, tense and aspect, temporal
relations, plurality, and modality are not expressed.
One of the effects of this kind of underspecification
is that several natural language queries, although
slightly different in meaning, produce the same logi-
cal form.
The main feature of the MLFs is the use of reifi-
cation (the expression of abstract concepts as con-
crete objects) to achieve flat expressions (Molla? et
al., 2000b). The MLFs are expressed as conjunc-
tions of predicates with all the variables existentially
bound with wide scope. For example, the MLF of
the sentence ?cp will quickly copy the files? is:
(1) holds(e4), object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type cp and of type command,
there is an entity x6 (a file), there is an entity e4,
which represents a copying event where the first ar-
gument is x1 and the second argument is x6, there
is an entity p3 which states that e4 is done quickly,
and the event e4, that is, the copying, holds. The
entities o1, o2, o3, e4, and p3 are the result of reifi-
cation. The reification of the event, e4, has been used
to express that the event is done quickly. The other
entities are not used in this MLF, but other more
complex sentences may need to refer to the reifica-
tion of properties (adjective-modifying adverbs) or
object predicates (non-intersective adjectives such as
the alleged suspect).
ExtrAns finds the answers to the questions by
forming the MLFs of the questions and then run-
ning Prolog?s default resolution mechanism to find
those MLFs that can prove the question. When no
direct proof for the user query is found, the system
is capable of relaxing the proof criteria in a stepwise
manner. First, hyponyms of the query terms will be
added as disjunctions in the logical form of the ques-
tion, thus making it more general but still logically
correct. If that fails, the system will attempt approx-
imate matching, in which the sentence (or sentences)
with the highest overlap of predicates with the query
is retrieved. The (partially) matching sentences are
scored and the best fits are returned. In the case
that this method finds too many answers because
the overlap is too low, the system will attempt key-
word matching, in which syntactic criteria are aban-
doned and only information about word classes is
used. This last step corresponds approximately to a
traditional passage-retrieval methodology with con-
sideration of the POS tags.
3 Dealing with Paraphrases
The system is capable of dealing with paraphrases
at two different levels. On the phrase level, differ-
ent surface realizations (terms) which refer to the
same domain concept will be mapped into a com-
mon identifier (synset identifier). On the sentence
level, paraphrases which involve a (simple) syntactic
transformation will be dealt with by mapping them
into the same logical form. In this section we will
describe these two approaches and discuss ways to
cope with complex types of parapharases.
3.1 Identifying Terminological Paraphrases
During the construction of the MLFs, thesaurus
terms are replaced by their synset identifiers. This
results in an implicit ?terminological normalization?
for the domain. The benefit to the QA process is
an assurance that a query and answer need not in-
volve exactly the same surface realization of a term.
Utilizing the synsets in the semantic representation
means that when the query includes a term, ExtrAns
returns sentences that logically answer the query, in-
Fastr
Term
Extraction
Hyponymy
Thesaurus ExtrAns
Document
Figure 4: Term Processing
volving any known paraphrase of that term.
For example, the logical form of the query Where
are the stowage compartments installed? is trans-
lated internally into the Horn query (2).
(2) evt(install,A,[B,C]),
object(D,E,[B]),
object(s stowage compartment,G,[C])
This means that a term (belonging to the same
synset as stowage compartment) is involved in an in-
stall event with an anonymous object. If there is
an MLF from the document that can match exam-
ple (2), then it is selected as a candidate answer and
the sentence it originates from is shown to the user.
The process of terminological variation is well
investigated (Ibekwe-SanJuan and Dubois, 2002;
Daille et al, 1996; Ibekwe-Sanjuan, 1998). The
primary focus has been to use linguistically based
variation to expand existing term sets through cor-
pus investigation or to produce domain representa-
tions. A subset of such variations identifies terms
which are strictly synonymous. ExtrAns gathers
these morpho-syntactic variations into synsets. The
sets are augmented with terms exhibiting three
weaker synonymy relations described by Hamon &
Nazarenko (2001). These synsets are organized into
a hyponymy (isa) hierarchy, a small example of which
can be seen in Figure 5. Figure 4 shows a schematic
representation of this process.
The first stage is to normalize any terms that con-
tain punctuation by creating a punctuation free ver-
sion and recording the fact that that the two are
strictly synonymous. Further processing is involved
in terms containing brackets to determine if the
bracketed token is an acronym or simply optional. In
the former case an acronym-free term is created and
the acronym is stored as a synonym of the remain-
ing tokens which contain it as a regular expression.
So evac is synonymous with evacuation and ohsc is
synonymous with overhead stowage compartment. In
cases such as emergency (hard landings) the brack-
eted tokens can not be interpreted as acronyms and
so are not removed.
The synonymy relations are identified using the
terminology tool Fastr (Jacquemin, 2001). Every to-
ken of each term is associated with its part-of-speech,
its morphological root, and its synonyms. Phrasal
rules represent the manner in which tokens combine
to form multi-token terms, and feature-value pairs
carry the token specific information. Metarules li-
cense the relation between two terms by constrain-
ing their phrase structures in conjunction with the
morphological and semantic information on the indi-
vidual tokens.
The metarules can identify simple paraphrases
that result from morpho-syntactic variation (cargo
compartment door ?? doors of the cargo compart-
ment), terms with synonymous heads (electrical ca-
ble ?? electrical line), terms with synonymous mod-
ifiers (fastener strip ?? attachment strip) and both
(functional test ?? operational check). For a de-
scription of the frequency and range of types of vari-
ation present in the AMM see Rinaldi et al (2002a).
3.2 Identifying Syntactic Paraphrases
An important effect of using a simplified semantic-
based representation such as the Minimal Logical
Forms is that various types of syntactic variations
are automatically captured by a common representa-
tion. This ensures that many potential paraphrases
in a user query can map to the same answer into the
manual.
For example the question shown in Figure 6 can
be answered thanks to the combination of two fac-
tors. On the lexical level ExtrAns knows that APU
is an abbreviation of Auxiliary Power Unit, while on
the syntactic level the active and passive voices (sup-
plies vs supplied with) map into the same underlying
representation (the same MLF).
Another type of paraphrase which can be detected
at this level is the kind that was classified as type (3)
in the introduction. For example the question: Is
the sensor connected to the APU ECB?, can locate
the answer This sensor is connected to the Elec-
tronic Control Box (ECB) of the APU. This has been
achieved by introducing meaning postulates that op-
erate at the level of the MLFs (such as ?any predicate
that affects an object will also affect the of -modifiers
of that object?).
3.3 Weaker Types of Paraphrases
When the thesaurus definition of terminological syn-
onymy fails to locate an answer from the docu-
ment collection, ExtrAns explores weaker types of
paraphrases, where the equivalence between the two
terms might not be complete.
TERM
doors of the cargo compartment
cargo compartment door
cargo comparment doors
cargo-compartment door
emergency ( hard landings )
emergency hard landings
emergency hard landing
emergency evacuation (evac)
emergency evacuation
evacuation
evac
electrical cable
electrical line
fastner strip
attachment strip
functional test
operational check
door functional test
stowage compartment
overhead stowage compartment
OHSC
1
2
3
5
6
7
10
9
8
11
Figure 5: A Sample of the TermBase
Figure 6: Active vs Passive Voice
First, ExtrAns makes use of the hyponymy rela-
tions, which can be considered as sort of unidirec-
tional paraphrases. Instead of looking for synset
members, the query is reformulated to included hy-
ponyms and hyperonyms of the terms:
(3) (object(s stowage compartment,A,[B]);
object(s overhead stowage compartment,A,[B])),
evt(install,C,[D,B]),
object(E,F,[D|G])
Now the alternative objects are in a logical OR rela-
tion. This query finds the answer in Figure 7 (where
stowage compartment is a hyperonym of overhead
stowage compartment).
We have implemented a very simple ad-hoc algo-
rithm to determine lexical hyponymy between terms.
Term A is a hyponym of term B if (i) A has more to-
kens than B, (ii) all the tokens of B are present in A,
and (iii) both terms have the same head. There are
three provisions. First, ignore terms with dashes and
brackets as cargo compartment is not a hyponym of
cargo - compartment and this relation (synonymy) is
already known from the normalisation process. Sec-
ond, compare lemmatised versions of the terms to
capture that stowage compartment is a hyperonym
of overhead stowage compartments. Finally, the head
of a term is the rightmost non-symbol token (i.e. a
word) which can be determined from the part-of-
speech tags. This hyponymy relation is compara-
ble to the insertion variations defined by Daille et
al. (1996).
The expressivity of the MLF can further be ex-
panded through the use of meaning postulates of the
type: ?If x is installed in y, then x is in y?. This
ensures that the query Where are the equipment and
furnishings? extracts the answer The equipment and
furnishings are installed in the cockpit.
4 Related Work
The importance of detecting paraphrasing in Ques-
tion Answering has been shown dramatically in
TREC9 by the Falcon system (Harabagiu et al,
2001), which made use of an ad-hoc module capable
of caching answers and detecting question similar-
ity. As in that particular evaluation the organisers
deliberately used a set of paraphrases of the same
questions, such approach certainly helped in boost-
ing the performance of the system. In an environ-
ment where the same question (in different formula-
tions) is likely to be repeated a number of times, a
module capable of detecting paraphrases can signif-
icantly improve the performance of a Question An-
Figure 7: Overhead stowage compartment is a Hyponym of Stowage compartment
swering system.
Another example of application of paraphrases for
Question Answering is given in (Murata and Isahara,
2001), which further argues for the importance of
paraphrases for other applications such Summarisa-
tion, error correction and speech generation.
Our approach for the acquisition of terminological
paraphrases might have some points in common with
the approach described in (Terada and Tokunaga,
2001). The motivation that they bring forward for
the necessity of identifying abbreviations is related to
the problem that we have called ?the Parsing Prob-
lem?.
A very different approach to paraphrases is taken
in (Takahashi et al, 2001) where they formulate the
problem as a special case of Machine Translation,
where the source and target language are the same
but special rules, based on different parameters, li-
cense different types of surface realizations.
Hamon & Nazarenko (2001) explore the termino-
logical needs of consulting systems. This type of IR
guides the user in query/keyword expansion or pro-
poses various levels of access into the document base
on the original query. A method of generating three
types of synonymy relations is investigated using gen-
eral language and domain specific dictionaries.
5 Conclusion
Automatic recognition of paraphrases is an effec-
tive technique to ease the information access bur-
den in a technical domain. We have presented some
techniques that we have adopted in a Question An-
swering system for dealing with paraphrases. These
techniques range from the detection of lexical para-
phrases and terminology variants, to the use of a
simplified logical form that provides the same repre-
sentation for morpho-syntactic paraphrases, and the
use of meaning postulates for paraphrases that re-
quire inferences.
References
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment dis-
ambiguation. In Proc. COLING ?94, volume 2,
pages 998?1004, Kyoto, Japan.
Beatrice Daille, Benot Habert, Christian Jacquemin,
and Jean Royaute?. 1996. Empirical observation of
term variations and principles for their description.
Terminology, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-
SanJuan, and Eric SanJuan. 2003. Complex
structuring of term variants for Question Answer-
ing. In Proc. ACL-2003 Workshop on Multiword
Expressions, Sapporo, Japan.
Christiane Fellbaum 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Thierry Hamon and Adeline Nazarenko. 2001. De-
tection of synonymy links between terms: Experi-
ment and results. In Didier Bourigault, Christian
Jacquemin, and Marie-Claude L?Homme, editors,
Recent Advances in Computational Terminology,
pages 185?208. John Benjamins Publishing Com-
pany.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana G??rju, Vasile Rus, and Paul Morarescu.
2001. Falcon: Boosting knowledge for answer
engines. In Voorhees and Harman (Voorhees and
Harman, 2001).
Fidelia Ibekwe-SanJuan and Cyrille Dubois. 2002.
Can Syntactic Variations Highlight Semantic
Links Between Domain Topics? In Proceedings
of the 6th International Conference on Terminol-
ogy and Knowledge Engineering (TKE02), pages
57?64, Nancy, August.
Fidelia Ibekwe-Sanjuan. 1998. Terminological Vari-
ation, a Means of Identifying Research Topics from
Texts. In Proceedings of COLING-ACL, pages
571?577, Quebec,Canada, August.
Christian Jacquemin. 2001. Spotting and Discover-
ing Terms through Natural Language Processing.
MIT Press.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Com-
putational Linguistics, 20(4):535?561.
Michael McCord, Arendse Bernth, Shalom Lap-
pin, and Wlodek Zadrozny. 1992. Natural lan-
guage processing within a slot grammar frame-
work. International Journal on Artificial Intelli-
gence Tools, 1(2):229?277.
Adam Meyers, Catherine Macleod, Roman Yangar-
ber, Ralph Grishman, Leslie Barrett, and Ruth
Reeves. 1998. Using NOMLEX to produce
nominalization patterns for information extrac-
tion. In Proceedings: the Computational Treat-
ment of Nominals, Montreal, Canada, (Coling-
ACL98 workshop), August.
Diego Molla?, Gerold Schneider, Rolf Schwitter, and
Michael Hess. 2000a. Answer Extraction using
a Dependency Grammar in ExtrAns. Traitement
Automatique de Langues (T.A.L.), Special Issue
on Dependency Grammar, 41(1):127?156.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000b. Extrans, an answer ex-
traction system. T.A.L. special issue on Informa-
tion Retrieval oriented Natural Language Process-
ing.
Diego Molla?, Rolf Schwitter, Fabio Rinaldi, James
Dowdall, and Michael Hess. 2003. Anaphora res-
olution in ExtrAns. In Proceedings of the Interna-
tional Symposium on Reference Resolution and Its
Applications to Question Answering and Summa-
rization, 23?25 June, Venice, Italy.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal model for paraphrasing - using transformation
based on a defined criteria. In Proceedings of the
NLPRS2001 Workshop on Automatic Paraphras-
ing: Theories and Applications.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002a. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Interna-
tional Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?
30 August.
Fabio Rinaldi, James Dowdall, Michael Hess, Diego
Molla?, and Rolf Schwitter. 2002b. Towards An-
swer Extraction: an application to Technical Do-
mains. In ECAI2002, European Conference on Ar-
tificial Intelligence, Lyon, 21?26 July.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, and Magnus Karlsson. 2003. The Role
of Technical Terminology in Question Answering.
In Proceedings of TIA-2003, Terminologie et In-
telligence Artificielle, Strasbourg, April.
Daniel D. Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proc. Third
International Workshop on Parsing Technologies,
pages 277?292.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, and
Kentaro Inui. 2001. Kura: A revision-based
lexico-structural paraphrasing engine. In Proceed-
ings of the NLPRS2001 Workshop on Automatic
Paraphrasing: Theories and Applications.
Akira Terada and Takenobu Tokunaga. 2001. Au-
tomatic disabbreviation by using context informa-
tion. In Proceedings of the NLPRS2001 Workshop
on Automatic Paraphrasing: Theories and Appli-
cations.
Ellen M. Voorhees and Donna Harman, editors.
2001. Proceedings of the Ninth Text REtrieval
Conference (TREC-9), Gaithersburg, Maryland,
November 13-16, 2000.
W.A. Woods. 1977. Lunar rocks in natural English:
Explorations in Natural Language Question An-
swering. In A. Zampolli, editor, Linguistic Struc-
tures Processing, volume 5 of Fundamental Studies
in Computer Science, pages 521?569. North Hol-
land.
Complex structuring of term variants for Question Answering
James Dowdall, Fabio Rinaldi
Institute of Computational Linguistics
University of Zurich
fdowdall,rinaldig@cl.unizh.ch
Fidelia Ibekwe-SanJuan
ERSICO
University of Lyon3
ibekwe@univ-lyon3.fr
Eric SanJuan
LITA EA3097
University of Metz
eric.sanjuan@iut.univ-metz.fr
Abstract
Question Answering provides a method of
locating precise answers to specic ques-
tions but in technical domains the amount
of Multi-Word Terms complicates this task.
This paper outlines the Question Answer-
ing task in such a domain and explores two
ways of detecting relations between Multi-
Word Terms. The rst targets specic se-
mantic relations, the second uses a cluster-
ing algorithm, but they are both based on
the idea of syntactic variation. The pa-
per demonstrates how the combination of
these two methodologies provide sophisti-
cated access to technical domains.
1 Introduction
Nominal compounds are inherently ambiguous on
both the syntactic and semantic fronts. Whilst the
number of syntactic possibilities increase exponen-
tially with word length (Isabelle, 1984), semantic in-
terpretation is at best contextually dependent and in
the worst cases determined by extra-linguistic (prag-
matic) factors.
1
Technical documentation is an at-
tractive domain in which to explore nominal com-
pounds for two reasons. First, they present an abun-
dance of compounds, secondly they restrict semantic
interpretation by excluding compounds with opaque
(extra-linguistic) interpretation. The result is multi-
word terms (MWT) which are both compositional,
their formation is a function of their constituent ele-
ments (Kageura, 2002) and endocentric, the com-
pound is a hyponym of its head (Barker and Sz-
pakowicz, 1998).
1
For example, \apple juice place" (Levi, 1979)
This paper addresses the issue of structuring the
Multi-Word Terms (MWTs) for Question Answer-
ing (QA) in technical domains. The central prob-
lem is that unfamiliarity with MWTs that character-
ize such domains creates an eective barrier against
users nding answers.
Section 2 outlines the domain of focus, the
MWT extraction method and examples character-
istic MWTs. Section 3 explores the QA task in tech-
nical domains by describing the ExtrAns system,
and how it structures the MWTs for the task. Sec-
tion 4 presents TermWatch which identies syntac-
tic variants and uses a hierarchical clustering algo-
rithm to build classes of term variants. The common
ground between these two approaches is in the use
of syntactic variants to structure the terminology as
a whole. Section 5 explores how the resulting struc-
tures can be used in the QA task. After surveying
some related work in Section 6 the paper ends by
drawing conclusions on the approaches presented.
2 MWT Extraction
Before the MWTs can be structured, the terms
need to be extracted from a corpus of texts. This
stage was performed using the INTEX linguistic
parser (Silberztein, 1993). INTEX is a nite state
transducer parser. The corpus used in the present
study concerns scientic publications on the bread-
making process. It was made available by the French
Institute of Scientic Information (INIST). Without
going into much detail regarding the candidate term
extraction rules, the approach adopted can be sum-
marized as selective NLP followed by shallow pars-
ing, much in the same way as (Evans et al, 1992).
We dened morpho-syntactic properties of complex
nominal syntagms written as nite state automata,
implemented in the INTEX linguistic toolbox. IN-
TEX is equipped with linguistic resources to perform
 Knowledge 
Base
Document
Linguistic
Analysis
MWT
processing
(a)
Oine
 Knowledge 
Base
ANSWERS
Query
Document
Linguistic
Analysis
MWT
Recognition
(b)
Online
Figure 1: Schematic: ExtrAns Processing Stages
an overall morpho-syntactic analysis on the texts.
The NP automata are applied in an iterative way
on the corpus until we reach a satisfactory medium-
grained noun phrase splitting. Our concern was to
extract more or less complex terms as they appeared
in the text corpus and not atomic NP extraction.
The rationale was to conserve the associations be-
tween terms as the scientists (authors) made them
during their write-up. Examples of candidate terms
extracted at this stage are: \hydrophilic powdered
lecithin, traditional sour dough starter cultures, de-
velopment of traditional bread avour". More details
on the NP splitting rules can be found in (Ibekwe-
SanJuan, 2001). Manual validation by a domain ex-
pert produced 3651 MWTs.
3 ExtrAns
Question Answering systems attempt to extract
small snippets of text in response to a natural lan-
guage query. Briey, ExtrAns achieves this in two
distinct stages:
O-line the entire document collection is sub-
jected to linguistic analysis, which produces a full
syntactic parse for each sentence. After some inter-
mediate steps, such as anaphora resolution and dis-
ambiguation, the syntactic parse is translated into a
semantic representation designed to capture the core
meaning of the sentences. These representations are
stored in a Knowledge Base.
On-line user queries are subjected to the same
linguistic analysis. The resulting semantic rep-
resentation of the query is `matched' against the
knowledge base. These `matches' can be identied
in their original document location, so users can
contextualize these potential answers. Interest in
the specics of this process should be directed
toward (Rinaldi et al, 2002) (Dowdall et al, 2002).
In dealing with technical domains we have iden-
tied two major obstacles for a QA system which
can be summarize as the Parsing Problem and the
Paraphrase Problem.
The Parsing Problem consists in the increased
di?culty of parsing text in a technical domain due to
domain-specic sublanguage. Various types of MWT
characterize these domains, in particular referring
to specic concepts like tools, parts or procedures.
These multi word expressions might include lexical
items which are either unknown to a generic lexicon
(e.g. \acetic acid") or have a specic meaning unique
to this domain. Abbreviations and acronyms are an-
other common source of incosistencies. In such cases
the parser might either fail to identify the compound
as a phrase and consequently fail to parse the sen-
tence including such items. Alternatively the parser
might attempt to `guess' their lexical category (in
the set of open class categories), leading to an ex-
ponential growth of the number of possible syntactic
parses. Not only the internal structure of the com-
pound can be multi-way ambiguous, even the bound-
aries of the compounds might be di?cult to detect
and the parsers might try odd combinations of the
tokens belonging to the compounds with neighbour-
ing tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who cannot be
expected to be completely familiar with the domain
terminology. Even experienced users, who know the
domain very well, might not remember the exact
wording of a MWT and use a paraphrase to refer to
the underlying domain concept. Besides even in the
documents themselves, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identied as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphases might be cre-
ated by the users each time they query the system.
The task of QA in technical domains is to identify:
`what' needs to be known about `which' multi-word
term. Then to extract sentences that provide the
answer. How to nd the `what' is dependant on the
approach. ExtrAns uses linguistic processing which
results in a semantic representation. However, in
the TREC domain of newswire, considerable success
has been achieved by statistical measures and even
pattern matching. Here, these distinctions are unim-
portant.
What is of concern is in how to meet the two com-
peting search needs of answering specic questions
and navigating through a domain of specialized, un-
familiar MWTs.
Designed specically for technical domains, Ex-
trAns involves strategies for exploiting the abundant
MWTs that these domains hold. The approach uti-
lizes WordNet to gather the MWTs into synonymy
sets based on variation rules. The terminology is also
related through an hyponymy hierarchy.
Synonymy between MWTs is either strict, or
detected through WordNet. Strictly synonymous
MWTs coreference a single object/concept. This
link is a result of morpho-syntactic variation taking
\chemical improver action" and producing the anit-
symmetrical term \action of chemical improver".
The process simply involves inverting the Head and
introducing modiers with a preposition.
WordNet synonymy, on the other hand, comes in
three types of symmetrical variation depending on
which tokens from two MWTs can be found in the
same synset:
 WordNet Head substitution, (\bread ingestion"
and \bread consumption")
 WordNet modier substitution (\quantity of
yeast" and \amount of yeast")
 WordNet Modier and head substitution (\key
ingredient" and \functional component").
However, synonymy identied through WordNet is
dened by WordNet. As a general lexical database
not designed for specilized domains it represents
common synonymy between words. The resulting
links created between multi-word terms translates
into concepts non-specialists cannot easily distin-
guish. These links produced 1277 synsets the vast
majority of which contain two MWTs.
Hyponymy The MWTs are organized into a lex-
ical hyponymy (is a) hierarchy that exploits their
endocentricity (Barker and Szpakowicz, 1998). The
hyponymy relation is identied through two types of
rules, Left Expansions which further modies \dough
stickiness" to be \intense" producing \intense dough
stickiness". Here the original head-modier rela-
tions of the hypernym are unaltered in the hyponym.
However, with Insertion rules these relations are
stickiness
dough
stickiness
surface
stickiness
dough
increase
stickiness
wheat
dough
surface
stickiness
measure
surface
stickiness
intense
dough
stickiness
diminished
dough
stickiness
wheat
dough
stickiness
Figure 2: Hyponymy Hierarchy
changed in the potential hyponym. For example,
whatever is going on in \wheat dough stickiness", in-
serting the word \surface" to produce \wheat dough
surface stickiness" has altered the original head-
modier relations. So a generic/specic relation is
less certain. For the moment such links are permit-
ted.
This process allows multiple parents for a given
term. So \wheat dough surface stickiness" is also
a hyponym of \surface stickiness" through a left-
expansion rule. An example of this kind of hierarchy
can be seen in gure 2.
These two structures are exploited in the search
process during `matching' of queries against answers.
The strengths they bring and the limitations imposed
are explored in Section 5 after description of an al-
ternative approach to term variant structuring.
4 The TermWatch system
TermWatch (Ibekwe-SanJuan and SanJuan, 2003)
clusters term variants into classes, thus producing
a three-level structuring of terms: term, connected
component and class levels. It integrates a visual
interface developed with the Aisee graphic visualiza-
tion to enable the user explore the classes and browse
through the links between terms. Earlier stages of
this work were presented in (Ibekwe-SanJuan, 1998).
The system comprises of two major modules: a
syntactic variant identier and a clustering module
whose results are loaded onto the Aisee visualization
tool.
2
4.1 Variants identier module
Automatic term variant identication has been ex-
tensively explored in (Jacquemin, 2001). In the sec-
tions below, we will recall briey the denitions of
the variation types we identify and give examples
each type.
2
http://www.aisee.com/
Expansions are subdivided along the grammati-
cal axis: those that aect the modier words in a
term and those that aect the head word. Modier
expansions (L-Exp) describes two elementary op-
erations: left-expansion (L-Exp) and Insertion (Ins).
They both denote the addition at the leftmost po-
sition (L-Exp) or inside a term (Insertion or Ins) of
new modier elements. For instance, \gas holding
property of dough" is a left-expansion of \gas holding
property" because by transformation to a nominal
compound structure, we obtain \dough gas holding
property". Likewise, \bread dough quality character-
istics" is an insertion variant (Ins) of \bread char-
acteristics". Head expansions (R-Exp) describes
the addition of one or more nominals in the head po-
sition of a term, thus shifting the former headword
to a modier position. Thus \frozen sweet dough
baking" is a R-Exp of \frozen sweet dough". A com-
bination of the two expansion types yield left-right
expansion (LR-Exp) in that it describes addition of
words both in the modier and head positions. For
example, the relation between \nonstarch polysac-
charide" and \functional property of rye nonstarch
polysaccharide" (\rye nonstarch polysaccharide func-
tional property"). These relations are constrained in
that the added or inserted words have to be con-
tiguous, otherwise, we may not have the expected
semantic relations. Only nominal elements are con-
sidered (nouns, adjectives).
Substitutions are also dened along the gram-
matical axis to yield two sub-types : modier and
head substitution. Modier substitution (M-Sub)
describes the replacing of one modier word in term
t
1
by another word in term t
2
. Thus \bread dough
leavening" is a modier substitution (M-Sub) of
\composite dough leavening". Head substitution
(H-Sub) relates terms which share the same modi-
ers but dierent heads : \eect of xanthan gum"
and \addition of xanthan gum". These relations
are equally constrained in that they can only link
terms of equal length where one and only one item
is dierent, thus guaranteeing the interpretability of
the relations. Substitutions, since they denote non-
directional relations between terms of equal length,
engender symmetrical relations between terms on the
formal level: t
1
t
2
. Their transitive closure cre-
ates classes of terms. For instance, a set of terms
related by modier substitution (M-Sub) seem to
point to a class of \properties/attributes" shared by
a same concept (the head word) as in \bread texture,
endosperm texture, good texture" for binary terms
and \sour corn bread, sour dough bread, sour maize
bread" for ternary terms. In this last case, the chang-
ing properties seem to point to the possible special-
izations (\sour-") of the concept (\bread"). Head
substitution on the other hand gathers together sets
of terms that share the same \properties" (the mod-
ier words), thus creating a class of \concepts". For
instance, the set of term variants \frozen dough bak-
ing, frozen dough characteristics, frozen dough prod-
ucts". The common attribute is \frozen dough",
shared by this class of concepts \products, char-
acteristics, baking". (Ibekwe-SanJuan, 1998) al-
ready put forward the idea of these semantic rela-
tions and (Jacquemin, 1995) reported similar con-
ceptual relations for his insertion and coordination
variants.
4.2 Variant Clustering Module
The second module of TermWatch is a hierarchical
clustering algorithm, CPCL (Classication by Pref-
erential Clustered Link), which clusters terms based
on the variations described above. The six elemen-
tary variation relations are represented as a di-graph.
Clustering is a two-stage process. First the algorithm
builds connected components using a subset of the
variation relations, usually the modier relations (L-
Exp, Ins, M-Sub), these are the COMP relations.
The transitive closure COMP* of COMP partitions
the whole set of terms into components. These con-
nected components are sub-graphs of term variants
that share the same headword. At the second stage,
the connected components are clustered into classes
using the head relations (R-Exp, LR-Exp, H-sub),
this subset of relations is called CLAS. At this stage,
components whose terms are in one of the CLAS re-
lations are grouped basing on an edge dierentiation
coe?cient computed thus:
d
ij
=
X
R2CLAS
n
R
(i; j)
jRj
where CLAS is the set of binary head relations
(Exp D, Exp GD, Sub C), and n
R
(i; j) is the num-
ber of variants of type R between components i and
j. This coe?cient is higher when terms of two com-
ponents share many CLAS relations of a rare type
in the corpus. Components with the highest d
ij
are
clustered rst. The CPCL algorithm can be iterated
several times to suit the user's requirement or un-
til it converges. This means that the user is free to
either set the number of iterations or leave the algo-
rithm to do all the iterations until convergence. The
user only has to specify which set of variations s/he
wants to play the COMP and the CLAS role. In
theory, this distinction is already made in the sys-
tem but the user can change it. On the linguistic
Component 1 component 2
bromate measurement dough stickiness
dough stickiness measurement diminished dough stickiness
dough surface stickiness measurement dough increase stickiness
stickiness measurement intense dough stickiness
measure surface stickiness
soft red winter wheat lines dough stickiness
surface stickiness
wheat dough stickiness
wheat dough surface stickiness
Table 1: Example of a class built by TermWatch.
level, a class contains at least two connected com-
ponents, each comprising of sets of term variants
around the same head word. Class here should be
understood in a formal way: it corresponds to group-
ings of connected components resulting from a hier-
archical clustering algorithm. They are not strictly
dened semantically. Although, we nd semanti-
cally related terms within these classes, the exact
semantic relations involved between pairs of terms
are not explicitly tagged. So on the semantic level,
a class here comprises subsets of term variants re-
ecting, \class of" relations (engendered by substitu-
tions) and \hypernym/hyponym" relations (engen-
dered by modier expansions). For instance, Table
1 displays the term variants found in one class.
This class was built around two components, one
structured around the concept of \stickiness mea-
surement" (most frequent repeated segment) and the
other around the concept of \dough stickiness". We
can observe the COMP relations between term vari-
ants inside each component. The variants that ini-
tiated this class formation are in italics (the ones
sharing CLAS relations).
The TermWatch programs have been implemented
in the AWK language and can run on a Unix or
Windows system. The system is computationally
tractable and processing time is quite acceptable for
real-life applications. For instance, it took 40 sec-
onds on a normal PC running Windows to process a
graph of 3651 term variants and to load the results
onto the Aisee graphic interface. 33 classes of vari-
able sizes were produced at the 3rd iteration of the
clustering algorithm. The smallest class had 4 terms
and the biggest 218 terms! So class size depends very
much on the number and types of variation relations
present in the initial graph.
3
3
TermWatch was initially designed as a scientic and
technology watch system, hence the choices made in
syntactic term variant denitions, the clustering algo-
rithm and visualization mechanisms are tightly related
to this application. A WWW interface is currently under
construction to facilitate the return to the source texts
5 Combining the two systems
The two outlined methodologies use the existence
of syntactic variation between multi-word terms to
structure the terminology as a whole. However, each
approach reects a dierent aspect of this structure.
The ExtrAns approach is designed to identify
explicit relations between terms. The results are
(relatively) small synsets and a hierarchy of types.
For TermWatch, the organizing principle results in
larger classes of terms built around dierent head
words related by syntactic substitution or expansion.
Whilst, not specically targeting semantic relations
the classes do exhibit related terms. Some of these
relations are denable within the classes. For exam-
ple, the class presented in Table 1 contains all of the
hyponyms of \stickiness" identied in ExtrAns (g-
ure 2), but the relations are not rendered explicit in
the class. Also the class contains other terms not
involved in a specic hyponymy relation.
The utility of the classes is in capturing more
\fuzzy" relations between terms whilst avoiding the
problems of trying to dene the relation. For exam-
ple, how can the relation between t
1
: \frozen sweet
dough" and t
2
: \frozen sweet dough baking" be de-
ned ? The most obvious candidate is a part whole
relation but this is defendable only on a formal level:
i.e. t
1
is a subset of t
2
, but does that make t
1
really
a part of t
2
in any semantic sense? In other words, is
\frozen sweet dough" really a part of \frozen sweet
dough baking"?
The TermWatch system does not grapple with this
issue. The interest of these classes for the QA task is
that they exhibit these fuzzy relations. These repre-
sent wider categories of terms to be used for specic
search types. For example, when looking for gen-
eral information on \frozen sweet dough" a user may
well be interested in \baking" it, but when extract-
ing specic information on the same term the rela-
tion is inappropriate. TermWatch was designed orig-
inally for scientic and technological watch (STW).
through hyperlinks.
Term
Extraction
Term
Structure
ExtrAns
Document
synonymy
hyponymy
TermWatch
WordNet
Figure 3: Using the structures in ExtrAns
In this type of application, the expert is less inter-
ested in strict semantic relations between terms in
a taxonomy but more in capturing the association
of research topics in his/her eld. So such \fuzzy"
relations become all important.
Currently ExtrAns uses the synsets and hyponymy
hierarchy during the `matching' of queries against
documents. However, when this fails to locate any-
thing the process is nished without providing users
with any information or any further access into the
domain. What is required is to \relax" the denition
of semantic relation, or facilitate domain investiga-
tion through visualization of the terminology.
The combination of the two methodologies (de-
picted in gure 3) results in a terminology structured
along four levels of granularity. This structure repre-
sents MWTs that are: Strictly synonymous, Word-
Net related, Hierarchy of types and Clustered by
Class.
These levels can be eectively exploited in lo-
cating answers. First, extract potential answers
that involve strictly synonymous MWTs. Second,
look for potential answers with WordNet related
MWTs. Third, try hypernyms/hyponyms of the
search MWT. Finally, allow the user to browse the
classes of MWTs to identify which are of interest in
answer to the question.
TermWatch allows a user-friendly navigation of
the clustering results. Classes are mapped out as
nodes connected by edges whose length denote the
distance between them. The longer the length, the
farther the classes are from one another and thus the
lower their edge coe?cient (d
ij
). The Aisee inter-
face oers standard navigation functions which allow
users to unfold a class into its components and then
into the terms they contain. It thus reects the three-
level structuring eected by the TermWatch mod-
ules.
Figure 4 gives the graphic representation of results
obtained on the corpus. Note that only classes linked
to others are shown in this gure. Classes are la-
beled automatically by the most active term. The
layout points out central or core classes, here classes
(32, 22) which can represent the dominant terminol-
ogy, and by extension, core research topics in the
eld. This layout also brings out interesting con-
gurations like complete graphs and linear graphs.
Complete graphs. The four classes labeled by the
terms \dough behaviour" (32), \wheat our bread"
(29), \wheat bran" (6) and \dough improver" (20)
form a complete graph. They are all linked by sym-
metrical head substitution relations. We found in
these classes term variants like \wheat our dough"
(class 32); \wheat our bread" (class 29), \wheat
our supplementation, wheat our blend, wheat our
fractionation" (class 6), and nally \wheat our com-
position" (class 20). This complete graph is thus
structured around the two modier elements \wheat
our" which can reect a property shared by the
concepts of these four classes. Linear graphs. The
anti-symmetrical relations engendered by insertions
and expansions generate linear graphs, i.e., chains of
relatively long vertices starting from a central class to
the border of the graph. The visualization tool natu-
rally aligns the elements of these linear graphs, thus
highlighting them. For instance, the linear graph
formed by the three classes \dough behaviour" (32),
\frozen dough baking" (10), \dough procedure" (21)
is structured around the set of variants: \frozen
sweet dough (32) ! \frozen sweet dough baking (10)
 \frozen dough baking" (10). The last term \frozen
dough baking" establishes a strong variation relation
with terms in the third class (21) in which we found
the modiers \frozen dough" associated to three dif-
ferent head words: \characteristic, method, prod-
uct".
Given that the syntactic variations which helped
group terms give o semantic links, and given our
restricted denitions of variation relation (see 4.1), a
user seeking information can be oered these class's
contents at this stage in order to see loosely related
terms semantically which a terminological resource
(thesaurus) or WordNet may not have identied.
For instance, in the class shown in Table 1, many
of the terms may not have been related by any se-
mantic relation in WordNet (bromate measurement
and dough stickiness) because none of the head or
the modier words are in any synsets. The clus-
tering algorithm, brings these terms in one class
because \bromate measurement" is a modier sub-
stitution of \stickiness measurement" which is why
they are in the same component. Both tell us some-
thing about \measurement (or rather about measur-
able objects). On the other hand, \dough surface
stickiness measurement", in the same component, is
a left expansion of \stickiness measurement". The
Figure 4: Navigating the clusters of MWTs
two could point to a `hypernym/hyponym' relation.
Thus, from link to link, these terms are connected
to terms of the second component owing to the one
anti-symmetrical link between \dough surface stick-
iness measurement" and \surface stickiness".
From this kind of investigation, a user can choose
the MWTs of interest. This set then becomes the
basis of a second round of answering specic ques-
tions. In this way the system can provide high preci-
sion access to answers, whilst facilitating navigation
through a domain of unfamiliar MWTs.
6 Related Work
The importance of multi-word expressions (MWE)
in various natural language tasks such as auto-
matic indexing, machine translation, information
retrieval/extraction and technology watch need no
longer be proved.
The Multi-word Expression Project aims at study-
ing the properties of a wide range of expressions
including collocations, metaphors and terminology.
The motivation is in explicitly dening the character-
istics of such phrases. The results of the project will
suggest e?cient strategies for overcoming the prob-
lems MWEs cause for NLP applications (Sag et al,
2002)
Much work has been dedicated to the process of
nominal compounding (Levi, 1979) and the seman-
tic interpretation of nominal compounds (Downing,
1977) (Finin, 1980). Other works have addressed
the specic problem of extracting nominal multi-
word expressions for IR applications (Evans et al,
1992) (Smeaton and Sheridan, 1992) (Smadja, 1993)
or of representing them semantically in order to en-
hance IR systems (Popowich et al, 1992) (Gay and
Croft, 1990).
Many systems are dedicated towards structur-
ing terminology for ontology building or terminol-
ogy knowledge base construction (Aussenac-Gilles
et al, 2003). These approaches use the corpus
to identify linguistic markers which in turn point
to certain semantic relations between terms (hy-
pernym/hyponym, synonyms, meronyms). The ap-
proaches we describe are dierent in that relations
are gained through syntactic variations between the
terms.
Active research by the computational terminol-
ogy community (Jacquemin, 2001) (Bourigault et
al., 2001) (Pearson, 1998) has highlighted the im-
portance of discourse as a means of capturing the
essence of terms, hence as a good basis for struc-
turing them. Jacquemin's extensive study has also
highlighted the fact that terms are given to varia-
tions in discourse, so any endeavor to capture the re-
lations between terminological units should integrate
the variation paradigm.
7 Conclusions
Dening and identifying semantic relations between
terms is problematic but can be utilized as part of the
QA process. However, clustering MWTs based on
syntactic variation uncovers classes of terms which
reect more \fuzzy" semantic relations. These are
ideally suited to enabling navigation through the do-
main identifying terms to be used in the Question
Answering process, oering sophisticated access to a
domain. The resulting term structure can be utilized
as a computational thesaurus or incorporated as part
of a larger domain ontology.
References
N. Aussenac-Gilles, B. Biebow, and S. Szulman.
2003. D'une methode a un guide pratique de
modelisation de connaissances a partir de textes.
In Proc. of the 5th Conference on Terminologie
et Intelligence Articielle, Strasbourg, March 31 -
April 1.
K. Barker and S. Szpakowicz. 1998. Semi-Automatic
Recognition of Noun Modier Relationships. In
Proc. of COLING-ACL98, Montreal, Quebec,
Canada, August 10-14.
D. Bourigault, C. Jacquemin, and M-C. L'Homme,
editors. 2001. Recent Advances in Computational
Terminology, volume 2. John Benjamins.
J. Dowdall, M. Hess, N. Kahusk, K. Kaljurand,
M. Koit, F. Rinaldi, and K. Vider. 2002. Tech-
nical Terminology as a Critical Resource. In Proc.
of LREC-02, Las Palmas, 29 { 31 May.
P. Downing. 1977. On the creation and use of english
compound nouns. Language, (53):810 { 842.
D.A Evans, R.G. Leerts, G. Grefenstette, S.K. Han-
derson, W.R. Hersh, and A.A.Archbold. 1992.
CLARIT TREC design, experiments and results.
Technical report, Carnegie Mellon University.
T. Finin. 1980. The semantic interpretation of nom-
inal compounds. In Proceedings "Articial Intelli-
gence, pages 310 { 312. Stanford.
L.S. Gay and W.B. Croft. 1990. Interpreting nomi-
nal compounds for information retrieval. Informa-
tion Processing and Management, 26(1):21 { 38.
F. Ibekwe-SanJuan and E. SanJuan. 2003. From
term variants to research topics. Journal of
Knowledge Organization (ISKO), special issue on
Human Language Technology, 29(3/4).
F. Ibekwe-SanJuan. 1998. Terminological variation,
a means of identifying research topics from texts.
In Proc. of Joint ACL-COLING'98, pages 564 {
570, Quebec, 10-14 August.
F. Ibekwe-SanJuan. 2001. Extraction termi-
nologique avec intex. In Proc.of the 4th Annual
INTEX Workshop, Bordeaux, 10-11 June.
P. Isabelle. 1984. Another look at nominal com-
pounds. In Proc. of the 10th International Con-
ference on Computational Linguistics (COLING
'84), pages 509{516, Stanford, USA.
C. Jacquemin. 1995. A symbolic and surgical ac-
quisition of terms through variation. In Proc. of
IJCAI95, Montreal.
C. Jacquemin. 2001. Spotting and discovering terms
through Natural Language Processing. MIT Press.
K. Kageura. 2002. The dynamics of Terminology: A
descriptive theory of term formation and termino-
logical growth. John Benjamins, Amsterdam.
J. N. Levi. 1979. The syntax and semantics of com-
plex nominals. Academic press, New York.
J. Pearson. 1998. Terms in Context. John Ben-
jamins, Amsterdam.
F. Popowich, P. Mcfetridge, D. Fass, and G. Hall.
1992. Processing complex noun phrases in a natu-
ral language interface to a statistical database. In
Proceedings COLING'92, pages 46 { 51, Nantes,
August 23 { 28.
F. Rinaldi, M. Hess, D. Molla, R. Schwitter, J. Dow-
dall, G. Schneider, and R. Fournier. 2002. Answer
Extraction in Technical Domains. In Proc. of CI-
CLing 2002, Mexico City, February.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword Expressions: a
Pain in the Neck for NLP. In Proc. of CICLing
2002, Mexico City, February.
M. Silberztein. 1993. Dictionnaires Electroniques
et Analyse Lexicale du Francais - Le Systeme IN-
TEX. Masson, Paris.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, (19):143 { 177.
A. F. Smeaton and P. Sheridan. 1992. The appli-
cation of morpho-syntactic language processing to
eective phrase-matching. Information Processing
and Management, 28(3):349 { 369.
Parmenides: an opportunity for ISO TC37 SC4?
Fabio Rinaldi
1
, James Dowdall
1
, Michael Hess
1
, Kaarel Kaljurand
1
, Andreas Persidis
2
,
Babis Theodoulidis
3
, Bill Black
3
, John McNaught
3
, Haralampos Karanikas
3
, Argyris Vasilakopoulos
3
,
Kelly Zervanou
3
, Luc Bernard
3
, Gian Piero Zarri
4
, Hilbert Bruins Slot
5
, Chris van der Touw
5
,
Margaret Daniel-King
6
, Nancy Underwood
6
, Agnes Lisowska
6
, Lonneke van der Plas
6
,
Veronique Sauron
6
, Myra Spiliopoulou
7
, Marko Brunzel
7
, Jeremy Ellman
8
,
Giorgos Orphanos
9
, Thomas Mavroudakis
10
, Spiros Taraviras
10
.
Abstract
Despite the many initiatives in recent years
aimed at creating Language Engineering
standards, it is often the case that dierent
projects use dierent approaches and often
dene their own standards. Even within the
same project it often happens that dierent
tools will require dierent ways to represent
their linguistic data.
In a recently started EU project focusing
on the integration of Information Extrac-
tion and Data Mining techniques, we aim
at avoiding the problem of incompatibility
among dierent tools by dening a Com-
mon Annotation Scheme internal to the
project. However, when the project was
started (Sep 2002) we were unaware of the
standardization eort of ISO TC37/SC4,
and so we commenced once again trying to
dene our own schema. Fortunately, as this
work is still at an early stage (the project
will last till 2005) it is still possible to redi-
rect it in a way that it will be compati-
ble with the standardization work of ISO.
In this paper we describe the status of the
work in the project and explore possible
synergies with the work in ISO TC37 SC4.
1 1
Institute of Computational Linguistics, Uni-
versity of Zurich, Switzerland;
2
Biovista, Athens,
Greece;
3
Centre for Research in Information Manage-
ment, UMIST, Manchester, UK;
4
CNRS, Paris, France;
5
Unilever Research and Development, Vlaardingen,
The Netherlands;
6
TIM/ISSCO, University of Geneva,
Switzerland;
7
Uni Magdeburg, Germany;
8
Wordmap
Ltd., Bath, UK;
9
Neurosoft, Athens, Greece;
10
The
Greek Ministry of National Defense, Athens, Greece
1 Introduction
It is by now widely accepted that some W3C stan-
dards (such as XML and RDF) provide a con-
venient and practical framework for the creation
of eld-specic markup languages (e.g. MathML,
VoiceXML). However XML provides only a common
\alphabet" for interchange among tools, the steps
that need to be taken before there is any real shar-
ing are still many (just as many human languages
share the same alphabets, that does not mean that
they can be mutually intelligible). The necessary
step to achieve mutual understanding in Language
Resources is to create a common data model.
The existence of a standard brings many other
advantages, like the ability to automatically com-
pare the results of dierent tools which provide the
same functionality, from the very basic (e.g. tok-
enization) to the most complex (e.g. discourse rep-
resentation). Some of the NIST-supported competi-
tive evaluations (e.g. MUC) greatly beneted by the
existence of scoring tools, which could automatically
compare the results of each participant against a gold
standard. The creation of such tools (and their ef-
fectiveness) was possible only because the organizing
institute had pre-dened and \imposed" upon the
participants the annotation scheme. However, that
sort of \brute force" approach might not always pro-
duce the best results. It is important to involve the
community in the denition of such standards at an
early stage, so that all the possible concerns can be
met and a wider acceptance can be achieved.
Another clear benet of agreed standards is that
they will increase interoperability among dierent
tools. It is not enough to have publicly available
APIs to ensure that dierent tools can be integrated.
In fact, if their representation languages (their \data
vocabulary") are too divergent, no integration will
be possible (or at least it will require a considerable
mapping eort). For all the above reasons we enthu-
siastically support any concertation work, aimed at
establishing common foundations for the eld.
In a recently started EU project (\Parmenides")
focusing on the integration of Information Extrac-
tion and Data Mining techniques (for Text Mining)
we aim at avoiding the problem of incompatibility
among dierent tools by dening a Common Annota-
tion Scheme internal to the project. However, when
the project was started (Sep 2002) we were unaware
of the standardization eort of ISO TC37 SC4, and
so we commenced once again trying to dene our own
schema. Fortunately, as this work is still at an early
stage (the project will last till 2005) it is still possible
to redirect it in a way that it will be compatible with
the standardization work of ISO.
In this paper we will describe the approach fol-
lowed so far in the denition of the Parmenides Com-
mon Annotation Scheme, even if its relation with ISO
is still only supercial. In the forthcoming months
our intention is to explore possible synergies between
our work and the current initiatives in ISO TC37
SC4, with the aim to get at a Parmenides annota-
tion scheme which is conformant to the approach cur-
rently discussed in the standardization committee.
2 The Parmenides Lingua Franca
In this section we will describe the XML-based anno-
tation scheme proposed for the Parmenides project.
In general terms the project is concerned with or-
ganisational knowledge management, specically, by
developing an ontology driven systematic approach
to integrating the entire process of information gath-
ering, processing and analysis.
The annotation scheme is intended to work as the
projects' lingua franca: all the modules will be re-
quired to be able to accept as input and generate
as output documents conformant to the (agreed) an-
notation scheme. The specication will be used to
create data-level compatibility among all the tools
involved in the project.
Each tool might choose to use or ignore part of
the information dened by the markup: some infor-
mation might not yet be available at a given stage
of processing or might not be required by the next
module. Facilities will need to be provided for lter-
ing annotations according to a simple conguration
le. This is in fact one of the advantages of using
XML: many readily available o-the-shelf tools can
be used for parsing and ltering the XML annota-
tions, according to the needs of each module.
The annotation scheme will be formally dened by
a DTD and an equivalent XML schema denition.
Ideally the schema should remain exible enough to
allow later additional entities when and if they are
needed. However the present document has only an
illustrative purpose, in particular the set of anno-
tation elements introduced needs to be further ex-
panded and the attributes of all elements need to be
veried.
There are a number of simplications which have
been taken in this document with the purpose of
keeping the annotation scheme as simple as possible,
however they might be put into question and more
complex approaches might be required. For instance
we assume that we will be able to identify a unique
set of tags, suitable for all the applications. If this
proves to be incorrect, a possible way to deal with
the problem is the use of XML namespaces. Our
assumptions allow us (for the moment) to keep all
XML elements in the same namespace (and there-
fore ignore the issue altogether).
2.1 Corpus Development
The annotation scheme will be used to create a de-
velopment corpus - a representative sample of the
domain, provided by the users as typical of the doc-
uments they manually process daily. In this phase,
the documents are annotated by domain experts for
the information of interest. This provides the bench-
mark against which algorithms can be developed and
tested to automate extraction as far as possible.
Of primary importance to the annotation process
is the consolidation of the \information of interest",
the text determined as the target of the Information
Extraction modules. Given the projects' goals, this
target will be both diverse and complex necessitating
clarity and consensus.
2.2 Sources Used for this Document
Parmenides aims at using consolidated Information
Extraction techniques, such as Named Entity Ex-
traction, and therefore this work builds upon well-
known approaches, such as the Named Entity anno-
tation scheme from MUC7 (Chinchor, 1997). Cru-
cially, attention will be paid to temporal annota-
tions, with the aim of using extracted temporal in-
formation for detection of trends (using Data Min-
ing techniques). Therefore we have investigated all
the recently developed approaches to such a problem,
and have decided for the adoption of the TERQAS
tagset (Ingria and Pustejovsky, 2002; Pustejovsky et
al., 2002).
Other sources that have been considered include
the GENIA tagset (GENIA, 2003), TEI (TEI Con-
sortium, 2003) and the GDA
1
tagset. The list of
entities introduced so far is by no means complete
1
http://www.i-content.org/GDA/tagset.html
but serves as the starting point, upon which to build
a picture of the domains from information types they
contain. The domain of interests (e.g. Biotechnol-
ogy) are also expected to be terminology-rich and
therefore require proper treatment of terminology.
To supplement the examples presented, a com-
plete document has been annotated according to the
outlined specication.
2
There are currently three
methods of viewing the document which oer dif-
fering ways to visualize the annotations. These
are all based on transformation of the same XML
source document, using XSLT and CSS (and some
Javascript for visualization of attributes). For exam-
ple, the basic view can be seen in gure (1).
3 Levels of Annotation
The set of Parmenides annotations is organized into
three levels:
 Structural Annotations
Used to dene the physical structure of the doc-
ument, it's organization into head and body,
into sections, paragraphs and sentences.
3
 Lexical Annotations
Associated to a short span of text (smaller than
a sentence), and identify lexical units that have
some relevance for the Parmenides project.
 Semantic Annotations
Not associated with any specic piece of text
and as such could be free-oating within the
document, however for the sake of clarity, they
will be grouped into a special unit at the end
of the document. They refer to lexical anno-
tations via co-referential Ids. They (partially)
correspond to what in MUC7 was termed `Tem-
plate Elements' and `Template Relations'.
Structural annotations apply to large text spans,
lexical annotations to smaller text spans (sub-
sentence). Semantic annotations are not directly
linked to a specic text span, however, they are
linked to text units by co-referential identiers.
All annotations are required to have an unique ID
and thus will be individually addressable, this allows
semantic annotations to point to the lexical annota-
tions to which they correspond. Semantic Annota-
tions themselves are given a unique ID, and therefore
can be elements of more complex annotations (\Sce-
nario Template" in MUC parlance).
2
available at http://www.ifi.unizh.ch/Parmenides
3
Apparently the term 'structure' is used with a dif-
ferent meaning in the ISO documentation, referring
to morpho-syntactical structure rather than document
structure.
Structural Annotations The structure of the
documents will be marked using an intuitively appro-
priate scheme which may require further adaptations
to specic documents. For the moment, the root
node is <ParDoc> (Parmenides Document) which
can contain <docinfo>, <body>, <ParAnn>. The
<docinfo> might include a title, abstract or sum-
mary of the documents contents, author informa-
tion and creation/release time. The main body
of the documents (<body>) will be split into sec-
tions (<sec>) which can themselves contain sec-
tions as well as paragraphs (<para>). Within the
paragraphs all sentences will be identied by the
<sentence> tag. The Lexical Annotations will
(normally) be contained within sentences. The -
nal section of all documents will be <ParAnn> (Par-
menides Annotations) where all of the semantic an-
notations that subsume no text are placed. Figure
(2) demonstrates the annotation visualization tool
displaying the documents structure (using nested
boxes).
Lexical Annotations Lexical Annotations are
used to mark any text unit (smaller than a sentence),
which can be of interest in Parmenides. They include
(but are not limited to):
1. Named Entities in the classical MUC sense
2. New domain-specic Named Entities
3. Terms
4. Temporal Expressions
5. Events
6. Descriptive phrases (chunks)
The set of Lexical Annotations described in this
document will need to be further expanded to cover
all the requirements of the project, e.g. names of
products (Acme Arms International's KryoZap (TM)
tear gas riot control gun), including e.g. names of
drugs (Glycocortex's Siderocephalos).
When visualizating the set of Lexical Tags in a
given annotated document, clicking on specic tags
displays the attribute values (see gure (3)).
Semantic Annotations The relations that exist
between lexical entities are expressed through the
semantic annotations. So lexically identied peo-
ple can be linked to their organisation and job ti-
tle, if this information is contained in the document
(see gure (4)). In terms of temporal annotations, it
is the explicit time references and events which are
identied lexically, the temporal relations are then
captured through the range of semantic tags.
Figure 1: Basic Annotation Viewing
3.1 Example
While the structural annotations and lexical annota-
tions should be easy to grasp as they correspond to
accepted notions of document structure and of con-
ventional span-based annotations, an example might
help to illustrate the role of semantic annotations.
(1) The recent ATP award is
<ENAMEX id="e8" type="ORGANIZATION">
Dyax
</ENAMEX>
's second, and follows a
<NUMEX id="n5" type="MONEY">
$4.3 million
</NUMEX>
<ENAMEX id="e9" type="ORGANIZATION">
NIST
</ENAMEX>
grant to
<ENAMEX id="e10" type="ORGANIZATION">
Dyax
</ENAMEX>
and
<ENAMEX id="e11" type="ORGANIZATION">
CropTech Development Corporation
</ENAMEX>
in
<TIMEX3 tid="t4" type="DATE" value="1997">
1997
</TMEX3>
There are two occurrences of Dyax in this short
text: the two Lexical Entities e8 and e10, but clearly
they correspond to the same Semantic Entity. To
capture this equivalence, we could use the syntactic
notion of co-reference (i.e. Identify the two as co-
referent). Another possible approach is to make a
step towards the conceptual level, and create a se-
mantic entity, of which both e8 and e10 are lexical
expressions (which could be dierent, e.g. \Dyax",
\Dyax Corp.", \The Dyax Corporation"). The sec-
ond approach can be implemented using an empty
XML element, created whenever a new entity is men-
tioned in text. For instance, in (2) we can use the tag
Figure 2: Visualization of Structural Annotations
<PEntity> (which stands for Parmenides Entity).
(2) <PEntity peid="obj1" type="ORGANIZATION"
mnem="Dyax" refid="e1 e3 e6 e8 e10 e12"/>
The new element is assigned (as usual) a unique
identication number and a type. The attribute mnem
contains just one of the possible ways to refer to the
semantic entity (a mnemonic name, possibly chosen
randomly). However, it also takes as the value of
the refid attribute as many coreferent ids as are
warranted by the document. In this way all lexical
manifestations of a single entity are identied. All
the lexical entities which refer to this semantic entity,
are possible ways to `name' it (see also g. 4).
Notice that the value of the `type' attribute has
been represented here as a string for readability pur-
poses, in the actual specication it will be a pointer
to a concept in a domain-specic Ontology.
Other semantic entities from (1) are:
(3) <PEntity peid="obj2" type="ORGANIZATION"
mnem="NIST" refid="e2 e4 e7 e9"/>
<PEntity peid="obj3" type="ORGANIZATION"
mnem="CropTech" refid="e11"/>
The newly introduced semantic entities can then
be used to tie together people, titles and organiza-
tions on the semantic level. Consider for example
the text fragment (4), which contains only Lexical
Annotations.
(4) ... said
<ENAMEX id="e17" type="PERSON">
Charles R. Wescott
</ENAMEX>
, Ph.D.,
<ROLE type='x' id="x5">
Senior Scientist
</ROLE>
at
<ENAMEX id="e60" type="ORGANIZATION">
Dyax Corp
</ENAMEX>
The Lexical Entity e17 requires the introduction
of a new semantic entity, which is given the arbitrary
identier `obj5':
(5) <PEntity peid="obj5" type="PERSON"
mnem="Charles R. Wescott" refid="e17"/>
Figure 3: Visualization of Lexical Annotations and their attributes
In turn, this entity is linked to the entity obj1
from (1) by a relation of type `workFor' (PRelation
stands for Parmenides Relation):
(6) <PRelation prid="rel2" source="obj5"
target="obj1" type="worksFor" role="Senior
Scientist" evidence="x5"/>
4 Discussion
As the status of the Parmenides annotation scheme
is still preliminary, we aim in this section to pro-
vide some justication for the choices done so far
and some comparison with existing alternatives.
4.1 Named Entities
One of the purposes of Named Entities is to instanti-
ate frames or templates representing facts involving
these elements. A minor reason to preserve the clas-
sic named entities is so that we can test an IE system
against the MUC evaluation suites and know how
it is doing compared to the competition and where
there may be lacunae. As such, the MUC-7 speci-
cation (Chinchor, 1997) is adopted with the minor
extension of a non-optional identication attribute
on each tag.
4.2 Terminology
A term is a means of referring to a concept of a spe-
cial subject language; it can be a single wordform,
a multiword form or a phrase, this does not matter.
The only thing that matters is that it has special
reference: the term is restricted to refer to its con-
cept of the special domain. The act of (analytically)
dening xes the special reference of a term to a con-
cept. Thus, it makes no sense to talk of a term not
having a denition. A concept is described by den-
ing it (using other certain specialised linguistic forms
(terms) and ordinary words), by relating it to other
concepts, and by assigning a linguistic form (term)
to it.
If we are interested in fact extraction from densely
terminological texts with few named entities apart
from perhaps names of authors, names of laborato-
ries, and probably many instances of amounts and
measures, then we would need to rely much more on
prior identication of terms in the texts, especially
where these are made up of several word forms.
A term can have many variants: even standard-
ised terms have variants e.g. singular, plural forms
of a noun. Thus we should perhaps more correctly
refer to a termform, at least when dealing with text.
Among variants one can also include acronyms and
reduced forms. You therefore nd a set of variants,
typically, all referring to the same concept in a special
domain: they are all terms (or termforms). Again
this problem pinpoints the need for a separation of
the lexical annotations (the surface variants within
the document) and semantic annotations (pointing
abstractly to the underlying concept).
4.3 Approaches to Temporal Annotations
TIDES (Ferro et al, 2001) is a temporal annota-
tion scheme that was developed at the MITRE Cor-
poration and it can be considered as an extension
of the MUC7 Named Entity Recognition (Tempo-
ral Entity Recognition - TIMEX Recognition) (Chin-
chor, 1997). It aims at annotating and normalizing
explicit temporal references. STAG (Setzer, 2001)
is an annotation scheme developed at the University
of She?eld. It has a wider focus than TIDES in
the sense that it combines explicit time annotation,
event annotation and the ability to annotate tempo-
ral relations between events and times.
TimeML (Ingria and Pustejovsky, 2002) stands for
\Time Markup Language" and represents the inte-
gration and consolidation of both TIDES and STAG.
It was created at the TERQAS Workshop
4
and is
designed to combine the advantages of the previous
temporal annotations schemes. It contains a set of
tags which are used to annotate events, time expres-
sions and various types of event-event, event-time
and time-time relations. TimeML is specically tar-
geted at the temporal attributes of events (time of
occurrence, duration etc.).
As the most complete and recent, TimeML should
be adopted for the temporal annotations. Broadly,
its organization follows the Parmenides distinction
between lexical/semantic annotations. Explicit tem-
poral expressions and events receive an appropriate
(text subsuming) lexical tag. The temporal rela-
tions existing between these entities are then cap-
tured through a range of semantic (non-text subsum-
ing) tags.
For example, each event introduces a correspond-
ing semantic tag. There is a distinction be-
tween event \tokens" and event \instances" moti-
vated by predicates that represent more than one
event. Accordingly, each event creates a semantic
<MAKEINSTANCE> tag that subsumes no text. Ei-
ther, one tag for each realised event or a single tag
with the number of events expressed as the value of
the cardinality attribute. The tag is introduced and
the event or to which it refers is determined by the
attributes eventID.
5 Conclusion
We believe that ISO TC37/SC4 provides a very in-
teresting framework within which specic research
concerns can be addressed without the risk of rein-
venting the wheel or creating another totally new
4
http://www.cs.brandeis.edu/~jamesp/arda/time
and incompatible annotation format. The set of an-
notations that we have been targeting so far in Par-
menides is probably a small subset of what is tar-
geted by ISO TC37/SC4. Although we had only lim-
ited access to the documentation available, we think
our approach is compatible with the work being done
in ISO.
It is, we believe, extremely important for a project
like ours, to be involved directly in the ongoing dis-
cussion. Moreover we are at precisely the right stage
for a more direct `exposure' to the ISO TC37/SC4
discussion, as we have completed the exploratory
work but no irrevocable modeling commitment has
so far been taken. Therefore we would hope to be-
come more involved in order to make our proposal
t exactly into that framework. The end result of
this process might be that Parmenides could become
a sort of \Guinea Pig" for at least a subset of ISO
TC37 SC4.
Acknowledgments
The Parmenides project is funded by the European
Commission (contract No. IST-2001-39023) and
by the Swiss Federal O?ce for Education and Sci-
ence (BBW/OFES). All the authors listed have con-
tributed to the (ongoing) work described in this pa-
per. Any remaining errors are the sole responsibility
of the rst author.
References
Nancy Chinchor. 1997. MUC-7 Named Entity Task Denition, Version
3.5. http://www.itl.nist.gov/iaui/894.02/
related projects/muc/proceedings/ne task.html.
Lisa Ferro, Inderjeet Mani, Beth Sundheim, and George Wilson. 2001.
Tides temporal annotation guidelines, version 1.0.2. Technical re-
port, The MITRE Corporation.
GENIA. 2003. Genia project home page. http://www-tsujii.is.s.u-
tokyo.ac.jp/~genia.
Bob Ingria and James Pustejovsky. 2002. TimeML
Specication 1.0 (internal version 3.0.9), July.
http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
James Pustejovsky, Roser Sauri, Andrea Setzer, Rob Giazauskas, and
Bob Ingria. 2002. TimeML Annotation Guideline 1.00 (internal
version 0.4.0), July. http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
Andrea Setzer. 2001. Temporal Information in Newswire Articles: An
Annotation Scheme and Corpus Study. Ph.D. thesis, University of
She?eld.
TEI Consortium. 2003. The text encoding initiative. http://www.tei-
c.org/.
Figure 4: Visualization of Semantic Annotations
Answering Questions in the Genomics Domain
Fabio Rinaldi, James Dowdall, Gerold Schneider
Institute of Computational Linguistics,
University of Zurich, CH-8057 Zurich
Switzerland
{rinaldi, dowdall, gschneid}@cl.unizh.ch
Andreas Persidis
Biovista, 34 Rodopoleos Str.,
Ellinikon, GR-16777 Athens,
Greece
andreasp@biovista.com
Abstract
In this paper we describe current efforts aimed at
adapting an existing Question Answering system to
a new document set, namely research papers in the
genomics domain. The system has been originally
developed for another restricted domain, however it
has already proved its portability. Nevertheless, the
process is not painless, and the specific purpose of
this paper is to describe the problems encountered.
1 Introduction
One of the core problems in exploiting scientific
papers in research and clinical settings is that the
knowledge that they contain is not easily acces-
sible. Although various resources which attempt
to consolidate such knowledge are being created
(e.g. UMLS1, SWISS-PROT, OMIM, GeneOntol-
ogy, GenBank, LocusLink), the amount of informa-
tion available keeps growing exponentially (Stapley
and Benoit, 2000).
There is accordingly a pressing need for intelli-
gent systems capable of accessing that information
in an efficient and user-friendly way. Question An-
swering systems aim at providing a focused way
to access the information contained in a document
collection. Specific research in the area of Ques-
tion Answering has been prompted in the last few
years in particular by the Question Answering track
of the Text REtrieval Conference (TREC-QA) com-
petitions (Voorhees, 2001). The TREC-QA compe-
titions focus on open-domain systems, i.e. systems
that can (potentially) answer any generic question.
As these competitions are based on large volumes
of text, the competing systems (normally) resort to a
relatively shallow text analysis.2 In contrast a ques-
tion answering system working on a restricted do-
main can take advantage of the formatting and style
1
http://www.nlm.nih.gov/research/umls/
2With some notable exception, e.g. (Harabagiu et al, 2001).
conventions in the text, can make use of the specific
domain-dependent terminology, and of full parsing.
In many restricted domains, including technical
documentation and research papers, terminology
plays a pivotal role. This is in fact one of the
major differences between restricted domains and
open domain texts. While in open domain systems
Named Entities play a major role, in technical doc-
umentation, as well as in research papers, they have
a secondary role, by contrast a far greater role is
played by domain terminology. Terminology is a
major obstacle for processing research papers and
at the same time a key access path to the knowledge
encoded in those papers. Terminology provides the
means to name and access domain-specific concepts
and objects.
Restricted domains present the additional prob-
lem of ?domain navigation?. Users of the system
cannot always be expected to be completely fa-
miliar with the domain terminology. Unfamiliar-
ity with domain terminology might lead to ques-
tions which contain imperfect formulations of do-
main terms. It becomes therefore essential to be
able to detect terminological variants and exploit the
relations between terms (like synonymy, meronymy,
antonymy). The process of variation is well in-
vestigated in terminological research (Daille et al,
1996). In the Biomedical domain, an example of a
system that deals with terminological variants (also
called ?aliases?) can be found in (Pustejovsky et al,
2002).
In the rest of this paper we will first briefly de-
scribe our existing Question Answering system, Ex-
trAns (section 2). In the following section (3) we
detail the specific problems encountered in the new
domain and the steps that we have taken to solve
them. We conclude the paper with an overview of
related research (section 4).
Figure 1: Example of document to be analyzed
2 The original Question Answering system
ExtrAns is a Question Answering system aimed at
restricted domains, in particular terminology-rich
domains. While open domain Question Answering
systems typically are targeted at large text collec-
tions and use relatively little linguistic information,
ExtrAns answers questions over such domains by
exploiting linguistic knowledge from the documents
and terminological knowledge about a specific do-
main. Various applications of the ExtrAns system
have been developed, from the original prototype
aimed at the Unix documentation files (Molla? et al,
2000) to a version targeting the Aircraft Mainte-
nance Manuals (AMM) of the Airbus A320 (Molla?
et al, 2003; Rinaldi et al, 2004). In the present pa-
per we describe current work in applying the system
to a different domain and text type: research papers
in the genomics area.
Our approach to Question Answering is particu-
larly computationally intensive; this allows a deeper
linguistic analysis to be performed, at the cost of
higher processing time. The documents are an-
alyzed in an off-line stage and transformed in a
semantic representation (called ?Minimal Logical
Forms? or MLFs), which is stored in a Knowledge
Base (KB). In an on-line phase (see fig. 2) the user
queries are analyzed using the same basic machin-
ery (however the cost of processing them is neg-
ligible, so that there is no visible delay) and their
semantic representation is matched in the KB. If a
match is encountered, the sentences that gave origin
to the match are presented as possible answer to the
question.
Documents (and queries) are first tokenized, then
they go through a terminology-processing module.
If a term belonging to a synset in the terminolog-
ical knowledge base is detected, then the term is
replaced by a synset identifier in the logical form.
This results in a canonical form, where the synset
identifier denotes the concept that each of the terms
in the synset names. In this way any term contained
in a user query is automatically mapped to all its
variants. This approach amounts to an implicit ?ter-
minological normalization? for the domain, where
the synset identifier can be taken as a reference to
SemanticMatching
DocumentKB
document logicalform
AnswersinDocument
DocumentLinguisticProcessingQUERY QueryFiltering
Thesaurus
QUERY+Synset
Figure 2: Schematic representation of the core QA engine
the ?concept? that each of the terms in the synset de-
scribes (Kageura, 2002).
ExtrAns depends heavily on its use of logical
forms, which are designed so that they are easy to
build and to use, yet expressive enough for the task
at hand (Molla?, 2001). The logical forms and asso-
ciated semantic interpretation methods are designed
to cope with problematic sentences, which include
very long sentences, even sentences with spelling
mistakes, and structures that are not recognized by
the syntactic analyzer. An advantage of ExtrAns?
Minimal Logical Forms (MLFs) is that they can be
produced with minimal domain knowledge. This
makes our technology easily portable to different
domains. The only true impact of the domain is
during the preprocessing stage of the input text and
during the creation of a thesaurus that reflects the
specific terms used in the chosen domain, their lex-
ical relations and their word senses.
Unlike sentences in documents, user queries
are processed on-line and the resulting MLFs are
proved by deduction over the MLFs of document
sentences stored in the KB. When no direct answer
for a user query can be found, the system is able to
relax the proof criteria in a stepwise manner. First,
hyponyms are added to the query terms. This makes
the query more general but maintains its logical cor-
rectness. If no answers can be found or the user
determines that they are not good answers, the sys-
tem will attempt approximate matching, in which
the sentence that has the highest overlap of predi-
cates with the query is retrieved. The matching sen-
tences are scored and the best matches are returned.
The MLFs contain pointers to the original text
which allow ExtrAns to identify and highlight those
words in the retrieved sentence that contribute most
to a particular answer. An example of the output of
ExtrAns can be seen in fig. 3. When the user clicks
on one of the answers provided, the corresponding
document will be displayed with the relevant pas-
sages highlighted. Another click displays the an-
swer in the context of the document and allows the
user to verify the justification of the answer.
3 Moving to the new domain
The first step in adapting the system to a new do-
main is identifying the specific set of documents to
be analyzed. We have experimented with two dif-
ferent collections in the genomics domain. The first
collection (here called the ?Biovista? corpus) has
been generated from Medline using two seed term
lists of genes and pathways (biological process) to
extract an initial corpus of research papers (full ar-
ticles). The second collection is constituted by the
GENIA corpus (Kim et al, 2003)3, which contains
2000 abstracts from Medline (a total of 18546 sen-
tences). The advantage of the latter is that domain-
specific terminology is already manually annotated.
However focusing only on that case would mean
disregarding a number of real-world problems (in
particular terminology detection).
3.1 Formatting information
An XML based filtering tool has been used to select
zones of the documents that need to be processed
in a specific fashion. Consider for instance the case
of bibliography. The initial structure of the docu-
ment allows to identify easily each bibliographical
item. Isolating the authors, titles and publication in-
formation is then trivial (because it follows a regular
structure). The name of the authors (together with
the html cross-references) can then be used to iden-
tify the citations within the main body of the paper.
If a preliminary zone identification (as described) is
not performed, the names of the authors used in the
citations would appear as spurious elements within
sentences, making their analysis very difficult.
Another common case is that of titles. Normally
they are Nominal Phrases rather than sentences. If
3
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
Figure 3: Example of interaction with the system
the parser was expecting to find a sentence it would
fail. However using the knowledge that a title is
being processed, we can modify the configuration
of the parser so that it accepts an NP as a correct
parse.
3.2 Terminology
The high frequency of terminology in technical text
produces various problems when locating answers.
A primary problem is the increased difficulty of
parsing text in a technical domain due to domain-
specific sublanguage. Various types of multi-word
terms characterize these domains, in particular re-
ferring to specific concepts (e.g. genome sequences,
proteins). These multi-word expressions might in-
clude lexical items which are either unknown to a
generic lexicon (e.g. ?argentine methylation?), have
a specific meaning unique to this domain or dever-
bal adjectives (and nouns) are often mistagged as
verbs (e.g. ?mediated activation?, ?cell killing?).
Abbreviations and acronyms, often complex (e.g.
bracketed inside NPs, like ?adenovirus (ad) infec-
tion?) are another common source of inconsisten-
cies. In such cases the parser might either fail to
identify the compound as a phrase and consequently
fail to parse the sentence including such items. Al-
ternatively a parser might attempt to ?guess? their
lexical category (in the set of open class categories),
leading to an exponential growth of the number of
possible syntactic parses and often incorrect deci-
sions. Not only the internal structure of the com-
pound can be multi-way ambiguous, also the bound-
aries of the compounds are difficult to detect and the
parsers may try odd combinations of the tokens be-
longing to the compounds with neighboring tokens.
We have described in (Rinaldi et al, 2002) some
approaches that might be taken towards terminology
extraction for a specific domain. The GENIA cor-
pus removes these problems completely by provid-
ing pre-annotated terminological units. This allows
attention to be focused on other challenges of the
QA task, rather than getting ?bogged down? with
terminology extraction and organization.
In the case of the Biovista corpus, we had to
perform a phase of terminology discovery, which
was facilitated by the existence of the seed lists of
genes and pathways. We first marked up those terms
which appear in the corpus using additional xml
tags. This identified 900 genes and 218 pathways
that occur in the corpus - represented as boxed to-
kens in fig. 4. Next the entire corpus is chunked into
nominal and verbal chunks using LT Chunk (Finch
and Mikheev, 1997). Ignoring prepositions and
gerunds the chunks are a minimal phrasal group -
represented as the square braces in fig. 4. The cor-
pus terms are then expanded to the boundary of the
phrasal chunk they appear in. For example, NP3 in
fig. 4 contains two terms of interest producing the
new term ?IFN-induced transcription?. The 1118
corpus terms were expanded into 6697 new candi-
date terms. 1060 involve a pathway in head position
and 1154 a gene. The remaining 4483 candidate
terms involve a novel head with at least one gene or
pathway as a modifier.
Once the terminology is available, it is necessary
to detect relations among terms in order to exploit
Argentine methylation of  STAT1  modulates   IFN -induced  transcription
NP1 VBZ
subj
NP2 NP3
prepmodpp
obj
Figure 4: An example of syntactic analysis
it. We have focused our attention in particular to
the relations of synonymy and hyponymy, which
are detected as described in (Dowdall et al, 2003)
and gathered in a Thesaurus. The organizing unit is
the WordNet style synset which includes strict syn-
onymy as well as three weaker synonymy relations.
These sets are further organized into a isa hierarchy
based on two definitions of hyponymy.
One of the most serious problems that we have
encountered in working in restricted domains is
the syntactic ambiguity generated by multi-word
units, in particular technical terms. Any generic
parser, unless developed specifically for the do-
main at hand, will have serious problems dealing
with those multi-words. The solution that we have
adopted is to parse multi-word terms as single syn-
tactic units. The tokenizer detects the terms (pre-
viously collected in the Thesaurus) as they appear in
the input stream, and packs them into single lexical
tokens prior to syntactical analysis, assigning them
the syntactic properties of their head word. In previ-
ous work this approach has proved to be particularly
effective, bringing a reduction in the complexity of
parsing of 46% (Rinaldi et al, 2002).
3.3 Parsing
The deep syntactic analysis builds upon the chunks
to identify sentence level syntactic relations be-
tween the heads of the chunks. The output is a
hierarchical structure of syntactic relations - func-
tional dependency structures - represented as the di-
rected arrows in fig. 4. The parser (Pro3Gres) uses
hand-written declarative rules to encode acknowl-
edged facts, such as verbs typically take one but
never two subjects, combined with a statistical lan-
guage model that calculates lexicalized attachment
probabilities, similar to (Collins, 1999). Parsing is
seen as a decision process, the probability of a total
parse is the product of probabilities of the individual
decisions at each ambiguous point in the derivation.
Probabilistic parsers generally have the advan-
tage that they are fast and robust, and that they
resolve syntactic ambiguities with high accuracy.
Both of these points are prerequisites for a statistical
analysis that is feasible over large amounts of text
and beneficial to the Q&A system?s performance.
In comparison to shallow processing methods,
parsing has the advantage that relations spanning
long stretches of text can still be recognized, and
that the parsing context largely contributes to the
disambiguation. In comparison to deep linguistic,
formal grammar-based parsers, however, the output
of probabilistic parsers is relatively shallow, pure
context-free grammar (CFG) constituency output,
tree structures that do not include grammatical func-
tion annotation nor co-indexation and empty nodes
annotation expressing long-distance dependencies
(LDD). In a simple example sentence ?John wants
to leave?, a deep-linguistic syntactic analysis ex-
presses the identity of the explicit matrix clause
subject and implicit subordinate clause subject by
means of co-indexing the explicit and the empty im-
plicit subject trace t: ?[John1 wants [t1 to leave]]?.
A parser that fails to recognize these implicit sub-
jects, so-called control subjects, misses very impor-
tant information, quantitatively about 3 % of all sub-
jects.
Although LDD annotation is actually provided in
Treebanks such as the Penn Treebank (Marcus et al,
1993) over which they are typically trained, most
probabilistic parsers largely or fully ignore this in-
formation. This means that the extraction of LDDs
and the mapping to shallow semantic representa-
tions such as MLF is not always possible, because
first co-indexation information is not available, sec-
ond a single parsing error across a tree fragment
containing an LDD makes its extraction impossible,
third some syntactic relations cannot be recovered
Figure 5: Dependency Tree output of the SWI Prolog graphical implementation of the parser
on configurational grounds only.
We therefore adapt ExtrAns to use a new statis-
tical broad-coverage parser that is as fast as a prob-
abilistic parser but more deep-linguistic because it
delivers grammatical relation structures which are
closer to predicate-argument structures and shallow
semantic structures like MLF, and more informative
if non-local dependencies are involved (Schneider,
2003). It has been evaluated and shown to have
state-of-the-art performance.
The parser expresses distinctions that are es-
pecially important for a predicate-argument based
shallow semantic representation, as far as they
are expressed in the Penn Treebank training data,
such as PP-attachment, most LDDs, relative clause
anaphora, participles, gerunds, and the argu-
ment/adjunct distinction for NPs.
In some cases functional relations distinctions
that are not expressed in the Penn Treebank are
made. Commas are e.g. disambiguated between
apposition and conjunction, or the Penn tag IN is
disambiguated between preposition and subordinat-
ing conjunction. Other distinctions that are less rel-
evant or not clearly expressed in the Treebank are
left underspecified, such as the distinction between
PP arguments and adjuncts, or a number of types of
subordinate clauses. The parser is robust in that it
returns the most promising set of partial structures
when it fails to find a complete parse for a sentence.
For sentences syntactically more complex than this
illustrative example, as many hierarchical relations
are returned as possible. A screenshot of its graphi-
cal interface can be seen in fig. 5. Its parsing speed
is about 300,000 words per hour.
Fig. 4 displays the three levels of analysis that are
performed on a simple sentence. Term expansion
yields NP3 as a complete candidate term. However,
NP1 and NP2 form two distinct, fully expanded
noun phrase chunks. Their formation into a noun
phrase with an embedded prepositional phrase is re-
covered from the parser?s syntactic relations giv-
ing the maximally projected noun phrase involv-
ing a term: ?Argentine methylation of STAT1? (or
juxtaposed ?STAT1 Argentine methylation?). Fi-
nally, the highest level syntactic relations (subj
and obj) identifies a transitive predicate relation
between these two candidate terms.
3.4 MLFs
The deep-linguistic dependency based parser partly
simplifies the construction of MLF. First, the map-
ping between labeled dependencies and a surface
semantic representation is often more direct than
across a complex constituency subtree (Schneider,
2003), and often more accurate (Johnson, 2002).
Dedicated labels can directly express complex re-
lations, the lexical participants needed for the con-
struction are more locally available.
Let us look at the example sentence ?Aden-
ovirus infection and transfection were used to model
changes in susceptibility to cell killing caused by
E1A expression?. The control relation (infection
is the implicit subject of model) and the PP rela-
tion (including the description noun) are available
locally. The reduced relative clause killing caused
by is expressed by a local dedicated label (modpart).
Only the conjunction infection and transfection, ex-
pressed here by bracketing, needs to be searched
across the syntactic hierarchy.
This leads to the following MLFs:
object(infection, o1, [o1]).
object(transfection, o2, [o2]).
object(change, o3, [o3]).
object(susceptibility, o4, [o4]).
object(killing, o5, [o5]).
object(expression, o6, [o6]).
object(cell, o7, [o7]).
evt(cause, e3, [o6]).
evt(model, e1, [(o1,o2), o3]).
evt(use, e2, [(o1,o2), e1]).
by(e3, o6).
in(o5, o7).
to(o4, o5).
in(o3, o4).
4 Related Work
Question Answering in Biomedicine is surveyed in
detail in (Zweigenbaum, 2003), in particular regard-
ing clinical questions. An example of a system ap-
plied to such questions is presented in (Niu et al,
2003), where it is applied in a setting for Evidence-
Based Medicine. This system identifies specific
?roles? within the document sentences and the ques-
tions, determining the answers is then a matter of
comparing the roles in each. To this aim, natural
language questions are translated into the PICO for-
mat (Sackett et al, 2000).
Automatic knowledge extraction (or strategies for
improving these methods) over Medline articles are
numerous. For example, (Craven and Kumlien,
1999) identifies possible drug-interaction relations
(predicates) between proteins and chemicals using
a ?bag of words? approach applied to the sentence
level. This produces inferences of the type: drug-
interactions (protein, pharmacologic-agent) where
an agent has been reported to interact with a pro-
tein.
(Sekimizu et al, 1998) uses frequently occurring
predicates and identifies the subject and object ar-
guments in the predication, in contrast (Rindflesch
et al, 2000) uses named entity recognition tech-
niques to identify drugs and genes, then identifies
the predicates which connect them. This type of
?object-relation-object? inference may also be im-
plied (Cimino and Barnet, 1993). This method
uses ?if then? rules to extract semantic relationships
between the medical entities depending on which
MeSH headings these entities appear under. For
example, if a citation has ?Electrocardiography?
with the subheading ?Methods? and has ?Myocar-
dial Infarction? with the subheading ?Diagnosis?
then ?Electrocardiography? diagnoses ?Myocardial
Infarction?.
(Spasic? et al, 2003) uses domain-relevant verbs
to improve on terminology extraction. The co-
occurrence in sentences of selected verbs and can-
didate terms reinforces their termhood. But where
such linguistic inferences are stored in a KB as facts,
statistical inferences are only used to visualize pos-
sible relations between objects for further investiga-
tion. (Stapley and Benoit, 2000) measures statistical
gene name co-occurrence and graphically displays
the results for an expert to investigate the dominant
patterns. The PubMed4 system uses the UMLS to
relate metathesaurus concepts against a controlled
vocabulary used to index the abstracts. This allows
efficient retrieval of abstracts from medical journals,
but it makes use of hyponymy and lexical synonymy
to organize the terms. It collects terminologies from
differing sub-domains in a metathesaurus of con-
cepts.
All such inferences (especially statistical) need to
be verified by an expert to ensure their validity. Syn-
tactic parsing, if any, is reserved to shallow NP iden-
tifying strategies (Sekimizu et al, 1998), or possi-
bly supplemented with PP information (Rindflesch
et al, 2000). Semantic interpretation of the docu-
ments is only attempted through their MeSH head-
ings (Mendonca and Cimino, 1999).
5 Conclusion
This paper documents our approach towards QA in
the genomics domain. Although some aspects of
the work described in this paper are still experimen-
tal, we think that the description of the problems
that we have encountered and the specific solutions
adopted or planned will provide an interesting con-
tribution to the workshop. We conclude by observ-
ing that Question Answering is currently seen as an
?advanced? topic in the Genomics Track of TREC5,
due to be targeted for the first time in Year 2 (2005).
Acknowledgments
The authors wish to thank the organizers of the workshop
and the anonymous reviewers for their helpful comments
and suggestions.
References
J.J. Cimino and G.O. Barnet. 1993. Automatic Knowl-
edge Acquisition from Medline. Methods of Informa-
tion in Medicine, 32(2):120?130.
Michael Collins. 1999. Head-Statistical Models for Nat-
ural Language Processing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, USA.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from
4
http://www.ncbi.nlm.nih.gov/pubmed/
5
http://medir.ohsu.edu/?genomics/roadmap.html
text sources. Proceedings of the 8th International
Conference on Intelligent Systems for Molecular Bi-
ology (ISMB-99).
B. Daille, B. Habert, C. Jacquemin, and J. Roy-
aute?. 1996. Empirical observation of term varia-
tions and principles for their description. Termino-
logy, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-Sanjuan,
and Eric Sanjuan. 2003. Complex Structuring of
Term Variants for Question Answering. In Proc. of the
ACL 03, Workshop on Multiword Expression: Analy-
sis, Acquisition and Treatment, Sapporo, Japan, July.
Steve Finch and Andrei Mikheev. 1997. A Workbench
for Finding Structure in Texts. In Proceedings of Ap-
plied Natural Language Processing, Washington, DC,
April.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Rox-
ana G??rju, Vasile Rus, and Paul Morarescu. 2001.
FALCON: Boosting knowledge for answer engines.
In Ellen M. Voorhees and Donna Harman, editors,
Proceedings of the Ninth Text REtrieval Conference
(TREC-9).
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL, University of Pennsylvania, Philadelphia.
Kyo Kageura. 2002. The Dynamics of Terminology, A
descriptive theory of term formation and terminologi-
cal growth. Terminology and Lexicography, Research
and Practice. John Benjamins Publishing.
J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(1):180?182.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics, 19:313?
330.
E. A. Mendonca and J. J. Cimino. 1999. Automated
Knowledge Extraction from Medline Citations. Med-
ical Informatics.
Diego Molla?, Rolf Schwitter, Michael Hess, and Rachel
Fournier. 2000. ExtrAns, an answer extraction sys-
tem. T.A.L. special issue on Information Retrieval ori-
ented Natural Language Processing, pages 495?522.
Diego Molla?, Fabio Rinaldi, Rolf Schwitter, James Dow-
dall, and Michael Hess. 2003. Answer Extraction
from Technical Texts. IEEE Intelligent Systems.
Diego Molla?. 2001. Ontologically promiscuous flat log-
ical forms for NLP. In Harry Bunt, Ielka van der Sluis,
and Elias Thijsse, editors, Proceedings of IWCS-4,
pages 249?265. Tilburg University.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia
Rodriguez-Gianolli. 2003. Answering clinical ques-
tions with role identification. In Sophia Ananiadou
and Jun?ichi Tsujii, editors, Proceedings of the ACL
2003 Workshop on Natural Language Processing in
Biomedicine, pages 73?80.
J. Pustejovsky, J. Castan?o, R. Saur?i, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creating
Large-scale Information Servers for Biomedical Li-
braries. In ACL 2002 Workshop on Natural Language
Processing in the Biomedical Domain. Philadel-
phia, PA. Available at http://www.medstract.
org/publications.html.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Inter-
national Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?30
August.
Fabio Rinaldi, Michael Hess, James Dowdall, Diego
Molla?, and Rolf Schwitter. 2004. Question answering
in terminology-rich technical domains. In Mark May-
bury, editor, New Directions in Question Answering.
AAAI Press.
T.C. Rindflesch, L. Tanabe, J. N. Weinstein, and
L. Hunter. 2000. Edgar: Extraction of drugs, genes
and relations from the biomedical literature. In Pacific
Symposium on Biocomputing, pages 514?25.
D. L. Sackett, S. E. Straus, W. S. Richardson,
W. Rosenberg, and R. B. Haynes. 2000. Evidence
Based Medicine: How to Practice and Teach EBM.
Churchill Livingstone.
Gerold Schneider. 2003. Extracting and Using Trace-
Free Functional Dependencies from the Penn Tree-
bank to Reduce Parsing Complexity. In Proceedings
of The Second Workshop on Treebanks and Linguis-
tic Theories (TLT 2003), Va?xjo?, Sweden, November
14-15.
T. Sekimizu, H. Park, and J Tsujii. 1998. Identifying the
interaction between genes and gene products based on
frequently seen verbs in Medline abstracts. Genome
Informatics, Universal Academy Press.
Irena Spasic?, Goran Nenadic?, and Sophia Ananiadou.
2003. Using domain-specific verbs for term classifi-
cation. In Sophia Ananiadou and Jun?ichi Tsujii, edi-
tors, Proceedings of the ACL 2003 Workshop on Nat-
ural Language Processing in Biomedicine, pages 17?
24.
B.J. Stapley and G. Benoit. 2000. Bibliometrics: infor-
mation retrieval and visualization from co-occurrence
of gene names in medline abstracts. In Proceedings
of the Pacific Symposium on Biocomputing (Oahu,
Hawaii), pages 529?540.
Ellen M. Voorhees. 2001. The TREC question answer-
ing track. Natural Language Engineering, 7(4):361?
378.
Pierre Zweigenbaum. 2003. Question answering in
biomedicine. In Proc. of EACL 03 Workshop: Natu-
ral Language Processing for Question Answering, Bu-
dapest.
Fast, Deep-Linguistic Statistical Dependency Parsing
Gerold Schneider, Fabio Rinaldi, James Dowdall
Institute of Computational Linguistics, University of Zurich
fgschneid,rinaldig@ifi.unizh.ch, j.m.dowdall@sussex.ac.uk
Abstract
We present and evaluate an implemented sta-
tistical minimal parsing strategy exploiting DG
charateristics to permit fast, robust, deep-
linguistic analysis of unrestricted text, and com-
pare its probability model to (Collins, 1999) and
an adaptation, (Dubey and Keller, 2003). We
show that DG allows for the expression of the
majority of English LDDs in a context-free way
and oers simple yet powerful statistical mod-
els.
1 Introduction
We present a fast, deep-linguistic statistical
parser that prots from DG characteristics and
that uses am minimal parsing strategy. First,
we rely on nite-state based approaches as long
as possible, secondly where parsing is neces-
sary we keep it context-free as long as possible
1
.
For low-level syntactic tasks, tagging and base-
NP chunking is used, parsing only takes place
between heads of chunks. Robust, successful
parsers (Abney, 1995; Collins, 1999) have shown
that this division of labour is particularly at-
tractive for DG.
Deep-linguistic, Formal Grammar parsers
have carefully crafted grammars written by pro-
fessional linguists. But unrestricted real-world
texts still pose a problem to NLP systems that
are based on Formal Grammars. Few hand-
crafted, deep linguistic grammars achieve the
coverage and robustness needed to parse large
corpora (see (Riezler et al, 2002), (Burke et al,
2004) and (Hockenmaier and Steedman, 2002)
for exceptions), and speed remains a serious
challenge. The typical problems can be grouped
as follows.
Grammar complexity Fully comprehensive
grammars are di?cult to maintain and consid-
1
Non-subject WH-question pronouns and support
verbs cannot be treated context-free with our approach.
We use a simple pre-parsing step to analyze them
erably increase parsing complexity.
Parsing complexity Typical formal gram-
mar parser complexity is much higher than
the O(n
3
) for CFG. The complexity of some
formal grammars is still unknown.
2
Pars-
ing algorithms able to treat completely un-
restricted long-distance dependencies are NP-
complete (Neuhaus and Broker, 1997).
Ranking Returning all syntactically possible
analyses for a sentence is not what is expected
of a syntactic analyzer. A clear indication of
preference is needed.
Pruning In order to keep search spaces man-
ageable it is necessary to discard unconvincing
alternatives already during the parsing process.
A number of robust statistical parsers that
oer solutions to these problems have become
available (Charniak, 2000; Collins, 1999; Hen-
derson, 2003). In a statistical parser, the rank-
ing of intermediate structures occurs naturally
and based on empirical grounds, while most
rule-based systems rely on ad hoc heuristics.
With an aggressive beam for parse-time prun-
ing (so in our parser), real-world parsing time
can be reduced to near-linear. If one were to
assume a constantly full xed beam, or uses an
oracle (Nivre, 2004) it is linear in practice
3
.
Also worst-case complexity for exhaustive
parsing is low, as these parsers are CFG-
based (Eisner, 2000)
4
. But they typically pro-
duce CFG constituency data as output, trees
that do not express long-distance dependen-
cies. Although grammatical function and empty
2
For Tree-Adjoining Grammars (TAG) it is O(n
7
) or
O(n
8
) depending on the implementation (Eisner, 2000).
(Sarkar et al, 2000) state that the theoretical bound of
worst time complexity for Head-Driven Phrase Structure
Grammar (HPSG) parsing is exponential.
3
In practical terms, beam or oracle approach have
very similar eects
4
Parsing complexity of the original Collins Models is
O(n
5
), but theoretically O(n
3
) would be possible
Antecedent POS Label Count Description Example
1 NP NP * 22,734 NP trace Sam was seen *
2 NP * 12,172 NP PRO * to sleep is nice
3 WHNP NP *T* 10,659 WH trace the woman who you saw *T*
(4) *U* 9,202 Empty units $ 25 *U*
(5) 0 7,057 Empty complementizers Sam said 0 Sasha snores
(6) S S *T* 5,035 Moved clauses Sam had to go, Sasha said *T*
7 WHADVP ADVP *T* 3,181 WH-trace Sam explained how to leave *T*
(8) SBAR 2,513 Empty clauses Sam had to go, said Sasha (SBAR)
(9) WHNP 0 2,139 Empty relative pronouns the woman 0 we saw
(10) WHADVP 0 726 Empty relative pronouns the reason 0 to leave
Table 1: The distribution of the 10 most frequent types of empty nodes and their antecedents in
the Penn Treebank (adapted from (Johnson, 2002)). Bracketed line numbers only involve LDDs as
grammar artifact
nodes annotation expressing long-distance de-
pendencies are provided in Treebanks such as
the Penn Treebank (Marcus et al, 1993), most
statistical Treebank trained parsers fully or
largely ignore them
5
, which entails two prob-
lems: rst, the training cannot prot from valu-
able annotation data. Second, the extraction
of long-distance dependencies (LDD) and the
mapping to shallow semantic representations is
not always possible from the output of these
parsers. This limitation is aggravated by a lack
of co-indexation information and parsing errors
across an LDD. In fact, some syntactic relations
cannot be recovered on congurational grounds
only. For these reasons, (Johnson, 2002) refers
to them as \half-grammars".
An approach that relies heavily on DG char-
acteristics is explored in this paper. It uses
a hand-written DG grammar and a lexicalized
probability model. It combines the low com-
plexity of a CFG parser, the pruning and rank-
ing advantages of statistical parsers and the
ability to express the majority of LDDs of For-
mal Grammars. After presenting the DG bene-
ts, we dene our DG and introduce our statis-
tical model. Then, we give an evaluation.
2 The Benet of DG Characteristics
In addition to some obvious benets, such as
the integration of chunking and parsing (Abney,
1995), where a chunk largely corresponds to a
nucleus (Tesniere, 1959), or that in an endocen-
tric theory projection can never fail, we present
eight characteristics in more detail, which in
their combination allow us to treat the majority
of English long-distance dependencies (LDD) in
our DG parser Pro3Gres in a context-fee way.
5
(Collins, 1999) Model 2 uses some of the functional
labels, and Model 3 some long-distance dependencies
The ten most frequent types of empty nodes
cover more than 60,000 of the approximately
64,000 empty nodes of sections 2-21 of the Penn
Treebank. Table 1, reproduced from (Johnson,
2002) [line numbers and counts from the whole
Treebank added], gives an overview.
2.1 No Empty Nodes
The fact that traditional DG does not know
empty nodes allows a DG parser to use the e?-
cient 0(n
3
) CYK algorithm.
2.2 Only Content Words are Nuclei
Only content words can be nuclei in a tradi-
tional DG. This means that empty units, empty
complementizers and empty relative pronouns
[lines 4,5,9,10] pose no problem for DG as they
are optional, non-head material. For example, a
complementizer is an optional dependent of the
subordinated verb.
2.3 No External Argument, ID/LP
Moved clauses [line 6] are mostly PPs or clausal
complements of verbs of utterance. Only verbs
of utterance allow subject-verb inversion in af-
rmative clauses [line 8]. Our hand-written
grammar provides rules with appropriate re-
strictions for them, allowing an inversion of the
\canonical" dependency direction under well-
dened conditions, distinguishing between or-
dre lineaire (linear precedence(LP)) and ordre
structural (immediate dominance(ID)). Fronted
positions are available locally to the verb in a
theory that does not posit a distinction between
internal and external arguments.
2.4 Exploiting Functional DG Labels
The fact that dependencies are often labeled is
a main dierence between DG and constituency.
We exploit this by using dedicated labels to
model a range of constituency LDDs, relations
Relation Label Example
verb{subject subj he sleeps
verb{rst object obj sees it
verb{second object obj2 gave (her) kisses
verb{adjunct adj ate yesterday
verb{subord. clause sentobj saw (they) came
verb{prep. phrase pobj slept in bed
noun{prep. phrase modpp draft of paper
noun{participle modpart report written
verb{complementizer compl to eat apples
noun{preposition prep to the house
Table 2: Important Pro3Gres Dependency
types
spanning several constituency levels, including
empty nodes and functional Penn Treebank la-
bels, by a purely local DG relation
6
. The selec-
tive mapping patterns for MLE counts of pas-
sive subjects and control subjects from the Penn
Treebank, the most frequent NP traces [line 1],
are e.g. (@ stands for arbitrary nestedness):
?
h
h
h
h
(
(
(
(
NP-SBJ-X@
noun
VP@
h
h
h
(
(
(
V
passive verb
NP
-NONE-
*-X
?
h
h
h
h
(
(
(
(
NP-SBJ-X@
noun
VP@
h
h
h
(
(
(
V
control-verb
S
NP-SBJ
-NONE-
*-X
Our approach employs nite-state approxima-
tions of long-distance dependencies, described
in (Schneider, 2003) for DG and (Cahill et al,
2004) for Lexical Functional Grammar (LFG)It
leaves empty nodes underspecied but largely
recoverable. Table 2 gives an overview of im-
portant dependencies.
2.5 Monostratalism and Functionalism
While multistratal DGs exist and several de-
pendency levels can be distinguished (Mel'cuk,
1988) we follow a conservative view close to the
original (Tesniere, 1959), which basically parses
directly for a simple LFG f-structure without
needing a c-structure detour.
6
In addition to taking less decisions due to the gained
high-level shallowness, it is ensured that the lexical in-
formation that matters is available in one central place,
allowing the parser to take one well-informed decision in-
stead of several brittle decisions plagued by sparseness.
Collapsing deeply nested structures into a single depen-
dency relation is less complex but has a similar eect as
selecting what goes in to the parse history in history-
based approaches.
2.6 Graphs
DG theory often conceives of DG structures
as graphs instead of trees (Hudson, 1984). A
statistical lexicalized post-processing module
in Pro3Gres transforms selected subtrees into
graphs, e.g. in order to express control.
2.7 Transformation to Semantic Layer
Pro3Gres is currently being applied in a Ques-
tion Answering system specically targeted at
technical domains (Rinaldi et al, 2004b). One
of the main advantages of a DG parser such as
Pro3Gres over other parsing approaches is that
a mapping from the syntactic layer to a seman-
tic layer (meaning representation) is partly sim-
plied (Molla et al, 2000).
2.8 Tesniere's Translations
The possible functional changes of a word called
translations (Tesniere, 1959) are an exception
to endocentricity. They are an important con-
tribution to a traceless theory. Gerunds (af-
ter winning/VBG the race) or innitives [line
2] may function as nouns, obviating the need
for an empty subject. In nounless NPs such as
the poor, adjectives function as nouns, obviating
the need for an empty noun head. Participles
may function as adjectives (Western industrial-
ized/VBN countries), again obviating the need
for an empty subject.
3 The Statistical Dependency Model
Most successful deep-linguistic Dependency
Parsers (Lin, 1998; Tapanainen and Jarvinen,
1997) do not have a statistical base. But one
DG advantage is precisely that it oers simple
but powerful statistical Maximum Likelihood
Estimation (MLE) models. We now dene our
DG and the probability model.
The rules of a context-free, unlabeled DG
are equivalent to binary-branching CFG rewrite
rules in which the head and the mother node are
isomorphic. When converting DG structures to
CFG, the order of application of these rules is
not necessarily known, but in a labeled DG, the
set of rules can specify the order (Covington,
1994). Fig. 1 shows such two structures, equiv-
alent except for the absence of functional la-
bels in CFG. Subj (but not PP ) has been used
in this example conversion to specify the appli-
cation order, hence we get a repetition of the
eat/V node, mirroring a traditional CFG S and
VP distinction.
In a binary CFG, any two constituents A and
B which are adjacent during parsing are candi-
ROOT the man eats apples with a fork
W
SENT

Subj

Det
W
Obj
W
PP
W
PObj

Det
eat/V
h
h
h
h
h
(
(
(
(
(
man/N
X
X


the/D
the
man/N
man
eat/V
h
h
h
h
h

(
(
(
(
(
eat/V
eats
apple/N
apples
with/P
h
h
(
(
with/P
with
fork/N
X
X


a/D
a
fork/N
fork
Figure 1: DG and CFG representation
dates for the RHS of a rewrite rule. As terminal
types we use word tags.
X ! AB; e:g:NP ! DT NN (1)
In DG, one of these is isomorphic to the LHS,
i.e. the head. This grammar is also a Bare
Phrase Structure grammar known from Mini-
malism (Chomsky, 1995).
B ! AB; e:g: NN ! DT NN (2)
A ! AB; e:g: V B ! V B PP (3)
Labeled DG rules additionally use a syntactic
relation label R. A non-lexicalized model would
be:
p(RjA ! AB)

=
#(R;A ! AB)
#(A ! AB)
(4)
Research on PCFG and PP-attachment has
shown the importance of probabilizing on lexical
heads (a and b).
p(RjA ! AB;a; b)

=
#(R;A ! AB; a; b)
#(A ! AB; a; b)
(5)
All that A ! AB expresses is that the depen-
dency relation is towards the right.
p(Rjright; a; b)

=
#(R; right; a; b)
#(right; a; b)
(6)
e.g. for the Verb-PP attachment relation pobj
(following (Collins and Brooks, 1995) including
the description noun
7
)
p(pobjjright; verb; prep; desc:noun)

=
#(pobj; right; verb; prep; desc:noun)
#(right; verb; prep; desc:noun)
The distance (measured in chunks) between a
head and a dependent is a limiting factor for the
probability of a dependency between them.
p(R; distjright; a; b)

=
#(R; dist; right; a; b)
#(right; a; b)
(7)
7
PP is considered to be an exocentric category, since
both the preposition and the description noun can be
seen as head; in LFG they appear as double-head
Many relations are only allowed towards one di-
rection, the left/right factor is absent for them.
Typical distances mainly depend on the rela-
tion. Objects usually immediately follow the
verb, while a PP attached to the verb may easily
follow only at the second or third position, after
the object and other PPs etc. By application of
the chain rule and assuming that distance is in-
dependent of the lexical heads we get:
p(R; distja; b)

=
#(R; a; b)
#(a; b)

#(R; dist)
#R
(8)
We now explore Pro3Gres' main probability
model by comparing it to (Collins, 1999), and
an adaptation of it, (Dubey and Keller, 2003).
3.1 Relation of Pro3Gres to Collins
Model 1
We will rst consider the non-generative Model
1 (Collins, 1999). Both (Collins, 1999) Model
1 and Pro3Gres are mainly dependency-based
statistical parsers over heads of chunks, a
close relation can thus be expected. The
(Collins, 1999) Model 1 MLE estimation is:
P (Rjha; atagi; hb; btagi; dist)

=
#(R; ha; atagi; hb; btagi; dist)
#(ha; atagi; hb; btagi; dist)
(9)
Dierences in comparison to (8) are:
 Pro3Gres does not use tag information.
This is because, rst, the licensing hand-
written grammar is based on Penn tags.
 The second reason for not using tag infor-
mation is because Pro3Gres backs o to se-
mantic WordNet classes (Fellbaum, 1998)
for nouns and to Levin classes (Levin, 1993)
for verbs instead of to tags, which has the
advantage of being more ne-grained.
 Pro3Gres uses real distances, measured in
chunks, instead of a feature vector. Dis-
tance is assumed to be dependent only on
R, which reduces the sparse data problem.
(Chung and Rim, 2003) made similar ob-
servations for Korean.
 The co-occurrence count in the MLE de-
nominator is not the sentence-context, but
the sum of counts of competing relations.
E.g. the object and adjunct relation are
in competition, as they are licensed by the
same tag sequence V B NN. Pro3Gres
models attachment (thus decision) proba-
bilities, viewing parsing as a decision pro-
cess.
 Relations (R) have a Functional DG de-
nition, including LDDs.
3.2 Relation to Collins Model 2
(Collins, 1999) Model 2 extends the parser to in-
clude a complement/adjunct distinction for NPs
and subordinated clauses, and it includes a sub-
categorisation frame model.
For the subcategorisation-dependent genera-
tion of dependencies in Model 2, rst the prob-
abilities of the possible subcat frames are calcu-
lated and the selected subcat frame is added as
a condition. Once a subcategorized constituent
has been found, it is removed from the subcat
frame, ensuring that non-subcategorized con-
stituents cannot be attached as complement,
which is one of the two major function of a
subcat frame. The other major function of a
subcat frame is to nd all the subcategorized
constituents. In order to ensure this, the prob-
ability when a rewrite rule can stop expanding
is calculated. Importantly, the probability of
a rewrite rule with a non-empty subcat frame
to stop expanding is low, the probability of a
rewrite rule with an empty subcat frame to stop
expanding is high.
Pro3Gres includes a complement/adjunct dis-
tinction for NPs. The examples given in sup-
port of the subcategorisation frame model in
(Collins, 1999) Model 2 are dealt with by the
hand-written grammar in Pro3Gres.
Every complement relation type, namely
subj, obj, obj2, sentobj, can only occur once per
verb, which ensures one of the two major func-
tions of a subcat frame, that non-subcategorized
constituents cannot be attached as comple-
ments. This amounts to keeping separate sub-
cat frames for each relation type, where the se-
lection of the appropriate frame and removing
the found constituent coincide, which has the
advantage of a reduced search space: no hy-
pothesized, but unfound subcat frame elements
need to be managed. As for the second major
function of subcat frames { to ensure that if pos-
sible all subcategorized constituents are found {
the same principle applies: selection of subcat
frame and removing of found constituents coin-
cide; lexical information on the verb argument
candidate is available at frame selection time al-
ready. This implies that Collins Model 2 takes
an unnecessary detour.
As for the probability of stopping the expan-
sion of a rule { since DG rules are always binary
{ it is always 0 before and 1 after the attach-
ment. But what is needed in place of interrela-
tions of constituents of the same rewrite rule is
proper cooperation of the dierent subcat types.
For example, the grammar rules only allow a
noun to be obj2 once obj has been found, or a
verb is required to have a subject unless it is
non-nite or a participle, or all objects need to
be closer to the verb than a subordinate clause.
3.3 Relation to Dubey & Keller 03
(Dubey and Keller, 2003) address the ques-
tion whether models such as Collins also im-
prove performance on freer word order lan-
guages, in their case German. German is con-
siderably more inectional which means that
discarding functional information is more harm-
ful, and which explains why the NEGRA an-
notation has been conceived to be quite at
(Skut et al, 1997). (Dubey and Keller, 2003)
observe that models such as Collins when ap-
plied directly perform worse than an unlexical-
ized PCFG baseline. The fact that learning
curves converge early indicates that this is not
mainly a sparse data eect. They suggest a lin-
guistically motivated change, which is shown to
outperform the baseline.
The (Collins, 1999) Model 2 rule generation
model for P ! L
m
:::L
1
HR
1
:::R
n
, is
P (RHSjLHS) = P
h
(HjP; t(P ); l(P ))

m
Y
i=0
P
l
(L
i
; t(L
i
); l(L
i
)jP;H; t(H); l(H); d(i))

n
Y
i=0
P
r
(R
i
; t(R
i
); l(R
i
)jP;H; t(H); l(H); d(i))
P
h
P of head t(H) tag of H head word
LHS left-hand side RHS right-hand side
P
l:1::m
P(words left of head) P
r:1::n
P(words right of head)
H LHS Head Category P RHS Mother Category
L left Constit. Cat. R right Constit. Cat.
l(H) head word of H d distance measure
Dubey & Keller suggest the following change
in order to respect the NEGRA atness: P
h
is
left unchanged, but P
l
and P
r
are conditioned
on the preceding sister instead of on the head:
P (RHSjLHS) = P
h
(HjP; t(P ); l(P ))

m
Y
i=0
P
l
(L
i
; t(L
i
); l(L
i
)jP;L
i 1
; t(L
i 1
); l(L
i 1
); d(i))

n
Y
i=0
P
r
(R
i
; t(R
i
); l(R
i
)jP;R
i 1
; t(R
i 1
); l(R
i 1
); d(i))
Their new model performs considerably better
and also outperforms the unlexicalized baseline.
The authors state that \[u]sing sister-head re-
lationships is a way of counteracting the at-
ness of the grammar productions; it implicitly
adds binary branching to the grammar." (ibid.).
DG is binary branching by denition; adding
binary branching implicitly converts the CFG
rules into an ad-hoc DG.
Whether the combination ((Chomsky, 1995)
merge) of two binary constituents directly
projects to a \real" CFG rule LHS or an im-
plicit intermediate constituent does not matter.
Observations
 What counts is each individual Functional
DG dependency, no matter whether it is ex-
pressed as a sister-head or a head-head de-
pendency, or stretches across several CFG
levels (control, modpart etc.)
 Not adjacency (i,i-1) but headedness
counts. Instead of conditioning on the pre-
ceding (i-1) sister, conditioning on the real
DG head is linguistically more motivated
8
.
 Not adjacency (i,i-1) but the type of GR
counts: the question why Dubey & Keller
did not use the NEGRA GR labels has to
arise when discussing a strongly inectional
language such as German.
 The use of a generative model, calculating
the probability of a rule and ultimately the
probability of producing a sentence given
the grammar only has theoretical advan-
tages. For practical purposes, modeling
parsetime decision probabilities is as valid.
With these observations in mind, we can com-
pare Pro3Gres to (Dubey and Keller, 2003).
As for the Base-NP Model, Pro3Gres only re-
spects the best tagging & chunking result re-
ported to it { a major source of errors (see sec-
tion 4). In DG, projection (although not ex-
pansion) is deterministic. H and P are usually
isomorphic, if not Tesniere-translations are rule-
based. Since in DG, only lexical nodes are cat-
egories, P=t(P). P
h
is thus l(h), the prior, we
ignore it for maximizing. In analogy, also cat-
egory (L/R) and their tags are identical. The
revised formula is
P (RHSjLHS)

=
l(h)

m
Y
i=0
P
l
(t(L
i
); l(L
i
)jP; t(L
i 1
); l(L
i 1
); d(i))

n
Y
i=0
P
r
(t(R
i
); l(R
i
)jP; t(R
i 1
); l(R
i 1
); d(i))
If a DG rule is head-right, P is L
i
or R
i
, if
it is head-left, P is L
i 1
or R
i 1
, respectively.
8
In primarily right-branching languages such as En-
glish or German (i-1) actually amounts to being the head
in the majority of, but not all cases. In a more functional
DG perspective such as the one taken in Pro3Gres, these
languages turn out to be less right-branching, however,
with prepositions or determiners analyzed as markers to
the nominal head or complementizers or relative pro-
nouns as markers to the verbal head of the subclause.
Headedness and not direction matters. L
i
/R
i
is replaced by H
i
and L/R
i 1=i+1
by H'. H' is
understood to be the DG dependent, although,
as mentioned, H' could also be the DG head in
this implicit ad-hoc DG.
P (RHSjLHS)

=
l(h)

n+m
Y
i=0
P
l;r
(t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
); d(i))
P (t(H
i
)jt(H
i
); t(H
0
i
)) is a projection or
attachment grammar model modeling the
unlexicalized probability of t(H) and t(H')
participating in a binary rule with t(H) as
head { the merge probability in Bare Phrase
Structure (Chomsky, 1995); an unlabeled ver-
sion of (4). P (t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
))
is a lexicalized version of the same pro-
jection or attachment grammar model;
P (t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
; d(i))) in
addition conditions on the distance
9
. Pro3Gres
expresses the unlexicalized rules by licensing
grammar rules for relation R. Tags are not used
in Pro3Gres' model, because semantic backos
and tag-based licensing rules are used.
P (d(i)jl(H
i
); l(H
0
i
)) (10)
The Pro3Gres main MLE estimation (8)
(l(H) = a; l(H
0
) = b) diers from (10) by using
labeled DG, and thus from the Dubey & Keller
Model by using a consistent functional DG.
4 Evaluation
(Lin, 1995; Carroll et al, 1999) suggest eval-
uating on the linguistically meaningful level of
dependency relations. Two such evaluations are
reported now.
First, a general-purpose evaluation using a
hand-compiled gold standard corpus (Carroll et
al., 1999), which contains the grammatical re-
lation data of 500 random sentences from the
Susanne corpus. The performance (table 3), ac-
cording to (Preiss, 2003), is similar to a large
selection of statistical parsers and a grammat-
ical relation nder. Relations involving LDDs
form part of these relations. A selection of them
is also given: WH-Subject (WHS), WH-Object
(WHO), passive Subject (PSubj), control Sub-
ject (CSubj), and the anaphor of the relative
clause pronoun (RclSubjA).
9
Since normalized probabilities are used
P (t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
; d(i))) =
P (t(H
i
); d(i)jt(H
i
); t(H
0
i
); l(H
i
); l(H
0
i
))
CARROLL Percentages for some relations, general, on Carroll testset only LDD-involving
Subject Object noun-PP verb-PP subord. clause WHS WHO PSubj CSubj RclSubjA
Precision 91 89 73 74 68 92 60 n/a 80 89
Recall 81 83 67 83 n/a 90 86 83 n/a 63
GENIA Percentages for some relations, general, on GENIA corpus
Subject Object noun-PP verb-PP subord. clause
Precision 90 94 83 82 71
Recall 86 95 82 84 75
Table 3: Evaluation on Carroll's test suite on subj, obj, PP-attachment and clause subord. relations
and a selection of 5 LDD relations, and on the terminology-annotated GENIA corpus
Secondly, to answer how the parser performs
over domains markedly dierent to the train-
ing corpus, to test whether terminology is the
key to a successful parsing system, and to assess
the impact of chunking errors, the parser has
been applied to the GENIA corpus (Kim et al,
2003), 2000 MEDLINE abstracts of more than
400,000 words describing the results of Biomed-
ical research, which is annotated for multi-word
terms and thus contains near-perfect chunking.
100 random sentences from the GENIA corpus
have been manually annotated and compared to
the parser output (Rinaldi et al, 2004a).
5 Conclusions
We have discussed how DG allows the expres-
sion of the majority of LDDs in a context-
free way and shown that DG allows for simple
but powerful statistical models. An evaluation
shows that the performance of its implementa-
tion is state-of-the-art
10
. Its parsing speed of
about 300,000 words per hour is very good for a
deep-linguistic parser and makes it fast enough
for unlimited application.
References
Steven Abney. 1995. Chunks and dependen-
cies: Bringing processing evidence to bear
on syntax. In Jennifer Cole, Georgia Green,
and Jerry Morgan, editors, Computational
Linguistics and the Foundations of Linguis-
tic Theory, pages 145{164. CSLI.
M. Burke, A. Cahill, R. O'Donovan, J. van
Genabith, and A. Way. 2004. Treebank-
based acquisistion of wide-coverage, proba-
bilistic LFG resources: Project overview, re-
sults and evaluation. In The First Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP-04), Workshop "Beyond
shallow analyses - Formalisms and statisti-
cal modeling for deep analyses", Sanya City,
China.
10
We are currently starting evaluation on the PARC
700 corpus
Aoife Cahill, Michael Burke, Ruth O'Donovan,
Josef van Genabith, and Andy Way. 2004.
Long-distance dependency resolution in au-
tomatically acquired wide-coverage PCFG-
based LFG approximations. In Proceedings of
ACL-2004, Barcelona, Spain.
John Carroll, Guido Minnen, and Ted Briscoe.
1999. Corpus annotation for parser evalua-
tion. In Proceedings of the EACL-99 Post-
Conference Workshop on Linguistically Inter-
preted Corpora, Bergen, Norway.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North
American Chapter of the ACL, pages 132{
139.
Noam Chomsky. 1995. The Minimalist Pro-
gram. The MIT Press, Cambridge, Mas-
sachusetts.
Hoojung Chung and Hae-Chang Rim. 2003. A
new probabilistic dependency parsing model
for head-nal, free word order languages. IE-
ICE Transaction on Information & System,
E86-D, No. 11:2490{2493.
Michael Collins and James Brooks. 1995.
Prepositional attachment through a backed-
o model. In Proceedings of the Third Work-
shop on Very Large Corpora, Cambridge,
MA.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania, Philadel-
phia, PA.
Michael A. Covington. 1994. An empirically
motivated reinterpretation of Dependency
Grammar. Technical Report AI1994-01, Uni-
versity of Georgia, Athens, Georgia.
Amit Dubey and Frank Keller. 2003. Proba-
bilistic parsing for German using sister-head
dependencies. In Proceedings of the 41st An-
nual Meeting of the Association for Compu-
tational Linguistics, Sapporo.
Jason Eisner. 2000. Bilexical grammars and
their cubic-time parsing algorithms. In Harry
Bunt and Anton Nijholt, editors, Advances in
Probabilistic and Other Parsing Technologies.
Kluwer.
Christiane Fellbaum, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
James Henderson. 2003. Inducing history
representations for broad coverage statisti-
cal parsing. In Proceedings of HLT-NAACL
2003, Edmonton, Canada.
Julia Hockenmaier and Mark Steedman. 2002.
Generative models for statistical parsing with
combinatory categorial grammar. In Proceed-
ings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, Philadel-
phia.
Richard Hudson. 1984. Word Grammar. Basil
Blackwell, Oxford.
Mark Johnson. 2002. A simple pattern-
matching algorithm for recovering empty
nodes and their antecedents. In Proceedings
of the 40th Meeting of the ACL, University of
Pennsylvania, Philadelphia.
J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsu-
jii. 2003. Genia corpus - a semantically an-
notated corpus for bio-textmining. Bioinfor-
matics, 19(1):i180{i182.
Beth C. Levin. 1993. English Verb Classes
and Alternations: a Preliminary Investiga-
tion. University of Chicago Press, Chicago,
IL.
Dekang Lin. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of IJCAI-95, Mon-
treal.
Dekang Lin. 1998. Dependency-based evalua-
tion of MINIPAR. In Workshop on the Eval-
uation of Parsing Systems, Granada, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313{330.
Igor Mel'cuk. 1988. Dependency Syntax: theory
and practice. State University of New York
Press, New York.
Diego Molla, Gerold Schneider, Rolf Schwit-
ter, and Michael Hess. 2000. Answer
Extraction using a Dependency Grammar
in ExtrAns. Traitement Automatique de
Langues (T.A.L.), Special Issue on Depen-
dency Grammar, 41(1):127{156.
Peter Neuhaus and Norbert Broker. 1997. The
complexity of recognition of linguistically ad-
equate dependency grammars. In Proceedings
of the 35th ACL and 8th EACL, pages 337{
343, Madrid, Spain.
Joakim Nivre. 2004. Inductive dependency
parsing. In Proceedings of Promote IT, Karl-
stad University.
Judita Preiss. 2003. Using grammatical rela-
tions to compare parsers. In Proc. of EACL
03, Budapest, Hungary.
Stefan Riezler, Tracy H. King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall
Street Journal using a Lexical-Functional
Grammar and discriminative estimation tech-
niques. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL'02), Philadephia, PA.
Fabio Rinaldi, James Dowdall, Gerold Schnei-
der, and Andreas Persidis. 2004a. Answer-
ing Questions in the Genomics Domain. In
ACL 2004 Workshop on Question Answering
in restricted domains, Barcelona, Spain, 21{
26 July.
Fabio Rinaldi, Michael Hess, James Dowdall,
Diego Molla, and Rolf Schwitter. 2004b.
Question answering in terminology-rich tech-
nical domains. In Mark Maybury, edi-
tor, New Directions in Question Answering.
MIT/AAAI Press.
Anoop Sarkar, Fei Xia, and Aravind Joshi.
2000. Some experiments on indicators of
parsing complexity for lexicalized grammars.
In Proc. of COLING.
Gerold Schneider. 2003. Extracting and using
trace-free Functional Dependencies from the
Penn Treebank to reduce parsing complex-
ity. In Proceedings of Treebanks and Linguis-
tic Theories (TLT) 2003, Vaxjo, Sweden.
Wojciech Skut, Brigitte Krenn, Thorsten
Brants, and Hans Uszkoreit. 1997. An anno-
tation scheme for free word order languages.
In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing (ANLP-
97), Washington, DC.
Pasi Tapanainen and Timo Jarvinen. 1997. A
non-projective dependency parser. In Pro-
ceedings of the 5th Conference on Applied
Natural Language Processing, pages 64{71.
Association for Computational Linguistics.
Lucien Tesniere. 1959. Elements de Syntaxe
Structurale. Librairie Klincksieck, Paris.
A Robust and Hybrid Deep-Linguistic Theory Applied to
Large-Scale Parsing
Gerold Schneider, James Dowdall, Fabio Rinaldi
Institute of Computational Linguistics, University of Zurich
{gschneid,rinaldi}@ifi.unizh.ch, j.m.dowdall@sussex.ac.uk
Abstract
Modern statistical parsers are robust and quite
fast, but their output is relatively shallow when
compared to formal grammar parsers. We sug-
gest to extend statistical approaches to a more
deep-linguistic analysis while at the same time
keeping the speed and low complexity of a sta-
tistical parser. The resulting parsing architec-
ture suggested, implemented and evaluated here
is highly robust and hybrid on a number of
levels, combining statistical and rule-based ap-
proaches, constituency and dependency gram-
mar, shallow and deep processing, full and near-
full parsing. With its parsing speed of about
300,000 words per hour and state-of-the-art per-
formance the parser is reliable for a number of
large-scale applications discussed in the article.
1 Introduction
Robustness in Computational Linguistics has
been recently recognized as a central issue for
the design of reliable, large-scale Natural Lan-
guage Processing (NLP) systems. While the
highest possible linguistic coverage is desirable,
speed and robustness are equally important in
practical applications.
Formal Grammar Parser have carefully
crafted grammars written by professional lin-
guists. In addition to expressing local relations,
i.e. relations between a mother and a direct
daughter node, a number of non-local relations,
i.e. relations involving more than two genera-
tions, are also modeled. An example of a non-
local relation is the subject control relation in
the sentence John wants to leave, where John is
not only the explicit subject of want, but equally
the implicit subject of leave. A parser that fails
to recognize control subjects misses important
information, quantitatively about 3 % of all sub-
jects.
But unrestricted real-world texts still pose a
problem to NLP systems that are based on For-
mal Grammars. Few hand-crafted, deep linguis-
tic grammars achieve the coverage and robust-
ness needed to parse large corpora (see (Riezler
et al, 2002) for an exception, and (Burke et al,
2004; Hockenmaier and Steedman, 2002) for ap-
proaches extracting formal grammars from the
Treebank), and speed remains a serious chal-
lenge. The typical problems can be grouped as
follows.
Grammar complexity Fully comprehensive
grammars are difficult to maintain and consid-
erably increase parsing complexity. Note that
statistical parsers can equally suffer from this
problem, see e.g. (Kaplan et al, 2004).
Parsing complexity Typical formal gram-
mar parser complexity is much higher than
the O(n3) for CFG (Eisner, 1997). The com-
plexity of some formal grammars is still un-
known. For Tree-Adjoining Grammars (TAG)
it is O(n7) or O(n8) depending on the im-
plementation (Eisner, 2000). (Sarkar et al,
2000) state that the theoretical bound of worst
time complexity for Head-Driven Phrase Struc-
ture Grammar (HPSG) parsing is exponential.
Parsing algorithms able to treat completely un-
restricted long-distance dependencies are NP-
complete (Neuhaus and Bro?ker, 1997).
Ranking Returning all syntactically possible
analyses for a sentence is not really what is ex-
pected of a syntactic analyzer if it should be of
practical use, since for a human there is usually
only one ?correct? interpretation. A clear in-
dication of preference, by means of ranking the
analyses in a preference order is needed.
Pruning In order to keep search spaces man-
ageable it is in fact necessary to discard uncon-
vincing alternatives already during the parsing
process. In a statistical parser, the ranking of
intermediate structures occurs naturally, while
a rule-based system has to rely on ad hoc heuris-
tics. With a beam search in a parse-time prun-
ing system, which means that the total number
of alternatives kept is constant from a certain
search complexity onwards, real-world parsing
time can be reduced to near-linear. If one were
to assume a constantly full beam, or uses an
oracle (Nivre, 2004) it is linear in practice.
A number of robust statistical parsers that
offer solutions to these problems have now be-
come available (Charniak, 2000; Collins, 1999;
Henderson, 2003), but they typically produce
CFG constituency data as output, trees that do
not express long-distance dependencies.
Although grammatical function and empty
nodes annotation expressing long-distance de-
pendencies are provided in Treebanks such as
the Penn Treebank (Marcus et al, 1993), most
statistical Treebank trained parsers fully or
largely ignore them1, which entails two prob-
lems: first, the training cannot profit from valu-
able annotation data. Second, the extraction
of long-distance dependencies (LDD) and the
mapping to shallow semantic representations is
not always possible from the output of these
parsers. This limitation is aggravated by a
lack of co-indexation information and parsing
errors across an LDD. In fact, some syntac-
tic relations cannot be recovered on configura-
tional grounds only. For these reasons, (John-
son, 2002) provocatively refers to them as ?half-
grammars?.
The paper is organized as follows. We first ex-
plore a deep-linguistic grammar theory for En-
glish that is inherently designed to be robust
by extending the low processing complexity and
the robustness of statistical approaches to a
more deep-linguistic level, by making careful
use of underspecification, grammar compression
techniques and using a grammar that directly
delivers simple predicate-argument structures.
This allow us to use a context-free grammar
at parse-time while successfully treating long-
distance dependencies using low-complexity ap-
proaches before and after parsing. Our ap-
proach is to use finite-state approximations
of long-distance dependencies, as they are de-
scribed in (Schneider, 2003a) for Dependency
Grammar (DG) and (Cahill et al, 2004) for
Lexical Functional Grammar (LFG). (Dienes
and Dubey, 2003) show that finite-state pre-
processing modules can successfully deal with
LDDs. Our approach is similar in also amount-
ing to a preprocessing recognition of LDDs.
Then we show that the implementation
(Pro3Gres) profits from hybridness and is fast
1(Collins, 1999) Model 2 uses some of the functional
labels, and Model 3 some long-distance dependencies
and robust enough to do large-scale parsing of
totally unrestricted texts and give an overview
of its applications. To conclude, two evaluations
are given.
2 A Robust Deep-Linguistic Theory
Generally, a linguistic analysis model aims at
complete and correct analysis, which means
that the mapping between the text data and its
syntactic and semantic analysis is sound (the
model extracts correct readings) and complete
(the model deals with all language phenomena).
In practice, however, both goals cannot be to-
tally reached. The main obstacle for soundness
is the all-pervasive characteristic of natural lan-
guage to be ambiguous, where ambiguities can
often only be resolved with world knowledge.
Statistical disambiguation such as (Collins
and Brooks, 1995) for PP-attachment or
(Collins, 1997; Charniak, 2000) for generative
parsing greatly improve disambiguation, but as
they model by imitation instead of by under-
standing, complete soundness has to remain elu-
sive.
As for completeness, already early ?na??ve?
statistical approaches have shown that the prob-
lem of grammar size is not solved but even ag-
gravated by a naive probabilistic parser imple-
mentation, in which e.g. all CFG rules permit-
ted in the Penn Treebank are extracted. From
his 300,000 words training part of the Penn
Treebank (Charniak, 1996) obtains more than
10,000 CFG rules, of which only about 3,000
occur more than once. It is therefore necessary
to either discard infrequent rules, do manual
editing, use a different rule format such as indi-
vidual dependencies (Collins, 1996) or gain full
linguistic control and insight by using a hand-
written grammar ? each of which sacrifices total
completeness.
2.1 Near-full Parsing
The approach we have chosen is to use a
manually-developed wide-coverage tag sequence
grammar (Abney, 1995; Briscoe and Carroll,
2002), and to exclude or restrict rare, marked
and error-prone phenomena. For example,
while it is generally possible for nouns to be
modified by more than one PP, only nouns seen
in the Treebank with several PPs are allowed to
have several PPs. Or, while it is generally possi-
ble for a subject to occur to the immediate right
of a verb (said she), this is only allowed for verbs
seen with a subject to the right in the train-
ing corpus, typically verbs of utterance, and
only in a comma-delimited or sentence-final con-
text. This entails that the parser profits from
a lean grammar but finds a complete structure
spanning the entire sentence in the majority of
real-world sentences and needs to resorts to col-
lecting partial parses in the remaining minority.
Starting from the most probable longest span,
recursively the most probable longest span to
left and right is searched.
Near-full parsing only leads to a very small
loss. If an analysis consists of two partial parses,
on the dependency relation level only the one,
usually high-level relation between the heads
of the two partial parses remains unexpressed.
The risk of returning ?garden path?, locally
correct but globally wrong, analyses diminishes
with increasing span length.
2.2 Functional Dependency Grammar
We follow the broad architecture suggested by
(Abney, 1995) which naturally integrates chunk-
ing and dependency parsing and has proven
to be practical, fast and robust (Collins, 1996;
Basili and Zanzotto, 2002). Tagging and chunk-
ing are very robust, finite-state approaches,
parsing then only occurs between heads of
chunks.2 The perspicuous rules of a hand-
written dependency grammar build up the pos-
sible syntactic structures, which are ranked and
pruned by calculating lexical attachment proba-
bilities for the majortiy of the dependency rela-
tions used in the grammar. The grammar con-
tains around 1000 rules containing the depen-
dent?s and the head?s tag, the direction of the
dependency, lexical information for closed class
words, and context restrictions3. Context re-
strictions express e.g. that only a verb which
has an object in its context is allowed to attach
a secondary object.
Our approach can be seen as an extension of
(Collins and Brooks, 1995) from PP-attachment
to most dependency relations. Training data
is a partial mapping of the Penn Treebank to
deep-linguistic dependency structures, similar
to (Basili et al, 1998).
Robustness also depends on the grammar
formalism. While many formalisms fail to
2Practical experiments using a toy NP and verb-
group grammar have shown that parsing between heads
of chunks only is about four times faster than parsing
between every word, i.e. without chunking.
3the number of rules is high because of tag combina-
torics leading to many almost identical rules. A subject
relations is e.g. possible between the 6 verb tags and the
4 noun tags
project when subcategorized arguments cannot
be found, in a grammar like DG, in which maxi-
mal projections and terminal nodes are isomor-
phic, projection can never fail.
In classical DG, only content words can be
heads, and there is no distinction between syn-
tactic and semantic dependency ? semantic de-
pendency is used as far as possible. These as-
sumptions entail that there are no functional
and no empty nodes, which means that low com-
plexity O(n3) algorithms such as CYK, which is
used here, can be employed.
The classical dependency grammar distinc-
tion between ordre line?aire and ordre struc-
tural, basically an immediate dominance / linear
precedence distinction (ID/LP) also has the ad-
vantage that a number of phenomena classically
assumed to involve long-distance dependencies,
fronted or inversed constituents, can be treated
locally. They only need rules that allow an in-
version of the ?canonical? dependency direction
under well-defined conditions. As for fronted el-
ements, since DG does not distinguish between
external and internal arguments, front positions
are always locally available to the verb.
2.3 Underspecification and
Disambiguation
The cheapest approach to dealing with the all-
pervasive NL ambiguity is to underspecifiy ev-
erything, which leads to a sound and complete
mapping, but one that is content-free and ab-
surd. But in few, carefully selected areas where
distinctions do not matter for the task at hand,
where the disambiguation task is particularly
unreliable, or where inter-annotator agreement
is very low, underspecification can serve as a
tool to greatly facilitate linguistic analysis. For
example, intra-base NP ambiguities, such as
quantifier scope ambiguities do not matter for
a parser like ours aiming at predicate-argument
structure, and are thus not attempted to an-
alyze. There is one part-of-speech distinction
where inter-annotator agreement is quite low
and the performance of taggers generally very
poor: the distinction between verbal particles
and prepositions. We currently leave the dis-
tinction underspecified, but a statistical disam-
biguator is being developed.
Conversely, the Penn Treebank annotation is
sometimes not specific enough. The parser dis-
tinguishes between the reading of the tag IN as
a complementizer or as a preposition, and dis-
ambiguates commas as far as it can, between
apposition, subordination and conjunction.
Some typical tagging errors can be robustly
corrected by the hand-written grammar. For
example, the distinction between verb past
tense VBD and participle VBN is unreliable,
but can usually be disambiguated in the pars-
ing process by leaving this tag distinction un-
derspecified for a number of constructions.
2.4 Long-distance Dependencies
Long-distance dependencies exponentially
increase parsing complexity (Neuhaus and
Bro?ker, 1997). We therefore use an approach
that preprocesses, post-processes and partly
underspecifies them, allowing us to use a
context-free grammar at parse time.
In detail, (1) before the parsing we model
dedicated patterns across several levels of con-
stituency subtrees partly leading to dedicated,
compressed and fully local dependency rela-
tions, (2) we use statistical lexicalized post-
processing, and (3) we rely on traditional De-
pendency Grammar assumptions (section 2.2).
2.4.1 Pre-processing
(Johnson, 2002) presents a pattern-matching al-
gorithm for post-processing the output of sta-
tistical parsers to add empty nodes to their
parse trees. While encouraging results are re-
ported for perfect parses, performance drops
considerably when using trees produced by a
statistical parser. ?If the parser makes a sin-
gle parsing error anywhere in the tree fragment
matched by the pattern, the pattern will no
longer match. This is not unlikely since the sta-
tistical model used by the parser does not model
these larger tree fragments. It suggests that one
might improve performance by integrating pars-
ing, empty node recovery and antecedent find-
ing in a single system ... ? (Johnson, 2002).
We have applied structural patterns to the
Penn Treebank, where like in perfect parses pre-
cision and recall are high, and where in addi-
tion functional labels and empty nodes are avail-
able, so that patterns similar to Johnson?s but
? like (Jijkoun, 2003) ? relying on functional
labels and empty nodes reach precision close to
100%. Unlike in Johnson, also patterns for local
dependencies are used; non-local patterns sim-
ply stretch across more subtree-levels. We use
the extracted lexical counts as lexical frequency
training material. Every dependency relation
has a group of structural extraction patterns
associated with it. This amounts to a partial
mapping of the Penn Treebank to Functional
Relation Label Example
verb?subject subj he sleeps
verb?first object obj sees it
verb?second object obj2 gave (her) kisses
verb?adjunct adj ate yesterday
verb?subord. clause sentobj saw (they) came
verb?prep. phrase pobj slept in bed
noun?prep. phrase modpp draft of paper
noun?participle modpart report written
verb?complementizer compl to eat apples
noun?preposition prep to the house
Table 1: The most important dependency types
used by the parser
?










NP-SBJ-X@
noun
VP@






V
passive verb
NP
-NONE-
*-X
?










NP-SBJ-X@
noun
VP@






V
control-verb
S
NP-SBJ
-NONE-
*-X
Figure 1: The extaction patterns for passive
subjects (top) and subject control (bottom)
DG (Hajic?, 1998), (Tapanainen and Ja?rvinen,
1997). Table 1 gives an overview of the most
important dependencies.
The subj relation, for example, has the head
of an arbitrarily nested NP with the functional
tag SBJ as dependent, and the head of an ar-
bitrarily nested VP as head for all active verbs.
In passive verbs, however, a movement involv-
ing an empty constituent is assumed, which cor-
responds to the extraction pattern in figure 1,
where VP@ is an arbitrarily nested VP, and NP-
SBJ-X@ the arbitrarily nested surface subject
and X the co-indexed, moved element. Move-
ments are generally supposed to be of arbitrary
length, but a closer investigation reveals that
this type of movement is fixed.
The same argument can be made for other
relations, for example control structures, which
have the extraction pattern shown in figure 1.
Grammatical role labels, empty node labels and
tree configurations spanning several local sub-
trees are used as integral part of some of the
patterns. This leads to much flatter trees, as
typical for DG, which has the advantages that
(1) it helps to alleviate sparse data by map-
ping nested structures that express the same
dependency relation, (2) the costly overhead for
dealing with unbounded dependencies can be
largely avoided, (3) it is ensured that the lex-
ical information that matters is available in one
central place, allowing the parser to take one
well-informed decision instead of several brittle
decisions plagued by sparseness, which greatly
reduces complexity and the risk of errors (John-
son, 2002). Collapsing deeply nested structures
into a single dependency relation is less complex
but has the same effect as carefully selecting
what goes in to the parse history in history-
based approaches. ?Much of the interesting
work is determining what goes into [the history]
H(c)?(Charniak, 2000).
(Schneider, 2003a) shows that the vast ma-
jority of LDDs can be treated in this way,
essentially compressing non-local subtrees into
dedicated relations even before grammar writ-
ing starts. The compressed trees correspond
to a simple LFG f-structure. The trees ob-
tained from parsing can be decompressed into
traditional constituency trees including empty
nodes and co-indexation, or into shallow seman-
tic structures such as Minimal Logical Forms
(MLF) (Rinaldi et al, 2004b; Schneider et al,
2000; Schwitter et al, 1999). This approach
leaves LDDs underspecified, but recoverable,
and makes no claims as to whether empty nodes
at an automonous syntactic level exist or not.
2.4.2 Post-Processing
After parsing, shared constituents can be ex-
tracted again. The parser explicitly does this
for control, raising and semi-auxiliary relations,
because the grammar does not distinguish be-
tween subordinating clauses with and without
control. A probability model based on the verb
semantics is invoked if a subordinate clause
without overt subject is seen, in order to decide
whether the matrix clause subject or object is
shared.
2.4.3 What do we lose?
Among the 10 most frequent types of empty
nodes, which cover more than 60,000 of the
64,000 empty nodes in the Penn treebank, there
are only two problematic LDD types: WH
Traces and indexed gerunds.
WH traces Only 113 of the 10,659 WHNP
antecedents in the Penn Treebank are actually
question pronouns. The vast majority, over
9,000, are relative pronouns. For them, an in-
version of the direction of the relation they have
to the verb is allowed if the relative pronoun
Figure 2: Pro3Gres flowchart
precedes the subject. This method succeeds in
most cases, but linguistic non-standard assump-
tions need to be made for stranded prepositions.
Only non-subject WH-question pronouns and
support verbs need to be treated as ?real?
non-local dependencies. In question sentences,
before the main parsing is started, the sup-
port verb is attached to any lonely participle
chunk in the sentence, and the WH-pronoun
pre-parses with any verb.
Indexed Gerunds Unlike in control, rais-
ing and semi-auxiliary constructions, the an-
tecedent of an indexed gerund cannot be es-
tablished easily. The fact that almost half of
the gerunds are non-indexed in the Penn Tree-
bank indicates that information about the un-
expressed participant is rather semantic than
syntactic in nature, much like in pronoun res-
olution. Currently, the parser does not try to
decide whether the target gerund is an indexed
or non-indexed gerund nor does it try to find the
identity of the lacking participant in the latter
case. This is an important reason why recall
values for the subject and object relations are
lower than the precision values.
3 Robustness ?in the small?
In addition to a robust deep-linguistic design
(robustness ?in the large?, section 2), the im-
plemented parser, Pro3Gres, uses a number of
practical robust approaches ?in the small? at
each processing level, such as relying on finite-
state tagging and chunking or collecting par-
tial parses if no complete analysis can be found,
or using incrementally more aggressive pruning
techniques in very long sentences. During the
parsing process, only a certain number of alter-
natives for each possible span are kept. Experi-
ments have shown that using a fixed number or
a number dependent on the parsing complex-
ity in terms of global chart entries lead to very
similar results. Using reasonable beam sizes in-
creases parsing speed by an order of magnitude
while hardly affecting parser performance. For
the fixed number model, performance starts to
collapse only when less than 4 alternatives per
span are kept.
When a certain complexity has been reached
(currently 1000 chart entries), only reductions
above a certain probability threshold are per-
missible. The threshold starts very low, but
is a function of the total number of chart en-
tries. This entails that even sentences with
hundreds of words can be parsed quickly, but
it is not aimed at finding complete parses for
them, rather a graceful degradation of perfor-
mance (Menzel, 1995) is intended.
4 A hybrid approach on many levels
Pro3Gres profits from being hybrid on many
levels. Hybridness means that the most robust
approach can be chosen for each task and each
processing level.
statistical vs. rule-based the most obvious
way in which Pro3Gres is a hybrid (Schneider,
2003b). Unlike formal grammars to which post-
hoc statistical disambiguators can be added,
Pro3Gres has been designed to be hybrid, care-
fully distinguishing between tasks that can best
be solved by finite-state methods, rule-based
methods and statistical methods. While e.g.
grammar writing is easy for a linguist, and a
naive Treebank grammar suffers from similar
complexity problems as a comprehensive for-
mal grammar, the scope of application and the
amount of ambiguity a rule creates is often be-
yond our imagination and best handled by a
statistical system.
shallow vs. deep the designing philosophy
for Pro3Gres has been to stay as shallow as pos-
sible to obtain reliable results at each level.
Treebank constituency vs. DG the obser-
vation that a DG that expresses grammatical
relations is more informative, but also more in-
tuitive to interpret for a non-expert, and that
Functional DG can avoid a number of LDD
types has made DG the formalism of our choice.
For lexicalizing the grammar, a partial mapping
from the largest manually annotated corpus
available, the Penn Treebank, was necessary, ex-
hibiting a number of mapping challenges.
history-based vs. mapping-based
Pro3Gres is not a parse-history-based ap-
proach. Instead of manually selecting what
goes into the history, as is usually done (see
(Henderson, 2003) for an exception), we man-
ually select how to linguistically meaningfully
map Treebank structures onto dependency re-
lations by the use of mapping patterns adapted
from (Johnson, 2002).
probabilistic vs. statistical Pro3Gres is
not a probabilistic system in the sense of a
PCFG. From a practical viewpoint, knowing the
probability of a certain rule expansion per se
is of little interest. Pro3Gres models decision
probabilities, the probability of a parse is un-
derstood to be the product of all the decision
probabilities taken during the derivation.
local subtress vs. DOP psycholinguistic
experiments and Data-Oriented Parsing (DOP)
(Bod et al, 2003) suggest that people store
subtrees of various sizes, from two-word frag-
ments to entire sentences. But (Goodman,
2003) suggests that the large number of sub-
trees can be reduced to a compact grammar that
makes DOP parsing computationally tractable.
In Pro3Gres, a subset of non-local fragments
which, based on linguistic intuition are espe-
cially important, are used.
generative vs. structure-generating DG
generally, although generative in the sense that
connected complete structures are generated, is
not generative in the sense that it is always
guaranteed to terminate if used for random gen-
eration of language. Since a complete or partial
hierarchical structure that follows CFG assump-
tions due to the employed grammar is built up
for each sentence. Pro3Gres? constraint to allow
each complement dependency type only once
per verb can be seen as a way of rendering it
generative in practice.
syntax vs. semantics instead of using
a back-off to tags (Collins, 1999), semantic
classes, Wordnet for nouns and Levin classes
for verbs, are used, in the hope that they better
manage better to express selectional restrictions
than tags. Practical experiments have shown,
however, that, in accordance to (Gildea, 2001)
on head-lexicalisation, there is almost no in-
crease in performance.
5 Applications and Evaluation
Pro3Gres is currently being applied in a Ques-
tion Answering system specifically targeted at
Figure 3: Dependency Tree output of the SWI Prolog graphical implementation of the parser
technical domains (Rinaldi et al, 2004b). One
of the main advantages of a dependency-based
parser such as Pro3Gres over other parsing ap-
proaches is that a mapping from the syntactic
layer to a semantic layer (meaning representa-
tion) is partly simplified (Molla? et al, 2000; Ri-
naldi et al, 2002).
The original version of the QA system used
the Link Grammar (LG) parser (Sleator and
Temperley, 1993), which however had a number
of significant shortcomings. In particular the
set of the dependency relations used in LG is
very idiosyncratic, which makes any syntactic-
semantic mapping created for LG necessarily
unportable and difficult to extend and maintain.
A recent line of research concerns applications
for the Semantic Web. The documents avail-
able in the World Wide Web are mostly written
in natural language. As such, they are under-
standable only to humans. One of the directions
of Semantic Web research is about adding a
layer to the documents that somehow formalizes
their content, making it understandable also to
software agents. Such Semantic Web annota-
tions can be seen as a way to mark explicitly
the meaning of certain parts of the documents.
The dependency relations provided by a parser
such as Pro3Gres, combined with domain spe-
cific axioms, allow the creation of (some of) the
semantic annotations, as described in (Rinaldi
et al, 2003; Kaljurand et al, 2004).
The modified QA system (using Pro3Gres) is
being exploited in the area of ?Life Sciences?, for
applications concerning Knowledge Discovery
over Medline abstracts (Rinaldi et al, 2004a;
Dowdall et al, 2004). We illustrate some of the
differences between general-purpose parsing and
the parsing of highly technical texts like Med-
line and give two evaluations.
5.1 General unrestricted texts
We first report an evaluation on sentences from
an open domain, which gives a good impression
of the performance of the parser on general, un-
restricted text.
In traditional constituency approaches,
parser evaluation is done in terms of the corre-
spondence of the bracketting between the gold
standard and the parser output. (Lin, 1995;
Carroll et al, 1999) suggest evaluating on the
linguistically more meaningful level of syntactic
relations. Two evaluations on the syntactic
relation level are reported in the following.
First, a general-purpose evaluation using a
hand-compiled gold standard corpus (Carroll
et al, 1999), which contains the grammatical
relation data of 500 random sentences from the
Susanne corpus.
The performance, shown in table 2, is, accord-
ing to (Preiss, 2003), similar to a large selection
of statistical parsers and a grammatical relation
finder. Relations involving long-distance depen-
dencies form part of these relations. In order to
measure specifically their performance, a selec-
tion of them is also given: WH-Subject (WHS),
WH-Object (WHO), passive Subject (PSubj),
control Subject (CSubj), and the anaphor of the
relative clause pronoun (RclSubjA).
5.2 Parsing highly technical language
While measuring general parsing performance is
fundamental in the development of any parsing
system there is a danger of fostering domain de-
pendence in concentrating on a single domain.
In order to answer how the parser performs
over domains markedly different to the training
corpus , the parser has been applied to the GE-
NIA corpus (Kim et al, 2003), 2000 MEDLINE
abstracts of more than 400,000 words describing
the results of Biomedical research.
Average sentence length is 27 words, the lan-
Percentage Values for some relations, general, on Carroll corpus only LDD-involving
Subject Object noun-PP verb-PP subord. cl. WHS WHO PSubj CSubj RclSubjA
Precision 91 89 73 74 68 92 60 n/a 80 89
Recall 81 83 67 83 n/a 90 86 83 n/a 63
Table 2: Results of evaluating the parser output on Carroll?s test suite on subject, object, PP-
attachment and clause subordination relations, and a selective evaluation of 5 relations involving
long-distance dependencies (LDD)
Percentage Values for some relations, general, on the GENIA corpus
Subject Object noun-PP verb-PP subord. clause
Precision 90 94 83 82 71
Recall 86 95 82 84 75
Table 3: Results of evaluating 100 random sentences from the terminology-annotated GENIA
corpus, on subject, object, PP-attachment and clause subordination relations
guage is very technical and extremely domain-
specific. But the most striking characteristic
of this domain is the frequency of MultiWord
Terms (MWT) which are known to cause seri-
ous problems for NLP systems (Sag et al, 2002),
(Dowdall et al, 2003). The token to chunk ra-
tio: NPs = 2.3 , VPs = 1.3 (number of tokens
divided by the number of chunks) is unusually
high.
The GENIA corpus does not include any syn-
tactic annotation (making standard evaluation
more difficult) but approx. 100, 000 MWTs are
annotated and assigned a semantic type from
the GENIA ontology.
This novel parsing application is designed to
determine how parsing performance interacts
with MWT recognition as well as the applica-
bility and possible improvements to the proba-
blistic model over this domain, to test the hy-
pothesis if terminology is the key to a successful
parsing system. We do not discard this infor-
mation, thus simulating a situation in which a
near-perfect terminology-recognition tool is at
one?s disposal. MWT are regarded as chunks,
the parsing thus takes place between between
the heads of MWT, words and chunks.
100 random sentences from the GENIA cor-
pus have been manually annotated for this eval-
uation and compared to the parser output. De-
spite the extreme complexity and technical lan-
guage, parsing performance under these condi-
tions is considerably better than on the Carroll
corpus when using automated chunking, as ta-
ble 3 reveals.
It is worth noting that 10 of the 17 subject
precision errors (out of 171 subjects) are ?hard?
cases involving long-distance dependencies (1
control, 4 relative pronouns) and 5 verb group
chunking errors. Equally interesting, 2 of the 4
object recall errors (out of 79 objects) are due
to 1 mistagging and 1 mischunking.
In practice, MWT extraction is still not au-
tomated to the level of chunking or Name En-
tity recognition simulated in this experiment
(for a comprehensive review of the state-of-the-
art see (Castellv et al, 2001)). This is, in
a large part, due to the lack of definitive or-
thographic, morphological and syntactic char-
acteristics to differentiante between MWTs and
canonical phrases. So MWT extraction remains
a semi-automated task performed in cycles with
the result of each cycle requiring manual valida-
tion. The return for this time consuming activ-
ity are the characteristics of MWTs which can
be use to fine tune the algorithms during the
next extraction cycle.
6 Conclusion
We have suggested a robust, deep-linguistic
grammar theory delivering grammatical rela-
tion structures as output, which are closer to
predicate-argument structures than pure con-
stituency structures, and more informative if
non-local dependencies are involved. We have
presented an implementation of the theory that
is used for large-scale parsing. An evaluation
at the grammatical relation level shows that its
performance is state-of-the-art.
References
Steven Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
In Jennifer Cole, Georgia Green, and Jerry Mor-
gan, editors, Computational Linguistics and the
Foundations of Linguistic Theory, pages 145?164.
CSLI.
Roberto Basili and Fabio Massimo Zanzotto. 2002.
Parsing engineering and empirical robustness.
Journal of Natural Language Engineering, 8/2-3.
Roberto Basili, Maria Teresa Pazienza, and
Fabio Massimo Zanzotto. 1998. Evaluating a ro-
bust parser for Italian language. In Proceedings of
Evaluations of Parsing Systems Workshop, held
jointly with 1st LREC, Granada,Spain.
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-Oriented Parsing. Center for the
Study of Language and Information, Studies in
Computational Linguistics (CSLI-SCL). Chicago
University Press.
Ted Briscoe and John Carroll. 2002. Robust accu-
rate statistical annotation of general text. In Pro-
ceedings of the 3rd International Conference on
Language Resources and Evaluation, pages 1499?
1504, Las Palmas, Gran Canaria.
M. Burke, A. Cahill, R. O?Donovan, J. van Gen-
abith, and A. Way. 2004. Treebank-based ac-
quisistion of wide-coverage, probabilistic LFG re-
sources: Project overview, results and evalua-
tion. In The First International Joint Confer-
ence on Natural Language Processing (IJCNLP-
04), Workshop ?Beyond shallow analyses - For-
malisms and statistical modeling for deep analy-
ses?, Sanya City, Hainan Island, China.
Aoife Cahill, Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way. 2004.
Long-distance dependency resolution in automat-
ically acquired wide-coverage PCFG-based LFG
approximations. In Proceedings of ACL-2004,
Barcelona, Spain.
John Carroll, Guido Minnen, and Ted Briscoe.
1999. Corpus annotation for parser evaluation.
In Proceedings of the EACL-99 Post-Conference
Workshop on Linguistically Interpreted Corpora,
Bergen, Norway.
M. Teresa Cabre? Castellv, Rosa Estopa?, and
Jordi Vivaldi Palatresi, 2001. Recent Advances in
Computational Terminology, chapter Automatic
term detection: A review of current systems,
pages 53?87. John Benjamins.
Eugene Charniak. 1996. Tree-bank grammar. Tech-
nical Report Technical Report CS-96-02, Depart-
ment of Computer Science, Brown University.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North
American Chapter of the ACL, pages 132?139.
Michael Collins and James Brooks. 1995. Preposi-
tional attachment through a backed-off model. In
Proceedings of the Third Workshop on Very Large
Corpora, Cambridge, MA.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the
Association for Computational Linguistics, pages
184?191, Philadelphia.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. of the 35th
Annual Meeting of the ACL, pages 16?23, Madrid,
Spain.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Pter Dienes and Amit Dubey. 2003. Antecedent
recovery: Experiments with a trace tagger. In
Proceedings of the 2003 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), Sapporo, Japan.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-
Sanjuan, and Eric SanJuan. 2003. Complex
structuring of term variants for question answer-
ing. In Proceedings of the ACL workshop on Mul-
tiWord Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, July.
James Dowdall, Fabio Rinaldi, Andreas Persidis,
Kaarel Kaljurand, Gerold Schneider, and Michael
Hess. 2004. Terminology expansion and re-
lation identification between genes and path-
ways. In Workshop on Terminology, Ontology
and Knowledge Representation. Universite Jean
Moulin (Lyon 3).
Jason Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the
5th International Workshop on Parsing Technolo-
gies, pages 54?65, MIT, Cambridge, MA, Septem-
ber.
Jason Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In Harry Bunt and
Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies. Kluwer Aca-
demic Publishers.
Daniel Gildea. 2001. Corpus variation and parser
performance. In Proceedings of the 2001 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 167?202, Pittsburgh,
PA.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Bod et al (Bod et al,
2003).
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In
Eva Hajic?ova?, editor, Issues of Valency and Mean-
ing. Studies in Honor of Jarmila Panevova?, pages
106?132. Karolinum, Charles University Press,
Prague.
James Henderson. 2003. Inducing history repre-
sentations for broad coverage statistical parsing.
In Proceedings of HLT-NAACL 2003, Edmonton,
Canada.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with combi-
natory categorial grammar. In Proceedings of 40th
Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia.
Valentin Jijkoun. 2003. Finding non-local depen-
dencies: beyond pattern matching. In Proceedings
of the ACL 03 Student Workshop, Budapest.
Mark Johnson. 2002. A simple pattern-matching
algorithm for recovering empty nodes and their
antecedents. In Proceedings of the 40th Meeting
of the ACL, University of Pennsylvania, Philadel-
phia.
Kaarel Kaljurand, Fabio Rinaldi, James Dowdall,
and Michael Hess. 2004. Exploiting language re-
sources for semantic web annotations. In Proceed-
ings of LREC 2004, Lisbon, May 24-30. accepted
for publication.
Ron Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alex Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow
and deep stochastic parsing. In Proceedings of
HLT/NAACL 2004, Boston, MA.
J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
Genia corpus - a semantically annotated corpus
for bio-textmining. Bioinformatics, 19(1):i180?
i182.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, Montreal.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Wolfgang Menzel. 1995. Robust processing of natu-
ral language. Lecture Notes in Computer Science,
981:19?34.
Diego Molla?, Gerold Schneider, Rolf Schwitter, and
Michael Hess. 2000. Answer Extraction using a
Dependency Grammar in ExtrAns. Traitement
Automatique de Langues (T.A.L.), Special Issue
on Dependency Grammar, 41(1):127?156.
Peter Neuhaus and Norbert Bro?ker. 1997. The com-
plexity of recognition of linguistically adequate
dependency grammars. In Proceedings of the 35th
ACL and 8th EACL, pages 337?343, Madrid,
Spain.
Joakim Nivre. 2004. Inductive dependency parsing.
In Proceedings of Promote IT, Karlstad Univer-
sity.
Judita Preiss. 2003. Using grammatical relations to
compare parsers. In Proc. of EACL 03, Budapest,
Hungary.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and discrim-
inative estimation techniques. In Proc. of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?02), Philadephia, PA.
Fabio Rinaldi, James Dowdall, Michael Hess, Diego
Molla?, and Rolf Schwitter. 2002. Towards Answer
Extraction: an application to Technical Domains.
In ECAI2002, European Conference on Artificial
Intelligence, Lyon, pages 460?464.
Fabio Rinaldi, Kaarel Kaljurand, James Dowdall,
and Michael Hess. 2003. Breaking the deadlock.
In ODBASE 2003 (International Conference on
Ontologies, Databases and Applications of Seman-
tics) Catania, Italy., volume 2889 of Lecture Notes
in CS. Springer Verlag.
Fabio Rinaldi, James Dowdall, Gerold Schneider,
and Andreas Persidis. 2004a. Answering Ques-
tions in the Genomics Domain. In ACL 2004
Workshop on Question Answering in restricted
domains, Barcelona, Spain, 21?26 July.
Fabio Rinaldi, Michael Hess, James Dowdall, Diego
Molla?, and Rolf Schwitter. 2004b. Question an-
swering in terminology-rich technical domains. In
Mark Maybury, editor, New Directions in Ques-
tion Answering. MIT/AAAI Press.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: a Pain in the Neck for NLP. In Pro-
ceedings of the Third International Conference,
CICLing 2002, pages 1?15, Mexico City, Febru-
rary.
Anoop Sarkar, Fei Xia, and Aravind Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proc. of
COLING.
Gerold Schneider, Diego Molla` Aliod, and Michael
Hess. 2000. Inkrementelle minimale logische for-
men fr die antwortextraktion. In Proceedings of
34th Linguistic Colloquium, September 1999, Uni-
versity of Mainz, FASK.
Gerold Schneider. 2003a. Extracting and using
trace-free Functional Dependencies from the Penn
Treebank to reduce parsing complexity. In Pro-
ceedings of Treebanks and Linguistic Theories
(TLT) 2003, Va?xjo?, Sweden.
Gerold Schneider. 2003b. A low-complexity, broad-
coverage probabilistic dependency parser for En-
glish. In Proceedings of HLT-NAACL 2003 Stu-
dent session, Edmonton, Canada.
Rolf Schwitter, Diego Molla? Aliod, and Michael
Hess. 1999. ExtrAns - answer extraction from
technical documents by minimal logical forms
and selective highlighting. In Proceedings of The
Third International Tbilisi Symposium on Lan-
guage, Logic and Computation, Batumi, Georgia.
Daniel D. Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proc. Third
International Workshop on Parsing Technologies,
pages 277?292.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, pages 64?71. Association for Compu-
tational Linguistics.
