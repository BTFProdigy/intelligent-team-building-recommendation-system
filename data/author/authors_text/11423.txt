Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 345?348,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Modeling Morphologically Rich Languages Using Split Words and
Unstructured Dependencies
Deniz Yuret
Koc? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Abstract
We experiment with splitting words into
their stem and suffix components for mod-
eling morphologically rich languages. We
show that using a morphological ana-
lyzer and disambiguator results in a sig-
nificant perplexity reduction in Turkish.
We present flexible n-gram models, Flex-
Grams, which assume that the n?1 tokens
that determine the probability of a given
token can be chosen anywhere in the sen-
tence rather than the preceding n?1 posi-
tions. Our final model achieves 27% per-
plexity reduction compared to the standard
n-gram model.
1 Introduction
Language models, i.e. models that assign prob-
abilities to sequences of words, have been proven
useful in a variety of applications including speech
recognition and machine translation (Bahl et al,
1983; Brown et al, 1990). More recently, good re-
sults on lexical substitution and word sense disam-
biguation using language models have also been
reported (Hawker, 2007; Yuret, 2007). Morpho-
logically rich languages pose a challenge to stan-
dard modeling techniques because of their rela-
tively large out-of-vocabulary rates and the regu-
larities they possess at the sub-word level.
The standard n-gram language model ignores
long-distance relationships between words and
uses the independence assumption of a Markov
chain of order n ? 1. Morphemes play an im-
portant role in the syntactic dependency structure
in morphologically rich languages. The depen-
dencies are not only between stems but also be-
tween stems and suffixes and if we use complete
words as unit tokens, we will not be able to rep-
resent these sub-word dependencies. Our work-
ing hypothesis is that the performance of a lan-
guage model is correlated by how much the prob-
abilistic dependencies mirror the syntactic depen-
dencies. We present flexible n-grams, FlexGrams,
in which each token can be conditioned on tokens
anywhere in the sentence, not just the preceding
n?1 tokens. We also experiment with words split
into their stem and suffix forms, and define stem-
suffix FlexGrams where one set of offsets is ap-
plied to stems and another to suffixes. We evaluate
the performance of these models on a morpholog-
ically rich language, Turkish.
2 The FlexGram Model
The FlexGram model relaxes the contextual as-
sumption of n-grams and assumes that the n ? 1
tokens that determine the probability of a given to-
ken can be chosen anywhere in the sentence rather
than at the preceding n? 1 positions. This allows
the ability to model long-distance relationships be-
tween tokens without a predefined left-to-right or-
dering and opens the possibility of using different
dependency patterns for different token types.
Formal definition An order-n FlexGram model
is specified by a tuple of dependency offsets
[d
1
, d
2
, . . . , d
n?1
] and decomposes the probability
of a given sequence of tokens into a product of
conditional probabilities for every token:
p(w
1
, . . . , w
k
) =
?
w
i
?S
p(w
i
|w
i+d
1
. . . w
i+d
n?1
)
The offsets can be positive or negative and the
same set of offsets is applied to all tokens in the
sequence. In order to represent a properly nor-
malized probability model over the set of all finite
length sequences, we check that the offsets of a
FlexGram model does not result in a cycle. We
show that using differing dependency offsets for
stems and suffixes can improve the perplexity.
345
3 Dataset
We used the Turkish newspaper corpus of Milliyet
after removing sentences with 100 or more tokens.
The dataset contains about 600 thousand sentences
in the training set and 60 thousand sentences in the
test set (giving a total of about 10 million words).
The versions of the corpus we use developed by
using different word-split strategies along with a
sample sentence are explained below:
1. The unsplit dataset contains the raw corpus:
Kasparov b?ukemedi?gi eli ?opecek
(Kasparov is going to kiss the hand he cannot bend)
2. The morfessor dataset was prepared using the
Morfessor (Creutz et al, 2007) algorithm:
Kasparov b?uke +medi?gi eli ?op +ecek
3. The auto-split dataset is obtained after using
our unsupervised morphological splitter:
Kaspar +ov b?uk +emedi?gi eli ?op +ecek
4. The split dataset contains words that are split
into their stem and suffix forms by using a
highly accurate supervised morphological an-
alyzer (Yuret and T?ure, 2006):
Kasparov b?uk +yAmA+dHk+sH el +sH ?op
+yAcAk
5. The split+0 version is derived from the split
dataset by adding a zero-suffix to any stem that
is not followed by a suffix:
Kasparov +0 b?uk +yAmA+dHk+sH el +sH
?op +yAcAk
Some statistics of the dataset are presented in
Table 1. The vocabulary is taken to be the to-
kens that occur more than once in the training set
and the OOV column shows the number of out-
of-vocabulary tokens in the test set. The unique
and 1-count columns give the number of unique
tokens and the number of tokens that only occur
once in the training set. Approximately 5% of the
tokens in the unsplit test set are OOV tokens. In
comparison, the ratio for a comparably sized En-
glish dataset is around 1%. Splitting the words
into stems and suffixes brings the OOV ratio closer
to that of English.
Model evaluation When comparing language
models that tokenize data differently:
1. We take into account the true cost of the OOV
tokens using a separate character-based model
similar to Brown et al (1992).
2. When reporting averages (perplexity, bits-per-
word) we use a common denominator: the
number of unsplit words.
Table 1: Dataset statistics (K for thousands, M for millions)
Dataset Train Test OOV Unique 1-count
unsplit 8.88M 0.91M 44.8K (4.94%) 430K 206K
morfessor 9.45M 0.98M 10.3K (1.05%) 167K 34.4K
auto-split 14.3M 1.46M 13.0K (0.89%) 128K 44.8K
split 12.8M 1.31M 17.1K (1.31%) 152K 75.4K
split+0 17.8M 1.81M 17.1K (0.94%) 152K 75.4K
4 Experiments
In this section we present a number of experiments
that demonstrate that when modeling a morpho-
logically rich language like Turkish, (i) splitting
words into their stem and suffix forms is beneficial
when the split is performed using a morphologi-
cal analyzer and (ii) allowing the model to choose
stem and suffix dependencies separately and flex-
ibly results in a perplexity reduction, however the
reduction does not offset the cost of zero suffixes.
We used the SRILM toolkit (Stolcke, 2002) to
simulate the behavior of FlexGram models by us-
ing count files as input. The interpolated Kneser-
Ney smoothing was used in all our experiments.
Table 2: Total log probability (M for millions of bits).
Split Dataset Unsplit Dataset
N Word logp OOV logp Word logp OOV logp
1 14.2M 0.81M 11.7M 2.32M
2 10.5M 0.64M 9.64M 1.85M
3 9.79M 0.56M 9.46M 1.59M
4 9.72M 0.53M 9.45M 1.38M
5 9.71M 0.51M 9.45M 1.25M
6 9.71M 0.50M 9.45M 1.19M
4.1 Using a morphological tagger and
disambiguator
The split version of the corpus contains words
that are split into their stem and suffix forms by
using a previously developed morphological an-
alyzer (Oflazer, 1994) and morphological disam-
biguator (Yuret and T?ure, 2006). The analyzer
produces all possible parses of a Turkish word us-
ing the two-level morphological paradigm and the
disambiguator chooses the best parse based on the
analysis of the context using decision lists. The in-
tegrated system was found to discover the correct
morphological analysis for 96% of the words on
a hand annotated out-of-sample test set. Table 2
gives the total log-probability (using log
2
) for the
split and unsplit datasets using n-gram models
of different order. We compute the perplexity
of the two datasets using a common denomina-
tor: 2
? log
2
(p)/N
where N=906,172 is taken to be
the number of unsplit tokens. The best combina-
tion (order-6 word model combined with an order-
9 letter model) gives a perplexity of 2,465 for
the split dataset and 3,397 for the unsplit dataset,
346
which corresponds to a 27% improvement.
4.2 Separation of stem and suffix models
Only 45% of the words in the split dataset have
suffixes. Each sentence in the split+0 dataset has
a regular [stem suffix stem suffix ...] structure. Ta-
ble 3 gives the average cost of stems and suffixes in
the two datasets for a regular 6-gram word model
(ignoring the common OOV words). The log-
probability spent on the zero suffixes in the split+0
dataset has to be spent on trying to decide whether
to include a stem or suffix following a stem in the
split dataset. As a result the difference in total log-
probability between the two datasets is small (only
6% perplexity difference). The set of OOV tokens
is the same for both the split and split+0 datasets;
therefore we ignore the cost of the OOV tokens as
is the default SRILM behavior.
Table 3: Total log probability for the 6-gram word models
on split and split+0 data.
split dataset split+0 dataset
token number of total number of total
type tokens ? log
2
p tokens ? log
2
p
stem 0.91M 7.80M 0.91M 7.72M
suffix 0.41M 1.89M 0.41M 1.84M
0-suffix ? ? 0.50M 0.21M
all 1.31M 9.69M 1.81M 9.78M
4.3 Using the FlexGram model
We perform a search over the space of dependency
offsets using the split+0 dataset and considered n-
gram orders 2 to 6 and picked the dependency off-
sets within a window of 4n + 1 tokens centered
around the target. Table 4 gives the best mod-
els discovered for stems and suffixes separately
and compares them to the corresponding regular
n-gram models on the split+0 dataset. The num-
bers in parentheses give perplexity and significant
reductions can be observed for each n-gram order.
Table 4: Regular ngram vs FlexGram models.
N ngram-stem ngram-suffix
2 -1 (1252) -1 (5.69)
3 -2,-1 (418) -2,-1 (5.29)
4 -3,-2,-1 (409) -3,-2,-1 (4.79)
5 -4,-3,-2,-1 (365) -4,-3,-2,-1 (4.80)
6 -5,-4,-3,-2,-1 (367) -5,-4,-3,-2,-1 (4.79)
N flexgram-stem flexgram-suffix
2 -2 (596) -1 (5.69)
3 +1,-2 (289) +1,-1 (4.21)
4 +2,+1,-1 (189) -2,+1,-1 (4.19)
5 +4,+2,+1,-1 (176) -3,-2,+1,-1 (4.12)
6 +4,+3,+2,+1,-1 (172) -4,-3,-2,+1,-1 (4.13)
However, some of these models cannot be used
in combination because of cycles as we depict on
the left side of Figure 1 for order 3. Table 5 gives
the best combined models without cycles. We
were able to exhaustively search all the patterns
for orders 2 to 4 and we used beam search for or-
ders 5 and 6. Each model is represented by its
offset tuple and the resulting perplexity is given
in parentheses. Compared to the regular n-gram
models from Table 4 we see significant perplexity
reductions up to order 4. The best order-3 stem-
suffix FlexGram model can be seen on the right
side of Figure 1.
Table 5: Best stem-suffix flexgram model combinations for
the split+0 dataset.
N flexgram-stem flexgram-suffix perplexity reduction
2 -2 (596) -1 (5.69) 52.3%
3 -4,-2 (496) +1,-1 (4.21) 5.58%
4 -4,-2,-1 (363) -3,-2,-1 (4.79) 11.3%
5 -6,-4,-2,-1 (361) -3,-2,-1 (4.79) 1.29%
6 -6,-4,-2,-1 (361) -3,-2,-1 (4.79) 1.52%
5 Related work
Several approaches attempt to relax the rigid or-
dering enforced by the standard n-gram model.
The skip-gram model (Siu and Ostendorf, Jan
2000) allows the skipping of one word within a
given n-gram. Variable context length language
modeling (Kneser, 1996) achieves a 10% per-
plexity reduction when compared to the trigrams
by varying the order of the n-gram model based
on the context. Dependency models (Rosenfeld,
2000) use the parsed dependency structure of sen-
tences to build the language model as in grammat-
ical trigrams (Lafferty et al, 1992), structured lan-
guage models (Chelba and Jelinek, 2000), and de-
pendency language models (Chelba et al, 1997).
The dependency model governs the whole sen-
tence and each word in a sentence is likely to have
a different dependency structure whereas in our
experiments with FlexGrams we use two connec-
tivity patterns: one for stems and one for suffixes
without the need for parsing.
6 Contributions
We have analyzed the effect of word splitting and
unstructured dependencies on modeling Turkish, a
morphologically complex language. Table 6 com-
pares the models we have tested on our test corpus.
We find that splitting words into their stem and
suffix components using a morphological analyzer
and disambiguator results in significant perplexity
reductions of up to 27%. FlexGram models out-
perform regular n-gram models (Tables 4 and 5)
347
Figure 1: Two FlexGram models where W represents a stem, s represents a suffix, and the arrows represent dependencies.
The left model has stem offsets [+1,-2] and suffix offsets [+1,-1] and cannot be used as a directed graphical model because
of the cycles. The right model has stem offsets [-4,-2] and suffix offsets [+1,-1] and is the best order-3 FlexGram model for
Turkish.
Table 6: Perplexity for compared models.
N unsplit split flexgram
2 3929 4360 5043
3 3421 2610 3083
4 3397 2487 2557
5 3397 2468 2539
6 3397 2465 2539
when using an alternating stem-suffix representa-
tion of the sentences; however Table 6 shows that
the cost of the alternating stem-suffix representa-
tion (zero-suffixes) offsets this gain.
References
Lalit R. Bahl, Frederick Jelinek, and Robert L.
Mercer. A maximum likelihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, 5(2):179?190, 1983.
Peter F. Brown, John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra, Frederick
Jelinek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. A statistical approach to ma-
chine translation. Computational Linguistics,
16(2):79?85, 1990.
Peter F. Brown et al An estimate of an upper
bound for the entropy of english. Computa-
tional Linguistics, 18(1):31?40, 1992.
Ciprian Chelba and Frederick Jelinek. Recog-
nition performance of a structured language
model. CoRR, cs.CL/0001022, 2000.
Ciprian Chelba, David Engle, Frederick Jelinek,
Victor M. Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald
Rosenfeld, Andreas Stolcke, and Dekai Wu.
Structure and performance of a dependency lan-
guage model. In Proc. Eurospeech ?97, pages
2775?2778, Rhodes, Greece, September 1997.
Mathias Creutz, Teemu Hirsim?aki, Mikko Ku-
rimo, Antti Puurula, Janne Pylkk?onen, Vesa
Siivola, Matti Varjokallio, Ebru Arisoy, Mu-
rat Saraclar, and Andreas Stolcke. Morph-
based speech recognition and modeling of out-
of-vocabulary words across languages. TSLP, 5
(1), 2007.
Tobias Hawker. USYD: WSD and lexical substitu-
tion using the Web1T corpus. In SemEval-2007:
4th International Workshop on Semantic Evalu-
ations, 2007.
R. Kneser. Statistical language modeling using a
variable context length. In Proc. ICSLP ?96,
volume 1, pages 494?497, Philadelphia, PA,
October 1996.
John Lafferty, Daniel Sleator, and Davy Tem-
perley. Grammatical trigrams: a probabilistic
model of link grammar. In AAAI Fall Sym-
posium on Probabilistic Approaches to NLP,
1992.
Kemal Oflazer. Two-level description of turkish
morphology. Literary and Linguistic Comput-
ing, 9(2):137?148, 1994.
Ronald Rosenfeld. Two decades of statistical lan-
guage modeling: Where do we go from here.
In Proceedings of the IEEE, volume 88, pages
1270?1278, 2000.
Manhung Siu and M. Ostendorf. Variable n-grams
and extensions for conversational speech lan-
guage modeling. Speech and Audio Processing,
IEEE Transactions on, 8(1):63?75, Jan 2000.
ISSN 1063-6676. doi: 10.1109/89.817454.
Andreas Stolcke. Srilm ? an extensible language
modeling toolkit. In Proc. Int. Conf. Spoken
Language Processing (ICSLP 2002), 2002.
Deniz Yuret. KU: Word sense disambiguation by
substitution. In SemEval-2007: 4th Interna-
tional Workshop on Semantic Evaluations, June
2007.
Deniz Yuret and Ferhan T?ure. Learning mor-
phological disambiguation rules for turkish. In
HLT-NAACL 06, June 2006.
348
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 234?240, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CNGL-CORE: Referential Translation Machines
for Measuring Semantic Similarity
Ergun Bic?ici
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
ebicici@computing.dcu.ie
Josef van Genabith
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
josef@computing.dcu.ie
Abstract
We invent referential translation machines
(RTMs), a computational model for identify-
ing the translation acts between any two data
sets with respect to a reference corpus selected
in the same domain, which can be used for
judging the semantic similarity between text.
RTMs make quality and semantic similarity
judgments possible by using retrieved rele-
vant training data as interpretants for reach-
ing shared semantics. An MTPP (machine
translation performance predictor) model de-
rives features measuring the closeness of the
test sentences to the training data, the diffi-
culty of translating them, and the presence of
acts of translation involved. We view seman-
tic similarity as paraphrasing between any two
given texts. Each view is modeled by an RTM
model, giving us a new perspective on the bi-
nary relationship between the two. Our pre-
diction model is the 15th on some tasks and
30th overall out of 89 submissions in total ac-
cording to the official results of the Semantic
Textual Similarity (STS 2013) challenge.
1 Semantic Textual Similarity Judgments
We introduce a fully automated judge for semantic
similarity that performs well in the semantic textual
similarity (STS) task (Agirre et al, 2013). STS is
a degree of semantic equivalence between two texts
based on the observations that ?vehicle? and ?car?
are more similar than ?wave? and ?car?. Accurate
prediction of STS has a wide application area in-
cluding: identifying whether two tweets are talk-
ing about the same thing, whether an answer is cor-
rect by comparing it with a reference answer, and
whether a given shorter text is a valid summary of
another text.
The translation quality estimation task (Callison-
Burch et al, 2012) aims to develop quality indicators
for translations at the sentence-level and predictors
without access to a reference translation. Bicici et
al. (2013) develop a top performing machine transla-
tion performance predictor (MTPP), which uses ma-
chine learning models over features measuring how
well the test set matches the training set relying on
extrinsic and language independent features.
The semantic textual similarity (STS) task (Agirre
et al, 2013) addresses the following problem. Given
two sentences S1 and S2 in the same language, quan-
tify the degree of similarity with a similarity score,
which is a number in the range [0, 5]. The semantic
textual similarity prediction problem involves find-
ing a function f approximating the semantic textual
similarity score given two sentences, S1 and S2:
f(S1, S2) ? q(S1, S2). (1)
We approach f as a supervised learning problem
with (S1, S2, q(S1, S2)) tuples being the training
data and q(S1, S2) being the target similarity score.
We model the problem as a translation task where
one possible interpretation is obtained by translat-
ing S1 (the source to translate, S) to S2 (the target
translation, T). Since linguistic processing can re-
veal deeper similarity relationships, we also look at
the translation task at different granularities of infor-
mation: plain text (R for regular) , after lemmatiza-
tion (L), after part-of-speech (POS) tagging (P), and
after removing 128 English stop-words (S) 1. Thus,
1http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/
234
we obtain 4 different perspectives on the binary re-
lationship between S1 and S2.
2 Referential Translation Machine (RTM)
Referential translation machines (RTMs) we de-
velop provide a computational model for quality and
semantic similarity judgments using retrieval of rel-
evant training data (Bic?ici and Yuret, 2011a; Bic?ici,
2011) as interpretants for reaching shared seman-
tics (Bic?ici, 2008). We show that RTM achieves very
good performance in judging the semantic similarity
of sentences and we can also use RTM to automat-
ically assess the correctness of student answers to
obtain better results (Bic?ici and van Genabith, 2013)
than the state-of-the-art (Dzikovska et al, 2012).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference cor-
pus selected in the same domain. RTM can be used
for automatically judging the semantic similarity be-
tween texts. An RTM model is based on the selec-
tion of common training data relevant and close to
both the training set and the test set where the se-
lected relevant set of instances are called the inter-
pretants. Interpretants allow shared semantics to be
possible by behaving as a reference point for simi-
larity judgments and providing the context. In semi-
otics, an interpretant I interprets the signs used to
refer to the real objects (Bic?ici, 2008). RTMs pro-
vide a model for computational semantics using in-
terpretants as a reference according to which seman-
tic judgments with translation acts are made. Each
RTM model is a data translation model between the
instances in the training set and the test set. We use
the FDA (Feature Decay Algorithms) instance se-
lection model for selecting the interpretants (Bic?ici
and Yuret, 2011a) from a given corpus, which can
be monolingual when modeling paraphrasing acts,
in which case the MTPP model (Section 2.1) is built
using the interpretants themselves as both the source
and the target side of the parallel corpus. RTMs map
the training and test data to a space where translation
acts can be identified. We view that acts of transla-
tion are ubiquitously used during communication:
Every act of communication is an act of
translation (Bliss, 2012).
src/backend/snowball/stopwords/
Translation need not be between different languages
and paraphrasing or communication also contain
acts of translation. When creating sentences, we use
our background knowledge and translate informa-
tion content according to the current context.
Given a training set train, a test set test, and
some monolingual corpus C, preferably in the same
domain as the training and test sets, the RTM steps
are:
1. T = train ? test.
2. select(T, C)? I
3. MTPP(I,train)? Ftrain
4. MTPP(I,test)? Ftest
5. learn(M,Ftrain)?M
6. predict(M,Ftest)? q?
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3, 4 use I to map train and test to a new
space where similarities between translation acts can
be derived more easily. Step 5 trains a learning
model M over the training features, Ftrain, and
Step 6 obtains the predictions. RTM relies on the
representativeness of I as a medium for building
translation models for translating between train
and test.
Our encouraging results in the STS task provides
a greater understanding of the acts of translation we
ubiquitously use when communicating and how they
can be used to predict the performance of transla-
tion, judging the semantic similarity between text,
and evaluating the quality of student answers. RTM
and MTPP models are not data or language specific
and their modeling power and good performance are
applicable across different domains and tasks. RTM
expands the applicability of MTPP by making it fea-
sible when making monolingual quality and simi-
larity judgments and it enhances the computational
scalability by building models over smaller but more
relevant training data as interpretants.
2.1 The Machine Translation Performance
Predictor (MTPP)
In machine translation (MT), pairs of source and tar-
get sentences are used for training statistical MT
(SMT) models. SMT system performance is af-
fected by the amount of training data used as well
235
as the closeness of the test set to the training set.
MTPP (Bic?ici et al, 2013) is a top performing ma-
chine translation performance predictor, which uses
machine learning models over features measuring
how well the test set matches the training set to pre-
dict the quality of a translation without using a ref-
erence translation. MTPP measures the coverage of
individual test sentence features and syntactic struc-
tures found in the training set and derives feature
functions measuring the closeness of test sentences
to the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of trans-
lation for data transformation.
2.2 MTPP Features for Translation Acts
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
parsing with CCL extracts links from base words
to head words, which allow us to obtain structures
representing the grammatical information instanti-
ated in the training and test data. Feature functions
use statistics involving the training set and the test
sentences to determine their closeness. Since they
are language independent, MTPP allows quality es-
timation to be performed extrinsically. Categories
for the 289 features used are listed below and their
detailed descriptions are presented in (Bic?ici et al,
2013) where the number of features are given in {#}.
? Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Synthetic Translation Performance {6}: Calcu-
lates translation scores achievable according to
the n-gram coverage.
? Length {4}: Calculates the number of words
and characters for S and T and their ratios.
? Feature Vector Similarity {16}: Calculates the
similarities between vector representations.
? Perplexity {90}: Measures the fluency of the
sentences according to language models (LM).
We use both forward ({30}) and backward
({15}) LM based features for S and T.
? Entropy {4}: Calculates the distributional sim-
ilarity of test sentences to the training set.
? Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set are
found in the training set.
? Diversity {6}: Measures the diversity of co-
occurring features in the training set.
? IBM1 Translation Probability {16}: Calculates
the translation probability of test sentences us-
ing the training set (Brown et al, 1993).
? Minimum Bayes Retrieval Risk {4}: Calculates
the translation probability for the translation
having the minimum Bayes risk among the re-
trieved training instances.
? Sentence Translation Performance {3}: Calcu-
lates translation scores obtained according to
q(T,R) using BLEU (Papineni et al, 2002),
NIST (Doddington, 2002), or F1 (Bic?ici and
Yuret, 2011b) for q.
? Character n-grams {4}: Calculates the cosine
between the character n-grams (for n=2,3,4,5)
obtained for S and T (Ba?r et al, 2012).
? LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bjo?rnsson, 1968) for
S and T. 2
3 Experiments
STS contains sentence pairs from news headlines
(headlines), sense definitions from semantic lexical
resources (OnWN is from OntoNotes (Pradhan et
al., 2007) and WordNet (Miller, 1995) and FNWN is
from FrameNet (Baker et al, 1998) and WordNet),
and statistical machine translation (SMT) (Agirre et
al., 2013). STS challenge results are evaluated with
the Pearson?s correlation score (r).
The test set contains 2250 (S1, S2) sentence pairs
with 750, 561, 189, and 750 sentences from each
type respectively. The training set contains 5342
sentence pairs with 1500 each from MSRpar and
MSRvid (Microsoft Research paraphrase and video
description corpus (Agirre et al, 2012)), 1592 from
SMT, and 750 from OnWN.
3.1 RTM Models
We obtain CNGL results for the STS task as fol-
lows. For each perspective described in Section 1,
we build an RTM model. Each RTM model views
the STS task from a different perspective using the
289 features extracted dependent on the interpre-
tants using MTPP. We extract the features both on
2LIX=AB + C
100
A , where A is the number of words, C is
words longer than 6 characters, B is words that start or end with
any of ?.?, ?:?, ?!?, ??? similar to (Hagstro?m, 2012).
236
r R P L S R
+
P
R
+
L
R
+
S
L
+
P
L
+
S
L
+
S
T
L
R
+
P+
L
R
+
P+
S
L
+
P+
S
L
+
P+
S
T
L
R
+
P+
L
+
S
R
+
P+
L
+
S
T
L
S1 ? S2
RR .7904 .7502 .8200 .7788 .8074 .8232 .8101 .8247 .8218 .8509 .8266 .8172 .8304 .8530 .8323 .8499
SVR .8311 .8060 .8443 .8330 .8404 .8517 .8498 .8501 .8593 .8556 .8496 .8422 .8586 .8579 .8527 .8564
S2 ? S1
RR .7922 .7651 .8169 .7891 .8064 .8196 .8136 .8219 .8257 .8257 .8226 .8164 .8284 .8284 .8313 .8324
SVR .8308 .8165 .8407 .8302 .8361 .8506 .8467 .8510 .8567 .8567 .8525 .8460 .8588 .8588 .8575 .8574
S1  S2
RR .8079 .787 .8279 .8101 .8216 .8333 .8275 .8346 .8375 .8409 .8361 .8312 .8412 .8434 .8432 .844
SVR .8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588
Table 1: CV performance on the training set with tuning. Underlined are the settings we use in our submissions. RTM
models in directions S1 ? S2, S2 ? S1, and the bi-directional models S1  S2 are displayed.
the training set and the test set. The training cor-
pus used is the English side of an out-of-domain
corpus on European parliamentary discussions, Eu-
roparl (Callison-Burch et al, 2012) 3. In-domain
corpora are likely to improve the performance. We
use the Stanford POS tagger (Toutanova et al, 2003)
to obtain the perspectives P and L. We use the train-
ing corpus to build a 5-gram target LM.
We use ridge regression (RR) and support vec-
tor regression (SVR) with RBF kernel (Smola and
Scho?lkopf, 2004). Both of these models learn a re-
gression function using the features to estimate a nu-
merical target value. The parameters that govern the
behavior of RR and SVR are the regularization ?
for RR and the C, , and ? parameters for SVR. At
testing time, the predictions are bounded to obtain
scores in the range [0, 5]. We perform tuning on a
subset of the training set separately for each RTM
model and optimize against the performance evalu-
ated with R2, the coefficient of determination.
We do not build a separate model for different
types of sentences and instead use all of the train-
ing set for building a large prediction model. We
also use transductive learning since using only the
relevant training data for training can improve the
performance (Bic?ici, 2011). Transductive learning
is performed at the sentence level where for each test
instance, we select 1250 relevant training instances
using the cosine similarity metric over the feature
vectors and build an individual model for the test in-
stance and predict the similarity score.
3We use WMT?13 corpora from www.statmt.org/wmt13/.
3.2 Training Results
Table 1 lists the 10-fold cross-validation (CV) re-
sults on the training set for RR and SVR for differ-
ent RTM systems using optimized parameters. As
we combine different perspectives, the performance
improves and we use the L+S with SVR for run 1
(LSSVR), L+P+S with SVR for run 2 (LPSSVR),
and L+P+S with SVR using transductive learning
for run 3 (LPSSVRTL) all in the translation direc-
tion S1 ? S2. Lemmatized RTM, L, performs the
best among the individual perspectives. We also
build RTM models in the direction S2 ? S1, which
gives similar results. The last main row combines
them to obtain the bi-directional results, S1  S2,
which improves the performance. Each additional
perspective adds another 289 features to the repre-
sentation and the bi-directional results double the
number of features. Thus, S1  S2 L+P+S is us-
ing 1734 features.
3.3 STS Challenge Results
Table 2 presents the STS challenge r and ranking
results containing our CNGL submissions, the best
system result, and the mean results over all submis-
sions. There were 89 submissions from 35 compet-
ing systems (Agirre et al, 2013). The results are
ranked according to the mean r obtained. We also
include the mean result over all of the submissions
and its corresponding rank.
According to the official results, CNGL-LSSVR
is the 30th system from the top based on the mean r
obtained and CNGL-LPSSVR is 15th according to
the results on OnWN out of 89 submissions in total.
237
System head OnWN FNWN SMT mean rank
CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30
CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33
CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36
UMBC-EB.-PW .7642 .7529 .5818 .3804 .6181 1
mean .6071 .5089 .2906 .3004 .4538 57
Table 2: STS challenge r and ranking results ranked ac-
cording to the mean r obtained. head is headlines and
mean is the mean of all submissions.
CNGL submissions perform unexpectedly low in the
FNWN task and only slightly better than the average
in the SMT task. The lower performance is likely to
be due to using an out-of-domain corpus for building
the RTM models and it may also be due to using and
optimizing a single model for all types of tasks.
3.4 Bi-directional RTM Models
The STS task similarity score is directional invari-
ant: q(S1, S2) = q(S2, S1). We develop RTM mod-
els in the reverse direction and obtain bi-directional
RTM models by combining both. Table 3 lists the
bi-directional results on the STS challenge test set
after tuning, which shows that slight improvement in
the scores are possible when compared with Table 2.
Transductive learning improves the performance in
general. We also compare with the performance ob-
tained when combining uni-directional models with
mean, min, or max functions. Taking the minimum
performs better than other combination approaches
and can achieve r = 0.5129 with TL. One can also
take the individual confidence scores obtained for
each score when combining scores.
4 Conclusion
Referential translation machines provide a clean
and intuitive computational model for automatically
measuring semantic similarity by measuring the acts
of translation involved and achieve to be the 15th on
some tasks and 30th overall in the STS challenge out
of 89 submissions in total. RTMs make quality and
semantic similarity judgments possible based on the
retrieval of relevant training data as interpretants for
reaching shared semantics.
System head OnWN FNWN SMT mean
LS
mean .6552 .6943 .2016 .3005 .5086
mean TL .6397 .6808 .1776 .3147 .5028
min .6512 .6947 .2003 .2984 .5066
min TL .6416 .6853 .1903 .3143 .5055
max .6669 .6680 .1867 .2737 .4958
max TL .6493 .6805 .1846 .3127 .5059
S1  S2 .6388 .6695 .1667 .2999 .4938
S1  S2 TL .6285 .6686 .0918 .2931 .4816
LPS
mean .6510 .6971 .1179 .2861 .4961
mean TL .6524 .6918 .1940 .3176 .5121
min .6608 .6953 .1704 .2922 .5053
min TL .6509 .6864 .1792 .3156 .5084
max .6588 .6800 .1355 .2868 .4961
max TL .6493 .6805 .1846 .3127 .5059
S1  S2 .6251 .6843 .0677 .2994 .4845
S1  S2 TL .6370 .6978 .0951 .2980 .4936
RLPS
mean .6517 .7136 .1002 .2880 .4996
mean TL .6383 .6841 .2434 .3063 .5059
min .6615 .7099 .1644 .2877 .5072
min TL .6606 .6987 .1972 .3059 .5129
max .6589 .7019 .0995 .2935 .5008
max TL .6362 .6896 .2044 .3153 .5063
S1  S2 .6300 .7011 .0817 .2798 .4850
S1  S2 TL .6321 .6956 .1995 .3128 .5052
Table 3: Bi-directional STS challenge r and ranking re-
sults ranked according to the mean r obtained. We com-
bine the two directions by taking the mean, min, or the
max or use the bi-directional RTM model S1  S2.
Acknowledgments
This work is supported in part by SFI (07/CE/I1142)
as part of the Centre for Next Generation Locali-
sation (www.cngl.ie) at Dublin City University and
in part by the European Commission through the
QTLaunchPad FP7 project (No: 296347). We also
thank the SFI/HEA Irish Centre for High-End Com-
puting (ICHEC) for the provision of computational
facilities and support.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
238
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics - Volume 1,
ACL ?98, pages 86?90, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The First Joint Conference on Lexical
and Computational Semantics and Proceedings of the
Seventh International Workshop on Semantic Evalua-
tion (SemEval 2013), Atlanta, Georgia, USA, 14-15
June. Association for Computational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 323?329, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using ex-
trinsic and language independent features. Machine
Translation.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervisor:
Deniz Yuret.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multiagent
and Grid Systems.
Carl Hugo Bjo?rnsson. 1968. La?sbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210, Montre?al, Canada, June. Association
for Computational Linguistics.
Kenth Hagstro?m. 2012. Swedish readability calcula-
tor. https://github.com/keha76/Swedish-Readability-
Calculator.
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?41,
November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Mar-
cus, Martha Palmer, Lance A. Ramshaw, and Ralph M.
Weischedel. 2007. Ontonotes: a unified relational
semantic representation. Int. J. Semantic Computing,
1(4):405?419.
Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D.
thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
239
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.
240
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 585?591, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CNGL: Grading Student Answers by Acts of Translation
Ergun Bic?ici
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
ebicici@computing.dcu.ie
Josef van Genabith
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
josef@computing.dcu.ie
Abstract
We invent referential translation machines
(RTMs), a computational model for identify-
ing the translation acts between any two data
sets with respect to a reference corpus se-
lected in the same domain, which can be used
for automatically grading student answers.
RTMs make quality and semantic similarity
judgments possible by using retrieved rele-
vant training data as interpretants for reach-
ing shared semantics. An MTPP (machine
translation performance predictor) model de-
rives features measuring the closeness of the
test sentences to the training data, the diffi-
culty of translating them, and the presence of
acts of translation involved. We view question
answering as translation from the question to
the answer, from the question to the reference
answer, from the answer to the reference an-
swer, or from the question and the answer to
the reference answer. Each view is modeled
by an RTM model, giving us a new perspective
on the ternary relationship between the ques-
tion, the answer, and the reference answer. We
show that all RTM models contribute and a
prediction model based on all four perspec-
tives performs the best. Our prediction model
is the 2nd best system on some tasks according
to the official results of the Student Response
Analysis (SRA 2013) challenge.
1 Automatically Grading Student Answers
We introduce a fully automated student answer
grader that performs well in the student response
analysis (SRA) task (Dzikovska et al, 2013) and es-
pecially well in tasks with unseen answers. Auto-
matic grading can be used for assessing the level of
competency for students and estimating the required
tutoring effort in e-learning platforms. It can also
be used to adapt questions according to the average
student performance. Low scored topics can be dis-
cussed further in classrooms, enhancing the overall
coverage of the course material.
The quality estimation task (QET) (Callison-
Burch et al, 2012) aims to develop quality indica-
tors for translations at the sentence-level and pre-
dictors without access to the reference. Bicici et
al. (2013) develop a top performing machine transla-
tion performance predictor (MTPP), which uses ma-
chine learning models over features measuring how
well the test set matches the training set relying on
extrinsic and language independent features.
The student response analysis (SRA)
task (Dzikovska et al, 2013) addresses the fol-
lowing problem. Given a question, a known correct
reference answer, and a student answer, assess the
correctness of the student?s answer. The student
answers are categorized as correct, partially correct
incomplete, contradictory, irrelevant, or non do-
main, in the 5-way task; as correct, contradictory,
or incorrect in the 3-way task; and as correct or
incorrect in the 2-way task.
The student answer correctness prediction prob-
lem involves finding a function f approximating the
student answer correctness given the question (Q),
the answer (A), and the reference answer (R):
f(Q,A,R) ? q(A,R). (1)
We approach f as a supervised learning problem
with (Q, A, R, q(A,R)) tuples being the training
585
data and q(A,R) being the target correctness score.
We model the problem as a translation task where
one possible interpretation is translating Q (source
to translate, S) to R (target translation, T) and evalu-
ating with A (as reference target, RT) (QRA). Since
the information appearing in the question may be re-
peated in the reference answer or may be omitted in
the student answer, it also makes sense to concate-
nate Q and A when translating to R (QARQA). We
obtain 4 different perspectives on the ternary rela-
tionship between Q, A, and R depending on how we
model their relationship as an instance of translation:
QAR : S = Q, T = A, RT = R.
QRA : S = Q, T = R, RT = A.
ARA : S = A, T = R, RT = A.
QARQA : S = Q+A, T = R, RT = Q+A.
2 The Machine Translation Performance
Predictor (MTPP)
In machine translation (MT), pairs of source and tar-
get sentences are used for training statistical MT
(SMT) models. SMT system performance is af-
fected by the amount of training data used as well
as the closeness of the test set to the training set.
MTPP (Bic?ici et al, 2013) is a top performing ma-
chine translation performance predictor, which uses
machine learning models over features measuring
how well the test set matches the training set to pre-
dict the quality of a translation without using a ref-
erence translation. MTPP measures the coverage of
individual test sentence features and syntactic struc-
tures found in the training set and derives feature
functions measuring the closeness of test sentences
to the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of trans-
lation involved.
Features for Translation Acts
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
parsing with CCL extracts links from base words
to head words, which allow us to obtain structures
representing the grammatical information instanti-
ated in the training and test data. Feature functions
use statistics involving the training set and the test
sentences to determine their closeness. Since they
are language independent, MTPP allows quality es-
timation to be performed extrinsically. Categories
for the 283 features used are listed below and their
detailed descriptions are presented in (Bic?ici et al,
2013) where the number of features are given in {#}.
? Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Synthetic Translation Performance {6}: Calcu-
lates translation scores achievable according to
the n-gram coverage.
? Length {4}: Calculates the number of words
and characters for S and T and their ratios.
? Feature Vector Similarity {16}: Calculates the
similarities between vector representations.
? Perplexity {90}: Measures the fluency of the
sentences according to language models (LM).
We use both forward ({30}) and backward
({15}) LM based features for S and T.
? Entropy {4}: Calculates the distributional sim-
ilarity of test sentences to the training set.
? Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set are
found in the training set.
? Diversity {6}: Measures the diversity of co-
occurring features in the training set.
? IBM1 Translation Probability {16}: Calculates
the translation probability of test sentences us-
ing the training set (Brown et al, 1993).
? Minimum Bayes Retrieval Risk {4}: Calculates
the translation probability for the translation
having the minimum Bayes risk among the re-
trieved training instances.
? Sentence Translation Performance {3}: Calcu-
lates translation scores obtained according to
q(T,R) using BLEU (Papineni et al, 2002),
NIST (Doddington, 2002), or F1 (Bic?ici and
Yuret, 2011b) for q.
3 Referential Translation Machine (RTM)
Referential translation machines (RTMs) we de-
velop provide a computational model for quality and
semantic similarity judgments using retrieval of rel-
evant training data (Bic?ici and Yuret, 2011a; Bic?ici,
2011) as interpretants for reaching shared seman-
tics (Bic?ici, 2008). We show that RTM achieves
586
very good performance in judging the semantic sim-
ilarity of sentences (Bic?ici and van Genabith, 2013)
and we can also use RTM to automatically assess
the correctness of student answers to obtain better
results than the baselines proposed by (Dzikovska et
al., 2012), which achieve the best performance on
some tasks (Dzikovska et al, 2013).
RTM is a computational model for identifying the
acts of translation for translating between any given
two data sets with respect to a reference corpus se-
lected in the same domain. RTM can be used for
automatically grading student answers. An RTM
model is based on the selection of common train-
ing data relevant and close to both the training set
and the test set where the selected relevant set of
instances are called the interpretants. Interpretants
allow shared semantics to be possible by behaving
as a reference point for similarity judgments and
providing the context. In semiotics, an interpretant
I interprets the signs used to refer to the real ob-
jects (Bic?ici, 2008). RTMs provide a model for com-
putational semantics using interpretants as a refer-
ence according to which semantic judgments with
translation acts are made. Each RTM model is a data
translation model between the instances in the train-
ing set and the test set. We use the FDA (Feature De-
cay Algorithms) instance selection model for select-
ing the interpretants (Bic?ici and Yuret, 2011a) from a
given corpus, which can be monolingual when mod-
eling paraphrasing acts, in which case the MTPP
model is built using the interpretants themselves as
both the source and the target side of the parallel cor-
pus. RTMs map the training and test data to a space
where translation acts can be identified. We view
that acts of translation are ubiquitously used during
communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different languages
and paraphrasing or communication also contain
acts of translation. When creating sentences, we use
our background knowledge and translate informa-
tion content according to the current context.
Given a training set train, a test set test, and
some monolingual corpus C, preferably in the same
domain as the training and test sets, the RTM steps
are:
1. T = train ? test.
2. select(T, C)? I
3. MTPP(I,train)? Ftrain
4. MTPP(I,test)? Ftest
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3 and 4 use I to map train and test to
a new space where similarities between the transla-
tion acts can be derived more easily. RTM relies on
the representativeness of I as a medium for building
translation models for translating between train
and test.
Our encouraging results in the SRA task provides
a greater understanding of the acts of translation we
ubiquitously use when communicating and how they
can be used to predict the performance of trans-
lation, judging the semantic similarity of text, and
evaluating the quality of student answers. RTM and
MTPP models are not data or language specific and
their modeling power and good performance are ap-
plicable across different domains and tasks. RTM
expands the applicability of MTPP by making it fea-
sible when making monolingual quality and simi-
larity judgments and it enhances the computational
scalability by building models over smaller but more
relevant training data as interpretants.
4 Experiments
SRA involves the prediction on Beetle (student
interactions when learning conceptual knowledge
in the basic electricity and electronics domain)
and SciEntsBank (science assessment questions)
datasets. SciEntsBank is harder due to contain-
ing questions from multiple domains (Dzikovska
et al, 2012). SRA challenge results are eval-
uated with the weighted average F1, Fw1 =
1
N
?
c?C NcF1(c) and the macro average F1, F
m
1 =
1
|C|
?
c?C F1(c) (Dzikovska et al, 2012).
The lexical baseline system is based on measures
of lexical overlap using 4 features: the number of
overlapping words, F1, Lesk (Lesk, 1986), and co-
sine scores over the words when comparing A and
R ({4}) and Q and R ({4}). Lesk score is calculated
as: L(A,R) =
?
p?M |p|
2/(|A||R|), where M con-
tains the maximal overlapping phrases that match in
587
A and R and |p| is the length of a phrase 1. This lex-
ical baseline is highly competitive: no submission
performed better in the 2-way Beetle unseen ques-
tions task.
4.1 RTM Models
We obtain CNGL results for the SRA task as fol-
lows. For each perspective described in Section 1,
we build an RTM model. Each RTM model views
the SRA task from a different perspective using the
283 features extracted dependent on the interpre-
tants using MTPP. We extract the features both on
the training set of 4155 and the test set of 1258 (Q,
A, R) sentence triples for the Beetle task and the
training set of 5251 and the test set of 5835 (Q, A,
R) sentence triples for the SciEntsBank task. The
addition of lexical overlap baseline features slightly
helps. We use the best reference answer if the refer-
ence answer is not identified in the training set.
The training corpus used is the English side of
an out-of-domain corpus on European parliamen-
tary discussions, Europarl (Callison-Burch et al,
2012) 2, to which we also add the unique sentences
from R. In-domain corpora are likely to improve the
performance. We do not perform any linguistic pro-
cessing or use other external resources. We use only
extrinsic features, or features that are ignorant of any
information intrinsic to, and dependent on, a given
language or domain. We use the training corpus to
build a 5-gram target LM. We use ridge regression
(RR) and support vector regression (SVR) with RBF
kernel (Smola and Scho?lkopf, 2004). Both of these
models learn a regression function using the features
to estimate a numerical target value. The parameters
that govern the behavior of RR and SVR are the reg-
ularization ? for RR and the C, , and ? parameters
for SVR. At testing time, the predictions are bound
so as to have scores in the range [0, 1], [0, 2], or [0, 4]
and rounded for finding the predicted category.
4.2 Training Results
Table 1 lists the 10-fold cross-validation (CV) re-
sults on the training set for RR and SVR for dif-
ferent RTM systems without the parameter op-
timization. As we combine different perspec-
tives, the performance improves and we use the
1http://search.cpan.org/dist/Text-Similarity/
2We use WMT?13 corpora from www.statmt.org/wmt13/.
QAR+QRA+ARA+QARQA system for our submis-
sions using RR for run 1, SVR for run 2. ARA per-
forms the best among individual perspectives. Each
additional perspective adds another 283 features to
the representation.
Fm1 / F
w
1 Beetle SciEntsBank
Model RR SVR RR SVR
QAR .38/.49 .45/.57 .21/.30 .28/.36
QRA .33/.50 .33/.53 .22/.31 .29/.42
ARA .45/.54 .50/.60 .21/.30 .30/.38
QARQA .35/.50 .40/.58 .20/.27 .27/.40
QAR+ARA .47/.55 .49/.61 .26/.36 .32/.39
QAR+ARA+QARQA .48/.57 .49/.62 .31/.38 .29/.40
QAR+QRA+ARA+QARQA .48/.56 .48/.61 .31/.38 .29/.40
Table 1: Performance on the training set without tuning.
We perform tuning on a subset of the Beetle
and SciEntsBank datasets separately after including
the baseline lexical overlap features and optimize
against the performance evaluated withR2, the coef-
ficient of determination. SVR performance is given
in Table 2. The CNGL system significantly outper-
forms the lexical overlap baseline in all tasks for
Beetle and in the 2-way task for SciEntsBank. For
3-way and 5-way, CNGL performs slightly better.
Fm1 / F
w
1 Beetle SciEntsBank
System 2 3 5 2 3 5
Lexical .74/.75 .53/.56 .46/.53 .61/.64 .43/.55 .29/.41
CNGL .84/.84 .61/.63 .55/.63 .74/.75 .47/.56 .30/.41
Table 2: Optimized SVR results vs. lexical overlap base-
line on the training set for 2-way, 3-way, or 5-way tasks.
4.3 SRA Challenge Results
The SRA task test set alo contains instances that be-
long to unseen questions (uQ) and unseen domains
(uD), which make it harder to predict. The train-
ing data provided for the task correspond to learning
with unseen answers (uA). Table 3 presents the SRA
challenge results containing the lexical overlap, our
CNGL SVR submission (RR is slightly worse), and
the maximum and mean results 3.
According to the official results, CNGL SVR is
the 2nd best system based on 5-way evaluation (4th
3Max is not the performance of the best performing system
but the maximum result obtained for each metric and subtask.
588
Fm1 / F
w
1 Beetle SciEntsBank
System uA uQ uA uQ uD
2
Lexical .80/.79 .74/.72 .64/.62 .65/.63 .66/.65
CNGL .80/.81 .67/.68 .55/.57 .56/.58 .56/.57
Mean .71/.72 .61/.62 .64/.66 .60/.62 .61/.63
Max .84/.84 .72/.73 .77/.77 .74/.74 .70/.71
3
Lexical .55/.58 .48/.50 .40/.52 .39/.52 .42/.55
CNGL .57/.59 .45/.47 .33/.38 .31/.37 .31/.36
Mean .54/.55 .41/.42 .48/.56 .39/.51 .39/.51
Max .72/.73 .58/.60 .65/.71 .47/.63 .49/.62
5
Lexical .42/.48 .41/.46 .30/.44 .26/.40 .25/.40
CNGL .43/.55 .38/.47 .20/.27 .21/.30 .22/.29
Mean .44/.51 .34/.40 .34/.46 .24/.38 .26/.37
Max .62/.70 .55/.61 .48/.64 .31/.49 .38/.47
Table 3: SRA challenge results: CNGL SVR submission,
the lexical overlap baseline, and the maximum and mean
results for 2-way, 3-way, or 5-way tasks. uA, uQ, and uD
correspond to unseen answers, questions, and domains.
result overall) and the 3rd best system based on 2-
way and 3-way evaluation (5th result overall) on the
uQ Beetle task. The SVR model performs better
than the lexical baseline and the mean result in the
Beetle task but performs worse in the SciEntsBank.
The lower performance is likely to be due to using an
out-of-domain training corpus for building the RTM
models and on the uQ and uD tasks, it may also be
due to optimizing on the uA task only. The lower
performance in SciEntsBank is also due to multiple
question domains (Dzikovska et al, 2012).
SVR Beetle SciEntsBank
Fw1 2 3 5 2 3 5
(a) QAR+ARA .86 .66 .64 .77 .56 .42
(b) QAR+ARA+QARQA .86 .66 .65 .77 .57 .45
(c) QAR+QRA+ARA+QARQA .85 .64 .63 .77 .58 .45
Fm1 2 3 5 2 3 5
(a) QAR+ARA .86 .64 .55 .76 .47 .34
(b) QAR+ARA+QARQA .85 .64 .55 .76 .48 .36
(c) QAR+QRA+ARA+QARQA .85 .62 .54 .76 .49 .35
Table 4: Improved SVR performance on the training set
with tuning for 2-way, 3-way, or 5-way tasks.
4.4 Improved RTM Models
We improve the RTM model with the expansion of
our representation by adding the following features:
? Character n-grams {4}: Calculates the cosine
between the character n-grams (for n=2,3,4,5)
obtained for S and T (Ba?r et al, 2012).
? LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bjo?rnsson, 1968) for
S and T. 4
Table 4 lists the improved results on the training set
after tuning, which shows about 0.04 increase in all
scores when compared with Table 1 and Table 2.
Fm1 /F
w
1 Beetle SciEntsBank
Model uA uQ uA uQ uD
2
(a) .81/.82 .70/.71 .55/.57 .58/.58 .56/.57
(b) .80/.81 .71/.72 .69/.70 .54/.56 .56/.58
(c) .79/.79 .70/.71 .60/.59 .57/.58 .55/.57
3
(a) .59/.61 .48/.49 .26/.34 .34/.40 .26/.32
(b) .60/.62 .47/.48 .36/.43 .31/.38 .29/.34
(c) .58/.60 .46/.48 .41/.48 .30/.39 .29/.34
5
(a) .47/.56 .37/.45 .19/.22 .22/.33 .22/.29
(b) .43/.56 .36/.45 .26/.37 .23/.33 .21/.30
(c) .42/.52 .40/.48 .27/.39 .24/.33 .20/.30
Table 5: Improved SVR results on the SRA task test set.
Fm1 /F
w
1 SciEntsBank
Model uA uQ uD
2
(a) .56/.57 .54/.55 .53/.55
(b) .57/.58 .53/.54 .56/.57
(c) .57/.58 .55/.57 .57/.59
3
(a) .36/.45 .33/.44 .39/.49
(b) .35/.40 .36/.44 .39/.48
(c) .37/.46 .36/.48 .40/.50
5
(a) .24/.34 .23/.33 .26/.39
(b) .24/.36 .25/.38 .26/.38
(c) .24/.36 .21/.32 .28/.39
Table 6: Improved TREE results on the SRA task test set.
Table 5 presents the improved SVR results on the
SRA task test set, which shows about 0.03 increase
in all scores when compared with Table 3. SVR be-
comes the 2nd best system and 2nd best result in
2-way evaluation and the 3rd best system from the
top based on 2-way and 3-way evaluation (5th result
overall) on the uQ Beetle task.
4LIX=AB + C
100
A , where A is the number of words, C is
words longer than 6 characters, B is words that start or end with
any of ?.?, ?:?, ?!?, ??? similar to (Hagstro?m, 2012).
589
We observe that decision tree regression (Hastie
et al, 2009) (TREE) generalizes to uQ and uD do-
mains better than the RR or SVR models especially
in the SciEntsBank corpus. Table 6 presents TREE
results on the SRA SciEntsBank test set, which
shows significant increase in uQ and uD tasks when
compared with Table 5.
5 Conclusion
Referential translation machines provide a clean
and intuitive computational model for automatically
grading student answers by measuring the acts of
translation involved and achieve to be the 2nd best
system on some tasks in the SRA challenge. RTMs
make quality and semantic similarity judgments
possible based on the retrieval of relevant training
data as interpretants for reaching shared semantics.
Acknowledgments
This work is supported in part by SFI (07/CE/I1142)
as part of the Centre for Next Generation Locali-
sation (www.cngl.ie) at Dublin City University and
in part by the European Commission through the
QTLaunchPad FP7 project (No: 296347). We also
thank the SFI/HEA Irish Centre for High-End Com-
puting (ICHEC) for the provision of computational
facilities and support.
References
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013. CNGL-
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The First Joint
Conference on Lexical and Computational Semantics,
Atlanta, Georgia, USA, 13-14 June. Association for
Computational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 323?329, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using ex-
trinsic and language independent features. Machine
Translation.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervisor:
Deniz Yuret.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multiagent
and Grid Systems.
Carl Hugo Bjo?rnsson. 1968. La?sbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210, Montre?al, Canada, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
590
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Kenth Hagstro?m. 2012. Swedish readability calcula-
tor. https://github.com/keha76/Swedish-Readability-
Calculator.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference and Prediction. Springer-Verlag,
2nd edition.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, pages 24?26, New York, NY,
USA. ACM.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D.
thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.
591
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 487?496,
Dublin, Ireland, August 23-24, 2014.
RTM-DCU: Referential Translation Machines for Semantic Similarity
Ergun Bic?ici
Centre for Global Intelligent Content
School of Computing
Dublin City University, Dublin, Ireland.
ebicici@computing.dcu.ie
Andy Way
Centre for Global Intelligent Content
School of Computing,
Dublin City University, Dublin, Ireland.
away@computing.dcu.ie
Abstract
We use referential translation machines
(RTMs) for predicting the semantic simi-
larity of text. RTMs are a computational
model for identifying the translation acts
between any two data sets with respect
to interpretants selected in the same do-
main, which are effective when making
monolingual and bilingual similarity judg-
ments. RTMs judge the quality or the se-
mantic similarity of text by using retrieved
relevant training data as interpretants for
reaching shared semantics. We derive fea-
tures measuring the closeness of the test
sentences to the training data via inter-
pretants, the difficulty of translating them,
and the presence of the acts of transla-
tion, which may ubiquitously be observed
in communication. RTMs provide a lan-
guage independent approach to all simi-
larity tasks and achieve top performance
when predicting monolingual cross-level
semantic similarity (Task 3) and good re-
sults in semantic relatedness and entail-
ment (Task 1) and multilingual semantic
textual similarity (STS) (Task 10). RTMs
remove the need to access any task or do-
main specific information or resource.
1 Semantic Similarity Judgments
We introduce a fully automated judge for seman-
tic similarity that performs well in three seman-
tic similarity tasks at SemEval-2014, Semantic
Evaluation Exercises - International Workshop on
Semantic Evaluation (Nakov and Zesch, 2014).
RTMs provide a language independent solution for
the semantic textual similarity (STS) task (Task
10) (Agirre et al., 2014), achieve top perfor-
mance when predicting monolingual cross-level
semantic similarity (Task 3) (Jurgens et al., 2014),
and achieve good results in the semantic related-
ness and entailment task (Task 1) (Marelli et al.,
2014a).
Referential translation machine (Section 2) is
a computational model for identifying the acts of
translation for translating between any given two
data sets with respect to a reference corpus se-
lected in the same domain. An RTM model is
based on the selection of interpretants, training
data close to both the training set and the test set,
which allow shared semantics by providing con-
text for similarity judgments. In semiotics, an in-
terpretant I interprets the signs used to refer to the
real objects (Bic?ici, 2008). Each RTM model is
a data translation and translation prediction model
between the instances in the training set and the
test set and translation acts are indicators of the
data transformation and translation. RTMs present
an accurate and language independent solution for
making semantic similarity judgments.
We describe the tasks we participated below.
Section 2 describes the RTM model and the fea-
tures used. Section 3 presents the training and test
results we obtain on the three tasks we competed
and the last section concludes.
Task 1 Evaluation of Compositional Distribu-
tional Semantic Models on Full Sentences
through Semantic Relatedness and Entail-
ment (SRE) (Marelli et al., 2014a):
Given two sentences, produce a related-
ness score indicating the extent to which
the sentences express a related meaning: a
number in the range [1, 5].
We model the problem as a translation perfor-
mance prediction task where one possible inter-
pretation is obtained by translating S
1
(the source
to translate, S) to S
2
(the target translation, T).
Since linguistic processing can reveal deeper sim-
ilarity relationships, we also look at the translation
task at different granularities of information: plain
487
text (R for regular) and after lemmatization (L).
We lowercase all text.
Task 3 Cross-Level Semantic Similarity
(CLSS) (Jurgens et al., 2014):
Given two text from different levels, pro-
duce a semantic similarity rating: a num-
ber in the range [0, 4].
CLSS task targets semantic similarity compar-
isons between text having different levels of gran-
ularity and we address the following level cross-
ings: paragraph to sentence, sentence to phrase,
and phrase to word. We model the problem as
a translation performance prediction task among
text from different levels.
Task 10 Multilingual Semantic Textual Similarity
(MSTS) (Agirre et al., 2014)
Given two sentences S
1
and S
2
in the same
language, quantify the degree of similar-
ity: a number in the range [0, 5].
MSTS task addresses the problem in English
and Spanish (score range is [0, 4]). We model the
problem as a translation performance prediction
task between S
1
and S
2
.
2 Referential Translation Machine
(RTM)
Referential translation machines provide a compu-
tational model for quality and semantic similarity
judgments in monolingual and bilingual settings
using retrieval of relevant training data (Bic?ici,
2011; Bic?ici and Yuret, 2014) as interpretants for
reaching shared semantics (Bic?ici, 2008). RTMs
are a language independent approach and achieve
top performance when predicting the quality of
translations (Bic?ici, 2013; Bic?ici and Way, 2014)
and when predicting monolingual cross-level se-
mantic similarity (Jurgens et al., 2014), and good
performance when evaluating the semantic relat-
edness of sentences and their entailment (Marelli
et al., 2014a), as an automated student answer
grader (Bic?ici and van Genabith, 2013b), and
when judging the semantic similarity of sen-
tences (Bic?ici and van Genabith, 2013a; Agirre et
al., 2014). We improve the RTM models by:
? using a parameterized, fast implementation
of FDA, FDA5, and our Parallel FDA5 in-
stance selection model (Bic?ici et al., 2014),
? better modeling of the language in which
Algorithm 1: Referential Translation Machine
Input: Training set train, test set test,
corpus C, and learning model M .
Data: Features of train and test, F
train
and F
test
.
Output: Predictions of similarity scores on
the test q?.
1 FDA5(train,test, C)? I
2 MTPP(I,train)? F
train
3 MTPP(I,test)? F
test
4 learn(M,F
train
)?M
5 predict(M,F
test
)? q?
similarity judgments are made with improved
optimization and selection of the LM data,
? using a general domain corpus to select inter-
pretants from,
? increased feature set for also modeling the
structural properties of sentences,
? extended learning models.
We use the Parallel FDA5 (Feature Decay Algo-
rithms) instance selection model for selecting the
interpretants (Bic?ici et al., 2014; Bic?ici and Yuret,
2014) this year, which allows efficient parameteri-
zation, optimization, and implementation of FDA,
and build an MTPP model (Section 2.1). We view
that acts of translation are ubiquitously used dur-
ing communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different lan-
guages and paraphrasing or communication also
contain acts of translation. When creating sen-
tences, we use our background knowledge and
translate information content according to the cur-
rent context.
The inputs to the RTM algorithm Algorithm 1
are a training set train, a test set test, some
corpus C, preferably in the same domain as the
training and test sets, and a learning model. Step 1
selects the interpretants, I, relevant to both the
training and test data. Steps 2 and 3 use I to map
train and test to a new space where similari-
ties between translation acts can be derived more
easily. Step 4 trains a learning model M over the
training features, F
train
, and Step 5 obtains the
predictions. Figure 1 depicts the RTM.
488
Figure 1: RTM depiction.
Our encouraging results in the semantic simi-
larity tasks increase our understanding of the acts
of translation we ubiquitously use when commu-
nicating and how they can be used to predict the
semantic similarity of text. RTM and MTPP mod-
els are not data or language specific and their mod-
eling power and good performance are applicable
in different domains and tasks. RTM expands the
applicability of MTPP by making it feasible when
making monolingual quality and similarity judg-
ments and it enhances the computational scalabil-
ity by building models over smaller and more rel-
evant set of interpretants.
2.1 The Machine Translation Performance
Predictor (MTPP)
MTPP (Bic?ici et al., 2013) is a state-of-the-art
and top performing machine translation perfor-
mance predictor, which uses machine learning
models over features measuring how well the test
set matches the training set to predict the quality
of a translation without using a reference trans-
lation. MTPP measures the coverage of individ-
ual test sentence features found in the training set
and derives indicators of the closeness of test sen-
tences to the available training data, the difficulty
of translating the sentence, and the presence of
acts of translation for data transformation.
2.2 MTPP Features for Translation Acts
MTPP feature functions use statistics involving
the training set and the test sentences to deter-
mine their closeness. Since they are language
independent, MTPP allows quality estimation to
be performed extrinsically. MTPP uses n-gram
features defined over text or common cover link
(CCL) (Seginer, 2007) structures as the basic units
of information over which similarity calculations
are made. Unsupervised parsing with CCL ex-
tracts links from base words to head words, rep-
resenting the grammatical information instantiated
in the training and test data.
We extend the MTPP model we used last
year (Bic?ici, 2013) in its learning module and the
features included. Categories for the features (S
for source, T for target) used are listed below
where the number of features are given in brackets
for S and T, {#S, #T}, and the detailed descriptions
for some of the features are presented in (Bic?ici et
al., 2013). The number of features for each task
differs since we perform an initial feature selection
step on the tree structural features (Section 2.3).
The number of features are in the range 337?437.
? Coverage {56, 54}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Perplexity {45, 45}: Measures the fluency of
the sentences according to language models
(LM). We use both forward ({30}) and back-
ward ({15}) LM features for S and T.
? TreeF {0, 10-110}: 10 base features and up
to 100 selected features of T among parse tree
structures (Section 2.3).
? Retrieval Closeness {16, 12}: Measures the
degree to which sentences close to the test set
are found in the selected training set, I, using
FDA (Bic?ici and Yuret, 2011a) and BLEU,
F
1
(Bic?ici, 2011), dice, and tf-idf cosine sim-
ilarity metrics.
? IBM2 Alignment Features {0, 22}: Calcu-
lates the sum of the entropy of the dis-
tribution of alignment probabilities for S
(
?
s?S
?p log p for p = p(t|s) where s and
t are tokens) and T, their average for S and
T, the number of entries with p ? 0.2 and
p ? 0.01, the entropy of the word align-
ment between S and T and its average, and
word alignment log probability and its value
in terms of bits per word. We also com-
pute word alignment percentage as in (Ca-
margo de Souza et al., 2013) and potential
BLEU, F
1
, WER, PER scores for S and T.
? IBM1 Translation Probability {4, 12}: Cal-
culates the translation probability of test
sentences using the selected training set,
I (Brown et al., 1993).
? Feature Vector Similarity {8, 8}: Calculates
similarities between vector representations.
489
CCL
numB depthB avg depthB R/L avg R/L
24.0 9.0 0.375 2.1429 3.401
2
1 1
1
1 13
1
1 2
1
1 8
1
2 10
1
3 1
1
3 4
1
5 1
1
7 15
Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).
? Entropy {2, 8}: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences (Bic?ici et al.,
2013).
? Length {6, 3}: Calculates the number of
words and characters for S and T and their
average token lengths and their ratios.
? Diversity {3, 3}: Measures the diver-
sity of co-occurring features in the training
set (Bic?ici et al., 2013).
? Synthetic Translation Performance {3, 3}:
Calculates translation scores achievable ac-
cording to the n-gram coverage.
? Character n-grams {5}: Calculates cosine
between character n-grams (for n=2,3,4,5,6)
obtained for S and T (B?ar et al., 2012).
? Minimum Bayes Retrieval Risk {0, 4}: Cal-
culates the translation probability for the
translation having the minimum Bayes risk
among the retrieved training instances.
? Sentence Translation Performance {0, 3}:
Calculates translation scores obtained ac-
cording to q(T,R) using BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), or
F
1
(Bic?ici and Yuret, 2011b) for q.
? LIX {1, 1}: Calculates the LIX readability
score (Wikipedia, 2013; Bj?ornsson, 1968) for
S and T.
1
2.3 Bracketing Tree Structural Features
We use the parse tree outputs obtained by CCL
to derive features based on the bracketing struc-
ture. We derive 5 statistics based on the geometric
properties of the parse trees: number of brackets
used (numB), depth (depthB), average depth (avg
1
LIX=
A
B
+ C
100
A
, where A is the number of words, C is
words longer than 6 characters, B is words that start or end
with any of ?.?, ?:?, ?!?, ??? similar to (Hagstr?om, 2012).
depthB), number of brackets on the right branches
over the number of brackets on the left (R/L)
2
, av-
erage right to left branching over all internal tree
nodes (avg R/L). The ratio of the number of right
to left branches shows the degree to which the sen-
tence is right branching or not. Additionally, we
capture the different types of branching present
in a given parse tree identified by the number of
nodes in each of its children.
Table 1 depicts the parsing output obtained by
CCL for the following sentence from WSJ23
3
:
Many fund managers argue that now ?s the time
to buy .
We use Tregex (Levy and Andrew, 2006) for vi-
sualizing the output parse trees presented on the
left. The bracketing structure statistics and fea-
tures are given on the right hand side. The root
node of each tree structural feature represents the
number of times that feature is present in the pars-
ing output of a document.
3 SemEval-14 Results
We develop individual RTM models for each task
and subtask that we participate at SemEval-2014
with the RTM-DCU team name. The interpre-
tants are selected from the LM corpora distributed
by the translation task of WMT14 (Bojar et al.,
2014) and the LM corpora provided by LDC for
English (Parker et al., 2011) and Spanish (
?
Angelo
Mendonc?a, 2011)
4
. We use the Stanford POS tag-
ger (Toutanova et al., 2003) to obtain the lemma-
tized corpora for the SRE task. For each RTM
2
For nodes with uneven number of children, the nodes in
the odd child contribute to the right branches.
3
Wall Street Journal (WSJ) corpus section 23, distributed
with Penn Treebank version 3 (Marcus et al., 1993).
4
English Gigaword 5th, Spanish Gigaword 3rd edition.
490
model, we extract the features both on the train-
ing set and the test set. The number of instances
we select for the interpretants in each task is given
in Table 2.
Task Setting Train LM
Task 1, SRE English 770 10770
Task 3, CLSS Par2S 302 2802
Task 3, CLSS S2Phrase 202 2702
Task 3, CLSS Phrase2W 102 2602
Task 10, MSTS English 504 8002
Task 10, MSTS English OnWN 504 8004
Task 10, MSTS Spanish 502 8002
Table 2: Number of sentences in I (in thousands)
selected for each task.
We use ridge regression (RR), support vector
regression (SVR) with RBF (radial basis func-
tions) kernel (Smola and Sch?olkopf, 2004), and
extremely randomized trees (TREE) (Geurts et al.,
2006) as the learning models. TREE is an en-
semble learning method over randomized decision
trees. These models learn a regression function
using the features to estimate a numerical target
value. We also use these learning models after
a feature subset selection with recursive feature
elimination (RFE) (Guyon et al., 2002) or a di-
mensionality reduction and mapping step using
partial least squares (PLS) (Specia et al., 2009),
both of which are described in (Bic?ici et al., 2013).
We optimize the learning parameters, the num-
ber of features to select, the number of dimen-
sions used for PLS, and the parameters for paral-
lel FDA5. More detailed descriptions of the opti-
mization processes are given in (Bic?ici et al., 2013;
Bic?ici et al., 2014). We optimize the learning pa-
rameters by selecting ? close to the standard devi-
ation of the noise in the training set (Bic?ici, 2013)
since the optimal value for ? is shown to have
linear dependence to the noise level for different
noise models (Smola et al., 1998). At testing time,
the predictions are bounded to obtain scores in the
corresponding ranges. We obtain the confidence
scores using support vector classification (SVC).
3.1 Task 1: Semantic Relatedness and
Entailment
MSTS contains sentence pairs from the SICK
(Sentences Involving Compositional Knowledge)
data set (Marelli et al., 2014b), which contain sen-
tence pairs that contain rich lexical, syntactic and
semantic phenomena. Official evaluation metric
in SRE is the Pearson?s correlation score, which
is used to select the top systems on the training
set. SRE task allows the submission of 5 entries.
We present the performance of the top 5 individ-
ual RTM models on the training set in Table 3.
ACC is entailment accuracy, r
P
is Pearson?s corre-
lation, r
S
is Spearman?s correlation, MSE is mean
squared error, MAE is mean absolute error, and
RAE is relative absolute error. L uses the lem-
matized corpora and R uses the true-cased corpora
corresponding to regular. R+L correspond to the
perspective using the features from both R and L,
which doubles the number of features. We com-
pute the entailment by SVC.
Data Model ACC r
P
r
S
MSE MAE RAE
L SVR 67.52 .7372 .6918 .6946 .5511 .6856
L PLS-SVR 67.04 .7539 .6927 .6763 .5369 .668
R+L PLS-SVR 66.76 .75 .6879 .6815 .539 .6705
R+L SVR 66.66 .7295 .6814 .7027 .5591 .6956
L PLS-RR 66.56 .7247 .6765 .7054 .5687 .7075
Table 3: SRE training results of the top 5 RTM
systems selected.
SRE challenge results on the test set are given
in Table 4. The setting R using PLS-SVR learning
becomes the 8th out of 17 submissions when pre-
dicting the semantic relatedness and 17th out of 18
submissions when predicting the entailment.
Data Model ACC r
P
r
S
RMSE MAE RAE
R PLS-SVR 67.20 .7639 .6877 .655 .5246 .6645
R+L PLS-SVR 67.65 .7688 .6918 .6492 .5194 .658
L SVR 67.65 .7559 .6887 .664 .531 .6726
R+L SVR 67.44 .7625 .6899 .6555 .5251 .6651
R PLS-SVR 66.61 .7570 .6683 .6637 .5324 .6744
Table 4: RTM-DCU test results on the SRE task.
Model r
P
RMSE MAE RAE
Par2S TREE 0.8013 0.8345 0.6277 0.5083
Par2S PLS-TREE 0.7737 0.8824 0.673 0.5449
Par2S SVR 0.7718 0.8863 0.6791 0.5499
S2Phrase TREE 0.6756 0.9887 0.7746 0.6665
S2Phrase PLS-TREE 0.6119 1.0616 0.8582 0.7384
S2Phrase SVR 0.6059 1.0662 0.8668 0.7458
Phrase2W TREE 0.201 1.3275 1.1353 0.9706
Phrase2W RR 0.1255 1.3463 1.1594 0.9912
Phrase2W SVR 0.0847 1.3548 1.1663 0.9972
Table 5: CLSS training results of the top 3 RTM
systems for each subtask. Levels correspond to
paragraph to sentence (Par2S), sentence to phrase
(S2Phrase), and phrase to word (Phrase2W).
491
3.2 Task 3: Cross-Level Semantic Similarity
CLSS contains sentence pairs from different gen-
res including text from newswire, travel, reviews,
metaphoric text, community question answering
sites, idiomatic text, descriptions, lexicographic
text, and search. Official evaluation metric in
CLSS is the sum of the Pearson?s correlation
scores for different levels
5
. CLSS task allows the
submission of 3 entries per subtask. We present
the performance of the top 3 individual RTM mod-
els on the training set in Table 5. RMSE is the root
mean squared error. As the compared text size de-
crease, the performance decrease since it can be-
come harder and more ambiguous to find the simi-
larity using less context. RTM-DCU results on the
CLSS challenge test set are provided in Table 6.
Model r
P
RMSE MAE RAE
Par2S TREE .8445 .7417 .5622 .4579
Par2S PLS-TREE .7847 .853 .6456 .5258
Par2S SVR .7858 .8428 .6539 .5325
S2Phrase TREE .75 .8827 .7053 .6255
S2Phrase PLS-TREE .6979 .9491 .7781 .69
S2Phrase SVR .6631 .9835 .7992 .7088
Phrase2W TREE .3053 1.3351 1.14 .9488
Phrase2W RR .2207 1.3644 1.1574 .9633
Phrase2W SVR .1712 1.3792 1.1792 .9815
Table 6: RTM-DCU test results on CLSS for the
top 3 RTM systems for each subtask.
Table 7 lists the results along with their ranks
for r
P
and r
S
, Spearman?s correlation, out of
CHECK submissions. The baseline in Table 7
is normalized longest common substring (LCS)
scaled in the range [0, 4]. Top individual rank row
lists the ranks in each subtask. We present the re-
sults for both our official and late (about 1 day)
submissions including word to sense (W2S) re-
sults
6
. RTM-DCU is able to obtain the top result
in Par2S in the CLSS task.
3.3 Task 10: Multilingual Semantic Textual
Similarity
MSTS contains sentence pairs from different do-
mains: sense definitions from semantic lexical re-
sources such as OnWN (from OntoNotes (Prad-
han et al., 2007) and WordNet (Miller, 1995)) and
FNWN (from FrameNet (Baker et al., 1998) and
WordNet), news headlines, image descriptions,
news title tweet comments, deft forum and news,
5
Giving advantage to participants submitting to all levels.
6
W2S results for the late submission is obtained from the
LCS baseline to calculate the ranks.
r
P
Par2S S2Phrase Phrase2W W2S Rank
LCS 0.527 0.562 0.165 0.109 25
Official
0.780 0.677 0.208 14
0.747 0.588 0.164 19
0.786 0.666 0.171 18
Late
0.845 0.750 0.305 0.109 6
0.785 0.698 0.221 0.109 13
0.786 0.663 0.171 0.109 17
Top Rank 1 5 3
r
S
Par2S S2Phrase Phrase2W W2S Rank
LCS 0.527 0.562 0.165 0.13 23
Official
0.780 0.677 0.208 17
0.747 0.588 0.164 22
0.786 0.666 0.171 18
Late
0.829 0.734 0.295 0.13 8
0.778 0.687 0.219 0.13 15
0.778 0.667 0.166 0.13 16
Top Rank 1 5 5
Table 7: RTM-DCU test results on CLSS.
paraphrases. Official evaluation metric in MSTS
is the Pearson?s correlation score.
MSTS task provides 7622 training instances
and 3750 test instances. For the OnWN domain,
1316 training instances are available and therefore,
we build a separate RTM model for this domain.
Separate modeling of the OnWN dataset results
with higher confidence scores on the test instances
than we would obtain using the overall model to
predict. MSTS task allows the submission of 3 en-
tries per subtask. We present the performance of
the top 3 individual RTM models on the training
set in Table 8.
Lang Model r
P
RMSE MAE RAE
E
n
g
l
i
s
h
TREE 0.6931 1.0627 0.8058 0.6649
PLS-TREE 0.6875 1.0753 0.8038 0.6632
PLS-SVR 0.6884 1.0698 0.8157 0.6730
O
n
W
N TREE 0.8094 0.9295 0.694 0.5245
PLS-TREE 0.7953 0.9604 0.7203 0.5444
PLS-SVR 0.7888 0.9779 0.7234 0.5468
S
p
a
n
i
s
h
TREE 0.6513 0.7341 0.5904 0.7508
PLS-TREE 0.4157 0.9007 0.7108 0.9039
PLS-SVR 0.4239 1.1427 0.8293 1.0545
Table 8: MSTS training results on the English, En-
glish OnWN, and Spanish tasks.
RTM results on the MSTS challenge test set are
provided in Table 9 along with the RTM results in
STS 2013 (Bic?ici and van Genabith, 2013a). Ta-
ble 10 and Table 11 lists the official results on En-
glish and Spanish tasks with rankings calculated
according to weighted r
P
, which weights accord-
ing to the number of instances in each domain.
RTM-DCU is able to become 10th in the OnWN
domain and 19th overall out of 38 submissions in
MSTS English and 18th out of 22 submissions in
492
Model r
P
RMSE MAE RAE
E
n
g
l
i
s
h
deft-forum
TREE .4341 1.4306 1.1609 1.0908
PLS-TREE .3965 1.4115 1.1472 1.078
PLS-SVR .3078 1.6277 1.3482 1.2669
deft-news
TREE .6974 1.1469 .9032 .8716
PLS-TREE .6811 1.1229 .8769 .8462
PLS-SVR .5562 1.2803 .9835 .9491
headlines
TREE .6199 1.1495 .9254 .7845
PLS-TREE .6125 1.1552 .9314 .7896
PLS-SVR .6301 1.1041 .8807 .7467
images
TREE .6995 1.2034 .9499 .7395
PLS-TREE .6656 1.2298 .9692 .7545
PLS-SVR .6474 1.4406 1.1057 .8607
OnWN
TREE .8058 1.3122 1.0028 .5585
PLS-TREE .7992 1.2997 .9815 .5467
PLS-SVR .8004 1.2913 .9449 .5263
tweet-news
TREE .6882 .9869 .831 .8093
PLS-TREE .6691 1.0101 .8433 .8213
PLS-SVR .5531 1.0633 .8653 .8427
S
p
a
n
i
s
h
News
TREE .7 1.5185 1.351 1.4141
PLS-TREE .6253 1.6523 1.4464 1.514
PLS-SVR .6411 1.554 1.3196 1.3813
Wikipedia
TREE .4216 1.5433 1.298 1.3579
PLS-TREE .3689 1.6655 1.4015 1.4662
PLS-SVR .4242 1.5998 1.3141 1.3748
S
T
S
2
0
1
3
E
n
g
l
i
s
h
headlines
L+S SVR .6552 1.5649 1.2763 1.0231
L+P+S SVR .651 1.4845 1.1984 .9607
L+P+S SVR TL .6385 1.4878 1.2008 .9626
OnWN
L+S SVR .6943 1.7065 1.3545 .8255
L+P+S SVR .6971 1.6737 1.333 .8124
L+P+S SVR TL .6755 1.7124 1.3598 .8287
SMT
L+S SVR .3005 .8833 .6886 1.6132
L+P+S SVR .2861 .8810 .6821 1.598
L+P+S SVR TL .3098 .8635 .6547 1.5339
FNWN
L+S SVR .2016 1.2957 1.0604 1.2633
L+P+S SVR .118 1.4369 1.1866 1.4136
L+P+S SVR TL .1823 1.3245 1.0962 1.3059
Table 9: RTM-DCU test results on MSTS for the
top 3 RTM systems for each subtask as well as
RTM results in STS 2013 (Bic?ici and van Gen-
abith, 2013a).
MSTS Spanish. The performance difference be-
tween MSTS English and MSTS Spanish may be
due to the fewer training data available for the
MSTS Spanish task, which may be decreasing the
performance of our supervised learning approach.
3.4 RTMs Across Tasks and Years
We compare the difficulty of tasks according to the
RAE levels achieved. RAE measures the error rel-
ative to the error when predicting the actual mean.
A high RAE is an indicator that the task is hard.
In Table 12, we list the RAE obtained for differ-
ent tasks and subtasks, also listing RTM results in
STS 2013 (Bic?ici and van Genabith, 2013a) and
RTM results (Bic?ici and Way, 2014) on the quality
estimation task (QET) (Bojar et al., 2014) where
post-editing effort (PEE), human-targeted transla-
Model Wikipedia News Weighted r
P
Rank
TREE 0.4216 0.7000 0.5878 18
PLS-TREE 0.3689 0.6253 0.5219 20
PLS-SVR 0.4242 0.6411 0.5537 19
Table 11: RTM-DCU test results on MSTS Span-
ish task. Rankings are calculated according to the
weighted Pearson?s correlation.
tion edit rate (HTER), or post-editing time (PET)
of translations are predicted.
The best results are obtained for the CLSS
Par2S subtask, which may be due to the larger
contextual information that paragraphs can pro-
vide for the RTM models. For the SRE task, we
can only reduce the error with respect to knowing
and predicting the mean by about 35%. Prediction
of bilingual similarity as in quality estimation of
translation can be expected to be harder and RTMs
achieve state-of-the-art performance in this task as
well (Bic?ici and Way, 2014).
4 Conclusion
Referential translation machines provide a clean
and intuitive computational model for automati-
cally measuring semantic similarity by measur-
ing the acts of translation involved and achieve to
be the top on some semantic similarity tasks at
SemEval-2014. RTMs make quality and seman-
tic similarity judgments possible based on the re-
trieval of relevant training data as interpretants for
reaching shared semantics.
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University, in part by SFI
(13/TIDA/I2740) for the project ?Monolin-
gual and Bilingual Text Quality Judgments
with Translation Performance Prediction?
(www.computing.dcu.ie/?ebicici/
Projects/TIDA RTM.html), and in part
by the European Commission through the QT-
LaunchPad FP7 project (No: 296347). We
also thank the SFI/HEA Irish Centre for High-
End Computing (ICHEC) for the provision of
computational facilities and support.
493
Model deft-forum deft-news headlines images OnWN tweet-news Weighted r
P
Rank
TREE .4341 .6974 .6199 .6995 .8058 .6882 .6706 20
PLS-TREE .3965 .6811 .6125 .6656 .7992 .6691 .6513 23
PLS-SVR .3078 .5562 .6301 .6475 .8004 .5531 .6076 27
Top Rank 17 16 25 26 16 13
W
i
t
h
C
o
n
f
.
TREE .4181 .6846 .6216 .6981 .8331 .6870 .6729 19
PLS-TREE .3831 .6739 .6094 .6629 .8260 .6691 .6534 23
PLS-SVR .2731 .5526 .6330 .6441 .8246 .5683 .6110 26
Top Rank 18 18 23 27 10 14
Table 10: RTM-DCU test results with ranks on MSTS English task.
Task Subtask Domain Model RAE
SRE English SICK
R PLS-SVR .6645
R+L PLS-SVR .6580
L SVR .6726
R+L SVR .6651
R PLS-SVR .6744
CLSS
Par2S
Mixed
TREE .4579
S2Phrase TREE .6255
Phrase2W TREE .9488
MSTS
English
deft-forum PLS-TREE 1.078
deft-news PLS-TREE .8462
headlines PLS-SVR .7467
images TREE .7395
OnWN PLS-SVR .5263
tweet-news TREE .8093
Spanish
News PLS-SVR 1.3813
Wikipedia TREE 1.3579
STS 2013 English
headlines L+P+S SVR .9607
OnWN L+P+S SVR .8124
SMT L+P+S SVR TL 1.5339
FNWN L+S SVR 1.2633
QET PEE
Spanish-English Europarl FS-RR .9000
Spanish-English Europarl PLS-RR .9409
English-German Europarl PLS-TREE .8883
English-German Europarl TREE .8602
English-Spanish Europarl TREE 1.0983
English-Spanish Europarl PLS-TREE 1.0794
German-English Europarl RR .8204
German-English Euruparl PLS-RR .8437
QET HTER
English-Spanish Europarl SVR .8532
English-Spanish Europarl TREE .8931
QET PET
English-Spanish Europarl SVR .7223
English-Spanish Europarl RR .7536
Table 12: Best RTM-DCU RAE test results for different tasks and subtasks as well as STS 2013 re-
sults (Bic?ici and van Genabith, 2013a) and results from quality estimation task of translation (Bojar et
al., 2014; Bic?ici and Way, 2014).
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilin-
gual semantic textual similarity. In Proc. of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proc. of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics - Volume 1, ACL
?98, pages 86?90, Stroudsburg, PA, USA.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics ? Volume 1: Proc. of the main conference and
the shared task, and Volume 2: Proc. of the Sixth In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012), pages 435?440, Montr?eal, Canada, 7-
8 June. Association for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013a. CNGL-
494
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013b. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The Second Joint Conference on Lex-
ical and Computational Semantics and Proc. of the
Seventh International Workshop on Semantic Eval-
uation (SemEval 2013), Atlanta, Georgia, USA, 14-
15 June. Association for Computational Linguistics.
Ergun Bic?ici and Andy Way. 2014. Referential trans-
lation machines for predicting translation quality. In
Proc. of the Ninth Workshop on Statistical Machine
Translation, Baltimore, USA, June. Association for
Computational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proc. of the Sixth Workshop on Sta-
tistical Machine Translation, pages 272?283, Ed-
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT sys-
tem for machine translation, system combination,
and evaluation. In Proc. of the Sixth Workshop on
Statistical Machine Translation, pages 323?329, Ed-
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation, 27:171?192, December.
Ergun Bic?ici, Qun Liu, and Andy Way. 2014. Paral-
lel FDA5 for fast deployment of accurate statistical
machine translation systems. In Proc. of the Ninth
Workshop on Statistical Machine Translation, Bal-
timore, USA, June. Association for Computational
Linguistics.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Referential translation machines
for quality estimation. In Proc. of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bj?ornsson. 1968. L?asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Ond?rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311, June.
Jos?e Guilherme Camargo de Souza, Christian Buck,
Marco Turchi, and Matteo Negri. 2013. FBK-
UEdin participation to the WMT13 quality estima-
tion shared task. In Proc. of the Eighth Workshop
on Statistical Machine Translation, pages 352?358,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proc. of the second interna-
tional conference on Human Language Technology
Research, pages 138?145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389?422.
Kenth Hagstr?om. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-level semantic similarity. In Proc. of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. In Proc. of the fifth international
conference on Language Resources and Evaluation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313?330, June.
495
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014a. SemEval-2014 Task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proc. of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland, August.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In
Proc. of LREC 2014, Reykjavik, Iceland.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Preslav Nakov and Torsten Zesch, editors. 2014.
Proc. of SemEval-2014 Semantic Evaluation Exer-
cises - International Workshop on Semantic Evalua-
tion. Dublin, Ireland, 23-24 August.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Sameer S. Pradhan, Eduard H. Hovy, Mitchell P.
Marcus, Martha Palmer, Lance A. Ramshaw, and
Ralph M. Weischedel. 2007. Ontonotes: a unified
relational semantic representation. Int. J. Semantic
Computing, 1(4):405?419.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Sch?olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
A. J. Smola, N. Murata, B. Sch?olkopf, and K.-R.
M?uller. 1998. Asymptotically optimal choice of ?-
loss for support vector machines. In L. Niklasson,
M. Boden, and T. Ziemke, editors, Proc. of the Inter-
national Conference on Artificial Neural Networks,
Perspectives in Neural Computing, pages 105?110,
Berlin. Springer.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proc. of the 13th Annual Conference of
the European Association for Machine Translation
(EAMT), pages 28?35, Barcelona, May. EAMT.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc. of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 173?180, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Wikipedia. 2013. LIX. http://en.wikipedia.org/
wiki/LIX.
David Graff Denise DiPersio
?
Angelo Mendonc?a,
Daniel Jaquette. 2011. Spanish Gigaword third edi-
tion, Linguistic Data Consortium.
496
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 276?281,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Adaptive Model Weighting and Transductive Regression for
Predicting Best System Combinations
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
S. Serdar Kozat
Koc? University
34450 Sariyer, Istanbul, Turkey
skozat@ku.edu.tr
Abstract
We analyze adaptive model weight-
ing techniques for reranking using in-
stance scores obtained by L1 regular-
ized transductive regression. Compet-
itive statistical machine translation is
an on-line learning technique for se-
quential translation tasks where we
try to select the best among com-
peting statistical machine translators.
The competitive predictor assigns a
probability per model weighted by
the sequential performance. We de-
fine additive, multiplicative, and loss-
based weight updates with exponential
loss functions for competitive statisti-
cal machine translation. Without any
pre-knowledge of the performance of
the translation models, we succeed in
achieving the performance of the best
model in all systems and surpass their
performance in most of the language
pairs we considered.
1 Introduction
When seen as independent instances, system
combination task can be solved with a sequen-
tial learning algorithm. Online learning algo-
rithms enable us to benefit from previous good
model choices to estimate the next best model.
We use transductive regression based machine
translation model to estimate the scores for
each sentence.
We analyze adaptive model weighting tech-
niques for system combination when the com-
peting translators are SMT models. We use
separate model weights weighted by the se-
quential performance. We use additive, mul-
tiplicative, or loss based weight updates to
update model weights. Without any pre-
knowledge of the performance of the transla-
tion models, we are able to achieve the per-
formance of the best model in all systems and
we can surpass its performance as well as the
regression based machine translation?s perfor-
mance.
The next section reviews the transductive
regression approach for machine translation,
which we use to obtain instance scores. In sec-
tion 3 we present competitive statistical ma-
chine translation model for solving sequential
translation tasks with competing translation
models. Section 4 presents our results and ex-
periments and the last section gives a sum-
mary of our contributions.
2 Transductive Regression Based
Machine Translation
Transduction uses test instances, which can
sometimes be accessible at training time, to
learn specific models tailored towards the test
set. Transduction has computational advan-
tages since we are not using the full train-
ing set and a smaller set of constraints exist
to satisfy. Transductive regression based ma-
chine translation (TRegMT) aims to reduce
the computational burden of the regression ap-
proach by reducing the dimensionality of the
training set and the feature set and also im-
prove the translation quality by using trans-
duction.
Regression Based Machine Translation:
Let n training instances be represented as
(x1,y1), . . . , (xn,yn) ? X
??Y ?, where (xi,yi)
corresponds to a pair of source and target lan-
guage token sequences. Our goal is to find
a mapping f : X? ? Y ? that can convert a
given set of source tokens to a set of target to-
kens that share the same meaning in the target
language.
276
We use feature mappers ?X : X? ?
FX = RNX and ?Y : Y ? ? FY =
RNY to represent the training set. Then,
MX ? RNX?n and MY ? RNY ?n such that
MX = [?X(x1), . . . ,?X(xn)] and MY =
[?Y (y1), . . . ,?Y (yn)]. The ridge regression
solution using L2 regularization is found as:
HL2 = arg min
H?RNY ?NX
?MY ?HMX ?
2
F +??H?
2
F . (1)
Two main challenges of the regression based
machine translation (RegMT) approach are
learning the regression function, g : X? ?
FY , and solving the pre-image problem, which,
given the features of the estimated target
string sequence, g(x) = ?Y (y?), attempts to
find y ? Y ?: f(x) = arg miny?Y ? ||g(x) ?
?Y (y)||2. Pre-image calculation involves a
search over possible translations minimizing
the cost function:
f(x) = arg min
y?Y ?
??Y (y)?H?X(x)?
2 . (2)
We use n-spectrum weighted word feature
mappers (Taylor and Cristianini, 2004) which
consider all word sequences up to order n.
L1 Regularized Regression for Learning:
HL2 is not a sparse solution as most of the co-
efficients remain non-zero. L1 norm behaves
both as a feature selection technique and a
method for reducing coefficient values.
HL1 = arg min
H?RNY ?NX
?MY ?HMX ?
2
F +??H?1 .(3)
Equation 3 presents the lasso (least absolute
shrinkage and selection operator) (Tibshirani,
1996) solution where the regularization term
is defined as ?H?1=
?
i,j |Hi,j |. We use for-
ward stagewise regression (FSR) (Hastie et
al., 2006) and quadratic programming (QP) to
find HL1 . The details of the TRegMT model
can be read in a separate submission to the
translation task (Bicici and Yuret, 2010).
3 Competitive Statistical Machine
Translation
We develop the Competitive Statistical Ma-
chine Translation (CSMT) framework for se-
quential translation tasks when the compet-
ing models are statistical machine translators.
CSMT uses the output of different translation
models to achieve a translation performance
that surpasses the translation performance of
all of the component models or achieves the
performance of the best.
CSMT uses online learning to update the
weights used for estimating the best perform-
ing translation model. Competitive predictor
assigns a weight per model estimated by their
sequential performance. At each step, m com-
ponent translation models are executed in par-
allel over the input source sentence sequence
and the loss lp[n] of model p at observation
n is calculated by comparing the desired data
y[n] with the output of model p, y?p[n]. CSMT
model selects a model based on the weights
and the performance of the selected model as
well as the remaining models to adaptively up-
date the weights given for each model. This
corresponds to learning in full information set-
ting where we have access to the loss for each
action (Blum and Mansour, 2007). CSMT
learning involves two main steps: estimation
and weight update:
y?c[n] = E(w[n],x[n]), (estimation)
lp[n] = y[n]? y?p[n], (instance loss)
Lp[n] =
?n
i=1 lp[i]
2, (cumulative loss)
w[n+ 1] = U(w[n], y?c[n],L[n]), (update)
(4)
where w[n] = (w1[n], . . . , wm[n]) for m mod-
els, Lp is the cumulative squared loss of model
p, L[n] stores cumulative and instance losses,
and y?c[n] is the competitive model estimated
for instance n. The learning problem is finding
an adaptive w that minimizes the cumulative
squared error with appropriate estimation and
update methods.
Related Work: Multistage adaptive filter-
ing (Kozat and Singer, 2002) combines the
output of multiple adaptive filters to outper-
form the best among them where the first
stage executes models in parallel and the sec-
ond stage updates parameters using the per-
formance of the combined prediction, y?c[n].
Macherey and Och (2007) investigate different
approaches for system combination including
candidate selection that maximize a weighted
combination of BLEU scores among different
system outputs. Their system uses a fixed
weight vector trained on the development set
277
to be multiplied with instance BLEU scores.
3.1 Estimating the Best Performing
Translation Model
We use additive, multiplicative, or loss based
updates to estimate model weights. We
measure instance loss with trLoss(y[i], y?p[i]),
which is a function that returns the transla-
tion performance of the output translation of
model p with respect to the reference transla-
tion at instance i. 1-BLEU (Papineni et al,
2001) is one such function with outputs in the
range [0, 1]. Cumulative squared loss of the
p-th translation model is defined as:
Lp[n] =
n?
i=1
trLoss(y[i], y?p[i])
2. (5)
We use exponentially re-weighted prediction to
estimate model performances, which uses ex-
ponentially re-weighted losses based on the
outputs of the m different translation models.
We define the additive exponential weight
update as follows:
wp[n+ 1] =
wp[n] + e?? lp[n]
m?
k=1
(
wk[n] + e
?? lk[n]
)
, (6)
where ? > 0 is the learning rate and the de-
nominator is used for normalization. The up-
date amount, e?? lp[n] is 1 when lp[n] = 0 and it
approaches zero with increasing instance loss.
Perceptrons, gradient descent, and Widrow-
Huff learning have additive weight updates.
We define the multiplicative exponential
weight update as follows:
wp[n+ 1] = wp[n]?
e?? lp[n]
2
m?
k=1
wk[n] e
?? lk[n]2
, (7)
where we use the squared instance loss. Equa-
tion 7 is similar to the update of Weighted Ma-
jority Algorithm (Littlestone and Warmuth,
1992) where the weights of the models that
make a mistake are multiplied by a fixed ?
such that 0 ? ? < 1.
We use Bayesian Information Criterion
(BIC) as a loss based re-weighting technique.
Assuming that instance losses are normally
distributed with variance ?2, BIC score is ob-
tained as (Hastie et al, 2009):
BICp[n] =
Lp[n]
?2
+ dp log(n), (8)
where ?2 is estimated by the average of model
sample variances of squared instance loss and
dp is the number of parameters used in model p
which we assume to be the same for all models;
therefore we can discard the second term. The
model with the minimum BIC value becomes
the one with the highest posterior probability
where the posterior probability of model p can
be estimated as (Hastie et al, 2009):
wp[n+ 1] =
e?
1
2BICp[n]
m?
k=1
e?
1
2BICk[n]
. (9)
The posterior probabilities become model
weights and we basically forget about the pre-
vious weights, whose information is presum-
ably contained in the cumulative loss, Lp. We
define multiplicative re-weighting with BIC
scores as follows:
wp[n+ 1] = wp[n]?
e?
1
2BICp
m?
k=1
wk[n] e
? 12BICk
. (10)
Model selection: We use stochastic or de-
terministic selection to choose the competitive
model for each instance. Deterministic choice
randomly selects among the maximum scor-
ing models with minimum translation length
whereas stochastic choice draws model p with
probability proportional to wp[n]. Random-
ization with the stochastic model selection
decreases expected mistake bounds in the
weighted majority algorithm (Littlestone and
Warmuth, 1992; Blum, 1996).
Auer et al (2002) show that optimal fixed
learning rate for the weighted majority algo-
rithm is found as ?[n] =
?
m/L?[n] where
L?[n] = min1?i?m Li[n], which requires prior
knowledge of the cumulative losses. We use
? =
?
m/(0.05n) for constant ?.
4 Experiments and Discussion
We perform experiments on the system com-
bination task for the English-German (en-
de), German-English (de-en), English-French
278
(en-fr), English-Spanish (en-es), and English-
Czech (en-cz ) language pairs using the trans-
lation outputs for all the competing systems
provided in WMT10. We experiment in a sim-
ulated online learning setting where only the
scores obtained from the TRegMT system are
used during both tuning and testing. We do
not use reference translations in measuring in-
stance performance in this simulated setting
for the results we obtain be in line with sys-
tem combination challenge?s goals.
4.1 Datasets
We use the training set provided in WMT10 to
index and select transductive instances from.
The challenge split the test set for the transla-
tion task of 2489 sentences into a tuning set of
455 sentences and a test set with the remain-
ing 2034 sentences. Translation outputs for
each system is given in a separate file and the
number of system outputs per translation pair
varies. We have tokenized and lowercased each
of the system outputs and combined these in
a single N -best file per language pair. We use
BLEU (Papineni et al, 2001) and NIST (Dod-
dington, 2002) evaluation metrics for measur-
ing the performance of translations automati-
cally.
4.2 Reranking Scores
The problem we are solving is online learn-
ing with prior information, which comes from
the comparative BLEU scores, LM scores, and
TRegMT scores at each step n. The scoring
functions are explained below:
1. TRegMT: Transductive regression based
machine translation scores as found by
Equation 2. We use the TRegMT scores
obtained by the FSR model.
2. CBLEU: Comparative BLEU scores we
obtain by measuring the average BLEU
performance of each translation relative
to the other systems? translations in the
N -best list.
3. LM: We calculate 5-gram language model
scores for each translation using the lan-
guage model trained over the target cor-
pus provided in the translation task.
To make things simpler, we use a single prior
TRegMT system score linearly combining the
three scores mentioned with weights learned
on the tuning set. The overall TRegMT sys-
tem score for instance n, model i is referred as
TRegScorei[n].
Since we do not have access to the refer-
ence translations nor to the translation model
scores each system obtained for each sentence,
we estimate translation model performance by
measuring the average BLEU performance of
each translation relative to other translations
in the N -best list. Thus, each possible transla-
tion in the N -best list is BLEU scored against
other translations and the average of these
scores is selected as the CBLEU score for the
sentence. Sentence level BLEU score calcula-
tion avoids singularities in n-gram precisions
by taking the maximum of the match count
and 12|si| for |si| denoting the length of the
source sentence si as used in (Macherey and
Och, 2007).
4.3 Adaptive Model Weighting
We initialize model weights to 1/m for all
models, which are updated after each instance
according to the losses based on the TRegMT
model. Table 1 presents the performance
of the algorithms on the en-de development
set. We have measured their performances
with stochastic (stoc.) or deterministic (det.)
model selection when using only the weights or
mixture weights obtained when instance scores
are also considered. Mixture weights are ob-
tained as: wi[n] = wi[n] TRegScorei[n], for
instance n, model i.
Baseline performance obtained with random
selection has .1407 BLEU and 4.9832 NIST
scores. TRegMT model obtains a performance
of .1661 BLEU and 5.3283 NIST with rerank-
ing. The best model performance among the
12 en-de translation models has .1644 BLEU
and 5.2647 NIST scores. Therefore, by using
TRegMT score, we are able to achieve better
scores.
Not all of the settings are meaningful. For
instance, stochastic model selection is used for
algorithms having multiplicative weight up-
dates. This is reflected in the Table 1 by low
performance on the additive and BIC models.
Similarly, using mixture weights may not re-
sult in better scores for algorithms with multi-
plicative updates, which resulted in decreased
279
Additive Multiplicative BIC BIC Weighting
Setting BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Stoc., W .1419 5.0016 ?.003 .1528 5.1710 ?.001 .1442 5.0468 .1568 ?.001 5.2052 ?.005
Stoc., M .1415 5.0001 .1525 5.1601 ?.001 .1459 5.0619 ?.004 .1566 ?.001 5.2030 ?.006
Det., W .1644 5.3208 .1638 5.2571 .1638 5.2542 .1646 5.2535
Det., M .1643 5.3173 .1536 5.1756 .1530 5.1871 .1507 5.1973
Table 1: Performances of the algorithms on the development set over 100 repetitions. W:
Weights, M: Mixture.
performance in Table 1. Decreased perfor-
mance with BIC hints that we may use other
techniques for mixture weights.
Table 2 presents reranking results on all of
the language pairs we considered with the ran-
dom, TRegMT, and CSMT models. Random
model score lists the random model perfor-
mance selected among the competing trans-
lations randomly and it can be used as a
baseline. Best model score lists the perfor-
mance of the best model performance. CSMT
models are named with the weighting model
used (Add for additive, Mul for multiplicative,
BICW for BIC weighting), model selection
technique (S for stochastic, D for determinis-
tic), and mixtures model (W for using only
weights, M for using mixture weights) with
hyphens in between. Our challenge submis-
sion is given in the last row of Table 2 where
we used multiplicative exponential weight up-
dates, deterministic model selection, and only
the weights during model selection. For the
challenge results, we initialized the weights to
the weights obtained in the development set.
We have presented scores that are better
than or close to the best model in bold. We
observe that the additive model performs the
best by achieving the performance of the best
competing translation model and performing
better than the best in most of the language
pairs. For the en-de language pair, addi-
tive model score achieves even better than the
TRegMT model, which is used for evaluating
instance scores.
5 Contributions
We have analyzed adaptive model weighting
techniques for system combination when the
competing translators are statistical machine
translation models. We defined additive, mul-
tiplicative, and loss-based weight updates with
exponential loss functions for the competitive
statistical machine translation framework.
Competitive SMT via adaptive weighting of
various translators is shown to be a powerful
technique for sequential translation tasks. We
have demonstrated its use in the system com-
bination task by using the instance scores ob-
tained by the TRegMT model. Without any
pre-knowledge of the performance of the trans-
lation models, we have been able to achieve the
performance of the best model in all systems
and we are able to surpass its performance as
well as TRegMT?s performance with the addi-
tive model.
Acknowledgments
The research reported here was supported in
part by the Scientific and Technological Re-
search Council of Turkey (TUBITAK). The
first author would like to thank Deniz Yuret
for helpful discussions and for guidance and
support during the term of this research.
References
Auer, Cesa-Bianchi, and Gentile. 2002. Adaptive
and self-confident on-line learning algorithms.
JCSS: Journal of Computer and System Sci-
ences, 64.
Ergun Bicici and Deniz Yuret. 2010. L1 regular-
ized regression for reranking and system combi-
nation in machine translation. In Proceedings of
the ACL 2010 Joint Fifth Workshop on Statis-
tical Machine Translation and Metrics MATR,
Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Avrim Blum and Yishay Mansour. 2007. Learn-
ing, regret minimization and equilibria. In
Noam Nisan, Tim Roughgarden, Eva Tar-
dos, and Vijay V. Vazirani, editors, Algorith-
mic Game Theory (Cambridge University Press,
2007).
Avrim Blum. 1996. On-line algorithms in machine
learning. In In Proceedings of the Workshop on
On-Line Algorithms, Dagstuhl, pages 306?325.
Springer.
280
en-de de-en en-fr en-es en-cz
Model BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Random .1490 5.6555 .2088 6.4886 .2415 6.8948 .2648 7.2563 .1283 4.9238
Best model .1658 5.9610 .2408 6.9861 .2864 7.5272 .3047 7.7559 .1576 5.4480
TRegMT .1689 5.9638 .2357 6.9254 .2947 7.7107 .3049 7.8156 .1657 5.5632
Add-D-W .1697 5.9821 .2354 6.9175 .2948 7.7094 .3043 7.8093 .1642 5.5463
Add-D-M .1698 5.9824 .2353 6.9152 .2949 7.7103 .3044 7.8091 .1642 5.5461
Mul-S-W .1574 5.7564 .2161 6.5950 .2805 7.4599 .2961 .7.6870 .1572 5.4394
Mul-D-W .1618 5.8912 .2408 6.9854 .2847 7.5085 .2785 7.4133 .1612 5.5119
BIC-D-W .1614 5.8852 .2408 6.9853 .2842 7.5022 .2785 7.4132 .1623 5.5236
BIC-D-M .1580 5.7614 .2141 6.5597 .2791 7.4309 .2876 7.5138 .1577 5.4488
BICW-S-W .1621 5.8795 .2274 6.8142 .2802 7.4873 .2892 7.5569 .1565 5.4126
BICW-S-M .1618 5.8730 .2196 6.6493 .2806 7.4948 .2849 7.4845 .1561 5.4099
BICW-D-W .1648 5.9298 .2355 6.9112 .2807 7.4648 .2785 7.4134 .1534 5.3458
Challenge .1567 5.73 .2394 6.9627 .2758 7.4333 .3047 7.7559 .1641 5.5435
Table 2: CSMT results where bold corresponds to scores better than or close to the best model.
Underlined scores are better than both the TregMT model and the best model.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Human Language Tech-
nology Research, pages 138?145.
Trevor Hastie, Jonathan Taylor, Robert Tibshi-
rani, and Guenther Walther. 2006. Forward
stagewise regression and the monotone lasso.
Electronic Journal of Statistics, 1.
Trevor Hastie, Robert Tibshirani, and Jerome
Friedman. 2009. The Elements of Statistical
Learning: Data Mining, Inference and Predic-
tion. Springer-Verlag, 2nd edition.
S.S. Kozat and A.C. Singer. 2002. Further re-
sults in multistage adaptive filtering. ICASSP,
2:1329?1332.
Nick Littlestone and Manfred K. Warmuth. 1992.
The Weighted Majority Algorithm. Technical
Report UCSC-CRL-91-28, University of Califor-
nia, Santa Cruz, Jack Baskin School of Engi-
neering, October 26,.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems.
In EMNLP-CoNLL, pages 986?995.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for au-
tomatic evaluation of machine translation. In
ACL, pages 311?318. ACL.
J. Shawe Taylor and N. Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge
University Press.
Robert J. Tibshirani. 1996. Regression shrinkage
and selection via the lasso. Journal of the Royal
Statistical Society, Series B, 58(1):267?288.
281
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 282?289,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
L1 Regularized Regression for Reranking and System Combination in
Machine Translation
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Deniz Yuret
Koc? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
We use L1 regularized transductive regres-
sion to learn mappings between source
and target features of the training sets
derived for each test sentence and use
these mappings to rerank translation out-
puts. We compare the effectiveness of L1
regularization techniques for regression to
learn mappings between features given in
a sparse feature matrix. The results show
the effectiveness of using L1 regulariza-
tion versus L2 used in ridge regression.
We show that regression mapping is ef-
fective in reranking translation outputs and
in selecting the best system combinations
with encouraging results on different lan-
guage pairs.
1 Introduction
Regression can be used to find mappings be-
tween the source and target feature sets derived
from given parallel corpora. Transduction learn-
ing uses a subset of the training examples that
are closely related to the test set without using
the model induced by the full training set. In
the context of SMT, we select a few training in-
stances for each test instance to guide the transla-
tion process. This also gives us a computational
advantage when considering the high dimension-
ality of the problem. The goal in transductive
regression based machine translation (TRegMT)
is both reducing the computational burden of the
regression approach by reducing the dimension-
ality of the training set and the feature set and
also improving the translation quality by using
transduction. Transductive regression is shown to
achieve higher accuracy than L2 regularized ridge
regression on some machine learning benchmark
datasets (Chapelle et al, 1999).
In an idealized feature mapping matrix where
features are word sequences, we would like to ob-
serve few target features for each source feature
derived from a source sentence. In this setting, we
can think of feature mappings being close to per-
mutation matrices with one nonzero item for each
column. L1 regularization helps us achieve solu-
tions close to the permutation matrices by increas-
ing sparsity.
We show that L1 regularized regression map-
ping is effective in reranking translation outputs
and present encouraging results on different lan-
guage pairs in the translation task of WMT10. In
the system combination task, different translation
outputs of different translation systems are com-
bined to find a better translation. We model system
combination task as a reranking problem among
the competing translation models and present en-
couraging results with the TRegMT system.
Related Work: Regression techniques can
be used to model the relationship between
strings (Cortes et al, 2007). Wang et al (2007)
applies a string-to-string mapping approach
to machine translation by using ordinary least
squares regression and n-gram string kernels to
a small dataset. Later they use L2 regularized
least squares regression (Wang and Shawe-Taylor,
2008). Although the translation quality they
achieve is not better than Moses (Koehn et al,
2007), which is accepted to be the state-of-the-art,
they show the feasibility of the approach. Ser-
rano et al (2009) use kernel regression to find
translation mappings from source to target feature
vectors and experiment with translating hotel
front desk requests. Ueffing (2007) approaches
the transductive learning problem for SMT by
bootstrapping the training using the translations
produced by the SMT system that have a scoring
performance above some threshold as estimated
by the SMT system itself.
282
Outline: Section 2 gives an overview of regres-
sion based machine translation, which is used to
find the mappings between the source and target
features of the training set. In section 3 we present
L1 regularized transductive regression for align-
ment learning. Section 4 presents our experiments,
instance selection techniques, and results on the
translation task for WMT10. In section 5, we
present the results on the system combination task
using reranking. The last section concludes.
2 An Overview of Regression Based
Machine Translation
Let X and Y correspond to the token sets used to
represent source and target strings, then a train-
ing sample of m inputs can be represented as
(x1, y1), . . . , (xm, ym) ? X
? ? Y ?, where (xi, yi)
corresponds to a pair of source and target language
token sequences. Our goal is to find a mapping
f : X? ? Y ? that can convert a given set of
source tokens to a set of target tokens that share
the same meaning in the target language.
X? Y ?-
? R ?
-FX FY
g
?X ?Y
6
??1Y
f
h
Figure 1: String-to-string mapping.
Figure 1 depicts the mappings between different
representations. ?X : X? ? FX = RNX and
?Y : Y ? ? FY = RNY map each string sequence
to a point in high dimensional real number space
where dim(FX) = NX and dim(FY ) = NY .
Let MX ? RNX?m and MY ? RNY ?m such
that MX = [?X(x1), . . . ,?X(xm)] and MY =
[?Y (y1), . . . ,?Y (ym)]. The ridge regression so-
lution using L2 regularization is found as:
HL2 = arg min
H?RNY ?NX
?MY ?HMX ?2F +??H?
2
F .(1)
Proposition 1 Solution to the cost function given
in Equation 1 is found by the following identities:
H = MY MTX(MXM
T
X + ?INX )
?1 (primal)
H = MY (KX + ?Im)?1MTX (dual)
(2)
where KX = MTXMX is the Gram matrix with
KX(i, j) = kX(xi, xj) and kX(xi, xj) is the ker-
nel function defined as kX(xi, xj) = ?(xi)T?(xj).
The primal solution involves the inversion of the
covariance matrix in the feature space (O(N3X))
and the dual solution involves the inversion of the
kernel matrix in the instance space (O(m3)) and
L2 regularization term prevents the normal equa-
tions to be singular. We use the dual solution when
computing HL2 .
Two main challenges of the RegMT approach
are learning the regression function, g : X? ?
FY , and solving the pre-image problem, which,
given the features of the estimated target string se-
quence, g(x) = ?Y (y?), attempts to find y ? Y ?:
f(x) = arg miny?Y ? ||g(x)??Y (y)||
2. Pre-image
calculation involves a search over possible transla-
tions minimizing the cost function:
f(x) = arg min
y?Y ?
??Y (y)?H?X(x)?
2
= arg min
y?Y ?
kY (y, y)? 2(K
y
Y )
T (KX + ?Im)
?1KxX ,(3)
where KyY =[kY (y, y1), . . . , kY (y, ym)]
T ? Rm?1
and KxX ? R
m?1 is defined similarly.
We use n-spectrum weighted word ker-
nel (Shawe-Taylor and Cristianini, 2004) as fea-
ture mappers which consider all word sequences
up to order n:
k(x, x?)=
nX
p=1
|x|?p+1X
i=1
|x?|?p+1X
j=1
p I(x[i : i+p?1]=x?[j :j+p?1])
(4)
where x[i : j] denotes a substring of x with the
words in the range [i, j], I(.) is the indicator func-
tion, and p is the number of words in the feature.
3 L1 Regularized Regression
In statistical machine translation, parallel cor-
pora, which contain translations of the same doc-
uments in source and target languages, are used
to estimate a likely target translation for a given
source sentence based on the observed transla-
tions. String kernels lead to very sparse represen-
tations of the feature space and we examine the ef-
fectiveness of L1 regularized regression to find the
mappings between sparsely observed feature sets.
3.1 Sparsity in Translation Mappings
We would like to observe only a few nonzero tar-
get feature coefficients corresponding to a source
feature in the coefficient matrix. An example solu-
tion matrix representing a possible alignment be-
tween unigram source and target features could be
the following:
283
H e1 e2 e3
f1 1 1
f2 1
f3 1
Here ei represents unigram source features and fi
represent unigram target features. e1 and e3 have
unambiguous translations whereas e2 is ambigu-
ous. Even if unigram features lead to ambiguity,
we expect higher order features like bigrams and
trigrams to help us resolve the ambiguity. Typical
H matrices have thousands of features. L1 regu-
larization helps us achieve solutions close to per-
mutation matrices by increasing sparsity (Bishop,
2006). In contrast, L2 solutions give us dense ma-
trices.
3.2 L1 Regularized Regression for Learning
HL2 does not give us a sparse solution and most
of the coefficients remain non-zero. L1 norm be-
haves both as a feature selection technique and a
method for reducing coefficient values.
HL1 = arg min
H?RNY ?NX
?MY ?HMX ?2F +??H?1 .(5)
Equation 5 presents the lasso (least absolute
shrinkage and selection operator) (Tibshirani,
1996) solution where the regularization term is
now the L1 matrix norm defined as ? H ?1=?
i,j |Hi,j |. Since L1 regularization cost is not
differentiable, HL1 is found by optimization or ap-
proximation techniques. We briefly describe three
techniques to obtain L1 regularized regression co-
efficients.
Forward Stagewise Regression (FSR): We
experiment with forward stagewise regression
(FSR) (Hastie et al, 2006), which approximates
the lasso. The incremental forward stagewise re-
gression algorithm increases the weight of the pre-
dictor variable that is most correlated with the
residual by a small amount, , multiplied with
the sign of the correlation at each step. As
 ? 0, the profile of the coefficients resemble the
lasso (Hastie et al, 2009).
Quadratic Programming (QP): We also use
quadratic programming (QP) to find HL1 . We can
pose lasso as a QP problem as follows (M?rup
and Clemmensen, 2007). We assume that the
rows of MY are independent and solve for each
row i, Myi ? R
1?m, using non-negative variables
h+i ,h
?
i ? R
NX?1 such that hi = h+i ? h
?
i :
hi = arg min
h
?Myi ? hMX?
2
F +?
NXX
k=1
|hk|, (6)
hi = arg min
h?i
1
2
h?igMXgMX
T
h?i
T
? h?i(gMXM
T
yi ? ?1), (7)
s.t. h?i > 0, gMX =
?
MX
?MX
?
, h?i =
?
h+i h
?
i
?
.
Linear Programming (LP): L1 minimization
can also be posed as a linear programming (LP)
problem by interpreting the error term as the con-
straint (Chen et al, 1998) and solving for each row
i:
hi = arg min
h
?h?1 subject to Myi = hMX , (8)
which can again be solved using non-negative
variables. This is a slightly different optimization
and the results can be different but linear program-
ming solvers offer computational advantages.
3.3 Transductive Regression
Transduction uses test instances, which can some-
times be accessible at training time, to learn spe-
cific models tailored towards the test set. Trans-
duction has computational advantages by not us-
ing the full training set and by having to satisfy a
smaller set of constraints. For each test sentence,
we pick a limited number of training instances de-
signed to improve the coverage of correct features
to build a regression model. Section 4.2 details our
instance selection methods.
4 Translation Experiments
We perform experiments on the translation task
of the English-German, German-English, English-
French, English-Spanish, and English-Czech lan-
guage pairs using the training corpus provided in
WMT10.
4.1 Datasets and Baseline
We developed separate SMT models using
Moses (Koehn et al, 2007) with default settings
with maximum sentence length set to 80 using 5-
gram language model and obtained distinct 100-
best lists for the test sets. All systems were tuned
with 2051 sentences and tested with 2525 sen-
tences. We have randomly picked 100 instances
from the development set to be used in tuning the
regression experiments (dev.100). The translation
challenge test set contains 2489 sentences. Num-
ber of sentences in the training set of each system
284
and baseline performances for uncased output (test
set BLEU, challenge test set BLEU) are given in
Table 1.
Corpus # sent BLEU BLEU Challenge
en-de 1609988 .1471 .1309
de-en 1609988 .1943 .1556
en-fr 1728965 .2281 .2049
en-es 1715158 .2237 .2106
en-cz 7320238 .1452 .1145
Table 1: Initial uncased performances of the trans-
lation systems.
Feature mappers used are 3-spectrum counting
word kernels, which consider all N -grams up to
order 3 weighted by the number of tokens in the
feature. We segment sentences using some of the
punctuation for managing the feature set better and
do not consider N -grams that cross segments.
We use BLEU (Papineni et al, 2001) and
NIST (Doddington, 2002) evaluation metrics for
measuring the performance of translations auto-
matically.
4.2 Instance Selection
Proper selection of training instances plays an im-
portant role to learn feature mappings with limited
computational resources accurately. In previous
work (Wang and Shawe-Taylor, 2008), sentence
based training instances were selected using tf-idf
retrieval. We transform test sentences to feature
sets obtained by the kernel mapping before mea-
suring their similarities and index the sentences
based on the features. Given a source sentence
of length 20, its feature representation would have
a total of 57 uni/bi/tri-gram features. If we select
closest sentences from the training set, we may not
have translations for all the features in this repre-
sentation. But if we search for translations of each
feature, then we have a higher chance of covering
all the features found in the sentence we are try-
ing to translate. The index acts as a dictionary of
source phrases storing training set entries whose
source sentence match the given source phrase.
The number of instances per feature is chosen
inversely proportional to the frequency of the fea-
ture determined by the following formula:
#instance(f) = n/ ln(1 + idfScore(f)/9.0), (9)
where idfScore(f) sums the idf (inverse document
frequency) of the tokens in feature f and n is a
small number.
4.3 Addition of Brevity Penalty
Detailed analysis of the results shows TRegMT
score achieves better N -gram match percentages
than Moses translation but suffers from the brevity
penalty due to selecting shorter translations. Due
to using a cost function that minimizes the squared
loss, TRegMT score tends to select shorter trans-
lations when the coverage is low. We also observe
that we are able to achieve higher scores for NIST,
which suggests the addition of a brevity penalty to
the score.
Precision based BLEU scoring divides N -gram
match counts toN -gram counts found in the trans-
lation and this gives an advantage to shorter trans-
lations. Therefore, a brevity penalty (BP) is added
to penalize short translations:
BP = min(1?
ref-length
trans-length
, 0) (10)
BLEU = e(log(ngramprec)+BP) (11)
where ngramprec represent the sum of n-gram
precisions. Moses rarely incurs BP as it has a word
penalty parameter optimized against BLEU which
penalizes translations that are too long or too short.
For instance, Moses 1-best translation for en-de
system achieves .1309 BLEU versus .1320 BLEU
without BP.
We handle short translations in two ways. We
optimize the ? parameter of QP, which manages
the sparsity of the solution (larger ? values cor-
respond to sparser solutions) against BLEU score
rather than the squared loss. Optimization yields
? = 20.744. We alternatively add a BP cost to the
squared loss:
BP = e
?
min(1?
|?Y (y)|
|pH?X (x)+?BP q|
,0)
?
(12)
f(x) = arg min
y?Y ?
??Y (y)?H?X(x)?
2 +?BPBP (13)
where |.| denotes the length of the feature vector,
p.q rounds feature weights to integers, ?BP is a
constant weight added to the estimation, and ?BP
is the weight given for the BP cost. |pH?X(x) +
?BP q| represents an estimate of the length of the
reference as found by the TRegMT system. This
BP cost estimate is similar to the cost used in (Ser-
rano et al, 2009) normalized by the length of the
reference. We found ?BP = 0.1316 and ?BP =
?13.68 when optimized on the en-de system. We
add a BP penalty to all of the reranking results
given in the next section and QP results also use
optimized ?.
285
en-de de-en en-fr en-es en-cz
Score BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Baseline .1309 5.1417 .1556 5.4164 .2049 6.3194 .2106 6.3611 .1145 4.5008
Oracle .1811 6.0252 .2101 6.2103 .2683 7.2409 .2770 7.3190 .1628 5.4501
L2 .1319 5.1680 .1555 5.4344 .2044 6.3370 .2132 6.4093 .1148 4.5187
FSR .1317* 5.1639 .1559 5.4383 .2053 6.3458 .2144 6.4168 .1150 4.5172
LP .1317 5.1695 .1561 5.4304 .2048 6.3245 .2109 6.4176 .1124 4.5143
QP .1309 5.1664 .1550 5.4553 .2033 6.3354* .2121 6.4271 .1150 4.5264
Table 2: Reranking results using TRegMT, TM, and LM scores. We use approximate randomization
test (Riezler and Maxwell, 2005) with 1000 repetitions to determine score difference significance: results
in bold are significant with p ? 0.01 and italic results with (*) are significant with p ? .05. The
difference of the remaining from the baseline are not statistically significant.
4.4 Reranking Experiments
We rerank N -best lists by using linear combina-
tions of the following scoring functions:
1. TRegMT: Transductive regression based ma-
chine translation scores as found by Equa-
tion 3.
2. TM: Translation model scores we obtain
from the baseline SMT system that is used
to generate the N -best lists.
3. LM: 5-gram language model scores that the
baseline SMT system uses when calculating
the translation model scores.
The training set we obtain may not contain all
of the features of the reference target due to low
coverage. Therefore, when performing reranking,
we also add the cost coming from the features of
?Y (y) that are not represented in the training set
to the squared loss as in:
??Y (y) \ FY ?2 + ??Y (y)?H?X(x)?2, (14)
where ?Y (y) \ FY represent the features of y not
represented in the training set.
We note that TRegMT score only contains or-
dering information as present in the bi/tri-gram
features in the training set. Therefore, the ad-
dition of a 5-gram LM score as well as the TM
score, which also incorporates the LM score in
itself, improves the performance. We are not
able to improve the BLEU score when we use
TRegMT score by itself however we are able to
achieve improvements in the NIST and 1-WER
scores. The performance increase is important for
two reasons. First of all, we are able to improve
the performance using blended spectrum 3-gram
features against translations obtained with 5-gram
language model and higher order features. Out-
performing higher order n-gram models is known
to be a difficult task (Galley and Manning, 2009).
Secondly, increasing the performance with rerank-
ing itself is a hard task since possible translations
are already constrained by the ones observed inN -
best lists. Therefore, an increase in the N -best list
size may increase the score gaps.
Table 2 presents reranking results on all of the
language pairs we considered, using TRegMT,
TM, and LM scores with the combination weights
learned in the development set. We are able to
achieve better BLEU and NIST scores on all of the
listed systems. We are able to see up to .38 BLEU
points increase for the en-es pair. Oracle reranking
performances are obtained by using BLEU scoring
metric.
If we used only the TM and LM scores when
reranking with the en-de system, then we would
obtain .1309 BLEU and 5.1472 NIST scores. We
only see a minor increase in the NIST score and no
change in the BLEU score with this setting when
compared with the baseline given in Table 2.
Due to computational reasons, we do not use
the same number of instances to train different
models. In our experiments, we used n = 3 for
L2, n = 1.5 for FSR, and n = 1.2 for QP and
LP solutions to select the number of instances in
Equation 9. The average number of instances used
per sentence in training corresponding to these
choices are approximately 140, 74, and 61. Even
with these decreased number of training instances,
L1 regularized regression techniques are able to
achieve comparable scores to L2 regularized re-
gression model in Table 2.
5 System Combination Experiments
We perform experiments on the system com-
bination task for the English-German, German-
English, English-French, English-Spanish, and
English-Czech language pairs using the training
286
en-de de-en en-fr en-es en-cz
Score BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST
Random .1490 5.6555 .2088 6.4886 .2415 6.8948 .2648 7.2563 .1283 4.9238
Best model .1658 5.9610 .2408 6.9861 .2864 7.5272 .3047 7.7559 .1576 5.4480
L2 .1694 5.9974 .2336 6.9398 .2948 7.7037 .3036 7.8120 .1657 5.5654
FSR .1689 5.9638 .2357 6.9254 .2947 7.7107 .3049 7.8156 .1657 5.5632
LP .1694 5.9954 .2368 6.8850 .2928 7.7157 .3027 7.7838 .1659 5.5680
QP .1692 5.9983 .2368 6.9172 .2913 7.6949 .3040 7.8086 .1662 5.5785
Table 3: Reranking results using TRegMT, TM, and LM scores. bold correspond to the best score in
each rectangle of scores.
corpus provided in WMT10.
5.1 Datasets
We use the training set provided in WMT10 to in-
dex and select transductive instances from. The
challenge split the test set for the translation task
of 2489 sentences into a tuning set of 455 sen-
tences and a test set with the remaining 2034 sen-
tences. Translation outputs for each system is
given in a separate file and the number of sys-
tem outputs per translation pair varies. We have
tokenized and lowercased each of the system out-
puts and combined these in a singleN -best file per
language pair. We also segment sentences using
some of the punctuation for managing the feature
set better. We use these N -best lists for TRegMT
reranking to select the best translation model. Fea-
ture mappers used are 3-spectrum counting word
kernels, which consider all n-grams up to order 3
weighted by the number of tokens in the feature.
5.2 Experiments
We rerank N -best lists by using combinations of
the following scoring functions:
1. TRegMT: Transductive regression based ma-
chine translation scores as found by Equa-
tion 3.
2. TM?: Translation model scores are obtained
by measuring the average BLEU perfor-
mance of each translation relative to the other
translations in the N -best list.
3. LM: We calculate 5-gram language model
scores for each translation using the language
model trained over the target corpus provided
in the translation task.
Since we do not have access to the reference
translations nor to the translation model scores
each system obtained for each sentence, we es-
timate translation model performance (TM?) by
measuring the average BLEU performance of each
translation relative to the other translations in the
N -best list. Thus, each possible translation in the
N -best list is BLEU scored against other transla-
tions and the average of these scores is selected
as the TM score for the sentence. Sentence level
BLEU score calculation avoids singularities in n-
gram precisions by taking the maximum of the
match count and 12|si| for |si| denoting the length
of the source sentence si as used in (Macherey and
Och, 2007).
Table 3 presents reranking results on all of the
language pairs we considered, using TRegMT,
TM, and LM scores with the same combination
weights as above. Random model score lists the
random model performance selected among the
competing translations randomly and it is used as
a baseline. Best model score lists the performance
of the best model performance. We are able to
achieve better BLEU and NIST scores in all of the
listed systems except for the de-en language pair
when compared with the performance of the best
competing translation system. The lower perfor-
mance in the de-en language pair may be due to
having a single best translation system that outper-
forms others significantly. The difference between
the best model performance and the mean as well
as the variance of the scores in the de-en language
pair is about twice their counterparts in en-de lan-
guage pair.
Due to computational reasons, we do not use
the same number of instances to train different
models. In our experiments, we used n = 4 for
L2, n = 1.5 for FSR, and n = 1.2 for QP and
LP solutions to select the number of instances in
Equation 9. The average number of instances used
per sentence in training corresponding to these
choices are approximately 189, 78, and 64.
287
6 Contributions
We use transductive regression to learn mappings
between source and target features of given paral-
lel corpora and use these mappings to rerank trans-
lation outputs. We compare the effectiveness ofL1
regularization techniques for regression. TRegMT
score has a tendency to select shorter transla-
tions when the coverage is low. We incorporate a
brevity penalty to the squared loss and optimize ?
parameter of QP to tackle this problem and further
improve the performance of the system.
The results show the effectiveness of using L1
regularization versus L2 used in ridge regression.
Proper selection of training instances plays an im-
portant role to learn correct feature mappings with
limited computational resources accurately. We
plan to investigate better instance selection meth-
ods for improving the translation performance.
TRegMT score has a tendency to select shorter
translations when the coverage is low. We incor-
porate a brevity penalty to the score and optimize
the ? parameter of QP to tackle this problem.
Acknowledgments
The research reported here was supported in
part by the Scientific and Technological Research
Council of Turkey (TUBITAK).
References
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer.
Olivier Chapelle, Vladimir Vapnik, and Jason Weston.
1999. Transductive inference for estimating values
of functions. In NIPS, pages 421?427.
Scott Shaobing Chen, David L. Donoho, and
Michael A. Saunders. 1998. Atomic decomposition
by basis pursuit. SIAM Journal on Scientific Com-
puting, 20(1):33?61.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A general regression framework for learn-
ing string-to-string mappings. In Gokhan H. Bakir,
Thomas Hofmann, and Bernhard Sch editors, Pre-
dicting Structured Data, pages 143?168. The MIT
Press, September.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 773?781,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Trevor Hastie, Jonathan Taylor, Robert Tibshirani, and
Guenther Walther. 2006. Forward stagewise regres-
sion and the monotone lasso. Electronic Journal of
Statistics, 1.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning:
Data Mining, Inference and Prediction. Springer-
Verlag, 2nd edition.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Assoc. for Compu-
tational Linguistics, pages 177?180, Prague, Czech
Republic, June.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems. In
EMNLP-CoNLL, pages 986?995.
M. M?rup and L. H. Clemmensen. 2007. Multiplica-
tive updates for the lasso. In Machine Learning for
Signal Processing MLSP, IEEE Workshop on, pages
33 ?38, Aug.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on
Pattern Recognition and Image Analysis, pages 394?
401.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
288
Robert J. Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of the Royal Statisti-
cal Society, Series B, 58(1):267?288.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 25?32, Prague, Czech Republic,
June. The Association for Computer Linguistics.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared transla-
tion task. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 155?158,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine trans-
lation. In Human Language Technologies 2007:
The Conference of the North American Chapter
of the Association for Computational Linguistics;
Companion Volume, Short Papers, pages 185?188,
Rochester, New York, April. Association for Com-
putational Linguistics.
289
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 272?283,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Instance Selection for Machine Translation using Feature Decay
Algorithms
Ergun Bi?ici
Ko? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Deniz Yuret
Ko? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
We present an empirical study of instance se-
lection techniques for machine translation. In
an active learning setting, instance selection
minimizes the human effort by identifying
the most informative sentences for transla-
tion. In a transductive learning setting, se-
lection of training instances relevant to the
test set improves the final translation qual-
ity. After reviewing the state of the art in
the field, we generalize the main ideas in a
class of instance selection algorithms that use
feature decay. Feature decay algorithms in-
crease diversity of the training set by devalu-
ing features that are already included. We
show that the feature decay rate has a very
strong effect on the final translation quality
whereas the initial feature values, inclusion
of higher order features, or sentence length
normalizations do not. We evaluate the best
instance selection methods using a standard
Moses baseline using the whole 1.6 million
sentence English-German section of the Eu-
roparl corpus. We show that selecting the
best 3000 training sentences for a specific
test sentence is sufficient to obtain a score
within 1 BLEU of the baseline, using 5% of
the training data is sufficient to exceed the
baseline, and a? 2 BLEU improvement over
the baseline is possible by optimally selected
subset of the training data. In out-of-domain
translation, we are able to reduce the train-
ing set size to about 7% and achieve a similar
performance with the baseline.
1 Introduction
Statistical machine translation (SMT) makes use
of a large number of parallel sentences, sentences
whose translations are known in the target lan-
guage, to derive translation tables, estimate param-
eters, and generate the actual translation. Not all
of the parallel corpus nor the translation table that
is generated is used during decoding a given set
of test sentences and filtering is usually performed
for computational advantage (Koehn et al, 2007).
Some recent regression-based statistical machine
translation systems rely on a small sized training
data to learn the mappings between source and tar-
get features (Wang and Shawe-Taylor, 2008; Ser-
rano et al, 2009; Bicici and Yuret, 2010). Regres-
sion has some computational disadvantages when
scaling to large number of training instances.
Previous work shows that the more the training
data, the better the translations become (Koehn,
2006). However, with the increased size of the
parallel corpus there is also the added noise, mak-
ing relevant instance selection important. Phrase-
based SMT systems rely heavily on accurately
learning word alignments from the given parallel
corpus. Proper instance selection plays an impor-
tant role in obtaining a small sized training set with
which correct alignments can be learned. Word-
level translation accuracy is also affected by the
number of times a word occurs in the parallel cor-
pus (Koehn and Knight, 2001). Koehn and Knight
find that about 50 examples per word are required
to achieve a performance close to using a bilingual
lexicon in their experiments. Translation perfor-
mance can improve as we include multiple possi-
ble translations for a given word, which increases
272
the diversity of the training set.
Transduction uses test instances, which can
sometimes be accessible at training time, to learn
specific models tailored towards the test set which
also reduces computation by not using the full
training set. Transductive retrieval selects train-
ing data close to the test set given a parallel corpus
and a test set. This work shows that transductive
retrieval of the training set for statistical machine
translation allows us to achieve a performance bet-
ter than using all of the parallel corpus. When se-
lecting training data, we seek to maximize the cov-
erage or the percentage of test source and target
features (i.e. n-grams) found in the training set us-
ing minimal number of target training features and
a fixed number of training instances. Diversifying
the set of training sentences can help us increase
the coverage. We show that target coverage bounds
the achievable BLEU score with a given training
set and small increases can result in large increases
on this BLEU bound.
We develop the feature decay algorithms (FDA)
that aim to maximize the coverage of the target
language features and achieve significant gains in
translation performance. We find that decaying
feature weights has significant effect on the per-
formance. We achieve improvements of ?2 BLEU
points using about 20% of the available training
data in terms of target words and ?1 BLEU points
with only about 5%. We show that selecting 3000
instances for a test sentence is sufficient to obtain
a score within 1 BLEU of the baseline. In the out-
of-domain translation task, we are able to reduce
the training set size to its 7% to achieve a similar
performance with the baseline.
The next section reviews related previous work.
We discuss the FDA in section 3. Section 4
presents our coverage and translation results both
in and out-of-domain and includes an instance se-
lection method also designed for improving word
alignment results. We list our contributions in the
last section.
2 Related Work
Transductive learning makes use of test instances,
which can sometimes be accessible at training
time, to learn specific models tailored towards the
test set. Selection of training instances relevant to
the test set improves the final translation quality as
in transductive learning and decreases human ef-
fort by identifying the most informative sentences
for translation as in active learning. Instance se-
lection in a transductive learning framework se-
lects the best instances for a given test set (L? et
al., 2007). Active learning selects training samples
that will benefit the learning algorithm the most
over the unlabeled dataset U from a labeled train-
ing set L or from U itself after labeling (Banko and
Brill, 2001). Active learning in SMT selects which
instances to add to the training set to improve the
performance of a baseline system (Haffari et al,
2009; Ananthakrishnan et al, 2010). Recent work
involves selecting sentence or phrase translation
tasks for external human effort (Bloodgood and
Callison-Burch, 2010). Below we present exam-
ples of both with a label indicating whether they
follow an approach close to active learning [AL] or
transductive learning [TL] and in our experiments
we use the transductive framework.
TF-IDF [TL]: L? et al (2007) use tf-idf infor-
mation retrieval technique based cosine score to se-
lect a subset of the parallel corpus close to the test
set for SMT training. They outperform the baseline
system when the top 500 training instances per test
sentence are selected. The terms used in their tf-idf
measure correspond to words where this work fo-
cuses on bigram feature coverage. When the com-
bination of the top N selected sentences are used
as the training set, they show increase in the per-
formance at the beginning and decrease when 2000
sentences are selected for each test sentence.
N-gram coverage [AL]: Eck et al (2005) use
n-gram feature coverage to sort and select training
instances using the following score:
?NGRAM (S) =
?n
i=1
?
unseen x ? Xi(S) C(x)
|S|
,
(1)
for sentence S with Xi(S) storing the i-grams
found in S and C(x) returning the count of x in
the parallel corpus. ?NGRAM score sums over un-
seen n-grams to increase the coverage of the train-
ing set. The denominator involving the length of
the sentence takes the translation cost of the sen-
tence into account. Eck et al (2005) also note
that longer sentences are more difficult for train-
ing SMT models. In their experiments, they are
not able to reach a performance above the baseline
2
273
system?s BLEU score, which is using all of the par-
allel corpus, but they achieve close performance by
using about 15% of the parallel corpus.
DWDS [AL]: Density weighted diversity sam-
pling (DWDS) (Ambati et al, 2010) score tries to
select sentences containing the n-gram features in
the unlabeled dataset U while increasing the di-
versity among the sentences selected, L (labeled).
DWDS increases the score of a sentence with in-
creasing frequency of its n-grams found in U and
decreases with increasing frequency in the already
selected set of sentences, L, in favor of diversity.
Let PU (x) denote the probability of feature x in U
and CL(x) denote its count in L. Then:
d(S) =
?
x?X(S) PU (x)e
??CL(x)
|X(S)|
(2)
u(S) =
?
x?X(S) I(x 6? X(L))
|X(S)|
(3)
?DWDS(S) =
2d(S)u(S)
d(S) + u(S)
, (4)
where X(S) stores the features of S and ? is a
decay parameter. d(S) denotes the density of S
proportional to the probability of its features in U
and inversely proportional to their counts in L and
u(S) its uncertainty, measuring the percentage of
new features in S. These two scores are combined
using harmonic mean. DWDS tries to select sen-
tences containing similar features in U with high
diversity. In their active learning experiments, they
selected 1000 training instances in each iteration
and retrained the SMT system.
Log-probability ratios [AL]: Haffari et
al. (2009) develop sentence selection scores using
feature counts in L and U , increasing for frequent
features in U and decreasing for frequent features
in L. They use geometric and arithmetic averages
of log-probability ratios in an active learning
setting where 200 sentences from U are selected
and added to L with their translations for 25
iterations (Haffari et al, 2009). Later, Haffari
et al (2009) distinguish between features found
in the phrase table, xreg, and features not found,
xoov. OOV features are segmented into subfeatures
(i.e. feature ?go to school? is segmented as:
(go to school), (go)(to school), (go to)(school),
(go)(to)(school)). Expected log probability ratio
(ELPR) score is used:
?ELPR(S) = 0.4|Xreg(S)|
?
x?Xreg(S)
log
PU (x)
PL(x)
+ 0.6|Xoov(S)|
?
x?Xoov(S)
?
h?H(x)
1
|H(x)|
?
y?Yh(x)
log
PU (y)
PL(y)
,
(5)
where H(x) return the segmentations of x and
Yh(x) return the features found in segment h.
?ELPR performs better than geometric average in
their experiments (Haffari and Sarkar, 2009).
Perplexity [AL & TL]: Perplexity of the train-
ing instance as well as inter-SMT-system disagree-
ment are also used to select training data for trans-
lation models (Mandal et al, 2008). The increased
difficulty in translating a parallel sentence or its
novelty as found by the perplexity adds to its im-
portance for improving the SMT model?s perfor-
mance. A sentence having high perplexity (a rare
sentence) in L and low perplexity (a common sen-
tence) in U is considered as a candidate for addi-
tion. They are able to improve the performance
of a baseline system trained on some initial corpus
together with additional parallel corpora using the
initial corpus and part of the additional data.
Alignment [TL]: Uszkoreit et al (2010) mine
parallel text to improve the performance of a base-
line translation model on some initial document
translation tasks. They retrieve similar documents
using inverse document frequency weighted cosine
similarity. Then, they filter nonparallel sentences
using their word alignment performance, which is
estimated using the following score:
score(A) =
?
(s,t)?A
ln
p(s, t)
p(s)p(t)
, (6)
where A stands for an alignment between source
and target words and the probabilities are estimated
using a word aligned corpus. The produced paral-
lel data is used to expand a baseline parallel corpus
and shown to improve the translation performance
of machine translation systems.
3 Instance Selection with Feature
Decay
In this section we will describe a class of instance
selection algorithms for machine translation that
3
274
use feature decay, i.e. increase the diversity of the
training set by devaluing features that have already
been included. Our abstraction makes three com-
ponents of such algorithms explicit permitting ex-
perimentation with their alternatives:
? The value of a candidate training sentence as
a function of its features.
? The initial value of a feature.
? The update of the feature value as instances
are added to the training set.
A feature decay algorithm (FDA) aims to max-
imize the coverage of the target language features
(such as words, bigrams, and phrases) for the test
set. A target language feature that does not ap-
pear in the selected training instances will be dif-
ficult to produce regardless of the decoding algo-
rithm (impossible for unigram features). In gen-
eral we do not know the target language features,
only the source language side of the test set is avail-
able. Unfortunately, selecting a training instance
with a particular source language feature does not
guarantee the coverage of the desired target lan-
guage feature. There may be multiple translations
of a feature appropriate for different senses or dif-
ferent contexts. For each source language feature
in the test set, FDA tries to find as many train-
ing instances as possible to increase the chances
of covering the appropriate target language feature.
It does this by reducing the value of the features
that are already included after picking each train-
ing instance. Algorithm 1 gives the pseudo-code
for FDA.
The input to the algorithm is a parallel corpus,
the number of desired training instances, and the
source language features of the test set. We use
unigram and bigram features; adding trigram fea-
tures does not seem to significantly affect the re-
sults. The user has the option of running the algo-
rithm for each test sentence separately, then possi-
bly combining the resulting training sets. We will
present results with these variations in Section 4.
The first foreach loop initializes the value of
each test set feature. We experimented with ini-
tial feature values that are constant, proportional
to the length of the n-gram, or log-inverse of the
corpus frequency. We have observed that the ini-
tial value does not have a significant effect on the
Algorithm 1: The Feature Decay Algorithm
Input: Bilingual corpus U , test set features F ,
and desired number of training
instances N .
Data: A priority queue Q, sentence scores
score, feature values fvalue.
Output: Subset of the corpus to be used as the
training data L ? U .
foreach f ? F do1
fvalue(f)? init(f,U)2
foreach S ? U do3
score(S)?
?
f?features(S) fvalue(f)4
push(Q, S,score(S))5
while |L| < N do6
S ? pop(Q)7
score(S)?
?
f?features(S) fvalue(f)8
if score(S) ? topval(Q) then9
L ? L ? {S}10
foreach f ? features(S) do11
fvalue(f)? decay(f,U ,L)12
else13
push(Q, S,score(S))14
quality of training instances selected. The feature
decay rule dominates the behavior of the algorithm
after the first few iterations. However, we prefer
the log-inverse values because they lead to fewer
score ties among candidate instances and result in
faster running times.
The second foreach loop initializes the score for
each candidate training sentence and pushes them
onto a priority queue. The score is calculated as the
sum of the feature values. Note that as we change
the feature values, the sentence scores in the prior-
ity queue will no longer be correct. However they
will still be valid upper bounds because the fea-
ture values only get smaller. Features that do not
appear in the test set are considered to have zero
value. This observation can be used to speed up
the initialization by using a feature index and only
iterating over the sentences that have features in
common with the test set.
Finally the while loop populates the training set
by picking candidate sentences with the highest
scores. This is done by popping the top scoring
candidate S from the priority queue at each itera-
tion. We recalculate its score because the values
4
275
of its features may have changed. We compare the
recalculated score of S with the score of the next
best candidate. If the score of S is equal or better
we are sure that it is the top candidate because the
scores in the priority queue are upper bounds. In
this case we place S in our training set and decay
the values of its features. Otherwise we push S
back on the priority queue with its updated score.
The feature decay function on Line 12 is the
heart of the algorithm. Unlike the choice of fea-
tures (bigram vs trigram) or their initial values
(constant vs log?inverse?frequency) the rate of de-
cay has a significant effect on the performance. We
found it is optimal to reduce feature values at a rate
of 1/n where n is the current training set count
of the feature. The results get significantly worse
with no feature decay. They also get worse with
faster, exponential feature decay, e.g. 1/2n. Ta-
ble 1 presents the experimental results that support
these conclusions. We use the following settings
for the experiments in Section 4:
init(f,U) = 1 or log(|U|/cnt(f,U))
decay(f,U ,L) =
init(f,U)
1 + cnt(f,L)
or
init(f,U)
1 + 2cnt(f,L)
init decay en?de de?en
1 none .761 .484 .698 .556
log(1/f) none .855 .516 .801 .604
1 1/n .967 .575 .928 .664
log(1/f) 1/n .967 .570 .928 .656
1 1/2n .967 .553 .928 .653
log(1/f) 1/2n .967 .557 .928 .651
Table 1: FDA experiments. The first two columns
give the initial value and decay formula used for
features. f is the corpus frequency of a feature
and n is its count in selected instances. The next
four columns give the expected coverage of the
source and target language bigrams of a test sen-
tence when 100 training sentences are selected.
4 Experiments
We perform translation experiments on the
English-German language pair using the parallel
corpus provided in WMT?10 (Callison-Burch et
al., 2010). The English-German section of the Eu-
roparl corpus contains about 1.6 million sentences.
We perform in-domain experiments to discriminate
among different instance selection techniques bet-
ter in a setting with low out-of-vocabulary rate. We
randomly select the test set test with 2, 588 tar-
get words and separate development set dev with
26, 178 target words. We use the language model
corpus provided in WMT?10 (Callison-Burch et
al., 2010) to build a 5-gram model.
We use target language bigram coverage, tcov,
as a quality measure for a given training set, which
measures the percentage of the target bigram fea-
tures of the test sentence found in a given training
set. We compare tcov and the translation perfor-
mance of FDA with related work. We also perform
small scale SMT experiments where only a couple
of thousand training instances are used for each test
sentence.
4.1 The Effect of Coverage on Translation
BLEU (Papineni et al, 2001) is a precision based
measure and uses n-gram match counts up to or-
der n to determine the quality of a given transla-
tion. The absence of a given word or translating
it as another word interrupts the continuity of the
translation and decreases the BLEU score even if
the order among the words is determined correctly.
Therefore, the target coverage of an out-of-domain
test set whose translation features are not found in
the training set bounds the translation performance
of an SMT system.
We estimate this translation performance bound
from target coverage by assuming that the miss-
ing tokens can appear randomly at any location of
a given sentence where sentence lengths are nor-
mally distributed with mean 25.6 and standard de-
viation 14.1. This is close to the sentence length
statistics of the German side Europarl corpus used
in WMT?10 (WMT, 2010). We replace all un-
known words found with an UNK token and calcu-
late the BLEU score. We perform this experiment
for 10, 000 instances and repeat for 10 times.
The obtained BLEU scores for target cover-
age values is plotted in Figure 1 with label esti-
mate. We also fit a third order polynomial func-
tion of target coverage 0.025 BLEU scores above
the estimate values to show the similarity with the
5
276
0.0 0.2 0.4 0.6 0.8 1.0tcov
0.0
0.2
0.4
0.6
0.8
1.0
1.2
BLE
U
BLEU vs. tcov
estimatef(x)=ax^3 + bx^2 + cx + d
Figure 1: Effect of coverage on translation perfor-
mance. BLEU bound is a third-order function of
target coverage. High coverage? High BLEU.
BLEU scores bound estimated, whose parameters
are found to be [0.56, 0.53,?0.09, 0.003] with a
least-squares fit. Figure 1 shows that the BLEU
score bound obtained has a third-order polyno-
mial relationship with target coverage and small
increases in the target coverage can result in large
increases on this BLEU bound.
4.2 Coverage Results
We select N training instances per test sentence
using FDA (Algorithm 1), TF-IDF with bigram
features, NGRAM scoring (Equation 1), DWDS
(Equation 4), and ELPR (Equation 5) techniques
from previous work. For the active learning algo-
rithms, source side test corpus becomes U and the
selected training set L. For all the techniques, we
compute 1-grams and 2-grams as the features used
in calculating the scores and add only one sentence
to the training set at each iteration except for TF-
IDF. We set ? parameter of DWDS to 1 as given
in their paper. We adaptively select the top scor-
ing instance at each step from the set of possible
sentences U with a given scorer ?(.) and add the
instance to the training set, L, until the size of L
reaches N for the related work other than TF-IDF.
We test all algorithms in this transductive setting.
We measure the bigram coverage when all of
the training sentences selected for each test sen-
tence are combined. The results are presented in
Figure 2 where the x-axis is the number of words
of the training set and y-axis is the target cover-
age obtained. FDA has a steep slope in its increase
and it is able to reach target coverage of ? 0.84.
DWDS performs worse initially but its target cov-
erage improve after a number of instances are se-
lected due to its exponential feature decay proce-
dure. TF-IDF performs worse than DWDS and it
provides a fast alternative to FDA instance selec-
tion but with some decrease in coverage. ELPR
and NGRAM instance selection techniques per-
form worse. NGRAM achieves better coverage
than ELPR, although it lacks a decay procedure.
When we compare the sentences selected, we
observe that FDA prefers longer sentences due to
summing feature weights and it achieves larger tar-
get coverage value. NGRAM is not able to discrim-
inate between sentences well and a lot of sentences
of the same length get the same score when the un-
seen n-grams belong to the same frequency class.
The statistics of L obtained with the instance se-
lection techniques differ from each other as given
in Table 2, where N = 1000 training instances se-
lected per test sentence. We observe that DWDS
has fewer unique target bigram features than TF-
IDF although it selects longer target sentences.
NGRAM obtains a large number of unique target
bigrams although its selected target sentences have
similar lengths with DWDS and ELPR prefers short
sentences.
Technique Unique bigrams Words per sent tcov
FDA 827,928 35.8 .74
DWDS 412,719 16.7 .67
TF-IDF 475,247 16.2 .65
NGRAM 626,136 16.6 .55
ELPR 172,703 10.9 .35
Table 2: Statistics of the obtained target L forN =
1000.
4.3 Translation Results
We develop separate phrase-based SMT models
using Moses (Koehn et al, 2007) using default set-
tings with maximum sentence length set to 80 and
obtained baseline system score as 0.3577 BLEU.
We use the training instances selected by FDA in
6
277
104 105 106 107
Training Set Size (words)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
tco
v
tcov vs. Training Set Size (words)
DWDS
ELPR
FDA
NGRAM
TFIDF
Figure 2: Target coverage curve comparison with previous work. Figure shows the rate of increase in
tcov as the size of L increase.
three learning settings:
L? L is the union of the instances selected for
each test sentence.
L?F L is selected using all of the features found
in the test set.
LI L is the set of instances selected for each test
sentence.
We develop separate Moses systems with each
training set and LI corresponds to developing a
Moses system for each test sentence. L? results
are plot in Figure 3 where we increasingly select
N ? {100, 200, 500, 1000, 2000, 3000, 5000,
10000} instances for each test sentence for train-
ing. The improvements over the baseline are sta-
tistically significant with paired bootstrap resam-
pling using 1000 samples (Koehn, 2004). As we
select more instances, the performance of the SMT
system increases as expected and we start to see a
decrease in the performance after selecting ?107
target words. We obtain comparable results for the
de-en direction. The performance increase is likely
to be due to the reduction in the number of noisy or
irrelevant training instances and the increased pre-
cision in the probability estimates in the generated
phrase tables.
105 106 107 108Training Set Size (words)
0.30
0.31
0.32
0.33
0.34
0.35
0.36
0.37
0.38
BLE
U
0.3058
0.3318
0.341
0.36450.3697
0.3758
0.36220.36530.3577
BLEU vs. Training Set Size (words)
Figure 3: BLEU vs. the number of target words in
L?.
L?F results given in Table 3 show that we can
achieve within 1 BLEU performance using about
3% of the parallel corpus target words (30,000 in-
stances) and better performance using only about
5% (50,000 instances).
The results with LI when building an individ-7
278
# sent # target words BLEU NIST
10,000 449,116 0.3197 5.7788
20,000 869,908 0.3417 6.0053
30,000 1,285,096 0.3492 6.0246
50,000 2,089,403 0.3711 6.1561
100,000 4,016,124 0.3648 6.1331
ALL 41,135,754 0.3577 6.0653
Table 3: Performance for en-de using L?F . ALL
corresponds to the baseline system using all of the
parallel corpus. bold correspond to statistically
significant improvement over the baseline result.
ual Moses model for each test sentence are given
in Table 4. Individual SMT training and transla-
tion can be preferable due to smaller computational
costs and high parallelizability. As we translate
a single sentence with each SMT system, tuning
weights becomes important. We experiment three
settings: (1) using 100 sentences for tuning, which
are randomly selected from dev.1000, (2) using the
mean of the weights obtained in (1), and (3) us-
ing the weights obtained in the union learning set-
ting (L?). We observe that we can obtain a perfor-
mance within 2 BLEU difference to the baseline
system by training on 3000 instances per sentence
(underlined) using the mean weights and 1 BLEU
difference using the union weights. We also exper-
imented with increasing the N -best list size used
during MERT optimization (Hasan et al, 2007),
with increased computational cost, and observed
some increase in the performance.
N 100 dev sents Mean Union
1000 0.3149 0.3242 0.3354
2000 0.3258 0.3352 0.3395
3000 0.3270 0.3374 0.3501
5000 0.3217 0.3303 0.3458
Table 4: LI performance for en-de using 100 sen-
tences for tuning or mean of the weights or dev
weights obtained with the union setting.
Comparison with related work: Table 5
presents the translation results compared with pre-
vious work selecting 1000 instances per test sen-
tence. We observe that coverage and translation
performance are correlated. Although the cover-
age increase of DWDS and FDA appear similar,
due to the third-order polynomial growth of BLEU
with respect to coverage, we achieve large BLEU
gains in translation. We observe increased BLEU
gains when compared with the results of TF-IDF,
NGRAM, and ELPR in order.
FDA DWDS TF-IDF NGRAM ELPR
0.3645 0.3547 0.3405 0.2572 0.2268
Table 5: BLEU results using different techniques
with N = 1000. High coverage? High BLEU.
We note that DWDS originally selects instances
using the whole test corpus to estimate PU (x) and
selects 1000 instances at each iteration. We exper-
imented with both of these settings and obtained
0.3058 and 0.3029 BLEU respectively. Lower
performance suggest the importance of updating
weights after each instance selection step.
4.4 Instance Selection for Alignment
We have shown that high coverage is an integral
part of training sets for achieving high BLEU per-
formance. SMT systems also heavily rely on the
word alignment of the parallel corpus to derive
a phrase table that can be used for translation.
GIZA++ (Och and Ney, 2003) is commonly used
for word alignment and phrase table generation,
which is prone to making more errors as the length
of the training sentence increase (Ravi and Knight,
2010). Therefore, we analyze instance selection
techniques that optimize coverage and word align-
ment performance and at the same time do not
produce very long sentences. Too few words per
sentence may miss the phrasal structure, whereas
too many words per sentence may miss the actual
word alignment for the features we are interested.
We are also trying to retrieve relevant training sen-
tences for a given test sentence to increase the fea-
ture alignment performance.
Shortest: A baseline strategy that can minimize
the training feature set?s size involves selecting the
shortest translations containing each feature.
Co-occurrence: We use co-occurrence of
words in the parallel corpus to retrieve sentences
containing co-occurring items. Dice?s coeffi-
cient (Dice, 1945) is used as a heuristic word align-
ment technique giving an association score for
each pair of word positions (Och and Ney, 2003).
8
279
We define Dice?s coefficient score as:
dice(x, y) =
2C(x, y)
C(x)C(y)
, (7)
where C(x, y) is the number of times x and y co-
occur and C(x) is the count of observing x in the
selected training set. Given a test source sentence,
SU , we can estimate the goodness of a training
sentence pair, (S, T ), by the sum of the alignment
scores:
?dice(SU , S, T ) =
X
x?X(SU )
|T |X
j=1
X
y?Y (x)
dice(y, Tj)
|T | log |S|
,
(8)
where X(SU ) stores the features of SU and Y (x)
lists the tokens in feature x. The difficulty of word
aligning a pair of training sentences, (S, T ), can be
approximated by |S||T |. We use a normalization
factor proportional to |T | log |S|.
The average target words per sentence using
?dice drops to 26.2 compared to 36.3 of FDA.
We still obtain a better performance than the base-
line en-de system with the union of 1000 train-
ing instances per sentence with 0.3635 BLEU and
6.1676 NIST scores. Coverage comparison with
FDA shows slight improvement with lower number
of target bigrams and similar trend for others (Fig-
ure 4). We note that shortest strategy achieves bet-
ter performance than both ELPR and NGRAM. We
obtain 0.3144 BLEU and 5.5 NIST scores in the
individual translation task with 1000 training in-
stances per sentence and 0.3171 BLEU and 5.4662
NIST scores when the mean of the weights is used.
4.5 Out-of-domain Translation Results
We have used FDA and dice algorithms to select
training sets for the out-of-domain challenge test
sets used in (Callison-Burch et al, 2011). The
parallel corpus contains about 1.9 million training
sentences and the test set contain 3003 sentences.
We built separate Moses systems using all of the
parallel corpus for the language pairs en-de, de-en,
en-es, and es-en. We created training sets using
all of the features of the test set to select train-
ing instances. The results given in Table 6 show
that we can achieve similar BLEU performance us-
ing about 7% of the parallel corpus target words
(200,000 instances) using dice and about 16% us-
ing FDA. In the out-of-domain translation task, we
are able to reduce the training set size to achieve
a performance close to the baseline. The sample
points presented in the table is chosen proportional
to the relative sizes of the parallel corpus sizes of
WMT?10 and WMT?11 datasets and the training
set size of the peak in Figure 3. We may be able
to achieve better performance in the out-of-domain
task as well. The sample points in Table 6 may be
on either side of the peak.
5 Contributions
We have introduced the feature decay algorithms
(FDA), a class of instance selection algorithms that
use feature decay, which achieves better target cov-
erage than previous work and achieves significant
gains in translation performance. We find that de-
caying feature weights has significant effect on the
performance. We demonstrate that target coverage
and translation performance are correlated, show-
ing that target coverage is also a good indicator of
BLEU performance. We have shown that target
coverage provides an upper bound on the transla-
tion performance with a given training set.
We achieve improvements of ?2 BLEU points
using about 20% of the available training data in
terms of target words with FDA and ? 1 BLEU
points with only about 5%. We have also shown
that by training on only 3000 instances per sen-
tence we can reach within 1 BLEU difference to
the baseline system. In the out-of-domain transla-
tion task, we are able to reduce the training set size
to achieve a similar performance with the baseline.
Our results demonstrate that SMT systems can
improve their performance by transductive train-
ing set selection. We have shown how to select in-
stances and achieved significant performance im-
provements.
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner,
and Daniel Tapias, editors, Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
9
280
104 105 106 107Total Training Set Size (words)
0.3
0.4
0.5
0.6
0.7
0.8
0.9
tco
v
tcov vs. Total Training Set Size (words)
FDAdiceshortest
0 5000 10000 15000 20000 25000 30000 35000 40000Average Training Set Size (words)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
tco
v
tcov vs. Average Training Set Size (words)
FDAdiceshortest
Figure 4: Target coverage per target words comparison. Figure shows the rate of increase in tcov as
the size of L increase. Target coverage curves for total training set size is given on the left plot and for
average training set size per test sentence on the right plot.
en-de de-en en-es es-en
BLEU
ALL 0.1376 0.2074 0.2829 0.2919
FDA 0.1363 0.2055 0.2824 0.2892
dice 0.1374 0.2061 0.2834 0.2857
# target words ?106
ALL 47.4 49.6 52.8 50.4
FDA 7.9 8.0 8.7 8.2
dice 6.9 7.0 3.9 3.6
% of ALL
FDA 17 16 16 16
dice 14 14 7.4 7.1
Table 6: Performance for the out-of-domain task of (Callison-Burch et al, 2011). ALL corresponds to
the baseline system using all of the parallel corpus.
10
281
Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative sample selection for statistical machine
translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 626?635, Cambridge, MA, October.
Association for Computational Linguistics.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages 26?
33, Toulouse, France, July. Association for Computa-
tional Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the ACL 2010
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics MATR, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 854?
864, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR. Associa-
tion for Computational Linguistics, Uppsala, Sweden,
July.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2011. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Edinburgh, England, July.
Lee R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?
302.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine transla-
tion based on n-gram coverage. In Proceedings of
the 10th Machine Translation Summit, MT Summit X,
pages 227?234, Phuket, Thailand, September.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 181?189, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 415?423, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Sa?a Hasan, Richard Zens, and Hermann Ney. 2007.
Are very large N-best lists useful for SMT? In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 57?60, Rochester, New York, April. As-
sociation for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. In Pro-
ceedings of the 2001 Conference on Empirical Meth-
ods in Natural Language Processing.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Annual Meeting of the Assoc. for Computational Lin-
guistics, pages 177?180, Prague, Czech Republic,
June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2006. Statistical machine translation:
the basic, the novel, and the speculative. Tutorial at
EACL 2006.
Yajuan L?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
A. Mandal, D. Vergyri, W. Wang, J. Zheng, A. Stol-
cke, G. Tur, D. Hakkani-Tur, and N.F. Ayan. 2008.
Efficient data selection for machine translation. In
Spoken Language Technology Workshop, 2008. SLT
2008. IEEE, pages 261 ?264.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
11
282
Computational Linguistics, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Sujith Ravi and Kevin Knight. 2010. Does giza++
make search errors? Computational Linguistics,
36(3):295?302.
Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on Pat-
tern Recognition and Image Analysis, pages 394?401.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 1101?1109, Beijing, China,
August. Coling 2010 Organizing Committee.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
WMT. 2010. ACL Workshop: Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
July.
12
283
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
RegMT System for Machine Translation, System Combination, and
Evaluation
Ergun Bic?ici
Koc? University
34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr
Deniz Yuret
Koc? University
34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr
Abstract
We present the results we obtain using our
RegMT system, which uses transductive re-
gression techniques to learn mappings be-
tween source and target features of given par-
allel corpora and use these mappings to gen-
erate machine translation outputs. Our train-
ing instance selection methods perform fea-
ture decay for proper selection of training in-
stances, which plays an important role to learn
correct feature mappings. RegMT uses L2
regularized regression as well as L1 regular-
ized regression for sparse regression estima-
tion of target features. We present transla-
tion results using our training instance selec-
tion methods, translation results using graph
decoding, system combination results with
RegMT, and performance evaluation with the
F1 measure over target features as a metric for
evaluating translation quality.
1 Introduction
Regression can be used to find mappings between
the source and target feature sets derived from given
parallel corpora. Transduction learning uses a sub-
set of the training examples that are closely related
to the test set without using the model induced by
the full training set. In the context of statistical ma-
chine translation, translations are performed at the
sentence level and this enables us to select a small
number of training instances for each test instance
to guide the translation process. This also gives us a
computational advantage when considering the high
dimensionality of the problem as each sentence can
be mapped to many features.
The goal in transductive regression based ma-
chine translation (RegMT) is both reducing the com-
putational burden of the regression approach by re-
ducing the dimensionality of the training set and the
feature set and also improving the translation quality
by using transduction.
We present translation results using our training
instance selection methods, translation results us-
ing graph decoding, system combination results with
RegMT, and performance evaluation with the F1
measure over target features as a metric for eval-
uating translation quality. RegMT work builds on
our previous regression-based machine translation
results (Bicici and Yuret, 2010) especially with in-
stance selection and additional graph decoding ca-
pability. We present our results to this year?s chal-
lenges.
Outline: Section 2 gives an overview of the
RegMT model. In section 3, we present our train-
ing instance selection techniques and WMT?11 re-
sults. In section 4, we present the graph decoding re-
sults on the Haitian Creole-English translation task.
Section 5 presents our system combination results
using reranking with the RegMT score. Section 6
evaluates the F1 measure that we use for the auto-
matic evaluation metrics challenge. The last section
present our contributions.
2 Machine Translation Using Regression
Let X and Y correspond to the sets of tokens
that can be used in the source and target strings,
then, m training instances are represented as
(x1, y1), . . . , (xm, ym) ? X
? ? Y ?, where (xi, yi)
corresponds to a pair of source and target language
323
token sequences for 1 ? i ? m. Our goal is to find
a mapping f : X? ? Y ? that can convert a source
sentence to a target sentence sharing the same mean-
ing in the target language (Figure 1).
X? Y ?-
? R ?
-FX FY
g
?X ?Y
6
??1Y
f
h
Figure 1: String-to-string mapping.
We define feature mappers ?X : X? ? FX =
RNX and ?Y : Y ? ? FY = RNY that map each
string sequence to a point in high dimensional real
number space. Let MX ? RNX?m and MY ?
RNY ?m such that MX = [?X(x1), . . . ,?X(xm)]
and MY = [?Y (y1), . . . ,?Y (ym)]. The ridge re-
gression solution usingL2 regularization is found by
minimizing the following cost:
WL2 = arg min
W?RNY ?NX
?MY ?WMX ?2F +??W?
2
F . (1)
Two main challenges of the regression based ma-
chine translation (RegMT) approach are learning
the regression function, h : FX ? FY , and
solving the pre-image problem, which, given the
features of the estimated target string sequence,
h(?X(x)) = ?Y (y?), attempts to find y ? Y ?:
y = arg miny?Y ? ||h(?X(x)) ? ?Y (y)||
2. Pre-
image calculation involves a search over possible
translations minimizing the cost function:
f(x) = arg min
y?Y ?
??Y (y)?W?X(x)?2 . (2)
2.1 L1 Regularized Regression
String kernels lead to sparse feature representations
and L1 regularized regression is effective to find the
mappings between sparsely observed features.We
would like to observe only a few nonzero target co-
efficients corresponding to a source feature in the co-
efficient matrix. L1 regularization helps us achieve
solutions close to permutation matrices by increas-
ing sparsity (Bishop, 2006) (page 145). In contrast,
L2 regularized solutions give us dense matrices.
WL2 is not a sparse solution and most of the coef-
ficients remain non-zero. We are interested in pe-
nalizing the coefficients better; zeroing the irrele-
vant ones leading to sparsification to obtain a solu-
tion that is closer to a permutation matrix. L1 norm
behaves both as a feature selection technique and a
method for reducing coefficient values.
WL1 = arg min
W?RNY ?NX
?MY ?WMX ?2F +??W?1 . (3)
Equation 3 presents the lasso (Tibshirani, 1996) so-
lution where the regularization term is now the L1
matrix norm defined as ?W?1=
?
i,j |Wi,j |. WL2
can be found by taking the derivative but since
L1 regularization cost is not differentiable, WL1 is
found by optimization or approximation techniques.
We use forward stagewise regression (FSR) (Hastie
et al, 2006), which approximates lasso for L1 regu-
larized regression.
2.2 Related Work:
Regression techniques can be used to model the
relationship between strings (Cortes et al, 2007).
Wang et al (2007) applies a string-to-string map-
ping approach to machine translation by using ordi-
nary least squares regression and n-gram string ker-
nels to a small dataset. Later they use L2 regularized
least squares regression (Wang and Shawe-Taylor,
2008). Although the translation quality they achieve
is not better than Moses (Koehn et al, 2007), which
is accepted to be the state-of-the-art, they show the
feasibility of the approach. Serrano et al (2009)
use kernel regression to find translation mappings
from source to target feature vectors and experiment
with translating hotel front desk requests. Locally
weighted regression solves separate weighted least
squares problems for each instance (Hastie et al,
2009), weighted by a kernel similarity function.
3 Instance Selection for Machine
Translation
Proper selection of training instances plays an im-
portant role for accurately learning feature mappings
with limited computational resources. Coverage of
the features is important since if we do not have the
correct features in the training matrices, we will not
be able to translate them. Coverage is measured by
the percentage of target features of the test set found
in the training set. For each test sentence, we pick
a limited number of training instances designed to
324
improve the coverage of correct features to build a
regression model.
We use two techniques for this purpose: (1)
Feature Decay Algorithm (FDA), which optimizes
source languge bigram coverage to maximize the
target coverage, (2) dice. Feature decay algorithms
(FDA) aim to maximize the coverage of the tar-
get language features (such as words, bigrams, and
phrases) for the test sentences. FDA selects training
instances one by one updating the coverage of the
features already added to the training set in contrast
to the features found in the test sentence.
We also use a technique that we call dice, which
optimizes source language bigram coverage such
that the difficulty of aligning source and target fea-
tures is minimized. We define Dice?s coefficient
score as:
dice(x, y) =
2C(x, y)
C(x)C(y)
, (4)
where C(x, y) is the number of times x and y co-
occurr and C(x) is the count of observing x in
the selected training set. Given a test source sen-
tence, SU , we can estimate the goodness of a train-
ing sentence pair, (S, T ), by the sum of the align-
ment scores:
?dice(SU , S, T ) =
?
x?X(SU )
|T |?
j=1
?
y?Y (x)
dice(y, Tj)
|T | log |S|
,
(5)
where X(SU ) stores the features of SU and Y (x)
lists the tokens in feature x. The difficulty of word
aligning a pair of training sentences, (S, T ), can be
approximated by |S||T |. We use a normalization fac-
tor proportional to |T | log |S|.
The details of both of these techniques and further
results can be found in (Bicici and Yuret, 2011).
3.1 Moses Experiments on the Translation
Task
We have used FDA and dice algorithms to select
training sets for the out-of-domain challenge test
sets used in (Callison-Burch et al, 2011). The par-
allel corpus contains about 1.9 million training sen-
tences and the test set contain 3003 sentences. We
built separate Moses systems using all of the paral-
lel corpus for the language pairs en-de, de-en, en-
es, and es-en. We created training sets using all
en-de de-en en-es es-en
BLEU
ALL .1376 .2074 .2829 .2919
FDA .1363 .2055 .2824 .2892
dice .1374 .2061 .2834 .2857
words
ALL 47.4 49.6 52.8 50.4
FDA 7.9 8.0 8.7 8.2
dice 6.9 7.0 3.9 3.6
% ALL
FDA 17 16 16 16
dice 14 14 7.4 7.1
Table 1: Performance for the out-of-domain task
of (Callison-Burch et al, 2011). ALL corresponds to the
baseline system using all of the parallel corpus. words
list the size of the target words used in millions.
of the features of the test set to select training in-
stances. The results given in Table 1 show that we
can achieve similar BLEU performance using about
7% of the parallel corpus target words (200,000 in-
stances) using dice and about 16% using FDA. In the
out-of-domain translation task, we are able to reduce
the training set size to achieve a performance close
to the baseline. We may be able to achieve better
performance in this out-of-domain task as well as
explained in (Bicici and Yuret, 2011).
4 Graph Decoding for RegMT
We perform graph-based decoding by first generat-
ing a De Bruijn graph from the estimated y? (Cortes et
al., 2007) and then finding Eulerian paths with max-
imum path weight. We use four features when scor-
ing paths: (1) estimation weight from regression, (2)
language model score, (3) brevity penalty as found
by e?(lR?|s|/|path|) for lR representing the length ra-
tio from the parallel corpus and |path| representing
the length of the current path, (4) future cost as in
Moses (Koehn et al, 2007) and weights are tuned
using MERT (Och, 2003) on the de-en dev set.
We demonstrate that sparse L1 regularized regres-
sion performs better than L2 regularized regression.
Graph based decoding can provide an alternative to
state of the art phrase-based decoding system Moses
in translation domains with small vocabulary and
training set size.
4.1 Haitian Creole to English Translation Task
with RegMT
We have trained a Moses system for the Haitian Cre-
ole to English translation task, cleaned corpus, us-
325
ing the options as described in section 3.1. Moses
achieves 0.3186 BLEU on this task. We observed
that graph decoding performs better where target
coverage is high such that the bigrams used lead
to a connected graph. To increase the connec-
tivity, we have included Moses translations in the
training set and performed graph decoding with
RegMT. RegMT with L2 regularized regression
achieves 0.2708 BLEU with graph decoding and
lasso achieves 0.26 BLEU.
Moses makes use of a number of distortion pa-
rameters and lexical weights, which are estimated
using all of the parallel corpus. Thus, our Moses
translation achieves a better performance than graph
decoding with RegMT using 100 training instances
for translating each source test sentence.
5 System Combination with RegMT
We perform experiments on the system com-
bination task for the English-German, German-
English, English-Spanish, and Spanish-English lan-
guage pairs using the training corpus provided in
WMT?11 (Callison-Burch et al, 2011). We have
tokenized and lowercased each of the system out-
puts and combined these in a single N -best file per
language pair. We use these N -best lists for rerank-
ing by RegMT to select the best translation model.
Feature mappers used are 2-spectrum counting word
kernels (Taylor and Cristianini, 2004).
We rerank N -best lists by a linear combination of
the following scoring functions:
1. RegMT: Regression based machine translation
scores as found by Equation 2.
2. CBLEU: Comparative BLEU scores we obtain
by measuring the average BLEU performance
of each translation relative to the other systems?
translations in the N -best list.
3. LM: We calculate 5-gram language model
scores for each translation using the language
model trained over the target corpus provided
in the translation task.
Since we do not have access to the reference trans-
lations nor to the translation model scores each sys-
tem obtained for each sentence, we estimate trans-
lation model performance (CBLEU) by measuring
the average BLEU performance of each translation
relative to the other translations in the N -best list.
Thus, each possible translation in the N -best list is
BLEU scored against other translations and the av-
erage of these scores is selected as the CBLEU score
for the sentence. Sentence level BLEU score calcu-
lation avoids singularities in n-gram precisions by
taking the maximum of the match count and 12|si| for
|si| denoting the length of the source sentence si as
used in (Macherey and Och, 2007).
Table 2 presents reranking results on all of the lan-
guage pairs we considered, using RegMT, CBLEU,
and LM scores with the same combination weights
as above. We also list the performance of the best
model (Max) as well as the worst (Min). We are
able to achieve close or better BLEU scores in all
of the listed systems when compared with the per-
formance of the best translation system except for
the ht-en language pair. The lower performance in
the ht-en language pair may be due to having a sin-
gle best translation system that outperforms others
significantly. This happens for instance when an un-
constrained model use external resources to achieve
a significantly better performance than the second
best model. 2nd best in Table 2 lists the second best
model?s performance to estimate how much the best
model?s performance is better than the rest.
BLEU en-de de-en en-es es-en ht-en
Min .1064 .1572 .2174 .1976 .2281
Max .1727 .2413 .3375 .3009 .3708
2nd best .1572 .2302 .3301 .2973 .3288
Average .1416 .1997 .292 .2579 .2993
Oracle .2529 .3305 .4265 .4233 .4336
RegMT .1631 .2322 .3311 .3052 .3234
Table 2: System combination results.
RegMT model may prefer sentences with lower
BLEU, which can sometimes cause it to achieve a
lower BLEU performance than the best model. This
is clearly the case for en-de with 1.6 BLEU points
difference with the second best model performance
and for de-en task with 1.11 BLEU points differ-
ence. Also this observation holds for en-es with
0.74 BLEU points difference and for ht-en with 4.2
BLEU points difference. For es-en task, there is 0.36
BLEU points difference with the second best model
and these models likely to complement each other.
326
The existence of complementing SMT models is
important for the reranking approach to achieve a
performance better than the best model, as there is
a need for the existence of a model performing bet-
ter than the best model on some test sentences. We
can use the competitive SMT model to achieve the
performance of the best with a guarantee even when
a single model is dominating the rest (Bicici and
Kozat, 2010). For competing translation systems
in an on-line machine translation setting adaptively
learning of model weights can be performed based
on the previous transaltion performance (Bicici and
Kozat, 2010).
6 Target F1 as a Performance Evaluation
Metric
We use target sentence F1 measure over the tar-
get features as a translation performance evaluation
metric. We optimize the parameters of the RegMT
model with the F1 measure comparing the target
vector with the estimate we get from the RegMT
model. F1 measure uses the 0/1-class predictions
over the target feature with the estimate vector,
?Y (y?). Let TP be the true positive, TN the true neg-
ative, FP the false positive, and FN the false negative
rates, we use the following measures for evaluation:
prec =
TP
TP + FP
, BER = ( FPTN+FP +
FN
TP+FN )/2 (6)
rec =
TP
TP + FN
, F1 =
2?prec?rec
prec+rec (7)
where BER is the balanced error rate, prec is pre-
cision, and rec is recall. The evaluation techniques
measure the effectiveness of the learning models in
identifying the features of the target sentence mak-
ing minimal error to increase the performance of the
decoder and its translation quality.
We use gapped word sequence kernels (Taylor
and Cristianini, 2004) when using F1 for evaluating
translations since a given translation system may not
be able to translate a given word but can correctly
identify the surrounding phrase. For instance, let the
reference translation be the following sentence:
a sound compromise has been reached
Some possible translations for the reference are
given in Table 3 together with their BLEU (Papineni
et al, 2001) and F1 scores for comparison. F1 score
does not have a brevity penalty but a brief transla-
tion is penalized by a low recall value. We use up
to 3 tokens as gaps. F1 measure is able to increase
the ranking of Trans4 by using a gapped sequence
kernel, which can be preferrable to Trans3.
We note that a missing token corresponds to vary-
ing decreases in the n-gram precision used in the
BLEU score. A sentence containing m tokens has
m 1-grams, m?1 2-grams, andm?n+1 n-grams.
A missing token degrades the performance more in
higher order n-gram precision values. A missing to-
ken decreases n-gram precision by 1m for 1-grams
and by nm?n+1 for n-grams. Based on this obser-
vation, we use F1 measure with gapped word se-
quence kernels to evaluate translations. Gapped fea-
tures allows us to consider the surrounding phrase
for a missing token as present in the translation.
Let the reference sentence be represented with
a b c d e f where a-f, x, y, z correspond to to-
kens in the sentence. Then, Trans3 has the form
a b x y f, and Trans4 has the form a c y f.
Then, F1 ranks Trans4 higher than Trans3 for orders
greater than 3 as there are two consecutive word er-
rors in Trans3. F1 can also prefer a missing token
rather than a word error as we see by comparing
Trans4 and Trans5 and it can still prefer contigu-
ity over a gapped sequence as we see by comparing
Trans5 and Trans6 in Table 3.
We calculate the correlation of F1 with BLEU on
the en-de development set. We use 5-grams with the
F1 measure as this increases the correlation with 4-
gram BLEU. Table 4 gives the correlation results us-
ing both Pearson?s correlation score and Spearman?s
correlation score. Spearman?s correlation score is a
better metric for comparing the relative orderings.
Metric No gaps Gaps
Pearson .8793 .7879
Spearman .9068 .8144
Table 4: F1 correlation with 4-gram BLEU using blended
5-gram gapped word sequence features on the develop-
ment set.
7 Contributions
We present the results we obtain using our RegMT
system, which uses transductive regression tech-
niques to learn mappings between source and tar-
327
Format BLEU F1
Ref: a sound compromise has been reached a b c d e f 4-grams 3-grams 4-grams 5-grams
Trans1: a sound agreement has been reached a b x d e f .2427 .6111 .5417 .5
Trans2: a compromise has reached a c d f .137 .44 .3492 .3188
Trans3: a sound agreement is reached a b x y f .1029 .2 .1558 .1429
Trans4: a compromise is reached a c y f .0758 .2 .1587 .1449
Trans5: a good compromise is reached a z c y f .0579 .1667 .1299 .119
Trans6: a good compromise is been a z c y e .0579 .2 .1558 .1429
Table 3: BLEU vs. F1 on sample sentence translation task.
get features of given parallel corpora and use these
mappings to generate machine translation outputs.
We also present translation results using our train-
ing instance selection methods, translation results
using graph decoding, system combination results
with RegMT, and performance evaluation with F1
measure over target features. RegMT work builds
on our previous regression-based machine transla-
tion results (Bicici and Yuret, 2010) especially with
instance selection and additional graph decoding ca-
pability.
References
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings of
the ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized re-
gression for reranking and system combination in ma-
chine translation. In Proceedings of the ACL 2010
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics MATR, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Ergun Bicici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the EMNLP 2011 Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, England, July.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2011. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Edinburgh, England, July.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A general regression framework for learn-
ing string-to-string mappings. In Gokhan H. Bakir,
Thomas Hofmann, and Bernhard Sch editors, Predict-
ing Structured Data, pages 143?168. The MIT Press,
September.
Trevor Hastie, Jonathan Taylor, Robert Tibshirani, and
Guenther Walther. 2006. Forward stagewise regres-
sion and the monotone lasso. Electronic Journal of
Statistics, 1.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference and Prediction. Springer-Verlag,
2nd edition.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Assoc. for Computational Linguistics,
pages 177?180, Prague, Czech Republic, June.
Wolfgang Macherey and Franz Josef Och. 2007. An
empirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. Association for Com-
putational Linguistics, 1:160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on Pat-
tern Recognition and Image Analysis, pages 394?401.
J. Shawe Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
328
Robert J. Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58(1):267?288.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine trans-
lation. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Companion
Volume, Short Papers, pages 185?188, Rochester, New
York, April. Association for Computational Linguis-
tics.
329
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 78?84,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Feature Decay Algorithms for Fast Deployment of Accurate Statistical
Machine Translation Systems
Ergun Bic?ici
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
ergun.bicici@computing.dcu.ie
Abstract
We use feature decay algorithms (FDA)
for fast deployment of accurate statistical
machine translation systems taking only
about half a day for each translation direc-
tion. We develop parallel FDA for solving
computational scalability problems caused
by the abundance of training data for SMT
models and LM models and still achieve
SMT performance that is on par with us-
ing all of the training data or better. Par-
allel FDA runs separate FDA models on
randomized subsets of the training data
and combines the instance selections later.
Parallel FDA can also be used for selecting
the LM corpus based on the training set
selected by parallel FDA. The high qual-
ity of the selected training data allows us
to obtain very accurate translation outputs
close to the top performing SMT systems.
The relevancy of the selected LM corpus
can reach up to 86% reduction in the num-
ber of OOV tokens and up to 74% reduc-
tion in the perplexity. We perform SMT
experiments in all language pairs in the
WMT13 translation task and obtain SMT
performance close to the top systems us-
ing significantly less resources for training
and development.
1 Introduction
Statistical machine translation (SMT) is a data in-
tensive problem. If you have the translations for
the source sentences you are translating in your
training set or even portions of it, then the trans-
lation task becomes easier. If some tokens are not
found in your training data then you cannot trans-
late them and if some translated word do not ap-
pear in your language model (LM) corpus, then it
becomes harder for the SMT engine to find their
correct position in the translation.
Current SMT systems also face problems
caused by the proliferation of various parallel cor-
pora available for building SMT systems. The
training data for many of the language pairs in
the translation task, part of the Workshop on Ma-
chine translation (WMT13) (Callison-Burch et al,
2013), have increased the size of the available par-
allel corpora for instance by web crawled corpora
over the years. The increased size of the training
material creates computational scalability prob-
lems when training SMT models and can increase
the amount of noisy parallel sentences found. As
the training set sizes increase, proper training set
selection becomes more important.
At the same time, when we are going to trans-
late just a couple of thousand sentences, possibly
belonging to the same target domain, it does not
make sense to invest resources for training SMT
models over tens of millions of sentences or even
more. SMT models like Moses already have filter-
ing mechanisms to create smaller parts of the built
models that are relevant to the test set.
In this paper, we develop parallel feature decay
algorithms (FDA) for solving computational scal-
ability problems caused by the abundance of train-
ing data for SMT models and LM models and still
achieve SMT performance that is on par with us-
ing all of the training data or better. Parallel FDA
runs separate FDA models on randomized subsets
of the training data and combines the instance se-
lections later. We perform SMT experiments in
all language pairs of the WMT13 (Callison-Burch
et al, 2013) and obtain SMT performance close to
the baseline Moses (Koehn et al, 2007) system us-
ing less resources for training. With parallel FDA,
we can solve not only the instance selection prob-
lem for training data but also instance selection for
the LM training corpus, which allows us to train
higher order n-gram language models and model
the dependencies better.
Parallel FDA improves the scalability of FDA
78
and allows rapid prototyping of SMT systems for
a given target domain or task. Parallel FDA can be
very useful for MT in target domains with limited
resources or in disaster and crisis situations (Lewis
et al, 2011) where parallel corpora can be gath-
ered by crawling and selected by parallel FDA.
Parallel FDA also improves the computational re-
quirements of FDA by selecting from smaller cor-
pora and distributing the work load. The high
quality of the selected training data allows us to
obtain very accurate translation outputs close to
the top performing SMT systems. The relevancy
of the LM corpus selected can reach up to 86% re-
duction in the number of OOV tokens and up to
74% reduction in the perplexity.
We organize our work as follows. We describe
FDA and parallel FDA models in the next section.
We also describe how we extend the FDA model
for LM corpus selection. In section 3, we present
our experimental results and in the last section, we
summarize our contributions.
2 Feature Decay Algorithms for Instance
Selection
In this section, we describe the FDA algorithm,
the parallel FDA model, and how FDA training
instance selection algorithms can be used also for
instance selection for language model corpora.
2.1 Feature Decay Algorithm (FDA)
Feature decay algorithms (Bic?ici and Yuret,
2011a) increase the diversity of the training set by
decaying the weights of n-gram features that have
already been included. FDAs try to maximize the
coverage of the target language features for the test
set. Translation performance can improve as we
include multiple possible translations for a given
word, which increases the diversity of the training
set. A target language feature that does not appear
in the selected training instances will be difficult to
produce regardless of the decoding algorithm (im-
possible for unigram features). FDA tries to find
as many training instances as possible to increase
the chances of covering the correct target language
feature by reducing the weight of the included fea-
tures after selecting each training instance.
Algorithm 1 gives the pseudo-code for FDA.
We improve FDA with improved scaling, where
the score for each sentence is scaled proportional
to the length of the sentence, which reduces the
average length of the training instances.
Algorithm 1: The Feature Decay Algorithm
Input: Parallel training sentences U , test set
features F , and desired number of
training instances N .
Data: A priority queue Q, sentence scores
score, feature values fval.
Output: Subset of the parallel sentences to be
used as the training data L ? U .
1 foreach f ? F do
2 fval(f)? init(f,U)
3 foreach S ? U do
4 score(S)? 1|S|s
?
f?features(S)
fval(f)
5 enqueue(Q, S,score(S))
6 while |L| < N do
7 S ? dequeue(Q)
8 score(S)? 1|S|s
?
f?features(S)
fval(f)
9 if score(S) ? topval(Q) then
10 L ? L ? {S}
11 foreach f ? features(S) do
12 fval(f)? decay(f,U ,L)
13 else
14 enqueue(Q, S,score(S))
The input to the algorithm consists of parallel
training sentences, the number of desired training
instances, and the source language features of the
test set. The feature decay function (decay) is
the most important part of the algorithm where
feature weights are multiplied by 1/n where n
is the count of the feature in the current train-
ing set. The initialization function (init) calcu-
lates the log of inverse document frequency (idf):
init(f,U) = log(|U|/(1 + C(f,U))), where
|U| is the sum of the number of features appear-
ing in the training corpus and C(f,U) is the num-
ber of times feature f appear in U . Further ex-
periments with the algorithm are given in (Bic?ici
and Yuret, 2011a). We improve FDA with a scal-
ing factor that prefers shorter sentences defined as:
|S|s, where s is the power of the source sentence
length and we set it to 0.9 after optimizing it over
the perplexity of the LM built over the selected
corpus (further discussed in Section 2.3).
2.2 Parallel FDA Model
FDA model obtains a sorting over all of the avail-
able training corpus based on the weights of the
features found on the test set. Each selected train-
79
Algorithm 2: Parallel FDA
Input: U , F , and N .
Output: L ? U .
1 U ? shuffle(U)
2 U ,M ? split(U , N)
3 L ? {}
4 S ? {}
5 foreach Ui ? U do
6 Li,Si ? FDA(Ui,F ,M)
7 add(L,Li)
8 add(S ,Si)
9 L ? merge(L,S )
ing instance effects which feature weights will be
decayed and therefore can result in a different or-
dering of the instances if previous instance selec-
tions are altered. This makes it difficult to par-
allelize the FDA algorithm fully. Parallel FDA
model first shuffles the parallel training sentences,
U , and distributes them to multiple splits for run-
ning individual FDA models on them.
The input to parallel FDA also consists of paral-
lel training sentences, the number of desired train-
ing instances, and the source language features of
the test set. The first step shuffles the parallel train-
ing sentences and the next step splits into equal
parts and outputs the split files and the adjusted
number of instances to select from each, M . Since
we split into equal parts, we select equal number
of sentences, M , from each split. Then we run
FDA on each file to obtain sorted files,L, together
with their scores, S . merge combines k sorted
lists into one sorted list in O(Mk log k) where
Mk is the total number of elements in all of the
input lists. 1 The obtained L is the new training set
to be used for SMT experiments. We compared the
target 2-gram feature coverage of the training sets
obtained with FDA and parallel FDA and found
that parallel FDA achieves close performance.
Parallel FDA improves the scalability of FDA
and allows rapid prototyping of SMT systems for
a given target domain or task. Parallel FDA also
improves the computational requirements of FDA
by selecting from smaller corpora and distributing
the work load, which can be very useful for MT in
disaster scenarios.
1 (Cormen et al, 2009), question 6.5-9. Merging k sorted
lists into one sorted list using a min-heap for k-way merging.
2.3 Instance Selection for the Language
Model Corpus
The language model corpus is very important for
improving the SMT performance since it helps
finding the correct ordering among the translated
tokens or phrases. Increased LM corpus size can
increase the SMT performance where doubling the
LM corpus can improve the BLEU (Papineni et
al., 2002) by 0.5 (Koehn, 2006). However, al-
though LM corpora resources are more abundant,
training on large LM corpora also poses compu-
tational scalability problems and until 2012, LM
corpora such as LDC Gigaword corpora were not
fully utilized due to memory limitations of com-
puters and even with large memory machines, the
LM corpora is split into pieces, interpolated, and
merged (Koehn and Haddow, 2012) or the LM
order is decreased to use up to 4-grams (Markus
et al, 2012) or low frequency n-gram counts are
omitted and better smoothing techniques are de-
veloped (Yuret, 2008). Using only the given train-
ing data for building the LM is another option
used for limiting the size of the corpus, which
can also obtain the second best performance in
Spanish-English translation task and in the top
tier for German-English (Guzman et al, 2012;
Callison-Burch et al, 2012). This can also indi-
cate that prior knowledge of the test set domain
and its similarity to the available parallel training
data may be diminishing the gains in SMT perfor-
mance through better language modeling or better
domain adaptation.
For solving the computational scalability prob-
lems, there is a need for properly selecting LM
training data as well. We select LM corpus with
parallel FDA based on this observation:
No word not appearing in the training
set can appear in the translation.
It is impossible for an SMT system to translate
a word unseen in the training corpus nor can it
translate it with a word not found in the target
side of the training set 2. Thus we are only in-
terested in correctly ordering the words appear-
ing in the training corpus and collecting the sen-
tences that contain them for building the LM. At
the same time, we want to be able to model longer
range dependencies more efficiently especially for
morphologically rich languages (Yuret and Bic?ici,
2Unless the translation is a verbatim copy of the source.
80
2009). Therefore, a compact and more relevant
LM corpus can be useful.
Selecting the LM corpus is harder. First of all,
we know which words should appear in the LM
corpus but we do not know which phrases should
be there since the translation model may reorder
the translated words, find different translations,
and generate different phrases. Thus, we use 1-
gram features for LM corpus selection. At the
same time, in contrast with selecting instances for
the training set, we are less motivated to increase
the diversity since we want predictive power on
the most commonly observed patterns. Thus, we
do not initialize feature weights with the idf score
and instead, we use the inverse of the idf score
for initialization, which is giving more importance
to frequently occurring words in the training set.
This way of LM corpus selection also allows us
to obtain a more controlled language and helps us
create translation outputs within the scope of the
training corpus and the closely related LM corpus.
We shuffle the LM corpus available before split-
ting and select from individual splits, to prevent
extreme cases. We add the training set directly
into the LM and also add the training set not se-
lected into the pool of sentences that can be se-
lected for the LM. The scaling parameter s is opti-
mized over the perplexity of the training data with
the LM built over the selected LM corpus.
3 Experiments
We experiment with all language pairs in
both directions in the WMT13 translation
task (Callison-Burch et al, 2013), which include
English-German (en-de), English-Spanish (en-es),
English-French (en-fr), English-Czech (en-cs),
and English-Russian (en-ru). We develop transla-
tion models using the phrase-based Moses (Koehn
et al, 2007) SMT system. We true-case all of the
corpora, use 150-best lists during tuning, set the
max-fertility of GIZA++ (Och and Ney, 2003) to
a value between 8-10, use 70 word classes learned
over 3 iterations with mkcls tool during GIZA++
training, and vary the language model order
between 5 to 9 for all language pairs. The de-
velopment set contains 3000 sentences randomly
sampled from among all of the development
sentences provided.
Since we do not know the best training set
size that will maximize the performance, we rely
on previous SMT experiments (Bic?ici and Yuret,
2011a; Bic?ici and Yuret, 2011b) to select the
proper training set size. We choose close to 15
million words and its corresponding number of
sentences for each training corpus and 10 million
sentences for each LM corpus not including the
selected training set, which is added later. This
corresponds to selecting roughly 15% of the train-
ing corpus for en-de and 35% for ru-en, and due to
their larger size, 5% for en-es, 6% for cs-en, 2%
for en-fr language pairs. The size of the LM cor-
pus allows us to build higher order models. The
statistics of the training data selected by the paral-
lel FDA is given in Table 1. Note that the training
set size for different translation directions differ
slightly since we run a parallel FDA for each.
cs / en de / en es / en fr / en ru / en
words (#M) 186 / 215 92 / 99 409 / 359 1010 / 886 41 / 44
sents (#K) 867 631 841 998 709
words (#M) 13 / 15 16 / 17 23 / 21 26 / 22 16 / 18
Table 1: Comparison of the training data available
and the selected training set by parallel FDA for
each language pair. The size of the parallel cor-
pora is given in millions (M) of words or thou-
sands (K) of sentences.
After selecting the training set, we select the
LM corpora using the words in the target side of
the training set as the features. For en, es, and
fr, we have access to the LDC Gigaword corpora,
from which we extract only the story type news
and for en, we exclude the corpora from Xinhua
News Agency (xin eng). The size of the LM cor-
pora from LDC and the monolingual LM corpora
provided by WMT13 are given in Table 2. For
all target languages, we select 10M sentences with
parallel FDA from the LM corpora and the remain-
ing training sentences and add the selected training
data to obtain the LM corpus. Thus the size of the
LM corpora is 10M plus the number of sentences
in the training set as given in Table 1.
#M cs de en es fr ru
LDC - - 3402 949 773 -
Mono 388 842 1389 341 434 289
Table 2: The size of the LM corpora from LDC
and the monolingual language model corpora pro-
vided in millions (M) of words.
With FDA, we can solve not only the instance
selection problem for the training data but also
the instance selection problem for the LM train-
ing corpus and achieve close target 2-gram cover-
81
S ? en en? T
cs-en de-en es-en fr-en ru-en en-cs en-de en-es en-fr en-ru
WMT13 .2620 .2680 .3060 .3150 .2430 .1860 .2030 .3040 .3060 .1880
BLEUc .2430 .2414 .2909 .2539 .2226 .1708 .1792 .2799 .2379 .1732
BLEUc diff .0190 .0266 .0151 .0611 .0204 .0152 .0238 .0241 .0681 .0148
LM order 7 9 7 9 6 5 5 5 7 5
BLEUc, n .2407, 5 .2396, 5 .2886, 8 .2532, 6 .2215, 9 .1698, 9 .1784, 9 .2794, 9 .2374, 9 .1719, 9
Table 3: Best BLEUc results obtained on the translation task together with the LM order used when
obtaining the result compared with the best constrained Moses results in WMT12 and WMT13. The last
row compares the BLEUc result with respect to using a different LM order.
age using about 5% of the available training data
and 5% of the available LM corpus for instance for
en. A smaller LM training corpus also allows us
to train higher order n-gram language models and
model the dependencies better and achieve lower
perplexity as given in Table 5.
3.1 WMT13 Translation Task Results
We run a number of SMT experiments for each
language pair varying the LM order used and ob-
tain different results and sorted these based on the
tokenized BLEU performance, BLEUc. The best
BLEUc results obtained on the translation task to-
gether with the LM order used when obtaining the
results are given in Table 3. We also list the top re-
sults from WMT13 (Callison-Burch et al, 2013) 3,
which use phrase-based Moses for comparison 4
and the BLEUc difference we obtain. For trans-
lation tasks with en as the target, higher order n-
gram LM perform better whereas for translation
tasks with en as the source, mostly 5-gram LM
perform the best. We can obtain significant gains
in BLEU (+0.0023) using higher order LMs.
For all translation tasks except fr-en and en-fr,
we are able to obtain very close results to the top
Moses system output (0.0148 to 0.0266 BLEUc
difference). This shows that we can obtain very
accurate translation outputs yet use only a small
portion of the training corpus available, signifi-
cantly reducing the time required for training, de-
velopment, and deployment of an SMT system for
a given translation task.
We are surprised by the lower performance in
en-fr or fr-en translation tasks and the reason is,
we believe, due to the inherent noise in the Gi-
gaFrEn training corpus 5. FDA is an instance se-
3We use the results from matrix.statmt.org.
4Phrase-based Moses systems usually rank in the top 3.
5We even found control characters in the corpora.
lection tool and it does not filter out target sen-
tences that are noisy since FDA only looks at the
source sentences when selecting training instance
pairs. Noisy instances may be caused by a sen-
tence alignment problem and one way to fix them
is to measure the sentence alignment accuracy by
using a similarity score over word distributions
such as the Zipfian Word Vectors (Bic?ici, 2008).
Since noisy parallel corpora can decrease the per-
formance, we also experimented with discarding
the GigaFrEn corpus in the experiments. However,
this decreased the results by 0.0003 BLEU in con-
trast to 0.004-0.01 BLEU gains reported in (Koehn
and Haddow, 2012). Also, note that the BLEU re-
sults we obtained are lower than in (Koehn and
Haddow, 2012), which may be an indication that
our training set size was small for this task.
3.2 Training Corpus Quality
We measure the quality of the training corpus by
the coverage of the target 2-gram features of the
test set, which is found to correlate well with the
BLEU performance achievable (Bic?ici and Yuret,
2011a). Table 4 presents the source (scov) and tar-
get (tcov) 2-gram feature coverage of both the par-
allel training corpora (train) that we select from
and the training sets obtained with parallel FDA.
We show that we can obtain coverages close to us-
ing all of the available training corpora.
3.3 LM Corpus Quality
We compare the perplexity of the LM trained on
all of the available training corpora for the de-en
language pair versus the LM trained on the paral-
lel FDA training corpus and the parallel FDA LM
corpus. The number of OOV tokens become 2098,
2255, and 291 respectively for English and 2143,
2555, and 666 for German. To be able to com-
pare the perplexities, we take the OOV tokens into
consideration during calculations. Tokenized LM
82
cs-en de-en es-en fr-en ru-en en-cs en-de en-es en-fr en-ru
train scov .70 .74 .85 .83 .66 .82 .82 .84 .87 .78tcov .82 .82 .84 .87 .78 .70 .74 .85 .83 .66
FDA scov .70 .74 .85 .82 .66 .82 .82 .84 .84 .78tcov .74 .75 .77 .78 .75 .59 .67 .78 .76 .61
Table 4: Source (scov) and target (tcov) 2-gram feature coverage comparison of the training corpora
(train) with the training sets obtained with parallel FDA (FDA).
corpus has 247M tokens for en and 218M tokens
for de. We assume that each OOV word in en or
de contributes log(1/218M) to the log probabil-
ity, which we round to ?19. We also present re-
sults for the case when we handle OOV words bet-
ter with a cost of ?11 each in Table 5.
Table 5 shows that we reduce the perplexity
with a LM built on the training set selected with
parallel FDA, which uses only 15% of the training
data for de-en. More significantly, the LM build on
the LM corpus selected by the parallel FDA is able
to decrease both the number of OOV tokens and
the perplexity and allows us to efficiently model
higher order relationships as well. We reach up to
86% reduction in the number of OOV tokens and
up to 74% reduction in the perplexity.
log OOV = ?19 log OOV = ?11
ppl train FDA FDA LM train FDA FDA LM
en
3 763 774 203 431 419 187
4 728 754 192 412 409 178
5 725 753 191 410 408 176
6 724 753 190 409 408 176
7 724 753 190 409 408 176
de
3 1255 1449 412 693 713 343
4 1216 1428 398 671 703 331
5 1211 1427 394 668 702 327
6 1210 1427 393 668 702 326
7 1210 1427 392 668 702 326
Table 5: Perplexity comparison of the LM built
from the training corpus (train), parallel FDA se-
lected training corpus (FDA), and the parallel FDA
selected LM corpus (FDA LM).
3.4 Computational Costs
In this section, we quantify how fast the overall
system runs for a given language pair. The in-
stance selection times are dependent on the num-
ber of training sentences available for the language
pair for training set selection and for the target lan-
guage for LM corpus selection. We give the av-
erage number of minutes it takes for the parallel
FDA to finish selection for each direction and for
each target language in Table 6.
time (minutes) en-fr en-ru
Parallel FDA train 50 18
Parallel FDA LM 66 50
Table 6: The average time in the number of min-
utes for parallel FDA to select instances for the
training set or for the LM corpus for language
pairs en-fr and en-ru.
Once the training set and the LM corpus are
ready, the training of the phrase-based SMT model
Moses takes about 12 hours. Therefore, we are
able to deploy an SMT system for the target trans-
lation task in about half a day and still obtain very
accurate translation results.
4 Contributions
We develop parallel FDA for solving computa-
tional scalability problems caused by the abun-
dance of training data for SMT models and LM
models and still achieve SMT performance that is
on par with the top performing SMT systems. The
high quality of the selected training data and the
LM corpus allows us to obtain very accurate trans-
lation outputs while the selected the LM corpus re-
sults in up to 86% reduction in the number of OOV
tokens and up to 74% reduction in the perplexity
and allows us to model higher order dependencies.
FDA and parallel FDA raise the bar of expec-
tations from SMT translation outputs with highly
accurate translations and lowering the bar to entry
for SMT into new domains and tasks by allowing
fast deployment of SMT systems in about half a
day. Parallel FDA provides a new step towards
rapid SMT system development in budgeted train-
ing scenarios and can be useful in developing ma-
chine translation systems in target domains with
limited resources or in disaster and crisis situations
where parallel corpora can be gathered by crawl-
ing and selected by parallel FDA. Parallel FDA is
also allowing a shift from general purpose SMT
systems towards task adaptive SMT solutions.
83
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin
City University and in part by the European Com-
mission through the QTLaunchPad FP7 project
(No: 296347). We also thank the SFI/HEA Irish
Centre for High-End Computing (ICHEC), Koc?
University, and Deniz Yuret for the provision of
computational facilities and support.
References
Ergun Bic?ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 272?283,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system
for machine translation, system combination, and
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici. 2008. Context-based sentence alignment
in parallel corpora. In Proceedings of the 9th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing 2008),
LNCS, Haifa, Israel, February.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2013.
Findings of the 2013 workshop on statistical ma-
chine translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
10?51. Association for Computational Linguistics,
August.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2009. Introduction to
Algorithms (3. ed.). MIT Press.
Francisco Guzman, Preslav Nakov, Ahmed Thabet, and
Stephan Vogel. 2012. Qcri at wmt12: Experi-
ments in spanish-english and german-english ma-
chine translation of news text. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 298?303, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Assoc. for Compu-
tational Linguistics, pages 177?180, Prague, Czech
Republic, June.
Philipp Koehn. 2006. Statistical machine translation:
the basic, the novel, and the speculative. Tutorial at
EACL 2006.
William Lewis, Robert Munro, and Stephan Vogel.
2011. Crisis mt: Developing a cookbook for mt
in crisis situations. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
501?511, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Freitag Markus, Peitz Stephan, Huck Matthias, Ney
Hermann, Niehues Jan, Herrmann Teresa, Waibel
Alex, Hai-son Le, Lavergne Thomas, Allauzen
Alexandre, Buschbeck Bianka, Crego Joseph Maria,
and Senellart Jean. 2012. Joint wmt 2012 submis-
sion of the quaero project. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 322?329, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Deniz Yuret and Ergun Bic?ici. 2009. Modeling mor-
phologically rich languages using split words and
unstructured dependencies. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
345?348, Suntec, Singapore, August. Association
for Computational Linguistics.
Deniz Yuret. 2008. Smoothing a tera-word language
model. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 141?144, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
84
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 343?351,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Referential Translation Machines for Quality Estimation
Ergun Bic?ici
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
ergun.bicici@computing.dcu.ie
Abstract
We introduce referential translation ma-
chines (RTM) for quality estimation of
translation outputs. RTMs are a computa-
tional model for identifying the translation
acts between any two data sets with re-
spect to a reference corpus selected in the
same domain, which can be used for esti-
mating the quality of translation outputs,
judging the semantic similarity between
text, and evaluating the quality of student
answers. RTMs achieve top performance
in automatic, accurate, and language inde-
pendent prediction of sentence-level and
word-level statistical machine translation
(SMT) quality. RTMs remove the need to
access any SMT system specific informa-
tion or prior knowledge of the training data
or models used when generating the trans-
lations. We develop novel techniques for
solving all subtasks in the WMT13 qual-
ity estimation (QE) task (QET 2013) based
on individual RTM models. Our results
achieve improvements over last year?s QE
task results (QET 2012), as well as our
previous results, provide new features and
techniques for QE, and rank 1st or 2nd in
all of the subtasks.
1 Introduction
Quality Estimation Task (QET) (Callison-Burch et
al., 2012; Callison-Burch et al, 2013) aims to de-
velop quality indicators for translations and pre-
dictors without access to the references. Predic-
tion of translation quality is important because the
expected translation performance can help in esti-
mating the effort required for correcting the trans-
lations during post-editing by human translators.
Bicici et al (2013) develop the Machine Trans-
lation Performance Predictor (MTPP), a state-of-
the-art, language independent, and SMT system
extrinsic machine translation performance predic-
tor, which achieves better performance than the
competitive QET baseline system (Callison-Burch
et al, 2012) by just looking at the test source sen-
tences and becomes the 2nd overall after also look-
ing at the translation outputs in QET 2012.
In this work, we introduce referential translation
machines (RTM) for quality estimation of transla-
tion outputs, which is a computational model for
identifying the acts of translation for translating
between any given two data sets with respect to
a reference corpus selected in the same domain.
RTMs reduce our dependence on any task depen-
dent resource. In particular, we do not use the
baseline software or the SMT resources provided
with the QET 2013 challenge. We believe having
access to glass-box features such as the phrase ta-
ble or the n-best lists is not realistic especially for
use-cases where translations may be provided by
different MT vendors (not necessarily from SMT
products) or by human translators. Even the prior
knowledge of the training corpora used for build-
ing the SMT models or any other model used when
generating the translations diverges from the goal
of independent and unbiased prediction of trans-
lation quality. Our results show that we do not
need to use any SMT system dependent informa-
tion to achieve the top performance when predict-
ing translation output quality.
2 Referential Translation Machine
(RTM)
Referential translation machines (RTMs) provide
a computational model for quality and seman-
tic similarity judgments using retrieval of rele-
vant training data (Bic?ici and Yuret, 2011a; Bic?ici,
2011) as interpretants for reaching shared seman-
tics (Bic?ici, 2008). RTMs achieve very good per-
formance in judging the semantic similarity of
sentences (Bic?ici and van Genabith, 2013a) and
we can also use RTMs to automatically assess the
343
correctness of student answers to obtain better re-
sults (Bic?ici and van Genabith, 2013b) than the
state-of-the-art (Dzikovska et al, 2012).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference cor-
pus selected in the same domain. RTM can be
used for predicting the quality of translation out-
puts. An RTM model is based on the selection of
common training data relevant and close to both
the training set and the test set of the task where
the selected relevant set of instances are called the
interpretants. Interpretants allow shared semantics
to be possible by behaving as a reference point for
similarity judgments and providing the context. In
semiotics, an interpretant I interprets the signs
used to refer to the real objects (Bic?ici, 2008).
RTMs provide a model for computational seman-
tics using interpretants as a reference according
to which semantic judgments with translation acts
are made. Each RTM model is a data translation
model between the instances in the training set
and the test set. We use the FDA (Feature De-
cay Algorithms) instance selection model for se-
lecting the interpretants (Bic?ici and Yuret, 2011a)
from a given corpus, which can be monolingual
when modeling paraphrasing acts, in which case
the MTPP model (Section 2.1) is built using the
interpretants themselves as both the source and the
target side of the parallel corpus. RTMs map the
training and test data to a space where translation
acts can be identified. We view that acts of transla-
tion are ubiquitously used during communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different lan-
guages and paraphrasing or communication also
contain acts of translation. When creating sen-
tences, we use our background knowledge and
translate information content according to the cur-
rent context. Given a training set train, a test
set test, and some monolingual corpus C, prefer-
ably in the same domain as the training and test
sets, the RTM steps are:
1. T = train ? test.
2. select(T, C)? I
3. MTPP(I,train)? Ftrain
4. MTPP(I,test)? Ftest
5. learn(M,Ftrain)?M
6. predict(M,Ftest)? q?
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3 and 4 use I to map train and test
to a new space where similarities between transla-
tion acts can be derived more easily. Step 5 trains
a learning model M over the training features,
Ftrain, and Step 6 obtains the predictions. RTM
relies on the representativeness of I as a medium
for building translation models for translating be-
tween train and test.
Our encouraging results in the QET challenge
provides a greater understanding of the acts of
translation we ubiquitously use when communi-
cating and how they can be used to predict the
performance of translation, judging the semantic
similarity between text, and evaluating the qual-
ity of student answers. RTM and MTPP models
are not data or language specific and their mod-
eling power and good performance are applicable
across different domains and tasks. RTM expands
the applicability of MTPP by making it feasible
when making monolingual quality and similarity
judgments and it enhances the computational scal-
ability by building models over smaller but more
relevant training data as interpretants.
2.1 The Machine Translation Performance
Predictor (MTPP)
In machine translation (MT), pairs of source and
target sentences are used for training statistical
MT (SMT) models. SMT system performance is
affected by the amount of training data used as
well as the closeness of the test set to the training
set. MTPP (Bic?ici et al, 2013) is a state-of-the-
art and top performing machine translation per-
formance predictor, which uses machine learning
models over features measuring how well the test
set matches the training set to predict the quality
of a translation without using a reference trans-
lation. MTPP measures the coverage of individ-
ual test sentence features and syntactic structures
found in the training set and derives feature func-
tions measuring the closeness of test sentences to
the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of
translation for data transformation.
2.2 MTPP Features for Translation Acts
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
344
parsing with CCL extracts links from base words
to head words, resulting in structures represent-
ing the grammatical information instantiated in the
training and test data. Feature functions use statis-
tics involving the training set and the test sen-
tences to determine their closeness. Since they are
language independent, MTPP allows quality esti-
mation to be performed extrinsically.
We extend MTPP (Bic?ici et al, 2013) in its
learning module, the features included, and their
representations. Categories for the 308 features
(S for source, T for target) used are listed below
where the number of features are given in {#} and
the detailed descriptions for some of the features
are presented in (Bic?ici et al, 2013).
? Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Synthetic Translation Performance {6}: Cal-
culates translation scores achievable accord-
ing to the n-gram coverage.
? Length {7}: Calculates the number of words
and characters for S and T and their average
token lengths and their ratios.
? Feature Vector Similarity {16}: Calculates
similarities between vector representations.
? Perplexity {90}: Measures the fluency of
the sentences according to language models
(LM). We use both forward ({30}) and back-
ward ({15}) LM features for S and T.
? Entropy {9}: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences.
? Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set
are found in the selected training set, I, us-
ing FDA (Bic?ici and Yuret, 2011a).
? Diversity {6}: Measures the diversity of co-
occurring features in the training set.
? IBM1 Translation Probability {16}: Cal-
culates the translation probability of test
sentences using the selected training set,
I, (Brown et al, 1993).
? IBM2 Alignment Features {11}: Calculates
the sum of the entropy of the distribution of
alignment probabilities for S (?s?S ?p log p
for p = p(t|s) where s and t are tokens) and
T, their average for S and T, the number of en-
tries with p ? 0.2 and p ? 0.01, the entropy
of the word alignment between S and T and
its average, and word alignment log probabil-
ity and its value in terms of bits per word.
? Minimum Bayes Retrieval Risk {4}: Calcu-
lates the translation probability for the trans-
lation having the minimum Bayes risk among
the retrieved training instances.
? Sentence Translation Performance {3}: Cal-
culates translation scores obtained accord-
ing to q(T,R) using BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), or
F1 (Bic?ici and Yuret, 2011b) for q.
? Character n-grams {4}: Calculates cosine
between character n-grams (for n=2,3,4,5)
obtained for S and T (Ba?r et al, 2012).
? LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bjo?rnsson, 1968) for
S and T. 1
For retrieval closeness, we use FDA instead
of dice for sentence selection. We also improve
FDA?s instance selection score by scaling with the
length of the sentence (Bic?ici and Yuret, 2011a).
IBM2 alignments and their probabilities are ob-
tained by first obtaining IBM1 alignments and
probabilities, which become the starting point for
the IBM2 model. Both models are trained for 25
to 75 iterations or until convergence.
3 Quality Estimation Task Results
We participate in all of the four challenges of the
quality estimation task (QET) (Callison-Burch
et al, 2013), which include English to Spanish
(en-es) and German to English translation direc-
tions. There are two main categories of chal-
lenges: sentence-level prediction (Task 1.*) and
word-level prediction (Task 2). Task 1.1 is about
predicting post-editing effort (PEE), Task 1.2 is
about ranking translations from different systems,
Task 1.3 is about predicting post-editing time
(PET), and Task 2 is about binary or multi-class
classification of word-level quality.
For each task, we develop RTM mod-
els using the parallel corpora and the LM
corpora distributed by the translation task
(WMT13) (Callison-Burch et al, 2013) and the
LM corpora provided by LDC for English and
Spanish 2. The parallel corpora contain 4.3M
sentences for de-en with 106M words for de and
111M words for en and 15M sentences for en-es
with 406M words for en and 455M words for
1LIX=AB + C 100A , where A is the number of words, C iswords longer than 6 characters, B is words that start or end
with any of ?.?, ?:?, ?!?, ??? similar to (Hagstro?m, 2012).
2English Gigaword 5th, Spanish Gigaword 3rd edition.
345
es. We do not use any resources provided by
QET including data, software, or baseline features
since they are SMT system dependent or language
specific. Instance selection for the training set and
the language model (LM) corpus is handled by a
parallel implementation of FDA (Bic?ici, 2013).
We tokenize and true-case all of the corpora. The
true-caser is trained on all of the training corpus
using Moses (Koehn et al, 2007). We prepare the
corpora by following this procedure: tokenize ?
train the true-caser ? true-case. Table 1 lists the
statistics of the data used in the training and test
sets for the tasks.
Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3 & 2
Train
sents 2254 32730 22338 803
words 63K (en) 762K (de) 528K (en) 18K (en)67K (es) 786K (en) 559K (es) 20K (es)
Test sents 500 1810 1315 284
Table 1: Data statistics for different tasks. The
number of words is listed after tokenization.
Since we do not know the best training set
size that will maximize the performance, we rely
on previous SMT experiments (Bic?ici and Yuret,
2011a; Bic?ici and Yuret, 2011b) and quality es-
timation challenges (Bic?ici and van Genabith,
2013a; Bic?ici and van Genabith, 2013b) to select
the proper training set size. For each training and
test sentence provided in each subtask, we choose
between 65 and 600 sentences from the parallel
training corpora to be added to the training set,
which creates roughly 400K sentences for train-
ing. We add the selected training set to the 8 mil-
lion sentences selected for each LM corpus. The
statistics of the training data selected by the par-
allel FDA and used as interpretants in the RTM
models is given in Table 2.
Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3 2
sents 406K 318K 299K 398K 397K
words 6.3M (en) 4.8M (de) 4.3M (en) 6.6M (en) 6.6M (en)6.9M (es) 4.9M (en) 4.6M (es) 7.2M (es) 7.2M (es)
Table 2: Statistics of the training data used as in-
terpretants in the RTM models in thousands (K) of
sentences or millions (M) of words.
3.1 Evaluation
In this section, we describe the metrics we use to
evaluate the learning performance. Let yi repre-
sent the actual target value for instance i, y? the
mean of the actual target values, y?i the value es-
timated by the learning model, and ??y the mean of
the estimated target values, then we use the fol-
lowing metrics to evaluate the learning models:
? Mean Absolute Error (MAE): |?| =
?n
i=1 |y?i?yi|
n
? Relative Absolute Error (RAE) : ??|| =
?n
i=1 |y?i?yi|?n
i=1 |y??yi|
? Root Mean Squared Error: RMSE =??n
i=1(y?i?yi)2
n
? DeltaAvg: ??(V, S) =
1
|S|/2?1
?|S|/2
n=2
(?n?1
k=1
?
s??ki=1 qi
V (s)
|?ki=1 qi|
)
? Correlation: r =
?n
i=1(y?i???y)(yi?y?)??n
i=1(y?i???y)2
??n
i=1(yi?y?)2
DeltaAvg (Callison-Burch et al, 2012) calculates
the average quality difference between the scores
for the top n ? 1 quartiles and the overall quality
for the test set. Relative absolute error measures
the error relative to the error when predicting the
actual mean. We use the coefficient of determina-
tion, R2 = 1 ??ni=1(y?i ? yi)2/
?n
i=1(y? ? yi)2,
during optimization where the models are
regression based and higher R2 values are better.
3.2 Task 1: Sentence-level Prediction of
Quality
In this subsection, we develop techniques for the
prediction of quality at the sentence-level. We first
discuss the learning models we use and how we
optimize them and then provide the results for the
individual subtasks and the settings used.
3.2.1 Learning Models and Optimization
The learning models we use for predicting the
translation quality include the ridge regression
(RR) and support vector regression (SVR) with
RBF (radial basis functions) kernel (Smola and
Scho?lkopf, 2004). Both of these models learn
a regression function using the features to esti-
mate a numerical target value such as the HTER
score, the F1 score (Bic?ici and Yuret, 2011b), or
the PET score. We also use these learning models
after a feature subset selection with recursive fea-
ture elimination (RFE) (Guyon et al, 2002) or a
dimensionality reduction and mapping step using
partial least squares (PLS) (Specia et al, 2009),
both of which are described in (Bic?ici et al, 2013).
The learning parameters that govern the behavior
of RR and SVR are the regularization ? for RR and
the C, ?, and ? parameters for SVR. We optimize
346
the learning parameters, the number of features
to select, and the number of dimensions used for
PLS. More detailed description of the optimiza-
tion process is given in (Bic?ici et al, 2013). In
our submissions, we only used the results we ob-
tained from SVR and SVR after PLS (SVRPLS)
since they perform the best during training.
Optimization can be a challenge for SVR due to
the large number of parameter settings to search.
In this work, we decrease the search space by se-
lecting ? close to the theoretically optimal values.
We select ? close to the standard deviation of the
noise in the training set since the optimal value
for ? is shown to have linear dependence to the
noise level for different noise models (Smola et al,
1998). We use RMSE of RR on the training set as
an estimate for the noise level (? of noise) and the
following formulas to obtain the ? with ? = 3:
? = ??
?
lnn
n (1)
and the C (Cherkassky and Ma, 2004; Chal-
imourda et al, 2004):
C = max(|y? + 3?y|, |y? ? 3?y|) (2)
Since the C obtained could be low (Chalimourda
et al, 2004), we use a range of C values in ad-
dition to the obtained C value including C values
with a couple of ?y values larger.
Table 3 lists the RMSE of the RR model on the
training set and the corresponding ? and C val-
ues for different subtasks. We also present the op-
timized parameter values for SVR and SVRPLS.
Table 3 shows that, empirically, Equation 1 and
Equation 2 gives results close to the best parame-
ters found after optimization.
Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3
RMSE RR .1397 .1169 .1569 68.06
? .0245 .0062 .01 18.64
C .8398 .8713 1.02 371.28
C? (SVR) .8398 .5 .5 100
? (SVR) .0005 .001 .0001 .0005
C? (SVRPLS) 1.5 .8713 1.02 100
? (SVRPLS) .0001 .0001 .0001 .001
# dim (SVRPLS) 60 60 60 60
Table 3: Optimal parameters predicted by Equa-
tion 1 and Equation 2 and the optimized parame-
ter values, C? and ? for SVR and SVRPLS and the
number of dimensions (# dim) for SVRPLS.
3.2.2 Task 1.1: Scoring and Ranking for
Post-Editing Effort
Task 1.1 involves the prediction of the case insen-
sitive translation edit rate (TER) scores obtained
by TERp (Snover et al, 2009) and their ranking.
In contrast, we derive features over sentences that
are true-cased. We obtain the rankings by sorting
according to the predicted TER scores.
Task 1.1 R2 r RMSE MAE RAE
RR 0.3510 0.5965 0.1393 0.1086 0.7888
RR PLS 0.4232 0.6509 0.1313 0.1023 0.7430
SVR 0.4394 0.6647 0.1295 0.0967 0.7023
SVR PLS 0.4305 0.6569 0.1305 0.1003 0.7284
Table 4: Task1.1 results on the training set.
Table 4 presents the learning performance on
the training set using the optimized parameters.
We are able to significantly improve the results
when compared with the QET 2012 (Callison-
Burch et al, 2012) and our previous results (Bic?ici
et al, 2013) especially in terms of MAE and RAE.
The results on the test set are given in Table 5.
Rank lists the overall ranking in the task. RTMs
with SVR PLS learning is able to achieve the top
rank in this task.
Ranking DeltaAvg r Rank
CNGL SVRPLS 11.09 0.55 1
CNGL SVR 9.88 0.51 4
Scoring MAE RMSE Rank
CNGL SVRPLS 13.26 16.82 3
CNGL SVR 13.85 17.28 8
Table 5: Task1.1 results on the test set.
3.2.3 Task 1.2: Ranking Translations from
Different Systems
Task 1.2 involves the prediction of the ranking
among up to 5 translation outputs produced by dif-
ferent MT systems. Evaluation is done against
the human rankings using the Kendall?s ? corre-
lation (Callison-Burch et al, 2013): ? = (c ?
d)/n(n?1)2 = c?dc+d where a pair is concordant, c, ifthe ordering agrees, discordant, d, if their ordering
disagrees, and neither concordant nor discordant if
their rankings are equal.
We use sentence-level F1 scores (Bic?ici and
Yuret, 2011b) as the target to predict. We use
F1 because it can be easily interpreted and it cor-
relates well with human judgments (more than
TER) (Bic?ici and Yuret, 2011b; Callison-Burch et
al., 2011). We also found that the ? of the rank-
ings obtained according to the F1 score over the
347
training set (0.2040) is better than BLEU (Pap-
ineni et al, 2002) (0.1780) and NIST (Dodding-
ton, 2002) (0.1907) for de-en. Table 6 presents the
learning performance on the training set using the
optimized parameters. Learning F1 becomes an
easier task than learning TER as observed from the
results but we have significantly more training in-
stances. We use the SVR model for predicting the
F1 scores on the training set and the test set. MAE
is a more important performance metric here since
we want to be as precise as possible when predict-
ing the actual performance.
Task 1.2 R2 r RMSE MAE RAE
de-en RR 0.6320 0.7953 0.1169 0.0733 0.5535SVR 0.7528 0.8692 0.0958 0.0463 0.3494
en-es RR 0.5101 0.7146 0.1569 0.1047 0.6323SVR 0.4819 0.7018 0.1613 0.0973 0.5873
Table 6: Task1.2 results on the training set.
Our next goal is to learn a threshold for judg-
ing if two translations are equal over the predicted
F1 scores. This threshold is used to determine
whether we need to alter the ranking. We try
to mimic the human decision process when de-
termining whether two translations are equivalent.
On some occasions where the sentences are close
enough, humans give them equal ranking. This
is also related to the granularity of the differences
visible with a 1 to 5 ranking schema.
We compared different threshold formulations
and used the following condition in our submis-
sions to decide whether the ranking of item i in a
set S of translations, i ? S, should be different:
?
j 6=i
F1(j)? F1(i)
|j ? i| /|S| > t, (3)
where t is the optimized threshold minimizing the
following loss for n training instances:
n?
i=1
?(f(t, qi), ri) (4)
where f(t, qi) is a function returning rankings
based on the threshold t and the quality scores for
instance i, qi and ?(rj , ri) calculates the ? score
based on the rankings rj and ri.
For both de-en and en-es subtasks, we found the
thresholds obtained to be very similar or the same.
The optimized values are given in Table 7. On the
test set, we used the same threshold, t = 0.00275
for both de-en and en-es, which is a little higher
than the optimal t to prevent overfitting.
Task 1.2 ? t # same # all
de-en .2339 .00013 236 25644.2287 .00275 494
en-es .2801 .00073 136 17752.2764 .00275 233
Table 7: Task1.2 optimized thresholds and the
corresponding comparisons that were found to be
equal (# same) over all comparisons (# all).
We believe that human judgments of linguis-
tic equality and the corresponding thresholds we
learned in this work can be useful for developing
better automatic evaluation metrics and can im-
prove the correlation of the scores obtained with
human judgments (as we did here). The results on
the test set are given in Table 8. We are also able
to achieve the top ranking in this task.
Ties penalized model ? Rank
de-en CNGL SVRPLS F1 0.17 3CNGL SVR F1 0.17 4
en-es CNGL SVRPLS F1 0.15 1CNGL SVR F1 0.13 2
Ties ignored model ? Rank
de-en CNGL SVRPLS F1 0.17 3CNGL SVR F1 0.17 4
en-es CNGL SVRPLS F1 0.16 2CNGL SVR F1 0.13 3
Table 8: Task1.2 results on the test set.
3.2.4 Task 1.3: Predicting Post-Editing Time
Task 1.3 involves the prediction of the post-editing
time (PET) for a translator to post-edit the MT out-
put. Table 9 presents the learning performance on
the training set using the optimized parameters.
Task 1.3 R2 r RMSE MAE RAE
RR 0.4463 0.6702 68.0628 39.5250 0.6694
RR PLS 0.5917 0.7716 58.4464 35.8759 0.6076
SVR 0.4062 0.6753 70.4853 36.5132 0.6184
SVR PLS 0.5316 0.7604 62.6031 33.5490 0.5682
Table 9: Task1.3 results on the training set.
The results on the test set are given in Table 10.
We are able to become the 2nd best system accord-
ing to MAE in this task.
3.3 Task 2: Word-level Prediction of Quality
In this subsection, we develop a learning model,
global linear models with dynamic learning rate
(GLMd), for the prediction of quality at the word-
level where the word-level quality is a binary (K:
keep, C: change) or multi-class classification prob-
lem (K: keep, S: substitute, D: delete). We first
discuss the GLMd learning model, then we present
348
Task 1.3 MAE Rank
CNGL SVR 49.2121 3
CNGL SVRPLS 49.6161 4
RMSE Rank
CNGL SVRPLS 86.6175 4
CNGL SVR 90.3650 7
Table 10: Task1.3 results on the test set.
the word-level features we use, and then present
our results on the test set.
3.3.1 Global Linear Models with Dynamic
Learning (GLMd)
Collins (2002) develops global learning models
(GLM), which rely on Viterbi decoding, percep-
tron learning, and flexible feature definitions. We
extend the GLM framework by parallel percep-
tron training (McDonald et al, 2010) and dynamic
learning with adaptive weight updates in the per-
ceptron learning algorithm:
w = w + ? (?(xi, yi)? ?(xi, y?)) , (5)
where ? returns a global representation for in-
stance i and the weights are updated by ? =
exp(log10(3?1/0)) with ?1 and 0 representing
the error of the previous and first iteration respec-
tively. ? decays the amount of the change during
weight updates at later stages and prevents large
fluctuations with updates. We used both the GLM
model and the GMLd models in our submissions.
3.3.2 Word-level Features
We introduce a number of novel features for the
prediction of word-level translation quality. In
broad categories, these word-level features are:
? CCL: Uses CCL links.
? Word context: Surrounding words.
? Word alignments: Alignments, their probabili-
ties, source and target word contexts.
? Length: Word lengths, n-grams over them.
? Location: Location of the words.
? Prefix and Suffix: Word prefixes, suffixes.
? Form: Capital, contains digit or punctuation.
We found that CCL links are the most discrimi-
native feature among these. In total, we used 511K
features for binary and 637K for multi-class clas-
sification. The learning curve is given in Figure 1.
The results on the test set are given in Table 11.
P, R, and A stand for precision, recall, and accu-
racy respectively. We are able to become the 2nd
according to A in this task.
Figure 1: Learning curve with the parallel GLM
and GLMd models.
Binary A P R F1 Rank (A)
CNGL dGLM .7146 .7392 .9261 .8222 2
CNGL GLM .7010 .7554 .8581 .8035 5
Multi-class A Rank
CNGL dGLM .7162 3
CNGL GLM .7116 4
Table 11: Task 2 results on the test set.
4 Contributions
Referential translation machines achieve top per-
formance in automatic, accurate, and language in-
dependent prediction of sentence-level and word-
level statistical machine translation (SMT) qual-
ity. RTMs remove the need to access any SMT
system specific information or prior knowledge of
the training data or models used when generating
the translations. We develop novel techniques for
solving all subtasks in the quality estimation (QE)
task (QET 2013) based on individual RTM mod-
els. Our results achieve improvements over last
year?s QE task results (QET 2012), as well as our
previous results, provide new features and tech-
niques for QE, and rank 1st or 2nd in all of the
subtasks.
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin
City University and in part by the European Com-
mission through the QTLaunchPad FP7 project
(No: 296347). We also thank the SFI/HEA Irish
Centre for High-End Computing (ICHEC) for the
provision of computational facilities and support.
349
References
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics ? Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 435?440, Montre?al,
Canada, 7-8 June. Association for Computational
Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013a. CNGL-
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013b. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The Second Joint Conference on Lex-
ical and Computational Semantics and Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, Georgia, USA,
14-15 June. Association for Computational Linguis-
tics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 272?283,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system
for machine translation, system combination, and
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bjo?rnsson. 1968. La?sbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omer F. Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, Edinburgh, England, July. As-
sociation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2013.
Findings of the 2013 workshop on statistical ma-
chine translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
10?51. Association for Computational Linguistics,
August.
Athanassia Chalimourda, Bernhard Scho?lkopf, and
Alex J. Smola. 2004. Experimentally optimal
? in support vector regression for different noise
models and parameter settings. Neural Networks,
17(1):127?141, January.
Vladimir Cherkassky and Yunqian Ma. 2004. Practical
selection of svm parameters and noise estimation for
svm regression. Neural Netw., 17(1):113?126, Jan-
uary.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and
Chris Brew. 2012. Towards effective tutorial feed-
back for explanation questions: A dataset and base-
lines. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
350
Computational Linguistics: Human Language Tech-
nologies, pages 200?210, Montre?al, Canada, June.
Association for Computational Linguistics.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389?422.
Kenth Hagstro?m. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Assoc. for Compu-
tational Linguistics, pages 177?180, Prague, Czech
Republic, June.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464, Los Angeles, California,
June. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
A. J. Smola, N. Murata, B. Scho?lkopf, and K.-R.
Mu?ller. 1998. Asymptotically optimal choice of
?-loss for support vector machines. In L. Niklas-
son, M. Boden, and T. Ziemke, editors, Proceedings
of the International Conference on Artificial Neural
Networks, Perspectives in Neural Computing, pages
105?110, Berlin. Springer.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 259?268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Con-
ference of the European Association for Machine
Translation (EAMT), pages 28?35, Barcelona, May.
EAMT.
Wikipedia. 2013. Lix.
http://en.wikipedia.org/wiki/LIX.
351
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 59?65,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Parallel FDA5 for Fast Deployment of Accurate
Statistical Machine Translation Systems
Ergun Bic?ici
Centre for Next Generation Localisation
School of Computing
Dublin City University
ergun.bicici@computing.dcu.ie
Qun Liu
Centre for Next Generation Localisation
School of Computing
Dublin City University
qliu@computing.dcu.ie
Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
away@computing.dcu.ie
Abstract
We use parallel FDA5, an efficiently pa-
rameterized and optimized parallel im-
plementation of feature decay algorithms
for fast deployment of accurate statistical
machine translation systems, taking only
about half a day for each translation di-
rection. We build Parallel FDA5 Moses
SMT systems for all language pairs in
the WMT14 translation task and obtain
SMT performance close to the top Moses
systems with an average of 3.49 BLEU
points difference using significantly less
resources for training and development.
1 Introduction
Parallel FDA5 is developed for fast deployment
of accurate statistical machine translation systems
using an efficiently parameterized and optimized
parallel implementation of feature decay algo-
rithms (Bic?ici and Yuret, 2014). Parallel FDA5
takes about half a day for each translation direc-
tion. We achieve SMT performance that is on par
with the top constrained Moses SMT systems.
Statistical machine translation (SMT) is a data
intensive problem. If you have the translations for
the source sentences you are translating in your
training set or even portions of it, then the trans-
lation task becomes easier. If some tokens are not
found in the training data then you cannot trans-
late them and if some translated word do not ap-
pear in your language model (LM) corpus, then it
becomes harder for the SMT engine to find its cor-
rect position in the translation. The importance of
parallel FDA5 increases with the proliferation of
training material available for building SMT sys-
tems. Table 2 presents the statistics of the avail-
able training and LM corpora for the constrained
(C) systems as well as the statistics of the Parallel
FDA5 selected training and LM corpora.
Parallel FDA5 runs separate FDA5 models on
randomized subsets of the training data and com-
bines the selections afterwards. We run parallel
FDA5 SMT experiments using Moses (Koehn et
al., 2007) in all language pairs in WMT14 (Bojar
et al., 2014) and obtain SMT performance close to
the top constrained Moses systems training using
all of the training material. Parallel FDA5 allows
rapid prototyping of SMT systems for a given tar-
get domain or task and can be very useful for MT
in target domains with limited resources or in dis-
aster and crisis situations (Lewis et al., 2011).
2 Parallel FDA5 for Instance Selection
2.1 FDA5
FDA is developed mainly for building high per-
formance SMT systems using fewer yet relevant
data that is selected for increasing the coverage of
the test set features while maximizing their diver-
sity (Bic?ici and Yuret, 2011; Bic?ici, 2011). Par-
allel FDA parallelize instance selection and sig-
nificantly reduces the time to deploy accurate MT
systems in the presence of large training data from
weeks to half a day and still achieve state-of-
the-art SMT performance (Bic?ici, 2013). FDA5
is developed for efficient parameterization, opti-
mization, and implementation of FDA (Bic?ici and
Yuret, 2014). FDA5 can be used in both trans-
ductive learning scenarios where test set is used to
select the training data or in active learning sce-
narios where training set itself is used to obtain a
sorting of the training data and select.
We run transductive learning experiments in
this work such that the instance selection is per-
formed for the given test set. According to
SMT experiments performed on the 2 million sen-
tence English-German section of the Europarl cor-
pus (Bic?ici and Yuret, 2014), FDA5 can increase
the performance by 0.41 BLEU points compared
to using all of the available training data and by
59
Algorithm 1: Parallel FDA5
Input: Parallel training sentences U , test set
features F , and desired number of
training instances N .
Output: Subset of the parallel sentences to be
used as the training data L ? U .
1 U ? shuffle(U)
2 U ,M ? split(U , N)
3 L? {}
4 foreach U
i
? U do
5 ?L
i
, s
i
? ? FDA5(U
i
,F ,M)
6 L? L ? ?L
i
, s
i
?
7 L ? merge(L)
3.22 BLEU points compared to random selection.
FDA5 is also used for selecting the training set
in the WMT14 medical translation task (Calixto
et al., 2014) and the tuning set in the WMT14
German-English translation task (Li et al., 2014).
FDA5 has 5 parameters that effect the instance
scores based on the three formulas used:
? Initialization:
init(f) = log(|U|/C
U
(f))
i
|f |
l
(1)
? Decay:
decay(f) = init(f)(1+C
L
(f))
?c
d
C
L
(f)
(2)
? Sentence score:
sentScore(S) =
1
|S|
s
?
f?F (S)
fvalue(f)
(3)
C
L
(f) returns the count of feature f in L. d
is the feature score polynomial decay factor, c is
the feature score exponential decay factor, s is
the sentence score length exponent, i is the initial
feature score idf exponent, and l is the initial
feature score n-gram length exponent. FDA5 is
available at http://github.com/bicici/FDA
and the FDA5 optimizer is available at
http://github.com/bicici/FDAOptimization.
2.2 Parallel FDA5
Parallel FDA5 (ParFDA5) is presented in Algo-
rithm 1, which first shuffles the training sentences,
U and runs individual FDA5 models on the multi-
ple splits from which equal number of sentences,
M , are selected. We use ParFDA5 for select-
ing parallel training data and LM data for build-
ing SMT systems. merge combines k sorted ar-
rays, L
i
, into one sorted array in O(Mk log k) us-
ing their scores, s
i
, where Mk is the total number
of elements in all of the input arrays.
1
ParFDA5
makes FDA5 more scalable to domains with large
training corpora and allows rapid deployment of
SMT systems. By selecting from random splits of
the original corpus, we work with different n-gram
feature distributions in each split and prevent fea-
ture values from becoming negligible, which can
enhance the diversity.
2.3 Language Model Data Selection
We select the LM training data with ParFDA5
based on the following observation (Bic?ici, 2013):
No word not appearing in the training
set can appear in the translation.
It is impossible for an SMT system to translate a
word unseen in the training corpus nor can it trans-
late it with a word not found in the target side of
the training set
2
. Thus we are only interested
in correctly ordering the words appearing in the
training corpus and collecting the sentences that
contain them for building the LM. At the same
time, a compact and more relevant LM corpus is
also useful for modeling longer range dependen-
cies with higher order n-gram models. We use
1-gram features for LM corpus selection since we
don?t know which phrases will be generated by the
translation model. After the LM corpus selection,
the target side of the parallel training data is added
to the LM corpus.
3 Results
We run ParFDA5 SMT experiments for all lan-
guage pairs in both directions in the WMT14
translation task (Bojar et al., 2014), which include
English-Czech (en-cs), English-German (en-de),
English-French (en-fr), English-Hindi (en-hi), and
English-Russian (en-ru). We true-case all of the
corpora, use 150-best lists during tuning, set the
LM order to a value between 7 and 10 for all lan-
guage pairs, and train the LM using SRILM (Stol-
cke, 2002). We set the maximum sentence length
filter to 126 and for GIZA++ (Och and Ney, 2003),
1
(Cormen et al., 2009), question 6.5-9. Merging k sorted
lists into one sorted list using a min-heap for k-way merging.
2
Unless the translation is a verbatim copy of the source.
60
S ? T
Training Data LM Data
Data #word S (M) #word T (M) #sent (K) SCOV TCOV #word (M) TCOV
en-cs C 253.5 223.4 16068 0.8282 0.7046 717.0 0.8539
en-cs ParFDA5 22.0 19.6 1205 0.8161 0.6062 325.8 0.8238
cs-en C 223.4 253.5 16068 0.7046 0.8282 5541.9 0.9552
cs-en ParFDA5 19.3 22.0 1205 0.7046 0.7581 351.0 0.9132
en-de C 116.0 109.5 4511 0.812 0.7101 1573.8 0.8921
en-de ParFDA5 16.7 16.8 845 0.8033 0.6316 206.9 0.8184
de-en C 109.5 116.0 4511 0.7101 0.812 5446.8 0.9525
de-en ParFDA5 17.8 19.6 845 0.7087 0.753 339.5 0.9082
en-fr C 1096.1 1287.8 40344 0.8885 0.9163 2534.5 0.9611
en-fr ParFDA5 22.6 26.6 1008 0.8735 0.8412 737.4 0.9491
fr-en C 1287.8 1096.1 40344 0.9163 0.8885 6255.8 0.9675
fr-en ParFDA5 20.9 19.3 1008 0.8963 0.7845 463.4 0.9282
en-hi C 3.4 5.0 306 0.5467 0.5986 36.3 0.7972
en-hi ParFDA5 3.3 4.9 254 0.5467 0.5976 41.2 0.8115
hi-en C 5.0 3.4 306 0.5986 0.5467 5350.4 0.9473
hi-en ParFDA5 5.0 3.3 284 0.5985 0.5466 966.8 0.9209
en-ru C 49.6 46.1 2531 0.7992 0.6823 590.8 0.8679
en-ru ParFDA5 19.6 18.6 1107 0.7991 0.6388 282.1 0.8447
ru-en C 46.1 49.6 2531 0.6823 0.7992 5380.6 0.9567
ru-en ParFDA5 16.6 19.4 1107 0.6821 0.7586 225.1 0.9009
Table 2: The data statistics for the available training and LM corpora for the constrained (C) submissions
compared with the ParFDA5 selected training and LM corpora statistics. #words is in millions (M) and
#sents is in thousands (K).
S ? T d c s i l
T
r
a
i
n
i
n
g
,
n
=
2
en-de 1.0 0.5817 1.4176 5.0001 -3.154
de-en 1.0 1.0924 1.3604 5.0001 -4.341
en-cs 1.0 0.0676 0.8299 5.0001 -0.8788
cs-en 1.0 1.5063 0.7777 3.223 -2.3824
en-ru 1.0 0.6519 1.6877 5.0001 -1.1888
ru-en 1.0 1.607 3.0001 0.0 -1.8247
en-hi 1.0 3.0001 3.0001 1.5701 -1.5699
hi-en 1.0 0.0 1.1001 5.0001 -0.8264
en-fr 1.0 0.8143 0.801 3.5996 -1.3394
fr-en 1.0 0.19 1.0106 5.0001 1.238
L
M
,
n
=
1
en-de 1.0 0.1924 1.0487 5.0001 4.9404
de-en 1.0 1.7877 3.0001 3.1213 -0.4147
en-cs 1.0 0.4988 1.1586 5.0001 -5.0001
cs-en 0.9255 0.2787 0.7439 3.7264 -2.0564
en-ru 1.0 1.4419 2.239 1.5543 -0.5097
ru-en 1.0 2.4844 3.0001 4.6669 3.7978
en-hi 1.0 0.0 0.0 5.0001 -4.944
hi-en 1.0 0.3053 3.0001 5.0001 4.1216
en-fr 1.0 3.0001 2.0452 3.0229 3.4364
fr-en 1.0 0.7467 0.7641 5.0001 5.0001
Table 1: Optimized ParFDA5 parameters for se-
lecting the training set using 2-grams or the LM
corpus using 1-grams.
max-fertility is set to 10, with the number of itera-
tions set to 7,3,5,5,7 for IBM models 1,2,3,4, and
the HMM model and 70 word classes are learned
over 3 iterations with the mkcls tool during train-
ing. The development set contains 5000 sentences,
2000 of which are randomly sampled from pre-
vious years? development sets (2008-2012) and
3000 come from the development set for WMT14.
3.1 Optimized ParFDA5 Parameters
Table 1 presents the optimized ParFDA5 parame-
ters obtained using the development set. Transla-
tion direction specific differences are visible. A
negative value for l shows that FDA5 prefers
shorter features, which we observe mainly when
the target language is English. We also observe
higher exponential decay rates when the target lan-
guage is mainly English. For optimizing the pa-
rameters for selecting LM corpus instances, we
still use a parallel corpus and instead of optimiz-
ing for TCOV, we optimize for SCOV such that
we select instances that are relevant for the target
training corpus but still maximize the coverage of
source features and be able to represent the source
sentences within a translation task. The selected
LM corpus is prepared for a translation task.
3.2 Data Selection
We select the same number of sentences with Par-
allel FDA (Bic?ici, 2013), which is roughly 15%
of the training corpus for en-de, 35% for ru-en,
6% for cs-en, and 2% for en-fr. After the training
set selection, we select the LM data using the tar-
get side of the training set as the target domain to
select LM instances for. For en and fr, we have
access to the LDC Gigaword corpora (Parker et
al., 2011; Graff et al., 2011), from which we ex-
tract only the story type news. We select 15 mil-
lion sentences for each LM not including the se-
61
S ? T
Time (Min) Space (MB)
ParFDA5 Moses
Overall
Moses
Train LM Total Train Tune Total PT LM ALL
en-cs 5 28 34 375 702 1162 1196 1871 5865 19746
cs-en 7 65 72 358 448 867 939 1808 4906 18650
en-de 8 29 38 302 1059 1459 1497 1676 2923 18313
de-en 8 85 93 358 474 924 1017 1854 5219 19247
en-fr 23 60 84 488 781 1372 1456 2309 9577 24362
fr-en 21 99 120 315 490 897 1017 1845 4888 17466
en-hi 2 9 11 91 366 511 522 269 817 4292
hi-en 1 36 37 91 330 467 504 285 9697 3845
en-ru 11 25 35 358 369 837 872 2174 4770 21283
ru-en 10 62 71 309 510 895 966 1939 2735 19537
Table 3: The space and time required for building the ParFDA5 Moses SMT systems. The sizes are in
MB and time in minutes. PT stands for the phrase table. ALL does not contain the size of the LM.
BLEUc
S ? en en? T
cs-en de-en fr-en hi-en ru-en en-cs en-de en-fr en-hi en-ru
WMT14C 0.288 0.28 0.35 0.139 0.318 0.21 0.201 0.358 0.111 0.287
ParFDA5 0.256 0.239 0.319 0.105 0.282 0.172 0.168 0.325 0.07 0.257
diff 0.032 0.041 0.031 0.034 0.036 0.038 0.033 0.033 0.041 0.03
LM order 9 9 9 9 9 9 9 7 10 9
Table 4: BLEUc for the top constrained result in WMT14 (WMT14C) and for ParFDA5 results, their
difference to WMT14C, and the LM order used are presented. Average difference is 3.49 BLEU points.
lected training set, which is added later. The statis-
tics of the ParFDA5 selected training data and the
available training data for the constrained transla-
tion task is given in Table 2. The size of the LM
corpora includes both the LDC and the monolin-
gual LM corpora provided by WMT14. Table 2
shows the significant size differences between the
constrained dataset (C) and the ParFDA5 selected
data. Table 2 also present the source and target
coverage (SCOV and TCOV) in terms of the 2-
grams of the test set observed in the training data
or the LM data. The quality of the training cor-
pus can be measured by TCOV, which is found to
correlate well with the BLEU performance achiev-
able (Bic?ici and Yuret, 2011; Bic?ici, 2011).
3.3 Computing Statistics
We quantify the time and space requirements for
running ParFDA5 SMT systems for each trans-
lation direction. The space and time required
for building the ParFDA5 Moses SMT systems
are given in Table 3 where the sizes are in MB
and the time in minutes. PT stands for the
phrase table. We used Moses version 2.1.1, from
www.statmt.org/moses. Building a ParFDA5
Moses SMT system takes about half a day.
3.4 Translation Results
The results of our two ParFDA5 SMT experiments
for each language pair and their tokenized BLEU
performance, BLEUc, together with the LM order
used and the top constrained submissions to the
WMT14 are given in Table 4
3
, which use phrase-
based Moses for comparison
4
. We observed sig-
nificant gains (+0.23 BLEU points) using higher
order LMs last year (Bic?ici, 2013) and therefore
we use LMs of order 7 to 10. The test set con-
tains 10,000 sentences and only 3000 of which are
used for evaluation, which can make the transduc-
tive learning application of ParFDA5 harder. In
the transductive learning setting, ParFDA5 is se-
lecting target test task specific SMT resources and
therefore, having irrelevant instances in the test set
may decrease the performance by causing FDA5
to select more domain specific data and less task
specific. ParFDA5 significantly reduces the time
required for training, development, and deploy-
ment of an SMT system for a given translation
3
We use the results from matrix.statmt.org.
4
Phrase-based Moses systems usually rank in the top 3.
62
ppl
OOV log OOV = ?19 log OOV = ?11
Translation T order train FDA5 FDA5 LM % red. train FDA5 FDA5 LM % red. train FDA5 FDA5 LM % red.
en-cs en
3
866 1205 525 0.39
1764 1731 938 0.47 1370 1218 805 0.41
4 1788 1746 877 0.51 1389 1229 753 0.46
5 1799 1752 868 0.52 1398 1233 745 0.47
6 1802 1753 867 0.52 1400 1234 744 0.47
cs-en cs
3
557 706 276 0.5
480 419 333 0.31 408 342 307 0.25
4 487 422 292 0.4 415 344 269 0.35
5 495 424 285 0.42 421 346 263 0.38
6 497 425 284 0.43 423 346 262 0.38
en-de en
3
1666 2116 744 0.55
1323 1605 747 0.44 831 890 607 0.27
4 1307 1596 689 0.47 821 885 560 0.32
5 1307 1596 680 0.48 822 885 553 0.33
6 1308 1596 679 0.48 822 885 552 0.33
de-en de
3
691 849 417 0.4
482 498 394 0.18 386 379 345 0.11
4 470 490 344 0.27 376 373 301 0.2
5 470 490 336 0.29 377 373 293 0.22
6 471 490 334 0.29 377 373 292 0.23
en-fr en
3
270 411 153 0.43
185 167 173 0.07 173 151 166 0.04
4 170 160 135 0.21 159 144 130 0.19
5 171 160 126 0.27 160 145 121 0.24
fr-en fr
3
306 604 179 0.42
349 325 275 0.21 320 275 261 0.19
4 338 321 235 0.3 310 271 224 0.28
5 342 322 228 0.33 314 272 217 0.31
en-hi en
3
2035 2123 950 0.53
242 246 114 0.53 168 168 96 0.43
4 237 241 87 0.63 164 165 73 0.55
5 238 242 78 0.67 165 165 66 0.6
6 239 242 75 0.68 165 165 64 0.62
hi-en hi
3
1842 1860 623 0.66
1894 1898 482 0.75 915 911 377 0.59
4 1910 1914 398 0.79 923 919 312 0.66
5 1915 1919 378 0.8 925 921 296 0.68
6 1915 1919 378 0.8 926 921 296 0.68
en-ru en
3
959 1176 585 0.39
1067 1171 668 0.37 814 840 566 0.3
4 1053 1159 603 0.43 803 831 511 0.36
5 1052 1159 591 0.44 802 831 501 0.38
6 1052 1159 588 0.44 802 831 498 0.38
ru-en ru
3
558 689 340 0.39
385 398 363 0.06 334 334 333 0.0
4 377 391 325 0.14 327 328 298 0.09
5 378 392 318 0.16 328 329 292 0.11
6 378 392 318 0.16 328 329 291 0.11
Table 5: Perplexity comparison of the LM built from the training corpus (train), ParFDA5 selected
training corpus (FDA5), and the ParFDA5 selected LM corpus (FDA5 LM). % red. column lists the
percentage of reduction.
task. The average difference to the top constrained
submission in WMT14 is 3.49 BLEU points. For
en-ru and en-cs, true-casing the LM using a true-
caser trained on all of the available training data
decreased the performance by 0.5 and 0.9 BLEU
points respectively and for cs-en and fr-en, in-
creased the performance by 0.2 and 0.5 BLEU
points. We use the true-cased LM results using
a true-caser trained on all of the available train-
ing data for all language pairs where for hi-en,
the true-caser is trained on the ParFDA5 selected
training data.
3.5 LM Data Quality
A LM training data selected for a given transla-
tion task allows us to train higher order language
models, model longer range dependencies better,
and at the same time, achieve lower perplexity
as given in Table 5. We compare the perplexity
of the ParFDA5 selected LM with a LM trained
on the ParFDA5 selected training data and a LM
trained using all of the available training corpora.
To be able to compare the perplexities, we take
the OOV tokens into consideration during calcu-
lations (Bic?ici, 2013). We present results for the
cases when we handle OOV words with a cost
of ?19 or ?11 each in Table 5. We are able to
achieve significant reductions in the number of
OOV tokens and the perplexity, reaching up to
66% reduction in the number of OOV tokens and
up to 80% reduction in the perplexity.
63
BLEUc
S ? en en? T
cs-en de-en fr-en ru-en en-cs en-de en-fr en-ru
ParFDA5 0.256 0.239 0.319 0.282 0.172 0.168 0.325 0.257
ParFDA 0.243 0.241 0.254 0.223 0.171 0.179 0.238 0.173
diff 0.013 -0.002 0.065 0.059 0.001 -0.011 0.087 0.084
Table 7: Parallel FDA5 WMT14 results compared with parallel FDA WMT13 results. Training set sizes
are given in millions (M) of words on the target side. Average difference is 3.7 BLEU points.
BLEUc
S ? en en? T
cs-en fr-en en-cs en-fr
ParFDA5 0.256 0.319 0.172 0.325
ParFDA5 15% 0.248 0.321 0.178 0.333
diff -0.008 0.002 0.006 0.008
Table 6: ParFDA5 results, ParFDA5 results using
15% of the training set, and their difference.
3.6 Using 15% of the Available Training Set
In the FDA5 results (Bic?ici and Yuret, 2014),
we found that selecting 15% of the best train-
ing set size maximizes the performance for the
English-German out-of-domain translation task
and achieves 0.41 BLEU points improvement over
a baseline system using all of the available train-
ing data. We run additional experiments select-
ing 15% of the training data for fr-en and cs-en
language pairs to see the effect of increased train-
ing sets selected with ParFDA5. The results are
given in Table 6 where most of the results improve.
The slight performance decrease for cs-en may be
due to using a true-caser trained on only the se-
lected training data. We observe larger gains in
the en? T translations.
3.7 ParFDA5 versus Parallel FDA
We compare this year?s results with the results
we obtained last year (Bic?ici, 2013) in Table 7.
The task setting is different in WMT14 since the
test set contains 10,000 sentences but only 3000
of these are used as the actual test set, which
can make the transductive learning application of
ParFDA5 harder. We select the same number of
instances for the training sets but 5 million more
instances for the LM corpus this year. The aver-
age difference to the top constrained submission
in WMT13 was 2.88 BLEU points (Bic?ici, 2013)
and this has increased to 3.49 BLEU points in
WMT14. On average, the performance improved
3.7 BLEU points when compared with ParFDA re-
sults last year. For the fr-en, en-fr, and en-ru trans-
lation directions, we observe increases in the per-
formance. This may be due to better modeling of
the target domain by better parameterization and
optimization that FDA5 is providing. We observe
some decrease in the performance in en-de and de-
en results. Since the training material remained
the same for WMT13 and WMT14 and the mod-
eling power of FDA5 increased, building a domain
specific rather than a task specific ParFDA5 model
may be the reason for the decrease.
4 Conclusion
We use parallel FDA5 for solving computational
scalability problems caused by the abundance of
training data for SMT models and LMs and still
achieve SMT performance that is on par with
the top performing SMT systems. Parallel FDA5
raises the bar of expectations from SMT with
highly accurate translations and lower the bar to
entry for SMT into new domains and tasks by al-
lowing fast deployment of SMT systems in about
half a day. Parallel FDA5 enables a shift from gen-
eral purpose SMT systems towards task adaptive
SMT solutions.
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University and in part by the
European Commission through the QTLaunchPad
FP7 project (No: 296347). We also thank the
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational
facilities and support.
References
Ergun Bic?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283, Ed-
64
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ond?rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Iacer Calixto, Ali Hosseinzadeh Vahid, Xiaojun Zhang,
Jian Zhang, Xiaofeng Wu, Andy Way, and Qun Liu.
2014. Experiments in medical translation shared
task at wmt 2014. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Baltimore,
USA, June. Association for Computational Linguis-
tics.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2009. Introduction to
Algorithms (3. ed.). MIT Press.
David Graff,
?
Angelo Mendonc?a, and Denise DiPersio.
2011. French Gigaword third edition, Linguistic
Data Consortium.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180, Prague, Czech Republic, June.
William Lewis, Robert Munro, and Stephan Vogel.
2011. Crisis mt: Developing a cookbook for mt
in crisis situations. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
501?511, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Liangyou Li, Xiaofeng Wu, Santiago Cortes Vaillo,
Jun Xie, Jia Xu, Andy Way, and Qun Liu. 2014.
The dcu-ictcas-tsinghua mt system at wmt 2014 on
german-english translation task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, USA, June. Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, pages 901?904.
65
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313?321,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Referential Translation Machines for Predicting Translation Quality
Ergun Bic?ici
Centre for Next Generation Localisation
School of Computing
Dublin City University, Dublin, Ireland.
ergun.bicici@computing.dcu.ie
Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University, Dublin, Ireland.
away@computing.dcu.ie
Abstract
We use referential translation machines
(RTM) for quality estimation of translation
outputs. RTMs are a computational model
for identifying the translation acts between
any two data sets with respect to interpre-
tants selected in the same domain, which
are effective when making monolingual
and bilingual similarity judgments. RTMs
achieve top performance in automatic, ac-
curate, and language independent predic-
tion of sentence-level and word-level sta-
tistical machine translation (SMT) qual-
ity. RTMs remove the need to access any
SMT system specific information or prior
knowledge of the training data or models
used when generating the translations and
achieve the top performance in WMT13
quality estimation task (QET13). We im-
prove our RTM models with the Parallel
FDA5 instance selection model, with ad-
ditional features for predicting the trans-
lation performance, and with improved
learning models. We develop RTM mod-
els for each WMT14 QET (QET14) sub-
task, obtain improvements over QET13 re-
sults, and rank 1st in all of the tasks and
subtasks of QET14.
1 Introduction
We use referential translation machines (RTM) for
quality estimation of translation outputs, which is
a computational model for identifying the acts of
translation for translating between any given two
data sets with respect to a reference corpus se-
lected in the same domain. RTMs reduce our de-
pendence on any task dependent resource. Predic-
tion of translation quality is important because the
expected translation performance can help in esti-
mating the effort required for correcting the trans-
lations during post-editing by human translators.
Bicici et al. (2013) develop the Machine Trans-
lation Performance Predictor (MTPP), a state-of-
the-art, language independent, and SMT system
extrinsic machine translation performance predic-
tor, which can predict translation quality by look-
ing at the test source sentences and becomes the
2nd overall after also looking at the translation
outputs as well in QET12 (Callison-Burch et al.,
2012). RTMs achieve the top performance in
QET13 (Bojar et al., 2013), ranking 1st or 2nd in
all of the subtasks. RTMs rank 1st in all of the
tasks and subtasks of QET14 (Bojar et al., 2014).
Referential translation models (Section 2)
present an accurate and language independent so-
lution for predicting the performance of natural
language tasks such as the quality estimation of
translation. We improve our RTM models (Bic?ici,
2013) by:
? using a parameterized, fast implementation
of FDA, FDA5, and our Parallel FDA5 in-
stance selection model (Bic?ici et al., 2014),
? better modeling of the language in which
similarity judgments are made with improved
optimization and selection of the LM data,
? increased feature set for also modeling the
structural properties of sentences,
? extended learning models.
2 Referential Translation Machine
(RTM)
Referential translation machines provide a compu-
tational model for quality and semantic similarity
judgments in monolingual and bilingual settings
using retrieval of relevant training data (Bic?ici,
2011; Bic?ici and Yuret, 2014) as interpretants for
reaching shared semantics (Bic?ici, 2008). RTMs
achieve top performance when predicting the qual-
ity of translations in QET14 and QET13 (Bic?ici,
313
2013), top performance when predicting mono-
lingual cross-level semantic similarity (Jurgens
et al., 2014), good performance when evaluat-
ing the semantic relatedness of sentences and
their entailment (Marelli et al., 2014), and a
language independent solution and good perfor-
mance when judging the semantic similarity of
sentences (Agirre et al., 2014; Bic?ici and Way,
2014).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference
corpus selected in the same domain. An RTM
model is based on the selection of interpretants,
data close to both the training set and the test set,
which allow shared semantics by providing con-
text for similarity judgments. In semiotics, an in-
terpretant I interprets the signs used to refer to the
real objects (Bic?ici, 2008). Each RTM model is
a data translation model between the instances in
the training set and the test set. We use the Parallel
FDA5 (Feature Decay Algorithms) instance selec-
tion model for selecting the interpretants (Bic?ici
et al., 2014; Bic?ici and Yuret, 2014) this year,
which allows efficient parameterization, optimiza-
tion, and implementation of FDA, and build an
MTPP model (Section 2.1). We view that acts of
translation are ubiquitously used during commu-
nication:
Every act of communication is an act of
translation (Bliss, 2012).
Given a training set train, a test set test, and
some corpus C, preferably in the same domain as
the training and test sets, the RTM steps are:
1. FDA5(train,test, C)? I
2. MTPP(I,train)? F
train
3. MTPP(I,test)? F
test
4. learn(M,F
train
)?M
5. predict(M,F
test
)? q?
Step 1 selects the interpretants, I, relevant to both
the training and test data. Steps 2 and 3 use I
to map train and test to a new space where
similarities between translation acts can be derived
more easily. Step 4 trains a learning modelM over
the training features, F
train
, and Step 5 obtains
the predictions. RTM relies on the representative-
ness of I as a medium for building data translation
models between train and test.
Our encouraging results in QET provides a
greater understanding of the acts of translation we
ubiquitously use and how they can be used to pre-
dict the performance of translation and judging the
semantic similarity between text. RTM and MTPP
models are not data or language specific and their
modeling power and good performance are appli-
cable in different domains and tasks.
2.1 The Machine Translation Performance
Predictor (MTPP)
MTPP (Bic?ici et al., 2013) is a state-of-the-art and
top performing machine translation performance
predictor, which uses machine learning models
over features measuring how well the test set
matches the training set to predict the quality of
a translation without using a reference translation.
2.2 MTPP Features for Translation Acts
MTPP measures the coverage of individual test
sentence features found in the training set and
derives indicators of the closeness of test sen-
tences to the available training data, the difficulty
of translating the sentence, and the presence of
acts of translation for data transformation. Fea-
ture functions use statistics involving the training
set and the test sentences to determine their close-
ness. Since they are language independent, MTPP
allows quality estimation to be performed extrin-
sically. MTPP uses n-gram features defined over
text or common cover link (CCL) (Seginer, 2007)
structures as the basic units of information over
which similarity calculations are made. Unsuper-
vised parsing with CCL extracts links from base
words to head words, representing the grammati-
cal information instantiated in the training and test
data.
We extend the MTPP model we used last
year (Bic?ici, 2013) in its learning module and the
features included. Categories for the features (S
for source, T for target) used are listed below
where the number of features are given in brackets
for S and T, {#S, #T}, and the detailed descriptions
for some of the features are presented in (Bic?ici et
al., 2013). The number of features for each task
differs since we perform an initial feature selection
step on the tree structural features (Section 2.3).
The number of features are in the range 337?437.
? Coverage {56, 54}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Perplexity {45, 45}: Measures the fluency of
the sentences according to language models
314
(LM). We use both forward ({30}) and back-
ward ({15}) LM features for S and T.
? TreeF {0, 10-110}: 10 base features and up
to 100 selected features of T among parse tree
structures (Section 2.3).
? Retrieval Closeness {16, 12}: Measures the
degree to which sentences close to the test set
are found in the selected training set, I, using
FDA (Bic?ici and Yuret, 2011a) and BLEU,
F
1
(Bic?ici, 2011), dice, and tf-idf cosine sim-
ilarity metrics.
? IBM2 Alignment Features {0, 22}: Calcu-
lates the sum of the entropy of the dis-
tribution of alignment probabilities for S
(
?
s?S
?p log p for p = p(t|s) where s and
t are tokens) and T, their average for S and
T, the number of entries with p ? 0.2 and
p ? 0.01, the entropy of the word align-
ment between S and T and its average, and
word alignment log probability and its value
in terms of bits per word. We also com-
pute word alignment percentage as in (Ca-
margo de Souza et al., 2013) and potential
BLEU, F
1
, WER, PER scores for S and T.
? IBM1 Translation Probability {4, 12}: Cal-
culates the translation probability of test
sentences using the selected training set,
I (Brown et al., 1993).
? Feature Vector Similarity {8, 8}: Calculates
similarities between vector representations.
? Entropy {2, 8}: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences (Bic?ici et al.,
2013).
? Length {6, 3}: Calculates the number of
words and characters for S and T and their
average token lengths and their ratios.
? Diversity {3, 3}: Measures the diversity of
co-occurring features in the training set.
? Synthetic Translation Performance {3, 3}:
Calculates translation scores achievable ac-
cording to the n-gram coverage.
? Character n-grams {5}: Calculates cosine
between character n-grams (for n=2,3,4,5,6)
obtained for S and T (B?ar et al., 2012).
? Minimum Bayes Retrieval Risk {0, 4}: Cal-
culates the translation probability for the
translation having the minimum Bayes risk
among the retrieved training instances.
? Sentence Translation Performance {0, 3}:
Calculates translation scores obtained ac-
cording to q(T,R) using BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), or
F
1
(Bic?ici and Yuret, 2011b) for q.
? LIX {1, 1}: Calculates the LIX readability
score (Wikipedia, 2013; Bj?ornsson, 1968) for
S and T.
1
For Task 1.1, we have additionally used com-
parative BLEU, NIST, and F
1
scores as additional
features, which are obtained by comparing the
translations with each other and averaging the re-
sult (Bic?ici, 2011).
2.3 Bracketing Tree Structural Features
We use the parse tree outputs obtained by CCL
to derive features based on the bracketing struc-
ture. We derive 5 statistics based on the geometric
properties of the parse trees: number of brackets
used (numB), depth (depthB), average depth (avg
depthB), number of brackets on the right branches
over the number of brackets on the left (R/L)
2
, av-
erage right to left branching over all internal tree
nodes (avg R/L). The ratio of the number of right
to left branches shows the degree to which the sen-
tence is right branching or not. Additionally, we
capture the different types of branching present
in a given parse tree identified by the number of
nodes in each of its children.
Table 1 depicts the parsing output obtained by
CCL for the following sentence from WSJ23
3
:
Many fund managers argue that now ?s the time
to buy .
We use Tregex (Levy and Andrew, 2006) for vi-
sualizing the output parse trees presented on the
left. The bracketing structure statistics and fea-
tures are given on the right hand side. The root
node of each tree structural feature represents the
number of times that feature is present in the pars-
ing output of a document.
3 RTM in the Quality Estimation Task
We participate in all of the four challenges of the
quality estimation task (QET) (Bojar et al., 2014),
which include English to Spanish (en-es), Span-
ish to English (es-en), English to German (en-
de), and German to English (de-en) translation di-
rections. There are two main categories of chal-
lenges: sentence-level prediction (Task 1.*) and
1
LIX=
A
B
+ C
100
A
, where A is the number of words, C is
words longer than 6 characters, B is words that start or end
with any of ?.?, ?:?, ?!?, ??? similar to (Hagstr?om, 2012).
2
For nodes with uneven number of children, the nodes in
the odd child contribute to the right branches.
3
Wall Street Journal (WSJ) corpus section 23, distributed
with Penn Treebank version 3 (Marcus et al., 1993).
315
CCL
numB depthB avg depthB R/L avg R/L
24.0 9.0 0.375 2.1429 3.401
2
1 1
1
1 13
1
1 2
1
1 8
1
2 10
1
3 1
1
3 4
1
5 1
1
7 15
Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).
word-level prediction (Task 2). Task 1.1 is about
predicting post-editing effort (PEE), Task 1.2 is
about predicting HTER (human-targeted transla-
tion edit rate) (Snover et al., 2006) scores of trans-
lations, Task 1.3 is about predicting post-editing
time (PET), and Task 2 is about binary, ternary, or
multi-class classification of word-level quality.
For each task, we develop individual RTM mod-
els using the parallel corpora and the LM corpora
distributed by the translation task (WMT14) (Bo-
jar et al., 2014) and the LM corpora provided by
LDC for English (Parker et al., 2011) and Span-
ish (
?
Angelo Mendonc?a, 2011)
4
. The parallel cor-
pora contain 4.5M sentences for de-en with 110M
words for de and 116M words for en and 15.1M
sentences for en-es with 412M words for en and
462M words for es. We do not use any resources
provided by QET including data, software, or
baseline features. Instance selection for the train-
ing set and the language model (LM) corpus is
handled by parallel FDA5 (Bic?ici et al., 2014),
whose parameters are optimized for each transla-
tion task. LM are trained using SRILM (Stolcke,
2002). We tokenize and true-case all of the cor-
pora. The true-caser is trained on all of the avail-
able training corpus using Moses (Koehn et al.,
2007). Table 2 lists the number of sentences in
the training and test sets for each task.
For each task or subtask, we select 375 thousand
(K) training instances from the available parallel
training corpora as interpretants for the individual
RTM models using parallel FDA5. We add the
selected training set to the 3 million (M) sentences
selected from the available monolingual corpora
for each LM corpus. The statistics of the training
data selected by and used as interpretants in the
4
English Gigaword 5th, Spanish Gigaword 3rd edition.
Task Train Test
Task 1.1 (en-es) 3816 600
Task 1.1 (es-en) 1050 450
Task 1.1 (en-de) 1400 600
Task 1.1 (de-en) 1050 450
Task 1.2 (en-es) 896 208
Task 1.3 (en-es) 650 208
Task 2 (en-es) 1957 382
Task 2 (es-en) 900 150
Task 2 (en-de) 715 150
Task 2 (de-en) 350 100
Table 2: Number of sentences in different tasks.
RTM models is given in Table 3. The details of
instance selection with parallel FDA5 are provided
in (Bic?ici et al., 2014).
Task S T
Task 1.1 (en-es) 6.2 6.9
Task 1.1 (es-en) 7.9 7.4
Task 1.1 (en-de) 6.1 6
Task 1.1 (de-en) 6.9 6.4
Task 1.2 (en-es) 6.1 6.7
Task 1.3 (en-es) 6.2 6.8
Task 2 (en-es) 6.2 6.8
Task 2 (es-en) 7.5 7
Task 2 (en-de) 5.9 5.9
Task 2 (de-en) 6.3 6.8
Table 3: Number of words in I (in millions) se-
lected for each task (S for source, T for target).
3.1 Learning Models and Optimization:
We use ridge regression (RR), support vector re-
gression (SVR) with RBF (radial basis functions)
kernel (Smola and Sch?olkopf, 2004), and ex-
316
Task Translation Model r RMSE MAE RAE
Task1.1
es-en FS-RR 0.3512 0.6394 0.5319 0.9114
es-en PLS-RR 0.3579 0.6746 0.5488 0.9405
en-de PLS-TREE 0.2922 0.7496 0.6223 0.9404
en-de TREE 0.2845 0.7485 0.6241 0.9431
en-es TREE 0.4485 0.619 0.45 0.9271
en-es PLS-TREE 0.4354 0.6213 0.4723 0.973
de-en RR 0.3415 0.7475 0.6245 0.9653
de-en PLS-RR 0.3561 0.7711 0.6236 0.9639
Task1.2
en-es SVR 0.4769 0.203 0.1378 0.8443
en-es TREE 0.4708 0.2031 0.1372 0.8407
Task1.3
en-es SVR 0.6974 21543 14866 0.6613
en-es RR 0.6991 21226 15325 0.6817
Table 4: Training performance of the top 2 individual RTM models prepared for different tasks.
tremely randomized trees (TREE) (Geurts et al.,
2006) as the learning models. TREE is an en-
semble learning method over randomized decision
trees. These models learn a regression function
using the features to estimate a numerical target
value. We also use these learning models after
a feature subset selection with recursive feature
elimination (RFE) (Guyon et al., 2002) or a di-
mensionality reduction and mapping step using
partial least squares (PLS) (Specia et al., 2009),
both of which are described in (Bic?ici et al., 2013).
We optimize the learning parameters, the num-
ber of features to select, the number of dimen-
sions used for PLS, and the parameters for paral-
lel FDA5. More detailed descriptions of the opti-
mization processes are given in (Bic?ici et al., 2013;
Bic?ici et al., 2014). We optimize the learning pa-
rameters by selecting ? close to the standard de-
viation of the noise in the training set (Bic?ici,
2013) since the optimal value for ? is shown to
have linear dependence to the noise level for dif-
ferent noise models (Smola et al., 1998). We select
the top 2 systems according to their performance
on the training set. For Task 2, we use both Global
Linear Models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) we developed last
year (Bic?ici, 2013). GLM relies on Viterbi de-
coding, perceptron learning, and flexible feature
definitions. GLMd extends the GLM framework
by parallel perceptron training (McDonald et al.,
2010) and dynamic learning with adaptive weight
updates in the perceptron learning algorithm:
w = w + ? (?(x
i
, y
i
)? ?(x
i
,
?
y)) , (1)
where ? returns a global representation for in-
stance i and the weights are updated by ?, which
dynamically decays the amount of the change dur-
ing weight updates at later stages and prevents
large fluctuations with updates.
3.2 Training Results
We use mean absolute error (MAE), relative
absolute error (RAE), root mean squared error
(RMSE), and correlation (r) to evaluate (Bic?ici,
2013). DeltaAvg (Callison-Burch et al., 2012) cal-
culates the average quality difference between the
top n ? 1 quartiles and the overall quality for the
test set. Table 4 provides the training results.
3.3 Test Results
Task 1.1: Predicting the Post-Editing Effort for
Sentence Translations: Task 1.1 is about pre-
dicting post-editing effort (PEE) and their rank-
ing. The results on the test set are given in Ta-
ble 5 where QuEst (Shah et al., 2013) SVR lists
the baseline system results. Rank lists the overall
ranking in the task out of about 10 submissions.
We obtain the rankings by sorting according to the
predicted scores and randomly assigning ranks in
case of ties. RTMs with SVR PLS learning is able
to achieve the top rank in this task.
Task 1.2: Predicting HTER of Sentence Trans-
lations Task 1.2 is about predicting HTER
(human-targeted translation edit rate) (Snover et
al., 2006), where case insensitive translation edit
rate (TER) scores obtained by TERp (Snover et
al., 2009) and their ranking. We derive features
over sentences that are true-cased. The results on
the test set are given in Table 6 where the ranks are
out of about 11 submissions. We are also able to
achieve the top ranking in this task.
317
Ranking Translations DeltaAvg r Rank
en-es
TREE 0.26 -0.41 1
PLS-TREE 0.26 -0.38 2
QuEst SVR 0.14 -0.22
es-en
PLS-RR 0.20 -0.35 2
FS-RR 0.19 -0.36 3
QuEst SVR 0.12 -0.21
en-de
TREE 0.39 -0.54 1
PLS-TREE 0.33 -0.42 2
QuEst SVR 0.23 -0.34
de-en
RR 0.38 -0.51 1
PLS-RR 0.35 -0.45 2
QuEst SVR 0.21 -0.25
Scoring Translations MAE RMSE Rank
en-es
TREE 0.49 0.61 1
PLS-TREE 0.49 0.61 2
QuEst SVR 0.52 0.66
es-en
FS-RR 0.53 0.64 1
PLS-RR 0.55 0.71 2
QuEst SVR 0.57 0.68
en-de
TREE 0.58 0.68 1
PLS-TREE 0.60 0.71 2
QuEst SVR 0.64 0.76
de-en
RR 0.55 0.67 1
PLS-RR 0.57 0.74 2
QuEst SVR 0.65 0.78
Table 5: RTM-DCU Task1.1 results on the test set
and baseline results.
Ranking Translations DeltaAvg r Rank
en-es
SVR 9.31 0.53 1
TREE 8.57 0.48 2
QuEst SVR 5.08 0.31
Scoring Translations MAE RMSE Rank
en-es
SVR 13.40 16.69 2
TREE 14.03 17.48 4
QuEst SVR 15.23 19.48
Table 6: RTM-DCU Task1.2 results on the test set
and baseline results.
Task 1.3: Predicting Post-Editing Time for Sen-
tence Translations Task 1.3 involves the predic-
tion of the post-editing time (PET) for a translator
to post-edit the MT output. The results on the test
set are given in Table 7 where the ranks are out of
about 10 submissions. RTMs become the top in all
metrics with RR and SVR learning models.
Task 2: Prediction of Word-level Translation
Quality Task 2 is about binary, ternary, or multi-
class classification of word-level quality. We de-
velop individual RTM models for each subtask and
use the GLM and GLMd learning models (Bic?ici,
2013), for predicting the quality at the word-level.
The features used are similar to last year?s (Bic?ici,
2013) and broadly categorized as CCL links, word
context based on surrounding words, word align-
ments, word lengths, word locations, word pre-
fixes and suffixes, and word forms (i.e. capital,
Ranking Translations DeltaAvg r Rank
en-es
RR 17.02 0.68 1
SVR 16.60 0.67 2
QuEst SVR 14.71 0.57
Scoring Translations MAE RMSE Rank
en-es
SVR 16.77 26.17 1
RR 17.50 25.97 7
QuEst SVR 21.49 34.28
Table 7: RTM-DCU Task1.3 results on the test set
and baseline results.
contains digit or punctuation).
The results on the test set are given in Table 8
where the ranks are out of about 8 submissions.
RTMs with GLM or GLMd learning becomes the
top this task as well.
Model
Binary Ternary Multi-class
wF
1
Rank wF
1
Rank wF
1
Rank
en-es
GLM 0.351 6 0.299 5 0.268 1
GLMd 0.329 7 0.266 6 0.032 7
es-en
GLM 0.269 2 0.220 2 0.087 1
GLMd 0.291 1 0.239 1 0.082 2
en-de
GLM 0.453 1 0.211 2 0.150 1
GLMd 0.369 2 0.219 1 0.125 2
en-es
GLM 0.261 1 0.083 2 0.024 2
GLMd 0.230 2 0.086 1 0.031 1
Table 8: RTM-DCU Task 2 results on the test set.
wF
1
is the average weighted F
1
score.
3.4 RTMs Across Tasks and Years
We compare the difficulty of tasks according to the
RAE levels achieved. RAE measures the error rel-
ative to the error when predicting the actual mean.
A high RAE is an indicator that the task is hard. In
Table 9, we list the test results including the RAE
obtained for different tasks and subtasks including
RTM results at QET13 (Bic?ici, 2013). The best
results are obtained for Task 1.3, which shows that
we can only reduce the error with respect to know-
ing and predicting the mean by about 28%.
4 Conclusion
Referential translation machines achieve top per-
formance in automatic, accurate, and language in-
dependent prediction of sentence-level and word-
level statistical machine translation (SMT) qual-
ity. RTMs remove the need to access any SMT
system specific information or prior knowledge of
the training data or models used when generating
the translations.
318
Task Translation Model r RMSE MAE RAE
Task1.1
es-en FS-RR 0.3285 0.6373 0.5308 0.9
es-en PLS-RR 0.3105 0.7124 0.5549 0.9409
en-de PLS-TREE 0.4427 0.7091 0.6028 0.8883
en-de TREE 0.5256 0.6788 0.5838 0.8602
en-es TREE 0.4087 0.6114 0.4938 1.0983
en-es PLS-TREE 0.4163 0.6084 0.4852 1.0794
de-en RR 0.5399 0.6735 0.5513 0.8204
de-en PLS-RR 0.4878 0.737 0.567 0.8437
Task1.2
en-es SVR 0.5499 0.1669 0.134 0.8532
en-es TREE 0.5175 0.1748 0.1403 0.8931
Task1.3
en-es SVR 0.6336 26174 16770 0.7223
en-es RR 0.6359 25966 17496 0.7536
QET13 Task1.1 en-es
PLS-SVR 0.5596 0.1683 0.1326 0.8849
SVR 0.5082 0.1728 0.1385 0.924
QET13 Task1.3 en-es
PLS-SVR 0.6752 86.62 49.62 0.6919
SVR 0.6682 90.36 49.21 0.6862
Table 9: Test performance of the top 2 individual RTM models prepared for different tasks and RTM
results from QET13 on similar tasks (Bic?ici, 2013).
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University and in part by the
European Commission through the QTLaunchPad
FP7 project (No: 296347). We also thank the
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational
facilities and support.
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics ? Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 435?440, Montr?eal,
Canada, 7-8 June. Association for Computational
Linguistics.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In SemEval-2014: Semantic Evaluation Exercises
- International Workshop on Semantic Evaluation,
Dublin, Ireland, 23-24 August.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 272?283,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system
for machine translation, system combination, and
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation, 27:171?192, December.
Ergun Bic?ici, Qun Liu, and Andy Way. 2014. Par-
allel FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, USA, June. Association for Compu-
tational Linguistics.
319
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Referential translation machines
for quality estimation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bj?ornsson. 1968. L?asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc. of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 1?44, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Ond?rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proc. of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Jos?e Guilherme Camargo de Souza, Christian Buck,
Marco Turchi, and Matteo Negri. 2013. FBK-
UEdin participation to the WMT13 quality estima-
tion shared task. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 352?
358, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389?422.
Kenth Hagstr?om. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180, Prague, Czech Republic, June.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. In Proceedings of the fifth interna-
tional conference on Language Resources and Eval-
uation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313?330, June.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. SemEval-2014 Task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland, August.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464, Los Angeles, California,
June. Association for Computational Linguistics.
320
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, and
Lucia Specia. 2013. Quest - design, implementation
and extensions of a framework for machine transla-
tion quality estimation. Prague Bull. Math. Linguis-
tics, 100:19?30.
Alex J. Smola and Bernhard Sch?olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
A. J. Smola, N. Murata, B. Sch?olkopf, and K.-R.
M?uller. 1998. Asymptotically optimal choice of
?-loss for support vector machines. In L. Niklas-
son, M. Boden, and T. Ziemke, editors, Proceedings
of the International Conference on Artificial Neural
Networks, Perspectives in Neural Computing, pages
105?110, Berlin. Springer.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas,.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 259?268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Con-
ference of the European Association for Machine
Translation (EAMT), pages 28?35, Barcelona, May.
EAMT.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, pages 901?904.
Wikipedia. 2013. Lix.
http://en.wikipedia.org/wiki/LIX.
David Graff Denise DiPersio
?
Angelo Mendonc?a,
Daniel Jaquette. 2011. Spanish Gigaword third edi-
tion, Linguistic Data Consortium.
321
