First Joint Conference on Lexical and Computational Semantics (*SEM), pages 441?448,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
TakeLab: Systems for Measuring Semantic Text Similarity
Frane S?aric?, Goran Glavas?, Mladen Karan,
Jan S?najder, and Bojana Dalbelo Bas?ic?
University of Zagreb
Faculty of Electrical Engineering and Computing
{frane.saric, goran.glavas, mladen.karan, jan.snajder, bojana.dalbelo}@fer.hr
Abstract
This paper describes the two systems for
determining the semantic similarity of short
texts submitted to the SemEval 2012 Task 6.
Most of the research on semantic similarity
of textual content focuses on large documents.
However, a fair amount of information is con-
densed into short text snippets such as social
media posts, image captions, and scientific ab-
stracts. We predict the human ratings of sen-
tence similarity using a support vector regres-
sion model with multiple features measuring
word-overlap similarity and syntax similarity.
Out of 89 systems submitted, our two systems
ranked in the top 5, for the three overall eval-
uation metrics used (overall Pearson ? 2nd
and 3rd, normalized Pearson ? 1st and 3rd,
weighted mean ? 2nd and 5th).
1 Introduction
Natural language processing tasks such as text clas-
sification (Sebastiani, 2002), text summarization
(Lin and Hovy, 2003; Aliguliyev, 2009), informa-
tion retrieval (Park et al, 2005), and word sense dis-
ambiguation (Schu?tze, 1998) rely on a measure of
semantic similarity of textual documents. Research
predominantly focused either on the document sim-
ilarity (Salton et al, 1975; Maguitman et al, 2005)
or the word similarity (Budanitsky and Hirst, 2006;
Agirre et al, 2009). Evaluating the similarity of
short texts such as sentences or paragraphs (Islam
and Inkpen, 2008; Mihalcea et al, 2006; Oliva et
al., 2011) received less attention from the research
community. The task of recognizing paraphrases
(Michel et al, 2011; Socher et al, 2011; Wan et
al., 2006) is sufficiently similar to reuse some of the
techniques.
This paper presents the two systems for auto-
mated measuring of semantic similarity of short
texts which we submitted to the SemEval-2012 Se-
mantic Text Similarity Task (Agirre et al, 2012). We
propose several sentence similarity measures built
upon knowledge-based and corpus-based similarity
of individual words as well as similarity of depen-
dency parses. Our two systems, simple and syn-
tax, use supervised machine learning, more specif-
ically the support vector regression (SVR), to com-
bine a large amount of features computed from pairs
of sentences. The two systems differ in the set of
features they employ.
Our systems placed in the top 5 (out of 89 sub-
mitted systems) for all three aggregate correlation
measures: 2nd (syntax) and 3rd (simple) for overall
Pearson, 1st (simple) and 3rd (syntax) for normal-
ized Pearson, and 2nd (simple) and 5th (syntax) for
weighted mean.
The rest of the paper is structured as follows. In
Section 2 we describe both knowledge-based and
corpus-based word similarity measures. In Section
3 we describe in detail the features used by our sys-
tems. In Section 4 we report the experimental results
cross-validated on the development set as well as the
official results on all test sets. Conclusions and ideas
for future work are given in Section 5.
2 Word Similarity Measures
Approaches to determining semantic similarity of
sentences commonly use measures of semantic sim-
441
ilarity between individual words. Our systems use
the knowledge-based and the corpus-based (i.e., dis-
tributional lexical semantics) approaches, both of
which are commonly used to measure the semantic
similarity of words.
2.1 Knowledge-based Word Similarity
Knowledge-based word similarity approaches rely
on a semantic network of words, such as Word-
Net. Given two words, their similarity can be esti-
mated by considering their relative positions within
the knowledge base hierarchy.
All of our knowledge-based word similarity mea-
sures are based on WordNet. Some measures use
the concept of a lowest common subsumer (LCS)
of concepts c1 and c2, which represents the lowest
node in the WordNet hierarchy that is a hypernym
of both c1 and c2. We use the NLTK library (Bird,
2006) to compute the PathLen similarity (Leacock
and Chodorow, 1998) and Lin similarity (Lin, 1998)
measures. A single word often denotes several con-
cepts, depending on its context. In order to compute
the similarity score for a pair of words, we take the
maximum similarity score over all possible pairs of
concepts (i.e., WordNet synsets).
2.2 Corpus-based Word Similarity
Distributional lexical semantics models determine
the meaning of a word through the set of all con-
texts in which the word appears. Consequently, we
can model the meaning of a word using its distribu-
tion over all contexts. In the distributional model,
deriving the semantic similarity between two words
corresponds to comparing these distributions. While
many different models of distributional semantics
exist, we employ latent semantic analysis (LSA)
(Turney and Pantel, 2010) over a large corpus to es-
timate the distributions.
For each word wi, we compute a vector xi using
the truncated singular value decomposition (SVD)
of a tf-idf weighted term-document matrix. The co-
sine similarity of vectors xi and xj estimates the
similarity of the corresponding words wi and wj .
Two different word-vector mappings were com-
puted by processing the New York Times Annotated
Corpus (NYT) (Sandhaus, 2008) and Wikipedia.
Aside from lowercasing the documents and remov-
ing punctuation, we perform no further preprocess-
Table 1: Evaluation of word similarity measures
Measure ws353 ws353-sim ws353-rel
PathLen 0.29 0.61 -0.05
Lin 0.33 0.64 -0.01
Dist (NYT) 0.50 0.50 0.51
Dist (Wikipedia) 0.62 0.66 0.55
ing (e.g., no stopwords removal or stemming). Upon
removing the words not occurring in at least two
documents, we compute the tf-idf. The word vec-
tors extracted from NYT corpus and Wikipedia have
a dimension of 200 and 500, respectively.
We compared the measures by computing the
Spearman correlation coefficient on the Word-
Sim3531 data set, as well as its similarity and re-
latedness subsets described in (Agirre et al, 2009).
Table 1 provides the results of the comparison.
3 Semantic Similarity of Sentences
Our systems use supervised regression with SVR as
a learning model, where each system exploits differ-
ent feature sets and SVR hyperparameters.
3.1 Preprocessing
We list all of the preprocessing steps our systems
perform. If a preprocessing step is executed by only
one of our systems, the system?s name is indicated
in parentheses.
1. All hyphens and slashes are removed;
2. The angular brackets (< and >) that enclose the
tokens are stripped (simple);
3. The currency values are simplified, e.g.,
$US1234 to $1234 (simple);
4. Words are tokenized using the Penn Treebank
compatible tokenizer;
5. The tokens n?t and ?m are replaced with not and
am, respectively (simple);
6. The two consecutive words in one sentence that
appear as a compound in the other sentence are
replaced by the said compound. E.g., cater pil-
lar in one sentence is replaced with caterpil-
lar only if caterpillar appears in the other sen-
tence;
1http://www.cs.technion.ac.il/?gabr/
resources/data/wordsim353/wordsim353.html
442
7. Words are POS-tagged using Penn Treebank
compatible POS-taggers: NLTK (Bird, 2006)
for simple, and OpenNLP2 for syntax;
8. Stopwords are removed using a list of 36 stop-
words (simple).
While we acknowledge that some of the prepro-
cessing steps we take may not be common, we did
not have the time to determine the influence of each
individual preprocessing step on the results to either
warrant their removal or justify their presence.
Since, for example, sub-par, sub par and subpar
are treated as equal after preprocessing, we believe it
makes our systems more robust to inputs containing
small orthographic differences.
3.2 Ngram Overlap Features
We use many features previously seen in paraphrase
classification (Michel et al, 2011). Several features
are based on the unigram, bigram, and trigram over-
lap. Before computing the overlap scores, we re-
move punctuation and lowercase the words. We con-
tinue with a detailed description of each individual
feature.
Ngram Overlap
Let S1 and S2 be the sets of consecutive ngrams
(e.g., bigrams) in the first and the second sentence,
respectively. The ngram overlap is defined as fol-
lows:
ngo(S1, S2) = 2 ?
(
|S1|
|S1 ? S2|
+
|S2|
|S1 ? S2|
)?1
(1)
The ngram overlap is the harmonic mean of the de-
gree to which the second sentence covers the first
and the degree to which the first sentence covers the
second. The overlap, defined by (1), is computed for
unigrams, bigrams, and trigrams.
Additionally we observe the content ngram over-
lap ? the overlap of unigrams, bigrams, and tri-
grams exclusively on the content words. The con-
tent words are nouns, verbs, adjectives, and adverbs,
i.e., the lemmas having one of the following part-of-
speech tags: JJ, JJR, JJS, NN, NNP, NNS, NNPS,
RB, RBR, RBS, VB, VBD, VBG, VBN, VBP, and
VBZ. Intuitively, the function words (prepositions,
2http://opennlp.apache.org/
conjunctions, articles) carry less semantics than con-
tent words and thus removing them might eliminate
the noise and provide a more accurate estimate of
semantic similarity.
In addition to the overlap of consecutive ngrams,
we also compute the skip bigram and trigram over-
lap. Skip-ngrams are ngrams that allow arbitrary
gaps, i.e., ngram words need not be consecutive in
the original sentence. By redefining S1 and S2 to
represent the sets of skip ngrams, we employ eq. (1)
to compute the skip-n gram overlap.
3.3 WordNet-Augmented Word Overlap
One can expect a high unigram overlap between very
similar sentences only if exactly the same words (or
lemmas) appear in both sentences. To allow for
some lexical variation, we use WordNet to assign
partial scores to words that are not common to both
sentences. We define the WordNet augmented cov-
erage PWN (?, ?):
PWN (S1, S2) =
1
|S2|
?
w1?S1
score(w1, S2)
score(w, S) =
?
?
?
1 if w ? S
max
w??S
sim(w,w?) otherwise
where sim(?, ?) represents the WordNet path length
similarity. The WordNet-augmented word over-
lap feature is defined as a harmonic mean of
PWN (S1, S2) and PWN (S2, S1).
Weighted Word Overlap
When measuring sentence similarities we give
more importance to words bearing more content, by
using the information content
ic(w) = ln
?
w??C freq(w
?
)
freq(w)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the cor-
pus. We use the Google Books Ngrams (Michel et
al., 2011) to obtain word frequencies because of its
excellent word coverage for English. Let S1 and S2
be the sets of words occurring in the first and second
sentence, respectively. The weighted word cover-
age of the second sentence by the first sentence is
443
given by:
wwc(S1, S2) =
?
w?S1?S2 ic(w)?
w??S2
ic(w?)
The weighted word overlap between two sen-
tences is calculated as the harmonic mean of the
wwc(S1, S2) and wwc(S2, S1).
This measure proved to be very useful, but it
could be improved even further. Misspelled frequent
words are more frequent than some correctly spelled
but rarely used words. Hence dealing with mis-
spelled words would remove the inappropriate heavy
penalty for a mismatch between correctly and incor-
rectly spelled words.
Greedy Lemma Aligning Overlap
This measure computes the similarity between
sentences using the semantic alignment of lem-
mas. First we compute the word similarity be-
tween all pairs of lemmas from the first and the
second sentence, using either the knowledge-based
or the corpus-based semantic similarity. We then
greedily search for a pair of most similar lemmas;
once the lemmas are paired, they are not considered
for further matching. Previous research by Lavie
and Denkowski (2009) proposed a similar alignment
strategy for machine translation evaluation. After
aligning the sentences, the similarity of each lemma
pair is weighted by the larger information content of
the two lemmas:
sim(l1, l2) = max(ic(l1), ic(l2)) ? ssim(l1, l2) (2)
where ssim(l1, l2) is the semantic similarity be-
tween lemmas l1 and l2.
The overall similarity between two sentences is
defined as the sum of similarities of paired lemmas
normalized by the length of the longer sentence:
glao(S1, S2) =
?
(l1,l2)?P sim(l1, l2)
max(length(S1), length(S2))
where P is the set of lemma pairs obtained by greedy
alignment. We take advantage of greedy align over-
lap in two features: one computes glao(?, ?) by us-
ing the Lin similarity for ssim(?, ?) in (2), while the
other feature uses the distributional (LSA) similarity
to calculate ssim(?, ?).
Vector Space Sentence Similarity
This measure is motivated by the idea of composi-
tionality of distributional vectors (Mitchell and La-
pata, 2008). We represent each sentence as a sin-
gle distributional vector u(?) by summing the dis-
tributional (i.e., LSA) vector of each word w in the
sentence S: u(S) =
?
w?S xw, where xw is the
vector representation of the word w. Another sim-
ilar representation uW (?) uses the information con-
tent ic(w) to weigh the LSA vector of each word
before summation: uW (S) =
?
w?S ic(w)xw.
The simple system uses |cos(u(S1), u(S2))| and
|cos(uW (S1), uW (S2))| for the vector space sen-
tence similarity features.
3.4 Syntactic Features
We use dependency parsing to identify the lemmas
with the corresponding syntactic roles in the two
sentences. We also compute the overlap of the de-
pendency relations of the two sentences.
Syntactic Roles Similarity
The similarity of the words or phrases having the
same syntactic roles in the two sentences may be in-
dicative of their overall semantic similarity (Oliva et
al., 2011). For example, two sentences with very dif-
ferent main predicates (e.g., play and eat) probably
have a significant semantic difference.
Using Lin similarity ssim(?, ?), we obtain the sim-
ilarity between the matching lemmas in a sentence
pair for each syntactic role. Additionally, for each
role we compute the similarity of the chunks that
lemmas belong to:
chunksim(C1, C2) =
?
l1?C1
?
l2?C2
ssim(l1, l2)
where C1 and C2 are the sets of chunks of
the first and second sentence, respectively. The
final similarity score of two chunks is the
harmonic mean of chunksim(C1, C2)/|C1| and
chunksim(C1, C2)/|C2| .
Syntactic roles that we consider are predicates (p),
subjects (s), direct (d), and indirect (i) (i.e., preposi-
tional) objects, where we use (o) to mean either (d)
or (i). The Stanford dependency parser (De Marn-
effe et al, 2006) produces the dependency parse of
the sentence. We infer (p), (s), and (d) from the syn-
tactic dependencies of type nsubj (nominal subject),
444
nsubjpass (nominal subject passive), and dobj (di-
rect object). By combining the prep and pobj de-
pendencies (De Marneffe and Manning, 2008), we
identify (i). Since the (d) in one sentence often se-
mantically corresponds to (i) in the other sentence,
we pair all (o) of one sentence with all (o) of the
other sentence and define object similarity between
the two sentences as the maximum similarity among
all (o) pairs. Because the syntactic role might be
absent from both sentences (e.g., the object in sen-
tences ?John sings? and ?John is thinking?), we in-
troduce additional binary features indicating if the
comparison for the syntactic role in question exists.
Many sentences (especially longer ones) have two
or more (p). In such cases it is necessary to align
the corresponding predicate groups (i.e., the (p) with
its corresponding arguments) between the two sen-
tences, while also aggregating the (p), (s), and (o)
similarities of all aligned (p) pairs. The similarity
of two predicate groups is defined as the sum of (p),
(s), and (o) similarities. In each iteration, the greedy
algorithm pairs all predicate groups of the first sen-
tence with all predicate groups of the second sen-
tence and searches for a pair with the maximum sim-
ilarity. Once the predicate groups of two sentences
have been aligned, we compute the (p) similarity as
a weighted sum of (p) similarities for each predicate
pair group. The weight of each predicate group pair
equals the larger information content of two predi-
cates. The (s) and (o) similarities are computed in
the same manner.
Syntactic Dependencies Overlap
Similar to the ngram overlap features, we measure
the overlap between sentences based on matching
dependency relations. A similar measure has been
proposed in (Wan et al, 2006). Two syntactic depen-
dencies are considered equal if they have the same
dependency type, governing lemma, and dependent
lemma. Let S1 and S2 be the set of all dependency
relations in the first and the second sentence, respec-
tively. Dependency overlap is the harmonic mean
between |S1 ? S2|/|S1| and |S1 ? S2|/|S2| . Con-
tent dependency overlap computes the overlap in the
same way, but considers only dependency relations
between content lemmas.
Similarly to weighted word overlap, we com-
pute the weighted dependency relations overlap.
The weighted coverage of the second sentence de-
pendencies with the first sentence dependencies is
given by:
wdrc(S1, S2) =
?
r?S1?S2 max(ic(g(r)), ic(d(r)))?
r?S2 max(ic(g(r)), ic(d(r)))
where g(r) is the governing word of the dependency
relation r, d(r) is the dependent word of the depen-
dency relation r, and ic(l) is the information con-
tent of the lemma l. Finally, the weighted depen-
dency relations overlap is the harmonic mean be-
tween wdrc(S1, S2) and wdrc(S2, S1).
3.5 Other Features
Although we primarily focused on developing the
ngram overlap and syntax-based features, some
other features significantly improve the performance
of our systems.
Normalized Differences
Our systems take advantage of the following fea-
tures that measure normalized differences in a pair
of sentences: (A) sentence length, (B) the noun
chunk, verb chunk, and predicate counts, and (C)
the aggregate word information content (see Nor-
malized differences in Table 2).
Numbers Overlap
The annotators gave low similarity scores to many
sentence pairs that contained different sets of num-
bers, even though their sentence structure was very
similar. Socher et al (2011) improved the perfor-
mance of their paraphrase classifier by adding the
following features that compare the sets of num-
bers N1 and N2 in two sentences: N1 = N2,
N1?N2 6= ?, and N1 ? N2?N2 ? N1. We replace
the first two features with log (1 + |N1|+ |N2|) and
2? |N1 ?N2|/(|N1|+ |N2|) . Additionally, the num-
bers that differ only in the number of decimal places
are treated as equal (e.g., 65, 65.2, and 65.234 are
treated as equal, whereas 65.24 and 65.25 are not).
Named Entity Features
Shallow NE similarity treats capitalized words as
named entities if they are longer than one character.
If a token in all caps begins with a period, it is clas-
sified as a stock index symbol. The simple system
445
Table 2: The usage of feature sets
Feature set simple syntax
Ngram overlap + +
Content-ngram overlap - +
Skip-ngram overlap - +
WordNet-aug. overlap + -
Weighted word overlap + +
Greedy align. overlap - +
Vector space similarity + -
Syntactic roles similarity - +
Syntactic dep. overlap - +
Normalized differences* A,C A,B
Shallow NERC + -
Full NERC - +
Numbers overlap + +
* See Section 3.5
uses the following four features: the overlap of cap-
italized words, the overlap of stock index symbols,
and the two features indicating whether these named
entities were found in either of the two sentences.
In addition to the overlap of capitalized words, the
syntax system uses the OpenNLP named entity rec-
ognizer and classifier to compute the overlap of en-
tities for each entity class separately. We recognize
the following entity classes: persons, organizations,
locations, dates, and rudimentary temporal expres-
sions. The absence of an entity class from both sen-
tences is indicated by a separate binary feature (one
feature for each class).
Feature Usage in TakeLab Systems
Some of the features presented in the previous sec-
tions were used by both of our systems (simple and
syntax), while others were used by only one of the
systems. Table 2 indicates the feature sets used for
the two submitted systems.
4 Results
4.1 Model Training
For each of the provided training sets we trained a
separate Support Vector Regression (SVR) model
using LIBSVM (Chang and Lin, 2011). To ob-
tain the optimal SVR parameters C, g, and p, our
systems employ a grid search with nested cross-
Table 3: Cross-validated results on train sets
MSRvid MSRpar SMTeuroparl
simple 0.8794 0.7566 0.7802
syntax 0.8698 0.7144 0.7308
validation. Table 3 presents the cross-validated per-
formance (in terms of Pearson correlation) on the
training sets. The models tested on the SMTnews
test set were trained on the SMTeuroparl train set.
For the OnWn test set, the syntax model was trained
on the MSRpar set, while the simple system?s model
was trained on the union of all train sets. The final
predictions were trimmed to a 0?5 range.
Our development results indicate that the
weighted word overlap, WordNet-augmented word
overlap, the greedy lemma alignment overlap, and
the vector space sentence similarity individually
obtain high correlations regardless of the devel-
opment set in use. Other features proved to be
useful on individual development sets (e.g., syntax
roles similarity on MSRvid and numbers overlap
on MSRpar). More research remains to be done in
thorough feature analysis and systematic feature
selection.
4.2 Test Set Results
The organizers provided five different test sets to
evaluate the performance of the submitted systems.
Table 4 illustrates the performance of our systems
on individual test sets, accompanied by their rank.
Our systems outperformed most other systems on
MSRvid, MSRpar, and OnWN sets (Agirre et al,
2012). However, they performed poorly on the
SMTeuroparl and SMTnews sets. While the corre-
lation scores on the MSRvid and MSRpar test sets
correspond to those obtained using cross-validation
on the corresponding train sets, the performance on
the SMT test sets is drastically lower than the cross-
validated performance on the corresponding train
set. The sentences in the SMT training set are signif-
icantly longer (30.4 tokens on average) than the sen-
tences in both SMT test sets (12.3 for SMTeuroparl
and 13.5 for SMTnews). Also there are several re-
peated pairs of extremely short and identical sen-
tences (e.g., ?Tunisia? ? ?Tunisia? appears 17 times
446
Table 4: Results on individual test sets
simple syntax
MSRvid 0.8803 (1) 0.8620 (8)
MSRpar 0.7343 (1) 0.6985 (2)
SMTeuroparl 0.4771 (26) 0.3612 (63)
SMTnews 0.3989 (46) 0.4683 (18)
OnWN 0.6797 (9) 0.7049 (6)
Table 5: Aggregate performance on the test sets
All ALLnrm Mean
simple 0.8133 (3) 0.8635 (1) 0.6753 (2)
syntax 0.8138 (2) 0.8569 (3) 0.6601 (5)
in the SMTeuroparl test set). The above measure-
ments indicate that the SMTeuroparl training set was
not representative of the SMTeuroparl test set for our
choice of features.
Table 5 outlines the aggregate performance of our
systems according to the three aggregate evaluation
measures proposed for the task (Agirre et al, 2012).
Both systems performed very favourably compared
to the other systems, achieving very high rankings
regardless of the aggregate evaluation measure.
The implementation of simple system is available
at http://takelab.fer.hr/sts.
5 Conclusion and Future Work
In this paper we described our submission to the
SemEval-2012 Semantic Textual Similarity Task.
We have identified some high performing features
for measuring semantic text similarity. Although
both of the submitted systems performed very well
on all but the two SMT test sets, there is still room
for improvement. The feature selection was ad-hoc
and more systematic feature selection is required
(e.g., wrapper feature selection). Introducing ad-
ditional features for deeper understanding (e.g., se-
mantic role labelling) might also improve perfor-
mance on this task.
Acknowledgments
This work was supported by the Ministry of Science,
Education and Sports, Republic of Croatia under the
Grant 036-1300646-1986. We would like to thank
the organizers for the tremendous effort they put into
formulating this challenging task.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 19?27. As-
sociation for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012). ACL.
Ramiz M. Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, COLING-ACL ?06, pages 69?
72. Association for Computational Linguistics.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine De Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8. Association for Computational
Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449?454.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
447
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine translation, 23(2):105?115.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 71?78. Association for
Computational Linguistics.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th inter-
national conference on Machine Learning, volume 1,
pages 296?304. San Francisco.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad,
and Alessandro Vespignani. 2005. Algorithmic detec-
tion of semantic similarity. In Proceedings of the 14th
international conference on World Wide Web, pages
107?116. ACM.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, et al 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. Proceedings of ACL-
08: HLT, pages 236?244.
Jesu?s Oliva, Jo?se Ignacio Serrano, Mar??a Dolores
Del Castillo, and A?ngel Iglesias. 2011. SyMSS: A
syntax-based measure for short-text semantic similar-
ity. Data & Knowledge Engineering.
Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang. 2005.
Techniques for improving web retrieval effectiveness.
Information processing & management, 41(5):1207?
1223.
Gerard Salton, Andrew Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational linguistics, 24(1):97?123.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1?47.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. Advances in Neural Infor-
mation Processing Systems, 24.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37(1):141?
188.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using dependency-based features to take the
?para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, volume
2006.
448
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 24?33,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Frequently Asked Questions Retrieval for Croatian
Based on Semantic Textual Similarity
Mladen Karan? Lovro ?mak? Jan ?najder?
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
?Studio Artlan, Andrije ?tangera 18, 51410 Opatija, Croatia
{mladen.karan,jan.snajder}@fer.hr lovro.zmak@studioartlan.hr
Abstract
Frequently asked questions (FAQ) are an
efficient way of communicating domain-
specific information to the users. Unlike
general purpose retrieval engines, FAQ re-
trieval engines have to address the lexi-
cal gap between the query and the usu-
ally short answer. In this paper we de-
scribe the design and evaluation of a FAQ
retrieval engine for Croatian. We frame
the task as a binary classification prob-
lem, and train a model to classify each
FAQ as either relevant or not relevant for
a given query. We use a variety of se-
mantic textual similarity features, includ-
ing term overlap and vector space features.
We train and evaluate on a FAQ test col-
lection built specifically for this purpose.
Our best-performing model reaches 0.47
of mean reciprocal rank, i.e., on average
ranks the relevant answer among the top
two returned answers.
1 Introduction
The amount of information available online is
growing at an exponential rate. It is becoming in-
creasingly difficult to navigate the vast amounts
of data and isolate relevant pieces of informa-
tion. Thus, providing efficient information access
for clients can be essential for many businesses.
Frequently asked questions (FAQ) databases are a
popular way to present domain-specific informa-
tion in the form of expert answers to users ques-
tions. Each FAQ consists of a question and an
answer, possibly complemented with additional
metadata (e.g., keywords). A FAQ retrieval engine
provides an interface to a FAQ database. Given a
user query in natural language as input, it retrieves
a ranked list of FAQs relevant to the query.
FAQ retrieval can be considered half way be-
tween traditional document retrieval and question
answering (QA). Unlike in full-blown QA, in FAQ
retrieval the questions and the answers are already
extracted. On the other hand, unlike in document
retrieval, FAQ queries are typically questions and
the answers are typically much shorter than doc-
uments. While FAQ retrieval can be approached
using simple keyword matching, the performance
of such systems will be severely limited due to the
lexical gap ? a lack of overlap between the words
that appear in a query and words from a FAQ pair.
As noted by Sneiders (1999), there are two causes
for this. Firstly, the FAQ database creators in gen-
eral do not know the user questions in advance.
Instead, they must guess what the likely questions
would be. Thus, it is very common that users? in-
formation needs are not fully covered by the pro-
vided questions. Secondly, both FAQs and user
queries are generally very short texts, which di-
minishes the chances of a keyword match.
In this paper we describe the design and the
evaluation of a FAQ retrieval engine for Croat-
ian. To address the lexical gap problem, we take
a supervised learning approach and train a model
that predicts the relevance of a FAQ given a query.
Motivated by the recent work on semantic textual
similarity (Agirre et al, 2012), we use as model
features a series of similarity measures based on
word overlap and semantic vector space similar-
ity. We train and evaluate the model on a FAQ
dataset from a telecommunication domain. On this
dataset, our best performing model achieves 0.47
of mean reciprocal rank, i.e., on average ranks the
relevant FAQ among the top two results.
In summary, the contribution of this paper is
twofold. Firstly, we propose and evaluate a
FAQ retrieval model based on supervised machine
learning. To the best of our knowledge, no previ-
24
ous work exists that addresses IR for Croatian in
a supervised setting. Secondly, we build a freely
available FAQ test collection with relevance judg-
ments. To the best of our knowledge, this is the
first IR test collection for Croatian.
The rest of the paper is organized as follows.
In the next section we give an overview of related
work. In Section 3 we describe the FAQ test col-
lection, while in Section 4 we describe the retrieval
model. Experimental evaluation is given in Sec-
tion 5. Section 6 concludes the paper and outlines
future work.
2 Related Work
Most prior work on FAQ retrieval has focused on
the problem of lexical gap, and various approaches
have been proposed for bridging it. Early work,
such as Sneiders (1999), propose to manually en-
rich the FAQ databases with additional meta data
such as the required, optional, and forbidden key-
words and keyphrases. This effectively reduces
FAQ retrieval to simple keyword matching, how-
ever in this case it is the manually assigned meta-
data that bridges the lexical gap and provides the
look and feel of semantic search.
For anything but a small-sized FAQ database,
manual creation of metadata is tedious and cost
intensive, and in addition requires expert knowl-
edge. An alternative is to rely on general lin-
guistic resources. FAQ finder (Burke et al, 1997)
uses syntax analysis to identify phrases, and then
performs matching using shallow lexical semantic
knowledge from WordNet (Miller, 1995). Yet an-
other way to bridge the lexical gap is smoothing
via clustering, proposed by Kim and Seo (2006).
First, query logs are expanded with word defini-
tions from a machine readable dictionary. Subse-
quently, query logs are clustered, and query simi-
larity is computed against the clusters, instead of
against the individual FAQs. As an alternative to
clustering, query expansion is often used to per-
form lexical smoothing (Voorhees, 1994; Navigli
and Velardi, 2003).
In some domains a FAQ engine additionally
must deal with typing errors and noisy user-
generated content. An example is the FAQ re-
trieval for SMS messages, described by Kothari et
al. (2009) and Contractor et al (2010).
Although low lexical overlap is identified as
the primary problem in FAQ retrieval, sometimes
it is the high lexical overlap that also presents a
problem. This is particularly true for large FAQ
databases in which a non-relevant document can
?accidentally? have a high lexical overlap with
a query. Moreo et al (2012) address the prob-
lem of false positives using case based reason-
ing. Rather than considering only the words, they
use phrases (?differentiator expressions?) that dis-
criminate well between FAQs.
The approaches described so far are essentially
unsupervised. A number of supervised FAQ re-
trieval methods have been described in the litera-
ture. To bridge the lexical gap, Xue et al (2008)
use machine translation models to ?translate? the
user query into a FAQ. Their system is trained on
very large FAQ knowledge bases, such as Yahoo
answers. Soricut and Brill (2004) describe another
large-scale FAQ retrieval system, which uses lan-
guage and transformation models. A good gen-
eral overview of supervised approaches to ranking
tasks is the work by Liu (2009).
Our system falls into the category of supervised
methods. In contrast to the above-described ap-
proaches, we use a supervised model with word
overlap and semantic similarity features. Taking
into account that FAQs are short texts, we use fea-
tures that have been recently proposed for deter-
mining the semantic similarity between pairs of
sentences (?aric? et al, 2012). Because we train
our model to output a relevance score for each
document, our approach is essentially a pointwise
learning-to-rank approach (Qin et al, 2008).
3 Croatian FAQ test collection
The standard procedure for IR evaluation requires
a test collection consisting of documents, queries,
and relevance judgments. We additionally require
an annotated dataset to train the model. As there
currently exists no standard IR test collection for
Croatian, we decided to build a FAQ test collec-
tion from scratch. We use this collection for both
model training and retrieval evaluation.
To obtain a FAQ test collection, we crawled the
web FAQ of Vip,1 a Croatian mobile phone opera-
tor. For each FAQ, we retrieved both the question
and the answer. In the Vip FAQ database ques-
tions are categorized into several broad categories
(e.g., by type of service). For each FAQ, we also
extract the category name assigned to it. We ob-
tained a total of 1344 FAQs. After removing the
1http://www.vipnet.hr/
pitanja-i-odgovori/ (accessed Sep 2009)
25
Query FAQ question FAQ answer
Kako se spaja na internet? (How to con-
nect to the internet?)
?to mi je potrebno da bih spojio rac?u-
nalo i koristio se internetom? (What do
I need to connect my computer and use
the internet)
Morate spojiti rac?unalo sa Homebox
ured?ajem LAN kabelom. . . (You must
connect your computer to the Homebox
device using a LAN cable . . . )
Putujem izvan Hrvatske i ?elim koristiti
svoj Vip mobilni ured?aj. Koliko c?e me
to ko?tati? (I am traveling abroad and
want to use my Vip mobile device. How
much will this cost?)
Koja je mre?a najpovoljnija za razgov-
ore, a koja za slanje SMS i MMS
poruka u roamingu? (Which network is
the best for conversations, and which
one for SMS and MMS messages in
roaming?)
Cijene za odlazne pozive u inozemstvu
su najpovoljnije u mre?ama Vodafone
partnera. . . (Outgoing calls cost less on
networks of Vodafone partners . . . )
Kako pogledati e-mail preko mobitela?
(How to check e-mail using a mobile
phone?)
Koja je cijena kori?tenja BlackBerry
Office usluge? (What is the price of us-
ing the BlackBerry Office service?)
. . . business e-mail usluga urac?unata je u
cijenu. . . (. . . business e-mail is included
in the price . . . )
Table 1: Examples of relevant answers to queries from the dataset
duplicates, 1222 unique FAQ pairs remain.
Next, we asked ten annotators to create at least
twelve queries each. They were instructed to in-
vent queries that they think would be asked by real
users of Vip services. To ensure that the queries
are as original as possible, the annotators were not
shown the original FAQ database. Following Lyti-
nen and Tomuro (2002), after creating the queries,
the annotators were instructed to rephrase them.
We asked the annotators to make between three
and ten paraphrases of each query. The paraphrase
strategies suggested were the following: (1) turn a
query into a multi-sentence query, (2) change the
structure (syntax) of the query, (3) substitute some
words with synonyms, while leaving the structure
intact, (4) turn the query into a declarative sen-
tence, and (5) any combination of the above. The
importance of not changing the underlying mean-
ing of a query was particularly stressed.
The next step was to obtain the binary relevance
judgments for each query. Annotating relevance
for the complete FAQ database is not feasible, as
the total number of query-FAQ pairs is too large.
On the other hand, not considering some of the
FAQs would make it impossible to estimate re-
call. A feasible alternative is the standard pooling
method predominantly used in IR evaluation cam-
paigns (Voorhees, 2002). In the pooling method,
the top-k ranked results of each evaluated system
are combined into a single list, which is then an-
notated for relevance judgments. For a sufficiently
large k, the recall estimate will be close to real
recall, as the documents that are not in the pool
are likely to be non-relevant. We simulate this set-
ting using several standard retrieval models: key-
word search, phrase search, tf-idf, and language
modeling. The number of combined results per
query is between 50 and 150. To reduce the an-
notators? bias towards top-ranked examples, the
retrieved results were presented in random order.
For each query, the annotators gave binary judg-
ments (?relevant? or ?not relevant?) to each FAQ
from the pooled list; FAQs not in the pool are as-
sumed to be not relevant. Although the appropri-
ateness of binary relevance has been questioned
(e.g., by Kek?l?inen (2005)), it is still commonly
used for FAQ and QA collections (Wu et al, 2006;
Voorhees and Tice, 2000). Table 1 shows exam-
ples of queries and relevant FAQs.
The above procedure yields a set of pairs
(Qr, Frel ), where Qr is a set of query paraphrases
and Frel is the set of relevant FAQs for any query
paraphrase from Qr. The total number of such
pairs is 117. From this set we generate a set of
pairs (q, Frel ), where q ? Qr is a single query.
The total number of such pairs is 419, of which
327 have at least one answer (Frel 6= ?), while
92 are not answered (Frel = ?). In this work
we focus on optimizing the performance on an-
swered queries and leave the detection and han-
dling of unanswered queries for future work. The
average number of relevant FAQs for a query is
1.26, while on average each FAQ is relevant for
1.44 queries. Test collection statistics is shown in
Table 2. We make the test collection freely avail-
able for research purposes.2
For further processing, we lemmatized the
query and FAQ texts using the morphological lex-
icon from ?najder et al (2008). We removed the
stopwords using a list of 179 Croatian stopwords.
2Available under CC BY-SA-NC license from
http://takelab.fer.hr/faqir
26
Word counts Form
Min Max Avg Quest. Decl.
Queries 1 25 8 372 47
FAQ questions 4 63 7 287 4
FAQ answers 1 218 30 ? ?
Table 2: FAQ test collection statistics
We retained the stopwords that constitute a part of
a service name (e.g., the pronoun ?me? (?me?) in
?Nazovi me? (?Call me?)).
4 Retrieval model
The task of the retrieval model is to rank the FAQs
by relevance to a given query. In an ideal case,
the relevant FAQs will be ranked above the non-
relevant ones. The retrieval model we propose is
a confidence-rated classifier trained on binary rel-
evance judgments, which uses as features the se-
mantic textual similarity between the query and
the FAQ. For a given a query-FAQ pair, the clas-
sifier outputs whether the FAQ is relevant (posi-
tive) or irrelevant (negative) for the query. More
precisely, the classifier outputs a confidence score,
which can be interpreted as the degree of rele-
vance. Given a single query as input, we run the
classifier on all query-FAQ pairs to obtain the con-
fidence scores for all FAQs from the database. We
then use these confidence scores to produce the fi-
nal FAQ ranking.
The training set consists of pairs (q, f) from the
test collection, where q ? Qr is a query from
the set of paraphrase queries and f ? Frel is a
FAQ from the set of relevant FAQs for this query
(cf. Section 3). Each (q, f) pair represents a posi-
tive training instance. To create a negative training
instance, we randomly select a (q, f) pair from the
set of positive instances and substitute the relevant
FAQ f with a randomly chosen non-relevant FAQ
f ?. As generating all possible negative instances
would give a very imbalanced dataset, we chose to
generate only 2N negative instances, where N is
the number of positive instances. Because |Frel|
varies depending on query q, number of instances
N per query also varies; on average, N is 329.
To train the classifier, we compute a feature vec-
tor for each (q, f) instance. The features measure
the semantic textual similarity between q and f .
More precisely, the features measure (1) the sim-
ilarity between query q and the question from f
and (2) the similarity between query q and the an-
swer from f . Considering both FAQ question and
answer has proven to be beneficial (Tomuro and
Lytinen, 2004). Additionally, ngram overlap fea-
tures are computed between the query and FAQ
category name.
As the classification model, we use the Support
Vector Machine (SVM) with radial basis kernel.
We use the LIBSVM implementation from Chang
and Lin (2011).
4.1 Term overlap features
We expect that FAQ relevance to be positively cor-
related with lexical overlap between FAQ text and
the user query. We use several lexical overlap
features. Similar features have been proposed by
Michel et al (2011) for paraphrase classification
and by ?aric? et al (2012) for semantic textual sim-
ilarity.
Ngram overlap (NGO). Let T1 and T2 be the
sets of consecutive ngrams (e.g., bigrams) in the
first and the second text, respectively. NGO is de-
fined as
ngo(T1, T2) = 2?
(
|T1|
|T1 ? T2|
+
|T2|
|T1 ? T2|
)?1
(1)
NGO measures the degree to which the first text
covers the second and vice versa. The two scores
are combined via a harmonic mean. We compute
NGO for unigrams and bigrams.
IC weighted word overlap (ICNGO). NGO
gives equal importance to all words. In practice,
we expect some words to be more informative than
others. The informativeness of a word can be mea-
sured by its information content (Resnik, 1995),
defined as
ic(w) = ln
?
w??C freq(w
?
)
freq(w)
(2)
where C is the set of words from the corpus and
freq(w) is the frequency of word w in the corpus.
We use the HRWAC corpus from Ljube?ic? and Er-
javec (2011) to obtain the word counts.
Let S1 and S2 be the sets of words occurring
in the first and second text, respectively. The IC-
weighted word coverage of the second text by the
first text is given by
wwc(S1, S2) =
?
w?S1?S2 ic(w)?
w??S2
ic(w?)
(3)
We compute the ICNGO feature as the harmonic
mean of wwc(S1, S2) and wwc(S2, S1).
27
4.2 Vector space features
Tf-idf similarity (TFIDF). The tf-idf (term fre-
quency/inverse document frequency) similarity of
two texts is computed as the cosine similarity of
their tf-idf weighted bag-of-words vectors. The tf-
idf weights are computed on the FAQ test collec-
tion. Here we treat each FAQ (without distinction
between question, answer, and category parts) as a
single document.
LSA semantic similarity (LSA). Latent seman-
tic analysis (LSA), first introduced by Deerwester
et al (1990), has been shown to be very effective
for computing word and document similarity. To
build the LSA model, we proceed along the lines
of Karan et al (2012). We build the model from
Croatian web corpus HrWaC from Ljube?ic? and
Erjavec (2011). For lemmatization, we use the
morphological lexicon from ?najder et al (2008).
Prior to the SVD, we weight the matrix elements
with their tf-idf values. Preliminary experiments
showed that system performance remained satis-
factory when reducing the vector space to only 25
dimensions, but further reduction caused deterio-
ration. We use 25 dimensions in all experiments.
LSA represents the meaning of a w by a vector
v(w). Motivated by work on distributional seman-
tic compositionality (Mitchell and Lapata, 2008),
we compute the semantic representation of text T
as the semantic composition (defined as vector ad-
dition) of the individual words constituting T :
v(T ) =
?
w?T
v(w) (4)
We compute the similarity between texts T1 and
T2 as the cosine between v(T1) and v(T2).
IC weighted LSA similarity (ICLSA). In the
LSA similarity feature all words occurring in a text
are considered to be equally important when con-
structing the compositional vector, ignoring the
fact that some words are more informative than
others. To acknowledge this, we use information
content weights defined by (2) and compute the IC
weighted compositional vector of a text T as
c(T ) =
?
wi?T
ic(wi)v(wi) (5)
Aligned lemma overlap (ALO). This feature
measures the similarity of two texts by semanti-
cally aligning their words in a greedy fashion. To
compare texts T1 and T2, first all pairwise sim-
ilarities between words from T1 and words from
T2 are computed. Then, the most similar pair is
selected and removed from the list. The procedure
is repeated until all words are aligned. The aligned
pairs are weighted by the larger information con-
tent of the two words:
sim(w1, w2) = (6)
max(ic(w1), ic(w2))? ssim(w1, w2)
where ssim(w1, w2) is the semantic similarity of
words w1 and w2 computed as the cosine similar-
ity of their LSA vectors, and ic is the information
content given by (2). The overall similarity be-
tween two texts is defined as the sum of weighted
pair similarities, normalized by the length of the
longer text:
alo(T1, T2) =
?
(w1,w2)?P sim(w1, w2)
max(length(T1), length(T2))
(7)
where P is the set of aligned lemma pairs. A sim-
ilar measure is proposed by Lavie and Denkowski
(2009) for machine translation evaluation, and has
been found out to work well for semantic textual
similarity (?aric? et al, 2012).
4.3 Question type classification (QC)
Related work on QA (Lytinen and Tomuro, 2002)
shows that the accuracy of QA systems can be im-
proved by question type classification. The intu-
ition behind this is that different types of ques-
tions demand different types of answers. Conse-
quently, information about the type of answer re-
quired should be beneficial as a feature.
To explore this line of improvement, we train
a simple question classifier on a dataset from
Lombarovic? et al (2011). The dataset consists
of 1300 questions in Croatian, classified into six
classes: numeric, entity, human, description, lo-
cation, and abbreviation. Following Lombarovic?
et al (2011), we use document frequency to select
the most frequent 300 words and 600 bigrams to
use as features. An SVM trained on this dataset
achieves 80.16% accuracy in a five-fold cross-
validation. This is slightly worse than the best re-
sult from Lombarovic? et al (2011), however we
use a smaller set of lexical features. We use the
question type classifier to compute two features:
the question type of the query and the question
type of FAQ question.
28
Feature RM1 RM2 RM3 RM4 RM5
NGO + + + + +
ICNGO + + + + +
TFIDF ? + + + +
LSA ? ? + + +
ICLSA ? ? + + +
ALO ? ? + + +
QED ? ? ? + +
QC ? ? ? ? +
Table 4: Features used by our models
4.4 Query expansion dictionary (QED)
Our error analysis revealed that some false nega-
tives could easily be eliminated by expanding the
query with similar/related words. To this end, we
constructed a small, domain-specific query expan-
sion dictionary. We aimed to (1) mitigate minor
spelling variances, (2) make the high similarity of
some some cross-POS or domain-specific words
explicit, and (3) introduce a rudimentary ?world
knowledge? useful for the domain at hand. The fi-
nal dictionary contains 53 entries; Table 3 shows
some examples.
5 Evaluation
5.1 Experimental setup
Because our retrieval model is supervised, we
evaluate it using five-fold cross-validation on the
FAQ test collection. In each fold we train our sys-
tem on the training data as described in Section
4, and evaluate the retrieval performance on the
queries from the test set. While each (q, Frel) oc-
curs in the test set exactly once, the same FAQ may
occur in both the train and test set. Note that this
does not pose a problem because the query part of
the pair will differ (due to paraphrasing).
To gain a better understanding of which features
contribute the most to retrieval performance, we
created several models. The models use increas-
ingly complex feature sets; an overview is given
in Table 4. We leave exhaustive feature analysis
and selection for future work.
As a baseline to compare against, we use a stan-
dard tf-idf weighted retrieval model. This model
ranks the FAQs by the cosine similarity of tf-idf
weighted vectors representing the query and the
FAQ. When computing the vector of the FAQ pair,
the question, answer, and category name are con-
catenated into a single text unit.
Model P R F1
RM1 14.1 68.5 23.1
RM2 25.8 75.1 37.8
RM3 24.4 75.4 36.3
RM4 25.7 77.7 38.2
RM5 25.3 76.8 37.2
Table 5: Classification results
5.2 Results
Relevance classification performance. Recall
that we use a binary classifier as a retrieval model.
The performance of this classifier directly deter-
mines the performance of the retrieval system as a
whole. It is therefore interesting to evaluate clas-
sifier performance separately. To generate the test
set, in each of the five folds we sample from the
test set the query-FAQ instances using the proce-
dure described in Section 4 (N positive and 2N
negative instance).
Precision, recall, and F1-score for each model
are shown in Table 5. Model RM4 outperforms
the other considered models. Model RM5, which
additionally uses question type classification, per-
forms worse than RM4, suggesting that the ac-
curacy of question type classification is not suf-
ficiently high. Our analysis of the test collection
revealed that this can be attributed to a domain
mismatch: the questions (mobile phone opera-
tor FAQ) are considerably different than those on
which the question classifier was trained (factoid
general questions). Moreover, some of the queries
and questions in our FAQ test collection are not
questions at all (cf. Table 2); e.g., ?Popravak mo-
bitela.? (?Mobile phone repair.?). Consequently,
it is not surprising that question classification fea-
tures do not improve the performance.
Retrieval performance. Retrieval results of the
five considered models are given in Table 6. We
report the standard IR evaluation measures: mean
reciprocal rank (MRR), average precision (AP),
and R-precision (RP). The best performance was
obtained with RM4 model, which uses all features
except the question type. The best MRR result
of 0.479 (with standard deviation over five folds
of ?0.04) indicates that, on average, model RM4
ranks the relevant answer among top two results.
Performance of other models expectedly in-
crease with the complexity of features used. How-
ever, RM5 is again an exception, performing
worse than RM4 despite using additional question
29
Query word Expansion words Remark
face facebook A lexical mismatch that would often occur
ogranic?iti (to limit) ogranic?enje (limit) Cross POS similarity important in the domain explicit
cijena (price) tro?ak (cost), ko?tati (to cost) Synonyms very often used in the domain
inozemstvo (abroad) roaming (roaming) Introduces world knowledge
ADSL internet Related words often used in the domain
Table 3: Examples from query expansions dictionary
Model MRR MAP RP
Baseline 0.341 21.77 15.28
RM1 0.326 20.21 17.6
RM2 0.423 28.78 24.37
RM3 0.432 29.09 24.90
RM4 0.479 33.42 28.74
RM5 0.475 32.37 27.30
Table 6: Retrieval results
type features, for the reasons elaborated above.
Expectedly, classification performance and
retrieval performance are positively correlated
(cf. Tables 5 and 6). A noteworthy case is RM4,
which improves the F1-score by only 5% over
RM3, yet improves IR measures by more than
10%. This suggest that, in addition to improving
the classifier decisions, the QED boosts the confi-
dence scores of already correct decisions.
A caveat to the above analysis is the fact that
the query expansion dictionary was constructed
base on the cross-validation result. While only
a small amount of errors were corrected with the
dictionary, this still makes models RM4 and RM5
slightly biased to the given dataset. An objective
estimate of maximum performance on unseen data
is probably somewhere between RM3 and RM4.
5.3 Error analysis
By manual inspection of false positive and false
negative errors, we have identified several char-
acteristic cases that account for the majority of
highly ranked irrelevant documents.
Lexical interference. While a query does have
a significant lexical similarity with relevant FAQ
pairs, it also has (often accidental) lexical simi-
larity with irrelevant FAQs. Because the classifier
appears to prefer lexical overlap, such irrelevant
FAQs interfere with results by taking over some of
the top ranked positions from relevant pairs.
Lexical gap. Some queries ask a very similar
question to an existing FAQ from the database, but
paraphrase it in such a way that almost no lexical
overlap remains. Even though the effect of this is
partly mitigated by our semantic vector space fea-
tures, in extreme cases the relevant FAQs will be
ranked rather low.
Semantic gap. Taken to the extreme, a para-
phrase can change a query to the extent that it
not only introduces a lexical gap, but also a se-
mantic gap, whose bridging would require logi-
cal inference and world knowledge. An exam-
ple of such query is ?Postoji li moguc?nost ko-
ri?tenja Vip kartice u Australiji?? (?Is it possi-
ble to use Vip sim card in Australia??). The asso-
ciated FAQ question is ?Kako mogu saznati pos-
toji li GPRS/EDGE ili UMTS/HSDPA roaming u
zemlji u koju putujem?? (?How can I find out if
there is GPRS/EDGE or UMTS/SPA roaming in
the country to which I am going??).
Word matching errors. In some cases words
which should match do not. This is most often
the case when one of the words is missing from
the morphological lexicon, and thus not lemma-
tized. A case in point is the word ?Facebook?, or
its colloquial Croatian variants ?fejs? and ?face?,
along with their inflected forms. Handling this is
especially important because a significant number
of FAQs from our dataset contain such words. An
obvious solution would be to complement lemma-
tization with stemming.
5.4 Cutoff strategies
Our model outputs a list of all FAQs from the
database, ranked by relevance to the input query.
As low-ranked FAQs are mostly not relevant, pre-
senting the whole ranked list puts an unnecessary
burden on the user. We therefore explored some
strategies for limiting the number of results.
First N (FN). This simply returns the N best
ranked documents.
Measure threshold criterion (MTC). We define
a threshold on FAQ relevance score, and re-
30
Figure 1: Recall vs. average number of documents
retrieved (for various cutoff strategies)
turn only the FAQs for which the classifier
confidence is above a specified threshold.
Cumulative threshold criterion (CTC). We de-
fine a threshold for cumulative relevance
score. The top-ranked FAQs for which the
sum of classifier confidences is below the
threshold are returned.
Relative threshold criterion (RTC). Returns all
FAQs whose relevance is within the given
percentage of the top-ranked FAQ relevance.
A good cutoff strategy should on average re-
turn a smaller number of documents, while still re-
taining high recall. To reflect this requirement we
measure the recall vs. average number of retrieved
documents (Fig. 1). While there is no substantial
difference between the four strategies, MTC and
RTC perform similarly and slightly better than FN
and CTC. As the number of documents increases,
the differences between the different cutoff strate-
gies diminish.
5.5 Performance and scalability
We have implemented the FAQ engine using in-
house code in Java. The only external library used
is the Java version of LIBSVM. Regarding system
performance, the main bottleneck is in generating
the features. Since all features depend on the user
query, they cannot be precomputed. Computation-
ally most intensive feature is ALO (cf. Section
4.2), which requires computing a large number of
vector cosines.
The response time of our FAQ engine is accept-
able ? on our 1222 FAQs test collection, the re-
sults are retrieved within one second. However, to
retrieve the results, the engine must generate fea-
tures and apply a classifier to every FAQ from the
database. This makes the response time linearly
dependent on the number of FAQs. For larger
databases, a preprocessing step to narrow down
the scope of the search would be required. To this
end, we could use a standard keyword-based re-
trieval engine, optimized for high recall. Unfortu-
nately, improving efficiency by precomputing the
features is impossible because it would require the
query to be known in advance.
6 Conclusion and Perspectives
We have described a FAQ retrieval engine for
Croatian. The engine uses a supervised retrieval
model trained on a FAQ test collection with bi-
nary relevance judgments. To bridge the notorious
lexical gap problem, we have employed a series of
features based on semantic textual similarity be-
tween the query and the FAQ. We have built a FAQ
test collection on which we have trained and evalu-
ated the model. On this test collection, our model
achieves a very good performance with an MRR
score of 0.47.
We discussed a number of open problems. Er-
ror analysis suggests that our models prefer the
lexical overlap features. Consequently, most er-
rors are caused by deceivingly high or low word
overlap. One way to address the former is to con-
sider not only words themselves, but also syntactic
structures. A simple way to do this is to use POS
patterns to detect similar syntactic structures. A
more sophisticated version could make use of de-
pendency relations obtained by syntactic parsing.
We have demonstrated that even a small,
domain-specific query expansion dictionary can
provide a considerable performance boost. An-
other venue of research could consider the auto-
matic methods for constructing a domain-specific
query expansion dictionary. As noted by a re-
viewer, one possibility would be to mine query
logs collected over a longer period of time, as em-
ployed in web search (Cui et al, 2002) and also
FAQ retrieval (Kim and Seo, 2006).
From a practical perspective, future work shall
focus on scaling up the system to large FAQ
databases and multi-user environments.
31
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croa-
tia under the Grant 036-1300646-1986. We thank
the reviewers for their constructive comments.
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics, pages 385?393. Association for
Computational Linguistics.
Robin D. Burke, Kristian J. Hammond, Vladimir Ku-
lyukin, Steven L. Lytinen, Noriko Tomuro, and Scott
Schoenberg. 1997. Question answering from fre-
quently asked question files: experiences with the
FAQ Finder system. AI magazine, 18(2):57.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/~cjlin/libsvm.
Danish Contractor, Govind Kothari, Tanveer A.
Faruquie, L. Venkata Subramaniam, and Sumit
Negi. 2010. Handling noisy queries in cross lan-
guage FAQ retrieval. In Proceedings of the EMNLP
2010, pages 87?96. Association for Computational
Linguistics.
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In Proceedings of the 11th interna-
tional conference on World Wide Web, pages 325?
332. ACM.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science, 41(6).
Mladen Karan, Jan ?najder, and Bojana Dalbelo Ba?ic?.
2012. Distributional semantics approach to detect-
ing synonyms in Croatian language. In Information
Society 2012 - Eighth Language Technologies Con-
ference, pages 111?116.
Jaana Kek?l?inen. 2005. Binary and graded rele-
vance in IR evaluations?comparison of the effects
on ranking of IR systems. Information processing &
management, 41(5):1019?1033.
Harksoo Kim and Jungyun Seo. 2006. High-
performance FAQ retrieval using an automatic clus-
tering method of query logs. Information processing
& management, 42(3):650?661.
Govind Kothari, Sumit Negi, Tanveer A. Faruquie,
Venkatesan T. Chakaravarthy, and L. Venkata Sub-
ramaniam. 2009. SMS based interface for FAQ re-
trieval. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, volume 2, pages 852?860.
Alon Lavie and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine translation, 23(2-3):105?115.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225?331.
Nikola Ljube?ic? and Toma? Erjavec. 2011. HrWaC
and SlWaC: compiling web corpora for Croatian and
Slovene. In Text, Speech and Dialogue, pages 395?
402. Springer.
Tomislav Lombarovic?, Jan ?najder, and Bojana Dal-
belo Ba?ic?. 2011. Question classification for a
Croatian QA system. In Text, Speech and Dialogue,
pages 403?410. Springer.
Steven Lytinen and Noriko Tomuro. 2002. The use
of question types to match questions in FAQ Finder.
In AAAI Spring Symposium on Mining Answers from
Texts and Knowledge Bases, pages 46?53.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, et al 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. Proceedings of
ACL-08: HLT, pages 236?244.
Alejandro Moreo, Maria Navarro, Juan L. Castro, and
Jose M. Zurita. 2012. A high-performance FAQ re-
trieval method using minimal differentiator expres-
sions. Knowledge-Based Systems.
Roberto Navigli and Paola Velardi. 2003. An anal-
ysis of ontology-based query expansion strategies.
In Proceedings of the 14th European Conference
on Machine Learning, Workshop on Adaptive Text
Extraction and Mining, Cavtat-Dubrovnik, Croatia,
pages 42?49.
Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2008.
How to make letor more useful and reliable. In Pro-
ceedings of the ACM Special Interest Group on In-
formation Retrieval 2008 Workshop on Learning to
Rank for Information Retrieval, pages 52?58.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In eprint
arXiv: cmp-lg/9511007, volume 1, page 11007.
32
Frane ?aric?, Goran Glava?, Mladen Karan, Jan ?na-
jder, and Bojana Dalbelo Ba?ic?. 2012. TakeLab:
systems for measuring semantic text similarity. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics, pages 441?448.
Association for Computational Linguistics.
Jan ?najder, Bojana Dalbelo Ba?ic?, and Marko Tadic?.
2008. Automatic acquisition of inflectional lexica
for morphological normalisation. Information Pro-
cessing & Management, 44(5).
Eriks Sneiders. 1999. Automated FAQ answering:
continued experience with shallow language under-
standing. In Question Answering Systems. Papers
from the 1999 AAAI Fall Symposium, pages 97?107.
Radu Soricut and Eric Brill. 2004. Automatic question
answering: beyond the factoid. In Proceedings of
HLT-NAACL, volume 5764.
Noriko Tomuro and Steven Lytinen. 2004. Retrieval
models and Q and A learning with FAQ files. New
Directions in Question Answering, pages 183?194.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceedings
of the 23rd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 200?207. ACM.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In SIGIR?94, pages 61?
69. Springer.
Ellen M. Voorhees. 2002. The philosophy of infor-
mation retrieval evaluation. In Evaluation of cross-
language information retrieval systems, pages 355?
370. Springer.
Chung-Hsien Wu, Jui-Feng Yeh, and Yu-Sheng Lai.
2006. Semantic segment extraction and matching
for internet FAQ retrieval. Knowledge and Data En-
gineering, IEEE Transactions on, 18(7):930?940.
Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft. 2008.
Retrieval models for question and answer archives.
In Proceedings of the 31st annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 475?482. ACM.
33
