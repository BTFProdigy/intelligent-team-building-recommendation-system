Developing an Arabic Treebank: Methods, Guidelines, Procedures, and Tools 
Mohamed MAAMOURI 
LDC, University of Pennsylvania  
3600 Market Street, Suite 810 
Philadelphia, PA 19104, USA 
maamouri@ldc.upenn.edu 
Ann BIES 
LDC, University of Pennsylvania  
3600 Market Street, Suite 810 
Philadelphia, PA 19104, USA 
bies@ldc.upenn.edu 
 
Abstract 
In this paper we address the following 
questions from our experience of the last two 
and a half years in developing a large-scale 
corpus of Arabic text annotated for 
morphological information, part-of-speech, 
English gloss, and syntactic structure:  (a) 
How did we ?leapfrog? through the stumbling 
blocks of both methodology and training in 
setting up the Penn Arabic Treebank (ATB) 
annotation? (b) How did we reconcile the 
Penn Treebank annotation principles and 
practices with the Modern Standard Arabic 
(MSA) traditional and more recent 
grammatical concepts? (c) What are the 
current issues and nagging problems? (d) 
What has been achieved and what are our 
future expectations? 
1 Introduction 
Treebanks are language resources that provide 
annotations of natural languages at various levels 
of structure: at the word level, the phrase level, and 
the sentence level. Treebanks have become 
crucially important for the development of data-
driven approaches to natural language processing 
(NLP), human language technologies, automatic 
content extraction (topic extraction and/or 
grammar extraction), cross-lingual information 
retrieval, information detection, and other forms of 
linguistic research in general. 
The Penn Arabic Treebank began in the fall of 
2001 and has now completed two full releases of 
data: (1) Arabic Treebank: Part 1 v 2.0, LDC 
Catalog No. LDC2003T06, roughly 166K words of 
written Modern Standard Arabic newswire from 
the Agence France Presse corpus; and (2) Arabic 
Treebank: Part 2 v 2.0, LDC Catalog No. 
LDC2004T02, roughly 144K words from Al-Hayat 
distributed by Ummah Arabic News Text.  New 
features of annotation in the UMAAH (UMmah 
Arabic Al-Hayat) corpus include complete 
vocalization (including case endings), lemma IDs, 
and more specific part-of-speech tags for verbs and 
particles.  Arabic Treebank: Part 3 is currently 
underway, and consists of text from An-Nahar. 
(Maamouri and Cieri, 2002) 
The ATB corpora are annotated for 
morphological information, part-of-speech, 
English gloss (all in the ?part-of-speech? phase of 
annotation), and for syntactic structure (Treebank 
II style). (Marcus, et al, 1993), (Marcus, et al, 
1994)  
In addition to the usual issues involved with the 
complex annotation of data, we have come to 
terms with a number of issues that are specific to a 
highly inflected language with a rich history of 
traditional grammar. 
2 Issues of methodology and training with 
Modern Standard Arabic 
2.1 Defining the specificities of ?Modern 
Standard Arabic? 
Modern Standard Arabic (MSA), the natural 
language under investigation, is not natively 
spoken by Arabs, who acquire it only through 
formal schooling.  MSA is the only form of written 
communication in the whole of the Arab world.  
Thus, there exists a living writing and reading 
community of MSA.  However, the level of MSA 
acquisition by its members is far from being 
homogeneous, and their linguistic knowledge, even 
at the highest levels of education, very unequal.  
This problem is going to have its impact on our 
corpus annotation training, routine, and results.  As 
in other Semitic languages, inflection in MSA is 
mostly carried by case endings, which are 
represented by vocalic diacritics appended in 
word-final position.  One must specify here that 
the MSA material form used in the corpus data we 
use consists of a graphic representation in which 
short vowel markers and other pertinent signs like 
the ?shaddah? (consonantal germination) are left 
out, as is typical in most written Arabic, especially 
news writing.  However, this deficient graphic 
representation does not indicate a deficient 
language system.  The reader reads the text and 
interprets its meaning by ?virtually providing? the 
missing grammatical information that leads to its 
acceptable interpretation. 
2.2 How important is the missing 
information? 
Our description and analysis of MSA linguistic 
structures is first done in terms of individual words 
and then expanded to syntactic functions.  Each 
corpus token is labeled in terms of its category and 
also in terms of its functions.  It is marked 
morphologically and syntactically, and other 
relevant relationship features also intervene such as 
concord, agreement and adjacency.  This 
redundancy decreases the importance of the 
absence of most vocalic features. 
2.3 The issue of vocalization 
The corpus for our annotation in the ATB 
requires that annotators complement the data by 
mentally supplying morphological information 
before choosing the automatic analysis, which 
amounts to a pre-requisite ?manual/human? 
intervention and which takes effect even before the 
annotation process begins.  Since no automatic 
vocalization of unvocalized MSA newswire data is 
provided prior to annotation, vocalization becomes 
the responsibility of annotators at both layers of 
annotation.  The part-of-speech (POS) annotators 
provide a first interpretation of the text/data and a 
vocalized output is created for the syntactic 
treebank (TB) annotators, who then engage in the 
responsibility of either validating the interpretation 
under their scrutiny or challenging it and providing 
another interpretation.  This can have drastic 
consequences as in the case of the so-called 
?Arabic deverbals? where the same bare graphemic 
structure can be two nouns in an ?idhafa 
(annexation or construct state) situation? with a 
genitive case ending on the second noun or a 
?virtual? verb or verbal function with a noun 
complement in the accusative to indicate a direct 
object.  In Example 1, genitive case is assigned 
under the noun interpretation, while accusative 
case is assigned by the same graphemic form of the 
word in its more verbal function (Badawi, et al, 
2004, cf. Section 2.10, pp. 237-246). 
Example 11 
Neutral form:  <xbArh Al+nb>        ?????? ?????  
Idhafa:  <ixbAruhu Al+naba>i         ?????????? ???????  
      his receipt (of) the news [news genitive] 
Verbal:  <ixbAruhu Al+naba>a         ?????????? ??????? 
      his telling the news [news accusative] 
These are sometimes difficult decisions to make, 
and annotators? agreement in this case is always at 
                                                     
1  For the transliteration system of all our Arabic 
corpora, we use Tim Buckwalter?s code, at 
http://www.ldc.upenn.edu/myl/morph/buckwalter.html 
its lowest.  Vocalization decisions have a non-
trivial impact on the overall annotation routine in 
terms of both accuracy and speed. 
Vocalization is a difficult problem, and we did 
not have the tools to address it when the project 
began.  We originally decided to treat our first 
corpus, AFP, by having annotators supply word-
internal lexical identity vocalization only, because 
that is how people normally read Arabic ? taking 
the normal risks taken by all readers, with the 
assumption that any interpretation of the case or 
mood chosen would be acceptable as the 
interpretation of an educated native speaker 
annotator.  In our second corpus, UMAAH, we 
decided that it would improve annotation and the 
overall usefulness of the corpus to vocalize the 
texts, by putting the necessary rules of syntax and 
vocalization at the POS level of annotation ? our 
annotators added case endings to nouns and voice 
to verbs, in addition to the word-internal lexical 
identity vocalization.  For our third corpus, 
ANNAHAR (currently in production), we have 
decided to fully vocalize the text, adding the final 
missing piece, mood endings for verbs. In 
conclusion, vocalization is a nagging but necessary 
?nuisance? because while its presence just 
enhances the linguistic analysis of the targeted 
corpus, its absence could be turned into an issue of 
quality of annotation and of grammatical 
credibility among Arab and non-Arab users. 
3 Reconciling Treebank annotation with 
traditional grammar concepts in Arabic 
The question we had to face in the early stages 
of ATB was how to develop a Treebank 
methodology ? an analysis of all the targeted 
syntactic structures ? for MSA represented by 
unvocalized written text data.  Since all Arabic 
readers ? Arabs and foreigners ? go through the 
process of virtually providing/inserting the 
required grammatical rules which allow them to 
reach an interpretation of the text and consequent 
understanding, and since all our recruited 
annotators are highly educated native Arabic 
speakers, we accepted going through our first 
corpus annotation with that premise. Our 
conclusion was that the two-level annotation was 
possible, but we noticed that because of the extra 
time taken hesitating about case markings at the 
TB level, TB annotation was more difficult and 
more time-consuming.  This led to including all 
possible/potential case endings in the POS 
alternatives provided by the morphological 
analyzer.  Our choice was to make the two 
annotation passes equal in difficulty by transferring 
the vocalization difficulty to the POS level.  We 
also thought that it is better to localize that 
difficulty at the initial level of annotation and to try 
to find the best solution to it.  So far, we are happy 
with that choice.  We are aware of the need to have 
a full and correct vocalization for our ATB, and we 
are also aware that there will never be an existing 
extensive vocalized corpus ? except for the 
Koranic text ? that we could totally trust.  The 
challenge was and still is to find annotators with a 
very high level of grammatical knowledge in 
MSA, and that is a tall order here and even in the 
Arab region. 
So, having made the change from unvocalized 
text in the ?AFP Corpus? to fully vocalized text 
now for the ?ANNAHAR Corpus,? we still need to 
ask ourselves the question of what is better: (a) an 
annotated corpus in which the ATB end users are 
left with the task of providing case endings to 
read/understand or (b) an annotated ATB corpus 
displaying case endings with a higher percentage 
of errors due to a significantly more complex 
annotation task? 
3.1 Training annotators, ATB annotation 
characteristics and speed 
The two main factors which affect annotation 
speed in our ATB experience are both related to 
the specific ?stumbling blocks? of the Arabic 
language. 
1.  The first factor which affects annotation 
accuracy and consistency pertains to the 
annotators? educational background (their 
linguistic ?mindset?) and more specifically to their 
knowledge ? often confused and not clear ? of 
traditional MSA grammar.  Some of the important 
obstacles to POS training come from the confusing 
overlap, which exists between the morphological 
categories as defined for Western language 
description and the MSA traditional grammatical 
framework.  The traditional Arabic framework 
recognizes three major morphological categories 
only, namely NOUN, VERB, and PARTICLE. 
This creates an important overlap which leads to 
mistakes/errors and consequent mismatches 
between the POS and syntactic categories.  We 
have noticed the following problems in our POS 
training: (a) the difficulty that annotators have in 
identifying ADJECTIVES as against NOUNS in a 
consistent way; (b) problems with defining the 
boundaries of the NOUN category presenting 
additional difficulties coming from the fact that the 
NOUN includes adjectives, adverbials, and 
prepositions, which could be formally nouns in 
particular functions (e.g., from fawq ??? NOUN to 
fawqa ????? PREP ?above? and fawqu ????? ADV 
etc.).  In this case, the NOUN category then 
overlaps with the adverbs and prepositions of 
Western languages, and this is a problem for our 
annotators who are linguistically savvy and have 
an advanced  knowledge of English and, most 
times, a third Western language. (c) Particles are 
very often indeterminate, and their category also 
overlaps with prepositions, conjunctions, 
negatives, etc. 
2.  The second factor which affects annotation 
accuracy and speed is the behemoth of 
grammatical tests.  Because of the frequency of 
obvious weaknesses among very literate and 
educated native speakers in their knowledge of the 
rules of ?<iErAb? (i.e., case ending marking), it 
became necessary to test the grammatical 
knowledge of each new potential annotator, and to 
continue occasional annotation testing at intervals 
in order to maintain consistency. 
While we have been able to take care of the first 
factor so far, the second one seems to be a very 
persistent problem because of the difficulty level 
encountered by Arab and foreign annotators alike 
in reaching a consistent and agreed upon use of 
case-ending annotation. 
4 Tools and procedures 
4.1 Lexicon and morphological analyzer 
The Penn Arabic Treebank uses a level of 
annotation more accurately described as 
morphological analysis than as part-of-speech 
tagging.  The automatic Arabic morphological 
analysis and part-of-speech tagging was performed 
with the Buckwalter Arabic Morphological 
Analyzer, an open-source software package 
distributed by the Linguistic Data Consortium 
(LDC catalog number LDC2002L49). 
The analyzer consists primarily of three Arabic-
English lexicon files: prefixes (299 entries), 
suffixes (618 entries), and stems (82158 entries 
representing 38600 lemmas).  The lexicons are 
supplemented by three morphological 
compatibility tables used for controlling prefix-
stem combinations (1648 entries), stem-suffix 
combinations (1285 entries), and prefix-suffix 
combinations (598 entries). 
The Arabic Treebank: Part 2 corpus contains 
125,698 Arabic-only word tokens (prior to the 
separation of clitics), of which 124,740 (99.24%) 
were provided with an acceptable morphological 
analysis and POS tag by the morphological parser, 
and 958 (0.76%) were items that the morphological 
parser failed to analyze correctly. 
 
Items with solution      124740   99.24% 
Items with no solution           958     0.76% 
Total                    125698 100.00% 
Table 1. Buckwalter lexicon coverage, UMAAH 
 
The ANNAHAR coverage statistics after POS 1 
(dated January 2004) are as follows:  
The ANNAHAR Corpus contains 340,281 
tokens, of which 47,246 are punctuation, numbers, 
and Latin strings, and 293,035 are Arabic word 
tokens.  
 
Punctuation, Numbers, Latin strings 47,246
Arabic Word Tokens 293,035
TOTAL 340,281
Table 2. Token distribution, ANNAHAR 
 
Of the 293,035 Arabic word tokens, 289,722 
(98.87%) were provided with an accurate 
morphological analysis and POS tag by the 
Buckwalter Arabic Morphological Analyzer.  
3,313 (1.13%) Arabic word tokens were judged to 
be incorrectly analyzed, and were flagged with a 
comment describing the nature of the inaccuracy.  
(Note that 204 of the 3,313 tokens for which no 
correct analysis was found were typos in the 
original text). 
 
Accurately analyzed 
Arabic Word Tokens 
289,722 98.87% 
Commented Arabic Word 
Tokens/ items with no 
solution 
3,313 1.13% 
TOTAL 293,035 100.00% 
Table 3. Lexicon coverage, ANNAHAR 
 
 
COMMENTS ON ITEMS WITH NO SOLUTION 
(no comment)  1741 52.55% 
MISC comment  566 17.08% 
ADJ    250 7.55% 
NOUN   233 7.03% 
TYPO   204 6.16% 
PASSIVE_FORM  110 3.32% 
DIALECTAL_FORM 68 2.05% 
VERB   37 1.12% 
FOREIGN WORD  34 1.03% 
IMPERATIVE  24 0.73% 
ADV    9 0.27% 
GRAMMAR_PROBLEM 9 0.27% 
NOUN_SHOULD_BE_ADJ 7 0.21% 
A_NAME   6 0.18% 
NUMERICAL  6 0.18% 
ABBREV   5 0.15% 
INTERR_PARTICLE 4 0.12% 
TOTAL   3313 100.00% 
Table 4. Distribution of items with no solution, 
     ANNAHAR 
 
4.2 Parsing engine 
In order to improve the speed and accuracy of 
the hand annotation, we automatically pre-parse 
the data after POS annotation and before TB 
annotation using Dan Bikel's parsing engine 
(Bikel, 2002).  Automatically pre-parsing the data 
allows the TB annotators to concentrate on the task 
of correcting a given parse and providing 
information about syntactic function (subject, 
direct object, adverbial, etc.). 
The parsing engine is capable of implementing a 
variety of generative, PCFG-style models 
(probabilistic context free grammar), including that 
of Mike Collins.  As such, in English, it gets 
results that are as good if not slightly better than 
the Collins parser.  Currently, this means that, for 
Section 00 of the WSJ of the English Penn 
Treebank (the development test set), the parsing 
engine gets a recall of 89.90 and a precision of 
90.15 on sentences of length <= 40 words.  The 
Arabic version of this parsing engine currently 
brackets AFP data with recall of 75.6 and precision 
of 77.4 on sentences of 40 words or less, and we 
are in the process of analyzing and improving the 
parser results. 
4.3 Annotation procedure 
Our annotation procedure is to use the automatic 
tools we have available to provide an initial pass 
through the data.  Annotators then correct the 
automatic output. 
First, Tim Buckwalter?s lexicon and 
morphological analyzer is used to generate a 
candidate list of ?POS tags? for each word (in the 
case of Arabic, these are compound tags assigned 
to each morphological segment for the word).  The 
POS annotation task is to select the correct POS 
tag from the list of alternatives provided.  Once 
POS is done, clitics are automatically separated 
based on the POS selection in order to create the 
segmentation necessary for treebanking.  Then, the 
data is automatically parsed using Dan Bikel?s 
parsing engine for Arabic.  Treebank annotators 
correct the automatic parse and add semantic role 
information, empty categories and their 
coreference, and complete the parse.  After that is 
done, we check for inconsistencies between the 
treebank and POS annotation.  Many of the 
inconsistencies are corrected manually by 
annotators or automatically by script if reliably 
safe and possible to do so.  
4.4 POS annotation quality control 
Five files with a total of 853 words (and a 
varying number of POS choices per word) were 
each tagged independently by five annotators for a 
quality control comparison of POS annotators.  Out 
of the total of 853 words, 128 show some 
disagreement.  All five annotators agreed on 85% 
of the words; the pairwise agreement is at least 
92.2%. 
For 82 out of the 128 words with some 
disagreement, four annotators agreed and only one 
disagreed.  Of those, 55 are items with ?no match? 
having been chosen from among the POS choices, 
due to one annotator?s definition of good-enough 
match differing from all of the others?.  The 
annotators have since reached agreement on which 
cases are truly ?no match,? and thus the rate of this 
disagreement should fall markedly in future POS 
files, raising the rate of overall agreement. 
5 Specifications for the Penn Arabic 
Treebank annotation guidelines 
5.1 Morphological analysis/Part-of-Speech 
The guidelines for the POS annotators are 
relatively straightforward, since the task essentially 
involves choosing the correct analysis from the list 
of alternatives provided by the morphological 
analyzer and adding the correct case ending.  The 
difficulties encountered by annotators in assigning 
POS and case endings are somewhat discussed 
above and will be reviewed by Tim Buckwalter in 
a separate presentation at COLING 2004.  
5.2 Syntactic analysis 
For the most part, our syntactic/predicate-
argument annotation of newswire Arabic follows 
the bracketing guidelines for the Penn English 
Treebank where possible. (Bies, et al 1995)  Our 
updated Arabic Treebank Guidelines is available 
on-line from the Linguistic Data Consortium at: 
http://www.ldc.upenn.edu/Catalog/docs/LDC2004
T02/ 
Some points where the Penn Arabic Treebank 
differs from the Penn English Treebank: 
? Arabic subjects are analyzed as VP 
internal, following the verb. 
? Matrix clause (S) coordination is 
possible and frequent. 
? The function of NP objects of transitive 
verbs is directly shown as NP-OBJ. 
We are also informed by on-going efforts to 
share data and reconcile annotations with the 
Prague Arabic Dependency Treebank (two Prague-
Penn Arabic Treebanking Workshops took place in 
2002 and 2003).  Some points where the Penn 
Arabic Treebank differs from the Prague Arabic 
Dependency Treebank: 
? Specific adverbial functions (LOC, 
TMP, etc.) are shown on the adverbial 
(PP, ADVP, clausal) modification of 
predicates. 
? The argument/adjunct distinction within 
NP is shown for noun phrases and 
clauses.  
? Empty categories (pro-drop subjects and 
traces of syntactic movement) are 
inserted. 
? Apposition is distinguished from other 
modification of nouns only for proper 
names. 
In spite of the considerable differences in word 
order between Modern Standard Arabic and 
English, we found that for the most part, it was 
relatively straightforward to adapt the guidelines 
for the Penn English Treebank to our Arabic 
Treebank.  In the interest of speed in starting 
annotation and of using existing tools to the 
greatest extent possible, we chose to adapt as much 
as possible from the English Treebank guidelines. 
There exists a long-standing, extensive, and 
highly valued paradigm of traditional grammar in 
Classical Arabic.  We chose to adapt the 
constituency approach from the Penn English 
Treebank rather than keeping to a strict and 
difficult adherence to a traditional Arabic grammar 
approach for several reasons: 
? Compatibility with existing treebanks, 
processing software and tools, 
? We thought it would be easier and more 
efficient to teach annotators, who come 
trained in Arabic grammar, to use our 
constituency approach than to teach 
computational linguists an old and 
complex Arabic-specific syntactic 
terminology.  
Nonetheless, it was important to adhere to an 
approach that did not strongly conflict with the 
traditional approach, in order to ease the cognitive 
load on our annotators, and also in order to be 
taken seriously by modern Arabic grammarians.  
Since there has been little work done on large data 
corpora in Arabic under any of the current 
syntactic theories in spite of the theoretical 
syntactic work being done (Mohamed, 2000), we 
have been working out solutions to Arabic syntax 
by combining the Penn Treebank constituency 
approach with pertinent insights from traditional 
grammar as well as modern theoretical syntax. 
For example, we analyze the underlying basic 
sentence structure as verb-initial, following the 
traditional grammar approach.  However, since the 
verb is actually not the first element in many 
sentences in the data, we adopt a topicalization 
structure for arguments that are fronted before the 
verb (as in Example 2, where the subject is 
fronted) and allow adverbials and conjunctions to 
appear freely before the verb (as in Example 3, 
where a prepositional phrase is pre-verbal).   
 
Example 2  
 
(S (NP-TPC-1 Huquwq+u  ??????? 
(NP Al+<inosAn+i  ??????????? )) 
(VP ta+qaE+u     ?????? 
(NP-SBJ-1 *T*)
(PP Dimona   ?????? 
(NP <ihotimAm+i+nA   ??????????? )
)))
  
??????? ??????????? ?????? ?????? ??????????? 
human rights exist within our concern 
 
 
Example 3 
 
(S (PP min  ??? 
(NP jih+ap+K   ??????   
>uxoraY  ?????? )) 
(VP ka$af+at     ???????  
(NP-SBJ maSAdir+u    ???????? 
miSoriy~+ap+N    ??????????? 
muT~aliE+ap+N  ???????????  )) 
(NP-OBJ Haqiyqata  ????????? 
(NP Al->amri  ??????? )))
 
 ???????? ???????? ??????????? ??????????? ???????? ??????  ??? ??????  ?????? 
from another side, well-informed Egyptian 
sources revealed the truth of the matter 
 
For many structures, the traditional approach and 
the treebank approach come together very easily.  
The traditional ?equational sentence,? for example, 
is a sentence that consists of a subject and a 
predicate without an overt verb (kAna or ?to be? 
does not appear overtly in the present tense).  This 
is quite satisfactorily represented in the same way 
that small clauses are shown in the Penn English 
Treebank, as in Example 4, since traditional 
grammar does not have a verb here, and we do not 
want to commit to the location of any potential 
verb phrase in these sentences. 
 
Example 4  
 
(S (NP-SBJ Al-mas>alatu ??????????? ) 
(ADJP-PRD basiyTatuN  ?????????)) 
  
???????????? ???????? 
the question is simple 
 
5.3 Current issues and nagging problems 
In a number of structures, however, the 
traditional grammar view does not line up 
immediately with the structural view that is 
necessary for annotation.  Often these are 
structures that are known to be problematic in a 
more general sense for either traditional grammar 
or theoretical syntax, or both.  We take both views 
into account and reconcile them in the best way 
that we can. 
5.3.1 Clitics 
The prevalence of cliticization in Arabic 
sentences of determiners, prepositions, 
conjunctions, and pronouns led to a necessary 
difference in tokenization between the POS files 
and the TB files.  Such cliticized constituents are 
written together with their host constituents in the 
text (e.g., Al+<inosAn+i  ???????????  ?the person? and 
??????????  bi+qirA?ati ?with reading?).  Clitics that 
play a role in the syntactic structure are split off 
into separate tokens (e.g., object pronouns 
cliticized to verbs, subject pronouns cliticized to 
complementizers, cliticized prepositions, etc.), so 
that their syntactic roles can be annotated in the 
tree.  Clitics that do not affect the structure are not 
separated (e.g., determiners).  Since the word 
boundaries necessary to separate the clitics are 
taken from the POS tags, and since it is not 
possible to show the syntactic structure unless the 
clitics are separated, correct POS tagging is 
extremely important in order to be able to properly 
separate clitics prior to the syntactic annotation. 
In the example below, both the conjunction wa 
?and? and the direct object hA ?it/them/her? are 
cliticized to the verb and also serve syntactic 
functions independent of the verb (sentential 
coordination and direct object). 
 
Example 5 
 
??????????? 
wasatu$AhiduwnahA 
wa/CONJ+sa/FUT+tu/IV2MP+$Ahid/VERB_IMP
ERFECT+uwna/IVSUFF_SUBJ:MP_MOOD:I+h
A/IVSUFF_DO:3FS 
and + will + you [masc.pl.] + 
watch/observe/witness + it/them/her 
 
The rest of the verbal inflections are also 
regarded as clitics in traditional grammar terms.  
However, for our purposes they do not require 
independent segmentation as they do not serve 
independent syntactic functions.  The subject 
inflection, for example, appears readily with full 
noun phrase subject in the sentence as well 
(although in this example, the subject is pro-
dropped).  The direct object pronoun clitic, in 
contrast, is in complementary distribution with full 
noun phrase direct objects.  Topicalized direct 
objects can appear with resumptive pronouns in the 
post-verbal direct object position.  However, 
resumptive pronouns in this structure should not be 
seen as problematic full noun phrases, as they are 
parasitic on the trace of movement ? and in fact 
they are taken to be evidence of the topicalization 
movement, since resumptive pronouns are 
common in relative clauses and with other 
topicalizations. 
Thus, we regard the cliticized object pronoun as 
carrying the full syntactic function of direct object.  
As such, we segment it as a separate token and 
represent it as a noun phrase constituent that is a 
sister to the verb (as shown in Example 6 below). 
 
Example 6 
 
(S wa-    -? 
(VP sa+tu+$Ahid+uwna-   ???????????? 
(NP-SBJ *)
(NP-OBJ ?hA      ??  )))
??????????? 
and you will observe her 
 
5.3.2 Gerunds (Masdar) and participials 
The question of the dual noun/verb nature of 
gerunds and participles in Arabic is certainly no 
less complex than for English or other languages.  
We have chosen to follow the Penn English 
Treebank practice to represent the more purely 
nominal masdar as noun phrases (NP) and the 
masdar that function more verbally as clauses (as 
S-NOM when in nominal positions).  In Example 
7, the masdar behaves like a noun in assigning 
genitive case.   
 
Example 7 
(PP bi-  -?? 
(NP qirA?ati        ???????? 
(NP kitAbi       ??????  
(NP Al-naHwi ??????? )))) 
 
??????????? ?????? ?????? 
with the reading of the book of syntax  
[book genitive] 
 
 
In Example 8, in contrast, the masdar functions 
more verbally, in assigning accusative case. 
 
 
Example 8 
 
(PP bi-     -?? 
(S-NOM (VP qirA?ati ????????) 
(NP-SBJ fATimata ???????? -) 
(NP-OBJ Al-kitAba  ????????  
                                                   ))))
 
 ???????????  ????????  ???????  
with Fatma?s reading the book  
[book accusative] 
 
This annotation scheme to allow for both the 
nominal and verbal functions of masdar is easily 
accepted and applied by annotators for the most 
part.  However, there are situations where the 
functions and behaviors of the masdar are in 
disagreement.  For example, a masdar can take a 
determiner ?Al-? (the behavior of a noun) and at 
the same time assign accusative case (the behavior 
of a verb). 
 
Example 9 
 
(PP bi     -?? 
(S-NOM
(VP Al+mukal~afi    ?????????? 
(NP-SBJ *)
(NP-OBJ <injAza   ??????? 
(NP Al+qarAri ?????????
Al+mawEuwdi
?????????? )))))
 
?????????????? ??????? ????????? ????????? 
with the (person in) charge of completion (of) 
the promised report [completion accusative] 
 
In this type of construction, the annotators must 
choose which behaviors to give precedence 
(accusative case assignment trumps determiners, 
for example).  However, it also brings up the issues 
and problems of assigning case ending and the 
annotators? knowledge of Arabic grammar and the 
rules of ?<iErAb.?  These examples are complex 
grammatically, and finding the right answer (even 
in strictly traditional grammar terms) is often 
difficult. 
This kind of ambiguity and decision-making 
necessarily slows annotation speed and reduces 
accuracy.  We are continuing our discussions and 
investigations into the best solutions for such 
issues. 
6 Future work 
Annotation for the Arabic Treebank is on-going, 
currently on a corpus of An-Nahar newswire 
(350K words).  We continue efforts to improve 
annotation accuracy, consistency and speed, both 
for POS and TB annotation.   
Conclusion 
In designing our annotation system for Arabic, 
we relied on traditional Arabic grammar, previous 
grammatical theories of Modern Standard Arabic 
and modern approaches, and especially the Penn 
Treebank approach to syntactic annotation, which 
we believe is generalizable to the development of 
other languages.  We also benefited from the 
existence at LDC of a rich experience in linguistic 
annotation.  We were innovative with respect to 
traditional grammar when necessary and when we 
were sure that other syntactic approaches 
accounted for the data.  Our goal is for the Arabic 
Treebank to be of high quality and to have 
credibility with regards to the attitudes and respect 
for correctness known to be present in the Arabic 
world as well as with respect to the NLP and wider 
linguistic communities.  The creation and use of 
efficient tools such as an automated morphological 
analyzer and an automated parsing engine ease and 
speed the annotation process.  These tools helped 
significantly in the successful creation of a process 
to analyze Arabic text grammatically and allowed 
the ATB team to publish the first significant 
database of morphologically and syntactically 
annotated Arabic news text in the world within one 
year.  Not only is this an important achievement 
for Arabic for which we are proud, but it also 
represents significant methodological progress in 
treebank annotation as our first data release was 
realized in significantly less time.  Half a million 
MSA words will be treebanked by end of 2004, 
and our choice of MSA corpora will be diversified 
to be representative of the current MSA writing 
practices in the Arab region and the world.  In spite 
of the above, we are fully aware of the humbling 
nature of the task and we fully understand and 
recognize that failures and errors may certainly be 
found in our work. The devil is in the details, and 
we remain committed to ironing out all mistakes.  
We count on the feedback of our users and readers 
to complete our work.  
8 Acknowledgements 
We gratefully acknowledge the tools and support 
provided to this project by Tim Buckwalter, Dan 
Bikel and Hubert Jin.  Our sincere thanks go to all 
of the annotators who have contributed their 
invaluable time and effort to Arabic part-of-speech 
and treebank annotation, and more especially to 
our dedicated treebank annotators, Wigdan El 
Mekki and Tasneem Ghandour. 
References  
Elsaid Badawi, M. G. Carter and Adrian Gully, 
2004. Modern Written Arabic: A Comprehensive 
Grammar.  Routledge: New York. 
Daniel M. Bikel, 2002. Design of a multi-lingual, 
parallel-processing statistical parsing engine. 
Proceedings of the Human Language 
Technology Workshop. 
Bracketing Guidelines for Treebank II Style, 1995. 
Eds: Ann Bies, Mark Ferguson, Karen Katz, 
Robert MacIntyre, Penn Treebank Project, 
University of Pennsylvania, CIS Technical 
Report MS-CIS-95-06. 
Mohamed Maamouri and Christopher Cieri, 2002. 
Resources for Arabic Natural Language 
Processing at the Linguistic Data Consortium. 
Proceedings of the International Symposium on 
Processing of Arabic.  Facult? des Lettres, 
University of Manouba, Tunisia. 
M. Marcus, G. Kim, M. Marcinkiewicz, R. 
MacIntyre, A. Bies, M. Ferguson, K. Katz & B. 
Schasberger, 1994. The Penn Treebank: 
Annotating predicate argument structure. 
Proceedings of the Human Language 
Technology Workshop, San Francisco. 
M. Marcus, B. Santorini and M.A. Marcinkiewicz, 
1993. Building a large annotated corpus of 
English: the Penn Treebank. Computational 
Linguistics. 
Mohamed A. Mohamed, 2000. Word Order, 
Agreement and Pronominalization in Standard 
and Palestinian Arabic. CILT 181. John 
Benjamins: Philadelphia. 
Zdenek ?abokrtsk? and Otakar Smr?, 2003. Arabic 
Syntactic Trees: from Constituency to 
Dependency. EACL 2003 Conferenceompanion. 
Association for Computational Linguistics, 
Hungary.  
 
 
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 725?735,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A New Approach to Lexical Disambiguation of Arabic Text
Rushin Shah
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
rnshah@cs.cmu.edu
Paramveer S. Dhillon, Mark Liberman,
Dean Foster, Mohamed Maamouri
and Lyle Ungar
University of Pennsylvania
3451 Walnut Street
Philadelphia, PA 19104, USA
{dhillon|myl|ungar}@cis.upenn.edu,
foster@wharton.upenn.edu,
maamouri@ldc.upenn.edu
Abstract
We describe a model for the lexical analy-
sis of Arabic text, using the lists of alterna-
tives supplied by a broad-coverage morpho-
logical analyzer, SAMA, which include sta-
ble lemma IDs that correspond to combina-
tions of broad word sense categories and POS
tags. We break down each of the hundreds
of thousands of possible lexical labels into
its constituent elements, including lemma ID
and part-of-speech. Features are computed
for each lexical token based on its local and
document-level context and used in a novel,
simple, and highly efficient two-stage super-
vised machine learning algorithm that over-
comes the extreme sparsity of label distribu-
tion in the training data. The resulting system
achieves accuracy of 90.6% for its first choice,
and 96.2% for its top two choices, in selecting
among the alternatives provided by the SAMA
lexical analyzer. We have successfully used
this system in applications such as an online
reading helper for intermediate learners of the
Arabic language, and a tool for improving the
productivity of Arabic Treebank annotators.
1 Background and Motivation
This paper presents a methodology for generating
high quality lexical analysis of highly inflected lan-
guages, and demonstrates excellent performance ap-
plying our approach to Arabic. Lexical analysis of
the written form of a language involves resolving,
explicitly or implicitly, several different kinds of am-
biguities. Unfortunately, the usual ways of talking
about this process are also ambiguous, and our gen-
eral approach to the problem, though not unprece-
dented, has uncommon aspects. Therefore, in order
to avoid confusion, we begin by describing how we
define the problem.
In an inflected language with an alphabetic writ-
ing system, a central issue is how to interpret strings
of characters as forms of words. For example, the
English letter-string ?winds? will normally be in-
terpreted in one of four different ways, all four
of which involve the sequence of two formatives
wind+s. The stem ?wind? might be analyzed as (1) a
noun meaning something like ?air in motion?, pro-
nounced [wInd] , which we can associate with an ar-
bitrary but stable identifier like wind n1; (2) a verb
wind v1 derived from that noun, and pronounced the
same way; (3) a verb wind v2 meaning something
like ?(cause to) twist?, pronounced [waInd]; or (4)
a noun wind n2 derived from that verb, and pro-
nounced the same way. Each of these ?lemmas?, or
dictionary entries, will have several distinguishable
senses, which we may also wish to associate with
stable identifiers. The affix ?-s? might be analyzed
as the plural inflection, if the stem is a noun; or as
the third-person singular inflection, if the stem is a
verb.
We see this analysis as conceptually divided into
four parts: 1) Morphological analysis, which rec-
ognizes that the letter-string ?winds? might be (per-
haps among other things) wind/N + s/PLURAL or
wind/V + s/3SING; 2) Morphological disambigua-
tion, which involves deciding, for example, that in
the phrase ?the four winds?, ?winds? is probably a
plural noun, i.e. wind/N + s/PLURAL; 3) Lemma
analysis, which involves recognizing that the stem
wind in ?winds? might be any of the four lem-
mas listed above ? perhaps with a further listing of
senses or other sub-entries for each of them; and 4)
Lemma disambiguation, deciding, for example, that
725
the phrase ?the four winds? probably involves the
lemma wind n1.
Confusingly, the standard word-analysis tasks in
computational linguistics involve various combina-
tions of pieces of these logically-distinguished op-
erations. Thus, ?part of speech (POS) tagging? is
mainly what we?ve called ?morphological disam-
biguation?, except that it doesn?t necessarily require
identifying the specific stems and affixes involved.
In some cases, it also may require a small amount of
?lemma disambiguation?, for example to distinguish
a proper noun from a common noun. ?Sense disam-
biguation? is basically a form of what we?ve called
?lemma disambiguation?, except that the sense dis-
ambiguation task may assume that the part of speech
is known, and may break down lexical identity more
finely than our system happens to do. ?Lemmatiza-
tion? generally refers to a radically simplified form
of ?lemma analysis? and ?lemma disambiguation?,
where the goal is simply to collapse different in-
flected forms of any similarly-spelled stems, so that
the strings ?wind?, ?winds?, ?winded?, ?winding? will
all be treated as instances of the same thing, without
in fact making any attempt to determine the identity
of ?lemmas? in the traditional sense of dictionary
entries.
Linguists use the term morphology to include all
aspects of lexical analysis under discussion here.
But in most computational applications, ?morpho-
logical analysis? does not include the disambigua-
tion of lemmas, because most morphological ana-
lyzers do not reference a set of stable lemma IDs.
So for the purposes of this paper, we will continue to
discuss lemma analysis and disambiguation as con-
ceptually distinct from morphological analysis and
disambiguation, although, in fact, our system dis-
ambiguates both of these aspects of lexical analysis
at the same time.
The lexical analysis of textual character-strings
is a more complex and consequential problem in
Arabic than it is in English, for several reasons.
First, Arabic inflectional morphology is more com-
plex than English inflectional morphology is. Where
an English verb has five basic forms, for example,
an Arabic verb in principle may have dozens. Sec-
ond, the Arabic orthographic system writes elements
such as prepositions, articles, and possessive pro-
nouns without setting them off by spaces, roughly
as if the English phrase ?in a way? were written ?in-
away?. This leads to an enormous increase in the
number of distinct ?orthographic words?, and a sub-
stantial increase in ambiguity. Third, short vowels
are normally omitted in Arabic text, roughly as if
English ?in a way? were written ?nway?.
As a result, a whitespace/punctuation-delimited
letter-string in Arabic text typically has many more
alternative analyses than a comparable English
letter-string does, and these analyses have many
more parts, drawn from a much larger vocabulary of
form-classes. While an English ?tagger? can spec-
ify the morphosyntactic status of a word by choos-
ing from a few dozen tags, an equivalent level of
detail in Arabic would require thousands of alterna-
tives. Similarly, the number of lemmas that might
play a role in a given letter-sequence is generally
much larger in Arabic than in English.
We start our labeling of Arabic text with the alter-
native analyses provided by SAMA v. 3.1, the Stan-
dard Arabic Morphological Analyzer (Maamouri et
al., 2009). SAMA is an updated version of the ear-
lier Buckwalter analyzers (Buckwalter, 2004), with
a number of significant differences in analysis to
make it compatible with the LDC Arabic Treebank
3-v3.2 (Maamouri et al, 2004). The input to SAMA
is an Arabic orthographic word (a string of letters
delimited by whitespace or punctuation), and the
output of SAMA is a set of alternative analyses, as
shown in Table 1. For a typical word, SAMA pro-
duces approximately a dozen alternative analyses,
but for certain highly ambiguous words it can pro-
duce hundreds of alternatives.
The SAMA analyzer has good coverage; for typ-
ical texts, the correct analysis of an orthographic
word can be found somewhere in SAMA?s list of
alternatives about 95% of the time. However, this
broad coverage comes at a cost; the list of analytic
alternatives must include a long Zipfian tail of rare
or contextually-implausible analyses, which collec-
tively are correct often enough to make a large con-
tribution to the coverage statistics. Furthermore,
SAMA?s long lists of alternative analyses are not
evaluated or ordered in terms of overall or contex-
tual plausibility. This makes the results less useful
in most practical applications.
Our goal is to rank these alternative analyses so
that the correct answer is as near to the top of the list
726
Token Lemma Vocalization Segmentation Morphology Gloss
yHlm Halam-u 1 yaHolumu ya + Holum +
u
IV3MS + IV + IV-
SUFF MOOD:I
he / it + dream + [ind.]
yHlm Halam-u 1 yaHoluma ya + Holum +
a
IV3MS + IV + IV-
SUFF MOOD:S
he / it + dream + [sub.]
yHlm Halum-u 1 yaHolumo ya + Holum +
o
IV3MS + IV + IV-
SUFF MOOD:J
he / it + be gentle + [jus.]
qbl qabil-a 1 qabila qabil + a PV + PV-
SUFF SUBJ:3MS
accept/receive/approve +
he/it [verb]
qbl qabol 1 qabol qabol NOUN Before
Table 1: Partial output of SAMA for yHlm and qbl. On average, every token produces more than 10 such analyses
as possible. Despite some risk of confusion, we?ll
refer to SAMA?s list of alternative analyses for an
orthographic word as potential labels for that word.
And despite a greater risk of confusion, we?ll refer to
the assignment of probabilities to the set of SAMA
labels for a particular Arabic word in a particular
textual context as tagging, by analogy to the oper-
ation of a stochastic part-of-speech tagger, which
similarly assigns probabilities to the set of labels
available for a word in textual context.
Although our algorithms have been developed for
the particular case of Arabic and the particular set
of lexical-analysis labels produced by SAMA, they
should be applicable without modification to the sets
of labels produced by any broad-coverage lexical
analyzer for the orthographic words of any highly-
inflected language.
In choosing our approach, we have been moti-
vated by two specific applications. One applica-
tion aims to help learners of Arabic in reading text,
by offering a choice of English glosses with asso-
ciated Arabic morphological analyses and vocaliza-
tions. SAMA?s excellent coverage is an important
basis for this help; but SAMA?s long, unranked list
of alternative analyses for a particular letter-string,
where many analyses may involve rare words or al-
ternatives that are completely implausible in the con-
text, will be confusing at best for a learner. It is
much more helpful for the list to be ranked so that
the correct answer is almost always near the top, and
is usually one of the top two or three alternatives.
In our second application, this same sort of rank-
ing is also helpful for the linguistically expert native
speakers who do Arabic Treebank analysis. These
annotators understand the text without difficulty, but
find it time-consuming and fatiguing to scan a long
list of rare or contextually-implausible alternatives
for the correct SAMA output. Their work is faster
and more accurate if they start with a list that is
ranked accurately in order of contextual plausibility.
Other applications are also possible, such as vo-
calization of Arabic text for text-to-speech synthe-
sis, or lexical analysis for Arabic parsing. However,
our initial goals have been to rank the list of SAMA
outputs for human users.
We note in passing that the existence of set of sta-
ble ?lemma IDs? is an unusual feature of SAMA,
which in our opinion ought to be emulated by ap-
proaches to lexical analysis in other languages. The
lack of such stable lemma IDs has helped to disguise
the fact that without lemma analysis and disam-
biguation, morphological analyses and disambigua-
tion is only a partial solution to the problem of lexi-
cal analysis.
In principle, it is obvious that lemma disambigua-
tion and morphological disambiguation are mutually
beneficial. If we know the answer to one of the ques-
tions, the other one is easier to answer. However,
these two tasks require rather different sets of con-
textual features. Lemma disambiguation is similar
to the problem of word-sense disambiguation ? on
some definitions, they are identical ? and as a re-
sult, it benefits from paragraph-level and document-
level bag-of-words attributes that help to character-
ize what the text is ?about? and therefore which lem-
mas are more likely to play a role in it. In contrast,
morphological disambiguation mainly depends on
features of nearby words, which help to character-
727
ize how inflected forms of these lemmas might fit
into local phrasal structures.
2 Problem and Methodology
Consider a collection of tokens (observations), ti, re-
ferred to by index i ? {1, . . . , n}, where each token
is associated with a set of p features, xij , for the jth
feature, and a label, li, which is a combination of
a lemma and a morphological analysis. We use in-
dicator functions yik to indicate whether or not the
kth label for the ith token is present. We represent
the complete set of features and labels for the en-
tire training data using matrix notation as X and Y ,
respectively. Our goal is to predict the label l (or
equivalently, the vector y for a given feature vector
x.
A standard linear regression model of this prob-
lem would be
y = x? +  (1)
The standard linear regression estimate of ? (ig-
noring, for simplicity the fact that the ys are 0/1) is:
?? = (XTtrainXtrain)
?1XTtrainYtrain (2)
where Ytrain is an n?h matrix containing 0s and
1s indicating whether or not each of the h possible
labels is the correct label (li) for each of the n tokens
ti, Xtrain is an n ? p matrix of context features for
each of the n tokens, the coefficients ?? are p? h.
However, this is a large, sparse, multiple label
problem, and the above formulation is neither statis-
tically nor computationally efficient. Each observa-
tion (x,y) consists of thousands of features associ-
ated with thousands of potential labels, almost all of
which are zero. Worse, the matrix of coefficients ?,
to be estimated is large (p? h) and one should thus
use some sort of transfer learning to share strength
across the different labels.
We present a novel principled and highly compu-
tationally efficient method of estimating this multi-
label model. We use a two stage procedure, first
using a subset (Xtrain1, Ytrain1) of training data
to give a fast approximate estimate of ?; we then
use a second smaller subset of the training data
(Xtrain2, Ytrain2,) to ?correct? these estimates in a
way that we will show can be viewed as a spe-
cialized shrinkage. Our first stage estimation ap-
proximates ?, but avoids the expensive computa-
tion of (XTtrainXtrain)
?1. Our second stage corrects
(shrinks) these initial estimates in a manner special-
ized to this problem. The second stage takes ad-
vantage of the fact that we only need to consider
those candidate labels produced by SAMA. Thus,
only dozens of the thousands of possible labels are
considered for each token.
We now present our algorithm. We start with a
corpus D of documents d of labeled Arabic text. As
described above, each token, ti is associated with a
set of features characterizing its context, computed
from the other words in the same document, and a la-
bel, li = (lemmai,morphologyi), which is a combi-
nation of a lemma and a morphological analysis. As
described below, we introduce a novel factorization
of the morphology into 15 different components.
Our estimation algorithm, shown in Algorithm 1,
has two stages. We partition the training corpus into
two subsets, one of which (Xtrain1) is used to es-
timate the coefficients ?s and the other of which
(Xtrain2) is used to optimally ?shrink? these coeffi-
cient estimates to reduce variance and prevent over-
fitting due to data sparsity.
For the first stage of our estimation procedure, we
simplify the estimate of the (?) matrix (Equation 2)
to avoid the inversion of the very high dimensional
(p?p) matrix (XTX) by approximating (XTX) by
its diagonal, Var(X), the inverse of which is trivial
to compute; i.e. we estimate ? using
?? = Var(Xtrain1)
?1XTtrain1Ytrain1 (3)
For the second stage, we assume that the coeffi-
cients for each feature can be shrunk differently, but
that coefficients for each feature should be shrunk
the same regardless of what label they are predict-
ing. Thus, for a given observation we predict:
g?ik =
p?
j=1
wj ??jkxij (4)
where the weightswj indicate how much to shrink
each of the p features.
In practice, we fold the variance of each of the j
features into the weight, giving a slightly modified
equation:
g?ik =
p?
j=1
?j?
?
jkxij (5)
728
where ?? = XTtrain1Ytrain1 is just a matrix of the
counts of how often each context feature shows up
with each label in the first training set. The vec-
tor ?, which we will estimate by regression, is just
the shrinkage weightsw rescaled by the feature vari-
ance.
Note that the formation here is different from the
first stage. Instead of having each observation be
a token, we now let each observation be a (token,
label) pair, but only include those labels that were
output by SAMA. For a given token ti and poten-
tial label lk, our goal is to approximate the indica-
tor function g(i, k), which is 1 if the kth label of
token ti is present, and 0 otherwise. We find candi-
date labels using a morphological analyzer (namely
SAMA), which returns a set of possible candidate
labels, say C(t), for each Arabic token t. Our pre-
dicted label for ti is then argmaxk?C(ti)g(i, k).
The regression model for learning the weights ?j
in the second stage thus has a row for each label
g(i, k) associated with a SAMA candidate for each
token i = ntrain1+1 . . . ntrain2 in the second train-
ing set. The value of g(i, k) is predicted as a func-
tion of the feature vector zijk = ??jkxij .
The shrinkage coefficients, ?j , could be estimated
from theory, using a version of James-Stein shrink-
age (James and Stein, 1961), but in practice, superior
results are obtained by estimating them empirically.
Since there are only p of them (unlike the p ? h ?s),
a relatively small training set is sufficient. We found
that regression-SVMs work slightly better than lin-
ear regression and significantly better than standard
classification SVMs for this problem.
Prediction is then done in the obvious way by tak-
ing the tokens in a test corpusDtest, generating con-
text features and candidate SAMA labels for each
token ti, and selected the candidate label with the
highest score g?(i, k) that we set out to learn. More
formally, The model parameters ?? and ? produced
by the algorithm allow one to estimate the most
likely label for a new token ti out of a set of can-
didate labels C(ti) using
kpred = argmaxk?C(ti)
p?
j=1
?j?
?
jkxij (6)
The most expensive part of the procedure is es-
timating ??, which requires for each token in cor-
Algorithm 1 Training algorithm.
Input: A training corpusDtrain of n observations
(Xtrain, Ytrain)
PartitionDtrain into two sets,D1 andD2, of sizes
ntrain1 and ntrain2 = n? ntrain1 observations
// Using D1, estimate ??
??jk =
?ntrain1
i=1 xijyik for the j
th feature and kth
label
// Using D2, estimate ?j
// Generate new ?features? Z and the true labels
g(i, k) for each of the SAMA candidate labels for
each of the tokens in D2
zijk = ??jkxij for i in i = ntrain1 + 1 . . . ntrain2
Estimate ?j for the above (feature,label) pairs
(zijk, g(i, k)) using Regression SVMs
Output: ? and ??
pus D1, (a subset of D), finding the co-occurrence
frequencies of each label element (a lemma, or a
part of the morphological segmentation) with the
target token and jointly with the token and with
other tokens or characters in the context of the to-
ken of interest. For example, given an Arabic to-
ken, ?yHlm?, we count what fraction of the time
it is associated with each lemma (e.g. Halam-
u 1), count(lemma=Halam-u 1, token=yHlm) and
each segment (e.g. ?ya?), count(segment=ya, to-
ken=yHlm). (Of course, most tokens never show up
with most lemmas or segments; this is not a prob-
lem.) We also find the base rates of the components
of the labels (e.g., count(lemma=Halam-u 1), and
what fraction of the time the label shows up in vari-
ous contexts, e.g. count(lemma=Halam-u 1, previ-
ous token = yHlm). We describe these features in
more detail below.
3 Features and Labels used for Training
Our approach to tagging Arabic differs from conven-
tional approaches in the two-part shrinkage-based
method used, and in the choice of both features and
labels used in our model. For features, we study
both local context variables, as described above, and
document-level word frequencies. For the labels, the
key question is what labels are included and how
they are factored. Standard ?taggers? work by doing
an n-way classification of all the alternatives, which
is not feasible here due to the thousands of possi-
729
ble labels. Standard approaches such as Conditional
Random Fields (CRFs) are intractable with so many
labels. Moreover, few if any taggers do any lemma
disambiguation; that is partly because one must start
with some standard inventory of lemmas, which are
not available for most languages, perhaps because
the importance of lemma disambiguation has been
underestimated.
We make a couple of innovations to deal with
these issues. First, we perform lemma disambigua-
tion in addition to ?tagging?. As mentioned above,
lemmas and morphological information are not in-
dependent; the choice of lemma often influences
morphology and vice versa. For example, Table 1
contains two analyses for the word qbl. For the first
analysis, where the lemma is qabil-a 1 and the gloss
is accept/receive/approve + he/it [verb], the word is
a verb. However, for the second analysis, where the
lemma is qabol 1 and the gloss is before, the word
is a noun.
Simultaneous lemma disambiguation and tagging
introduces additional complexity: An analysis of
ATB and SAMA shows that there are approximately
2,200 possible morphological analyses (?tags?) and
40,000 possible lemmas; even accounting for the
fact that most combinations of lemmas and morpho-
logical analyses don?t occur, the size of the label
space is still in the order of tens of thousands. To
deal with data sparsity, our second innovation is to
factor the labels. We factor each label l into a set of
16 label elements (LEs). These include lemmas, as
well as morphological elements such as basic part-
of-speech, suffix, gender, number, mood, etc. These
are explained in detail below. Thus, since each la-
bel l is a set of 15 categorical variables, each y in
the first learning stage is actually a vector with 16
nonzero components and thousands of zeros. Since
we do simultaneous estimation of the entire set of
label elements, the value g(i, k) being predicted in
the second learning phase is 1 if the entire label set
is correct, and zero otherwise. We do not learn sep-
arate models for each label.
3.1 Label Elements (LEs)
The fact that there are tens of thousands of possible
labels presents the problem of extreme sparsity of
label distribution in the training data. We find that a
model that estimates coefficients ?? to predict a sin-
LE Description
lemma Lemma
pre1 Closer prefix
pre2 Farther prefix
det Determiner
pos Basic POS
dpos Additional data on basic pos
suf Suffix
perpos Person (basic pos)
numpos Number (basic pos)
genpos Gender (basic pos)
persuf Person (suffix)
numsuf Number (suffix)
gensuf Gender (suffix)
mood Mood of verb
pron Pronoun suffix
Table 2: Label Elements (LEs). Examples of additional
data on basic POS include whether a noun is proper or
common, whether a verb is transitive or not, etc. Both
the basic POS and its suffix may have person, gender and
number data.
gle label (a label being in the Cartesian product of
the set of label elements) yields poor performance.
Therefore, as just mentioned, we factor each label
l into a set of label elements (LEs), and learn the
correlations ?? between features and label elements,
rather than features and entire label sets. This re-
duces, but does not come close to eliminating, the
problem sparsity. A complete list of these LEs and
their possible values is detailed in Table 2.
3.2 Features
3.2.1 Local Context Features
We take (t, l) pairs from D2, and for each such
pair generate features Z based on co-occurrence
statistics ?? in D1, as mentioned in Algorithm 2.
These statistics include unigram co-occurrence fre-
quencies of each label with the target token and bi-
gram co-occurrence of the label with the token and
with other tokens or characters in the context of the
target token. We define them formally in Table 3.
Let Zbaseline denote the set of all such basic features
based on the local context statistics of the target to-
ken, namely the words and letters preceding and fol-
lowing it. We will use this set to create a baseline
model.
730
Statistic Description
Freq countD1(t, l)
PrevWord countD1(t, l, t?1)
NextWord countD1(t, l, t+1)
PreviLetter countD1(t, l, first letter(t?1))
NextiLetter countD1(t, l, first letter(t+1)
PrevfLetter countD1(t, l, last letter(t?1)
NextfLetter countD1(t, l, last letter(t+1)
Table 3: Co-occurrence statistics ??. We use these to
generate feature sets for our regression SVMs.
For each label element (LE) e, we define a set of
features Ze similar to Zbaseline; these features are
based on co-occurrence frequencies of the particular
LE e, not the entire label l.
Finally, we define an aggregate feature set Zaggr
as follows:
Zaggr = Zbaseline
?
{Ze} (7)
where e ? {lemma, pre1, pre2, det, pos, dpos,
suf, perpos, numpos, genpos, persuf, numsuf, gensuf,
mood, pron}.
3.2.2 Document Level Features
When trying to predict the lemma, it is useful to
include not just the words and characters immedi-
ately adjacent to the target token, but also the all the
words in the document. These words capture the
?topic? of the document, and help to disambiguate
different lemmas, which tend to be used or not used
based on the topic being discussed, similarly to the
way that word sense disambiguation systems in En-
glish sometimes use the ?bag of words? the docu-
ment to disambiguate, for example a ?bank? for de-
positing money from a ?bank? of a river. More pre-
cisely, we augment the features for each target token
with the counts of each word in the document (the
?term frequency? tf) in which the token occurs with
a given label.
Zfull = Zaggr
?
Ztf (8)
This setZfull is our final feature set. We useZfull
to train an SVM model Mfull; this is our final pre-
dictive model.
3.3 Corpora used for Training and Testing
We use three modules of the Penn Arabic Tree-
bank (ATB) (Maamouri et al, 2004), namely ATB1,
ATB2 and ATB3 as our corpus of labeled Ara-
bic text, D. Each ATB module is a collection
of newswire data from a particular agency. ATB1
uses the Associated Press as a source, ATB2 uses
Ummah, and ATB3 uses Annahar. D contains a total
of 1,835 documents, accounting for approximately
350,000 words. We construct the training and test-
ing setsDtrain andDtest fromD using 10-fold cross
validation, and we constructD1 andD2 fromDtrain
by randomly performing a 9:1 split.
As mentioned earlier, we use the SAMA mor-
phological analyzer to obtain candidate labels C(t)
for each token t while training and testing an SVM
model on D2 and Dtest respectively. A sample out-
put of SAMA is shown in Table 1. To improve cov-
erage, we also add to C(t) all the labels l seen for t
in D1. We find that doing so improves coverage to
98%. This is an upper bound on the accuracy of our
model.
C(t) = SAMA(t)
?
{l|(t, l) ? D1} (9)
4 Results
We use two metrics of accuracy: A1, which mea-
sures the percentage of tokens for which the model
assigns the highest score to the correct label or LE
value (or E1= 100?A1, the corresponding percent-
age error), and A2, which measures the percentage
of tokens for which the correct label or LE value
is one of the two highest ranked choices returned
by the model (or E2 = 100 ? A2). We test our
modelMfull onDtest and achieve A1 and A2 scores
of 90.6% and 96.2% respectively. The accuracy
achieved by our Mfull model is, to the best of our
knowledge, higher than prior approaches have been
able to achieve so far for the problem of combined
morphological and lemma disambiguation. This is
all the more impressive considering that the upper
bound on accuracy for our model is 98% because,
as described above, our set of candidate labels is in-
complete.
In order to analyze how well different LEs can be
predicted, we train an SVM model Me for each LE
e using the feature set Ze, and test all such models
731
on Dtest. The results for all the LEs are reported in
the form of error percentages E1 and E2 in Table 4.
Model E1 E2 Model E1 E2
Mlemma 11.1 4.9 Mpre1 1.9 1.4
Mpre2 0.2 0 Mdet 0.7 0.1
Mpos 23.4 4.0 Mdpos 10.3 1.9
Msuf 7.6 2.5 Mperpos 3.0 0.1
Mnumpos 3.2 0.2 Mgenpos 1.8 0.1
Mpersuf 3.2 0.1 Mnumsuf 8.2 0.5
Mgensuf 11.6 0.4 Mmood 1.6 1.4
Mpron 1.8 0.6 Mcase 14.7 5.9
Mfull 9.4 3.8 - - -
Table 4: Results of Me for each LE e. Note: The results
reported are 10 fold cross validation test accuracies and
no parameters have been tuned on them.
A comparison of the results for Mfull with the
results for Mlemma and Mpos is particularly infor-
mative. We see that Mfull is able to achieve a sub-
stantially lower E1 error score (9.4%) than Mlemma
(11.1%) and Mpos (23.4%); in other words, we find
that our full model is able to predict lemmas and ba-
sic parts-of-speech more accurately than the individ-
ual models for each of these elements.
We examine the effect of varying the size of D2,
i.e. the number of SVM training instances, on the
performance of Mfull on Dtest, and find that with
increasing sizes of D2, E1 reduces only slightly
from 9.5% to 9.4%, and shows no improvement
thereafter. We also find that the use of document-
level features in Mlemma reduces E1 and E2 per-
centages for Mlemma by 5.7% and 3.2% respec-
tively.
4.1 Comparison to Alternate Approaches
4.1.1 Structured Prediction Models
Preliminary experiments showed that knowing the
predicted labels (lemma + morphology) of the sur-
rounding words can slightly improve the predic-
tive accuracy of our model. To further investi-
gate this effect, we tried running experiments us-
ing different structured models, namely CRF (Con-
ditional Random Fields) (Lafferty et al, 2001),
(Structured) MIRA (Margin Infused Relaxation Al-
gorithm) (Crammer et al, 2006) and Structured
Perceptron (Collins, 2002). We used linear chain
CRFs as implemented in MALLET Toolbox (Mc-
Callum, 2001) and for Structured MIRA and Per-
ceptron we used their implementations from EDLIN
Toolbox (Ganchev and Georgiev, 2009). However,
given the vast label space of our problem, running
these methods proved infeasible. The time complex-
ity of these methods scales badly with the number of
labels; It took a week to train a linear chain CRF
for only ? 50 labels and though MIRA and Per-
ceptron are online algorithms, they also become in-
tractable beyond a few hundred labels. Since our
label space contains combinations of lemmas and
morphologies, so even after factoring, the dimension
of the label space is in the order of thousands.
We also tried a na??ve version (two-pass approxi-
mation) of these structured models. In addition to
the features in Zfull, we include the predicted la-
bels for the tokens preceding and following the tar-
get token as features. This new model is not only
slow to train, but also achieves only slightly lower
error rates (1.2% lower E1 and 1.0% lower E2) than
Mfull. This provides an upper bound on the bene-
fit of using the more complex structured models, and
suggests that given their computational demands our
(unstructured) model Mfull is a better choice.
4.1.2 MADA
(Habash and Rambow, 2005) perform morpho-
logical disambiguation using a morphological ana-
lyzer. (Roth et al, 2008) augment this with lemma
disambiguation; they call their system MADA. Our
work differs from theirs in a number of respects.
Firstly, they don?t use the two step regression proce-
dure that we use. Secondly, they use only ?unigram?
features. Also, they do not learn a single model from
a feature set based on labels and LEs; instead, they
combine models for individual elements by using
weighted agreement. We trained and tested MADA
v2.32 using its full feature set on the same Dtrain
andDtest. We should point out that this is not an ex-
act comparison, since MADA uses the older Buck-
walter morphological analyzer.1
4.1.3 Other Alternatives
Unfactored Labels: To illustrate the benefit ob-
tained by breaking down each label l into
1A new version of MADA was released very close to the
submission deadline for this conference.
732
LEs, we contrast the performance of our Mfull
model to an SVM model Mbaseline trained us-
ing only the feature set Zbaseline, which only
contains features based on entire labels, those
based on individual LEs.
Independent lemma and morphology prediction:
Another alternative approach is to pre-
dict lemmas and morphological analyses
separately. We construct a feature set
Zlemma? = Zfull ? Zlemma and train an SVM
model Mlemma? using this feature set. Labels
are then predicted by simply combining the
results predicted independently by Mlemma
and Mlemma? . Let Mind denote this approach.
Unigram Features: Finally, we also consider a
context-less approach, i.e. using only ?uni-
gram? features for labels as well as LEs. We
call this feature set Zuni, and the correspond-
ing SVM model Muni.
The results of these various models, along with
those of Mfull are summarized in Table 5. We see
thatMfull has roughly half the error rate of the state-
of-the-art MADA system.
Model E1 E2
Mbaseline 13.6 9.1
Mind 18.7 6.0
Muni 11.6 6.4
Mcheat 8.2 2.8
MADA 16.9 12.6
Mfull 9.4 3.8
Table 5: Percent error rates of alternative approaches.
Note: The results reported are 10 fold cross validation
test accuracies and no parameters have been tuned on
them. We used same train-test splits for all the datasets.
5 Related Work
(Hajic, 2000) show that for highly inflectional
languages, the use of a morphological analyzer
improves accuracy of disambiguation. (Diab et
al., 2004) perform tokenization, POS tagging
and base phrase chunking using an SVM based
learner. (Ahmed and Nu?rnberger, 2008) perform
word-sense disambiguation using a Naive Bayesian
model and rely on parallel corpora and match-
ing schemes instead of a morphological ana-
lyzer. (Kulick, 2010) perform simultaneous tok-
enization and part-of-speech tagging for Arabic by
separating closed and open-class items and focus-
ing on the likelihood of possible stems of open-
class words. (Mohamed and Ku?bler, 2010) present
a hybrid method between word-based and segment-
based POS tagging for Arabic and report good re-
sults. (Toutanova and Cherry, 2009) perform joint
lemmatization and part-of-speech tagging for En-
glish, Bulgarian, Czech and Slovene, but they do
not use the two step estimation-shrinkage model de-
scribed in this paper; nor do they factor labels. The
idea of joint lemmatization and part-of-speech tag-
ging has also been discussed in the context of Hun-
garian in (Kornai, 1994).
A substantial amount of relevant work has been
done previously for Hebrew. (Adler and Elhadad,
2006) perform Hebrew morphological disambigua-
tion using an unsupervised morpheme-based HMM,
but they report lower scores than those achieved by
our model. Moreover, their analysis doesn?t include
lemma IDs, which is a novelty of our model. (Gold-
berg et al, 2008) extend the work of (Adler and El-
hadad, 2006) by using an EM algorithm, and achieve
an accuracy of 88% for full morphological analy-
sis, but again, this does not include lemma IDs. To
the best of our knowledge, there is no existing re-
search for Hebrew that does what we did for Arabic,
namely to use simultaneous lemma and morpholog-
ical disambiguation to improve both. (Dinur et al,
2009) show that prepositions and function words can
be accurately segmented using unsupervised meth-
ods. However, by using this method as a preprocess-
ing step, we would lose the power of a simultaneous
solution for these problems. Our method is closer in
style to a CRF, giving much of the accuracy gains of
simultaneous solution, while being about 4 orders of
magnitude easier to train.
We believe that our use of factored labels is novel
for the problem of simultaneous lemma and mor-
phological disambiguation; however, (Smith et al,
2005) and (Hatori et al, 2008) have previously
made use of features based on parts of labels in
CRF models for morphological disambiguation and
word-sense disambiguation respectively. Also, we
note that there is a similarity between our two-stage
733
machine learning approach and log-linear models in
machine translation that break the data in two parts,
estimating log-probabilities of generative models
from one part, and discriminatively re-weighting the
models using the second part.
6 Conclusions
We introduced a new approach to accurately predict
labels consisting of both lemmas and morphologi-
cal analyses for Arabic text. We obtained an accu-
racy of over 90% ? substantially higher than current
state-of-the-art systems. Key to our success is the
factoring of labels into lemma and a large set of mor-
phosyntactic elements, and the use of an algorithm
that computes a simple initial estimate of the coef-
ficient relating each contextual feature to each la-
bel element (simply by counting co-occurrence) and
then regularizes these features by shrinking each of
the coefficients for each feature by an amount deter-
mined by supervised learning using only the candi-
date label sets produced by SAMA.
We also showed that using features of word n-
grams is preferable to using features of only individ-
ual tokens of data. Finally, we showed that a model
using a full feature set based on labels as well as
factored components of labels, which we call label
elements (LEs) works better than a model created
by combining individual models for each LE. We
believe that the approach we have used to create our
model can be successfully applied not just to Arabic
but also to other languages such as Turkish, Hungar-
ian and Finnish that have highly inflectional mor-
phology. The current accuracy of of our model, get-
ting the correct answer among the top two choices
96.2% of the time is high enough to be highly use-
ful for tasks such as aiding the manual annotation
of Arabic text; a more complete automation would
require that accuracy for the single top choice.
Acknowledgments
We woud like to thank everyone at the Linguis-
tic Data Consortium, especially Christopher Cieri,
David Graff, Seth Kulick, Ann Bies, Wajdi Za-
ghouani and Basma Bouziri for their help. We also
wish to thank the anonymous reviewers for their
comments and suggestions.
References
Meni Adler and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics.
Farag Ahmed and Andreas Nu?rnberger. 2008. Ara-
bic/English Word Translation Disambiguation using
Parallel Corpora and Matching Schemes. In Proceed-
ings of EAMT?08, Hamburg, Germany.
Tim Buckwalter. 2004. Buckwalter Arabic Morphologi-
cal Analyzer version 2.0.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP?02.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic text: From Raw Text to
Base Phrase Chunks. In Proceedings of the 5th Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics/Human Language
Technologies Conference (HLT-NAACL?04).
Elad Dinur, Dmitry Davidov, and Ari Rappoport. 2009.
Unsupervised Concept Discovery in Hebrew Using
Simple Unsupervised Word Prefix Segmentation for
Hebrew and Arabic. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages.
Kuzman Ganchev and Georgi Georgiev. 2009. Edlin:
An Easy to Read Linear Learning Framework. In Pro-
ceedings of RANLP?09.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM Can Find Pretty Good HMM POS-Taggers (When
Given a Good Start)*. In Proceedings of ACL?08.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
ACL?05, Ann Arbor, MI, USA.
Jan Hajic. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL?00).
Jun Hatori, Yusuke Miyao, and Jun?ichi Tsujii. 2008.
Word Sense Disambiguation for All Words using Tree-
Structured Conditional Random Fields. In Proceed-
ings of COLing?08.
W. James and Charles Stein. 1961. Estimation with
Quadratic Loss. In Proceedings of the Fourth Berkeley
734
Symposium on Mathematical Statistics and Probabil-
ity, Volume 1.
Andra?s Kornai. 1994. On Hungarian morphology (Lin-
guistica, Series A: Studia et Dissertationes 14). Lin-
guistics Institute of Hungarian Academy of Sciences,
Budapest.
Seth Kulick. 2010. Simultaneous Tokenization and Part-
of-Speech Tagging for Arabic without a Morphologi-
cal Analyzer. In Proceedings of ACL?10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML?01, pages 282?289.
Mohamed Maamouri, Ann Bies, and Tim Buckwalter.
2004. The Penn Arabic Treebank: Building a Large
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR Conference on Arabic Language Resources
and Tools.
Mohamed Maamouri, David Graff, Basma Bouziri, Son-
dos Krouna, and Seth Kulick. 2009. LDC Standard
Arabic Morphological Analyzer (SAMA) v. 3.0.
Andrew McCallum, 2001. MALLET: A Machine Learn-
ing for Language Toolkit. Software available at
http://mallet.cs.umass.edu.
Emad Mohamed and Sandra Ku?bler. 2010. Arabic Part
of Speech Tagging. In Proceedings of LREC?10.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL?08, Columbus, Ohio, USA.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-Based Morphological Disambiguation
with Random Fields*. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Kristina Toutanova and Colin Cherry. 2009. A Global
Model for Joint Lemmatization and Part-of-Speech
Prediction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 486?494.
735
Proceedings of NAACL-HLT 2013, pages 550?555,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Derivation Trees for Informative Treebank Inter-Annotator
Agreement Evaluation
Seth Kulick and Ann Bies and Justin Mott
and Mohamed Maamouri
Linguistic Data Consortium
University of Pennsylvania
{skulick,bies,jmott,maamouri}
@ldc.upenn.edu
Beatrice Santorini and
Anthony Kroch
Department of Linguistics
University of Pennsylvania
{beatrice,kroch}
@ling.upenn.edu
Abstract
This paper discusses the extension of a sys-
tem developed for automatic discovery of tree-
bank annotation inconsistencies over an entire
corpus to the particular case of evaluation of
inter-annotator agreement. This system makes
for a more informative IAA evaluation than
other systems because it pinpoints the incon-
sistencies and groups them by their structural
types. We evaluate the system on two corpora
- (1) a corpus of English web text, and (2) a
corpus of Modern British English.
1 Introduction
This paper discusses the extension of a system de-
veloped for automatic discovery of treebank annota-
tion inconsistencies over an entire corpus to the par-
ticular case of evaluation of inter-annotator agree-
ment (IAA). In IAA, two or more annotators anno-
tate the same sentences, and a comparison identi-
fies areas in which the annotators might need more
training, or the annotation guidelines some refine-
ment. Unlike other IAA evaluation systems, this
system application results in a precise pinpointing of
inconsistencies and the grouping of inconsistencies
by their structural types, making for a more infor-
mative IAA evaluation.
Treebank annotation, consisting of syntactic
structure with words as the terminals, is by its na-
ture more complex and therefore more prone to error
than many other annotation tasks. However, high an-
notation consistency is crucial to providing reliable
training and testing data for parsers and linguistic
research. Error detection is therefore an important
area of research, and the importance of work such as
Dickinson and Meurers (2003) is that errors and an-
notation inconsistencies might be automatically dis-
covered, and once discovered, be targeted for subse-
quent quality control.
A recent approach to this problem (Kulick et al,
2011; Kulick et al, 2012) (which we will call the
KBM system) improves upon Dickinson and Meur-
ers (2003) by decomposing the full syntactic tree
into smaller units, using ideas from Tree Adjoining
Grammar (TAG) (Joshi and Schabes, 1997). This al-
lows the comparison to be based on small syntactic
units instead of string n-grams, improving the detec-
tion of inconsistent annotation.
The KBM system, like that of Dickinson and
Meurers (2003) before it, is based on the notion of
comparing identical strings. In the general case, this
is a problematic assumption, since annotation in-
consistencies are missed because of superficial word
differences between strings which one would want
to compare.1 However, this limitation is not present
for IAA evaluation, since the strings to compare are,
by definition, identical.2 The same is also true of
parser evaluation, since the parser output and the
gold standard are based on the same sentences.
We therefore take the logical step of applying the
KBM system developed for automatic discovery of
annotation inconsistency to the special case of IAA.3
1Boyd et al (2007) and other current work tackles this prob-
lem. However, that is not the focus of this paper.
2Aside from possible tokenization differences by annotators.
3In this paper, we do not yet apply the system to parser eval-
uation, although it is conceptually the same problem as IAA
evaluation. We wanted to first refine the system using annota-
tor input for the IAA application before applying it to parser
550
(1) a. NP-SBJ
NP
NP
The word
NP
renaissance
NP
-LRB- NP
Rinascimento
PP
in NP
Italian
-RRB-
b. NP-SBJ
The word renaissance PRN
-LRB- FRAG
NP
Rinascimento
PP
in NP
Italian
-RRB-
Figure 1: Two example trees showing a difference in IAA
To our knowledge, this work is the first to utilize
such a general system for this special case.
The advantages of the KBM system play out
somewhat differently in the context of IAA evalu-
ation than in the more general case. In this con-
text, the comparison of word sequences based on
syntactic units allows for a precise pinpointing of
differences. The system also retains the ability to
group inconsistencies together by their structural
type, which we have found to be useful for the more
general case. Together, these two properties make
for a useful and informative system for IAA evalua-
tion.
In Section 2 we describe the basic working of our
system. In Section 3 we discuss in more detail the
advantages of this approach. In Section 4 we evalu-
ate the system on two treebanks, a corpus of English
web text and a corpus of Modern British English.
Section 5 discusses future work.
2 System Overview
The basic idea of the KBM system is to detect word
sequences that are annotated in inconsistent ways by
evaluation.
wordNP
The RinascimentoNP -RRB--LRB-renaissance
NP
renaissanceNPb1The -RRB--LRB- Rinascimento
FRAGword PRN
a1
a2
a3
a4
a5
a8
b2
b3 b5
b4 b8
inPPa6 ItalianNP a7
inPP b6 ItalianNPb7
NP
A
A
A A A A
A
A
MM MM M
A
(2) a.
b.
Figure 2: E-trees and derivation trees corresponding to
(1ab)
comparing local syntactic units. Following Dickin-
son and Meurers (2003), we refer to sequences ex-
amined for inconsistent annotation as nuclei. The
sentence excerpts (1ab) in Figure 1, from the test
corpora used in this work, illustrate an inconsistency
in the annotation of corresponding strings. We fo-
cus here on the difference in the annotation of the
nucleus The word renaissance, which in (1a) is an-
notated as an appositive structure, while in (1b) it is
flat.
Following the TAG approach, KBM decomposes
the full phrase structure into smaller chunks called
elementary trees (henceforth, e-trees). The relation-
ship of the e-trees underlying a full phrase struc-
ture to each other is recorded in a derivation tree,
in which each node is an e-tree, related to its par-
ent node by a composition operation, as shown in
(2ab).4
KBM uses two composition operations, each with
left and right variants, shown in Figure 3: (1) ad-
4The decomposition is based on head-finding heuristics,
with the result here that word is the head of (1a), while renais-
sance is the head of (1b), as reflected in their respective deriva-
tion trees (2a) and (2b). We omit the POS tags in (1ab) and
(2ab) to avoid clutter.
551
wo ro wo wo ro
dNPTheRinaschNmswot wo
-BLhebN1hBFeRinaschNms
ro t ro
-BLheRinaschNmswot wo
dNPThebN1hBFeRinaschNmsro
tro
woro ro wo wo
Figure 3: Composition operations (left and right)
junction, which attaches one tree to a target node in
another tree by creating a copy of the target node,
and (2) sister adjunction, which attaches one tree as
a sister to a target node in another tree. Each arc in
Figure 2 is labeled by an ?M? for adjunction and ?A?
for sister-adjunction. 5
The system uses the tree decomposition and re-
sulting derivation tree for the comparison of differ-
ent instances of the same nucleus. The full deriva-
tion tree for a sentence is not used, but rather only
that slice of it having e-trees with words that are in
the nucleus being examined, which we call a deriva-
tion tree fragment. That is, for a given nucleus with
a set of instances, we compare the derivation frag-
ments for each instance.
For example, for the nucleus The word renais-
sance, the derivation tree fragment for the instance
in (1a) consists of the e-trees a1, a2, a3 (and their
arcs) in (2a), and likewise the derivation tree from
the instance in (1b) consists of the e-trees b1, b2, b3
in (2b). These derivation fragments have a differ-
ent structure, and so the two instances of The word
renaissance are recognized as inconsistent.
Two important aspects of the overall system re-
quire mention here: (1) Nuclei are identified by us-
ing sequences that occur as a constituent anywhere
5KBM is based on a variant of Spinal TAG (Shen et al,
2008), and uses sister adjunction without substitution. Space
prohibits full discussion, but multiple adjunction to a single
node (e.g., a4, a6, a8 to a5 in (2a)) does not create multiple
levels of recursion, while a special specification handles the ex-
tra NP recursion for the apposition with a2, a3, and a5. For
reasons of space, we also leave aside a precise comparison to
Tree Insertion Grammar (Chiang, 2003) and Spinal TAG (Shen
et al, 2008).
in the corpus, even if other instances of the same
sequence are not constituents. Both instances of
The word renaissance are compared, because the
sequence occurs at least once as a constituent. (2)
We partition each comparison of the instances of a
nucleus by the lowest nonterminal in the derivation
tree fragment that covers the sequence. The two in-
stances of The word renaissance are compared be-
cause the lowest nonterminal is an NP in both in-
stances.
3 Advantages of this approach
As Kulick et al (2012) stressed, using derivation
tree fragments allows the comparison to abstract
away from interference by irrelevant modifiers, an
issue with Dickinson and Meurers (2003). However,
in the context of IAA, this advantage of KBM plays
out in a different way, in that it allows for a pre-
cise pinpointing of the inconsistencies. For IAA,
the concern is not whether an inconsistent annota-
tion will be reported, since at some level higher in
the tree every difference will be found, even if the
context is the entire tree. KBM, however, will find
the inconsistencies in a more informative way, for
example reporting just The word renaissance, not
some larger unit. Likewise, it reports Rinascimento
in Italian as an inconsistently annotated sequence.6
A critical desirable property of KBM that carries
over from the more general case is that it allows for
different nuclei to be grouped together in the sys-
tem?s output if they have the same annotation in-
consistency type. As in Kulick et al (2011), each
nucleus found to be inconsistent is categorized by
an inconsistency type, which is simply the collec-
tion of different derivation tree fragments used for
the comparison of its instances, including POS tags
but not the words. For example, the inconsistency
type of the nucleus The word renaissance in (1ab) is
the pair of derivation tree fragments (a1,a2,a3) and
(b1,b2,b3) from (2ab), with the POS tags. This nu-
6Note however that it does not report -LRB- Rinascimento
in Italian -RRB- which is also a constituent, and so might be
expected to be compared. The lowest nonterminal above this
substring in the two derivation trees in Figure 2 is the NP in a5
and the FRAG in b5, thus exempting them from comparison. It
is exactly this sort of case that motivated the ?external check?
discussed in Kulick et al (2012), which we have not yet imple-
mented for IAA.
552
Inconsistency type # Found # Accurate
Function tags only 53 53
POS tags only 18 13
Structural 129 122
Table 1: Inconsistency types found for system evaluation
cleus is then reported together with other nuclei that
use the same derivation fragments. In this case, it
therefore also reports the nucleus The term renais-
sance, which appears elsewhere in the corpus with
the two annotations from the different annotators as
in (3):
(3) a. NP
NP
The term
NP
renaissance
b. NP
The term renaissance
KBM reports The word renaissance and The term
renaissance together because they are inconsistently
annotated in exactly the same way, in spite of the dif-
ference in words. This grouping together of incon-
sistencies based on structural characteristics of the
inconsistency is critically important for understand-
ing the nature of the annotation inconsistencies.
It is the combination of these two characteristics -
(1) pinpointing of errors and (2) grouping by struc-
ture - that makes the system so useful for IAA. This
is an improvement over alternatives such as using
evalb (Sekine and Collins, 2008) for IAA. No other
system to our knowledge groups inconsistencies by
structural type, as KBM does. The use of the deriva-
tion tree fragments greatly lessens the multiple re-
porting of a single annotation difference, which is
a difficulty for using evalb (Manning and Schuetze,
1999, p. 436) or Dickinson and Meurers (2003).
4 Evaluation
4.1 English web text
We applied our approach to pre-release subset of
(Bies et al, 2012), dually annotated and used for
annotator training, from which the examples in Sec-
tions 2 and 3 are taken. It is a small section of the
corpus, with 4,270 words dually annotated.
For this work, we also took the further step of
characterizing the inconsistency types themselves,
allowing for an even higher-level view of the incon-
sistencies found. In addition to grouping together
different strings as having the same inconsistent an-
notation, the types can also be grouped together for
comparison at a higher level. For this IAA sample,
we separated the inconsistency types into the three
groups in Table 1, with the derivation tree fragments
differing (1) only on function tags, (2) only on POS
tags7, and (3) on structural differences. We man-
ually examined each inconsistency group to deter-
mine if it was an actual inconsistency found, or a
spurious false positive. As shown in Table 1, the pre-
cision of the reported inconsistencies is very high.
It is in fact even higher than it appears, because
the seven (out of 129) instances incorrectly listed
as structural problems were actually either POS or
function tag inconsistencies, that were discovered
by the system only by a difference in the derivation
tree fragment, and so were categorized as structural
problems instead of POS or function tag inconsis-
tencies. 8
Because of the small size of the corpus, there
are relatively few nuclei grouped into inconsistency
types. The 129 structural inconsistency types in-
clude 130 nuclei, with the only inconsistency type
with more than one nucleus being the type with The
word renaissance and The term renaissance, as dis-
cussed above. There is more grouping together in
the ?POS tags only? case (37 nuclei included in
the 18 inconsistency types), and the ?function tags
only? case (56 nuclei included in the 53 inconsis-
tency types).
4.2 Modern British English corpus
We also applied our approach to a supplemental sec-
tion (Kroch and Santorini, in preparation) to a cor-
pus of modern British English (Kroch et al, 2010),
part of a series of corpora used for research into lan-
guage change. The annotation style is similar to that
of the Penn Treebank, although with some differ-
ences. In this case, because neither the function tags
nor part-of-speech tags were part of the IAA work,
7As mentioned in footnote 4, although POS tags were left
out of Figure 2 for readability, they are included in the actual e-
trees. This allows POS differences in a similar syntactic context
to be naturally captured within the overall KBM framework.
8A small percentage of inconsistencies are the result of lin-
guistic ambiguities and not an error by one of the annotators.
553
we do not separate out the inconsistency types, as
done in Section 4.1.
The supplement section consisted of 82,701
words dually annotated. The larger size, as com-
pared with the corpus in Section 4.1, results in some
differences in the system output. Because of the
larger size, there are more substantial cases of dif-
ferent nuclei grouped together as the same inconsis-
tency type than in Section 4.1. The first inconsis-
tency type (sorted by number of nuclei) has 88 nu-
clei, and the second has 37 nuclei. In total, there are
1,532 inconsistency types found, consisting of 2,194
nuclei in total. We manually examined the first 20
inconsistency types (sorted by number of nuclei),
consisting in total of 375 nuclei. All were found to
be true instances of inconsistent annotation.
(4) a. NP
the ADJP
only true
thing
b. NP
the only true thing
(5) a. NP
their ADJP
only actual
argument
b. NP
their only actual argument
The trees in (4) and (5) show two of the 88 nu-
clei grouped into the first inconsistency type. As
with The word renaissance and The term renais-
sance in the English web corpus, nuclei with similar
(although not identical) words are often grouped into
the same inconsistency type. To repeat the point,
this is not because of any search for similarity of
the words in the nuclei. It arises from the fact that
the nuclei are annotated inconstantly in the same
way. Of course not all nuclei in an inconsistency
type have the same words. Nuclei found in this in-
consistency type include only true and only actual
as shown above, and also nuclei such as new En-
glish, greatest possible, thin square, only necessary.
Taken together, they clearly indicate an issue with
the annotation of multi-word adjective phrases.9
9Note that the inconsistencies discussed throughout this pa-
per are not taken from the the published corpora. These results
are only from internal annotator training files.
5 Future work
There are several ways in which we plan to improve
the current approach. As mentioned above, there is
a certain class of inconsistencies which KBM will
not pinpoint precisely, which requires adopting the
?external check? from Kulick et al (2012). The ab-
straction on inconsistency types described in Sec-
tion 4 can also be taken further. For example, one
might want to examine in particular inconsistency
types that arise from PP attachment or that have to
do with the PRN function tag.
One main area for future work is the application
of this work to parser evaluation as well as IAA. For
this area, there is some connection to the work of
Goldberg and Elhadad (2010) and Dickinson (2010),
which are both concerned with examining depen-
dency structures of more than one edge. The con-
nection is that those works are focused on depen-
dency representations, and ithe KBM system does
phrase structure analysis using a TAG-like deriva-
tion tree, which strongly resembles a dependency
tree (Rambow and Joshi, 1997). There is much in
this area of common concern that is worth examin-
ing further.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-11-C-0145.
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred. This applies to the
first four authors. The first, fifth, and sixth authors
were supported in part by National Science Foun-
dation Grant # BCS-114749. We would also like
to thank Colin Warner, Aravind Joshi, Mitch Mar-
cus, and the computational linguistics group at the
University of Pennsylvania for helpful conversations
and feedback.
554
References
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Adriane Boyd, Markus Dickinson, and Detmar Meurers.
2007. Increasing the recall of corpus annotation er-
ror detection. In Proceedings of the Sixth Workshop
on Treebanks and Linguistic Theories (TLT 2007),
Bergen, Norway.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted Tree Adjoining Grammar. In Data
Oriented Parsing. CSLI.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing inconsistencies in treebanks. In Proceedings of the
Second Workshop on Treebanks and Linguistic The-
ories (TLT 2003), Sweden. Treebanks and Linguistic
Theories.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 729?738,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Inspecting
the structural biases of dependency parsing algorithms.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 234?
242, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, Volume 3: Beyond
Words, pages 69?124. Springer, New York.
Anthony Kroch and Beatrice Santorini. in preparation.
Supplement to the Penn Parsed Corpus of Modern
British English.
Anthony Kroch, Beatrice Santorini, and Ariel Dier-
tani. 2010. Penn Parsed Corpus of Mod-
ern British English. http://www.ling.upenn.edu/hist-
corpora/PPCMBE-RELEASE-1/index.html.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 693?698, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Further
developments in treebank error detection using deriva-
tion trees. In LREC 2012: 8th International Confer-
ence on Language Resources and Evaluation, Istanbul.
Christopher Manning and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167?190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new re-
source for incremental, dependency and semantic pars-
ing. Language Resources and Evaluation, 42(1):1?19.
555
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 1?10,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Correction and Extension of Morphological Annotations
Ramy Eskander, Nizar Habash
Center for Computational Learning Systems, Columbia University
{reskander,habash}@ccls.columbia.edu
Ann Bies, Seth Kulick, Mohamed Maamouri
Linguistic Data Consortium, University of Pennsylvania
{bies,skulick,maamouri}@ldc.upenn.edu
Abstract
For languages with complex morpholo-
gies, limited resources and tools, and/or
lack of standard grammars, developing an-
notated resources can be a challenging
task. Annotated resources developed un-
der time/money constraints for such lan-
guages tend to tradeoff depth of represen-
tation with degree of noise. We present
two methods for automatic correction and
extension of morphological annotations,
and demonstrate their success on three di-
vergent Egyptian Arabic corpora.
1 Introduction
Annotated corpora are essential for most research
in natural language processing (NLP). For exam-
ple, the development of treebanks, such as the
Penn Treebank and the Penn Arabic Treebank,
has been essential in pushing research on part-
of-speech (POS) tagging and parsing of English
and Arabic (Marcus et al, 1993; Maamouri et al,
2004). The creation of such resources tends to be
quite expensive and time consuming: guidelines
need to be developed, annotators hired, trained,
and regularly evaluated for quality control. For
languages with complex morphologies, limited re-
sources and tools, and/or lack of standard gram-
mars, such as any of the Dialectal Arabic (DA)
varieties, developing annotated resources can be a
challenging task. As a result, annotated resources
developed under time/money constraints for such
languages tend to tradeoff depth of representation
with degree of noise. In the extremes, we find rich
morphological representations that may be noisy
and inconsistent or simple by highly consistent
and reliable annotations that have limited usabil-
ity. Furthermore, such resources are often devel-
oped by different research groups leading to many
inconstancies that make pooling these resources
not a very easy task.
In this paper, we describe two general tech-
niques to address the limitations of the two types
of annotations: corrections of rich noisy annota-
tions and extensions of clean but shallow ones.
We present our work on Egyptian Arabic, an im-
portant Arabic dialect with limited resources, and
rich and ambiguous morphology. Resulting from
this effort is the largest Egyptian Arabic corpus
annotated in one common representation by pool-
ing resources from three very different sources:
a non-final, pre-release version of the ARZ1 cor-
pora from the Linguistic Data Consortium (LDC)
(Maamouri et al, 2012g), the LDC?s CallHome
Egypt transcripts (Gadalla et al, 1997) and CMU?s
Egyptian Arabic corpus (CMUEAC) (Mohamed et
al., 2012).
Although the paper focuses on Arabic, the ba-
sic problem is relevant to other languages, espe-
cially spontaneously written colloquial language
forms such as those used in social media. The
general solutions we propose are language inde-
pendent given availability of specific language re-
sources.
Next we discuss some related work and rel-
evant linguistic facts (Sections 2 and 3, respec-
tively). Section 4 presents our annotation cor-
rection technique; and Section 5 presents out an-
notation extension technique. Finally, Section 6
presents some statistics on the Egyptian Arabic
corpus annotated in one unified representation re-
sulting from our correction and extension work.
2 Related Work
Much work has been done on automatic spelling
correction. Both supervised and unsupervised ap-
proaches have been used employing a variety of
1ARZ is the language code for Egyptian Ara-
bic, http://www-01.sil.org/iso639-3/
documentation.asp?id=arz
1
tools, resources, and heuristics, e.g., morpholog-
ical analyzers, language models, annotated data
and edit-distance measures, respectively (Kukich,
1992; Oflazer, 1996; Shaalan et al, 2003; Hassan
et al, 2008; Kolak and Resnik, 2002; Magdy and
Darwish, 2006). Our work is different from these
approaches in that it extends beyond spelling of
word forms to deeper annotations. However, we
use some of these techniques to correct not just
the words, but also malformed POS tags.
A number of efforts exist on treebank en-
richment for many languages including Arabic
(Palmer et al, 2008; Hovy et al, 2006; Alkuh-
lani and Habash, 2011; Alkuhlani et al, 2013).
Our morphological extension effort is similar to
Alkuhlani et al (2013)?s work except that they
start with tokenizations, reduced POS tags and de-
pendency trees and extend them to full morpho-
logical information.
There has been a lot of work on Arabic POS tag-
ging and morphological disambiguation (Habash
and Rambow, 2005; Smith et al, 2005; Hajic? et
al., 2005; Habash, 2010; Habash et al, 2013).
The work by Habash et al (2013) uses one of the
resources we improve on in this paper. In their
work, they simply attempt to ?synchronize? un-
known/malformed annotations with the morpho-
logical analyzer they use, thus forcing a reading on
the word to make the unknown/malformed annota-
tion usable. In our work, we address the cleaning
issue directly. We intend to make these automatic
corrections and extensions available in the future
so that they can be used in future disambiguation
tools.
Maamouri et al (2009) described a set of man-
ual and automatic techniques used to improve on
the quality of the Penn Arabic Treebank. Their
work is most similar to ours except in the follow-
ing aspects: we work only on morphology and for
dialectal Arabic, whereas their work is primarily
on syntax and standard Arabic. Furthermore, the
challenge of malformed tags is not a major prob-
lem for them, while it is a core problem for us.
Furthermore, we work with data that has partial
annotations that we extend, while their work was
for very rich syntax/morphology annotations.
3 Linguistic Facts
The Arabic language is a collection of variants,
most prominent amongst which is Modern Stan-
dard Arabic (MSA), the official language of the
media and education. The other variants, the Ara-
bic dialects, are the day-to-day native vernaculars
spoken in the Arab World. While MSA is the of-
ficial language, it is not the native language of any
modern day Arabic speakers. Their differences
from MSA are comparable to the differences be-
tween Romance languages and Latin.2
Egyptian Arabic poses many challenges for
NLP. Arabic in general is a morphologically com-
plex language which includes rich inflectional
morphology, expressed both templatically and af-
fixationally, and several classes of attachable cl-
itics. For example, the Egyptian Arabic word
A??J.

J?J
?? wi+ha+yi-ktib-uw+hA
3 ?and they will
write it? has two proclitics (+? wi+ ?and? and + ?
ha+ ?will?), one prefix -?


yi- ?3rd person?, one
suffix ?- -uw ?masculine plural? and one pronom-
inal enclitic A?+ +hA ?it/her?. The word is consid-
ered an inflected form of the lemma katab ?write
[lit. he wrote]?. An important challenge for NLP
work on dialectal Arabic in general is the lack of
an orthographic standard. Egyptian Arabic writ-
ers are often inconsistent even in their own writ-
ing (Habash et al, 2012a), e.g., the future particle
h Ha appears as a separate word or as a proclitic
+h/+? Ha+/ha+, reflecting different pronuncia-
tions. Arabic orthography in general drops dia-
critical marks that mark short vowels and gemi-
nation. However in analyses, we want these dia-
critics to be indicated. Moreover, some letters in
Arabic (in general) are often spelled inconsistently
which leads to an increase in both sparsity (multi-
ple forms of the same word) and ambiguity (same
form corresponding to multiple words), e.g., vari-
ants of Hamzated Alif,

@ ? or @ A?, are often writ-
ten without their Hamza (Z ?): @ A; and the Alif-
Maqsura (or dotless Ya) ? ? and the regular dotted
Ya ?


y are often used interchangeably in word fi-
nal position (El Kholy and Habash, 2010). For the
purposes of normalizing the representations used
in computational models, we follow the work of
Habash et al (2012a) who devised a conventional
orthography for dialectal Arabic (CODA) for use
in computational processing of Arabic dialects..
An analysis of an Egyptian word for our work
consists of a surface form that may not be in
2Habash and Rambow (2006) reported that a state-of-the-
art MSA morphological analyzer has only 60% coverage of
Levantine Arabic verb forms.
3Arabic orthographic transliteration is presented in the
Habash-Soudi-Buckwalter scheme (Habash et al, 2007):
@ H.
H H h. h p X
	
XP 	P ? ? ?
	
? ?
	
? ?
	
?
	
?

? ? ? ?
	
? ? ? ?


A b t ? j H x d?r z s ? S D T D? ? ? f q k l m n hw y
in addition to ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', ~ ?, ? ?.
2
CODA (henceforth, RAW), a fully diacritized
CODA form (henceforth, DIAC), a morpheme
split form (henceforth, MORPH), which may
slightly differ from the allomorphic DIAC surface
forms, a POS tag for each morpheme and stem,
and a lemma (henceforth LEM). For instance,
the Egyptian Arabic example used above has the
following analysis:
RAW whyktbuwhA
DIAC wiHayiktibuwhA
MORPH wi+Ha+yi+ktib+uwA+hA
POS CONJ+FUT_PART+IV3P+IV
+IVSUFF_SUBJ:3P+IVSUFF_DO:3FS
LEM katab
The morphological analyzers we use in the pa-
per, CALIMA (Habash et al, 2012b) and SAMA
(Graff et al, 2009), both generate the different lev-
els of representation discussed above.
4 Automatic Morphological Correction
In this section, we present the effort on auto-
matic morphological correction of rich noisy an-
notations. We next describe the data set we work
with and the problems it has. This is followed by
a discussion of our approach and results including
an error analysis.
4.1 Data
We use a non-final, pre-release version of six man-
ually annotated Egyptian Arabic corpora devel-
oped by the LDC, and labeled as ?ARZ?, parts one
through six. The published versions of these cor-
pora (Maamouri et al, 2012a-f) do not include the
annotation errors discussed in this paper. Rather,
in the official releases of the data from the LDC,
such problematic cases with an unknown POS tag
sequence (as in the example at the end of Sec-
tion 4.2) were caught and given a NO_FUNC POS
tag instead, in order to allow syntactic annotation
of the data to proceed, and in order to meet data
publication deadlines. The combined corpus con-
sists of about 274K words. The annotations are
very detailed contextually selected morphological
analyses that include for each RAW word its LEM,
POS, MORPH and DIAC as described earlier. The
LDC used the CALIMA4 Egyptian Arabic mor-
phological analyzer (Habash et al, 2012b) to pro-
vide the annotators with sets of analyses to se-
lect from.5 CALIMA?s non-lexical morphologi-
4Columbia Arabic Language and dIalect Morphological
Analyzer
5SAMA, the Standard Arabic Morphological Analyzer
(Graff et al, 2009), was used to provide the annotators with
cal coverage (i.e. model of affixes and stem POS
combinations) is almost complete; and its lexical
entries are of high precision. However, CALIMA
lacks some lexical items, i.e., its lexical recall is
not perfect ? Habash et al (2012b) report coverage
of 84% for basic CALIMA and 92% for CALIMA
extended with SAMA (Graff et al, 2009) (hence-
forth, CALIMA+SAMA or simply the analyzer).6
Many missing entries are a result of spelling vari-
ants that are not modeled in CALIMA. In cases
when CALIMA fails to provide analyses or the
annotators disagree with all the provided analy-
ses, the annotators enter the information manually
or copy and modify CALIMA provided analyses,
which sometimes introduces errors.
For the purpose of this work, we consider
all analyses in the corpus that are in the CAL-
IMA+SAMA morphological analyzer to be cor-
rect. We will not attempt to modify them. Al-
most 30% of the corpus analyses are not in the
analyzer, i.e. analyzer out-of-vocabulary (OOV).
We discuss next the general patterns of these anal-
yses. We refer to the original corpus analyses as
the ?Baseline? analyses.
4.2 Patterns of OOV Analyses in Baseline
About 3.3% of all OOV analyses (and 1% of all
corpus words) are tagged as TYPOs.7 We do not
address these cases in this paper.
Over half of the POS OOVs (56%) in the
pre-release data involve a different category of
a nominal (NOUN/NOUN_PROP/ADJ). This is
a well known issue even in MSA. The rest
of the cases involve incorrect feature combina-
tions such as giving the unaccusative verb
	
Y
	
?
	
J

K @
Aitnaf?i? ?be performed? the POS PV_PASS
(passive perfective).8 Another example is assign-
ing the feminine singular pronoun ?


X diy the
POS DEM_PRON instead of DEM_PRON_FS.
Or the imperative verb @? 	?? @ AilguwA ?cancel [you
plural]? the POS CV+CVSUFF_SUBJ:2MS (for
?you masculine singular?) instead of the correct
CV+CVSUFF_SUBJ:2MP. A tiny percentage of
all POS tags in the corpus (0.02%) include case-
related variation (e.g. CONJ vs Conj); these add
to type sparsity, but are trivial to handle.
analyses for the MSA tokens.
6In our work, we distinguish between morphological anal-
ysis, which refers to producing the various readings of a word
out of context, and morphological tagging (or disambigua-
tion), which identifies the appropriate analysis in context.
7The rate of TYPO words in the ARZ data is almost 18
times the rate in the MSA PATB data sets.
8The inflected verb Aitnaf?i? is the passive voice of the
verb with the lemma naf?a? or the active voice of the verb
with the lemma Aitnaf?i?.
3
Among LEMs and DIACs, there is consider-
able variation in the Arabic spelling, particularly
involving the spelling of Alif/Hamza forms, the
Egyptian long vowels /e:/ and /o:/ and often re-
quiring adjustment to conform to CODA guide-
lines.9 The following are some examples. Specific
CODA cases include spelling ?Y? kidah ?as such?
as @Y? kdA or spelling ?


?

? qawiy [pronounced
/awi/] ?very? as ?


?@ Awy. The preposition ?J

	
? fiyh
?in it? is incorrectly spelled as fiyuh (allomorphic
form is incorrect). The word I
K. bayt ?house? is
spelled biyt (long vowel spelling error). And fi-
nally the interjection

B l? ?no!? is spelled as (the
implausible form) Z? la?.
Among LEMs, over 63% of the errors is due
to inconsistency in assigning lemmas of punctu-
ation and digit, a trivial challenge. 29% of the
cases are spelling errors such as those discussed
above. The remaining 10% are due to not follow-
ing the specific format guidelines of lemmas (e.g.,
must be singular, uncliticized, and with a sense id
number). Among DIACs, almost all of the mis-
matches are non-CODA-compliant spelling varia-
tions. One third is Alif/Hamza forms, and another
quarter is long vowel spelling. One eighth involves
diacritic choice.
Combinations of these error types occur,
of course. One extreme case is the pro-
gressive particle prefix bi, which should be
tagged as bi/PROG_PART, but appears addi-
tionally as b/PROG_PART, ba/PROG_PART,
bi/PART_PROG, bi/PRO_PART, and
bi/FUT_PART.
Example For the rest of this section, we con-
sider the example word @??g.

AJ
k Hy?jlwA ?and
they will postpone?. Figure 1 contrasts an erro-
neous analysis in the pre-release data with a cor-
rected version of it. There are multiple problems
in this example. First, the POS tag is both in-
ternally inconsistent and is inconsistent with the
MORPH choice. The POS has a singular subject
prefix (IV3MS) and a plural subject suffix (IV-
SUFF_SUBJ:P); and the plural subject suffix is
written using the morpheme (+uh), which corre-
sponds to a direct object enclitic. The two mor-
phemes, +uh and +uwA, are homophonous, which
is the most likely cause for this error. Second, the
future marker (Ha+) is written in a non-CODA-
9LDC annotators were not asked to comply with CODA
guidelines during the annotation task. Therefore, multiple
spelling variants for OOV Egyptian Arabic words were to be
expected.
compliant way (ha+) in the analysis. And finally,
the lemma is malformed, containing multiple ex-
tra sense id digits. It is important to point out
that there are multiple ways to correct the anal-
ysis. For example, it can be Ha+yi+?aj?il+uh
FUT_PART+IV3MS+IV+IVSUFF_DO:3MS ?he
will postpone it?.10
4.3 Approach
Our target is to provide correct morphological
analyses for the OOV annotations in the pre-
release version of the ARZ corpus. Since not
all of the OOV annotations are wrong in prin-
ciple, we do not force map them all to CAL-
IMA+SAMA in-vocabulary variants, especially
for open class categories, where we know CAL-
IMA+SAMA may be deficient. As such, our gen-
eral solution focuses on correcting closed classes
(some stems and all of the affixes) by mapping
them to in-vocabulary variants. We also use a set
of language-specific preprocessing corrections for
common orthographic variations (for all open and
closed classes). An important tool we use through-
out to rank choices and break ties is modified Lev-
enshtein edit distance.11
Next, we present the four steps of our correction
process: annotation preprocessing, morpheme-
POS correction, lemma correction and surface
DIAC generation.
Annotation Preprocessing When first reading
the pre-release annotations, we perform a prepro-
cessing step that includes a set of deterministic
corrections for common non-CODA-compliant or-
thographic variations and errors, and POS tagging
typos. The corrections apply to the POS tags, lem-
mas, morphemes and surface forms. Examples of
these corrections include the following: reorder-
ing diacritics, e.g., saji?l? saj?il; removing du-
plicate diacritics, e.g., saj?iil? saj?il; adjusting
Alif-Hamza forms to match the diacritics that fol-
10Since our approach currently considers words out of con-
text, such a correction is not preferred because it requires
more character edits (see Figure 2). We acknowledge this
to be a limitation and plan to address it in the future.
11The Levenshtein edit distance is defined as the minimum
number of single-character edits (insertion, deletion and sub-
stitution) required to change one string into the other. For
Arabic words and morphemes, we modify the cost of sub-
stitutions involving two phonologically or orthographically
similar letters to count as half edits. We acquire the list of
such letter substitutions from Eskander et al (2013), who re-
port them as the most frequent source of errors in Egyptian
Arabic orthography. We map all diacritic-only morphemes
to empty morphemes in both ways at a cost of half edit also.
For POS tag edit distance, we use the standard definition of
Levenshtein edit distance. Edit cost is an area where a lot of
tuning could be done and we plan to explore it in the future.
4
RAW ??g. AJ
? hyAjlw
Analysis Incorrect Annotation Correct Annotation
DIAC hayi?aj?iluh Hayi?aj?iluwA
MORPH ha+yi+?aj?il+uh Ha+yi+?aj?il+uwA
POS FUT_PART+IV3MS+IV+IVSUFF_SUBJ:P FUT_PART+IV3P+IV+IVSUFF_SUBJ:P
LEM ?aj?ill1 ?aj?il_1
Figure 1: An incorrect annotation example with a possible correction.
low them, e.g., A?aSl? ?aSl; and POS tag capital-
ization, e.g., Fut_Part? FUT_PART.
Morpheme-POS Correction For morpheme
correction purposes, we define an abstract rep-
resentation that combines all the closed-class
morphemes and POS tags. For open-class
stems, we simply use the POS tag. For exam-
ple, the abstract morpheme representation for
the correct version of the word in Figure 1 is
Ha/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P.
We will refer to this representation as the inflec-
tional morph-tag (IMT).
We build two models for this task. First, we
build an IMT language model from the CAL-
IMA+SAMA databases. This models all possible
inflections in the analyzer without the open class
stems. This model includes 304K sequences. Sec-
ond, we construct a map from all the seen IMTs
in the ARZ corpus to all the in-vocabulary IMTs
in the IMT language model. The mapping in-
cludes a cost that is based on the edit distance dis-
cussed earlier. Figure 2 shows the top mappings
for the IMTs in our example. Both models are im-
plemented as finite state machines using the ATT
FSM toolkit (Mohri et al, 1998).
The input, possibly incorrect, IMT is con-
verted into an FSM that is then composed
with the mapping transducer and the language
model automaton to generate a cost-ranked list
of mappings. The output for our example is
listed in Figure 3. We then replace the input
POS and MORPH with the top ranked correction:
Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_SUBJ:P
at a cost of 4.0. The open class stem is not modi-
fied.
Lemma Correction We generate a map that in-
cludes all the possible lemmas for every possi-
ble stem morpheme in CALIMA+SAMA. For a
given ARZ word analysis, if the stem morpheme
is in CALIMA+SAMA, then we pick the lemma
from its corresponding lemma set. When there is
more than one possible lemma, we pick the lemma
that is closest to the provided pre-release ARZ
Base IMT
Morpheme
Mapped IMT
Morphemes Cost
ha/FUT_PART Ha/FUT_PART 0.5
sa/FUT_PART 1.0
yi/IV3MS
yi/IV3MS 0.0
ya/IV3MS 1.0
y/IV3MS 1.0
yu/IV3MS 1.0
yi/IV3P 2.0
IV
IV 0.0
PV 1.0
CV 1.0
uh/IVSUFF_SUBJ:P uwA/IVSUFF_SUBJ:P 1.5
na/IVSUFF_SUBJ:FP 3.0
Figure 2: Top mappings for the IMT morphemes
ha/FUT_PART, yi/IV3P, IV and uh/IVSUFF_SUBJ:P
Input: ha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:P
FSM Output Cost
Ha/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P 4.0
Ha/FUT_PART+y/IV3P+IV+uwA/IVSUFF_SUBJ:P 5.0
Ha/FUT_PART+ti/IV2P+IV+uwA/IVSUFF_SUBJ:P 6.0
Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_DO:3MS 6.5
Ha/FUT_PART+yi/IV3MS+IV+kuw/IVSUFF_DO:2P 7.0
Ha/FUT_PART+yi/IV3MS+IV+nA/IVSUFF_DO:1P 7.0
Ha/FUT_PART+tu/IV2P+IV+uwA/IVSUFF_SUBJ:P 7.0
sa/FUT_PART+ya/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0
sa/FUT_PART+yu/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0
Ha/FUT_PART+yi/IV3MS+IV+kum/IVSUFF_DO:2P 7.5
Figure 3: Top corrections for the input
ha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:P
lemma, based on their string edit distance as de-
fined earlier. If the stem morpheme is not in CAL-
IMA+SAMA (e.g., open class), then we keep the
ARZ lemma as it is.
In our example, the stem morpheme ?aj?il/IV
is paired in CALIMA+SAMA with the lemma
?aj?il_1. Accordingly, ?aj?il_1 replaces the in-
put pre-release ARZ lemma.
Surface DIAC Generation After correcting the
morphemes and POS tags in the input word,
we use them to generate a new surface DIAC
form. For all the closed-class morphemes and
in-vocabulary open-class stems, we use CAL-
IMA+SAMA to identify all the MORPH+POS to
DIAC mappings. For open-class stems that are
5
OOVs, we use their corresponding DIAC form in
the input word.12 This may lead to many possible
sequences. We rank them by their edit distance
(defined above) to the surface DIAC of the input
word.
In our example, this process is rather trivial:
every morpheme is paired with only one surface
DIAC in the morphological analyzer. The surface
DIACs corresponding to Ha/FUT_PART, yi/IV3P,
?aj?il/IV and uwA/IVSUFF_SUBJ:P are Ha, yi,
?aj?il and uwA, respectively. The final combined
surface is Hayi?aj?iluwA.
A more interesting example is the word A 	JJ
??
?alay+nA ?upon us? which has the analysis
?ala?/PREP+nA/PRON_1P. The MORPH stem
?ala? has two DIAC forms: ?ala? and ?alay. The
second form is only used when an enclitic is
present. It is selected in this example because it
has a smaller edit distance to the full word input
DIAC form than the surface stem ?ala?. In the
future, we plan to use more sophisticated genera-
tion and detokenization techniques (El Kholy and
Habash, 2010).
4.4 Results and Error Analysis
Results We conducted a manual evaluation for
1,000 words from the internal, pre-release ARZ
after applying the automatic correction process.
This set is a blind test set, i.e., not used as part
of the development. The results are listed in Ta-
ble 1 for the lemmas, POS tags, diacritized mor-
phemes and diacritized surface forms, in addition
to the complete morphological analyses (token-
based), where the correction output is compared
to the pre-release ARZ annotations (the baseline).
The results are listed for different subsets of
the data. The first row lists the results consider-
ing the complete 1,000 words, where all the in-
vocabulary words are considered correct. This is
only intended to give an overall estimate of the
correctness of the set. The second row lists the re-
sults for CALIMA+SAMA OOV words only. The
third row is the same as the second, but exclud-
ing punctuations, digits and typos. Focusing on
the last row, we see that we achieve between 58%
and 24% error reduction on different features, and
reach almost 40% error reduction on all features
combined.
Error Analysis For POS, 99.7% of all the cor-
rect cases in the Baseline were not changed. Only
12Since the surface DIAC splits are not provided, we deter-
mine the exact boundary of the surface DIAC stem by mini-
mizing the edit distance between the prefixing/suffixing mor-
phemes and the full input surface DIAC form.
one case was changed and it was caused by an er-
ror in the input MORPH splits. Of the erroneous
cases in the Baseline, 40% were not changed.
Among the attempted changes, 71% successfully
fixed the baseline problem. Almost all of the failed
changes are due to implausible null pronouns in
the Baseline that were not handled in the cur-
rent implementation, which only considered cor-
rect null pronouns. We plan to address these in
the future. Among the errors that were not ad-
dressed, the most common case involves nominal
form (41%) followed by hard features to resolve
and open class passive-voice inconsistency (each
27%).
Regarding lemmas, 93.9% of all correct base-
line lemmas remained correct. In the rest, over-
correction attempts resulting from matching the
OOV lemma to the wrong in-vocabulary lemma
backfired. Around 8.7% of the erroneous baseline
lemmas were not modified and 1.6% were mod-
ified incorrectly. The rest, 92.8%, were success-
fully fixed. Almost all of the system errors result-
ing from changes involve over correction by map-
ping to incorrect INV lemma forms.
Finally, as for diacritized forms, 96.9% of the
correct baseline DIACs remained correct; the rest
fell victim to over-correction. Among incorrect
baseline cases, 43% remained unchanged; and
45% were fixed; 4% were over-corrected and 8%
only partially corrected. Remaining DIAC errors
are mostly in open classes where the analyzer re-
call problems cannot help.
5 Automatic Morphological Extension
In this section, we present the general technique
we use to extend shallow annotations. We discuss
the data sets, the approach and evaluation results
next.
5.1 Data
We conduct our experiments on two differ-
ent Egyptian Arabic corpora: the CALLHOME
Egypt (CHE) corpus (Gadalla et al, 1997) and
Carnegie Mellon University Egyptian Arabic cor-
pus (CMUEAC) (Mohamed et al, 2012).
CHE The CHE corpus contains 140 telephone
conversation transcripts of about 179K words.
Each word is represented by its phonological form
and undiacritized Arabic script orthography. The
orthography used is quite similar to the CODA
standard we use. Being a transcript corpus, it is
quite clean and free of spelling variations. We use
a technique described in more detail in Habash et
6
POS
LEM POS MORPH DIAC +MORPH All
All words
Baseline 79.8% 93.2% 92.2% 91.1% 87.3% 72.7%
System 95.7% 95.5% 93.8% 93.6% 91.5% 90.0%
Analyzer OOV
Baseline 47.1% 82.4% 79.7% 76.8% 66.8% 28.4%
System 88.9% 88.42% 83.9% 83.4% 77.9% 73.9%
Analyzer OOV, no Baseline 71.3% 82.5% 74.1% 69.7% 59.0% 43.0%
Punc/Digit/Typos System 88.0% 87.3% 80.5% 79.7% 71.3% 65.3%
Table 1: Accuracy of the automatic morphological correction of internal, pre-release ARZ data.
al. (2012b) to combine the phonological form and
undiacritized Arabic script into diacritized Arabic
script, i.e. DIAC. For example, the undiacritized
word ? 	JJ
? ?ynh ?his eye? is combined with its pro-
nunciation /?e:nu/ producing the diacritized form
?aynuh.
CMUEAC The CMUEAC corpus includes
about 23K words that are only annotated for
morph splits. The corpus text includes sponta-
neously written Egyptian Arabic text collected off
the web. To use the same example as above, the
word ? 	JJ
? ?ynh ?his eye? is segmented as ?yn+h in-
dicating that there is a base word plus an enclitic.
5.2 Approach
Our approach to morphological extension is to au-
tomatically annotate the corpus using a very rich
morphological tagger, and then use the limited
manual annotations to adjust the morphological
choice. We use a morphological tagger, MADA-
ARZ (Morphological Analysis and Disambigua-
tion for Egyptian Arabic) (Habash et al, 2013).
MADA-ARZ produces, for each input word, a
contextually ranked list of analyses specifying all
the morphological interpretations of that word as
provided by the CALIMA+SAMA morphological
analyzer.
CHE In the case of CHE, we select the first
choice from the ranked list of analyses whose
DIAC matches the diacritized word in CHE. For
example, for the word ? 	JJ
? ?ynh MADA-ARZ
generates 45 different morphological analyses
with different lemmas, POS, orthographies and
diacritics: ?ayn+uh ?his eye?, ?ay?in+a~ ?sam-
ple? and ?ay?in+uh ?he appointed him?. The
diacritized word ?ayn+uh allows us to select the
following full analysis:
Metric CHE CMUEAC
LEM 97.2 82.0
POS 95.2 79.6
MORPH 96.8 77.6
DIAC 97.2 78.4
POS+MORPH 92.8 74.0
All 92.8 72.0
Table 2: Accuracy of automatic morphological ex-
tension of CHE and CMUEAC.
RAW Eynh
DIAC Eaynuh
MORPH Eayn+uh
POS NOUN+POSS_PRON_3MS
LEM Eayn_1
Although this example may not require the full
power of a tagger, but just the out-of-context an-
alyzer, other cases involving POS ambiguity un-
realized through diacritization necessitate the use
of a tagger, e.g., the word I.

KA? kAtib can be
an ADJ meaning ?writing? or a NOUN meaning
?writer/author?.
CMUEAC In the case of CMUEAC, we se-
lect the first choice from the ranked list of anal-
yses whose undiacritized MORPH splits match the
word tokenization. In the case of the word ? 	JJ
?
?yn+h, the tokenization cannot distinguish be-
tween the noun reading ?ayn+uh ?his eye? and
the verbal reading ?ay?in+uh ?he appointed him?.
MADA-ARZ effectively selects in such cases.
We expect the performance on CMUEAC to
be worse than CHE given the difference in the
amount of information between the two corpora.
5.3 Results and Error Analysis
We evaluate the accuracy of the morphological
extension process on both CHE and CMUEAC
using two 300 word samples that were manu-
ally enriched. Table 2 presents the accuracies of
the assigned LEMs, POS tags, DIAC forms and
7
MORPHs, in addition to the complete morpholog-
ical analysis. All results are token-based.
CHE CHE analyses have high accuracies rang-
ing between 95.2% and 97.2% for the different
analysis features, with the complete analysis hav-
ing an accuracy of 92.8%. One third of the er-
rors is due to gold diacritization errors in the
CHE corpus. 28% of the errors are due to wrong
verbal features (person, number and gender) for
forms that are not distinguishable in DIAC, e.g.,
I.

J? katabt ?I/you wrote? and I.

J?

K tiktib ?you
write/she writes?. The rest of the errors are be-
cause of failure in assigning the correct POS tags
for nouns, particles and verbs with percentages of
22%, 11% and 6%, respectively.
CMUEAC CMUEAC analyses have much
lower accuracies compared to CHE, ranging be-
tween 77.6% and 82.0% for different features,
with the complete analysis accuracy at 72.0%. The
CMUEAC is much harder to extend for two rea-
sons: the text, being naturally occurring, con-
tains a lot of orthographic noise; and tokeniza-
tion information is not sufficient to disambiguate
many analyses. For CMUEAC, a quarter of the
errors is due to gold tokenization errors in the
original CMUEAC corpus. Another quarter of
the errors results from MADA-ARZ assigning an
MSA analysis instead of an Egyptian Arabic anal-
ysis.13 Failure to assign the correct POS tags for
particles, verbs and nouns represents 14%, 10%
and 7% of the errors, respectively. Other errors
are because of wrong verbal features (13%) and
wrong diacritization (6%).
As expected, relatively richer annotations (i.e.,
diacritics) are easier to extend to full morpholog-
ical information that relatively poorer annotations
(i.e., tokenization). Of course, the tradeoff is still
there as tokenizations are much easier and cheaper
to annotate. We plan to explore the question of
what would be an optimal set of poor annotations
that can help us extend to the full morphology at
high accuracy in the future.
6 Egyptian Corpus
After applying morphological corrections to pre-
release ARZ and morphological extensions to
CHE and CMUEAC, we have now three big cor-
pora that are automatically adjusted to include
the same rich morphological information, that is:
13MADA-ARZ is trained on a combination of MSA and
Egyptian Arabic text and as such may select an MSA analysis
in cases that are ambiguous.
lemma, POS tag, diacritized morphemes, and dia-
critized surface. We combine the three resources
together in one morphologically rich corpus that
contains about 46K sentences and 447K words,
representing 61K unique lemmas. We intend to
make these automatic corrections and extensions
available in the future to provide extensive sup-
port for Egyptian Arabic processing for different
purposes.
7 Conclusion and Future Work
We presented two methods for automatic correc-
tion and extension of morphological annotations
and demonstrated their success on three different
Egyptian Arabic corpora, which now have annota-
tions that are automatically adjusted to include the
same rich morphological information although at
different degrees of quality that correspond to the
amount of initial information.
We presented two methods for automatic cor-
rection and extension of morphological annota-
tions and demonstrated their success on three dif-
ferent Egyptian Arabic corpora, which now have
annotations that are automatically adjusted to in-
clude the same rich morphological information al-
though at different degrees of quality that corre-
spond to the amount of initial information.
In the future, we plan to study how to optimize
the amount of basic information to annotate man-
ually in order to maximize the benefit of auto-
matic extensions. We also plan to provide feed-
back to the annotation process to reduce the per-
centage of errors generated by the annotators, per-
haps through a tighter integration of the correc-
tion/extension techniques with the annotation pro-
cess. We also plan on using the cleaned up corpus
to extend the existing analyzer for Egyptian Ara-
bic.
Acknowledgment
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under contracts No. HR0011-12-C-
0014 and HR0011-11-C-0145. Any opinions,
findings and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of DARPA. We
also would like to thank Emad Mohamed and Ke-
mal Oflazer for providing us with the CMUEAC
corpus. We thank Ryan Roth for help with
MADA-ARZ. Finally, we thank Owen Rambow,
Mona Diab and Warren Churchill for helpful dis-
cussions.
8
References
Sarah Alkuhlani and Nizar Habash. 2011. A Corpus
for Modeling Morpho-Syntactic Agreement in Ara-
bic: Gender, Number and Rationality. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL?11), Portland,
Oregon, USA.
Sarah Alkuhlani, Nizar Habash, and Ryan Roth. 2013.
Automatic morphological enrichment of a morpho-
logically underspecified treebank. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 460?470, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic Morphological Detokenization and Or-
thographic Denormalization. In Proceedings of the
seventh International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing spontaneous orthog-
raphy. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 585?595, Atlanta, Georgia, June.
Association for Computational Linguistics.
Hassan Gadalla, Hanaa Kilany, Howaida Arram,
Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby,
Krisjanis Karins, Everett Rowson, Robert MacIn-
tyre, Paul Kingsbury, David Graff, and Cynthia
McLemore. 1997. CALLHOME Egyptian Ara-
bic Transcripts. In Linguistic Data Consortium,
Philadelphia.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the
Arabic Dialects. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 681?688, Sydney, Aus-
tralia.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In NAACL-HLT 2012 Workshop on Com-
putational Morphology and Phonology (SIGMOR-
PHON2012), pages 1?9, Montr?al, Canada.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Otakar Smr?, Tim Buckwalter, and Hubert
Jin. 2005. Feature-based tagger of approximations
of functional Arabic morphology. In Proceedings of
the Workshop on Treebanks and Linguistic Theories
(TLT), Barcelona, Spain.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction us-
ing Finite State Automata. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2008).
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In NAACL ?06: Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short
Papers on XX, pages 57?60, Morristown, NJ, USA.
Okan Kolak and Philip Resnik. 2002. OCR error cor-
rection using a noisy channel model. In Proceed-
ings of the second international conference on Hu-
man Language Technology Research.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Sur-
veys, 24(4).
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2009. Creating a methodology for large-scale cor-
rection of treebank annotation: The case of the ara-
bic treebank. In MEDAR Second International Con-
ference on Arabic Language Resources and Tools,
Egypt. Citeseer.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012a.
Egyptian Arabic Treebank DF Part 1 V2.0. LDC
catalog number LDC2012E93.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012b.
Egyptian Arabic Treebank DF Part 2 V2.0. LDC
catalog number LDC2012E98.
9
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012c.
Egyptian Arabic Treebank DF Part 3 V2.0. LDC
catalog number LDC2012E89.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012d.
Egyptian Arabic Treebank DF Part 4 V2.0. LDC
catalog number LDC2012E99.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012e.
Egyptian Arabic Treebank DF Part 5 V2.0. LDC
catalog number LDC2012E107.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012f.
Egyptian Arabic Treebank DF Part 6 V2.0. LDC
catalog number LDC2012E125.
Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,
Nadia Hamrouni, and Nizar Habash. 2012g. Egyp-
tian Arabic Morphological Annotation Guidelines.
Walid Magdy and Kareem Darwish. 2006. Ara-
bic OCR Error Correction Using Character Segment
Correction, Language Modeling, and Shallow Mor-
phology. In Proceedings of 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 408?414, Sydney, Austrailia.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Annotating and Learning Morphological Seg-
mentation of Egyptian Colloquial Arabic. In Pro-
ceedings of the Language Resources and Evaluation
Conference (LREC), Istanbul.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 1998. A rational design for a weighted finite-
state transducer library. In D. Wood and S. Yu, ed-
itors, Automata Implementation, Lecture Notes in
Computer Science 1436, pages 144?58. Springer.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?90.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohamed Maamouri, Aous Mansouri, and
Wajdi Zaghouani. 2008. A Pilot Arabic Prop-
bank. In Proceedings of LREC, Marrakech, Mo-
rocco, May.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Conference on Language Engineering,
ELSE, Cairo, Egypt.
Noah Smith, David Smith, and Roy Tromble. 2005.
Context-Based Morphological Disambiguation with
Random Fields. In Proceedings of the 2005 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP05), pages 475?482, Vancou-
ver, Canada.
10
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 93?103,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Transliteration of Arabizi into Arabic Orthography: Developing a 
Parallel Annotated Arabizi-Arabic Script SMS/Chat Corpus 
 
Ann Bies, Zhiyi Song, Mohamed Maamouri, Stephen Grimes, Haejoong Lee,  
Jonathan Wright, Stephanie Strassel, Nizar Habash?, Ramy Eskander?, Owen Rambow? 
Linguistic Data Consortium, University of Pennsylvania 
{bies,zhiyi,maamouri,sgrimes,haejoong, 
jdwright,strassel}@ldc.upenn.edu 
?Computer Science Department, New York University Abu Dhabi 
?
nizar.habash@nyu.edu 
?Center for Computational Learning Systems, Columbia University 
?
{reskander,rambow}@ccls.columbia.edu 
 
  
 
Abstract 
This paper describes the process of creating a 
novel resource, a parallel Arabizi-Arabic 
script corpus of SMS/Chat data.  The lan-
guage used in social media expresses many 
differences from other written genres: its vo-
cabulary is informal with intentional devia-
tions from standard orthography such as re-
peated letters for emphasis; typos and non-
standard abbreviations are common; and non-
linguistic content is written out, such as 
laughter, sound representations, and emoti-
cons.  This situation is exacerbated in the 
case of Arabic social media for two reasons.  
First, Arabic dialects, commonly used in so-
cial media, are quite different from Modern 
Standard Arabic phonologically, morphologi-
cally and lexically, and most importantly, 
they lack standard orthographies. Second, 
Arabic speakers in social media as well as 
discussion forums, SMS messaging and 
online chat often use a non-standard romani-
zation called Arabizi.  In the context of natu-
ral language processing of social media Ara-
bic, transliterating from Arabizi of various 
dialects to Arabic script is a necessary step, 
since many of the existing state-of-the-art re-
sources for Arabic dialect processing expect 
Arabic script input.  The corpus described in 
this paper is expected to support Arabic NLP 
by providing this resource. 
1 Introduction 
The language used in social media expresses 
many differences from other written genres: its 
vocabulary is informal with intentional devia-
tions from standard orthography such as repeated 
letters for emphasis; typos and non-standard ab-
breviations are common; and non-linguistic con-
tent is written out, such as laughter, sound repre-
sentations, and emoticons. 
This situation is exacerbated in the case of Ar-
abic social media for two reasons.  First, Arabic 
dialects, commonly used in social media, are 
quite different from Modern Standard Arabic 
(MSA) phonologically, morphologically and lex-
ically, and most importantly, they lack standard 
orthographies (Maamouri et.al. 2014). Second, 
Arabic speakers in social media as well as dis-
cussion forums, Short Messaging System (SMS) 
text messaging and online chat often use a non-
standard romanization called ?Arabizi? (Dar-
wish, 2013).  Social media communication in 
Arabic takes place using a variety of orthogra-
phies and writing systems, including Arabic 
script, Arabizi, and a mixture of the two.  Alt-
hough not all social media communication uses 
Arabizi, the use of Arabizi is prevalent enough to 
pose a challenge for Arabic NLP research. 
In the context of natural language processing 
of social media Arabic, transliterating from 
Arabizi of various dialects to Arabic script is a 
necessary step, since many of the existing state-
of-the-art resources for Arabic dialect processing 
and annotation expect Arabic script input (e.g., 
Salloum and Habash, 2011; Habash et al. 2012c; 
Pasha et al., 2014). 
To our knowledge, there are no naturally oc-
curring parallel texts of Arabizi and Arabic 
script.  In this paper, we describe the process of 
creating such a novel resource at the Linguistic 
Data Consortium (LDC).  We believe this corpus 
will be essential for developing robust tools for 
converting Arabizi into Arabic script. 
93
The rest of this paper describes the collection 
of Egyptian SMS and Chat data and the creation 
of a parallel text corpus of Arabizi and Arabic 
script for the DARPA BOLT program.1  After 
reviewing the history and features in Arabizi 
(Section 2) and related work on Arabizi (Section 
3), in Section 4, we describe our approach to col-
lecting the Egyptian SMS and Chat data and the 
annotation and transliteration methodology of the 
Arabizi SMS and Chat into Arabic script, while 
in Section 5, we discuss the annotation results, 
along with issues and challenges we encountered 
in annotation. 
2 Arabizi and Egyptian Arabic Dialect 
2.1 What is Arabizi? 
Arabizi is a non-standard romanization of Arabic 
script that is widely adopted for communication 
over the Internet (World Wide Web, email) or 
for sending messages (instant messaging and 
mobile phone text messaging) when the actual 
Arabic script alphabet is either unavailable for 
technical reasons or otherwise more difficult to 
use.  The use of Arabizi is attributed to different 
reasons, from lack of good input methods on 
some mobile devices to writers? unfamiliarity 
with Arabic keyboard.  In some cases, writing in 
Arabizi makes it easier to code switch to English 
or French, which is something educated Arabic 
speakers often do.  Arabizi is used by speakers of 
a variety of Arabic dialects. 
Because of the informal nature of this system, 
there is no single ?correct? encoding, so some 
character usage overlaps.  Most of the encoding 
in the system makes use of the Latin character 
(as used in English and French) that best approx-
imates phonetically the Arabic letter that one 
wants to express (for example, either b or p cor-
responds to ?).  This may sometimes vary due to 
regional variations in the pronunciation of the 
Arabic letter (e.g., j is used to represent ? in the 
Levantine dialect, while in Egyptian dialect g is 
used) or due to differences in the most common 
non-Arabic second language (e.g., sh corre-
sponds to ? in the previously English dominated 
Middle East Arab countries, while ch shows a 
predominantly French influence as found in 
North Africa and Lebanon).  Those letters that do 
not have a close phonetic approximate in the Lat-
in script are often expressed using numerals or 
other characters, so that the numeral graphically 
                                                 
1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op 
erational_Language_Translation_%28BOLT%29.aspx 
approximates the Arabic letter that one wants to 
express (e.g., the numeral 3 represents ? because 
it looks like a mirror reflection of the letter). 
Due to the use of Latin characters and also 
frequent code switching in social media Arabizi, 
it can be difficult to distinguish between Arabic 
words written in Arabizi and entirely unrelated 
foreign language words (Darwish 2013).  For 
example, mesh can be the English word, or 
Arabizi for ?? ?not?.  However, in context these 
cases can be clearly labeled as either Arabic or a 
foreign word.  An additional complication is that 
many words of foreign origin have become Ara-
bic words (?borrowings?).  Examples include 
banadoora ?????? ?tomato? and mobile ?????? 
?mobile phone?.  It is a well-known practical and 
theoretical problem to distinguish borrowings 
(foreign words that have become part of a lan-
guage and are incorporated fully into the mor-
phological and syntactic system of the host lan-
guage) from actual code switching (a bilingual 
writer switches entirely to a different language, 
even if for only a single word).  Code switching 
is easy to identify if we find an extended passage 
in the foreign language which respects that lan-
guage?s syntax and morphology, such as Bas eh 
ra2yak I have the mask.  The problem arises 
when single foreign words appear without Arabic 
morphological marking: it is unclear if the writer 
switched to the foreign language for one word or 
whether he or she simply is using an Arabic 
word of foreign origin.  In the case of banadoora 
?????? ?tomato?, there is little doubt that this has 
become a fully Arabic word and the writer is not 
code switching into Italian; this is also signaled 
by the fact that a likely Arabizi spelling (such as 
banadoora) is not in fact the Italian orthography 
(pomodoro).  However, the case is less clear cut 
with mobile ?????? ?mobile phone?: even if it is a 
borrowing (clearly much more recent than bana-
doora ?????? ?tomato?), a writer will likely spell 
the word with the English orthography as mobile 
rather than write, say, mubail.  More research is 
needed on this issue.  However, because of the 
difficulty of establishing the difference between 
code switching and borrowing, we do not attempt 
to make this distinction in this annotation 
scheme. 
2.2 Egyptian Arabic Dialect 
Arabizi is used to write in multiple dialects of 
Arabic, and differences between the dialects 
themselves have an effect on the spellings cho-
sen by individual writers using Arabizi.  Because 
Egyptian Arabic is the dialect of the corpus cre-
94
ated for this project, we will briefly discuss some 
of the most relevant features of Egyptian Arabic 
with respect to Arabizi transliteration.  For a 
more extended discussion of the differences be-
tween MSA and Egyptian Arabic, see Habash et 
al. (2012a) and Maamouri et al. (2014). 
Phonologically, Egyptian Arabic is character-
ized by the following features, compared with 
MSA: 
(a) The loss of the interdentals /?/ and /?/ 
which are replaced by /d/ or /z/ and /t/ or /s/ 
respectively, thus giving those two original 
consonants a heavier load. Examples in-
clude  ??? /zakar/ ?to mention?, ???  /daba?/ 
?to slaughter?,  ???  /talg/ ?ice?,  ???  /taman/ 
?price?, and  ???  /sibit/ ?to stay in place, 
become immobile?. 
(b) The exclusion of /q/ and /?/ from the conso-
nantal system, being replaced by the /?/ and 
/g/, e.g., ???  /?u?n/ ?cotton?, and  ???  
/gamal/ ?camel?. 
At the level of morphology and syntax, the 
structures of Egyptian Arabic closely resemble 
the overall structures of MSA with relatively mi-
nor differences to speak of.  Finally, the Egyptian 
Arabic lexicon shows some significant elements 
of semantic differentiation. 
The most important morphological difference 
between Egyptian Arabic and MSA is in the use 
of some Egyptian clitics and affixes that do not 
exist in MSA.  For instance, Egyptian Arabic has 
the future proclitics h+ and ?+ as opposed to the 
standard equivalent s+. 
Lexically, there are lexical differences be-
tween Egyptian Arabic and MSA where no ety-
mological connection or no cognate spelling is 
available.  For example, the Egyptian Arabic ??  
/bu??/ ?look? is ???? /?unZur/ in MSA. 
3 Related Work 
Arabizi-Arabic Script Transliteration  Previ-
ous efforts on automatic transliterations from 
Arabizi to Arabic script include work by Chalabi 
and Gerges (2012), Darwish (2013) and Al-
Badrashiny et al. (2014).  All of these approaches 
rely on a model for character-to-character map-
ping that is used to generate a lattice of multiple 
alternative words which are then selected among 
using a language model.  The training data used 
by Darwish (2013) is publicly available but it is 
quite limited (2,200 word pairs).  The work we 
are describing here can help substantially im-
prove the quality of such system.  We use the 
system of Al-Badrashiny et al. (2014) in this pa-
per as part of the automatic transliteration step 
because they target the same conventional or-
thography of dialectal Arabic (CODA) (Habash 
et al., 2012a, 2012b), which we also target.  
There are several commercial products that con-
vert Arabizi to Arabic script, namely: Microsoft 
Maren, 2  Google Ta3reeb, 3  Basis Arabic chat 
translator4 and Yamli.5  Since these products are 
for commercial purposes, there is little infor-
mation available about their approaches, and 
whatever resources they use are not publicly 
available for research purposes.  Furthermore, as 
Al-Badrashiny et al. (2014) point out, Maren, 
Ta3reeb and Yamli are primarily intended as in-
put method support, not full text transliteration.  
As a result, their users? goal is to produce Arabic 
script text not Arabizi text, which affects the 
form of the romanization they utilize as an in-
termediate step.  The differences between such 
?functional romanization? and real Arabizi in-
clude that the users of these systems will use less 
or no code switching to English, and may em-
ploy character sequences that help them arrive at 
the target Arabic script form faster, which other-
wise they would not write if they were targeting 
Arabizi (Al-Badrashiny et al., 2014). 
Name Transliteration  There has been some 
work on machine transliteration by Knight and 
Graehl (1997).  Al-Onaizan and Knight (2002) 
introduced an approach for machine translitera-
tion of Arabic names. Freeman et al. (2006) also 
introduced a system for name matching between 
English and Arabic.  Although the general goal 
of transliterating from one script to another is 
shared between these efforts and ours, we are 
considering a more general form of the problem 
in that we do not restrict ourselves to names. 
Code Switching  There is some work on code 
switching between Modern Standard Arabic 
(MSA) and dialectal Arabic (DA).  Zaidan and 
Callison-Burch (2011) were interested in this 
problem at the inter-sentence level.  They 
crawled a large dataset of MSA-DA news com-
mentaries, and used Amazon Mechanical Turk to 
annotate the dataset at the sentence level.  
Elfardy et al. (2013) presented a system, AIDA, 
that tags each word in a sentence as either DA or 
MSA based on the context.  Lui et al. (2014) 
proposed a system for language identification in 
                                                 
2 http://www.getmaren.com 
3 http://www.google.com/ta3reeb 
4 http://www.basistech.com/arabic-chat-translator-
transforms-social-media-analysis/ 
5 http://www.yamli.com/ 
95
multilingual documents using a generative mix-
ture model that is based on supervised topic 
modeling algorithms.  Darwish (2013) and Voss 
et al. (2014) deal with exactly the problem of 
classifying tokens in Arabizi as Arabic or not.  
More specifically, Voss et al. (2014) deal with 
Moroccan Arabic, and with both French and 
English, meaning they do a three-way classifica-
tion.  Darwish (2013)'s data is more focused on 
Egyptian and Levantine Arabic and code switch-
ing with English. 
Processing Social Media Text  Finally, while 
English NLP for social media has attracted con-
siderable attention recently (Clark and Araki, 
2011; Gimpel et al., 2011; Gouws et al., 2011; 
Ritter et al., 2011; Derczynski et al., 2013), there 
has not been much work on Arabic yet.  Darwish 
et al. (2012) discuss NLP problems in retrieving 
Arabic microblogs (tweets).  They discuss many 
of the same issues we do, notably the problems 
arising from the use of dialectal Arabic such as 
the lack of a standard orthography.  Eskander et 
al. (2013) described a method for normalizing 
spontaneous orthography into CODA. 
4 Corpus Creation 
This work was prepared as part of the DARPA 
Broad Operational Language Translation 
(BOLT) program which aims at developing tech-
nology that enables English speakers to retrieve 
and understand information from informal for-
eign language sources including chat, text mes-
saging and spoken conversations. LDC collects 
and annotates informal linguistic data of English, 
Chinese and Arabic, with Egyptian Arabic being 
the representative of the Arabic language family.  
 
 
Egyptian Arabic has the advantage over all other 
dialects of Arabic of being the language of the 
largest linguistic community in the Arab region, 
and also of having a rich level of internet com-
munication.  
4.1 SMS and Chat Collection 
In BOLT Phase 2, LDC collected large volumes 
of naturally occurring informal text (SMS) and 
chat messages from individual users in English, 
Chinese and Egyptian Arabic (Song et al., 2014).  
Altogether we recruited 46 Egyptian Arabic par-
ticipants, and of those 26 contributed data.  To 
protect privacy, participation was completely 
anonymous, and demographic information was 
not collected.  Participants completed a brief lan-
guage test to verify that they were native Egyp-
tian Arabic speakers.  On average, each partici-
pant contributed 48K words.  The Egyptian Ara-
bic SMS and Chat collection consisted of 2,140 
conversations in a total of 475K words after 
manual auditing by native speakers of Egyptian 
Arabic to exclude inappropriate messages and 
messages that were not Egyptian Arabic.  96% of 
the collection came from the personal SMS or 
Chat archives of participants, while 4% was col-
lected through LDC?s platform, which paired 
participants and captured their live text messag-
ing (Song et al., 2014).  A subset of the collec-
tion was then partitioned into training and eval 
datasets.   
Table 1 shows the distribution of Arabic script 
vs. Arabizi in the training dataset.  The conversa-
tions that contain Arabizi were then further anno-
tated and transliterated to create the Arabizi-
Arabic script parallel corpus, which consists of 
 
 
 Total Arabic 
script only 
Arabizi 
only 
Mix of Arabizi and Arabic script 
Arabizi Arabic script 
Conversations 1,503 233 987 283 
Messages 101,292 18,757 74,820 3,237 4,478 
Sentence units 94,010 17,448 69,639 3,017 3,906 
Words 408,485 80,785 293,900 10,244 23,556 
Table 1. Arabic SMS and Chat Training Dataset 
 
1270 conversations. 6   All conversations in the 
training dataset were also translated into English 
to provide Arabic-English parallel training data. 
                                                 
6 In order to form single, coherent units (Sentence units) of 
an appropriate size for downstream annotation tasks using 
this data, messages that were split mid-sentence (often mid-
Not surprisingly, most Egyptian conversations 
in our collection contain at least some Arabizi; 
                                                                          
word) due to SMS messaging character limits were rejoined, 
and very long messages (especially common in chat) were 
split into two or more units, usually no longer than 3-4 sen-
tences. 
96
only 15% of conversations are entirely written in 
Arabic script, while 66% are entirely Arabizi.  
The remaining 19% contain a mixture of the two 
at the conversation level.  Most of the mixed 
conversations were mixed in the sense that one 
side of the conversation was in Arabizi and the 
other side was in Arabic script, or in the sense 
that at least one of the sides switched between 
the two forms in mid-conversation.  Only rarely 
are individual messages in mixed scripts.  The 
annotation for this project was performed on the 
Arabizi tokens only.  Arabic script tokens were 
not touched and were kept in their original 
forms.  
The use of Arabizi is predominant in the SMS 
and Chat Egyptian collection, in addition to the 
presence of other typical cross-linguistic text ef-
fects in social media data.  For example, the use 
of emoticons and emoji is frequent.  We also ob-
served the frequent use of written out representa-
tions of speech effects, including representations 
of laughter (e.g., hahaha), filled pauses (e.g., 
um), and other sounds (e.g., hmmm).  When these 
representations are written in Arabizi, many of 
them are indistinguishable from the same repre-
sentations in English SMS data.  Neologisms are 
also frequently part of SMS/Chat in Egyptian  
 
Arabic, as they are in other languages.  English 
words use Arabic morphology or determiners, as 
in el anniversary ?the anniversary?.  Sometimes 
English words are spelled in a way that is closer 
phonetically to the way an Egyptian speaker 
would pronounce them, for example lozar for 
?loser?, or beace for ?peace?. 
The adoption of Arabizi for SMS and online 
chat may also go some way to explaining the 
high frequency of code mixing in the Egyptian 
Arabic collection.  While the auditing process 
eliminated messages that were entirely in a non-
target language, many of the acceptable messag-
es contain a mixture of Egyptian Arabic and 
English. 
4.2 Annotation Methodology 
All of the Arabizi conversations, including the 
conversations containing mixtures of Arabizi and 
Arabic script were then annotated and translit-
erated: 
1. Annotation on the Arabizi source text to 
flag certain features 
2. Correction and normalization of the trans-
literation according to CODA conventions 
 
 
 
Figure 1. Arabizi Annotation and Transliteration Tool 
 
The annotators were presented with the source 
conversations in their original Arabizi form as 
well as the transliteration output from an auto-
matic Arabization system, and used a web-based 
tool developed by LDC (see Figure 1) to perform 
the two annotation tasks, which allowed annota-
tors perform both annotation and transliteration 
token by token, sentence by sentence and review 
the corrected transliteration in full context.  The 
GUI shows the full conversation in both the orig-
inal Arabizi and the resulting Arabic script trans-
literation for each sentence.  Annotators must 
97
annotate each sentence in order, and the annota-
tion is displayed in three columns.  The first col-
umn shows the annotation of flag features on the 
source tokens, the second column is the working 
panel where annotators correct the automatic 
transliteration and retokenize, and the third col-
umn displays the final corrected and retokenized 
result. 
Annotation was performed according to anno-
tation guidelines developed at the Linguistic Da-
ta Consortium specifically for this task (LDC, 
2014). 
4.3 Automatic Transliteration 
To speed up the annotation process, we utilized 
an automatic Arabizi-to-Arabic script translitera-
tion system (Al-Badrashiny et al., 2014) which 
was developed using a small vocabulary of 2,200 
words from Darwish (2013) and an additional 
6,300 Arabic-English proper name pairs (Buck-
walter, 2004).  The system has an accuracy of 
69.4%.  We estimate that using this still allowed 
us to cut down the amount of time needed to type 
in the Arabic script version of the Arabizi by 
two-thirds.  This system did not identify Foreign 
words or Names and transliterated all of the 
words.  In one quarter of the errors, the provided 
answer was plausible but not CODA-compliant 
(Al-Badrashiny et al., 2014). 
4.4 Annotation on Arabizi Source Text to 
Flag Features 
This annotation was performed only on sentences 
containing Arabizi words, with the goal of tag-
ging any words in the source Arabizi sentences 
that would be kept the same in the output of an 
English translation with the following flags: 
 
? Punctuation (not including emoticons) 
o Eh ?!//Punct  
o Ma32ula ?!//Punct 
o Ebsty ?//Punct  
 
? Sound effects, such as laughs (?haha? or 
variations), filled pauses, and other sounds 
(?mmmm? or ?shh? or ?um? etc.) 
o hahhhahhah//Sound akeed 3arfa :p da 
enty t3rafy ablia :pp 
o Hahahahaahha//Sound Tb ana ta7t fel 
ahwaa 
o Wala Ana haha//Sound 
o Mmmm//Sound okay 
 
? Foreign language words and numbers.  All 
cases of code switching and all cases of bor-
rowings which are rendered in Arabizi us-
ing standard English orthography are 
marked as ?Foreign?. 
o ana kont mt25er fe t2demm l pro-
jects//Foreign 
o oltilik okay//Foreign ya Babyy//Foreign 
balashhabal!!!! 
o zakrty ll sat//Foreign 
o Bat3at el whatsapp//Foreign 
o La la la merci//Foreign gedan bs la2 
o We 9//Foreign galaeeb dandash lel ban-
at 
 
? Names, mainly person names 
o Youmna//Name 7atigi?? 
 
4.5 Correction and Normalization of the 
Transliteration According to CODA 
Conventions 
The goal of this task was to correct all spelling in 
the Arabic script transliteration to CODA stand-
ards (Habash et al., 2012a, 2012b).  This meant 
that annotators were required to confirm both (1) 
that the word was transliterated into Arabic script 
correctly and also (2) that the transliterated word 
conformed to CODA standards.  The automatic 
transliteration was provided to the annotators, 
and manually corrected by annotators as needed. 
Correcting spelling to a single standard (CO-
DA), however, necessarily included some degree 
of normalization of the orthography, as the anno-
tators had to correct from a variety of dialect 
spellings to a single CODA-compliant spelling 
for each word.  Because the goal was to reach a 
consistent representation of each word, ortho-
graphic normalization was almost the inevitable 
effect of correcting the automatic transliteration.  
This consistent representation will allow down-
stream annotation tasks to take better advantage 
of the SMS/Chat data.  For example, more con-
sistent spelling of Egyptian Arabic words will 
lead to better coverage from the CALIMA mor-
phological analyzer and therefore improve the 
manual annotation task for morphological anno-
tation, as in Maamouri et al. (2014). 
 
Modern Standard Arabic (MSA) cognates and 
Egyptian Arabic sound changes 
Annotators were instructed to use MSA or-
thography if the word was a cognate of an MSA 
98
root, including for those consonants that have 
undergone sound changes in Egyptian Arabic.7 
? use mqfwl ?????  and not ma>fwl ?????  for 
?locked? 
? use HAfZ ???? and not HAfz ???? for the 
name (a proper noun)  
 
Long vowels 
Annotators were instructed to reinstate miss-
ing long vowels, even when they were written as 
short vowels in the Arabizi source, and to correct 
long vowels if they were included incorrectly. 
? use sAEap ???? and not saEap  ???  for 
?hour? 
? use qAlt   ????  and not qlt ??? for ?(she) 
said? 
 
Consonantal ambiguities 
Many consonants are ambiguous when written 
in Arabizi, and many of the same consonants are 
also difficult for the automatic transliteration 
script.  Annotators were instructed to correct any 
errors of this type.   
? S vs. s/ ?  vs. ? 
o use SAyg ????  and not  sAyg  ????  for 
?jeweler? 
? D vs. Z/ ?  vs. ? 
o use DAbT ????  and not  ZAbT ???? for 
?officer? 
o use Zlmp  ????  and not Dlmp  ????  for 
?darkness? 
? Dotted ya vs. Alif Maqsura/ ? vs. ?.  Alt-
hough the dotted ya/ ? and Alif Maqsura/ ? 
are often used interchangeably in Egyptian 
Arabic writing conventions, it was neces-
sary to make the distinction between the 
two for this task. 
o use Ely ???  and not ElY  ???  for ?Ali? 
(the proper name)  
? Taa marbouta.  In Arabizi and so also in the 
Arabic script transliteration, the taa mar-
bouta/ ? may be written for both nominal fi-
nal -h/ ? and verbal final -t/ ?, but for dif-
ferent reasons. 
o mdrsp Ely  ???  ?????  ?Ali?s school? 
o mdrsth  ??????  ?his school? 
 
Morphological ambiguities 
Spelling variation and informal usage can 
combine to create morphological ambiguities as 
well.  For example, the third person masculine 
                                                 
7 Both Arabic script and the Buckwalter transliteration 
(http://www.qamus.org/transliteration.htm) are shown for 
the transliterated examples in this paper. 
singular pronoun and the third person plural ver-
bal suffix can be ambiguous in informal texts.  
For example: 
? use byHbwA bED  ???  ?????? and not byHbh 
bED  ???  ?????  for ?(They) loved each oth-
er? 
? use byEmlwA  ???????  and not byEmlh  ??????  
for ?(They) did? or ?(They) worked? 
In addition, because final -h is sometimes re-
placed in speech by final /-uw/, it was occasion-
ally necessary to correct cases of overuse of the 
third person plural verbal suffix (-wA) to the 
pronoun -h as well. 
 
Merging and splitting tokens written with in-
correct word boundaries 
Annotators were instructed to correct any 
word that was incorrectly segmented.  The anno-
tation tool allowed both the merging and splitting 
of tokens. 
Clitics were corrected to be attached when 
necessary according to (MSA) standard writing 
conventions.  These include single letter proclit-
ics (both verbal and nominal) and the negation 
suffix -$, as well as pronominal clitics such as 
possessive pronouns and direct object pronouns.  
For example, 
? use fAlbyt  ??????    and not  
fAl  byt  ???  ??? or  flbyt  ?????   for ?in the 
house? 
? use EAlsTH ?????? and not  
EAl sTH ??? ??? or ElsTH ????? for ?on the 
roof? 
The conjunction w- / -? is always attached to 
its following word. 
? use wkAn  ????  and not w kAn  ??? ?    for 
?and was? 
? use wrAHt  ????? and not w  rAHt ????  ?  
for ?and (she) left? 
Words that were incorrectly segmented in the 
Arabizi source were also merged.  For example, 
? use msHwrp ?????? and not  
ms Hwrp ???? ??  for ?bewitched 
(fem.sing.)? 
? use $ErhA ????? and not $Er hA  ?? ???   for 
?her hair? 
Particles that are not attached in standard 
MSA written forms were corrected as necessary 
by the splitting function of the tool.  For exam-
ple,  
? use yA Emry  ???? ?? and not yAEmry  
??????  for ?Hey, dear!? 
? use lA trwH  ????  ? and not lAtrwH  ?????  
for ?Do not go? 
99
 
Abbreviations in Arabizi 
Three abbreviations in Arabizi received spe-
cial treatment: msa, isa, 7ma.  These three abbre-
viations only were expanded out to their full 
form using Arabic words in the corrected Arabic 
script transliteration. 
? msa: use mA $A' All~h  ? ???  ?? for ?As 
God wills? 
? isa: use <n $A' All~h  ? ???  ?? for ?God 
willing? 
? 7ma: use AlHmd ll~h for     ??? ??  ?Thank 
God, Praised be the Lord? 
All other Arabic abbreviations were not ex-
panded, and were transliterated simply letter for 
letter.  When the abbreviation was in English or 
another foreign language, it was kept as is in the 
transliteration, using both consonants and semi-
vowels to represent it. 
? use Awkyh  ????  for ?OK? (note that this is 
an abbreviation in English, but not in Egyp-
tian Arabic) 
 
Correcting Arabic typos 
Annotators were instructed to correct typos in 
the transliterated Arabic words, including typos 
in proper names.  However, typos and non-
standard spellings in the transliteration of a for-
eign words were kept as is and not corrected. 
? Ramafan  ?????  should be corrected to 
rmDAn  ?????  for ?Ramadan? 
? babyy  ????  since it is the English word ?ba-
by? it should not be corrected 
 
Flagged tokens in the correction task 
Tokens flagged during task 1 as Sound and 
Foreign were transliterated into Arabic script but 
were not corrected during task 2.  Note that even 
when a whole phrase or sentence appeared in 
English, the transliteration was not corrected. 
? ks  ??  for ?kiss? 
? Dd yA hAf fAn  ??? ???  ??  ??  for ?did you 
have fun? 
The transliteration of proper names was cor-
rected in the same way as all other words. 
Emoticons and emoji were replaced in the 
transliteration with #.  Emoticons refer to a set of 
numbers or letters or punctuation marks used to 
express feelings or mood.  Emoji refers to a spe-
cial set of images used in messages.  Both Emot-
icons and Emoji are frequent in SMS/Chat data. 
5 Discussion 
Annotation and transliteration were performed 
on all sentence units that contain Arabizi.  Sen-
tence units that contain only Arabic script were 
ignored and untouched during annotation.  In 
total, we reviewed 1270 conversations, among 
which over 42.6K sentence units (more than 
300K words) were deemed to be containing 
Arabizi and hence annotated and transliterated. 
The corpus files are in xml format.  All con-
versations have six layers: source, annotation on 
the source Arabizi tokens, automatic translitera-
tion via 3ARRIB, manual correction of the au-
tomatic transliteration, re-tokenized corrected 
transliteration, and human translation.  See Ap-
pendix A for examples of the file format. 
Each conversation was annotated by one anno-
tator, with 10 percent of the data being reviewed 
by a second annotator as a QC procedure.  Twen-
ty six conversations (roughly 3400 words) were 
also annotated dually by blind assignment to 
gauge inter-annotator agreement. 
As we noted earlier, code switching is fre-
quent in the SMS and Chat Arabizi data.  There 
were about 23K words flagged as foreign words.  
Written out speech effects in this type of data are 
also prevalent, and 6610 tokens were flagged as 
Sounds (laughter, filled pause, etc.).  Annotators 
most often agreed with each other in the detec-
tion and flagging of tokens as Foreign, Name, 
Sound or Punctuation, with over 98% agreement 
for all flags. 
The transliteration annotation was more diffi-
cult than the flagging annotation, because apply-
ing CODA requires linguistic knowledge of Ara-
bic.  Annotators went through several rounds of 
training and practice and only those who passed 
a test were allowed to work on the task.  In an 
analysis of inter-annotator agreement in the dual-
ly annotated files, the overall agreement between 
the two annotators was 86.4%.  We analyzed all 
the disagreements and classified them in four 
high level categories: 
? CODA  60% of the disagreements were related 
to CODA decisions that did not carefully follow 
the guidelines.  Two-fifths of these cases were 
related to Alif/Ya spelling (mostly Alif Hamza-
tion, rules of hamza support) and about one-fifth 
involved the spelling of common dialectal words.  
An additional one-third were due to non-CODA 
root, pattern or affix spelling.  Only one-tenth of 
the cases were because of split or merge deci-
sions.  These issues suggest that additional train-
ing may be needed.  Additionally, since some of 
100
the CODA errors may be easy to detect and cor-
rect using available tools for morphological 
analysis of Egyptian Arabic (such as the CALI-
MA-ARZ analyzer), we will consider integrating 
such support in the annotation interface in the 
future.  
? Task  In 23% of the overall disagreements, the 
annotators did not follow the task guidelines for 
handling punctuation, sounds, emoticons, names 
or foreign words.  Examples include disagree-
ment on whether a question mark should be split 
or kept attached, or whether a non-Arabic word 
should be corrected or not.  Many of these cases 
can also be caught as part of the interface; we 
will consider the necessary extensions in the fu-
ture. 
? Ambiguity  In 12% of the cases, the annota-
tors? disagreement reflected a different reading 
of the Arabizi resulting in a different lemma or 
inflectional feature.  These differences are una-
voidable and reflect the natural ambiguity in the 
task. 
? Typos  Finally, in less than 5% of the cases, 
the disagreement was a result of a typographical 
error unrelated to any of the above issues.  
Among the cases that were easy to adjudicate, 
one of the two annotators was correct 60% more 
than the other.  This is consistent with the obser-
vation that more training may be needed to fill in 
some of the knowledge gaps or increase the an-
notator?s attention to detail. 
6 Conclusion 
This is the first Arabizi-Arabic script parallel 
corpus that supports research on transliteration 
from Arabizi to Arabic script.  We expect to 
make this corpus available through the Linguistic 
Data Consortium in the near future. 
This work focuses on the novel challenges of 
developing a corpus like this, and points out the 
close interaction between the orthographic form 
of written informal genres of Arabic and the spe-
cific features of individual Arabic dialects.  The 
use of Arabizi and the use of Egyptian Arabic in 
this corpus come together to present a host of 
spelling ambiguities and multiplied forms that 
were resolved in this corpus by the use of CODA 
for Egyptian Arabic.  Developing a similar cor-
pus and transliteration for other Arabic dialects 
would be a rich area for future work. 
We believe this corpus will be essential for 
NLP work on Arabic dialects and informal gen-
res.  In fact, this corpus has recently been used in 
development by Eskander et al. (2014). 
Acknowledgements 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
(DARPA) under Contract No. HR0011-11-C-
0145. The content does not necessarily reflect the 
position or the policy of the Government, and no 
official endorsement should be inferred. 
Nizar Habash performed most of his contribu-
tion to this paper while he was at the Center for 
Computational Learning Systems at Columbia 
University. 
References 
Mohamed Al-Badrashiny, Ramy Eskander, Nizar Ha-
bash, and Owen Rambow. 2014. Automatic Trans-
literation of Romanized Dialectal Arabic. In Pro-
ceedings of the Conference on Computational Nat-
ural Language Learning (CONLL), Baltimore, 
Maryland, 2014. 
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number 
LDC2004L02, ISBN 1-58563-324-0.  
Achraf Chalabi and Hany Gerges. 2012. Romanized 
Arabic Transliteration. In Proceedings of the Sec- 
ond Workshop on Advances in Text Input Methods 
(WTIM 2012).  
Eleanor Clark and Kenji Araki. 2011. Text normaliza-
tion in social media: Progress, problems and ap- 
plications for a pre-processing system of casual 
English. Procedia - Social and Behavioral Scienc-
es, 27(0):2 ? 11. 
Kareem Darwish, Walid Magdy, and Ahmed Mourad. 
2012. Language processing for arabic microblog 
re- trieval. In Proceedings of the 21st ACM Inter-
national Conference on Information and 
Knowledge Management, CIKM ?12, pages 2427?
2430, New York, NY, USA. ACM.  
Kareem Darwish. 2013. Arabizi Detection and Con- 
version to Arabic. CoRR, arXiv:1306.6755 [cs.CL]. 
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina 
Bontcheva. 2013. Twitter part-of-speech tagging 
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference Recent 
Advances in Natural Language Processing RANLP 
2013, pages 198?206, Hissar, Bulgaria, September. 
INCOMA Ltd. Shoumen, Bulgaria.  
Heba Elfardy, Mohamed Al-Badrashiny, and Mona 
Diab. 2013. Code Switch Point Detection in Ara-
bic. In Proceedings of the 18th International Con-
ference on Application of Natural Language to In-
formation Systems (NLDB2013), MediaCity, UK, 
June.  
Ramy Eskander, Mohamed Al-Badrashiny, Nizar Ha-
bash and Owen Rambow. 2014. Foreign Words 
101
and the Automatic Processing of Arabic Social 
Media Text Written in Roman Script. In Arabic 
Natural Language Processing Workshop, EMNLP, 
Doha, Qatar. 
Ramy Eskander, Nizar Habash, Owen Rambow, and 
Nadi Tomeh. 2013. Processing Spontaneous Or- 
thography. In Proceedings of the 2013 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (NAACL-HLT), Atlanta, GA.  
Andrew T. Freeman, Sherri L. Condon and Christo-
pher M. Ackerman. 2006. Cross Linguistic Name 
Matching in English and Arabic: A ?One to Many 
Mapping? Extension of the Levenshtein Edit Dis-
tance Algorithm. In Proceedings of HLT-NAACL, 
New York, NY. 
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, 
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Mi-
chael Heilman, Dani Yogatama, Jeffrey Flanigan, 
and Noah A. Smith. 2011. Part-of-speech tagging 
for twitter: Annotation, features, and experiments. 
In Proceedings of ACL-HLT ?11.  
Stephan Gouws, Donald Metzler, Congxing Cai, and 
Eduard Hovy. 2011. Contextual bearing on linguis-
tic variation in social media. In Proceedings of the 
Workshop on Languages in Social Media, LSM 
?11, pages 20?29, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics. 
Nizar Habash, Mona Diab, and Owen Rambow 
(2012a).Conventional Orthography for Dialectal 
Arabic: Principles and Guidelines ? Egyptian Ara-
bic. Technical Report CCLS-12-02, Columbia 
University Center for Computational Learning Sys-
tems.  
Nizar Habash, Mona Diab, and Owen Rabmow. 
2012b. Conventional Orthography for Dialectal 
Arabic. In Proceedings of the Language Resources 
and Evaluation Conference (LREC), Istanbul.  
Nizar Habash, Ramy Eskander, and Abdelati Haw-
wari. 2012c. A Morphological Analyzer for Egyp-
tian Arabic. In Proceedings of the Twelfth Meeting 
of the Special Interest Group on Computational 
Morphology and Phonology, pages 1?9, Montr?al, 
Canada.  
Kevin Knight and Jonathan Graehl. 1997. Machine 
Transliteration. In Proceedings of the Conference 
of the Association for Computational Linguistics 
(ACL). 
Linguistic Data Consortium. 2014. BOLT Program: 
Romanized Arabic (Arabizi) to Arabic Translitera-
tion and Normalization Guidelines, Version 3.1. 
Linguistic Data Consortium, April 21, 2014.  
Marco Lui, Jey Han Lau, and Timothy Baldwin. 
2014. Automatic detection and language identifica-
tion of multilingual documents. In Proceedings of 
the Language Resources and Evaluation Confer-
ence (LREC), Reykjavik, Iceland.  
Mohamed Maamouri, Ann Bies, Seth Kulick, Michael 
Ciul, Nizar Habash and Ramy Eskander. 2014. De-
veloping a dialectal Egyptian Arabic Treebank: 
Impact of Morphology and Syntax on Annotation 
and Tool Development. In Proceedings of the Lan-
guage Resources and Evaluation Conference 
(LREC), Reykjavik, Iceland. 
Yaser Al-Onaizan and Kevin Knight. 2002. Machine 
Transliteration of Names in Arabic Text. In Pro-
ceedings of ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, 
Ahmed El Kholy, Ramy Eskander, Nizar Habash, 
Manoj Pooleery, Owen Rambow, and Ryan M. 
Roth. 2014. MADAMIRA: A Fast, Comprehensive 
Tool for Morphological Analysis and Disambigua-
tion of Arabic. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Rey-
kjavik, Iceland.  
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11. 
Wael Salloum and Nizar Habash. 2011. Dialectal to 
Standard Arabic Paraphrasing to Improve Arabic- 
English Statistical Machine Translation. In Pro- 
ceedings of the First Workshop on Algorithms and 
Resources for Modelling of Dialects and Language 
Varieties, pages 10?21, Edinburgh, Scotland.  
Zhiyi Song, Stephanie Strassel, Haejoong Lee, Kevin 
Walker, Jonathan Wright, Jennifer Garland, Dana 
Fore, Brian Gainor, Preston Cabe, Thomas Thom-
as, Brendan Callahan, Ann Sawyer. Collecting 
Natural SMS and Chat Conversations in Multiple 
Languages: The BOLT Phase 2 Corpus. In Pro-
ceedings of the Language Resources and Evalua-
tion Conference (LREC) 2014, Reykjavik, Iceland. 
Clare Voss, Stephen Tratz, Jamal Laoudi, and Dou- 
glas Briesch. 2014. Finding romanized Arabic dia-
lect in code-mixed tweets. In Proceedings of the 
Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, 
Iceland. 
Omar F Zaidan and Chris Callison-Burch. 2011. The 
arabic online commentary dataset: an annotated da-
taset of informal arabic with high dialectal content. 
In Proceedings of ACL, pages 37?41. 
102
Appendix A: File Format Examples 
 
 
 
Example 1: 
 
<su id="s1582"> 
  <source>marwan ? ana walahi knt gaya today :/</source> 
   <annotated_arabizi> 
        <token id="t0" tag="name">marwan</token> 
       <token id="t1" tag="punctuation">?</token> 
       <token id="t2">ana</token> 
      <token id="t3">walahi</token> 
   <token id="t4">knt</token> 
        <token id="t5">gaya</token> 
       <token id="t6" tag="foreign">today</token> 
        <token id="t7">:/</token> 
     </annotated_arabizi> 
    <auto_transliteration> :/ ???? ???? ??? ?? ???  ?????? </auto_transliteration> 
<corrected_transliteration> # ????  ???? ??? ?? ???  ?????? </corrected_transliteration> 
<retokenized_transliteration> # ???? ???? ??? ?? ???  ?????? </retokenized_transliteration> 
     <translation lang="eng">Marwan? I swear I was coming today :/</translation> 
     <messages> 
<message id="m2377" time="2013-10-01 22:03:34 UTC" participant="139360">marwan ? ana 
walahi knt gaya today :/</message> 
     </messages> 
  </su> 
 
Example 2: 
 
<su id="s3"> 
<source>W sha3rak ma2sersh:D haha</source> 
<annotated_arabizi> 
<token id="t0">W</token> 
<token id="t1">sha3rak</token> 
<token id="t2">ma2sersh:D</token> 
<token id="t3" tag="sound">haha</token> 
</annotated_arabizi> 
<auto_transliteration> ?? # [-]????? ???? [+]? </auto_transliteration> 
<corrected_transliteration> ?? #[-]????[-]?? ???? [+]? </corrected_transliteration> 
<retokenized_transliteration> ?? # ???? ?? ????? </retokenized_transliteration> 
<translation lang="eng">And your hair did not become short? :D Haha</translation> 
<messages> 
<message id="m0004" medium="IM" time="2012-12-22 15:36:31 UTC" participant="138112">W 
sha3rak ma2sersh:D haha</message> 
</messages> 
</su> 
 
 
 
103
