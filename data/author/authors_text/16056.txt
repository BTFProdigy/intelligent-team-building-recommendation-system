Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1414?1422, Dublin, Ireland, August 23-29 2014.
Modeling Newswire Events using Neural Networks for Anomaly Detection
Pradeep Dasigi
Language Technologies Institute
5000 Forbes Avenue
Pittsburgh, PA 15213
USA
pdasigi@cs.cmu.edu
Eduard Hovy
Language Technologies Institute
5000 Forbes Avenue
Pittsburgh, PA 15213
USA
hovy@cmu.edu
Abstract
Automatically identifying anomalous newswire events is a hard problem. We discuss the com-
plexity of the problem and introduce a novel technique to model events based on recursive neural
networks to represent events as composition of their semantic arguments. Our model learns to
differentiate between normal and anomalous events. We model anomaly detection as a binary
classification problem and show that the model learns useful features to classify anomaly. We
use headlines from the weird news category publicly available on newswire websites to extract
anomalous training examples and those from Gigaword as normal examples. We evaluate the
classifier on human annotated data and obtain an accuracy of 65.44%. We also show that our
model is at least as competent as the least competent human annotator in anomaly detection.
1 Introduction
Understanding events is a fundamental prerequisite for deeper semantic analysis of language. We intro-
duce the problem of automatic anomalous event detection in this paper and propose a novel event model
that can learn to differentiate between normal and anomalous events. We generally define anomalous
events as those that are unusual compared to the general state of affairs and might invoke surprise when
reported. For example, given the event mention in the following sentence
Man recovering after being shot by his dog.
one might think it is strange because dogs are not expected to shoot men. But the mentions
Man recovering after being shot by cops.
Man recovering after being bitten by a dog.
are not as unusual as the previous one. While all three sentences are equally valid syntactically, and it
is not unclear what any of them means, it is our knowledge about the role fillers ?both individually
and specifically in combination? that enables us to differentiate between normal and anomalous events.
Hence we hypothesize that anomaly is a result of unexpected or unusual combination of semantic role
fillers. Given this idea, an automatic anomaly detection algorithm has to encode the goodness of semantic
role filler coherence.
It has to be noted that event level anomaly is not the same as semantic incoherence. An event con-
structed by randomly choosing words to form each of the semantic arguments is not anomalous since we
cannot argue whether the event is normal or anomalous when it is unclear what the event means. Hence,
we define anomalous events to be the sub class of those that are semantically coherent, but are unusual
only based on real world knowledge.
Automatic anomalous event detection is a hard problem since determining what a good combination
of role fillers requires deep semantic and pragmatic knowledge. Moreover, manual judgment of anomaly
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1414
itself may be difficult and people often may not agree with each other in this regard. We describe the
difficulty in human judgment in greater detail in Section 4.4. Automatic detection of anomaly requires
encoding complex information, which has to be composed from the semantics of the individual words
in the sentence. A fundamental problem in doing so is the sparsity in semantic space due to the discrete
representations of meaning of words.
In this paper, we describe an attempt to model newswire events as a composition of the predicate with
its semantic arguments. Our approach is based on the recent models used for semantic composition using
recursive neural networks (RNN). It has been previously shown by Socher et al. (2010) and Socher et
al. (2013b) among others that RNN can effectively deal with sparsity in semantic space by represent-
ing meaning at a higher level of abstraction than the surface forms of words, and thus being able to
learn more general patterns. These models are very relevant to modeling event semantics because the
sparsity problem ranges from polysemy and synonymy at the lexical semantic level to entity and event
co-reference at the discourse level.
2 Background
2.1 Selectional Preference and Thematic Fit
Selectional preference, a notion introduced by Wilks (1973), refers to the phenomenon of the predicate
and the fillers of its arguments affecting the likelihood of fillers of other arguments. Thus the idea is that
predicate and the role fillers ?prefer? some fillers for other roles. For example, given that the predicate is
writes, the agent author prefers the patient book, while the agent programmer prefers the patient code.
This idea is used by Elman (2009), and is very similar to the role-filler composition that we use for
anomaly detection.
Erk et al. (2010) also model selectional preferences using vector spaces. They measure the goodness
of the fit of a noun with a verb in terms of the similarity between the vector of the noun and some
?exemplar? nouns taken by the verb in the same argument role. Baroni and Lenci (2010) also measure
selectional preference similarly, but instead of exemplar nouns, they calculate a prototype vector for that
role based on the vectors of the most common nouns occurring in that role for the given verb. Lenci
(2011) builds on this work and models the phenomenon that the expectations of the verb or its role-fillers
change dynamically given other role fillers.
2.2 Recursive Neural Networks
Recursive Neural Networks (RNN), first introduced by Goller and Kuchler (1996), are multilayer neural
network models used for efficient processing of structured objects of arbitrary shape. These have been
successfully used for modeling semantics of sentences of arbitrary length by Socher et al. (2010), for
sentiment analysis by Socher et al. (2013b), for syntactic parsing by Socher et al. (2013a) and for learn-
ing morphologically aware word representations by Luong et al. (2013). RNN are attractive because they
can encode compositions of meaning guided by syntax or some other linguistic structure known a priori.
Moreover, they provide flexibility in terms of learning composition weights based on supervised or un-
supervised objectives. Consequently RNN learn feature representations depending on the task. Hence,
this is a good choice for modeling event composition.
In its simplest form, an RNN processes information backed by a Directed Acyclic Graph (DAG),
where each node represents a neural network with the same parameters. The output produced at each
intermediate step of encoding usually has the same dimensionality as each of the inputs, hence RNN
projects the representation of a structure of arbitrary length into the same space as the inputs. This
property is what makes RNN recursive. An example RNN with a binary DAG (tree) structure is shown
in Figure 1. The activation from each neural network node is
c = g(y
1
?y
2
) = Sg(W (y
1
?y
2
) + b)
where ? represents concatenation of vector representations of the inputs, y
1
, y
2
? R
n?1
are the inputs,
W ? R
n?2n
is the composition weight matrix and b ? R
n?1
is the bias. Sg is a element wise sigmoid
1415
Figure 1: Example of a Recursive Neural Network backed by a binary tree
Figure 2: Example of an event tree
function. Apart from encoding the composition, RNN also produce a score of composition
s = S
?
c
where S ? R
n?1
is a scoring operator and s is a score that shows how good the composition is. (Col-
lobert et al., 2011) take an unsupervised approach to training RNN for semantic composition based on
the contrastive estimation technique proposed by (Smith and Eisner, 2005) and assuming that any word
and its context is a positive example and a random word in the same context is a negative training ex-
ample. (Socher et al., 2013b) among others use a supervised objective that is based on the label error at
the topmost node in the RNN. The parameters of the simplest model are W , b and S. For representation
learning, the inputs x
i
are also made parameters. Goller and Kuchler (1996) propose Backpropagation
through structure (BPTS), that respects the underlying DAG structure during backpropagation of gradi-
ents.
3 Neural Event Model
We define an event as the pair (V,A), where V is the predicate or a semantic verb
1
, and A is the set of its
semantic arguments like agent, patient, time, location, so on. Our aim is to obtain a vector representation
of the event that is composed from representations of individual words, while explicitly guided by the
semantic role structure. This representation can be understood as an embedding of the event in an event
space.
Neural Event Model (NEM) is a kind of RNN that is guided by a tree representation of events like the
one shown in Figure 2. The edges connected to the root of the tree correspond to the predicate and its
semantic roles (arguments). All the other edges form binary sub-trees of arguments. NEM is a super-
vised model that learns to differentiate between anomalous and normal events by classifying the event
embeddings. The inputs to NEM are the semantic arguments, and the representations of words in each
argument. We recursively compose the words in each argument to obtain argument level representations,
which are then composed to obtain an event embedding.
Intra-argument composition (called argument composition henceforth) is unsupervised, and we use
contrastive estimation to learn the parameters. The structure of the binary tree backing argument compo-
sition is determined dynamically, composing at each stage the two nodes which give the best composition
1
By semantic verb, we mean an action word whose syntactic category is not necessarily a verb. For example, in Terrorist
attacks on the World Trade Center.., attacks is not a verb but is still an action word.
1416
Figure 3: Neural Event Model: Encoding
score. Inter-argument composition (called event composition henceforth) is supervised and we use label
error to learn the parameters. Figure 3 shows how NEM encodes the event shown in Figure 2. The blue
boxes show argument composition and the red box shows event composition.
3.1 Training
NEM is trained in two phases. The first, argument composition, is unsupervised while the second, event
composition, is supervised.
3.1.1 Argument Composition
An argument composition node takes inputs of dimensionality 2n and produces an composed output
representation of dimensionality n and a composition score. Accordingly, we define the node in terms
of the parameters ?
arg
= {W
arg
? R
n?2n
; b
arg
, S
arg
? R
n?1
;V } where W
arg
, b
arg
and S
arg
are the
composition weight, bias and the scoring operators respectively as described previously, and V is the set
of representations of all the words in the vocabulary. All nodes performing argument composition use
the same parameters. Training is done in contrastive estimation fashion and the objective is
argmin
?
arg
J
arg
= argmin
?
arg
max(0, 1? s+ s
c
)
where s is the score of the composition of the entire argument produced by the root node of the argument,
and s
c
is the score produced by randomly replacing one of the words in the argument at a time. The
structure of the binary tree backing each argument is determined dynamically. This is done by starting
with leaf nodes in the tree for each of the words in the argument, comparing the composition scores of
every pair of adjacent leaf nodes, and actually composing the pair that gives the highest score, which
gives a new node. The process is repeated until we build a complete binary tree for each argument.
3.1.2 Event Composition
Event composition takes argument representations and produces the event representation and label in-
dicating whether the event is normal or anomalous. We define the event composition node in terms of
the parameters ?
event
= {W
event
? R
n?kn
; b
event
, L
event
? R
n?1
} where k is the number of semantic
arguments per event. L
event
is the label operator. The objective of this phase is
argmin
?
event
J
event
= argmin
?
event
(?l log h(e) + (1? l) log(1? h(e)))
where l is the reference binary label indicating whether the event is normal or anomalous, e is the event
representation and h(e) is the output of the logistic function. Concretely,
h(e) =
1
1 + e
?L
?
event
e
We implement the functions and perform stochastic gradient descent using Theano (Bergstra et al., 2010).
1417
4 Experiments
4.1 Event Extraction
We extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).
SENNA uses PropBank (Palmer et al., 2005) style semantic tags. We consider only the roles A0, A1,
AM-TMP and AM-LOC as the arguments of our events
2
. For example, the event in the tree shown in
Figure 2 is extracted from the sentence
Two Israeli helicopters killed 70 soldiers in Gaza strip.
and SENNA identifies the following as the semantic roles
verb:killed A0:Two Israeli helicopters A1:70 soldiers AM-LOC:in Gaza strip
4.2 Data
Since the second phase of training NEM is supervised, we need newswire events that are normal and
those that are anomalous. We crawl 3684 ?weird news? headlines available publicly on the website of
NBC news
3
, such as the following:
? India weaponizes world?s hottest chili.
? Man recovering after being shot by his dog.
? Thai snake charmer puckers up to 19 cobras.
We assume that the events extracted from this source, called NBC Weird Events (NWE) henceforth, are
anomalous for training. NWE contains 4271 events extracted using SENNA?s SRL. We use 3771 of those
events as our negative training data, and the remaining for testing. Similarly, we extract events also from
headlines in the AFE section of Gigaword, called Gigaword Events (GWE) henceforth. We assume these
events are normal. To use as positive examples for training event composition, we sample roughly the
same number of events from GWE as our negative examples from NWE. It has to be noted that each
headline may contain multiple events and some may not contain events at all.
For argument composition, we use about 100k whole sentences from AFE headlines and the weird
news headlines from which NWE are extracted. Since we are training argument composition, we do not
use the event structure in the first phase. It has to be noted that all our training data are easily available
and do not require any human annotation.
We test the performance of NEM on 1003 events which are not part of the training dataset. These
events are sampled with equal probabilities from NWE and GWE and are human annotated for anomaly.
Section 4.4 has details of the annotation task.
4.3 Word Vector Initialization
We initialize the vector representations of the words in our vocabulary using the embeddings available in
SENNA 3.0 (Collobert et al., 2011) if available, and randomly if not. For event composition, if the event
does not have a specific role filler, we input a zero vector for the role.
4.4 Annotation
We post the annotation of the test set containing 1003 events as Human Intelligence Tasks (HIT) on
Amazon Mechanical Turk (AMT). We break the task into 20 HITs and ask the workers to select one
of the four options - highly unusual, strange, normal and cannot say for each event. We ask them to
select highly unusual when the event seems too strange to be true, strange if it seems unusual but still
plausible, and cannot say only if the information present in the event is not sufficient to make a decision.
We present each event along with the original headline and the semantic arguments. Along with marking
2
These four types cover about 85% of all arguments in our training and test datasets.
3
http://www.nbcnews.com/html/msnbc/3027113/3032524/4429950/4429950_1.html
1418
Total number of annotators 22
Normal annotations 56.3%
Strange annotations 28.6%
Highly unusual annotations 10.3%
Cannot Say annotations 4.8%
Avg. events annotated per worker 344
4-way Inter annotator agreement (?) 0.34
3-way Inter annotator agreement (?) 0.56
Table 1: Annotation Statistics
one of the four options above, if an event is strange or highly unusual, we ask the annotators to select the
parts of the headline that make it so. Since there can be multiple events in the headline, the annotators
decision regarding the parts of the sentence that cause anomaly help us identify which particular event in
the headline is anomalous.
Table 1 shows some statistics of the annotation task. We compute the Inter Annotator Agreement
(IAA) in terms of Kripendorff?s alpha (Krippendorff, 1980). The advantage of using this measure instead
of the more popular Kappa is that the former can deal with missing information, which is the case with
our task since annotators work on different overlapping subsets of the test set. The 4-way IAA shown
in the table corresponds to agreement over the original 4-way decision (including cannot say) while the
3-way IAA is measured after merging the highly unusual and strange decisions.
Additionally we use MACE (Hovy et al., 2013) to assess the quality of annotation. MACE models the
annotation task as a generative process of producing the observed labels conditioned on the true labels
and the competence of the annotators, and predicts both the latent variables. The average of competence
of annotators, a value that ranges from 0 to 1, for our task is 0.49 for the 4-way decision and 0.59 for the
3-way decision.
We generate true label predictions produced by MACE, discard the events for which the prediction
remains to be cannot say, and use the rest as reference for evaluating NEM, which is described in Sec-
tion 4.5. This leaves 949 events as our reference dataset, of which only 41% of the labels are strange or
highly unusual. It has to be noted that even though our test set has equal size samples from both NWE
and GWE, the true distribution is not uniform.
Language Model Separability Given the annotations, we test to see if the sentences corresponding
to anomalous events can be separated from normal events by simpler features. We build a n-gram lan-
guage model from the training data set used for argument composition and measure the perplexity of
the sentences in the test set. Figure 4 shows a comparison of the perplexity scores for different labels.
If the n-gram features are enough to separate different classes of sentences, one would expect the sen-
tences corresponding to strange and highly unusual labels to have higher perplexity ranges than normal
sentences, because the language model is built from a dataset that is expected to have a distribution of
sentences where majority of them contain normal events. As it can be seen in Figure 4, except for a few
outliers, most data points in all the categories are in similar perplexity ranges. Hence, sentences with
different labels cannot be separated based on an n-gram language model features.
4.5 Evaluation
We evaluate the performance of event composition by comparing the predicted labels from the classifier
against the ones given by MACE. We merge the two anomaly classes and calculate accuracy of the binary
classifier, and the precision and recall of anomaly detection.
Baseline We compare the performance of our model against a baseline that is based on how well
the semantic arguments in the event match the selectional preferences of the predicate. We measure
selectional preference using Point-wise Mutual Information (PMI) (Church and Hanks, 1990) of the head
words of each semantic argument with the predicate. The baseline model is built as follows. We perform
1419
Figure 4: Comparison of perplexity scores for different labels
NEM Baseline
Accuracy 65.44% 45.22%
Anomalous
Precision 56.55% 36.30%
Recall 48.22% 59.50%
Normal
Precision 64.62% 42.08%
Recall 77.66% 33.60 %
Table 2: Classification Performance and Comparison with Baseline
dependency parsing using MaltParser (Nivre et al., 2007) on the sentences in the training data used in
the first phase of training to obtain the head words of the semantic arguments. We then calculate the PMI
values of all the pairs < h
A
, p > where h is the head word of argument A and p is the predicate of the
event. For training our baseline classifier, we use the labeled training data from the event composition
phase. The features to this classifier are the PMI measures of the < h
A
, p > pairs estimated from the
larger dataset. The classifier thus trained to distinguish between anomalous and normal events is applied
to the test set.
Table 2 shows the results and a comparison with the PMI based baseline. The accuracy of the baseline
classifier is lower than 50%, which is the expected accuracy of a classifier that assigns labels randomly.
The precision of that random classifier in predicting anomalous events is expected to be 41%, since that is
the percentage of anomaly labels in our reference set as described in Section 4.4. The accuracy of NEM
is higher than the baseline model. One possible reason for the PMI based baseline having higher recall
in predicting anomaly and lower precision is that the statistics estimated from larger training data cannot
be generalized to the test set due to sparsity issues. This indicates the advantage of using continuous
representations at a higher level of abstraction as features for classification.
To further compare NEM with human annotators, we give to MACE, the binary labels produced by
NEM along with the annotations and measure the competence. For the sake of comparison, we also
give to MACE, a list of random binary labels as one of the annotations to measure the competence of a
hypothetical worker that made random choices. These results are reported in Table 3. It can be seen that
the performance of NEM is comparable at least to the least competent human.
5 Discussion and Future Work
The two evaluation experiments show that the neural network does learn to distinguish between normal
and anomalous events. Future improvements to this model will include better event extraction techniques.
Since the current approach is supervised, the training data size for learning event composition is lim-
ited. We plan to develop unsupervised approaches that can learn good models of normal events, and
detect anomalies based on how well new events fit in the model. One possible approach is to do learning
1420
Human average 0.59
Human highest 0.70
Human lowest 0.26
Random 0.02
NEM 0.26
Table 3: Anomaly Detection Competence
based on contrastive estimation in the second phase as well. The assumption behind taking this approach
for learning is that a randomly generated data point is likely to be a negative example, which is not neces-
sarily true for learning event composition. Generating malformed events that are syntactically valid but
anomalous without much human effort can greatly help in developing such an unsupervised algorithm.
One important aspect of anomaly that is currently not handled by NEM is the level of generality of the
concepts the events contain. Usually more general concepts cause events to be more normal since they
convey lesser information. For example, an American soldier shooting another American soldier may be
considered unusual, while a soldier shooting another soldier may not be as unusual, and at the highest
level of generalization, a person shooting another person is normal. This information of generality has
to be incorporated into the event model. This can be achieved by integrating real world knowledge
from knowledge bases like Wordnet (Miller, 1995) or from corpus statistics like the work by Lin (1998)
into the event model. Bordes et al. (2011) learn continuous representations of entities and relations in
knowledge bases. More recently, an alternative approach for doing the same was proposed by Chen et
al. (2013). These representations can greatly help modeling events.
Finally, the idea of modeling event composition can help processing event data in general and can be
applied to other tasks like finding co-referent events.
6 Conclusion
We introduced the problem of anomalous newswire event detection and illustrated its difficulty. Our
approach is similar to the ones successfully used for modeling semantic composition. We showed that
while our event composition model does learn to distinguish between normal and anomalous events,
there is scope for improved models that can effectively incorporate real world information and can be
trained in an unsupervised fashion. We note that in general event composition is more difficult than
traditional semantic composition since the former also deals with pragmatics. Consequently the set of
nonsensical events is different from the set of anomalous sentences, and while meaningless events and
well composed normal events are two ends of the semantic spectrum, semantically valid anomalous
events lie somewhere between them.
Acknowledgements
This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT pro-
gram.
References
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?721.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins,
Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a cpu and gpu math expression
compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), volume 4.
Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. 2011. Learning structured embeddings of
knowledge bases. In AAAI.
1421
Danqi Chen, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2013. Learning new facts from knowl-
edge bases with neural tensor networks and semantic word vectors. arXiv preprint arXiv:1301.3618.
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Computational linguistics, 16(1):22?29.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Jeffrey L Elman. 2009. On the meaning of words and dinosaur bones: Lexical knowledge without a lexicon.
Cognitive science, 33(4):547?582.
Katrin Erk, Sebastian Pad?o, and Ulrike Pad?o. 2010. A flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics, 36(4):723?763.
Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backprop-
agation through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pages
347?352. IEEE.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
mace. In Proceedings of NAACL-HLT, pages 1120?1130.
Klaus Krippendorff. 1980. Content analysis: An introduction to its methodology. Sage Publications (Beverly
Hills).
Alessandro Lenci. 2011. Composing and updating verb argument expectations: A distributional semantic model.
In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 58?66. As-
sociation for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recur-
sive neural networks for morphology. CoNLL-2013, 104.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Noah A Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354?362.
Association for Computational Linguistics.
Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations
and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and
Unsupervised Feature Learning Workshop, pages 1?9.
Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compositional
vector grammars. In In Proceedings of the ACL conference. Citeseer.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christo-
pher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631?1642.
Yorick Wilks. 1973. Preference semantics. Technical report, DTIC Document.
1422
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 399?409,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Subgroup Detection in Ideological Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Mona Diab
Center for Computational Learning Systems
Columbia University
New York, NY, USA
mdiab@ccls.columbia.edu
Pradeep Dasigi
Department of Computer Science
Columbia University
New York, NY, USA
pd2359@columbia.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
The rapid and continuous growth of social
networking sites has led to the emergence of
many communities of communicating groups.
Many of these groups discuss ideological and
political topics. It is not uncommon that the
participants in such discussions split into two
or more subgroups. The members of each sub-
group share the same opinion toward the dis-
cussion topic and are more likely to agree with
members of the same subgroup and disagree
with members from opposing subgroups. In
this paper, we propose an unsupervised ap-
proach for automatically detecting discussant
subgroups in online communities. We analyze
the text exchanged between the participants of
a discussion to identify the attitude they carry
toward each other and towards the various as-
pects of the discussion topic. We use attitude
predictions to construct an attitude vector for
each discussant. We use clustering techniques
to cluster these vectors and, hence, determine
the subgroup membership of each participant.
We compare our methods to text clustering
and other baselines, and show that our method
achieves promising results.
1 Introduction
Online forums discussing ideological and political
topics are common1. When people discuss a dis-
puted topic they usually split into subgroups. The
members of each subgroup carry the same opinion
1www.politicalforum.com, www.createdebate.com,
www.forandagainst.com, etc
toward the discission topic. The member of a sub-
group is more likely to show positive attitude to the
members of the same subgroup, and negative atti-
tude to the members of opposing subgroups.
For example, let us consider the following two
snippets from a debate about the enforcement of a
new immigration law in Arizona state in the United
States:
(1) Discussant 1: Arizona immigration law is good.
Illegal immigration is bad.
(2) Discussant 2: I totally disagree with you. Ari-
zona immigration law is blatant racism, and quite
unconstitutional.
In (1), the writer is expressing positive attitude
regarding the immigration law and negative attitude
regarding illegal immigration. The writer of (2) is
expressing negative attitude towards the writer of
(1) and negative attitude regarding the immigration
law. It is clear from this short dialog that the writer
of (1) and the writer of (2) are members of two
opposing subgroups. Discussant 1 is supporting the
new law, while Discussant 2 is against it.
In this paper, we present an unsupervised ap-
proach for determining the subgroup membership of
each participant in a discussion. We use linguistic
techniques to identify attitude expressions, their po-
larities, and their targets. The target of attitude could
be another discussant or an entity mentioned in the
discussion. We use sentiment analysis techniques
to identify opinion expressions. We use named en-
399
tity recognition and noun phrase chunking to iden-
tify the entities mentioned in the discussion. The
opinion-target pairs are identified using a number of
syntactic and semantic rules.
For each participant in the discussion, we con-
struct a vector of attitude features. We call this vec-
tor the discussant attitude profile. The attitude pro-
file of a discussant contains an entry for every other
discussant and an entry for every entity mentioned
in the discission. We use clustering techniques to
cluster the attitude vector space. We use the clus-
tering results to determine the subgroup structure of
the discussion group and the subgroup membership
of each participant.
The rest of this paper is organized as follows. Sec-
tion 2 examines the previous work. We describe the
data used in the paper in Section 2.4. Section 3
presents our approach. Experiments, results and
analysis are presented in Section 4. We conclude
in Section 5
2 Related Work
2.1 Sentiment Analysis
Our work is related to a huge body of work on sen-
timent analysis. Previous work has studied senti-
ment in text at different levels of granularity. The
first level is identifying the polarity of individual
words. Hatzivassiloglou and McKeown (1997) pro-
posed a method to identify the polarity of adjec-
tives based on conjunctions linking them. Turney
and Littman (2003) used pointwise mutual infor-
mation (PMI) and latent semantic analysis (LSA)
to compute the association between a given word
and a set of positive/negative seed words. Taka-
mura et al (2005) proposed using a spin model to
predict word polarity. Other studies used Word-
Net to improve word polarity prediction (Hu and
Liu, 2004a; Kamps et al, 2004; Kim and Hovy,
2004; Andreevskaia and Bergler, 2006). Hassan
and Radev (2010) used a random walk model built
on top of a word relatedness network to predict the
semantic orientation of English words. Hassan et
al. (2011) proposed a method to extend their random
walk model to assist word polarity identification in
other languages including Arabic and Hindi.
Other work focused on identifying the subjectiv-
ity of words. The goal of this work is to deter-
mine whether a given word is factual or subjective.
We use previous work on subjectivity and polar-
ity prediction to identify opinion words in discus-
sions. Some of the work on this problem classi-
fies words as factual or subjective regardless of their
context (Wiebe, 2000; Hatzivassiloglou and Wiebe,
2000; Banea et al, 2008). Some other work no-
ticed that the subjectivity of a given word depends
on its context. Therefor, several studies proposed
using contextual features to determine the subjec-
tivity of a given word within its context (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-
sukawa and Yi, 2003; Popescu and Etzioni, 2005).
The second level of granularity is the sentence
level. Hassan et al (2010) presents a method for
identifying sentences that display an attitude from
the text writer toward the text recipient. They de-
fine attitude as the mental position of one partici-
pant with regard to another participant. A very de-
tailed survey that covers techniques and approaches
in sentiment analysis and opinion mining could be
found in (Pang and Lee, 2008).
2.2 Opinion Target Extraction
Several methods have been proposed to identify
the target of an opinion expression. Most of the
work have been done in the context of product re-
views mining (Hu and Liu, 2004b; Kobayashi et
al., 2007; Mei et al, 2007; Stoyanov and Cardie,
2008). In this context, opinion targets usually refer
to product features (i.e. product components or at-
tributes, as defined by Liu (2009)). In the work of
Hu and Liu (2004b), they treat frequent nouns and
noun phrases as product feature candidates. In our
work, we extract as targets frequent noun phrases
and named entities that are used by two or more dif-
ferent discussants. Scaffidi et al (2007) propose a
language model approach to product feature extrac-
tion. They assume that product features are men-
tioned more often in product reviews than they ap-
pear in general English text. However, such statistics
may not be reliable when the corpus size is small.
In another related work, Jakob and
Gurevych (2010) showed that resolving the
anaphoric links in the text significantly improves
opinion target extraction. In our work, we use
anaphora resolution to improve opinion-target
400
Participant A posted: I support Arizona because they have every right to do so. They are just upholding well-established
federal law. All states should enact such a law.
Participant B commented on A?s
post:
I support the law because the federal government is either afraid or indifferent to the issue. Arizona
has the right and the responsibility to protect the people of the State of Arizona. If this requires a
possible slight inconvenience to any citizen so be it.
Participant C commented on B?s
post:
That is such a sad thing to say. You do realize that under the 14th Amendment, the very interaction
of a police officer asking you to prove your citizenship is Unconstitutional? As soon as you start
trading Constitutional rights for ?security?, then you?ve lost.
Table 1: Example posts from the Arizona Immigration Law thread
pairing as shown in Section 3 below.
2.3 Community Mining
Previous work also studied community mining in so-
cial media sites. Somasundaran and Wiebe (2009)
presents an unsupervised opinion analysis method
for debate-side classification. They mine the web
to learn associations that are indicative of opinion
stances in debates and combine this knowledge with
discourse information. Anand et al (2011) present
a supervised method for stance classification. They
use a number of linguistic and structural features
such as unigrams, bigrams, cue words, repeated
punctuation, and opinion dependencies to build a
stance classification model. This work is limited to
dual sided debates and defines the problem as a clas-
sification task where the two debate sides are know
beforehand. Our work is characterized by handling
multi-side debates and by regarding the problem as
a clustering problem where the number of sides is
not known by the algorithm. This work also uti-
lizes only discussant-to-topic attitude predictions for
debate-side classification. Out work utilizes both
discussant-to-topic and discussant-to-discussant at-
titude predictions.
In another work, Kim and Hovy (2007) predict
the results of an election by analyzing discussion
threads in online forums that discuss the elections.
They use a supervised approach that uses unigrams,
bigrams, and trigrams as features. In contrast, our
work is unsupervised and uses different types infor-
mation. Moreover, although this work is related to
ours at the goal level, it does not involve any opinion
analysis.
Another related work classifies the speakers side
in a corpus of congressional floor debates, using
the speakers final vote on the bill as a labeling
for side (Thomas et al, 2006; Bansal et al, 2008;
Yessenalina et al, 2010). This work infers agree-
ment between speakers based on cases where one
speaker mentions another by name, and a simple al-
gorithm for determining the polarity of the sentence
in which the mention occurs. This work shows that
even with the resulting sparsely connected agree-
ment structure, the MinCut algorithm can improve
over stance classification based on textual informa-
tion alone. This work also requires that the de-
bate sides be known by the algorithm and it only
identifies discussant-to-discussant attitude. In our
experiments below we show that identifying both
discussant-to-discussant and discussant-to-topic at-
titudes achieves better results.
2.4 Data
In this section, we describe the datasets used in
this paper. We use three different datasets. The
first dataset (politicalforum, henceforth) consists of
5,743 posts collected from a political forum2. All
the posts are in English. The posts cover 12 dis-
puted political and ideological topics. The discus-
sants of each topic were asked to participate in a
poll. The poll asked them to determine their stance
on the discussion topic by choosing one item from a
list of possible arguments. The list of participants
who voted for each argument was published with
the poll results. Each poll was accompanied by a
discussion thread. The people who participated in
the poll were allowed to post text to that thread to
justify their choices and to argue with other partic-
ipants. We collected the votes and the discussion
thread of each poll. We used the votes to identify
the subgroup membership of each participant.
The second dataset (createdebate, henceforth)
comes from an online debating site 3. It consists of
2http://www.politicalforum.com
3http://www.createdebate.com
401
Source Topic Question #Sides #Posts #Participants
Politicalforum
Arizona Immigration Law Do you support Arizona in its decision to enact their
Immigration Enforcement law?
2 738 59
Airport Security Should we pick muslims out of the line and give ad-
ditional scrutiny/screening?
4 735 69
Vote for Obama Will you vote for Obama in the 2012 Presidential
elections?
2 2599 197
Createdebate
Evolution Has evolution been scientifically proved? 2 194 98
Social networking sites It is easier to maintain good relationships in social
networking sites such as Facebook.
2 70 31
Abortion Should abortion be banned 3 477 70
Wikipedia
Ireland Misleading description of Irland island partition 3 40 10
South Africa Goverment Was the current form of South African government
born in May 1910?
3 23 5
Oil Spill Obama?s response to gulf oil spill 3 30 12
Table 2: Example threads from our three datasets
30 debates containing a total of 2,712 posts. Each
debate is about one topic. The description of each
debate states two or more positions regarding the de-
bate topic. When a new participant enters the discus-
sion, she explicitly picks a position and posts text to
support it, support a post written by another partici-
pant who took the same position, or to dispute a post
written by another participant who took an opposing
position. We collected the discussion thread and the
participant positions for each debate.
The third dataset (wikipedia, henceforth) comes
from the Wikipedia4 discussion section. When a
topic on Wikipedia is disputed, the editors of that
topic start a discussion about it. We collected 117
Wikipeida discussion threads. The threads contains
a total of 1,867 posts.
The politicalforum and createdebate datasets are
self labeled as described above. To annotate the
Wikipedia data, we asked an expert annotator (a
professor in sociolinguistics who is not one of the
authors) to read each of the Wikipedia discussion
threads and determine whether the discussants split
into subgroups in which case he was asked to deter-
mine the subgroup membership of each discussant.
Table 2 lists few example threads from our three
datasets. Table 1 shows a portion of discussion
thread between three participants about enforcing a
new immigration law in Arizona. This thread ap-
peared in the polictalforum dataset. The text posted
by the three participants indicates that A?s position
4http://www.wikipedia.com
is with enforcing the law, that B agrees with A, and
that C disagrees with both. This means that A and B
belong to the same opinion subgroup, while belongs
to an opposing subgroup.
We randomly selected 6 threads from our datasets
(2 from politicalforum, 2 from createdebate, and 2
from Wikipedia) and used them as development set.
This set was used to develop our approach.
3 Approach
In this section, we describe a system that takes a
discussion thread as input and outputs the subgroup
membership of each discussant. Figure 1 illustrates
the processing steps performed by our system to de-
tect subgroups. In the following subsections we de-
scribe the different stages in the system pipeline.
3.1 Thread Parsing
We start by parsing the thread to identify posts, par-
ticipants, and the reply structure of the thread (i.e.
who replies to whom). In the datasets described in
Section 2.4, all this information was explicitly avail-
able in the thread. We tokenize the text of each post
and split it into sentences using CLAIRLib (Abu-
Jbara and Radev, 2011).
3.2 Opinion Word Identification
The next step is to identify the words that express
opinion and determine their polarity (positive or
negative). Lehrer (1974) defines word polarity as
the direction the word deviates to from the norm. We
402
use OpinionFinder (Wilson et al, 2005a) to identify
polarized words and their polarities.
The polarity of a word is usally affected by
the context in which it appears. For example, the
word fine is positive when used as an adjective and
negative when used as a noun. For another example,
a positive word that appears in a negated context
becomes negative. OpinionFinder uses a large set of
features to identify the contextual polarity of a given
polarized word given its isolated polarity and the
sentence in which it appears (Wilson et al, 2005b).
Snippet (3) below shows the result of applying this
step to snippet (1) above (O means neutral; POS
means positive; NEG means negative).
(3) Arizona/O Immigration/O law/O good/POS ./O
Illegal/O immigration/O bad/NEG ./O
3.3 Target Identification
The goal of this step is to identify the possible tar-
gets of opinion. A target could be another discus-
sant or an entity mentioned in the discussion. When
the target of opinion is another discussant, either the
discussant name is mentioned explicitly or a second
person pronoun is used to indicate that the opinion
is targeting the recipient of the post. For example,
in snippet (2) above the second person pronoun you
indicates that the opinion word disagree is targeting
Discussant 1, the recipient of the post.
The target of opinion can also be an entity
mentioned in the discussion. We use two methods to
identify such entities. The first method uses shallow
parsing to identify noun groups (NG). We use the
Edinburgh Language Technology Text Tokenization
Toolkit (LT-TTT) (Grover et al, 2000) for this pur-
pose. We consider as an entity any noun group that
is mentioned by at least two different discussants.
We replace each identified entity with a unique
placeholder (ENTITYID). For example, the noun
group Arizona immigration law is mentioned by
Discussant 1 and Discussant 2 in snippets 1 and 2
above respectively. Therefore, we replace it with a
placehold as illustrated in snippets (4) and (5) below.
(4) Discussant 1: ENTITY1 is good. Illegal im-
NER NP Chunking
Barack Obama the Republican nominee
Middle East the maverick economists
Bush conservative ideologues
Bob McDonell the Nobel Prize
Iraq Federal Government
Table 3: Some of the entities identified using NER and
NP Chunking in a discussion thread about the US 2012
elections
migration is bad.
(5) Discussant 2: I totally disagree with you. ENTITY1
is blatant racism, and quite unconstitutional.
We only consider as entities noun groups that
contain two words or more. We impose this require-
ment because individual nouns are very common
and regarding all of them as entities will introduce
significant noise.
In addition to this shallow parsing method, we
also use named entity recognition (NER) to identify
more entities. We use the Stanford Named Entity
Recognizer (Finkel et al, 2005) for this purpose. It
recognizes three types of entities: person, location,
and organization. We impose no restrictions on the
entities identified using this method. Again, we re-
place each distinct entity with a unique placeholder.
The final set of entities identified in a thread is the
union of the entities identified by the two aforemen-
tioned methods. Table 3
Finally, a challenge that always arises when
performing text mining tasks at this level of gran-
ularity is that entities are usually expressed by
anaphorical pronouns. Previous work has shown
that For example, the following snippet contains
an explicit mention of the entity Obama in the first
sentence, and then uses a pronoun to refer to the
same entity in the second sentence. The opinion
word unbeatable appears in the second sentence
and is syntactically related to the pronoun He.
In the next subsection, it will become clear why
knowing which entity does the pronoun He refers to
is essential for opinion-target pairing.
(6) It doesn?t matter whether you vote for Obama.
403
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Opinion Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Target Identification 
? Anaphora resolution 
? Identify named entities 
? Identify Frequent noun 
phrases. 
? Identify mentions of 
other discussants 
Opinion-Target Pairing 
? Dependency Rules 
 
 
 
Discussant Attitude 
Profiles (DAPs)  
 
 
 
Clustering 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text. 
? Split posts into sentences 
 
Figure 1: An overview of the subgroups detection system
He is unbeatable.
Jakob and Gurevych (2010) showed experi-
mentally that resolving the anaphoric links in the
text significantly improves opinion target extraction.
We use the Beautiful Anaphora Resolution Toolkit
(BART) (Versley et al, 2008) to resolve all the
anaphoric links within the text of each post sepa-
rately. The result of applying this step to snippet (6)
is:
(6) It doesn?t matter whether you vote for Obama.
Obama is unbeatable.
Now, both mentions of Obama will be recog-
nized by the Stanford NER system and will be
identified as one entity.
3.4 Opinion-Target Pairing
At this point, we have all the opinion words and
the potential targets identified separately. The next
step is to determine which opinion word is target-
ing which target. We propose a rule based approach
for opinion-target pairing. Our rules are based on
the dependency relations that connect the words in
a sentence. We use the Stanford Parser (Klein and
Manning, 2003) to generate the dependency parse
tree of each sentence in the thread. An opinion word
and a target form a pair if they stratify at least one
of our dependency rules. Table 4 illustrates some
of these rules 5. The rules basically examine the
types of the dependencies on the shortest path that
connect the opinion word and the target in the de-
pendency parse tree. It has been shown in previous
work on relation extraction that the shortest depen-
dency path between any two entities captures the in-
formation required to assert a relationship between
them (Bunescu and Mooney, 2005).
If a sentence S in a post written by participant
Pi contains an opinion word OPj and a target TRk,
and if the opinion-target pair satisfies one of our de-
pendency rules, we say that Pi expresses an attitude
towards TRk. The polarity of the attitude is deter-
mined by the polarity of OPj . We represent this as
Pi
+
? TRk if OPj is positive and Pi
?
? TRk if OPj
is negative.
It is likely that the same participant Pi express
sentiment toward the same target TRk multiple
times in different sentences in different posts. We
keep track of the counts of all the instances of posi-
tive/negative attitude Pi expresses toward TRk. We
represent this as Pi
m+
???
n?
TRk where m (n) is the
number of times Pi expressed positive (negative) at-
titude toward TRk.
3.5 Discussant Attitude Profile
We propose a representation of discussantsa?ttitudes
towards the identified targets in the discussion
thread. As stated above, a target could be another
discussant or an entity mentioned in the discussion.
5The code will be made publicly available at the time of
publication
404
ID Rule In Words Example
R1 OP ? nsubj ? TR The target TR is the nominal subject of the opinion
word OP
ENTITY1TR is goodOP .
R2 OP ? dobj ? TR The target T is a direct object of the opinion OP I hateOP ENTITY2TR
R3 OP ? prep ? ? TR The target TR is the object of a preposition that
modifies the opinion word OP
I totally disagreeOP with youTR.
R4 TR? amod? OP The opinion is an adjectival modifier of the target The badOP ENTITY3TR is spreading lies
R5 OP ? nsubjpass? TR The target TR is the nominal subject of the passive
opinion word OP
ENTITY4TR is hatedOP by everybody.
R6 OP ? prep ? ? poss? TR The opinion word OP connected through a prep ?
relation as in R2 to something possessed by the
target TR
The main flawOP in yourTR analysis is
that it?s based on wrong assumptions.
R7 OP ? dobj ? poss? TR The target TR possesses something that is the direct
object of the opinion word OP
I likeOP ENTITY5TR?s brilliant ideas.
R8 OP ? csubj ? nsubj ? TR The opinon word OP is a causal subject of a phrase
that has the target TR as its nominal subject
What ENTITY6TR announced was
misleadingOP .
Table 4: Examples of the dependency rules used for opinion-target pairing.
Our representation is a vector containing numeri-
cal values. The values correspond to the counts of
positive/negative attitudes expressed by the discus-
sant toward each of the targets. We call this vector
the discussant attitude profile (DAP). We construct a
DAP for every discussant. Given a discussion thread
with d discussants and e entity targets, each attitude
profile vector has n = (d + e) ? 3 dimensions. In
other words, each target (discussant or entity) has
three corresponding values in the DAP: 1) the num-
ber of times the discussant expressed positive atti-
tude toward the target, 2) the number of times the
discussant expressed a negative attitude towards the
target, and 3) the number of times the the discussant
interacted with or mentioned the target. It has to be
noted that these values are not symmetric since the
discussions explicitly denote the source and the tar-
get of each post.
3.6 Clustering
At this point, we have an attitude profile (or vec-
tor) constructed for each discussant. Our goal is to
use these attitude profiles to determine the subgroup
membership of each discussant. We can achieve this
goal by noticing that the attitude profiles of discus-
sants who share the same opinion are more likely to
be similar to each other than to the attitude profiles
of discussants with opposing opinions. This sug-
gests that clustering the attitude vector space will
achieve the goal and split the discussants into sub-
groups according to their opinion.
4 Evaluation
In this section, we present several levels of evalu-
ation of our system. First, we compare our sys-
tem to baseline systems. Second, we study how the
choice of the clustering algorithm impacts the re-
sults. Third, we study the impact of each component
in our system on the performance. All the results
reported in this section that show difference in the
performance are statistically significant at the 0.05
level (as indicated by a 2-tailed paired t-test). Be-
fore describing the experiments and presenting the
results, we first describe the evaluation metrics we
use.
4.0.1 Evaluation Metrics
We use two evaluation metrics to evaluate sub-
groups detection accuracy: Purity and Entropy. To
compute Purity (Manning et al, 2008), each clus-
ter is assigned the class of the majority vote within
the cluster, and then the accuracy of this assignment
is measured by dividing the number of correctly as-
signed members by the total number of instances. It
can be formally defined as:
purity(?, C) =
1
N
?
k
max
j
|?k ? cj | (1)
where ? = {?1, ?2, ..., ?k} is the set of clusters
and C = {c1, c2, ..., cJ} is the set of classes. ?k is
interpreted as the set of documents in ?k and cj as
405
the set of documents in cj . The purity increases as
the quality of clustering improves.
The second metric is Entropy. The Entropy of a
cluster reflects how the members of the k distinct
subgroups are distributed within each resulting clus-
ter; the global quality measure is computed by aver-
aging the entropy of all clusters:
Entropy = ?
j? nj
n
i?
P (i, j)? log2P (i, j)
(2)
where P (i, j) is the probability of finding an ele-
ment from the category i in the cluster j, nj is the
number of items in cluster j, and n the total num-
ber of items in the distribution. In contrast to purity,
the entropy decreases as the quality of clustering im-
proves.
4.1 Comparison to Baseline Systems
We compare our system (DAPC) that was described
in Section 3 to two baseline methods. The first base-
line (GC) uses graph clustering to partition a net-
work based on the interaction frequency between
participants. We build a graph where each node
represents a participant. Edges link participants if
they exchange posts, and edge weights are based on
the number of interactions. We tried two methods
for clustering the resulting graph: spectral partition-
ing (Luxburg, 2007) and a hierarchical agglomera-
tion algorithm which works by greedily optimizing
the modularity for graphs (Clauset et al, 2004).
The second baseline (TC) is based on the premise
that the member of the same subgroup are more
likely to use vocabulary drawn from the same lan-
guage model. We collect all the text posted by each
participant and create a tf-idf representations of the
text in a high dimensional vector space. We then
cluster the vector space to identify subgroups. We
use k-means (MacQueen, 1967) as our clustering
algorithm in this experiment (comparison of vari-
ous clustering algorithms is presented in the next
subsection). The distances between vectors are
Eculidean distances. Table 5 shows that our sys-
tem performs significantly better the baselines on the
three datasets in terms of both the purity (P ) and the
entropy (E) (notice that lower entropy values indi-
cate better clustering). The values reported are the
Method Createdebate Politicalforum Wikipedia
P E P E P E
GC - Spectral 0.50 0.85 0.50 0.88 0.49 0.89
GC - Hierarchical 0.48 0.86 0.47 0.89 0.49 0.87
TC - kmeans 0.51 0.84 0.49 0.88 0.52 0.85
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
Table 5: Comparison to baseline systems
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC - EM 0.63 0.71 0.61 0.82 0.63 0.61
DAPC - FF 0.63 0.70 0.60 0.83 0.64 0.59
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
Table 6: Comparison of different clustering algorithms
average results of the threads of each dataset. We
believe that the baselines performed poorly because
the interaction frequency and the text similarity are
not key factors in identifying subgroup structures.
Many people would respond to people they disagree
with more, while others would mainly respond to
people they agree with most of the time. Also, peo-
ple in opposing subgroups tend to use very similar
text when discussing the same topic and hence text
clustering does not work as well.
4.2 Choice of the clustering algorithm
We experimented with three different clustering al-
gorithms: expectation maximization (EM), and k-
means (MacQueen, 1967), and FarthestFirst (FF)
(Hochbaum and Shmoys, 1985; Dasgupta, 2002).
As we did in the previous subsection, we use
Eculidean distance to measure the distance between
vectors All the system (DAP) components are in-
cluded as described in Section 3. The purity and
entropy values using each algorithm are shown in
Table 6. Although k-means seems to be performing
slightly better than other algorithms, the differences
in the results are not significant. This indicates that
the choice of the clustering algorithm does not have
a noticeable impact on the results. We also exper-
imented with using Manhattan distance and cosine
similarity instead of Euclidean distance to measure
the distance between attitude vectors. We noticed
that the choice of the distance does not have signifi-
cant impact on the results as well.
406
4.3 Component Evaluation
In this subsection, we evaluate the impact of the dif-
ferent components in the pipeline on the system per-
formance. We do that by removing each component
from the pipeline and measuring the change in per-
formance. We perform the following experiments:
1) We run the full system with all its components
included (DAPC). 2) We run the system and in-
clude only discussant-to-discussant attitude features
in the attitude vectors (DAPC-DD). 3) We include
only discussant-to-entity attitude features in the atti-
tude vectors (DAPC-DE). 4) We include only senti-
ment features in the attitude vector; i.e. we exclude
the interaction count features (DAPC-SE). 5) We in-
clude only interaction count features to the attitude
vector; i.e. we exclude sentiment features (DAPC-
INT). 6) We skip the anaphora resolution step in the
entity identification component (DAPC-NO AR). 7)
We only use named entity recognition to identify en-
tity targets; i.e. we exclude the entities identified
through noun phrasing chunking (DAPC-NER). 8)
Finally, we only noun phrase chunking to identify
entity targets (DAPC-NP). In all these experiments
k-means is used for clustering and the number of
clusters is set as explained in the previous subsec-
tion.
The results show that all the components in the
system contribute to better performance of the sys-
tem. We notice from the results that the performance
of the system drops significantly if sentiment fea-
tures are not included. This is result corroborates
our hypothesis that interaction features are not suffi-
cient factors for detecting rift in discussion groups.
Including interaction features improve the perfor-
mance (although not by a big difference) because
they help differentiate between the case where par-
ticipants A and B never interacted with each other
and the case where they interact several time but
never posted text that indicate difference in opin-
ion between them. We also notice that the perfor-
mance drops significantly in DAPC-DD and DAPC-
DD which also supports our hypotheses that both
the sentiment discussants show toward one another
and the sentiment they show toward the aspects of
the discussed topic are important for the task. Al-
though using both named entity recognition (NER)
and noun phrase chunking achieves better results, it
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC 0.64 0.68 0.61 0.80 0.66 0.55
DAPC-DD 0.59 0.77 0.57 0.86 0.62 0.61
DAPC-DE 0.60 0.69 0.58 0.84 0.58 0.78
DAPC-SE 0.62 0.70 0.60 0.83 0.61 0.62
DAPC-INT 0.54 0.88 0.52 0.91 0.57 0.85
DAPC-NO AR 0.62 0.72 0.60 0.84 0.64 0.60
DAPC-NER 0.61 0.71 0.58 0.86 0.63 0.59
DAPC-NP 0.63 0.75 0.59 0.84 0.65 0.62
Table 7: Impact of system components on the perfor-
mance
can also be noted from the results that NER con-
tributes more to the system performance. Finally,
the results support Jakob and Gurevych (2010) find-
ings that anaphora resolution aids opinion mining
systems.
5 Conclusions
In this paper, we presented an approach for subgroup
detection in ideological discussions. Our system
uses linguistic analysis techniques to identify the at-
titude the participants of online discussions carry to-
ward each other and toward the aspects of the discus-
sion topic. Attitude prediction as well as interaction
frequency to construct an attitude vector for each
participant. The attitude vectors of discussants are
then clustered to form subgroups. Our experiments
showed that our system outperforms text clustering
and interaction graph clustering. We also studied the
contribution of each component in our system to the
overall performance.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
407
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib:
A toolkit for natural language processing, information
retrieval, and network analysis. In Proceedings of the
ACL-HLT 2011 System Demonstrations, pages 121?
126, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Workshop
on Computational Approaches to Subjectivity and Sen-
timent Analysis (WASSA 2.011), pages 1?9, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL?06.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724?731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70:066111.
Sanjoy Dasgupta. 2002. Performance guarantees for
hierarchical clustering. In 15th Annual Conference
on Computational Learning Theory, pages 351?363.
Springer.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. Lt ttt - a flexible tokenisation
tool. In In Proceedings of Second International Con-
ference on Language Resources and Evaluation, pages
1147?1154.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Ahmed Hassan, Amjad AbuJbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 592?597, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Hochbaum and Shmoys. 1985. A best possible heuristic
for the k-center problem. Mathematics of Operations
Research, 10(2):180?184.
Minqing Hu and Bing Liu. 2004a. Mining and summa-
rizing customer reviews. In KDD?04, pages 168?177.
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 263?268, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423?430.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL.
Adrienne Lehrer. 1974. Semantic fields and lezical struc-
ture. North Holland, Amsterdam and New York.
408
Bing Liu. 2009. Web Data Mining: Exploring Hyper-
links, Contents, and Usage Data (Data-Centric Sys-
tems and Applications). Springer, 1st ed. 2007. corr.
2nd printing edition, January.
Ulrike Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17:395?416, December.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman, editors, Proc. of the fifth Berkeley
Symposium on Mathematical Statistics and Probabil-
ity, volume 1, pages 281?297. University of California
Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ?07, pages 171?180, New York, NY,
USA. ACM.
Soo min Kim and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In In EMNLP-
CoNLL 2007.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP?05, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226?234, Suntec, Singapore, August. Association for
Computational Linguistics.
Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In In Col-
ing.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In In Proceedings
of EMNLP, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. Bart:
A modular toolkit for coreference resolution. In Pro-
ceedings of the ACL-08: HLT Demo Session, pages
9?12, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjectiv-
ity analysis. In Proceedings of HLT/EMNLP on Inter-
active Demonstrations, HLT-Demo ?05, pages 34?35,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Vancou-
ver, Canada.
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
409
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 65?69,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Genre Independent Subgroup Detection in Online Discussion Threads: A
Pilot Study of Implicit Attitude using Latent Textual Semantics
Pradeep Dasigi
pd2359@columbia.edu
Weiwei Guo
weiwei@cs.columbia.edu
Center for Computational Learning Systems, Columbia University
Mona Diab
mdiab@ccls.columbia.edu
Abstract
We describe an unsupervised approach to
the problem of automatically detecting sub-
groups of people holding similar opinions in
a discussion thread. An intuitive way of iden-
tifying this is to detect the attitudes of discus-
sants towards each other or named entities or
topics mentioned in the discussion. Sentiment
tags play an important role in this detection,
but we also note another dimension to the de-
tection of people?s attitudes in a discussion: if
two persons share the same opinion, they tend
to use similar language content. We consider
the latter to be an implicit attitude. In this pa-
per, we investigate the impact of implicit and
explicit attitude in two genres of social media
discussion data, more formal wikipedia dis-
cussions and a debate discussion forum that
is much more informal. Experimental results
strongly suggest that implicit attitude is an im-
portant complement for explicit attitudes (ex-
pressed via sentiment) and it can improve the
sub-group detection performance independent
of genre.
1 Introduction
There has been a significant increase in discus-
sion forum data in online media recently. Most of
such discussion threads have a clear debate compo-
nent in them with varying levels of formality. Auto-
matically identifying the groups of discussants with
similar attitudes, or subgroup detection, is an inter-
esting problem which allows for a better understand-
ing of the data in this genre in a manner that could
directly benefit Opinion Mining research as well as
Community Mining from Social Networks.
A straight-forward approach to this problem is
to apply Opinion Mining techniques, and extract
each discussant?s attitudes towards other discussants
and entities being discussed. But the challenge is
that Opinion Mining is not mature enough to ex-
tract all the correct opinions of discussants. In ad-
dition, without domain knowledge, using unsuper-
vised techniques to do this is quite challenging.
On observing interactions from these threads, we
believe that there is another dimension of attitude
which is expressed implicitly. We find that people
sharing the same opinion tend to speak about the
same topics even though they do not explicitly ex-
press their sentiment. We refer to this as Implicit
Attitude. One such example may be seen in the two
posts in Table 1. It can be seen that even though dis-
cussants A and B do not express explicit sentiments,
they hold similar views. Hence it can be said that
there is an agreement in their implicit attitudes.
Attempting to find a surface level word similar-
ity between posts of two discussants is not sufficient
as there are typically few overlapping words shared
among the posts. This is quite significant a problem
especially given the relative short context of posts.
Accordingly, in this work, we attempt to model the
implicit latent similarity between posts as a means of
identifying the implicit attitudes among discussants.
We apply variants on Latent Dirichelet Allocation
(LDA) based topic models to the problem (Blei et
al., 2003).
Our goal is identify subgroups with respect to dis-
cussants? attitudes towards each other, the entities
and topics in a discussion forum. To our knowl-
edge, this is the first attempt at using text similar-
ity as an indication of user attitudes. We investigate
the influence of the explicit and implicit attitudes on
two genres of data, one more formal than the other.
We find an interesting trend. Explicit attitude alone
65
as a feature is more useful than implicit attitude in
identifying sub-groups in informal data. But in the
case of formal data, implicit attitude yields better re-
sults. This may be due to the fact that in informal
data, strong subjective opinions about entities/events
or towards other discussants are expressed more ex-
plicitly. This is generally not the case in the formal
genre where ideas do not have as much sentiment as-
sociated with them, and hence the opinions are more
?implicit?. Finally, we observe that combining both
kinds of features improves performance of our sys-
tems for both genres.
2 Related Work
Substantial research exists in the fields of Opin-
ion Identification and Community Mining that is re-
lated to our current work. (Ganapathibhotla and
Liu, 2008) deal with the problem of finding opin-
ions from comparative sentences. Many previous
research efforts related to Opinion Target Identifi-
cation (Hu and Liu, 2004; Kobayashi et al, 2007;
Jakob and Gurevych, 2010), focus on the domain of
product reviews where they exploit the genre in mul-
tiple ways. Somasundaran and Wiebe (2009) used
unsupervised methods to identify stances in online
debates. They mine the web to find associations
indicative of opinions and combine them with dis-
course information. Their problem essentially deals
with the debate genre and finding the stance of an in-
dividual given two options. Ours is a more general
problem since we deal with discussion data in gen-
eral and not debates on specific topics. Hence our
aim is to identify multiple groups, not just two.
In terms of Sentiment Analysis, the work done by
Hassan et al(2010) in using part-of-speech and de-
pendency structures to identify polarities of attitudes
is similar to our work. But they predict binary po-
larities in attitudes, and our goal of identification of
sub-groups is a more general problem in that we aim
at identifying multiple subgroups.
3 Approach
We tackle the problem using Vector Space Mod-
eling techniques to represent the discussion threads.
Each vector represents a discussant in the thread cre-
ating an Attitude Profile (AP). We use a clustering
algorithm to partition the vector space of APs into
multiple sub-groups. The idea is that resulting clus-
ters would comprise sub-groups of discussants with
similar attitudes.
3.1 Basic Features
We use two basic features, namely Negative and
Positive sentiment towards specific discussants and
entities like in the work done by (Abu-Jbara et al,
2012). We start off by determining sentences that
express attitude in the thread, attitude sentences
(AS). We use OpinionFinder (Wilson et al, 2005)
which employs negative and positive polarity cues.
For determining discussant sentiment, we need to
first identify who the target of their sentiment is: an-
other discussant, or an entity, where an entity could
be a topic or a person not participating in the dis-
cussion. Sentiment toward another discussant:
This is quite challenging since explicit sentiment ex-
pressed in a post is not necessarily directed towards
another discussant to whom it is a reply. It is pos-
sible that a discussant may be replying to another
poster but expressing an attitude towards a third en-
tity or discussant. However as a simplifying assump-
tion, similar to the work of (Hassan et al, 2010),
we adopt the view that replies in the sentences that
are determined to be attitudinal and contain second-
person pronouns (you, your, yourself) are assumed
to be directed towards the recipients of the replies.
Sentiment toward an entity: We again adopt a sim-
plifying view by modeling all the named entities in
a sentence without heeding the roles these entities
play, i.e. whether they are targets or not. Accord-
ingly, we extract all the named entities in a sentence
using Stanford?s Name Entity Recognizer (Finkel et
al., 2005). We only focus on Person and Organiza-
tion named entities.
3.2 Extracting Implicit Attitudes
We define implicit attitudes as the semantic sim-
ilarity between texts comprising discussant utter-
ances or posts in a thread. We cannot find enough
overlapping words between posts, since some posts
are very short. Hence we apply LDA (Blei et al,
2003) on texts to extract latent semantics of texts.
We split text into sentences, i.e., each sentence is
treated as a single document. Accordingly, each sen-
tence is represented as a K-dimension vector. By
computing the similarity on these vectors, we obtain
a more accurate semantic similarity.
66
A: There are a few other directors in the history of cinema who have achieved such a singular and consistent worldview as Kubrick.
His films are very philosophically deep, they say something about everything, war, crime, relationships, humanity, etc.
B: All of his films show the true human nature of man and their inner fights and all of them are very
philosophical. Alfred was good in suspense and all, but his work is not as deep as Kubrick?s
Table 1: Example of Agreement based on Implicit Attitude
WIKI CD
Median No. of Discussants (n) 6 29
Predicted No. of Clusters (d
?
n
2 e) 2 4
Median No. of Actual Classes 3 3
Table 2: Number of Clusters
3.3 Clustering Attitude Space
A tree-based (hierarchical) clustering algorithm,
SLINK (Sibson, 1973) is used to cluster the vec-
tor space. Cosine Similarity between the vectors is
used as the inter-data point similarity measure for
clustering.1 We choose the number of clusters to be
d
?n
2 e, described as the rule of thumb by (Mardia et
al., 1979), where n is the number of discussants in
the group. This rule seems to be validated by the fact
that in the data sets with which we experiment, we
note that the predicted number of clusters according
to this rule and the classes identified in the gold data
are very close as illustrated in Table 2. On average
we note that the gold data has the number of classes
per thread to be roughly 2-5.
4 Data
We use data from two online forums - Cre-
ate Debate [CD]2 and discussions from Wikipedia
[WIKI]3. There is a significant difference in the kind
of discussions in these two sources. Our WIKI data
comprises 117 threads crawled from Wikipedia. It is
relatively formal with short threads. It does not have
much negative polarity and discussants essentially
discuss the Wikipedia page in question. Hence it is
closer to an academic discussion forum. The threads
are manually annotated with sub-group information.
Given a thread, the annotator is asked to identify if
there are any sub-groups among the discussants with
similar opinions, and if yes, the membership of those
1We also experimented with K-means (MacQueen, 1967)
and found that it yields worse results compared to SLINK.
There is a fundamental difference between the two algorithms.
Where as K-Means does a random initialization of clusters,
SLINK is a deterministic algorithm. The difference in the per-
formance may be attributed to the fact that the number of initial
data points is too small for random initialization. Hence, tree
based clustering algorithms are more well suited for the current
task.
2http://www.createdebate.com
3en.wikipedia.org
Property WIKI CD
Threads 117 34
Posts per Thread 15.5 112
Sentences per Post 4.5 7.7
Tokens per Post 78.9 118.3
Word Types per Post 11.1 10.6
Discussants per Thread 6.5 34.15
Entities Discovered per Thread 6.15 32.7
Table 3: Data Statistics
subgroups.
On the other hand, CD is a forum where people
debate a specific topic. The CD data we use com-
prises 34 threads. It is more informal (with per-
vasive negative language and personal insults) than
WIKI and has longer threads. It is closer to the de-
bate genre. It has a poll associated with every de-
bate. The votes cast by the discussants in the poll
are used as the class labels for our experiments. De-
tailed statistics related to both the data sets and a
comparison can be found in Table 3.
5 Experimental Conditions
The following three features represent discussant
attitudes:
? Sentiment towards other discussants (SD) - This
corresponds to 2 ? n dimensions in the Attitude Pro-
file (AP) vector, n being the number of discussants
in the thread. This is because there are two polari-
ties and n possible targets. The value representing
this feature is the number of sentences with the re-
spective polarity ? negative or positive ? towards the
particular discussant.
? Sentiment towards entities in discussion (SE) -
Number of dimensions corresponding to this feature
is 2?e, where e is the number of entities discovered.
Similar to SD, the value taken by this feature is the
number of sentences in which that specific polarity
is shown by the discussant towards the entity.
? Implicit Attitude (IA) - n ? t dimensions are ex-
pressed using this feature, where t is the number of
topics that the topic model contains. This means that
the AP of every discussant contains the topic model
distribution of his/her interactions with every other
member in the thread. Hence, the topics in the inter-
ation between the given discussant and other mem-
bers in the thread are being modeled here. Accord-
67
ingly, high vector similarity due to IA between two
members in a thread means that they discussed sim-
ilar topics with the same people in the thread. In
our experiments, we set t = 50. We use the Gibbs
sampling based LDA (Griffiths and Steyvers, 2004).
The LDA model is built on definitions of two online
dictionaries WordNet, and Wiktionary, in addition
to the Brown corpus (BC). To create more context,
each sentence from BC is treated as a document.
The whole corpus contains 393,667 documents and
5,080,369 words.
The degree of agreement among discussants in
terms of these three features is used to identify sub-
groups among them. Our experiments are aimed at
investigating the effect of explicit attitude features
(SD and SE) in comparison with implicit feature
(IA) and how they perform when combined. So
the experimental conditions are: the three features
in isolation, each of the explicit features SD and SE
together with IA, and then all three features together.
SWD-BASE: As a baseline, we employ a simple
word frequency based model to capture topic dis-
tribution, Surface Word Distribution (SWD). SWD
is still topic modeling in the vector space, but the di-
mensions of the vectors are the frequencies of all the
unique words used by the discussant in question.
RAND-BASE: We also apply a very simple base-
line using random assignment of discussants to
groups, however the number of clusters is deter-
mined by the rule of thumb described in Section 3.3.
6 Results and Analysis
Three metrics are used for evaluation, as de-
scribed in (Manning et al, 2008): Purity, Entropy
and F-measure. Table 4 shows the results of the
9 experimental conditions. The following observa-
tions can be made: All the individual conditions SD,
SE and IA clearly outperform SWD-BASE. All the
experimental conditions outperform RAND-BASE
which indicates that using clustering is contributing
positively to the problem. SE performs worse than
SD across both datasets CD and WIKI. This may
be due to two reasons: Firstly, since the problem
is of clustering the discussant space, SD should be
a better indicator than SE. Secondly, as seen from
the comparison in Table 5, there are more polarized
sentences indicating SD than SE. IA clearly outper-
forms SD, SE and SD+SE in the case of WIKI. In
Property WIKI CD
Positive Sentences towards Discussants 5.15 17.94
Negative Sentences towards Discussants 6.75 40.38
Positive Sentences towards Entities 1.65 8.85
Negative Sentences towards Entities 1.59 8.53
Table 5: Statistics of the Attitudinal Sentences per
each Thread in the two data sets
the case of CD, it is exactly the opposite. This is an
interesting result and we believe it is mainly due to
the genre of the data. Explicit expression of senti-
ment usually increases with the increase in the in-
formal nature of discussions. Hence IA is more use-
ful in WIKI which is more formal compared to CD,
where there is less overt sentiment expression. We
note the same trend with the SWD-BASE where per-
formance on WIKI is much better than its perfor-
mance on CD. This also suggests that WIKI might
be an easier data set. A qualitative comparison of the
inter-discussant relations can be gleaned from Ta-
ble 5. There is significantly more negative language
than positive language in CD when compared with
the ratios of negative to positive language in WIKI,
which are almost the same. The best results over-
all are yielded from the combination of IA with SD
and SE, the implicit and explicit features together for
both data sets, which suggests that Implicit and ex-
plicit attitude features complement each other cap-
turing more information than each of them individ-
ually.
7 Conclusions
We proposed the use of LDA based topic mod-
eling as an implicit agreement feature for the task
of identifying similar attitudes in online discussions.
We specifically applied latent modeling to the prob-
lem of sub-group detection. We compared this with
explicit sentiment features in different genres both
in isolation and in combination. We highlighted the
difference in genre in the datasets and the necessity
for capturing different forms of information from
them for the task at hand. The best yielding con-
dition in both the dat sets combines implicit and ex-
plicit features suggesting that there is a complemen-
tarity between the two tpes of feaures.
Acknowledgement
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab.
68
Condition
WIKI CD
Purity Entropy F-measure Purity Entropy F-measure
RAND-BASE 0.6745 0.5629 0.6523 0.3986 0.9664 0.407
SWD-BASE 0.7716 0.4746 0.6455 0.4514 0.9319 0.4322
SD 0.8342 0.3602 0.667 0.8243 0.3942 0.5964
SE 0.8265 0.3829 0.6554 0.7933 0.4216 0.5818
SD+SE 0.8346 0.3614 0.6649 0.82 0.3851 0.6039
IA 0.8527 0.3209 0.6993 0.787 0.3993 0.5891
SD+IA 0.8532 0.3199 0.6977 0.8487 0.3328 0.6152
SE+IA 0.8525 0.3216 0.7015 0.7884 0.3986 0.591
SD+SE+IA 0.8572 0.3104 0.7032 0.8608 0.3149 0.6251
Table 4: Experimental Results
References
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir Radev. 2012. Subgroup detection in ideo-
logical discussions. In Proceedings of the 5oth Annual
Meeting of ACL.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics (Coling 2008).
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing,.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning.
J. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings
of Fifth Berkeley Symposium on Mathematical Statis-
tics and Probability.
Christopher D. Manning, Prabhakar Raghavan, , and Hin-
rich Schtze. 2008. . 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY,USA.
K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multi-
variate Analysis. Publisher.
R. Sibson. 1973. Slink: An optimally efficient algorithm
for the single-link cluster method. In The Computer
Journal (1973) 16 (1): 30-34.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, JanyceWiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP 2005 Demonstration.
69
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 549?555,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reranking with Linguistic and Semantic Features
for Arabic Optical Character Recognition
Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra
Center for Computational Learning Systems, Columbia University
{nadi,habash,ryanr,noura}@ccls.columbia.edu
Pradeep Dasigi Mona Diab
Safaba Translation Solutions The George Washington University
pradeep@safaba.com mtdiab@gwu.edu
Abstract
Optical Character Recognition (OCR) sys-
tems for Arabic rely on information con-
tained in the scanned images to recognize
sequences of characters and on language
models to emphasize fluency. In this paper
we incorporate linguistically and seman-
tically motivated features to an existing
OCR system. To do so we follow an n-best
list reranking approach that exploits recent
advances in learning to rank techniques.
We achieve 10.1% and 11.4% reduction in
recognition word error rate (WER) relative
to a standard baseline system on typewrit-
ten and handwritten Arabic respectively.
1 Introduction
Optical Character Recognition (OCR) is the task
of converting scanned images of handwritten,
typewritten or printed text into machine-encoded
text. Arabic OCR is a challenging problem due
to Arabic?s connected letter forms, consonantal
diacritics and rich morphology (Habash, 2010).
Therefore only a few OCR systems have been de-
veloped (Ma?rgner and Abed, 2009). The BBN
Byblos OCR system (Natajan et al, 2002; Prasad
et al, 2008; Saleem et al, 2009), which we use
in this paper, relies on a hidden Markov model
(HMM) to recover the sequence of characters from
the image, and uses an n-gram language model
(LM) to emphasize the fluency of the output. For
an input image, the OCR decoder generates an n-
best list of hypotheses each of which is associated
with HMM and LM scores.
In addition to fluency as evaluated by LMs,
other information potentially helps in discrimi-
nating good from bad hypotheses. For example,
Habash and Roth (2011) use a variety of linguistic
(morphological and syntactic) and non-linguistic
features to automatically identify errors in OCR
hypotheses. Another example presented by De-
vlin et al (2012) shows that using a statistical ma-
chine translation system to assess the difficulty of
translating an Arabic OCR hypothesis into English
gives valuable feedback on OCR quality. There-
fore, combining additional information with the
LMs could reduce recognition errors. However,
direct integration of such information in the de-
coder is difficult.
A straightforward alternative which we advo-
cate in this paper is to use the available informa-
tion to rerank the hypotheses in the n-best lists.
The new top ranked hypothesis is considered as
the new output of the system. We propose com-
bining LMs with linguistically and semantically
motivated features using learning to rank meth-
ods. Discriminative reranking allows each hypoth-
esis to be represented as an arbitrary set of features
without the need to explicitly model their interac-
tions. Therefore, the system benefits from global
and potentially complex features which are not
available to the baseline OCR decoder. This ap-
proach has successfully been applied in numerous
Natural Language Processing (NLP) tasks includ-
ing syntactic parsing (Collins and Koo, 2005), se-
mantic parsing (Ge and Mooney, 2006), machine
translation (Shen et al, 2004), spoken language
understanding (Dinarelli et al, 2012), etc. Fur-
thermore, we propose to combine several ranking
methods into an ensemble which learns from their
predictions to further reduce recognition errors.
We describe our features and reranking ap-
proach in ?2, and we present our experiments and
results in ?3.
2 Discriminative Reranking for OCR
Each hypothesis in an n-best list {hi}ni=1 is repre-
sented by a d-dimensional feature vector xi ? Rd.
Each xi is associated with a loss li to generate a
labeled n-best list H = {(xi, li)}ni=1. The loss is
computed as the Word Error Rate (WER) of the
549
hypotheses compared to a reference transcription.
For supervised training we use a set of n-best lists
H = {H(k)}Mk=1.
2.1 Learning to rank approaches
Major approaches to learning to rank can be di-
vided into pointwise score regression, pairwise
preference satisfaction, and listwise structured
learning. See Liu (2009) for a survey. In this
paper, we explore all of the following learning to
rank approaches.
Pointwise In the pointwise approach, the rank-
ing problem is formulated as a regression, or ordi-
nal classification, for which any existing method
can be applied. Each hypothesis constitutes a
learning instance. In this category we use a regres-
sion method called Multiple Additive Regression
Trees (MART) (Friedman, 2000) as implemented
in RankLib.1 The major problem with pointwise
approaches is that the structure of the list of hy-
potheses is ignored.
Pairwise The pairwise approach takes pairs of
hypotheses as instances in learning, and formal-
izes the ranking problem as a pairwise classifica-
tion or pairwise regression. We use several meth-
ods from this category.
RankSVM (Joachims, 2002) is a method based
on Support Vector Machines (SVMs) for which
we use only linear kernels to keep complexity low.
Exact optimization of the RankSVM objective can
be computationally expensive as the number of
hypothesis pairs can be very large. Approximate
stochastic training strategies reduces complexity
and produce comparable performance. There-
fore, in addition to RankSVM, we use stochas-
tic sub-gradient descent (SGDSVM), Pegasos (Pe-
gasosSVM) and Passive-Aggressive Perceptron
(PAPSVM) as implemented in Sculley (2009).2
RankBoost (Freund et al, 2003) is a pairwise
boosting approach implemented in RankLib. It
uses a linear combination of weak rankers, each of
which is a binary function associated with a single
feature. This function is 1 when the feature value
exceeds some threshold and 0 otherwise.
RankMIRA is a ranking method presented in (Le
Roux et al, 2012).3 It uses a weighted linear
combination of features which assigns the highest
1http://people.cs.umass.edu/?vdang/
ranklib.html
2http://code.google.com/p/sofia-ml
3https://github.com/jihelhere/
adMIRAble
score to the hypotheses with the lowest loss. Dur-
ing training, the weights are updated according to
the Margin-Infused Relaxed Algorithm (MIRA),
whenever the highest scoring hypothesis differs
from the hypothesis with the lowest error rate.
In pairwise approaches, the group structure of
the n-best list is still ignored. Additionally, the
number of training pairs generated from an n-best
list depends on its size, which could result in train-
ing a model biased toward larger hypothesis lists
(Cao et al, 2006).
Listwise The listwise approach takes n-best lists
as instances in both learning and prediction. The
group structure is considered explicitly and rank-
ing evaluation measures can be directly optimized.
The listwise methods we use are implemented in
RankLib.
AdaRank (Xu and Li, 2007) is a boosting ap-
proach, similar to RankBoost, except that it opti-
mizes an arbitrary ranking metric, for which we
use Mean Average Precision (MAP).
Coordinate Ascent (CA) uses a listwise linear
model whose weights are learned by a coordinate
ascent method to optimize a ranking metric (Met-
zler and Bruce Croft, 2007). As with AdaRank we
use MAP.
ListNet (Cao et al, 2007) uses a neural network
model whose parameters are learned by gradient
descent method to optimize a listwise loss based
on a probabilistic model of permutations.
2.2 Ensemble reranking
In addition to the above mentioned approaches,
we couple simple feature selection and reranking
models combination via a straightforward ensem-
ble learning method similar to stacked general-
ization (Wolpert, 1992) and Combiner (Chan and
Stolfo, 1993). Our goal is to generate an overall
meta-ranker that outperforms all base-rankers by
learning from their predictions how they correlate
with each other.
To obtain the base-rankers, we train each of the
ranking models of ?2.1 using all the features of
?2.3 and also using each feature family added to
the baseline features separately. Then, we use the
best model for each ranking approach to make pre-
dictions on a held-out data set of n-best lists. We
can think of each base-ranker as computing one
feature for each hypothesis. Hence, the scores
generated by all the rankers for a given hypothe-
sis constitute its feature vector.
The held-out n-best lists and the predictions of
550
the base-rankers represent the training data for the
meta-ranker. We choose RankSVM4 as the meta-
ranker since it performed well as a base-ranker.
2.3 Features
Our features fall into five families.
Base features include the HMM and LM scores
produced by the OCR system. These features are
used by the baseline system5 as well as by the var-
ious reranking methods.
Simple features (?simple?) include the baseline
rank of the hypothesis and a 0-to-1 range normal-
ized version of it. We also use a hypothesis confi-
dence feature which corresponds to the average of
the confidence of individual words in the hypoth-
esis; ?confidence? for a given word is computed
as the fraction of hypotheses in the n-best list
that contain the word (Habash and Roth, 2011).
The more consensus words a hypothesis contains,
the higher its assigned confidence. We also use
the average word length and the number of con-
tent words (normalized by the hypothesis length).
We define ?content words? as non-punctuation and
non-digit words. Additionally, we use a set of bi-
nary features indicating if the hypothesis contains
a sequence of duplicated characters, a date-like se-
quence and an occurrence of a specific character
class (punctuation, alphabetic and digit).
Word LM features (?LM-word?) include the
log probabilities of the hypothesis obtained us-
ing n-gram LMs with n ? {1, . . . , 5}. Separate
LMs are trained on the Arabic Gigaword 3 corpus
(Graff, 2007), and on the reference transcriptions
of the training data (see ?3.1). The LM models
are built using the SRI Language Modeling Toolkit
(Stolcke, 2002).
Linguistic LM features (?LM-MADA?) are
similar to the word LM features except that they
are computed using the part-of-speech and the
lemma of the words instead of the actual words.6
Semantic coherence feature (?SemCoh?) is
motivated by the fact that semantic information
can be very useful in modeling the fluency of
phrases, and can augment the information pro-
vided by n-gram LMs. In modeling contextual
4RankSVM has also been shown to be a good choice for
the meta-learner in general stacking ensemble learning (Tang
et al, 2010).
5The baseline ranking is simply based on the sum of the
logs of the HMM and LM scores.
6The part-of-speech and the lemmas are obtained using
MADA 3.0, a tool for Arabic morphological analysis and
disambiguation (Habash and Rambow, 2005; Habash et al,
2009).
lexical semantic information, simple bag-of-words
models usually have a lot of noise; while more
sophisticated models considering positional infor-
mation have sparsity issues. To strike a balance
between these two extremes, we introduce a novel
model of semantic coherence that is based on a
measure of semantic relatedness between pairs of
words. We model semantic relatedness between
two words using the Information Content (IC) of
the pair in a method similar to the one used by Lin
(1997) and Lin (1998).
IC(w1,d, w2) = log
f(w1, d, w2)f(?,d, ?)
f(w1, d, ?)f(?,d, w2)
Here, d can generally represent some form of re-
lation between w1 and w2. Whereas Lin (1997)
and Lin (1998) used dependency relation between
words, we use distance. Given a sentence, the dis-
tance between w1 and w2 is one plus the number
of words that are seen after w1 and before w2 in
that sentence. Hence, f(w1, d, w2) is the number
of times w1 occurs before w2 at a distance d in
all the sentences in a corpus. ? is a placeholder
for any word, i.e., f(?, d, ?) is the frequency of all
word pairs occurring at distance d. The distances
are directional and not absolute values. A simi-
lar measure of relatedness was also used by Kolb
(2009).
We estimate the frequencies from the Arabic
Gigaword. We set the window size to 3 and cal-
culate IC values of all pairs of words occurring at
distance within the window size. Since the dis-
tances are directional, it has to be noted that given
a word, its relations with three words before it and
three words after it are modeled. During testing,
for each phrase in our test set, we measure se-
mantic relatedness of pairs of words using the IC
values estimated from the Arabic Gigaword, and
normalize their sum by the number of pairs in the
phrase to obtain a measure of Semantic Coherence
(SC) of the phrase. That is,
SC(p) = 1m ?
?
1?d?W
1?i+d<n
IC(wi,d, wi+d)
where p is the phrase being evaluated, n is the
number of words in it, d is the distance between
words, W is the window size (set to 3), and m is
the number of all possible wi, wi+d pairs in the
phrase given these conditions.
551
print hand
|H?| n |h| |H?| n |h|
Hb 1,560 62 9 2,295 225 8
Hm 1,000 76 9 1,000 225 9
Ht 1,000 64 9 1,000 227 9
Table 1: Data sets statistics. |H?| refers to the
number of n-best lists, n is the average size of the
lists, and |h| is the average length of a hypothesis.
print hand
Baseline 13.8% 35%
Oracle 9.8% 20.9%
Best result 12.4% 30.9%
Table 2: WER for baseline, oracle and best
reranked hypotheses.
3 Experiments
3.1 Data and baselines
We used two data sets derived from high-
resolution image scans of typewritten and hand-
written Arabic text along with ground truth tran-
scriptions.7 The BBN Byblos system was then
used to process these scanned images into se-
quences of segments (sentence fragments) and
generate a ranked n-best list of hypotheses for
each segment (Natajan et al, 2002; Prasad et al,
2008; Saleem et al, 2009). We divided each of the
typewritten data set (?print?) and handwritten data
set (?hand?) into three disjoint parts: a training set
for the base-rankersHb, a training set for the meta-
ranker Hm and a test set Ht. Table 1 presents
some statistics about these data sets. Our base-
line is based on the sum of the logs of the HMM
and LM scores. Table 2 presents the WER for our
baseline hypothesis, the best hypothesis in the list
(our oracle) and our best reranking results which
we describe in details in ?3.2.
For LM training we used 220M words from
Arabic Gigaword 3, and 2.4M words from each
?print? and ?hand? ground truth annotations.
Effect of n-best training size onWER The size
of the training n-best lists is crucial to the learning
of the ranking model. In particular, it determines
the number of training instances per list. To deter-
mine the optimal n to use for the rest of this pa-
per, we conducted the following experiment aims
to understand the effect of the size of n-best lists
7The Anfal data set discussed here was collected by the
Linguistic Data Consortium.
30.5 
31 
31.5 
32 
32.5 
33 
33.5 
34 
12 
12.5 
13 
13.5 
14 
14.5 
15 
5 15 25 35 45 55 
Size of each training n-best list 
WER print 
hand 
Figure 1: Effect of the size of training n-best lists
on WER. The horizontal axis represents the max-
imum size of the n-best lists and the vertical axis
represents WER, left is ?print? and right is ?hand?.
on the reranking performance for one of our best
reranking models, namely RankSVM. We trained
each model with different sizes of n-best, varying
from n = 5 to n = 60 for ?print? data, and be-
tween n = 5 and n = 150 for ?hand? data. The
top n hypotheses according to the baseline are se-
lected for each n. Figure 1 plots WER as a func-
tion of the size of the training list n for both ?print?
and ?hand? data.
The lowest WER scores are achieved for n =
10 and n = 15 for both ?print? and ?hand? data.
We note that a small number of hypotheses per list
is sufficient for RankSVM to obtain a good per-
formance, but also increasing n further seems to
increase the error rate. For the rest of this paper
we use the top 10-best hypotheses per segment.
3.2 Reranking results
The reranking results for ?print? and ?hand? are
presented in Table 3. The results are presented
as the difference in WER from the baseline WER.
See the caption in Table 3 for more information.
For ?print?, the pairwise approaches clearly out-
perform the listwise approaches and achieve the
lowest WER of 12.4% (10.1% WER reduction rel-
ative to the baseline) with 7 different combinations
of rankers and feature families. While both ap-
proaches do not minimize WER directly, the pair-
wise methods have the advantage of using objec-
tives that are simpler to optimize, and they are
trained on much larger number of examples which
may explain their superiority. RankBoost, how-
ever, is less competitive with a performance closer
to that of listwise approaches. All the methods
improved over the baseline with any feature fam-
ily, except for the pointwise approach which did
552
Pointwise Listwise Pairwise
Features MA
RT
Ad
aR
ank
Lis
tNe
t
CA Ra
nkB
oos
t
Ra
nkS
VM
SG
DS
VM
Ra
nkM
IRA
Peg
a.S
VM
PA
PS
VM
Pr
int
Base 1.1 -0.4 -1.0 -1.0 -1.0 -1.1 -1.2 -1.2 -1.3 -1.3
+simple -0.1 0.0 -0.1 -0.2 0.0 -0.1 0.1 0.0 0.1 0.0
+LM-word -1.0 -0.2 0.1 -0.1 -0.1 -0.3 -0.2 -0.1 0.0 -0.1
+LM-MADA 0.0 -0.3 0.1 -0.2 -0.1 0.0 -0.1 -0.2 -0.1 -0.1
+SemCoh 0.0 -0.4 0.0 -0.2 -0.1 -0.1 0.0 -0.1 0.0 0.1
+All 0.6 0.1 0.0 0.1 0.0 0.1 0.2 0.2 0.2 0.0
Ha
nd
Base 4.2 -3.1 -3.2 -3.4 -2.9 -3.2 -3.5 -3.8 -3.6 -3.8
+simple 0.3 -0.1 0.1 0.2 0.1 -0.1 0.2 -0.2 0.1 0.2
+LM-word 0.4 -0.1 0.1 0.8 -0.2 -0.7 -0.2 -0.1 0.0 0.1
+LM-MADA 0.0 -0.5 0.1 0.0 0.1 -0.4 -0.1 0.3 -0.2 0.1
+SemCoh 0.0 -0.1 0.0 -0.4 0.0 -0.2 -0.3 -0.2 -0.2 0.0
+All 0.2 0.4 0.0 0.4 0.2 0.4 0.2 0.1 0.2 0.0
Table 3: Reranking results for the ?print? and ?hand? data sets; the ?print? baseline WER is 13.9% and the ?hand? baseline
WER is 35.0%. The ?Base? numbers represent the difference in WER between the corresponding ranker using ?Base? features
only and the baseline, which uses the same ?Base? features. The ?+features? numbers represent additional gain (relative to
?Base?) obtained by adding the corresponding feature family. The ?+All? numbers represent the gain of using all features,
relative to the best single-family system. The actual WER of a ranker can be obtained by summing the baseline WER and the
corresponding ?Base? and ?+features? scores. Bolded values are the best performers overall.
worse than the baseline. When combined with
the ?Base? features, ?LM-words? lead to improve-
ments with 8 out of 10 rankers, and proved to be
the most helpful among feature families. ?LM-
MADA? follows with improvements with 7 out of
10 rankers. The lowest WER is achieved using
one of these two LM-based families. Combining
all feature families did not help and in many cases
resulted in a higher WER than the best family.
Similar improvements are observed for ?hand?.
The lowest achieved WER is 31% (11.4% WER
reduction relative to the baseline). Here also,
the pointwise method increased the WER by 12%
relative to the baseline (as opposed to 7% for
?print?). Again, the listwise approaches are over-
all less effective than their pairwise counterparts,
except for RankBoost which resulted in the small-
est WER reduction among all rankers. The two
best rankers correspond to RankMIRA with the
?simple? and the ?SemCoh? features. The ?Sem-
Coh? feature resulted in improvements for 6 out of
the 10 rankers, and thus was the best single feature
on average for the ?hand? data set. As observed
with ?print? data, combining all the features does
not lead to the best performance.
In an additional experiment, we selected the
best model for each ranking method and combined
them to build an ensemble as described in ?2.2.
For ?hand?, the ensemble slightly outperformed
all the individual rankers and achieved the lowest
WER of 30.9%. However, for the ?print? data, the
ensemble failed to improve over the base-rankers
and resulted in a WER of 12.4%.
The best overall results are presented in Table 2.
Our best results reduce the distance to the oracle
top line by 35% for ?print? and 29% for ?hand?.
4 Conclusion
We presented a set of experiments on incorporat-
ing features into an existing OCR system via n-
best list reranking. We compared several learn-
ing to rank techniques and combined them us-
ing an ensemble technique. We obtained 10.1%
and 11.4% reduction in WER relative to the base-
line for ?print? and ?hand? data respectively. Our
best systems used pairwise reranking which out-
performed the other methods, and used the MADA
based features for ?print? and our novel semantic
coherence feature for ?hand?.
Acknowledgment
We would like to thank Rohit Prasad and Matin
Kamali for providing the data and helpful dis-
cussions. This work was funded under DARPA
project number HR0011-08-C-0004. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this paper are those of the au-
thors and do not necessarily reflect the views of
DARPA. The last two authors, Dasigi and Diab,
worked on this project while at Columbia Univer-
sity.
553
References
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou
Huang, and Hsiao-Wuen Hon. 2006. Adapting
ranking SVM to document retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?06, pages 186?193.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24th international conference on Machine learning,
ICML ?07, pages 129?136.
Philip K. Chan and Salvatore J. Stolfo. 1993. Exper-
iments on multistrategy learning by meta-learning.
In Proceedings of the second international confer-
ence on Information and knowledge management,
CIKM ?93, pages 314?323.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Comput.
Linguist., 31(1):25?70, March.
Jacob Devlin, Matin Kamali, Krishna Subramanian,
Rohit Prasad, and Prem Natarajan. 2012. Statisti-
cal Machine Translation as a Language Model for
Handwriting Recognition. In ICFHR, pages 291?
296.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative Reranking for
Spoken Language Understanding. IEEE Transac-
tions on Audio, Speech & Language Processing,
20(2):526?539.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm
for combining preferences. J. Mach. Learn. Res.,
4:933?969, December.
Jerome H. Friedman. 2000. Greedy Function Approx-
imation: A Gradient Boosting Machine. Annals of
Statistics, 29:1189?1232.
Ruifang Ge and Raymond J. Mooney. 2006. Discrimi-
native Reranking for Semantic Parsing. In ACL.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan, June.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
Arabic handwriting recognition. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?02,
pages 133?142.
Peter Kolb. 2009. Experiments on the difference be-
tween semantic similarity and relatedness. In Pro-
ceedings of the 17th Nordic Conference of Computa-
tional Linguistics, NEALT Proceedings Series Vol.
4.
Joseph Le Roux, Benoit Favre, Alexis Nasr, and
Seyed Abolghasem Mirroshandel. 2012. Gener-
ative Constituent Parsing and Discriminative De-
pendency Reranking: Experiments on English and
French. In SP-SEM-MRL 2012.
Dekang Lin. 1997. Using syntactic dependency as
local context to resolve word sense ambiguity. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, EACL ?97, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98.
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval. Now Publishers Inc., Hanover, MA,
USA.
Volker Ma?rgner and Haikal El Abed. 2009. Ara-
bic Word and Text Recognition - Current Develop-
ments. In Khalid Choukri and Bente Maegaard, ed-
itors, Proceedings of the Second International Con-
ference on Arabic Language Resources and Tools,
Cairo, Egypt, April. The MEDAR Consortium.
Donald Metzler and W. Bruce Croft. 2007. Linear
feature-based models for information retrieval. Inf.
Retr., 10(3):257?274, June.
Premkumar Natajan, Zhidong Lu, Richard Schwartz,
Issam Bazzi, and John Makhoul. 2002. Hid-
den Markov models. chapter Multilingual machine
printed OCR, pages 43?63. World Scientific Pub-
lishing Co., Inc., River Edge, NJ, USA.
554
Rohit Prasad, Shirin Saleem, Matin Kamali, Ralf Meer-
meier, and Premkumar Natarajan. 2008. Improve-
ments in hidden Markov model based Arabic OCR.
In Proceedings of International Conference on Pat-
tern Recognition (ICPR), pages 1?4.
Shirin Saleem, Huaigu Cao, Krishna Subramanian,
Matin Kamali, Rohit Prasad, and Prem Natarajan.
2009. Improvements in BBN?s HMM-Based Of-
fline Arabic Handwriting Recognition System. In
Proceedings of the 2009 10th International Confer-
ence on Document Analysis and Recognition, IC-
DAR ?09, pages 773?777.
D. Sculley. 2009. Large scale learning to rank. In
NIPS 2009 Workshop on Advances in Ranking.
Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative Reranking for Machine Trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 177?184, Boston, Massachusetts, USA,
May.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Buzhou Tang, Qingcai Chen, Xuan Wang, and Xiao-
long Wang. 2010. Reranking for stacking ensem-
ble learning. In Proceedings of the 17th interna-
tional conference on Neural information processing:
theory and algorithms - Volume Part I, ICONIP?10,
pages 575?584.
David H. Wolpert. 1992. Original Contribution:
Stacked generalization. Neural Netw., 5(2):241?
259, February.
Jun Xu and Hang Li. 2007. AdaRank: a boosting al-
gorithm for information retrieval. In Proceedings of
the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?07, pages 391?398.
555
