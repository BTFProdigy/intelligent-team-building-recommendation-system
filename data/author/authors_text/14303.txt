Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79?84,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
QuEst - A translation quality estimation framework
Lucia Specia?, Kashif Shah?, Jose G. C. de Souza? and Trevor Cohn?
?Department of Computer Science
University of Sheffield, UK
{l.specia,kashif.shah,t.cohn}@sheffield.ac.uk
?Fondazione Bruno Kessler
University of Trento, Italy
desouza@fbk.eu
Abstract
We describe QUEST, an open source
framework for machine translation quality
estimation. The framework allows the ex-
traction of several quality indicators from
source segments, their translations, exter-
nal resources (corpora, language models,
topic models, etc.), as well as language
tools (parsers, part-of-speech tags, etc.). It
also provides machine learning algorithms
to build quality estimation models. We
benchmark the framework on a number of
datasets and discuss the efficacy of fea-
tures and algorithms.
1 Introduction
As Machine Translation (MT) systems become
widely adopted both for gisting purposes and to
produce professional quality translations, auto-
matic methods are needed for predicting the qual-
ity of a translated segment. This is referred to as
Quality Estimation (QE). Different from standard
MT evaluation metrics, QE metrics do not have
access to reference (human) translations; they are
aimed at MT systems in use. QE has a number of
applications, including:
? Deciding which segments need revision by a
translator (quality assurance);
? Deciding whether a reader gets a reliable gist
of the text;
? Estimating how much effort it will be needed
to post-edit a segment;
? Selecting among alternative translations pro-
duced by different MT systems;
? Deciding whether the translation can be used
for self-training of MT systems.
Work in QE for MT started in the early 2000?s,
inspired by the confidence scores used in Speech
Recognition: mostly the estimation of word pos-
terior probabilities. Back then it was called confi-
dence estimation, which we believe is a narrower
term. A 6-week workshop on the topic at John
Hopkins University in 2003 (Blatz et al, 2004)
had as goal to estimate automatic metrics such as
BLEU (Papineni et al, 2002) and WER. These
metrics are difficult to interpret, particularly at the
sentence-level, and results of their very many trials
proved unsuccessful. The overall quality of MT
was considerably lower at the time, and therefore
pinpointing the very few good quality segments
was a hard problem. No software nor datasets
were made available after the workshop.
A new surge of interest in the field started re-
cently, motivated by the widespread used of MT
systems in the translation industry, as a conse-
quence of better translation quality, more user-
friendly tools, and higher demand for translation.
In order to make MT maximally useful in this
scenario, a quantification of the quality of trans-
lated segments similar to ?fuzzy match scores?
from translation memory systems is needed. QE
work addresses this problem by using more com-
plex metrics that go beyond matching the source
segment with previously translated data. QE can
also be useful for end-users reading translations
for gisting, particularly those who cannot read the
source language.
QE nowadays focuses on estimating more inter-
pretable metrics. ?Quality? is defined according to
the application: post-editing, gisting, etc. A num-
ber of positive results have been reported. Exam-
ples include improving post-editing efficiency by
filtering out low quality segments which would re-
quire more effort or time to correct than translating
from scratch (Specia et al, 2009; Specia, 2011),
selecting high quality segments to be published as
they are, without post-editing (Soricut and Echi-
habi, 2010), selecting a translation from either
an MT system or a translation memory for post-
editing (He et al, 2010), selecting the best trans-
lation from multiple MT systems (Specia et al,
79
2010), and highlighting sub-segments that need re-
vision (Bach et al, 2011).
QE is generally addressed as a supervised ma-
chine learning task using a variety of algorithms to
induce models from examples of translations de-
scribed through a number of features and anno-
tated for quality. For an overview of various al-
gorithms and features we refer the reader to the
WMT12 shared task on QE (Callison-Burch et
al., 2012). Most of the research work lies on
deciding which aspects of quality are more rel-
evant for a given task and designing feature ex-
tractors for them. While simple features such as
counts of tokens and language model scores can be
easily extracted, feature engineering for more ad-
vanced and useful information can be quite labour-
intensive. Different language pairs or optimisation
against specific quality scores (e.g., post-editing
time vs translation adequacy) can benefit from
very different feature sets.
QUEST, our framework for quality estimation,
provides a wide range of feature extractors from
source and translation texts and external resources
and tools (Section 2). These go from simple,
language-independent features, to advanced, lin-
guistically motivated features. They include fea-
tures that rely on information from the MT sys-
tem that generated the translations, and features
that are oblivious to the way translations were
produced (Section 2.1). In addition, by inte-
grating a well-known machine learning toolkit,
scikit-learn,1 and algorithms that are known
to perform well on this task, QUEST provides a
simple and effective way of experimenting with
techniques for feature selection and model build-
ing, as well as parameter optimisation through grid
search (Section 2.2). In Section 3 we present
experiments using the framework with nine QE
datasets.
In addition to providing a practical platform
for quality estimation, by freeing researchers from
feature engineering, QUEST will facilitate work
on the learning aspect of the problem. Quality
estimation poses several machine learning chal-
lenges, such as the fact that it can exploit a large,
diverse, but often noisy set of information sources,
with a relatively small number of annotated data
points, and it relies on human annotations that are
often inconsistent due to the subjectivity of the
task (quality judgements). Moreover, QE is highly
1http://scikit-learn.org/
non-linear: unlike many other problems in lan-
guage processing, considerable improvements can
be achieved using non-linear kernel techniques.
Also, different applications for the quality predic-
tions may benefit from different machine learn-
ing techniques, an aspect that has been mostly ne-
glected so far. Finally, the framework will also
facilitate research on ways of using quality predic-
tions in novel extrinsic tasks, such as self-training
of statistical machine translation systems, and for
estimating quality in other text output applications
such as text summarisation.
2 The QUEST framework
QUEST consists of two main modules: a feature
extraction module and a machine learning mod-
ule. The first module provides a number of feature
extractors, including the most commonly used fea-
tures in the literature and by systems submitted to
the WMT12 shared task on QE (Callison-Burch et
al., 2012). More than 15 researchers from 10 in-
stitutions contributed to it as part of the QUEST
project.2 It is implemented in Java and provides
abstract classes for features, resources and pre-
processing steps so that extractors for new features
can be easily added.
The basic functioning of the feature extraction
module requires raw text files with the source and
translation texts, and a few resources (where avail-
able) such as the source MT training corpus and
language models of source and target. Configura-
tion files are used to indicate the resources avail-
able and a list of features that should be extracted.
The machine learning module provides
scripts connecting the feature files with the
scikit-learn toolkit. It also uses GPy, a
Python toolkit for Gaussian Processes regression,
which outperformed algorithms commonly used
for the task such as SVM regressors.
2.1 Feature sets
In Figure 1 we show the types of features that
can be extracted in QUEST. Although the text
unit for which features are extracted can be of any
length, most features are more suitable for sen-
tences. Therefore, a ?segment? here denotes a sen-
tence.
From the source segments QUEST can extract
features that attempt to quantify the complexity
2http://www.dcs.shef.ac.uk/?lucia/
projects/quest.html
80
Confidence indicatorsComplexity indicators Fluency indicators
Adequacyindicators
Source text TranslationMT system
Figure 1: Families of features in QUEST.
of translating those segments, or how unexpected
they are given what is known to the MT system.
Examples of features include:
? number of tokens in the source segment;
? language model (LM) probability of source
segment using the source side of the parallel
corpus used to train the MT system as LM;
? percentage of source 1?3-grams observed in
different frequency quartiles of the source
side of the MT training corpus;
? average number of translations per source
word in the segment as given by IBM 1
model with probabilities thresholded in dif-
ferent ways.
From the translated segments QUEST can ex-
tract features that attempt to measure the fluency
of such translations. Examples of features include:
? number of tokens in the target segment;
? average number of occurrences of the target
word within the target segment;
? LM probability of target segment using a
large corpus of the target language to build
the LM.
From the comparison between the source and
target segments, QUEST can extract adequacy
features, which attempt to measure whether the
structure and meaning of the source are pre-
served in the translation. Some of these are based
on word-alignment information as provided by
GIZA++. Features include:
? ratio of number of tokens in source and target
segments;
? ratio of brackets and punctuation symbols in
source and target segments;
? ratio of percentages of numbers, content- /
non-content words in the source & target seg-
ments;
? ratio of percentage of nouns/verbs/etc in the
source and target segments;
? proportion of dependency relations between
(aligned) constituents in source and target
segments;
? difference between the depth of the syntactic
trees of the source and target segments;
? difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP phrases in
the source and target;
? difference between the number of per-
son/location/organization entities in source
and target sentences;
? proportion of person/location/organization
entities in source aligned to the same type of
entities in target segment;
? percentage of direct object personal or pos-
sessive pronouns incorrectly translated.
When available, information from the MT sys-
tem used to produce the translations can be very
useful, particularly for statistical machine transla-
tion (SMT). These features can provide an indi-
cation of the confidence of the MT system in the
translations. They are called ?glass-box? features,
to distinguish them from MT system-independent,
?black-box? features. To extract these features,
QUEST assumes the output of Moses-like SMT
systems, taking into account word- and phrase-
alignment information, a dump of the decoder?s
standard output (search graph information), global
model score and feature values, n-best lists, etc.
For other SMT systems, it can also take an XML
file with relevant information. Examples of glass-
box features include:
? features and global score of the SMT system;
? number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? average size of the target phrases;
? proportion of pruned search graph nodes;
? proportion of recombined graph nodes.
We note that some of these features are
language-independent by definition (such as the
confidence features), while others can be depen-
dent on linguistic resources (such as POS taggers),
or very language-specific, such as the incorrect
translation of pronouns, which was designed for
Arabic-English QE.
Some word-level features have also been im-
plemented: they include standard word posterior
probabilities and n-gram probabilities for each tar-
81
get word. These can also be averaged across the
whole sentence to provide sentence-level value.
The complete list of features available is given
as part of QUEST?s documentation. At the current
stage, the number of BB features varies from 80
to 123 depending on the language pair, while GB
features go from 39 to 48 depending on the SMT
system used (see Section 3).
2.2 Machine learning
QUEST provides a command-line interface mod-
ule for the scikit-learn library implemented
in Python. This module is completely indepen-
dent from the feature extraction code and it uses
the extracted feature sets to build QE models.
The dependencies are the scikit-learn li-
brary and all its dependencies (such as NumPy3
and SciPy4). The module can be configured to
run different regression and classification algo-
rithms, feature selection methods and grid search
for hyper-parameter optimisation.
The pipeline with feature selection and hyper-
parameter optimisation can be set using a con-
figuration file. Currently, the module has an
interface for Support Vector Regression (SVR),
Support Vector Classification, and Lasso learn-
ing algorithms. They can be used in conjunction
with the feature selection algorithms (Randomised
Lasso and Randomised decision trees) and the grid
search implementation of scikit-learn to fit
an optimal model of a given dataset.
Additionally, QUEST includes Gaussian Pro-
cess (GP) regression (Rasmussen and Williams,
2006) using the GPy toolkit.5 GPs are an ad-
vanced machine learning framework incorporating
Bayesian non-parametrics and kernel machines,
and are widely regarded as state of the art for
regression. Empirically we found the perfor-
mance to be similar to SVR on most datasets,
with slightly worse MAE and better RMSE.6 In
contrast to SVR, inference in GP regression can
be expressed analytically and the model hyper-
parameters optimised directly using gradient as-
cent, thus avoiding the need for costly grid search.
This also makes the method very suitable for fea-
ture selection.
3http://www.numpy.org/
4http://www.scipy.org/
5https://github.com/SheffieldML/GPy
6This follows from the optimisation objective: GPs use a
quadratic loss (the log-likelihood of a Gaussian) compared to
SVR which penalises absolute margin violations.
Data Training Test
WMT12 (en-es) 1,832 422
EAMT11 (en-es) 900 64
EAMT11 (fr-en) 2,300 225
EAMT09-s1-s4 (en-es) 3,095 906
GALE11-s1-s2 (ar-en) 2,198 387
Table 1: Number of sentences used for training
and testing in our datasets.
3 Benchmarking
In this section we benchmark QUEST on nine ex-
isting datasets using feature selection and learning
algorithms known to perform well in the task.
3.1 Datasets
The statistics of the datasets used in the experi-
ments are shown in Table 1.7
WMT12 English-Spanish sentence translations
produced by an SMT system and judged for
post-editing effort in 1-5 (worst-best), taking a
weighted average of three annotators.
EAMT11 English-Spanish (EAMT11-en-es)
and French-English (EAMT11-fr-en) sentence
translations judged for post-editing effort in 1-4.
EAMT09 English sentences translated by four
SMT systems into Spanish and scored for post-
editing effort in 1-4. Systems are denoted by s1-s4.
GALE11 Arabic sentences translated by two
SMT systems into English and scored for ade-
quacy in 1-4. Systems are denoted by s1-s2.
3.2 Settings
Amongst the various learning algorithms available
in QUEST, to make our results comparable we se-
lected SVR with radial basis function (RBF) ker-
nel, which has been shown to perform very well
in this task (Callison-Burch et al, 2012). The op-
timisation of parameters is done with grid search
using the following ranges of values:
? penalty parameter C: [1, 10, 10]
? ?: [0.0001, 0.1, 10]
? : [0.1, 0.2, 10]
where elements in list denote beginning, end and
number of samples to generate, respectively.
For feature selection, we have experimented
with two techniques: Randomised Lasso and
7The datasets can be downloaded from http://www.
dcs.shef.ac.uk/?lucia/resources.html
82
Gaussian Processes. Randomised Lasso (Mein-
shausen and Bu?hlmann, 2010) repeatedly resam-
ples the training data and fits a Lasso regression
model on each sample. A feature is said to be se-
lected if it was selected (i.e., assigned a non-zero
weight) in at least 25% of the samples (we do this
1000 times). This strategy improves the robust-
ness of Lasso in the presence of high dimensional
and correlated inputs.
Feature selection with Gaussian Processes is
done by fitting per-feature RBF widths (also
known as the automatic relevance determination
kernel). The RBF width denotes the importance
of a feature, the narrower the RBF the more impor-
tant a change in the feature value is to the model
prediction. To make the results comparable with
our baseline systems we select the 17 top ranked
features and then train a SVR on these features.8
As feature sets, we select all features available
in QUEST for each of our datasets. We differen-
tiate between black-box (BB) and glass-box (GB)
features, as only BB are available for all datasets
(we did not have access to the MT systems that
produced the other datasets). For the WMT12 and
GALE11 datasets, we experimented with both BB
and GB features. For each dataset we build four
systems:
? BL: 17 baseline features that performed well
across languages in previous work and were
used as baseline in the WMT12 QE task.
? AF: All features available for dataset.
? FS: Feature selection for automatic ranking
and selection of top features with:
? RL: Randomised Lasso.
? GP: Gaussian Process.
Mean Absolute Error (MAE) and Root Mean
Squared Error (RMSE) are used to evaluate the
models.
3.3 Results
The error scores for all datasets with BB features
are reported in Table 2, while Table 3 shows the re-
sults with GB features, and Table 4 the results with
BB and GB features together. For each table and
dataset, bold-faced figures are significantly better
than all others (paired t-test with p ? 0.05).
It can be seen from the results that adding more
BB features (systems AF) improves the results in
most cases as compared to the baseline systems
8More features resulted in further performance gains on
most tasks, with 25?35 features giving the best results.
Dataset System #feats. MAE RMSE
WMT12
BL 17 0.6802 0.8192
AF 80 0.6703 0.8373
FS(RL) 69 0.6628 0.8107
FS(GP) 17 0.6537 0.8014
EAMT11(en-es)
BL 17 0.4867 0.6288
AF 80 0.4696 0.5438
FS(RL) 29 0.4657 0.5424
FS(GP) 17 0.4640 0.5420
EAMT11(fr-en)
BL 17 0.4387 0.6357
AF 80 0.4275 0.6211
FS(RL) 65 0.4266 0.6196
FS(GP) 17 0.4240 0.6189
EAMT09-s1
BL 17 0.5294 0.6643
AF 80 0.5235 0.6558
FS(RL) 73 0.5190 0.6516
FS(GP) 17 0.5195 0.6511
EAMT09-s2
BL 17 0.4604 0.5856
AF 80 0.4734 0.5973
FS(RL) 59 0.4601 0.5837
FS(GP) 17 0.4610 0.5825
EAMT09-s3
BL 17 0.5321 0.6643
AF 80 0.5437 0.6827
FS(RL) 67 0.5338 0.6627
FS(GP) 17 0.5320 0.6630
EAMT09-s4
BL 17 0.3583 0.4953
AF 80 0.3569 0.5000
FS(RL) 40 0.3554 0.4995
FS(GP) 17 0.3560 0.4949
GALE11-s1
BL 17 0.5456 0.6905
AF 123 0.5359 0.6665
FS(RL) 56 0.5358 0.6649
FS(GP) 17 0.5410 0.6721
GALE11-s2
BL 17 0.5532 0.7177
AF 123 0.5381 0.6933
FS(RL) 54 0.5369 0.6955
FS(GP) 17 0.5424 0.6999
Table 2: Results with BB features.
Dataset System #feats. MAE RMSE
WMT12 AF 47 0.7036 0.8476FS(RL) 26 0.6821 0.8388
FS(GP) 17 0.6771 0.8308
GALE11-s1 AF 39 0.5720 0.7392FS(RL) 46 0.5691 0.7388
FS(GP) 17 0.5711 0.7378
GALE11-s2
AF 48 0.5510 0.6977
FS(RL) 46 0.5512 0.6970
FS(GP) 17 0.5501 0.6978
Table 3: Results with GB features.
Dataset System #feats. MAE RMSE
WMT12 AF 127 0.7165 0.8476FS(RL) 26 0.6601 0.8098
FS(GP) 17 0.6501 0.7989
GALE11-s1 AF 162 0.5437 0.6741FS(RL) 69 0.5310 0.6681
FS(GP) 17 0.5370 0.6701
GALE11-s2
AF 171 0.5222 0.6499
FS(RL) 82 0.5152 0.6421
FS(GP) 17 0.5121 0.6384
Table 4: Results with BB and GB features.
83
BL, however, in some cases the improvements are
not significant. This behaviour is to be expected
as adding more features may bring more relevant
information, but at the same time it makes the rep-
resentation more sparse and the learning prone to
overfitting. In most cases, feature selection with
both or either RL and GP improves over all fea-
tures (AF). It should be noted that RL automati-
cally selects the number of features used for train-
ing while FS(GP) was limited to selecting the top
17 features in order to make the results compara-
ble with our baseline feature set. It is interesting
to note that system FS(GP) outperformed the other
systems in spite of using fewer features. This tech-
nique is promising as it reduces the time require-
ments and overall computational complexity for
training the model, while achieving similar results
compared to systems with many more features.
Another interesting question is whether these
feature selection techniques identify a common
subset of features from the various datasets. The
overall top ranked features are:
? LM perplexities and log probabilities for
source and target;
? size of source and target sentences;
? average number of possible translations of
source words (IBM 1 with thresholds);
? ratio of target by source lengths in words;
? percentage of numbers in the target sentence;
? percentage of distinct unigrams seen in the
MT source training corpus.
Interestingly, not all top ranked features are
among the baseline 17 features which are report-
edly best in literature.
GB features on their own perform worse than
BB features, but in all three datasets, the combi-
nation of GB and BB followed by feature selec-
tion resulted in significantly lower errors than us-
ing only BB features with feature selection, show-
ing that the two features sets are complementary.
4 Remarks
The source code for the framework, the datasets
and extra resources can be downloaded from
http://www.quest.dcs.shef.ac.uk/.
The project is also set to receive contribution from
interested researchers using a GitHub repository:
https://github.com/lspecia/quest.
The license for the Java code, Python and shell
scripts is BSD, a permissive license with no re-
strictions on the use or extensions of the software
for any purposes, including commercial. For pre-
existing code and resources, e.g., scikit-learn, GPy
and Berkeley parser, their licenses apply, but fea-
tures relying on these resources can be easily dis-
carded if necessary.
Acknowledgments
This work was supported by the QuEst (EU
FP7 PASCAL2 NoE, Harvest program) and QT-
LaunchPad (EU FP7 CSA No. 296347) projects.
References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: a method for measuring machine translation
confidence. In ACL11, pages 211?219, Portland.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence Estimation for Machine Transla-
tion. In Coling04, pages 315?321, Geneva.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 workshop on statistical machine translation. In
WMT12, pages 10?51, Montre?al.
Y. He, Y. Ma, J. van Genabith, and A. Way. 2010.
Bridging SMT and TM with Translation Recom-
mendation. In ACL10, pages 622?630, Uppsala.
N. Meinshausen and P. Bu?hlmann. 2010. Stability se-
lection. Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology), 72:417?473.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL02, pages 311?318,
Philadelphia.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaus-
sian processes for machine learning, volume 1. MIT
Press, Cambridge.
R. Soricut and A. Echihabi. 2010. Trustrank: Induc-
ing trust in automatic translations via ranking. In
ACL11, pages 612?621, Uppsala.
L. Specia, M. Turchi, N. Cancedda, M. Dymetman,
and N. Cristianini. 2009. Estimating the Sentence-
Level Quality of Machine Translation Systems. In
EAMT09, pages 28?37, Barcelona.
L. Specia, D. Raj, and M. Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, 24(1):39?50.
L. Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In
EAMT11, pages 73?80, Leuven.
84
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392?399,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Translation Model Adaptation by Resampling
Kashif Shah, Lo??c Barrault, Holger Schwenk
LIUM, University of Le Mans
Le Mans, France.
FirstName.LastName@lium.univ-lemans.fr
Abstract
The translation model of statistical ma-
chine translation systems is trained on par-
allel data coming from various sources and
domains. These corpora are usually con-
catenated, word alignments are calculated
and phrases are extracted. This means
that the corpora are not weighted accord-
ing to their importance to the domain of
the translation task. This is in contrast
to the training of the language model for
which well known techniques are used to
weight the various sources of texts. On
a smaller granularity, the automatic cal-
culated word alignments differ in quality.
This is usually not considered when ex-
tracting phrases either.
In this paper we propose a method to auto-
matically weight the different corpora and
alignments. This is achieved with a resam-
pling technique. We report experimen-
tal results for a small (IWSLT) and large
(NIST) Arabic/English translation tasks.
In both cases, significant improvements in
the BLEU score were observed.
1 Introduction
Two types of resources are needed to train statis-
tical machine translation (SMT) systems: parallel
corpora to train the translation model and mono-
lingual texts in the target language to build the
language model. The performance of both mod-
els depends of course on the quality and quantity
of the available resources.
Today, most SMT systems are generic, i.e. the
same system is used to translate texts of all kinds.
Therefore, it is the domain of the training re-
sources that influences the translations that are se-
lected among several choices. While monolingual
texts are in general easily available in many do-
mains, the freely available parallel texts mainly
come from international organisations, like the
European Union or the United Nations. These
texts, written in particular jargon, are usually
much larger than in-domain bitexts. As an exam-
ple we can cite the development of an NIST Ara-
bic/English phrase-based translation system. The
current NIST test sets are composed of a news
wire part and a second part of web-style texts.
For both domains, there is only a small number
of in-domain bitexts available, in comparison to
almost 200 millions words of out-of-domain UN
texts. The later corpus is therefore likely to domi-
nate the estimation of the probability distributions
of the translation model.
It is common practice to use a mixture language
model with coefficients that are optimized on the
development data, i.e. by these means on the do-
main of the translation task. Domain adaptation
seems to be more tricky for the translation model
and it seems that very little research has been done
that seeks to apply similar ideas to the translation
model. To the best of our knowledge, there is no
commonly accepted method to weight the bitexts
coming from different sources so that the transla-
tion model is best optimized to the domain of the
task. Mixture models are possible when only two
different bitexts are available, but are rarely used
for more corpora (see discussion in the next sec-
tion).
In this work we propose a new method to adapt
the translation model of an SMT system. We only
perform experiments with phrase-based systems,
but the method is generic and could be easily ap-
plied to an hierarchical or syntax-based system.
We first associate a weighting coefficient to each
bitext. The main idea is to use resampling to pro-
duce a new collection of weighted alignment files,
followed by the standard procedure to extract the
phrases. In a second step, we also consider the
392
alignment score of each parallel sentence pair, em-
phasizing by these means good alignments and
down-weighting less reliable ones. All the param-
eters of our procedure are automatically tuned by
optimizing the BLEU score on the development
data.
The paper is organized as follows. The next
section describes related work on weighting the
corpora and model adaptation. Section 3 de-
scribes the architecture allowing to resample and
to weight the bitexts. Experimental results are pre-
sented in section 4 and the paper concludes with a
discussion.
2 Related Work
Adaptation of SMT systems is a topic of in-
creasing interest since few years. In previous
work, adaptation is done by using mixture mod-
els, by exploiting comparable corpora and by self-
enhancement of translation models.
Mixture models were used to optimize the co-
efficients to the adaptation domain. (Civera and
Juan, 2007) proposed a model that can be used
to generate topic-dependent alignments by exten-
sion of the HMM alignment model and derivation
of Viterbi alignments. (Zhao et al, 2004) con-
structed specific language models by using ma-
chine translation output as queries to extract sim-
ilar sentences from large monolingual corpora.
(Foster and Kuhn, 2007) applied a mixture model
approach to adapt the system to a new domain by
using weights that depend on text distances to mix-
ture components. The training corpus was divided
into different components, a model was trained on
each part and then weighted appropriately for the
given context. (Koehn and Schroeder, 2007) used
two language models and two translation models:
one in-domain and other out-of-domain to adapt
the system. Two decoding paths were used to
translate the text.
Comparable corpora are exploited to find addi-
tional parallel texts. Information retrieval tech-
niques are used to identify candidate sentences
(Hildebrand et al, 2005). (Snover et al, 2008)
used cross-lingual information retrieval to find
texts in the target language that are related to the
domain of the source texts.
A self-enhancing approach was applied by
(Ueffing, 2006) to filter the translations of the
test set with the help of a confidence score and
to use reliable alignments to train an additional
phrase table. This additional table was used with
the existing generic phrase table. (Ueffing, 2007)
further refined this approach by using transduc-
tive semi-supervised methods for effective use of
monolingual data from the source text. (Chen et
al., 2008) performed domain adaptation simulta-
neously for the translation, language and reorder-
ing model by learning posterior knowledge from
N-best hypothesis. A related approach was in-
vestigated in (Schwenk, 2008) and (Schwenk and
Senellart, 2009) in which lightly supervised train-
ing was used. An SMT system was used to trans-
late large collections of monolingual texts, which
were then filtered and added to the training data.
(Matsoukas et al, 2009) propose to weight each
sentence in the training bitext by optimizing a dis-
criminative function on a given tuning set. Sen-
tence level features were extracted to estimate the
weights that are relevant to the given task. Then
certain parts of the training bitexts were down-
weighted to optimize an objective function on the
development data. This can lead to parameter
over-fitting if the function that maps sentence fea-
tures to weights is complex.
The technique proposed in this paper is some-
how related to the above approach of weighting
the texts. Our method does not require an ex-
plicit specification of the in-domain and out-of-
domain training data. The weights of the corpora
are directly optimized on the development data us-
ing a numerical method, similar to the techniques
used in the standard minimum error training of the
weights of the feature functions in the log-linear
criterion. All the alignments of the bitexts are re-
sampled and given equal chance to be selected and
therefore, influence the translation model in a dif-
ferent way. Our proposed technique does not re-
quire the calculation of extra sentence level fea-
tures, however, it may use the alignments score as-
sociated with each aligned sentence pair as a con-
fidence score.
3 Description of the algorithm
The architecture of the algorithm is summarized in
figure 1. The starting point is an (arbitrary) num-
ber of parallel corpora. We first concatenate these
bitexts and perform word alignments in both direc-
tions using GIZA++. This is done on the concate-
nated bitexts since GIZA++ may perform badly
if some of the individual bitexts are rather small.
Next, the alignments are separated in parts corre-
393
Figure 1: Architecture of SMT Weighting System
sponding to the individual bitexts and a weighting
coefficient is associated to each one. We are not
aware of a procedure to calculate these coefficients
in an easy and fast way without building an actual
SMT system. Note that there is an EM procedure
to do this for language modeling.
In the next section, we will experimentally com-
pare equal coefficients, coefficients set to the same
values than those obtained when building an inter-
polated language model on the source language,
and a new method to determine the coefficients by
optimizing the BLEU score on the development
data.
One could imagine to directly use these coef-
ficients when calculating the various probabilities
of the extracted phrases. In this work, we propose
a different procedure that makes no assumptions
on how the phrases are extracted and probabilities
are calculated. The idea is to resample alignments
from the alignment file corresponding to the indi-
vidual bitexts according to their weighting coeffi-
cients. By these means, we create a new, poten-
tially larger alignment file, which then in turn will
be used by the standard phrase extraction proce-
dure.
3.1 Resampling the alignments
In statistics, resampling is based upon repeated
sampling within the same sample until a sample
is obtained which better represents a given data
set (Yu, 2003). Resampling is used for validating
models on given data set by using random subsets.
It overcomes the limitations to make assumptions
about the distribution of the data. Usually resam-
pling is done several times to better estimate and
select the samples which better represents the tar-
get data set. The more often we resample, the
closer we get to the true probability distribution.
In our case we performed resampling with re-
placement according to the following algorithm:
Algorithm 1 Resampling
1: for i = 0 to required size do
2: Select any alignment randomly
3: Alscore ? normalized alignment score
4: Threshold? rand[0, 1]
5: if Alscore > Threshold then
6: keep it
7: end if
8: end for
Let us call resampling factor, the number of
times resampling should be done. An interesting
question is to determine the optimal value of this
resampling factor.
It actually depends upon the task or data we are
experimenting on. We may start with one time
resampling and could stop when results becomes
stable. Figure 2 plots a typical curve of the BLEU
score as a function of the number of times we re-
sample. It can be observed that the curve is grow-
ing proportionally to the resampling factor until it
becomes stable after a certain point.
3.2 Weighting Schemes
We concentrated on translation model adaptation
when the bitexts are heterogeneous, e.g. in-
domain and out-of-domain or of different sizes. In
this case, weighting these bitexts seems interest-
ing and can be used in order to select data which
better represent the target domain. Secondly when
sentences are aligned, some alignments are reli-
able and some are less. Using unreliable align-
ments can put negative effect on the translation
quality. So we need to exclude or down-weight
394
 52
 52.5
 53
 53.5
 54
 54.5
 55
 55.5
 56
 0  5  10  15  20
BL
EU
Resampling factor
dev
test
baseline(test)
Figure 2: The curve shows that by increasing the
resampling factor we get better and stable results
on Dev and Test.
unreliable alignments and keep or up-weight the
good ones. We conceptually divided the weight-
ing in two parts that is (i) weighting the corpora
and (ii) weighting the alignments
3.2.1 Weighting Corpora
We started to resample the bitexts with equal
weights to see the effect of resampling. This gives
equal importance to each bitext without taking into
account the domain of the text to be translated.
However, it should be better to give appropriate
weights according to a given domain as shown in
equation 1
?1bitext1 + ?2bitext2 + ..+ ?nbitextn (1)
where the ?n are the coefficients to optimize.
One important question is how to find out the ap-
propriate coefficient for each corpus. We investi-
gated a technique similar to the algorithm used to
minimize the perplexity of an interpolated target
LM. Alternatively, it is also possible to construct a
interpolated language model on the source side of
bitexts. This approach was implemented and these
coefficients were used as the weights for each bi-
text. One can certainly ask the question whether
the perplexity is a good criterion for weighting bi-
texts. Therefore, we worked on direct optimiza-
tion of these coefficients by CONDOR (Berghen
and Bersini, 2005). This freely available tool is a
numerical optimizer based on Powell?s UOBYQA
algorithm (Powell, 1994). The aim of CONDOR
is to minimize a objective function using the least
number of function evaluations. Formally, it is
used to find x? ? Rn with given constraints which
satisfies
F (x?) = min
x
F (x) (2)
where n is the dimension of search space and x?
is the optimum of x. The following algorithm was
used to weight the bitexts.
Algorithm 2 WeightingCorpora
1: Determine word to word alignment with
GIZA++ on concatenated bitext.
2: while Not converged do
3: Run Condor initialized with LM weights.
4: Create new alignment file by resampling
according to weights given by Condor.
5: Use the alignment file to extract phrases
and build the translation table (phrase table)
6: Tune the system with MERT (this step can
be skipped until weights are optimized to
save time)
7: Calculate the BLEU score
8: end while
3.2.2 Weighting Alignments
Alignments produced by GIZA++ have alignment
scores associated with each sentence pair in both
direction, i.e. source to target and target to source.
We used these alignment scores as confidence
measurement for each sentence pair. Alignment
scores depend upon the length of each sentence,
therefore, they must be normalized regarding the
size of the sentence. Alignment scores have a very
large dynamic range and we have applied a loga-
rithmic mapping in order to flatten the probability
distribution :
log(? ?
( ntrg
?
asrc trg + nsrc
?
atrg src)
2
) (3)
where a is the alignment score, n the size of a
sentence and ? a coefficient to optimize. This is
also done by Condor.
Of course, some alignments will appear several
times, but this will increase the probability of cer-
tain phrase-pairs which are supposed to be more
related to the target domain. We have observed
that the weights of an interpolated LM build on
the source side of the bitext are good initial val-
ues for CONDOR. Moreover, weights optimized
by Condor are in the same order than these ?LM
weights?. Therefore, we do not perform MERT
of the SMT systems build at each step of the op-
timization of the weights ?i and ? by CONDOR,
395
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) Dev (NIST06) Test (NIST08)
Baseline 53.98 53.37 43.16 42.21
With equal weights 53.71 53.20 43.10 42.11
With LM weights 54.20 53.71 43.42 42.22
Condor weights 54.80 53.98 43.49 42.28
Table 1: BLEU scores when weighting corpora (one time resampling)
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) Dev (NIST06) Test (NIST08)
Baseline 53.98 53.37 43.16 42.21
With equal weights 53.80 53.30 43.13 42.15
With LM weights 54.32 53.91 43.54 42.37
Condor weights 55.10 54.13 43.80 42.40
Table 2: BLEU scores when weighting corpora (optimum number of resampling)
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) TER(Test) Dev (NIST06) Test (NIST08) TER(Test)
Baseline 53.98 53.37 32.75 43.16 42.21 51.69
With equal weights 53.85 53.33 32.80 43.28 42.21 51.72
With LM weights 54.80 54.10 31.50 43.42 42.41 51.50
Condor weights 55.48 54.58 31.31 43.95 42.54 51.35
Table 3: BLEU and TER scores when weighting corpora and alignments (optimum number of resam-
pling)
but use the values obtained by running MERT on
a system obtained by using the ?LM weights? to
weight the alignments. Once CONDOR has con-
verged to optimal weights, we can then tune our
system by MERT. This saves lot of time taken by
the tuning process and it had no impact on the re-
sults.
4 Experimental evaluation
The baseline system is a standard phrase-based
SMT system based on the Moses SMT toolkit
(Koehn and et al, 2007). In our system we
used fourteen features functions. These features
functions include phrase and lexical translation
probabilities in both directions, seven features for
lexicalized distortion model, a word and phrase
penalty, and a target language model. The MERT
tool is used to tune the coefficients of these fea-
ture functions. We considered Arabic to English
translation. Tokenization of the Arabic source
texts is done by a tool provided by SYSTRAN
which also performs a morphological decompo-
sition. We considered two well known official
evaluation tasks to evaluate our approach, namely
NIST and IWSLT.
For IWSLT, we used the BTEC bitexts (194M
words), Dev1, Dev2, Dev3 (60M words each) as
training data, Dev6 as development set and Dev7
as test set. From previous experiments, we have
evidence that the various development corpora are
not equally important and weighting them cor-
rectly should improve the SMT system. We an-
alyze the translation quality as measured by the
BLEU score for the three methods: equal weights,
LM weights and Condor weights and considering
one time resampling. Further experiments were
performed using the optimized number of resam-
pling with and without weighting the alignments.
We have realized that it is beneficial to always in-
clude the original alignments. Even if we resample
many times there is a chance that some alignments
might never be selected but we do not want to
loose any information. By keeping original align-
ments, all alignments are given a chance to be se-
396
lected at least once. All these results are summa-
rized in tables 1, 2 and 3.
One time resampling along with equal weights
gave worse results than the baseline system while
improvements in the BLEU score were observed
with LM and Condor weights for the IWSLT task,
as shown in table 1. Resampling many times al-
ways gave more stable results, as already shown
in figure 2 and as theoretically expected. For this
task, we resampled 15 times. The improvements
in the BLEU score are shown in table 2. Fur-
thermore, using the alignment scores resulted in
additional improvements in the BLEU score. For
the IWSLT task, we achieved and overall improve-
ment of 1.5 BLEU points on the development set
and 1.2 BLEU points on the test set as shown in
table 3
To validate our approach we further experi-
mented with the NIST evaluation task. Most of
the training data used in our experiments for the
NIST task is made available through the LDC. The
bitexts consist of texts from the GALE project1
(1.6M words), various news wire translations2
(8.0M words) on development data from pre-
vious years (1.6M words), LDC treebank data
(0.4M words) and the ISI extracted bitexts (43.7M
words). The official NIST06 evaluation data was
used as development set and the NIST08 evalua-
tion data was used as test set. The same procedure
was adapted for the NIST task as for the IWSLT
task. Results are shown in table 1 by using differ-
ent weights and one time resampling. Further im-
provements in the results are shown in table 2 with
the optimum number of resampling which is 10
for this task. Finally, results by weighting align-
ments along with weighting corpora are shown in
table 3. Our final system achieved an improve-
ment of 0.79 BLEU points on the development set
and 0.33 BLEU points on the test set. TER scores
are also shown on test set of our final system in
table 3. Note that these results are state-of-the-art
when compared to the official results of the 2008
NIST evaluation3.
The weights of the different corpora are shown
in table 4 for the IWSLT and NIST task. In both
cases, the weights optimized by CONDOR are
substantially different form those obtained when
1LDC2005E83, 2006E24, E34, E85 and E92
2LDC2003T07, 2004E72, T17, T18, 2005E46 and
2006E25.
3http://www.nist.gov/speech/tests/mt/
2008/
creating an interpolated LM on the source side of
the bitexts. In any case, the weights are clearly
non uniform, showing that our algorithm has fo-
cused on in-domain data. This can be nicely seen
for the NIST task. The Gale texts were explictely
created to contain in-domain news wire and WEB
texts and actually get a high weight despite their
small size, in comparison to the more general news
wire collection from LDC.
5 Conclusion and future work
We have proposed a new technique to adapt the
translation model by resampling the alignments,
giving a weight to each corpus and using the
alignment score as confidence measurement of
each aligned phrase pair. Our technique does not
change the phrase pairs that are extracted,4 but
only the corresponding probability distributions.
By these means we hope to adapt the translation
model in order to increase the weight of transla-
tions that are important to the task, and to down-
weight the phrase pairs which result from unreli-
able alignments.
We experimentally verified the new method on
the low-resource IWSLT and the resource-rich
NIST?08 tasks. We observed significant improve-
ment on both tasks over state-of-the-art baseline
systems. This weighting scheme is generic and
it can be applied to any language pair and target
domain. We made no assumptions on how the
phrases are extracted and it should be possible to
apply the same technique to other SMT systems
which rely on word-to-word alignments.
On the other hand, our method is computation-
ally expensive since the optimisation of the coef-
ficients requires the creation of a new phrase table
and the evaluation of the resulting system in the
tuning loop. Note however, that we run GIZA++
only once.
In future work, we will try to directly use the
weights of the corpora and the alignments in the
algorithm that extracts the phrase pairs and cal-
culates their probabilities. This would answer
the interesting question whether resampling itself
is needed or whether weighting the corpora and
alignments is the key to the observed improve-
ments in the BLEU score.
Finally, it is straight forward to consider more
feature functions when resampling the alignments.
This may be a way to integrate linguistic knowl-
4when also including the original alignments
397
IWSLT Task BTEC Dev1 Dev2 Dev3
# of Words 194K 60K 60K 60K
LM Coeffs 0.7233 0.1030 0.0743 0.0994
Condor Coeffs 0.6572 0.1058 0.1118 0.1253
NIST TASK Gale NewsWire TreeBank Dev ISI
# of words 1.6M 8.1M 0.4M 1.7M 43.7M
LM Coeffs 0.3215 0.1634 0.0323 0.1102 0.3726
Condor Coeffs 0.4278 0.1053 0.0489 0.1763 0.2417
Table 4: Weights of the different bitexts.
edge into the SMT system, e.g. giving low scores
to word alignments that are ?grammatically not
reasonable?.
Acknowledgments
This work has been partially funded by the Eu-
ropean Commission under the project Euromatrix
and by the Higher Education Commission(HEC)
Pakistan as Overseas scholarship. We are very
thankful to SYSTRAN who provided support for
the Arabic tokenization.
References
Frank Vanden Berghen and Hugues Bersini.
2005. CONDOR, a new parallel, constrained
extension of Powell?s UOBYQA algorithm:
Experimental results and comparison with the
DFO algorithm. Journal of Computational and
Applied Mathematics, 181:157?175, Septem-
ber.
Boxing Chen, Min Zhang, Aiti Aw, and
Haizhou Li. 2008. Exploiting n-best hypothe-
ses for SMT self- enhancement. In Association
for Computational Linguistics, pages 157?160.
Jorge Civera and Alfons Juan. 2007. Do-
main adaptation in statistical machine transla-
tion with mixture modelling. In Second Work-
shop on SMT, pages 177?180.
George Foster and Roland Kuhn. 2007.
Mixture-model adaptation for SMT. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, pages 128?135. Associa-
tion for Computational Linguistics.
Almut Silja Hildebrand, Matthias Eck, Stephan
Vogel, and Alex Waibel. 2005. Adaptation
of the translation model for statistical machine
translation based on information retrieval. In
EAMT, pages 133?142.
Philipp Koehn and et al 2007. Moses: Open
source toolkit for statistical machine transla-
tion. In Association for Computational Linguis-
tics, demonstration session., pages 224?227.
Philipp Koehn and Josh Schroeder. 2007. Ex-
periments in domain adaptation for statistical
machine translation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 224?227. Association for Computa-
tional Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and
Bing Zhang. 2009. Discriminative corpus
weight estimation for machine translation. In
Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing,
pages 708?717.
M.J.D. Powell. 1994. A direct search opti-
mization method that models the objective and
constraint functions by linar interpolation. In
In Advances in Optimization and Numerical
Analysis, Proceedings of the sixth Workshop
on Optimization and Numerical Analysis, Oax-
aca, Mexico, volume 275, pages 51?67. Kluwer
Academic Publishers.
Holger Schwenk and Jean Senellart. 2009.
Translation model adaptation for an Ara-
bic/French news translation system by lightly-
supervised training. In MT Summit.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical
machine translation. In IWSLT, pages 182?189.
Matthew Snover, Bonnie Dorr, and Richard
Schwartz. 2008. Language and translation
398
model adaptation using comparalble corpora.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 857?866.
Nicola Ueffing. 2006. Using monolingual sour-
cce language data to improve MT performance.
In IWSLT, pages 174?181.
Nicola Ueffing. 2007. Transductive learning for
statistical machine translation. In Association
for Computational Linguistics, pages 25?32.
Chong Ho Yu. 2003. Resampling methods:
Concepts, applications, and justification. In
Practical Assessment Research and Evaluation.
Bing Zhao, Matthias Ech, and Stephen Vogal.
2004. Language model adaptation for statistical
machine translation with structured query mod-
els. In Proceedings of the 20th international
conference on Computational Linguistics. As-
sociation for Computational Linguistics.
399
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 464?469,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2011
WMT shared task evaluation. Our main sys-
tems were standard phrase-based statistical
systems based on the Moses decoder, trained
on the provided data only, but we also per-
formed initial experiments with hierarchical
systems. Additional, new features this year in-
clude improved translation model adaptation
using monolingual data, a continuous space
language model and the treatment of unknown
words.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2011 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Lambert et
al., 2010) are as follows: use of more training data
as provided by the organizers, improved translation
model adaptation by unsupervised training, a con-
tinuous space language model for the translation
into French, some attempts to automatically induce
translations of unknown words and first experiments
with hierarchical systems. These different points are
described in the rest of the paper, together with a
summary of the experimental results showing the
impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations were then used
directly with the source texts to create additional bi-
texts. In a second stage, these additional bilingual
data were incorporated into the system (see Sec-
tion 5 and Tables 4 and 5).
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
6) were used. We also took as training data a sub-
set of the French?English Gigaword (109) corpus.
We applied the same filters as last year to select this
subset. The first one is a lexical filter based on the
IBM model 1 cost (Brown et al, 1993) of each side
of a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter was
trained on a corpus composed of Eparl, NC, and UN
data. The other filter is an n-gram language model
(LM) cost of the target sentence (see Section 3), nor-
malised with respect to its length. This filter was
trained with all monolingual resources available ex-
cept the 109 data. We generated two subsets, both
by selecting sentence pairs with a lexical cost infe-
rior to 4, and an LM cost respectively inferior to 2.3
(1091, 115 million English words) and 2.6 (10
9
2, 232
million English words).
464
2.2 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the task do-
main.
First, we generated automatic translations of the
provided monolingual News corpus and selected the
sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitext contain no new translations, since
all words of the translation output come from the
translation model, but it contains new combinations
(phrases) of known words, and reinforces the prob-
ability of some phrase pairs (Schwenk, 2008). This
year, we improved this method in the following way.
In the original approach, the automatic translations
are added to the human translated bitexts and a com-
plete new system is build, including time consuming
word alignment with GIZA++. For WMT?11, we
directly used the word-to-word alignments produced
by the decoder at the output instead of GIZA?s align-
ments. This speeds-up the procedure and yields the
same results in our experiments. A detailed compar-
ison is given in (Lambert et al, 2011).
Second, as in last year?s evaluation, we automat-
ically extracted and aligned parallel sentences from
comparable in-domain corpora. We used the AFP
and APW news texts since there are available in the
French and English LDC Gigaword corpora. The
general architecture of our parallel sentence extrac-
tion system is described in detail by Abdul-Rauf and
Schwenk (2009). We first translated 91M words
from French into English using our first stage SMT
system. These English sentences were then used to
search for translations in the English AFP and APW
texts of the Gigaword corpus using information re-
trieval techniques. The Lemur toolkit (Ogilvie and
Callan, 2001) was used for this purpose. Search
was limited to a window of ?5 days of the date of
the French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER below
75% were kept. Sentences with a large length differ-
ence (French versus English) or containing a large
fraction of numbers were also discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the develop-
ment and test periods were removed from the Giga-
word collections.
2.4 Development data
All development was done on newstest2009, and
newstest2010 was used as internal test set. The de-
fault Moses tokenization was used. However, we
added abbreviations for the French tokenizer. All
our models are case sensitive and include punctua-
tion. The BLEU scores reported in this paper were
calculated with the tool multi-bleu.perl and are case
sensitive.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Our main system is a phrase-based system
(Koehn et al, 2003; Och and Ney, 2003), but we
have also performed some experiments with a hier-
archical system (Chiang, 2007). Both use a log lin-
ear framework in order to introduce several models
explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och
and Ney, 2002). The phrase-based system uses four-
teen features functions, namely phrase and lexical
translation probabilities in both directions, seven
features for the lexicalized distortion model, a word
and a phrase penalty and a target language model
(LM). The hierarchical system uses only 8 features:
a LM weight, a word penalty and six weights for the
translation model.
Both systems are based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
465
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases, lexical reorderings or hierarchical rules
are extracted using the default settings of the Moses
toolkit. The parameters of Moses were tuned on
newstest2009, using the ?new? MERT tool. We re-
peated the training process three times, each with a
different seed value for the optimisation algorithm.
In this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build on
each data source with the SRI LM toolkit (Stolcke,
2002) and then linearly interpolated, optimizing the
coefficients with an EM procedure. The perplexities
of these LMs were 99.4 for French and 129.7 for
English. In addition, we build a 5-gram continuous
space language model for French (Schwenk, 2007).
This model was trained on all the available French
texts using a resampling technique. The continu-
ous space language model is interpolated with the
4-gram back-off model and used to rescore n-best
lists. This reduces the perplexity by about 8% rela-
tive.
4 Treatment of unknown words
Finally, we propose a method to actually add new
translations to the system inspired from (Habash,
2008). For this, we propose to identity unknown
words and propose possible translations.
Moses has two options when encountering an un-
known word in the source language: keep it as it is
or drop it. The first option may be a good choice
for languages that use the same writing system since
the unknown word may be a proper name. The sec-
ond option is usually used when translating between
language based on different scripts, e.g. translating
1The source is available at http://www.cs.cmu.edu/
?qing/
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
from Arabic to English. Alternatively, we propose to
infer automatically possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case, we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
target segment works are finis
stemmed word found fini
translations found finished, ended
segment proposed works are finished
works are ended
segment kept works are finished
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
comparing the source and the target segment in order
to detect identical words. Once the unknown word
is selected, we are looking for its stemmed form in
the dictionary and propose some translations for the
unknown word based on lexical score of the phrase
table (see Table 2 for some examples). The snowball
466
Bitext #Fr Words PT size newstest2009 newstest2010
(M) (M) BLEU BLEU TER METEOR
Eparl+NC 56 7.1 26.74 27.36 (0.19) 55.11 (0.14) 60.13 (0.05)
Eparl+NC+1091 186 16.3 27.96 28.20 (0.04) 54.46 (0.10) 60.88 (0.05)
Eparl+NC+1092 323 25.4 28.20 28.57 (0.10) 54.12 (0.13) 61.20 (0.05)
Eparl+NC+news 140 8.4 27.31 28.41 (0.13) 54.15 (0.14) 61.13 (0.04)
Eparl+NC+1092+news 406 25.5 27.93 28.70 (0.24) 54.12 (0.16) 61.30 (0.20)
Eparl+NC+1092+IR 351 25.3 28.07 28.51 (0.18) 54.07 (0.06) 61.18 (0.07)
Eparl+NC+1092+news+IR 435 26.1 27.99 28.93 (0.02) 53.84 (0.07) 61.46 (0.07)
+larger beam+pruned PT 435 8.2 28.44 29.05 (0.14) 53.74 (0.16) 61.68 (0.09)
Table 4: French?English results: number of French words (in million), number of entries in the filtered phrase-table
(in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different
systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3
values (see Section 3)
corpus newstest2010 subtest2010
number of sentences 2489 109
number of words 70522 3586
number of UNK detected 118 118
nbr of sentences containing UNK 109 109
BLEU Score without UNK process 29.43 24.31
BLEU Score with UNK process 29.43 24.33
TER Score without UNK process 53.08 58.54
TER Score with UNK process 53.08 58.59
Table 3: Statistics of the unknown word (UNK) process-
ing algorithm on our internal test (newstest2010) and its
sub-part containing only the processed sentences (sub-
test2010).
stemmer2 was used. Then the different hypothesis
are evaluated with the target language model.
We processed the produced translations with this
method. It can happen that some words are transla-
tions of themselves, e.g. the French word ?duel? can
be translated by the English word ?duel?. If theses
words are present into the extracted dictionary, we
keep them. If we do not find any translation in our
dictionary, we keep the translation. By these means
we hope to keep named entities.
Several statistics made on our internal test (new-
stest2010) are shown in Table 3. Its shows that the
influence of the detected unknown words is minimal.
Only 0.16% of the words in the corpus are actually
unknown. However, the main goal of this process
is to increase the human readability and usefulness
without degrading automatic metrics. We also ex-
pect a larger impact in other tasks for which we have
2http://snowball.tartarus.org/
smaller amounts of parallel training data. In future
versions of this detection process, we will try to de-
tect unknown words before the translation process
and propose alternatives hypothesis to the Moses de-
coder.
5 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 4 and 5, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between two
average scores is less than the sum of the standard
deviations, we can say that this difference is not sig-
nificant. The reverse is not true. Note that most of
the improvements shown in the tables are small and
not significant. However many of the gains are cu-
mulative and the sum of several small gains makes a
significant difference.
Baseline French?English System
The first section of Table 4 shows results of the de-
velopment of the baseline SMT system, used to gen-
erate automatic translations.
Although no French translations were generated,
we did similar experiments in the English?French
direction (first section of Table 5).
467
Bitext #En Words newstest2009 newstest2010
(M) BLEU BLEU TER
Eparl+NC 52 26.20 28.06 (0.22) 56.85 (0.08)
Eparl+NC+1091 167 26.84 29.08 (0.12) 55.83 (0.14)
Eparl+NC+1092 284 26.95 29.29 (0.03) 55.77 (0.19)
Eparl+NC+1092+news 299 27.34 29.56 (0.14) 55.44 (0.18)
Eparl+NC+1092+IR 311 27.14 29.43 (0.12) 55.48 (0.06)
Eparl+NC+1092+news+IR 371 27.32 29.73 (0.21) 55.16 (0.20)
+rescoring with CSLM 371 27.46 30.04 54.79
Table 5: English?French results: number of English words (in million) and BLEU scores in the development (new-
stest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number
in parentheses are the average and standard deviation over 3 values (see Section 3.)
In both cases the best system is the one trained
on the Europarl, News-commentary and 1092 cor-
pora. This system was used to generate the auto-
matic translations. We did not observe any gain
when adding the United Nations data, so we dis-
carded this data.
Impact of the Additional Bitexts
With the baseline French?English SMT system (see
above), we translated the French News corpus to
generate an additional bitext (News). We also trans-
lated some parts of the French LDC Gigaword cor-
pus, to serve as queries to our IR system (see section
2.2). The resulting additional bitext is referred to as
IR. The second section of Tables 4 and 5 summarize
the system development including the additional bi-
texts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate the
automatic translations, but with less than half of
the data. Adding the News corpus to a larger cor-
pus, such as Eparl+NC+1092, has less impact but
still yields some improvement: 0.1 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. This
effect is studied in detail in a separate paper (Lam-
bert et al, 2011). With the IR additional bitext added
to Eparl+NC+1092, we observe no improvement in
French to English, and a very small improvement
in English to French. However, added to the base-
line system (Eparl+NC+1092) adapted with the News
data, the IR additional bitexts yield a small (0.2
BLEU) improvement in both translation directions.
Final System
In both translation directions our best system was the
one trained on Eparl+NC+1092+News+IR. We fur-
ther achieved small improvements by pruning the
phrase-table and by increasing the beam size. To
prune the phrase-table, we used the ?sigtest-filter?
available in Moses (Johnson et al, 2007), more pre-
cisely the ??  filter3.
We also build hierarchical systems on the various
human translated corpora, using up to 323M words
(corpora Eparl+NC+1092). The systems yielded sim-
ilar results than the phrase-based approach, but re-
quired much more computational resources, in par-
ticular large amounts of main memory to perform
the translations. Running the decoder was actually
only possible with binarized rule-tables. Therefore,
the hierarchical system was not used in the evalua-
tion system.
3The p-value of two-by-two contingency tables (describing
the degree of association between a source and a target phrase)
is calculated with Fisher exact test. This probability is inter-
preted as the probability of observing by chance an association
that is at least as strong as the given one, and hence as its sig-
nificance. An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and each of the
component phrases occurs exactly once in its side of the paral-
lel corpus (1-1-1 phrase pairs). In this case the negative log of
the p-value is ? = logN (N is number of sentence pairs in the
corpus). ? ?  is the largest threshold that results in all of the
1-1-1 phrase pairs being included.
468
6 Conclusions and Further Work
We presented the development of our statistical ma-
chine translation systems for the French?English
and English?French 2011 WMT shared task. In the
official evaluation the English?French system was
ranked first according to the BLEU score and the
French?English system second.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 121?126, Uppsala, Sweden, July.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
model adaptation using monolingual data. In Sixth
Workshop on SMT, page this volume.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
469
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
SHEF-Lite: When Less is More for Translation Quality Estimation
Daniel Beck and Kashif Shah and Trevor Cohn and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.uk
Abstract
We describe the results of our submissions
to the WMT13 Shared Task on Quality
Estimation (subtasks 1.1 and 1.3). Our
submissions use the framework of Gaus-
sian Processes to investigate lightweight
approaches for this problem. We focus on
two approaches, one based on feature se-
lection and another based on active learn-
ing. Using only 25 (out of 160) fea-
tures, our model resulting from feature
selection ranked 1st place in the scoring
variant of subtask 1.1 and 3rd place in
the ranking variant of the subtask, while
the active learning model reached 2nd
place in the scoring variant using only
?25% of the available instances for train-
ing. These results give evidence that
Gaussian Processes achieve the state of
the art performance as a modelling ap-
proach for translation quality estimation,
and that carefully selecting features and
instances for the problem can further im-
prove or at least maintain the same per-
formance levels while making the problem
less resource-intensive.
1 Introduction
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al,
2004; Specia et al, 2009; Callison-burch et al,
2012). A common use of quality predictions is
the decision between post-editing a given machine
translated sentence and translating its source from
scratch, based on whether its post-editing effort is
estimated to be lower than the effort of translating
the source sentence.
The WMT13 QE shared task defined a group
of tasks related to QE. In this paper, we present
the submissions by the University of Sheffield
team. Our models are based on Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2006), a
non-parametric probabilistic framework. We ex-
plore the application of GP models in two con-
texts: 1) improving the prediction performance by
applying a feature selection step based on opti-
mised hyperparameters and 2) reducing the dataset
size (and therefore the annotation effort) by per-
forming Active Learning (AL). We submitted en-
tries for two of the four proposed tasks.
Task 1.1 focused on predicting HTER scores
(Human Translation Error Rate) (Snover et al,
2006) using a dataset composed of 2254 English-
Spanish news sentences translated by Moses
(Koehn et al, 2007) and post-edited by a profes-
sional translator. The evaluation used a blind test
set, measuring MAE (Mean Absolute Error) and
RMSE (Root Mean Square Error), in the case of
the scoring variant, and DeltaAvg and Spearman?s
rank correlation in the case of the ranking vari-
ant. Our submissions reached 1st (feature selec-
tion) and 2nd (active learning) places in the scor-
ing variant, the task the models were optimised
for, and outperformed the baseline by a large mar-
gin in the ranking variant.
The aim of task 1.3 aimed at predicting post-
editing time using a dataset composed of 800
English-Spanish news sentences also translated by
Moses but post-edited by five expert translators.
Evaluation was done based on MAE and RMSE
on a blind test set. For this task our models were
not able to beat the baseline system, showing that
more advanced modelling techniques should have
been used for challenging quality annotation types
and datasets such as this.
2 Features
In our experiments, we used a set of 160 features
which are grouped into black box (BB) and glass
box (GB) features. They were extracted using the
337
open source toolkit QuEst1 (Specia et al, 2013).
We briefly describe them here, for a detailed de-
scription we refer the reader to the lists available
on the QuEst website.
The 112 BB features are based on source and
target segments and attempt to quantify the source
complexity, the target fluency and the source-
target adequacy. Examples of them include:
? Word and n-gram based features:
? Number of tokens in source and target
segments;
? Language model (LM) probability of
source and target segments;
? Percentage of source 1?3-grams ob-
served in different frequency quartiles of
the source side of the MT training cor-
pus;
? Average number of translations per
source word in the segment as given by
IBM 1 model with probabilities thresh-
olded in different ways.
? POS-based features:
? Ratio of percentage of nouns/verbs/etc
in the source and target segments;
? Ratio of punctuation symbols in source
and target segments;
? Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
? Syntactic features:
? Source and target Probabilistic Context-
free Grammar (PCFG) parse log-
likelihood;
? Source and target PCFG average confi-
dence of all possible parse trees in the
parser?s n-best list;
? Difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP
phrases in the source and target;
? Other features:
? Kullback-Leibler divergence of source
and target topic model distributions;
? Jensen-Shannon divergence of source
and target topic model distributions;
1http://www.quest.dcs.shef.ac.uk
? Source and target sentence intra-lingual
mutual information;
? Source-target sentence inter-lingual mu-
tual information;
? Geometric average of target word prob-
abilities under a global lexicon model.
The 48 GB features are based on information
provided by the Moses decoder, and attempt to in-
dicate the confidence of the system in producing
the translation. They include:
? Features and global score of the SMT model;
? Number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? Average size of the target phrases;
? Relative frequency of the words in the trans-
lation in the n-best list;
? Ratio of SMT model score of the top transla-
tion to the sum of the scores of all hypothesis
in the n-best list;
? Average size of hypotheses in the n-best list;
? N-best list density (vocabulary size / average
sentence length);
? Fertility of the words in the source sentence
compared to the n-best list in terms of words
(vocabulary size / source sentence length);
? Edit distance of the current hypothesis to the
centre hypothesis;
? Proportion of pruned search graph nodes;
? Proportion of recombined graph nodes.
3 Model
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : RF ? R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
f(x) ? GP(0, k(x,x?))
which is parameterized by a mean function (here,
0) and a covariance kernel function k(x,x?). Each
338
response value is then generated from the function
evaluated at the corresponding input, yi = f(xi)+
?, where ? ? N (0, ?2n) is added white-noise.
Prediction is formulated as a Bayesian inference
under the posterior:
p(y?|x?,D) =
?
f
p(y?|x?, f)p(f |D)
where x? is a test input, y? is the test response
value andD is the training set. The predictive pos-
terior can be solved analitically, resulting in:
y? ? N (kT? (K + ?2nI)?1y,
k(x?, x?)? kT? (K + ?2nI)?1k?)
where k? = [k(x?,x1)k(x?,x2) . . . k(x?,xd)]T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs.
A nice property of this formulation is that y?
is actually a probability distribution, encoding the
model uncertainty and making it possible to inte-
grate it into subsequent processing. In this work,
we used the variance values given by the model in
an active learning setting, as explained in Section
4.
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work on QE using GP (Cohn and
Specia, 2013; Shah et al, 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
k(x,x?) = ?2f exp
(
?12
F?
i=1
xi ? x?i
li
)
where F is the number of features, ?2f is the co-
variance magnitude and li > 0 are the feature
length scales.
The resulting model hyperparameters (SE vari-
ance ?2f , noise variance ?2n and SE length scales li)
were learned from data by maximising the model
likelihood. In general, the likelihood function is
non-convex and the optimisation procedure may
lead to local optima. To avoid poor hyperparam-
eter values due to this, we performed a two-step
procedure where we first optimise a model with all
the SE length scales tied to the same value (which
is equivalent to an isotropic model) and we used
the resulting values as starting point for the ARD
optimisation.
All our models were trained using the GPy2
toolkit, an open source implementation of GPs
written in Python.
3.1 Feature Selection
To perform feature selection, we followed the ap-
proach used in Shah et al (2013) and ranked the
features according to their learned length scales
(from the lowest to the highest). The length scales
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
For task 1.1, we performed this feature selection
over all 160 features mentioned in Section 2. For
task 1.3, we used a subset of the 80 most general
BB features as in (Shah et al, 2013), for which we
had all the necessary resources available for the
extraction. We selected the top 25 features for both
models, based on empirical results found by Shah
et al (2013) for a number of datasets, and then
retrained the GP using only the selected features.
4 Active Learning
Active Learning (AL) is a machine learning
paradigm that let the learner decide which data it
wants to learn from (Settles, 2010). The main goal
of AL is to reduce the size of the dataset while
keeping similar model performance (therefore re-
ducing annotation costs). In previous work with
17 baseline features, we have shown that with only
?30% of instances it is possible to achieve 99%
of the full dataset performance in the case of the
WMT12 QE dataset (Beck et al, 2013).
To investigate if a reduced dataset can achieve
competitive performance in a blind evaluation set-
ting, we submitted an entry for both tasks 1.1 and
1.3 composed of models trained on a subset of in-
stances selected using AL, and paired with fea-
ture selection. Our AL procedure starts with a
model trained on a small number of randomly se-
lected instances from the training set and then uses
this model to query the remaining instances in the
training set (our query pool). At every iteration,
the model selects the more ?informative? instance,
asks an oracle for its true label (which in our case
is already given in the dataset, and therefore we
2http://sheffieldml.github.io/GPy/
339
only simulate AL) and then adds it to the training
set. Our procedure started with 50 instances for
task 1.1 and 20 instances for task 1.3, given its re-
duced training set size. We optimised the Gaussian
Process hyperparameters every 20 new instances,
for both tasks.
As a measure of informativeness we used Infor-
mation Density (ID) (Settles and Craven, 2008).
This measure leverages between the variance
among instances and how dense the region (in the
feature space) where the instance is located is:
ID(x) = V ar(y|x)?
(
1
U
U?
u=1
sim(x,x(u))
)?
The ? parameter controls the relative impor-
tance of the density term. In our experiments, we
set it to 1, giving equal weights to variance and
density. The U term is the number of instances
in the query pool. The variance values V ar(y|x)
are given by the GP prediction while the similar-
ity measure sim(x,x(u)) is defined as the cosine
distance between the feature vectors.
In a real annotation setting, it is important to
decide when to stop adding new instances to the
training set. In this work, we used the confidence
method proposed by Vlachos (2008). This is an
method that measures the model?s confidence on
a held-out non-annotated dataset every time a new
instance is added to the training set and stops the
AL procedure when this confidence starts to drop.
In our experiments, we used the average test set
variance as the confidence measure.
In his work, Vlachos (2008) showed a correla-
tion between the confidence and test error, which
motivates its use as a stop criterion. To check if
this correlation also occurs in our task, we measure
the confidence and test set error for task 1.1 using
the WMT12 split (1832/422 instances). However,
we observed a different behaviour in our experi-
ments: Figure 1 shows that the confidence does
not raise or drop according to the test error but it
stabilises around a fixed value at the same point as
the test error also stabilises. Therefore, instead of
using the confidence drop as a stop criterion, we
use the point where the confidence stabilises. In
Figure 2 we can observe that the confidence curve
for the WMT13 test set stabilises after ?580 in-
stances. We took that point as our stop criterion
and used the first 580 selected instances as the AL
dataset.
Figure 1: Test error and test confidence curves
for HTER prediction (task 1.1) using the WMT12
training and test sets.
Figure 2: Test confidence for HTER prediction
(task 1.1) using the official WMT13 training and
test sets.
We repeated the experiment with task 1.3, mea-
suring the relationship between test confidence
and error using a 700/100 instances split (shown
on Figure 3). For this task, the curves did not fol-
low the same behaviour: the confidence do not
seem to stabilise at any point in the AL proce-
dure. The same occurred when using the official
training and test sets (shown on Figure 4). How-
ever, the MAE curve is quite flat, stabilising after
about 100 sentences. This may simply be a conse-
quence of the fact that our model is too simple for
post-editing time prediction. Nevertheless, in or-
der to analyse the performance of AL for this task
we submitted an entry using the first 100 instances
chosen by the AL procedure for training.
The observed peaks in the confidence curves re-
340
Task 1.1 - Ranking Task 1.1 - Scoring Task 1.3
DeltaAvg ? Spearman ? MAE ? RMSE ? MAE ? RMSE ?
SHEF-Lite-FULL 9.76 0.57 12.42 15.74 55.91 103.11
SHEF-Lite-AL 8.85 0.50 13.02 17.03 64.62 99.09
Baseline 8.52 0.46 14.81 18.22 51.93 93.36
Table 1: Submission results for tasks 1.1 and 1.3. The bold value shows a winning entry in the shared
task.
Figure 3: Test error and test confidence curves
for post-editing time prediction (task 1.3) using a
700/100 split on the WMT13 training set.
Figure 4: Test confidence for post-editing time
prediction (task 1.3) using the official WMT13
training and test sets.
sult from steps where the hyperparameter optimi-
sation got stuck at bad local optima. These de-
generated results set the variances (?2f , ?2n) to very
high values, resulting in a model that considers all
data as pure noise. Since this behaviour tends to
disappear as more instances are added to the train-
ing set, we believe that increasing the dataset size
helps to tackle this problem. We plan to investi-
gate this issue in more depth in future work.
For both AL datasets we repeated the feature se-
lection procedure explained in Section 3.1, retrain-
ing the models on the selected features.
5 Results
Table 1 shows the results for both tasks. SHEF-
Lite-FULL represents GP models trained on the
full dataset (relative to each task) with a feature
selection step. SHEF-Lite-AL corresponds to the
same models trained on datasets obtained from
each active learning procedure and followed by
feature selection.
For task 1.1, our submission SHEF-Lite-FULL
was the winning system in the scoring subtask, and
ranked third in the ranking subtask. These results
show that GP models achieve the state of the art
performance in QE. These are particularly positive
results considering that there is room for improve-
ment in the feature selection procedure to identify
the optimal number of features to be selected. Re-
sults for task 1.3 were below the baseline, once
again evidencing the fact that the noise model used
in our experiments is probably too simple for post-
editing time prediction. Post-editing time is gener-
ally more prone to large variations and noise than
HTER, especially when annotations are produced
by multiple post-editors. Therefore we believe that
kernels that encode more advanced noise models
(such as the multi-task kernel used by Cohn and
Specia (2013)) should be used for better perfor-
mance. Another possible reason for that is the
smaller set of features used for this task (black-
box features only).
Our SHEF-Lite-AL submissions performed bet-
ter than the baseline in both scoring and ranking
in task 1.1, ranking 2nd place in the scoring sub-
task. Considering that the dataset is composed by
only ?25% of the full training set, these are very
encouraging results in terms of reducing data an-
341
notation needs. We note however that these results
are below those obtained with the full training set,
but Figure 1 shows that it is possible to achieve
the same or even better results with an AL dataset.
Since the curves shown in Figure 1 were obtained
using the full feature set, we believe that advanced
feature selection strategies can help AL datasets to
achieve better results.
6 Conclusions
The results obtained by our submissions confirm
the potential of Gaussian Processes to become the
state of the art approach for Quality Estimation.
Our models were able to achieve the best perfor-
mance in predicting HTER. They also offer the ad-
vantage of inferring a probability distribution for
each prediction. These distributions provide richer
information (like variance values) that can be use-
ful, for example, in active learning settings.
In the future, we plan to further investigate these
models by devising more advanced kernels and
feature selection methods. Specifically, we want
to employ our feature set in a multi-task kernel set-
ting, similar to the one proposed by Cohn and Spe-
cia (2013). These kernels have the power to model
inter-annotator variance and noise, which can lead
to better results in the prediction of post-editing
time.
We also plan to pursue better active learning
procedures by investigating query methods specif-
ically tailored for QE, as well as a better stop cri-
teria. Our goal is to be able to reduce the dataset
size significantly without hurting the performance
of the model. This is specially interesting in the
case of QE, since it is a very task-specific problem
that may demand a large annotation effort.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Kashif Shah and Lucia Specia).
References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013.
Reducing Annotation Effort for Quality Estimation
via Active Learning. In Proceedings of ACL (to ap-
pear).
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Chris Callison-burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL (to appear).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV (to appear).
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jose? G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session (to appear).
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech & Language,
22(3):295?312, July.
342
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 307?312,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation
Quality Estimation
Daniel Beck and Kashif Shah and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,kashif.shah,l.specia}@sheffield.ac.uk
Abstract
We describe our systems for the WMT14
Shared Task on Quality Estimation (sub-
tasks 1.1, 1.2 and 1.3). Our submissions
use the framework of Multi-task Gaus-
sian Processes, where we combine multi-
ple datasets in a multi-task setting. Due to
the large size of our datasets we also ex-
periment with Sparse Gaussian Processes,
which aim to speed up training and predic-
tion by providing sensible sparse approxi-
mations.
1 Introduction
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al.,
2004; Specia et al., 2009; Bojar et al., 2013). A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
The WMT 2014 QE shared task defined a group
of tasks related to QE. In this paper, we de-
scribe our submissions for subtasks 1.1, 1.2 and
1.3. Our models are based on Gaussian Pro-
cesses (GPs) (Rasmussen and Williams, 2006),
a non-parametric kernelised probabilistic frame-
work. We propose to combine multiple datasets
to improve our QE models by applying GPs in
a multi-task setting. Our hypothesis is that us-
ing sensible multi-task learning settings gives im-
provements over simply pooling all datasets to-
gether.
Task 1.1 focuses on predicting post-editing ef-
fort for four language pairs: English-Spanish
(en-es), Spanish-English (es-en), English-German
(en-de), and German-English (de-en). Each con-
tains a different number of source sentences and
their human translations, as well as 2-3 versions
of machine translations: by a statistical (SMT)
system, a rule-based system (RBMT) system and,
for en-es/de only, a hybrid system. Source sen-
tences were extracted from tests sets of WMT13
and WMT12, and the translations were produced
by top MT systems of each type and a human
translator. Labels range from 1 to 3, with 1 in-
dicating a perfect translation and 3, a low quality
translation.
The purpose of task 1.2 is to predict HTER
scores (Human Translation Error Rate) (Snover
et al., 2006) using a dataset composed of 896
English-Spanish sentences translated by a MT sys-
tem and post-edited by a professional translator.
Finally, task 1.3 aims at predicting post-editing
time, using a subset of 650 sentences from the
Task 1.2 dataset.
For each task, participants can submit two types
of results: scoring and ranking. For scoring, eval-
uation is made in terms of Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE). For
ranking, DeltaAvg and Spearman?s rank correla-
tion were used as evaluation metrics.
2 Model
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : R
F
? R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
f(x) ? GP(0, k(x,x
?
)),
which is parameterised by a mean function (here,
0) and a covariance kernel function k(x,x
?
). Each
response value is then generated from the function
evaluated at the corresponding input, y
i
= f(x
i
)+
?, where ? ? N (0, ?
2
n
) is added white-noise.
307
Prediction is formulated as a Bayesian inference
under the posterior:
p(y
?
|x
?
,D) =
?
f
p(y
?
|x
?
, f)p(f |D),
where x
?
is a test input, y
?
is the test response
value andD is the training set. The predictive pos-
terior can be solved analitically, resulting in:
y
?
? N (k
T
?
(K + ?
2
n
I)
?1
y,
k(x
?
, x
?
)? k
T
?
(K + ?
2
n
I)
?1
k
?
),
where k
?
= [k(x
?
,x
1
)k(x
?
,x
2
) . . . k(x
?
,x
n
)]
T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs (the Gram matrix).
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work in QE using GP (Cohn and
Specia, 2013; Shah et al., 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
k(x,x
?
) = ?
2
f
exp
(
?
1
2
F
?
i=1
x
i
? x
?
i
l
i
)
,
where F is the number of features, ?
2
f
is the co-
variance magnitude and l
i
> 0 are the feature
lengthscales.
The resulting model hyperparameters (SE vari-
ance ?
2
f
, noise variance ?
2
n
and SE lengthscales l
i
)
were learned from data by maximising the model
likelihood. All our models were trained using the
GPy
1
toolkit, an open source implementation of
GPs written in Python.
2.1 Multi-task learning
The GP regression framework can be extended to
multiple outputs by assuming f(x) to be a vec-
tor valued function. These models are commonly
referred as coregionalization models in the GP lit-
erature (
?
Alvarez et al., 2012). Here we refer to
them as multi-task kernels, to emphasize our ap-
plication.
In this work, we employ a separable multi-task
kernel, similar to the one used by Bonilla et al.
(2008) and Cohn and Specia (2013). Consider-
ing a set of D tasks, we define the corresponding
multi-task kernel as:
k((x, d), (x
?
, d
?
)) = k
data
(x,x
?
)?B
d,d
?
, (1)
1
http://sheffieldml.github.io/GPy/
where k
data
is a kernel on the input points, d and
d
?
are task or metadata information for each input
and B ? R
D?D
is the multi-task matrix, which
encodes task covariances. For task 1.1, we con-
sider each language pair as a different task, while
for tasks 1.2 and 1.3 we use additional datasets
for the same language pair (en-es), treating each
dataset as a different task.
To perform the learning procedure the multi-
task matrix should be parameterised in a sensible
way. We follow the parameterisations proposed
by Cohn and Specia (2013), which we briefly de-
scribe here:
Independent: B = I. In this setting each task is
modelled independently. This is not strictly
equivalent to independent model training be-
cause the tasks share the same data kernel
(and the same hyperparameters);
Pooled: B = 1. Here the task identity is ignored.
This is equivalent to pooling all datasets in a
single task model;
Combined: B = 1 + ?I. This setting lever-
ages between independent and pooled mod-
els. Here, ? > 0 is treated as an hyperparam-
eter;
Combined+: B = 1 + diag(?). Same as ?com-
bined?, but allowing one different ? value per
task.
2.2 Sparse Gaussian Processes
The performance bottleneck for GP models is the
Gram matrix inversion, which is O(n
3
) for stan-
dard GPs, with n being the number of training in-
stances. For multi-task settings this can be a po-
tential issue because these models replicate the in-
stances for each task and the resulting Gram ma-
trix has dimensionality nd ? nd, where d is the
number of tasks.
Sparse GPs tackle this problem by approximat-
ing the Gram matrix using only a subset of m in-
ducing inputs. Without loss of generalisation, con-
sider these m points as the first instances in the
training data. We can then expand the Gram ma-
trix in the following way:
K =
[
K
mm
K
m(n?m)
K
(n?m)m
K
(n?m)(n?m)
]
.
Following the notation in (Rasmussen and
Williams, 2006), we refer K
m(n?m)
as K
mn
and
308
its transpose as K
nm
. The block structure of K
forms the basis of the so-called Nystr?om approxi-
mation:
?
K = K
nm
K
?1
mm
K
mn
, (2)
which results in the following predictive posterior:
y
?
? N (k
T
m?
?
G
?1
K
mn
y, (3)
k(x
?
,x
?
)? k
T
m?
K
?1
mm
k
m?
+
?
2
n
k
T
m?
?
G
?1
k
m?
),
where
?
G = ?
2
n
K
mm
+ K
mn
K
nm
and k
m?
is the
vector of kernel evaluations between test input x
?
and the m inducing inputs. The resulting training
complexity is O(m
2
n).
The remaining question is how to choose the in-
ducing inputs. We follow the approach of Snelson
and Ghahramani (2006), which note that these in-
ducing inputs do not need to be a subset of the
training data. Their method considers each in-
put as a hyperparameter, which is then optimised
jointly with the kernel hyperparameters.
2.3 Features
For all tasks we used the QuEst framework (Spe-
cia et al., 2013) to extract a set of 80 black-box
features as in Shah et al. (2013), for which we had
all the necessary resources available. Examples of
the features extracted include:
? N-gram-based features:
? Number of tokens in source and target
segments;
? Language model (LM) probability of
source and target segments;
? Percentage of source 1?3-grams ob-
served in different frequency quartiles of
a large corpus of the source language;
? Average number of translations per
source word in the segment as given by
IBM 1 model from a large parallel cor-
pus of the language, with probabilities
thresholded in different ways.
? POS-based features:
? Ratio of percentage of nouns/verbs/etc
in the source and target segments;
? Ratio of punctuation symbols in source
and target segments;
? Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
For the full set of features we refer readers to
QuEst website.
2
To perform feature selection, we followed the
approach used in Shah et al. (2013) and ranked
the features according to their learned lengthscales
(from the lowest to the highest). The lengthscale
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
3 Preliminary Experiments
Our submissions are based on multi-task settings.
For task 1.1, we consider each language pair as a
different task, training one model for all pairs. For
tasks 1.2 and 1.3, we used additional datasets and
encoded each one as a different task (totalling 3
tasks):
WMT13: these are the datasets provided in last
year?s QE shared task (Bojar et al., 2013).
We combined training and test sets, totalling
2, 754 sentences for HTER prediction and
1, 003 sentences for post-editing time predic-
tion, both for English-Spanish.
EAMT11: this dataset is provided by Specia
(2011) and is composed of 1, 000 English-
Spanish sentences annotated in terms of
HTER and post-editing time.
For each task we prepared two submissions: one
trained on a standard GP with the full 80 features
set and another one trained on a sparse GP with
a subset of 40 features. The features were chosen
by training a smaller model on a subset of 400 in-
stances and following the procedure explained in
Section 2.3 for feature selection, with a pre-define
cutoff point on the number of features (40), based
on previous experiments. The sparse models were
trained using 400 inducing inputs.
To select an appropriate multi-task setting for
our submissions we performed preliminary exper-
iments using a 90%/10% split on the correspond-
ing training set for each task. The resulting MAE
scores are shown in Tables 1 and 2, for standard
and sparse GPs, respectively. The boldface fig-
ures correspond to the settings we choose for the
2
http://www.quest.dcs.shef.ac.uk/
quest_files/features_blackbox
309
Task 1.1 Task 1.2 Task 1.3
en-es es-en en-de de-en en-es en-es
Independent 0.4905 0.5325 0.5962 0.5452 0.2047 0.4486
Pooled 0.4957 0.5171 0.6012 0.5612 0.2036 0.8599
Combined 0.4939 0.5162 0.6007 0.5550 0.2321 0.7489
Combined+ 0.4932 0.5182 0.5990 0.5514 0.2296 0.4472
Table 1: MAE results for preliminary experiments on standard GPs. Post-editing time scores for task 1.3
are shown on log time per word.
Task 1.1 Task 1.2 Task 1.3
en-es es-en en-de de-en en-es en-es
Independent 0.5036 0.5274 0.6002 0.5532 0.3432 0.3906
Pooled 0.4890 0.5131 0.5927 0.5532 0.1597 0.6410
Combined 0.4872 0.5183 0.5871 0.5451 0.2871 0.6449
Combined+ 0.4935 0.5255 0.5864 0.5458 0.1659 0.4040
Table 2: MAE results for preliminary experiments on sparse GPs. Post-editing time scores for task 1.3
are shown on log time per word.
official submissions, after re-training on the corre-
sponding full training sets.
To check the speed-ups obtained from using
sparse GPs, we measured wall clock times for
training and prediction in Task 1.1 using the ?In-
dependent? multi-task setting. Table 3 shows the
resulting times and the corresponding speed-ups
when comparing to the standard GP. For compar-
ison, we also trained a model using 200 inducing
inputs, although we did not use the results of this
model in our submissions.
Time (secs) Speed-up
Standard GP 12122 ?
Sparse GP (m=400) 3376 3.59x
Sparse GP (m=200) 978 12.39x
Table 3: Wall clock times and speed-ups for GPs
training and prediction: full versus sparse GPs.
4 Official Results and Discussion
Table 4 shows the results for Task 1.1. Us-
ing standard GPs we obtained improved results
over the baseline for English-Spanish and English-
German only, with particularly substantial im-
provements for English-Spanish, which also hap-
pens for sparse GPs. This may be related to the
larger size of this dataset when compared to the
others. Our results here are mostly inconclusive
though and we plan to investigate this setting more
in depth in the future. Specifically, due to the
coarse behaviour of the labels, ordinal regression
GP models (like the one proposed in (Chu et al.,
2005)) could be useful for this task.
Results for Task 1.2 are shown in Table 5. The
standard GP model performed unusually poorly
when compared to the baseline or the sparse GP
model. To investigate this, we inspected the re-
sulting model hyperparameters. We found out that
the noise ?
2
n
was optimised to a very low value,
close to zero, which characterises overfitting. The
same behaviour was not observed with the sparse
model, even though it had a much higher number
of hyperparameters to optimise, and was therefore
more prone to overfitting. We plan to investigate
this issue further but a possible cause could be bad
starting values for the hyperparameters.
Table 6 shows results for Task 1.3. In this task,
the standard GP model outperformed the base-
line, with the sparse GP model following very
closely. These figures represent significant im-
provements compared to our submission to the
same task in last year?s shared task (Beck et al.,
2013), where we were not able to beat the baseline.
The main differences between last year?s and this
year?s models are the use of additional datasets
and a higher number of features (25 vs. 40). The
competitive results for the sparse GP models are
very promising because they show we can com-
bine multiple datasets to improve post-editing time
prediction while employing a sparse model to cope
with speed issues.
310
en-es es-en en-de de-en
? ? ? ? ? ? ? ?
Standard GP 0.21 -0.33 0.11 -0.15 0.26 -0.36 0.24 -0.27
Sparse GP 0.17 0.27 0.12 -0.17 0.23 -0.33 0.14 -0.17
Baseline 0.14 -0.22 0.12 -0.21 0.23 -0.34 0.21 -0.25
en-es es-en en-de de-en
MAE RMSE MAE RMSE MAE RMSE MAE RMSE
Standard GP 0.49 0.63 0.62 0.77 0.63 0.74 0.65 0.77
Sparse GP 0.54 0.69 0.54 0.69 0.64 0.75 0.66 0.79
Baseline 0.52 0.66 0.57 0.68 0.64 0.76 0.65 0.78
Table 4: Official results for task 1.1. The top table shows results for the ranking subtask (?: DeltaAvg;
?: Spearman?s correlation). The bottom table shows results for the scoring subtask.
Ranking Scoring
? ? MAE RMSE
Standard GP 0.72 0.09 18.15 23.41
Sparse GP 7.69 0.43 15.04 18.38
Baseline 5.08 0.31 15.23 19.48
Table 5: Official results for task 1.2.
Ranking Scoring
? ? MAE RMSE
Standard GP 16.08 0.64 17.13 27.33
Sparse GP 16.33 0.63 17.42 27.35
Baseline 14.71 0.57 21.49 34.28
Table 6: Official results for task 1.3.
5 Conclusions
We proposed a new setting for training QE mod-
els based on Multi-task Gaussian Processes. Our
settings combined different datasets in a sensible
way, by considering each dataset as a different
task and learning task covariances. We also pro-
posed to speed-up training and prediction times
by employing sparse GPs, which becomes crucial
in multi-task settings. The results obtained are
specially promising in the post-editing time task,
where we obtained the same results as with stan-
dard GPs and improved over our models from the
last evaluation campaign.
In the future, we plan to employ our multi-task
models in large-scale settings, like datasets an-
notated through crowdsourcing platforms. These
datasets are usually labelled by dozens of annota-
tors and multi-task GPs have proved an interest-
ing framework for learning the annotation noise
(Cohn and Specia, 2013). However, multiple tasks
can easily make training and prediction times pro-
hibitive, and thus another direction if work is to
use recent advances in sparse GPs, like the one
proposed by Hensman et al. (2013). We believe
that the combination of these approaches could
further improve the state-of-the-art performance in
these tasks.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from European Union?s Seventh Framework
Programme for research, technological develop-
ment and demonstration under grant agreement
no. 296347 (QTLaunchPad).
References
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1?37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337?342.
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Ondej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of WMT13, pages 1?44.
311
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Wei Chu, Zoubin Ghahramani, Francesco Falciani, and
David L Wild. 2005. Biomarker discovery in mi-
croarray gene expression data with Gaussian pro-
cesses. Bioinformatics, 21(16):3385?93, August.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
James Hensman, Nicol`o Fusi, and Neil D. Lawrence.
2013. Gaussian Processes for Big Data. In Pro-
ceedings of UAI.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Edward Snelson and Zoubin Ghahramani. 2006.
Sparse Gaussian Processes using Pseudo-inputs. In
Proceedings of NIPS.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jos?e G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
312
