Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 329?336
Manchester, August 2008
Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction
William P. Headden III, David McClosky, Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw,dmcc,ec}@cs.brown.edu
Abstract
This paper explores the relationship be-
tween various measures of unsupervised
part-of-speech tag induction and the per-
formance of both supervised and unsuper-
vised parsing models trained on induced
tags. We find that no standard tagging
metrics correlate well with unsupervised
parsing performance, and several metrics
grounded in information theory have no
strong relationship with even supervised
parsing performance.
1 Introduction
There has been a great deal of recent interest in
the unsupervised discovery of syntactic structure
from text, both parts-of-speech (Johnson, 2007;
Goldwater and Griffiths, 2007; Biemann, 2006;
Dasgupta and Ng, 2007) and deeper grammatical
structure like constituency and dependency trees
(Klein and Manning, 2004; Smith, 2006; Bod,
2006; Seginer, 2007; Van Zaanen, 2001). While
some grammar induction systems operate on raw
text, many of the most successful ones presume
prior part-of-speech tagging. Meanwhile, most re-
cent work in part-of-speech induction focuses on
increasing the degree to which their tags match
hand-annotated ones such as those in the Penn
Treebank.
In this work our goal is to evaluate how im-
provements in part-of-speech tag induction affects
grammar induction. Using several different unsu-
pervised taggers, we induce tags and train three
grammar induction systems on the results.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
We then explore the relationship between the
performance on common unsupervised tagging
metrics and the performance of resulting grammar
induction systems. Disconcertingly we find that
they bear little to no relationship.
This paper is organized as follows. In Section 2
we discuss unsupervised part-of-speech induction
systems and common methods of evaluation. In
Section 3, we describe grammar induction in gen-
eral and discuss the systems with which we evalu-
ate taggings. We present our experiments in Sec-
tion 4, and finally conclude in Section 5.
2 Part-of-speech Tag Induction
Part-of-speech tag induction can be thought of as a
clustering problem where, given a corpus of words,
we aim to group word tokens into syntactic classes.
Two tasks are commonly labeled unsupervised
part-of-speech induction. In the first, tag induction
systems are allowed the use of a tagging dictionary,
which specifies for each word a set of possible
parts-of-speech (Merialdo, 1994; Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007). In the
second, only the word tokens and sentence bound-
aries are given. In this work we focus on this latter
task to explore grammar induction in a maximally
unsupervised context.
Tag induction systems typically focus on two
sorts of features: distributional and morphologi-
cal. Distributional refers to what sorts of words
appear in close proximity to the word in question,
while morphological refers to modeling the inter-
nal structure of a word. All the systems below
make use of distributional information, whereas
only two use morphological features.
We primarily focus on the metrics used to evalu-
ate induced taggings. The catalogue of recent part-
of-speech systems is large, and we can only test
329
the tagging metrics using a few systems. Recent
work that we do not explore explicitly includes
(Biemann, 2006; Dasgupta and Ng, 2007; Freitag,
2004; Smith and Eisner, 2005). We have selected
a few systems, described below, that represent a
broad range of features and techniques to make our
evaluation of the metrics as broad as possible.
2.1 Clustering using SVD and K-means
Schu?tze (1995) presents a series of part-of-speech
inducers based on distributional clustering. We
implement the baseline system, which Klein and
Manning (2002) use for their grammar induction
experiments with induced part-of-speech tags. For
each word type w in the vocabulary V , the system
forms a feature row vector consisting of the num-
ber of times each of the F most frequent words oc-
cur to the left of w and to the right of w. It normal-
izes these row vectors and assembles them into a
|V |?2F matrix. It then performs a Singular Value
Decomposition on the matrix and rank reduces it to
decrease its dimensionality to d principle compo-
nents (d < 2F ). This results in a representation
of each word as a point in a d dimensional space.
We follow Klein and Manning (2002) in using K-
means to cluster the d dimensional word vectors
into parts-of-speech. We use the F = 500 most
frequent words as left and right context features,
and reduce to a dimensionality of d = 50. We re-
fer to this system as SVD in our experiments.
The other systems described in Schu?tze (1995)
make use of more complicated feature models. We
chose the baseline system primarily to match pre-
vious evaluations of grammar induction using in-
duced tags (Klein and Manning, 2002).
2.2 Hidden Markov Models
One simple family of models for part-of-speech in-
duction are the Hidden Markov Models (HMMs),
in which there is a sequence of hidden state vari-
ables t
1
...t
n
(for us, the part-of-speech tags). Each
state t
i
is conditioned on the previous n ? 1 states
t
i?1
...t
i?n+1
, and every t
i
emits an observed word
w
i
conditioned on t
i
. There is a single start state
that emits nothing, as well as a single stop state,
which emits an end-of-sentence marker with prob-
ability 1 and does not transition further. In our ex-
periments we use the bitag HMM, in which each
state t
i
depends only on state t
i?1
.
The classic method of training HMMs for part-
of-speech induction is the Baum-Welch (Baum,
1972) variant of the Expectation-Maximization
(EM) algorithm, which searches for a local max-
imum in the likelihood of the observed words.
Other methods approach the problem from
a Bayesian perspective. These methods place
Dirichlet priors over the parameters of each transi-
tion and emission multinomial. For an HMM with
a set of states T and a set of output symbols V :
?t ? T ?
t
? Dir(?
1
, ...?
|T |
) (1)
?t ? T ?
t
? Dir(?
1
, ...?
|V |
) (2)
t
i
|t
i?1
, ?
t
i
?1
? Multi(?
t
i?1
) (3)
w
i
|t
i
, ?
t
i
? Multi(?
t
i
) (4)
One advantage of the Bayesian approach is that
the prior allows us to bias learning toward sparser
structures, by setting the Dirichlet hyperparame-
ters ?, ? to a value less than one (Johnson, 2007;
Goldwater and Griffiths, 2007). This increases the
probability of multinomial distributions which put
most of their mass on a few events, instead of dis-
tributing them broadly across many events. There
is evidence that this leads to better performance
on some part-of-speech induction metrics (John-
son, 2007; Goldwater and Griffiths, 2007).
There are both MCMC and variational ap-
proaches to estimating HMMs with sparse Dirich-
let priors; we chose the latter (Variational Bayes
or VB) due to its simple implementation as a
minor modification to Baum-Welch. Johnson
(2007) evaluates both estimation techniques on the
Bayesian bitag model; Goldwater and Griffiths
(2007) emphasize the advantage in the MCMC ap-
proach of integrating out the HMM parameters in a
tritag model, yielding a tagging supported by many
different parameter settings.
Following the setup in Johnson (2007), we ini-
tialize the transition and emission distributions to
be uniform with a small amount of noise, and run
EM and VB for 1000 iterations. We label these
systems as HMM-EM and HMM-VB respectively
in our experiments. In our VB experiments we set
?
i
= ?
j
= 0.1,?i ? {1, ..., |T |} , j ? {1, ..., |V |},
which yielded the best performance on most re-
ported metrics in Johnson (2007). We use max-
imum marginal decoding, which Johnson (2007)
reports performs better than Viterbi decoding.
2.3 Systems with Morphology
Clark (2003) presents several part-of-speech in-
duction systems which incorporate morphological
as well as distributional information. We use the
330
implementation found on his website.1
2.3.1 Ney-Essen with Morphology
The simplest model is based on work by (Ney et
al., 1994). It uses a bitag HMM, with the restric-
tion that each word type in the vocabulary can only
be generated by a single part-of-speech. Thus the
tag induction task here reduces to finding a multi-
way partition of the vocabulary. The learning al-
gorithm greedily reassigns each word type to the
part-of-speech that results in the greatest increase
in likelihood.
In order to incorporate morphology, Clark
(2003) associates with each part-of-speech a HMM
with letter emissions. The vocabulary is gener-
ated by generating a series of word types from
the letter HMM of each part-of-speech. These can
model very basic concatenative morphology. The
parameters of the HMMs are estimated by running
a single iteration of Forward-Backward after each
round of reassigning words to tags. In our exper-
iments we evaluate both the model without mor-
phology (NE in our experiments), and the morpho-
logical model, trying both 5 and 10 states in the let-
ter HMM (NEMorph5, NEMorph10 respectively).
2.3.2 Two-Level HMM
The final part-of-speech inducer we try from
Clark (2003) is a two-level HMM. This is similar
to the previous model, except it lifts the restriction
that a word appear under only one part-of-speech.
Alternatively, one could think of this model as a
standard HMM, whose emission distributions in-
corporate a mixture of a letter HMM and a stan-
dard multinomial. Training uses a simple variation
of Forward-Backward. In the experiments in this
paper, we initialize the mixture parameters to .5,
and try 5 states in the letter HMM. We refer to this
model as 2HMM.
2.4 Tag Evaluation
Objective evaluation in any clustering task is al-
ways difficult, since there are many ways to de-
fine good clusters. Typically it involves a mix-
ture of subjective evaluation and a comparison of
the clusters to those found by human annotators.
In the realm of part-of-speech induction, there are
several common ways of doing the latter. These
split into two groups: accuracy and information-
theoretic criteria.
1http://www.cs.rhul.ac.uk/home/alexc/pos.tar.gz
Accuracy, given some mapping between the set
of induced classes and the gold standard labels, is
the number of words in the corpus that have been
marked with the correct gold label divided by the
total number of word tokens. The main challenge
facing these metrics is deciding how to to map each
induced part-of-speech class to a gold tag. One
option is what Johnson (2007) calls ?many-to-one?
(M-to-1) accuracy, in which each induced tag is
labeled with its most frequent gold tag. Although
this results in a situation where multiple induced
tags may share a single gold tag, it does not punish
a system for providing tags of a finer granularity
than the gold standard.
In contrast, ?one-to-one? (1-to-1) accuracy re-
stricts each gold tag to having a single induced
tag. The mapping typically is made to try to give
the most favorable mapping in terms of accuracy,
typically using a greedy assignment (Haghighi and
Klein, 2006). In cases where the number of gold
tags is different than the number of induced tags,
some must necessarily remain unassigned (John-
son, 2007).
In addition to accuracy, there are several infor-
mation theoretic criteria presented in the literature.
These escape the problem of trying to find an ap-
propriate mapping between induced and gold tags,
at the expense of perhaps being less intuitive.
Let T
I
be the tag assignments to the words
in the corpus created by an unsupervised tag-
ger, and let T
G
be the gold standard tag as-
signments. Clark (2003) uses Shannon?s condi-
tional entropy of the gold tagging given the in-
duced tagging H(T
G
|T
I
). Lower entropy indi-
cates less uncertainty in the gold tagging if we al-
ready know the induced tagging. Freitag (2004)
uses the similar ?cluster-conditional tag perplex-
ity? which is merely exp(H(T
G
|T
I
))
2
. Since
cluster-conditional tag perplexity is a monotonic
function of H(T
G
|T
I
), we only report the latter.
Goldwater and Griffiths (2007) propose using
the Variation of Information of Meila? (2003):
V I(T
G
;T
I
) = H(T
G
|T
I
) + H(T
I
|T
G
)
VI represents the change in information when go-
ing from one clustering to another. It holds the
nice properties of being nonnegative, symmetric,
as well as fulfilling the triangle inequality.
2Freitag (2004) measures entropy in nats, while we use
bits. The difference is a constant factor.
331
3 Grammar Induction
In addition to parts-of-speech, we also want to dis-
cover deeper syntactic relationships. Grammar in-
duction is the problem of determining these re-
lationships in an unsupervised fashion. This can
be thought of more concretely as an unsupervised
parsing task. As there are many languages and do-
mains with few treebank resources, systems that
can learn syntactic structure from unlabeled data
would be valuable. Most work on this problem has
focused on either dependency induction, which we
discuss in Section 3.2, or on constituent induction,
which we examine in the next section.
The Grammar Induction systems we use to eval-
uate the above taggers are the Constituent-Context
Model (CCM), the Dependency Model with Va-
lence (DMV), and a model which combines the
two (CCM+DMV) outlined in (Klein and Man-
ning, 2002; Klein and Manning, 2004).
3.1 Constituent Grammar Induction
Klein and Manning (2002) present a generative
model for inducing constituent boundaries from
part-of-speech tagged text. The model first gener-
ates a bracketing B = {B
ij
}
1?i?j?n
, which spec-
ifies whether each span (i, j) in the sentence is a
constituent or a distituent. Next, given the con-
stituency or distituency of the span B
ij
, the model
generates the part-of-speech yield of the span
t
i
...t
j
, and the one-tag context window of the span
t
i?1
, t
j+1
. P (t
i
...t
j
|B
ij
) and P (t
i?1
, t
j+1
|B
ij
)
are multinomial distributions. The model is trained
using EM.
We evaluate induced constituency trees against
those of the Penn Treebank using the versions of
unlabeled precision, recall, and F-score used by
Klein and Manning (2002). These ignore triv-
ial brackets and multiple constituents spanning the
same bracket. They evaluate their CCM system
on the Penn Treebank WSJ sentences of length 10
or less, using part-of-speech tags induced by the
baseline system of Schu?tze (1995). They report
that switching to induced tags decreases the overall
bracketing F-score from 71.1 to 63.2, although the
recall of VP and S constituents actually improves.
Additionally, they find that NP and PP recall de-
creases substantially with induced tags. They at-
tribute this to the fact that nouns end up in many
induced tags.
There has been quite a bit of other work on
constituency induction. Smith and Eisner (2004)
present an alternative estimation technique for
CCM which uses annealing to try to escape local
maxima. Bod (2006) describes an unsupervised
system within the Data-Oriented-Parsing frame-
work. Several approaches try to learn structure
directly from raw text. Seginer (2007) has an in-
cremental parsing approach using a novel repre-
sentation called common-cover-links, which can
be converted to constituent brackets. Van Zaanen
(2001)?s ABL attempts to align sentences to deter-
mine what sequences of words are substitutable.
The work closest in spirit to this paper is Cramer
(2007), who evaluates several grammar induction
systems on the Eindhoven corpus (Dutch). One
of his experiments compares the grammar induc-
tion performance of these systems starting with
tags induced using the system described by Bie-
mann (2006), to the performance of the systems
on manually-marked tags. However he does not
evaluate to what degree better tagging performance
leads to improvement in these systems.
3.2 Dependency Grammar Induction
A dependency tree is a directed graph whose nodes
are words in the sentence. A directed edge exists
between two words if the target word (argument) is
a dependent of the source word (head). Each word
token may be the argument of only one head, but a
head may have several arguments. One word is the
head of the sentence, and is often thought of as the
argument of a virtual ?Root? node.
Klein and Manning (2004) present their Depen-
dency Model with Valence (DMV) for the un-
supervised induction of dependencies. Like the
constituency model, DMV works from parts-of-
speech. Under this model, for a given head, h,
they first generate the parts-of-speech of the argu-
ments to the right of h, and then those to the left.
Generating the arguments in a particular direction
breaks down into two parts: deciding whether to
stop generating in this direction, and if not, what
part-of-speech to generate as the argument. The ar-
gument decision conditions on h and the direction.
The stopping decision conditions on this and also
on whether h has already generated an argument in
this direction, thereby capturing the limited notion
of valence from which the model takes its name.
It is worth noting that this model can only repre-
sent projective dependency trees, i.e. those without
crossing edges.
Dependencies are typically evaluated using di-
332
Tagging Metrics Grammar Induction Metrics
Tagger No. Tags CCM CCM+DMV DMV
1-to-1 H(T
G
|T
I
) M-to-1 VI UF1 DA UA UF1 DA UA
Gold 1.00 0.00 1.00 0.00 71.50 52.90 67.60 56.50 45.40 63.80
HMM-EM 10 0.39 2.67 0.41 4.39 58.89 40.12 59.26 59.43 36.77 57.37
HMM-EM 20 0.43 2.28 0.48 4.54 57.31 51.16 64.66 61.33 38.65 58.57
HMM-EM 50 0.36 1.83 0.58 4.92 56.56 48.03 63.84 58.02 39.30 58.84
HMM-VB 10 0.40 2.75 0.41 4.42 39.05 27.72 52.84 58.64 23.94 51.64
HMM-VB 20 0.40 2.63 0.43 4.65 37.60 33.77 55.97 40.30 30.36 51.53
HMM-VB 50 0.38 2.70 0.42 5.01 34.68 37.29 57.72 39.82 29.03 50.50
NE 10 0.34 2.74 0.40 4.32 28.80 20.70 50.60 32.70 26.20 48.90
NE 20 0.48 2.02 0.55 3.76 32.50 36.00 59.30 40.60 32.80 54.00
NEMorph10 10 0.44 2.46 0.47 3.74 29.03 25.99 53.80 34.58 26.98 48.72
NEMorph10 20 0.48 1.94 0.56 3.65 31.95 35.85 57.93 38.22 30.45 50.72
NEMorph10 50 0.47 1.24 0.72 3.60 31.07 36.29 57.76 39.28 31.50 52.83
NEMorph5 10 0.45 2.50 0.47 3.76 29.04 22.72 51.58 32.67 23.62 47.89
NEMorph5 20 0.44 2.02 0.56 3.80 31.94 24.17 52.43 32.90 22.41 47.17
NEMorph5 50 0.47 1.27 0.72 3.64 31.39 38.63 59.44 40.23 34.26 54.63
2HMM 10 0.38 2.78 0.41 4.55 31.63 36.35 58.87 44.97 28.43 49.32
2HMM 20 0.41 2.35 0.48 4.71 42.39 43.91 60.74 50.85 29.32 50.69
2HMM 50 0.37 1.92 0.58 5.11 41.18 49.94 64.87 57.84 39.24 59.14
SVD 10 0.31 3.07 0.34 4.99 37.77 27.64 49.56 36.46 20.74 45.52
SVD 20 0.33 2.73 0.40 4.99 37.17 30.14 51.66 37.66 22.24 46.25
SVD 50 0.34 2.37 0.47 5.18 36.87 37.66 56.49 52.83 22.50 46.52
SVD 100 0.34 2.03 0.53 5.37 45.46 41.68 58.83 64.20 20.81 44.36
SVD 200 0.32 1.72 0.59 5.59 61.90 34.79 52.25 59.93 22.66 42.30
Table 1: The performance of the taggers regarding both tag and grammar induction metrics on WSJ
sections 0-10, averaged over 10 runs. Bold indicates the result was within 10 percent of the best-scoring
induced system for a given metric.
rected and undirected accuracy. These are the to-
tal number of proposed edges that appear in the
gold tree divided by the total number of edges (the
number of words in the sentence). Directed accu-
racy gives credit to a proposed edge if it is in the
gold tree and is in the correct direction, while undi-
rected accuracy ignores the direction.
Klein and Manning (2004) also present a model
which combines CCM and DMV into a single
model, which we show as CCM+DMV. In their
experiments, this model performed better on both
the constituency and dependency induction tasks.
As with CCM, Klein and Manning (2004) simi-
larly evaluate the combined CCM+DMV system
using tags induced with the same method. Again
they find that overall bracketing F-score decreases
from 77.6 to 72.9 and directed dependency accu-
racy measures decreases from 47.5 to 42.3 when
switching to induced tags from gold. However for
each metric, the systems still do quite well with
induced tags.
As in the constituency case, Smith (2006)
presents several alternative estimation procedures
for DMV, which try to minimize the local maxi-
mum problems inherent in EM. It is thus possible
these methods might yield better performance for
the models when run off of induced tags.
4 Experiments
We induce tags with each system on the Penn Tree-
bank Wall Street Journal (Marcus et al, 1994), sec-
tions 0-10, which contain 20,260 sentences. We
vary the number of tags (10, 20, 50) and run each
system 10 times for a given setting. The result of
each run is used as the input to the CCM, DMV,
and CCM+DMV systems. While the tags are in-
duced from all sentences in the section, following
the practice in (Klein and Manning, 2002; Klein
and Manning, 2004), we remove punctuation, and
consider only sentences of length not greater than
10 in our grammar induction experiments. Tag-
gings are evaluated after punctuation is removed,
but before filtering for length.
To explore the relationship between tagging
metrics and the resulting performance of grammar
induction systems, we examine each pair of tag-
ging and grammar induction metrics. Consider the
following two examples: DMV directed accuracy
vs. H(T
G
|T
I
) (Figure 1), and CCM f-score vs.
variation of information (Figure 2). These were se-
lected because they have relatively high magnitude
?s. From these plots it is clear that although there
333
may be a slight correspondence, the relationships
are weak at best.
Each tagging and grammar induction metric
gives us a ranking over the set of taggings of the
data generated over the course of our experiments.
These are ordered from best to worst according to
the metric, so for instance H(T
G
|T
I
) would give
highest rank to its lowest value. We can com-
pare the two rankings using Kendall?s ? (see Lap-
ata (2006) for an overview), a nonparametric mea-
sure of correspondence for rankings. ? measures
the difference between the number of concordant
pairs (items the two rankings place in the same or-
der) and discordant pairs (those the rankings place
in opposite order), divided by the total number of
pairs. A value of 1 indicates the rankings have per-
fect correspondence, -1 indicates they are in the
opposite order, and 0 indicates they are indepen-
dent. The ? values are shown in Table 2. The
scatter-plot in Figure 1 shows the ? with the great-
est magnitude. However, we can see that even
these rankings have barely any relationship.
An objection one might raise is that the lack of
correspondence reflects poorly not on these met-
rics, but upon the grammar induction systems we
use to evaluate them. There might be something
about these models in particular which yields these
low correlations. For instance these grammar in-
ducers all estimate their models using EM, which
can get caught easily in a local maximum.
To this possibility, we respond by pointing to
performance on gold tags, which is consistently
high for all grammar induction metrics. There is
clearly some property of the gold tags which is ex-
ploited by the grammar induction systems even in
the absence of better estimation procedures. This
property is not reflected in the tagging metrics.
The scores for each system for tagging and
grammar induction, averaged over the 10 runs, are
shown in Table 1. Additionally, we included runs
of the SVD-tagger for 100 and 200 tags, since run-
ning this system is still practical with these num-
bers of tags. The Ney-Essen with Morphology tag-
gers perform at or near the top on the various tag-
ging metrics, but not well on the grammar induc-
tion tasks on average. HMM-EM seems to perform
on average quite well on all the grammar induction
tasks, while the SVD-based systems yield the top
bracketing F-scores, making use of larger numbers
of tags.
Grammar Induction Metrics
Tagging CCM CCM+DMV DMV
Metrics UF1 DA UA UF1 DA UA
1-to-1 -0.22 -0.04 0.05 -0.13 0.13 0.12
M-to-1 -0.09 0.17 0.24 0.03 0.26 0.25
H(T
G
|T
I
) 0.01 0.21 0.27 0.07 0.29 0.28
VI -0.25 -0.17 -0.06 -0.20 0.07 0.07
Table 2: Kendall?s ? , between tag and grammar
induction criteria.
4.1 Supervised Experiments
One question we might ask is whether these tag-
ging metrics capture information relevant to any
parsing task. We explored this by experimenting
with a supervised parser, training off trees where
the gold parts-of-speech have been removed and
replaced with induced tags. Our expectation was
that the brackets, the head propagation paths, and
the phrasal categories in the training trees would
be sufficient to overcome any loss in information
that the gold tags might provide. Additionally it
was possible the induced tags would ignore rare
parts-of-speech such as FW, and make better use
of the available tags, perhaps using new distribu-
tional clues not in the original tags.
To this end we modified the Charniak Parser
(Charniak, 2000) to train off induced parts-of-
speech. The Charniak parser is a lexicalized PCFG
parser for which the part-of-speech of a head word
is a key aspect of its model. During training, the
head-paths from the gold part-of-speech tags are
retained, but we replace the tags themselves.
We ran experiments using the bitag HMM from
Section 2.2 trained using EM, as well as with the
Schu?tze SVD tagger from Section 2.1. The parser
was trained on sections 2-21 of the Penn Treebank
for training and section 24 was used for evaluation.
As before we calculated ? scores between each
tagging metric and supervised f-score. Unlike the
unsupervised evaluation where we used the metric
UF1, we use the standard EVALB calculation of
unlabeled f-score. The results are shown in Table
3.
The contrast with the unsupervised case is vast,
with very high ?s for both accuracy metrics. Con-
sider f-score vs. many-to-one, plotted in Figure 3.
The correspondence here is very clear: taggings
with high accuracy do actually reflect on better
parser performance. Note, however, that the corre-
spondence between the information theoretic mea-
sures and parsing performance is still rather weak.
334
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
 
15
20
25
30
35
40
45
50
55
D
M
V
 
D
A
NEMorph
HMM-VB
2HMM
NE
SVD
HMM-EM
Gold
Figure 1: DMV Directed Accuracy vs. H(T
G
|T
I
)
0 1 2 3 4 5 6
 VI
20
30
40
50
60
70
80
C
C
M
 
U
F
1
NEMorph
HMM-VB
2HMM
NE
SVD
HMM-EM
Gold
Figure 2: CCM fscore vs. tagging variation of in-
formation.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
 M-to-1
0.78
0.80
0.82
0.84
0.86
0.88
0.90
C
h
a
r
n
i
a
k
 
p
a
r
s
e
r
 
F
1
SVD 10 tags
SVD 50 tags
Gold
HMM-EMbitag 10 tags
HMM-EMbitag 50 tags
Figure 3: Supervised parsing f-score vs. tagging
many-to-one accuracy.
Tagging Metric Supervised F1
1-to-1 0.62
M-to-1 0.83
H(T
G
|T
I
) -0.19
VI 0.25
Table 3: Kendall?s ? , between tag induction cri-
teria and supervised parsing unlabeled bracketing
F-score.
Interestingly, parsing performance and speed
does degrade considerably when training off in-
duced tags. We are not sure what causes this. One
possibility is in the lexicalized stage of the parser,
where the probability of a head word is smoothed
primarily by its part-of-speech tag. This requires
that the tag be a good proxy for the syntactic role
of the head. In any case this warrants further in-
vestigation.
5 Conclusion and Future Work
In this work, we found that none of the most com-
mon part-of-speech tagging metrics bear a strong
relationship to good grammar induction perfor-
mance. Although our experiments only involve
English, the poor correspondence we find between
the various tagging metrics and grammar induc-
tion performance raises concerns about their re-
lationship more broadly. We additionally found
that while tagging accuracy measures do corre-
late with better supervised parsing, common infor-
mation theoretic ones do not strongly predict bet-
ter performance on either task. Furthermore, the
supervised experiments indicate that informative
part-of-speech tags are important for good parsing.
The next step is to explore better tagging met-
rics that correspond more strongly to better gram-
mar induction performance. A good metric should
use all the information we have, including the gold
trees, to evaluate. Finally, we should explore gram-
mar induction schemes that do not rely on prior
parts-of-speech, instead learning them from raw
text at the same time as deeper structure.
Acknowledgments
We thank Dan Klein for his grammar induction
code, as well as Matt Lease and other members of
BLLIP for their feedback. This work was partially
supported by DARPA GALE contract HR0011-06-
2-0001 and NSF award 0631667.
335
References
Baum, L.E. 1972. An inequality and associated maxi-
mization techniques in statistical estimation of prob-
abilistic functions of Markov processes. Inequali-
ties, 3:1?8.
Biemann, Chris. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
Proceedings of the COLING/ACL 2006 Student Re-
search Workshop, pages 7?12, Sydney, Australia.
Bod, Rens. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of Coling/ACL 2006,
pages 865?872.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Ameri-
can Chapter of the ACL 2000, pages 132?139.
Clark, Alexander. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59?66,
Budapest, Hungary.
Cramer, Bart. 2007. Limitations of current grammar
induction algorithms. In Proceedings of the ACL
2007 Student Research Workshop, pages 43?48.
Dasgupta, Sajib and Vincent Ng. 2007. Unsupervised
part-of-speech acquisition for resource-scarce lan-
guages. In Proceedings of the EMNLP/CoNLL 2007,
pages 218?227.
Freitag, Dayne. 2004. Toward unsupervised whole-
corpus tagging. In Proceedings of Coling 2004,
pages 357?363, Aug 23?Aug 27.
Goldwater, Sharon and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL 2007, pages
744?751.
Haghighi, Aria and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT/NAACL 2006, pages 320?327, New York, USA.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers? In Proceedings of the
EMNLP/CoNLL 2007, pages 296?305.
Klein, Dan and Christopher Manning. 2002. A gener-
ative constituent-context model for improved gram-
mar induction. In Proceedings of ACL 2002.
Klein, Dan and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, pages 478?485, Barcelona, Spain, July.
Lapata, Mirella. 2006. Automatic evaluation of infor-
mation ordering: Kendall?s tau. Computational Lin-
guistics, 32(4):1?14.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating Predicate
Argument Structure. In Proceedings of the 1994
ARPA Human Language Technology Workshop.
Meila?, Marina. 2003. Comparing clusterings. Pro-
ceedings of the Conference on Computational Learn-
ing Theory (COLT).
Merialdo, Bernard. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):154?172.
Ney, Herman, Ute Essen, and Renhard Knesser. 1994.
On structuring probabilistic dependencies in stochas-
tic language modelling. Computer Speech and Lan-
guage, 8:1?38.
Schu?tze, Hinrich. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th conference of the
EACL, pages 141?148.
Seginer, Yoav. 2007. Fast unsupervised incremental
parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384?391, Prague, Czech Republic.
Smith, Noah A. and Jason Eisner. 2004. Anneal-
ing techniques for unsupervised statistical language
learning. In Proceedings of ACL 2004, pages 487?
494, Barcelona, Spain.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL 2005, pages 354?362,
Ann Arbor, Michigan.
Smith, Noah A. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Nat-
ural Language Text. Ph.D. thesis, Department of
Computer Science, Johns Hopkins University, Octo-
ber.
Van Zaanen, Menno M. 2001. Bootstrapping Structure
into Language: Alignment-Based Learning. Ph.D.
thesis, University of Leeds, September.
336
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 101?109,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Unsupervised Dependency Parsing
with Richer Contexts and Smoothing
William P. Headden III, Mark Johnson, David McClosky
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw,mj,dmcc}@cs.brown.edu
Abstract
Unsupervised grammar induction models tend
to employ relatively simple models of syntax
when compared to their supervised counter-
parts. Traditionally, the unsupervised mod-
els have been kept simple due to tractabil-
ity and data sparsity concerns. In this paper,
we introduce basic valence frames and lexi-
cal information into an unsupervised depen-
dency grammar inducer and show how this
additional information can be leveraged via
smoothing. Our model produces state-of-the-
art results on the task of unsupervised gram-
mar induction, improving over the best previ-
ous work by almost 10 percentage points.
1 Introduction
The last decade has seen great strides in statisti-
cal natural language parsing. Supervised and semi-
supervised methods now provide highly accurate
parsers for a number of languages, but require train-
ing from corpora hand-annotated with parse trees.
Unfortunately, manually annotating corpora with
parse trees is expensive and time consuming so for
languages and domains with minimal resources it is
valuable to study methods for parsing without re-
quiring annotated sentences.
In this work, we focus on unsupervised depen-
dency parsing. Our goal is to produce a directed
graph of dependency relations (e.g. Figure 1) where
each edge indicates a head-argument relation. Since
the task is unsupervised, we are not given any ex-
amples of correct dependency graphs and only take
words and their parts of speech as input. Most
of the recent work in this area (Smith, 2006; Co-
hen et al, 2008) has focused on variants of the
The big dog barks
Figure 1: Example dependency parse.
Dependency Model with Valence (DMV) by Klein
and Manning (2004). DMV was the first unsu-
pervised dependency grammar induction system to
achieve accuracy above a right-branching baseline.
However, DMV is not able to capture some of the
more complex aspects of language. Borrowing some
ideas from the supervised parsing literature, we
present two new models: Extended Valence Gram-
mar (EVG) and its lexicalized extension (L-EVG).
The primary difference between EVG and DMV is
that DMV uses valence information to determine the
number of arguments a head takes but not their cat-
egories. In contrast, EVG allows different distri-
butions over arguments for different valence slots.
L-EVG extends EVG by conditioning on lexical in-
formation as well. This allows L-EVG to potentially
capture subcategorizations. The downside of adding
additional conditioning events is that we introduce
data sparsity problems. Incorporating more valence
and lexical information increases the number of pa-
rameters to estimate. A common solution to data
sparsity in supervised parsing is to add smoothing.
We show that smoothing can be employed in an un-
supervised fashion as well, and show that mixing
DMV, EVG, and L-EVG together produces state-of-
the-art results on this task. To our knowledge, this is
the first time that grammars with differing levels of
detail have been successfully combined for unsuper-
vised dependency parsing.
A brief overview of the paper follows. In Section
2, we discuss the relevant background. Section 3
presents how we will extend DMV with additional
101
features. We describe smoothing in an unsupervised
context in Section 4. In Section 5, we discuss search
issues. We present our experiments in Section 6 and
conclude in Section 7.
2 Background
In this paper, the observed variables will be a corpus
of n sentences of text s = s1 . . . sn, and for each
word sij an associated part-of-speech ?ij . We denote
the set of all words as Vw and the set of all parts-of-
speech as V? . The hidden variables are parse trees
t = t1 . . . tn and parameters ?? which specify a dis-
tribution over t. A dependency tree ti is a directed
acyclic graph whose nodes are the words in si. The
graph has a single incoming edge for each word in
each sentence, except one called the root of ti. An
edge from word i to word j means that word j is
an argument of word i or alternatively, word i is the
head of word j. Note that each word token may be
the argument of at most one head, but a head may
have several arguments.
If parse tree ti can be drawn on a plane above the
sentence with no crossing edges, it is called projec-
tive. Otherwise it is nonprojective. As in previous
work, we restrict ourselves to projective dependency
trees. The dependency models in this paper will be
formulated as a particular kind of Probabilistic Con-
text Free Grammar (PCFG), described below.
2.1 Tied Probabilistic Context Free Grammars
In order to perform smoothing, we will find useful a
class of PCFGs in which the probabilities of certain
rules are required to be the same. This will allow
us to make independence assumptions for smooth-
ing purposes without losing information, by giving
analogous rules the same probability.
Let G = (N ,T , S,R, ?) be a Probabilistic Con-
text Free Grammar with nonterminal symbols N ,
terminal symbols T , start symbol S ? N , set of
productions R of the form N ? ?, N ? N , ? ?
(N ? T )?. Let RN indicate the subset of R whose
left-hand sides are N . ? is a vector of length |R|, in-
dexed by productions N ? ? ? R. ?N?? specifies
the probability that N rewrites to ?. We will let ?N
indicate the subvector of ? corresponding to RN .
A tied PCFG constrains a PCFG G with a tying
relation, which is an equivalence relation over rules
that satisfies the following properties:
1. Tied rules have the same probability.
2. Rules expanding the same nonterminal are
never tied.
3. If N1 ? ?1 and N2 ? ?2 are tied then the ty-
ing relation defines a one-to-one mapping be-
tween rules in RN1 and RN2 , and we say that
N1 and N2 are tied nonterminals.
As we see below, we can estimate tied PCFGs using
standard techniques. Clearly, the tying relation also
defines an equivalence class over nonterminals. The
tying relation allows us to formulate the distribu-
tions over trees in terms of rule equivalence classes
and nonterminal equivalence classes. Suppose R? is
the set of rule equivalence classes and N? is the set
of nonterminal equivalence classes. Since all rules
in an equivalence class r? have the same probability
(condition 1), and since all the nonterminals in an
equivalence class N? ? N? have the same distribu-
tion over rule equivalence classes (condition 1 and
3), we can define the set of rule equivalence classes
R?N? associated with a nonterminal equivalence class
N? , and a vector ?? of probabilities, indexed by rule
equivalence classes r? ? R? . ??N? refers to the sub-
vector of ?? associated with nonterminal equivalence
class N? , indexed by r? ? R?N? . Since rules in the
same equivalence class have the same probability,
we have that for each r ? r?, ?r = ??r?.
Let f(t, r) denote the number of times rule r ap-
pears in tree t, and let f(t, r?) = ?r?r? f(t, r). We
see that the complete data likelihood is
P (s, t|?) = ?
r??R?
?
r?r?
?f(t,r)r =
?
r??R?
??f(t,r?)r?
That is, the likelihood is a product of multinomi-
als, one for each nonterminal equivalence class, and
there are no constraints placed on the parameters of
these multinomials besides being positive and sum-
ming to one. This means that all the standard es-
timation methods (e.g. Expectation Maximization,
Variational Bayes) extend directly to tied PCFGs.
Maximum likelihood estimation provides a point
estimate of ??. However, often we want to incorpo-
rate information about ?? by modeling its prior distri-
bution. As a prior, for each N? ? N? we will specify a
102
Dirichlet distribution over ??N? with hyperparameters
?N? . The Dirichlet has the density function:
P (??N? |?N? ) =
?(?r??R?N? ?r?)?
r??R?N? ?(?r?)
?
r??R?N?
???r??1r? ,
Thus the prior over ?? is a product of Dirichlets,which
is conjugate to the PCFG likelihood function (John-
son et al, 2007). That is, the posterior P (??|s, t, ?)
is also a product of Dirichlets, also factoring into a
Dirichlet for each nonterminal N? , where the param-
eters ?r? are augmented by the number of times rule
r? is observed in tree t:
P (??|s, t, ?) ? P (s, t|??)P (??|?)
?
?
r??R?
??f(t,r?)+?r??1r?
We can see that ?r? acts as a pseudocount of the num-
ber of times r? is observed prior to t.
To make use of this prior, we use the Variational
Bayes (VB) technique for PCFGs with Dirichlet Pri-
ors presented by Kurihara and Sato (2004). VB es-
timates a distribution over ??. In contrast, Expec-
tation Maximization estimates merely a point esti-
mate of ??. In VB, one estimates Q(t, ??), called
the variational distribution, which approximates the
posterior distribution P (t, ??|s, ?) by minimizing the
KL divergence of P from Q. Minimizing the KL
divergence, it turns out, is equivalent to maximiz-
ing a lower bound F of the log marginal likelihood
log P (s|?).
log P (s|?) ? ?
t
?
??
Q(t, ??) log P (s, t, ??|?)Q(t, ??) = F
The negative of the lower bound, ?F , is sometimes
called the free energy.
As is typical in variational approaches, Kuri-
hara and Sato (2004) make certain independence as-
sumptions about the hidden variables in the vari-
ational posterior, which will make estimating it
simpler. It factors Q(t, ??) = Q(t)Q(??) =?n
i=1 Qi(ti)
?
N??N? Q(??N? ). The goal is to recover
Q(??), the estimate of the posterior distribution over
parameters and Q(t), the estimate of the posterior
distribution over trees. Finding a local maximum of
F is done via an alternating maximization of Q(??)
and Q(t). Kurihara and Sato (2004) show that each
Q(??N? ) is a Dirichlet distribution with parameters
??r = ?r + EQ(t)f(t, r).
2.2 Split-head Bilexical CFGs
In the sections that follow, we frame various de-
pendency models as a particular variety of CFGs
known as split-head bilexical CFGs (Eisner and
Satta, 1999). These allow us to use the fast Eisner
and Satta (1999) parsing algorithm to compute the
expectations required by VB in O(m3) time (Eis-
ner and Blatz, 2007; Johnson, 2007) where m is the
length of the sentence.1
In the split-head bilexical CFG framework, each
nonterminal in the grammar is annotated with a ter-
minal symbol. For dependency grammars, these
annotations correspond to words and/or parts-of-
speech. Additionally, split-head bilexical CFGs re-
quire that each word sij in sentence si is represented
in a split form by two terminals called its left part
sijL and right part sijR. The set of these parts con-
stitutes the terminal symbols of the grammar. This
split-head property relates to a particular type of de-
pendency grammar in which the left and right depen-
dents of a head are generated independently. Note
that like CFGs, split-head bilexical CFGs can be
made probabilistic.
2.3 Dependency Model with Valence
The most successful recent work on dependency
induction has focused on the Dependency Model
with Valence (DMV) by Klein and Manning (2004).
DMV is a generative model in which the head of
the sentence is generated and then each head recur-
sively generates its left and right dependents. The
arguments of head H in direction d are generated
by repeatedly deciding whether to generate another
new argument or to stop and then generating the
argument if required. The probability of deciding
whether to generate another argument is conditioned
on H , d and whether this would be the first argument
(this is the sense in which it models valence). When
DMV generates an argument, the part-of-speech of
that argument A is generated given H and d.
1Efficiently parsable versions of split-head bilexical CFGs
for the models described in this paper can be derived using the
fold-unfold grammar transform (Eisner and Blatz, 2007; John-
son, 2007).
103
Rule Description
S ? YH Select H as root
YH ? LH RH Move to split-head representation
LH ? HL STOP | dir = L, head = H,val = 0
LH ? L1H CONT | dir = L, head = H, val = 0
L?H ? HL STOP | dir = L, head = H,val = 1
L?H ? L1H CONT | dir = L, head = H, val = 1
L1H ? YA L?H Arg A | dir = L, head = H
Figure 2: Rule schema for DMV. For brevity, we omit
the portion of the grammar that handles the right argu-
ments since they are symmetric to the left (all rules are
the same except for the attachment rule where the RHS is
reversed). val ? {0, 1} indicates whether we have made
any attachments.
The grammar schema for this model is shown in
Figure 2. The first rule generates the root of the sen-
tence. Note that these rules are for ?H,A ? V? so
there is an instance of the first schema rule for each
part-of-speech. YH splits words into their left and
right components. LH encodes the stopping deci-
sion given that we have not generated any arguments
so far. L?H encodes the same decision after generat-
ing one or more arguments. L1H represents the distri-
bution over left attachments. To extract dependency
relations from these parse trees, we scan for attach-
ment rules (e.g., L1H ? YA L?H) and record that
A depends on H . The schema omits the rules for
right arguments since they are symmetric. We show
a parse of ?The big dog barks? in Figure 3.2
Much of the extensions to this work have fo-
cused on estimation procedures. Klein and Manning
(2004) use Expectation Maximization to estimate
the model parameters. Smith and Eisner (2005) and
Smith (2006) investigate using Contrastive Estima-
tion to estimate DMV. Contrastive Estimation max-
imizes the conditional probability of the observed
sentences given a neighborhood of similar unseen
sequences. The results of this approach vary widely
based on regularization and neighborhood, but often
outperforms EM.
2Note that our examples use words as leaf nodes but in our
unlexicalized models, the leaf nodes are in fact parts-of-speech.
S
Ybarks
Lbarks
L1barks
Ydog
Ldog
L1dog
YThe
LThe
TheL
RThe
TheR
L?dog
L1dog
Ybig
Lbig
bigL
Rbig
bigR
L?dog
dogL
Rdog
dogR
L?barks
barksL
Rbarks
barksR
Figure 3: DMV split-head bilexical CFG parse of ?The
big dog barks.?
Smith (2006) also investigates two techniques for
maximizing likelihood while incorporating the lo-
cality bias encoded in the harmonic initializer for
DMV. One technique, skewed deterministic anneal-
ing, ameliorates the local maximum problem by flat-
tening the likelihood and adding a bias towards the
Klein and Manning initializer, which is decreased
during learning. The second technique is structural
annealing (Smith and Eisner, 2006; Smith, 2006)
which penalizes long dependencies initially, grad-
ually weakening the penalty during estimation. If
hand-annotated dependencies on a held-out set are
available for parameter selection, this performs far
better than EM; however, performing parameter se-
lection on a held-out set without the use of gold de-
pendencies does not perform as well.
Cohen et al (2008) investigate using Bayesian
Priors with DMV. The two priors they use are the
Dirichlet (which we use here) and the Logistic Nor-
mal prior, which allows the model to capture correla-
tions between different distributions. They initialize
using the harmonic initializer of Klein and Manning
(2004). They find that the Logistic Normal distri-
bution performs much better than the Dirichlet with
this initialization scheme.
Cohen and Smith (2009), investigate (concur-
104
Rule Description
S ? YH Select H as root
YH ? LH RH Move to split-head representation
LH ? HL STOP | dir = L, head = H,val = 0
LH ? L?H CONT | dir = L, head = H, val = 0
L?H ? L1H STOP | dir = L, head = H,val = 1
L?H ? L2H CONT | dir = L, head = H, val = 1
L2H ? YA L?H Arg A | dir = L, head = H,val = 1
L1H ? YA HL Arg A | dir = L, head = H,val = 0
Figure 4: Extended Valence Grammar schema. As be-
fore, we omit rules involving the right parts of words. In
this case, val ? {0, 1} indicates whether we are generat-
ing the nearest argument (0) or not (1).
rently with our work) an extension of this, the
Shared Logistic Normal prior, which allows differ-
ent PCFG rule distributions to share components.
They use this machinery to investigate smoothing
the attachment distributions for (nouns/verbs), and
for learning using multiple languages.
3 Enriched Contexts
DMV models the distribution over arguments iden-
tically without regard to their order. Instead, we
propose to distinguish the distribution over the argu-
ment nearest the head from the distribution of sub-
sequent arguments. 3
Consider the following changes to the DMV
grammar (results shown in Figure 4). First, we will
introduce the rule L2H ? YA L?H to denote the deci-
sion of what argument to generate for positions not
nearest to the head. Next, instead of having L?H ex-
pand to HL or L1H , we will expand it to L1H (attach
to nearest argument and stop) or L2H (attach to non-
nearest argument and continue). We call this the Ex-
tended Valence Grammar (EVG).
As a concrete example, consider the phrase ?the
big hungry dog? (Figure 5). We would expect that
distribution over the nearest left argument for ?dog?
to be different than farther left arguments. The fig-
3McClosky (2008) explores this idea further in an un-
smoothed grammar.
.
.
.
Ldog
L1dog
YThe
TheL TheR
L?dog
L1dog
Ybig
bigL bigR
L?dog
dogL
.
.
.
Ldog
L?dog
L2dog
YThe
TheL TheR
L?dog
L1dog
Ybig
bigL bigR
dogL
Figure 5: An example of moving from DMV to EVG
for a fragment of ?The big dog.? Boxed nodes indicate
changes. The key difference is that EVG distinguishes
between the distributions over the argument nearest the
head (big) from arguments farther away (The).
ure shows that EVG allows these two distributions to
be different (nonterminals L2dog and L1dog) whereas
DMV forces them to be equivalent (both use L1dog as
the nonterminal).
3.1 Lexicalization
All of the probabilistic models discussed thus far
have incorporated only part-of-speech information
(see Footnote 2). In supervised parsing of both de-
pendencies and constituency, lexical information is
critical (Collins, 1999). We incorporate lexical in-
formation into EVG (henceforth L-EVG) by extend-
ing the distributions over argument parts-of-speech
A to condition on the head word h in addition to the
head part-of-speech H , direction d and argument po-
sition v. The argument word a distribution is merely
conditioned on part-of-speech A; we leave refining
this model to future work.
In order to incorporate lexicalization, we extend
the EVG CFG to allow the nonterminals to be anno-
tated with both the word and part-of-speech of the
head. We first remove the old rules YH ? LH RH
for each H ? V? . Then we mark each nonter-
minal which is annotated with a part-of-speech as
also annotated with its head, with a single excep-
tion: YH . We add a new nonterminal YH,h for each
H ? V? , h ? Vw, and the rules YH ? YH,h and
YH,h ? LH,h RH,h. The rule YH ? YH,h cor-
responds to selecting the word, given its part-of-
speech.
105
4 Smoothing
In supervised estimation one common smoothing
technique is linear interpolation, (Jelinek, 1997).
This section explains how linear interpolation can
be represented using a PCFG with tied rule proba-
bilities, and how one might estimate smoothing pa-
rameters in an unsupervised framework.
In many probabilistic models it is common to esti-
mate the distribution of some event x conditioned on
some set of context information P (x|N(1) . . . N(k))
by smoothing it with less complicated condi-
tional distributions. Using linear interpolation
we model P (x|N(1) . . . N(k)) as a weighted aver-
age of two distributions ?1P1(x|N(1), . . . , N(k)) +
?2P2(x|N(1), . . . , N(k?1)), where the distribution
P2 makes an independence assumption by dropping
the conditioning event N(k).
In a PCFG a nonterminal N can encode a collec-
tion of conditioning events N(1) . . . N(k), and ?N de-
termines a distribution conditioned on N(1) . . . N(k)
over events represented by the rules r ? RN . For
example, in EVG the nonterminal L1NN encodes
three separate pieces of conditioning information:
the direction d = left , the head part-of-speech
H = NN , and the argument position v = 0;
?L1NN?YJJ NNL represents the probability of gener-
ating JJ as the first left argument of NN . Sup-
pose in EVG we are interested in smoothing P (A |
d,H, v) with a component that excludes the head
conditioning event. Using linear interpolation, this
would be:
P (A | d,H, v) = ?1P1(A | d,H, v)+?2P2(A | d, v)
We will estimate PCFG rules with linearly interpo-
lated probabilities by creating a tied PCFG which
is extended by adding rules that select between the
main distribution P1 and the backoff distribution P2,
and also rules that correspond to draws from those
distributions. We will make use of tied rule proba-
bilities to make the independence assumption in the
backoff distribution.
We still use the original grammar to parse the sen-
tence. However, we estimate the parameters in the
extended grammar and then translate them back into
the original grammar for parsing.
More formally, suppose B ? N is a set of non-
terminals (called the backoff set) with conditioning
events N(1) . . . N(k?1) in common (differing in a
conditioning event N(k)), and with rule sets of the
same cardinality. If G is our model?s PCFG, we can
define a new tied PCFG G? = (N ?,T , S,R?, ?),
where N ? = N ? {N b? | N ? B, ? ? {1, 2}},
meaning for each nonterminal N in the backoff
set we add two nonterminals N b1 , N b2 represent-
ing each distribution P1 and P2. The new rule
set R? = (?N?N ?R?N ) where for all N ? B
rule set R?N =
{
N ? N b? | ? ? {1, 2}}, mean-
ing at N in G? we decide which distribution P1, P2
to use; and for N ? B and ? ? {1, 2} ,
R?Nb? =
{
N b? ? ? | N ? ? ? RN
}
indicating a
draw from distribution P?. For nonterminals N 6? B,
R?N = RN . Finally, for each N,M ? B we
specify a tying relation between the rules in R?Nb2
and R?Mb2 , grouping together analogous rules. This
has the effect of making an independence assump-
tion about P2, namely that it ignores the condition-
ing event N(k), drawing from a common distribution
each time a nonterminal N b2 is rewritten.
For example, in EVG to smooth P (A = DT |
d = left ,H = NN , v = 0) with P2(A = DT |
d = left , v = 0) we define the backoff set to
be
{
L1H | H ? V?
}
. In the extended grammar we
define the tying relation to form rule equivalence
classes by the argument they generate, i.e. for each
argument A ? V? , we have a rule equivalence class{
L1b2H ? YA HL | H ? V?
}
.
We can see that in grammar G? each N ? B even-
tually ends up rewriting to one of N ?s expansions ?
in G. There are two indirect paths, one through N b1
and one through N b2 . Thus this defines the proba-
bility of N ? ? in G, ?N??, as the probability of
rewriting N as ? in G? via N b1 and N b2 . That is:
?N?? = ?N?Nb1?Nb1?? + ?N?Nb2?Nb2??
The example in Figure 6 shows the probability that
L1dog rewrites to Ybig dogL in grammar G.
Typically when smoothing we need to incorporate
the prior knowledge that conditioning events that
have been seen fewer times should be more strongly
smoothed. We accomplish this by setting the Dirich-
let hyperparameters for each N ? N b1 , N ? N b2
decision to (K, 2K), where K = |RNb1 | is the num-
ber of rewrite rules for A. This ensures that the
model will only start to ignore the backoff distribu-
106
PG
0
B
B
@
L1dog
Ybig dogL
1
C
C
A
= PG?
0
B
B
B
B
B
B
B
@
L1dog
L1b1dog
Ybig dogL
1
C
C
C
C
C
C
C
A
+ PG?
0
B
B
B
B
B
B
B
@
L1dog
L1b2dog
Ybig dogL
1
C
C
C
C
C
C
C
A
Figure 6: Using linear interpolation to smooth L1dog ?
Ybig dogL: The first component represents the distri-
bution fully conditioned on head dog, while the second
component represents the distribution ignoring the head
conditioning event. This later is accomplished by tying
the rule L1b2dog ? Ybig dogL to, for instance, L1b2cat ?
Ybig catL, L1b2fish ? Ybig fishL etc.
tion after having seen a sufficiently large number of
training examples. 4
4.1 Smoothed Dependency Models
Our first experiments examine smoothing the dis-
tributions over an argument in the DMV and EVG
models. In DMV we smooth the probability of argu-
ment A given head part-of-speech H and direction d
with a distribution that ignores H . In EVG, which
conditions on H , d and argument position v we back
off two ways. The first is to ignore v and use back-
off conditioning event H, d. This yields a backoff
distribution with the same conditioning information
as the argument distribution from DMV. We call this
EVG smoothed-skip-val.
The second possibility is to have the backoff
distribution ignore the head part-of-speech H and
use backoff conditioning event v, d. This assumes
that arguments share a common distribution across
heads. We call this EVG smoothed-skip-head. As
we see below, backing off by ignoring the part-of-
speech of the head H worked better than ignoring
the argument position v.
For L-EVG we smooth the argument part-of-
speech distribution (conditioned on the head word)
with the unlexicalized EVG smoothed-skip-head
model.
5 Initialization and Search issues
Klein and Manning (2004) strongly emphasize the
importance of smart initialization in getting good
performance from DMV. The likelihood function is
full of local maxima and different initial parameter
values yield vastly different quality solutions. They
offer what they call a ?harmonic initializer? which
4We set the other Dirichlet hyperparameters to 1.
initializes the attachment probabilities to favor ar-
guments that appear more closely in the data. This
starts EM in a state preferring shorter attachments.
Since our goal is to expand the model to incor-
porate lexical information, we want an initializa-
tion scheme which does not depend on the details
of DMV. The method we use is to create M sets of
B random initial settings and to run VB some small
number of iterations (40 in all our experiments) for
each initial setting. For each of the M sets, the
model with the best free energy of the B runs is
then run out until convergence (as measured by like-
lihood of a held-out data set); the other models are
pruned away. In this paper we use B = 20 and
M = 50.
For the bth setting, we draw a random sample
from the prior ??(b). We set the initial Q(t) =
P (t|s, ??(b)) which can be calculated using the
Expectation-Maximization E-Step. Q(??) is then ini-
tialized using the standard VB M-step.
For the Lexicalized-EVG, we modify this proce-
dure slightly, by first running MB smoothed EVG
models for 40 iterations each and selecting the best
model in each cohort as before; each L-EVG dis-
tribution is initialized from its corresponding EVG
distribution. The new P (A|h,H, d, v) distributions
are set initially to their corresponding P (A|H, d, v)
values.
6 Results
We trained on the standard Penn Treebank WSJ cor-
pus (Marcus et al, 1993). Following Klein and Man-
ning (2002), sentences longer than 10 words after
removing punctuation are ignored. We refer to this
variant as WSJ10. Following Cohen et al (2008),
we train on sections 2-21, used 22 as a held-out de-
velopment corpus, and present results evaluated on
section 23. The models were all trained using Varia-
tional Bayes, and initialized as described in Section
5. To evaluate, we follow Cohen et al (2008) in us-
ing the mean of the variational posterior Dirichlets
as a point estimate ???. For the unsmoothed models
we decode by selecting the Viterbi parse given ???, or
argmaxtP (t|s, ???).
For the smoothed models we find the Viterbi parse
of the unsmoothed CFG, but use the smoothed prob-
abilities. We evaluate against the gold standard
107
Model Variant Dir. Acc.
DMV harmonic init 46.9*
DMV random init 55.7 (8.0)
DMV log normal-families 59.4*
DMV shared log normal-families 62.4?
DMV smoothed 61.2 (1.2)
EVG random init 53.3 (7.1)
EVG smoothed-skip-val 62.1 (1.9)
EVG smoothed-skip-head 65.0 (5.7)
L-EVG smoothed 68.8 (4.5)
Table 1: Directed accuracy (DA) for WSJ10, section 23.
*,? indicate results reported by Cohen et al (2008), Co-
hen and Smith (2009) respectively. Standard deviations
over 10 runs are given in parentheses
dependencies for section 23, which were extracted
from the phrase structure trees using the standard
rules by Yamada and Matsumoto (2003). We mea-
sure the percent accuracy of the directed dependency
edges. For the lexicalized model, we replaced all
words that were seen fewer than 100 times with
?UNK.? We ran each of our systems 10 times, and
report the average directed accuracy achieved. The
results are shown in Table 1. We compare to work
by Cohen et al (2008) and Cohen and Smith (2009).
Looking at Table 1, we can first of all see the
benefit of randomized initialization over the har-
monic initializer for DMV. We can also see a large
gain by adding smoothing to DMV, topping even
the logistic normal prior. The unsmoothed EVG ac-
tually performs worse than unsmoothed DMV, but
both smoothed versions improve even on smoothed
DMV. Adding lexical information (L-EVG) yields a
moderate further improvement.
As the greatest improvement comes from moving
to model EVG smoothed-skip-head, we show in Ta-
ble 2 the most probable arguments for each val, dir,
using the mean of the appropriate variational Dirich-
let. For d = right, v = 1, P (A|v, d) largely seems
to acts as a way of grouping together various verb
types, while for d = left, v = 0 the model finds
that nouns tend to act as the closest left argument.
Dir,Val Arg Prob Dir,Val Arg Prob
left, 0 NN 0.65 right, 0 NN 0.26
NNP 0.18 RB 0.23
DT 0.12 NNS 0.12
IN 0.11
left, 1 CC 0.35 right, 1 IN 0.78
RB 0.27
IN 0.18
Table 2: Most likely arguments given valence and direc-
tion, according to smoothing distributionP (arg|dir, val)
in EVG smoothed-skip-head model with lowest free en-
ergy.
7 Conclusion
We present a smoothing technique for unsupervised
PCFG estimation which allows us to explore more
sophisticated dependency grammars. Our method
combines linear interpolation with a Bayesian prior
that ensures the backoff distribution receives proba-
bility mass. Estimating the smoothed model requires
running the standard Variational Bayes on an ex-
tended PCFG. We used this technique to estimate a
series of dependency grammars which extend DMV
with additional valence and lexical information. We
found that both were helpful in learning English de-
pendency grammars. Our L-EVG model gives the
best reported accuracy to date on the WSJ10 corpus.
Future work includes using lexical information
more deeply in the model by conditioning argument
words and valence on the lexical head. We suspect
that successfully doing so will require using much
larger datasets. We would also like to explore us-
ing our smoothing technique in other models such
as HMMs. For instance, we could do unsupervised
HMM part-of-speech induction by smooth a tritag
model with a bitag model. Finally, we would like to
learn the parts-of-speech in our dependency model
from text and not rely on the gold-standard tags.
Acknowledgements
This research is based upon work supported by
National Science Foundation grants 0544127 and
0631667 and DARPA GALE contract HR0011-06-
2-0001. We thank members of BLLIP for their feed-
back.
108
References
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL-HLT 2009.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems 21.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Jason Eisner and John Blatz. 2007. Program transforma-
tions for optimization of parsing algorithms and other
weighted logic programs. In Proceedings of the 11th
Conference on Formal Grammar.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of ACL 1999.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL 2007.
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proceedings of ACL 2007.
Dan Klein and Christopher Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of ACL 2002.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, July.
Kenichi Kurihara and Taisuke Sato. 2004. An applica-
tion of the variational bayesian approach to probabilis-
tics context-free grammars. In IJCNLP 2004 Work-
shop Beyond Shallow Analyses.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky. 2008. Modeling valence effects in un-
supervised grammar induction. Technical Report CS-
09-01, Brown University, Providence, RI, USA.
Noah A. Smith and Jason Eisner. 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In International Joint Conference on Artificial
Intelligence Workshop on Grammatical Inference Ap-
plications.
Noah A. Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction.
In Proceedings of COLING-ACL 2006.
Noah A. Smith. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Natural
Language Text. Ph.D. thesis, Department of Computer
Science, Johns Hopkins University.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
In Proceedings of the International Workshop on Pars-
ing Technologies.
109
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301?307,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Phrasal Categories
William P. Headden III, Eugene Charniak and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw|ec|mj}@cs.brown.edu
Abstract
In this work we learn clusters of contex-
tual annotations for non-terminals in the
Penn Treebank. Perhaps the best way
to think about this problem is to contrast
our work with that of Klein and Man-
ning (2003). That research used tree-
transformations to create various gram-
mars with different contextual annotations
on the non-terminals. These grammars
were then used in conjunction with a CKY
parser. The authors explored the space
of different annotation combinations by
hand. Here we try to automate the process
? to learn the ?right? combination auto-
matically. Our results are not quite as good
as those carefully created by hand, but they
are close (84.8 vs 85.7).
1 Introduction and Previous Research
It is by now commonplace knowledge that accu-
rate syntactic parsing is not possible given only
a context-free grammar with standard Penn Tree-
bank (Marcus et al, 1993) labels (e.g., S, NP ,
etc.) (Charniak, 1996). Instead researchers
condition parsing decisions on many other fea-
tures, such as parent phrase-marker, and, fa-
mously, the lexical-head of the phrase (Mager-
man, 1995; Collins, 1996; Collins, 1997; Johnson,
1998; Charniak, 2000; Henderson, 2003; Klein
and Manning, 2003; Matsuzaki et al, 2005) (and
others).
One particularly perspicuous way to view the
use of extra conditioning information is that of
tree-transformation (Johnson, 1998; Klein and
Manning, 2003). Rather than imagining the parser
roaming around the tree for picking up the infor-
mation it needs, we rather relabel the nodes to di-
rectly encode this information. Thus rather than
have the parser ?look? to find out that, say, the
parent of some NP is an S, we simply relabel the
NP as an NP [S].
This viewpoint is even more compelling if one
does not intend to smooth the probabilities. For
example, consider p(NP ? PRN | NP [S]) If
we have no intention of backing off this probabil-
ity to p(NP ? PRN | NP ) we can treat NP [S]
as an uninterpreted phrasal category and run all
of the standard PCFG algorithms without change.
The result is a vastly simplified parser. This is ex-
actly what is done by Klein and Manning (2003).
Thus the ?phrasal categories? of our title refer
to these new, hybrid categories, such as NP [S].
We hope to learn which of these categories work
best given that they cannot be made too specific
because that would create sparse data problems.
The Klein and Manning (2003) parser is an un-
lexicalized PCFG with various carefully selected
context annotations. Their model uses some par-
ent annotations, and marks nodes which initiate or
in certain cases conclude unary productions. They
also propose linguistically motivated annotations
for several tags, including V P , IN , CC ,NP and
S. This results in a reasonably accurate unlexical-
ized PCFG parser.
The downside of this approach is that their fea-
tures are very specific, applying different annota-
tions to different treebank nonterminals. For in-
stance, they mark right-recursive NP s and not
V P s (i.e., an NP which is the right-most child
of another NP ). This is because data sparsity is-
sues preclude annotating the nodes in the treebank
too liberally. The goal of our work is to automate
the process a bit, by annotating with more general
features that apply broadly, and by learning clus-
301
ters of these annotations.
Mohri and Roark (2006) tackle this problem by
searching for what they call ?structural zeros?or
sets of events which are individually very likely,
but are unlikely to coincide. This is to be con-
trasted with sets of events that do not appear to-
gether simply because of sparse data. They con-
sider a variety of statistical tests to decide whether
a joint event is a structural zero. They mark the
highest scoring nonterminals that are part of these
joint events in the treebank, and use the resulting
PCFG.
Coming to this problem from the standpoint of
tree transformation, we naturally view our work
as a descendent of Johnson (1998) and Klein and
Manning (2003). In retrospect, however, there are
perhaps even greater similarities to that of (Mager-
man, 1995; Henderson, 2003; Matsuzaki et al,
2005). Consider the approach of Matsuzaki et al
(2005). They posit a series of latent annotations
for each nonterminal, and learn a grammar using
an EM algorithm similar to the inside-outside al-
gorithm. Their approach, however, requires the
number of annotations to be specified ahead of
time, and assigns the same number of annotations
to each treebank nonterminal. We would like to
infer the number of annotations for each nonter-
minal automatically.
However, again in retrospect, it is in the work of
Magerman (1995) that we see the greatest similar-
ity. Rather than talking about clustering nodes, as
we do, Magerman creates a decision tree, but the
differences between clustering and decision trees
are small. Perhaps a more substantial difference
is that by not casting his problem as one of learn-
ing phrasal categories Magerman loses all of the
free PCFG technology that we can leverage. For
instance, Magerman must use heuristic search to
find his parses and incurs search errors because of
it. We use an efficient CKY algorithm to do ex-
haustive search in reasonable time.
Belz (2002) considers the problem in a man-
ner more similar to our approach. Beginning with
both a non-annotated grammar and a parent anno-
tated grammar, using a beam search they search
the space of grammars which can be attained via
merging nonterminals. They guide the search us-
ing the performance on parsing (and several other
tasks) of the grammar at each stage in the search.
In contrast, our approach explores the space of
grammars by starting with few nonterminals and
splitting them. We also consider a much wider
range of contextual information than just parent
phrase-markers.
2 Background
A PCFG is a tuple (V,M,?0, R, q : R ? [0, 1]),
where V is a set of terminal symbols; M = {?i}
is a set of nonterminal symbols; ?0 is a start or
root symbol; R is a set of productions of the form
?i ? ?, where ? is a sequence of terminals and
nonterminals; and q is a family of probability dis-
tributions over rules conditioned on each rule?s
left-hand side.
As in (Johnson, 1998) and (Klein and Man-
ning, 2003), we annotate the Penn treebank non-
terminals with various context information. Sup-
pose ? is a Treebank non-terminal. Let ? = ?[?]
denote the non-terminal category annotated with a
vector of context features ?. A PCFG is derived
from the trees in the usual manner, with produc-
tion rules taken directly from the annotated trees,
and the probability of an annotated rule q(? ?
?) = C(???)C(?) where C(? ? ?) and C(?) are the
number of observations of the production and its
left hand side, respectively.
We refer to the grammar resulting from extract-
ing annotated productions directly out of the tree-
bank as the base grammar.
Our goal is to partition the set of annotated non-
terminals into clusters ? = {?i}. Each possible
clustering corresponds to a PCFG, with the set of
non-terminals corresponding to the set of clusters.
The probability of a production under this PCFG
is
p(?i ? ?j?k) =
C(?i ? ?j?k)
C(?i)
where ?s ? ? are clusters of annotated non-
terminals and where:
C(?i ? ?j?k . . .) =
?
(?i,?j ,?k...)??i??j??k...C(?i ? ?j?k . . .)
We refer to the PCFG of some clustering as the
clustered grammar.
2.1 Features
Most of the features we use are fairly standard.
These include the label of the parent and grand-
parent of a node, its lexical head, and the part of
speech of the head.
Klein and Manning (2003) find marking non-
terminals which have unary rewrites to be helpful.
302
They also find useful annotating two preterminals
(DT ,RB) if they are the product of a unary pro-
duction. We generalize this via two width features:
the first marking a node with the number of non-
terminals to which it rewrites; the second marking
each preterminal with the width of its parent.
Another feature is the span of a nonterminal, or
the number of terminals it dominates, which we
normalize by dividing by the length of the sen-
tence. Hence preterminals have normalized spans
of 1/(length of the sentence), while the root has a
normalized span of 1.
Extending on the notion of a Base NP, intro-
duced by Collins (1996), we mark any nonter-
minal that dominates only preterminals as Base.
Collins inserts a unary NP over any base NPs with-
out NP parents. However, Klein and Manning
(2003) find that this hurts performance relative to
just marking the NPs, and so our Base feature does
not insert.
We have two features describing a node?s posi-
tion in the expansion of its parent. The first, which
we call the inside position, specifies the nonter-
minal?s position relative to the heir of its parent?s
head, (to the left or right) or whether the nontermi-
nal is the heir. (By ?heir? we mean the constituent
donates its head, e.g. the heir of an S is typically
the V P under the S.) The second feature, outside
position, specifies the nonterminal?s position rel-
ative to the boundary of the constituent: it is the
leftmost child, the rightmost child, or neither.
Related to this, we further noticed that several
of Klein & Manning?s (2003) features, such as
marking NP s as right recursive or possessive have
the property of annotating with the label of the
rightmost child (when they are NP and POS re-
spectively). We generalize this by marking all
nodes both with their rightmost child and (an anal-
ogous feature) leftmost child.
We also mark whether or not a node borders
the end of a sentence, save for ending punctuation.
(For instance, in this sentence, all the constituents
with the second ?marked? rightmost in their span
would be marked).
Another Klein and Manning (2003) feature we
try includes the temporal NP feature, where TMP
markings in the treebank are retained, and propa-
gated down the head inheritance path of the tree.
It is worth mentioning that all the features here
come directly from the treebank. For instance, the
part of speech of the head feature has values only
from the raw treebank tag set. When a preterminal
cluster is split, this assignment does not change the
value of this feature.
3 Clustering
The input to the clusterer is a set of annotated
grammar productions and counts. Our clustering
algorithm is a divisive one reminiscent of (Martin
et al, 1995). We start with a single cluster for each
Treebank nonterminal and one additional cluster
for intermediate nodes, which are described in sec-
tion 3.2.
The clustering method has two interleaved
parts: one in which candidate splits are generated,
and one in which we choose a candidate split to
enact.
For each of the initial clusters, we generate a
candidate split, and place that split in a prior-
ity queue. The priority queue is ordered by the
Bayesian Information Criterion (BIC), e.g.(Hastie
et al, 2003).
The BIC of a model M is defined as -2*(log
likelihood of the data according to M ) +dM*(log
number of observations). dM is the number of de-
grees of freedom in the model, which for a PCFG
is the number of productions minus the number
of nonterminals. Thus in this context BIC can be
thought of as optimizing the likelihood, but with a
penalty against grammars with many rules.
While the queue is nonempty, we remove a can-
didate split to reevaluate. Reevaluation is neces-
sary because, if there is a delay between when a
split is proposed and when a split is enacted, the
grammar used to score the split will have changed.
However, we suppose that the old score is close
enough to be a reasonable ordering measure for
the priority queue. If the reevaluated candidate is
no longer better than the second candidate on the
queue, we reinsert it and continue. However, if it
is still the best on the queue, and it improves the
model, we enact the split; otherwise it is discarded.
When a split is enacted, the old cluster is re-
moved from the set of nonterminals, and is re-
placed with the two new nonterminals of the split.
A candidate split for each of the two new clusters
is generated, and placed on the priority queue.
This process of reevaluation, enacting splits,
and generating new candidates continues until the
priority queue is empty of potential splits.
We select a candidate split of a particular cluster
as follows. For each context feature we generate
303
S^ROOT
NP^S
NNP^NP
Rex
CC^NP
and
NNP^NP
Ginger
VP^S
VBD^VP
ran
NP^VP
NN
home
Figure 1: A Parent annotated tree.
a potential nominee split. To do this we first par-
tition randomly the values for the feature into two
buckets. We then repeatedly try to move values
from one bucket to the other. If doing so results
in an improvement to the likelihood of the training
data, we keep the change, otherwise we reject it.
The swapping continues until moving no individ-
ual value results in an improvement in likelihood.
Suppose we have a grammar derived from a cor-
pus of a single tree, whose nodes have been anno-
tated with their parent as in Figure 1. The base
productions for this corpus are:
S[ROOT ] ? NP [S] V P [S] 1/1
V P [S] ? V BD[V P ] NP [V P ] 1/1
NP [S] ? NP [NP ] CC[NP ] NP [NP ] 1/1
NP [V P ] ? NN [NP ] 1/1
NP [NP ] ? NNP [NP ] 2/2
Suppose we are in the initial state, with a single
cluster for each treebank nonterminal. Consider
a potential split of the NP cluster on the par-
ent feature, which in this example has three val-
ues: S, V P , and NP . If the S and V P val-
ues are grouped together in the left bucket, and
the NP value is alone in the right bucket, we get
cluster nonterminals NPL = {NP [S], NP [V P ]}
and NPR = {NP [NP ]}. The resulting grammar
rules and their probabilities are:
S ? NPL V P 1/1
V P ? V BD NPL 1/1
NPL ? NPR CC NPR 1/2
NPL ? NN 1/2
NPR ? NNP 2/2
If however, V P is swapped to the right bucket
with NP , the rules become:
S ? NPL V P 1/1
V P ? V BD NPR 1/1
NPL ? NPR CC NPR 1/1
NPR ? NN 1/3
NPR ? NNP 2/3
The likelihood of the tree in Figure 1 is 1/4 under
the first grammar, but only 4/27 under the second.
Hence in this case we would reject the swap of V P
from the right to the left buckets.
The process of swapping continues until no im-
provement can be made by swapping a single
value.
The likelihood of the training data according to
the clustered grammar is
?
r?R
p(r)C(r)
for R the set of observed productions r = ?i ?
?j . . . in the clustered grammar. Notice that when
we are looking to split a cluster ?, only produc-
tions that contain the nonterminal ? will have
probabilities that change. To evaluate whether a
change increases the likelihood, we consider the
ratio between the likelihood of the new model, and
the likelihood of the old model.
Furthermore, when we move a value from one
bucket to another, only a fraction of the rules will
have their counts change. Suppose we are mov-
ing value x from the left bucket to the right when
splitting ?i. Let ?x ? ?i be the set of base nonter-
minals in ?i that have value x for the feature being
split upon. Only clustered rules that contain base
grammar rules which use nonterminals in ?x will
have their probability change. These observations
allow us to process only a relatively small number
of base grammar rules.
Once we have generated a potential nominee
split for each feature, we select the partitioning
which leads to the greatest improvement in the
BIC as the candidate split of this cluster. This can-
didate is placed on the priority queue.
One odd thing about the above is that in the lo-
cal search phase of the clustering we use likeli-
hood, while in the candidate selection phase we
use BIC. We tried both measures in each phase,
but found that this hybrid measure outperformed
using only one or the other.
3.1 Model Selection
Unfortunately, the grammar that results at the end
of the clustering process seems to overfit the train-
ing data. We resolve this by simply noting period-
ically the intermediate state of the grammar, and
using this grammar to parse a small tuning set (we
use the first 400 sentences of WSJ section 24, and
parse this every 50 times we enact a split). At the
conclusion of clustering, we select the grammar
304
AB C <D> E F
(a)
A
B [C,<D>,E,F]
C [<D>,E,F]
[<D>,E]
D E
F
(b)
Figure 2: (a) A production. (b) The production,
binarized.
with the highest f-score on this tuning set as the
final model.
3.2 Binarization
Since our experiments make use of a CKY
(Kasami, 1965) parser 1 we must modify the tree-
bank derived rules so that each expands to at most
two labels. We perform this in a manner simi-
lar to Klein and Manning (2003) and Matsuzaki
et al (2005) through the creation of intermediate
nodes, as in Figure 2. In this example, the nonter-
minal heir of A?s head is D, indicated in the figure
by marking D with angled brackets. The square
brackets indicate an intermediate node, and the la-
bels inside the brackets indicate that the node will
eventually be expanded into those labels.
Klein and Manning (2003) employ Collins?
(1999) horizontal markovization to desparsify
their intermediate nodes. This means that given
an intermediate node such as [C ?D?EF ] in Fig-
ure 2, we forget those labels which will not be ex-
panded past a certain horizon. Klein and Manning
(2003) use a horizon of two (or less, in some cases)
which means only the next two labels to be ex-
panded are retained. For instance in in this exam-
ple [C ?D?EF ] is markovized to [C ?D? . . . F ],
since C and F are the next two non-intermediate
labels.
Our mechanism lays out the unmarkovized in-
termediate rules in the same way, but we mostly
use our clustering scheme to reduce sparsity. We
do so by aligning the labels contained in the in-
termediate nodes in the order in which they would
be added when increasing the markovization hori-
1The implementation we use was created by Mark John-
son and used for the research in (Johnson, 1998). It is avail-
able at his homepage.
zon from zero to three. We also always keep
the heir label as a feature, following Klein and
Manning (2003). So for instance, [C ?D?EF ]
is represented as having Treebank label ?IN-
TERMEDIATE?, and would have feature vector
(D,C,F,E,D),while [?D?EF ] would have fea-
ture vector (D,F,E,D,?), where the first item
is the heir of the parent?s head. The ?-? in-
dicates that the fourth item to be expanded is
here non-existent. The clusterer would consider
each of these five features as for a single pos-
sible split. We also incorporate our other fea-
tures into the intermediate nodes in two ways.
Some features, such as the parent or grandpar-
ent, will be the same for all the labels in the in-
termediate node, and hence only need to be in-
cluded once. Others, such as the part of speech
of the head, may be different for each label. These
features we align with those of corresponding la-
bel in the Markov ordering. In our running ex-
ample, suppose each child node N has part of
speech of its head PN , and we have a parent fea-
ture. Our aligned intermediate feature vectors then
become (A,D,C, PC , F, PF , E, PE ,D, PD) and
(A,D,F, PF , E, PE ,D, PD,?,?). As these are
somewhat complicated, let us explain them by un-
packing the first, the vector for [C ?D?EF ]. Con-
sulting Figure 2 we see that its parent is A. We
have chosen to put parents first in the vector, thus
explaining (A, ...). Next comes the heir of the
constituent, D. This is followed by the first con-
stituent that is to be unpacked from the binarized
version, C , which in turn is followed by its head
part-of-speech PC , giving us (A,D,C, PC , ...).
We follow with the next non-terminal to be un-
packed from the binarized node and its head part-
of-speech, etc.
It might be fairly objected that this formulation
of binarization loses the information of whether a
label is to the left, right, or is the heir of the par-
ent?s head. This is solved by the inside position
feature, described in Section 2.1 which contains
exactly this information.
3.3 Smoothing
In order to ease comparison between our work
and that of Klein and Manning (2003), we follow
their lead in smoothing no production probabilities
save those going from preterminal to nonterminal.
Our smoothing mechanism runs roughly along the
lines of theirs.
305
LP LR F1 CB 0CB
Klein & Manning 86.3 85.1 85.7 1.31 57.2
Matsuzaki et al 86.1 86.0 86.1 1.39 58.3
This paper 84.8 84.8 84.8 1.47 57.1
Table 1: Parsing results on final test set (Section
23).
Run LP LR F1 CB 0CB
1 85.3 85.6 85.5 1.29 59.5
2 85.8 85.9 85.9 1.29 59.4
3 85.1 85.5 85.3 1.36 58.0
4 85.3 85.7 85.5 1.30 59.9
Table 2: Parsing results for grammars generated
using clusterer with different random seeds. All
numbers here are on the development test set (Sec-
tion 22).
Preterminal rules are smoothed as follows. We
consider several classes of unknown words, based
on capitalization, the presence of digits or hy-
phens, and the suffix. We estimate the probabil-
ity of a tag T given a word (or unknown class)
W , as p(T | W ) = C(T,W )+hp(T |unk)C(W )+h , where
p(T | unk) = C(T, unk)/C(unk) is the prob-
ability of the tag given any unknown word class.
In order to estimate counts of unknown classes,we
let the clusterer see every tree twice: once un-
modified, and once with the unknown class re-
placing each word seen less than five times. The
production probability p(W | T ) is then p(T |
W )p(W )/p(T ) where p(W ) and p(T ) are the re-
spective empirical distributions.
The clusterer does not use smoothed probabil-
ities in allocating annotated preterminals to clus-
ters, but simply the maximum likelihood estimates
as it does elsewhere. Smoothing is only used in the
parser.
4 Experiments
We trained our model on sections 2-21 of the Penn
Wall Street Journal Treebank. We used the first
400 sentences of section 24 for model selection.
Section 22 was used for testing during develop-
ment, while section 23 was used for the final eval-
uation.
5 Discussion
Our results are shown in Table 1. The first three
columns show the labeled precision, recall and f-
measure, respectively. The remaining two show
the number of crossing brackets per sentence,
and the percentage of sentences with no crossing
brackets.
Unfortunately, our model does not perform
quite as well as those of Klein and Manning (2003)
or Matsuzaki et al (2005). It is worth noting that
Matsuzaki?s grammar uses a different parse evalu-
ation scheme than Klein & Manning or we do.
We select the parse with the highest probability
according to the annotated grammar. Matsuzaki,
on the other hand, argues that the proper thing to
do is to find the most likely unannotated parse.
The probability of this parse is the sum over the
probabilities of all annotated parses that reduce
to that unannotated parse. Since calculating the
parse that maximizes this quantity is NP hard, they
try several approximations. One is what Klein &
Manning and we do. However, they have a better
performing approximation which is used in their
reported score. They do not report their score
on section 23 using the most-probable-annotated-
parse method. They do however compare the per-
formance of different methods using development
data, and find that their better approximation gives
an absolute improvement in f-measure in the .5-1
percent range. Hence it is probable that even with
their better method our grammar would not out-
perform theirs.
Table 2 shows the results on the development
test set (Section 22) for four different initial ran-
dom seeds. Recall that when splitting a cluster, the
initial partition of the base grammar nonterminals
is made randomly. The model from the second run
was used for parsing the final test set (Section 23)
in Table 1.
One interesting thing our method allows is for
us to examine which features turn out to be useful
in which contexts. We noted for each trereebank
nonterminal, and for each feature, how many times
that nonterminal was split on that feature, for the
grammar selected in the model selection stage. We
ran the clustering with these four different random
seeds.
We find that in particular, the clusterer only
found the head feature to be useful in very spe-
cific circumstances. It was used quite a bit to
split preterminals; but for phrasals it was only
used to split ADJP ,ADV P ,NP ,PP ,V P ,QP ,
and SBAR. The part of speech of the head was
only used to split NP and V P .
Furthermore, the grandparent tag appears to be
of importance primarily for V P and PP nonter-
306
minals, though it is used once out of the four runs
for NP s.
This indicates that perhaps lexical parsers might
be able to make do by only using lexical head and
grandparent information in very specific instances,
thereby shrinking the sizes of their models, and
speeding parsing. This warrants further investiga-
tion.
6 Conclusion
We have presented a scheme for automatically
discovering phrasal categories for parsing with a
standard CKY parser. The parser achieves 84.8%
precision-recall f-measure on the standard test-
section of the Penn WSJ-Treebank (section 23).
While this is not as accurate as the hand-tailored
grammar of Klein and Manning (2003), it is close,
and we believe there is room for improvement.
For starters, the particular clustering scheme is
only one of many. Our algorithm splits clus-
ters along particular features (e.g., parent, head-
part-of-speech, etc.). One alternative would be to
cluster simultaneously on all the features. It is
not obvious which scheme should be better, and
they could be quite different. Decisions like this
abound, and are worth exploring.
More radically, it is also possible to grow many
decision trees, and thus many alternative gram-
mars. We have been impressed by the success of
random-forest methods in language modeling (Xu
and Jelinek, 2004). In these methods many trees
(the forest) are grown, each trying to predict the
next word. The multiple trees together are much
more powerful than any one individually. The
same might be true for grammars.
Acknowledgement
The research presented here was funded in part by
DARPA GALE contract HR 0011-06-20001.
References
Anja Belz. 2002. Learning grammars for different
parsing tasks by partition search. In Proceedings of
the 19th international conference on Computational
Linguistics, pages 1?7.
Eugene Charniak. 1996. Tree-bank grammars. In
Proceedings of the Thirteenth National Conference
on Artificial Intelligence, pages 1031?1036. AAAI
Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139.
Michael J. Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In The Pro-
ceedings of the 34th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 184?191.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In The Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, The
University of Pennsylvania.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2003. The Elements of Statistical Learning.
Springer, New York.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of HLT-NAACL 2003, pages 25?31.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Tadao Kasami. 1965. An efficient recognition and syn-
tax algorithm for context-free languages. Technical
Report AF-CRL-65-758, Air Force Cambridge Re-
search Laboratory.
Dan Klein and Christopher Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In The Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 276?283.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Sven Martin, Jo?rg Liermann, and Hermann Ney. 1995.
Algorithms for bigram and trigram word cluster-
ing. In Proceedings of the European Conference
on Speech, Communication and Technology, pages
1253?1256.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 2005 Meeting of the Association
for Computational Linguistics.
Mehryar Mohri and Brian Roark. 2006. Effective self-
training for parsing. In Proceedings of HLT-NAACL
2006.
Peng Xu and Fred Jelinek. 2004. Random forests
in language modeling. In Proceedings of EMNLP
2004.
307
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 214?222, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes
Robert V. Lindsey
University of Colorado, Boulder
robert.lindsey@colorado.edu
William P. Headden III
Two Cassowaries Inc.
headdenw@twocassowaries.com
Michael J. Stipicevic
Google Inc.
stip@google.com
Abstract
Topic models traditionally rely on the bag-
of-words assumption. In data mining appli-
cations, this often results in end-users being
presented with inscrutable lists of topical un-
igrams, single words inferred as representa-
tive of their topics. In this article, we present
a hierarchical generative probabilistic model
of topical phrases. The model simultane-
ously infers the location, length, and topic of
phrases within a corpus and relaxes the bag-
of-words assumption within phrases by using
a hierarchy of Pitman-Yor processes. We use
Markov chain Monte Carlo techniques for ap-
proximate inference in the model and perform
slice sampling to learn its hyperparameters.
We show via an experiment on human subjects
that our model finds substantially better, more
interpretable topical phrases than do compet-
ing models.
1 Introduction
Probabilistic topic models have been the focus of
intense study in recent years. The archetypal topic
model, Latent Dirichlet Allocation (LDA), posits
that words within a document are conditionally
independent given their topic (Blei et al2003).
This ?bag-of-words? assumption is a common sim-
plification in which word order is ignored, but
one which introduces undesirable properties into
a model meant to serve as an unsupervised ex-
ploratory tool for data analysis.
When an end-user runs a topic model, the output
he or she is often interested in is a list of topical
unigrams, words probable in a topic (hence, repre-
sentative of it). In many situations, such as during
the use of the topic model for the analysis of a new
or ill-understood corpus, these lists can be insuffi-
ciently informative. For instance, if a layperson ran
LDA on the NIPS corpus, he would likely get a topic
whose most prominent words include policy, value,
and reward. Seeing these words isolated from their
context in a list would not be particularly insightful
to the layperson unfamiliar with computer science
research. An alternative to LDA which produced
richer output like policy iteration algorithm, value
function, and model-based reinforcement learning
alongside the unigrams would be much more en-
lightening. Most situations where a topic model is
actually useful for data exploration require a model
whose output is rich enough to dispel the need for
the user?s extensive prior knowledge of the data.
Furthermore, lists of topical unigrams are often
made only marginally interpretable by virtue of their
non-compositionality, the principle that a colloca-
tion?s meaning typically is not derivable from its
constituent words (Schone and Jurafsky, 2001). For
example, the meaning of compact disc as a mu-
sic medium comes from neither the unigram com-
pact nor the unigram disc, but emerges from the bi-
gram as a whole. Moreover, non-compositionality
is topic dependent; compact disc should be inter-
preted as a music medium in a music topic, and as
a small region bounded by a circle in a mathemati-
cal topic. LDA is prone to decompose collocations
into different topics and violate the principle of non-
compositionality, and its unigram lists are harder to
interpret as a result.
214
We present an extension of LDA called Phrase-
Discovering LDA (PDLDA) that satisfies two
desiderata: providing rich, interpretable output and
honoring the non-compositionality of collocations.
PDLDA is built in the tradition of the ?Topical N-
Gram? (TNG) model of Wang et al2007). TNG is
a topic model which satisfies the first desideratum by
producing lists of representative, topically cohesive
n-grams of the form shown in Figure 1. We diverge
from TNG by our addressing the second desidera-
tum, and we do so through a more straightforward
and intuitive definition of what constitutes a phrase
and its topic. In the furtherance of our goals, we
employ a hierarchical method of modeling phrases
that uses dependent Pitman-Yor processes to ame-
liorate overfitting. Pitman-Yor processes have been
successfully used in the past in n-gram (Teh, 2006)
and LDA-based models (Wallach, 2006) for creat-
ing Bayesian language models which exploit word
order, and they prove equally useful in this scenario
of exploiting both word order and topics.
This article is organized as follows: after describ-
ing TNG, we discuss PDLDA and how PDLDA ad-
dresses the limitations of TNG. We then provide de-
tails of our inference procedures and evaluate our
model against competing models on a subset of the
TREC AP corpus (Harman, 1992) in an experi-
ment on human subjects which assesses the inter-
pretability of topical n-gram lists. The experiment
is premised on the notion that topic models should
be evaluated through a real-world task instead of
through information-theoretic measures which often
negatively correlate with topic quality (Chang et al
2009).
2 Background: LDA and TNG
LDA represents documents as probabilistic mixtures
of latent topics. Each wordw in a corpus w is drawn
from a distribution ? indexed by a topic z, where z is
drawn from a distribution ? indexed by its document
d. The formal definition of LDA is
?d ? Dirichlet (?) zi | d, ? ? Discrete (?d)
?z ? Dirichlet (?) wi | zi, ? ? Discrete (?zi)
where ?d is document d?s topic distribution, ?z is
topic z?s distribution over words, zi is the topic as-
signment of the ith token, and wi is the ith word.
? and ? are hyperparameters to the Dirichlet priors.
Here and throughout the article, we use a bold font
for vector notation: for example, z is the vector of all
topic assignments, and its ith entry, zi, corresponds
to the topic assignment of the ith token in the corpus.
TNG extends LDA to model n-grams of arbitrary
length in order to create the kind of rich output for
text mining discussed in the introduction. It does
this by representing a joint distribution P (z, c|w)
where each ci is a Boolean variable that signals the
start of a new n-gram beginning at the ith token. c
partitions a corpus into consecutive non-overlapping
n-grams of various lengths. Formally, TNG differs
from LDA by the distributional assumptions
wi | wi?1, zi, ci = 1, ? ? Discrete(?zi)
wi | wi?1, zi, ci = 0, ? ? Discrete(?ziwi?1)
ci | wi?1, zi?1, pi ? Bernoulli(pizi?1wi?1)
where the new distributions pizw and ?zw are en-
dowed with conjugate prior distributions: pizw ?
Beta(?) and ?zw ? Dirichlet(?). When ci = 0,
word wi is joined into a topic-specific bigram with
wi?1. When ci = 1, wi is drawn from a topic-
specific unigram distribution and is the start of a new
n-gram.
An unusual feature of TNG is that words within
a topical n-gram, a sequence of words delineated
by c, do not share the same topic. To compen-
sate for this after running a Gibbs sampler, Wang
et al2007) analyze each topical n-gram post hoc
as if the topic of the final word in the n-gram was
the topic assignment of the entire n-gram. Though
this design simplifies inference, we perceive it as a
shortcoming since the aforementioned principle of
non-compositionality supports the intuitive idea that
each collocation ought to be drawn from a single
topic. Another potential drawback of TNG is that
the topic-specific bigram distributions ?zw share no
probability mass between each other or with the un-
igram distributions ?z . Hence, observing a bigram
under one topic does not make it more likely under
another topic or make its constituent unigrams more
probable. To be more concrete, in TNG, observing
space shuttle under a topic z (or under two topics,
one for each word) regrettably does not make space
shuttle more likely under a topic z? 6= z, nor does it
make observing shuttle more likely under any topic.
Smoothing, the sharing of probability mass between
215
matter
atoms
elements
electrons
atom
molecules
form
oxygen
hydrogen
particles
element
solution
substance
reaction
nucleus
chemical reactions
atomic number
hydrogen atoms
hydrogen atom
periodic table
chemical change
physical properties
chemical reaction
water molecules
sodium chloride
small amounts
positive charge
carbon atoms
physical change
chemical properties
like charges repel
positively charged nucleus
unlike charges attract
outer energy level
reaction takes place
negatively charged electrons
chemical change takes place
form new substances
physical change takes place
form sodium chloride
modern atomic theory
electrically charged particles
increasing atomic number
second ionization energies
higher energy levels
(a) Topic 1
president
congress
vote
party
constitution
state
members
office
government
states
elected
representatives
senate
house
washington
supreme court
new york
democratic party
vice president
political parties
national government
executive branch
civil rights
new government
political party
andrew jackson
chief justice
federal government
state legislatures
public opinion
civil rights act
civil rights movement
supreme court ruled
president theodore roosevelt
second continental congress
equal rights amendment
strong central government
sherman antitrust act
civil rights legislation
public opinion polls
major political parties
congress shall make
federal district court
supreme court decisions
american foreign policy
(b) Topic 2
words
word
sentence
write
writing
paragraph
sentences
meaning
use
subject
language
read
example
verb
topic
main idea
topic sentence
english language
following paragraph
words like
quotation marks
direct object
word processing
sentence tells
figurative language
writing process
following sentences
subject matter
standard english
use words
word processing center
word processing systems
word processing equipment
speak different languages
use quotation marks
single main idea
use words like
topic sentence states
present perfect tense
express complete thoughts
word processing software
use formal english
standard american english
collective noun refers
formal standard english
(c) Topic 3
energy
used
oil
heat
coal
use
fuel
produce
power
source
light
electricity
burn
gas
gasoline
natural resources
natural gas
heat energy
iron ore
carbon dioxide
potential energy
solar energy
light energy
fossil fuels
hot water
steam engine
large amounts
sun's energy
radiant energy
nuclear energy
nuclear power plants
nuclear power plant
important natural resources
electric power plants
called fossil fuels
important natural resource
produce large amounts
called solar energy
electric light bulb
use electrical energy
use solar energy
carbon dioxide gas
called potential energy
gas called carbon dioxide
called crude oil
(d) Topic 4
water
air
temperature
heat
liquid
gas
gases
hot
pressure
atmosphere
warm
cold
surface
oxygen
clouds
water vapor
air pollution
air pressure
warm air
cold water
earth's surface
room temperature
boiling point
drinking water
atmospheric pressure
cold war
high temperatures
liquid water
cold air
warm water
water vapor condenses
warm air rises
cold air mass
called water vapor
water vapor changes
process takes place
warm air mass
clean air act
gas called water vapor
dry spell holds
air pressure inside
sewage treatment plant
air pollution laws
high melting points
high melting point
(e) Topic 5
china
africa
india
europe
people
chinese
asia
egypt
world
rome
land
east
trade
countries
empire
middle east
western europe
north africa
mediterranean sea
years ago
roman empire
far east
southeast asia
west africa
saudi arabia
capital letter
asia minor
united states
capital city
centuries ago
2000 years ago
east india company
eastern united states
4000 years ago
southwestern united states
middle atlantic states
northeastern united states
western united states
southeastern united states
200 years ago
middle atlantic region
indus river valley
western roman empire
british north america act
coast guard station
(f) Topic 6
Figure 1: Six out of one hundred topics found by our model, PDLDA, on the Touchstone Applied Science
Associates (TASA) corpus (Landauer and Dumais, 1997). Each column within a box shows the top fifteen
phrases for a topic and is restricted to phrases of a minimum length of one, two, or three words, respectively.
The rows are ordered by likelihood.
216
  
... z
i-1
z
i
z
i+1
w
i-1
w
i+1
c
i
c
i+1
...
...
...
G
?
?
?
?
w
i
D
T
u
a b
? ?
...
...
V
|u|
Figure 2: PDLDA drawn in plate notation.
contexts, is desirable so that a model like this does
not need to independently infer the probability of
every bigram under every topic. The advantages of
smoothing are especially pronounced for small cor-
pora or for a large number of topics. In these sit-
uations, the observed number of bigrams in a given
topic will necessarily be very small and thus not sup-
port strong inferences.
3 PDLDA
A more natural definition of a topical phrase, one
which meets our second desideratum, is to have each
phrase possess a single topic. We adopt this in-
tuitive idea in PDLDA. It can also be understood
through the lens of Bayesian changepoint detection.
Changepoint detection is used in time series mod-
els in which the generative parameters periodically
change abruptly (Adams and MacKay, 2007). View-
ing a sentence as a time series of words, we posit that
the generative parameter, the topic, changes period-
ically in accordance with the changepoint indicators
c. Because there is no restriction on the number of
words between changepoints, topical phrases can be
arbitrarily long but will always have a single topic
drawn from ?d.
The full definition of PDLDA is given by
wi | u ? Discrete(Gu)
Gu ? PYP(a|u|, b|u|, Gpi(u))
G? ? PYP(a0, b0, H)
zi | d, zi?1, ?d, ci ?
{
?zi?1 if ci = 0
Discrete (?d) if ci = 1
ci | wi?1, zi?1, pi ? Bernoulli
(
piwi?1zi?1
)
with the prior distriutions over the parameters as
?d ? Dirichlet (?) pizw ? Beta (?)
a|u| ? Beta (?) b|u| ? Gamma ()
Like TNG, PDLDA assumes that the probability
of a changepoint ci+1 after the ith token depends on
the current topic zi and word wi. This causes the
length of a phrase to depend on its topic and con-
stituent words. The changepoints explicitly model
which words tend to start and end phrases in each
document. Depending on ci, zi is either set deter-
ministically to the preceding topic (when ci = 0)
or is drawn anew from ?d (when ci = 1). In this
way, each topical phrase has a single topic drawn
from its document?s topic distribution. As in TNG,
the parameters pizw and ?d are given conjugate priors
parameterized by ? and ?.
Let u be a context vector consisting of the
phrase topic and the past m words: u , <
zi, wi?1, wi?2, . . . , wi?m >. The operator pi(u) de-
notes the prefix of u, the vector with the rightmost
element of u removed. |u| denotes the length of u,
and ? represents an empty context. For practical rea-
sons, we pad u with a special start symbol when the
context overlaps a phrase boundary. For example,
the first word wi of a phrase beginning at a position
i necessarily has ci = 1; consequently, all the pre-
ceding words wi?j in the context vector are treated
as start symbols so that wi is effectively drawn from
a topic-specific unigram distribution.
In PDLDA, each token is drawn from a distribu-
tion conditioned on its context u. When m = 1,
this conditioning is analogous to TNG?s word dis-
tribution. However, in contrast with TNG, the word
217
  
.
i -1
i -1+???
i z-1
?????? ???
i -1+?????
?? ?? ?????
i - ?
i - ? +???
- ?
?????? ???
i - ? +?????
?? ?? ?????
Figure 3: Illustration of the hierarchical Pitman-Yor
process for a toy two-word vocabulary V = {honda,
civic} and two-topic (T = 2) model with m = 1.
Each node G in the tree is a Pitman-Yor process
whose base distribution is its parent node, andH is a
uniform distribution over V . When, for example, the
context is u = z1 : honda, the darkened path is fol-
lowed and the probability of the next word is calcu-
lated from the shaded node using Equation 1, which
combines predictions from all the nodes along the
darkened path.
distributions used are Pitman-Yor processes (PYPs)
linked together into a tree structure. This hierar-
chical construction creates the desired smoothing
among different contexts. The next section explains
this hierarchical distribution in more detail.
3.1 Hierarchical Pitman-Yor process
Words in PDLDA are emitted from Gu, which has
a PYP prior (Pitman and Yor, 1997). PYPs are a
generalization of the Dirichlet Process, with the ad-
dition of a discount parameter 0 ? a ? 1. When
considering the distribution of a sequence of words
w drawn iid from a PYP-distributed G, one can an-
alytically marginalize G and consider the resulting
conditional distribution of w given its parameters a,
b, and base distribution ?. This marginal can best
be understood by considering the distribution of any
wi|w1, . . . , wi?1, a, b, ?, which is characterized by
a generative process known as the generalized Chi-
nese Restaurant Process (CRP) (Pitman, 2002). In
the CRP metaphor, one imagines a restaurant with
an unbounded number of tables, where each table
has one shared dish (a draw from ?) and can seat an
unlimited number of customers. The CRP specifies a
process by which customers entering the restaurant
choose a table to sit at and, consequently, the dish
they eat. The first customer to arrive always sits at
the first table. Subsequent customers sit at an occu-
pied table k with probability proportional to ck ? a
and choose a new unoccupied table with probabil-
ity proportional to b + ta, where ck is the number
of customers seated at table k and t is the number
of occupied tables in G. For our language modeling
purposes, ?customers? are word tokens and ?dishes?
are word types.
The hierarchical PYP (HPYP) is an intuitive re-
cursive formulation of the PYP in which the base
distribution ? is itself PYP-distributed. Figure 3
demonstrates this principle as applied to PDLDA.
The hierarchy forms a tree structure, where leaves
are restaurants corresponding to full contexts and in-
ternal nodes correspond to partial contexts. An edge
between a parent and child node represents a depen-
dency of the child on the parent, where the base dis-
tribution of the child node is its parent. This smooths
each context?s distribution like the Bayesian n-gram
model of Teh (2006), which is a Bayesian version
of interpolated Kneser-Ney smoothing (Chen and
Goodman, 1998). One ramification of this setup
is that if a word occurs in a context u, the shar-
ing makes it more likely in other contexts that have
something in common with u, such as a shared topic
or word.
The HPYP gives the following probability for a
word following the context u being w:
Pu(w | ?,a,b) =
cuw? ? a|u|tuw
b|u| + cu??
+
b|u| + a|u|tu?
b|u| + cu??
Ppi(u)(w | ?,a,b) (1)
where Ppi(?)(w|?,a,b) = G?(w), cuw? is the num-
ber of customers eating dish w in restaurant u, and
tuw is the number of tables serving w in restau-
rant u, and ? represents the current seating arrange-
ment. Here and throughout the rest of the paper, we
use a dot to indicate marginal counts: e.g., cuw? =?
k cuwk where cuwk is the number of customers
eating w in u at table k. The base distribution of
G? was chosen to be uniform: H(w) = 1/V with V
being the vocabulary size. The above equation an in-
terpolation between distributions of context lengths
218
|u|, |u| ? 1, . . . 0 and realizes the sharing of statisti-
cal strength between different contexts.
3.2 Inference
In this section, we describe Markov chain Monte
Carlo procedures to sample from P (z, c, ? |w, U),
the posterior distribution over topic assignments z,
phrase boundaries c, and seating arrangements ?
given an observed corpus w. Let U be short-
hand for ?, ?,a,b. In order to draw samples
from P (z, c, ? |w, U), we employ a Metropolis-
Hastings sampler for approximate inference. The
sampler we use is a collapsed sampler (Griffiths and
Steyvers, 2004), wherein ?, ?, and G are analyti-
cally marginalized. Because we marginalize eachG,
we use the Chinese Restaurant Franchise representa-
tion of the hierarchical PYPs (Teh, 2006). However,
rather than onerously storing the table assignment
of every token in w, we store only the counts of how
many tables there are in a restaurant and how many
customers are sitting at each table in that restaurant.
We refer the inquisitive reader to the appendix of
Teh (2006) for further details of this procedure.
Our sampling strategy for a given token i in doc-
ument d is to jointly propose changes to the change-
point ci and topic assignment zi, and then to the
seating arrangement ? . Recall that according to the
model, if ci = 0, zi = zi?1; otherwise zi is gen-
erated from the topic distribution for document d.
Since the topic assignment remains the same until a
new changepoint at a position i? is reached, each to-
ken wj for j from position i until i? ? 1 will depend
on zi because for these j, zj = zi. We call this set of
tokens the phrase suffix of the ith token and denote
it s(i). More formally, let s(i) be the maximal set
of continuous indices j ? i including i such that, if
j 6= i, cj = 0. That is, s(i) are the indices compris-
ing the remainder of the phrase beginning at position
i. In addition, let x(i) indicate the extended suffix
version of s(i) which includes one additional index:
x(i) , {s(i) ? {max (s(i)) + 1}}. In addition to
the words in the suffix s(i), the changepoint indica-
tor variables cj for j in x(i) are also conditioned on
zi. To make these dependencies more explicit, we
refer to zs(i) , zj ?j ? s(i), which are constrained
by the model to share a topic.
The variables that depend directly on zi, ci are
zs(i),ws(i), cx(i). The proposal distribution first
draws from a multinomial over T + 1 options: one
option for ci = 0, zi = zi ? 1; and one for ci = 1
paired with each possible zi = z ? 1 . . . T . This is
given by
P (zs(i), ci | z?s(i), c?i, ??s(i),w, U) ?
?
j?x(i)
n?x(j)zj?1wj?1cj + ?cj
n?x(j)zj?1wj?1? + ?0 + ?1
?
j?s(i)
P (zj | c, z?s(j), U) Puj (wj | ??s(i), U)
with
P (zj | c, z?s(j), U) =
?
???
???
n?s(j)dzj + ?
n?s(j)d? + T?
if cj = 1
?zj ,zj?1 if cj = 0
where Puj (wj | ??s(i), U) is given by Equation 1,
T is the number of topics, n?s(j)dz is the number of
phrases in document d that have topic z when s(j)?s
assignment is excluded, and n?s(j)zwc is the number of
times a changepoint c has followed a word w with
topic z when s(j)?s assignments are excluded.
After drawing a proposal for ci, zs(i) for token i,
the sampler adds a customer eating wi to a table
serving wi in restaurant ui. An old table k is se-
lected with probability ? max(0, cuwk ? a|u|) and
a new table is selected with probability ? (b|ui| +
a|ui|tui?)Ppi(u)(wi).
Let z?s(i), c
?
i, ?
?
s(i) denote the proposed change to
zs(i), ci, ?s(i). We accept the proposal with probabil-
ity min(A, 1) where
A =
P? (z?s(i), c
?
i, ?
?
s(i)) Q(zs(i), ci, ?s(i))
P? (zs(i), ci, ?s(i)) Q(z?s(i), c
?
i, ?
?
s(i))
where Q is the proposal distribution and P? is the
true unnormalized distribution. P? differs from Q in
that the probability of each word wj and the seating
arrangement depends only on ?s(j), as opposed to
the simplification of using ?s(i). Almost all propos-
als are accepted; hence, this theoretically motivated
Metropolis Hastings correction step makes little dif-
ference in practice.
Because the parameters a and b have no intuitive
interpretation and we lack any strong belief about
what they should be, we give them vague priors
where ?1 = ?2 = 1 and 1 = 10, 2 = .1. We then
219
interleave a slice sampling algorithm (Neal, 2000)
between sweeps of the Metropolis-Hastings sampler
to learn these parameters. We chose not to do infer-
ence on ? in order to make the tests of our model
against TNG more equitable.
4 Related Work
An integral part of modeling topical phrases is the
relaxation of the bag-of-words assumption in LDA.
There are many models that make this relaxation.
Among them, Griffiths and Steyvers (2005) present
a model in which words are generated either con-
ditioned on a topic or conditioned on the previous
word in a bigram, but not both. They use this to
model human performance on a word-association
task. Wallach (2006) experiments with incorpo-
rating LDA into a bigram language model. Her
model uses a hierarchical Dirichlet to share param-
eters across bigrams in a topic in a manner similar
to our use of PYPs, but it lacks a notion of the topic
being shared between the words in an n-gram. The
Hidden Topic Markov Model (HTMM) (Gruber et
al., 2007) assumes that all words in a sentence have
the same topic, and consecutive sentences are likely
to have the same topic. By dropping the indepen-
dence assumption among topics, HTMM is able to
achieve lower perplexity scores than LDA at mini-
mal additional computational costs. These models
are unconcerned with topical n-grams and thus do
not model phrases.
Johnson (2010) presents an Adaptor Grammar
model of topical phrases. Adaptor Grammars are
a framework for specifying nonparametric Bayesian
models over context-free grammars in which certain
subtrees are ?memoized? or remembered for reuse.
In Johnson?s model, subtrees corresponding to com-
mon phrases for a topic are memoized, resulting in a
model in which each topic is associated with a distri-
bution over whole phrases. While it is a theoretically
elegant method for finding topical phrases, for large
corpora we found inference to be impractically slow.
5 Phrase Intrusion Experiment
Perplexity is the typical information theoretic mea-
sure of language model quality used in lieu of ex-
trinsic measures, which are more difficult and costly
to run. However, it is well known that perplexity
Trial 1 of 80 
countries 
britain 
france 
museum 
Trial 2 of 80 
air force 
beverly hills 
defense minister 
u.s. troops 
Trial 3 of 80 
fda 
book 
smoking 
cigarettes 
Trial 4 of 80 
roman catholic church 
air traffic controllers 
roman catholic priest 
roman catholic bishop 
Figure 4: Experimental setup of the phrase intrusion
experiment in which subjects must click on the n-
gram that does not belong.
scores may negatively correlate with actual quality
as assessed by humans (Chang et al2009). With
that fact in mind, we expanded the methodology of
Chang et al2009) to create a ?phrase intrusion?
task that quantitatively compares the quality of the
topical n-gram lists produced by our model against
those of other models.
Each of 48 subjects underwent 80 trials of a web-
based experiment on Amazon Mechanical Turk, a
reliable (Paolacci et al2010) and increasingly com-
mon venue for conducting online experiments. In
each trial, a subject is presented with a randomly or-
dered list of four n-grams (cf. Figure 4). Each sub-
ject?s task is to select the intruder phrase, a spurious
n-gram not belonging with the others in the list. If,
other than the intruder, the items in the list are all
on the same topic, then subjects can easily identify
the intruder because the list is semantically cohesive
and makes sense. If the list is incohesive and has no
discernible topic, subjects must guess arbitrarily and
performance is at random.
To construct each trial?s list, we chose two top-
ics z and z? (z 6= z?), then selected the three most
probable n-grams from z and the intruder phrase, an
n-gram probable in z? and improbable in z. This
design ensures that the intruder is not identifiable
due solely to its being rare. Interspersed among the
phrase intrusion trials were several simple screen-
ing trials intended to affirm that subjects possessed
a minimal level of attentiveness and reading com-
prehension. For example, one such screening trial
presented subjects with the list banana, apple, tele-
vision, orange. Subjects who got any of these trials
220
Unigrams Bigrams Trigrams0
0.2
0.4
0.6
0.8
1
Model 
Precisi
on
 
 PDLDATNGLDA
(a) Word repetition allowed within a list.
Bigrams Trigrams0
0.2
0.4
0.6
0.8
1
Model 
Precisi
on
 
 PDLDATNG
(b) Word repetition not allowed.
Figure 5: An across-subject measure of the ability to detect intruders as a function of n-gram size and model.
Excluding trials with repeated words does not qualitatively affect the results.
wrong were excluded from our analyses.
Each subject was presented with trials constructed
from the output of PDLDA and TNG for unigrams,
bigrams, and trigrams. For unigrams, we also tested
the output of the original smoothed LDA (Blei et
al., 2003). The experiment was conducted twice for
a 2,246-document subset of the TREC AP corpus
(Blei et al2003; Harman, 1992): the first time pro-
ceeded as described above, but the second time did
not allow word repetition within a topic?s list. The
topical phrases found by TNG and PDLDA often
revolve around a central n-gram, with other words
pre- or post- appended to it. In this intrusion exper-
iment, any n-gram not containing the central word
or phrase may be trivially identifiable, regardless of
its relevance to the topic. For example, the intruder
in Trial 4 of Figure 4 is easily identifiable even if
a subject does not understand English. This second
experiment was designed to test whether our conclu-
sions hinge on word repetition.
We used the MALLET toolbox (McCallum,
2002) for the implementations of LDA and TNG.
Each model was run with 100 topics for 5,000 it-
erations. We set m = 2, ? = .01, ? = .01, ? = 1,
pi1 = pi2 = 1, ?1 = 10, and ?2 = .1. For all mod-
els, we treated certain punctuation as the start of a
phrase by setting cj = 1 for all tokens j immediately
following periods, commas, semicolons, and excla-
mation and question marks. To reduce runtime, we
removed stopwords occuring in the MALLET tool-
box?s stopword list. Because TNG and LDA had
trouble with single character words not in the sto-
plist, we manually removed them before the experi-
ment. Any token immediately following a removed
word was treated as if it were the start of a phrase.
As in Chang et al2009), performance is mea-
sured via model precision, the fraction of subjects
agreeing with the model. It is defined as MPm,nk =?
s
1(im,nk,s = ?
m,n
k,s )/S where ?
m,n
k,s is the index of
the intruding n-gram for subject s among the words
generated from the kth topic of model m, im,nk,s is the
intruder selected by s, and S is the number of sub-
jects. The model precisions are shown in Figure 5.
PDLDA achieves the highest precision in all condi-
tions. Model precision is low in all models, which is
a reflection of how challenging the task is on a small
corpus laden with proper nouns and low-frequency
words. Figure 5b demonstrates that the outcome of
the experiment does not depend strongly on whether
the topical n-gram lists have repeated words.
6 Conclusion
We presented a topic model which simultaneously
segments a corpus into phrases of varying lengths
and assigns topics to them. The topical phrases
found by PDLDA are much richer sources of in-
formation than the topical unigrams typically pro-
duced in topic modeling. As evidenced by the
phrase-intrusion experiment, the topical n-gram lists
that PDLDA finds are much more interpretable than
221
those found by TNG.
The formalism of Bayesian changepoint detection
arose naturally from the intuitive assumption that the
topic of a sequence of tokens changes periodically,
and that the tokens in between changepoints com-
prise a phrase. This formalism provides a principled
way to discover phrases within the LDA framework.
We presented a model embodying these principles
and showed how to incorporate dependent Pitman-
Yor processes into it.
Acknowledgements
The first author is supported by an NSF Graduate
Research Fellowship. The first and second authors
began this project while working at J.D. Power &
Associates. We are indebted to Michael Mozer, Matt
Wilder, and Nicolas Nicolov for their advice.
References
Ryan Prescott Adams and David J.C. MacKay. 2007.
Bayesian online changepoint detection. Technical re-
port, University of Cambridge, Cambridge, UK.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alation. Jour-
nal of Machine Learning Research, 3:993?1022.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems (NIPS).
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems 17, pages 537?544. MIT Press.
Thomas L. Griffiths, Joshua B. Tenenbaum, and Mark
Steyvers. 2007. Topics in semantic representation.
Psychological Review, 114:211?244.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007.
Hidden topic Markov models. Journal of Machine
Learning Research - Proceedings Track, 2:163?170.
Donna Harman. 1992. Overview of the first text re-
trieval conference (trec?1). In Proceedings of the
first Text REtrieval Conference (TREC?1), Washing-
ton DC, USA.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and the
Structure of Proper Names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211 ? 240.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31:705?767.
Gabriele Paolacci, Jesse Chandler, and Panagiotis G.
Ipeirotis. 2010. Running experiments on Amazon
Mechanical Turk. Judgment and Decision Making,
5(5):411?419.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855?900.
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Lillian Lee and
Donna Harman, editors, Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Language
Processing, pages 100?108.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, ACL-
44, pages 985?992, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the 23rd International
Conference on Machine Learning, pages 977?984.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings of
the 7th IEEE International Conference on Data Min-
ing.
222
