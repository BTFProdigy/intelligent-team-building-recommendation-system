Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 25?32,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Aggregating Machine Learning and Rule Based Heuristics for Named 
Entity Recognition 
 Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla and  
Dipti Misra Sharma 
Language Technologies Research Centre, 
International Institute of Information Technology, 
Hyderabad, India. 
karthikg@students.iiit.ac.in, surana.h@gmail.com,  
ashwini_vaidya@research.iiit.ac.in, praneethms@students.iiit.ac.in, 
dipti@iiit.ac.in 
 
 
 
Abstract 
This paper, submitted as an entry for the 
NERSSEAL-2008 shared task, describes a 
system build for Named Entity Recognition 
for South and South East Asian Languages.  
Our paper combines machine learning 
techniques with language specific heuris-
tics to model the problem of NER for In-
dian languages. The system has been tested 
on five languages: Telugu, Hindi, Bengali, 
Urdu and Oriya. It uses CRF (Conditional 
Random Fields) based machine learning, 
followed by post processing which in-
volves using some heuristics or rules. The 
system is specifically tuned for Hindi and 
Telugu, we also report the results for the 
other four languages. 
1 Introduction 
Named Entity Recognition (NER) is a task that 
seeks to locate and classify entities (?atomic ele-
ments?) in a text into predefined categories such as 
the names of persons, organizations, locations, ex-
pressions of times, quantities, etc. It can be viewed 
as a two stage process: 
  
1. Identification of entity boundaries 
2. Classification into the correct category 
 
For example, if ?Mahatma Gandhi? is a named 
entity in the corpus, it is necessary to identify the 
beginning and the end of this entity in the sentence. 
Following this step, the entity must be classified 
into the predefined category, which is NEP 
(Named Entity Person) in this case. 
This task is the precursor for many natural lan-
guage processing applications. It has been used in 
Question Answering (Toral et al 2005) as well as 
Machine Translation (Babych et al 2004). 
The NERSSEAL contest has used 12 categories 
of named entities to define a tagset. The data has 
been manually tagged for training and testing pur-
poses for the contestants. 
The task of building a named entity recognizer 
for South and South East Asian languages presents 
several problems related to their linguistic charac-
teristics. We will first discuss some of these lin-
guistic issues, followed by a description of the 
method used. Further, we show some of the heuris-
tics used for post-processing and finally an analy-
sis of the results obtained.  
2 Previous Work  
The linguistic methods generally use rules 
manually written by linguists. There are several 
rule based NER systems, containing mainly lexi-
calized grammar, gazetteer lists, and list of trigger 
words, which are capable of providing upto 92% f-
measure accuracy for English (McDonald, 1996; 
Wakao et al, 1996).  
Linguistic approach uses hand-crafted rules 
which need skilled linguistics. The chief disadvan-
tage of these rule-based techniques is that they re-
quire huge experience and grammatical knowledge 
of the particular language or domain and these sys-
tems are not transferable to other languages or do-
mains. However, given the closer nature of many 
Indian languages, the cost of adaptation of a re-
25
source from one language to another could be quite 
less (Singh and Surana, 2007). 
Various machine learning techniques have also 
been successfully used for the NER task. Generally 
hidden markov model (Bikel et al,1997), maxi-
mum entropy (Borthwick, 1999), conditional ran-
dom field (Li and Mccallum, 2004) are more popu-
lar machine learning techniques used for the pur-
pose of NER. 
Hybrid systems have been generally more effec-
tive at the task of NER. Given lesser data and more 
complex NE classes which were present in 
NERSSEAL shared task, hybrid systems make 
more sense. Srihari et al (2000) combines MaxEnt, 
hidden markov model (HMM) and handcrafted 
rules to build an NER system. 
Though not much work has been done for other 
South Asian languages, some previous work fo-
cuses on NER for Hindi. It has been previously 
attempted by Cucerzan and Yarowsky in their lan-
guage independent NER work which used morpho-
logical and contextual evidences (Cucerzan and 
Yarowsky, 1999). They ran their experiment with 
5 different languages. Among these the accuracy 
for Hindi was the worst. For Hindi the system 
achieved 42% f-value with a recall of 28% and 
about 85% precision. A result which highlights 
lack of good training data, and other various issues 
involved with linguistic handling of Indian lan-
guages. 
Later approaches have resulted in better results 
for Hindi. Hindi NER system developed by Wei Li 
and Andrew Mccallum (2004) using conditional 
random fields (CRFs) with feature induction have 
achieved f-value of 71%. (Kumar and Bhat-
tacharyya, 2006) used maximum entropy markov 
model to achieve f-value of upto 80%. 
3 Some Linguistic Issues 
3.1 Agglutinative Nature 
Some of the SSEA languages have agglutinative 
properties.  For example, a Dravidian language like 
Telugu has a number of postpositions attached to a 
stem to form a single word. An example is: 
 
guruvAraMwo = guruvAraM + wo  
up to Wednesday = Wednesday + up to 
 
Most of the NERs are suffixed with several dif-
ferent postpositions, which increase the number of 
distinct words in the corpus.  This in turn affects 
the machine learning process. 
3.2 No Capitalization 
All the five languages have scripts without graphi-
cal cues like capitalization, which could act as an 
important indicator for NER.  For a language like 
English, the NER system can exploit this feature to 
its advantage. 
3.3 Ambiguity 
One of the properties of the named entities in these 
languages is the high overlap between common 
names and proper names. For instance Kamal (in 
Hindi) can mean ?lotus?, which is not a named en-
tity, but it can also be a person?s name, in which 
case, it is a named entity. 
Among the named entities themselves, there is 
ambiguity between a location name Bangalore ek 
badzA shaher heI (Bangalore is a big city) or a per-
son?s surname ?M. Bangalore shikshak heI? (M. 
Bangalore is a teacher). 
3.4 Low POS Tagging Accuracy for Nouns 
For English, the available tools like POS (Part of 
Speech) tagger can be used to provide features for 
machine learning. This is not very helpful for 
SSEA languages because the accuracy for noun 
and proper noun tags is quite low (PVS and G., 
2006) Hence, features based on POS tags cannot 
be used for NER for these languages. 
To illustrate this difficulty, we conducted the 
following experiment. A POS tagger (described in 
PVS & G.,2006) was run on the Hindi test data.  
The data had 544 tokens with NEL, NEP, NEO 
tags.  The POS tagger should have given the NNP 
(proper noun) tag for all those named entities. 
However the tagger was able to tag only 80 tokens 
accurately. This meant that only 14.7% of the 
named entities were correctly recognized. 
3.5 Spelling Variation 
One other important language related issue is the 
variation in the spellings of proper names. For in-
stance the same name Shri Ram Dixit can be writ-
ten as Sri. Ram Dixit, Shree Ram Dixit, Sh. R. Dixit 
and so on. This increases the number of tokens to 
be learnt by the machine and would perhaps also 
require a higher level task like co-reference resolu-
tion. 
 
26
2.6 Pattern of suffixes We have converted this format into the BIO 
format as described in Ramshaw et. al. For exam-
ple, the above format will now be shown as: 
 
Named entities of Location (NEL) or Person 
(NEP) will share certain common suffixes, which 
can be exploited by the learning algorthm. For in-
stance, in Hindi, -pur (Rampur, Manipur) or -giri 
(Devgiri) are suffixes that will appear in the named 
entities for Location. Similarly, there are suffixes 
like -swamy (Ramaswamy, Krishnaswamy) or -
deva (Vasudeva, Mahadeva) which can be com-
monly found in named entities for person. These 
suffixes are cues for some of the named entities in 
the SSEA languages. 
 
Rabindranath  B-NEP 
Tagore   I-NEP 
ne   O 
kahaa   O 
 
The training data set contains (approximately) 
400,000 Hindi, 50,000 Telugu, 35,000 Urdu, 
93,000 Oriya and 120,000 Bengali words respec-
tively.  
A NER system can be rule-based, statistical or 
hybrid. A rule-based NER system uses hand-
written rules to tag a corpus with named entities. A 
statistical NER system learns the probabilities of 
named entities using training data, whereas hybrid 
systems use both. 
5 Conditional Random Fields 
Conditional Random Fields (CRFs) are undirected 
graphical models used to calculate the conditional 
probability of values on designated output nodes 
given values assigned to other designated input 
nodes. Developing rule-based taggers for NER can be cumbersome as it is a language specific process. 
Statistical taggers require large amount of anno-
tated data (the more the merrier) to train.  Our sys-
tem is a hybrid NER tagger which first uses Condi-
tional Random Fields (CRF) as a machine learning 
technique followed by some rule based post-
processing. 
In the special case in which the output nodes of 
the graphical model are linked by edges in a linear 
chain, CRFs make a first-order Markov independ-
ence assumption, and thus can be understood as 
conditionally-trained finite state machines (FSMs). 
Let o = (o,,o
We treat the named entity recognition problem 
as a sequential token-based tagging problem. 
According to Lafferty et. al. CRF outperforms 
other Machine Learning algorithms viz., Hidden 
Markov Models (HMM), Maximum Entropy 
Markov Model (MEMM) for  sequence labeling 
tasks.  
4 Training data 
The training data given by the organizers was in 
SSF format1. For example in SSF format, the 
named entity ?Rabindranath Tagore? will be shown 
in the following way: 
0 (( SSF 
1  ((  NP  <ne=NEP> 
1.1  Rabindranath 
1.2 Tagore 
)) 
2 ne 
3 kahaa 
 )) 
 
                                                          
1 http://shiva.iiit.ac.in/SPSAL2007/ssf-analysis-representation.pdf
2,o3 ,o4 ,... oT  ) be some observed in-
put data sequence, such as a sequence of words in 
text in a document,(the values on n input nodes of 
the graphical model). Let S be a set of FSM states, 
each of which is associated with a label, l ? ?. 
Let s = (s ,s ,s  ,s  ,... s1 2 3 4 T ) be some sequence of 
states, (the values on T output nodes). By the 
Hammersley-Clifford theorem, CRFs define the 
conditional probability of a state sequence given an 
input sequence to be: 
 
where Zo is a normalization factor over all state 
sequences is an arbitrary feature function over its 
arguments, and ?k is a learned weight for each fea-
ture function. A feature function may, for example, 
be defined to have value 0 or 1. Higher ? weights 
make their corresponding FSM transitions more 
likely. CRFs define the conditional probability of a 
label sequence based on the total probability over 
the state sequences, 
 
 
27
 
where l(s) is the sequence of labels correspond-
ing to the labels of the states in sequence s. 
Note that the normalization factor, Zo, (also 
known in statistical physics as the partition func-
tion) is the sum of the scores of all possible states. 
 
And that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrar-
ily-structured CRFs, calculating the partition func-
tion in closed form is intractable, and approxima-
tion methods such as Gibbs sampling or loopy be-
lief propagation must be used. In linear-chain 
structured CRFs (in use here for sequence model-
ing), the partition function can be calculated effi-
ciently by dynamic programming. 
6 CRF Based Machine Learning 
We used the CRF model to perform the initial tag-
ging followed by post-processing. 
6.1 Statistical Tagging 
In the first phase, we have used language inde-
pendent features to build the model using CRF. 
Orthographic features (like capitalization, decimals), 
affixes (suffixes and prefixes), context (previous 
words and following words), gazetteer features, POS 
and morphological features etc. are generally used for 
NER. In English and some other languages, capitali-
zation features play an important role as NEs are 
 generally capitalized for these languages. Unfortu-
nately as explained above this feature is not applica-
ble for the Indian languages. 
Precision Recall F-Measure  
Pm Pn Pl Rm Rn Rl Fm Fn Fl  
Bengali 53.34 49.28 58.27 26.77 25.88 31.19 35.65 33.94 40.63 
Hindi 59.53 63.84 64.84 41.21 41.74 40.77 48.71 50.47 50.06 
Oriya 39.16 40.38 63.70 23.39 19.24 28.15 29.29 26.06 39.04 
Telugu 10.31 71.96 65.45 68.00 30.85 29.78 08.19 43.19 40.94 
Urdu 43.63 44.76 48.96 36.69 34.56 39.07 39.86 39.01 43.46 
Table 1: Evaluation of the NER System for Five Languages 
The exact set of features used are described be-
low. 
6.2 Window of the Words 
Words preceding or following the target word may 
be useful for determining its category. Following a 
few trials we found that a suitable window size is 
five. 
6.3 Suffixes 
Statistical suffixes of length 1 to 4 have been con-
sidered. These can capture information for named 
entities having the NEL tag like Hyderabad, 
Secunderabad, Ahmedabad etc., all of which end 
in -bad. We have collected lists of such suffixes for 
NEP (Named Entity Person) and NEL (Named En-
tity Location) for Hindi. In the machine learning 
model, this resource can be used as a binary fea-
ture. A sample of these lists is as follows: 
 
Type of NE Example suffixes 
(Hindi) 
NE- Location -desa, -vana, -nagara,  
-garh, -rashtra, -giri  
NE ? Person -raja, -natha, -lal, -bhai,-
pathi, -krishnan 
 Table 2: Suffixes for Hindi NER 
28
7 Heuristics Based Post Processing 6.4 Prefixes 
Statistical prefixes of length 1 to 4 have been con-
sidered. These can take care of the problems asso-
ciated with a large number of distinct tokens. As 
mentioned earlier, agglutinative languages can 
have a number of postpositions. The use of pre-
fixes will increase the probability of   Hyderabad 
and Hyderabadlo (Telugu for ?in Hyderabad?) be-
ing treated as the same token. 
Complex named entities like fifty five kilograms 
contain a Named Entity Number within a Named 
Entity Measure. We observed that these were not 
identified accurately enough in the machine learn-
ing based system. Hence, instead of applying ma-
chine learning to handle nested entities we make 
use of rule-based post processing.  
7.1 Second Best Tag 
Table 3: F-Measure (Lexical) for NE Tags 
 Bengali Hindi Oriya Telugu Urdu 
It was observed that the recall of the CRF model is 
low. In order to improve recall, we have used the 
following rule:  if the best tag given by the CRF 
model is O (not a named entity) and the confidence 
of the second best tag is greater than 0.15, then the 
second best tag is considered as the correct tag. 
NEP 35.22 54.05 52.22 01.93 31.22 
NED NA 42.47 01.97 NA 21.27 
NEO 11.59 45.63 14.50 NA 19.13 
NEA NA 61.53 NA NA NA 
We observed an increase of 7% in recall and 3% 
decrease in precision. This resulted in a 4% in-
crease in the F-measure, which is a significant in-
crease in performance. The decrease in precision is 
expected as we are taking the second tag. 
NEB NA NA NA NA NA 
NETP 42.30 NA NA NA NA 
NETO 33.33 13.77 NA 01.66 NA 
NEL 45.27 62.66 48.72 01.49 57.85 
7.2 Nested Entities NETI 55.85 79.09 40.91 71.35 63.47 
NEN 62.67 80.69 24.94 83.17 13.75 One of the important tasks in the contest was to 
identify nested named entities. For example if we 
consider eka kilo (Hindi: one kilo) as NEM 
(Named Entity Measure), it contains a NEN 
(Named Entity Number) within it. 
NEM 60.51 43.75 19.00 26.66 84.10 
NETE 19.17 31.52 NA 08.91 NA
The CRF model tags eka kilo as NEM and in or-
der to tag eka as NEN we have made use of other 
resources like a gazetteer for the list of numbers. 
We used such lists for four languages. 
6.5 Start of a sentence 
There is a possibility of confusing the NEN 
(Named Entity Number) in a sentence with the 
number that appears in a numbered list. The num-
bered list will always have numbers at the begin-
ning of a sentence and hence a feature that checks 
for this property will resolve the ambiguity with an 
actual NEN. 
7.3 Gazetteers 
For Hindi, we made use of three different kinds of 
gazetteers. These consisted of lists for measures 
(entities like kilogram, millimetre, lakh), numerals 
and quantifiers (one, first, second) and time ex-
pressions (January, minutes, hours) etc. Similar 
lists were used for all the other languages except 
Urdu. These gazetteers were effective in identify-
ing this relatively closed class of named entities 
and showed good results for these languages. 
6.6 Presence of digits 
Usually, the presence of digits indicates that the 
token is a named entity. For example, the tokens 
92, 10.1 will be identified as Named Entity Num-
ber based on the binary feature ?contains digits?. 
6.7 Presence of  four digits 8 Evaluation 
If the token is a four digit number, it is likelier to 
be a NETI (Named Entity Time). For example, 
1857, 2007 etc. are most probably years. 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
 
29
1. Maximal Matches: The largest possible 
named entities are matched with the refer-
ence data. 
The amount of annotated corpus available for 
Hindi was substantially more. This should have 
ideally resulted in better results for Hindi with the 
machine learning approach. But, the results were 
only marginally better than other languages. A ma-
jor reason for this was that a very high percentage 
(44%) of tags in Hindi were NETE. The tagset 
gives examples like ?Horticulture?, ?Conditional 
Random Fields? for the tag NETE. It has also been 
mentioned that even manual annotation is harder 
for NETE as it is domain specific. This affected the 
overall results for Hindi because the performance 
for NETE was low (Table 3). 
2. Nested Matches: The largest possible as 
well as nested named entities are matched. 
3. Lexical Item Matches: The lexical items 
inside largest possible named entities are 
matched. 
9 Results 
The results of evaluation as explained in the previ-
ous section are shown in the Table-1. The F-
measures for nested lexical match are also shown 
individually for each named entity tag separately in 
Table-3 
 Num of 
NE tokens
Num of 
known NE 
% of un-
known NE
Bengali 1185 277 23.37 
10 Unknown Words Hindi 1120 417 37.23 
Table 4 shows the number of unknown words pre-
sent in the test data when compared with the train-
ing data. 
Oriya 1310 563 42.97 
Telugu 1150 145 12.60 
First column shows the number of unique 
Named entity tags present in the test data for each 
language. Second column shows the number of 
unique known named entities present in the test 
data. Third column shows the percentage of unique 
unknown words present in the test data of different 
languages when compared to training data. 
Urdu 631 179 28.36 
Table 4: Unknown Word 
 
Also, the F-measures of NEN, NETI, and NEM 
could have been higher because they are relatively 
closed classes. However, certain NEN can be am-
biguous (Example: eka is a NEN for ?one? in 
Hindi, but in a different context it can be a non-
number. For instance eka-doosra is Hindi for ?each 
other?). 
11 Error Analysis 
We can observe from the results that the maximal 
F-measure for Telugu is very low when compared 
to lexical F-measure and nested F-measure. The 
reason is that the test data of Telugu contains a 
large number of long named entities (around 6 
words), which in turn contain around 4 - 5 nested 
named entities. Our system was able to tag nested 
named entities correctly unlike maximal named 
entity. 
In a language like Telugu, NENs will appear as 
inflected words. For example 2001lo, guru-
vaaramto. 
10     Conclusion and Further Work 
In this paper we have presented the results of using 
a two stage hybrid approach for the task of named 
entity recognition for South and South East Asian 
Languages. We have achieved decent Lexical F-
measures of 40.63, 50.06, 39.04, 40.94, and 43.46 
for Bengali, Hindi, Oriya, Telugu and Urdu respec-
tively without using many language specific re-
sources. 
We can also observe that the maximal F-
measure for Telugu is very low when compared to 
other languages. This is because Telugu test data 
has very few known words. 
Urdu results are comparatively low chiefly be-
cause gazetteers for numbers and measures were 
unavailable.  
We plan to extend our work by applying our 
method to other South Asian languages, and by 
using more language specific constraints and re-
sources. We also plan to incorporate semi-
supervised extraction of rules for NEs (Saha et. al, 
30
2008) and use transliteration techniques to produce 
Indian language gazetteers (Surana and Singh, 
2008). Use of character models for increasing the 
lower recalls (Shishtla et. al, 2008) is also under-
way. We also plan to enrich the Indian dependency 
tree bank (Begum et. al, 2008) by use of our NER 
system. 
 
11 Acknowledgments 
 
   We would like to thank the organizer Mr. Anil 
Kumar Singh deeply for his continuous support 
during the shared task.  
References 
B. Babych, and A. Hartley, Improving Machine transla-
tion Quality with Automatic Named Entity Recognition. 
www.mt-archive.info/EAMT-2003- Babych.pdf 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra 
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for Indian languages. In 
Proceedings of IJCNLP-2008, Hyderabad, India. 
M. Bikel Daniel, Miller Scott, Schwartz Richard and 
Weischedel Ralph. 1997. Nymble: A High Perfor 
mance Learning Name-finder. In Proceedings of the 
Fifth Conference on Applied Natural Language 
Processing. 
S. Cucerzan, and D. Yarowsky, 1999. Language inde-
pendent named entity recognition combining mor-
phological and contextual evidence. Proceedings of 
the Joint SIGDAT Conference on EMNLP and VLC. 
N. Kumar and Pushpak Bhattacharyya. 2006. Named 
Entity Recognition in Hindi using MEMM. In Tech-
nical Report, IIT Bombay, India. 
John Lafferty, Andrew McCallum and Fernando          
Pereira. 2001. Conditional Random Fields: Probabil-
istic Models for Segmenting and Labeling Sequence 
Data. Proc.   18th International Conf. on Machine 
Learning. 
D. McDonald 1996. Internal and external evidence in 
the identification and semantic categorization of 
proper names. In B. Boguraev and J. Pustejovsky, 
editors, Corpus Processing for Lexical Acquisition. 
Avinesh PVS and Karthik G. Part-Of-Speech Tagging 
and Chunking Using Conditional Random Fields and 
Transformation Based Learning. Proceedings of the 
SPSAL workshop during IJCAI?07. 
Lance Ramshaw and Mitch Marcus. Text Chunking 
Using Transformation-Based Learning. Proceedings 
of the Third Workshop on Very Large Corpora. 
S.K. Saha , S. Chatterji , S. Dandapat , S. Sarkar  and P. 
Mitra 2008. A Hybrid Approach for Named Entity 
Recognition in Indian Languages. In Proceedings of 
IJCNLP Workshop on NER for South and South East 
Asian Languages. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. In the Proceedings 
of HLT-NAACL. 
P. Shishtla, P. Pingali , V. Varma  2008. A Character n-
gram Based Approach for Improved Recall in Indian 
Language NER. In Proceedings of IJCNLP Work-
shop on NER for South and South East Asian Lan-
guages. 
Cucerzan Silviu and Yarowsky David. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and 
VLC. 
A. K. Singh and H. Surana  Can Corpus Based Meas-
ures be Used for Comparative Study of Languages? 
In Proceedings of Ninth Meeting of the ACL Special 
Interest Group in Computational Morphology and 
Phonology. ACL. 2007. 
R. Srihari, C. Niu and W. Li  2000. A Hybrid Approach 
for Named Entity and Sub-Type Tagging. In Pro-
ceedings of the sixth conference on Applied natural 
language processing. 
H. Surana and A. K. Singh 2008. A More Discerning 
and Adaptable Multilingual Transliteration Mecha-
nism for Indian Languages. In Proceedings of the 
Third International Joint Conference on Natural 
Language Processing. 
Charles Sutton, An Introduction to Conditional Random 
Fields for Relational Learning. 
T. Wakao , R. Gaizauskas  and Y. Wilks 1996. Evalua-
tion of an algorithm for the recognition and classifi-
cation of proper names. In Proceedings of COLING. 
 
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. In 
ACM Transactions on Computational Logic. 
CRF++:.Yet another Toolkit. 
http://crfpp.sourceforge.net/ 
 
 
31
 32
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 67?74,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
A Character n-gram Based Approach for Improved Recall
in Indian Language NER
Praneeth M Shishtla
praneethms
@students.iiit.ac.in
Prasad Pingali
pvvpr
@iiit.ac.in
Vasudeva Varma
vv@iiit.ac.in
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
Abstract
Named Entity Recognition (NER) is the
task of identifying and classifying all proper
nouns in a document as person names, or-
ganization names, location names, date &
time expressions and miscellaneous. Previ-
ous work (Cucerzan and Yarowsky, 1999)
was done using the complete words as fea-
tures which suffers from a low recall prob-
lem. Character n-gram based approach
(Klein et al, 2003) using generative mod-
els, was experimented on English language
and it proved to be useful over the word
based models. Applying the same technique
on Indian Languages, we experimented with
Conditional Random Fields (CRFs), a dis-
criminative model, and evaluated our sys-
tem on two Indian Languages Telugu and
Hindi. The character n-gram based models
showed considerable improvement over the
word based models. This paper describes the
features used and experiments to increase
the recall of Named Entity Recognition Sys-
tems which is also language independent.
1 Introduction
The objective of NER is to classify all tokens in a
text document into predefined classes such as per-
son, organization, location, miscellaneous. NER is
a precursor to many language processing tasks. The
creation of a subtask for NER in Message Under-
standing Conference (MUC) (Chinchor, 1997) re-
flects the importance of NER in Information Extrac-
tion (IE). NER also finds aplication in question an-
swering systems (Toral et al, 2005; Molla et al,
2006), and machine translation (Babych and Hart-
ley, 2003). NER is an essential subtask in organizing
and retrieving biomedical information (Tsai, 2006).
NER can be treated as a two step process
? identification of proper nouns.
? classification of these identified proper nouns.
Challenges in named entity recognition.
Many named entities (NEs) occur rarely in corpus
if at all.
Ambiguity of NEs. Ex Washington can be a per-
son?s name or location.
There are many ways of mentioning the same
NE. Ex: Mahatma Gandhi, M.K.Gandhi, Mohandas
Karamchand Gandhi, Gandhi all refer to the same
person. New Jersey, NJ both refer to the same loca-
tion.
In English, the problem of identifying NEs is solved
to some extent by using the capitalization feature.
Most of the named entities begin with a capital let-
ter which is a discriminating feature for classifying a
token as named entity. In addition to the above chal-
lenges, the complexity of Indian Languages pose
few more problems. In case of Indian languages
there is no concept of capitalization. Ex: The per-
son name Y.S.R (in english) is represented as ysr in
the Indian Languages.
Agglutinative property of the Indian Languages
makes the identification more difficult. For exam-
ple: hyderabad, hyderabad ki, hyderabadki, hyder-
abadlo, hyderabad ni, hyderabad ko etc .. all refer
to the place Hyderabad. where lo, ki, ni are all post-
postion markers in Telugu and ko is a postposition
67
marker in Hindi.
There are many ways of representing acronyms.
The letters in acronyms could be the English alpha-
bet or the native alphabet. Ex: B.J.P and BaJaPa
both are acronyms of Bharatiya Janata Party. In-
dian Languages lack particular standard for forming
acronyms.
Due to these wide variations and the agglutina-
tive nature of Indian languages, probabilistic graph-
ical models result in very less recall. If we are able
to identify the presence of a named entity with a
fairly good amount of accuracy, classification then
can be done efficiently. But, when the machine fails
to identify the presence of named entities, there is
no chance of entity classification because we miss
many of the named entities (less recall which results
in less F-measure,F?=1). So we focus mainly on the
ways to improve the recall of the system. Also, In-
dian Languages have a relatively free word order,
i.e. the words (named entities) can occupy any place
in the sentence. This change in the word position is
compensated using case markers.
2 Related Work & Our Contributions
The state-of-art techniques for Indic lan-
guages(Telugu and Hindi) use word based models
which suffer from low recall, use gazetteers and
are language dependent. As such there is no
NER system for Telugu. Previously (Klein et al,
2003) experimented with character-level models
for English using character based HMM which is
a generative model. We experimented using the
discriminative model for English, Hindi and Telugu.
? We propose an approach that increases the re-
call of Indic languages (even the agglutinative
languages).
? The model is language independent as none of
the language resources is needed.
3 Problem Statement
3.1 NER as sequence labelling task
Named entity recognition (NER) can be modelled
as a sequence labelling task (Lafferty et al, 2001).
Given an input sequence of words W n1 = w1w2w3
...wn, the NER task is to construct a label sequence
Ln1 = l1l2l3 ...ln , where label li either belongs to
the set of predefined classes for named entities or
is none (representing words which are not proper
nouns). The general label sequence ln1 has the high-
est probability of occuring for the word sequence
W n1 among all possible label sequences, that is
?Ln1 = argmax {Pr (Ln1 | W n1 ) }
3.2 Tagging Scheme
We followed the IOB tagging scheme (Ramshaw
and Marcus, 1995) for all the three languages (En-
glish, Hindi and Telugu). In this scheme each line
contains a word at the beginning followed by its
tag. The tag encodes the type of named entity
and whether the word is in the beginning or inside
the NE. Empty lines represent sentence (document)
boundaries. An example of the IOB tagging scheme
is given in Table 1.
Words tagged with O are outside of named entities
Token Named Entity Tag
Dr. B-PER
Talcott I-PER
led O
a O
team O
of O
researchers O
from O
the O
National B-ORG
Cancer I-ORG
Institute I-ORG
Table 1: IOB tagging scheme.
and the I-XXX tag is used for words inside a named
entity of type XXX. Whenever two entities of type
XXX are immediately next to each other, the first
word of the second entity will be tagged B-XXX in
order to show that it starts another entity. This tag-
ging scheme is the IOB scheme originally put for-
ward by Ramshaw and Marcus (Ramshaw and Mar-
cus, 1995).
4 Conditional Random Fields
Conditional Random Fields (CRFs) (Wallach, 2004)
are undirected graphical models used to calculate
68
the conditional probability of values on designated
output nodes given the values assigned to other des-
ignated input nodes. In the special case in which
the output nodes of the graphical model are linked
by edges in a linear chain, CRFs make a first-order
Markov independence assumption, and thus can be
understood as conditionally-trained Finite State Ma-
chines (FSMs).
Let o = ? O1,O2,...OT ? be some observed input
data sequence, such as a sequence of words in text
in a document, (the values on n input nodes of the
graphical model). Let S be a set of Finite State Ma-
chine (FSM) states, each of which is associated with
a label, l ? L .
Let s = ? s1,s2,... sT ,? be some sequence of states,(the
values on T output nodes). By the Hammersley-
Clifford theorem CRFs define the conditional prob-
ability of a state sequence given an input sequence
to be
P(s|o) = 1
Zo
? exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
where Zo is a normalization factor over all state
sequences, is an arbitrary feature function over its ar-
guments, and ?k is a learned weight for each feature
function. A feature function may, for example, be
defined to have value 0 or 1. Higher ? weights make
their corresponding FSM transitions more likely.
CRFs define the conditional probability of a la-
bel sequence based on total probability over the state
sequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is the
sequence of labels corresponding to the labels of the
states in sequence s. Note that the normalization fac-
tor, Zo, (also known in statistical physics as the parti-
tion function) is the sum of the scores of all possible
state sequences,
Zo = ?
s?ST
?exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
and that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrarily-
structured CRFs, calculating the partition function in
closed form is intractable, and approximation meth-
ods such as Gibbs sampling, or loopy belief propa-
gation must be used.
5 Features
There are many types of features used in NER sys-
tems.
Many systems use binary features i.e. the
word-internal features, which indicate the presence
or absence of particular property in the word.
(Mikheev, 1997; Wacholder et al, 1997; Bikel et
al., 1997). Following are examples of commonly
used binary features: All-Caps (IBM), internal
capitalization (eBay), initial capital (Abdul Kalam),
uncapitalized word (can), 2-digit number (83, 73),
4-digit number (1983, 2007), all digits (8, 28, 1273)
etc. The features that correspond to the capitaliza-
tion are not applicable to Indian languages. Also,
we have not used any of the binary features in any
of our models.
Dictionaries: Dictionaries are used to check if a
part of the named entity is present in the dictionary.
These dictionaries are called as gazetteers. The
problem with the Indian languages is that there are
no proper gazetteers in Indian languages.
Lexical features like a sliding window
[w?2,w?1,wo,w1,w2] are used to create a lexi-
cal history view. Prefix and suffix tries were also
used previously (Cucerzan and Yarowsky, 1999).
Linguistics features like Part Of Speech, Chunk,
etc are also used. In our approach we don?t use any
of these language specific (linguistic) information.
5.1 Our Features
In our experiments, we considered and character n-
grams (ASCII characters) as tokens.
For example for the word Vivekananda, the 4-gram
model would result in 8 tokens namely Vive, ivek,
veka, ekan, kana, anan, nand and anda. If our cur-
rent token (w0) is kana
Feature Example
current token: w0 kana
previous 3 tokens: w?3,w?2,w?1 ivek,veka,ekan
next 3 tokens: w1,w2,w3 anan,nand,anda
compound feature: w0 w1 kanaanan
compound feature: w?1 w0 ekankana
In Indian Languages suffixes and other inflections
get attached to the words increasing the length of the
word and reducing the number of occurences of that
word in the entire corpus. The character n-grams
69
can capture these variations. The compound features
also help in capturing such variations. The sliding
window feature helps in guessing the class of the en-
tity using the context. In total 9 features were used
in training and testing. All the features are languge
independent and no binary features are used.
6 Experimental Setup
6.1 Corpus
We conducted the experiments on three languages
namely Telugu, Hindi and English. We collected the
Telugu corpus from Eenadu, a telugu daily news-
paper. The topics included politics, health and
medicine, sports, education, general issues etc. The
annotated corpus had 45714 tokens, out of which
4709 were named entities. We collected the English
corpus from the Wall Street Journal (WSJ) news ar-
ticles. The corpus had 45870 tokens out of which
4287 were named entities. And we collected the
hindi corpus from various sources. The topics in the
corpus included social sciences, biological sciences,
financial articles, religion, etc. The hindi corpus is
not a news corpus. The corpus had 45380 tokens out
of which 3140 were named entities. We evaluated
the hand-annotated corpus once to check for any er-
rors.
6.2 Experiments
We conducted various experiments on Telugu and
Hindi. Also, to verify the correctness of our model
for other languages, we have conducted some ex-
periments on English data also. In this section we
describe the various experiments conducted on the
Telugu, Hindi and English data sets.
We show the average performance of the system
in terms of precision, recall and F-measure for Tel-
ugu, Hindi and English in Table 6 and then for the
impact of training data size on performance of the
system in Table 7 (Telugu), Table 8 (English) and
Table 9 (Hindi). Here, precision measures the num-
ber of correct Named Entities (NEs) in the machine
tagged file over the total number of NEs in the ma-
chine tagged file and the recall measures the number
of correct NEs in the machine tagged file over the to-
tal number of NEs in the golden standard file while
F-measure is the weighted harmonic mean of preci-
sion and recall:
F =
(? 2 + 1) RP
? 2R + P
with
? 2 = 1
where P is Precision, R is Recall and F is F-measure.
Precision Recall F?=1
words 89.66% 29.21% 44.07
n=2 77.36% 46.07% 57.75
n=3 85.45% 52.81% 65.28
n=4 79.63% 48.31% 60.14
n=5 74.47% 39.33% 51.47
n=6 76.32% 32.58% 45.67
Table 2: Precision,Recall and F?=1 measure for Date
& Time expressions in Telugu.
Precision Recall F?=1
words 83.65% 28.71% 42.75
n=2 80.29% 36.30% 50
n=3 78.26% 35.64% 48.98
n=4 81.03% 31.02% 44.87
n=5 75.42% 29.37% 42.28
n=6 53.21% 27.39% 36.17
Table 3: Precision,Recall & F?=1 measure values for
location names in Telugu.
Precision Recall F?=1
words 51.11% 18.70% 27.38
n=2 53.41% 38.21% 44.55
n=3 69.35% 34.96% 46.49
n=4 69.35% 34.96% 46.49
n=5 55.00% 26.83% 36.07
n=6 50.98% 21.14% 29.89
Table 4: Precision,Recall and F?=1 measure values
for organisation names in Telugu.
Table:6 shows the average precison(P),recall(R)
and F-measure(F) values for NEs in Telugu.
Tables 2 to 5 show the P,R,F values for the indi-
vidual categories of NEs in Telugu. Interestingly,
70
Precision Recall F?=1
words 57.32% 18.65% 28.14
n=2 55.77% 34.52% 42.65
n=3 61.04% 37.30% 46.31
n=4 56.92% 29.37% 38.74
n=5 60.50% 28.57% 38.81
n=6 54.21% 23.02% 32.31
Table 5: Precision,Recall and F?=1 measure values
for Person names in Telugu.
though we have not used any of the features per-
taining to years and numbers we have acheived an
appreciable F-measure of 65.28 for date & time ex-
pressions.
In each table the model with the highest F-
measure is higlighted in bold. And, the tri-gram
model performed best in most of the cases except
with locations where bi-gram model performed well.
But, even the tri-gram model (F?=1=48.98) per-
formed close to the bi-gram model ((F?=1=50).
For Hindi, the recall of the n-gram models(Table
6) is more than the word based models but the
amount of increase in recall and F-measure is less.
On examining, we found that the average number of
named entities in the Hindi data were quite less. This
is because the articles for hindi were taken from gen-
eral articles. Whereas in case of English and Telugu,
the corpus was collected from news articles, which
had more probability of having new and more named
entities, which can occur in a similar repeating pat-
tern.
The character n-gram approach showed consider-
able improvement in recall and F-measure (with a
drop in precision) in Telugu and Hindi, which are
agglutinative in nature. In Telugu, there is a differ-
ence of 14.19 and 14.02 in recall and F-measure re-
spectively between the word based model and the
best performing n-gram model (n=3) of size 3. In
Hindi, there is a difference of 2.34 and 2.33 in re-
call and F-measure respectively between the word
based model and the best performing n-gram model
(n=5). Even in case of non-agglutinative language
like English there is a considerable improvement of
1.48 and 1.91 in recall and F-measure respectively
between the word based model and best performing
n-gram model (n=2) of size 2.
In almost all the cases the character based models
performed better in terms of recall and F-measure
than the word based models.
We also experimented changing the training data
size keeping the testing data size unchanged for Tel-
ugu(Table 7) and English(Table 8) and Hindi(Table
9). From Table 7:All the models (words,character
n-gram models) are able to learn as we increase the
training data size. And the recall of the character
n-gram models is considerably more than recall of
the word based model. Also the 3-gram model per-
formed well in almost all the runs. The rate of learn-
ing is more in case of 30K.
From Table 8, in all the runs, the bi-gram char-
acter model constantly performed the best. Also
interestingly the model is able to achieve a least
F-measure of 44.75 with just 10K words of train-
ing data. But, in case of Telugu,(Table 7) an F-
measure of 44+ was reached with training data of
size 35K i.e the learning rate for english is more for
less amount of data. This is due to the reason that
Telugu (Entropy=15.625 bits per character) (Bharati
et al, 1998) is comparitively a high entropy lan-
guage than English (Brown and Pietra, 1992). How-
ever for Hindi, the relative jump in the performance
(compared to Telugu and English)is less. Even the
entropy of Hindi (Entorpy=11.088) (Bharati et al,
1998) is more than English. This is also observed
from the table (Table 10). The numbers in the sec-
ond, third and fourth columns are the number of fea-
tures for English,Telugu and Hindi respectively.
English Telugu Hindi
words 29145 320260 685032
n=2 27707 267340 647109
n=3 45580 680720 1403352
n=4 64284 1162320 1830438
n=5 65248 1359980 1735614
n=6 57297 1278790 1433322
Table 10: Number of features calculated in the word
based model for English,Telugu and Hindi.
7 Conclusion & Future Work
The character based n-gram approach worked bet-
ter than the word based approach even with agglu-
tinative languages. A considerably good NER for
71
Language English Telugu Hindi
Precision Recall F?=1 Precision Recall F?=1 Precision Recall F?=1
Words 92.42% 47.29% 62.56 70.38% 23.83% 35.6 51.66% 36.45% 42.74
n=2 81.21% 68.77% 74.47 65.67% 37.11% 47.42 37.30% 36.06% 36.67
n=3 88.37% 62.45% 73.18 71.39% 38.02% 49.62 54.89% 37.23% 44.37
n=4 93.17% 59.19% 72.39 70.17% 33.07% 44.96 54.67% 37.62% 44.57
n=5 90.71% 58.30% 70.98 66.57% 29.82% 41.19 53.78% 38.79% 45.07
n=6 91.03% 56.14% 69.45 55.68% 25.52% 35 51.79% 36.65% 42.92
Table 6: Average Precision, Recall and F?=1 measure for English, Telugu and Hindi ?n? indicates the number
of n-gram characters
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 58.04 8.46 14.77 56.54 14.06 22.52 67.90 21.48 32.64 71.03 23.31 35.1
n=2 53.81 13.80 21.97 60.31 25.52 35.86 63.68 31.51 42.16 65.16 35.55 46
n=3 68.07 14.71 24.2 64.71 24.35 35.38 70.22 32.55 44.48 71.79 37.11 48.93
n=4 71.23 13.54 22.76 63.42 21.22 31.8 68.14 28.12 39.82 68.16 31.77 43.34
n=4 69.92 11.20 19.3 61.20 19.92 30.06 63.90 26.04 37 66.96 29.30 40.76
n=6 52.38 8.59 14.77 52.70 16.54 25.17 56.13 22.66 32.28 55.16 24.35 33.79
Table 7: Effect of training data size on Average Precision,Recall and F?=1 measure for Telugu.
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 81.84 30.79 44.75 86.54 40.93 55.57 89.04 45.95 60.62 89.80 46.35 61.14
n=2 71.49 42.00 52.92 74.80 58.40 65.59 75.46 61.03 67.49 76.63 61.87 68.46
n=3 76.09 28.85 41.84 81.15 50.03 61.9 81.31 54.28 65.11 82.18 56.84 67.2
n=4 83.42 25.75 39.36 83.35 42.93 56.67 88.01 48.70 62.7 87.40 50.25 63.81
n=5 81.95 25.64 39.06 84.48 41.00 55.21 86.81 44.47 58.81 88.07 47.43 61.66
n=6 79.24 26.89 40.16 83.31 38.18 52.36 89.34 42.88 57.95 87.71 44.32 58.88
Table 8: Effect of training data size on Average Precision,Recall and F?=1 measure for English.
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 43.13 30.60 35.80 47.97 34.50 40.14 48.67 35.67 41.17 51.92 36.84 43.10
n=2 39.29 30.41 34.29 40.73 34.70 37.47 37.58 36.26 36.90 37.91 36.06 36.96
n=3 48.17 33.33 39.40 50.56 35.28 41.56 47.72 36.65 41.46 50.68 36.06 42.14
n=4 49.18 35.09 40.96 49.21 36.26 41.75 52.14 35.67 42.36 54.87 38.40 45.18
n=5 41.08 34.11 37.27 41.93 33.92 37.50 48.72 37.23 42.21 53.12 39.77 45.48
n=6 41.43 31.58 35.84 44.59 33.72 38.40 46.35 35.87 40.44 50.67 36.84 42.66
Table 9: Effect of training data size on Average Precision,Recall and F?=1 measure for Hindi.
English can be built with less amount of data when
we use character based models and for high entropy
languages large amount of training data is necessary
to build a considerably good NER. We are able to
achieve an F-measure (49.62 for Telugu and 45.07
for Hindi) even without any extra features like regu-
72
lar expressions and gazetteer information. The char-
acter based n-gram models have worked well even
with the discriminative models. A total of 9 features
were used in training and testing. We have not used
any of the language dependent resources and any bi-
nary features. To improve the efficiency of the sys-
tem we plan to experiment with language specific
resources like Part Of Speech (POS) Taggers, Chun-
kers, Morphological analyzers.. etc and also include
some regular expressions and gazetteer information.
References
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition.
Akshar Bharati, Prakash Rao K, Rajeev Sangal, and
S.M.Bendre. 1998. Basic statistical analysis of cor-
pus and cross comparison among corpora. Tech-
nical report, International Institute of Information
Technology-Hyderabad(IIIT-H).
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder.
Peter E Brown and Vincent J. Della Pietra. 1992. An
estimate of an upper bound for the entropy of english.
Nancy Chinchor. 1997. Muc-7 named entity task defini-
tion. Technical Report Version 3.5, Science Applica-
tions International Corporation, Fairfax, Virginia.
S. Cucerzan and D. Yarowsky. 1999. Language indepen-
dent named entity recognition combining morphologi-
cal and contextual evidence.
D. Klein, J. Smarr, H. Nguyen, and C. Manning. 2003.
Named entity recognition with character-level models.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrei Mikheev. 1997. Automatic rule induction
for unknown-word guessing. Comput. Linguist.,
23(3):405?423.
Diego Molla, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question answer-
ing. In Proceedings of Australasian Language Tech-
nology Workshop 2006, Sydney, Australia.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question answering
using named entity recognition. In Proceedings of the
10th NLDB congress, Lecture notes in Computer Sci-
ence, Alicante, Spain. Springer-Verlag.
Richard Tzong-Han Tsai. 2006. A hybrid approach to
biomedical named entity recognition and semantic role
labeling. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 243?246, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text.
Hanna M. Wallach. 2004. Conditional random fields: An
introduction. Technical Report MS-CIS-04-21, Uni-
versity of Pennsylvania, Department of Computer and
Information Science, University of Pennsylvania.
73
74
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 105?110,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Experiments in Telugu NER: A Conditional Random Field Approach
Praneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma
{praneethms,karthikg}@students.iiit.ac.in,{pvvpr,vv}@iiit.ac.in
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
Abstract
Named Entity Recognition(NER) is the task
of identifying and classifying tokens in a
text document into predefined set of classes.
In this paper we show our experiments
with various feature combinations for Tel-
ugu NER. We also observed that the prefix
and suffix information helps a lot in find-
ing the class of the token. We also show
the effect of the training data on the perfor-
mance of the system. The best performing
model gave an F?=1 measure of 44.91. The
language independent features gave an F?=1
measure of 44.89 which is close to F?=1
measure obtained even by including the lan-
guage dependent features.
1 Introduction
The objective of NER is to identify and classify all
tokens in a text document into predefined classes
such as person, organization, location, miscella-
neous. The Named Entity information in a document
is used in many of the language processing tasks.
NER was created as a subtask in Message Under-
standing Conference (MUC) (Chinchor, 1997). This
reflects the importance of NER in the area of Infor-
mation Extraction (IE). NER has many applications
in the areas of Natural Language Processing, Infor-
mation Extraction, Information Retrieval and speech
processing. NER is also used in question answer-
ing systems (Toral et al, 2005; Molla et al, 2006),
and machine translation systems (Babych and Hart-
ley, 2003). It is also a subtask in organizing and re-
trieving biomedical information (Tsai, 2006).
The process of NER consists of two steps
? identification of boundaries of proper nouns.
? classification of these identified proper nouns.
The Named Entities(NEs) should be correctly iden-
tified for their boundaries and later correctly classi-
fied into their class. Recognizing NEs in an English
document can be done easily with a good amount
of accuracy(using the capitalization feature). Indian
Languages are very much different from the English
like languages.
Some challenges in named entity recognition that
are found across various languages are: Many
named entities(NEs) occur rarely in the corpus i.e
they belong to the open class of nouns. Ambiguity
of NEs. Ex Washington can be a person?s name or a
place name. There are many ways of mentioning the
same Named Entity(NE). In case of person names,
Ex: Abdul Kalam, A.P.J.Kalam, Kalam refer to the
same person. And, in case of place names Waran-
gal, WGL both refer to the same location. Named
Entities mostly have initial capital letters. This dis-
criminating feature of NEs can be used to solve the
problem to some extent in English.
Indian Languages have some additional chal-
lenges: We discuss the challenges that are specific
to Telugu. Absence of capitalization. Ex: The con-
densed form of the person name S.R.Shastry is writ-
ten as S.R.S in English and is represented as srs in
Telugu. Agglutinative property of the Indian Lan-
guages makes the identification more difficult. Ag-
glutinative languages such as Turkish or Finnish,
Telugu etc. differ from languages like English in
105
the way lexical forms are generated. Words are
formed by productive affixations of derivational and
inflectional suffixes to roots or stems. For example:
warangal, warangal ki, warangalki, warangallo,
warangal ni etc .. all refer to the place Waran-
gal. where lo, ki, ni are all postpostion markers
in Telugu. All the postpositions get added to the
stem hyderabad. There are many ways of represent-
ing acronyms. The letters in acronyms could be the
English alphabet or the native alphabet. Ex: B.J.P
and BaJaPa both are acronyms of Bharatiya Janata
Party. Telugu has a relatively free word order when
compared with English. The morpohology of Tel-
ugu is very complex. The Named Entity Recogni-
tion algorithm must be able handle most of these
above variations which otherwise are not found in
languages like English. There are not rich and robust
tools for the Indian Languages. For Telugu, though
a Part Of Speech(POS) Tagger for Telugu, is avail-
able, the accuracy is less when compared to English
and Hindi.
2 Problem Statement
NER as sequence labelling task
Named entity recognition (NER) can be modelled
as a sequence labelling task (Lafferty et al, 2001).
Given an input sequence of words W n1 = w1w2w3
...wn, the NER task is to construct a label sequence
Ln1 = l1l2l3 ...ln , where label li either belongs to
the set of predefined classes for named entities or
is none(representing words which are not proper
nouns). The general label sequence ln1 has the high-
est probability of occuring for the word sequence
W n1 among all possible label sequences, that is
L?n1 = argmax {Pr (L
n
1 |W
n
1 ) }
3 Conditional Random Fields
Conditional Random Fields (CRFs) (Wallach, 2004)
are undirected graphical models used to calculate the
conditional probability of values on designated out-
put nodes given values assigned to other designated
input nodes. In the special case in which the output
nodes of the graphical model are linked by edges in a
linear chain, CRFs make a first-order Markov inde-
pendence assumption, and thus can be understood as
conditionally-trained finite state machines(FSMs).
Let o = ? O1,O2,...OT ? be some observed input
data sequence, such as a sequence of words in text
in a document,(the values on n input nodes of the
graphical model). Let S be a set of FSM states, each
of which is associated with a label, l ?L .
Let s = ? s1,s2,... sT ,? be some sequence of states,(the
values on T output nodes). By the Hammersley-
Clifford theorem CRFs define the conditional prob-
ability of a state sequence given an input sequence
to be
P(s|o) =
1
Zo
? exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
where Zo is a normalization factor over all state
sequences, is an arbitrary feature function over its ar-
guments, and ?k is a learned weight for each feature
function. A feature function may, for example, be
defined to have value 0 or 1. Higher ? weights make
their corresponding FSM transitions more likely.
CRFs define the conditional probability of a la-
bel sequence based on total probability over the state
sequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is the
sequence of labels corresponding to the labels of the
states in sequence s. Note that the normalization fac-
tor, Zo, (also known in statistical physics as the parti-
tion function) is the sum of the scores of all possible
state sequences,
Zo = ?
s?ST
?exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
and that the number of state sequences is expo-
nential in the input sequence length,T. In arbitrarily-
structure CRFs, calculating the partition function in
closed form is intractable, and approximation meth-
ods such as Gibbs sampling, or loopy belief propa-
gation must be used.
4 Features
There are many types of features used in general
NER systems. Many systems use binary features
i.e. the word-internal features, which indicate the
presence or absence of particular property in the
word. (Mikheev, 1997; Wacholder et al, 1997;
Bikel et al, 1997). Following are examples of
binary features commonly used. All-Caps (IBM),
Internal capitalization (eBay), initial capital (Abdul
Kalam), uncapitalized word (can), 2-digit number
106
(83, 28), 4-digit number (1273, 1984), all digits (8,
31, 1228) etc. The features that correspond to the
capitalization are not applicable to Telugu. We have
not used any binary features in our experiments.
Gazetteers are used to check if a part of the
named entity is present in the gazetteers. We don?t
have proper gazetteers for Telugu.
Lexical features like a sliding window
[w?2,w?1,wo,w1,w2] are used to create a lexi-
cal history view. Prefix and suffix tries were also
used previously(Cucerzan and Yarowsky,1999).
Linguistics features like Part Of Speech, Chunk,
etc are also used.
4.1 Our Features
We donot have a highly accurate Part Of
Speech(POS) tagger. In order to obtain some
POS and chunk information, we ran a POS Tagger
and chunker for telugu (PVS and G, 2007) on the
data. And from that, we used the following features
in our experiments.
Language Independent Features
current token: w0
previous 3 tokens: w?3,w?2,w?1
next 3 tokens: w1,w2,w3
compound feature:w0 w1
compound feature:w?1 w0
prefixes (len=1,2,3,4) of w0: pre0
suffixes (len=1,2,3,4) of w0: su f0
Language Dependent Features
POS of current word: POS0
Chunk of current word: Chunk0
Each feature is capable of providing some infor-
mation about the NE.
The word window helps in using the context in-
formation while guessing the tag of the token. The
prefix and suffix feature to some extent help in cap-
turing the variations that may occur due to aggluti-
nation.
The POS tag feature gives a hint whether the word
is a proper noun. When this is a proper noun it has
a chance of being a NE. The chunk feature helps in
finding the boundary of the NE.
In Indian Languages suffixes and other inflections
get attached to the words increasing the length of the
word and reducing the number of occurences of that
word in the entire corpus. The character n-grams can
capture these variations.
5 Experimental Setup
5.1 Corpus
We conducted the experiments on the developement
data released as a part of NER for South and South-
East Asian Languages (NERSSEAL) Competetion.
The corpus in total consisted of 64026 tokens out
of which 10894 were Named Entities(NEs). We di-
vided the corpus into training and testing sets. The
training set consisted of 46068 tokens out of which
8485 were NEs. The testing set consisted of 17951
tokens out of which 2407 were NEs. The tagset as
mentioned in the release, was based on AUKBC?s
ENAMEX,TIMEX and NAMEX, has the follow-
ing tags: NEP (Person), NED (Designation), NEO
(Organization), NEA (Abbreviation), NEB (Brand),
NETP (Title-Person), NETO (Title-Object), NEL
(Location), NETI (Time), NEN (Number), NEM
(Measure) & NETE (Terms).
5.2 Tagging Scheme
The corpus is tagged using the IOB tagging scheme
(Ramshaw and Marcus, 1995). In this scheme each
line contains a word at the beginning followed by
its tag. The tag encodes the type of named entity
and whether the word is in the beginning or inside
the NE. Empty lines represent sentence(document)
boundaries. An example is given in table 1.
Words tagged with O are outside of named en-
tities and the I-XXX tag is used for words inside a
named entity of type XXX. Whenever two entities
of type XXX are immediately next to each other,
the first word of the second entity will be tagged B-
XXX in order to show that it starts another entity.
This tagging scheme is the IOB scheme originally
put forward by Ramshaw and Marcus (1995).
5.3 Experiments
To evaluate the performance of our Named Entity
Recognizer, we used three standard metrics namely
precision, recall and f-measure. Precision measures
the number of correct Named Entities(NEs) in the
107
Token Named Entity Tag
Swami B-NEP
Vivekananda I-NEP
was O
born O
on O
January B-NETI
, I-NETI
12 I-NETI
in O
Calcutta B-NEL
. O
Table 1: IOB tagging scheme.
machine tagged file over the total number of NEs in
the machine tagged file and the recall measures the
number of correct NEs in the machine tagged file
over the total number of NEs in the golden standard
file while F-measure is the weighted harmonic mean
of precision and recall:
F =
(
? 2+1
)
RP
? 2R+P
with
? = 1
where P is Precision, R is Recall and F is F-measure.
W?n+n: A word window :w?n, w?n+1, .., w?1, w0,
w1, .., wn?1, wn.
POSn: POS nth token.
Chn: Chunk of nth token.
pren: Prefix information of nth token. (prefix
length=1,2,3,4)
su fn: Suffix information of nth token. (suffix
length=1,2,3,4)
The more the features, the better is the perfor-
mance. The inclusion of the word window, prefix
and suffix features have increased the F?=1 mea-
sure significantly. Whenever the suffix feature is
included, the performance of the system increased.
This shows that the system is able to caputure those
agglutinative language variations. We also have ex-
perimented changing the training data size. While
varying the training data size, we have tested the
performance on the same amount of testing data of
17951 tokens.
6 Conclusion & Future Work
The inclusion of prefix and suffix feature helps in
improving the F?=1 measure (also recall) of the sys-
tem. As the size of the training data is increased,
the F?=1 measure is increased. Even without the
language specific information the system is able to
perform well. The suffix feature helped improve the
recall. This is due to the fact that the POS tagger
also uses the same features in predicting the POS
tags. Prefix, suffix and word are three non-linguistic
features that resulted in good performance. We plan
to experiment with the character n-gram approach
(Klein et al, 2003) and include gazetteer informa-
tion.
References
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition. In Proceedings of Seventh Inter-
national EAMT Workshop on MT and other language
technology tools, Budapest, Hungary.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
the fifth conference on Applied natural language pro-
cessing, pages 194?201, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Nancy Chinchor. 1997. Muc-7 named entity task defini-
tion. Technical Report Version 3.5, Science Applica-
tions International Corporation, Fairfax, Virginia.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003, pages 180?183, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrei Mikheev. 1997. Automatic rule induction
for unknown-word guessing. Comput. Linguist.,
23(3):405?423.
108
Features Precision Recall F?=1
Ch0 51.41% 9.19% 15.59
POS0 46.32% 9.52% 15.80
POS0.Ch0 46.63% 9.69% 16.05
W?3+3.Ch0 59.08% 19.50% 29.32
W?3+3.POS0 58.43% 19.61% 29.36
Ch0.pren 53.97% 24.76% 33.95
POS0.pren 53.94% 24.93% 34.10
POS0.Ch0.pren 53.94% 25.32% 34.46
POS0.su fn 47.51% 29.36% 36.29
POS0.Ch0.su fn 48.02% 29.24% 36.35
Ch0.su fn 48.55% 29.13% 36.41
W?3+3.POS0.pren 62.98% 27.45% 38.24
W?3+3.POS0.Ch0.pren 62.95% 27.51% 38.28
W?3+3.Ch0.pren 62.88% 27.62% 38.38
W?3+3.POS0.su fn 60.09% 30.53% 40.49
W?3+3.POS0.Ch0.su fn 59.93% 30.59% 40.50
W?3+3.Ch0.su fn 61.18% 30.81% 40.98
POS0.Ch0.pren.su fn 57.83% 34.57% 43.27
POS0.pren.su fn 57.41% 34.73% 43.28
Ch0.pren.su fn 57.80% 34.68% 43.35
W?3+3.Ch0.pren.su fn 64.12% 34.34% 44.73
W?3+3.POS0.pren.su fn 64.56% 34.29% 44.79
W?3+3.POS0.Ch0.pren.su fn 64.07% 34.57% 44.91
Table 2: Average Precision,Recall and F?=1 measure for different language dependent feature combinations.
Features Precision Recall F?=1
w 57.05% 20.62% 30.29
pre 53.65% 23.87% 33.04
suf 47.75% 29.19% 36.23
w.pre 63.08% 27.56% 38.36
w.suf 60.93% 30.76% 40.88
pre.suf 57.94% 34.96% 43.61
w.pre.suf 64.80% 34.34% 44.89
Table 3: Average Precision,Recall and F?=1 measure for different language independent feature combina-
tions.
Diego Molla, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question answer-
ing. In Proceedings of Australasian Language Tech-
nology Workshop 2006, Sydney, Australia.
Avinesh PVS and Karthik G. 2007. Part-of-speech tag-
ging and chunking using conditional random fields and
transformation based learning. In In Proceedings of
SPSAL-2007 Workshop.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question answering
using named entity recognition. In Proceedings of the
10th NLDB congress, Lecture notes in Computer Sci-
ence, Alicante, Spain. Springer-Verlag.
109
Number of Words Precision Recall F?=1
2500 51.37% 9.47% 15.99
5000 64.74% 11.93% 20.15
7500 61.32% 13.50% 22.13
10000 66.88% 23.31% 34.57
12500 63.42% 27.39% 38.26
15000 63.55% 31.26% 41.91
17500 60.58% 30.64% 40.70
20000 58.32% 30.03% 39.64
22500 57.72% 29.75% 39.26
25000 59.33% 29.92% 39.78
27500 60.91% 30.03% 40.23
30000 62.77% 30.42% 40.98
32500 62.66% 30.64% 41.16
35000 62.08% 30.81% 41.18
37500 61.02% 30.87% 41.00
40000 61.60% 31.09% 41.33
42500 62.12% 32.44% 42.62
45000 62.70% 32.77% 43.05
47500 63.20% 32.72% 43.12
50000 64.29% 34.29% 44.72
Table 4: The effect of training data size on the performance of the NER.
Richard Tzong-Han Tsai. 2006. A hybrid approach to
biomedical named entity recognition and semantic role
labeling. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 243?246, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Nina Wacholder, Yael Ravin, and Misook Choi. 1997.
Disambiguation of proper names in text. In Proceed-
ings of the fifth conference on Applied natural lan-
guage processing, pages 202?208, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
HannaM.Wallach. 2004. Conditional random fields: An
introduction. Technical Report MS-CIS-04-21, Uni-
versity of Pennsylvania, Department of Computer and
Information Science, University of Pennsylvania.
110
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 40?43,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Language-Independent Transliteration Schema Using Character
Aligned Models At NEWS 2009
Praneeth Shishtla, Surya Ganesh V, Sethuramalingam Subramaniam, Vasudeva Varma
Language Technologies Research Centre,
IIIT-Hyderabad, India
praneethms@students.iiit.ac.in
{suryag,sethu}@research.iiit.ac.in, vv@iiit.ac.in
Abstract
In this paper we present a statistical
transliteration technique that is language
independent. This technique uses statis-
tical alignment models and Conditional
Random Fields (CRF). Statistical align-
ment models maximizes the probability of
the observed (source, target) word pairs
using the expectation maximization algo-
rithm and then the character level align-
ments are set to maximum posterior pre-
dictions of the model. CRF has efficient
training and decoding processes which is
conditioned on both source and target lan-
guages and produces globally optimal so-
lution.
1 Introduction
A significant portion of out-of-vocabulary (OOV)
words in machine translation systems, information
extraction and cross language retrieval models are
named entities (NEs). If the languages are written
in different scripts, these named entities must be
transliterated. Transliteration is defined as the pro-
cess of obtaining the phonetic translation of names
across languages. A source language word can
have more than one valid transliteration in the tar-
get language. In areas like Cross Language Infor-
mation Retrieval (CLIR), it is important to gener-
ate all possible transliterations of a Named Entity.
Most current transliteration systems use a gen-
erative model for transliteration such as freely
available GIZA++1 (Och and Ney , 2000), an
implementation of the IBM alignment mod-
els (Brown et al, 1993) and HMM alignment
model. These systems use GIZA++ to get charac-
ter level alignments from word aligned data. The
1http://www.fjoch.com/GIZA++.html
transliteration system (Nasreen and Larkey , 2003)
is built by counting up the alignments and convert-
ing the counts to conditional probabilities.
In this paper, we describe our participation
in NEWS 2009 Machine Transliteration Shared
Task (Li et al, 2009). We present a simple statis-
tical, language independent technique which uses
statistical alignment models and Conditional Ran-
dom Fields (CRFs) (Hanna , 2004). Using this
technique a desired number of transliterations are
generated for a given word.
2 Previous work
One of the works on Transliteration is done by
Arababi et al (Arababi et. al., 1994). They
model forward transliteration through a combina-
tion of neural net and expert systems. Work in
the field of Indian Language CLIR was done by
Jaleel and Larkey (Larkey et al, 2003). They
did this based on their work in English-Arabic
transliteration for CLIR (Nasreen and Larkey ,
2003). Their approach was based on HMM us-
ing GIZA++ (Och and Ney , 2000). Prior work in
Arabic-English transliteration for machine trans-
lation purpose was done by Arababi (Arbabi et al,
1994). They developed a hybrid neural network
and knowledge-based system to generate multi-
ple English spellings for Arabic person names.
Knight and Graehl (Knight and Graehl , 1997) de-
veloped a five stage statistical model to do back
transliteration, that is, recover the original En-
glish name from its transliteration into Japanese
Katakana. Stalls and Knight (Stalls and Knight ,
1998) adapted this approach for back translitera-
tion from Arabic to English of English names. Al-
Onaizan and Knight (Onaizan and Knight , 2002)
have produced a simpler Arabic/English translit-
erator and evaluates how well their system can
match a source spelling. Their work includes an
40
evaluation of the transliterations in terms of their
reasonableness according to human judges. None
of these studies measures their performance on a
retrieval task or on other NLP tasks. Fujii and
Ishikawa (Fujii and Ishikawa , 2001) describe a
transliteration system for English-Japanese CLIR
that requires some linguistic knowledge. They
evaluate the effectiveness of their system on an
English-Japanese CLIR task.
3 Problem Description
The problem can be stated formally as a se-
quence labeling problem from one language al-
phabet to other. Consider a source language word
x1x2..xi..xN where each xi is treated as a word
in the observation sequence. Let the equivalent
target language orthography of the same word be
y1y2..yi..yN where each yi is treated as a label in
the label sequence. The task here is to generate a
valid target language word (label sequence) for the
source language word (observation sequence).
x1 ?????? y1
x2 ?????? y2
. ??????- .
. ??????- .
. ??????- .
xN ?????? yN
Here the valid target language alphabet (yi) for a
source language alphabet (xi) in the input source
language word may depend on various factors like
1. The source language alphabet in the input
word.
2. The context (alphabets) surrounding source
language alphabet (xi) in the input word.
3. The context (alphabets) surrounding target
language alphabet (yi) in the desired output
word.
4 Transliteration using alignment models
and CRF
Our approach for transliteration is divided
into two phases. The first phase induces
character alignments over a word-aligned
bilingual corpus, and the second phase uses
some statistics over the alignments to translit-
erate the source language word and generate
the desired number of target language words.
The selected statistical model for transliteration
is based on a combination of statistical alignment
models and CRF. The alignment models maximize
the probability of the observed (source, target)
word pairs using the expectation maximization
algorithm. After the maximization process is
complete, the character level alignments are
set to maximum posterior predictions of the
model. This alignment is used to get character
level alignment of source and target language
words. From the character level alignment
obtained we compare each source language
character to a word and its corresponding tar-
get language character to a label. Conditional
random fields (CRFs) are a probabilistic frame-
work for labeling and segmenting sequential
data. We use CRF to generate target language
word (similar to label sequence) from source
language word (similar to observation sequence).
CRFs are undirected graphical models which
define a conditional distribution over a label se-
quence given an observation sequence. We define
CRFs as conditional probability distributions
P (Y |X) of target language words given source
language words. The probability of a particular
target language word Y given source language
word X is the normalized product of potential
functions each of the form
e(
?
j
?jtj(Yi?1,Yi,X,i))+(
?
k
?ksk(Yi,X,i))
where tj(Yi?1, Yi, X, i) is a transition feature
function of the entire source language word and
the target language characters at positions i and
i? 1 in the target language word; sk(Yi, X, i) is a
state feature function of the target language word
at position i and the source language word; and ?j
and ?k are parameters to be estimated from train-
ing data.
Fj(Y,X) =
n?
i=1
fj(Yi?1, Yi, X, i)
where each fj(Yi?1, Yi, X, i) is either a state
function s(Yi?1, Yi, X, i) or a transition function
t(Yi?1, Yi, X, i). This allows the probability of a
target language word Y given a source language
word X to be written as
P (Y |X,?) = (
1
Z(X)
)e(
?
?jFj(Y,X))
Z(X) is a normalization factor.
41
5 Our Transliteration system
The whole model has three important phases. Two
of them are off-line processes and the other is a on-
line process. The two off-line phases are prepro-
cessing the parallel corpora and training the model
using CRF++2 (Lafferty et al, 2001). CRF++ is a
simple, customizable, and open source implemen-
tation of Conditional Random Fields (CRFs) for
segmenting/labeling sequential data. The on-line
phase involves generating desired number of target
language transliterations (UTF-8 encoded) for the
given English input word. In our case, the source
is always an English word. The same system is
used for every language pair which makes it a lan-
guage independent. The target languages consist
of Chinese, Hindi, Kannada Tamil and Russian
words.
5.1 Preprocessing
The training file is converted into a format re-
quired by CRF++. The sequence of steps in pre-
processing are
1. Both source and target language words were
prefixed with a begin symbol B and suffixed
with an end symbol E which correspond to
start and end states. English words were con-
verted to lower case.
2. The training words were segmented in to
unigrams and the source-target word pairs
were aligned using GIZA++ (IBM model1,
HMM alignment model, IBM model3 and
IBM model4).
3. The alignment consist of NULLs on source
language i.e., a target language unigram is
aligned to NULL on the source language.
These NULLs are problematic during on-
line phase (as positions of NULLs are un-
known). So, these NULLs are removed by
appending the target language unigram to the
unigram of its previous alignment. For exam-
ple, the following alignment,
k ? K
NULL ? A
transforms to -
k ? KA
2http://crfpp.sourceforge.net/
So, in the final alignment, the source side al-
ways contains unigrams and the target side
might contain ngrams which depends on al-
phabet size of the languages. These three
steps are performed to get the character level
alignment for each source and target lan-
guage training words.
4. This final alignment is transformed to train-
ing format as required by CRF++ to work.
In the training format, a source language un-
igram aligned to a target language ngram is
called a token. Each token must be repre-
sented in one line, with the columns sepa-
rated by white space (spaces or tabular char-
acters). Each token should have equal num-
ber of columns.
5.2 Training Phase
The preprocessing phase converts the corpus into
CRF++ input file format. This file is used to
train the CRF model. The training requires a tem-
plate file which specifies the features to be selected
by the model. The training is done using Lim-
ited memory Broyden-Fletcher-Goldfarb-Shannon
method (L-BFGS) (Liu and Nocedal, 1989) which
uses quasi-newton algorithm for large scale nu-
merical optimization problem. We used English
characters as features for our model and a window
size of 5.
5.3 Transliteration
For a language pair, the list of English words that
need to be transliterated is taken. These words are
converted into CRF++ test file format and translit-
erated using the trained model which gives the top
n probable English words. CRF++ uses forward
Viterbi and backward A* search whose combina-
tion produces the exact n-best results. This process
is repeated for all the five language pairs.
6 Results
In this section, we present the results of our par-
ticipation in the NEWS-2009 shared task. We
conducted our experiments on five language pairs
namely English-Chinese (Li et al, 2004), English-
{Hindi, Kannada, Tamil, Russian} (Kumaran and
Kellner , 2007). As specified in NEWS 2009 Ma-
chine Transliteration Shared Task (Li et al, 2009),
we submitted our standard runs on all the five lan-
guage pairs. Table 1 shows the results of our sys-
tem.
42
Language Pair Accuracy in top-1 Mean F-score MRR MAPref MAP10 MAPsys
English-Tamil 0.406 0.894 0.542 0.399 0.193 0.193
English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195
English-Russian 0.548 0.916 0.640 0.548 0.210 0.210
English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192
English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175
Table 1: Transliteration results for the language pairs
7 Conclusion
In this paper, we have described our translitera-
tion system build on a discriminative model using
CRF and statistical alignment models. As men-
tioned earlier, our system is language independent
and works on any language pair provided parallel
word lists are available for training in the particu-
lar language pair. The main advantage of our sys-
tem is that we use no language-specific heuristics
in any of our modules and hence it is extensible to
any language-pair with least effort.
References
A. Kumaran, Tobias Kellner. 2007. A generic frame-
work for machine transliteration, Proc. of the 30th
SIGIR.
A. L. Berger. 1997. The improved iterative scaling
algorithm: A gentle introduction.
Arbabi, M. and Fischthal, S. M. and Cheng, V. C. and
Bart, E. 1994. Algorithms for Arabic name translit-
eration, IBM Journal of Research And Development.
Al-Onaizan Y, Knight K. 2002. Machine translation of
names in Arabic text. Proceedings of the ACL con-
ference workshop on computational approaches to
Semitic languages.
Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng,
and Elizabeth Bar. 1994. Algorithms for Arabic
name transliteration. IBM Journal of research and
Development.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large-scale optimization, Math.
Programming 45 (1989), pp. 503?528.
Fujii Atsushi and Tetsuya Ishikawa. 2001.
Japanese/English Cross-Language Information
Retrieval: Exploration of Query Translation and
Transliteration. Computers and the Humanities,
Vol.35, No.4, pp.389-420.
H. M. Wallach. 2002. Efficient training of conditional
random fields. Masters thesis, University of Edin-
burgh.
HannaM.Wallach. 2004. Conditional Random Fields:
An Introduction.
Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervou-
chine. 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Haizhou Li, A Kumaran, Vladimir Pervouchine, Min
Zhang. 2009. Report on NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Haizhou Li, Min Zhang, Jian Su. 2004. A joint source
channel model for machine transliteration. Proc. of
the 42nd ACL.
J. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathe-
matical Statistics, 43:14701480.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML, pp.282-289.
Knight Kevin and Graehl Jonathan. 1997. Machine
transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics, pp. 128-135. Morgan Kaufmann.
Larkey, Connell,AbdulJaleel. 2003. Hindi CLIR in
Thirty Days.
Nasreen Abdul Jaleel and Leah S. Larkey. 2003.
Statistical Transliteration for English-Arabic Cross
Language Information Retrieval.
Och Franz Josef and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447, Hong Kong, China.
P. F. Brown, S. A. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Lin-
guistics, 19(2):263-311.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
Stalls Bonnie Glover and Kevin Knight. 1998. Trans-
lating names and technical terms in Arabic text.
43
