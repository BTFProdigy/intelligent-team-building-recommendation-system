Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1002?1010,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Word Sense Disambiguation Using OntoNotes:
An Empirical Study
Zhi Zhong and Hwee Tou Ng and Yee Seng Chan
Department of Computer Science
National University of Singapore
Law Link, Singapore 117590
{zhongzhi, nght, chanys}@comp.nus.edu.sg
Abstract
The accuracy of current word sense disam-
biguation (WSD) systems is affected by the
fine-grained sense inventory of WordNet as
well as a lack of training examples. Using the
WSD examples provided through OntoNotes,
we conduct the first large-scale WSD evalua-
tion involving hundreds of word types and tens
of thousands of sense-tagged examples, while
adopting a coarse-grained sense inventory. We
show that though WSD systems trained with a
large number of examples can obtain a high
level of accuracy, they nevertheless suffer a
substantial drop in accuracy when applied to
a different domain. To address this issue, we
propose combining a domain adaptation tech-
nique using feature augmentation with active
learning. Our results show that this approach
is effective in reducing the annotation effort
required to adapt a WSD system to a new do-
main. Finally, we propose that one can maxi-
mize the dual benefits of reducing the annota-
tion effort while ensuring an increase in WSD
accuracy, by only performing active learning
on the set of most frequently occurring word
types.
1 Introduction
In language, many words have multiple meanings.
The process of identifying the correct meaning, or
sense of a word in context, is known as word sense
disambiguation (WSD). WSD is one of the funda-
mental problems in natural language processing and
is important for applications such as machine trans-
lation (MT) (Chan et al, 2007a; Carpuat and Wu,
2007), information retrieval (IR), etc.
WSD is typically viewed as a classification prob-
lem where each ambiguous word is assigned a sense
label (from a pre-defined sense inventory) during the
disambiguation process. In current WSD research,
WordNet (Miller, 1990) is usually used as the sense
inventory. WordNet, however, adopts a very fine
level of sense granularity, thus restricting the accu-
racy of WSD systems. Also, current state-of-the-art
WSD systems are based on supervised learning and
face a general lack of training data.
To provide a standardized test-bed for evalua-
tion of WSD systems, a series of evaluation exer-
cises called SENSEVAL were held. In the English
all-words task of SENSEVAL-2 and SENSEVAL-
3 (Palmer et al, 2001; Snyder and Palmer, 2004),
no training data was provided and systems must tag
all the content words (noun, verb, adjective, and
adverb) in running English texts with their correct
WordNet senses. In SENSEVAL-2, the best per-
forming system (Mihalcea and Moldovan, 2001) in
the English all-words task achieved an accuracy of
69.0%, while in SENSEVAL-3, the best perform-
ing system (Decadt et al, 2004) achieved an accu-
racy of 65.2%. In SemEval-2007, which was the
most recent SENSEVAL evaluation, a similar En-
glish all-words task was held, where systems had to
provide the correct WordNet sense tag for all the
verbs and head words of their arguments in run-
ning English texts. For this task, the best perform-
ing system (Tratz et al, 2007) achieved an accuracy
of 59.1%. Results of these evaluations showed that
state-of-the-art English all-words WSD systems per-
formed with an accuracy of 60%?70%, using the
fine-grained sense inventory of WordNet.
The low level of performance by these state-of-
the-art WSD systems is a cause for concern, since
WSD is supposed to be an enabling technology
to be incorporated as a module into applications
1002
such as MT and IR. As mentioned earlier, one of
the major reasons for the low performance is that
these evaluation exercises adopted WordNet as the
reference sense inventory, which is often too fine-
grained. As an indication of this, inter-annotator
agreement (ITA) reported for manual sense-tagging
on these SENSEVAL English all-words datasets is
typically in the mid-70s. To address this issue, a
coarse-grained English all-words task (Navigli et al,
2007) was conducted during SemEval-2007. This
task used a coarse-grained version of WordNet and
reported an ITA of around 90%. We note that the
best performing system (Chan et al, 2007b) of this
task achieved a relatively high accuracy of 82.5%,
highlighting the importance of having an appropri-
ate level of sense granularity.
Another issue faced by current WSD systems is
the lack of training data. We note that the top per-
forming systems mentioned in the previous para-
graphs are all based on supervised learning. With
this approach, however, one would need to obtain
a corpus where each ambiguous word occurrence is
manually annotated with the correct sense, to serve
as training data. Since it is time consuming to per-
form sense annotation of word occurrences, only a
handful of sense-tagged corpora are publicly avail-
able. Among the existing sense-tagged corpora, the
SEMCOR corpus (Miller et al, 1994) is one of the
most widely used. In SEMCOR, content words have
been manually tagged with WordNet senses. Cur-
rent supervised WSD systems (which include all
the top-performing systems in the English all-words
task) usually rely on this relatively small manually
annotated corpus for training examples, and this has
inevitably affected the accuracy and scalability of
current WSD systems.
Related to the problem of a lack of training data
for WSD, there is also a lack of test data. Having
a large amount of test data for evaluation is impor-
tant to ensure the robustness and scalability of WSD
systems. Due to the expensive process of manual
sense-tagging, the SENSEVAL English all-words
task evaluations were conducted on relatively small
sets of evaluation data. For instance, the evaluation
data of SENSEVAL-2 and SENSEVAL-3 English
all-words task consists of 2,473 and 2,041 test exam-
ples respectively. In SemEval-2007, the fine-grained
English all-words task consists of only 465 test ex-
amples, while the SemEval-2007 coarse-grained En-
glish all-words task consists of 2,269 test examples.
Hence, it is necessary to address the issues of
sense granularity, and the lack of both training and
test data. To this end, a recent large-scale anno-
tation effort called the OntoNotes project (Hovy et
al., 2006) was started. Building on the annotations
from the Wall Street Journal (WSJ) portion of the
Penn Treebank (Marcus et al, 1993), the project
added several new layers of semantic annotations,
such as coreference information, word senses, etc.
In its first release (LDC2007T21) through the Lin-
guistic Data Consortium (LDC), the project man-
ually sense-tagged more than 40,000 examples be-
longing to hundreds of noun and verb types with an
ITA of 90%, based on a coarse-grained sense inven-
tory, where each word has an average of only 3.2
senses. Thus, besides providing WSD examples that
were sense-tagged with a high ITA, the project also
addressed the previously discussed issues of a lack
of training and test data.
In this paper, we use the sense-tagged data pro-
vided by the OntoNotes project to investigate the
accuracy achievable by current WSD systems when
adopting a coarse-grained sense inventory. Through
our experiments, we then highlight that domain
adaptation for WSD is an important issue as it sub-
stantially affects the performance of a state-of-the-
art WSD system which is trained on SEMCOR but
evaluated on sense-tagged examples in OntoNotes.
To address this issue, we then show that by com-
bining a domain adaptation technique using feature
augmentation with active learning, one only needs
to annotate a small amount of in-domain examples
to obtain a substantial improvement in the accuracy
of the WSD system which is previously trained on
out-of-domain examples.
The contributions of this paper are as follows.
To our knowledge, this is the first large-scale WSD
evaluation conducted that involves hundreds of word
types and tens of thousands of sense-tagged exam-
ples, and that is based on a coarse-grained sense in-
ventory. The present study also highlights the practi-
cal significance of domain adaptation in word sense
disambiguation in the context of a large-scale empir-
ical evaluation, and proposes an effective method to
address the domain adaptation problem.
In the next section, we give a brief description of
1003
our WSD system. In Section 3, we describe exper-
iments where we conduct both training and evalu-
ation using data from OntoNotes. In Section 4, we
investigate the WSD performance when we train our
system on examples that are gathered from a differ-
ent domain as compared to the OntoNotes evalua-
tion data. In Section 5, we perform domain adapta-
tion experiments using a recently introduced feature
augmentation technique. In Section 6, we investi-
gate the use of active learning to reduce the annota-
tion effort required to adapt our WSD system to the
domain of the OntoNotes data, before concluding in
Section 7.
2 The WSD System
For the experiments reported in this paper, we fol-
low the supervised learning approach of (Lee and
Ng, 2002), by training an individual classifier for
each word using the knowledge sources of local col-
locations, parts-of-speech (POS), and surrounding
words.
For local collocations, we use 11 features:
C?1,?1, C1,1, C?2,?2, C2,2, C?2,?1, C?1,1, C1,2,
C?3,?1, C?2,1, C?1,2, and C1,3, where Ci,j refers to
the ordered sequence of tokens in the local context
of an ambiguous word w. Offsets i and j denote the
starting and ending position (relative to w) of the se-
quence, where a negative (positive) offset refers to a
token to its left (right). For parts-of-speech, we use
7 features: P?3, P?2, P?1, P0, P1, P2, P3, where
P0 is the POS of w, and P?i (Pi) is the POS of the
ith token to the left (right) of w. For surrounding
words, we consider all unigrams (single words) in
the surrounding context of w. These words can be in
a different sentence from w. For our experiments re-
ported in this paper, we use support vector machines
(SVM) as our learning algorithm, which was shown
to achieve good WSD performance in (Lee and Ng,
2002; Chan et al, 2007b).
3 Training and Evaluating on OntoNotes
The annotated data of OntoNotes is drawn from the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank corpus, divided into sections 00-24. These
WSJ documents have been widely used in various
NLP tasks such as syntactic parsing (Collins, 1999)
and semantic role labeling (SRL) (Carreras and Mar-
Section No. of No. of word tokens
word types Individual Cumulative
02 248 425 425
03 79 107 532
04 186 389 921
05 287 625 1546
06 224 446 1992
07 270 549 2541
08 177 301 2842
09 308 677 3519
10 648 3048 6567
11 724 4071 10638
12 740 4296 14934
13 749 4577 19511
14 710 3900 23411
15 748 4768 28179
16 306 576 28755
17 219 398 29153
18 266 566 29719
19 219 389 30108
20 288 536 30644
21 262 470 31114
23 685 3755 -
Table 1: Size of the sense-tagged data in the various WSJ
sections.
quez, 2005). In these tasks, the practice is to use
documents from WSJ sections 02-21 as training data
and WSJ section 23 as test data. Hence for our ex-
periments reported in this paper, we follow this con-
vention and use the annotated instances from WSJ
sections 02-21 as our training data, and instances in
WSJ section 23 as our test data.
As mentioned in Section 1, the OntoNotes data
provided WSD examples for a large number of
nouns and verbs, which are sense-tagged accord-
ing to a coarse-grained sense inventory. In Table 1,
we show the amount of sense-tagged data available
from OntoNotes, across the various WSJ sections.1
In the table, for each WSJ section, we list the num-
ber of word types, the number of sense-tagged ex-
amples, and the cumulative count on the number of
1We removed erroneous examples which were simply
tagged with ?XXX? as sense-tag, or tagged with senses that were
not found in the sense-inventory provided. Also, since we will
be comparing against training on SEMCOR later (which was
tagged using WordNet senses), we removed examples tagged
with OntoNotes senses which were not mapped to WordNet
senses. On the whole, about 7% of the original OntoNotes ex-
amples were removed as a result.
1004
sense-tagged examples. From the table, we see that
sections 02-21, which will be used as training data
in our experiments, contain a total of slightly over
31,000 sense-tagged examples.
Using examples from sections 02-21 as training
data, we trained our WSD system and evaluated on
the examples from section 23. In our experiments,
if a word type in section 23 has no training exam-
ples from sections 02-21, we randomly select an
OntoNotes sense as the answer. Using these ex-
perimental settings, our WSD system achieved an
accuracy of 89.1%. We note that this accuracy is
much higher than the 60%?70% accuracies achieved
by state-of-the-art English all-words WSD systems
which are trained using the fine-grained sense inven-
tory of WordNet. Hence, this highlights the impor-
tance of having an appropriate level of sense granu-
larity.
Besides training on the entire set of examples
from sections 02-21, we also investigated the per-
formance achievable from training on various sub-
sections of the data and show these results as ?ON?
in Figure 1. From the figure, we see that WSD accu-
racy increases as we add more training examples.
The fact that current state-of-the-art WSD sys-
tems are able to achieve a high level of perfor-
mance is important, as this means that WSD systems
will potentially be more usable for inclusion in end-
applications. For instance, the high level of perfor-
mance by syntactic parsers allows it to be used as an
enabling technology in various NLP tasks. Here, we
note that the 89.1% WSD accuracy we obtained is
comparable to state-of-the-art syntactic parsing ac-
curacies, such as the 91.0% performance by the sta-
tistical parser of Charniak and Johnson (2005).
4 Building WSD Systems with
Out-of-Domain Data
Although our WSD system had achieved a high
accuracy of 89.1%, this was achieved by train-
ing on a large amount (about 31,000) of manually
sense annotated examples from sections 02-21 of the
OntoNotes data. Further, all these training data and
test data are gathered from the same domain of WSJ.
In reality, however, since manual sense annotation is
time consuming, it is not feasible to collect such a
large amount of manually sense-tagged data for ev-
ery domain of interest. Hence, in this section, we in-
vestigate the performance of our WSD system when
it is trained on out-of-domain data.
In the English all-words task of the previous SEN-
SEVAL evaluations (SENSEVAL-2, SENSEVAL-
3, SemEval-2007), the best performing English
all-words task systems with the highest WSD ac-
curacy were trained on SEMCOR (Mihalcea and
Moldovan, 2001; Decadt et al, 2004; Chan et al,
2007b). Hence, we similarly trained our WSD sys-
tem on SEMCOR and evaluated on section 23 of the
OntoNotes corpus. For those word types in section
23 which do not have training examples from SEM-
COR, we randomly chose an OntoNotes sense as
the answer. In training on SEMCOR, we have also
ensured that there is a domain difference between
our training and test data. This is because while
the OntoNotes data was gathered from WSJ, which
contains mainly business related news, the SEMCOR
corpus is the sense-tagged portion of the Brown Cor-
pus (BC), which is a mixture of several genres such
as scientific texts, fictions, etc.
Evaluating on the section 23 test data, our WSD
system achieved only 76.2% accuracy. Compared to
the 89.1% accuracy achievable when we had trained
on examples from sections 02-21, this is a substan-
tially lower and disappointing drop of performance
and motivates the need for domain adaptation.
The need for domain adaptation is a general and
important issue for many NLP tasks (Daume III and
Marcu, 2006). For instance, SRL systems are usu-
ally trained and evaluated on data drawn from the
WSJ. In the CoNLL-2005 shared task on SRL (Car-
reras and Marquez, 2005), however, a task of train-
ing and evaluating systems on different domains was
included. For that task, systems that were trained on
the PropBank corpus (Palmer et al, 2005) (which
was gathered from the WSJ), suffered a 10% drop
in accuracy when evaluated on test data drawn from
BC, as compared to the performance achievable
when evaluated on data drawn from WSJ. More re-
cently, CoNLL-2007 included a shared task on de-
pendency parsing (Nivre et al, 2007). In this task,
systems that were trained on Penn Treebank (drawn
from WSJ), but evaluated on data drawn from a
different domain (such as chemical abstracts and
parent-child dialogues) showed a similar drop in per-
formance. For research involving training and eval-
1005
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
02 02-03
02-04
02-05
02-06
02-07
02-08
02-09
02-10
02-12
02-14
02-21
W
SD
 a
cc
ur
ac
y 
(%
)
Section number
WSD Accuracies on Section 23
59
.2
76
.8
77
.5
60
.5
77
.1
77
.5
64
.4
77
.1
77
.6
73
.3
78
.9 80
.3
76
.8
79
.3 80
.9
80
.2
79
.9 8
2.
1
80
.5
80
.5 8
2.
6
81
.6
80
.8 83
.1 8
5.
8
83
.3 85
.6 87
.5
86
.1 87
.6 88
.3
87
.2 88
.7
89
.1
87
.9 88
.9
ON
SC+ON
SC+ON Augment
Figure 1: WSD accuracies evaluated on section 23, using SEMCOR and different OntoNotes sections as training
data. ON: only OntoNotes as training data. SC+ON: SEMCOR and OntoNotes as training data, SC+ON Augment:
Combining SEMCOR and OntoNotes via the Augment domain adaptation technique.
uating WSD systems on data drawn from different
domains, several prior research efforts (Escudero et
al., 2000; Martinez and Agirre, 2000) observed a
similar drop in performance of about 10% when a
WSD system that was trained on the BC part of the
DSO corpus was evaluated on the WSJ part of the
corpus, and vice versa.
In the rest of this paper, we perform domain adap-
tation experiments for WSD, focusing on domain
adaptation methods that use in-domain annotated
data. In particular, we use a feature augmentation
technique recently introduced by Daume III (2007),
and active learning (Lewis and Gale, 1994) to per-
form domain adaptation of WSD systems.
5 Combining In-Domain and
Out-of-Domain Data for Training
In this section, we will first introduce the AUGMENT
technique of Daume III (2007), before showing the
performance of our WSD system with and without
using this technique.
5.1 The AUGMENT technique for Domain
Adaptation
The AUGMENT technique introduced by Daume III
(2007) is a simple yet very effective approach to per-
forming domain adaptation. This technique is appli-
cable when one has access to training data from the
source domain and a small amount of training data
from the target domain.
The technique essentially augments the feature
space of an instance. Assuming x is an instance and
its original feature vector is ?(x), the augmented
feature vector for instance x is
??(x) =
{
< ?(x),?(x),0 > if x ? Ds
< ?(x),0,?(x) > if x ? Dt
,
where 0 is a zero vector of size |?(x)|, Ds and
Dt are the sets of instances from the source and
target domains respectively. We see that the tech-
nique essentially treats the first part of the aug-
mented feature space as holding general features that
are not meant to be differentiated between different
1006
domains. Then, different parts of the augmented fea-
ture space are reserved for holding source domain
specific, or target domain specific features. Despite
its relative simplicity, this AUGMENT technique has
been shown to outperform other domain adaptation
techniques on various tasks such as named entity
recognition, part-of-speech tagging, etc.
5.2 Experimental Results
As mentioned in Section 4, training our WSD sys-
tem on SEMCOR examples gave a relatively low ac-
curacy of 76.2%, as compared to the 89.1% accuracy
obtained from training on the OntoNotes section 02-
21 examples. Assuming we have access to some in-
domain training data, then a simple method to poten-
tially obtain better accuracies is to train on both the
out-of-domain and in-domain examples. To investi-
gate this, we combined the SEMCOR examples with
various amounts of OntoNotes examples to train our
WSD system and show the resulting ?SC+ON? ac-
curacies obtained in Figure 1. We also performed
another set of experiments, where instead of simply
combining the SEMCOR and OntoNotes examples,
we applied the AUGMENT technique when combin-
ing these examples, treating SEMCOR examples as
out-of-domain (source domain) data and OntoNotes
examples as in-domain (target domain) data. We
similarly show the resulting accuracies as ?SC+ON
Augment? in Figure 1.
Comparing the ?SC+ON? and ?SC+ON Aug-
ment? accuracies in Figure 1, we see that the AUG-
MENT technique always helps to improve the ac-
curacy of our WSD system. Further, notice from
the first few sets of results in the figure that when
we have access to limited in-domain training exam-
ples from OntoNotes, incorporating additional out-
of-domain training data from SEMCOR (either using
the strategies ?SC+ON? or ?SC+ON Augment?)
achieves better accuracies than ?ON?. Significance
tests using one-tailed paired t-test reveal that these
accuracy improvements are statistically significant
at the level of significance 0.01 (all significance tests
in the rest of this paper use the same level of signif-
icance 0.01). These results validate the contribution
of the SemCor examples. This trend continues till
the result for sections 02-06.
The right half of Figure 1 shows the accuracy
trend of the various strategies, in the unlikely event
DS ? the set of SEMCOR training examples
DA? the set of OntoNotes sections 02-21 examples
DT ? empty
while DA 6= ?
pmin ??
??WSD system trained on DS and DT using AUGMENT
technique
for each d ? DA do
bs? word sense prediction for d using ?
p? confidence of prediction bs
if p < pmin then
pmin? p, dmin ? d
end
end
DA? DA ? {dmin}
provide correct sense s for dmin and add dmin to DT
end
Figure 2: The active learning algorithm.
that we have access to a large amount of in-domain
training examples. Although we observe that in
this scenario, ?ON? performs better than ?SC+ON?,
?SC+ON Augment? continues to perform better
than ?ON? (where the improvement is statistically
significant) till the result for sections 02-09. Beyond
that, as we add more OntoNotes examples, signif-
icance testing reveals that the ?SC+ON Augment?
and ?ON? strategies give comparable performance.
This means that the ?SC+ON Augment? strategy,
besides giving good performance when one has few
in-domain examples, does continue to perform well
even when one has a large number of in-domain ex-
amples.
6 Active Learning with AUGMENT
Technique
So far in this paper, we have seen that when we have
access to some in-domain examples, a good strategy
is to combine the out-of-domain and in-domain ex-
amples via the AUGMENT technique. This suggests
that when one wishes to apply a WSD system to a
new domain of interest, it is worth the effort to an-
notate a small number of examples gathered from
the new domain. However, instead of randomly se-
lecting in-domain examples to annotate, we could
use active learning (Lewis and Gale, 1994) to help
select in-domain examples to annotate. By doing
so, we could minimize the manual annotation effort
needed.
1007
WSD Accuracies on Section 23
76
78
80
82
84
86
88
90
SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34
Iteration Number
W
SD
 A
cc
ur
ac
y 
(%
)
50 100 150 200
300 400 500 all
Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each
curve represents the adaptation process of applying active learning on a certain number of most frequently occurring
word types.
In WSD, several prior research efforts have suc-
cessfully used active learning to reduce the annota-
tion effort required (Zhu and Hovy, 2007; Chan and
Ng, 2007; Chen et al, 2006; Fujii et al, 1998). With
the exception of (Chan and Ng, 2007) which tried
to adapt a WSD system trained on the BC part of
the DSO corpus to the WSJ part of the DSO corpus,
the other researchers simply applied active learning
to reduce the annotation effort required and did not
deal with the issue of adapting a WSD system to a
new domain. Also, these prior research efforts only
experimented with a few word types. In contrast, we
perform active learning experiments on the hundreds
of word types in the OntoNotes data, with the aim of
adapting our WSD system trained on SEMCOR to
the WSJ domain represented by the OntoNotes data.
For our active learning experiments, we use the
uncertainty sampling strategy (Lewis and Gale,
1994), as shown in Figure 2. For our experiments,
the SEMCOR examples will be our initial set of
training examples, while the OntoNotes examples
from sections 02-21 will be used as our pool of
adaptation examples, from which we will select ex-
amples to annotate via active learning. Also, since
we have found that the AUGMENT technique is use-
ful in increasing WSD accuracy, we will apply the
AUGMENT technique during each iteration of active
learning to combine the SEMCOR examples and the
selected adaptation examples.
As shown in Figure 2, we train an initial WSD
system using only the set DS of SEMCOR exam-
ples. We then apply our WSD system on the set DA
of OntoNotes adaptation examples. The example in
DA which is predicted with the lowest confidence
will be removed from DA and added to the set DT
of in-domain examples that have been selected via
active learning thus far. We then use the AUGMENT
technique to combine the set of examples in DS and
DT to train a new WSD system, which is then ap-
plied again on the set DA of remaining adaptation
examples, and this active learning process continues
until we have used up all the adaptation examples.
Note that because we are using OntoNotes sections
02-21 (which have already been sense-tagged be-
forehand) as our adaptation data, the annotation of
the selected example during each active learning it-
eration is simply simulated by referring to its tagged
sense.
6.1 Experimental Results
As mentioned earlier, we use the examples in
OntoNotes sections 02-21 as our adaptation exam-
1008
ples during active learning. Hence, we perform
active learning experiments on all the word types
that have sense-tagged examples from OntoNotes
sections 02-21, and show the evaluation results on
OntoNotes section 23 as the topmost ?all? curve in
Figure 3. Since our aim is to reduce the human an-
notation effort required in adapting a WSD system
to a new domain, we may not want to perform active
learning on all the word types in practice. Instead,
we can maximize the benefits by performing active
learning only on the more frequently occurring word
types. Hence, in Figure 3, we also show via var-
ious curves the results of applying active learning
only to various sets of word types, according to their
frequency, or number of sense-tagged examples in
OntoNotes sections 02-21. Note that the various ac-
curacy curves in Figure 3 are plotted in terms of
evaluation accuracies over all the test examples in
OntoNotes section 23, hence they are directly com-
parable to the results reported thus far in this pa-
per. Also, since the accuracies for the various curves
stabilize after 35 active learning iterations, we only
show the results of the first 35 iterations.
From Figure 3, we note that by performing ac-
tive learning on the set of 150 most frequently oc-
curring word types, we are able to achieve a WSD
accuracy of 82.6% after 10 active learning iterations.
Note that in Section 4, we mentioned that training
only on the out-of-domain SEMCOR examples gave
an accuracy of 76.2%. Hence, we have gained an
accuracy improvement of 6.4% (82.6% ? 76.2%)
by just using 1,500 in-domain OntoNotes examples.
Compared with the 12.9% (89.1% ? 76.2%) im-
provement in accuracy achieved by using all 31,114
OntoNotes sections 02-21 examples, we have ob-
tained half of this maximum increase in accuracy, by
requiring only about 5% (1,500/31,114) of the total
number of sense-tagged examples. Based on these
results, we propose that when there is a need to apply
a previously trained WSD system to a different do-
main, one can apply the AUGMENT technique with
active learning on the most frequent word types, to
greatly reduce the annotation effort required while
obtaining a substantial improvement in accuracy.
7 Conclusion
Using the WSD examples made available through
OntoNotes, which are sense-tagged according to a
coarse-grained sense inventory, we show that our
WSD system is able to achieve a high accuracy
of 89.1% when we train and evaluate on these ex-
amples. However, when we apply a WSD system
that is trained on SEMCOR, we suffer a substan-
tial drop in accuracy, highlighting the need to per-
form domain adaptation. We show that by com-
bining the AUGMENT domain adaptation technique
with active learning, we are able to effectively re-
duce the amount of annotation effort required for do-
main adaptation.
References
M. Carpuat and D. Wu. 2007. Improving Statistical Ma-
chine Translation Using Word Sense Disambiguation.
In Proc. of EMNLP-CoNLL07, pages 61?72.
X. Carreras and L. Marquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005, pages 152?164.
Y. S. Chan and H. T. Ng. 2007. Domain Adaptation with
Active Learning for Word Sense Disambiguation. In
Proc. of ACL07, pages 49?56.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007a. Word Sense
Disambiguation Improves Statistical Machine Transla-
tion. In Proc. of ACL07, pages 33?40.
Y. S. Chan, H. T. Ng, and Z. Zhong. 2007b. NUS-PT: Ex-
ploiting Parallel Texts for Word Sense Disambiguation
in the English All-Words Tasks. In Proc. of SemEval-
2007, pages 253?256.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proc. of ACL05, pages 173?180.
J. Y. Chen, A. Schein, L. Ungar, and M. Palmer. 2006.
An Empirical Study of the Behavior of Active Learn-
ing for Word Sense Disambiguation. In Proc. of
HLT/NAACL06, pages 120?127.
M. Collins. 1999. Head-Driven Statistical Model for
Natural Language Parsing. PhD dissertation, Univer-
sity of Pennsylvania.
H. Daume III and D. Marcu. 2006. Domain Adaptation
for Statistical Classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
H. Daume III. 2007. Frustratingly Easy Domain Adap-
tation. In Proc. of ACL07, pages 256?263.
B. Decadt, V. Hoste, and W. Daelemans. 2004. GAMBL,
Genetic Algorithm Optimization of Memory-Based
WSD. In Proc. of SENSEVAL-3, pages 108?112.
1009
G. Escudero, L. Marquez, and G. Riagu. 2000. An
Empirical Study of the Domain Dependence of Super-
vised Word Sense Disambiguation Systems. In Proc.
of EMNLP/VLC00, pages 172?180.
A. Fujii, K. Inui, T. Tokunaga, and H. Tanaka. 1998. Se-
lective Sampling for Example-based Word Sense Dis-
ambiguation. Computational Linguistics, 24(4).
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solution.
In Proc. of HLT-NAACL06, pages 57?60.
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP02,
pages 41?48.
D. D. Lewis and W. A. Gale. 1994. A Sequential Al-
gorithm for Training Text Classifiers. In Proc. of SI-
GIR94.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
D. Martinez and E. Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. In Proc. of
EMNLP/VLC00, pages 207?215.
R. Mihalcea and D. Moldovan. 2001. Pattern Learning
and Active Feature Selection for Word Sense Disam-
biguation. In Proc. of SENSEVAL-2, pages 127?130.
G. A. Miller, M. Chodorow, S. Landes, C. Leacock, and
R. G. Thomas. 1994. Using a Semantic Concordance
for Sense Identification. In Proc. of ARPA Human
Language Technology Workshop, pages 240?243.
G. A. Miller. 1990. WordNet: An On-line Lexi-
cal Database. International Journal of Lexicography,
3(4):235?312.
R. Navigli, K. C. Litkowski, and O. Hargraves. 2007.
SemEval-2007 Task 07: Coarse-Grained English All-
Words Task. In Proc. of SemEval-2007, pages 30?35.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
EMNLP-CoNLL07, pages 915?932.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H. T.
Dang. 2001. English Tasks: All-Words and Verb Lex-
ical Sample. In Proc. of SENSEVAL-2, pages 21?24.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?105.
B. Snyder and M. Palmer. 2004. The English All-Words
Task. In Proc. of SENSEVAL-3, pages 41?43.
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation. In Proc. of SemEval-2007, pages 264?267.
J. B. Zhu and E. Hovy. 2007. Active Learning for Word
Sense Disambiguation with Methods for Addressing
the Class Imbalance Problem. In Proc. of EMNLP-
CoNLL07, pages 783?790.
1010
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 253?256,
Prague, June 2007. c?2007 Association for Computational Linguistics
NUS-PT: Exploiting Parallel Texts for
Word Sense Disambiguation in the English All-Words Tasks
Yee Seng Chan and Hwee Tou Ng and Zhi Zhong
Department of Computer Science, National University of Singapore
3 Science Drive 2, Singapore 117543
{chanys, nght, zhongzhi}@comp.nus.edu.sg
Abstract
We participated in the SemEval-2007
coarse-grained English all-words task
and fine-grained English all-words task.
We used a supervised learning approach
with SVM as the learning algorithm. The
knowledge sources used include local col-
locations, parts-of-speech, and surrounding
words. We gathered training examples
from English-Chinese parallel corpora,
SEMCOR, and DSO corpus. While the
fine-grained sense inventory of WordNet
was used to train our system employed for
the fine-grained English all-words task, our
system employed for the coarse-grained
English all-words task was trained with the
coarse-grained sense inventory released by
the task organizers. Our scores (for both
recall and precision) are 0.825 and 0.587
for the coarse-grained English all-words
task and fine-grained English all-words task
respectively. These scores put our systems
in the first place for the coarse-grained
English all-words task1 and the second
place for the fine-grained English all-words
task.
1 Introduction
In this paper, we describe the systems we devel-
oped for the coarse-grained English all-words task
1A system developed by one of the task organizers of the
coarse-grained English all-words task gave the highest over-
all score for the coarse-grained English all-words task, but this
score is not considered part of the official scores.
and fine-grained English all-words task of SemEval-
2007. In the coarse-grained English all-words task,
systems have to perform word sense disambiguation
(WSD) of all content words (noun, adjective, verb,
and adverb) occurring in five documents, using a
coarse-grained version of the WordNet sense inven-
tory. In the fine-grained English all-words task, sys-
tems have to predict the correct sense of verbs and
head nouns of the verb arguments occurring in three
documents, according to the fine-grained sense in-
ventory of WordNet.
Results from previous SENSEVAL English all-
words task have shown that supervised learning
gives the best performance. Further, the best per-
forming system in SENSEVAL-3 English all-words
task (Decadt et al, 2004) used training data gathered
from multiple sources, highlighting the importance
of having a large amount of training data. Hence,
besides gathering examples from the widely used
SEMCOR corpus, we also gathered training exam-
ples from 6 English-Chinese parallel corpora and the
DSO corpus (Ng and Lee, 1996).
We developed 2 separate systems; one for each
task. For both systems, we performed supervised
word sense disambiguation based on the approach
of (Lee and Ng, 2002) and using Support Vector
Machines (SVM) as our learning algorithm. The
knowledge sources used include local collocations,
parts-of-speech (POS), and surrounding words. Our
system employed for the coarse-grained English all-
words task was trained with the coarse-grained sense
inventory released by the task organizers, while our
system employed for the fine-grained English all-
words task was trained with the fine-grained sense
253
inventory of WordNet.
In the next section, we describe the different
sources of training data used. In Section 3, we de-
scribe the knowledge sources used by the learning
algorithm. In Section 4, we present our official eval-
uation results, before concluding in Section 5.
2 Training Corpora
We gathered training examples from parallel cor-
pora, SEMCOR (Miller et al, 1994), and the DSO
corpus. In this section, we describe these corpora
and how examples gathered from them are combined
to form the training data used by our systems. As
these data sources use an earlier version of the Word-
Net sense inventory as compared to the test data of
the two tasks we participated in, we also discuss the
need to map between different versions of WordNet.
2.1 Parallel Text
Research in (Ng et al, 2003; Chan and Ng, 2005)
has shown that examples gathered from parallel texts
are useful for WSD. In this evaluation, we gath-
ered training data from 6 English-Chinese parallel
corpora (Hong Kong Hansards, Hong Kong News,
Hong Kong Laws, Sinorama, Xinhua News, and
English translation of Chinese Treebank), available
from the Linguistic Data Consortium (LDC). To
gather examples from these parallel corpora, we fol-
lowed the approach in (Ng et al, 2003). Briefly, af-
ter ensuring the corpora were sentence-aligned, we
tokenized the English texts and performed word seg-
mentation on the Chinese texts (Low et al, 2005).
We then made use of the GIZA++ software (Och and
Ney, 2000) to perform word alignment on the paral-
lel corpora. Then, we assigned some possible Chi-
nese translations to each sense of an English word
w. From the word alignment output of GIZA++, we
selected those occurrences of w which were aligned
to one of the Chinese translations chosen. The En-
glish side of these occurrences served as training
data for w, as they were considered to have been dis-
ambiguated and ?sense-tagged? by the appropriate
Chinese translations.
We note that frequently occurring words are usu-
ally highly polysemous and hard to disambiguate.
To maximize the benefits of using parallel texts, we
gathered training data from parallel texts for the set
of most frequently occurring noun, adjective, and
verb types in the Brown Corpus (BC). These word
types (730 nouns, 326 adjectives, and 190 verbs)
represent 60% of the noun, adjective, and verb to-
kens in BC.
2.2 SEMCOR
The SEMCOR corpus (Miller et al, 1994) is one
of the few currently available, manually sense-
annotated corpora for WSD. It is widely used by
various systems which participated in the English
all-words task of SENSEVAL-2 and SENSEVAL-3,
including one of the top performing teams (Hoste
et al, 2001; Decadt et al, 2004) which had per-
formed consistently well in both SENSEVAL all-
words tasks. Hence, we also gathered examples
from SEMCOR as part of our training data.
2.3 DSO Corpus
Besides SEMCOR, the DSO corpus (Ng and Lee,
1996) also contains manually annotated examples
for WSD. As part of our training data, we gath-
ered training examples for each of the 70 verb types
present in the DSO corpus.
2.4 Combination of Training Data
Similar to the top performing supervised systems
of previous SENSEVAL all-words tasks, we used
the annotated examples available from the SEMCOR
corpus as part of our training data. In gathering ex-
amples from parallel texts, a maximum of 1,000 ex-
amples were gathered for each of the frequently oc-
curring noun and adjective types, while a maximum
of 500 examples were gathered for each of the fre-
quently occurring verb types. In addition, a max-
imum of 500 examples were gathered for each of
the verb types present in the DSO corpus. For each
word, the examples from the parallel corpora and
DSO corpus were randomly chosen but adhering to
the sense distribution (proportion of each sense) of
that word in the SEMCOR corpus.
2.5 Sense Inventory
The test data of the two SemEval-2007 tasks we par-
ticipated in are based on the WordNet-2.1 sense in-
ventory. However, the examples we gathered from
the parallel texts and the SEMCOR corpus are based
on the WordNet-1.7.1 sense inventory. Hence, there
254
is a need to map these examples from WordNet-1.7.1
to WordNet-2.1 sense inventory. For this, we rely
primarily on the WordNet sense mappings automat-
ically generated by the work of (Daude et al, 2000).
To ensure the accuracy of the mappings, we per-
formed some manual corrections of our own, focus-
ing on the set of most frequently occurring nouns,
adjectives, and verbs. For the verb examples from
the DSO corpus which are based on the WordNet-
1.5 sense inventory, we manually mapped them to
WordNet-2.1 senses.
3 WSD System
Following the approach of (Lee and Ng, 2002), we
train an SVM classifier for each word using the
knowledge sources of local collocations, parts-of-
speech (POS), and surrounding words. We omit the
syntactic relation features for efficiency reasons. For
local collocations, we use 11 features: C?1,?1, C1,1,
C?2,?2, C2,2, C?2,?1, C?1,1, C1,2, C?3,?1, C?2,1,
C?1,2, and C1,3, where Ci,j refers to the ordered
sequence of tokens in the local context of an am-
biguous word w. Offsets i and j denote the starting
and ending position (relative to w) of the sequence,
where a negative (positive) offset refers to a token
to its left (right). For parts-of-speech, we use 7 fea-
tures: P?3, P?2, P?1, P0, P1, P2, P3, where P0 is
the POS of w, and P?i (Pi) is the POS of the ith to-
ken to the left (right) of w. For surrounding words,
we consider all unigrams (single words) in the sur-
rounding context of w. These words can be in a dif-
ferent sentence from w.
4 Evaluation
We participated in two tasks of SemEval-2007: the
coarse-grained English all-words task and the fine-
grained English all-words task. In both tasks, when
there is no training data at all for a particular word,
we tag all test examples of the word with its first
sense in WordNet. Since our systems give exactly
one answer for each test example, recall is the same
as precision. Hence we will just report the micro-
average recall in this section.
4.1 Coarse-Grained English All-Words Task
Our system employed for the coarse-grained En-
glish all-words task was trained with the coarse-
English all-words Training data
task SC+DSO SC+DSO+PT
Coarse-grained 0.817 0.825
Fine-grained 0.578 0.587
Table 1: Scores for the coarse-grained English all-
words task and fine-grained English all-words task,
using different sets of training data. SC+DSO
refers to using examples gathered from SEMCOR
and DSO corpus. Similarly, SC+DSO+PT refers to
using examples gathered from SEMCOR, DSO cor-
pus, and parallel texts.
Doc-ID Recall No. of test instances
d001 0.883 368
d002 0.881 379
d003 0.834 500
d004 0.761 677
d005 0.814 345
Table 2: Score of each individual test document, for
the coarse-grained English all-words task.
grained WordNet-2.1 sense inventory released by
the task organizers. We obtained a score of 0.825
in this task, as shown in Table 1 under the column
SC + DSO + PT . It turns out that among the
16 participants of this task, the system which re-
turned the best score was developed by one of the
task organizers. Since the score of this system is
not considered part of the official scores, our score
puts our system in the first position among the par-
ticipants of this task. For comparison, the WordNet
first sense baseline score as calculated by the task
organizers is 0.789. To gauge the contribution of
parallel text examples, we retrained our system us-
ing only examples gathered from the SEMCOR and
DSO corpus. As shown in Table 1 under the col-
umn SC + DSO, this gives a score of 0.817 when
scored against the answer keys released by the task
organizers. Although adding examples from parallel
texts gives only a modest improvement in the scores,
we note that this improvement is achieved from a
relatively small set of word types which are found
to be frequently occurring in BC. Future work can
explore expanding the set of word types by automat-
ing the process of assigning Chinese translations to
each sense of an English word, with the use of suit-
255
able bilingual lexicons.
As part of the evaluation results, the task organiz-
ers also released the scores of our system on each of
the 5 test documents. We show in Table 2 the score
we obtained for each document, along with the to-
tal number of test instances in each document. We
note that our system obtained a relatively low score
on the fourth document, which is a Wikipedia entry
on computer programming. To determine the rea-
son for the low score, we looked through the list of
test words in that document. We noticed that the
noun program has 20 test instances occurring in that
fourth document. From the answer keys released by
the task organizers, all 20 test instances belong to the
sense of ?a sequence of instructions that a computer
can interpret and execute?, which we do not have
any training examples for. Similarly, we noticed that
another noun programming has 27 test instances oc-
curring in the fourth document which belong to the
sense of ?creating a sequence of instructions to en-
able the computer to do something?, which we do
not have any training examples for. Thus, these two
words alone account for 47 of the errors made by our
system in this task, representing 2.1% of the 2,269
test instances of this task.
4.2 Fine-Grained English All-Words Task
Our system employed for the fine-grained English
all-words task was trained on examples tagged
with fine-grained WordNet-2.1 senses (mapped from
WordNet-1.7.1 senses and 1.5 senses as described
earlier). Unlike the coarse-grained English all-
words task, the correct POS tag and lemma of each
test instance are not given in the fine-grained task.
Hence, we used the POS tag from the mrg parse
files released as part of the test data and performed
lemmatization using WordNet. We obtained a score
of 0.587 in this task, as shown in Table 1. This ranks
our system in second position among the 14 partic-
ipants of this task. If we exclude parallel text ex-
amples and train only on examples gathered from
the SEMCOR and DSO corpus, we obtain a score of
0.578.
5 Conclusion
In this paper, we describe the approach taken by
our systems which participated in the coarse-grained
English all-words task and fine-grained English all-
words task of SemEval-2007. Using training exam-
ples gathered from parallel texts, SEMCOR, and the
DSO corpus, we trained supervised WSD systems
with SVM as the learning algorithm. Evaluation re-
sults show that this approach achieves good perfor-
mance in both tasks.
6 Acknowledgements
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076).
References
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proc. of AAAI05,
pages 1037?1042.
Jordi Daude, Lluis Padro, and German Rigau. 2000. Mapping
WordNets using structural information. In Proc. of ACL00,
pages 504?511.
Bart Decadt, Veronique Hoste, Walter Daelemans, and Antal
van den Bosch. 2004. GAMBL, genetic algorithm opti-
mization of memory-based WSD. In Proc. of SENSEVAL-3,
pages 108?112.
Veronique Hoste, Anne Kool, and Walter Daelemans. 2001.
Classifier optimization and combination in the English all
words task. In Proc. of SENSEVAL-2, pages 83?86.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evalu-
ation of knowledge sources and learning algorithms for word
sense disambiguation. In Proc. of EMNLP02, pages 41?48.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmentation.
In Proc. of the Fourth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 161?164.
George A. Miller, Martin Chodorow, Shari Landes, Claudia
Leacock, and Robert G. Thomas. 1994. Using a seman-
tic concordance for sense identification. In Proc. of HLT94
Workshop on Human Language Technology, pages 240?243.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating mul-
tiple knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proc. of ACL96, pages 40?47.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploit-
ing parallel texts for word sense disambiguation: An empir-
ical study. In Proc. of ACL03, pages 455?462.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proc. of ACL00, pages 440?447.
256
Proceedings of the ACL 2010 System Demonstrations, pages 78?83,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
It Makes Sense: A Wide-Coverage Word Sense Disambiguation System
for Free Text
Zhi Zhong and Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
{zhongzhi, nght}@comp.nus.edu.sg
Abstract
Word sense disambiguation (WSD)
systems based on supervised learning
achieved the best performance in SensE-
val and SemEval workshops. However,
there are few publicly available open
source WSD systems. This limits the use
of WSD in other applications, especially
for researchers whose research interests
are not in WSD.
In this paper, we present IMS, a supervised
English all-words WSD system. The flex-
ible framework of IMS allows users to in-
tegrate different preprocessing tools, ad-
ditional features, and different classifiers.
By default, we use linear support vector
machines as the classifier with multiple
knowledge-based features. In our imple-
mentation, IMS achieves state-of-the-art
results on several SensEval and SemEval
tasks.
1 Introduction
Word sense disambiguation (WSD) refers to the
task of identifying the correct sense of an ambigu-
ous word in a given context. As a fundamental
task in natural language processing (NLP), WSD
can benefit applications such as machine transla-
tion (Chan et al, 2007a; Carpuat and Wu, 2007)
and information retrieval (Stokoe et al, 2003).
In previous SensEval workshops, the supervised
learning approach has proven to be the most suc-
cessful WSD approach (Palmer et al, 2001; Sny-
der and Palmer, 2004; Pradhan et al, 2007). In
the most recent SemEval-2007 English all-words
tasks, most of the top systems were based on su-
pervised learning methods. These systems used
a set of knowledge sources drawn from sense-
annotated data, and achieved significant improve-
ments over the baselines.
However, developing such a system requires
much effort. As a result, very few open source
WSD systems are publicly available ? the only
other publicly available WSD system that we are
aware of is SenseLearner (Mihalcea and Csomai,
2005). Therefore, for applications which employ
WSD as a component, researchers can only make
use of some baselines or unsupervised methods.
An open source supervised WSD system will pro-
mote the use of WSD in other applications.
In this paper, we present an English all-words
WSD system, IMS (It Makes Sense), built using a
supervised learning approach. IMS is a Java im-
plementation, which provides an extensible and
flexible platform for researchers interested in us-
ing a WSD component. Users can choose differ-
ent tools to perform preprocessing, such as trying
out various features in the feature extraction step,
and applying different machine learning methods
or toolkits in the classification step. Following
Lee and Ng (2002), we adopt support vector ma-
chines (SVM) as the classifier and integrate mul-
tiple knowledge sources including parts-of-speech
(POS), surrounding words, and local collocations
as features. We also provide classification mod-
els trained with examples collected from parallel
texts, SEMCOR (Miller et al, 1994), and the DSO
corpus (Ng and Lee, 1996).
A previous implementation of the IMS sys-
tem, NUS-PT (Chan et al, 2007b), participated in
SemEval-2007 English all-words tasks and ranked
first and second in the coarse-grained and fine-
grained task, respectively. Our current IMS im-
plementation achieves competitive accuracies on
several SensEval/SemEval English lexical-sample
and all-words tasks.
The remainder of this paper is organized as
follows. Section 2 gives the system description,
which introduces the system framework and the
details of the implementation. In Section 3, we
present the evaluation results of IMS on SensE-
78
val/SemEval English tasks. Finally, we conclude
in Section 4.
2 System Description
In this section, we first outline the IMS system,
and introduce the default preprocessing tools, the
feature types, and the machine learning method
used in our implementation. Then we briefly ex-
plain the collection of training data for content
words.
2.1 System Architecture
Figure 1 shows the system architecture of IMS.
The system accepts any input text. For each con-
tent word w (noun, verb, adjective, or adverb) in
the input text, IMS disambiguates the sense of w
and outputs a list of the senses of w, where each
sense si is assigned a probability according to the
likelihood of si appearing in that context. The
sense inventory used is based on WordNet (Miller,
1990) version 1.7.1.
IMS consists of three independent modules:
preprocessing, feature and instance extraction, and
classification. Knowledge sources are generated
from input texts in the preprocessing step. With
these knowledge sources, instances together with
their features are extracted in the instance and fea-
ture extraction step. Then we train one classifica-
tion model for each word type. The model will be
used to classify test instances of the corresponding
word type.
2.1.1 Preprocessing
Preprocessing is the step to convert input texts into
formatted information. Users can integrate differ-
ent tools in this step. These tools are applied on the
input texts to extract knowledge sources such as
sentence boundaries, part-of-speech tags, etc. The
extracted knowledge sources are stored for use in
the later steps.
In IMS, preprocessing is carried out in four
steps:
? Detect the sentence boundaries in a raw input
text with a sentence splitter.
? Tokenize the split sentences with a tokenizer.
? Assign POS tags to all tokens with a POS tag-
ger.
? Find the lemma form of each token with a
lemmatizer.
By default, the sentence splitter and POS tag-
ger in the OpenNLP toolkit1 are used for sen-
tence splitting and POS tagging. A Java version of
Penn TreeBank tokenizer2 is applied in tokeniza-
tion. JWNL3, a Java API for accessing the Word-
Net (Miller, 1990) thesaurus, is used to find the
lemma form of each token.
2.1.2 Feature and Instance Extraction
After gathering the formatted information in the
preprocessing step, we use an instance extractor
together with a list of feature extractors to extract
the instances and their associated features.
Previous research has found that combining
multiple knowledge sources achieves high WSD
accuracy (Ng and Lee, 1996; Lee and Ng, 2002;
Decadt et al, 2004). In IMS, we follow Lee and
Ng (2002) and combine three knowledge sources
for all content word types4:
? POS Tags of Surrounding Words We use
the POS tags of three words to the left and
three words to the right of the target ambigu-
ous word, and the target word itself. The
POS tag feature cannot cross sentence bound-
ary, which means all the associated surround-
ing words should be in the same sentence as
the target word. If a word crosses sentence
boundary, the corresponding POS tag value
will be assigned as null.
For example, suppose we want to disam-
biguate the word interest in a POS-tagged
sentence ?My/PRP$ brother/NN has/VBZ
always/RB taken/VBN a/DT keen/JJ inter-
est/NN in/IN my/PRP$ work/NN ./.?. The 7
POS tag features for this instance are <VBN,
DT, JJ, NN, IN, PRP$, NN>.
? Surrounding Words Surrounding words fea-
tures include all the individual words in the
surrounding context of an ambiguous word
w. The surrounding words can be in the cur-
rent sentence or immediately adjacent sen-
tences.
However, we remove the words that are in
a list of stop words. Words that contain
no alphabetic characters, such as punctuation
1http://opennlp.sourceforge.net/
2http://www.cis.upenn.edu/?treebank/
tokenizer.sed
3http://jwordnet.sourceforge.net/
4Syntactic relations are omitted for efficiency reason.
79
I n p u t D o c u m e n t C l a s s i f i
c
a
t
i
o n O u t p u t
M a c h i n e L e a r n i n g
T o o l k i t
P r e p r o c e
s s i
n g
I n
s
t
a
n c e E x t r
a
c t
i
o n
I n
s
t
a
n c e E x t r
a
c t o r
F e
a
t u r e E x t r
a
c t
i
o n
P O S F e a t u r e
E x t r a c t o r
??
L o c a l C o l l o c a t i o n
E x t r a c t o r
S u r r o u n d i n g W o r d
E x t r a c t o r
S e n t e n c e S p l i t t e r
T o k e n i z e r
P O S T a g g e r
L e m m a t i z e r
??
Figure 1: IMS system architecture
symbols and numbers, are also discarded.
The remaining words are converted to their
lemma forms in lower case. Each lemma is
considered as one feature. The feature value
is set to be 1 if the corresponding lemma oc-
curs in the surrounding context of w, 0 other-
wise.
For example, suppose there is a set of sur-
rounding words features {account, economy,
rate, take} in the training data set of the word
interest. For a test instance of interest in
the sentence ?My brother has always taken a
keen interest in my work .?, the surrounding
word feature vector will be <0, 0, 0, 1>.
? Local Collocations We use 11 local collo-
cations features including: C?2,?2, C?1,?1,
C1,1, C2,2, C?2,?1, C?1,1, C1,2, C?3,?1,
C?2,1, C?1,2, and C1,3, where Ci,j refers to
an ordered sequence of words in the same
sentence of w. Offsets i and j denote the
starting and ending positions of the sequence
relative to w, where a negative (positive) off-
set refers to a word to the left (right) of w.
For example, suppose in the training data set,
the word interest has a set of local colloca-
tions {?account .?, ?of all?, ?in my?, ?to
be?} for C1,2. For a test instance of inter-
est in the sentence ?My brother has always
taken a keen interest in my work .?, the value
of feature C1,2 will be ?in my?.
As shown in Figure 1, we implement one fea-
ture extractor for each feature type. The IMS soft-
ware package is organized in such a way that users
can easily specify their own feature set by im-
plementing more feature extractors to exploit new
features.
2.1.3 Classification
In IMS, the classifier trains a model for each word
type which has training data during the training
process. The instances collected in the previous
step are converted to the format expected by the
machine learning toolkit in use. Thus, the classifi-
cation step is separate from the feature extraction
step. We use LIBLINEAR5 (Fan et al, 2008) as
the default classifier of IMS, with a linear kernel
and all the parameters set to their default values.
Accordingly, we implement an interface to convert
the instances into the LIBLINEAR feature vector
format.
The utilization of other machine learning soft-
ware can be achieved by implementing the corre-
sponding module interfaces to them. For instance,
IMS provides module interfaces to the WEKA ma-
chine learning toolkit (Witten and Frank, 2005),
LIBSVM6, and MaxEnt7.
The trained classification models will be ap-
plied to the test instances of the corresponding
word types in the testing process. If a test instance
word type is not seen during training, we will out-
put its predefined default sense, i.e., the WordNet
first sense, as the answer. Furthermore, if a word
type has neither training data nor predefined de-
fault sense, we will output ?U?, which stands for
the missing sense, as the answer.
5http://www.bwaldvogel.de/
liblinear-java/
6http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
7http://maxent.sourceforge.net/
80
2.2 The Training Data Set for All-Words
Tasks
Once we have a supervised WSD system, for the
users who only need WSD as a component in
their applications, it is also important to provide
them the classification models. The performance
of a supervised WSD system greatly depends on
the size of the sense-annotated training data used.
To overcome the lack of sense-annotated train-
ing examples, besides the training instances from
the widely used sense-annotated corpus SEMCOR
(Miller et al, 1994) and DSO corpus (Ng and Lee,
1996), we also follow the approach described in
Chan and Ng (2005) to extract more training ex-
amples from parallel texts.
The process of extracting training examples
from parallel texts is as follows:
? Collect a set of sentence-aligned parallel
texts. In our case, we use six English-Chinese
parallel corpora: Hong Kong Hansards, Hong
Kong News, Hong Kong Laws, Sinorama,
Xinhua News, and the English translation of
Chinese Treebank. They are all available
from the Linguistic Data Consortium (LDC).
? Perform tokenization on the English texts
with the Penn TreeBank tokenizer.
? Perform Chinese word segmentation on the
Chinese texts with the Chinese word segmen-
tation method proposed by Low et al (2005).
? Perform word alignment on the parallel texts
using the GIZA++ software (Och and Ney,
2000).
? Assign Chinese translations to each sense of
an English word w.
? Pick the occurrences of w which are aligned
to its chosen Chinese translations in the word
alignment output of GIZA++.
? Identify the senses of the selected occur-
rences of w by referring to their aligned Chi-
nese translations.
Finally, the English side of these selected occur-
rences together with their assigned senses are used
as training data.
We only extract training examples from paral-
lel texts for the top 60% most frequently occur-
ring polysemous content words in Brown Corpus
(BC), which includes 730 nouns, 190 verbs, and
326 adjectives. For each of the top 60% nouns and
adjectives, we gather a maximum of 1,000 training
examples from parallel texts. For each of the top
60% verbs, we extract not more than 500 examples
from parallel texts, as well as up to 500 examples
from the DSO corpus. We also make use of the
sense-annotated examples from SEMCOR as part
of our training data for all nouns, verbs, adjectives,
and 28 most frequently occurring adverbs in BC.
POS noun verb adj adv
# of types 11,445 4,705 5,129 28
Table 1: Statistics of the word types which have
training data for WordNet 1.7.1 sense inventory
The frequencies of word types which we have
training instances for WordNet sense inventory
version 1.7.1 are listed in Table 1. We generated
classification models with the IMS system for over
21,000 word types which we have training data.
On average, each word type has 38 training in-
stances. The total size of the models is about 200
megabytes.
3 Evaluation
In our experiments, we evaluate our IMS system
on SensEval and SemEval tasks, the benchmark
data sets for WSD. The evaluation on both lexical-
sample and all-words tasks measures the accuracy
of our IMS system as well as the quality of the
training data we have collected.
3.1 English Lexical-Sample Tasks
SensEval-2 SensEval-3
IMS 65.3% 72.6%
Rank 1 System 64.2% 72.9%
Rank 2 System 63.8% 72.6%
MFS 47.6% 55.2%
Table 2: WSD accuracies on SensEval lexical-
sample tasks
In SensEval English lexical-sample tasks, both
the training and test data sets are provided. A com-
mon baseline for lexical-sample task is to select
the most frequent sense (MFS) in the training data
as the answer.
We evaluate IMS on the SensEval-2 and
SensEval-3 English lexical-sample tasks. Table 2
compares the performance of our system to the top
81
two systems that participated in the above tasks
(Yarowsky et al, 2001; Mihalcea and Moldovan,
2001; Mihalcea et al, 2004). Evaluation results
show that IMS achieves significantly better accu-
racies than the MFS baseline. Comparing to the
top participating systems, IMS achieves compara-
ble results.
3.2 English All-Words Tasks
In SensEval and SemEval English all-words tasks,
no training data are provided. Therefore, the MFS
baseline is no longer suitable for all-words tasks.
Because the order of senses in WordNet is based
on the frequency of senses in SEMCOR, the Word-
Net first sense (WNs1) baseline always assigns the
first sense in WordNet as the answer. We will use
it as the baseline in all-words tasks.
Using the training data collected with the
method described in Section 2.2, we apply our sys-
tem on the SensEval-2, SensEval-3, and SemEval-
2007 English all-words tasks. Similarly, we also
compare the performance of our system to the top
two systems that participated in the above tasks
(Palmer et al, 2001; Snyder and Palmer, 2004;
Pradhan et al, 2007). The evaluation results are
shown in Table 3. IMS easily beats the WNs1
baseline. It ranks first in SensEval-3 English fine-
grained all-words task and SemEval-2007 English
coarse-grained all-words task, and is also compet-
itive in the remaining tasks. It is worth noting
that because of the small test data set in SemEval-
2007 English fine-grained all-words task, the dif-
ferences between IMS and the best participating
systems are not statistically significant.
Overall, IMS achieves good WSD accuracies on
both all-words and lexical-sample tasks. The per-
formance of IMS shows that it is a state-of-the-art
WSD system.
4 Conclusion
This paper presents IMS, an English all-words
WSD system. The goal of IMS is to provide a
flexible platform for supervised WSD, as well as
an all-words WSD component with good perfor-
mance for other applications.
The framework of IMS allows us to integrate
different preprocessing tools to generate knowl-
edge sources. Users can implement various fea-
ture types and different machine learning methods
or toolkits according to their requirements. By
default, the IMS system implements three kinds
of feature types and uses a linear kernel SVM as
the classifier. Our evaluation on English lexical-
sample tasks proves the strength of our system.
With this system, we also provide a large num-
ber of classification models trained with the sense-
annotated training examples from SEMCOR, DSO
corpus, and 6 parallel corpora, for all content
words. Evaluation on English all-words tasks
shows that IMS with these models achieves state-
of-the-art WSD accuracies compared to the top
participating systems.
As a Java-based system, IMS is platform
independent. The source code of IMS and
the classification models can be found on the
homepage: http://nlp.comp.nus.edu.
sg/software and are available for research,
non-commercial use.
Acknowledgments
This research is done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.
References
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
Proceedings of the 20th National Conference on Ar-
tificial Intelligence (AAAI), pages 1037?1042, Pitts-
burgh, Pennsylvania, USA.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007a. Word sense disambiguation improves sta-
tistical machine translation. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 33?40, Prague,
Czech Republic.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007b.
NUS-PT: Exploiting parallel texts for word sense
disambiguation in the English all-words tasks. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages
253?256, Prague, Czech Republic.
Bart Decadt, Veronique Hoste, and Walter Daelemans.
2004. GAMBL, genetic algorithm optimization of
memory-based WSD. In Proceedings of the Third
82
SensEval-2 SensEval-3 SemEval-2007
Fine-grained Fine-grained Fine-grained Coarse-grained
IMS 68.2% 67.6% 58.3% 82.6%
Rank 1 System 69.0% 65.2% 59.1% 82.5%
Rank 2 System 63.6% 64.6% 58.7% 81.6%
WNs1 61.9% 62.4% 51.4% 78.9%
Table 3: WSD accuracies on SensEval/SemEval all-words tasks
International Workshop on Evaluating Word Sense
Disambiguation Systems (SensEval-3), pages 108?
112, Barcelona, Spain.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 41?48, Philadelphia, Pennsylvania, USA.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161?164, Jeju Island, Korea.
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL) Interactive Poster and Demonstration
Sessions, pages 53?56, Ann Arbor, Michigan, USA.
Rada Mihalcea and Dan Moldovan. 2001. Pattern
learning and active feature selection for word sense
disambiguation. In Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems (SensEval-2), pages 127?130,
Toulouse, France.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The SensEval-3 English lexical sam-
ple task. In Proceedings of the Third International
Workshop on Evaluating Word Sense Disambigua-
tion Systems (SensEval-3), pages 25?28, Barcelona,
Spain.
George Miller, Martin Chodorow, Shari Landes, Clau-
dia Leacock, and Robert Thomas. 1994. Using a
semantic concordance for sense identification. In
Proceedings of ARPA Human Language Technology
Workshop, pages 240?243, Morristown, New Jersey,
USA.
George Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 40?47,
Santa Cruz, California, USA.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 440?447, Hong
Kong.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. En-
glish tasks: All-words and verb lexical sample. In
Proceedings of the Second International Workshop
on Evaluating Word Sense Disambiguation Systems
(SensEval-2), pages 21?24, Toulouse, France.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87?92,
Prague, Czech Republic.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the Third
International Workshop on Evaluating Word Sense
Disambiguation Systems (SensEval-3), pages 41?
43, Barcelona, Spain.
Christopher Stokoe, Michael P. Oakes, and John Tait.
2003. Word sense disambiguation in information
retrieval revisited. In Proceedings of the Twenty-
Sixth Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval (SIGIR), pages 159?166, Toronto, Canada.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
David Yarowsky, Radu Florian, Siviu Cucerzan, and
Charles Schafer. 2001. The Johns Hopkins
SensEval-2 system description. In Proceedings of
the Second International Workshop on Evaluating
Word Sense Disambiguation Systems (SensEval-2),
pages 163?166, Toulouse, France.
83
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 273?282,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Word Sense Disambiguation Improves Information Retrieval
Zhi Zhong and Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive, Singapore 117417
{zhongzhi, nght}@comp.nus.edu.sg
Abstract
Previous research has conflicting conclu-
sions on whether word sense disambiguation
(WSD) systems can improve information re-
trieval (IR) performance. In this paper, we
propose a method to estimate sense distribu-
tions for short queries. Together with the
senses predicted for words in documents, we
propose a novel approach to incorporate word
senses into the language modeling approach
to IR and also exploit the integration of syn-
onym relations. Our experimental results on
standard TREC collections show that using the
word senses tagged by a supervised WSD sys-
tem, we obtain significant improvements over
a state-of-the-art IR system.
1 Introduction
Word sense disambiguation (WSD) is the task of
identifying the correct meaning of a word in context.
As a basic semantic understanding task at the lexi-
cal level, WSD is a fundamental problem in natural
language processing. It can be potentially used as
a component in many applications, such as machine
translation (MT) and information retrieval (IR).
In recent years, driven by Senseval/Semeval
workshops, WSD systems achieve promising perfor-
mance. In the application of WSD to MT, research
has shown that integrating WSD in appropriate ways
significantly improves the performance of MT sys-
tems (Chan et al, 2007; Carpuat and Wu, 2007).
In the application to IR, WSD can bring two kinds
of benefits. First, queries may contain ambiguous
words (terms), which have multiple meanings. The
ambiguities of these query words can hurt retrieval
precision. Identifying the correct meaning of the
ambiguous words in both queries and documents
can help improve retrieval precision. Second, query
words may have tightly related meanings with other
words not in the query. Making use of these relations
between words can improve retrieval recall.
Overall, IR systems can potentially benefit from
the correct meanings of words provided by WSD
systems. However, in previous investigations of the
usage of WSD in IR, different researchers arrived
at conflicting observations and conclusions. Some
of the early research showed a drop in retrieval per-
formance by using word senses (Krovetz and Croft,
1992; Voorhees, 1993). Some other experiments ob-
served improvements by integrating word senses in
IR systems (Schu?tze and Pedersen, 1995; Gonzalo
et al, 1998; Stokoe et al, 2003; Kim et al, 2004).
This paper proposes the use of word senses to
improve the performance of IR. We propose an ap-
proach to annotate the senses for short queries. We
incorporate word senses into the language modeling
(LM) approach to IR (Ponte and Croft, 1998), and
utilize sense synonym relations to further improve
the performance. Our evaluation on standard TREC1
data sets shows that supervised WSD outperforms
two other WSD baselines and significantly improves
IR.
The rest of this paper is organized as follows. In
Section 2, we first review previous work using WSD
in IR. Section 3 introduces the LM approach to IR,
including the pseudo relevance feedback method.
We describe our WSD system and the method of
1http://trec.nist.gov/
273
generating word senses for query terms in Section
4, followed by presenting our novel method of in-
corporating word senses and their synonyms into the
LM approach in Section 5. We present experiments
and analyze the results in Section 6. Finally, we con-
clude in Section 7.
2 Related Work
Many previous studies have analyzed the benefits
and the problems of applying WSD to IR. Krovetz
and Croft (1992) studied the sense matches between
terms in query and the document collection. They
concluded that the benefits of WSD in IR are not as
expected because query words have skewed sense
distribution and the collocation effect from other
query terms already performs some disambiguation.
Sanderson (1994; 2000) used pseudowords to intro-
duce artificial word ambiguity in order to study the
impact of sense ambiguity on IR. He concluded that
because the effectiveness of WSD can be negated
by inaccurate WSD performance, high accuracy of
WSD is an essential requirement to achieve im-
provement. In another work, Gonzalo et al (1998)
used a manually sense annotated corpus, SemCor, to
study the effects of incorrect disambiguation. They
obtained significant improvements by representing
documents and queries with accurate senses as well
as synsets (synonym sets). Their experiment also
showed that with the synset representation, which
included synonym information, WSD with an error
rate of 40%?50% can still improve IR performance.
Their later work (Gonzalo et al, 1999) verified that
part of speech (POS) information is discriminatory
for IR purposes.
Several works attempted to disambiguate terms
in both queries and documents with the senses pre-
defined in hand-crafted sense inventories, and then
used the senses to perform indexing and retrieval.
Voorhees (1993) used the hyponymy (?IS-A?) rela-
tion in WordNet (Miller, 1990) to disambiguate the
polysemous nouns in a text. In her experiments, the
performance of sense-based retrieval is worse than
stem-based retrieval on all test collections. Her anal-
ysis showed that inaccurate WSD caused the poor
results.
Stokoe et al (2003) employed a fine-grained
WSD system with an accuracy of 62.1% to dis-
ambiguate terms in both the text collections and
the queries in their experiments. Their evalua-
tion on TREC collections achieved significant im-
provements over a standard term based vector space
model. However, it is hard to judge the effect
of word senses because of the overall poor perfor-
mances of their baseline method and their system.
Instead of using fine-grained sense inventory, Kim
et al (2004) tagged words with 25 root senses of
nouns in WordNet. Their retrieval method main-
tained the stem-based index and adjusted the term
weight in a document according to its sense match-
ing result with the query. They attributed the im-
provement achieved on TREC collections to their
coarse-grained, consistent, and flexible sense tag-
ging method. The integration of senses into the tra-
ditional stem-based index overcomes some of the
negative impact of disambiguation errors.
Different from using predefined sense inventories,
Schu?tze and Pedersen (1995) induced the sense in-
ventory directly from the text retrieval collection.
For each word, its occurrences were clustered into
senses based on the similarities of their contexts.
Their experiments showed that using senses im-
proved retrieval performance, and the combination
of word-based ranking and sense-based ranking can
further improve performance. However, the cluster-
ing process of each word is a time consuming task.
Because the sense inventory is collection dependent,
it is also hard to expand the text collection without
re-doing preprocessing.
Many studies investigated the expansion effects
by using knowledge sources from thesauri. Some
researchers achieved improvements by expanding
the disambiguated query words with synonyms and
some other information from WordNet (Voorhees,
1994; Liu et al, 2004; Liu et al, 2005; Fang, 2008).
The usage of knowledge sources from WordNet in
document expansion also showed improvements in
IR systems (Cao et al, 2005; Agirre et al, 2010).
The previous work shows that the WSD errors can
easily neutralize its positive effect. It is important
to reduce the negative impact of erroneous disam-
biguation, and the integration of senses into tradi-
tional term index, such as stem-based index, is a pos-
sible solution. The utilization of semantic relations
has proved to be helpful for IR. It is also interest-
274
ing to investigate the utilization of semantic relations
among senses in IR.
3 The Language Modeling Approach to IR
This section describes the LM approach to IR and
the pseudo relevance feedback approach.
3.1 The language modeling approach
In the language modeling approach to IR, language
models are constructed for each query q and each
document d in a text collection C. The documents
in C are ranked by the distance to a given query q
according to the language models. The most com-
monly used language model in IR is the unigram
model, in which terms are assumed to be indepen-
dent of each other. In the rest of this paper, language
model will refer to the unigram language model.
One of the commonly used measures of the sim-
ilarity between query model and document model
is negative Kullback-Leibler (KL) divergence (Laf-
ferty and Zhai, 2001). With unigram model, the neg-
ative KL-divergence between model ?q of query q
and model ?d of document d is calculated as follows:
?D(?q||?d)=?
?
t?V
p(t|?q) log
p(t|?q)
p(t|?d)
=
?
t?V
p(t|?q) log p(t|?d)?
?
t?V
p(t|?q) log p(t|?q)
=
?
t?V
p(t|?q) log p(t|?d) + E(?q), (1)
where p(t|?q) and p(t|?d) are the generative proba-
bilities of a term t from the models ?q and ?d, V is
the vocabulary of C, and E(?q) is the entropy of q.
Define tf (t, d) and tf (t, q) as the frequencies of t
in d and q, respectively. Normally, p(t|?q) is calcu-
lated with maximum likelihood estimation (MLE):
p(t|?q) =
tf (t,q)P
t??q tf (t
?,q) . (2)
In the calculation of p(t|?d), several smoothing
methods have been proposed to overcome the data
sparseness problem of a language model constructed
from one document (Zhai and Lafferty, 2001b). For
example, p(t|?d) with the Dirichlet-prior smoothing
can be calculated as follows:
p(t|?d) =
tf (t, d) + ? p(t|?C)
?
t??V tf (t
?, d) + ?
, (3)
where ? is the prior parameter in the Dirichlet-prior
smoothing method, and p(t|?C) is the probability of
t in C, which is often calculated with MLE:
p(t|?C) =
P
d??C tf (t,d
?)
P
d??C
P
t??V tf (t
?,d?) .
3.2 Pseudo relevance feedback
Pseudo relevance feedback (PRF) is widely used in
IR to achieve better performance. It is constructed
with two retrieval steps. In the first step, ranked doc-
uments are retrieved from C by a normal retrieval
method with the original query q. In the second step,
a number of terms are selected from the top k ranked
documents Dq for query expansion, under the as-
sumption that these k documents are relevant to the
query. Then, the expanded query is used to retrieve
the documents from C.
There are several methods to select expansion
terms in the second step (Zhai and Lafferty, 2001a).
For example, in Indri2, the terms are first ranked by
the following score:
v(t,Dq) =
?
d?Dq log(
tf (t,d)
|d| ?
1
p(t|?C)
),
as in Ponte (1998). Define p(q|?d) as the probability
score assigned to d. The topm terms Tq are selected
with weights calculated based on the relevance
model described in Lavrenko and Croft (2001):
w(t,Dq) =
?
d?Dq
[
tf (t,d)
|d| ? p(q|?d)? p(?d)
]
,
which calculates the sum of weighted probabilities
of t in each document. After normalization, the
probability of t in ?rq is calculated as follows:
p(t|?rq) =
w(t,Dq)P
t??Tq
w(t?,Dq)
.
Finally, the relevance model is interpolated with the
original query model:
p(t|?prfq ) = ? p(t|?
r
q) + (1? ?)p(t|?q), (4)
where parameter ? controls the amount of feedback.
The new model ?prfq is used to replace the original
one ?q in Equation 1.
Collection enrichment (CE) (Kwok and Chan,
1998) is a technique to improve the quality of the
feedback documents by making use of an external
target text collection X in addition to the original
target C in the first step of PRF. The usage of X is
supposed to provide more relevant feedback docu-
ments and feedback query terms.
2http://lemurproject.org/indri/
275
4 Word Sense Disambiguation
In this section, we first describe the construction of
our WSD system. Then, we propose the method of
assigning senses to query terms.
4.1 Word sense disambiguation system
Previous research shows that translations in another
language can be used to disambiguate the meanings
of words (Chan and Ng, 2005; Zhong and Ng, 2009).
We construct our supervised WSD system directly
from parallel corpora.
To generate the WSD training data, 7 parallel cor-
pora were used, including Chinese Treebank, FBIS
Corpus, Hong Kong Hansards, Hong Kong Laws,
Hong Kong News, Sinorama News Magazine, and
Xinhua Newswire. These corpora were already
aligned at sentence level. We tokenized English
texts with Penn Treebank Tokenizer, and performed
word segmentation on Chinese texts. Then, word
alignment was performed on the parallel corpora
with the GIZA++ software (Och and Ney, 2003).
For each English morphological root e, the En-
glish sentences containing its occurrences were ex-
tracted from the word aligned output of GIZA++,
as well as the corresponding translations of these
occurrences. To minimize noisy word alignment
result, translations with no Chinese character were
deleted, and we further removed a translation when
it only appears once, or its frequency is less than 10
and also less than 1% of the frequency of e. Finally,
only the most frequent 10 translations were kept for
efficiency consideration.
The English part of the remaining occurrences
were used as training data. Because multiple En-
glish words may have the same Chinese transla-
tion, to differentiate them, each Chinese translation
is concatenated with the English morphological root
to form a word sense. We employed a supervised
WSD system, IMS3, to train the WSD models. IMS
(Zhong and Ng, 2010) integrates multiple knowl-
edge sources as features. We used MaxEnt as the
machine learning algorithm. Finally, the system can
disambiguate the words by assigning probabilities to
different senses.
3http://nlp.comp.nus.edu.sg/software/ims
4.2 Estimating sense distributions for query
terms
In IR, both terms in queries and the text collection
can be ambiguous. Hence, WSD is needed to disam-
biguate these ambiguous terms. In most cases, doc-
uments in a text collection are full articles. There-
fore, a WSD system has sufficient context to dis-
ambiguate the words in the document. In contrast,
queries are usually short, often with only two or
three terms in a query. Short queries pose a chal-
lenge to WSD systems since there is insufficient
context to disambiguate a term in a short query.
One possible solution to this problem is to find
some text fragments that contain a query term. Sup-
pose we already have a basic IR method which does
not require any sense information, such as the stem-
based LM approach. Similar to the PRF method,
assuming that the top k documents retrieved by the
basic method are relevant to the query, these k docu-
ments can be used to represent query q (Broder et al,
2007; Bendersky et al, 2010; He and Wu, 2011). We
propose a method to estimate the sense probabilities
of each query term of q from these top k retrieved
documents.
Suppose the words in all documents of the text
collection are disambiguated with a WSD system,
and each word occurrence w in document d is as-
signed a vector of senses, S(w). Define the proba-
bility of assigning sense s to w as p(w, s, d). Given
a query q, suppose Dq is the set of top k documents
retrieved by the basic method, with the probability
score p(q|?d) assigned to d ? Dq.
Given a query term t ? q
S(t, q) = {}
sum = 0
for each document d ? Dq
for each word occurrence w ? d, whose stem form is
identical to the stem form of t
for each sense s ? S(w)
S(t, q) = S(t, q) ? {s}
p(t, s, q) = p(t, s, q) + p(q|?d) p(w, s, d)
sum = sum + p(q|?d) p(w, s, d)
for each sense s ? S(t, q)
p(t, s, q) = p(t, s, q)/sum
Return S(t, q), with probability p(t, s, q) for s ? S(t, q)
Figure 1: Process of generating senses for query terms
Figure 1 shows the pseudocode of calculating the
276
sense distribution for a query term t in q with Dq,
where S(t, q) is the set of senses assigned to t and
p(t, s, q) is the probability of tagging t as sense s.
Basically, we utilized the sense distribution of the
words with the same stem form in Dq as a proxy to
estimate the sense probabilities of a query term. The
retrieval scores are used to weight the information
from the corresponding retrieved documents in Dq.
5 Incorporating Senses into Language
Modeling Approaches
In this section, we propose to incorporate senses into
the LM approach to IR. Then, we describe the inte-
gration of sense synonym relations into our model.
5.1 Incorporating senses as smoothing
With the method described in Section 4.2, both the
terms in queries and documents have been sense
tagged. The next problem is to incorporate the sense
information into the language modeling approach.
Suppose p(t, s, q) is the probability of tagging a
query term t ? q as sense s, and p(w, s, d) is the
probability of tagging a word occurrence w ? d as
sense s. Given a query q and a document d in text
collection C, we want to re-estimate the language
models by making use of the sense information as-
signed to them.
Define the frequency of s in d as:
stf (s, d) =
?
w?d p(w, s, d),
and the frequency of s in C as:
stf (s, C) =
?
d?C stf (s, d).
Define the frequencies of sense set S in d and C as:
stf (S, d) =
?
s?S stf (s, d),
stf (S,C) =
?
s?S stf (s, C).
For a term t ? q, with senses S(t, q):{s1, ..., sn},
suppose V :{p(t, s1, q), ..., p(t, sn, q)} is the vector
of probabilities assigned to the senses of t and
W :{stf (s1, d), ..., stf (sn, d)} is the vector of fre-
quencies of S(t, q) in d. The function cos(t, q, d)
calculates the cosine similarity between vector V
and vector W . Assume D is a set of documents
in C which contain any sense in S(t, q), we define
function cos(t, q) =
?
d?D cos(t, q, d)/|D|, which
calculates the mean of the sense cosine similarities,
and define function ?cos(t, q, d) = cos(t, q, d) ?
cos(t, q), which calculates the difference between
cos(t, q, d) and the corresponding mean value.
Given a query q, we re-estimate the term fre-
quency of query term t in d with sense information
integrated as smoothing:
tf sen(t, d) = tf (t, d) + sen(t, q, d), (5)
where function sen(t, q, d) is a measure of t?s sense
information in d, which is defined as follows:
sen(t, q, d) = ??cos(t,q,d)stf (S(t, q), d). (6)
In sen(t, q, d), the last item stf (S(t, q), d) calcu-
lates the sum of the sense frequencies of t senses in
d, which represents the amount of t?s sense informa-
tion in d. The first item ??cos(t,q,d) is a weight of the
sense information concerning the relative sense sim-
ilarity ?cos(t, q, d), where ? is a positive parame-
ter to control the impact of sense similarity. When
?cos(t, q, d) is larger than zero, such that the sense
similarity of d and q according to t is above the av-
erage, the weight for the sense information is larger
than 1; otherwise, it is less than 1. The more similar
they are, the larger the weight value. For t /? q, be-
cause the sense set S(t, q) is empty, stf (S(t, q), d)
equals to zero and tf sen(t, d) is identical to tf (t, d).
With sense incorporated, the term frequency is in-
fluenced by the sense information. Consequently,
the estimation of probability of t in d becomes query
specific:
p(t|?send ) =
tf sen(t, d) + ? p(t|?
sen
C )?
t??V tf sen(t
?, d) + ?
, (7)
where the probability of t in C is re-calculated as:
p(t|?senC ) =
P
d??C tf sen (t,d
?)
P
d??C
P
t??V tf sen (t
?,d?) .
5.2 Expanding with synonym relations
Words usually have some semantic relations with
others. Synonym relation is one of the semantic re-
lations commonly used to improve IR performance.
In this part, we further integrate the synonym rela-
tions of senses into the LM approach.
Suppose R(s) is the set of senses having syn-
onym relation with sense s. Define S(q) as the set
of senses of query q, S(q) =
?
t?q S(t, q), and de-
fine R(s, q)=R(s)?S(q). We update the frequency
of a query term t in d by integrating the synonym
relations as follows:
tf syn(t, d) = tf sen(t, d) + syn(t, q, d), (8)
277
where syn(t, q, d) is a function measuring the syn-
onym information in d:
syn(t, q, d) =
?
s?S(t)
?(s, q)p(t, s, q)stf (R(s, q), d).
The last item stf (R(s, q), d) in syn(t, q, d) is the
sum of the sense frequencies of R(s, q) in d. Notice
that the synonym senses already appearing in S(q)
are not included in the calculation, because the infor-
mation of these senses has been used in some other
places in the retrieval function. The frequency of
synonyms, stf (R(s, q), d), is weighted by p(t, s, q)
together with a scaling function ?(s, q):
?(s, q) = min(1, stf (s,C)stf (R(s,q),C)).
When stf (s, C), the frequency of sense s in C, is
less than stf (R(s, q), C), the frequency of R(s, q)
in C, the function ?(s, q) scales down the impact
of synonyms according to the ratio of these two fre-
quencies. The scaling function makes sure that the
overall impact of the synonym senses is not greater
than the original word senses.
Accordingly, we have the probability of t in d up-
dated to:
p(t|?synd ) =
tf syn(t, d) + ? p(t|?
syn
C )?
t??V tf syn(t
?, d) + ?
, (9)
and the probability of t in C is calculated as:
p(t|?synC ) =
P
d??C tf syn (t,d
?)
P
d??C
P
t??V tf syn (t
?,d?) .
With this language model, the probability of a query
term in a document is enlarged by the synonyms of
its senses; The more its synonym senses in a doc-
ument, the higher the probability. Consequently,
documents with more synonym senses of the query
terms will get higher retrieval rankings.
6 Experiments
In this section, we evaluate and analyze the mod-
els proposed in Section 5 on standard TREC collec-
tions.
6.1 Experimental settings
We conduct experiments on the TREC collection.
The text collection C includes the documents from
TREC disk 4 and 5, minus the CR (Congressional
Record) corpus, with 528,155 documents in total. In
addition, the other documents in TREC disk 1 to 5
are used as the external text collection X .
We use 50 queries from TREC6 Ad Hoc task
as the development set, and evaluate on 50 queries
from TREC7 Ad Hoc task, 50 queries from TREC8
Ad Hoc task, 50 queries from ROBUST 2003
(RB03), and 49 queries from ROBUST 2004
(RB04). In total, our test set includes 199 queries.
We use the terms in the title field of TREC topics as
queries. Table 1 shows the statistics of the five query
sets. The first column lists the query topics, and the
column #qry is the number of queries. The column
Ave gives the average query length, and the column
Rels is the total number of relevant documents.
Query Set Topics #qry Ave Rels
TREC6 301?350 50 2.58 4,290
TREC7 351?400 50 2.50 4,674
TREC8 401?450 50 2.46 4,728
RB03 601?650 50 3.00 1,658
RB044 651?700 49 2.96 2,062
Table 1: Statistics of query sets
We use the Lemur toolkit (Ogilvie and Callan,
2001) version 4.11 as the basic retrieval tool, and se-
lect the default unigram LM approach based on KL-
divergence and Dirichlet-prior smoothing method in
Lemur as our basic retrieval approach. Stop words
are removed from queries and documents using the
standard INQUERY stop words list (Allan et al,
2000), and then the Porter stemmer is applied to per-
form stemming. The stem forms are finally used for
indexing and retrieval.
We set the smoothing parameter ? in Equation 3
to 400 by tuning on TREC6 query set in a range of
{100, 400, 700, 1000, 1500, 2000, 3000, 4000, 5000}.
With this basic method, up to 10 top ranked docu-
ments Dq are retrieved for each query q from the
extended text collection C ? X , for the usage of
performing PRF and generating query senses.
For PRF, we follow the implementation of Indri?s
PRF method and further apply the CE technique as
described in Section 3.2. The number of terms se-
lected from Dq for expansion is tuned from range
{20, 25, 30, 35, 40} and set to 25. The interpolation
parameter ? in Equation 4 is set to 0.7 from range
4Topic 672 is eliminated, since it has no relevant document.
278
Method TREC7 TERC8 RB03 RB04 Comb Impr #ret-rel
Top 1 0.2530 0.3063 0.3704 0.4019 - - -
Top 2 0.2488 0.2876 0.3065 0.4008 - - -
Top 3 0.2427 0.2853 0.3037 0.3514 - - -
Stemprf (Baseline) 0.2634 0.2944 0.3586 0.3781 0.3234 - 9248
Stemprf+MFS 0.2655 0.2971 0.3626? 0.3802 0.3261? 0.84% 9281
Stemprf+Even 0.2655 0.2972 0.3623? 0.3814 0.3263? 0.91% 9284
Stemprf+WSD 0.2679? 0.2986? 0.3649? 0.3842 0.3286? 1.63% 9332
Stemprf+MFS+Syn 0.2756? 0.3034? 0.3649? 0.3859 0.3322? 2.73% 9418
Stemprf+Even+Syn 0.2713? 0.3061? 0.3657? 0.3859? 0.3320? 2.67% 9445
Stemprf+WSD+Syn 0.2762? 0.3126? 0.3735? 0.3891? 0.3376? 4.39% 9538
Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the
next row shows the performance of the baseline method, and the rest rows are the results of our method with different
settings. Single dagger (?) and double dagger (?) indicate statistically significant improvement over Stemprf at the
95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
{0.1, 0.2, ..., 0.9}. The CE-PRF method with this
parameter setting is chosen as the baseline.
To estimate the sense distributions for terms in
query q, the method described in Section 4.2 is ap-
plied with Dq. To disambiguate the documents in
the text collection, besides the usage of the super-
vised WSD system described in Section 4.1, two
WSD baseline methods, Even and MFS, are applied
for comparison. The method Even assigns equal
probabilities to all senses for each word, and the
method MFS tags the words with their correspond-
ing most frequent senses. The parameter ? in Equa-
tion 6 is tuned on TREC6 from 1 to 10 in increment
of 1 for each sense tagging method. It is set to 7,
6, and 9 for the supervised WSD method, the Even
method, and the MFS method, respectively.
Notice that the sense in our WSD system is con-
ducted with two parts, a morphological root and a
Chinese translation. The Chinese parts not only dis-
ambiguate senses, but also provide clues of connec-
tions among different words. Assume that the senses
with the same Chinese part are synonyms, there-
fore, we can generate a set of synonyms for each
sense, and then utilize these synonym relations in
the method proposed in Section 5.2.
6.2 Experimental results
For evaluation, we use average precision (AP) as the
metric to evaluate the performance on each query q:
AP(q) =
PR
r=1 [p(r)rel(r)]
relevance(q) ,
where relevance(q) is the number of documents rel-
evant to q, R is the number of retrieved documents,
r is the rank, p(r) is the precision of the top r re-
trieved documents, and rel(r) equals to 1 if the rth
document is relevant, and 0 otherwise. Mean aver-
age precision (MAP) is a metric to evaluate the per-
formance on a set of queries Q:
MAP(Q) =
P
q?Q AP(q)
|Q| ,
where |Q| is the number of queries in Q.
We retrieve the top-ranked 1,000 documents for
each query, and use the MAP score as the main com-
paring metric. In Table 2, the first four columns are
the MAP scores of various methods on the TREC7,
TREC8, RB03, and RB04 query sets, respectively.
The column Comb shows the results on the union of
the four test query sets. The first three rows list the
results of the top three systems that participated in
the corresponding tasks. The row Stemprf shows the
performance of our baseline method, the stem-based
CE-PRF method. The column Impr calculates the
percentage improvement of each method over the
baseline Stemprf in column Comb. The last column
#ret-rel lists the total numbers of relevant documents
retrieved by different methods.
The rows Stemprf +{MFS, Even, WSD} are the re-
sults of Stemprf incorporating with the senses gen-
erated for the original query terms, by applying the
approach proposed in Section 5.1, with the MFS
method, the Even method, and our supervised WSD
method, respectively. Comparing to the baseline
method, all methods with sense integrated achieve
consistent improvements on all query sets. The
usage of the supervised WSD method outperforms
the other two WSD baselines, and it achieves sta-
279
tistically significant improvements over Stemprf on
TREC7, TREC8, and RB03.
The integration of senses into the baseline method
has two aspects of impact. First, the morphologi-
cal roots of senses conquer the irregular inflection
problem. Thus, the documents containing the irreg-
ular inflections are retrieved when senses are inte-
grated. For example, in topic 326 {ferry sinkings},
the stem form of sinkings is sink. As sink is an irreg-
ular verb, the usage of senses improves the retrieval
recall by retrieving the documents containing the in-
flection forms sunk, sank, and sunken.
Second, the senses output by supervised WSD
system help identify the meanings of query terms.
Take topic 357 {territorial waters dispute} for ex-
ample, the stem form of waters is water and its ap-
propriate sense in this query should be water ??
(body of water) instead of the most frequent sense
of water ? (H2O). In Stemprf +WSD, we correctly
identify the minority sense for this query term. In
another example, topic 425 {counterfeiting money},
the stem form of counterfeiting is counterfeit. Al-
though the most frequent sense counterfeit ??
(not genuine) is not wrong, another sense counter-
feit ?? (forged money) is more accurate for this
query term. The Chinese translation in the latter
sense represents the meaning of the phrase in orig-
inal query. Thus, Stemprf +WSD outperforms the
other two methods on this query by assigning the
highest probability for this sense.
Overall, the performance of Stemprf +WSD is bet-
ter than Stemprf +{MFS, Even} on 121 queries and
119 queries, respectively. The t-test at the confi-
dence level of 99% indicates that the improvements
are statistically significant.
The results of expanding with synonym relations
in the above three methods are shown in the last
three rows, Stemprf +{MFS, Even, WSD}+Syn. The
integration of synonym relations further improves
the performance no matter what kind of sense tag-
ging method is applied. The improvement varies
with different methods on different query sets. As
shown in the last column of Table 2, the number of
relevant documents retrieved is increased for each
method. Stemprf +Even+Syn retrieves more rele-
vant documents than Stemprf +MFS+Syn, because
the former method expands more senses. Overall,
the improvement achieved by Stemprf +WSD+Syn is
larger than the other two methods. It shows that
the WSD technique can help choose the appropriate
senses for synonym expansion.
Among the different settings, Stemprf +WSD+Syn
achieves the best performance. Its improvement
over the baseline method is statistically significant
at the 95% confidence level on RB04 and at the 99%
confidence level on the other three query sets, with
an overall improvement of 4.39%. It beats the best
participated systems on three out of four query sets5,
including TREC7, TREC8, and RB03.
7 Conclusion
This paper reports successful application of WSD
to IR. We proposed a method for annotating senses
to terms in short queries, and also described an ap-
proach to integrate senses into an LM approach for
IR. In the experiment on four query sets of TREC
collection, we compared the performance of a su-
pervised WSD method and two WSD baseline meth-
ods. Our experimental results showed that the incor-
poration of senses improved a state-of-the-art base-
line, a stem-based LM approach with PRF method.
The performance of applying the supervised WSD
method is better than the other two WSD base-
line methods. We also proposed a method to fur-
ther integrate the synonym relations to the LM ap-
proaches. With the integration of synonym rela-
tions, our best performance setting with the super-
vised WSD achieved an improvement of 4.39% over
the baseline method, and it outperformed the best
participating systems on three out of four query sets.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
E. Agirre, X. Arregi, and A. Otegi. 2010. Document ex-
pansion based on WordNet for robust IR. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics, pages 9?17.
5The top two systems on RB04 are the results of the same
participant with different configurations. They used lots of web
resources, such as search engines, to improve the performance.
280
J. Allan, M. E. Connell, W.B. Croft, F.F. Feng, D. Fisher,
and X. Li. 2000. INQUERY and TREC-9. In Pro-
ceedings of the 9th Text REtrieval Conference, pages
551?562.
M. Bendersky, W. B. Croft, and D. A. Smith. 2010.
Structural annotation of search queries using pseudo-
relevance feedback. In Proceedings of the 19th ACM
Conference on Information and Knowledge Manage-
ment, pages 1537?1540.
A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josi-
fovski, and T. Zhang. 2007. Robust classification of
rare queries using web knowledge. In Proceedings
of the 30th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 231?238.
G. Cao, J. Y. Nie, and J. Bai. 2005. Integrating word
relationships into language models. In Proceedings
of the 28th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 298?305.
M. Carpuat and D. Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 61?72.
Y. S. Chan and H. T. Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proceed-
ings of the 20th National Conference on Artificial In-
telligence, pages 1037?1042.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
33?40.
H. Fang. 2008. A re-examination of query expansion us-
ing lexical resources. In Proceedings of the 46th An-
nual Meeting of the Association of Computational Lin-
guistics: Human Language Technologies, pages 139?
147.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarrin. 1998.
Indexing with WordNet synsets can improve text re-
trieval. In Proceedings of the COLING-ACL Workshop
on Usage of WordNet in Natural Language Processing
Systems, pages 38?44.
J. Gonzalo, A. Penas, and F. Verdejo. 1999. Lexical
ambiguity and information retrieval revisited. In Pro-
ceedings of the 1999 Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, pages 195?202.
D. He and D. Wu. 2011. Enhancing query transla-
tion with relevance feedback in translingual informa-
tion retrieval. Information Processing & Management,
47(1):1?17.
S. B. Kim, H. C. Seo, and H. C. Rim. 2004. Informa-
tion retrieval using word senses: root sense tagging ap-
proach. In Proceedings of the 27th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 258?265.
R. Krovetz and W. B. Croft. 1992. Lexical ambiguity
and information retrieval. ACM Transactions on In-
formation Systems, 10(2):115?141.
K. L. Kwok and M. Chan. 1998. Improving two-stage
ad-hoc retrieval for short queries. In Proceedings
of the 21st International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 250?256.
J. Lafferty and C. Zhai. 2001. Document language mod-
els, query models, and risk minimization for informa-
tion retrieval. In Proceedings of the 24th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 111?119.
V. Lavrenko and W. B. Croft. 2001. Relevance based
language models. In Proceedings of the 24th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 120?127.
S. Liu, F. Liu, C. Yu, and W. Meng. 2004. An ef-
fective approach to document retrieval via utilizing
WordNet and recognizing phrases. In Proceedings
of the 27th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 266?272.
S. Liu, C. Yu, and W. Meng. 2005. Word sense disam-
biguation in queries. In Proceedings of the 14th ACM
Conference on Information and Knowledge Manage-
ment, pages 525?532.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235?312.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
P. Ogilvie and J. Callan. 2001. Experiments using the
Lemur toolkit. In Proceedings of the 10th Text RE-
trieval Conference, pages 103?108.
J. M. Ponte and W. B. Croft. 1998. A language model-
ing approach to information retrieval. In Proceedings
of the 21st International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 275?281.
J. M. Ponte. 1998. A Language Modeling Approach
to Information Retreival. Ph.D. thesis, Department of
Computer Science, University of Massachusetts.
M. Sanderson. 1994. Word sense disambiguation and in-
formation retrieval. In Proceedings of the 17th Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 142?151.
281
M. Sanderson. 2000. Retrieving with good sense. Infor-
mation Retrieval, 2(1):49?69.
H. Schu?tze and J. O. Pedersen. 1995. Information re-
trieval based on word senses. In Proceedings of the
4th Annual Symposium on Document Analysis and In-
formation Retrieval, pages 161?175.
C. Stokoe, M. P. Oakes, and J. Tait. 2003. Word sense
disambiguation in information retrieval revisited. In
Proceedings of the 26th International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 159?166.
E. M. Voorhees. 1993. Using WordNet to disam-
biguate word senses for text retrieval. In Proceedings
of the 16th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 171?180.
E. M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 61?69.
C. Zhai and J. Lafferty. 2001a. Model-based feedback
in the language modeling approach to information re-
trieval. In Proceedings of the 10th ACM Conference
on Information and Knowledge Management, pages
403?410.
C. Zhai and J. Lafferty. 2001b. A study of smoothing
methods for language models applied to ad hoc infor-
mation retrieval. In Proceedings of the 24th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 334?342.
Z. Zhong and H. T. Ng. 2009. Word sense disambigua-
tion for all words without hard labor. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence, pages 1616?1621.
Z. Zhong and H. T. Ng. 2010. It Makes Sense: A wide-
coverage word sense disambiguation system for free
text. In Proceedings of the 48th Annual Meeting of
the Association of Computational Linguistics: System
Demonstrations, pages 78?83.
282
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143?152,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Towards Robust Linguistic Analysis Using OntoNotes
Sameer Pradhan1, Alessandro Moschitti2,3, Nianwen Xue4, Hwee Tou Ng5
Anders Bjo?rkelund6, Olga Uryupina2, Yuchen Zhang4 and Zhi Zhong5
1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA
2 University of Trento, University of Trento, 38123 Povo (TN), Italy
3 QCRI, Qatar Foundation, 5825 Doha, Qatar
4 Brandeis University, Brandeis University, Waltham, MA 02453, USA
5 National University of Singapore, Singapore, 117417
6 University of Stuttgart, 70174 Stuttgart, Germany
Abstract
Large-scale linguistically annotated cor-
pora have played a crucial role in advanc-
ing the state of the art of key natural lan-
guage technologies such as syntactic, se-
mantic and discourse analyzers, and they
serve as training data as well as evaluation
benchmarks. Up till now, however, most
of the evaluation has been done on mono-
lithic corpora such as the Penn Treebank,
the Proposition Bank. As a result, it is still
unclear how the state-of-the-art analyzers
perform in general on data from a vari-
ety of genres or domains. The completion
of the OntoNotes corpus, a large-scale,
multi-genre, multilingual corpus manually
annotated with syntactic, semantic and
discourse information, makes it possible
to perform such an evaluation. This paper
presents an analysis of the performance of
publicly available, state-of-the-art tools on
all layers and languages in the OntoNotes
v5.0 corpus. This should set the bench-
mark for future development of various
NLP components in syntax and semantics,
and possibly encourage research towards
an integrated system that makes use of the
various layers jointly to improve overall
performance.
1 Introduction
Roughly a million words of text from the Wall
Street Journal newswire (WSJ), circa 1989, has
had a significant impact on research in the lan-
guage processing community ? especially those
in the area of syntax and (shallow) semantics, the
reason for this being the seminal impact of the
Penn Treebank project which first selected this text
for annotation. Taking advantage of a solid syn-
tactic foundation, later researchers who wanted to
annotate semantic phenomena on a relatively large
scale, also used it as the basis of their annota-
tion. For example the Proposition Bank (Palmer et
al., 2005), BBN Name Entity and Pronoun coref-
erence corpus (Weischedel and Brunstein, 2005),
the Penn Discourse Treebank (Prasad et al, 2008),
and many other annotation projects, all annotate
the same underlying body of text. It was also con-
verted to dependency structures and other syntac-
tic formalisms such as CCG (Hockenmaier and
Steedman, 2002) and LTAG (Shen et al, 2008),
thereby creating an even bigger impact through
these additional syntactic resources. The most re-
cent one of these efforts is the OntoNotes corpus
(Weischedel et al, 2011). However, unlike the
previous extensions of the Treebank, in addition
to using roughly a third of the same WSJ subcor-
pus, OntoNotes also added several other genres,
and covers two other languages ? Chinese and
Arabic: portions of the Chinese Treebank (Xue et
al., 2005) and the Arabic Treebank (Maamouri and
Bies, 2004) have been used to sample the genre of
text that they represent.
One of the current hurdles in language process-
ing is the problem of domain, or genre adaptation.
Although genre or domain are popular terms, their
definitions are still vague. In OntoNotes, ?genre?
means a type of source ? newswire (NW), broad-
cast news (BN), broadcast conversation (BC), mag-
azine (MZ), telephone conversation (TC), web data
(WB) or pivot text (PT). Changes in the entity and
event profiles across source types, and even in the
same source over a time duration, as explicitly ex-
pressed by surface lexical forms, usually account
for a lot of the decrease in performance of mod-
els trained on one source and tested on another,
usually because these are the salient cues that are
relied upon by statistical models.
Large-scale corpora annotated with multiple
layers of linguistic information exist in various
languages, but they typically consist of a single
source or collection. The Brown corpus, which
consists of multiple genres, have been usually used
to investigate issues of genres of sensitivity, but it
is relatively small and does not include any infor-
1A portion of the English data in the OntoNotes corpus
is a selected set of sentences that were annotated for parse
and word sense information. These sentences are present in a
document of their own, and so the documents for parse layers
for English are inflated by about 3655 documents and for the
word sense are inflated by about 8797 documents.
143
Language Parse Proposition Sense Name Coreference
Documents Words Documents Verb Prop. Noun Prop. Documents Verb Sense Noun Sense Documents Words Documents Words
English 7,9671 2.6M 6,124 300K 18K 12K 173K 120K 3,637 2.0M 2,384(3493) 1.7M
Chinese 2002 1.0M 1861 148K 7K 1573 83K 1K 1,911 988K 1,729(2,280) 950K
Arabic 599 402K 599 30K - 310 4.3K 8.7K 446 298K 447(447) 300K
Table 1: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, and
some other attributes. The numbers in parenthesis are the total number of parts in the documents.
mal genres such as web data. Very seldom has it
been the case that the exact same phenomena have
been annotated on a broad cross-section of the
same language before OntoNotes. The OntoNotes
corpus thus provides an opportunity for studying
the genre effect on different syntactic, semantic
and discourse analyzers.
Parts of the OntoNotes Corpus have been used
for various shared tasks organized by the language
processing community. The word sense layer was
the subject of prediction in two SemEval-2007
tasks, and the coreference layer was the subject
of prediction in the SemEval-20102 (Recasens et
al., 2010), CoNLL-2011 and 2012 shared tasks
(Pradhan et al, 2011; Pradhan et al, 2012). The
CoNLL-2012 shared task provided predicted in-
formation to the participants, however, that did not
include a few layers such as the named entities
for Chinese and Arabic, propositions for Arabic,
and for better comparison of the English data with
the CoNLL-2011 task, a smaller OntoNotes v4.0
portion of the English parse and propositions was
used for training.
This paper is a first attempt at presenting a co-
herent high-level picture of the performance of
various publicly available state-of-the-art tools on
all the layers of OntoNotes in all three languages,
so as to pave the way for further explorations in
the area of syntax and semantics processing.
The possible avenues for exploratory studies
on various fronts are enormous. However, given
space considerations, in this paper, we will re-
strict our presentation of the performance on all
layers of annotation in the data by using a strat-
ified cross-section of the corpus for training, de-
velopment, and testing. The paper is organized
as follows: Section 2 gives an overview of the
OntoNotes corpus. Section 3 explains the param-
eters of the evaluation and the various underlying
assumptions. Section 4 presents the experimental
results and discussion, and Section 5 concludes the
paper.
2 OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of
2A small portion 125K words in English was used for this
evaluation.
multiple layers of syntactic, semantic and dis-
course information in text. The English lan-
guage portion comprises roughly 1.7M words and
Chinese language portion comprises roughly 1M
words of newswire, magazine articles, broadcast
news, broadcast conversations, web data and con-
versational speech data3. The Arabic portion is
smaller, comprising 300K words of newswire ar-
ticles. This rich, integrated annotation covering
many layers aims at facilitating the development
of richer, cross-layer models and enabling bet-
ter automatic semantic analysis. The corpus is
tagged with syntactic trees, propositions for most
verb and some noun instances, partial verb and
noun word senses, coreference, and named enti-
ties. Table 1 gives an overview of the number of
documents that have been annotated in the entire
OntoNotes corpus.
2.1 Layers of Annotation
This section provides a very concise overview of
the various layers of annotations in OntoNotes.
For a more detailed description, the reader is re-
ferred to (Weischedel et al, 2011) and the docu-
mentation accompanying the v5.04 release.
2.1.1 Syntax
This represents the layer of syntactic annotation
based on revised guidelines for the Penn Tree-
bank (Marcus et al, 1993; Babko-Malaya et al,
2006), the Chinese Treebank (Xue et al, 2005)
and the Arabic Treebank (Maamouri and Bies,
2004). There were two updates made to the parse
trees as part of the OntoNotes project: i) the in-
troduction of NML phrases, in the English portion,
to mark nominal sub-constituents of flat NPs that
do not follow the default right-branching structure,
and ii) re-tokenization of hyphenated tokens into
multiple tokens in English and Chinese. The Ara-
bic Treebank on the other hand was also signifi-
cantly revised in an effort to increase consistency.
2.1.2 Word Sense
Coarse-grained word senses are tagged for the
most frequent polysemous verbs and nouns, in or-
3These numbers are for the portion that has all layers of
annotations. The word count for each layer is mentioned in
Table 1
4For all the layers of data used in this study, the
OntoNotes v4.99 pre-release that was used for the CoNLL-
2012 shared task is identical to the v5.0 release.
144
der to maximize token coverage. The word sense
granularity is tailored to achieve very high inter-
annotator agreement as demonstrated by Palmer et
al. (2007). These senses are defined in the sense
inventory files. In the case of English and Arabic
languages, the sense-inventories (and frame files)
are defined separately for each part of speech that
is realized by the lemma in the text. For Chinese,
however the sense inventories (and frame files) are
defined per lemma ? independent of the part of
speech realized in the text.
2.1.3 Proposition
The propositions in OntoNotes are PropBank-style
semantic roles for English, Chinese and Arabic.
Most English verbs and few nouns were anno-
tated using the revised guidelines for the English
PropBank (Babko-Malaya et al, 2006) as part of
the OntoNotes effort. Some enhancements were
made to the English PropBank and Treebank to
make them synchronize better with each other:
one of the outcomes of this effort was that two
types of LINKs that represent pragmatic coref-
erence (LINK-PCR) and selectional preferences
(LINK-SLC) were added to the original PropBank
(Palmer et al, 2005). More details can be found in
the addendum to the PropBank guidelines5 in the
OntoNotes v5.0 release. A part of speech agnostic
Chinese PropBank (Xue and Palmer, 2009) guide-
lines were used to annotate most frequent lem-
mas in Chinese. Many verbs and some nouns and
adjectives were annotated using the revised Ara-
bic PropBank guidelines (Palmer et al, 2008; Za-
ghouani et al, 2010).
2.1.4 Named Entities
The corpus was tagged with a set of 18 well-
defined proper named entity types that have been
tested extensively for inter-annotator agreement
by Weischedel and Burnstein (2005).
2.1.5 Coreference
This layer captures general anaphoric corefer-
ence that covers entities and events not limited
to noun phrases or a limited set of entity types
(Pradhan et al, 2007). It considers all pronouns
(PRP, PRP$), noun phrases (NP) and heads of verb
phrases (VP) as potential mentions. Unlike En-
glish, Chinese and Arabic have dropped subjects
and objects which were also considered during
coreference annotation6. The mentions formed by
these dropped pronouns total roughly about 11%
for both Chinese and Arabic. Coreference is the
only document-level phenomenon in OntoNotes.
Some of the documents in the corpus ? especially
the ones in the broadcast conversation, web data,
5doc/propbank/english-propbank.pdf
6As we will see later these are not used during the task.
and telephone conversation genre ? are very long
which prohibited efficient annotation in their en-
tirety. These are split into smaller parts, and each
part is considered a separate document for the sake
of coreference evaluation.
3 Evaluation Setting
Given the scope of the corpus and the multitude of
settings one can run evaluations, we had to restrict
this study to a relatively focused subset. There has
already been evidence of models trained on WSJ
doing poorly on non-WSJ data on parses (Gildea,
2001; McClosky et al, 2006), semantic role label-
ing (Carreras and Ma`rquez, 2005; Pradhan et al,
2008), word sense (Escudero et al, 2000; ?), and
named entities. The phenomenon of coreference is
somewhat of an outlier. The winning system in the
CoNLL-2011 shared task was one that was com-
pletely rule-based and not directly trained on the
OntoNotes corpus. Given this overwhelming evi-
dence, we decided not to focus on potentially com-
plex cross-genre evaluations. Instead, we decided
on evaluating the performance on each layer of an-
notation using an appropriately selected, stratified
training, development and test set, so as to facili-
tate future studies.
3.1 Training, Development and Test
Partitions
In this section we will have a brief discussion
on the logic behind the partitioning of the data
into training, development and test sets. Before
we do that, it would help to know that given the
range and peculiarities of the layers of annota-
tion and presence of various resource and techni-
cal constraints, not all the documents in the cor-
pus are annotated with all the layers of informa-
tion, and token-centric phenomena (such as word
sense and propositions of predicates) were not an-
notated with 100% coverage. Most of the propo-
sition annotation in English and Arabic is for the
verb predicates, with a few nouns annotated in
English and some adjectives in Arabic. In Chi-
nese, the selection is part of speech agnostic, and is
based on the lemmas that can be considered predi-
cates. Some documents in the corpora are actually
snippets from larger documents, and have been an-
notated for a combination of parse, propositions,
word sense and names, but not coreference. If one
considers each layer independently, then an ideal
partitioning scheme would create a separate parti-
tion for each layer such that it maximizes the num-
ber of examples that can be extracted for that layer
from the corpus. The upside is that one would
get as much data there is to train and estimate the
performance of each layer across the entire cor-
pus. The downside is that this might cover vari-
145
ous cross sections of the documents in the corpus,
and would not provide a clean picture when look-
ing at the collective performance for all the lay-
ers. The documents that are annotated with coref-
erence correspond to the intersection of all anno-
tations. These are the documents that have also
been annotated with all the other layers of infor-
mation. The amount of data we can get together
in such a test set is big enough to be represen-
tative. Therefore, we decided that it would be
ideal to choose a portion of these documents as
the test collection for all layers. An additional ad-
vantage is that it is the exact same test set used
in the CoNLL-2012 shared task, and so in a way
is already a standard. On the training and devel-
opment side however, one can still imagine using
all possible information for training models for a
particular layer, and that is what we decided to
do. The training and development data is gener-
ated by providing all documents with all available
layers of annotation for input, however, the test
set is generated by providing as input to the algo-
rithm the set of documents in the corpus that have
been annotated for coreference. This algorithm
tries to reuse previously established partitions for
English, i.e., the WSJ portion. Unfortunately, in
the case of Chinese and Arabic, either the histor-
ical partitions were not in the selection used for
OntoNotes, or were partially overlapping with the
ones created using this scheme, and/or had a very
small portion of OntoNotes covered in the test set.
Therefore, we decided to create a fresh partition
for the Chinese and Arabic data. Note, however,
that the these test sets also match the ones used
in the CoNLL-2012 evaluation. The algorithm for
selecting the training, development and test parti-
tions is described on the CoNLL-2012 shared task
webpage, along with the list of training, develop-
ment, and test document IDs7.
3.2 Assumptions
Next we had to decide on a set of assumptions
to use while designing the experiments to mea-
sure the automatic prediction accuracy for each of
the layers. Since some of these decisions affect
more than one layer of annotation, we will de-
scribe these in this section instead of in the section
where we discuss the experiment with a particular
layer of annotation.
7http://conll.cemantix.org/2012/download/ids/
For each language there are two sub-directories ? ?all?
contains more general lists which include documents
that had at least one of the layers of annotation, and
?coref? contains the lists that include documents that
have coreference annotation. The former were used to
generate training, development, test sets for layers other
than coreference, and the latter was used to generate
training/development/test sets for the coreference layer
used in the CoNLL-2012 shared task.
Word Segmentation The three languages that
we are evaluating are from quite different lan-
guage families. Arabic has a complex morphol-
ogy, English has limited morphology, whereas
Chinese has very little morphology. English word
segmentation amounts to rule-based tokenization,
and is close to perfect. In the case of Chinese and
Arabic, although the tokenization/segmentation is
not as good as English, the accuracies are in the
high 90s. Given this we decided to use gold,
Treebank segmentation for all languages. In the
case of Chinese, the words themselves are lem-
mas, whereas in English they can be predicted
with very high accuracy. For Arabic, by default
written text is unvocalised, and lemmatization is a
complex process which we considered out of the
scope of this study, so we decided to use correct,
gold standard lemmas, along with the correct vo-
calized version of the tokens.
Traces and Function Tags Treebank traces
have hardly played a role in the mainstream parser
and semantic role labeling evaluation. Function
tags also have received similar treatment in the
parsing community, and though they are impor-
tant, there is also a significant information overlap
between them and the proposition structure pro-
vided by the PropBank layer. Whereas in English,
most traces represent syntactic phenomena such
as movement and raising, in Chinese and Arabic,
they can also represent dropped subjects/objects.
These subset of traces directly affect the corefer-
ence layer, since, unlike English, traces in Chinese
and Arabic (*pro* and * respectively) are legit-
imate targets of mentions and are considered for
coreference annotation in OntoNotes. Recovering
traces in text is a hard problem, and the most re-
cently reported numbers in literature for Chinese
are around a F-score of 50 (Yang and Xue, 2010;
Cai et al, 2011). For Arabic there have not been
much studies on recovering these. A study by
Gabbard (2010) shows that these can be recovered
with an F-score of 55 with automatic parses and
roughly 65 using gold parses. Considering the low
level of prediction accuracy of these tokens, and
their relative low frequency, we decided to con-
sider predicting traces in trees out of the scope of
this study. In other words, we removed the man-
ually identified traces and function tags from the
Treebanks across all three languages, in all the
three ? training, development and test partitions.
This meant removing any and all dependent an-
notation in layers such as PropBank and Coref-
erence. In the case of PropBank these are the
argument bearing traces, whereas in coreference
these are the mentions formed by these elided sub-
jects/objects.
146
Disfluencies One thing that needs to be dealt
with in conversational data is the presence of dis-
fluencies (restarts, etc.). In the English parses of
the OntoNotes, disfluencies are marked using a
special EDITED8 phrase tag ? as was the case for
the Switchboard Treebank. Computing the accu-
racy of identifying disfluencies is also out of the
scope of this study. Given the frequency of dis-
fluencies and the performance with which one can
identify them automatically,9 a probable process-
ing pipeline would filter them out before parsing.
We decided to remove them using oracle infor-
mation available in the English Treebank, and the
coreference chains were remapped to trees with-
out disfluencies. Owing to various technical con-
straints, we decided to retain the disfluencies in the
Chinese data.
Spoken Genre Given the scope of this study, we
make another significant assumption. For the spo-
ken genres ? BC, BN and TC ? we use the manual
transcriptions rather than the output of a speech
recognizer, as would be the case in real world. The
performance on various layers for these genres
would therefore be artificially inflated, and should
be taken into account while analyzing results. Not
many studies have previously reported on syntac-
tic and semantic analysis for spoken genre. Favre
et al (2010) report the performance on the English
subset of an earlier version of OntoNotes.
Discourse The corpus contains information on
the speaker for broadcast communication, conver-
sation, telephone conversation and writer for the
web data. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be
automatically deduced, but is also not within the
scope of our study. Therefore, we decided to pro-
vide gold, instead of predicted, data both during
training and testing. Table 2 lists the status of the
layers.
4 Experiments
In this section, we will report on the experiments
carried out using all available data in the train-
ing set for training models for a particular layer,
and using the CoNLL-2012 test set as the test set.
8There is another phrase type ? EMBED in the telephone
conversation genre which is similar to the EDITED phrase
type, and sometimes identifies insertions, but sometimes con-
tains logical continuation of phrases by different speakers, so
we decided not to remove that from the data.
9A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 precision
and 67 recall.
10The predicted part of speech for Arabic are a mapped
down version of the richer gold version present in the Tree-
bank
Layer English Chinese Arabic
Segmentation ? ? ?
Lemma ? ? ?
Parse ? ? ?10
Proposition ? ? ?
Predicate Frame ? ? ?
Word Sense ? ? ?
Name Entities ? ? ?
Coreference ? ? ?
Speaker ? ? ?
Number ? ? ?
Gender ? ? ?
Table 2: Status of layers used during prediction
of other layers. A ??? indicates gold annotation,
a ??? indicates predicted, a ??? indicates an ab-
sence of the predicted layer, and a ??? indicates
that the layer is not applicable to the language.
The predicted annotation layers input to down-
stream models were automatically annotated by
using NLP processors learned with n-cross fold
validation on the training data. This way, the n
chunks of training data are annotated avoiding de-
pendencies with the data used for training the NLP
processors.
4.1 Syntax
Predicted parse trees for English were produced
using the Charniak parser11 (Charniak and John-
son, 2005). Some additional tag types used in
the OntoNotes trees were added to the parser?s
tagset, including the nominal (NML) tag, and the
rules used to determine head words were extended
correspondingly. Chinese and Arabic parses were
generated using the Berkeley parser (Petrov and
Klein, 2007). In the case of Arabic, the pars-
ing community uses a mapping from rich Arabic
part of speech tags to Penn-style part of speech
tags. We used the mapping that is included with
the Arabic Treebank. The predicted parses for
the training portion of the data were generated us-
ing 10-fold (5-folds for Arabic) cross-validation.
For testing, we used a model trained on the entire
training portion. Table 3 shows the precision, re-
call and F1-scores of the re-trained parsers on the
CoNLL-2012 test along with the part of speech ac-
curacies (POS) using the standard evalb scorer.
The performance on the PT genre for English is
the highest among other English genres. This is
possibly because of the professional, clean trans-
lations of the underlying text, and are mostly
shorter sentences. The MZ genre and the NW both
of which contain well edited text, share similar
scores. There is a few points gap between these
and the other genres. As for Chinese, the per-
formance on MZ is the highest followed by BN.
Surprisingly, the WB genre has a similar score and
the others are close behind except for TC. As ex-
pected, the Arabic parser performance is the low-
11http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz
147
All Sentences
N POS P R F
English BC 2,211 97.33 86.36 86.11 86.23
BN 1,357 97.32 87.61 87.03 87.32
MZ 780 96.58 89.90 89.49 89.70
NW 2,327 97.15 87.68 87.25 87.47
TC 1,366 96.11 85.09 84.13 84.60
WB 1,787 96.03 85.46 85.26 85.36
PT 1,869 98.77 95.29 94.66 94.98
Overall 11,697 97.09 88.08 87.65 87.87
Chinese BC 885 94.79 80.17 79.35 79.76
BN 929 93.85 83.49 80.13 81.78
MZ 451 97.06 88.48 83.85 86.10
NW 481 94.07 82.26 77.28 79.69
TC 968 92.22 71.90 69.19 70.52
WB 758 92.37 82.57 78.92 80.70
Overall 4,472 94.12 82.23 78.93 80.55
Arabic NW 1,003 94.12 74.71 75.67 75.19
Table 3: Parser performance on the CoNLL-2012
test set.
est among the three languages.
4.2 Word Sense
We used the IMS12 (It Makes Sense) (Zhong and
Ng, 2010) word sense tagger. IMS was trained on
all the word sense data that is present in the train-
ing portion of the OntoNotes corpus using cross-
validated predictions on the input layers similar
to the proposition tagger. During testing, for En-
glish and Arabic, IMS must first use the auto-
matic POS information to identify the nouns and
verbs in the test data, and then assign senses to
the automatically identified nouns and verbs. In
the case of Arabic, IMS uses gold lemmas. Since
automatic POS tagging is not perfect, IMS does
not always output a sense to all word tokens that
need to be sense tagged due to wrongly predicted
POS tags. As such, recall is not the same as pre-
cision on the English and Arabic test data. For
Chinese the measure of performance is just the
accuracy since the senses are defined per lemma
rather than per part of speech. Since we provide
gold word segmentation, IMS attempts to sense
tag all correctly segmented Chinese words, so re-
call and precision are the same and so is the F1-
score. Table 4 shows the performance of this clas-
sifier aggregated over both the verbs and nouns
in the CoNLL-2012 test set and an overall score
split by nouns and verbs for English and Ara-
bic. For both nouns and verbs in English, the
F1-score is over 80%. The performance on En-
glish nouns is slightly higher than English verbs.
Comparing to the other two languages, the perfor-
mance on Arabic is relatively lower, especially the
performance on Arabic verbs, whose F1-score is
less than 70%. For English, genres PT and TC,
and for Chinese genres TC and WB, no gold stan-
dard senses were available, and so their accuracies
could not be computed. Previously, Zhong et al
(2008) reported the word sense performance on
the Wall Street Journal portion of an earlier ver-
12http://www.comp.nus.edu.sg/?nlp/sw/IMS v0.9.2.1.tar.gz
Performance
P R F A
English BC 81.2 81.3 81.2 -
BN 82.0 81.5 81.7 -
MZ 79.1 78.8 79.0 -
NW 85.7 85.7 85.7 -
WB 77.5 77.6 77.5 -
Overall 82.5 82.5 82.5 -
Nouns 83.4 83.1 83.2 -
Verbs 81.8 81.9 81.8 -
Chinese BC - - - 80.5
BN - - - 85.4
MZ - - - 82.4
NW - - - 89.1
Overall - - - 84.3
Arabic NW 75.9 75.2 75.6 -
Nouns 79.2 77.7 78.4 -
Verbs 68.8 69.5 69.1 -
Table 4: Word sense performance on the CoNLL-
2012 test set.
sion of OntoNotes, but the results are not directly
comparable.
4.3 Proposition
The revised PropBank has introduced two new
links ? LINK-SLC and LINK-PCR. Since the com-
munity is not used to the new PropBank represen-
tation which (i) relies heavily on the trace struc-
ture in the Treebank and (ii) we decided to ex-
clude, we unfold the LINKs back to their original
representation as in the PropBank 1.0 release. We
used ASSERT15 (Pradhan et al, 2005) to predict
the propositional structure for English. We made
a small modification to ASSERT, and replaced
the TinySVM classifier with a CRF16 to speed
up training the model on all the data. The Chi-
nese propositional structure was predicted with the
Chinese semantic role labeler described in (Xue,
2008), retrained on the OntoNotes v5.0 data. The
Arabic propositional structure was predicted us-
ing the system described in Diab et al (2008).
(Diab et al, 2008) Table 5 shows the detailed per-
14The Frame ID column indicates the F-score for English
and Arabic, and accuracy for Chinese for the same reasons as
word sense.
15http://cemantix.org/assert.html
16http://leon.bottou.org/projects/sgd
Frame Total Total % Perfect Argument ID + Class
ID Sent. Prop. Prop. P R F
English BC 93.2 1994 5806 52.89 80.76 69.69 74.82
BN 92.7 1218 4166 54.78 80.22 69.36 74.40
MZ 90.8 740 2655 50.77 79.13 67.78 73.02
NW 92.8 2122 6930 46.45 79.80 66.80 72.72
TC 91.8 837 1718 49.94 79.85 72.35 75.91
WB 90.7 1139 2751 42.86 80.51 69.06 74.35
PT 96.6 1208 2849 67.53 89.35 84.43 86.82
Overall 92.8 9,261 26,882 51.66 81.30 70.53 75.53
Chinese BC 87.7 885 2,323 31.34 53.92 68.60 60.38
BN 93.3 929 4,419 35.44 64.34 66.05 65.18
MZ 92.3 451 2,620 31.68 65.04 65.40 65.22
NW 96.6 481 2,210 27.33 69.28 55.74 61.78
TC 82.2 968 1,622 32.74 48.70 59.12 53.41
WB 87.8 758 1,761 35.21 62.35 68.87 65.45
Overall 90.9 4,472 14,955 32.62 61.26 64.48 62.83
Arabic NW 85.6 1,003 2337 24.18 52.99 45.03 48.68
Table 5: Proposition and frameset disambiguation
performance14 in the CoNLL-2012 test set.
148
formance numbers17. The CoNLL-2005 scorer18
was used to compute the scores. At first glance,
the performance on the English newswire genre is
much lower than what has been reported for WSJ
Section 23. This could be attributed to several fac-
tors: i) the newswire in OntoNotes not only con-
tains WSJ data, but also Xinhua news, and some
other newswire evaluation data, ii) The WSJ train-
ing and test portions in OntoNotes are a subset of
the standard ones that have been used to report
performance earlier; iii) the PropBank guidelines
were significantly revised during the OntoNotes
project in order to synchronize well with the Tree-
bank, and finally iv) it includes propositions for
be verbs missing from the original PropBank. It
looks like the newly added Pivot Text data (com-
prised of the New Testament) shows very good
performance. The Chinese and Arabic19 accuracy
is much worse. In addition to automatically pre-
dicting the arguments, we also trained the IMS
system to tag PropBank frameset IDs.
Language Genre Entity Performance
Count P R F
English BC 1671 80.17 77.20 78.66
BN 2180 88.95 85.69 87.29
MZ 1161 82.74 82.17 82.45
NW 4679 86.79 84.25 85.50
TC 362 74.09 61.60 67.27
WB 1133 77.72 68.05 72.56
Overall 11186 84.04 80.86 82.42
Chinese BC 667 72.49 58.47 64.73
BN 3158 82.17 71.50 76.46
NW 1453 86.11 76.39 80.96
MZ 1043 65.16 56.66 60.62
TC 200 48.00 60.00 53.33
WB 886 80.60 51.13 62.57
Overall 7407 78.20 66.45 71.85
Arabic NW 2550 74.53 62.55 68.02
Table 6: Performance of the named entity recog-
nizer on the CoNLL-2012 test set.
4.4 Named Entities
We retrained the Stanford named entity recog-
nizer20 (Finkel et al, 2005) on the OntoNotes data.
Table 6 shows the performance details for all the
languages across all 18 name types broken down
by genre. In English, BN has the highest perfor-
mance followed by the NW genre. There is a sig-
nificant drop from those and the TC and WB genre.
Somewhat similar trend is observed in the Chi-
nese data, with Arabic having the lowest scores.
Since the Pivot Text portion (PT) of OntoNotes
was not tagged with names, we could not com-
pute the accuracy for that cross-section of the data.
Previously Finkel and Manning (2009) performed
17The number of sentences in this table are a subset of the
ones in the table showing parser performance, since these are
the sentences for which at least one predicate has been tagged
with its arguments
18http://www.lsi.upc.es/?srlconll/srl-eval.pl
19The system could not not use the morphology features in
Diab et al (2008).
20http://nlp.stanford.edu/software/CRF-NER.shtml
a joint estimation of named entity and parsing.
However, it was on an earlier version of the En-
glish portion of OntoNotes using a different cross-
section for training and testing and therefore is not
directly comparable.
4.5 Coreference
The task is to automatically identify mentions of
entities and events in text and to link the corefer-
ring mentions together to form entity/event chains.
The coreference decisions are made using auto-
matically predicted information on other structural
and semantic layers including the parses, seman-
tic roles, word senses, and named entities that
were produced in the earlier sections. Each docu-
ment part from the documents that were split into
multiple parts during coreference annotation were
treated as separate document.
We used the number and gender predictions
generated by Bergsma and Lin (2006). Unfortu-
nately neither Arabic, nor Chinese have compara-
ble data available. Chinese, in particular, does not
have number or gender inflections for nouns, but
(Baran and Xue, 2011) look at a way to infer such
information.
We trained the Bjo?rkelund and Farkas (2012)
coreference system21 which uses a combination of
two pair-wise resolvers, the first is an incremen-
tal chain-based resolution algorithm (Bjo?rkelund
and Farkas, 2012), and the second is a best-first
resolver (Ng and Cardie, 2002). The two resolvers
are combined by stacking, i.e., the output of the
first resolver is used as features in the second one.
The system uses a large feature set tailored for
each language which, in addition to classic coref-
erence features, includes both lexical and syntactic
information.
Recently, it was discovered that there is pos-
sibly a bug in the official scorer used for the
CoNLL 2011/2012 and the SemEval 2010 corefer-
ence tasks. This relates to the mis-implementation
of the method proposed by (Cai and Strube, 2010)
for scoring predicted mentions. This issue has also
been recently reported in Recasens et al, (2013).
As of this writing, the BCUBED metric has been
fixed, and the correctness of the CEAFm, CEAFe
and BLANC metrics is being verified. We will
be updating the CoNLL shared task webpages22
with more detailed information and also release
the patched scripts as soon as they are available.
We will also re-generate the scores for previous
shared tasks, and the coreference layer in this pa-
per and make them available along with the mod-
els and system outputs for other layers. Table
7 shows the performance of the system on the
21http://www.ims.uni-stuttgart.de/?anders/coref.html
22http://conll.cemantix.org
149
CoNLL-2012 test set, broken down by genre. The
same metrics that were used for the CoNLL-2012
shared task are computed, with the CONLL col-
umn being the official CONLL measure.
Language Genre MD MUC BCUBED CEAFm CEAFe BLANC CONLL
PREDICTED MENTIONS
English BC 73.43 63.92 61.98 54.82 42.68 73.04 56.19
BN 73.49 63.92 65.85 58.93 48.14 72.74 59.30
MZ 71.86 64.94 71.38 64.03 50.68 78.87 62.33
NW 68.54 60.20 65.11 57.54 45.10 73.72 56.80
PT 86.95 79.09 68.33 65.52 50.83 77.74 66.08
TC 80.81 76.78 71.35 65.41 45.44 82.45 64.52
WB 74.43 66.86 61.43 54.76 42.05 73.54 56.78
Overall 75.38 67.58 65.78 59.20 45.87 75.8 59.74
Chinese BC 68.02 59.6 59.44 53.12 40.77 73.63 53.27
BN 68.57 61.34 67.83 60.90 48.10 77.39 59.09
MZ 55.55 48.89 58.83 55.63 46.04 74.25 51.25
NW 89.19 80.71 73.64 76.30 70.89 82.56 75.08
TC 77.72 73.59 71.65 64.30 48.52 83.14 64.59
WB 72.61 65.79 62.32 56.71 43.67 77.45 57.26
Overall 66.37 58.61 66.56 59.01 48.19 76.07 57.79
Arabic NW 60.55 47.82 61.16 53.42 44.30 69.63 51.09
GOLD MENTIONS
English BC 85.63 76.09 68.70 61.73 49.87 76.24 64.89
BN 82.11 73.56 71.52 63.67 52.29 75.70 65.79
MZ 85.65 77.73 78.82 72.75 60.09 83.88 72.21
NW 80.68 73.52 73.08 65.63 51.96 81.06 66.19
PT 93.20 85.72 73.25 70.76 58.81 79.78 72.59
TC 90.68 86.83 78.94 73.87 56.26 85.82 74.01
WB 88.12 80.61 69.86 63.45 51.13 76.48 67.20
Overall 86.16 78.7 72.67 66.32 53.23 79.22 68.2
Chinese BC 84.88 76.34 69.89 62.02 49.29 76.89 65.17
BN 80.97 74.89 76.88 68.91 55.56 81.94 69.11
MZ 78.85 73.06 70.15 61.68 46.86 78.78 63.36
NW 93.23 86.54 86.70 80.60 76.60 85.75 83.28
TC 92.91 88.31 84.51 79.49 63.87 90.04 78.90
WB 85.87 77.61 69.24 60.71 47.47 77.67 64.77
Overall 83.47 76.85 76.30 68.30 56.61 81.56 69.92
Arabic NW 76.43 60.81 67.29 59.50 49.32 74.61 59.14
Table 7: Performance of the coreference system
on the CoNLL-2012 test set.
The varying results across genres mostly meet
our expectations. In English, the system does best
on TC and the PT genres. The text in the TC set
often involve long chains where the speakers re-
fer to themselves which, given speaker informa-
tion, is fairly easy to resolve. The PT section
includes many references to god (e.g. god and
the lord) which the lexicalized resolver is quite
good at picking up during training. The more dif-
ficult genres consist of texts where references to
many entities are interleaved in the discourse and
is as such harder to resolve correctly. For Chi-
nese the numbers on the TC genre are also quite
good, and the explanation above also holds here
? many mentions refer to either of the speak-
ers. For Chinese the NW section displays by far
the highest scores, however, and the reason for
this is not clear to us. Not surprisingly, restricting
the set of mentions only to gold mentions gives
a large boost across all genres and all languages.
This shows that mention detection (MD) and sin-
gleton detection (which is not part of the annota-
tion) remain a big source of errors for the coref-
erence resolver. For these experiments we used
a combination of training and development data
for training ? following the CoNLL-2012 shared
task specification. Leaving out the development
set has a very negligible effect on the CoNLL-
score for all the languages (English: 0.14; Chi-
nese 0.06; Arabic: 0.40 F-score respectively). The
effect on Arabic is the most (0.40 F-score) most
likely because of its much smaller size. To gauge
the performance improvement between 2011 and
2012 shared tasks, we performed a clean com-
parison of over the best performing system and
an earlier version of this system (Bjo?rkelund and
Nugues, 2011) on the CoNLL 2011 test set us-
ing the CoNLL 2011 train and development set
for training. The current system has a CoNLL
score of 60.09 (64.92+69.84+45.513 )23 as opposed tothe 54.53 reported in bjo?rkelund (Bjo?rkelund and
Nugues, 2011), and the 57.79 reported for the best
performing system of CoNLL-2011. One caveat
is that these score comparison are done using the
earlier version (v4) of the CoNLL scorer. Nev-
ertheless, it is encouraging to see that within a
short span of a year, there has been significant
improvement in system performance ? partially
owing to cross-pollination of research generated
through the shared tasks.
5 Conclusion
In this paper we reported work on finding a rea-
sonable training, development and test split for
the various layers of annotation in the OntoNotes
v5.0 corpus, which consists of multiple genres in
three typologically very different languages. We
also presented the performance of publicly avail-
able, state-of-the-art algorithms on all the different
layers of the corpus for the different languages.
The trained models as well as their output will
be made publicly available24 to serve as bench-
marks for language processing community. Train-
ing so many different NLP components is very
time-consuming, thus, we hope the work reported
here has lifted the burden of having to create rea-
sonable baselines for researchers who wish to use
this corpus to evaluate their systems. We created
just one data split in training, development and test
set, covering a collection of genres for each layer
of annotation in each language in order to keep the
workload manageable However, the results do not
discriminate the performance on individual gen-
res: we believe such a setup is still a more realistic
gauge for the performance of the state-of-the-art
NLP components than a monolithic corpus such
as the Wall Street Journal section of the Penn Tree-
bank. It can be used as a starting point for devel-
oping the next generation of NLP components that
are more robust and perform well on a multitude
of genres for a variety of different languages.
23(MUC + BCUBED + CEAFe)/3
24http://cemantix.org
150
6 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022for sponsoring the creation of the OntoNotes
corpus. This work was partially supported
by grants R01LM10090 and U54LM008748
from the National Library Of Medicine, and
R01GM090187 from the National Institutes ofGeneral Medical Sciences. We are indebted toSlav Petrov for helping us to retrain his syntactic
parser for Arabic. Alessandro Moschitti and
Olga Uryupina have been partially funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under the grant
number 288024 (LIMOSINE). The content
is solely the responsibility of the authors and
does not necessarily represent the official views
of the National Institutes of Health. NianwenXue and Yuchen Zhang are supported in part
by the DAPRA via contract HR0011-11-C-0145
entitled ?Linguistic Resources for Multilingual
Processing.?
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Libin
Shen. 2006. Issues in synchronizing the English treebank
and propbank. In Workshop on Frontiers in Linguistically
Annotated Corpora 2006, July.
Elizabeth Baran and Nianwen Xue. 2011. Singular or plural?
exploiting parallel corpora for Chinese number prediction.
In Proceedings of Machine Translation Summit XIII, Xia-
men, China.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-
based pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 33?40, Sydney, Australia, July.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-driven
multilingual coreference resolution using resolver stack-
ing. In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 49?55, Jeju Island, Korea, July. Association
for Computational Linguistics.
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring lex-
icalized features for coreference resolution. In Proceed-
ings of the Fifteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 45?50, Port-
land, Oregon, USA, June. Association for Computational
Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation metrics for
end-to-end coreference resolution systems. In Proceed-
ings of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, SIGDIAL ?10, pages
28?36.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 212?216, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL), Ann Arbor, MI,
June.
Eugene Charniak and Mark Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings of the
Second Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Ann Arbor, MI,
June.
Mona Diab, Alessandro Moschitti, and Daniele Pighin. 2008.
Semantic role labeling systems for Arabic using kernel
methods. In Proceedings of ACL-08: HLT, pages 798?
806, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Gerard Escudero, Lluis Marquez, and German Rigau. 2000.
An empirical study of the domain dependence of super-
vised word disambiguation systems. In 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 172?
180, Hong Kong, China, October. Association for Com-
putational Linguistics.
Benoit Favre, Bernd Bohnet, and D. Hakkani-Tur. 2010.
Evaluation of semantic role labeling and dependency
parsing of automatic speech recognition output. In
Proceedings of 2010 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), page
5342?5345.
Jenny Rose Finkel and Christopher D. Manning. 2009. Joint
parsing and named entity recognition. In Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 326?334, Boulder,
Colorado, June. Association for Computational Linguis-
tics.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into in-
formation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, page 363?370.
Ryan Gabbard. 2010. Null Element Restoration. Ph.D. the-
sis, University of Pennsylvania.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In 2001 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2002. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference, page
1974?1981.
Mohamed Maamouri and Ann Bies. 2004. Developing an
Arabic treebank: Methods, guidelines, procedures, and
tools. In Ali Farghaly and Karine Megerdoomian, edi-
tors, COLING 2004 Computational Approaches to Arabic
Script-based Languages, pages 2?9, Geneva, Switzerland,
August 28th. COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330, June.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceedings
of the Human Language Technology Conference/North
American Chapter of the Association for Computational
Linguistics (HLT/NAACL), New York City, NY, June.
151
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the Association for Computational Linguistics
(ACL-02), pages 104?111.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.
2007. Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Journal of
Natural Language Engineering, 13(2).
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona Diab,
Mohammed Maamouri, Aous Mansouri, and Wajdi Za-
ghouani. 2008. A pilot Arabic propbank. In Proceedings
of the International Conference on Language Resources
and Evaluation (LREC), Marrakech, Morocco, May 28-
30.
Slav Petrov and Dan Klein. 2007. Improved inferencing for
unlexicalized parsing. In Proc of HLT-NAACL.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1):11?39.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel, Jes-
sica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Indentifying entities and events in
OntoNotes. In Proceedings of the IEEE International
Conference on Semantic Computing (ICSC), September
17-19.
Sameer Pradhan, Wayne Ward, and James H. Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics Special Issue on Semantic Role Labeling,
34(2).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha
Palmer, Ralph Weischedel, and Nianwen Xue. 2011.
CoNLL-2011 shared task: Modeling unrestricted corefer-
ence in OntoNotes. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learning:
Shared Task, pages 1?27, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga
Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared
task: Modeling multilingual unrestricted coreference in
OntoNotes. In Joint Conference on EMNLP and CoNLL -
Shared Task, pages 1?40, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki,
Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008.
The Penn discourse treebank 2.0. In Proceedings of the
Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, May.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena, M. Anto`nia
Mart??, Mariona Taule?, Ve?ronique Hoste, Massimo Poesio,
and Yannick Versley. 2010. Semeval-2010 task 1: Coref-
erence resolution in multiple languages. In Proceedings of
the 5th International Workshop on Semantic Evaluation,
pages 1?8, Uppsala, Sweden, July.
Marta Recasens, Marie-Catherine de Marneffe, and Christo-
pher Potts. 2013. The life and death of discourse enti-
ties: Identifying singleton mentions. In Proceedings of
the 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 627?633, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind K. Joshi. 2008.
LTAG-spinal and the treebank. Language Resources and
Evaluation, 42(1):1?19, March.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog no.:
LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Robert Belvin, Sameer Pradhan, Lance Ramshaw,
and Nianwen Xue. 2011. OntoNotes: A large train-
ing corpus for enhanced processing. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language Ex-
ploitation. Springer.
Nianwen Xue and Martha Palmer. 2009. Adding semantic
roles to the Chinese Treebank. Natural Language Engi-
neering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11(2):207?238.
Nianwen Xue. 2008. Labeling Chinese predicates with se-
mantic roles. Computational Linguistics, 34(2):225?255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese treebank.
In Proceedings of the 23rd International Conference on
Computational Linguistics (COLING), Beijing, China.
Wajdi Zaghouani, Mona Diab, Aous Mansouri, Sameer Prad-
han, and Martha Palmer. 2010. The revised Arabic prop-
bank. In Proceedings of the Fourth Linguistic Annotation
Workshop, pages 222?226, Uppsala, Sweden, July.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free text.
In Proceedings of the ACL 2010 System Demonstrations,
pages 78?83, Uppsala, Sweden.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An empiri-
cal study. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1002?
1010.
152
