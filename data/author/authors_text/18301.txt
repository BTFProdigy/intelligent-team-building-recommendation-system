Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 150?155,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Humor as Circuits in Semantic Networks
Igor Labutov
Cornell University
iil4@cornell.edu
Hod Lipson
Cornell University
hod.lipson@cornell.edu
Abstract
This work presents a first step to a general im-
plementation of the Semantic-Script Theory
of Humor (SSTH). Of the scarce amount of
research in computational humor, no research
had focused on humor generation beyond sim-
ple puns and punning riddles. We propose
an algorithm for mining simple humorous
scripts from a semantic network (Concept-
Net) by specifically searching for dual scripts
that jointly maximize overlap and incongruity
metrics in line with Raskin?s Semantic-Script
Theory of Humor. Initial results show that a
more relaxed constraint of this form is capable
of generating humor of deeper semantic con-
tent than wordplay riddles. We evaluate the
said metrics through a user-assessed quality of
the generated two-liners.
1 Introduction
While of significant interest in linguistics and phi-
losophy, humor had received less attention in the
computational domain. And of that work, most re-
cent is predominately focused on humor recognition.
See (Ritchie, 2001) for a good review. In this pa-
per we focus on the problem of humor generation.
While humor/sarcasm recognition merits direct ap-
plication to the areas such as information retrieval
(Friedland and Allan, 2008), sentiment classifica-
tion (Mihalcea and Strapparava, 2006), and human-
computer interaction (Nijholt et al, 2003), the ap-
plication of humor generation is not any less sig-
nificant. First, a good generative model of humor
has the potential to outperform current discrimina-
tive models for humor recognition. Thus, ability to
!
Figure 1: Semantic circuit
generate humor will potentially lead to better humor
detection. Second, a computational model that con-
forms to the verbal theory of humor is an accessi-
ble avenue for verifying the psycholinguistic theory.
In this paper we take the Semantic Script Theory
of Humor (SSTH) (Attardo and Raskin, 1991) - a
widely accepted theory of verbal humor and build a
generative model that conforms to it.
Much of the existing work in humor generation
had focused on puns and punning riddles - hu-
mor that is centered around wordplay. And while
more recent of such implementations (Hempelmann
et al, 2006) take a knowledge-based approach that
is rooted in the linguistic theory (SSTH), the con-
straint, nevertheless, significantly limits the poten-
tial of SSTH. To our knowledge, our work is the first
attempt to instantiate the theory at the fundamental
level, without imposing constraints on phonological
similarity, or a restricted set of domain oppositions.
150
1.1 Semantic Script Theory of Humor
The Semantic Script Theory of Humor (SSTH) pro-
vides machinery to formalize the structure of most
types of verbal humor (Ruch et al, 1993). SSTH
posits an existence of two underlying scripts, one of
which is more obvious than the other. To be humor-
ous, the underlying scripts must satisfy two condi-
tions: overlap and incongruity. In the setup phase of
the joke, instances of the two scripts are presented
in a way that does not give away the less obvious
script (due to their overlap). In the punchline (res-
olution), a trigger expression forces the audience
to switch their interpretation to the alternate (less
likely) script. The alternate script must differ sig-
nificantly in meaning (be incongruent with the first
script) for the switch to have a humorous effect. An
example below illustrates this idea (S1 is the obvi-
ous script, and S2 is the alternate script. Bracketed
phrases are labeled with the associated script).
??Is the [doctor]S1 at home???
the [patient]S1 asked in his
[bronchial]S1 [whisper]S2. ??No,??
the [doctor?s]S1 [young and pretty
wife]S2 [whispered]S2 in reply.
[??Come right in.??]S2 (Raskin, 1985)
2 Related Work
Of the early prototypes of pun-generators, JAPE
(Binsted and Ritchie, 1994), and its successor,
STANDUP (Ritchie et al, 2007), produced ques-
tion/answer punning riddles from general non-
humorous lexicon. While humor in the generated
puns could be explained by SSTH, the SSTH model
itself was not employed in the process of generation.
Recent work of Hempelmann (2006) comes closer
to utilizing SSTH. While still focused on generating
puns, they do so by explicitly defining and applying
script opposition (SO) using ontological semantics.
Of the more successful pun generators are systems
that exploit lexical resources. HAHAcronym (Stock
and Strapparava, 2002), a system for generating hu-
morous acronyms, for example, utilizes WordNet-
Domains to select phonologically similar concepts
from semantically disparate domains. While the de-
gree of humor sophistication from the above systems
varies with the sophistication of the method (lexi-
cal resources, surface realizers), they all, without ex-
ception, rely on phonological constraints to produce
script opposition, whereas a phonological constraint
is just one of the many ways to generate script op-
position.
3 System overview
ConceptNet (Liu and Singh, 2004) lends itself as an
ideal ontological resource for script generation. As a
network that connects everyday concepts and events
with a set of causal and spatial relationships, the re-
lational structure of ConceptNet parallels the struc-
ture of the fabula model of story generation - namely
the General Transition Network (GTN) (Swartjes
and Theune, 2006). As such, we hypothesize that
there exist paths within the ConceptNet graph that
can be represented as feasible scripts in the sur-
face form. Moreover, multiple paths between two
given nodes represent overlapping scripts - a nec-
essary condition for verbal humor in SSTH. Given
a semantic network hypergraph G = (V,L) where
V ? Concepts, L ? Relations, we hypothesize
that it is possible to search for script-pairs as seman-
tic circuits that can be converted to a surface form
of the Question/Answer format. We define a circuit
as two paths from root A that terminate at a common
node B. Our approach is composed of three stages -
(1) we build a script model (SM) that captures likely
transitions between concepts in a surface-realizable
sequence, (2) The script model (SM) is then em-
ployed to generate a set of feasible circuits from a
user-specified root node through spreading activa-
tion, producing a set of ranked scripts. (3) Ranked
scripts are converted to surface form by aligning a
subset of its concepts to natural language templates
of the Question/Answer form. Alignment is per-
formed through a scoring heuristic which greedily
optimizes for incongruity of the surface form.
3.1 Script model
We model a script as a first order Markov chain of
relations between concepts. Given a seed concept,
depth-first search is performed starting from the root
concept, considering all directed paths terminating
at the same node as candidates for feasible script
pairs. Most of the found semantic circuits, however,
151
do not yield a meaningful surface form and need
to be pruned. Feasible circuits are learned in a su-
pervised way, where binary labels assign each can-
didate circuit one of the two classes {feasible,
infeasible} (we used 8 seed concepts, with 300
generated circuits for each concept). Learned tran-
sition probabilities are capable of capturing primi-
tive stories with events, consequences, as well as
appropriate qualifiers of certainty, time, size, loca-
tion. Given a chain of concepts S (from hereon re-
ferred to as a script) c1, c2...cn, we obtain its likeli-
hood Pr(S) =
?
Pr(rij |rjk), where rij and rjk are
directed relations joining concepts < ci, cj >, and
< cj , ck > respectively, and the conditionals are
computed from the maximum likelihood estimate of
the training data.
3.2 Semantic overlap and spreading activation
While the script model is able to capture seman-
tically meaningful transitions in a single script, it
does not capture inter-script measures such as over-
lap and incongruity. We employ a modified form
of spreading activation with fan-out and path con-
straints to find semantic circuits while maximizing
their semantic overlap. Activation starts at the user-
specified root concept and radiates along outgoing
edges. Edge pairs are weighted with their respective
transition probabilities Pr(rij |rjk) and a decay fac-
tor ? < 1 to penalize for long scripts. An additional
fan-out constraint penalizes nodes with a large num-
ber of outgoing edges (concepts that are too gen-
eral to be interesting). The weight of a current node
w(ci) is given by:
w(ci) =
?
ck?fin(cj)
?
cj?fin(ci)
Pr(rij |rjk)
|fout(ci)|
?w(cj) (1)
Termination condition is satisfied when the activa-
tion weights fall below a threshold (loop checking
is performed to prevent feedback). Upon termina-
tion, nodes are ranked by their activation weight, and
for each node above a specified rank, a set of paths
(scripts) Sk ? S is scored according to:.
?k = |Sk| log ? +
|Sk|?
i
log Prk(ri+1|ri) (2)
where ?k is decay-weighted log-likelihood of script
Sk in a given circuit and |Sk| is the length of script
A
Q
Q
Q
S1
S2
C1
C2
Figure 2: Question(Q) and Answer(A) concepts within
the semantic circuit. Areas C1 and C2 represent differ-
ent semantic clusters. Note that the answer(A) concept is
chosen from a different cluster than the question concepts
Sk (number of nodes in the kth chain). A set of
scripts S with the highest scores in the highest rank-
ing circuits represent scripts that are likely to be fea-
sible and display a significant amount of semantic
overlap within the circuit.
3.3 Incongruity and surface realization
The task is to select a script pair {Si, Sj i 6= j} ?
S ? S and a set of concepts C ? Si ? Sj that will
align with some surface template, while maximiz-
ing inter-script incongruity. As a measure of con-
cept incongruity, we hierarchically cluster the entire
ConceptNet using a Fast Community Detection al-
gorithm (Clauset et al, 2004). We observe that clus-
ters are generated for related concepts, such as reli-
gion, marriage, computers. Each template presents
up to two concepts {c1 ? Si, c2 ? Sj i 6= j} in the
question sentence (Q in Figure 2), and one concept
c3 ? Si ? Sj in the answer sentence (A in Figure
2). The motivation of this approach is that the two
concepts in the question are selected from two dif-
ferent scripts but from the same cluster, while the an-
swer concept is selected from one of the two scripts
and from a different cluster. The effect the generated
two-liner produces is that of a setup and resolution
(punchline), where the question intentionally sets up
two parallel and compatible scripts, and the answer
triggers the script switch. Below are the top-ranking
two-liners as rated by a group of fifteen subjects
(testing details in the next section). Each concept
is indicated in brackets and labeled with the script
from which the concept had originated:
Why does the [priest]root [kneel]S1 in
[church]S2? Because the [priest]root
wants to [propose woman]S1
152
Why does the [priest]root [drink
coffee]S1 and [believe god]S2?
Because the [priest]root wants to
[wake up]S1
Why is the [computer]root [hot]S1 in
[mit]S2? Because [mit]S2 is [hell]S2
Why is the [computer]root in
[hospital]S1? Because the
[computer]root has [virus]S2
4 Results
We evaluate the generated two-liners by presenting
them as human-generated to remove possible bias.
Fifteen subjects (N = 15, 12 male, 3 female - grad-
uate students in Mechanical Engineering and Com-
puter Science departments) were presented 48 high-
est ranking two-liners, and were asked to rate each
joke on the scale of 1 to 4 according to four cat-
egories: hilarious (4), humorous (3), not humor-
ous (2), nonsense(1). Each two-liner was generated
from one of the three root categories (12 two-liners
in each): priest, woman, computer, robot, and to
normalize against individual humor biases, human-
made two-liners were mixed in in the same cate-
gories. Two-liners generated by three different al-
gorithms were evaluated by each subject:
Script model + Concept clustering (SM+CC)
Both script opposition and incongruity are
favored through spreading activation and
concept clustering.
Script model only (SM) No concept clustering is
employed. Adherence of scripts to the script
model is ensured through spreading activation.
Baseline Loops are generated from a user-specified
root using depth first search. Loops are pruned
only to satisfy surface templates.
We compare the average scores between the two-
liners generated using both the script model and con-
cept clustering (SM+CC) (MEAN=1.95, STD=0.27)
and the baseline (MEAN=1.06, STD=0.58). We
observe that SM+CC algorithm yields significantly
higher-scoring two-liners (one-sided t-test) with
95% confidence.
0 
20 
40 
60 
80 
100 
Baseline SM SM+CC Human  
% 
(N
=1
5) 
Nonsense 
Non-
humorous 
Humorous 
Hilarious 
Figure 3: Human blind evaluation of generated two-liners
We observe that the fraction of non-humorous and
nonsensical two-liners generated is still significant.
Many non-humorous (but semantically sound) two-
liners were formed due to erroneous labels on the
concept clusters. While clustering provides a fun-
damental way to generate incongruity, noise in the
ConceptNet often leads of cluster overfitting, and as-
signs related concepts into separate clusters.
Nonsensical two-liners are primarily due to the in-
consistencies in POS with relation types within the
ConceptNet. Because our surface form templates
assume a part of speech, or a phrase type from the
ConceptNet specification, erroneous entries produce
nonsensical results. We partially address the prob-
lem by pruning low-scoring concepts (ConceptNet
features a SCORE attribute reflecting the number of
user votes for the concept), and all terminal nodes
from consideration (nodes that are not expanded by
users often indicate weak relationships).
5 Future Work
Through observation of the generated semantic
paths, we note that more complex narratives, beyond
questions/answer forms can be produced from the
ConceptNet. Relaxing the rigid template constraint
of the surface realizer will allow for more diverse
types of generated humor. To mitigate the fragility
of concept clustering, we are augmenting the Con-
ceptNet with additional resources that provide do-
main knowledge. Resources such as SenticNet
(WordNet-Affect aligned with ConceptNet) (Cam-
bria et al, 2010b), and WordNet-Domains (Kolte
and Bhirud, 2008) are both viable avenues for robust
concept clustering and incongruity generation.
153
Acknowledgement
This paper is for my Babishan - the most important
person in my life.
Huge thanks to Max Kelner - those everyday teas at
Mattins and continuous inspiration.
This work was supported in part by NSF CDI Grant
ECCS 0941561. The content of this paper is solely
the responsibility of the authors and does not neces-
sarily represent the official views of the sponsoring
organizations.
References
S. Attardo and V. Raskin. 1991. Script theory revis (it)
ed: Joke similarity and joke representation model. Hu-
mor: International Journal of Humor Research; Hu-
mor: International Journal of Humor Research.
K. Binsted and G. Ritchie. 1994. A symbolic description
of punning riddles and its computer implementation.
Arxiv preprint cmp-lg/9406021.
K. Binsted, A. Nijholt, O. Stock, C. Strapparava,
G. Ritchie, R. Manurung, H. Pain, A. Waller, and
D. O?Mara. 2006. Computational humor. Intelligent
Systems, IEEE, 21(2):59?69.
K. Binsted. 1996. Machine humour: An implemented
model of puns.
E. Cambria, A. Hussain, C. Havasi, and C. Eckl. 2010a.
Senticspace: visualizing opinions and sentiments in
a multi-dimensional vector space. Knowledge-Based
and Intelligent Information and Engineering Systems,
pages 385?393.
E. Cambria, R. Speer, C. Havasi, and A. Hussain. 2010b.
Senticnet: A publicly available semantic resource for
opinion mining. In Proceedings of the 2010 AAAI Fall
Symposium Series on Commonsense Knowledge.
A. Clauset, M.E.J. Newman, and C. Moore. 2004. Find-
ing community structure in very large networks. Phys-
ical review E, 70(6):066111.
F. Crestani. 1997. Retrieving documents by constrained
spreading activation on automatically constructed hy-
pertexts. In EUFIT 97-5th European Congress on In-
telligent Techniques and Soft Computing. Germany.
Citeseer.
L. Friedland and J. Allan. 2008. Joke retrieval: recogniz-
ing the same joke told differently. In Proceeding of the
17th ACM conference on Information and knowledge
management, pages 883?892. ACM.
C.F. Hempelmann, V. Raskin, and K.E. Triezenberg.
2006. Computer, tell me a joke... but please make it
funny: Computational humor with ontological seman-
tics. In Proceedings of the Nineteenth International
Florida Artificial Intelligence Research Society Con-
ference, Melbourne Beach, Florida, USA, May 11, vol-
ume 13, pages 746?751.
S.G. Kolte and S.G. Bhirud. 2008. Word sense disam-
biguation using wordnet domains. In Emerging Trends
in Engineering and Technology, 2008. ICETET?08.
First International Conference on, pages 1187?1191.
IEEE.
H. Liu and P. Singh. 2004. Conceptneta practical com-
monsense reasoning tool-kit. BT technology journal,
22(4):211?226.
R. Mihalcea and C. Strapparava. 2006. Learning to laugh
(automatically): Computational models for humor
recognition. Computational Intelligence, 22(2):126?
142.
M.E.J. Newman. 2006. Modularity and community
structure in networks. Proceedings of the National
Academy of Sciences, 103(23):8577?8582.
A. Nijholt, O. Stock, A. Dix, and J. Morkes. 2003. Hu-
mor modeling in the interface. In CHI?03 extended ab-
stracts on Human factors in computing systems, pages
1050?1051. ACM.
V. Raskin. 1998. The sense of humor and the truth. The
Sense of Humor. Explorations of a Personality Char-
acteristic, Berlin: Mouton De Gruyter, pages 95?108.
G. Ritchie, R. Manurung, H. Pain, A. Waller, R. Black,
and D. OMara. 2007. A practical application of com-
putational humour. In Proceedings of the 4th. Inter-
national Joint Workshop on Computational Creativity,
London, UK.
G. Ritchie. 2001. Current directions in computational
humour. Artificial Intelligence Review, 16(2):119?
135.
W. Ruch, S. Attardo, and V. Raskin. 1993. Toward an
empirical verification of the general theory of verbal
humor. Humor: International Journal of Humor Re-
search; Humor: International Journal of Humor Re-
search.
J. Savoy. 1992. Bayesian inference networks and spread-
ing activation in hypertext systems. Information pro-
cessing & management, 28(3):389?406.
S. Spagnola and C. Lagoze. 2011. Edge dependent
pathway scoring for calculating semantic similarity in
conceptnet. In Proceedings of the Ninth International
Conference on Computational Semantics, pages 385?
389. Association for Computational Linguistics.
O. Stock and C. Strapparava. 2002. Hahacronym:
Humorous agents for humorous acronyms. Stock,
Oliviero, Carlo Strapparava, and Anton Nijholt. Eds,
pages 125?135.
I. Swartjes and M. Theune. 2006. A fabula model for
emergent narrative. Technologies for Interactive Digi-
tal Storytelling and Entertainment, pages 49?60.
154
J.M. Taylor and L.J. Mazlack. 2004. Humorous word-
play recognition. In Systems, Man and Cybernetics,
2004 IEEE International Conference on, volume 4,
pages 3306?3311. IEEE.
J. Taylor and L. Mazlack. 2005. Toward computational
recognition of humorous intent. In Proceedings of
Cognitive Science Conference, pages 2166?2171.
J.M. Taylor. 2009. Computational detection of humor: A
dream or a nightmare? the ontological semantics ap-
proach. In Proceedings of the 2009 IEEE/WIC/ACM
International Joint Conference on Web Intelligence
and Intelligent Agent Technology-Volume 03, pages
429?432. IEEE Computer Society.
155
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 489?493,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Re-embedding Words
Igor Labutov
Cornell University
iil4@cornell.edu
Hod Lipson
Cornell University
hod.lipson@cornell.edu
Abstract
We present a fast method for re-purposing
existing semantic word vectors to improve
performance in a supervised task. Re-
cently, with an increase in computing re-
sources, it became possible to learn rich
word embeddings from massive amounts
of unlabeled data. However, some meth-
ods take days or weeks to learn good em-
beddings, and some are notoriously dif-
ficult to train. We propose a method
that takes as input an existing embedding,
some labeled data, and produces an em-
bedding in the same space, but with a bet-
ter predictive performance in the super-
vised task. We show improvement on the
task of sentiment classification with re-
spect to several baselines, and observe that
the approach is most useful when the train-
ing set is sufficiently small.
1 Introduction
Incorporating the vector representation of a word
as a feature, has recently been shown to benefit
performance in several standard NLP tasks such
as language modeling (Bengio et al, 2003; Mnih
and Hinton, 2009), POS-tagging and NER (Col-
lobert et al, 2011), parsing (Socher et al, 2010),
as well as in sentiment and subjectivity analysis
tasks (Maas et al, 2011; Yessenalina and Cardie,
2011). Real-valued word vectors mitigate sparsity
by ?smoothing? relevant semantic insight gained
during the unsupervised training over the rare and
unseen terms in the training data. To be effective,
these word-representations ? and the process by
which they are assigned to the words (i.e. embed-
ding) ? should capture the semantics relevant to
the task. We might, for example, consider dra-
matic (term X) and pleasant (term Y) to correlate
with a review of a good movie (task A), while find-
ing them of opposite polarity in the context of a
dating profile (task B). Consequently, good vectors
for X and Y should yield an inner product close to
1 in the context of task A, and ?1 in the context
of task B. Moreover, we may already have on our
hands embeddings for X and Y obtained from yet
another (possibly unsupervised) task (C), in which
X and Y are, for example, orthogonal. If the em-
beddings for task C happen to be learned from a
much larger dataset, it would make sense to re-
use task C embeddings, but adapt them for task
A and/or task B. We will refer to task C and its
embeddings as the source task and the source em-
beddings, and task A/B, and its embeddings as the
target task and the target embeddings.
Traditionally, we would learn the embeddings
for the target task jointly with whatever unla-
beled data we may have, in an instance of semi-
supervised learning, and/or we may leverage la-
bels from multiple other related tasks in a multi-
task approach. Both methods have been applied
successfully (Collobert and Weston, 2008) to learn
task-specific embeddings. But while joint train-
ing is highly effective, a downside is that a large
amount of data (and processing time) is required
a-priori. In the case of deep neural embeddings,
for example, training time can number in days. On
the other hand, learned embeddings are becoming
more abundant, as much research and computing
effort is being invested in learning word represen-
tations using large-scale deep architectures trained
on web-scale corpora. Many of said embeddings
are published and can be harnessed in their raw
form as additional features in a number of super-
vised tasks (Turian et al, 2010). It would, thus, be
advantageous to learn a task-specific embedding
directly from another (source) embedding.
In this paper we propose a fast method for re-
embedding words from a source embedding S to a
target embedding T by performing unconstrained
optimization of a convex objective. Our objec-
tive is a linear combination of the dataset?s log-
489
likelihood under the target embedding and the
Frobenius norm of the distortion matrix ? a ma-
trix of component-wise differences between the
target and the source embeddings. The latter acts
as a regularizer that penalizes the Euclidean dis-
tance between the source and target embeddings.
The method is much faster than joint training and
yields competitive results with several baselines.
2 Related Work
The most relevant to our contribution is the work
by Maas et.al (2011), where word vectors are
learned specifically for sentiment classification.
Embeddings are learned in a semi-supervised
fashion, and the components of the embedding are
given an explicit probabilistic interpretation. Their
method produces state-of-the-art results, however,
optimization is non-convex and takes approxi-
mately 10 hours on 10 machines1. Naturally, our
method is significantly faster because it operates in
the space of an existing embedding, and does not
require a large amount of training data a-priori.
Collobert and Weston (2008), in their seminal
paper on deep architectures for NLP, propose a
multilayer neural network for learning word em-
beddings. Training of the model, depending on
the task, is reported to be between an hour and
three days. While the obtained embeddings can
be ?fine-tuned? using backpropogation for a su-
pervised task, like all multilayer neural network
training, optimization is non-convex, and is sensi-
tive to the dimensionality of the hidden layers.
In machine learning literature, joint semi-
supervised embedding takes form in methods such
as the LaplacianSVM (LapSVM) (Belkin et al,
2006) and Label Propogation (Zhu and Ghahra-
mani, 2002), to which our approach is related.
These methods combine a discriminative learner
with a non-linear manifold learning technique in a
joint objective, and apply it to a combined set of
labeled and unlabeled examples to improve per-
formance in a supervised task. (Weston et al,
2012) take it further by applying this idea to deep-
learning architectures. Our method is different in
that the (potentially) massive amount of unlabeled
data is not required a-priori, but only the resultant
embedding.
1as reported by author in private correspondence. The
runtime can be improved using recently introduced tech-
niques, see (Collobert et al, 2011)
3 Approach
Let ?S ,?T ? R|V |?K be the source and target
embedding matrices respectively, where K is the
dimension of the word vector space, identical in
the source and target embeddings, and V is the set
of embedded words, given by VS ? VT . Following
this notation, ?i ? the ith row in ? ? is the respec-
tive vector representation of wordwi ? V . In what
follows, we first introduce our supervised objec-
tive, then combine it with the proposed regularizer
and learn the target embedding ?T by optimizing
the resulting joint convex objective.
3.1 Supervised model
We model each document dj ? D (a movie re-
view, for example) as a collection of words wij
(i.i.d samples). We assign a sentiment label sj ?
{0, 1} to each document (converting the star rating
to a binary label), and seek to optimize the con-
ditional likelihood of the labels (sj)j?{1,...,|D|},
given the embeddings and the documents:
p(s1, ..., s|D||D; ?T ) =
?
dj?D
?
wi?dj
p(sj |wi; ?T )
where p(sj = 1|wi,?T ) is the probability of as-
signing a positive label to document j, given that
wi ? dj . As in (Maas et al, 2011), we use logistic
regression to model the conditional likelihood:
p(sj = 1|wi; ?T ) =
1
1 + exp(??T?i)
where ? ? RK+1 is a regression parameter vector
with an included bias component. Maximizing the
log-likelihood directly (for ? and ?T ), especially
on small datasets, will result in severe overfitting,
as learning will tend to commit neutral words to
either polarity. Classical regularization will mit-
igate this effect, but can be improved further by
introducing an external embedding in the regular-
izer. In what follows, we describe re-embedding
regularization? employing existing (source) em-
beddings to bias word vector learning.
3.2 Re-embedding regularization
To leverage rich semantic word representations,
we employ an external source embedding and in-
corporate it in the regularizer on the supervised
objective. We use Euclidean distance between the
source and the target embeddings as the regular-
490
ization loss. Combined with the supervised objec-
tive, the resulting log-likelihood becomes:
argmax
?,?T
?
dj?D
?
wi?dj
log p(sj |wi; ?T )? ?||??||2F (1)
where ?? = ?T??S , ||?||F is a Frobenius norm,
and ? is a trade-off parameter. There are almost
no restrictions on ?S , except that it must match
the desired target vector space dimension K. The
objective is convex in ? and ?T , thus, yielding a
unique target re-embedding. We employ L-BFGS
algorithm (Liu and Nocedal, 1989) to find the op-
timal target embedding.
3.3 Classification with word vectors
To classify documents, re-embedded word vectors
can now be used to construct a document-level
feature vector for a supervised learning algorithm
of choice. Perhaps the most direct approach is to
compute a weighted linear combination of the em-
beddings for words that appear in the document
to be classified, as done in (Maas et al, 2011)
and (Blacoe and Lapata, 2012). We use the docu-
ment?s binary bag-of-words vector vj , and com-
pute the document?s vector space representation
through the matrix-vector product ?T vj . The re-
sulting K + 1-dimensional vector is then cosine-
normalized and used as a feature vector to repre-
sent the document dj .
4 Experiments
Data: For our experiments, we employ a large,
recently introduced IMDB movie review dataset
(Maas et al, 2011), in place of the smaller dataset
introduced in (Pang and Lee, 2004) more com-
monly used for sentiment analysis. The dataset
(50,000 reviews) is split evenly between training
and testing sets, each containing a balanced set of
highly polar (? 7 and? 4 stars out of 10) reviews.
Source embeddings: We employ three external
embeddings (obtained from (Turian et al, 2010))
induced using the following models: 1) hierarchi-
cal log-bilinear model (HLBL) (Mnih and Hinton,
2009) and two neural network-based models ? 2)
Collobert and Weston?s (C&W) deep-learning ar-
chitecture, and 3) Huang et.al?s polysemous neural
language model (HUANG) (Huang et al, 2012).
C&W and HLBL were induced using a 37M-word
newswire text (Reuters Corpus 1). We also induce
a Latent Semantic Analysis (LSA) based embed-
ding from the subset of the English project Guten-
berg collection of approximately 100M words. No
pre-processing (stemming or stopword removal),
beyond case-normalization is performed in either
the external or LSA-based embedding. For HLBL,
C&W and LSA embeddings, we use two variants
of different dimensionality: 50 and 200. In total,
we obtain seven source embeddings: HLBL-50,
HLBL-200, C&W-50, C&W-200, HUANG-
50, LSA-50, LSA-200.
Baselines: We generate two baseline embeddings
? NULL and RANDOM. NULL is a set of zero
vectors, and RANDOM is a set of uniformly
distributed random vectors with a unit L2-norm.
NULL and RANDOM are treated as source vec-
tors and re-embedded in the same way. The
NULL baseline is equivalent to regularizing on
the target embedding without the source embed-
ding. As additional baselines, we use each of the
7 source embeddings directly as a target without
re-embedding.
Training: For each source embedding matrix ?S ,
we compute the optimal target embedding matrix
?T by maximizing Equation 1 using the L-BFGS
algorithm. 20 % of the training set (5,000 docu-
ments) is withheld for parameter (?) tuning. We
use LIBLINEAR (Fan et al, 2008) logistic re-
gression module to classify document-level em-
beddings (computed from the ?T vj matrix-vector
product). Training (re-embedding and document
classification) on 20,000 documents and a 16,000
word vocabulary takes approximately 5 seconds
on a 3.0 GHz quad-core machine.
5 Results and Discussion
The main observation from the results is that our
method improves performance for smaller training
sets (? 5000 examples). The reason for the perfor-
mance boost is expected ? classical regularization
of the supervised objective reduces overfitting.
However, comparing to the NULL and RAN-
DOM baseline embeddings, the performance is
improved noticeably (note that a percent differ-
ence of 0.1 corresponds to 20 correctly classi-
fied reviews) for word vectors that incorporate the
source embedding in the regularizer, than those
that do not (NULL), and those that are based on
the random source embedding (RANDOM). We
hypothesize that the external embeddings, gen-
erated from a significantly larger dataset help
?smooth? the word-vectors learned from a small
labeled dataset alne. Further observations in-
clude:
491
Features Number of training examples
+ Bag-of-words features
.5K 5K 20K .5K 5K 20K
A. Re-embeddings (our method)
HLBL-50 74.01 79.89 80.94 78.90 84.88 85.42
HLBL-200 74.33 80.14 81.05 79.22 85.05 85.95
C&W-50 74.52 79.81 80.48 78.92 84.89 85.87
C&W-200 74.80 80.25 81.15 79.34 85.28 86.15
HUANG-50 74.29 79.90 79.91 79.03 84.89 85.61
LSA-50 72.83 79.67 80.67 78.71 83.44 84.73
LSA-200 73.70 80.03 80.91 79.12 84.83 85.31
B. Baselines
RANDOM-50 w/ re-embedding 72.90 79.12 80.21 78.29 84.01 84.87
RANDOM-200 w/ re-embedding 72.93 79.20 80.29 78.31 84.08 84.91
NULL w/ re-embedding 72.92 79.18 80.24 78.29 84.10 84.98
HLBL-200 w/o re-embedding 67.88 72.60 73.10 79.02 83.83 85.83
C&W-200 w/o re-embedding 68.17 72.72 73.38 79.30 85.15 86.15
HUANG-50 w/o re-embedding 67.89 72.63 73.12 79.13 84.94 85.99
C. Related methods
Joint training (Maas, 2011) ? ? 84.65 ? ? 88.90
Bag of Words SVM ? ? ? 79.17 84.97 86.14
Table 1: Classification accuracy for the sentiment task (IMDB
movie review dataset (Maas et al, 2011)). Subtable A compares
performance of the re-embedded vocabulary, induced from a
given source embedding. Subtable B contains a set of base-
lines: X-w/o re-embedding indicates using a source embedding
X directly without re-embedding.
BORING
source: lethal, lifestyles, masterpiece . . .
target: idiotic, soft-core, gimmicky
BAD
source: past, developing, lesser, . . .
target: ill, madonna, low, . . .
DEPRESSING
source: versa, redemption, townsfolk . . .
target: hate, pressured, unanswered ,
BRILLIANT
source: high-quality, obsession, hate . . .
target: all-out, bold, smiling . . .
Table 2: A representative set of words from the 20 closest-
ranked (cosine-distance) words to (boring, bad, depressing,
brilliant) extracted from the source and target (C&W-200)
embeddings. Source embeddings give higher rank to words
that are related, but not necessarily indicative of sentiment,
e.g. brilliant and obsession. Target words tend to be tuned
and ranked higher based on movie-sentiment-based rela-
tions.
Training set size: We note that with a sufficient
number of training instances for each word in the
test set, additional knowledge from an external
embedding does little to improve performance.
Source embeddings: We find C&W embeddings
to perform best for the task of sentiment classi-
fication. These embeddings were found to per-
form well in other NLP tasks as well (Turian et
al., 2010).
Embedding dimensionality: We observe that for
HLBL, C&W and LSA source embeddings (for all
training set sizes), 200 dimensions outperform 50.
While a smaller number of dimensions has been
shown to work better in other tasks (Turian et al,
2010), re-embedding words may benefit from a
larger initial dimension of the word vector space.
We leave the testing of this hypothesis for future
work.
Additional features: Across all embeddings, ap-
pending the document?s binary bag-of-words rep-
resentation increases classification accuracy.
6 Future Work
While ?semantic smoothing? obtained from intro-
ducing an external embedding helps to improve
performance in the sentiment classification task,
the method does not help to re-embed words that
do not appear in the training set to begin with. Re-
turning to our example, if we found dramatic and
pleasant to be ?far? in the original (source) em-
bedding space, but re-embed them such that they
are ?near? (for the task of movie review sentiment
classification, for example), then we might ex-
pect words such as melodramatic, powerful, strik-
ing, enjoyable to be re-embedded nearby as well,
even if they did not appear in the training set.
The objective for this optimization problem can be
posed by requiring that the distance between ev-
ery pair of words in the source and target embed-
dings is preserved as much as possible, i.e. min
(??i??j ? ?i?j)2 ?i, j (where, with some abuse of
notation, ? and ?? are the source and target em-
beddings respectively). However, this objective is
no longer convex in the embeddings. Global re-
embedding constitutes our ongoing work and may
pose an interesting challenge to the community.
7 Conclusion
We presented a novel approach to adapting exist-
ing word vectors for improving performance in
a text classification task. While we have shown
promising results in a single task, we believe that
the method is general enough to be applied to
a range of supervised tasks and source embed-
dings. As sophistication of unsupervised methods
grows, scaling to ever-more massive datasets, so
will the representational power and coverage of in-
duced word vectors. Techniques for leveraging the
large amount of unsupervised data, but indirectly
through word vectors, can be instrumental in cases
where the data is not directly available, training
time is valuable and a set of easy low-dimensional
?plug-and-play? features is desired.
492
8 Acknowledgements
This work was supported in part by the NSF CDI
Grant ECCS 0941561 and the NSF Graduate fel-
lowship. The content of this paper is solely the
responsibility of the authors and does not neces-
sarily represent the official views of the sponsor-
ing organizations. The authors would like to thank
Thorsten Joachims and Bishan Yang for helpful
and insightful discussions.
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled exam-
ples. The Journal of Machine Learning Research,
7:2399?2434.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556. Association for Compu-
tational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142?150. As-
sociation for Computational Linguistics.
Andriy Mnih and Geoffrey E Hinton. 2009. A scalable
hierarchical distributed language model. Advances
in neural information processing systems, 21:1081?
1088.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Jason Weston, Fre?de?ric Ratle, Hossein Mobahi, and
Ronan Collobert. 2012. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks
of the Trade, pages 639?655. Springer.
Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment anal-
ysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
172?182. Association for Computational Linguis-
tics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propa-
gation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
493
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 562?571,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Generating Code-switched Text for Lexical Learning
Igor Labutov
Cornell University
iil4@cornell.edu
Hod Lipson
Cornell University
hod.lipson@cornell.edu
Abstract
A vast majority of L1 vocabulary acqui-
sition occurs through incidental learning
during reading (Nation, 2001; Schmitt et
al., 2001). We propose a probabilistic ap-
proach to generating code-mixed text as
an L2 technique for increasing retention
in adult lexical learning through reading.
Our model that takes as input a bilingual
dictionary and an English text, and gener-
ates a code-switched text that optimizes a
defined ?learnability? metric by construct-
ing a factor graph over lexical mentions.
Using an artificial language vocabulary,
we evaluate a set of algorithms for gener-
ating code-switched text automatically by
presenting it to Mechanical Turk subjects
and measuring recall in a sentence com-
pletion task.
1 Introduction
Today, an adult trying to learn a new language is
likely to embrace an age-old and widely accepted
practice of learning vocabulary through curated
word lists and rote memorization. Yet, it is not
uncommon to find yourself surrounded by speak-
ers of a foreign language and instinctively pick up
words and phrases without ever seeing the defini-
tion in your native tongue. Hearing ?pass le sale
please? at the dinner table from your in-laws vis-
iting from abroad, is unlikely to make you think
twice about passing the salt. Humans are extraor-
dinarily good at inferring meaning from context,
whether this context is your physical surround-
ing, or the surrounding text in the paragraph of the
word that you don?t yet understand.
Recently, a novel method of L2 language teach-
ing had been shown effective in improving adult
lexical acquisition rate and retention
1
. This tech-
1
authors? unpublished work
nique relies on a phenomenon that elicits a nat-
ural simulation of L1-like vocabulary learning in
adults ? significantly closer to L1 learning for L2
learners than any model studied previously. By in-
fusing foreign words into text in the learner?s na-
tive tongue into low-surprisal contexts, the lexi-
cal acquisition process is facilitated naturally and
non-obtrusively. Incidentally, this phenomenon
occurs ?in the wild? and is termed code-switching
or code-mixing, and refers to the linguistic pattern
of bilingual speakers swapping words and phrases
between two languages during speech. While this
phenomenon had received significant attention
from both a socio-linguistic (Milroy and Muysken,
1995) and theoretical linguistic perspectives (Be-
lazi et al, 1994; Bhatt, 1997) (including some
computational studies), only recently has it been
hypothesizes that ?code-switching? is a marking
of bilingual proficiency, rather than deficiency
(Genesee, 2001).
Until recently it was widely believed that inci-
dental lexical acquisition through reading can only
occur for words that occur at sufficient density
in a single text, so as to elicit the ?noticing? ef-
fect needed for lexical acquisition to occur (Cobb,
2007). Recent neurophysiological findings, how-
ever, indicate that even a single incidental expo-
sure to a novel word in a sufficiently constrained
context is sufficient to trigger an early integra-
tion of the word in the brain?s semantic network
(Borovsky et al, 2012).
An approach explored in this paper, and moti-
vated by the above findings, exploits ?constrain-
ing? contexts in text to introduce novel words. A
state-of-the-art approach for generating such text
is based on an expert annotator whose job is to
decide which words to ?switch out? with novel
foreign words (from hereon we will refer to the
?switched out? word as the source word and to the
?switched in? word as the target word). Conse-
quently the process is labor-intensive and leads to
562
a ?one size fits all solution? that is insensitive to
the learner?s skill level or vocabulary proficiency.
This limitation is also cited in literature as a sig-
nificant roadblock to the widespread adaptation
of graded reading series (Hill, 2008). A reading-
based tool that follows the same principle, i.e. by
systematic exposure of a learner to an incremen-
tally more challenging text, will result in more ef-
fective learning (Lantolf and Appel, 1994).
To address the above limitation, we develop an
approach for automatically generating such ?code-
switched? text with an explicit goal of maximizing
the lexical acquisition rate in adults. Our method
is based on a global optimization approach that
incorporates a ?knowledge model? of a user with
the content of the text, to generate a sequence of
lexical ?switches?. To facilitate the selection of
?switch points?, we learn a discriminative model
for predicting switch point locations on a corpus
that we collect for this purpose (and release to the
community). Below is a high-level outline of this
paper.
? We formalize our approach within a prob-
abilistic graphical model framework, infer-
ence in which yields ?code-switched? text
that maximizes a surrogate to the acquisition
rate objective.
? We compare this global method to sev-
eral baseline techniques, including the strong
?high-frequency? baseline.
? We analyze the operating range in which
our model is effective and motivate the near-
future extension of this approach with the
proposed improvements.
2 Related Work
Our proposed approach to the computational gen-
eration of code-switched text, for the purpose of
L2 pedagogy, is influenced by a number of fields
that studied aspects of this phenomenon from dis-
tinct perspectives. In this section, we briefly de-
scribe a motivation from the areas of socio- and
psycho- linguistics and language pedagogy re-
search that indicate the promise of this approach.
2.1 Code-switching as a natural phenomenon
Code-switching (or code-mixing) is a widely stud-
ied phenomenon that received significant attention
over the course of the last three decades, across
the disciplines of sociolinguistics, theoretical and
psycholinguistics and even literary and cultural
studies (predominantly in the domain of Spanish-
English code-switching) (Lipski, 2005).
Code-switching that occurs naturally in bilin-
gual populations, and especially in children, has
for a long time been considered a marking of
incompetency in the second language. A more
recent view on this phenomenon, however, sug-
gests that due to the underlying syntactic com-
plexity of code-switching, code-switching is ac-
tually a marking of bilingual fluency (Genesee,
2001). More recently, the idea of employing
code-switching in the classroom, in a form of
conversation-based exercises, has attracted the
attention of multiple researchers and educators
(Moodley, 2010; Macaro, 2005), yielding promis-
ing results in an elementary school study in South-
Africa.
2.2 Computational Approaches to
Code-switching
Additionally, there has been a limited number
of studies of the computational approaches to
code-switching, and in particular code-switched
text generation. Solorio and Liu (2008), record
and transcribe a corpus of Spanish-English code-
mixed conversation to train a generative model
(Naive Bayes) for the task of predicting code-
switch points in conversation. Additionally they
test their trained model in its ability to generate
code-switched text with convincing results. Build-
ing on their work, (Adel et al, 2012) employ ad-
ditional features and a recurrent network language
model for modeling code-switching in conversa-
tional speech. Adel and collegues (2011) propose
a statistical machine translation-based approach
for generating code-switched text. We note, how-
ever, that the primary goal of these methods is in
the faithful modeling of the natural phenomenon
of code-switching in bilingual populations, and
not as a tool for language teaching. While useful
in generating coherent, syntactically constrained
code-switched texts in its own right, none of these
methods explicitly consider code-switching as a
vehicle for teaching language, and thus do not
take on an optimization-based view with an ob-
jective of improving lexical acquisition through
the reading of the generated text. More recently,
and concurrently with our work, Google?s Lan-
guage Immersion app employs the principle of
563
code-switching for language pedagogy, by gener-
ating code-switched web content, and allowing its
users to tune it to their skill level. It does not, how-
ever, seem to model the user explicitly, nor is it
clear if it performs any optimization in generating
the text, as no studies have been published to date.
2.3 Computational Approaches to Sentence
Simplification
Although not explicitly for teaching language,
computational approaches that facilitate accessi-
bility to texts that might otherwise be too difficult
for its readers, either due to physical or learning
disabilities, or language barriers, are relevant. In
the recent work of (Kauchak, 2013), for example
demonstrates an approach to increasing readability
of texts by learning from unsimplified texts. Ap-
proaches in this area span methods for simplify-
ing lexis (Yatskar et al, 2010; Biran et al, 2011),
syntax (Siddharthan, 2006; Siddharthan et al,
2004), discourse properties (Hutchinson, 2005),
and making technical terminology more accessible
to non-experts (Elhadad and Sutaria, 2007). While
the resulting texts are of great potential aid to lan-
guage learners and may implicitly improve upon a
reader?s language proficiency, they do not explic-
itly attempt to promote learning as an objective in
generating the simplified text.
2.4 Recent Neurophysiological findings
Evidence for the potential effectiveness of code-
switching for language acquisition, stem from the
recent findings of (Borovsky et al, 2012), who
have shown that even a single exposure to a novel
word in a constrained context, results in the inte-
gration of the word within your existing semantic
base, as indicated by a change in the N400 elec-
trophysiological response recorded from the sub-
jects? scalps. N400 ERP marker has been found
to correlate with the semantic ?expectedness? of a
word (Kutas and Hillyard, 1984), and is believed
to be an early indicator of word learning. Further-
more, recent work of (Frank et al, 2013), show
that word surprisal predicts N400, providing con-
crete motivation for artificial manipulation of text
to explicitly elicit word learning through natural
reading, directly motivating our approach. Prior to
the above findings, it was widely believed that for
evoking ?incidental? word learning through read-
ing alone, the word must appear with sufficiently
high frequency within the text, such as to elicit the
?noticing? effect ? a prerequisite to lexical acqui-
sition (Schmidt and Schmidt, 1995; Cobb, 2007).
3 Model
3.1 Overview
The formulation of our model is primarily moti-
vated by two hypotheses that have been validated
experimentally in the cognitive science literature.
We re-state these hypotheses in the language of
?surprisal?:
1. Inserting a target word into a low surprisal
context increases the rate of that word?s inte-
gration into a learner?s lexicon.
2. Multiple exposures to the word in low sur-
prisal contexts increases rate of that word?s
integration.
Hypothesis 1 is supported by evidence from
(Borovsky et al, 2012; Frank et al, 2013), and hy-
pothesis 2 is supported by evidence from (Schmidt
and Schmidt, 1995). We adopt the term ?low-
surprisal? context to identify contexts (e.g. n-
grams) that are highly predictive of the target word
(e.g. trailing word in the n-gram). The motiva-
tion stems from the recent evidence (Frank et al,
2013) that low-surprisal contexts affect the N400
response and thus correlate with word acquisi-
tion. To realize a ?code-switched? mixture that
adheres maximally to the above postulates, it is
self-evident that a non-trivial optimization prob-
lem must be solved. For example, naively select-
ing a few words that appear in low-surprisal con-
texts may facilitate their acquisition, but at the ex-
pense of other words within the same context that
may appear in a larger number of low-surprisal
contexts further in the text.
To address this problem, we approach it with
a formulation of a factor graph that takes global
structure of the text into account. Factor graph for-
malism allows us to capture local features of indi-
vidual contexts, such as lexical and syntactic sur-
prisal, while inducing dependencies between con-
sequent ?switching decisions? in the text. Max-
imizing likelihood of the joint probability under
the factorization of this graph yields an optimal
sequence of these ?switching decisions? in the en-
tirety of the text. Maximizing joint likelihood, as
we will show in the next section, is a surrogate to
maximizing the probability of the learner acquir-
ing novel words through the process of reading the
generated text.
564
wi Known     + Constrained Unknown + Constrained Unknown + Unconstrained Known     + Unconstrained 
. . .
w1 w2 w3 w4 w5 w|V |
KNOW DON?T KNOW ?ik
Meaning of  malhela?Existingknowledgeof word
User?s lexical knowledge model
z
ik
The door   malhela   to the beach
wiinfused word
KNOW DON?T KNOW ContextualInterpretationof word
Updatedknowledgebelief
UpdatedKnowldgeModel
LEGEND
Mixed-Language Content
Figure 1: Overview of the approach. Probabilistic learner model (PLM) provides the current value of the
belief in the learner?s knowledge of any given word. Local contextual model provides the value of the
belief in learning the word from the context alone. Upon exposure of the learner to the word in the given
context, PLM is updated with the posterior belief in the user?s knowledge of the word.
3.2 Language Learner Model
A simplified model of the learner, that we shall
term a Probabilistic Learner Model (PLM) serves
as a basis for our approach. PLM is a model of
a learner?s lexical knowledge at any given time.
PLM models the learner as a vector of indepen-
dent Bernoulli distributions, where each compo-
nent represents a probability of the learner know-
ing the corresponding word. We motivate a proba-
bilistic approach by taking the perspective of mea-
suring our belief in the learner?s knowledge of any
given word, rather than the learner?s uncertainty in
own knowledge. Formally, we can fully specify
this model for learner i as follows:
U
i
= (pi
i
0
, pi
i
1
, . . . , pi
i
|V |
) (1)
where V is the vocabulary set ? identical
across all users, and pi
i
j
is our degree of belief in
the learner i?s knowledge of a target wordw
j
? V .
Statistical estimation techniques exist for estimat-
ing an individual?s vocabulary size, such as (Bhat
and Sproat, 2009; Beglar, 2010), and can be di-
565
rectly employed for estimating the parameters of
this model as our prior belief about user i?s knowl-
edge.
The primary motivation behind a probabilistic
user model, is to provide a mechanism for up-
dating these probabilities as the user progresses
through her reading. Maximizing the parameters
of the PLM under a given finite span of code-
switched text, thus, provides a handle for generat-
ing optimal code-switched content. Additionally,
a probabilistic approach allows for a natural inte-
gration of the user model with the uncertainty in
other components of the system, such as uncer-
tainty in determining the degree of constraint im-
posed by the context, and in bitext alignment.
3.3 Model overview
At the high level, as illustrated in Figure 1, our ap-
proach integrates the model of the learner (PLM)
with the local contextual features to update the
PLM parameters incrementally as the learner pro-
gresses through the text. The fundamental as-
sumption behind our approach is that the learner?s
knowledge of a given word after observing it in
a sentence is a function of 1) the learner?s previ-
ous knowledge of the word, prior to observing it
in a given sentence and 2) a degree of constraint
that a given context imposes on the meaning of the
novel word, and is directly related to the surprisal
of novel word in that context. Broadly, as the
learner progresses from one sentence to the next,
exposing herself to more novel words, the updated
parameters of the language model in turn guide
the selection of new ?switch-points? for replac-
ing source words with the target foreign words. In
practice, however, this process is carried out im-
plicitly and off-line by optimizing the estimated
progress of the learner?s PLM, without dynamic
feedback. Next, we describe the model in detail.
3.4 Switching Factor Graph Model
To aid in the specification of the factor graph struc-
ture, we introduce new terminology. Because the
PLM is updated progressively, we will refer to the
parameters of the PLM for a given word w
i
after
observing its k
th
appearance (instance) in the text,
as the learner?s state of knowledge of that word,
and denote it as a binary random variable z
i
k
.
P (z
i
k
= 1) =
?
?
?
Probability that
word w
i
? V
is understood on k
th
exposure
Without explicit testing of the user, this variable
is hidden. We can view the prior learning model
as the parameters of the vector of random variables
(z
0
0
, z
1
0
, . . . z
|V |
0
).
The key to our approach is in how the param-
eters of these hidden variables are updated from
repeated exposures to words in various contexts.
Intuitively, an update to the parameter of z
i
k
from
z
i
k?1
occurs after the learner observes word w
i
in
a context (this may be an n-gram, an entire sen-
tence or paragraph containing w
i
, but we will re-
strict our attention to fixed-length n-grams). In-
tuitively an update to the parameter of z
i
k?1
will
depend on how ?constrained? the meaning of w
i
is in the given context. We will refer to it as the
?learnability?, denoted by L
k
i
, of word w
i
on its
k
th
appearance, given its context. Formally, we
will define ?learnability? as follows:
P (L
i
k
= 1|w
i
,w
\i
, z
\i
k
) =
P (constrained(w
i
) = 1|w)
?
i 6=j
P (z
j
k
= 1)
(2)
where w
\i
represents the set of words that com-
prise the context window of w
i
, not including w
i
,
and z
\i
k
are the states corresponding to each of the
words in w
\i
. P (constrained(w
i
) = 1|w) is a real
value (scaled between 0 and 1) that represents the
degree of constraint imposed on the meaning of
word w
i
by its context. This value comes from
a binary prediction model trained to predict the
?predictability? of a word in its context, and is
based on the dataset that we collected (described
later in the paper). Generally, this value may
come directly from the surprisal quantity given by
a language model, or may incorporate additional
features that are found informative in predicting
the constraint on the word. Finally, the quantity
is weighted by the parameters of the state vari-
ables corresponding to the words other than w
i
contained in the context. This encodes an intu-
ition that a degree of predictability of a given word
given its context is related to the learner?s knowl-
edge of the other words in that context. If, for ex-
ample, in the sentence ?pass me the salt and pep-
per, please?, both ?salt? and ?pepper? are substi-
tuted with their foreign translations that the learner
is unlikely to know, it?s equally unlikely that she
will learn them after being exposed to this con-
text, as the context itself will not offer sufficient
566
information for both words to be inferred simulta-
neously. On the other hand, substituting ?salt? and
?pepper? individually, is likely to make it much
easier to infer the meaning of the other.
z
i
k 1
z
i
k
L
i
k
Figure 2: A noisy-OR combination of the learner?s
previous state of knowledge of the word z
i
k?1
and
the word?s ?learnability? in the observed context
L
i
k
The updated parameter of z
i
k
is obtained from a
noisy-OR combination of the parameters of z
i
k?1
and L
i
k
:
P (z
i
k
= 1|z
i
k?1
, L
i
k
) =
1? [1? P (L
i
k
= 1)][1? P (z
k?1
= 1)]
A noisy-OR-based CPD provides a convenient
and tractable approximation in capturing the in-
tended intuition: updated state of knowledge of a
given word will increase if the word is observed in
a ?good? context, or if the learner already knows
the word.
Combining Equation 2 for each word in the con-
text using the noisy-OR, the updated state for word
w
i
will now be conditioned on z
i
k?1
, z
\i
k
,w
k
. Be-
cause of the dependence of each z in the context
on all other hidden variables in that context, we
can capture the dependence using a single factor
per context, with all of the z variables taking part
in a clique, whose dimension is the size of the con-
text.
We will now introduce a dual interpretation of
the z variables: as ?switching? variables that de-
cide whether a given word will be replaced with its
translation in the foreign language. If, for exam-
ple, all of the words have high probability of be-
ing known by a learner, than maximizing the joint
likelihood of the model will result in most of the
words ?switched-out? ? a desired result. For an
arbitrary prior PLM and the input text, maximiz-
ing joint likelihood will result in the selection of
?switched-out? words that have the highest final
probability of being ?known? by the learner.
3.5 Inference
The problem of selecting ?switch-points? reduces
to the problem of inference in the resulting factor
graph. Unfortunately, without a fairly strong con-
straint on the collocation of switched words, the
resulting graph will contain loops, requiring tech-
niques of approximate inference. To find the opti-
mal settings of the z variables, we apply the loopy
max-sum algorithm. While variants of loopy be-
lief propagation, in general, are not guaranteed to
converge, we found that the convergence does in-
deed occur in our experiments.
3.6 Predicting ?predictable? words
We carried out experiments to determine which
words are likely to be inferred from their context.
The collected data-set is then used to train a logis-
tic regression classifier to predict which words are
likely to be easily inferred from their context. We
believe that this dataset may also be useful to re-
searchers in studying related phenomena, and thus
make it publicly available.
For this task, we focus only on the following
context features for predicting the ?predictability?
of words: n-gram probability, vector-space simi-
larity score, coreferring mentions. N-gram prob-
ability and vector-space similarity
2
score are all
computed within a fixed-size window of the word
(trigrams using Microsoft N-gram service). Coref-
erence feature is a binary feature which indicates
whether the word has a co-referring mention in a
3-sentence window preceding a given context (ob-
tained using Stanford?s CoreNLP package). We
train L2-regularized logistic regression to predict
a binary label L ? {Constrained,Unconstrained}
using a crowd-sourced corpus described below.
3.7 Corpus Construction
For collecting data about which words are likely
to be ?predicted? given their content, we devel-
oped an Amazon Mechanical Turk task that pre-
sented turkers with excerpts of a short story (En-
glish translation of ?The Man who Repented? by
2
we employ C&W word embeddings from http://
metaoptimize.com/projects/wordreprs/
567
wi
wj
wi
wi
wj
wj
wkwi
S 1
S 2
S 3
S 4
S 5
S 6
Original Text Factor GraphMapping f 1
f 2
f 3
f 4
f 5
z
i0
z
i1
z
i2
z
i3z j0
z
j1
z
j2 zk0
Figure 3: Sequence of sentences in the text (left) is mapped into a factor graph, whose nodes correspond
to specific occurences of individual words, connected in a clique corresponding to a context in which the
word occurs.
Ana Maria Matute), with some sentences contain-
ing a blank in place of a word. Only content words
were considered for the task. Turkers were re-
quired to type in their best guess, and the num-
ber of semantically similar guesses were counted
by an average number of 6 other turkers. A ra-
tio of the median of semantically similar guesses
to the total number of guesses was then taken as
the score representing ?predictability? of the word
being guessed in the given context. All words cor-
responding to blanks whose scores were equal to
and above 0.6 were than taken as a positive la-
bel (Constrained) and scores below 0.6 were taken
as a negative label (Unconstrained). Turkers that
judged the semantic similarity of the guesses of
other turkers achieved an average Cohen?s kappa
agreement of 0.44, indicating fair to poor agree-
ment.
4 Experiments
We carried out experiments on the effectiveness
of our approach using the Amazon Mechanical
Turk platform. Our experimental procedure was
as follows: 162 turkers were partitioned into four
groups, each corresponding to a treatment con-
dition: OPT (N=34), HF (N=41), RANDOM
(N=43), MAN (N=44). Each condition corre-
Figure 4: Visualization of the most ?predictable?
words in an excerpt from the ?The Man who Re-
pented? by Ana Maria Matute (English transla-
tion). Font-size correlates with the score given by
judge turkers in evaluating guesses of other turk-
ers that were presented with the same text, but the
word replaced with a blank. Snippet of the dataset
that we release publicly.
sponded to a model used to generate the presented
code-switched text. For all experiments, the text
used was a short story ?Lottery? by Shirley Jack-
son, and a total number of replaced words was
controlled (34). Target vocabulary consisted of
words from an artificial language, generated stat-
ically by a mix of words from several languages.
Below we describe the individual treatment condi-
tions:
RANDOM (Baseline): words for switching are
568
selected at random from content only words.
HF (High Frequency) Baseline: words for
switching are selected at random from a ranked
list of words that occur most frequently in the pre-
sented text.
MAN (Manual) Baseline: words for switch-
ing are selected manually by the author, based on
the intuition of which words are most likely to be
guessed in context.
OPT (Optimization-based): factor graph-based
model proposed in this paper is used for generat-
ing code-switched content. The total number of
switched words generated by this method is used
as a constant for all baselines.
Turkers were solicited to participate in a study
that involved ?reading a short story with a twist?
(title of HIT). Not the title, nor the description
gave away the purpose of the study, nor that it
would be followed by a quiz. Time was not con-
trolled for this study, but on average turkers took
27 minutes to complete the reading. Upon com-
pleting the reading portion of the task, turkers
were presented with novel sentences that featured
the words observed during reading, where only
one of the sentences used the word in a semanti-
cally correct way. Turkers were asked to select the
sentence that ?made the most sense?. An example
of the sentences presented during the test:
Example 1
XMy edzino loves to go shopping every
weekend.
The edzino was too big to explore on our
own, so went with a group.
English word: wife
Example 2
X His unpreadvers were utterly confus-
ing and useless.
The unpreadvers was so strong, that he
had to go to a hospital.
English word: directions
A ?recall? metric was computed for each turker,
defined as the ratio of correctly selected sentences
to the total number of sentences presented. The
?grand-average recall? across all turkers was then
computed and reported here.
5 Results
We perform a one-way ANOVA across the four
groups listed above, with the resulting F = 11.38
and p = 9.7e?7. Consequently, multiple pairwise
comparison of the models was performed with the
Bonferroni-corrected pairwise t-test, yielding the
only significantly different recall means between
HF ? MAN (p = 0.00018), RANDOM ?
MAN (p = 2.8e ? 6), RANDOM ? OPT
(p = 0.00587). The results indicate that, while
none of the automated methods (RANDOM ,
HF , OPT ) outperform manually generated code-
switched text, OPT outperforms the RANDOM
baseline (no decisive conclusion can be drawn
with respect to the HF ? RANDOM pair).
Additionally, we note, that for words with fre-
quency less than 4, OPT produces recall that is
on average higher than theHF baseline (p=0.043,
Welch?s t-test), but at the expense of higher fre-
quency words.
l
l
l
l
l
l
0.00
0.25
0.50
0.75
HF MAN OPT RANDOMCondition
Rec
all
Condition
HF
MAN
OPT
RANDOM
Figure 5: Results presented for 4 groups, sub-
jected to 4 treatment conditions: RANDOM ,
HF , MAN , OPT . Recall performance for
each group corresponds to the average ratio of
selected sentences that correctly utilize code-
switched words in novel contexts, across all turk-
ers.
6 Discussion
We observe from our experiments that the
optimization-based approach does not in general
outperform the HF baseline. The strength of the
569
ll
l0.0
0.2
0.4
0.6
0.8
HF OPTCondition
Rec
all ConditionHF
OPT
Figure 6: Subset of the results for 2 of the 4 treat-
ment conditions: HF and OPT that correspond
to recall only for words with item frequency in the
presented text below 4.
frequency-based baseline is attributed to a well-
known phenomenon that item frequency promotes
the ?noticing? effect during reading, critical for
triggering incidental lexical acquisition. Gener-
ating code-switched text by replacing high fre-
quency content words, thus, in general is a sim-
ple and viable approach for generating effective
reading-based L2 curriculum aids. However, this
method is fundamentally less flexible than the
optimization-based method proposed in this paper,
for several reasons:
? The optimization-based method explicitly
models the learner and thus generates code-
switched text progressively more fit for a
given individual, even across a sequence of
multiple texts. A frequency-based baseline
alone would generate content at approxi-
mately the same level of difficulty consis-
tently, with the pattern that words that tend to
have high frequency in the natural language
in general to be the ones that are ?switched-
out? most often.
? An optimization-based approach is able to
elicit higher recall in low frequency words,
as the mechanism for their selection is driven
by the context in which these words appear,
rather than frequency alone, favoring those
that are learned more readily through context.
Moreover, the proposed method in this pa-
per is extensible to more sophisticated learner
models, with a potential to surpass the results
presented here. Another worthwhile applica-
tion of this method is as a nested component
within a larger optimization-based tool, that
in addition to generating code-switched text
as demonstrated here, aids in selecting con-
tent (such as popular books) as units in the
code-switched curriculum.
7 Future Work
In this work we demonstrated a pilot implemen-
tation of a model-based, optimization-based ap-
proach to content generation for assisting in the
reading-based L2 language acquisition. Our ap-
proach is based on static optimization, and while
it would, in theory progress in difficulty with more
reading, its open-loop nature precludes it from
maintaining an accurate model of the learner in
the long-term. For generating effecting L2 con-
tent, it is important that the user be kept in a ?zone
of proximal development? ? a tight region where
the level of the taught content is at just the right
difficulty. Maintaining an accurate internal model
of the learner is the single most important require-
ment for achieving this functionality. Closed-loop
learning, with active user feedback is, thus, going
to be functionally critical component of any sys-
tem of this type that is designed to function in the
long-term.
Additionally, our approach is currently a proof-
of-concept of an automated method for generat-
ing content for assisted L2 acquisition, and is lim-
ited to artificial language and only isolated lexi-
cal items. The next step would be to integrate
bitext alignment across texts in two natural lan-
guages, inevitably introducing another stochas-
tic component into the pipeline. Extending this
method to larger units, like chunks and simple
grammar is another important avenue along which
we are taking this work. Early results from concur-
rent research indicate that ?code-switched based?
method proposed here is also effective in eliciting
acquisition of multi-word chunks.
References
Heike Adel, Ngoc Thang Vu, Franziska Kraus, Tim
Schlippe, Haizhou Li, and Tanja Schultz. 2012. Re-
570
current neural network language modeling for code
switching conversational speech. ICASSP.
David Beglar. 2010. A rasch-based validation of the
vocabulary size test. Language Testing, 27(1):101?
118.
Hedi M Belazi, Edward J Rubin, and Almeida Jacque-
line Toribio. 1994. Code switching and x-bar the-
ory: The functional head constraint. Linguistic in-
quiry, pages 221?237.
Suma Bhat and Richard Sproat. 2009. Knowing
the unseen: estimating vocabulary size over unseen
samples. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 109?117. Association for Computational
Linguistics.
Rakesh Mohan Bhatt. 1997. Code-switching,
constraints, and optimal grammars. Lingua,
102(4):223?251.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification.
Fabian Blaicher. 2011. SMT-based Text Generation
for Code-Switching Language Models. Ph.D. thesis,
Nanyang Technological University, Singapore.
Arielle Borovsky, Jeffrey L Elman, and Marta Kutas.
2012. Once is enough: N400 indexes semantic inte-
gration of novel word meanings from a single expo-
sure in context. Language Learning and Develop-
ment, 8(3):278?302.
Tom Cobb. 2007. Computing the vocabulary demands
of l2 reading. Language Learning & Technology,
11(3):38?63.
Noemie Elhadad and Komal Sutaria. 2007. Min-
ing a lexicon of technical terms and lay equivalents.
In Proceedings of the Workshop on BioNLP 2007:
Biological, Translational, and Clinical Language
Processing, pages 49?56. Association for Compu-
tational Linguistics.
Stefan L Frank, Leun J Otten, Giulia Galli, and
Gabriella Vigliocco. 2013. Word surprisal predicts
n400 amplitude during reading. In Proceedings of
the 51st annual meeting of the Association for Com-
putational Linguistics, pages 878?883.
Fred Genesee. 2001. Bilingual first language acqui-
sition: Exploring the limits of the language faculty.
Annual Review of Applied Linguistics, 21:153?168.
David R Hill. 2008. Graded readers in english. ELT
journal, 62(2):184?204.
Ben Hutchinson. 2005. Modelling the substitutabil-
ity of discourse connectives. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 149?156. Association for
Computational Linguistics.
David Kauchak. 2013. Improving text simplification
language modeling using unsimplified text data. In
Proceedings of ACL.
Marta Kutas and Steven A Hillyard. 1984. Brain po-
tentials during reading reflect word expectancy and
semantic association. Nature.
James P Lantolf and Gabriela Appel. 1994. Vy-
gotskian approaches to second language research.
Greenwood Publishing Group.
John M Lipski. 2005. Code-switching or borrowing?
no s?e so no puedo decir, you know. In Selected Pro-
ceedings of the Second Workshop on Spanish Soci-
olinguistics, pages 1?15.
Ernesto Macaro. 2005. Codeswitching in the l2
classroom: A communication and learning strat-
egy. In Non-native language teachers, pages 63?84.
Springer.
Lesley Milroy and Pieter Muysken. 1995. One
speaker, two languages: Cross-disciplinary per-
spectives on code-switching. Cambridge University
Press.
Visvaganthie Moodley. 2010. Code-switching and
communicative competence in the language class-
room. Journal for Language Teaching, 44(1):7?22.
Ian SP Nation. 2001. Learning vocabulary in another
language. Ernst Klett Sprachen.
Richard C Schmidt and Richard W Schmidt. 1995. At-
tention and awareness in foreign language learning,
volume 9. Natl Foreign Lg Resource Ctr.
Norbert Schmitt, Diane Schmitt, and Caroline
Clapham. 2001. Developing and exploring the be-
haviour of two new versions of the vocabulary levels
test. Language testing, 18(1):55?88.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of the 20th international
conference on Computational Linguistics, page 896.
Association for Computational Linguistics.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77?109.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365?368. Association for
Computational Linguistics.
571
