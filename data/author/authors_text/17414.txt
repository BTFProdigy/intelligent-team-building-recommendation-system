Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 675?683, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SCAI: Extracting drug-drug interactions using a rich feature vector
Tamara Bobic?1,2, Juliane Fluck1, Martin Hofmann-Apitius1,2
1Fraunhofer SCAI
Schloss Birlinghoven
53754 Sankt Augustin
Germany
2B-IT, Bonn Universita?t
Dahlmannstra?e 2
53113 Bonn
Germany
{tbobic, jfluck, hofmann-apitius}@scai.fraunhofer.de
Abstract
Automatic relation extraction provides great
support for scientists and database curators in
dealing with the extensive amount of biomed-
ical textual data. The DDIExtraction 2013
challenge poses the task of detecting drug-
drug interactions and further categorizing
them into one of the four relation classes. We
present our machine learning system which
utilizes lexical, syntactical and semantic based
feature sets. Resampling, balancing and en-
semble learning experiments are performed to
infer the best configuration. For general drug-
drug relation extraction, the system achieves
70.4% in F1 score.
1 Introduction
Drug-drug interactions (DDI) describe possible in-
terference between pharmacological substances and
are of critical importance in drug development and
administration (August et al, 1997). A drug may
alter the metabolism of another, thus causing an en-
hanced, reduced or even toxic effect in certain med-
ical treatments. For example: ?Fluvoxamine in-
hibits the CYP2C9 catalyzed biotransformation of
tolbutamide.? Automated extraction of DDI from
biomedical literature allows for a more efficient
maintenance of the drug knowledge databases and
is beneficial for patients, health care professionals
and the pharmaceutical industry.
Having in mind their biomedical importance, the
objective of the first DDIExtraction challenge1 in
1http://labda.inf.uc3m.es/
DDIExtraction2011/
2011 was to motivate the development and to eval-
uate the automatic relation extraction (RE) systems
for DDI. Given annotated drug entities, the partic-
ipants addressed the task of identifying undirected
binary relations among them. The knowledge ex-
traction was performed on the sentence level and the
best system achieved 65.74% F1 score (Thomas et
al., 2011a).
The 2013 DDIExtraction challenge2 (organized
as Task 9 of SemEval 2013 (Segura-Bedmar et al,
2013)) is based on a similar task definition, but ad-
ditionally includes the disambiguation between four
types of interaction: mechanism, effect, advise and
int. The evaluation of participating systems is two-
fold, i. e. partial and strict. Partial evaluation con-
siders that a prediction is correct when the pair la-
bel matches the gold annotation, while strict eval-
uation requires also a correct relation type to be
assigned. The train and test corpora were gener-
ated from textual resources of DrugBank (Knox et
al., 2011) database and MedLine3 abstracts, dealing
with the topic of DDI.
In the following sections we describe our super-
vised machine learning based approach for the ex-
traction of DDI, using a rich feature vector (see Sec-
tion 2.1). The base system employed LibLINEAR
classifier, generating the first run submitted to the
DDIExtraction challenge. Configurations coming
from the two ensemble strategies (Section 2.2) pro-
duced the remaining prediction runs. Furthermore,
we experimentally investigated the impact of train
2http://www.cs.york.ac.uk/semeval-2013/
task9/
3http://www.ncbi.nlm.nih.gov/pubmed/
675
corpora imbalance on DDI detection through resam-
pling strategies (Section 2.3). Finally, relation type
disambiguation methodology is presented in Sec-
tion 2.4.
2 Methods
We formulate the task of relation extraction as
feature-based classification of co-occurring entities
in a sentence. A sentence with n entities contains at
most
(n
2
)
interacting pairs. For entity pairs that the
classifier detects as ?true?, a post-processing step is
performed where one of the four relation types is
assigned, depending on the identified type-specific
trigger words.
2.1 Features
To improve generalization of lexical information
Porter stemming algorithm (Porter, 1980) was ap-
plied. All entities present in the sentence, which
were not a part of the investigated pair, are renamed
to a common neutral name (entity blinding).
For the generation of dependency-based features,
sentences in the provided corpora were parsed using
Charniak-Lease parser (Lease and Charniak, 2005;
Thomas et al, 2011b). The resulting constituent
parse trees were converted into Stanford dependency
graphs (Marneffe et al, 2006). Following the idea of
Thomas et al (2011b), similar relations are treated
equally by using their common parent type (unifica-
tion of dependency types). An example is generaliz-
ing relations ?subj?, ?nsubj? and ?csubj? to a parent
relation ?subj?.
In the following subsections the three groups of
features (lexical, syntactical and semantic) with their
corresponding members are described. Table 1 gives
a more structured overview of the feature vector, or-
ganized by type. It should be noted that the listed
features are used for the generation of all three pre-
diction sets submitted to the DDI challenge.
2.1.1 Lexical features
Lexical features capture the token information
around the inspected entity pair (EP). The sentence
text is divided into three parts: text between the EP,
text before the EP (left from the first entity) and text
after the EP (right from the second entity). It has
been observed that much of the relation information
can be extracted by only considering these three con-
texts (Bunescu and Mooney, 2005b; Giuliano et al,
2006).
The majority of features are n-grams based, with
n ? {1, 2, 3}. They encompass a narrow (win-
dow=3) and wide (window=10) surrounding con-
text, along with the area between the entities. Addi-
tionally, combinations of the tokens from the three
areas is considered, thus forming before-between,
between-after and before-after conjunct features
(narrow context).
2.1.2 Syntactic/Dependency features
Vertices (v) in the dependency graph are analyzed
from a lexical (stemmed token text) and syntacti-
cal (POS tag) perspective, while the edges (e) are
included using the grammatical relation they repre-
sent.
The majority of dependency-based features are
constructed using the properties of edges and ver-
tices along the shortest path (SP) of an entity pair.
The shortest path subtree is conceived to encode
grammatical relations with highest information con-
tent for a specific EP (Bunescu and Mooney, 2005a).
Similarly to lexical features, n-grams of vertices
(edges) along the SP are captured. Furthermore, al-
ternating sequences of vertices and edges (v-walks
and e-walks) of length 3 are accounted for, follow-
ing previous work (Kim et al, 2010; Miwa et al,
2010).
Apart from the SP-related features, incorporat-
ing information about the entities? parents and their
common ancestor in the dependency graph is also
beneficial. The lexical and syntactical properties of
these vertices are encoded, along with the grammat-
ical relations on the path from the entities to their
common ancestor.
2.1.3 Semantic features
Semantic group of features deals with understand-
ing and meaning of the context in which a particular
entity pair appears.
A feature that accounts for hypothetical state-
ments was introduced in order to reduce the num-
ber of false positives (phrases that indicate investi-
gation in progress, but not actual facts). Negation
(e. g. ?not?) detected close to the entity pair (narrow
context) along with a check whether entities in the
676
pair refer to the same real-word object (abbreviation
or a repetition) represent features which also con-
tribute to the reduction of false positive predictions.
Drug entities in the corpora were annotated with
one of four classes (drug, drug n, brand, group),
which provided another layer of relation informa-
tion for the classifier. Prior knowledge about true
DDI coming from the train corpora is used as a fea-
ture, if a previously known EP is observed in the test
data. Presence of other entities (which are not part
of the inspected EP) in the sentence text is captured,
together with their position relative to the EP.
Finally, mentions of general trigger (interaction)
terms are checked in all three context areas. More-
over, interaction phrases specific to a certain DDI
type (see Section 2.4) are accounted for.
2.2 Ensemble learning
Combining different machine learning algorithms
was proposed as a direction for improvement of the
classification accuracy (Bauer and Kohavi, 1999).
A synthesis of predictions using LibLINEAR,
Na??ve Bayes and Voting Perceptron classifiers is an
attempt to approach and learn the relation informa-
tion from different angles with a goal of increasing
the system?s performance. The three base models in-
cluded in the ensemble are employed through their
WEKA4 (Hall et al, 2009) implementation with de-
fault parameter values and trained on the full feature
vector described in Section 2.1.
LibLINEAR (Fan et al, 2008) is a linear support
vector machine classifier, which has shown high per-
formance (in runtime as well as model accuracy) on
large and sparse data sets. Support vector machines
(SVM, Cortes and Vapnik (1995)) have gained a lot
of popularity in the past decade and very often are
state-of-the-art approach for text mining challenges.
Na??ve Bayes (Domingos and Pazzani, 1996) is
a simple form of Bayesian networks which relies
on the assumption that every feature is independent
from all other features. Despite their naive design
and apparently oversimplified assumptions, Na??ve
Bayes can often outperform more sophisticated clas-
sification methods and has worked quite well in
many complex real-world situations. Furthermore,
it can be robust to noise features and is quite insen-
4http://www.cs.waikato.ac.nz/ml/weka/
Corpus Pos Neg Total
MedLine 232 (0.13) 1,555 (0.87) 1,787
DrugBank 3,788 (0.15) 22,217 (0.85) 26,005
Table 2: Ratio of positive and negative instances in the
DrugBank and MedLine train corpora.
sitive to stratification (Provost, 2000), which is of
high value in class imbalance scenarios.
Voting Perceptron (Freund and Schapire, 1999)
combines a series of perceptrons, which are lin-
ear classification algorithms that process elements
in the train set one at a time (?online?). The sys-
tem stores the number of iterations the perceptron
?survives?, i. e. when the training set instances are
classified correctly. The obtained count represents a
weight used for combining the prediction vectors by
a weighted majority vote.
In the ensemble learning scenario we consider
two strategies that aim at increasing the system?s
performance by either favoring precision or recall:
1. ?majority? ? a pair represents true relation only
if majority of the classifiers support that claim
2. ?union? ? a pair represents true relation if at
least one of the classifiers supports that claim
2.3 Train corpora imbalance
Analysis of the basic train corpora statistics re-
veals an unequal ratio of positive and negative in-
stances, i. e. under-representation of true interacting
pairs (see Table 2). Class distribution imbalance of-
ten causes machine learning algorithms to perform
poorly on the minority class (Hulse et al, 2007),
thus, in this case, affecting the recall of true rela-
tions.
In order to explore the sensitivity of our system
to the positive/negative ratio, we performed random
undersampling of the data, artificially obtaining a
desirable ratio (50-50). All positive instances in the
dataset were kept, while the same number of neg-
ative instances were randomly chosen. The reverse
approach of oversampling was considered, but given
the ample train data provided by the organizers, such
strategy could pose run-time challenges.
The experimental setting is described as follows.
MedLine and DrugBank train corpora were divided
further into train (exp-train) and test (exp-test) sets,
677
Feature
L
ex
ic
al
1. n-grams of tokens between the EP
2. n-grams of tokens before the EP (narrow context, window = 3)
3. n-grams of tokens after the EP (narrow context, window = 3)
4. n-grams of tokens before the EP (wide context, window = 10)
5. n-grams of tokens after the EP (wide context, window = 10)
6. conjucted positions: before-between, between-after and before-after
S
yn
ta
ct
ic
al
/D
ep
en
de
nc
y
7. dependency n-grams on the SP
8. syntactical n-grams on the SP
9. lexical n-grams on the SP
10. lexical and syntactical e-walks
11. lexical and syntactical v-walks
12. SP length (number of edges)
13. lexical and syntactical information of the entities? parents
14. lexical and syntactical information of the entities? common ancestor
15. dependency n-grams from both entities to their common ancestor
16. common ancestor represents a verb or a noun
S
em
an
ti
c
17. hypothetical context
18. negation close to the EP
19. entities refer to the same object
20. type of entities that form the EP
21. prior knowledge (from the train data)
22. other entities present close to the EP
23. DDI trigger words (general)
24. DDI types trigger words (specific)
Table 1: Overview of features used, stratified into groups. EP denotes an entity pair, SP represent the shortest path.
27.5	 ?
64	 ?
36.4	 ? 36.9	 ?
41.2	 ?
55.9	 ?
41.1	 ?
69.5	 ?
43.8	 ?
69.3	 ?
46	 ?
62	 ?
51.6	 ?
72	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
MedLine	 ? DrugBank	 ?
F1	 ?
	 ?
lex	 ? sem	 ? syn	 ? lex+sem	 ? lex+syn	 ? sem+syn	 ? lex+sem+syn	 ?
Figure 1: Contribution of individual feature sets and their combinations to the system?s performance, evaluated by 10-
fold cross-validation on the train corpora. Lex is an abbreviation for lexical, sem for semantic and syn for syntactical
features.
678
Corpus Exp-train pairs Exp-test pairs
MedLine 1,259 (70.4%) 528 (29.6%)
DrugBank 18,148 (69.8%) 7,857 (30.2%)
Table 3: Experimental train and test subsets derived from
the MedLine and DrugBank train corpora.
Relation MedLine DrugBank
mechanism 62 (0.27) 1257 (0.33)
effect 152 (0.66) 1535 (0.41)
advise 8 (0.03) 818 (0.21)
int 10 (0.04) 178 (0.05)
Table 4: The number of positive pairs for different DDI
types in the train corpora. Ratios are given in brackets.
with an approximate ratio of 70-30. Instances from
a particular document were always sampled to the
same subset, in order to avoid information leakage.
Table 3 gives an overview of the number of entity
pairs each set comprises. The exp-train corpora were
used for training the model in an original (full-size)
and balanced (subsample) scenario, evaluated on the
exp-test sets.
It should be noted that undersampling experi-
ments were performed on the train corpora in order
to inspect the impact of data imbalance on our sys-
tem (results shown in Section 3.4). However, due
to the challenge limitation of submitting only three
runs, this configuration was ignored in favor of uti-
lizing the complete train corpora.
2.4 Relation type assignment
The DDIExtraction challenge guidelines specify
four classes of relations: advise, mechanism, effect
and int. Table 4 illustrates the ratio of positive pairs
assigned to each type in MedLine and DrugBank
train corpora.
In Section 2.4.1, a brief outlook on the interaction
type characteristics is given, along with some of the
most common relation (trigger) phrases specific to
them. Section 2.4.2 explains the methodology be-
hind the process of relation type assignment.
2.4.1 Relations overview
Advise pertains to recommendations regarding co-
administration of two or more drugs. Sentences de-
scribing these relations usually contain words such
as: should, recommended, advisable, caution, avoid
etc., as seen in the following examples:
? Barbiturates and glutethimide should not be
administered to patients receiving coumarin
drugs.
? Concurrent therapy with ORENCIA and TNF
antagonists is not recommended.
? The co-administration of Fluvoxamine Tablets
and diazepam is generally not advisable.
Effect is a relation type describing the signs or
symptoms linked to the DDI, including the phar-
macodynamic effect, i. e. mechanism of interaction.
Some of the phrases often found to denote this type
of relation are: effect, cause, decrease, increase, in-
hibit, activate, modulate etc. The following exam-
ples present expressions of an effect relation:
? Pretreatment of megakaryocytes with extracel-
lular RR (50 microM) also inhibited InsP(3)-
induced responses.
? It is concluded that neurotensin modulates in
an opposite way the function of the enkephalin-
ergic neurons and the central action of tuftsin.
? Diazepam at doses of 0.25 mg/kg and 2.5
mg/kg injected with morphine was found to
decrease the antinociceptive effect of mor-
phine.
Mechanism illustrates a more detailed description
of the observed pharmacokinetic changes that in-
cludes biochemical information about metabolism,
absorption, biotransformation, excretion etc. Mech-
anism relations often include mentions of effect-
related interaction phrases, but provide an additional
knowledge layer by addressing more complex bio-
logical concepts:
? Cholestyramine, an anionic-binding resin, has
a considerable effect in lowering the rate and
extent of fluvastatin bioavailability.
? Additional iron significantly inhibited the
absorption of cobalt in both dietary cobalt
treatments.
? Macrolide antibiotics inhibit the metabolism of
HMG-CoA reductase inhibitors that are me-
tabolized by CYP3A4.
679
Int relation implies sentences which only state
that an interaction occurs, without providing much
additional information about it. Trigger phrases that
can be found in such sentences are usually limited to
different lexical forms of ?interaction?:
? Rifampin and warfarin: a drug interaction.
? In vitro interaction of prostaglandin F2alpha
and oxytocin in placental vessels.
? Treatment with antidepressant drugs can di-
rectly interfere with blood glucose levels or
may interact with hypoglycemic agents.
2.4.2 Type disambiguation methodology
We approach the problem of relation type disam-
biguation as a post-processing step, utilizing identi-
fied (sentence level) trigger words as classification
determinants. Precompiled relation trigger lists are
generated by manual inspection of the train corpora,
largely focusing on MedLine. The lists are specific
to the four interaction types and non-overlapping.
Cases when a sentence contains trigger phrases
from different relation classes are resolved by fol-
lowing a priority list:
1. advise
2. mechanism
3. effect
4. int
The rationale behind such priority assignment are
the following observed patterns in the train corpora.
Regardless of effect or mechanism connotation, if
the sentence contains recommendation-like phrases
(e. g. ?should?, ?advisable?), it is almost always
classified as an advise. Likewise, even though a re-
lation might be describing an effect, if it contains
a more detailed biochemical description, it is most
likely representing mechanism. Finally, effect has
advantage over int due to the simplicity of the int
relation, along with the lowest observed frequency.
3 Results and Discussion
3.1 Baseline relation extraction performance
Performances of the submitted prediction runs are
shown in Table 5, where the first row (run1) repre-
sents a system trained on the original (unbalanced)
train corpora, using LibLINEAR classifier and a rich
feature vector (see Section 2.1). The table offers re-
sults overview on MedLine, DrugBank and joined
test corpora (?All?), using partial evaluation (general
DDI detection).
The difference in performance on MedLine and
Drugbank is apparent, measuring up to almost 25
percentage points (pp) in F1 score (46.2% for Med-
Line and 71.1% for DrugBank). Due to a consid-
erably larger size of the DrugBank corpus, overall
results are greatly influenced by this corpus (F1 =
69.0%).
The results imply system?s sensitivity towards
class imbalance, which manifests in favored preci-
sion over recall. However, this discrepancy is much
less observed on DrugBank test corpus. Despite the
similarity in class ratio, DrugBank is a more com-
pact and homogenous corpus, with a relatively uni-
fied writing style. Coming from a manually curated
database, it has a rather standardized way of describ-
ing interactions, resulting in higher performance of
the relation extraction system. MedLine corpora,
however, are derived from different journals and re-
search groups which gives rise to extremely diverse
writing styles and a more challenging task for infor-
mation extraction.
3.2 Features contribution
Figure 1 illustrates the performance of the LibLIN-
EAR classifier, when all combinations of the three
different feature sets are explored.
It can be observed that the highest performance is
always achieved when all the features are included
during training (lex+syn+sem), resulting in 51.6%
and 72.0% F1 score for 10-fold cross-validation on
MedLine and DrugBank train corpora respectively.
Lexical features appear to be most useful for the
DrugBank corpus, achieving 88.9% of the maxi-
mum performance when used solely. MedLine, on
the other hand, benefits the most from syntactic fea-
tures that reach 79.8% of the best result, compared to
53.3% with lexical features. Semantic group of fea-
tures exhibits a uniform performance for both cor-
pora, achieving 36.4% and 36.9% of F1 score. Fi-
nally, grouping of two or all three feature sets is
always beneficial and results in higher performance
than the constituting base configurations.
680
MedLine DrugBank All
Classifier P R F1 P R F1 P R F1
run1: LibLINEAR 68.8 34.7 46.2 83.6 61.9 71.1 82.6 59.2 69.0
run2: Majority 68.6 25.3 36.9 83.7 61.7 71.0 82.9 58.1 68.3
run3: Union 43.1 52.6 47.4 79.6 68.1 73.4 74.8 66.6 70.4
Table 5: Results of the three submitted runs on the test corpora.
Classifier DrugBank MedLine
LibLinear 654 48
Na??ve Bayes 854 88
V. Perceptron 608 30
Majority 693 35
Union 980 116
Table 6: Number of positive predictions on MedLine and
DrugBank test corpora, using different configurations.
3.3 Ensemble experiments
Performance of the majority and union ensemble
configurations on the test corpora is presented in Ta-
ble 5. Table 6 gives an overview of the number of
predicted positive pairs by the ensemble, as well as
those by the individual base classifiers.
Voting Perceptron behaves similarly to LibLin-
ear, while Na??ve Bayes demonstrates insensitivity
in terms of class imbalance, predicting the high-
est number of positive pairs for both MedLine and
DrugBank test corpora.
Union voting strategy tends to overcome the lim-
itations of poor recall, resulting in highest perfor-
mance on all test corpora (47.4% for MedLine,
73.4% for DrugBank and 70.4% for All) among the
three runs. The superior result is obtained by dimin-
ishing precision in favor or recall, which was shown
as beneficial in these use-cases. However, the F1
score difference is slight (1.2 pp, 2.3 pp and 1.4 pp),
as compared to the baseline system (run1).
Predictions using the union ensemble ranked 3rd
in the general DDI extraction evaluation, achieving
5.5 pp and 9.6 pp of F1 score less than the top two
participating teams.
MedLine DrugBank
Train set P R F1 P R F1
original 48.4 39.6 43.6 75.1 62.4 68.2
balanced 37.2 70.4 48.7 60.8 72.7 66.2
Table 7: Comparison of results on the full train set and
a balanced subsample, as evaluated on the MedLine and
DrugBank train corpora.
3.4 Balanced training corpora
Table 7 presents relation extraction performance for
training on a balanced subset, compared to the orig-
inal unbalanced corpus.
In case of MedLine, an increase of around 5 pp
in F1 score can be observed for the balanced sub-
sample. However, given a relatively high initial per-
formance on DrugBank and the characteristics of
that corpus, training on a subsample results in 2 pp
reduced F1 score. The raise of 30.8 pp in recall
contributes greatly to the increased performance on
MedLine, even though 11.2 pp of precision are lost.
However, in case of DrugBank, a 10.3 pp increase
in recall is not enough to compensate for the 14.3 pp
loss in precision.
It can be observed that although undersampling
approach removes information from the model train-
ing stage, the class balance plays a more significant
role for the final performance.
3.5 Relation type disambiguation
Correct classification of interacting pairs into four
defined classes was evaluated using macro and mi-
cro average measures.
While micro-averaged F1 score is calculated by
constructing a global contingency table and then
calculating precision and recall, macro-averaged F1
score is obtained by first calculating precision and
recall for each relation type and then taking their
681
MedLine DrugBank All
P R F1 P R F1 P R F1
micro avg. 62.5 31.6 42.0 51.3 43.9 47.3 55.1 39.5 46.0
macro avg. 42.0 19.7 26.9 66.5 35.3 46.1 66.6 33.8 44.8
mechanism 70.0 29.2 41.2 58.0 39.2 46.8 53.2 39.1 45.0
effect 64.7 35.5 45.8 52.4 44.6 48.2 48.8 43.9 46.2
advise 18.2 28.6 22.2 50.7 65.0 57.0 50.5 63.3 56.2
int 0 0 0 100 1.1 2.1 100 1.0 2.1
Table 8: Results of DDI extraction when relation class detection is evaluated.
average (Segura-Bedmar et al, 2013). Therefore,
macro average takes into consideration the relative
frequency of each interaction class, while micro av-
erage treats all classes equally.
Table 8 shows an overview of performances for
DDI extraction with relation class disambiguation,
evaluated for each type separately, as well as cumu-
latively using micro and macro scores. For Med-
Line test corpus, the micro average F1 score of 42%
ranked 1st among all participating systems. How-
ever, the macro average score is much lower, due to
poor performance on advise and int relation classes
and occupies 5th position. Considering that our
methodology gives advantage to relations which are
observed more frequently, it is more adapted to-
wards the micro measure.
The process of manually generating type-specific
trigger lists was largely based on the MedLine train
corpus due to its size, with the assumption that
the relations in DrugBank are similarly expressed.
However, both micro and macro scores for Drug-
Bank ranked 7th, showing that adaptation of trigger
word lists needs to be done, depending on the target
corpus.
In general, lower performance for relation class
assignment is partially due to incompleteness of the
trigger lists, but also coming intrinsically from the
relation priority hierarchy. Most of classification
errors occur when a trigger word belonging to a
?higher? priority class is identified in the sentence.
In the following example the word ?should? im-
plies advise relation, although guanfacine and CNS-
depressant drug express an effect relation:
The potential for increased sedation when guan-
facine is given with other CNS-depressant drug
should be appreciated.
Another example is a sentence mentioning ?ef-
fect?, but actually describing a simple int relation:
Chloral hydrate and methaqualone interact
pharmacologically with orally administered antico-
agulant agents, but the effect is not clinically signif-
icant.
Furthermore, a lot of missclassifications occur in
sentences which contain pairs and triggers from dif-
ferent types, resulting in all relations being assigned
to the highest identified type.
4 Conclusion
We present a machine learning based system for
extraction of drug-drug interactions, using lexical,
syntactic and semantic properties of the sentence
text. The system achieves competitive performance
for the general DDI extraction, albeit demonstrat-
ing sensitivity to the train corpora class imbalance.
We show that, depending on the use case, resam-
pling, balancing and ensemble strategies are suc-
cessful in tuning the system to favor recall over pre-
cision. The post-processing step of relation type as-
signment achieves top ranked results for the Med-
Line corpus, however, needs more adaption in case
of DrugBank. Future work includes a comparison
with a multi-classifier approach, which circumvents
the manual task of trigger list generation, supporting
the fully automated scenario of relation extraction.
Acknowledgments
The authors would like to thank Roman Klinger for
fruitful discussions. T. Bobic? was funded by the
Bonn-Aachen International Center for Information
Technology (B-IT) Research School.
682
References
J.T. August, F. Murad, W. Anders, J.T. Coyle, and A.P. Li.
1997. Drug-Drug Interactions: Scientific and Regula-
tory Perspectives: Scientific and Regulatory Perspec-
tives. Advances in pharmacology. Elsevier Science.
E. Bauer and R. Kohavi. 1999. An empirical comparison
of voting classification algorithms: Bagging, boosting,
and variants. Machine Learning, 36(1-2).
R. C. Bunescu and R. J. Mooney. 2005a. A shortest path
dependency kernel for relation extraction. In HLT and
EMNLP.
R. C. Bunescu and R. J. Mooney. 2005b. Subsequence
Kernels for Relation Extraction. NIPS.
C. Cortes and V. Vapnik. 1995. Support vector networks.
In Machine Learning.
P. Domingos and M. Pazzani. 1996. Beyond indepen-
dence: Conditions for the optimality of the simple
bayesian classifier. In ICML.
E. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
LIBLINEAR: A Library for Large Linear Classifica-
tion. Machine Learning Research, 9.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3).
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proc. of the 11st
Conf. of the European Chapter of the Association for
Computational Linguistics (EACL?06).
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data mining
software: An update. SIGKDD Explorations, 11.
J. V. Hulse, T. M. Khoshgoftaar, and A. Napolitano.
2007. Experimental perspectives on learning from im-
balanced data. In ICML.
S. Kim, J. Yoon, J. Yang, and S. Park. 2010. Walk-
weighted subsequence kernels for protein-protein in-
teraction extraction. BMC Bioinformatics, 11.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoumbou,
R. Eisner, A. Chi Guo, and D.S Wishart. 2011. Drug-
bank 3.0: a comprehensive resource for ?omics? re-
search on drugs. Nucleic Acids Res, 39.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In Proc. of IJCNLP?05.
M. C. De Marneffe, B. Maccartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In LREC.
M. Miwa, R. Saetre, J. D. Kim, and J. Tsujii. 2010. Event
extraction with complex event classification using rich
features. Journal of bioinformatics and computational
biology, 8.
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14.
F. Provost. 2000. Machine learning from imbalanced
data sets 101 (extended abstract).
I. Segura-Bedmar, P. Martnez, and M. Herrero-Zazo.
2013. Semeval-2013 task 9: Extraction of drug-drug
interactions from biomedical texts. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011a. Relation extraction for drug-drug interactions
using ensemble learning. In Proceedings of the 1st
Challenge Task on Drug-Drug Interaction Extraction
2011.
P. Thomas, S. Pietschmann, I. Solt, D. Tikk, and U. Leser.
2011b. Not all links are equal: Exploiting dependency
types for the extraction of protein-protein interactions
from text. In Proceedings of BioNLP 2011 Workshop.
683
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 35?43,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Improving Distantly Supervised Extraction of Drug-Drug and
Protein-Protein Interactions
Tamara Bobic?,1,2? Roman Klinger,1? Philippe Thomas,3 and Martin Hofmann-Apitius1,2
1Fraunhofer Institute for
Algorithms and Scientific
Computing (SCAI)
Schloss Birlinghoven
53754 Sankt Augustin
Germany
2Bonn-Aachen Center for
Information Technology
Dahlmannstra?e 2
53113 Bonn
Germany
3Computer Science Institut
Humboldt-Universita?t
Unter den Linden 6
10099 Berlin
Germany
{tbobic,klinger,hofmann-apitius}@scai.fraunhofer.de
thomas@informatik.hu-berlin.de
Abstract
Relation extraction is frequently and suc-
cessfully addressed by machine learning
methods. The downside of this approach
is the need for annotated training data, typi-
cally generated in tedious manual, cost inten-
sive work. Distantly supervised approaches
make use of weakly annotated data, like au-
tomatically annotated corpora.
Recent work in the biomedical domain
has applied distant supervision for protein-
protein interaction (PPI) with reasonable
results making use of the IntAct database.
Such data is typically noisy and heuristics
to filter the data are commonly applied. We
propose a constraint to increase the qual-
ity of data used for training based on the
assumption that no self-interaction of real-
world objects are described in sentences.
In addition, we make use of the Univer-
sity of Kansas Proteomics Service (KUPS)
database. These two steps show an increase
of 7 percentage points (pp) for the PPI cor-
pus AIMed. We demonstrate the broad appli-
cability of our approach by using the same
workflow for the analysis of drug-drug in-
teractions, utilizing relationships available
from the drug database DrugBank. We
achieve 37.31 % in F1 measure without man-
ually annotated training data on an indepen-
dent test set.
1 Introduction
Assuming co-mentioned entities to be related is
an approach of extracting relations of real-world
objects with limited precision. Extracting high
quality interaction pairs from free text allows for
?These authors contributed equally.
building networks, e. g. of proteins, which need
less manual curation to serve as a model for further
knowledge processing steps. Nevertheless, just as-
suming co-occurrence to model an interaction or
relation is common, as the development of inter-
action extraction systems can be time-consuming
and complex.
Currently, a lot of relation extraction (RE) sys-
tems rely on machine learning, namely classifying
pairs of entities to be related or not (Airola et al,
2008; Miwa et al, 2009; Kim et al, 2010). De-
spite the fact that machine learning has been most
successful in identifying relevant relations in text,
a drawback is the need for manually annotated
training data. Domain experts have to dedicate
time and effort to this tedious and labor-intensive
process.
Specific biomedical domains have been ex-
plored more extensively than others, thus creating
an imbalance in the number of existing corpora
for a specific RE task. Protein-protein interactions
(PPI) have been investigated the most, which gave
rise to a number of available corpora. Pyysalo et al
(2008) standardized five PPI corpora to a unified
XML format. Recently, a drug-drug-interaction
(DDI) corpus is made available in the same for-
mat, originally for the DDI Extraction Workshop1
(Segura-Bedmar et al, 2011b).
As a consequence of the overall scarcity of an-
notated corpora for RE in the biomedical domain,
the approach of distant supervision, e. g. to auto-
matically label a training set is emerging. Many
approaches make use of the distant supervision as-
sumption (Mintz et al, 2009; Riedel et al, 2010):
1Associated with the conference of the spanish society
for natural language processing (SEPLN) in 2011, http:
//labda.inf.uc3m.es/DDIExtraction2011/
35
If two entities participate in a relation,
all sentences that mention these two en-
tities express that relation.
Obviously, this assumption does not hold in gen-
eral, and therefore exceptions need to be detected
which are not used for training a model. Thomas et
al. (2011b) successfully used simple filtering tech-
niques in a distantly supervised setting to extract
PPI. In contrast to their work, we introduce a more
generic filter to detect frequent exceptions from
the distant supervision assumption and make use
of more data sources, by merging the interaction
information from IntAct and KUPS databases (dis-
cussed in Section 2.1). In addition, we present the
first system (to our knowledge), evaluating distant
supervision for drug-drug interaction with promis-
ing results.
1.1 Related work
Distant supervision approaches have received con-
siderable attention in the past few years. However,
most of the work is focusing on domains other
than biomedical texts.
Mintz et al (2009) use distant supervision to
learn to extract relations that are represented in
Freebase (Bollacker et al, 2008). Yao et al (2010)
use Freebase as a source of supervision, dealing
with entity identification and relation extraction
in a joint fashion. Entity types are restricted to
those compatible with selected relations. Riedel et
al. (2010) argue that distant supervision leads to
noisy training data that hurts precision and suggest
a two step approach to reduce this problem. They
identify the sentences which express the known re-
lations (?expressed-at-least-once? assumption) and
thus frame the problem of distant supervision as
an instance of constraint-driven semi-supervision,
achieving 31 % of error reduction.
Vlachos et al (2009) tackle the problem of
biomedical event extraction. The scope of their
interest is to identify different event types without
using a knowledge base as a source of supervision,
but explore the possibility of inferring relations
from the text based on the trigger words and de-
pendency parsing, without previously annotated
data.
Thomas et al (2011b) develop a distantly la-
beled corpus for protein-protein interaction extrac-
tion. Different strategies are evaluated to select
valuable training instances. Competitive results
are obtained, compared to purely supervised meth-
ods.
Very recent work examines the usability of
knowledge from PharmGKB (Gong et al, 2008)
to generate training sets that capture gene-drug,
gene-disease and drug-disease relations (Buyko et
al., 2012). They evaluate the RE for the three inter-
action classes in intrisic and extrinsic experimental
settings, reaching F1 measure of around 80 % and
up to 77.5 % respectively.
2 Resources
2.1 Interaction Databases
The IntAct database (Kerrien et al, 2012) con-
tains protein-protein interaction information. It is
freely available, manually curated and frequently
updated. It consists of 290,947 binary interaction
evidences, including 39,235 unique pairs of inter-
acting proteins for human species.2
In general, PPI databases are underanno-
tated and the overlap between them is marginal
(De Las Rivas and Fontanillo, 2010). Combining
several databases allows to cover a larger fraction
of known interactions resulting in a more complete
knowledge base. KUPS (Chen et al, 2010) is a
database that combines entries from three manu-
ally curated PPI databases (IntAct, MINT (Chatr-
aryamontri et al, 2007) and HPRD50 (Prasad et al,
2009)) and contains 185,446 positive pairs from
various model organisms, out of which 69,600
belong to human species.3 Enriching IntAct inter-
action information with the KUPS database leads
to 57,589 unique pairs.4
The database DrugBank (Knox et al, 2011)
combines detailed drug data with comprehensive
drug target information. It consists of 6,707 drug
entries. Apart from information about its targets,
for certain drugs known interactions with other
drugs are given. Altogether, we obtain 11,335
unique DDI pairs.
2.2 Corpora
For evaluation of protein-protein interaction, the
five corpora made available by Pyysalo et al
(2008) are used. Their properties, like size and ra-
tio of positive and negative examples, differ greatly,
2As of January 27th, 2012.
3As of August 16th, 2010.
4Only 45,684 out of 69,600 human PPI pairs are available
from the KUPS web service due to computational and storage
limitations (personal communication).
36
Corpus Positive pairs Negative pairs Total
AIMed 1000 (0.17) 4,834 (0.82) 5,834
BioInfer 2,534 (0.26) 7,132 (0.73) 9,666
HPRD50 163 (0.38) 270 (0.62) 433
IEPA 335 (0.41) 482 (0.59) 817
LLL 164 (0.49) 166 (0.50) 330
DDI train 2,400 (0.10) 21,411 (0.90) 23,811
DDI test 755 (0.11) 6,275 (0.89) 7,030
Table 1: Basic statistics of the five PPI and two DDI
corpora. Ratios are given in brackets.
the latter being the main cause of performance dif-
ferences when evaluating on these corpora. More-
over, annotation guidelines and contexts differ:
AIMed (Bunescu et al, 2005) and HPRD50 (Fun-
del et al, 2007) are human-focused, LLL (Nedel-
lec, 2005) on Bacillus subtilis, BioInfer (Pyysalo
et al, 2007) contains information from various or-
ganisms and IEPA (Ding et al, 2002) is made of
sentences that describe 10 selected chemicals, the
majority of which are proteins, and their interac-
tions.
For the purposes of DDI extraction, the corpus
published by Segura-Bedmar et al (2011b) is used.
This corpus is generated from web-documents de-
scribing drug effects. It is divided into a training
and testing set. An overview of the corpora is
given in Table 1.
3 Methods
In this section, the relation extraction system used
for classification of interacting pairs is presented.
Furthermore, the process of generating an automat-
ically labeled corpus is explained in more detail,
along with specific characteristics of the PPI and
DDI task.
3.1 Interaction Classification
We formulate the task of relation extraction as
feature-based classification of co-occurring enti-
ties in a sentence. Those are assigned to be either
related or not, without identifying the type of re-
lation. Our RE system is based on rich feature
vectors and the linear support vector machine clas-
sifier LibLINEAR, which has shown high perfor-
mance (in runtime as well as model accuracy) on
large and sparse data sets (Fan et al, 2008).
The approach is based on lexical features, op-
tionally with dependency parsing features created
using the Stanford parser (Marneffe et al, 2006).
Lexical features are bag-of-words (BOW) and n-
Methods P R F1
Thomas et al (2011a) 60.54 71.92 65.74
Chowdhury et al (2011) 58.59 70.46 63.98
Chowdhury and Lavelli (2011) 58.39 70.07 63.70
Bjo?rne et al (2011) 58.04 68.87 62.99
Minard et al (2011) 55.18 64.90 59.65
Our system (lex) 63.30 52.32 57.28
Our system (lex+dep) 66.46 56.69 61.19
Table 2: Comparison of fully supervised relations ex-
traction systems for DDI. (lex denotes the use of lexi-
cal features, lex+dep the additional use of dependency
parsing-based features.)
grams based, with n ? {1, 2, 3, 4}. They encom-
pass the local (window size 3) and global (window
size 13) context left and right of the entity pair,
along with the area between the entities (Li et al,
2010). Additionally, dictionary based domain spe-
cific trigger words are taken into account.
The respective dependency parse tree is in-
cluded through following the shortest dependency
path hypothesis (Bunescu and Mooney, 2005), by
using the syntactical and dependency information
of edges (e) and vertices (v). So-called v-walks
and e-walks of length 3 are created as well as n
grams along the shortest path (Miwa et al, 2010).
3.2 Automatically Labeling a Corpus in
General
One of the most important source of publications
in the biomedical domain is MEDLINE5, currently
containing more than 21 million citations.6 The
initial step is annotation of named entities ? in
our case performed by ProMiner (Hanisch et al,
2005), a tool proving state-of-the-art results in e. g.
the BioCreative competition (Fluck et al, 2007).
Based on the named entity recognition, only sen-
tences containing co-occurrences are further pro-
cessed. Based on the distant supervision assump-
tion, each pair of entities is labeled as related if
mentioned so in a structured interaction databases.
Note that this requires the step of entity normaliza-
tion.
3.3 Filtering Noise
A sentence may contain two entities of an inter-
acting pair (as known from a database), but does
not describe their interaction. Likewise, a sentence
5http://www.ncbi.nlm.nih.gov/pubmed/
6As of January, 2012.
37
may talk about a novel interaction which has not
been stored in the database. Therefore, filtering
strategies need to be employed to help in decid-
ing which pairs are annotated as being related and
which not.
Thomas et al (2011b) propose the use of trigger
words, i. e., an entity pair of a certain sentence is
marked as positive (related) if the database has in-
formation about their interaction and the sentence
contains at least one trigger word. Similarly, a
negative (non-related) example is a pair of entities
that does not interact according to the database
and their sentence does not contain any trigger
word. Pairs which do not fulfil both constraints are
discarded.
Towards improvement of the heuristics for re-
ducing noise, we introduce the constraint of ?auto-
interaction filtering? (AIF): If entities from an en-
tity pair both refer to the same real-world object,
the pair is labeled as not interacting. Even though
self-interactions are known for proteins and drugs,
such pairs can rarely be observed to describe an
interaction but rather are repeated occurences or
abbreviations. Moreover, the fundamental advan-
tage of AIF is that it requires no additional manual
effort.
3.4 Application on Protein-Protein
Interaction and Drug-Drug Interaction
In biomedical texts there are often mentions of
multiple proteins in the same sentence. However,
this co-occurrence does not necessarily signal that
the sentence is talking about their relation. Hence,
to reduce noise, a list of trigger words specific to
the problem is required. The rationale behind this
filter is that the interaction between two entities is
usually expressed by a specific (trigger) word. For
protein-protein-interactions, we use the trigger list
compiled by Thomas et al (2011b)7. In addition to
using IntAct alone, we introduce the use of KUPS
database (as described in Section 2.2).
For drug-drug-interaction, to our knowledge,
no DDI-specific trigger word list developed by
domain experts is available. Therefore, filtering
via such term occurrences is not applied in this
case.
7http://www2.informatik.hu-berlin.de/
?thomas/pub/2011/iwords.txt
4 Results
In this section, we start with an overview of state-
of-the-art results for fully supervised relation ex-
traction on PPI and DDI corpora (see Table 1).
Furthermore, experimental settings for distant su-
pervision are explained. Finally, we present spe-
cific results for models trained on distantly labeled
data, when evaluated on manually annotated PPI
and DDI corpora.
4.1 Performance overview of supervised RE
systems
Protein-protein interactions has been extensively
investigated in the past decade because of their bio-
logical significance. Machine learning approaches
have shown the best performance in this domain
(e. g. BioNLP (Cohen et al, 2011) and DDIExtrac-
tion Shared Task (Segura-Bedmar et al, 2011a)).
Table 3 gives a comparison of RE systems? per-
formances on 5 PPI corpora, determined by doc-
ument level 10-fold cross-validation.8 The use of
dependency parsing-based features increases the
F1 measure by almost 4 pp.
Table 2 shows results of the five best perform-
ing systems on the held out test data set of the
DDI extraction workshop (Segura-Bedmar et al,
2011b). In addition, the result of our system is
shown. Note that the first three systems use ensem-
ble based methods combining the output of several
different systems.
The results presented in Table 2 and 3 give a
performance overview of the RE system used in
distant learning strategies.
4.2 Experimental Setting
To avoid information leakage and biased classifi-
cation, all documents which are contained in the
test corpus are removed. For each experiment we
sample random subsets to reduce processing time.
This allows us to evaluate the impact of different
combinations of subset size and the ratio of related
and non-related (pos/neg) entity pairs, having in
mind the problem of imbalanced datasets (Chawla
et al, 2004). All experiments are performed five
times to reduce the influence of sampling differ-
ent subsets. This leads to more reliable precision,
recall, and F1 values.
8Separating into training and validation sets is performed
on document level, not on instance (entity pair) level. The
latter could lead to an unrealisticallly optimistic estimate
(Van Landeghem et al, 2008)
38
AIMed BioInfer HPRD50 IEPA LLL
P R F1 P R F1 P R F1 P R F1 P R F1
(Airola et al, 2008) 52.9 61.8 56.4 56.7 67.2 61.3 64.3 65.8 63.4 69.6 82.7 75.1 72.5 87.2 76.8
(Kim et al, 2010) 61.4 53.2 56.6 61.8 54.2 57.6 66.7 69.2 67.8 73.7 71.8 72.9 76.9 91.1 82.4
(Fayruzov et al, 2009) 39.0 34.0 56.0 72.0 76.0
(Liu et al, 2010) 54.7 59.8 64.9 62.1 78.1
(Miwa et al, 2009) 55.0 68.8 60.8 65.7 71.1 68.1 68.5 76.1 70.9 67.5 78.6 71.7 77.6 86.0 80.1
(Tikk et al, 2010) 47.5 65.5 54.5 55.1 66.5 60.0 64.4 67 64.2 71.2 69.3 69.3 74.5 85.3 74.5
Our s. (lex) 62.3 46.3 53.1 59.1 54.3 56.6 69.7 69.4 69.6 67.5 73.2 70.2 66.9 84.6 74.7
Our s. (lex+dep) 65.1 48.6 55.7 64.7 57.6 61.0 69.3 69.8 69.5 67.0 72.5 69.7 71.2 86.3 78.0
Table 3: Comparison of fully supervised relations extraction systems for PPI.
Strategy Pairs Positive pairs Sentences
1 3,304,033 511,665 (0.155) 842,339
2 5,560,975 1,389,036 (0.250) 1,172,920
3 2,764,626 359,437 (0.130) 780,658
4 3,454,805 650,455 (0.188) 896,344
Table 4: Statistics of the fours strategies used in distant
supervision for PPI task: 1) IntAct, 2) IntAct + KUPS,
3) IntAct + AIF, 4) IntAct + KUPS + AIF. Ratios are
given in brackets.
4.3 Protein-protein interaction
We explore four strategies to determine the impact
of using additional database knowledge (IntAct
and KUPS) and to test the utility of our novel
condition (AIF).
Table 4 shows the difference in retrieved num-
ber of sentences and protein pairs, including the
percentage of positive examples in the whole data
set. As expected, by using more background know-
ledge, the number of sentences and instances re-
trieved from MEDLINE rises. An increase of both
negative and positive pairs is observed, since a
relevant sentence can have negative pairs along
with the positive ones. After applying additional
interaction knowledge, the fraction of positive ex-
amples (see 3rd column in Table 4) increases from
15.5 % (IntAct) to 25 % (IntAct+KUPS). However,
employment of the AIF condition to both IntAct
and IntAct+KUPS strategies leads to a reduction
of these values (e. g. fraction of positive examples
reduces from 15.5 % to 13 % and from 25 % to
18.8 %).
For simplicity reasons all runs are performed
using only lexical features.
Table 5 shows the average values of distant super-
vision experiments carried out for the PPI task. A
significant correlation between pos/neg ratio and
precision/recall holds. This clearly indicates the
tendency of classifiers to assign more test instances
to the class more often observed during training.
In accordance with their class distribution, AIMed
reaches highest performance in case of lower frac-
tion of positive instances (i. e. 30 % or 40 %), while
for IEPA and LLL the optimal ratio is in favor of
the positive class (i. e. 70 % or 80 %).
Comparative results of the distant learning
strategies IntAct and IntAct+KUPS tested on five
PPI corpora indicate that additional knowledge
bases do not help per se. Supplementary employ-
ment of the KUPS database leads to a drop in
performances seen in four out of five test cases (a
decrease of 1.7 pp in F1 measure is most notably
observed in case of HPRD50). However, introduc-
tion of the novel filtering condition, in both strate-
gies IntAct+AIF and IntAct+KUPS+AIF, shows
a favorable effect on the precision and leads to an
increase of up to 6 pp in F1 measure, compared to
IntAct and IntAct+KUPS.
Applying AIF to the baseline IntAct increases
F1 measure of AIMed and HPRD50 from 34.4 %
to 37.8 % and from 56.1 % to 59.1 %, respectively.
An even larger impact is observed when compar-
ing IntAct+KUPS and IntAct+KUPS+AIF. For
AIMed, HPRD50 and IEPA an increase of around
6 pp is achieved, while F1 measure of BioInfer
and LLL is improved around 3 pp. Table 5 clearly
shows that IntAct+KUPS+AIF is outperforming
other strategies in all five test cases by achiev-
ing F1 measures of 39.0 % for AIMed, 52.0 % for
BioInfer, 60.2 % for HPRD50, 63.4 % for IEPA
and 69.3 % for LLL.
Analysis of the database (IntAct+KUPS) pairs
reveals that in total there are 5,550 (around 10 %)
proteins that interact with themselves, with 4,918
(89 %) originating from the KUPS database. This
indicates a number of instances that represent auto-
interacting proteins which contribute to increase of
false positives. Such proportion where a majority
of them come from KUPS explains the decrease
39
AIMed BioInfer HPRD50 IEPA LLL
Strategy pos/neg P R F1 P R F1 P R F1 P R F1 P R F1
IntAct
30-70 22.3 75.8 34.4 41.7 54.1 46.9 42.6 73.8 53.9 44.6 70.3 54.5 58.9 63.5 61.0
40-60 21.5 83.5 34.2 40.0 61.9 48.5 42.0 81.7 55.5 44.4 78.0 56.6 55.7 73.3 63.2
50-50 20.8 87.0 33.5 38.7 67.1 49.0 41.4 86.9 56.1 43.7 82.2 57.1 54.6 80.7 65.1
60-40 20.0 90.8 32.8 37.3 72.6 49.2 40.5 91.2 56.1 43.2 85.6 57.4 52.4 86.7 65.3
70-30 19.0 94.5 32.1 35.4 79.5 48.9 39.6 93.4 55.6 42.6 89.3 57.7 50.7 92.1 65.4
80-20 18.6 96.8 31.2 33.5 86.5 48.3 38.6 96.2 55.1 42.1 93.3 58.1 49.4 96.7 65.0
IntAct
+
KUPS
30-70 20.6 48.9 29.0 37.5 30.0 33.3 38.6 45.8 41.8 33.1 25.3 28.6 55.3 25.4 34.6
40-60 21.6 70.3 33.0 39.3 47.4 42.9 40.7 70.2 51.5 41.0 49.6 44.9 58.6 49.3 53.2
50-50 20.8 81.6 33.2 38.2 59.4 46.5 39.6 80.4 53.0 42.9 65.3 51.8 58.5 61.1 59.5
60-40 20.0 89.0 32.7 37.0 68.8 48.2 38.9 87.4 53.8 43.4 76.8 55.4 55.2 74.4 63.2
70-30 19.2 94.3 31.9 35.2 79.1 48.7 38.6 92.3 54.4 42.9 86.2 57.2 52.8 88.5 66.1
80-20 18.3 97.5 30.9 32.2 88.6 47.3 37.8 96.1 54.2 41.9 92.7 57.8 50.8 97.0 66.6
IntAct
+
AIF
30-70 25.1 76.7 37.8 42.8 54.1 47.7 45.7 75.7 57.0 49.9 77.2 60.6 58.4 69.5 63.4
40-60 24.5 78.9 37.4 42.3 56.5 48.3 46.1 79.2 58.3 49.2 79.0 60.7 58.2 72.8 64.6
50-50 23.9 81.1 36.9 42.3 59.2 49.2 45.9 83.1 59.1 49 81.6 61.2 57.8 75.5 65.3
60-40 23.1 83.8 36.1 41.8 63.3 50.3 44.9 85.3 58.8 48.4 84.7 61.6 56.8 79.2 66.1
70-30 22.1 85.8 35.2 40.8 66.4 50.5 43.9 86.5 58.2 47.6 87.9 61.8 56.3 82.1 66.7
80-20 21.3 88.3 34.3 39.6 69.9 50.5 42.9 89.8 58.1 46.0 91.6 61.3 54.0 84.9 66.0
IntAct
+
KUPS
+
AIF
30-70 26.6 72.1 38.8 43.8 50.8 47.0 48.1 78.6 59.7 51.1 75.3 60.9 60.2 63.7 61.8
40-60 26.0 77.8 39.0 43.2 55.4 48.5 47.6 82.5 60.4 50.7 80.6 62.2 58.8 68.7 63.3
50-50 25.5 81.6 38.8 44.8 56.2 49.8 46.0 83.9 59.4 51.4 78.7 62.2 60.3 72.2 65.6
60-40 24.6 84.1 38.0 44.5 60.0 51.1 45.6 88.6 60.2 50.6 83.8 63.1 59.4 77.8 67.3
70-30 23.6 86.7 37.1 43.3 64.4 51.8 44.3 90.5 59.5 49.3 88.8 63.4 59.4 83.3 69.3
80-20 22.1 90.4 35.5 41.0 71.3 52.0 42.5 93.4 58.4 46.8 91.8 62.0 56.2 88.2 68.6
Thomas et al (2011b) 22.3 81.3 35.0 38.7 76.0 51.2 45.6 92.9 61.2 42.6 88.3 57.3 53.7 93.3 68.1
Tikk et al (2010) 28.3 86.6 42.6 62.8 36.5 46.2 56.9 68.7 62.2 71.0 52.5 60.4 79.0 57.3 66.4
Our system 34.3 74.0 46.9 70.8 22.5 34.2 63.3 61.3 62.3 70.0 46.0 55.5 82.4 45.7 58.8
Co-occurrence 17.1 100 29.3 26.2 100 41.5 37.6 100 54.7 41.0 100 58.2 49.7 100 66.4
Table 5: Results achieved with lexical features, trained on 10,000 distantly labeled instances and tested on 5 PPI
corpora.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
AIMed BioInfer HPRD50 IEPA LLL DDI
F 1
Co-occurrence
IntAct/DrugBank
IntAct+KUPS
IntAct+AIF
IntAct+KUPS+AIF
Figure 1: Comparison of four distant learning strategies with co-occurrence baseline. ?IntAct/DrugBank? denotes
the database used as source of supervision for PPI corpora and DDI corpus, respectively.
40
of performance in strategy IntAct+KUPS and the
recovery after applying the AIF condition.
The strategy IntAct+KUPS+AIF results in a
higher quality of data used for training and
achieves the best performance in all five test cases
thus proving the effectiveness of the novel condi-
tion. More knowledge is beneficial, but only when
appropriate filtering of the data is applied.
Distantly supervised systems outperform
co-occurrence results for all five PPI corpora.
Considering the best performing strategy
(IntAct+KUPS+AIF), F1 measure of AIMed and
BioInfer, for which we assume to have the most
realistic pos/neg ratio, increased around 10 pp.
HPRD50, IEPA and LLL have an improvement of
5.5 pp, 5.2 pp and 2.9 pp respectively, due to high
fractions of positive instances (leading to a strong
co-occurrence baseline).
Cross-learning9 evaluation may be more realis-
tic to be compared to distant-learning than cross
validation (Airola et al, 2008). For AIMed and
HPRD50 our approach performs on a par with Tikk
et al (2010) or better (up to 6 pp for BioInfer).
4.4 Drug-drug interaction
The problem of drug-drug interactions has not
been previously explored in terms of distant super-
vision. It is noteworthy that DDI corpora are gener-
ated from web documents discussing drug effects
which are in general not contained in MEDLINE.
Hence, this evaluation corpus can be considered as
out-domain and provides additional insights on the
robustness of distant-supervision. The AIF setting
is not evaluated for the DDI task, because only 1
of all 11,335 unique pairs describes a self interac-
tion. In MEDLINE, only 7 sentences with multiple
mentions of this drug (Sulfathiazole, DrugBank
identifier DB06147) are found.
Table 6 gives an overview of the results for dis-
tant supervision on DDI, with the parameter of
size of the training corpus and the pos/neg ratio. A
slight increase in F1 measure can be observed with
additional training instances, both in case of using
just lexical features and when dependency based
features are additionally utilized (e. g. (lex+dep)
from 36.2 % (5k) to 37.3 % (25k) in F1 measure).
Accounting for dependency parsing features
leads to an increase of 0.5 pp in F1 measure, i. e.
from 36.5 % to 37.0 % (10k) and 36.7%? to 37.3 %
9For five PPI corpora: train on four, test on the remaining.
size pos/neg P R F1
5k
30-70 35.4 32.4 33.7
40-60 33.3 37.0 34.9
50-50 31.9 41.7 36.0
50-50 (lex+dep) 32.7 40.7 36.2
60-40 30.1 46.6 36.5
70-30 27.4 51.8 35.7
10k
30-70 36.0 34.4 34.9
40-60 34.2 38.9 36.3
50-50 32.9 41.0 36.5
50-50 (lex+dep) 33.8 41.1 37.0
60-40 30.8 44.8 36.4
70-30 28.2 48.7 35.6
25k
30-70 35.8 35.0 35.3
40-60 34.3 38.6 36.2
50-50 33.2 41.1 36.7
50-50 (lex+dep) 32.5 43.7 37.3
60-40 31.7 42.6 36.3
70-30 28.9 47.2 35.7
Co-occurrence 10.7 100 19.4
Table 6: Results for distant supervision with only lexi-
cal features on the DDI test corpus.
(25k)), the latter being our best result obtained for
weakly supervised DDI.
Compared to co-occurence, a gain of around
18 pp is achieved. Taking into account the high
class imbalance of the DDI test set (see Table 1),
which is most similar to AIMed corpus, the F1
measure of 37.3 % is encouraging.
Figure 1 shows the results of PPI and DDI experi-
ments in addition. The error bars denote the stan-
dard deviation over 5 differently sampled training
corpora.
5 Discussion
This paper presents the application of distant su-
pervision on the task to find protein-protein inter-
actions and drug-drug interactions. The first is
addressed using the databases IntAct and KUPS,
the second using DrugBank.
More database knowledge does not necessar-
ily have a positive impact on a trained model, ap-
propriate instance selection methods need to be
applied. This is demonstrated with the KUPS
database and the automatic curation via auto-
interaction filtering leading to state-of-the-art re-
sults for weakly supervised protein-protein inter-
action detection.
We present the first results of applying the dis-
tant supervision paradigm to drug-drug-interaction.
41
The results may seem comparatively limited in
comparison to protein-protein interaction, but are
encouraging when taking into account the imbal-
ance of the test corpus and its differing source
domain.
Future development of noise reduction ap-
proaches is important to make use of the full poten-
tial of available database knowledge. The results
shown are encouraging that manual annotation of
corpora can be avoided in other application areas
as well. Another future direction is the investiga-
tion of specifically difficult structures, e. g. listings
and enumerations of entities in a sentence.
Acknowledgments
We would like to thank the reviewers for their
valuable feedback. Thanks to Sumit Madan and
Theo Mevissen for fruitful discussions. T. Bobic?
was partially funded by the Bonn-Aachen Inter-
national Center for Information Technology (B-
IT) Research School. P. Thomas was funded by
the German Federal Ministry of Education and
Research (grant No 0315417B). R. Klinger was
partially funded by the European Community?s
Seventh Framework Programme [FP7/2007-2011]
under grant agreement no. 248726. We acknowl-
edge financial support provided by the IMI-JU,
grant agreement no. 115191 (Open PHACTS).
References
A. Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala, F. Ginter,
and T. Salakoski. 2008. All-paths Graph Kernel for
Protein-protein Interaction Extraction with Evalua-
tion of Cross-corpus Learning. BMC Bioinformatics,
9(Suppl 11):S2.
J. Bjo?rne, A. Airola, T. Pahikkala, and T. Salakoski.
2011. Drug-drug interaction extraction with RLS
and SVM classiffers. In Challenge Task on Drug-
Drug Interaction Extraction, pages 35?42.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively created
graph database for structuring human knowledge. In
SIGMOD.
R. C. Bunescu and R. J. Mooney. 2005. A shortest
path dependency kernel for relation extraction. In
HLT and EMNLP.
R. C. Bunescu, R. Ge, R. J. Kate, E. M. Marcotte, R. J.
Mooney, A. K. Ramani, and Y. Wah Wong. 2005.
Comparative experiments on learning information
extractors for proteins and their interactions. Artif
Intell Med, 33(2):139?155, Feb.
E. Buyko, E. Beisswanger, and U. Hahn. 2012. The ex-
traction of pharmacogenetic and pharmacogenomic
relations?a case study using pharmgkb. PSB, pages
376?387.
A. Chatr-aryamontri, A. Ceol, L. M. Palazzi,
G. Nardelli, M.V. Schneider, L. Castagnoli, and
G. Cesareni. 2007. MINT: the Molecular INTer-
action database. Nucleic Acids Res, 35(Database
issue):D572?D574.
N. V. Chawla, N Japkowicz, and A. Kotcz. 2004. Ed-
itorial: special issue on learning from imbalanced
data sets. SIGKDD Explor. Newsl., 6:1?6.
X. Chen, J. C. Jeong, and P. Dermyer. 2010.
KUPS: constructing datasets of interacting and non-
interacting protein pairs with associated attributions.
Nucleic Acids Res, 39(Database issue):D750?D754.
F. M. Chowdhury and A. Lavelli. 2011. Drug-drug
interaction extraction using composite kernels. In
Challenge Task on Drug-Drug Interaction Extrac-
tion, pages 27?33.
F. M. Chowdhury, A. B. Abacha, A. Lavelli, and
P. Zweigenbaum. 2011. Two different machine
learning techniques for drug-drug interaction extrac-
tion. In Challenge Task on Drug-Drug Interaction
Extraction, pages 19?26.
K. B. Cohen, D. Demner-Fushman, S. Ananiadou,
J. Pestian, J. Tsujii, and B. Webber, editors. 2011.
Proceedings of the BioNLP.
J. De Las Rivas and C. Fontanillo. 2010. Protein-
protein interactions essentials: key concepts to build-
ing and analyzing interactome networks. PLoS Com-
put Biol, 6:e1000807+.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining MEDLINE: abstracts, sentences, or
phrases? Pac Symp Biocomput, pages 326?337.
E. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008. LIBLINEAR: A Library for Large Linear
Classification. Machine Learning Research, 9:1871?
1874.
T. Fayruzov, M. De Cock, C. Cornelis, and V. Hoste.
2009. Linguistic feature analysis for protein interac-
tion extraction. BMC Bioinformatics, 10(1):374.
J. Fluck, H. T. Mevissen, H. Dach, M. Oster, and
M. Hofmann-Apitius. 2007. ProMiner: Recognition
of Human Gene and Protein Names using regularly
updated Dictionaries. In BioCreative 2, pages 149?
151.
K. Fundel, R. Kuffner, and R. Zimmer. 2007. Relex-
relation extraction using dependency parse trees.
Bioinformatics, 23(3):365?371.
L. Gong, R. P. Owen, W. Gor, R. B. Altman, and T. E.
Klein. 2008. PharmGKB: an integrated resource of
pharmacogenomic data and knowledge. Curr Protoc
Bioinformatics, Chapter 14:Unit14.7.
D. Hanisch, K. Fundel, H. T. Mevissen, R. Zimmer,
and J. Fluck. 2005. ProMiner: rule-based protein
and gene entity recognition. BMC Bioinformatics,
6(Suppl 1):S14.
42
S. Kerrien, B. Aranda, L. Breuza, A. Bridge,
F. Broackes-Carter, C. Chen, M. Duesbury, M. Du-
mousseau, M. Feuermann, U. Hinz, C. Jandrasits,
R.C. Jimenez, J. Khadake, U. Mahadevan, P. Masson,
I. Pedruzzi, E. Pfeiffenberger, P. Porras, A. Raghu-
nath, B. Roechert, S. Orchard, and H. Hermjakob.
2012. The IntAct molecular interaction database in
2012. Nucleic Acids Res, 40:D841?D846.
S. Kim, J. Yoon, J. Yang, and S. Park. 2010. Walk-
weighted subsequence kernels for protein-protein
interaction extraction. BMC Bioinformatics, 11:107.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. Chi Guo, and D.S Wishart. 2011.
Drugbank 3.0: a comprehensive resource for ?omics?
research on drugs. Nucleic Acids Res, 39(Database
issue):D1035?D1041.
Y. Li, X. Hu, H. Lin, and Z. Yang. 2010. Learning
an enriched representation from unlabeled data for
protein-protein interaction extraction. BMC Bioin-
formatics, 11(Suppl 2):S7.
B. Liu, L. Qian, H. Wang, and G. Zhou. 2010.
Dependency-driven feature-based learning for ex-
tracting protein-protein interactions from biomedical
text. In COLING, pages 757?765.
M. C. De Marneffe, B. Maccartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
A. L. Minard, L. Makour, A. L. Ligozat, and B. Grau.
2011. Feature Selection for Drug-Drug Interac-
tion Detection Using Machine-Learning Based Ap-
proaches. In Challenge Task on Drug-Drug Interac-
tion Extraction, pages 43?50.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In ACL-IJCNLP, pages 1003?1011.
M. Miwa, R. Saetre, Y. Miyao, and J. Tsujii. 2009.
A Rich Feature Vector for Protein-Protein Interac-
tion Extraction from Multiple Corpora. EMNLP,
1(1):121?130.
M. Miwa, R. Saetre, J. D. Kim, and J. Tsujii. 2010.
Event extraction with complex event classification
using rich features. J Bioinform Comput Biol,
8(1):131?146.
C. Nedellec. 2005. Learning language in logic-genic
interaction extraction challenge. In Proc. of the
ICML05 workshop: Learning Language in Logic
(LLL?05), volume 18, pages 97?99.
T. S. Prasad, R. Goel, K. Kandasamy, S. Keerthiku-
mar, S. Kumar, S. Mathivanan, D. Telikicherla,
R. Raju, B. Shafreen, A. Venugopal, L. Balakrish-
nan, A. Marimuthu, S. Banerjee, D. S. Somanathan,
A. Sebastian, S. Rani, S. Ray, C. J. Kishore, S. Kanth,
M. Ahmed, M. K. Kashyap, R. Mohmood, Y. L.
Ramachandra, V. Krishna, B. A.Rahiman, S. Mo-
han, P. Ranganathan, S. Ramabadran, R. Chaerkady,
and A. Pandey. 2009. Human Protein Refer-
ence Database?2009 update. Nucleic Acids Res,
37(Database issue):D767?D772.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007. Bioinfer: A
corpus for information extraction in the biomedical
domain. BMC Bioinformatics, 8(50).
S. Pyysalo, A. Airola, J. Heimonen, J. Bjo?rne, F. Gin-
ter, and T. Salakoski. 2008. Comparative analysis
of five protein?protein interaction corpora. BMC
Bioinformatics, 9 Suppl 3:S6.
S. Riedel, L. Yao, and A. McCallum. 2010. Modeling
Relations and Their Mentions without Labeled Text.
In ECML PKDD.
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros, editors. 2011a. Proceedings of the 1st
Challenge Task on Drug-Drug Interaction Extrac-
tion.
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros. 2011b. The 1st DDIExtraction-2011 chal-
lenge task: Extraction of Drug-Drug Interactions
from biomedical texts. In Challenge Task on Drug-
Drug Interaction Extraction 2011, pages 1?9.
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011a. Relation Extraction for Drug-Drug Interac-
tions using Ensemble Learning. In Challenge Task
on Drug-Drug Interaction Extraction, pages 11?18.
P. Thomas, I. Solt, R. Klinger, and U. Leser. 2011b.
Learning Protein Protein Interaction Extraction us-
ing Distant Supervision. In Robust Unsupervised
and Semi-Supervised Methods in Natural Language
Processing, pages 34?41.
D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and
U. Leser. 2010. A comprehensive benchmark of ker-
nel methods to extract protein-protein interactions
from literature. PLoS Comput Biol, 6:e1000837.
S. Van Landeghem, Y. Saeys, B. De Baets, and
Y. Van de Peer. 2008. Extracting protein-protein
interactions from text using rich feature vectors and
feature selection. SMBM, pages 77?84.
A. Vlachos, P. Buttery, D. O? Se?aghdha, and T. Briscoe.
2009. Biomedical Event Extraction without Training
Data. In BioNLP, pages 37?40.
L. Yao, S. Riedel, and A. McCallum. 2010. Collec-
tive Cross-Document Relation Extraction Without
Labeled Data. In EMNLP.
43
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 80?88,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
BEL networks derived from qualitative translations of BioNLP Shared Task annotations 	 ?Juliane	 ?Fluck1*,	 ?Alexander	 ?Klenner1,	 ?Sumit	 ?Madan1,	 ?Sam	 ?Ansari2,	 ?Tamara	 ?Bobic1,3,	 ?Julia	 ?Hoeng2,	 ?Martin	 ?Hofmann-??Apitius1,3,	 ?Manuel	 ?C.	 ?Peitsch2	 ?1Fraunhofer	 ?Institute	 ?for	 ?Algorithms	 ?and	 ?Scientific	 ?Computing,	 ?Schloss	 ?Birlinghoven,	 ?Sankt	 ?Augustin,	 ?Germany.	 ?2Philip	 ?Morris	 ?International	 ?R&D,	 ?Philip	 ?Morris	 ?Products	 ?S.A.,	 ?Quai	 ?Jeanrenaud	 ?5,	 ?2000	 ?Neuch?tel,	 ?Switzerland.	 ?3Bonn-??Aachen	 ?International	 ?Centre	 ?for	 ?Information	 ?Technology,	 ?Dahlmannstr.	 ?2,	 ?Bonn,	 ?	 ?Germany	 ?	 ? 	 ?{jfluck, smadan, aklenner, tbobic, mhofmann-apitius}@scai.fraunhofer.de,  {sam.ansari, julia.hoeng, manuel.peitsch}@pmi.com     Abstract 
      Interpreting the rapidly increasing amount of experimental data requires the availability and representation of biological knowledge in a computable form. The Biological expres-sion language (BEL) encodes the data in form of causal relationships, which describe the as-sociation between biological events. BEL can successfully be applied to large data and sup-port causal reasoning and hypothesis genera-tion. With the rapid growth of biomedical litera-ture, automated methods are a crucial prereq-uisite for handling and encoding the available knowledge. The BioNLP shared tasks support the development of such tools and provide a linguistically motivated format for the anno-tation of relations. On the other hand, BEL statements and the corresponding evidence sentences might be a valuable resource for fu-ture BioNLP shared task training data genera-tion. In this paper, we briefly introduce BEL and investigate how far BioNLP-shared task an-notations could be converted to BEL state-ments and in such a way directly support  BEL statement generation. We present the first results of the automatic BEL statement generation and emphasize the need for more training data that captures the underlying bio-logical meaning.   1 Introduction Currently a lot of effort is made to extract infor-mation from scientific articles and encode the relevant parts in machine-readable language. In order to tackle these tasks, curators must be ex-
perts in both biological domain and computa-tional representation of knowledge.    With the introduction of BEL, a new knowledge coding convention was made availa-ble, thus simplifying the curation process and ensuring machine readability1. BEL was initially designed and used in 2003 by Selventa (operat-ing as Genstruct? Inc. at the time) to capture relationships between biological entities in scien-tific literature (Slater and Song 2012). It is flexible enough to store content from multiple knowledge layers and a broad range of analytical and deci-sion-supporting applications. Knowledge bases encoded in BEL are suitable for querying, inter-preting, reasoning and visualising of networks.    BEL represents scientific findings by capturing causal and correlative relationships in a given context, including information about the biologi-cal system and experimental conditions. The supporting evidences are captured and linked to the publication references. It is specifically de-signed to adopt external vocabularies and ontol-ogies, and therefore represents life-science knowledge in language and schema known by the community. Entities in BEL statements are mapped to widely accepted namespaces, which specify a set of domain entities (e.g., HGNC2, CHEBI 3 ). Continuous development and com-mercial use in more than 80 life science projects in the last ten years qualify BEL as suitable for displaying causal networks for both humans and computers. Various networks built in BEL were mainly focusing on disease mechanisms (Schlage                                                 1http://wiki.openbel.org/display/BLD/BEL+Language+Documentation+v1.0+-+Current           2 http://www.genenames.org/ 3 http://www.ebi.ac.uk/chebi/ 
80
et al, 2011) and are used for causal reasoning (Chindelevitch et al, 2012, Huang et al, 2012 and Selventa 2012). Since 2012, BEL is also available in the public domain through the OpenBEL consortium. The OpenBel portal4 de-fines the BEL language standard and provides formatted content and compatible tools for re-search.    The necessary information to develop a BEL knowledge base is currently harvested mainly by manual translation of literature into BEL state-ments. To support automated extraction of statements by text mining techniques, additional efforts and adaptations of existing text mining platforms are necessary.     The BioNLP community has developed various approaches, which may already support the au-tomated extraction of BEL statements. To esti-mate how far current tools can generate BEL re-lationships, we focused on the BioNLP shared tasks series5. The BioNLP-shared tasks specify fine-grained information extraction tasks for bio-logically relevant targets, mainly centred on pro-teins and genes. In the two previous events, Bio-NLP-ST 2009 and 2011, more than 30 teams par-ticipated with their systems, a number of which are available as open source. In BioNLP-ST 2013 series, additional training data for pathway curation including chemical entities is available.     The organizers develop a linguistically based event representation and provide annotated train-ing and test data to the participants. The annotat-ed events in training data can directly be used for comparison with BEL definitions and available BEL statements. If the conversion of said event annotations to BEL statements (and vice versa) is successful on the semantic level, we have a promising opportunity to support both domains. Information encoded in the BEL statements in combination with corresponding evidence sen-tences could be used as training data to support further tool development.   2 Related Network Representations For pathway representations there exist two widely adopted machine readable representa-tions: Systems Biology Markup Language (SBML)6 (Hucka et al, 2003) and Biological Pathway Exchange (BioPAX) (Demir et al, 2010). SBML is an XML-based data exchange                                                 4 http://www.openbel.org/ 5 http://2013.bionlp-st.org/ 6 http://sbml.org 
format that supports a formal mathematical rep-resentation of chemical reactions including kinet-ic parameters. BioPAX is an RDF/OWL-based standard language enabling integration, ex-change, visualization, and analysis of biological pathway data. Pathway representations in BioPax were already compared to the BioNLP-ST repre-sentations (Ohta et al, (1) 2011) and let to the introduction of the Pathway curation task in 20137. For this task additional entity types and event types were proposed and resulted in a set of new annotations (Ohta et al, (2) 2011). A comparison between BEL and BioPax can be found at the OpenBEL Portal8. BioPAX focuses on pathway construction and partly may require more information than available in most publica-tions. BEL?s design enables the representation of causal relationships across a wide range of mechanistic detail and between the levels of mo-lecular event, cellular process, and organism-scale phenotype. BEL is designed to represent discrete scientific findings and their relevant con-textual information as qualitative causal relation-ships that can drive knowledge-based analytics. BEL enables biological interference by applica-tions but furthermore is intended as an intuitive language of discourse for biologists. In such a way BEL is well aligned to the communications done in publications. The condensed representa-tion of BEL statements and human as well as machine readability are great advantages of the BEL language. 	 ?  3 Overview of basic concepts in BEL BEL defines semantic triples that are stored in structured human readable BEL document files. A semantic triple is defined as a subject ? predi-cate ? object triple, where subject is always a BEL term, object either a BEL term or a BEL statement (recursive nature of BEL) and the predicate one of the BEL relationship types. A BEL term is composed of a BEL function, a cor-responding entity and a referencing namespace. The two main classes of BEL terms define abun-dance of an entity (e.g., gene) or a biological process (e.g., disease).  Optionally, statements can be enriched by con-                                                7 https://sites.google.com/site/bionlpst2013/tasks/pathway-curation 8 Comparison of BEL V1.0 and BioPAX Level3.pdf   http://www.openbel.org/content/bel-lang-resource-documents 
81
text information annotations like the evidence sentences, tissue type, species or cell line. Two annotation types are reserved, i.e., ?Citation? and ?Evidence?. ?Evidence? should state the exact sentence that holds the statement?s information, where ?Citation? is the source of this knowledge.     Predefined namespaces cover a variety of bio-logical entities: genes, proteins, chemicals, dis-eases and biological processes. For a complete definition of BEL we refer to the BEL Language documentation.    BEL Expression Explanation p(HGNC:AKT1)  Term: Protein Abundance function p(Ns:entity) r(HGNC:AKT1) Term: RNA Abundance function r(Ns:entity) a(CHEBI:phosphoenolpyruvate) Term: Chemical Abundance function a(Ns:entity)  p(HGNC:AKT1, sub(V,243,P)) Term: Protein Abundance function with substitution modification p(Ns:entity, sub(Aai,Pos,Aaj)) p(HGNC:AKT1, pmod(P,S,21)) Term: Protein Abundance function with phosphoryla-tion modification p(Ns:entity,pmod(P,Aa,Pos)) kin (p(HGNC:AKT1)) Term: Protein Abundance function with kinase modifi-cation kin(p(Ns:entity)) complex (p(HGNC:CHUK), p(HGNC:IKBKB), p(HGNC:IKBKG)) 
Term: Complex Abundance function complex (p(Ns:entity)i,?, p(Ns:entity)n) tloc(p(HGNC:EGFR), MESHCL: ?Cell Mem-brane?, MESCL:Endosomes) 
Term: Translocation function for Protein Abundance speci-fying the original and target location Tloc(p(Ns:entity), Ns:entity, Ns:entity) deg(p(HGNC:AKT1)) Term: Degradation function for protein abundance  deg(p(nNs:protein)) Reaction: rxn(reactants(a(CHEBI: phosphoenolpyruvate), a(CHEBI:ADP)), products (a(CHEBI:pyruvate), a(CHEBI:ATP))) 
Statement: reaction express-ing the transformation of products into reactants, each defined by a list of abun-dances rxn(reactants(a(Ns:entity)...),  products(a(Ns:entity)...) p(HGNC:IL6) -> r(HGNC:ENO1) Statement: increase  Term ->Term or Term -> Statement p(HGNC:TNF) -| r(HGNC:NOS3) Statement: decrease  Term -|Term or Term -| Statement  p(HGNC:TNF) --r(HGNC:NOS3) Statement: association Term --Term or Term --Statement   Table 1: Example BEL terms and statements.  Abbreviations: Ns=namespace, Aa=amino acid, Pos=position 
 In this work we focus mainly on protein-protein relationships (for simplification ?protein? refers to the corresponding gene, the RNA intermediate and the gene product itself9). Protein-protein re-lationships are a main focus of the BioNLP shared tasks and cover core relationships of BEL. An overview of possible statements is given in Table 1 and shortly described below. Protein en-tities are represented by BEL terms, consisting of the abundance function, the normalized entity and optionally modifications expressed as addi-tional arguments within the abundance function:  BEL statement: p(HGNC:AKT1, pmod(P, S, 21)) Entity: AKT1 Namespace: HGNC Optional modification: pmod(P,S,21)      The used namespace denotes the approved symbol of HUGO Gene Nomenclature Commit-tee10. An overview of currently used namespaces is given at the OpenBEL portal. The pmod() function explicitly denotes the modification type (here P=phosphorylation), the 1-letter code for the corresponding amino acid (S=Serin) and the position in the protein sequence. Other modifica-tions are represented with different codes, e.g., M=methylation or U=ubiquitination.    BEL terms may contain protein activity infor-mation such as kinase or transcription factor ac-tivity or certain functions like complex, degrada-tion, translocation or reaction in addition.   
   Figure 1: Example of enriched BEL Statement     By default (but not mandatory) ?Evidence? and ?Citation? annotations are provided for each                                                 9 according to BioNLP shared tasks annotations 10http://www.genenames.org/data/hgnc_data.php?hgnc_id=391 
SET Citation = {"PubMed","Cell","16962653","2006-10-07","Jacinto E|Facchinetti V|Liu D|Soto N|Wei S|Jung SY|Huang Q|Qin J|Su B",""} SET Cell = "Fibroblasts? SET Species = "10090" SET Evidence = "We next examined the Akt T-loop Thr308 phosphorylation in wild-type and SIN1?/? cells. We found that although Ser473 phosphorylation was completely abolished in the SIN1?/? cells, Thr308 phosphorylation of Akt was not blocked (Figure 3A)."  p(MGI:Mapkap1) -> p(MGI:Akt1,pmod(P,S,473)) p(MGI:Mapkap1) causesNoChange p(MGI:Akt1,pmod(P,T,308))  
82
statement. In case of extraction from literature the reference source and the evidence sentences are given. Alternative evidences may be derived from tables, figures, supplementary material or other knowledge sources. Optionally, the BEL statements can be annotated with specified in-formation about experimental methods, the bio-logical system in which the facts are represented, or even information in which part of the full text the evidence has been found. An example of such a BEL statement from a small sample set at the OpenBEL portal11 is shown in Figure 1. Such detailed information from literature, in combina-tion with the BEL statements, could serve as ide-al source for the generation of training data for text mining purposes to facilitate the develop-ment of future automated extraction algorithms.  4 Analysis of basic concepts in the Bio-NLP shared task annotations In the main BioNLP shared task (GE12) nine event types are defined (cf Table 2). ?Gene ex-pression?, ?Transcription?, ?Protein catabolism?, ?Phosphorylation? and ?Localization? are simple events, having one protein as Theme argument.   Event Primary Arg. Second-ary Arg. Gene Expression Theme(Protein)  Transcription Theme(Protein)  Protein Catabolism Theme(Protein)  Phosphorylation Theme(Protein) Site  Localization Theme(Protein) AtLoc, ToLoc Binding Theme(Protein)+ Site+ Regulation, Positive Regulation, Nega-tive Regulation Theme(Protein/Event),Cause(Protein/Event) Cause, Site, CSite  Table 2: Event types defined in the BioNLP competitions (adapted from (Kim et al, 2012). A ?+? sign indicates multiple occurrences allowed.      Events ?Phosphorylation? and ?Localization? may have additional secondary arguments, like the phosphorylation site or the localization ar-guments ToLoc and AtLoc. ?Binding? events can have an arbitrary number of proteins as Themes. Events ?Positive regulation?, ?Negative regula-                                                11 https://github.com/OpenBEL/openbel-framework-resources/blob/master/knowledge/small_corpus.bel 12 https://sites.google.com/site/bionlpst/home/genia-event-extraction-genia 
tion? and ?Regulation? are Regulation Events and have a primary Theme argument and an optional Cause argument, both being either a protein or an event. The trigger is always the textual repre-sentation of the entities. Table 3 depicts an ex-ample annotation for the following sentences13:   S1) E1-4: ?RFLAT-1: a new zinc finger transcription factor that activates RANTES gene expression in T lymphocytes.?  S2) E5-9: ?In this study we hypothesized that the phosphor-ylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain.?   ID Theme Type Trigger Theme Cause T1 Protein RFLAT-1   T2 Protein RANTES   E3 Gene  Expression gene ex-pression T2   E4 Positive  Regulation activates E3 T1 T5 Protein TRAF-2   T6 Protein CD40   E7 Phosphorylation phosphor-ylation T5   E8 Binding binding T6 T5 E9 Negative  Regulation inhibits E8 E7  Table 3: Example BioNLP 09 shared task anno-tation. The gene/protein entities with the Ids T1, T2, T5, and T6 were already provided. The task was to detect the events E3, E4, E7, E8 and E9.  5 Syntactic mapping from BioNLP an-notation to BEL statements For mapping of the BEL statements and the out-put of the BioNLP shared tasks systems we com-pared the training data for the GENIA BioNLP task with the BEL statements found in the small corpus at the OpenBEL website. The BioNLP shared task provides no normalization of the en-tities to namespaces. Since we are mainly inter-ested in the transformation of the event, we ig-nore the normalization aspect in the conversion process. For most Shared Task events we could                                                 13 Examples taken from http://www.nactem.ac.uk/tsujii/ GENIA/SharedTask/detail.shtml 
83
generate BEL Terms which are summarized with the rule set in Table 4 and Table 5. Standard translation for all protein Themes is protein abundance p(namespace:entity). In a later network generation step within the BEL frame-work RNA abundance and gene abundance are added automatically to the network of statements for all protein abundances. Due to this reason, we only consider RNA or gene abundance if we de-tect strong evidences for those states. For Gene_expression, the protein abundance is only converted to RNA abundance (r(name-space:entity)) if the trigger word is ?gene expres-sion?.  1.1 GeneExpression(Theme(protein)) ?? p(Ns:protein)  If the GeneExpression trigger word is stemmed to ?express? 1.2 GeneExpression(Theme(protein)) ?? r(Ns:protein)  For all other GeneExpression trigger words. 2 Transcription(Theme(protein) ) ?? r(Ns:entity) 3 Phosphorylation(Theme(protein), <Site>) ?? p(Ns:protein, <pmod(P,Aa, Pos)>) 4 ProteinCatabolism(Theme(protein))?? deg(p(Ns:protein)) 5.1 Localization(Theme(protein)) ?? sec (p(Ns:protein))   If the Localization trigger is stemmed to ?secrete? 5.2 Localization(Theme(protein),AtLoc) ?? surf(p(Ns:protein))  If the Localization trigger is stemmed to ?express? and If AtLoc is ?cell surface? or ?surface? 5.3 Localization(Theme(protein),AtLoc, ToLoc) ?? tloc (p(Ns:protein),Ns:AtLoc,Ns:ToLoc)  In BEL statements it is necessary to have AtLoc and ToLoc; for some cases the missing information can be inferred otherwise artificial location information is given. 6 Binding(Theme(protein)+,Site+) ?? com-plex(p(ns:protein),+)  The site information will be ignored.  Table 4: Rule set 1 to map BioNLP annotations to BEL statements.   If the trigger word ?expression? is used, both RNA and protein expression might be meant by the authors, hence we keep the protein abun-dance in those cases. Similarly for Transcription, the abundance is changed to RNA abundance. All complexes are translated to protein abun-dance and chemical names are directly translated into abundance (a(ns:chemical names)). Protein modification events such as Phosphorylation can be directly converted to BEL terms. The different 
modification events are translated to a single let-ter code in BEL. If the position information is given in the site expression it can directly be converted to the amino acid single letter code (Aa) and the position information (Pos). For the simple events Protein degradation and Binding, the translation is straightforward given their similar representation. The site information of the Binding event is omitted in the BEL state-ment conversion. It would only be included if there is an experiment showing that a mutation of the site would lead to a suppression of the com-plex building.     In the case of ?Localization?, depending on the localisation trigger different BEL functions are possible. Given the localization trigger ?secrete? the BEL annotation is converted to the secretion (sec) function. If trigger words ?surface? or ?cell surface? are identified, the cellSurface (surf) function is assigned. For other Atloc and ToLoc triggers the function translocation (tloc) is used. This function always needs two arguments of location. If one of the arguments (AtLoc or ToLoc) is missing, a general annotation of MESHCL:?Intracellular Space? is proposed as unknown intracellular location. Activity status like gtp(p(protein)), kin (p(protein)), tscript(p(protein)),  cat(p(protein)), phos(p(protein)) are often found in the BEL ex-ample corpus. This information might be partly inferred through the evidence information. In the first example sentence from Table 2, RFLAT-1 might be directly translated into tscript (p(RFLAT-1)).  In other cases if a protein phos-phorylates another protein directly, the kin(p(protein)) annotation can be added as well. However, in most cases the information cannot directly be inferred from the sentences (cf. Fig-ure 1). The annotators obviously use their back-ground knowledge to include this information. In the actual status of the Shared Task to BEL con-version we omitted those functions.     Looking at the rule-set for transferring Shared-Task events to BEL statements, it is observed that for most events (six out of nine) only BEL terms are generated, i.e., only the left or right hand side of a complete statement. Three rules generate complete BEL statements out of the following events: Regulation, Positive Regula-tion and Negative Regulation. Analysis of the distribution of Events in Shared-Tasked training set (BioNLP ST 2011) reveals that approximate-ly half of the events are Regulation events and 
84
thus, could lead to a set of complete statements. In Table 5, we describe the rules which generate complete BEL statements.  7 PositiveRegulation(Theme(Protein/Event),  Cause(Protein/Event)) ??  p(ns:protein)/B(Event) -> p(ns:protein)/B(Event) 8 NegativeRegulation(Theme(Protein/Event), Cause(Protein/Event)) ??  p(ns:protein)/B(Event) -| p(ns:protein)/B(Event) 9 Regulation(Theme(Protein/Event), Cause(Protein/Event)) ??  p(ns:protein)/B(Event) -- p(ns:protein)/B(Event)  Table 5: Rule set 2 to map BioNLP annotations to BEL statements.     For all ?Regulation? events the Theme is trans-lated to the object of the BEL statement and might be a protein or another BEL statement       (B(Event)). The Cause is integrated as subject within the statement and can be a protein or a statement. All ?Positive Regulation? events in the Shared Task annotations are converted to ?in-crease? statements of BEL. We do not differenti-ate between ?increase? and ?directly increase? in the conversion process. Similarly, all ?Negative Regulation? events are converted to a ?decrease? statement ignoring ?directly decrease?. In the BEL annotations those two statement groups are the most frequent statements in both corpora. In the Shared Tasks relations we have the additional relation Regulation. There is no directly corre-sponding BEL relation for a general regulation event, since it restricts the impact for causal rea-soning. The event which has the most similar meaning is the statement ?association?. It is used for associations of proteins but also for associa-tions of proteins and diseases when no further information is available in the text. The addition-al annotations Site and CSite are currently ig-nored since there is no structure in BEL to in-clude this information directly. In all three regulation events the Cause is an optional argument and might be missing. Out of the 7574 regulation events 2152 events contain a cause and thus can be converted to a complete BEL statements. For all other events the left hand side of the statement is missing.  For obtaining an overview of the conversion process we converted the event annotations from the GENIA training corpus to BEL statements (all relations containing a speculation or a nega-tion were omitted). The automatically generated BEL documents were checked for syntactical errors with the OpenBEL framework parser and 
validator.  Several adaptations were necessary in the automatic conversion process to generate syntactically correct BEL statements. Since we have no namespaces available we designed an artificial namespace to generate cor-rect statements. Furthermore incomplete state-ments with missing subjects (Causes) were not accepted by the BEL framework. An example of such an incomplete BEL statement is the follow-ing (converted form the shared task annotation depicted in Figure 2):  -| p(BioNLP:STAT4) -| p(BioNLP:IL10)  For all missing Causes we included an artifi-cial Cause resulting in the following statement for the given example:  p(BioNLP:FIXME)-| p(BioNLP:STAT4) -| p(BioNLP:IL10)  
  Figure 2: An example sentence from BioNLP-ST 2011 GE train corpus, visualized using brat. 14  Overall 5333 BEL statements were generated resulting in 588 full statements, 3057 incomplete statements (where the CAUSE is missing and FIXME was introduced) and 1688 BEL terms without any relation. Remaining syntactic errors were caused through BEL statements containing more than two relations (118 statements), which could not be handled by the BEL framework. A first version of the converted corpus is available under: http://www.scai.fraunhofer.de/ge2011-to-bel.html.  6 Preliminary comparison of converted statements with BEL knowledge re-sources In the BioNLP shared tasks all possible events that fulfill the guidelines are annotated. In real life use-cases irrelevant or unproven interactions are omitted and biological experts extract BEL statements when they are in focus of their inter-est. Furthermore experimental evidence for the relation should be should be given in the text.                                                 14 http://brat.nlplab.org 
85
     In addition biologists are able to do a semantic interpretation of the experimental results and generate inferred statements. To find solutions for semantic interpretation for a number of in-complete statements in the direct conversion for the BioNLP-ST annotations we compared sen-tences such as annotated in figure 2 with evi-dence sentences in the BEL sample set. In the following examples we show how an expert cu-rator conversely infers BEL statements by inter-preting experiment readouts.   Example 1: Evidence = "PI 3- kinase/PKC?, but not PI 3-kinase/Akt signaling pathway, is inhibited in IRS-2-deficient brown adipocytes upon insulin stimulation"  p(HGNC:IRS2)-> kinase(p(HGNC:PRKCZ)) p(HGNC:IRS2) causesNoChange kin(p(HGNC:AKT1))  Example  2:  Evidence = "transient transfection of primary brown adipo-cytes with a dominant negative form of p21 Ras completely abolished insulin-induced UCP-1-CAT transactivation."  p(PFH:"RAS Family") -> (p(HGNC:INS) -> r(HGNC:UCP1))  Example  3:  Evidence = "We next examined the Akt T-loop Thr308 phosphorylation in wild-type and SIN1?/? cells. We found that Thr308 phosphorylation was completely abolished in the SIN1?/? cells.?  p(MGI:Mapkap1) -> p(MGI:Akt1,pmod(P,T,308))      The examples given above demonstrate a standard experimental setting. In most cases the functionality of a gene is abolished and the effect (e.g. increase, decrease or no effect) on the cor-responding interaction targets is observed. Some-times, observed effects are compared to cell sys-tems where the normal form (wild type or con-trol) is transfected as well (cf. Example 3).     All examples share the readout: The BEL statement is not describing the experiment (given in the sentence), but the observed implication inferred from the experiment (cf. Example 2). Instead of encoding that a dysfunctional p21 RAS leads to an abolishment of insulin induced UCP1 transactivation, the final BEL statement represents the resulting implication, i.e. wild-type p21 RAS increases INS, which subsequent-ly increases UCP1:   p(PFH:"RAS Family") -> (p(HGNC:INS) -> r(HGNC:UCP1)) 
 Similarly, in Example 3 from the abolishment of a function, the converse argument is derived, i.e. Mapkap 1 increases the phosphorylation of Akt1 at T308. This example shows another main issue in deriving BEL statements: two or more sen-tences are needed to get al information neces-sary to create a valid BEL statement. Human cu-rators use multiple sentences as evidence and do additional interpretation of the provided infor-mation. In Example 3, the AKT phosphorylation is given in the first sentence and the phosphory-lation event is given in the following sentence only in referring to the site and not to the protein.  BioNLP-ST already includes annotation span-ning several sentences but interpretation and merging of those annotations is not trivial. To complete such statements two different relations have to be combined and that is true for many modification relations. Especially in the case of phosphorylation, which is a regular activating signal in kinase pathways, we need solutions in-cluding information from different sentences. The BEL corpus has a high number of phosphor-ylation events and can serve as a base for the generation of further training data.     Another commonly observed experiment uses luciferase and CAT vectors. Those systems are used to analyze transcriptional activity of pro-moters in dependence of stimuli. The result of such an experiment is oftentimes given only as a relation to CAT or luciferase like in the follow-ing example:  Example  4:  Evidence = "introduction of miR-145, but not miR-143, with the luciferase vector in Cos cells resulted in relief of the repression and an ~150-fold increase in luciferase activi-ty compared to the CMV-luciferase- Myocd 3' UTR-luciferase vector alone.?  miR(HGNC:MIR145) -> p(HGNC:MYOCD) miR(HGNC:MIR143) causesNoChange p(HGNC:MYOCD)     BioNLP shared task annotation would capture positive regulation of luciferase activity with the cause miR-145. The derived statement however does not state an abundance function for lucifer-ase but the originally tested protein (indirectly via its promotor) i.e., Myocd.  Here, the inserted promoter information is given at the end of the sentence, although it is often provided in a sepa-rate sentence.     The second BEL statement in Example 4 pro-vides another relation type, which is not directly captured by the shared task annotations. Nega-
86
tive results are annotated in BEL statements with the relation causesNoChange and are valuable relations in causal reasoning. They might be in-terpreted using the negation annotation in shared task to capture this type of event.       Those examples are only a few out of numer-ous others. For the development of suitable sys-tems, annotated training corpora are crucial. The BEL documents might be a good starting point to generate further training corpora containing a high number of such evidence examples. How-ever, the conversion of the BEL statements to BioNLP shared task annotation is not trivial, since position information is completely missing. Nevertheless, it might reduce the annotation ef-fort, give good examples and serve as a basis for biological interpretation of the relations. For ini-tial automatic systems it might be even sufficient to offer such experimental evidence sentences in addition to the extracted relations to users.  7 Discussion and Conclusions  Generally, a syntactic conversion of BioNLP shared task annotations to BEL terms and state-ments is possible and in most cases without in-formation loss. Tools developed or adapted for the BioNLP shared task are principally suited for the generation of causal BEL networks. Howev-er, the analysis of the automatically converted BEL statements from the BioNLP shared tasks shows that in a number of cases incomplete BEL statements were generated. Part of the reason is the need for an additional interpretation layer that would help in generating biologically mean-ingful statements. Another reason for the failure to extract full statements is the distribution of the relation over more than one sentence.     The properties of BEL statements and the addi-tional information coded in the BEL documents represent a valuable resource for generating fur-ther training data for the development of more real-world oriented systems. Unfortunately, the information of the BEL documents cannot direct-ly be converted back to textual annotation. The main reason is that the position information of entities within the relation is missing. Reverse engineering is also challenging because the trig-ger words are not given. Furthermore, normaliza-tion to namespaces used in BEL statements makes the direct mapping difficult.     Nevertheless, the text mining community can learn from the BEL documents what are relevant 
statements for causal reasoning and from which evidence sentences humans extract the infor-mation. The example BEL statements given show that humans use a number of experimental systems such as inactive versions of proteins or reporter genes to prove existing relationships. It might be a realistic task to use BEL documents as a starting point to generate training corpora for the automatic classification of such sentences and for information extraction systems to extract relations from those sentences. For some rela-tions like the phosphorylation or the reporter genes, we might be even able to extract relations over sentences when enough training data is available.     Another problem not tackled by the BioNLP shared tasks is the mapping to the name spaces. There are already systems available combining BioNLP based relation extraction systems and named entity recognition (NER) systems allow-ing for normalization and (eg. Bj?rne et al, 2012 and Van Landeghem et al, 2013).  Future sys-tems have to combine relation extraction and NER systems allowing for normalization. Gene and protein names have already been in the focus of the BioCreative assessments during the last years (cf. Morgan et al, 2008 and Lu et al,  2011). In addition, chemical entities are coming more and more into the focus of the community (e.g., in the BioCreative 2013 task15). In the ex-amples from the BEL corpus we see additional problems coming from the area of engineered genes. Name variants are often used (e.g., Sin-/- or CMV-luciferase- Myocd 3' UTR-luciferase), which causes further problems in the normaliza-tion task.    Bridging the BEL and the BioNLP-ST com-munity offers benefits for both sides. The Bio-NLP shared tasks are a considerable start for the automatic generation of causal networks. Moreo-ver, already available BEL documents can sup-port the generation of the huge amount of addi-tional training data, which is necessary for fur-ther relation extraction development.         
                                                15 http://www.biocreative.org/events/biocreative-iv/CFP/ 
87
Acknowledgments We would like to thank Natalie Catlett and Ted Slater of Selventa for providing their time and expertise in helping us understand BEL. We acknowledge support of our research from Philip Morris International.  References  Jari Bj?rne, Sofie Van Landeghem, Sampo Pyysalo, Tomoko Ohta, Filip Ginter, Yves Van de Peer, Sophia Ananiadou and Tapio Salakoski. 2012. PubMed-Scale Event Extraction for Post-Translational Modifications, Epigenetics and Pro-tein Structural Relations. Proceedings of BioNLP 2012, 82-90  Leonid Chindelevitch, Daniel Ziemek, Ahmed Enaye-tallah, Ranjit Randhawa, Ben Sidders, Christoph Brockel and Enoch Huang. 2012. Causal reason-ing on biological networks: interpreting transcrip-tional changes. Bioinformatics. 28(8):1114-21. Emek Demir et al 2010. The BioPAX community standard for pathway data sharing. Nature bio-technology , 28(9):935?942. Chia-Ling Huang, John Lamb, Leonid Chindelevitch, Jarek Kostrowicki, Justin Guinney, Charles DeLisi and Daniel Ziemek. 2012. Correlation set analysis: detecting active regulators in disease populations using prior causal knowledge. BMC Bioinformat-ics. 2012 13:46.  Michael Hucka et al 2003. The systems biology markup language (SBML): a medium for re-presentation and exchange of biochemical network models. Bioinformatics , 19(4):524?531. Jim-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsujii, Toshihisa Takagi and Akinori Yonezawa. 2012. The Genia Event and  Protein Coreference tasks of the BioNLP Shared Task 2011. BMC Bio-informatics. 13 Suppl 11:S1. Zhiyong Lu et al 2011. The gene normalization task in BioCreative III. BMC Bioinformatics , 12(Suppl 8):S2. Alexander A Morgan et al 2008. Overview of Bio-Creative II gene normalization. Genome Biol. 9 Suppl 2:S3. Tomoko Ohta,_ Sampo Pyysalo, Sophia Ananiadou and Jun?ichi Tsujii. 2011. Pathway Curation Sup-port as an Information Extraction Task. Procee-dings of the Fourth International Symposium on Languages in Biology and Medicine (LBM 2011).   Tomoko Ohta, Sampo Pyysalo and_Jun?ichi Tsujii. 2011. From Pathways to Biomolecular Events: Opportunities and Challenges. Proceedings of the 2011 Workshop on Biomedical Natural Language Processing , ACL-HLT 2011, pages 105?113.  
Walter K. Schlage, et al 2011. A computable cellular stress network model for non-diseased pulmonary and cardiovascular tissue. BMC Syst Biol. 5:168.  Ted Slater and Diana H. Song. 2012. Saved by the BEL: ringing in a common language for the life sciences. Drug Discovery World Fall 2012 75:80 Selventa 2012 Reverse Causal Reasoning Methods Whitepaper  http://www.selventa.com/publications/white-papers Sofia Van Landeghem, Jari Bj?rne, Chih H Wei, Kai Hakala , Sampo Pyysalo, Sophia Ananiadou,  Hung-Yu Kao,  Zhiyong Lu, Tapio Salakoski, Yves Van de Peer, and Filip Ginter. 2013. Large-Scale Event Extraction from Literature with Multi-Level Gene Normalization. PLoS ONE 8(4): e55814.      
88
