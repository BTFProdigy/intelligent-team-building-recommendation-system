Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66?74,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Interactive Feature Space Construction using Semantic Information
Dan Roth and Kevin Small
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{danr,ksmall}@illinois.edu
Abstract
Specifying an appropriate feature space is an
important aspect of achieving good perfor-
mance when designing systems based upon
learned classifiers. Effectively incorporat-
ing information regarding semantically related
words into the feature space is known to pro-
duce robust, accurate classifiers and is one ap-
parent motivation for efforts to automatically
generate such resources. However, naive in-
corporation of this semantic information may
result in poor performance due to increased
ambiguity. To overcome this limitation, we
introduce the interactive feature space con-
struction protocol, where the learner identi-
fies inadequate regions of the feature space
and in coordination with a domain expert adds
descriptiveness through existing semantic re-
sources. We demonstrate effectiveness on an
entity and relation extraction system includ-
ing both performance improvements and ro-
bustness to reductions in annotated data.
1 Introduction
An important natural language processing (NLP)
task is the design of learning systems which per-
form well over a wide range of domains with limited
training data. While the NLP community has a long
tradition of incorporating linguistic information into
statistical systems, machine learning approaches to
these problems often emphasize learning sophisti-
cated models over simple, mostly lexical, features.
This trend is not surprising as a primary motivation
for machine learning solutions is to reduce the man-
ual effort required to achieve state of the art perfor-
mance. However, one notable advantage of discrimi-
native classifiers is the capacity to encode arbitrarily
complex features, which partially accounts for their
popularity. While this flexibility is powerful, it often
overwhelms the system designer causing them to re-
sort to simple features. This work presents a method
to partially automate feature engineering through an
interactive learning protocol.
While it is widely accepted that classifier perfor-
mance is predicated on feature engineering, design-
ing good features requires significant effort. One un-
derutilized resource for descriptive features are ex-
isting semantically related word lists (SRWLs), gen-
erated both manually (Fellbaum, 1998) and automat-
ically (Pantel and Lin, 2002). Consider the follow-
ing named entity recognition (NER) example:
His father was rushed to [Westlake
Hospital]ORG, an arm of [Resurrection
Health Care]ORG, in west suburban
[Chicagoland]LOC.
For such tasks, it is helpful to know that west is
a member of the SRWL [Compass Direction] and
other such designations. If extracting features using
this information, we would require observing only
a subset of the SRWL in the data to learn the cor-
responding parameter. This statement suggests that
one method for learning robust classifiers is to in-
corporate semantic information through features ex-
tracted from the more descriptive representation:
His father was rushed to Westlake [Health
Care Institution], an [Subsidiary] of Resur-
rection Health Care, [Locative Preposition]
[Compass Direction] suburban Chicagoland.
66
Deriving discriminative features from this rep-
resentation often results in more informative fea-
tures and a correspondingly simpler classification
task. Although effective approaches along this vein
have been shown to induce more accurate classi-
fiers (Boggess et al, 1991; Miller et al, 2004; Li and
Roth, 2005), naive approaches may instead result in
higher sample complexity due to increased ambi-
guity introduced through these semantic resources.
Features based upon SRWLs must therefore balance
the tradeoff between descriptiveness and noise.
This paper introduces the interactive feature
space construction (IFSC) protocol, which facil-
itates coordination between a domain expert and
learning algorithm to interactively define the feature
space during training. This paper describes the par-
ticular instance of the IFSC protocol where seman-
tic information is introduced through abstraction of
lexical terms in the feature space with their SRWL
labels. Specifically, there are two notable contri-
butions of this work: (1) an interactive method for
the expert to directly encode semantic knowledge
into the feature space with minimal effort and (2) a
querying function which uses both the current state
of the learner and properties of the available SRWLs
to select informative instances for presentation to
the expert. We demonstrate the effectiveness of this
protocol on an entity and relation extraction task in
terms of performance and labeled data requirements.
2 Preliminaries
Following standard notation, let x ? X represent
members of an input domain and y ? Y represent
members of an output domain where a learning al-
gorithm uses a training sample S = {(xi, yi)}mi=1
to induce a prediction function h : X ? Y . We
are specifically interested in discriminative classi-
fiers which use a feature vector generating procedure
?(x) ? x, taking an input domain member x and
generating a feature vector x. We further assume the
output assignment of h is based upon a scoring func-
tion f : ?(X ) ? Y ? R such that the prediction is
stated as y? = h(x) = argmaxy??Y f(x, y?).
The feature vector generating procedure is com-
posed of a vector of feature generation functions
(FGFs), ?(x) = ??1(x),?2(x), . . . ,?n(x)?, where
each feature generation function, ?i(x) ? {0, 1},
takes the input x and returns the appropriate fea-
ture vector value. Consider the text ?in west sub-
urban Chicagoland? where we wish to predict the
entity classification for Chicagoland. In this case,
example active FGFs include ?text=Chicagoland,
?isCapitalized, and ?text(?2)=west while FGFs such
as ?text=and would remain inactive. Since we are
constructing sparse feature vectors, we use the infi-
nite attribute model (Blum, 1992).
Semantically related word list (SRWL) feature
abstraction begins with a set of variable sized
word lists {W} such that each member lexical
element (i.e. word, phrase) has at least one
sense that is semantically related to the concept
represented by W (e.g. Wcompass direction =
north, east, . . . , southwest). For the purpose of
feature extraction, whenever the sense of a lexical el-
ement associated with a particularW appears in the
corpus, it is replaced by the name of the correspond-
ing SRWL. This is equivalent to defining a FGF for
the specified W which is a disjunction of the func-
tionally related FGFs over the member lexical ele-
ments (e.g. ?text?Wcompass direction = ?text=north ??text=east ? . . . ? ?text=southwest).
3 Interactive Feature Space Construction
The machine learning community has become in-
creasingly interested in protocols which allow inter-
action with a domain expert during training, such as
the active learning protocol (Cohn et al, 1994). In
active learning, the learning algorithm reduces the
labeling effort by using a querying function to in-
crementally select unlabeled examples from a data
source for annotation during learning. By care-
fully selecting examples for annotation, active learn-
ing maximizes the quality of inductive information
while minimizing label acquisition cost.
While active learning has been shown to reduce
sample complexity, we contend that it significantly
underutilizes the domain expert ? particularly for
complex annotation tasks. More precisely, when a
domain expert receives an instance, world knowl-
edge is used to reason about the instance and sup-
ply an annotation. Once annotated and provided for
training, the learner must recover this world knowl-
edge and incorporate it into its model from a small
number of instances, exclusively through induction.
67
Learning algorithms generally assume that the
feature space and model are specified before learn-
ing begins and remain static throughout learning,
where training data is exclusively used for parameter
estimation. Conversely, the interactive feature space
construction (IFSC) protocol relaxes this static fea-
ture space assumption by using information about
the current state of the learner, properties of knowl-
edge resources (e.g. SRWLs, gazetteers, unlabeled
data, etc.), and access to the domain expert during
training to interactively improve the feature space.
Whereas active learning focuses on the labeling ef-
fort, IFSC reduces sample complexity and improves
performance by modifying the underlying represen-
tation to simplify the overall learning task.
The IFSC protocol for SRWL abstraction is pre-
sented in Algorithm 1. Given a labeled data set S,
an initial feature vector generating procedure ?0, a
querying function Q : S ? h ? Sselect, and an
existing set of semantically related word lists, {W}
(line 1), an initial hypothesis is learned (line 3). The
querying function scores the labeled examples and
selects an instance for interaction (line 6). The ex-
pert selects lexical elements from this instance for
which feature abstractions may be performed (line
8). If the expert doesn?t deem any elements vi-
able for interaction, the algorithm returns to line 5.
Once lexical elements are selected for interaction,
the SRWLW associated with each selected element
is retrieved (line 11) and refined by the expert (line
12). Using the validated SRWL definition W? , the
lexical FGFs are replaced with the SRWL FGF (line
14). This new feature vector generating procedure
?t+1 is used to train a new classifier (line 18) and
the algorithm is repeated until the annotator halts.
3.1 Method of Expert Interaction
The method of interaction for active learning is
very natural; data annotation is required regardless.
To increase the bandwidth between the expert and
learner, a more sophisticated interaction must be al-
lowed while ensuring that the expert task of remains
reasonable. We require the interaction be restricted
to mouse clicks. When using this protocol to in-
corporate semantic information, the primary tasks
of the expert are (1) selecting lexical elements for
SRWL feature abstraction and (2) validating mem-
bership of the SRWL for the specified application.
Algorithm 1 Interactive Feature Space Construction
1: Input: Labeled training data S, feature vector
generating procedure ?0, querying function Q,
set of known SRWLs {W}, domain expert A?
2: t? 0
3: ht ? A(?t,S); learn initial hypothesis
4: Sselected ? ?
5: while annotator is willing do
6: Sselect ? Q(S\Sselected, ht); Q proposes
(labeled) instance for interaction
7: Sselected ? Sselected ? Sselect; mark selected
examples to prevent reselection
8: Eselect ? A?(Sselect); the expert selects lex-
ical elements for semantic abstraction
9: ?t+1 ? ?t; initialize new FGF vector with
existing FGFs
10: for each  ? Eselect do
11: Retrieve word listW
12: W? ? A?(W); the expert refines the ex-
isting semantic classW for this task
13: for each ? ?  do
14: ?t+1 ? (?t+1\?) ? ?W? ; re-place features with SRWL features (e.g.
?text= ? ?text?W? )
15: end for
16: end for
17: t? t+ 1
18: ht ? A(?t,S); learn new hypothesis
19: end while
20: Output: Learned hypothesis hT , final feature
space ?T , refined semantic classes {W?}
3.1.1 Lexical Feature Selection (Line 8)
Once an instance is selected by the querying func-
tion (line 6), the the domain expert selects lexical el-
ements (i.e. words, phrases) believed appropriate for
SRWL feature abstraction. This step is summarized
by Figure 1 for the example introduced in Section 1.
For this NER example, features extracted include
the words and bigrams which form the named en-
tity and those within a surrounding two word win-
dow. All lexical elements which have membership
to at least one SRWL and are used for feature ex-
traction are marked with a box and may be selected
by the user for interaction. In this particular case,
the system has made a mistake in classification of
68
His father was rushed to [Westlake
Hospital ]ORG, an arm of [Resurrection
Health Care ]ORG, in west suburban
[Chicagoland]ORG.
Figure 1: Lexical Feature Selection ? All lexical ele-
ments with SRWL membership used to derive features
are boxed. Elements used for the incorrect prediction for
Chicagoland are double-boxed. The expert may select
any boxed element for SRWL validation.
Chicagoland and the lexical elements used to derive
features for this prediction are emphasized with a
double-box for expository purposes. The expert se-
lects lexical elements which they believe will result
in good feature abstractions; the querying function
must present examples believed to have high impact.
3.1.2 Word List Validation (Lines 11 &12)
Once the domain expert has selected a lexical el-
ement for SRWL feature abstraction, they are pre-
sented with the SRWL W to validate membership
for the target application as shown in Figure 2. In
this particular case, the expert has chosen to perform
two interactions, namely for the lexical elements
west and suburban. Once they have chosen which
words and phrases will be included in this particular
feature abstraction,W is updated and the associated
features are replaced with their SRWL counterpart.
For example, ?text=west, ?text=north, etc. would all
be replaced with ?text?WA1806 later in lines 13 & 14.
A1806: southeast, northeast, south
southeast, northeast, south, north, south-
west, west, east, northwest, inland, outside
A1558: suburban, nearby, downtown
suburban, nearby, downtown, urban,
metropolitan, neighboring, near, coastal
Figure 2: Word List Validation ? Completing two domain
expert interactions. Upon selecting either double-boxed
element in Figure 1, the expert validates the respective
SRWL for feature extraction.
Accurate sense disambiguation is helpful for ef-
fective SRWL feature abstraction to manage situa-
tions where lexical elements belong to multiple lists.
In this work, we first disambiguate by predicted part
of speech (POS) tags. In cases of multiple SRWL
senses for a POS, the given SRWLs (Pantel and Lin,
2002) rank list elements according their semantic
representativeness which we use to return the high-
est ranked sense for a particular lexical element.
Also, as SRWL resources emphasize recall over pre-
cision, we reduce expert effort by using the Google
n-gram counts (Brandts and Franz, 2006) to auto-
matically prune SRWLs.
3.2 Querying Function (Line 6)
A primary contribution of this work is designing an
appropriate querying function. In doing so, we look
to maximize the impact of interactions while min-
imizing the total number. Therefore, we look to
select instances for which (1) the current hypoth-
esis indicates the feature space is insufficient and
(2) the resulting SRWL feature abstraction will help
improve performance. To account for these two
somewhat orthogonal goals, we design two query-
ing functions and aggregate their results.
3.2.1 Hypothesis-Driven Querying
To find areas of the feature space which are be-
lieved to require more descriptiveness, we look to
emphasize those instances which will result in the
largest updates to the hypothesis. To accomplish
this, we adopt an idea from the active learning
community and score instances according to their
margin relative to the current learned hypothesis,
?(ft, xi, yi) (Tong and Koller, 2001). This results
in the hypothesis-driven querying function
Qmargin = argsort
i=1,...,m
?(ft, xi, yi)
where the argsort operator is used to sort the input
elements in ascending order (for multiple instance
selection). Unlike active learning, where selection
is from an unlabeled data source, the quantity of la-
beled data is fixed and labeled data is selected during
each round. Therefore, we use the true margin and
not the expected margin. This means that we will
first select instances which have large mistakes, fol-
lowed by those instances with small mistakes, and
finally instances that make correct predictions in the
order of their confidence.
69
3.2.2 SRWL-Driven Querying
An equally important goal of the querying func-
tion is to present examples which will result in
SRWL feature abstractions of broad usability. Intu-
itively, there are two criteria distinguishing desirable
SRWLs for this purpose. First of all, large lists are
desirable as there are many lists of cities, countries,
corporations, etc. which are extremely informative.
Secondly, preference should be given to lists where
the distribution of lexical elements within a particu-
lar word list,  ? W , is more uniform. For example,
consider W = {devour, feed on, eat, consume}.
While all of these terms belong to the same SRWL,
learning features based on eat is sufficient to cover
most examples. To derive a SRWL-driven querying
function based on these principles, we use the word
list entropy, H(W) = ???W p() log p(). The
querying score for a sentence is determined by its
highest entropy lexical element used for feature ex-
traction, resulting in the querying function
Qentropy = argsort
i=1,...,m
[
argmin
??xi
?H(W)
]
This querying function is supported by the under-
lying assumption of SRWL abstraction is that there
exists a true feature space ??(x) which is built upon
SRWLs and lexical elements but is being approxi-
mated by ?(x), which doesn?t use semantic infor-
mation. In this context, a lexical feature provides
one bit of information to the prediction function
while a SRWL feature provides information content
proportional to its SRWL entropy H(W).
To study one aspect of this phenomena empiri-
cally, we examine the rate at which words are first
encountered in our training corpus from Section 4,
as shown by Figure 3. The first observation is
the usefulness of SRWL feature abstraction in gen-
eral as we see that when including an entire SRWL
from (Pantel and Lin, 2002) whenever the first ele-
ment of the list is encountered, we cover the unigram
vocabulary much more rapidly. The second observa-
tion is that when sentences are presented in the or-
der of the average SRWL entropy of their words, this
coverage rate is further accelerated. Figure 3 helps
explain the recall focused aspect of SRWL abstrac-
tion while we rely on hypothesis-driven querying to
target interactions for the specific task at hand.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200  400  600  800  1000
un
igr
am
 ty
pe
 co
ve
rag
e (
%)
sentences
SRWL - Entropy
SRWL - Sequence
Lexical - Sequence
Figure 3: The Impact of SRWL Abstraction and SRWL-
driven Querying ? The first occurrence of words occur at
a much lower rate than the first occurrence of words when
abstracted through SRWLs, particularly when sentences
are introduced as ranked by average SRWL entropy cal-
culated using (Brandts and Franz, 2006).
3.2.3 Aggregating Querying Functions
To combine these two measures, we use the Borda
count method of rank aggregation (Young, 1974) to
find a consensus between the two querying func-
tions without requiring calibration amongst the ac-
tual ranking scores. Defining the rank position of
an instance by r(x), the Borda count based querying
function is stated by
QBorda = argsort
i=1,...,m
[rmargin(xi) + rentropy(xi)]
QBorda selects instances which consider both wide
applicability through rentropy and which focus on
the specific task through rmargin.
4 Experimental Evaluation
To demonstrate the IFSC protocol on a practical ap-
plication, we examine a three-stage pipeline model
for entity and relation extraction, where the task is
decomposed into sequential stages of segmentation,
entity classification, and relation classification (Roth
and Small, 2008). Extending the standard classifi-
cation task, a pipeline model decomposes the over-
all classification into a sequence of D stages such
that each stage d = 1, . . . , D has access to the in-
put instance along with the classifications from all
previous stages, y?(d). Each stage of the pipeline
model uses a feature vector generating procedure
70
?(d)(x, y?(0), . . . , y?(d?1)) ? x(d) to learn a hypoth-
esis h(d). Once each stage of the pipelined classifier
is learned, predictions are made sequentially, where
y? = h(x) =
?
argmax
y??Y(d)
f (d)
(
x(d), y?
)?D
d=1
Each pipeline stage requires a classifier which
makes multiple interdependent predictions based on
input from multiple sentence elements x ? X1 ?
? ? ? ? Xnx using a structured output space, y(d) ?
Y(d)1 ? ? ? ? ? Y
(d)
ny . More specifically, segmenta-
tion makes a prediction for each sentence word over
Y ? {begin, inside, outside} and constraints are
enforced between predictions to ensure that an in-
side label can only follow a begin label. Entity clas-
sification begins with the results of the segmenta-
tion classifier and classifies each segment into Y ?
{person, location, organization}. Finally, rela-
tion classification labels each predicted entity pair
with Y ? {located in, work for, org based in,
live in, kill} ? {left, right}+ no relation.
The data used for empirical evaluation was taken
from (Roth and Yih, 2004) and consists of 1436 sen-
tences, which is split into a 1149 (80%) sentence
training set and a 287 (20%) sentence testing set
such that all have at least one active relation. SR-
WLs are provided by (Pantel and Lin, 2002) and
experiments were conducted using a custom graphi-
cal user interface (GUI) designed specifically for the
IFSC protocol. The learning algorithm used for each
stage of the classification task is a regularized vari-
ant of the structured Perceptron (Collins, 2002). Re-
sources used to perform experiments are available at
http://L2R.cs.uiuc.edu/?cogcomp/.
We extract features in a method similar to (Roth
and Small, 2008), except that we do not include
gazetteer features in ?(d)0 as we will include this
type of external information interactively. Secondly,
we use SRWL features as introduced. The segmen-
tation features include the word/SRWL itself along
with the word/SRWL of three words before and two
words after, bigrams of the word/SRWL surround-
ing the word, capitalization of the word, and capi-
talization of its neighbor on each side. Entity clas-
sification uses the segment size, the word/SRWL
members within the segment, and a window of two
word/SRWL elements on each side. Relation clas-
sification uses the same features as entity classifica-
tion along with the entity labels, the length of the
entities, and the number of tokens between them.
4.1 Interactive Querying Function
When using the interactive feature space construc-
tion protocol for this task, we require a querying
function which captures the hypothesis-driven as-
pect of instance selection. We observed that basing
Qmargin on the relation stage performs best, which
is not surprising given that this stage makes the most
mistakes, benefits the most from semantic informa-
tion, and also has many features which are similar to
features from previous stages. Therefore, we adapt
the querying function described by (Roth and Small,
2008) for the relation classification stage and define
our margin for the purposes of instance selection as
?relation = mini=1,...,ny
[
fy+(x, i)? fy?+(x, i)
]
where y? = argmaxy??Y\y fy?(x), the highest scor-
ing class which is not the true label, and Y+ =
Y\no relation.
4.2 Interactive Protocol on Entire Data Set
The first experiments we conduct uses all available
training data (i.e. |S| = 1149) to examine the im-
provement achieved with a fixed number of IFSC
interactions. A single interaction is defined by the
expert selecting a lexical element from a sentence
presented by the querying function and validating
the associated word list. Therefore, it is possible that
a single sentence may result in multiple interactions.
The results for this experimental setup are sum-
marized in Table 1. For each protocol configura-
tion, we report F1 measure for all three stages of
the pipeline. As our simplest baseline, we first train
using the default feature set without any semantic
features (Lexical Features). The second baseline
is to replace all instances of any lexical element
with its SRWL representation as provided by (Pan-
tel and Lin, 2002) (Semantic Features). The next
two baselines attempt to automatically increase pre-
cision by defining each semantic class using only the
top fraction of the elements in each SRWL (Pruned
Semantic (top {1/2,1/4})). This pruning procedure
often results in smaller SRWLs with a more precise
specification of the semantic concept.
71
Pruned Pruned 50 interactions
Lexical Semantic Semantic Semantic Interactive Interactive
Features Features (top 1/2) (top 1/4) (select only) (select & validate)
Segmentation 90.23 90.14 90.77 89.71 92.24 93.43
Entity Class. 82.17 83.28 83.93 83.04 85.81 88.76
Relation Class. 54.67 55.20 56.34 56.21 59.14 62.08
Table 1: Relative performance of the stated experiments conducted over the entire available dataset. The interactive
feature construction protocol outperforms all non-interactive baselines, particularly for later stages of the pipeline
while requiring only 50 interactions.
Finally, we consider the interactive feature space
construction protocol at two different stages. We
first consider the case where 50 interactions are per-
formed such that the algorithm assumes W? = W ,
that is, the expert selects features for abstraction,
but doesn?t perform validation (Interactive (select
only)). The second experiment performs the entire
protocol, including validation (Interactive (select &
validate)) for 50 interactions. On the relation ex-
traction task, we observe a 13.6% relative improve-
ment over the lexical model and a 10.2% relative im-
provement over the best SRWL baseline F1 score.
4.3 Examination of the Querying Function
As stated in section 3.2, an appropriate querying
function presents sentences which will result in the
expert selecting features from that example and for
which the resulting interactions will result in a large
performance increase. The former is difficult to
model, as it is dependent on properties of the sen-
tence (such as length), will differ from user to user,
and anecdotally is negligibly different for the three
querying functions for earlier interactions. How-
ever, we are able to measure the performance im-
provement of interactions associated with different
querying functions. For our second experiment, we
evaluate the relative performance of the three query-
ing functions defined after every ten interactions in
terms of the F1 measure for relation extraction. The
results of this experiment are shown in figure 4,
where we first see that the Qrandom generally leads
to the least useful interactions. Secondly, while
Qentropy performs well early, Qmargin works bet-
ter as more interactions are performed. Finally, we
also observe that QBorda exceeds the performance
envelope of the two constituent querying functions.
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 0.63
 0  10  20  30  40  50
rel
ati
on
 ex
tra
cti
on
 (F
1)
interactions
QBordaQentropyQmarginQrandom
Figure 4: Relative performance of interactions generated
through the respective querying functions. We see that
Qentropy performs well for a small number of interac-
tions, Qmargin performs well as more interactions are
performed and QBorda outperforms both consistently.
4.4 Robustness to Reduced Annotation
The third set of experiments consider the relative
performance of the configurations from the first set
of experiments as the amount of available training
data is reduced. To study this scenario, we per-
form the same set of experiments with 50 interac-
tions while varying the size of the training set (e.g.
|S| = {250, 500, 600, 675, 750, 1000}), summariz-
ing the results in Figure 5. One observation is that
the interactive feature space construction protocol
outperforms all other configurations at all annota-
tion levels. A second important observation is made
when comparing these results to those presented in
(Roth and Small, 2008), where this data is labeled
using active learning. In (Roth and Small, 2008),
once 65% of the labeled data is observed, a perfor-
mance level is achieved comparable to training on
the entire labeled dataset. In this work, an interpo-
72
lation of the performance at 600 and 675 labeled in-
stances implies that we achieve a performance level
comparable to training on all of the data of the base-
line learner while about 55% of the labeled data is
observed at random. Furthermore, as more labeled
data is introduced, the performance continues to im-
prove with only 50 interactions. This supports the
hypothesis that a good representation is often more
important than additional training data, even when
the data is carefully selected.
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 300  400  500  600  700  800  900  1000
rel
ati
on
 ex
tra
cti
on
 (F
1)
labeled data
Interactive (select & verify) 
Pruned Semantic (top 1/2)
Semantic Features
Lexical Features
Baseline (Lexical Features)
Figure 5: Relative performance of several baseline al-
gorithm configurations and the interactive feature space
construction protocol with variable labeled dataset sizes.
The interactive protocol outperforms other baseline meth-
ods in all cases. Furthermore, the interactive protocol (In-
teractive) outperforms the baseline lexical system (Base-
line) trained on all 1149 sentences even when trained
with a significantly smaller subset of labeled data.
5 Related Work
There has been significant recent work on designing
learning algorithms which attempt to reduce annota-
tion requirements through a more sophisticated an-
notation method. These methods allow the annota-
tor to directly specify information about the feature
space in addition to providing labels, which is then
incorporated into the learning algorithm (Huang and
Mitchell, 2006; Raghavan and Allan, 2007; Zaidan
et al, 2007; Druck et al, 2008; Zaidan and Eisner,
2008). Additionally, there has been recent work us-
ing explanation-based learning techniques to encode
a more expressive feature space (Lim et al, 2007).
Amongst these works, the only interactive learning
protocol is (Raghavan and Allan, 2007) where in-
stances are presented to an expert and features are
labeled which are then emphasized by the learning
algorithm. Thus, in this case, although additional
information is provided the feature space itself re-
mains static. To the best of our knowledge, this is
the first work that interactively modifies the feature
space by abstracting the FGFs.
6 Conclusions and Future Work
This work introduces the interactive feature space
construction protocol, where the learning algorithm
selects examples for which the feature space is be-
lieved to be deficient and uses existing semantic
resources in coordination with a domain expert to
abstract lexical features with their SRWL names.
While the power of SRWL abstraction in terms of
sample complexity is evident, incorporating this in-
formation is fraught with pitfalls regarding the in-
troduction of additional ambiguity. This interactive
protocol finds examples for which the domain ex-
pert will recognize promising semantic abstractions
and for which those semantic abstraction will signif-
icantly improve the performance of the learner. We
demonstrate the effectiveness of this protocol on a
named entity and relation extraction system.
As a relatively new direction, there are many
possibilities for future work. The most immedi-
ate task is effectively quantifying interaction costs
with a user study, including the impact of includ-
ing users with varying levels of expertise. Recent
work on modeling the costs of the active learn-
ing protocol (Settles et al, 2009; Haertel et al,
2009) provides some insight on modeling costs as-
sociated with interactive learning protocols. A sec-
ond potentially interesting direction would be to
incorporate other semantic resources such as lexi-
cal patterns (Hearst, 1992) or Wikipedia-generated
gazetteers (Toral and Mun?oz, 2006).
Acknowledgments
The authors would like to thank Ming-Wei Chang,
Margaret Fleck, Julia Hockenmaier, Alex Klemen-
tiev, Ivan Titov, and the anonymous reviewers for
their valuable suggestions. This work is supported
by DARPA funding under the Bootstrap Learning
Program and by MIAS, a DHS-IDS Center for Mul-
timodal Information Access and Synthesis at UIUC.
73
References
Avrim Blum. 1992. Learning boolean functions in an
infinite attribute space. Machine Learning, 9(4):373?
386.
Lois Boggess, Rajeev Agarwal, and Ron Davis. 1991.
Disambiguation of prepositional phrases in automat-
ically labelled technical text. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 155?159.
Thorsten Brandts and Alex Franz. 2006. Web 1T 5-gram
Version 1.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201?222.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of the Conference
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 1?8.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ization expectation criteria. In Proc. of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 595?602.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Robbie Haertel, Kevin D. Seppi, Eric K. Ringger, and
James L. Carroll. 2009. Return on investment for
active learning. In NIPS Workshop on Cost Sensitive
Learning.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. the In-
ternational Conference on Computational Linguistics
(COLING), pages 539?545.
Yifen Huang and Tom M. Mitchell. 2006. Text clustering
with extended user feedback. In Proc. of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 413?420.
Xin Li and Dan Roth. 2005. Learning question clas-
sifiers: The role of semantic information. Journal of
Natural Language Engineering, 11(4).
Siau Hong Lim, Li-Lun Wang, and Gerald DeJong. 2007.
Explanation-based feature construction. In Proc. of
the International Joint Conference on Artificial Intelli-
gence (IJCAI), pages 931?936.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 337?342.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proc. of the International Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 613?619.
Hema Raghavan and James Allan. 2007. An interactive
algorithm for asking and incorporating feature feed-
back into support vector machines. In Proc. of Inter-
national Conference on Research and Development in
Information Retrieval (SIGIR), pages 79?86.
Dan Roth and Kevin Small. 2008. Active learning for
pipeline models. In Proceedings of the National Con-
ference on Artificial Intelligence (AAAI), pages 683?
688.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proc. of the Annual Conference on
Computational Natural Language Learning (CoNLL),
pages 1?8.
Burr Settles, Mark Craven, and Lewis Friedland. 2009.
Active learning with real annotation costs. In NIPS
Workshop on Cost Sensitive Learning.
Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Research,
2:45?66.
Antonio Toral and Rafael Mun?oz. 2006. A proposal
to automatically build and maintain gazetteers using
wikipedia. In Proc. of the Annual Meeting of the
European Association of Computational Linguistics
(EACL), pages 56?61.
H. Peyton Young. 1974. An axiomatization of borda?s
rule. Journal of Economic Theory, 9(1):43?52.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: A generative approach to learning from annota-
tor rationales. In Proc. of the Conference on Empirical
Methods for Natural Language Processing (EMNLP),
pages 31?40.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In Proc. of the Annual
Meeting of the North American Association of Compu-
tational Linguistics (NAACL), pages 260?267.
74
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 44?52,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Object Search: Supporting Structured Queries in Web Search Engines
Kim Cuong Pham?, Nicholas Rizzolo?, Kevin Small?, Kevin Chen-Chuan Chang?, Dan Roth?
University of Illinois at Urbana-Champaign?
Department of Computer Science
{kimpham2, rizzolo, kcchang, danr}@illinois.edu
Tufts University?
Department of Computer Science
kevin.small@tufts.edu
Abstract
As the web evolves, increasing quantities of
structured information is embedded in web
pages in disparate formats. For example, a
digital camera?s description may include its
price and megapixels whereas a professor?s
description may include her name, univer-
sity, and research interests. Both types of
pages may include additional ambiguous in-
formation. General search engines (GSEs)
do not support queries over these types of
data because they ignore the web document
semantics. Conversely, describing requi-
site semantics through structured queries into
databases populated by information extraction
(IE) techniques are expensive and not easily
adaptable to new domains. This paper de-
scribes a methodology for rapidly develop-
ing search engines capable of answering struc-
tured queries over unstructured corpora by uti-
lizing machine learning to avoid explicit IE.
We empirically show that with minimum ad-
ditional human effort, our system outperforms
a GSE with respect to structured queries with
clear object semantics.
1 Introduction
General search engines (GSEs) are sufficient for
fulfilling the information needs of most queries.
However, they are often inadequate for retrieving
web pages that concisely describe real world ob-
jects as these queries require analysis of both un-
structured text and structured data contained in web
pages. For example, digital cameras with specific
brand, megapixel, zoom, and price attributes might
be found on an online shopping website, or a pro-
fessor with her name, university, department, and
research interest attributes might be found on her
homepage. Correspondingly, as the web continues
to evolve from a general text corpus into a hetero-
geneous collection of documents, targeted retrieval
strategies must be developed for satisfying these
more precise information needs. We accomplish this
by using structured queries to capture the intended
semantics of a user query and learning domain spe-
cific ranking functions to represent the hidden se-
mantics of object classes contained in web pages.
It is not uncommon for a user to want to pose an
object query on the web. For example, an online
shopper might be looking for shopping pages that
sell canon digital cameras with 5 megapixels cost-
ing no more than $300. A graduate student might
be looking for homepages of computer science pro-
fessors who work in the information retrieval area.
Such users expect to get a list web pages containing
objects they are looking for, or object pages, which
we will define more precisely in later sections.
GSEs rarely return satisfactory results when the
user has a structured query in mind for two primary
reasons. Firstly, GSEs only handle keyword queries
whereas structured queries frequently involve data
field semantics (e.g. numerical constraints) and ex-
hibit field interdependencies. Secondly, since GSEs
are domain-agnostic, they will generally rank cam-
era pages utilizing the same functions as a profes-
sor?s homepage, ignoring much of the structured in-
formation specific to particular domains.
Conversely, vertical search engines (e.g. DBLife,
cazoodle.com, Rexa.info, etc.) approach this prob-
44
lem from the information extraction (IE) perspec-
tive. Instead of searching an inverted index directly,
they first extract data records from text (Kushmer-
ick et al, 1997; McCallum et al, 2000). IE solu-
tions, even with large scale techniques (Agichtein,
2005), do not scale to the entire web and cost signif-
icantly more than GSEs. Secondly, creating domain-
specific models or wrappers require labeling training
examples and human expertise for each individual
site. Thirdly, pre-extracting information lacks flexi-
bility; decisions made during IE are irrevocable, and
at query time, users may find additional value in par-
tial or noisy records that were discarded by the IE
system.
These issues motivate our novel approach for de-
signing a GSE capable of answering complex struc-
tured queries, which we refer to as Object Search.
At a high level, we search web pages containing
structured information directly over their feature in-
dex, similarly to GSEs, adding expressivity by re-
formulating the structured query such that it can be
executed on a traditional inverted index. Thus, we
avoid the expense incurred by IE approaches when
supporting new object domains. From a techni-
cal perspective, this work describes a principled ap-
proach to customizing GSEs to answer structured
queries from any domain by proposing a composi-
tional ranking model for ranking web pages with
regards to structured queries and presenting an in-
teractive learning approach that eases the process of
training for a new domain.
2 The Object Search Problem
The Object Search problem is to find the object
pages that answer a user?s object query. An object
query belongs to an object domain. An object do-
main defines a set of object attributes. An object
query is simply a set of constraints over these at-
tributes. Thus we define an object query as a tuple
of n constraints q ? c1 ? c2 ? .. ? cn, where ci is a
constraint on attribute ai. More specifically, a con-
straint ci is defined as a set of acceptable values ?i
for attribute ai; i.e. ci = (ai ? ?i). For example, an
equality constraint such as ?the brand is Canon? can
be specified as (abrand ? {Canon}) and a numeric
range constraint such as ?the price is at most $200?
can be specified as (aprice ? [0, 200]). When the
user does not care about an attribute, the constraint
is the constant true.
Given an object query, we want a set of satis-
fying object pages. Specifically, object pages are
pages that represent exactly one inherent object on
the web. Pages that list several objects such as a
department directory page or camera listing pages
are not considered object pages because even though
they mentioned the object, they do not represent any
particular object. There is often a single object page
but there are many web pages that mention the ob-
ject.
The goal of Object Search is similar to learning to
rank problems (Liu, 2009), in that its goal is to learn
a ranking function ? : D ? Q ? R that ranks any
(document, query) pairs. This is accomplished by
learning an function over a set of relevant features.
Each feature can be modeled as a function that takes
the pair and outputs a real value ? : D ? Q ? R.
For example, a term frequency feature outputs the
number of times the query appears in the document.
We define a function ? = (?1, ?2, ...?n) that takes a
(document, query) pair and outputs a vector of fea-
tures. The original ranking function can be written
as ?(d, q) = ??(?(d, q)) where ?? : Rn ? R is the
function; i.e.:
? = ?? ? ? (1)
Despite the similarities, Object Search differs
from traditional information retrieval (IR) problems
in many respects. First, IR can answer only keyword
queries whereas an object query is structured by
keyword constraints as well as numeric constraints.
Second, Object Search results are ?focused?, in the
sense that they must contain an object, as opposed
to the broad notion of relevance in IR. Finally, since
object pages of different domains might have little
in common, we cannot apply the same ranking func-
tion for different object domains.
As a consequence, in a learning to rank problem,
the set of features ? are fixed for all query. The
major concern is learning the function ??. In Object
Search settings, we expect different ? for each ob-
ject domain. Thus, we have to derive both ? and
??.
There are a number of challenges in solving these
problems. First, we need a deeper understanding of
45
structured information embedded in web pages. In
many cases, an object attribute such as professor?s
university might appear only once in his homepage.
Thus, using a traditional bag-of-words model is of-
ten insufficient, because one cannot distinguish the
professor own university from other university men-
tioned in his homepage. Second, we will need train-
ing data to train a new ranking function for each
new object domain. Thus, we require an efficient
bootstrapping method to tackle this problem. Fi-
nally, any acceptable solution must scale to the size
of the web. This requirement poses challenges for
efficient query processing and efficient ranking via
the learned ranking function.
3 Object Search Framework
In this section, we illustrate the primary intuitions
behind our aproach for an Object Search solu-
tion. We describe its architecture, which serves
as a search engine framework to support structured
queries of any domain. The technical details of ma-
jor components are left for subsequent sections.
3.1 Intuition
The main idea behind our proposed approach is that
we develop different vertical search engines to sup-
port object queries in different domains. However,
we want to keep the cost of supporting each new
domain as small as possible. The key principles to
keep the cost small are to 1) share as much as pos-
sible between search engines of different domains
and 2) automate the process as much as possible
using machine learning techniques. To illustrate
our proposed approach, we suppose that an user is
searching the web for cameras. Her object query is
q = abrand ? {canon} ? aprice ? [0, 200].
First, we have to automatically learn a function ?
that ranks web pages given an object query as de-
scribed in Section 2. We observe web pages rele-
vant to the query and notice several salient features
such as ?the word canon appears in the title?, ?the
word canon appears near manufacturer?, ?interest-
ing words that appear include powershot, eos, ixus?,
and ?a price value appears after ?$? near the word
price or sale?. Intuitively, pages containing these
features have a much higher chance of containing
the Canon camera being searched. Given labeled
training data, we can learn a ranking function that
combines these features to produce the probability
of a page containing the desired camera object.
Furthermore, we need to answer user query at
query time. We need to be able to look up these
features efficiently from our index of the web. A
na??ve method to index the web is to store a list of
web pages that have the above features, and at query
time, union all pages that have one or more features,
aggregate the score for each web page, and return
the ranked result. There are three problems with this
method. First, these features are dependent on each
object domain; thus, the size of the index will in-
crease as the number of domains grows. Second,
each time a new domain is added, a new set of fea-
tures needs to be indexed, and we have to extract
features for every single web page again. Third, we
have to know beforehand the list of camera brands,
megapixel ranges, price ranges, etc, which is infea-
sible for most object domain.
However, we observe that the above query de-
pendent features can be computed efficiently from
a query independent index. For example, whether
?the word canon appears near manufacturer? can be
computed if we index all occurrences of the words
canon and manufacturer. Similarly, the feature ?the
word canon appears in the title? can be computed if
we index all the words from web pages? title, which
only depends on the web pages themselves. Since
the words and numbers from different parts of a web
page can be indexed independently of the object do-
main, we can share them across different domains.
Thus, we follow the first principle mentioned above.
Of course, computing query dependent features
from the domain independent index is more expen-
sive than computing it from the na??ve index above.
However, this cost is scalable to the web. As a mat-
ter of fact, these features are equivalent to ?phrase
search? features in modern search engines.
Thus, at a high level, we solve the Object Search
problem by learning a domain dependent ranking
function for each object domain. We store basic do-
main independent features of the web in our index.
At query time, we compute domain dependent fea-
tures from this index and apply the ranking function
to return a ranked list of web pages. In this paper, we
focus on the learning problems, leaving the problem
of efficient query processing for future work.
46
Figure 1: Object Search Architecture
3.2 System Architecture
The main goal of our Object Search system is to en-
able searching the web with object queries. In order
to do this, the system must address the challenges
described in Section 2. From the end-user?s point
of view, the system must promptly and accurately
return web pages for their object query. From the
developer?s point of view, the system must facilitate
building a new search engine to support his object
domain of interest. The goal of the architecture is to
orchestrate all of these requirements.
Figure 1 depicts Object Search architecture. It
shows how different components of Object Search
interact with an end-user and a developer. The end-
user can issue any object query of known domains.
Each time the system receives an object query from
the end-user, it translates the query into a domain in-
dependent feature query. Then the Query Processor
executes the feature query on the inverted index, ag-
gregates the features using learned function ??, and
returns a ranked list of web pages to the user.
The developer?s job is to define his object domain
and train a ranking function for it. He does it by
incrementally training the function. He starts by an-
notating a few web pages and running a learning al-
gorithm to produce a ranking function, which is then
used to retrieve more data for the developer to anno-
tate. The process iterates until the developer is satis-
fied with his trained ranking function for the object
domain.
More specifically, the Ranking Function Learner
module learns the function ?? and ? as mentioned in
Section 2. The Query Translator instantiates ? with
user object query q, resulting in ?(q). Recall that ?
is a set of feature functions ?i. Each ?i is a function
of a (d, q) pair such as ?term frequency of ak in title?
(ak is an attribute of the object). Thus we can instan-
tiate ?(q) by replacing ak with ?k, which is part of
the query q. For example, if ?k = {canon} in the
previous example, then ?(q) is ?term frequency of
canon in title?. Thus ?(q) becomes a query indepen-
dent feature and ?(q) becomes a feature query that
can be executed in our inverted index by the Query
Processor.
4 Learning for Structured Ranking
We now describe how we learn the domain depen-
dent ranking function ?, which is the core learn-
ing aspect of Object Search. As mentioned in the
previous section, ? differs from existing learning
to rank work due to the structure in object queries.
We exploit this structure to decompose the ranking
function into several components (Section 4.1) and
combine them using a probabilistic model. Exist-
ing learning to rank methods can then be leveraged
to rank the individual components. Section 4.2 de-
scribes how we fit individual ranking scores into our
probabilistic model by calibrating their probability.
4.1 Ranking model
As stated, ? models the joint probability distribu-
tion over the space of documents and queries ? =
P (d, q). Once estimated, this distribution can rank
documents inD according to their probability of sat-
isfying q. Since we are only interested in finding
satisfying object pages, we introduce a variable ?
which indicates if the document d is an object page.
Furthermore, we introduce n variables ?i which in-
dicate whether constraint ci in the query q is satis-
fied. The probability computed by ? is then:
P (d, q) = P (?1, . . . , ?n, d)
= P (?1, . . . , ?n, d, ?)
+P (?1, . . . , ?n, d, ?)
= P (d)P (?|d)P (?1, . . . , ?n|d, ?)
+P (d)P (?|d)P (?1, . . . , ?n|d, ?)
= P (d)P (?|d)P (?1, . . . , ?n|d, ?) (2)
47
' P (?|d)
n
?
i=1
P (?i|d, ?) (3)
Equation 2 holds because non-object pages do
not satisfy the query, thus, P (?1, . . . , ?n|d, ?) = 0.
Equation 3 holds because we assume a uniform dis-
tribution over d and conditional independence over
?i given d and ?.
Thus, the rest of the problem is estimating P (?|d)
and P (?i|d, ?). The difference between these prob-
ability estimates lies in the features we use. Since ?
depends only in d but not q, we use query indepen-
dent features. Similarly, ?i only depends on d and
ci, thus we use features depending on ci and d.
4.2 Calibrating ranking probability
In theory, we can use any learning algorithm men-
tioned in (Liu, 2009)?s survey to obtain the terms in
Equation 3. In practice, however, such learning al-
gorithms often output a ranking score that does not
estimate the probability. Thus, in order to use them
in our ranking model, we must transform that rank-
ing score into a probability.
For empirical purposes, we use the averaged Per-
ceptron (Freund and Schapire, 1999) to discrimina-
tively train each component of the factored distri-
bution independently. This algorithm requires a set
of input vectors, which we obtain by applying the
relational feature functions to the paired documents
and queries. For each constraint ci, we have a fea-
ture vector xi = ?i(d, q). The algorithm produces a
weight vector of parameterswi as output. The prob-
ability of ci being satisfied by d given that d contains
an object can then be estimated with a sigmoid func-
tion as:
P (ci|d, ?) ? P (true|?i(d, q)) ?
1
1 + exp(?wTi xi)
(4)
Similarly, to estimate P (?|d), we use a fea-
ture vector that is dependent only on d. De-
noting the function as ?0, we have P (?|d) =
P (true|?0(d, q)), which can be obtained from (4).
While the sigmoid function has performed well
empirically, probabilities it produces are not cali-
brated. For better calibrated probabilities, one can
apply Platt scaling (Platt, 1999). This method intro-
duces two parameters A and B, which can be com-
puted using maximum likelihood estimation:
P (true|?i(d, q)) ?
1
1 + exp(AwTi ?i(d, q) + B)
(5)
In contrast to the sigmoid function, Platt scaling can
also be applied to methods that give un-normalized
scores such as RankSVM (Cao et al, 2006).
Substituting (4) and (5) into (3), we see that our
final learned ranking function has the form
?(d, q) =
n
?
i=0
1
(1 + exp(AiwTi ?i(d, q) + Bi))
(6)
5 Learning Based Programming
Learning plays a crucial role in developing a new ob-
ject domain. In addition to using supervised meth-
ods to learn ?, we also exploit active learning to ac-
quire training data from unlabeled web pages. The
combination of these efforts would benefit from a
unified framework and interface to machine learn-
ing. Learning Based Programming (LBP) (Roth,
2005) is such a principled framework. In this sec-
tion, we describe how we applied and extended LBP
to provide a user friendly interface for the developer
to specify features and guide the learning process.
Section 5.1 describes how we structured our frame-
work around Learning Based Java (LBJ), an instance
of LBP. Section 5.2 extends the framework to sup-
port interactive learning.
5.1 Learning Based Java
LBP is a programming paradigm for systems whose
behaviors depend on naturally occurring data and
that require reasoning about data and concepts in
ways that are hard, if not impossible, to write explic-
itly. This is exactly our situation. Not only do we
not know how to specify a ranking function for an
object query, we might not even know exactly what
features to use. Using LBP, we can specify abstract
information sources that might contribute to deci-
sions and apply a learning operator to them, thereby
letting a learning algorithm figure out their impor-
tances in a data-driven way.
Learning Based Java (LBJ) (Rizzolo and Roth,
2007) is an implementation of LBP which we used
and extended for our purposes. The most useful
abstraction in LBJ is that of the feature generation
48
function (FGF). This allows the programmer to rea-
son in terms of feature types, rather than specifying
individual features separately, and to treat them as
native building blocks in a language for constructing
learned functions. For example, instead of specify-
ing individual features such as the phrases ?profes-
sor of?,?product description?, etc., we can specify a
higher level feature type called ?bigram?, and let an
algorithm select individual features for ranking pur-
poses.
From the programming point of view, LBJ pro-
vides a clean interface and abstracts away the te-
dium of feature extraction and learning implemen-
tations. This enabled us to build our system quickly
and shorten our development cycle.
5.2 Interactive Machine Learning
We advocate an interactive training process (Fails
and Olsen, 2003), in which the developer iteratively
improves the learner via two types of interaction
(Algorithm 1).
The first type of interaction is similar to active
learning where the learner presents unlabeled in-
stances to the developer for annotation which it be-
lieves will most positively impact learning. In rank-
ing problems, top ranked documents are presented
as they strongly influence the loss function. The
small difference from traditional active learning in
our setting is that the developer assists this process
by also providing more queries other than those en-
countered in the current training set.
The second type of interaction is feature selec-
tion. We observed that feature selection contributed
significantly in the performance of the learner espe-
cially when training data is scarce. This is because
with little training data and a huge feature space, the
learner tends to over-fit. Fortunately in web search,
the features used in ranking are in natural language
and thereby intuitive to the developer. For example,
one type of feature used in ranking the university
constraint of a professor object query is the words
surrounding the query field as in ?university of ...?
or ?... university?. If the learner only sees examples
from the University of Anystate at Anytown, then
it?s likely that Anytown will have a high weight in
addition to University and of. However, the Any-
town feature will not generalize for documents from
other universities. Having background knowledge
like this, the developer can unselect such features.
Furthermore, the fact that Anytown has a high weight
is also an indication that the developer needs to pro-
vide more examples of other universities so that the
learner can generalize (the first type of interaction).
Algorithm 1 Interactive Learning Algorithm
1: The developer uses keyword search to find and
annotate an initial training set.
2: The system presents a ranked list of features
computed from labeled data.
3: The developer adds/removes features.
4: The system learns the ranking function using se-
lected features.
5: The developer issues queries and annotates top
ranked unlabeled documents returned by the
system.
6: If performance is not satisfactory, go to step 2.
The iterative algorithm starts with zero training
data and continues until the learner?s performance
reaches a satisfactory point. At step 2, the developer
is presented with a ranked list of features. To deter-
mine which features played the biggest role in the
classifier?s decision making, we use a simple rank-
ing metric called expected entropy loss (Glover et
al., 2001). Let f represent the event that a given
feature is active. Let C be the event that the given
example is classified as true. The conditional en-
tropy of the classification distribution given that
f occurs is H(C|f) ? ?P (C|f) log(P (C|f)) ?
P (C|f) log(P (C|f) and similarly, when f does not
occur, we replace f by f . The expected entropy loss
is
L(C|f) ? H(C)? E[H(C|f)]
= H(C)? (P (f)H(C|f) +
P (f)H(C|f) (7)
The intuition here is that if the classification loses
a lot of entropy when conditioned on a particular
feature, that feature must be very discriminative and
correlated with the classification itself.
It is noted that feature selection plays two impor-
tant roles in our framework. First, it avoids over-
fitting when training data is scarce, thus increas-
ing the effectiveness of our active learning protocol.
Second, since search time depends on how many
49
domain # pages train test
homepage 22.1 11.1 11
laptop 21 10.6 10.4
camera 18 9 9
random 97.8 48.9 48.8
total 158.9 79.6 79.2
Table 1: Number of web pages (in thousands) collected
for experiment
features we use to query the web pages, keeping the
number of features small will ensure that searching
is fast enough to be useful.
6 Experimental Results
In this section we present an experiment that com-
pares Object Search with keyword search engines.
6.1 Experimental Setting
Since we are the first to tackle this problem of an-
swering structured query on the web, there is no
known dataset available for our experiment. We col-
lected the data ourselves using various sources from
the web. Then we labeled search results from differ-
ent object queries using the same annotation proce-
dure described in Section 5.
We collected URLs from two main sources: the
open directory (DMOZ) and existing search en-
gines (SE). For DMOZ, we included URLs from
relevant categories. For SE, we manually entered
queries with keywords related to professors? home-
pages, laptops, and digital cameras, and included
all returned URLs. Having collected the URLs, we
crawled their content and indexed them. Table 1
summarizes web page data we have collected.
We split the data randomly into two parts, one for
training and one for testing, and created a single in-
verted index for both of them. The developer can
only see the training documents to select features
and train ranking functions. At testing time, we ran-
domly generate object queries, and evaluate on the
testing set. Since Google?s results come not from
our corpus but the whole web, it might not be fair to
compare against our small corpus. To accommodate
this, we also added Google?s results into our testing
corpus. We believe that most ?difficult? web pages
that hurt Google?s performance would have been in-
Field Keywords Example
Laptop domain
brand laptop,notebook lenovo laptop
processor ghz, processor 2.2 ghz
price $, price $1000..1100
Professor domain
name professor, re-
search professor,
faculty
research profes-
sor scott
university university, uni-
versity of
stanford
university
Table 2: Sample keyword reformulation for Google
cluded in the top Google result. Thus, they are also
available to test ours. In the future, we plan to im-
plement a local IR engine to compare against ours
and conduct a larger scale experiment to compare to
Google.
We evaluated the experiment with two different
domains: professor and laptop. We consider home-
pages and online shopping pages as object pages for
the professor and laptop domains respectively.
For each domain, we generated 5 random object
queries with different field configurations. Since
Google does not understand structured queries, we
reformulated each structured query into a simple
keyword query. We do so by pairing the query field
with several keywords. For example, a query field
abrand ? {lenovo} can be reformulated as ?lenovo
laptop?. We tried different combinations of key-
words as shown in table 2. To deal with numbers,
we use Google?s advanced search feature that sup-
ports numeric range queries1. For example, a price
constraint aprice ? [100, 200] might be reformulated
as ?price $100..200?. Since it is too expensive to
find the best keyword formulations for every query,
we picked the combination that gives the best result
for the first Google result page (Top 10 URLs).
6.2 Result
We measure the ranking performance with average
precision. Table 3 shows the results for our search
engine (OSE) and Google. Our ranking function
outperforms Google for most queries, especially in
1A numeric range written as ?100..200? is treated as a key-
word that appears everywhere a number in the range appears
50
Qry Professor LaptopOSE Google OSE Google
1 0.92 (71) 0.90(65) 0.7 (15) 0.44 (12)
2 0.83(88) 0.91(73) 0.62 (12) 0.26 (11)
3 0.51(73) 0.66(48) 0.44 (40) 0.31 (24)
4 0.42(49) 0.3(30) 0.36 (3) 0.09 (1)
5 0.91(18) 0.2(16) 0.77 (17) 0.42 (3)
Table 3: Average precision for 5 random queries. The
number of positive documents are in brackets
the laptop domain. In the professor domain, Google
wins in two queries (?UC Berkeley professor? and
?economics professors?). This suggests that in cer-
tain cases, reformulating to keyword query is a sen-
sible approach, especially if all the fields in the ob-
ject query are keywords. Even though Google can
be used to reformulate some queries, it is not clear
how and when this will succeed. Therefore, we need
a principled solution as proposed in this paper.
7 Related Work
Many recent works propose methods for supporting
structured queries on unstructured text (Jain et al,
2007), (Cafarella et al, 2007), (Gruhl et al, 2004).
These works follow a typical extract-then-query ap-
proach, which has several problems as we discussed
in section 1. (Agichtein, 2005) proposed using sev-
eral large scale techniques. Their idea of using spe-
cialized index and search engine is similar to our
work. However those methods assumes that struc-
tured data follows some textual patterns whereas our
system can flexibly handle structured object using
textual patterns as well as web page features.
Interestingly, the approach of translating struc-
tured queries to unstructured queries has been stud-
ied in (Liu et al, 2006). The main difference is
that SEMEX relies on carefully hand-tuned heuris-
tics on open-domain SQL queries while we use ma-
chine learning to do the translation on domain spe-
cific queries.
Machine Learning approaches to rank documents
have been studied extensively in IR (Liu, 2009).
Even though much of existing works can be used to
rank individual constraints in the structured query.
We proposed an effective way to aggregate these
ranking scores. Further more, existing learning to
rank works assumed a fixed set of features, whereas,
the feature set in object search depends on object
domain. As we have shown, the effectiveness of
the ranking function depends much on the set of
features. Thus, an semi-automatic method to learn
these was proposed in section 5.
Our interactive learning protocol inherits features
from existing works in Active Learning (see (Set-
tles, 2009) for a survey). (Fails and Olsen, 2003)
coined the term ?interactive machine learning? and
showed that a learner can take advantage of user in-
teraction to quickly acquire necessary training data.
(Roth and Small, 2009) proposed another interactive
learning protocol that improves upon a relation ex-
traction task by incremetally modifying the feature
representation.
Finally, this work is related to document re-
trieval mechanisms used for question answering
tasks (Voorhees, 2001) where precise retrieval meth-
ods are necessary to find documents which con-
tain specific information for answering factoids
(Agichtein et al, 2001).
8 Conclusion
We introduces the Object Search framework that
searches the web for documents containing real-
world objects. We formalized the problem as a
learning to rank for IR problem and showed an ef-
fective method to solve it. Our approach goes be-
yond the traditional bag-of-words representation and
views each web page as a set of domain independent
features. This representation enabled us to rank web
pages with respect to object query. Our experiments
showed that, with small human effort, it is possi-
ble to create specialized search engines that out-
performs GSEs on domain specific queries. More-
over, it is possible to search the web for documents
with deeper meaning, such as those found in object
pages. Our work is a small step toward semantic
search engines by handling deeper semantic queries.
Acknowledgement
This work is supported by DARPA funding under
the Bootstrap Learning Program, MIAS, a DHS-
IDS Center for Multimodal Information Access and
Synthesis at UIUC, NSF grant NSF SoD-HCER-
0613885 and a grant from Yahoo! Inc.
51
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2001. Learning search engine specific query trans-
formations for question answering. In WWW ?01:
Proceedings of the 10th international conference on
World Wide Web, pages 169?178, New York, NY,
USA. ACM.
Eugene Agichtein. 2005. Scaling Information Extraction
to Large Document Collections. IEEE Data Eng. Bull,
28:3.
Michael Cafarella, Christopher Re, Dan Suciu, and Oren
Etzioni. 2007. Structured Querying of Web Text Data:
A Technical Challenge. In CIDR.
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang,
and Hsiao-Wuen Hon. 2006. Adapting Ranking SVM
to Document Retrieval. In SIGIR ?06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 186?193, New York, NY, USA. ACM.
Jerry Alan Fails and Dan R. Olsen, Jr. 2003. Interactive
machine learning. In IUI ?03: Proceedings of the 8th
international conference on Intelligent user interfaces,
pages 39?45, New York, NY, USA. ACM.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Eric J. Glover, Gary W. Flake, Steve Lawrence, Andries
Kruger, David M. Pennock, William P. Birmingham,
and C. Lee Giles. 2001. Improving Category Specific
Web Search by Learning Query Modifications. Ap-
plications and the Internet, IEEE/IPSJ International
Symposium on, 0:23.
D. Gruhl, L. Chavet, D. Gibson, J. Meyer, P. Pattanayak,
A. Tomkins, and J. Zien. 2004. How to Build a Web-
Fountain: An Architecture for Very Large Scale Text
Analytics. IBM Systems Journal.
A. Jain, A. Doan, and L. Gravano. 2007. SQL Queries
Over Unstructured Text Databases. In Data Engineer-
ing, 2007. ICDE 2007. IEEE 23rd International Con-
ference on, pages 1255?1257.
N. Kushmerick, D. Weld, and R. Doorenbos. 1997.
Wrapper Induction for Information Extraction. In IJ-
CAI, pages 729?737.
Jing Liu, Xin Dong, and Alon Halevy. 2006. Answering
Structured Queries on Unstructured Data. In WebDB.
Tie-Yan Liu. 2009. Learning to Rank for Information
Retrieval. Found. Trends Inf. Retr., 3(3):225?331.
Andrew Kachites McCallum, Kamal Nigam, Jason Ren-
nie, and Kristie Seymore. 2000. Automating the Con-
struction of Internet Portals with Machine Learning.
Information Retrieval, 3(2):127?163.
J. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. In In Advances in Large Margin Classifiers.
MIT Press.
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597?604, Irvine, California, September. IEEE.
Dan Roth and Kevin Small. 2009. Interactive feature
space construction using semantic information. In
CoNLL ?09: Proceedings of the Thirteenth Conference
on Computational Natural Language Learning, pages
66?74, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Dan Roth. 2005. Learning Based Programming. Innova-
tions in Machine Learning: Theory and Applications.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, University
of Wisconsin-Madison.
Ellen M. Voorhees. 2001. The trec question answering
track. Nat. Lang. Eng., 7(4):361?378.
52
