Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 97?104
Manchester, August 2008
ParaMetric: An Automatic Evaluation Metric for Paraphrasing
Chris Callison-Burch
Center for Speech and Language Processing
Johns Hopkins University
3400 N. Charles St.
Baltimore, MD 21218
Trevor Cohn Mirella Lapata
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Abstract
We present ParaMetric, an automatic eval-
uation metric for data-driven approaches to
paraphrasing. ParaMetric provides an ob-
jective measure of quality using a collec-
tion of multiple translations whose para-
phrases have been manually annotated.
ParaMetric calculates precision and recall
scores by comparing the paraphrases dis-
covered by automatic paraphrasing tech-
niques against gold standard alignments of
words and phrases within equivalent sen-
tences. We report scores for several estab-
lished paraphrasing techniques.
1 Introduction
Paraphrasing is useful in a variety of natural lan-
guage processing applications including natural
language generation, question answering, multi-
document summarization and machine translation
evaluation. These applications require paraphrases
for a wide variety of domains and language us-
age. Therefore building hand-crafted lexical re-
sources such as WordNet (Miller, 1990) would be
far too laborious. As such, a number of data-driven
approaches to paraphrasing have been developed
(Lin and Pantel, 2001; Barzilay and McKeown,
2001; Barzilay and Lee, 2003; Pang et al, 2003;
Quirk et al, 2004; Bannard and Callison-Burch,
2005). Despite this spate of research, no objective
evaluation metric has been proposed.
In absence of a repeatable automatic evaluation,
the quality of these paraphrasing techniques was
gauged using subjective manual evaluations. Sec-
tion 2 gives a survey of the various evaluation
methodologies used in previous research. It has
not been possible to directly compare paraphrasing
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
techniques, because each one was evaluated using
its own idiosyncratic experimental design. More-
over, because these evaluations were performed
manually, they are difficult to replicate.
We introduce an automatic evaluation metric,
called ParaMetric, which uses paraphrasing tech-
niques to be compared and enables an evaluation
to be easily repeated in subsequent research. Para-
Metric utilizes data sets which have been annotated
with paraphrases. ParaMetric compares automatic
paraphrases against reference paraphrases.
In this paper we:
? Present a novel automatic evaluation metric
for data-driven paraphrasing methods;
? Describe how manual alignments are cre-
ated by annotating correspondences between
words in multiple translations;
? Show how phrase extraction heuristics from
statistical machine translation can be used to
enumerate paraphrases from the alignments;
? Report ParaMetric scores for a number of ex-
isting paraphrasing methods.
2 Related Work
No consensus has been reached with respect to the
proper methodology to use when evaluating para-
phrase quality. This section reviews past methods
for paraphrase evaluation.
Researchers usually present the quality of their
automatic paraphrasing technique in terms of a
subjective manual evaluation. These have used
a variety of criteria. For example, Barzilay
and McKeown (2001) evaluated their paraphrases
by asking judges whether paraphrases were ?ap-
proximately conceptually equivalent.? Ibrahim
et al (2003) asked judges whether their para-
phrases were ?roughly interchangeable given the
genre.? Bannard and Callison-Burch (2005) re-
placed phrases with paraphrases in a number of
97
sentences and asked judges whether the substi-
tutions ?preserved meaning and remained gram-
matical.? These subjective evaluations are rather
vaguely defined and not easy to reproduce.
Others evaluate paraphrases in terms of whether
they improve performance on particular tasks.
Callison-Burch et al (2006b) measure improve-
ments in translation quality in terms of Bleu score
(Papineni et al, 2002) and in terms of subjective
human evaluation when paraphrases are integrated
into a statistical machine translation system. Lin
and Pantel (2001) manually judge whether a para-
phrase might be used to answer questions from the
TREC question-answering track. To date, no one
has used task-based evaluation to compare differ-
ent paraphrasing methods. Even if such an eval-
uation were performed, it is unclear whether the
results would hold for a different task. Because of
this, we strive for a general evaluation rather than
a task-specific one.
Dolan et al (2004) create a set of manual word
alignments between pairs of English sentences.
We create a similar type of data, as described in
Section 4. Dolan et al use heuristics to draw pairs
of English sentences from a comparable corpus
of newswire articles, and treat these as potential
paraphrases. In some cases these sentence pairs
are good examples of paraphrases, and in some
cases they are not. Our data differs because it
is drawn from multiple translations of the same
foreign sentences. Barzilay (2003) suggested that
multiple translations of the same foreign source
text were a perfect source for ?naturally occur-
ring paraphrases? because they are samples of text
which convey the same meaning but are produced
by different writers. That being said, it may be
possible to use Dolan et als data toward a similar
end. Cohn et al (to appear) compares the use of
the multiple translation corpus with the MSR cor-
pus for this task.
The work described here is similar to work in
summarization evaluation. For example, in the
Pyramid Method (Nenkova et al, 2007) content
units that are similar across human-generated sum-
maries are hand-aligned. These can have alter-
native wordings, and are manually grouped. The
idea of capturing these and building a resource for
evaluating summaries is in the same spirit as our
methodology.
3 Challenges for Evaluating Paraphrases
Automatically
There are several problems inherent to automati-
cally evaluating paraphrases. First and foremost,
developing an exhaustive list of paraphrases for
any given phrase is difficult. Lin and Pantel (2001)
illustrate the difficulties that people have generat-
ing a complete list of paraphrases, reporting that
they missed many examples generated by a sys-
tem that were subsequently judged to be correct. If
a list of reference paraphrases is incomplete, then
using it to calculate precision will give inaccurate
numbers. Precision will be falsely low if the sys-
tem produces correct paraphrases which are not in
the reference list. Additionally, recall is indeter-
minable because there is no way of knowing how
many correct paraphrases exist.
There are further impediments to automatically
evaluating paraphrases. Even if we were able to
come up with a reasonably exhaustive list of para-
phrases for a phrase, the acceptability of each para-
phrase would vary depending on the context of
the original phrase (Szpektor et al, 2007). While
lexical and phrasal paraphrases can be evaluated
by comparing them against a list of known para-
phrases (perhaps customized for particular con-
texts), this cannot be naturally done for struc-
tural paraphrases which may transform whole sen-
tences.
We attempt to resolve these problems by hav-
ing annotators indicate correspondences in pairs
of equivalent sentences. Rather than having peo-
ple enumerate paraphrases, we asked that they per-
form the simper task of aligning paraphrases. Af-
ter developing these manual ?gold standard align-
ments? we can gauge how well different automatic
paraphrases are at aligning paraphrases within
equivalent sentences. By evaluating the perfor-
mance of paraphrasing techniques at alignment,
rather than at matching a list of reference para-
phrases, we obviate the need to have a complete
set of paraphrases.
We describe how sets of reference paraphrases
can be extracted from the gold standard align-
ments. While these sets will obviously be frag-
mentary, we attempt to make them more complete
by aligning groups of equivalent sentences rather
than only pairs. The paraphrase sets that we extract
are appropriate for the particular contexts. More-
over they may potentially be used to study struc-
tural paraphrases, although we do not examine that
98
an
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resign
to
him
want
others
while
,
him
impeach
to
propose
people
some
a
n
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resignation
his
tender
to
him
want
who
those
and
him
impeaching
propose
who
those
are
there
.
voluntarily
office
leave
to
him
want
some
and
him
against
indictment
an
proposing
are
some
a
n
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
Figure 1: Pairs of English sentences were aligned
by hand. Black squares indicate paraphrase corre-
spondences.
in this paper.
4 Manually Aligning Paraphrases
We asked monolingual English speakers to align
corresponding words and phrases across pairs of
equivalent English sentences. The English sen-
tences were equivalent because they were transla-
tions of the same foreign language text created by
different human translators. Our annotators were
instructed to align parts of the sentences which
had the same meaning. Annotators were asked
to prefer smaller one-to-one word alignments, but
were allowed to create one-to-many and many-to-
many alignments where appropriate. They were
given a set of annotation guidelines covering spe-
cial cases such as repetition, pronouns, genitives,
phrasal verbs and omissions (Callison-Burch et al,
2006a). The manual correspondences are treated
as gold standard alignments.
We use a corpus that contains eleven En-
glish translations of Chinese newswire documents,
which were commissioned from different transla-
tion agencies by the Linguistics Data Consortium
1
.
The data was created for the Bleu machine trans-
lation evaluation metric (Papineni et al, 2002),
which uses multiple translations as a way of cap-
turing allowable variation in translation. Whereas
the Bleu metric requires no further information,
our method requires a one-off annotation to explic-
itly show which parts of the multiple translations
constitute paraphrases.
The rationale behind using a corpus with eleven
translations was that a greater number of transla-
tions would likely result in a greater number of
paraphrases for each phrase. Figure 1 shows the
alignments that were created between one sen-
tence and three of its ten corresponding transla-
tions. Table 1 gives a list of non-identical words
and phrases that can be paired by way of the word
alignments. These are the basic paraphrases con-
tained within the three sentence pairs. Each phrase
has up to three paraphrases. The maximum num-
ber of paraphrases for a given span in each sen-
tence is bounded by the number of equivalent sen-
tences that it is paired with.
In addition to these basic paraphrases, longer
paraphrases can also be obtained using the heuris-
tic presented in Och and Ney (2004) for extract-
ing phrase pairs (PP) from word alignments A, be-
tween a foreign sentence f
J
1
and an English sen-
1
See LDC catalog number 2002T01.
99
some some people, there are those who
want propose, are proposing
to impeach an indictment against, impeach-
ing
and while
others some, those who
expect want
step down resign, leave office voluntarily,
tender his resignation
Table 1: Non-identical words and phrases which
are identified as being in correspondence by the
alignments in Figure 1.
tence e
I
1
:
PP (f
J
1
, e
I
1
, A) = {(f
j+m
j
, e
i+n
i
) :
?(i
?
, j
?
) ? A : j ? j
?
? j + m ? i ? i
?
? i + n
??(i
?
, j
?
) ? A : j ? j
?
? j + m? ? i ? i
?
? i + n}
When we apply the phrase extraction heuris-
tic to aligned English sentences, we add the con-
straint f
j+m
j
6= e
i+n
i
to exclude phrases that are
identical. This heuristic would allow ?some peo-
ple propose to impeach him,? ?some are proposing
an indictment against him,? and ?there are those
who propose impeaching him? to be extracted
as paraphrases of ?some want to impeach him.?
The heuristic extracts a total of 142 non-identical
phrase pairs from the three sentences given in Fig-
ure 1.
For the results reported in this paper, annotators
aligned 50 groups of 10 pairs of equivalent sen-
tences, for a total of 500 sentence pairs. These
were assembled by pairing the first of the LDC
translations with the other ten (i.e. 1-2, 1-3, 1-4,
..., 1-11). The choice of pairing one sentence with
the others instead of doing all pairwise combina-
tions was made simply because the latter would
not seem to add much information. However, the
choice of using the first translator as the key was
arbitrary.
Annotators corrected a set of automatic word
alignments that were created using Giza++ (Och
and Ney, 2003), which was trained on a total of
109,230 sentence pairs created from all pairwise
combinations of the eleven translations of 993 Chi-
nese sentences.
The average amount of time spent on each of
the sentence pairs was 77 seconds, with just over
eleven hours spent to annotate all 500 sentence
pairs. Although each sentence pair in our data
set was annotated by a single annotator, Cohn et
al. (to appear) analyzed the inter-annotator agree-
ment for randomly selected phrase pairs from the
same corpus, and found inter-annotator agreement
of
?
C = 0.85 over the aligned words and
?
C = 0.63
over the alignments between basic phrase pairs,
where
?
C is measure of inter-rater agreement in the
style of Kupper and Hafner (1989).
5 ParaMetric Scores
We can exploit the manually aligned data to com-
pute scores in two different fashions. First, we
can calculate how well an automatic paraphrasing
technique is able to align the paraphrases in a sen-
tence pair. Second, we can calculate the lower-
bound on precision for a paraphrasing technique
and its relative recall by enumerating the para-
phrases from each of the sentence groups. The first
of these score types does not require groups of sen-
tences, only pairs.
We calculate alignment accuracy by comparing
the manual alignments for the sentence pairs in the
test corpus with the alignments that the automatic
paraphrasing techniques produce for the same
sentence pairs. We enumerate all non-identical
phrase pairs within the manually word-aligned
sentence pairs and within the automatically word
aligned sentence pairs using PP . We calculate the
precision and recall of the alignments by taking
the intersection of the paraphrases extracted from
the manual alignments M , and the paraphrases
extracted from a system?s alignments S:
Align
Prec
=
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S) ? PP (e
1
, e
2
,M)|
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S)|
Align
Recall
=
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
, S) ? PP (e
1
, e
2
,M)|
?
<e
1
,e
2
>?C
|PP (e
1
, e
2
,M)|
Where e
1
, e
2
are pairs of English sentence from
the test corpus.
Measuring a paraphrasing method?s perfor-
mance on the task of aligning the paraphrases is
somewhat different than what most paraphrasing
methods do. Most methods produce a list of para-
phrases for a given input phrase, drawing from
a large set of rules or a corpus larger than our
small test set. We therefore also attempt to mea-
sure precision and recall by comparing the set of
100
paraphrases that method M produces for phrase p
that occurs in sentence s. We denote this set as
para
M
(p, s), where s is an optional argument for
methods that constrain their paraphrases based on
context.
Our reference sets of paraphrases are generated
in a per group fashion. We enumerate the reference
paraphrases for phrase p in sentence s in group G
as
para
REF
(p
1
, s
1
, G) =
{p
2
: ?(p
1
, p
2
) ?
?
<s
1
,s
2
,A>?G
PP (s
1
, s
2
, A)}
The maximum size of this set is the number of
sentence pairs in G. Because this set of reference
paraphrases is incomplete, we can only calculate
a lower bound on the precision of a paraphrasing
method and its recall relative to the reference
paraphrases. We call these LB-Precision and
Rel-Recall and calculate them as follows:
LB-Precision =
?
<s,G>?C
?
p?s
|para
M
(p, s) ? para
REF
(p
1
, s,G)|
|para
M
(p, s)|
Rel-Recall =
?
<s,G>?C
?
p?s
|para
M
(p, s) ? para
REF
(p
1
, s,G)|
|para
REF
(p
1
, s,G)|
For these metrics we require the test corpus
C to be a held-out set and restrict the automatic
paraphrasing techniques from drawing paraphrases
from it. The idea is instead to see how well these
techniques are able to draw paraphrases from the
other sources of data which they would normally
use.
6 Paraphrasing Techniques
There are a number of established methods for
extracting paraphrases from data. We describe
the following methods in this section and evaluate
them in the next:
? Pang et al (2003) used syntactic alignment to
merge parse trees of multiple translations,
? Quirk et al (2004) treated paraphrasing as
monolingual statistical machine translation,
? Bannard and Callison-Burch (2005) used
bilingual parallel corpora to extract para-
phrases.
S
NP VP
NN
persons
AUX
were
CD
12
VP
VB
killed
S
NP VP
NN
people
VB
died
CD
twelve
VB
NP VP
CD NN
12
twelve
people
persons
...
were
...
died
...
killed
AUX VP
BEG END
12
twelve
people
persons
died
were
killed
Tree 1 Tree 2
+
Parse Forest
Word Lattice
Merge
Linearize
Figure 2: Pang et al (2003) created word graphs
by merging parse trees. Paths with the same start
and end nodes are treated as paraphrases.
Pang et al (2003) use multiple translations to
learn paraphrases using a syntax-based alignment
algorithm, illustrated in Figure 2. Parse trees were
merged into forests by grouping constituents of the
same type (for example, the two NPs and two VPs
are grouped). Parse forests were mapped onto fi-
nite state word graphs by creating alternative paths
for every group of merged nodes. Different paths
within the resulting word lattice are treated as para-
phrases of each other. For example, in the word lat-
tice in Figure 2, people were killed, persons died,
persons were killed, and people died are all possi-
ble paraphrases of each other.
Quirk et al (2004) treated paraphrasing as
?monolingual statistical machine translation.?
They created a ?parallel corpus? containing pairs
of English sentences by drawing sentences with a
low edit distance from news articles that were writ-
ten about the same topic on the same date, but pub-
lished by different newspapers. They formulated
the problem of paraphrasing in probabilistic terms
in the same way it had been defined in the statisti-
cal machine translation literature:
e?
2
= argmax
e
2
p(e
2
|e
1
)
= argmax
e
2
p(e
1
|e
2
)p(e
2
)
101
I do not believe in mutilating dead bodies
 
cad?veresno soy partidaria mutilarde
cad?veres de inmigrantes ilegales ahogados a la playatantosarrojaEl mar ...
corpsesSo many of drowned illegals get washed up on beaches ...
Figure 3: Bannard and Callison-Burch (2005) ex-
tracted paraphrases by equating English phrases
that share a common translation.
Where p(e
1
|e
2
) is estimated by training word
alignment models over the ?parallel corpus? as in
the IBM Models (Brown et al, 1993), and phrase
translations are extracted from word alignments as
in the Alignment Template Model (Och, 2002).
Bannard and Callison-Burch (2005) also used
techniques from statistical machine translation to
identify paraphrases. Rather than drawing pairs
of English sentences from a comparable corpus,
Bannard and Callison-Burch (2005) used bilingual
parallel corpora. They identified English para-
phrases by pivoting through phrases in another lan-
guage. They located foreign language translations
of an English phrase, and treated the other En-
glish translations of those foreign phrases as poten-
tial paraphrases. Figure 3 illustrates how a Span-
ish phrase can be used as a point of identifica-
tion for English paraphrases in this way. Bannard
and Callison-Burch (2005) defined a paraphrase
probability p(e
2
|e
1
) in terms of the translation
model probabilities p(f |e
1
) and p(e
2
|f). Since e
1
can translate as multiple foreign language phrases,
they sum over f , and since multiple parallel cor-
pora can be used they summed over each parallel
corpus C:
e?
2
= arg max
e
2
6=e
1
p(e
2
|e
1
)
? arg max
e
2
6=e
1
?
C
?
f in C
p(f |e
1
)p(e
2
|f)
7 Comparing Paraphrasing Techniques
with ParaMetric
7.1 Training data for word alignments
In order to calculate Align
Prec
and Align
Recall
for the different paraphrasing techniques, we had
them automatically align the 500 manually aligned
sentence pairs in our test sets.
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
Align
Prec
.62 .65 .73
Align
Recall
.11 .10 .46
LB-Precision .14 .33 .68
Rel-Recall .07 .03 .01
Table 2: Summary results for scoring the different
paraphrasing techniques using our proposed auto-
matic evaluations.
Bo Pang provided syntactic alignments for the
500 sentence pairs. The word lattices combine the
groups of sentences. When measuring alignment
quality, we took pains to try to limit the extracted
phrase pairs to those which occurred in each sen-
tence pair, but we acknowledge that our methodol-
ogy may be flawed.
We created training data for the monolingual
statistical machine translation method using all
pairwise combination of eleven English transla-
tions in LDC2002T01. All combinations of the
eleven translations of the 993 sentences in that
corpus resulted in 109,230 sentence pairs with
3,266,769 words on each side. We used this data
to train an alignment model, and applied it to the
500 sentence pairs in our test set.
We used the parallel corpus method to align
each pair of English sentences by creating interme-
diate alignments through their Chinese source sen-
tences. The bilingual word alignment model was
trained on a Chinese-English parallel corpus from
the NIST MT Evaluation consisting of 40 million
words. This was used to align the 550 Chinese-
English sentence pairs constructed from the test
set.
7.2 Training data for precision and recall
Each of the paraphrasing methods generated para-
phrases for LB-Precision and Rel-Recall us-
ing larger training sets of data than for the align-
ments. For the syntax-based alignment method,
we excluded the 50 word lattices corresponding
to the test set. We used the remaining 849 lat-
tices for the LDC multiple translation corpus.
For the monolingual statistical machine transla-
tion method, we downloaded the Microsoft Re-
search Paraphrase Phrase Table, which contained
paraphrases for nearly 9 million phrases, gener-
102
ated from the method described in Quirk et al
(2004). For the parallel corpus method, we de-
rived paraphrases from the entire Europarl corpus,
which contains parallel corpora between English
and 10 other languages, with approximately 30
million words per language. We limited both the
Quirk et al (2004) and the Bannard and Callison-
Burch (2005) paraphrases to those with a probabil-
ity greater than or equal to 1%.
7.3 Results
Table 2 gives a summary of how each of the para-
phrasing techniques scored using the four different
automatic metrics. The precision of their align-
ments was in the same ballpark, with each para-
phrasing method reaching above 60%. The mono-
lingual SMT method vastly outstripped the others
in terms of recall and therefore seems to be the
best on the simplified task of aligning paraphrases
within pairs of equivalent sentences.
For the task of generating paraphrases from un-
restricted resources, the monolingual SMT method
again had the highest precision, although time
time its recall was quite low. The 500 manually
aligned sentence pairs contained 14,078 unique
paraphrases for phrases of 5 words or less. The
monolingual SMT method only posited 230 para-
phrases with 156 of them being correct. By con-
trast, the syntactic alignment method posited 1,213
with 399 correct, and the parallel corpus method
posited 6,914 with 998 correct. Since the refer-
ence lists are incomplete by their very nature, the
LB-Precision score gives a lower-bound on the
precision, and the Rel-Recall gives recall only
with respect to the partial list of paraphrases.
Table 3 gives the performance of the differ-
ent paraphrasing techniques for different phrase
lengths.
8 Conclusions
In this paper we defined a number of automatic
scores for data-driven approaches to paraphrasing,
which we collectively dub ?ParaMetric?. We dis-
cussed the inherent difficulties in automatically as-
sessing paraphrase quality. These are due primar-
ily to the fact that it is exceedingly difficult to
create an exhaustive list of paraphrases. To ad-
dress this problem, we introduce an artificial task
of aligning paraphrases within pairs of equivalent
English sentences, which guarantees accurate pre-
cision and recall numbers. In order to measure
alignment quality, we create a set of gold standard
alignments. While the creation of this data does
require some effort, it seems to be a manageable
amount, and the inter-annotator agreement seems
reasonable.
Since alignment is not perfectly matched with
what we would like automatic paraphrasing tech-
niques to do, we also use the gold standard align-
ment data to measure a lower bound on the preci-
sion of a method?s paraphrases, as well as its recall
relative to the limited set of paraphrases. Future
studies should examine how well these scores rank
different paraphrasing methods when compared to
human judgments. Follow up work should inves-
tigate the number of equivalent English sentences
that are required for reasonably complete lists of
paraphrases. In this work we aligned sets of eleven
different English sentences, but we acknowledge
that such a data set is rare and might make it dif-
ficult to port this method to other domains or lan-
guages.
The goal of this work is to develop a set of
scores that both allows different paraphrasing tech-
niques to be compared objectively and provides an
easily repeatable method for automatically evalu-
ating paraphrases. This has hitherto not been pos-
sible. The availability of an objective, automatic
evaluation metric for paraphrasing has the poten-
tial to impact research in the area in a number of
ways. It not only allows for the comparison of dif-
ferent approaches to paraphrasing, as shown in this
paper, but also provides a way to tune the parame-
ters of a single system in order to optimize its qual-
ity.
Acknowledgments
The authors are grateful to Bo Pang for providing
the word lattices from her method, to Stefan Rie-
zler for his comments on an early draft of this pa-
per, and to Michelle Bland for proofreading. This
work was supported by the National Science Foun-
dation under Grant No. 0713448. The views and
findings are the authors? alone.
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-2005), Ann Ar-
bor, Michigan.
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
103
Align
Prec
Align
Recall
LB-Precision Rel-Recall
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
P
a
r
a
l
l
e
l
C
o
r
p
o
r
a
S
y
n
t
a
c
t
i
c
A
l
i
g
n
m
e
n
t
M
o
n
o
l
i
n
g
u
a
l
S
M
T
Length = 1 .54 .48 .64 .24 .18 .56 .15 .25 .59 .20 .16 .02
Length ? 2 .56 .56 .69 .19 .13 .52 .15 .31 .66 .18 .10 .03
Length ? 3 .59 .60 .71 .14 .12 .49 .15 .32 .66 .13 .06 .02
Length ? 4 .60 .63 .72 .12 .11 .48 .14 .33 .68 .09 .04 .01
Length ? 5 .62 .65 .73 .11 .10 .46 .14 .33 .68 .07 .03 .01
Table 3: Results for paraphrases of continuous subphrases of various lengths.
ing multiple-sequence alignment. In Proceedings of
HLT/NAACL-2003, Edmonton, Alberta.
Barzilay, Regina and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2001).
Barzilay, Regina. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Gener-
ation. Ph.D. thesis, Columbia University, New York.
Brown, Peter, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?311, June.
Callison-Burch, Chris, Trevor Cohn, and Mirella Lap-
ata. 2006a. Annotation guidelines for paraphrase
alignment. Tech report, University of Edinburgh.
Callison-Burch, Chris, Philipp Koehn, and Miles Os-
borne. 2006b. Improved statistical machine
translation using paraphrases. In Proceedings of
HLT/NAACL-2006, New York, New York.
Cohn, Trevor, Chris Callison-Burch, and Mirella Lap-
ata. to appear. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Compu-
tational Linguistics.
Dolan, Bill, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Ibrahim, Ali, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the Second Inter-
national Workshop on Paraphrasing (ACL 2003).
Kupper, Lawrence L. and Kerry B. Hafner. 1989. On
assessing interrater agreement for multiple attribute
responses. Biometrics, 45(3):957?967.
Lin, Dekang and Patrick Pantel. 2001. Discovery of
inference rules from text. Natural Language Engi-
neering, 7(3):343?360.
Miller, George A. 1990. Wordnet: An on-line lexical
database. Special Issue of the International Journal
of Lexicography, 3(4).
Nenkova, Ani, Rebecca Passonneau, and Kathleen
McKeown. 2007. The pyramid method: incorporat-
ing human content selection variation in summariza-
tion evaluation. ACM Transactions on Speech and
Language Processing, 4(2).
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Och, Franz Josef. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen Department of
Computer Science, Aachen, Germany.
Pang, Bo, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences.
In Proceedings of HLT/NAACL-2003, Edmonton,
Alberta.
Papineni, Kishore, Salim Roukos, ToddWard, andWei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2002), Philadelphia, Penn-
sylvania.
Quirk, Chris, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2004), Barcelona, Spain.
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007), Prague, Czech Republic.
104
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137?144
Manchester, August 2008
Sentence Compression Beyond Word Deletion
Trevor Cohn and Mirella Lapata
School of Informatics
University of Edinburgh
{tcohn,mlap}@inf.ed.ac.uk
Abstract
In this paper we generalise the sen-
tence compression task. Rather than sim-
ply shorten a sentence by deleting words
or constituents, as in previous work, we
rewrite it using additional operations such
as substitution, reordering, and insertion.
We present a new corpus that is suited
to our task and a discriminative tree-to-
tree transduction model that can naturally
account for structural and lexical mis-
matches. The model incorporates a novel
grammar extraction method, uses a lan-
guage model for coherent output, and can
be easily tuned to a wide range of compres-
sion specific loss functions.
1 Introduction
Automatic sentence compression can be broadly
described as the task of creating a grammatical
summary of a single sentence with minimal in-
formation loss. It has recently attracted much at-
tention, in part because of its relevance to appli-
cations. Examples include the generation of sub-
titles from spoken transcripts (Vandeghinste and
Pan, 2004), the display of text on small screens
such as mobile phones or PDAs (Corston-Oliver,
2001), and, notably, summarisation (Jing, 2000;
Lin, 2003).
Most prior work has focused on a specific
instantiation of sentence compression, namely
word deletion. Given an input sentence of
words, w
1
, w
2
. . . w
n
, a compression is formed
by dropping any subset of these words (Knight
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
and Marcu, 2002). The simplification renders the
task computationally feasible, allowing efficient
decoding using a dynamic program (Knight and
Marcu, 2002; Turner and Charniak, 2005; McDon-
ald, 2006). Furthermore, constraining the problem
to word deletion affords substantial modeling flex-
ibility. Indeed, a variety of models have been suc-
cessfully developed for this task ranging from in-
stantiations of the noisy-channel model (Knight
and Marcu, 2002; Galley and McKeown, 2007;
Turner and Charniak, 2005), to large-margin learn-
ing (McDonald, 2006; Cohn and Lapata, 2007),
and Integer Linear Programming (Clarke, 2008).
However, the simplification also renders the task
somewhat artificial. There are many rewrite opera-
tions that could compress a sentence, besides dele-
tion, including reordering, substitution, and inser-
tion. In fact, professional abstractors tend to use
these operations to transform selected sentences
from an article into the corresponding summary
sentences (Jing, 2000).
Therefore, in this paper we consider sentence
compression from a more general perspective and
generate abstracts rather than extracts. In this
framework, the goal is to find a summary of the
original sentence which is grammatical and con-
veys the most important information without nec-
essarily using the same words in the same or-
der. Our task is related to, but different from,
paraphrase extraction (Barzilay, 2003). We must
not only have access to paraphrases (i.e., rewrite
rules), but also be able to combine them in order to
generate new text, while attempting to produce a
shorter resulting string. Quirk et al (2004) present
an end-to-end paraphrasing system inspired by
phrase-based machine translation that can both ac-
quire paraphrases and use them to generate new
strings. However, their model is limited to lexical
substitution ? no reordering takes place ? and is
137
lacking the compression objective.
Once we move away from extractive compres-
sion we are faced with two problems. First, we
must find an appropriate training set for our ab-
stractive task. Compression corpora are not natu-
rally available and existing paraphrase corpora do
not normally contain compressions. Our second
problem concerns the modeling task itself. Ideally,
our learning framework should handle structural
mismatches and complex rewriting operations.
In what follows, we first present a new cor-
pus for abstractive compression which we created
by having annotators compress sentences while
rewriting them. Besides obtaining useful data for
modeling purposes, we also demonstrate that ab-
stractive compression is a meaningful task. We
then present a tree-to-tree transducer capable of
transforming an input parse tree into a compressed
parse tree. Our approach is based on synchronous
tree substitution grammar (STSG, Eisner (2003)),
a formalism that can account for structural mis-
matches, and is trained discriminatively. Specifi-
cally, we generalise the model of Cohn and Lapata
(2007) to our abstractive task. We present a novel
tree-to-tree grammar extraction method which ac-
quires paraphrases from bilingual corpora and en-
sure coherent output by including a ngram lan-
guage model as a feature. We also develop a num-
ber of loss functions suited to the abstractive com-
pression task. We hope that some of the work de-
scribed here might be of relevance to other gen-
eration tasks such as machine translation (Eisner,
2003), multi-document summarisation (Barzilay,
2003), and text simplification (Carroll et al, 1999).
2 Abstractive Compression Corpus
A stumbling block to studying abstractive sentence
compression is the lack of widely available corpora
for training and testing. Previous work has been
conducted almost exclusively on Ziff-Davis, a cor-
pus derived automatically from document abstract
pairs (Knight and Marcu, 2002), or on human-
authored corpora (Clarke, 2008). Unfortunately,
none of these data sources are suited to our prob-
lem since they have been produced with a sin-
gle rewriting operation, namely word deletion. Al-
though there is a greater supply of paraphrasing
corpora, such as the Multiple-Translation Chinese
(MTC) corpus
1
and theMicrosoft Research (MSR)
Paraphrase Corpus (Quirk et al, 2004), they are
also not ideal, since they have not been created
1
Available by the LDC, Catalog Number LDC2002T01,
ISBN 1-58563-217-1.
with compression in mind. They contain ample
rewriting operations, however they do not explic-
itly target information loss.
For the reasons just described, we created our
own corpus. We collected 30 newspaper articles
(575 sentences) from the British National Corpus
(BNC) and the American News Text corpus, for
which we obtained manual compressions. In or-
der to confirm that the task was feasible, five of
these documents were initially compressed by two
annotators (not the authors). The annotators were
given instructions that explained the task and de-
fined sentence compression with the aid of exam-
ples. They were asked to paraphrase while preserv-
ing the most important information and ensuring
the compressed sentences remained grammatical.
They were encouraged to use any rewriting opera-
tions that seemed appropriate, e.g., to delete words,
add new words, substitute them or reorder them.
Assessing inter-annotator agreement is notori-
ously difficult for paraphrasing tasks (Barzilay,
2003) since there can be many valid outputs for
a given input. Also our task is doubly subjective
in deciding which information to remove from the
sentence and how to rewrite it. In default of an
agreement measure that is well suited to the task
and takes both decisions into account, we assessed
them separately. We first examined whether the an-
notators compressed at a similar level. The com-
pression rate was 56% for one annotator and 54%
for the other.
2
We also assessed whether they
agreed in their rewrites by measuring BLEU (Pap-
ineni et al, 2002). The inter-annotator BLEU score
was 23.79%, compared with the source agreement
BLEU of only 13.22%. Both the compression rate
and BLEU score indicate that the task is well-
defined and the compressions valid. The remain-
ing 25 documents were compressed by a single an-
notator to ensure consistency. All our experiments
used the data from this annotator.
3
Table 1 illustrates some examples from our cor-
pus. As can be seen, some sentences contain a sin-
gle rewrite operation. For instance, a PP is para-
phrased with a genitive (see (1)), a subordinate
clause with a present participle (see (2)), a passive
sentence with an active one (see (3)). However, in
most cases many rewrite decisions take place all
at once. Consider sentence (4). Here, the conjunc-
tion high winds and snowfalls is abbreviated to
2
The term ?compression rate? refers to the percentage of
words retained in the compression.
3
Available from http://homepages.inf.ed.ac.uk/
tcohn/paraphrase.
138
1a. The future of the nation is in your hands.
1b. The nation?s future is in your hands.
2a. As he entered a polling booth in Katutura, he said.
2b. Entering a polling booth in Katutura, he said.
3a. Mr Usta was examined by Dr Raymond Crockett, a
Harley Street physician specialising in kidney disease.
3b. Dr Raymond Crockett, a Harley Street physician, ex-
amined Mr Usta.
4a. High winds and snowfalls have, however, grounded
at a lower level the powerful US Navy Sea Stallion
helicopters used to transport the slabs.
4b. Bad weather, however, has grounded the helicopters
transporting the slabs.
5a. To experts in international law and relations, the US
action demonstrates a breach by a major power of in-
ternational conventions.
5b. Experts say the US are in breach of international con-
ventions.
Table 1: Compression examples from our corpus; (a) sen-
tences are the source, (b) sentences the target.
bad weather and the infinitive clause to transport
to the present participle transporting. Note that the
prenominal modifiers US Navy Sea Stallion and
the verb used have been removed. In sentence (5),
the verb say is added and the NP a breach by a
major power of international conventions is para-
phrased by the sentence the US are in breach of
international conventions.
3 Basic Model
Our work builds on the model developed by Cohn
and Lapata (2007). They formulate sentence com-
pression as a tree-to-tree rewriting task. A syn-
chronous tree substitution grammar (STSG, Eisner
(2003)) licenses the space of all possible rewrites.
Each grammar rule is assigned a weight, and
these weights are learnt in discriminative training.
For prediction, a specialised generation algorithm
finds the best scoring compression using the gram-
mar rules. Cohn and Lapata apply this model to ex-
tractive compression with state-of-the-art results.
This model is appealing for our task for several
reasons. Firstly, the synchronous grammar pro-
vides expressive power to model consistent syn-
tactic effects such as reordering, changes in non-
terminal categories and lexical substitution. Sec-
ondly, it is discriminatively trained, which allows
for the incorporation of all manner of powerful fea-
tures. Thirdly, the learning framework can be tai-
lored to the task by choosing an appropriate loss
function. In the following we describe their model
in more detail with emphasis on the synchronous
grammar, the model structure, and the prediction
and training algorithms. Section 4 presents our ex-
tensions and modifications.
Grammar The grammar defines a space of
tree pairs over uncompressed and compressed sen-
Grammar rules:
?S, S? ? ?NP
1
VBD
2
NP
3
, NP
1
VBD
2
NP
3
?
?S, S? ? ?NP
1
VBD
2
NP
3
, NP
3
was VBN
2
by NP
1
?
?NP, NP? ? ?he, him?
?NP, NP? ? ?he, he?
?NP, NP? ? ?he, Peter?
?VBD, VBN? ? ?sang, sung?
?NP, NP? ? ?a song, a song?
Input tree:
[S [NP He
NP
[VP sang
VBD
[NP a
DT
song
NN
]]]
Output trees:
[S [NP He] [VP sang [NP a song]]]
[S [NP Him] [VP sang [NP a song]]]
[S [NP Peter] [VP sang [NP a song]]]
[S [NP A song] [VP was [VP sung [PP by he]]]]
[S [NP A song] [VP was [VP sung [PP by him]]]]
[S [NP A song] [VP was [VP sung [PP by Peter]]]]
Figure 1: Example grammar and the output trees it licences
for an input tree. The numbered boxes in the rules denote
linked variables. Pre-terminal categories are not shown for the
output trees for the sake of brevity.
tences, which we refer to henceforth as the source
and target. We use the grammar to find the set of
sister target sentences for a given source sentence.
Figure 1 shows a toy grammar and the set of possi-
ble target (output) trees for the given source (input)
tree. Each output tree is created by applying a se-
ries of grammar rules, where each rule matches a
fragment of the source and creates a fragment of
the target tree. A rule in the grammar consists of
a pair of elementary trees and a mapping between
the variables (frontier non-terminals) in both trees.
A derivation is a sequence of rules yielding a target
tree with no remaining variables.
Cohn and Lapata (2007) extract a STSG from
a parsed, word-aligned corpus of source and tar-
get sentences. Specifically, they extract the mini-
mal set of synchronous rules which can describe
each tree pair. These rules are minimal in the sense
that they cannot be made smaller (e.g., by replac-
ing a subtree with a variable) while still honouring
the word-alignment.
Decoding The grammar allows us to search
for all sister trees for a given tree. The decoder
maximises over this space:
y
?
=argmax
y:S(y)=x
?(y) (1)
where ?(y) =
?
r?y
??(r, S(y)), ?? (2)
Here x is the source (uncompressed) tree, y
is a derivation which produces the source tree,
S(y) = x, and a target tree, T (y),
4
and r is a gram-
mar rule. The ? function scores the derivation and
4
Equation 1 optimises over derivations rather than target
trees to allow tractable inference.
139
is defined in (2) as a linear function over the rules
used. Each rule?s score is an inner product between
its feature vector, ?(r,y
S
), and the model parame-
ters, ?. The feature functions are set by hand, while
the model parameters are learned in training.
The maximisation problem in (1) can be solved
efficiently using a dynamic program. Derivations
will have common sub-structures whenever they
transduce the same source sub-tree into a target
sub-tree. This is captured in a chart, leading to
an efficient bottom-up algorithm. The asymptotic
time complexity of this search is O(SR) where S
is the number of source nodes andR is the number
of rules matching a given node.
Training The model is trained using
SVM
struct
, a large margin method for structured
output problems (Joachims, 2005; Tsochantaridis
et al, 2005). This training method allows the use
of a configurable loss function, ?(y
?
,y), which
measures the extent to which the model?s predic-
tion, y, differs from the reference, y
?
. Central
to training is the search for a derivation which
is both high scoring and has high loss compared
to the gold standard.
5
This requires finding the
maximiser of H(y) in one of:
H
s
= (1? ??(y
?
)??(y), ??)?(y
?
,y)
H
m
= ?(y
?
,y)? ??(y
?
)??(y), ??
(3)
where the subscripts s and m denote slack and
margin rescaling, which are different formulations
of the training problem (see Tsochantaridis et al
(2005) and Taskar et al (2003) for details).
The search for the maximiser of H(y) in (3)
requires the tracking of the loss value. This can
be achieved by extending the decoding algorithm
such that the chart cells also store the loss param-
eters (e.g., for precision, the number of true and
false positives (Joachims, 2005)). Consequently,
this extension leads to a considerably higher time
and space complexity compared to decoding. For
example, with precision loss the time complexity
is O(S
3
R) as each step must consider O(S
2
) pos-
sible loss parameter values.
4 Extensions
In this section we present our extensions of Cohn
and Lapata?s (2007) model. The latter was de-
signed with the simpler extractive compression in
mind and cannot be readily applied to our task.
5
Spurious ambiguity in the grammar means that there are
often many derivations linking the source and target. We fol-
low Cohn and Lapata (2007) by choosing the derivation with
the most rules, which should provide good generalisation.
Grammar It is relatively straightforward to
extract a grammar from our corpus. This grammar
will contain many rules encoding deletions and
structural transformations but there will be many
unobserved paraphrases, no matter how good the
extraction method (recall that our corpus consists
solely of 565 sentences). For this reason, we ex-
tract a grammar from our abstractive corpus in the
manner of Cohn and Lapata (2007) (see Section 5
for details) and augment it with a larger gram-
mar obtained from a parallel bilingual corpus. Cru-
cially, our second grammar will not contain com-
pression rules, just paraphrasing ones. We leave it
to the model to learn which rules serve the com-
pression objective.
Our paraphrase grammar extraction method
uses bilingual pivoting to learn paraphrases over
syntax tree fragments, i.e., STSG rules. Pivoting
treats the paraphrasing problem as a two-stage
translation process. Some English text is translated
to a foreign language, and then translated back into
English (Bannard and Callison-Burch, 2005):
p(e
?
|e) =
?
f
p(e
?
|f)p(f |e) (4)
where p(f |e) is the probability of translating
an English string e into a foreign string f and
p(e
?
|f) the probability of translating the same for-
eign string into some other English string e
?
. We
thus obtain English-English translation probabili-
ties p(e
?
|e) by marginalizing out the foreign text.
Instead of using strings (Bannard and Callison-
Burch, 2005), we use elementary trees on the En-
glish side, resulting in a monolingual STSG. We
obtain the elementary trees and foreign strings us-
ing the GKHM algorithm (Galley et al, 2004).
This takes as input a bilingual word-aligned corpus
with trees on one side, and finds the minimal set
of tree fragments and their corresponding strings
which is consistent with the word alignment. This
process is illustrated in Figure 2 where the aligned
pair on the left gives rise to the rules shown on
the right. Note that the English rules and for-
eign strings shown include variable indices where
they have been generalised. We estimate p(f |e)
and p(e
?
|f) from the set of tree-to-string rules
and then then pivot each tree fragment to produce
STSG rules. Figure 3 illustrates the process for the
[VP does not VP] fragment.
Modeling and Decoding Our grammar is
much larger and noisier than a grammar extracted
solely for deletion-based compression. So, in or-
der to encourage coherence and inform lexical se-
140
SNP VP
VBZ
does
RB
goHe not
ne pasIl va
PRP VP
NP
He
Il
PRP
go
vaVP
VP
VBZ
does
RB
not
ne    pas
VP
S
NP VP
       
1
2
1
1
1 2
Figure 2: Tree-to-string grammar extraction using the GHKM
algorithm, showing the aligned sentence pair and the resulting
rules as tree fragments and their matching strings. The boxed
numbers denote variables.
VP
VBZ
does
RB
not
ne     pas
VP
n ' 
ne
ne peut
...
VP
MD
will
RB
not
VB
VP
VBP
do
RB
not
VB
1
1
1
1
1
1
1
Figure 3: Pivoting the [VP does not VP] fragment.
lection we incorporate a ngram language model
(LM) as a feature. This requires adapting the scor-
ing function, ?, in (2) to allow features over target
ngrams:
?(y) =
?
r?y
??(r, S(y)), ??+
?
m?T (y)
??(m,S(y)), ??
(5)
where m are the ngrams and ? is a new fea-
ture function over these ngrams (we use only one
ngram feature: the trigram log-probability). Sadly,
the scoring function in (5) renders the chart-based
search used for training and decoding intractable.
In order to provide sufficient context to the chart-
based algorithm, we must also store in each chart
cell the n ? 1 target tokens at the left and right
edges of its yield. This is equivalent to using as
our grammar the intersection between the original
grammar and the ngram LM (Chiang, 2007), and
increases the decoding complexity to an infeasible
O(SRL
2(n?1)V
)whereL is the size of the lexicon.
We adopt a popular approach in syntax-inspired
machine translation to address this problem (Chi-
ang, 2007). The idea is to use a beam-search over
the intersection grammar coupled with the cube-
pruning heuristic. The beam limits the number of
items in a given chart cell to a fixed constant, re-
gardless of the number of possible LM contexts
and non-terminal categories. Cube-pruning further
limits the number of items considered for inclu-
sion in the beam, reducing the time complexity
to a more manageable O(SRBV ) where B is the
beam size. We refer the interested reader to Chiang
(2007) for details.
Training The extensions to the model in (5)
also necessitate changes in the training proce-
dure. Recall that training the basic model of Cohn
and Lapata (2007) requires finding the maximiser
of H(y) in (3). Their model uses a chart-based al-
gorithm for this purpose. As in decoding we also
use a beam search for training, thereby avoiding
the exponential time complexity of exact search.
The beam search requires an estimate of the qual-
ity for incomplete derivations. We use the margin
rescaling objective, H
m
in (3), and approximate
the loss using the current (incomplete) loss param-
eter values in each chart cell. We use a wide beam
of 200 unique items or 500 items in total to reduce
the impact of the approximation.
Our loss functions are tailored to the task and
draw inspiration from metrics developed for ex-
tractive compression but also for summarisation
and machine translation. They are based on the
Hamming distance over unordered bags of items.
This measures the number of predicted items that
did not appear in the reference, along with a
penalty for short output:
?
hamming
(y
?
,y) = f+max (l ? (t+ f), 0) (6)
where t and f are the number of true and false
positives, respectively, when comparing the pre-
dicted target, y, with the reference, y
?
, and l is
the length of the reference. The second term pe-
nalises short output, as predicting very little or
nothing would otherwise be unpenalised. We have
three Hamming loss functions over: 1) tokens,
2) ngrams (n ? 3), or 3) CFG productions. These
losses all operate on unordered bags and there-
fore might reward erroneous predictions. For ex-
ample, a permutation of the reference tokens has
zero token-loss. The CFG and ngram losses have
overlapping items which encode a partial order,
and therefore are less affected.
In addition, we developed a fourth loss func-
tion to measure the edit distance between the
model?s prediction and the reference, both as bags-
of-tokens. This measures the number of insertions
and deletions. In contrast to the previous loss func-
tions, this requires the true positive counts to be
clipped to the number of occurrences of each type
in the reference. The edit distance is given by:
?
edit
(y
?
,y) = p+ q ? 2
?
i
min(p
i
, q
i
) (7)
where p and q denote the number of target tokens
in the predicted and reference derivation, respec-
tively, and p
i
and q
i
are the counts for type i.
141
?ADJP,NP? ? ?subject [PP to NP
1
], part [PP of NP
1
]? (T)
?ADVP,RB? ? ?as well, also? (T)
?ADJP,JJ? ? ?too little, insufficient? (P)
?S,S? ? ?S
1
and S
2
, S
2
and S
1
? (P)
?NP,NP? ? ?DT
1
NN
2
, DT
1
NN
2
? (S)
?NP,NP? ? ?DT
1
NN
2
, NN
2
? (S)
Table 2: Sample grammar rules extracted from the training
set (T), pivoted set (P) or generated from the source (S).
5 Experimental Design
In this section we present our experimental set-
up for assessing the performance of our model.
We give details on the corpora and grammars we
used, model parameters and features,
6
the baseline
used for comparison with our approach, and ex-
plain how our system output was evaluated.
Grammar Extraction Our grammar used
rules extracted directly from our compression cor-
pus (the training partition, 480 sentences) and a
bilingual corpus (see Table 2 for examples). The
former corpus was word-aligned using the Berke-
ley aligner (Liang et al, 2006) initialised with
a lexicon of word identity mappings, and parsed
with Bikel?s (2002) parser. From this we extracted
grammar rules following the technique described
in Cohn and Lapata (2007). For the pivot grammar
we use the French-English Europarl v2 which con-
tains approximately 688K sentences. Again, the
corpus was aligned using the Berkeley aligner and
the English side was parsed with Bikel?s parser. We
extracted tree-to-string rules using our implemen-
tation of the GHKM method. To ameliorate the ef-
fects of poor alignments on the grammar, we re-
moved singleton rules before pivoting.
In addition to the two grammars described, we
scanned the source trees in the compression cor-
pus and included STSG rules to copy each CFG
production or delete up to two of its children. This
is illustrated in Table 2 where the last two rules are
derived from the CFG production NP?DT NN in
the source tree. All trees are rooted with a distin-
guished TOP non-terminal which allows the ex-
plicit modelling of sentence spanning sub-trees.
These grammars each had 44,199 (pivot), 7,813
(train) and 22,555 (copy) rules. We took their
union, resulting in 58,281 unique rules and 13,619
unique source elementary trees.
Model Parameters Our model was trained
on 480 sentences, 36 sentences were used for de-
velopment and 59 for testing. We used a variety
of syntax-based, lexical and compression-specific
6
The software and corpus can be downloaded from
http://homepages.inf.ed.ac.uk/tcohn/paraphrase.
For every rule:
origin of rule
for each origin, o: log p
o
(s, t), log p
o
(s|t), log p
o
(t|s)
s
R
, t
R
, s
R
? t
R
s, t, s ? t, s = t
both s and t are pre-terminals and s = t or s 6= t
number of terminals/variables/dropped variables
ordering of variables as numbers/non-terminals
non-terminal sequence of vars identical after reordering
pre-terminal or terminal sequences are identical
number/identity of common/inserted/dropped terminals
source is shorter/longer than target
target is a compression of the source using deletes
For every ngram :
log p(w
i
|w
i?1
i?(n?1)
)
Table 3: The feature set. Rules were drawn from the training
set, bilingual pivoting and directly from the source trees. s and
t are the source and target elementary trees in a rule, the sub-
script
R
references the root non-terminal, w are the terminals
in the target tree.
features (196,419 in total). These are summarised
in Table 3. We also use a trigram language model
trained on the BNC (100 million words) using the
SRI Language Modeling toolkit (Stolcke, 2002),
with modified Kneser-Ney smoothing.
An important parameter in our modeling frame-
work is the choice of loss function. We evaluated
the loss functions presented in Section 4 on the de-
velopment set. We ran our system for each of the
four loss functions and asked two human judges
to rate the output on a scale of 1 to 5. The Ham-
ming loss over tokens performed best with a mean
rating of 3.18, closely followed by the edit dis-
tance (3.17). We chose the former over the latter
as it is less coarsely approximated during search.
Baseline There are no existing models that
can be readily trained on our abstractive com-
pression data. Instead, we use Cohn and Lapata?s
(2007) extractive model as a baseline. The latter
was trained on an extractive compression corpus
drawn from the BNC (Clarke, 2008) and tuned
to provide a similar compression rate to our sys-
tem. Note that their model is a strong baseline:
it performed significantly better than competitive
approaches (McDonald, 2006) across a variety of
compression corpora.
Evaluation Methodology Sentence compres-
sion output is commonly evaluated by eliciting
human judgments. Following Knight and Marcu
(2002), we asked participants to rate the grammati-
cality of the target compressions and howwell they
preserved the most important information from
the source. In both cases they used a five point
rating scale where a high number indicates bet-
ter performance. We randomly selected 30 sen-
tences from the test portion of our corpus. These
142
Models Grammaticality Importance CompR
Extract 3.10
?
2.43
?
82.5
Abstract 3.38
?
2.85
?
? 79.2
Gold 4.51 4.02 58.4
Table 4: Mean ratings on compression output elicited by hu-
mans;
?
: significantly different from the gold standard;
?
: sig-
nificantly different from the baseline.
sentences were compressed automatically by our
model and the baseline. We also included gold
standard compressions. Our materials thus con-
sisted of 90 (30 ? 3) source-target sentences. We
collected ratings from 22 unpaid volunteers, all
self reported native English speakers. Both studies
were conducted over the Internet using a custom
built web interface.
6 Results
Our results are summarised in Table 4, where we
show the mean ratings for our system (Abstract),
the baseline (Extract), and the gold standard. We
first performed an Analysis of Variance (ANOVA)
to examine the effect of different system compres-
sions. The ANOVA revealed a reliable effect on
both grammaticality and importance (significant
over both subjects and items (p < 0.01)).
We next examined in more detail between-
system differences. Post-hoc Tukey tests revealed
that our abstractive model received significantly
higher ratings than the baseline in terms of impor-
tance (? < 0.01). We conjecture that this is due
to the synchronous grammar we employ which
is larger and more expressive than the baseline.
In the extractive case, a word sequence is either
deleted or retained. We may, however, want to re-
tain the meaning of the sequence while rendering
the sentence shorter, and this is precisely what our
model can achieve, e.g., by allowing substitutions.
As far as grammaticality is concerned, our abstrac-
tive model is numerically better than the extrac-
tive baseline but the difference is not statistically
significant. Note that our model has to work a lot
harder than the baseline to preserve grammatical-
ity since we allow arbitrary rewrites which may
lead to agreement or tense mismatches, and selec-
tional preference violations. The scope for errors is
greatly reduced when performing solely deletions.
Finally, both the abstractive and extractive out-
puts are perceived as significantly worse than the
gold standard both in terms of grammaticality
and importance (? < 0.01). This is not surpris-
ing: human-authored compressions are more fluent
and tend to omit genuinely superfluous informa-
tion. This is also mirrored in the compression rates
shown in Table 4. When compressing, humans em-
O: Kurtz came from Missouri, and at the age of 14, hitch-
hiked to Los Angeles seeking top diving coaches.
E: Kurtz came from Missouri, and at 14, hitch-hiked to Los
Angeles seeking top diving coaches.
A: Kurtz hitch-hiked to Los Angeles seeking top diving
coaches.
G: Kurtz came from Missouri, and at 14, hitch-hiked to Los
Angeles seeking diving coaches.
O: The scheme was intended for people of poor or moderate
means.
E: The scheme was intended for people of poor means.
A: The scheme was planned for poor people.
G: The scheme was intended for the poor.
O: He died last Thursday at his home from complications
following a fall, said his wife author Margo Kurtz.
E: He died last at his home from complications following a
fall, said wife, author Margo Kurtz.
A: His wife author Margo Kurtz died from complications
after a decline.
G: He died from complications following a fall.
O: But a month ago, she returned to Britain, taking the chil-
dren with her.
E: She returned to Britain, taking the children.
A: But she took the children with him.
G: But she returned to Britain with the children.
Table 5: Compression examples including human and system
output (O: original sentence, E: Extractive model, A: Abstrac-
tive model, G: gold standard)
ploy not only linguistic but also world knowledge
which is not accessible to our model. Although the
system can be forced to match the human compres-
sion rate, the grammaticality and information con-
tent both suffer. More sophisticated features could
allow the system to narrow this gap.
We next examined the output of our system in
more detail by recording the number of substitu-
tions, deletions and insertions it performed on the
test data. Deletions accounted for 67% of rewrite
operations, substitutions for 27%, and insertions
for 6%. Interestingly, we observe a similar ratio
in the human compressions. Here, deletions are
also the most common rewrite operation (69%) fol-
lowed by substitutions (24%), and insertions (7%).
The ability to perform substitutions and insertions
increases the compression potential of our system,
but can also result in drastic meaning changes. In
most cases (63%) the compressions produced by
our system did not distort the meaning of the orig-
inal. Humans are clearly better at this, 96.5% of
their compressions were meaning preserving.
We illustrate example output of our system in
Table 5. For comparison we also present the gold
standard compressions and baseline output. In the
first sentence the system rendered Kurtz the sub-
ject of hitch-hiked. At the same time it deleted the
verb and its adjunct from the first conjunct (came
from Missouri ) as well as the temporal modi-
fier at the age of 14 from the second conjunct.
The second sentence shows some paraphrasing:
the verb intended is substituted with planned and
143
poor is now modifying people rather than means.
In the third example, our system applies multi-
ple rewrites. It deletes last Thursday at his home,
moves wife author Margo Kurtz to the subject po-
sition, and substitutes fall with decline. Unfortu-
nately, the compressed sentence expresses a rather
different meaning from the original. It is not Margo
Kurtz who died but her husband. Finally, our last
sentence illustrates a counter-intuitive substitution,
the pronoun her is rewritten as him. This is because
they share the French translation lui and thus piv-
oting learns to replace the less common word (in
legal corpora) her with him. This problem could
be addressed by pivoting over multiple bitexts with
different foreign languages.
Possible extensions and improvements to the
current model are many and varied. Firstly, as
hinted at above, the model would benefit from ex-
tensive feature engineering, including source con-
ditioned features and ngram features besides the
LM. A richer grammar would also boost perfor-
mance. This could be found by pivoting over more
bitexts in many foreign languages or making use
of existing or paraphrase corpora. Finally, we plan
to apply the model to other paraphrasing tasks in-
cluding fully abstractive document summarisation
(Daum?e III and Marcu, 2002).
Acknowledgements
The authors acknowledge the support of EPSRC
(grants GR/T04540/01 and GR/T04557/01).
Special thanks to Phil Blunsom, James Clarke and
Miles Osborne for their insightful suggestions.
References
C. Bannard, C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proceedings of
the 43rd ACL, 255?262, Ann Arbor, MI.
R. Barzilay. 2003. Information Fusion for Multi-
Document Summarization: Praphrasing and Gener-
ation. Ph.D. thesis, Columbia University.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of the HLT, 24?27, San Diego, CA.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,
J. Tait. 1999. Simplifying text for language impaired
readers. In Proceedings of the 9th EACL, 269?270,
Bergen, Norway.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
J. Clarke. 2008. Global Inference for Sentence Com-
pression: An Integer Linear Programming Approach.
Ph.D. thesis, University of Edinburgh.
T. Cohn, M. Lapata. 2007. Large margin synchronous
generation and its application to sentence compres-
sion. In Proceedings of the EMNLP/CoNLL, 73?82,
Prague, Czech Republic.
S. Corston-Oliver. 2001. Text Compaction for Dis-
play on Very Small Screens. In Proceedings of the
NAACL Workshop on Automatic Summarization, 89?
98, Pittsburgh, PA.
H. Daum?e III, D. Marcu. 2002. A noisy-channel model
for document compression. In Proceedings of the
40th ACL, 449?456, Philadelphia, PA.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of
the ACL Interactive Poster/Demonstration Sessions,
205?208, Sapporo, Japan.
M. Galley, K. McKeown. 2007. Lexicalized Markov
grammars for sentence compression. In Proceedings
of the NAACL/HLT, 180?187, Rochester, NY.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004.
What?s in a translation rule? In Proceedings of the
HLT/NAACL, 273?280, Boston, MA.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of the ANLP, 310?
315, Seattle, WA.
T. Joachims. 2005. A support vector method for multi-
variate performance measures. In Proceedings of the
22nd ICML, 377?384, Bonn, Germany.
K. Knight, D. Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
P. Liang, B. Taskar, D. Klein. 2006. Alignment by
agreement. In Proceedings of the HLT/NAACL, 104?
111, New York, NY.
C.-Y. Lin. 2003. Improving summarization perfor-
mance by sentence compression ? a pilot study. In
Proceedings of the 6th International Workshop on
Information Retrieval with Asian Languages, 1?8,
Sapporo, Japan.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proceed-
ings of the 11th EACL, 297?304, Trento, Italy.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th ACL,
311?318, Philadelphia, PA.
C. Quirk, C. Brockett, W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation.
In Proceedings of the EMNLP, 142?149, Barcelona,
Spain.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the ICSLP, Den-
ver, CO.
B. Taskar, C. Guestrin, D. Koller. 2003. Max margin
Markov networks. In Proceedings of NIPS 16.
I. Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun.
2005. Large margin methods for structured and in-
terdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
J. Turner, E. Charniak. 2005. Supervised and unsu-
pervised learning for sentence compression. In Pro-
ceedings of 43rd ACL, 290?297, Ann Arbor, MI.
V. Vandeghinste, Y. Pan. 2004. Sentence compression
for automated subtitling: A hybrid approach. In Pro-
ceedings of the ACL Workshop on Text Summariza-
tion, 89?95, Barcelona, Spain.
144
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 73?82, Prague, June 2007. c?2007 Association for Computational Linguistics
Large Margin Synchronous Generation
and its Application to Sentence Compression
Trevor Cohn and Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, United Kingdom
{tcohn,mlap}@inf.ed.ac.uk
Abstract
This paper presents a tree-to-tree transduc-
tion method for text rewriting. Our model
is based on synchronous tree substitution
grammar, a formalism that allows local dis-
tortion of the tree topology and can thus
naturally capture structural mismatches. We
describe an algorithm for decoding in this
framework and show how the model can
be trained discriminatively within a large
margin framework. Experimental results on
sentence compression bring significant im-
provements over a state-of-the-art model.
1 Introduction
Recent years have witnessed increasing interest in
text-to-text generation methods for many natural
language processing applications ranging from text
summarisation to question answering and machine
translation. At the heart of these methods lies the
ability to perform rewriting operations according to
a set of prespecified constraints. For example, text
simplification identifies which phrases or sentences
in a document will pose reading difficulty for a given
user and substitutes them with simpler alternatives
(Carroll et al, 1999). Sentence compression pro-
duces a summary of a single sentence that retains the
most important information while remaining gram-
matical (Jing, 2000).
Ideally, we would like a text-to-text rewriting sys-
tem that is not application specific. Given a parallel
corpus of training examples, we should be able to
learn rewrite rules and how to combine them in order
to generate new text. A great deal of previous work
has focused on the rule induction problem (Barzilay
andMcKeown, 2001; Pang et al, 2003; Lin and Pan-
tel, 2001; Shinyama et al, 2002), whereas relatively
little emphasis has been placed on the actual gen-
eration task (Quirk et al, 2004). A notable excep-
tion is sentence compression for which end-to-end
rewriting systems are commonly developed (Knight
and Marcu, 2002; Turner and Charniak, 2005; Gal-
ley and McKeown, 2007; Riezler et al, 2003; Mc-
Donald, 2006). The appeal of this task lies in its
simplified formulation as a single rewrite operation,
namely word deletion (Knight and Marcu, 2002).
Solutions to the compression task have been cast
mostly in a supervised learning setting (but see
Clarke and Lapata (2006a), Hori and Furui (2004),
and Turner and Charniak (2005) for unsupervised
methods). Rewrite rules are learnt from a parsed
parallel corpus and subsequently used to find the
best compression from the set of all possible com-
pressions for a given sentence. A common assump-
tion is that the tree structures representing long sen-
tences and their compressions are isomorphic. Con-
sequently, the models are not generally applicable
to other text rewriting problems since they cannot
readily handle structural mismatches and more com-
plex rewriting operations such as substitutions or
insertions. A related issue is that the tree structure
of the compressed sentences is often poor; most al-
gorithms delete words or constituents without pay-
ing too much attention to the structure of the com-
pressed sentence. However, without an explicit gen-
eration mechanism that allows tree transformations,
there is no guarantee that the compressions will have
well-formed syntactic structures. And it will not be
easy to process them for subsequent generation or
analysis tasks.
In this paper we present a text-to-text rewriting
73
model that scales to non-isomorphic cases and can
thus naturally account for structural and lexical di-
vergences. Our approach is inspired by synchronous
tree substitution grammar (STSG, Eisner (2003))
a formalism that allows local distortion of the tree
topology. We show how such a grammar can be in-
duced from a parallel corpus and propose a large
margin model for the rewriting task which can be
viewed as a weighted tree-to-tree transducer. Our
learning framework makes use of the algorithm put
forward by Tsochantaridis et al (2005) which ef-
ficiently learns a prediction function to minimise a
given loss function. Experiments on sentence com-
pression show significant improvements over the
state-of-the-art. Beyond sentence compression and
related text-to-text generation problems (e.g., para-
phrasing), our model is generally applicable to tasks
involving structural mapping. Examples include ma-
chine translation (Eisner, 2003) or semantic parsing
(Zettlemoyer and Collins, 2005).
2 Related Work
Knight and Marcu (2002) proposed a noisy-channel
formulation of sentence compression based on syn-
chronous context-free grammar (SCFG). The lat-
ter is a generalisation of the context-free grammar
(CFG) formalism to simultaneously produce strings
in two languages. In the case of sentence compres-
sion, the grammar rules have two right hand sides,
one corresponding to the source (long) sentence and
the other to its target compression. The synchronous
derivations are learnt from a parallel corpus and their
probabilities are estimated generatively.
Given a long sentence, l, the aim is to find the
corresponding compressed sentence, s, which max-
imises P(s)P(l|s) (here P(s) is the source model
and P(l|s) the channel model.) Modifications of this
model are reported in Turner and Charniak (2005)
and Galley and McKeown (2007) with improved re-
sults. The channel model is limited to tree deletion
and does not allow any type of tree re-organisation.
Non-isomorphic tree structures are common when
translating between languages. It is therefore not
surprising that most previous work on tree rewrit-
ing falls within the realm of machine translation.
Proposals include Eisner?s (2003) synchronous tree
substitution grammar (STSG), Melamed?s (2004)
multitext grammar, and Graehl and Knight?s (2004)
tree-to-tree transducers. Despite differences in for-
malism, all these approaches model the translation
process using tree-based probabilistic transduction
rules. The grammar induction process requires EM
training which can be computationally expensive es-
pecially if all synchronous rules are considered.
Our work formulates sentence compression in the
framework of STSG (Eisner, 2003). We propose a
novel grammar induction algorithm that does not
require EM training and is coupled with a sepa-
rate large margin training process (Tsochantaridis
et al, 2005) for weighting each rule. McDonald
(2006) also presents a sentence compression model
that uses a discriminative large margin algorithm.
However, we differ in two important respects. First,
our generation algorithm is more powerful, perform-
ing complex tree transformations, whereas McDon-
ald only considers simple word deletion. Being tree-
based, the generation algorithm is better able to pre-
serve the grammaticality of the compressed output.
Second, our model can be tuned to a wider range of
loss functions (e.g.,tree-based measures).
3 Problem Formulation
We formulate sentence compression as an instance
of the general problem of learning a mapping from
input patterns x ? X to discrete structured objects
y ? Y . Our training sample consists of a parallel
corpus of input (uncompressed) and output (com-
pressed) pairs (x1,y1) . . .(xn,yn) ? X ? Y and our
task is to predict a target labelled tree y from a
source labelled tree x. As we describe below, y is
not precisely a target tree, but instead derivations
which generate both the source and the target tree.
We model the dependency between x and y as a
weighted STSG. Grammar rules are of the form
?X ,Y ? ? ??,?,?? where ? and ? are elementary
trees composed of a mixture of terminal and non-
terminals rooted with non-terminals X andY respec-
tively, and ? is a set of variable correspondences
between pairs of frontier non-terminals in ? and ?.
A grammar rule specifies that we can substitute the
trees ? and ? for corresponding X and Y nodes in the
source and target trees respectively. For example, the
rule:
?NP, NP? ? ?[DT 1 ADJP NN 2 ]NP, [DT 1 NN 2 ]NP?
74
allows adjective phrases to be dropped from the
source tree within an NP. The indices x are used to
specify the variable correspondences, ?.
Each grammar rule has a score from which the
overall score of a compression y for sentence x
can be derived. These scores are learnt discrimina-
tively using the large margin technique proposed by
Tsochantaridis et al (2005). The synchronous rules
are combined using a chart-based parsing algorithm
(Eisner, 2003) to generate the derivation (i.e., com-
pressed tree) with the highest score.
We begin by describing our STSG generation al-
gorithm in Section 3.1. We next explain how a syn-
chronous grammar is induced from a parallel corpus
of original sentences and their compressions (Sec-
tion 3.2) and give the details of our learning frame-
work (Section 3.3).
3.1 Generation
Generation aims to find the best target tree for a
given source tree using the transformations specified
by the synchronous grammar. (We discuss how we
obtain this grammar in the following section.)
y? =max
y?Y
score(x,y;w) (1)
where y ranges over all target derivations (and there-
fore trees), w is a parameter vector and score(?) is
an objective function measuring the quality of the
derivation. In common with many parsing methods,
we encounter a problem with spurious ambiguity:
i.e., there may be many derivations (sequences of
rule applications) which produce the same target
tree. Ideally we would sum up the scores over all
these derivations, however for the sake of tractability
we instead take the maximum score. This allows us
to pose the maximisation problem over derivations
rather than target trees.
The generation algorithm uses a dynamic pro-
gram defined over the constituents in the source
tree as shown in Figure 1 (see also Eisner (2003)).
The algorithm makes the assumption that the scor-
ing function decomposes with the derivation, such
that a partial score can be evaluated at each step,
i.e., score(x,y;w) = ?r?y score(r;w) where r are
the rules used in the derivation. This method builds
a chart of the best scoring partial derivation for
each source subtree headed by a given target non-
terminal. The inductive step is applied recursively
1: for all nodes, n, in source tree (bottom-up) do
2: for all rules, r with left side matching node, nr = n do
3: s = score(r)
4: for all variables v in r do
5: score = score+ chart[nv,cv]
6: end for
7: update chart[n,cr] with score, s, if better than current
8: end for
9: end for
10: cbest = argmaxc chart[root,c]
11: find best derivation using back-pointers from (root,cbest)
Figure 1: Generation algorithm to find the best
derivation. nr and nv are the source nodes indexed
by the rule?s source side (root and variable), while
cr and cv are the non-terminal categories of the rule?s
target side (root and variable).
is very good and includes ...
AUX RB JJ CC
VPVP
ADJP
VP
Figure 2: Example of a rule application during gen-
eration. The dashed area shows a matching rule for
the VP node.
bottom-up, and involves applying a grammar rule
to a node in the source tree. Rules with substitution
variables in their frontier are scored with reference
to the chart for the matching nodes and target non-
terminal categories. Once the process is complete,
we can read the best score from the chart cell for the
root node, and the best derivation can be constructed
by traversing back-pointers also stored in the chart.
This is illustrated in Figure 2 where the rule
?VP,VP?? ?[[isAUX ADJP 1 ]VP CCVP]VP, [isAUX NP 1 ]VP? is
applied to the top VP node. The score of the result-
ing tree would reference the chart to calculate the
score for the best target tree at the ADJP node with
syntactic category NP.
3.2 Grammar Induction
Our induction algorithm automatically finds gram-
mar rules from a word-aligned parsed parallel cor-
pus. The rules are pairs of elementary trees (i.e., tree
fragments) whose leaf nodes are linked by the word
alignments. These leaves can be either terminal or
non-terminal symbols. Initially, the algorithm ex-
75
tracts tree pairs from word aligned text by choos-
ing aligned constituents in the source and the tar-
get. These pairs are then generalised using subtrees
which are also extracted, resulting in synchronous
rules with variable nodes. The set of aligned tree
pairs are extracted using the alignment template
method (Och and Ney, 2004), constrained to syntac-
tic constituent pairs:
C = {(nS,nT ), (?(s, t) ? A ? s ? Y (nS)? t ? Y (nT ))?
(@(s, t) ? A ? (s ? Y (nS)Y t ? Y (nT )))}
where nS and nT are source and target tree nodes
(subtrees),A = {(s, t)} is the set of word alignments
(pairs of word-indices), Y (?) returns the yield span
for a subtree and Y is the exclusive-or operator.
The next step is to generalise the candidate pairs
by replacing subtrees with variable nodes. We could
fully trust the word alignments and adopt a strat-
egy in which the rules are generalised as much as
possible and thus include little lexicalisation. Fig-
ure 3 shows a simple sentence pair and the result-
ing synchronous rules according to this generalisa-
tion strategy. Alternatively, we could extract every
possible rule by including unlexicalised rules, lexi-
calised rules and their combination. The downside
here is that the total number of possible rules is fac-
torial in the size of the candidate set. We address this
problem by limiting the number of variables and the
recursion depth, and by filtering out singleton rules.
There is no guarantee that the induced rules will
generalise well to a testing set. For example, the test-
ing data may have a rule which was not seen in the
training set (e.g., a new terminal or non terminal).
In this case no rule can be applied and subsequently
generation fails. For this reason we allow the model
to duplicate any CFG production from the source
tree, and uses a feature to flag that this rule was un-
seen in training. These SCFG rules are then merged
with the induced rules and fed into the feature detec-
tion module (see Section 3.3 for details).
3.3 The Large Margin Model
We now describe how the parameters of our STSG
generation system are fit to a supervised training set.
For a given source tree, the space of sister target
trees implied by the synchronous grammar is often
very large, and the majority of these trees are un-
Th
e
do
cu
m
en
ta
tio
n
is ve
ry
go
od
an
d
in
cl
ud
es
a tu
to
ria
l
to ge
t
yo
u
st
ar
te
d
.
Documentation
is
very
good
.
S
.
V
P
V
P
N
P
S
V
P
V
P
S
V
P
V
B
N
N
P
P
R
P
V
B
TON
N
D
T
V
B
Z
C
C
V
P
A
D
JP
JJR
B
A
U
X
N
P
N
N
D
T
S
.
VP
ADJP
JJ
RB
AUX
NP NN
?S,S? ? ?[NP 1 VP 2 . 3 ]S, [NP 1 VP 2 . 3 ]S?
?NP,NP? ? ?[DT NN 1 ]NP, [NN 1 ]NP?
?NN,NN? ? ?documentationNN ,DocumentationNN?
?VP,VP? ? ?VP 1 CC VP,VP 1 ?
?VP,VP? ? ?AUX 1 ADJP 2 ,AUX 1 ADJP 2 ?
?AUX ,AUX? ? ?isAUX , isAUX ?
?ADJP,ADJP? ? ?[RB 1 JJ 2 ]ADJP, [RB 1 JJ 2 ]ADJP?
?RB,RB? ? ?veryRB,veryRB?
?JJ,JJ? ? ?goodADJ ,goodADJ?
?., .? ? ?.., ..?
Figure 3: Induced synchronous grammar from a sen-
tence pair using a strategy that extracts general rules.
grammatical or are poor compressions. The train-
ing procedure learns weights such that the model
can discriminate between these trees and predict a
good target tree. For this we develop a discriminative
training process which learns a weighted tree-to-tree
transducer. Our model is based on Tsochantaridis et
al.?s (2005) framework for learning Support Vector
Machines (SVMs) with structured output spaces, us-
ing the SVMstruct implementation.1 We briefly sum-
marise the approach below; for a more detailed de-
scription we refer the interested reader to Tsochan-
taridis et al (2005).
Traditionally SVMs learn a linear classifier that
separates two or more classes with the largest pos-
sible margin. Analogously, structured SVMs at-
tempt to separate the correct structure from all other
1http://svmlight.joachims.org/svm struct.html
76
structures with a large margin. Given an input in-
stance x, we search for the optimum output y under
the assumption that x and y can be adequately de-
scribed using a combined feature vector representa-
tion ?(x,y). Recall that x are the source trees and y
are synchronous derivations which generate both x
and a target tree.
f (x;w) = argmax
y?Y
?w,?(x,y)? (2)
The goal of the training procedure is to find a param-
eter vector w such that it satisfies the condition:
?i,?y ? Y \yi : ?w,?(xi,yi)??(xi,y)? ? 0 (3)
where xi,yi are the ith training source tree and tar-
get derivation. To obtain a unique solution ? there
will be several parameter vectors w satisfying (3)
if the training instances are linearly separable ?
Tsochantaridis et al (2005) select the w that max-
imises the minimum distance between yi and the
closest runner-up structure.
The framework also incorporates a loss function.
This property is particularly appealing in the context
of sentence compression and generally text-to-text
generation. For example, a compression that differs
from the gold standard with respect to one or two
words should be treated differently from a compres-
sion that bears no resemblance to it. Another impor-
tant factor is the length of the compression. Com-
pressions whose length is similar to the gold stan-
dard should be be preferable to longer or shorter
output. A loss function ?(yi,y) quantifies the accu-
racy of prediction y with respect to the true output
value yi. We give details of the loss functions we
employed for the compression task below.
We are now ready to state the learning objective
for the structured SVM. We use the soft-margin for-
mulation which allows errors in the training set, via
the slack variables ?i:
min
w,?
1
2
||w||2 +
C
n
n
?
i=1
?i, ?i ? 0 (4)
?i,?y ? Y \yi : ?w,??(y)? ? 1?
?i
?(yi,y)
Slack variables ?i are introduced here for each train-
ing example xi, C is a constant that controls the
trade-off between training error minimisation and
margin maximisation, and ??(y) is a shorthand for
?(xi,yi)??(xi,y) (see (3)). Note that slack vari-
ables are rescaled with the inverse loss incurred in
each of the linear constraints.2
The optimisation problem in (4) is approximated
using a polynomial time cutting plane algorithm
(Tsochantaridis et al, 2005). This optimisation cru-
cially relies on finding the constraint incurring the
maximum cost. The cost function for slack rescaling
can be formulated as:
H(y) = (1????i(y),w?)?(yi,y) (5)
In order to adapt this framework to our genera-
tion problem, we must provide the feature map-
ping ?(x,y), a loss function ?(yi,y), and a max-
imiser y? = argmaxy?Y H(y) (see (5)). The following
sections describe how these are instantiated in the
sentence compression task.
Feature Mapping We devised a general feature
set suitable for compression and paraphrasing. Our
feature space is defined over source trees (x) and
target derivations (y). All features apply to a single
grammar rule; a feature vector for a derivation is ex-
pressed as the sum of the feature vectors for each
rule in this derivation.
We make use of syntactic, lexical, and com-
pression specific features. Our simplest syntac-
tic feature is the identity of a synchronous rule.
Specifically, we record its source tree, its target
tree and their combination. We also include rule
frequencies ?(target|source), ?(source|target) and
?(source, target). Another feature records the fre-
quencies of the CFG productions used in the tar-
get side of a rule. This allows the model to learn
the weights of a CFG generation grammar, as a
proxy for a language model. Using scores from a
pre-trained CFG grammar or an n-gram language
model might be preferable when the training sample
is small, however we leave this as future work. Our
last syntactic feature keeps track of the source root
and the target root non-terminals. Our lexical fea-
tures contain the list of tokens in the source yield,
target yield, and both. We also use words as features.
2Alternatively, the loss function can be used to rescale the
margin. This approach is less desirable as it is not scale invari-
ant (Tsochantaridis et al, 2005). We also found empirically that
slack-rescaling slightly outperforms margin rescaling on our
compression task.
77
Finally, we have implemented a set of
compression-specific features. These include a
feature that detects if the yield of the target side
of a synchronous rule is a subset of the yield of
its source. We also take note of the edit operations
(i.e., removal, insertion) required to transform the
source side into the target. Edit operations are
recorded separately for trees and their yields. In
order to encourage compression, we also count the
number of words on the target, the number of rules
used in the derivation and the number of dropped
variables.
Loss Functions The large margin configuration
sketched above is quite modular and in theory a wide
range of loss functions could be specified. Examples
include edit-distance, precision, F-score, BLEU and
tree-based measures. In practice, the loss function
should be compatible with our maximisation algo-
rithm which requires the objective function to de-
compose along the same lines as the tree derivation.3
Given this restriction, we define a loss based
on position-independent unigram precision (Prec)
which penalises errors in the yield independently
for each word. Although fairly intuitive, this loss
is far from ideal. First, it maximally rewards re-
peatedly predicting the same word if the latter is
in the reference target tree. Secondly, it may bias
towards overly short output which drops core in-
formation ? one-word compressions will tend to
have higher precision than longer output. To coun-
teract this, we introduce two brevity penalty mea-
sures (BP) inspired by BLEU (Papineni et al, 2002)
which we incorporate into the loss function, using a
product, loss = 1?Prec ?BP:
BP1 = exp(1?max(1,
r
c
)) (6)
BP2 = exp(1?max(
c
r
,
r
c
))
where r is the reference length and c is the candidate
length.
BP1 is asymmetric, it has value one when c ? r
and decays to zero when c < r. Note that precision
should decay when c > r as extra output will often
not match the reference. BP2 is two-sided: it has
3Optimising non-decompositional loss functions compli-
cates the objective function, which then cannot be solved ef-
ficiently using a dynamic program.
value one when c = r and decays towards zero for
c < r and c > r. In both cases, brevity is assessed
against the gold standard target (not the source) to
allow the system to learn the correct degree of com-
pression from the training data.
Maximisation Algorithm Our algorithm finds the
maximising derivation for H(y) in (5). This deriva-
tion will have a high loss and a high score under the
model, and therefore represents the most-violated
constraint which is then added to the SVM?s work-
ing set of constraints (see (4)).
The standard generation method from Section 3.1
cannot be used without modification to find the best
scoring derivation since it does not account for the
loss function or the gold standard derivation. In-
stead, we stratify the generation chart with the num-
ber of true and false positive tokens predicted, as de-
scribed in Joachims (2005). These contingency val-
ues allow us to compute the precision and brevity
penalty (see (6)) for each complete derivation. This
is then combined with the derivation score and the
gold standard derivation score to give H(y).
The gold standard derivation features, ?(xi,yi),
must be calculated from a derivation linking the
source tree to the gold target tree. As there may
be many such derivations, we find a unique deriva-
tion using the smallest rules possible (for maximum
generality). This is done using a dynamic program,
similar to the inside-outside algorithm used in pars-
ing. Other strategies are also possible, however we
leave this to future work. Finally, we can find the
global maximum H(y) by maximising over all the
root chart entries.
4 Evaluation Set-up
In this section we present our experimental set-up
for assessing the performance of the max margin
model described above. We give details of the cor-
pora used, briefly introduce McDonald?s (2006) sen-
tence compression model used for comparison with
our approach, and explain how system output was
evaluated.
Corpora We evaluated our system on two dif-
ferent corpora. The first is the compression cor-
pus of Knight and Marcu (2002) derived automati-
cally from the document-abstract pairs of the Ziff-
78
Davis corpus. Previous compression work has al-
most exclusively used this corpus. Our experiments
follow Knight and Marcu?s partition of training, test,
and development sets (1,002/36/12 instances). We
also present results on Clarke and Lapata?s (2006a)
Broadcast News corpus.4 This corpus was created
manually (annotators were asked to produce com-
pressions for 50 Broadcast news stories) and poses
more of a challenge than Ziff-Davis. Being a speech
corpus, it often contains incomplete and ungram-
matical utterances and speech artefacts such as dis-
fluencies, false starts and hesitations. Furthermore,
spoken utterances have varying lengths, some are
very wordy whereas others cannot be reduced any
further. Thus a hypothetical compression system
trained on this domain should be able to leave some
sentences uncompressed. Again we used Clarke and
Lapata?s training, test, and development set split
(882/410/78 instances).
Comparison with State-of-the-art We evaluated
our approach against McDonald?s (2006) discrimi-
native model. This model is a good basis for compar-
ison for several reasons. First, it achieves compet-
itive performance with Knight and Marcu?s (2002)
decision tree and noisy channel models. Second, it
also uses large margin learning. Sentence compres-
sion is formulated as a string-to-substring mapping
problem with a deletion-based Hamming loss. Re-
call that our formulation involves a tree-to-tree map-
ping. Third, it uses a feature space complementary to
ours. For example features are defined between ad-
jacent words, and syntactic evidence is incorporated
indirectly into the model. In contrast our model re-
lies on synchronous rules to generate valid compres-
sions and does not explicitly incorporate adjacency
features. We used an implementation of McDonald
(2006) for comparison of results (Clarke and Lapata,
2007).
Evaluation Measures In line with previous work
we assessed our model?s output by eliciting hu-
man judgements. Participants were presented with
an original sentence and its compression and asked
to rate the latter on a five point scale based on the in-
formation retained and its grammaticality. We con-
ducted two separate elicitation studies, one for the
4The corpus can be downloaded from http://homepages.
inf.ed.ac.uk/s0460084/data/.
O: I just wish my parents and my other teachers could
be like this teacher, so we could communicate.
M: I wish my teachers could be like this teacher.
S: I wish my teachers could be like this, so we could
communicate.
G: I wish my parents and other teachers could be like
this, so we could communicate.
O: Earlier this week, in a conference call with analysts,
the bank said it boosted credit card reserves by $350
million.
M: Earlier said credit card reserves by $350 million.
S: In a conference call with analysts, the bank boosted
card reserves by $350 million.
G: In a conference call with analysts the bank said it
boosted credit card reserves by $350 million.
Table 1: Compression examples from the Broadcast
news corpus (O: original sentence, M: McDonald
(2006), S: STSG, G: gold standard)
Ziff-Davis and one for the Broadcast news dataset.
In both cases our materials consisted of 96 source-
target sentences. These included gold standard com-
pressions and the output of our system and Mc-
Donald?s (2006). We were able to obtain ratings on
the entire Ziff-Davis test set as it has only 32 in-
stances; this was not possible for Broadcast news
as the test section consists of 410 instances. Conse-
quently, we randomly selected 32 source-target sen-
tences to match the size of the Ziff-Davis test set.5
We collected ratings from 60 unpaid volunteers, all
self reported native English speakers. Both studies
were conducted over the Internet. Examples of our
experimental items are given in Table 1.
We also report results using F1 computed over
grammatical relations (Riezler et al, 2003). We
chose F1 (as opposed to accuracy or edit distance-
based measures) as Clarke and Lapata (2006b) show
that it correlates reliably with human judgements.
5 Experiments
The framework presented in Section 3 is quite flex-
ible. Depending on the grammar induction strategy,
choice of features, loss function and maximisation
algorithm, different classes of models can be de-
rived. Before presenting our results in detail we dis-
cuss the specific model employed in our experiments
and explain how its parameters were instantiated.
In order to build a compression model we need
5A Latin square design ensured that subjects did not see two
different compressions of the same sentence.
79
60 65 70 75 80 85
45
50
55
60
compression rate
F1
ll
l
l
PrecPrec.BP1Prec.BP2
Figure 4: Compression rate vs. grammatical rela-
tions F1 using unigram precision alone and in com-
bination with two brevity penalties.
a parallel corpus of syntax trees. We obtained syn-
tactic analyses for source and target sentences with
Bikel?s (2002) parser. Our corpora were automat-
ically aligned with Giza++ (Och et al, 1999) in
both directions between source and target and sym-
metrised using the intersection heuristic (Koehn et
al., 2003). Each word in the lexicon was also aligned
with itself. This was necessary in order to inform
Giza++ about word identity. Unparseable sentences
and those longer than 50 tokens were removed from
the data set.
We induced a synchronous tree substitution gram-
mar from the Ziff-Davis and Broadcast news cor-
pora using the method described in Section 3.2. We
extracted all maximally general synchronous rules.
These were complemented with more specific rules
from conjoining pairs of general rules. The specific
rules were pruned to remove singletons and those
rules with more than 3 variables. Grammar rules
were represented by the features described in Sec-
tion 3.3.
An important parameter for our compression task
is the appropriate choice of loss function. Ideally, we
would like a loss function that encourages compres-
sion without overly aggressive information loss. Fig-
ure 4 plots compression rate against grammatical re-
lations F1 using each of the loss functions presented
in Section 3.3 on the Ziff-Davis development set.6
As can be seen with unigram precision alone (Prec)
6We obtained a similar plot for the Broadcast News corpus
but omit it due to lack of space.
Ziff-Davis CompR RelF1
McDonald06 66.2 45.8
STSG 56.8 54.3
Gold standard 57.2 ?
Broadcast News CompR RelF1
McDonald06 68.6 47.6
STSG 73.7 53.4?
Gold standard 76.1 ?
Table 2: Results using grammatical relations F1
(?: sig. diff. from McDonald06; p < 0.01 using the
Student t test)
the system produces overly short output, whereas
the one-sided brevity penalty (BP1) achieves the op-
posite effect. The two-sided brevity penalty (BP2)
seems to strike the right balance: it encourages com-
pression while achieving good F-scores. This sug-
gests that important information is retained in spite
of significant compression. We also varied the regu-
larisation parameter C (see (4)) over a range of val-
ues on the development set and found that setting it
to 0.01 yields overall good performance across cor-
pora and loss functions.
We now present our results on the test set. These
were obtained with a model that uses slack rescal-
ing and a precision-based loss function with a two-
sided brevity penalty (C = 0.01). Table 2 shows the
average compression rates (CompR) for McDonald
(2006) and our model (STSG) as well as their perfor-
mance according to grammatical relations F1. The
row ?Gold standard? displays human-produced com-
pression rates. Notice that our model obtains com-
pression rates similar to the gold standard, whereas
McDonald tends to compress less on Ziff-Davis and
more on Broadcast news. As far as F1 is concerned,
we see that STSG outperforms McDonald on both
corpora. The difference in F1 is statistically signifi-
cant on Broadcast news but not on Ziff-Davis (which
consists solely of 32 sentences).
Table 3 presents the results of our elicitation
study. We carried out an Analysis of Variance
(ANOVA) to examine the effect of system type (Mc-
Donald06, STSG, Gold standard) on the compres-
sion ratings. The ANOVA revealed a reliable effect
on both corpora. We used post-hoc Tukey tests to
80
Model Ziff-Davis Broadcast news
McDonald06 2.82? 2.16?
STSG 3.20?? 2.63?
Gold standard 3.72 3.05
Table 3: Mean ratings on compression output
elicited by humans (?: sig. diff. from McDon-
ald06 (? < 0.05); ? sig. diff. from Gold standard
(? < 0.01); using post-hoc Tukey tests)
examine whether the mean ratings for each sys-
tem differed significantly. The Tukey tests showed
that STSG is perceived as significantly better than
McDonald06. There is no significant difference be-
tween STSG and the gold standard compressions on
the Broadcast news; both systems are significantly
worse than the gold standard on Ziff-Davis.
These results are encouraging, indicating that our
highly expressive framework is a good model for
sentence compression. Under several experimental
conditions we obtain better performance than previ-
ous work. Importantly, the model described here is
not compression-specific, it could be easily adapted
to other tasks, corpora or languages (for which
syntactic analysis tools are available). Being su-
pervised, our model learns to fit the compression
rate of the training data. In this sense, it is some-
what inflexible as it cannot easily adapt to a spe-
cific rate given by a user or imposed by an appli-
cation (e.g., when displaying text on small screens).
Compression rate can be indirectly manipulated by
adopting loss functions that encourage or discourage
compression (see Figure 4), but admittedly in other
frameworks (e.g., Clarke and Lapata (2006a)) the
length of the compression can be influenced more
naturally.
In our formulation of the compression problem,
a derivation is characterised by a single inventory
of features. This entails that the feature space can-
not in principle distinguish between derivations that
use the same rules, applied in a different order. Al-
though, this situation does not arise often in our
dataset, we believe that it can be ameliorated by in-
tersecting a language model with our generation al-
gorithm (Chiang, 2005).
6 Conclusions and Future Work
In this paper we have presented a novel method
for sentence compression cast in the framework of
structured learning. We develop a system that gener-
ates compressions using a synchronous tree substi-
tution grammar whose weights are discriminatively
trained within a large margin model. We also de-
scribe an appropriate algorithm than can be used in
both training (i.e., learning the model weights) and
decoding (i.e., finding the most plausible compres-
sion under the model). The proposed formulation al-
lows us to capture rewriting operations that go be-
yond word deletion and can be easily tuned to spe-
cific loss functions directly related to the problem at
hand. We empirically evaluate our approach against
a state-of-the art model (McDonald, 2006) and show
performance gains on two compression corpora.
Future research will follow three directions. First,
we will extend the framework to incorporate po-
sition dependent loss functions. Examples include
the Hamming distance or more sophisticated func-
tions that take the tree structure of the source and
target sentences into account. Such functions can
be supported by augmenting our generation algo-
rithm with a beam search. Secondly, the present pa-
per used a relatively simple feature set. Our inten-
tion was to examine our model?s performance with-
out extensive feature engineering. Nevertheless, im-
provements should be possible by incorporating fea-
tures defined over n-grams and dependencies (Mc-
Donald, 2006). Finally, the experiments presented
in this work use a grammar acquired from the train-
ing corpus. However, there is nothing inherent in our
formalisation that restricts us to this particular gram-
mar. We therefore plan to investigate the potential
of our method with unsupervised or semi-supervised
grammar induction techniques for additional rewrit-
ing tasks including paraphrase generation and ma-
chine translation.
Acknowledgements The authors acknowledge the sup-
port of EPSRC (grants GR/T04540/01 and GR/T04557/01).
We are grateful to James Clarke for sharing his implementation
of McDonald (2006) with us. Special thanks to Philip Blunsom
for insightful comments and suggestions.
81
References
R. Barzilay, K. McKeown. 2001. Extracting paraphrases
from a parallel corpus. In Proceedings of ACL/EACL,
50?57, Toulouse, France.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of HLT, 24?27, San Diego, CA.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,
J. Tait. 1999. Simplifying text for language impaired
readers. In Proceedings of EACL, 269?270, Bergen,
Norway.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the
43rd ACL, 263?270, Ann Arbor, MI.
J. Clarke, M. Lapata. 2006a. Constraint-based sentence
compression: An integer programming approach. In
Proceedings of COLING/ACLMain Conference Poster
Sessions, 144?151, Sydney, Australia.
J. Clarke, M. Lapata. 2006b. Models for sentence com-
pression: A comparison across domains, training re-
quirements and evaluation measures. In Proceedings
of COLING/ACL, 377?384, Sydney, Australia.
J. Clarke, M. Lapata. 2007. Modelling compression
with discourse constraints. In Proceedings of EMNLP-
CoNLL, Prague, Czech Republic.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of the ACL
Interactive Poster/Demonstration Sessions, 205?208,
Sapporo, Japan.
M. Galley, K. McKeown. 2007. Lexicalized Markov
grammars for sentence compression. In Proceedings
of NAACL/HLT, 180?187, Rochester, NY.
J. Grael, K. Knight. 2004. Training tree transducers. In
Proceedings of NAACL/HLT, 105?112, Boston, MA.
C. Hori, S. Furui. 2004. Speech summarization: an ap-
proach through word extraction and a method for eval-
uation. IEICE Transactions on Information and Sys-
tems, E87-D(1):15?25.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP, 310?315,
Seattle, WA.
T. Joachims. 2005. A support vector method for mul-
tivariate performance measures. In Proceedings of
ICML, 377?384, Bonn, Germany.
K. Knight, D. Marcu. 2002. Summarization beyond sen-
tence extraction: a probabilistic approach to sentence
compression. Artificial Intelligence, 139(1):91?107.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT/NAACL, 48?
54, Edmonton, Canada.
D. Lin, P. Pantel. 2001. Discovery of inference rules for
question answering. Natural Language Engineering,
7(4):342?360.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL, 297?304, Trento, Italy.
I. D. Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL, 653?660, Barcelona,
Spain.
F. J. Och, H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
F. J. Och, C. Tillmann, H. Ney. 1999. Improved align-
ment models for statistical machine translation. In
Proceedings of EMNLP/VLC, 20?28, College Park,
MD.
B. Pang, K. Knight, D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proceedings
of NAACL, 181?188, Edmonton, Canada.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, 311?318,
Philadelphia, PA.
C. Quirk, C. Brockett, W. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In Pro-
ceedings of EMNLP, 142?149, Barcelona, Spain.
S. Riezler, T. H. King, R. Crouch, A. Zaenen. 2003. Sta-
tistical sentence condensation using ambiguity pack-
ing and stochastic disambiguation methods for lexical-
functional grammar. In Proceedings of HLT/NAACL,
118?125, Edmonton, Canada.
Y. Shinyama, S. Sekine, K. Sudo, R. Grishman. 2002.
Automatic paraphrase acquisition from news articles.
In Proceedings of HLT, 40?46, San Diego, CA.
I. Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun.
2005. Large margin methods for structured and in-
terdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
J. Turner, E. Charniak. 2005. Supervised and unsuper-
vised learning for sentence compression. In Proceed-
ings of ACL, 290?297, Ann Arbor, MI.
L. S. Zettlemoyer, M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of UAI, 825?830, Edinburgh, UK.
82
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352?361,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Bayesian Model of Syntax-Directed Tree to String Grammar Induction
Trevor Cohn and Phil Blunsom
School of Informatics
University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
{tcohn,pblunsom}@inf.ed.ac.uk
Abstract
Tree based translation models are a com-
pelling means of integrating linguistic in-
formation into machine translation. Syn-
tax can inform lexical selection and re-
ordering choices and thereby improve
translation quality. Research to date has
focussed primarily on decoding with such
models, but less on the difficult problem of
inducing the bilingual grammar from data.
We propose a generative Bayesian model
of tree-to-string translation which induces
grammars that are both smaller and pro-
duce better translations than the previous
heuristic two-stage approach which em-
ploys a separate word alignment step.
1 Introduction
Many recent advances in statistical machine trans-
lation (SMT) are a result of the incorporation of
syntactic knowledge into the translation process
(Marcu et al, 2006; Zollmann and Venugopal,
2006). This has been facilitated by the use of syn-
chronous grammars to model translation as a gen-
erative process over pairs of strings in two lan-
guages. Such models are particularly attractive
for translating between languages with divergent
word orders, such as Chinese and English, where
syntax-inspired translation rules can succinctly de-
scribe the requisite reordering operations. In con-
trast, standard phrase-based models (Koehn et al,
2003) assume a mostly monotone mapping be-
tween source and target, and therefore cannot
adequately model these phenomena. Currently
the most successful paradigm for the use of syn-
chronous grammars in translation is that of string-
to-tree transduction (Galley et al, 2004; Zollmann
and Venugopal, 2006; Galley et al, 2006; Marcu
et al, 2006). In this case a grammar is extracted
from a parallel corpus, with strings on its source
side and syntax trees on its target side, which is
then used to translate novel sentences by perform-
ing inference over the space of target syntax trees
licensed by the grammar.
To date grammar-based translation models have
relied on heuristics to extract a grammar from a
word-aligned parallel corpus. These heuristics are
extensions of those developed for phrase-based
models (Koehn et al, 2003), and involve sym-
metrising two directional word alignments fol-
lowed by a projection step which uses the align-
ments to find a mapping between source words and
nodes in the target parse trees (Galley et al, 2004).
However, such approaches leave much to be de-
sired. Word-alignments rarely factorise cleanly
with parse trees (i.e., alignment points cross con-
stituent structures), resulting in large and implau-
sible translation rules which generalise poorly to
unseen data (Fossum et al, 2008). The principal
reason for employing a grammar based formal-
ism is to induce rules which capture long-range
reorderings between source and target. However
if the grammar itself is extracted using word-
alignments induced with models that are unable
to capture such reorderings, it is unlikely that the
grammar will live up to expectations.
In this work we draw on recent advances in
Bayesian modelling of grammar induction (John-
son et al, 2007; Cohn et al, 2009) to propose a
non-parametric model of synchronous tree substi-
tution grammar (STSG), continuing a recent trend
in SMT to seek principled probabilistic formula-
tions for heuristic translation models (Zhang et al,
2008; DeNero et al, 2008; Blunsom et al, 2009b;
Blunsom et al, 2009a). This model leverages a
hierarchical Bayesian prior to induce a compact
translation grammar directly from a parsed paral-
lel corpus, unconstrained by word-alignments. We
show that the induced grammars are more plausi-
ble and improve translation output.
This paper is structured as follows: In Section
352
2 we introduce the STSG formalism and describe
current heuristic approaches to grammar induc-
tion. We define a principled Bayesian model of
string-to-tree translation in Section 3, and describe
an inference technique using Gibbs sampling in
Section 4. In Section 5 we analyse an induced
grammar on a corpus of Chinese?English trans-
lation, comparing them with a heuristic grammar
in terms of grammar size and translation quality.
2 Background
Current tree-to-string translation models are a
form of Synchronous Tree Substitution Grammar
(STSG; Eisner (2003)). Formally, a STSG is a
5-tuple, G = (T, T
?
, N, S,R), where T and T
?
are sets of terminal symbols in the target and
source languages respectively, N is a set of non-
terminal symbols, S ? N is the distinguished
root non-terminal and R is a set of productions
(a.k.a. rules). Each production is a tuple compris-
ing an elementary tree and a string, the former
referring to a tree fragment of depth ? 1 where
each internal node is labelled with a non-terminal
and each leaf is labelled with either a terminal or
a non-terminal. The string part of the rule de-
scribes the lexical component of the rule in the
source language and includes a special variable for
each frontier non-terminal in the elementary tree.
These variables describe the reordering and form
the recursion sites in the generative process of cre-
ating tree and string pairs with the grammar. For
example, the rule
?(NP NP 1 (PP (IN of) NP 2 )), 2 ? 1 ? (1)
rewrites a noun-phrase (NP) as a NP and prepo-
sitional phrase (PP) headed by ?of? in the target
language. The rule generates the token ??? in
the source and reverses the order of the two child
noun-phrases, indicated by the numbering of the
variables in the string part.
A derivation creates a (tree, string) pair by start-
ing with the root non-terminal and an empty string,
then choosing a rule to rewrite (substitute) the non-
terminal and expand the string. This process re-
peats by rewriting all frontier non-terminals until
there are none remaining. A Probabilistic STSG
assigns a probability to each rule in the grammar.
The probability of a derivation is the product of
the probabilities of its component rules, and the
probability of a (tree, string) pair is the sum of the
probabilities over all its derivations.
2.1 Heuristic Grammar Induction
Grammar based SMT models almost exclusively
follow the same two-stage approach to gram-
mar induction developed for phrase-based meth-
ods (Koehn et al, 2003). In this approach they
induce a finite-state grammar with phrase-pairs as
rules by taking a sentence aligned parallel cor-
pus and 1) predicting word alignments before 2)
extracting transduction rules that are ?consistent?
with the word aligned data. Although empiri-
cally effective, this two stage approach is less than
ideal due to the disconnect between the word-
based models used for alignment and the phrase-
based translation model. This is problematic as the
word-based model cannot recognise phrase-based
phenomena. Moreover, it raises the problem of
identifying and weighting the rules from the word
alignment.
The same criticisms levied at the phrase-based
models apply equally to the two-stage technique
used for synchronous grammar induction (Galley
et al, 2004; Zollmann and Venugopal, 2006; Gal-
ley et al, 2006; Marcu et al, 2006). Namely that
the word alignment models typically do not use
any syntax and therefore will not be able to model,
e.g., consistent syntactic reordering effects, or the
impact of the syntactic category on phrase transla-
tions. The identification and estimation of gram-
mar rules from word aligned data is also non-
trivial. Galley et al (2004) describe an algorithm
for inducing a string-to-tree grammar using a par-
allel corpus with syntax trees on target side. Their
method projects the source strings onto nodes of
the target tree using the word alignment, and then
extracts the minimal transduction rules as well as
rules composed of adjacent minimal units. The
production weights are estimated either by heuris-
tic counting (Koehn et al, 2003) or using the EM
algorithm. Both estimation techniques are flawed.
The heuristic method is inconsistent in the limit
(Johnson, 2002) while EM is degenerate, placing
disproportionate probability mass on the largest
rules in order to describe the data with as few a
rules as possible (DeNero et al, 2006). With no
limit on rule size this method will learn a single
rule for every training instance, and therefore will
not generalise to unseen sentences. These prob-
lems can be ameliorated by imposing limits on
rule size or early stopping of EM training, how-
ever neither of these techniques addresses the un-
derlying problems.
353
In contrast, our model is trained in a single step,
i.e., the alignment model is the translation model.
This allows syntax to directly inform the align-
ments. We infer a grammar without resorting to
word alignment constraints or limits on rule size.
The model uses a prior to bias towards a compact
grammar with small rules, thus solving the degen-
eracy problem.
3 Model
Our training data comprises parallel target trees
and source strings and our aim is to induce a STSG
that best describes this data. This is achieved
by inferring a distribution over the derivations for
each training instance, where the set of derivations
collectively specify the grammar. In the follow-
ing, we denote the source trees as t, target strings
s, and derivations r which are sequences of gram-
mar rules, r.
As described in section 2.1, previous methods
for estimating a STSG have suffered from degen-
eracy. A principled way to correct for such degen-
erated behaviour is to use a prior over rules which
biases towards small rules. This matches our intu-
ition: we expect good translation rules to be small,
with few internal nodes, frontier non-terminals
and terminal strings. However, we recognise that
on occasion larger rules will be necessary; we al-
low such rules when there is sufficient support in
the data.
We model the grammar as a set of distributions,
G
c
, over the productions for each non-terminal
symbol, c. We adopt a non-parametric Bayesian
approach by treating eachG
c
as a random variable
with a Dirichlet process (DP) prior,
r|c ? G
c
G
c
|?
c
, P
0
? DP(?
c
, P
0
(?|c)) ,
where P
0
(?|c) (the base distribution) is a distribu-
tion over the infinite space of trees rooted with c,
and ?
c
(the concentration parameter) controls the
model?s tendency towards either reusing existing
rules or creating novel ones as each training in-
stance is encountered (and consequently, the ten-
dency to infer larger or smaller grammars). We
discuss the base distribution in more detail below.
Rather than representing the distribution G
c
ex-
plicitly, we integrate over all possible values ofG
c
.
This leads to the following predictive distribution
for the rule r
i
given the previously observed rules
r
?i
= r
1
. . . r
i?1
,
p(r
i
|r
?i
, c, ?
c
, P
0
) =
n
?i
r
i
+ ?
c
P
0
(r
i
|c)
n
?i
c
+ ?
c
, (2)
where n
?i
r
i
is the number number of times
r
i
has been used to rewrite c in r
?i
, and
n
?i
c
=
?
r,R(r)=c
n
?i
r
is the total count of rewrit-
ing c (hereR(r) is the root non-terminal of r). The
distribution is exchangeable, meaning that all per-
mutations of the input sequence are assigned the
same probability. This allows us to treat any item
as being the last, which is fundamental for efficient
Gibbs sampling. Henceforth we adopt the notation
r
?
and n
?
to refer to the rules and counts for the
whole data set excluding the current rules under
consideration, irrespective of their location in the
corpus.
The base distribution, P
0
, assigns a prior prob-
ability to an infinite number of rules, where each
rule is an (elementary tree, source string) pair de-
noted r = (e,w). While there are a myriad of
possible distributions, we developed a very sim-
ple one. We decompose the probability into two
factors,
P
0
(e,w|c) = P (e|c)P (w|e) , (3)
the probability of the target elementary tree
and the probability of the source string, where
c = R(r).
The tree probability, P (e|c) in (3), is modelled
using generative process whereby the root cate-
gory c is expanded into a sequence of child non-
terminals, then each of these are either expanded
or left as-is. This process continues until there
are no unprocessed children. The number of child
nodes for each expansion is drawn from a geo-
metric prior with parameter p
child
, except in the
case of pre-terminals where the number of chil-
dren is always one. The binary expansion deci-
sions are drawn from a Bernoulli prior with pa-
rameter p
expand
, and non-terminals and terminals
are drawn uniformly from N and T respectively.
For example, the source side of rule (1) was gen-
erated as follows: 1) the NP was rewritten as two
children; 2) an NP; and 4) a PP; 5) the NP child
was not expanded; 6) the PP child was expanded;
7) as an IN; and 8) a NP; 9) the IN was expanded to
the terminal ?of?; and 10) the final NP was not ex-
panded. Each of these steps is a draw from the rel-
evant distribution, and the total probability is the
product of the probabilities for each step.
354
Hong/NNP Kong/NNP
?? ? ? ? ? ?? ? ?? ? ?? ?
S
NP
VP ./.
NP PP
Every/DT corner/NN of/IN NP
is/VBZ VP
filled/VBN PP
with/IN NP
fun/NN
0 1 2 3 4 5 6 7 8 109 11
[2,3)
[2,6)
[3,6) [0,1)
[0,2)
[0,11)
[10,11)[6,10)
[6,10)
[7,9) [9,10)
[9,10)
?
?
? ? ? ?
?
Figure 1: Example derivation. Each node is annotated with their span in the target string (aligned nodes are shaded). The dotted
edges show the implied alignments. Preterminals are displayed with their child terminal in the leaf nodes.
The second factor in (3) is P (w|e), the prob-
ability of the source string (a sequence of source
terminals and variables). We assume that the el-
ementary tree is generated first, and condition the
string probability on l = F (e), its number of fron-
tier nodes (i.e., variables). The string is then cre-
ated by choosing a number of terminals from a ge-
ometric prior with parameter p
term
then drawing
each terminal from a uniform distribution over T
?
.
Finally each of the l variables are inserted into the
string one at a time using a uniform distribution
over the possible placements. For the example rule
in (1) the generative process corresponds to 1) de-
ciding to create one terminal; 2) with value ?; 3)
inserting the first variable after the terminal; and
4) inserting the second variable before the termi-
nal. Again, the probability of the string is simply
the product of the probabilities for each step.
Together both the factors in the base distribu-
tion penalise large trees with many nodes and long
strings with many terminals and variables. P
0
de-
creases exponentially with rule size, thus discour-
aging the model from using larger rules; for this
to occur the rules must significantly increase the
likelihood.
4 Training
To train our model we use Gibbs sampling (Geman
and Geman, 1984), a Markov chain Monte Carlo
method (Gilks et al, 1996) in which variables are
repeatedly sampled conditioned on the values of
all other variables in the model.
1
After a period of
burn-in, each sampler state (set of variable assign-
ments) is a sample from the posterior distribution
of the model. In our case, we wish to sample from
the posterior over the grammar, P (r|t, s, ?).
To simplify matters we associate an alignment
variable, a, with every internal node of the trees
in the training set. This variable specifies the span
of source tokens to which node is aligned. Alter-
natively, the node can be unaligned, which is en-
coded as an empty span. I.e. a ? (J ? J) ? ?
where J is the set of target word indices. Spans
are written [i, j): inclusive of i and exclusive of
j. Each aligned node (a 6= ?) forms the root
of a rule as well as being a frontier non-terminal
of an ancestor rule, while unaligned nodes form
part of an ancestor rule.
2
The set of valid align-
ments are constrained by the tree in a number of
ways. Child nodes can be aligned only to sub-
spans of their ancestor nodes? alignments and no
two nodes? alignments can overlap. Finally, the
root node of the tree must be aligned to the full
1
Previous approaches to bilingual grammar induction
have used variational inference to optimise a bound on the
data log-likelihood (Zhang et al, 2008; Blunsom et al,
2009b). Both these approaches truncated the grammar a pri-
ori in order to permit tractable inference. In contrast our
Gibbs sampler can perform inference over the full space of
grammars. See also Blunsom et al (2009a) where we present
a Gibbs sampler for inducing SCFGs without truncation.
2
The Gibbs sampler is an extension of our sampler for
monolingual tree-substitution grammar (Cohn et al, 2009),
which used a binary substitution variable at each node to en-
code the segmentation of a training tree into elementary trees.
355
?(S (NP NP 1 PP 2 ) VP 3 . 4 ), 2 1 3 4 ?
?(NP DT 1 NN 2 ), 1 2 ?
?(DT Every),??
?(NN corner),?????
?(PP (IN of) NP 1 ), 1 ??
?(NP (NNP Hong) (NNP Kong)),???
?(VP (VBZ is) VP 1 ), 1 ?
?(VP VBN 1 PP 2 ),? 1 2 ?
?(VBN filled),???
?(PP (IN with) (NP NN 1 )), 1 ?
?(NN fun),??
?(. .),??
Table 1: Grammar rules specified by the derivation in Fig-
ure 1. Each rule is shown as a tuple comprising a tar-
get elementary tree and a source string. Boxed numbers
show the alignment between string variables and frontier non-
terminals.
span of source words.
Collectively, the training trees and alignment
variables specify the sequence of rules r, which
in turn specify the grammar. Figure 1 shows an
example derivation with alignment variables. The
corresponding STSG rules are shown in Table 1.
4.1 Gibbs operators
The Gibbs sampler works by sampling new val-
ues of the alignment variables, using two differ-
ent Gibbs operators to make the updates. The first
operator, EXPAND, takes a tree node, v, and sam-
ples a new alignment, a
v
, given the alignments of
all other nodes in the same tree and all other trees
in the corpus, denoted a
?
. The set of valid la-
bels is constrained by the other alignments in the
tree, specifically that of the node?s closest aligned
ancestor, a
p
, its closest aligned descendants, a
d
,
and its aligned siblings, a
s
(the aligned descen-
dants of a). The alignment variable may be empty,
a
v
= ?, while non-empty values must obey the
tree constraints. Specifically the span must be a
subspan of its ancestor, a
v
? a
p
, subsume its de-
scendants, a
v
?
?
a
d
, and not overlap its siblings,
j 6?
?
a
s
,?j ? a
v
. Figure 2 shows an exam-
ple with the range of valid values for corner/NN?s
alignment variable and the corresponding align-
ments that these encode.
Each alignment in the set of valid outcomes de-
fines a set of grammar rules. The non-aligned out-
come results in a single rule r
p
rooted at ancestor
node p. While the various aligned outcomes re-
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[3,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[4,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[5,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[4,5)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[3,4)
6 6
6
6
6 6
?
Figure 2: Possible state updates for the (NN corner) node us-
ing the EXPAND operator.
sult in a pair of rules, r
p
?
and r
v
, rooted at p and v
respectively. In the example in Figure 2, the top-
right outcome has a
v
= ? and
r
p
= ?(NP DT 1 (NN corner)), 1 ????? .
The bottom-right outcome, a
v
= [4, 5), describes
the pair of rules:
r
p
?
= ?(NP DT 1 NN 2 ), 1 ? 2 ??? and
r
v
= ?(NN corner),?? .
The set of valid options are then scored according
to the probability of their rules as follows:
P (r
p
|r
?
) =
n
?
r
p
+ ?P
0
(r
p
|c
p
)
n
?
c
p
+ ?
(4)
P (r
p
?
, r
v
|r
?
) = P (r
p
?
|r
?
)P (r
v
|r
?
, r
p
?
)
=
n
?
r
p
?
+ ?P
0
(r
p
?
|c
p
)
n
?
c
p
+ ?
? (5)
n
?
r
v
+ ?(r
p
?
, r
v
) + ?P
0
(r
v
|c
v
)
n
?
c
v
+ ?(c
p
, c
v
) + ?
where c
p
is the non-terminal at node p (simi-
larly for c
v
), n
?
denote counts of trees (e.g., n
?
r
p
)
or the sum over all trees expanding a non-
terminal (e.g., n
?
c
v
) in the conditioning context,
r
?
, and ?(?, ?) is the Kronecker delta function,
which returns 1 when its arguments are identi-
cal and 0 otherwise. For clarity, we have omit-
ted some items from the conditioning context
356
in (4) and (5), namely t, s and hyper-parameters
?, p
child
, p
expand
, p
term
. The ? terms in the sec-
ond factor of (5) account for the changes to n
?
that would occur after observing r
p
?
, which forms
part of the conditioning context for r
v
. If the rules
r
p
?
and r
v
are identical, then the count n
?
r
v
would
increase by one, and if the rules expand the same
root non-terminal, then n
?
c
v
would increase by one.
Equation (4) is evaluated once for the unaligned
outcome, a
v
= ?, and (5) is evaluated for each
valid alignment. The probabilities are normalised
and an outcome sampled.
The EXPAND operator is sufficient to move
from one derivation to any other valid derivation,
however it may take many steps to do so. These
intermediate steps may require the sampler to pass
through highly improbable regions of the state
space, and consequently such moves are unlikely.
The second operator, SWAP, is designed to help
address this problem by increasing the mobility of
the sampler, allowing it to mix more quickly. The
operator considers pairs of nodes, v, w, in one tree
and attempts to swap their alignment values.
3
This
is illustrated in the example in Figure 3. There are
two options being compared: preserving the align-
ments (left) or swapping them (right). This can
change three rules implied by the derivation: that
rooted at the nodes? common aligned ancestor, p,
and those rooted at v and w. For the example, the
left option implies rules
{r
p
= ?(NP DT 1 NN 2 ), 1 2 ?,
r
v
= ?(DT Every),??,
r
w
= ?(NN corner),?????} ,
and the right option implies rules
{r
p
= ?(NP DT 1 NN 2 ), 2 1 ?,
r
v
= ?(DT Every),?????,
r
w
= ?(NN corner),??} .
We simply evaluate the probability of
both triples of rules under our model,
P (r
p
, r
v
, r
w
|r
?
) = P (r
p
|r
?
)P (r
v
|r
?
, r
p
)
P (r
w
|r
?
, r
p
, r
v
), where the additional rules in
the conditioning context signify their inclusion
in the counts r
?
before applying (2) to evaluate
the probability (much the same as in (5) where
3
We rarely need to consider the full quadratic space of
node pairs, as the validity constraints mean that the only
candidates for swapping are siblings (i.e., share the closest
aligned ancestor) which do not have any aligned descendants.
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[2,3)
[2,6)
[3,6)
? ? ? ??
NP
Every/DT corner/NN
2 3 4 5
[3,6)
[2,6)
6 6
[2,3)
Figure 3: Possible state updates for the pair of nodes
(DT every) and (NN corner) using the SWAP operator.
English? Chinese
Sentences 300k
Words or Segments 11.0M 8.6M
Avg. Sent. Length 36 28
Longest Sent. 80 80
Table 2: NIST Chinese-English corpora statistics
(LDC2003E14, LDC2005E47).
the ? functions encode the changes to the counts).
An outcome is then sampled according to the
normalised probabilities of the preserve vs. swap
rules.
The Gibbs sampler makes use of both operators.
The algorithm visits each (tree, string) pair in the
training set in random order and applies the EX-
PAND operator to every node in the tree. After the
tree has been processed, the SWAP operator is ap-
plied to all candidate pairs of nodes. Visiting all
sentence pairs in this way constitutes a single sam-
ple from the Gibbs sampler.
5 Experiments
We evaluate our non-parametric model of gram-
mar induction on a subset of the NIST Chinese-
English translation evaluation, representing a real-
istic SMT experiment with millions of words and
long sentences. The Chinese-English training data
consists of the FBIS corpus (LDC2003E14) and
the first 100k sentence pairs from the Sinorama
corpus (LDC2005E47). The Chinese text was seg-
mented with a CRF-based Chinese segmenter op-
timized for MT (Chang et al, 2008), and the En-
glish text was parsed using the Stanford parser
(Klein and Manning, 2003).
As a baseline we implemented the heuristic
grammar extraction technique of Galley et al
(2004) (henceforth GHKM). This method finds
the minimum sized translation rules which are
consistent with a word-aligned sentence pair, as
357
described in section 2.1. The rules are then
treated as events in a relative frequency esti-
mate.
4
We used Giza++ Model 4 to obtain
word alignments (Och and Ney, 2003), using
the grow-diag-final-and heuristic to sym-
metrise the two directional predictions (Koehn et
al., 2003).
The model was sampled for 300 iterations to
?burn-in?, where in each iteration we applied both
sampling operators to all nodes (or node pairs)
of all training instances. We initialised the sam-
pler using the GHKM derivation of the training
data (the baseline system). The final state of the
sampler was used to extract the grammar. The
hyperparameters were set by hand to ? = 10
6
,
p
child
= 0.5, p
expand
= 0.5, and p
term
= 0.5.
5
Overall the model took on average 2,218s per full
iteration of Gibbs sampling and 1 week in total
to train, using a single core of a 2.3Ghz AMD
Opteron machine.
5.1 Grammar Analysis
The resulting grammar had 1.62M rules, al-
most identical to the GHKM grammar which had
1.63M. Despite their similarity in size the gram-
mars were quite different, as illustrated in Fig-
ure 4, which shows histograms over various mea-
sures of rule size for the two grammars. Under
each measure the sampled grammar finds many
more simple rules ? shallower with fewer internal
nodes, fewer variables and fewer terminals ? than
the GHKM method. This demonstrates that the
prior is effective in shifting mass away from com-
plex rules. To show how the rules themselves dif-
fer, Table 3 lists rules in the sampled grammar that
are not in the GHKM grammar. Note that many of
these rules are highly plausible, describing regular
tree structures and lexicalisation. These rules have
not been specified to the same extent in the GHKM
grammar. For example the first rule incorporates
4
Our implementation of the GHKM algorithm attaches
unaligned source words to the highest possible node in the
source tree, rather than allowing all attachment points as in
the original presentation (Galley et al, 2004). Allowing all
attachments made no difference to translation performance,
but did make the grammar considerably larger. We imple-
mented only the minimal rule extraction, i.e., with no rule
composition (Galley et al, 2006). Consequently there is no
derivational ambiguity, obviating the need for expectation
maximisation or similar for rule estimation.
5
Note that although ? seems large, it still encourages
sparse distributions as the P
0
values are typically much
smaller than its reciprocal, 10
?6
, especially if the rule is
large. ?P
0
< 1 implies a sparse Dirichlet prior.
1 2 3 4 5 6 7 8 9 10 11
GKHMGibbs
maximum tree depth
num
ber o
f rule
s
0
20k
40k
60k
80k
0 1 2 3 4 5 6 7 8 9 10
variables
num
ber o
f rule
s
0
20k
40k
60k
80k
0 1 2 3 4 5 6 7 8 9 10
source terminals
num
ber o
f rule
s
0
20k
40k
60k
80k
0 1 2 3 4 5 6 7 8 9 10
target terminals
num
ber o
f rule
s
0
25k
50k
75k
100k
Figure 4: Histograms over rule statistics comparing the
heuristic grammar (GHKM) and learnt grammar (Gibbs).
the TOP symbol, while the GHKM grammar in-
stead relies on the rule ?(TOP S 1 ), 1 ? to produce
the same fragment. The model has learnt to dis-
tinguish between sentence-spanning and subsen-
tential S constituents, which typically do not in-
clude final punctuation. The third and ninth (last)
rule are particularly interesting. These rules en-
code reordering effects relating to noun phrases
and subordinate prepositional phrases, in partic-
ular that Chinese prepositional modifiers precede
the nouns they modify. Differences in word or-
der such as these are quite common in Chinese-
English corpora, so it is imperative that they are
modelled accurately.
The rules in the GHKM grammar that do not
appear in the sampled grammar are shown in
Table 4. In contrast to the rules only present in
the sampled grammar, these have much lower
counts, i.e., are less probable. Each of these rules
has been specified further by the Bayesian model.
358
?(TOP (S NP 1 VP 2 . 3 )), 1 2 3 ?
?(S (VP (TO to) VP 1 )), 1 ?
?(NP NP 1 (PP (IN of) NP 2 )), 2 1 ?
?(PP (IN in) NP 1 ), ? 1 ?
?(NP NP 1 (PP (IN of) NP 2 )), 1 2 ?
?(NP (DT the) NN 1 ), ? 1 ?
?(S (VP TO 1 VP 2 )), 1 2 ?
?(VP (VBZ is) NP 1 ), ? 1 ?
?(NP (NP (DT the) NN 1 ) (PP (IN of) NP 2 )), 2 1 ?
Table 3: Top ten rules in the sampled grammar that do not
appear in the GHKM grammar. All the above rules are quite
high probability, with counts between 37,118 and 7,275 from
first to last.
?(PP (IN at) (NP DT 1 (NNS levels))), 1 ??
?(NP NP 1 , 2 NP 3 (, ,) CC 4 NP 5 ), 1 2 3 4 5 ?
?(NP NP 1 , 2 NP 3 , 4 NP 5 (, ,) (CC and) NP 6 ), 1 2 3 4 5 , 6 ?
?(S S 1 (NP (PRP They)) VP 2 . 3 ), 1 2 3 ?
?(S PP 1 , 2 NP 3 VP 4 . 5 ? 6 ), 1 2 3 4 6 5 ?
?(S PP 1 , 2 NP 3 VP 4 . 5 ), 1 ? 2 3 4 5 ?
?(NP (NNP Foreign) (NNP Ministry) NN 1 (NNP Zhu) (NNP Bangzao)),
??? 1 ????
?(S S 1 S 2 ), 1 2 ?
?(S S 1 (NP (PRP We)) VP 2 . 3 ), 1 2 3 ?
?(NP (DT the) (NNS people) POS 1 ), ?? 1 ?
Table 4: Top ten rules in the GHKM grammar that do not ap-
pear in the sampled grammar. These are quite low probability
rules: their counts range from 1,137 to 103.
For example, every instance of the first rule
had the same determiner and target translation,
?(PP (IN at) (NP (DT all) (NNS levels))),???,
and therefore the model specified the determiner,
resulting in a single rule. The model has correctly
learnt that other translations for (DT all) are
not appropriate in this context (e.g., ?, ??
or ??). In a number of the remaining rules
the commas were lexicalised, or S rules were
extended to include the TOP symbol.
To further illustrate the differences between the
grammars, Table 5 shows the rules which include
the possessive particle, ?, and at least one vari-
able. In both grammars there are many fully lex-
icalised rules which translate the token to, e.g., a
determiner or a preposition. The grammars differ
on the complex rules which combine lexicalisa-
tion and frontier non-terminals. The GHKM rules
are all very simple depth-1 SCFG rules, contain-
ing minimal information. In contrast, the sampled
rules are more lexicalised, licensing the insertion
of various English tokens and tree substructure.
Note particularly the second and forth rule which
succinctly describe the reordering of prepositional
Sampled Grammar
?(NP (DT the) NN 1 ),? 1 ?
?(NP (NP (DT the) NN 1 ) (PP (IN of) NP 2 )), 2 ? 1 ?
?(NP (DT the) NN 1 ), 1 ??
?(NP (NP (DT the) JJ 1 NN 2 ) (PP (IN of) NP 3 )), 3 ? 1 2 ?
?(PP (IN of) NP 1 ), 1 ??
GHKM Grammar
?(NP JJ 1 NNS 2 ), 1 ? 2 ?
?(NP JJ 1 NN 2 ), 1 ? 2 ?
?(NP DT 1 JJ 2 NN 3 ), 1 2 ? 3 ?
?(NP PRP$ 1 NN 2 ), 1 ? 2 ?
?(NP NP 1 PP 2 ), 2 ? 1 ?
Table 5: Top five rules which include the possessive particle
? and at least one variable.
phrases with an noun phrase.
5.2 Translation
In order to test the translation performance of
the grammars induced by our model and the
GHKM method
6
we report BLEU (Papineni et
al., 2002) scores on sentences of up to twenty
words in length from the MT03 NIST evaluation.
We built a synchronous beam search decoder to
find the maximum scoring derivation, based on
the CYK+ chart parsing algorithm and the cube-
pruning method of Chiang (2007). Parse edges for
all constituents spanning a given chart cell were
cube-pruned together using a beam of width 1000,
and only edges from the top ten constituents in
each cell were retained. No artificial glue-rules or
rule span limits were employed.
7
The parameters
of the translation system were trained to maximize
BLEU on the MT02 test set (Och, 2003). Decoding
took roughly 10s per sentence for both grammars,
using a 8-core 2.6Ghz Intel Xeon machine.
Table 6 shows the BLEU scores for the baseline
using the GHKM rule induction algorithm, and
our non-parametric Bayesian grammar induction
method. We see a small increase in generalisation
performance from our model. Our previous anal-
6
Our decoder was unable to process unary rules (those
which consume nothing in the source). Monolingual pars-
ing with unary productions is fairly straightforward (Stolcke,
1995), however in the transductive setting these rules can li-
cence infinite insertions in the target string. This is further
complicated by the language model integration. Therefore
we composed each unary rule instance with its descendant
rule(s) to create a non-unary rule.
7
Our decoder lacks certain features shown to be beneficial
to synchronous grammar decoding, in particular rule binari-
sation (Zhang et al, 2006). As such the reported results for
MT03 lag the state-of-the-art: the Moses phrase-based de-
coder (Koehn et al, 2007) achieves 26.8. We believe that im-
provements from a better decoder implementation would be
orthogonal to the improvements presented here (and would
allow us to relax the length restriction on the test set).
359
Model BLEU score
GHKM 26.0
Our model 26.6
Table 6: Translation results on the NIST test set MT03 for
sentences of length ? 20.
ysis (Section 5.1) of the grammars produced by
the two approaches showed our method produced
better lexicalised rules than those induced by the
GHKM algorithm. Galley et al (2006) noted that
the GHKM algorithm often over generalised and
proposed combining minimal rules to form com-
posed rules as a solution. Although composing
rules was effective at improving BLEU scores, the
result was a massive expansion in the size of the
grammar. By learning the appropriate level of lex-
icalisation we believe that our inference algorithm
is having a similar effect as composing rules (Gal-
ley et al, 2006), however the resulting grammar
remains compact, a significant advantage of our
approach.
6 Conclusion
In this paper we have presented a method for in-
ducing a tree-to-string grammar which removes
the need for various heuristics and constraints
from models of word alignment. Instead the model
is capable of directly inferring a grammar in one
step, using the syntactic fragments that it has learnt
to better align the source and target data. Using a
prior which favours sparse distributions and sim-
pler rules, we demonstrate that the model finds
a more parsimonious grammar than the heuristic
technique. Moreover, this grammar results in im-
proved translations on a standard evaluation set.
We expect that various extensions to the model
would improve its performance. One avenue is to
develop a more sophisticated prior over rules, e.g.,
one that recognises common types of rule via the
shape of the tree and ordering pattern in the tar-
get. A second avenue is to develop better means
of inference under the grammar, in order to ensure
faster mixing and a means to escape from local
optima. Finally, we wish to develop a method for
decoding under the full Bayesian model, instead of
the current beam search. With these extensions we
expect that our model of grammar induction has
the potential to greatly improve translation output.
Acknowledgements
The authors acknowledge the support of the EP-
SRC (grants GR/T04557/01 and EP/D074959/1).
This work has made use of the resources pro-
vided by the Edinburgh Compute and Data Facility
(ECDF). The ECDF is partially supported by the
eDIKT initiative.
References
P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009a.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of the Joint conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Pro-
cessing, Singapore. To appear.
P. Blunsom, T. Cohn, M. Osborne. 2009b. Bayesian
synchronous grammar induction. In D. Koller,
D. Schuurmans, Y. Bengio, L. Bottou, eds., Ad-
vances in Neural Information Processing Systems
21, 161?168. MIT Press, Cambridge, MA.
P.-C. Chang, M. Galley, C. D. Manning. 2008. Op-
timizing Chinese word segmentation for machine
translation performance. In Proceedings of the
Third Workshop on Statistical Machine Translation,
224?232, Columbus, Ohio.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
T. Cohn, S. Goldwater, P. Blunsom. 2009. Inducing
compact but accurate tree-substitution grammars.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, 548?556, Boulder, Colorado.
J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006.
Why generative phrase models underperform sur-
face heuristics. In Proceedings on the Workshop on
Statistical Machine Translation, 31?38, New York
City, NY.
J. DeNero, A. Bouchard-C?ot?e, D. Klein. 2008. Sam-
pling alignment structure under a Bayesian transla-
tion model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, 314?323, Honolulu, Hawaii.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion
Volume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, 205?
208, Sapporo, Japan.
V. Fossum, K. Knight, S. Abney. 2008. Using syn-
tax to improve word alignment precision for syntax-
based machine translation. In Proceedings of the
360
Third Workshop on Statistical Machine Translation,
44?52, Columbus, Ohio.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004.
What?s in a translation rule? In Proceedings of the
2004 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, 273?280, Boston, MA.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation mod-
els. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, 961?968, Sydney, Australia.
S. Geman, D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
W. Gilks, S. Richardson, D. J. Spiegelhalter, eds. 1996.
Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
M. Johnson, T. L. Griffiths, S. Goldwater. 2007.
Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In
B. Sch?olkopf, J. Platt, T. Hoffman, eds., Advances
in Neural Information Processing Systems 19, 641?
648. MIT Press, Cambridge, MA.
M. Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Lingusitics,
28(1):71?76.
D. Klein, C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
In In Advances in Neural Information Processing
Systems 15, 3?10. MIT Press.
P. Koehn, F. J. Och, D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of
the 2003 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics, 48?54, Edmonton,
Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
177?180, Prague, Czech Republic.
D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006.
SPMT: Statistical machine translation with syntact-
ified target language phrases. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, 44?52, Sydney, Australia.
F. J. Och, H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, 311?318, Philadelphia, PA.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2).
H. Zhang, L. Huang, D. Gildea, K. Knight. 2006. Syn-
chronous binarization for machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics, 256?
263.
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008.
Bayesian learning of non-compositional phrases
with synchronous parsing. In Proceedings of ACL-
08: HLT, 97?105, Columbus, Ohio.
A. Zollmann, A. Venugopal. 2006. Syntax augmented
machine translation via chart parsing. In Proceed-
ings of the 2006 Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, 138?141,
New York City, NY.
361
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 719?727,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Word Lattices for Multi-Source Translation
Josh Schroeder, Trevor Cohn, and Philipp Koehn
School of Informatics
University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
{jschroe1, tcohn, pkoehn}@inf.ed.ac.uk
Abstract
Multi-source statistical machine transla-
tion is the process of generating a single
translation from multiple inputs. Previous
work has focused primarily on selecting
from potential outputs of separate transla-
tion systems, and solely on multi-parallel
corpora and test sets. We demonstrate how
multi-source translation can be adapted for
multiple monolingual inputs. We also ex-
amine different approaches to dealing with
multiple sources, including consensus de-
coding, and we present a novel method
of input combination to generate lattices
for multi-source translation within a single
translation model.
1 Introduction
Multi-source statistical machine translation was
first formally defined by Och and Ney (2001)
as the process of translating multiple meaning-
equivalent source language texts into a single tar-
get language. Multi-source translation is of par-
ticular use when translating a document that has
already been translated into several languages, ei-
ther by humans or machines, and needs to be fur-
ther translated into other target languages. This
situation occurs often in large multi-lingual organ-
isations such as the United Nations and the Euro-
pean Parliament, which must translate their pro-
ceedings into the languages of the member in-
stitutions. It is also common in multi-national
companies, which need to translate product and
marketing documentation for their different mar-
kets. Clearly, any existing translations for a docu-
ment can help automatic translation into other lan-
guages. These different versions of the input can
resolve deficiencies and ambiguities (e.g., syntac-
tic and semantic ambiguity) present in a single in-
put, resulting in higher quality translation output.
In this paper, we present three models of multi-
source translation, with increasing degrees of so-
phistication, which we compare empirically on a
number of different corpora. We generalize the
definition of multi-source translation to include
any translation case with multiple inputs and a sin-
gle output, allowing for, e.g., multiple paraphrased
inputs in a single language. Our methods include
simple output selection, which treats the multi-
source translation task as many independent trans-
lation steps followed by selection of one of their
outputs (Och and Ney, 2001), and output combina-
tion, which uses consensus decoding to construct
a string from n-gram fragments of the translation
outputs (Bangalore et al, 2001). We also present
a novel method, input combination, in which we
compile the input texts into a compact lattice, over
which we perform a single decoding pass. We
show that as we add additional inputs, the simplest
output selection method performs quite poorly rel-
ative to a single input translation system, while the
latter two methods are able to make better use of
the additional inputs.
The paper is structured as follows. ?2 presents
the three methods for multi-source translation in
detail: output selection, output combination, and
our novel lattice-based method for input combina-
tion. We report experiments applying these tech-
niques to three different corpora, with both mono-
lingual inputs (?3) and multilingual inputs (?4).
We finish in ?5 by analyzing the benefits and draw-
backs of these approaches.
2 Approaches to Multi-Source
Translation
We now present three ways to combine multiple
inputs into a single output translation, in the con-
text of related work for each technique.
719
2.1 Output Selection
The most straightforward approach to multi-
source translation, proposed by Och and Ney
(2001), is to independently translate each of the
N source languages and then select a single
translation from the outputs. Given N sources
sN1 = s1, . . . , sN , first translate each with a sep-
arate translation system, p1, . . . , pN , to obtain N
target translations, tN1 = t1, . . . , tN . Och and Ney
present two approaches for selecting a single tar-
get from these outputs.
The first, PROD, finds the maximiser of the
product, argmaxt?tN1 p(t)
?N
n=1 pn(sn|t), where
p(t) is the language model probability. For rea-
sons of tractability, the maximisation is performed
only over targets generated by the translation sys-
tems, tN1 , not the full space of all translations.
The PROD method requires each model to pro-
vide a model score for each tn generated by the
other models. However, this is often impossible
due to the models? highly divergent output spaces
(Schwartz, 2008), and therefore the technique can-
not be easily applied.
The second approach, MAX, solves
argmaxt?tN1 max
N
n=1 p(t)pn(sn|t), which is
much easier to calculate. As with PROD, the
translation models? outputs are used for the
candidate translations. While different models
may have different score ranges, Och and Ney
(2001) state that there is little benefit in weighting
these scores to normalise the output range. In their
experiments, they show that MAX used on pairs or
triples of language inputs can outperform a model
with single language input, but that performance
degrades as more languages are added.
These methods limit the explored space to a full
translation output of one of the inputs, and there-
fore cannot make good use of the full diversity of
the translations. In this paper we present MAX
scores as a baseline for output selection, and ap-
proximate an oracle using the BLEU metric as an
upper bound for the output selection technique.
2.2 Output Combination
Consensus decoding as a form of system combi-
nation is typically used to integrate the outputs of
multiple translation systems into a single synthetic
output that seeks to combine the best fragments
from each component system. Multi-source trans-
lation can be treated as a special case of consen-
sus decoding. Indeed, several authors have seen
the  dog barked very loudly
a big dog barked  loudly
sub insert ? shift delete ?
Table 1: Example minimum TER edit script.
0 1thea 2?big 3dog 4barked 5very? 6loudly
Figure 1: Conversion of TER script from Table 1
to a confusion network.
improvements in translation quality by perform-
ing multi-source translation using generic system
combination techniques (Matusov et al, 2006;
Paulik et al, 2007).
One class of approaches to consensus decoding
focuses on construction of a confusion network
or lattice1 from translation outputs, from which
new sentences can be created using different re-
orderings or combinations of translation fragments
(e.g., Bangalore et al (2001); Rosti et al (2007b)).
These methods differ in the types of lattices used,
their means of creation, and scoring method used
to extract the best consensus output from the lat-
tice. The system used in this paper is a variant of
the one proposed in Rosti et al (2007a), which we
now describe in detail.
The first step in forming a lattice is to align the
inputs. Consensus decoding systems often use the
script of edit operations that minimises the transla-
tion edit rate (TER; Snover et al (2006)). TER is
a word-based measure of edit distance which also
allows n-gram shifts when calculating the best
match between a hypothesis and reference. Be-
cause TER describes the correspondence between
the hypothesis and reference as a sequence of in-
sertions, substitutions, deletions, and shifts, the
edit script it produces can be used to create a con-
fusion network.
Consider a reference of ?The dog barked very
loudly? and a hypothesis ?A big dog loudly
barked.? The TER alignment is shown in Ta-
ble 1, along with the edit operations. Note that the
matching ?barked? tokens are labelled shift, as one
needs to be shifted for this match to occur. Using
the shifted hypothesis, we can form a confusion
1Different authors refer to ?lattices,? ?confusion net-
works,? ?word sausages,? etc. to describe these data struc-
tures, and specific terminology varies from author to author.
We define a lattice as a weighted directed acyclic graph, and
a confusion network as a special case where each node n in
the ordered graph has word arcs only to node n + 1.
720
  
 
 
 Confusion Network 1
 Confusion Network 2
 Confusion Network 3
 
Figure 2: Structure of a lattice of confusion net-
works for consensus decoding.
network as in Figure 1. Additional sentences can
be added by aligning them to the reference as well.
Each link is weighted by the number of component
sentences sharing that particular word at the given
location.
Similar to Rosti et al (2007a), we let each hy-
pothesis take a turn as the ?reference? for TER,
using it as a skeleton for a confusion network. We
then form a lattice of confusion networks (Fig-
ure 2), assigning a prior weight to each confusion
network based on the average TER of the selected
skeleton with the other hypotheses. This allows
each system to set the word order for a component
confusion network, but at the cost of a more com-
plex lattice structure.
We can score pathsP through these lattices with
the assistance of a language model. Formally, the
path score is given by:
w(P) = ? log pLM (t(P))
+
?
d?P
[
N?
n=1
?n log pn(d|sn)
+ ??(d, ) + ?(1? ?(d, ))
]
where pLM is the language model probability of
the target string specified by the lattice path, t(P),
pn(d|sn) is the proportion of system n?s k-best
outputs that use arc d in path P , and the last two
terms count the number of epsilon and non-epsilon
transitions in the path. The model parameters are
?1, . . . , ?n, ?, ?, ?, which are trained using Pow-
ell?s search to maximise the BLEU score for the
highest scoring path, argmaxP w(P).
2.3 Input Combination
Loosely defined, input combination refers to find-
ing a compact single representation of N transla-
tion inputs. The hope is that the new input pre-
serves as many of the salient differences between
the inputs as possible, while eliminating redundant
information. Lattices are well suited to this task.
0 1
?watchit 2
?out's 3?for 4
thepursepicka?
5
robberthief
snatcherburglarcrookpocket
6.
Figure 3: A monolingual confusion network.
Thicker lines indicate higher probability word
arcs.
When translating speech recognition output,
previous work has shown that representing the
ambiguity in the recognized text via confusion
networks leads to better translations than simply
translating the single best hypothesis of the speech
recognition system (Bertoldi et al, 2007). The ap-
plication of input lattices to other forms of input
ambiguity has been limited to encoding input re-
orderings, word segmentation, or morphological
segmentation, all showing improvements in trans-
lation quality (Costa-jussa` et al, 2007; Xu et al,
2005; Dyer et al, 2008). However, these appli-
cations encode the ambiguity arising from a sin-
gle input, while in this work we combine distinct
inputs into a more compact and expressive single
input format.
When given many monolingual inputs, we can
apply TER and construct a confusion network as
in Section 2.2.2 In this application of confusion
networks, arc weights are calculated by summing
votes from each input for a given word, and nor-
malizing all arcs leaving a node to sum to 1.
Figure 3 shows an example of a TER-derived
input from IWSLT data. Because the decoder will
handle reordering, we select the input with the
lowest average TER against the other inputs to
serve as the skeleton system, and do not create a
lattice with multiple skeletons.
The problem becomes more complex when we
consider cases of multi-lingual multi-source trans-
lation. We cannot easily apply TER across lan-
guages because there is no clear notion of an exact
match between words. Matusov et al (2006) pro-
pose using a statistical word alignment algorithm
as a more robust way of aligning (monolingual)
outputs into a confusion network for system com-
2Barzilay and Lee (2003) construct lattices over para-
phrases using an iterative pairwise multiple sequence align-
ment (MSA) algorithm. Unlike our approach, MSA does not
allow reordering of inputs.
721
bination. We take a similar approach for multi-
lingual lattice generation.
Our process consists of four steps: (i) Align
words for each of the N(N ? 1) pairs of inputs;
(ii) choose an input (or many inputs) to be the
lattice skeleton; (iii) extract all minimal consis-
tent alignments between the skeleton and the other
inputs; and (iv) add links to the lattice for each
aligned phrase pair.
A multi-parallel corpus such as Europarl
(Koehn, 2005) is ideally suited for training this
setup, as training data is available for each pair of
input languages needed by the word aligner. We
used the GIZA++ word alignment tool (Och and
Ney, 2003) for aligning inputs, trained on a por-
tion of the Europarl training data for each pair.
We select a skeleton input based on which
single-language translation system performs the
best when translating a development set. For our
Europarl test condition, this was French.
We define a minimal consistent alignment
(MCA) as a member of the set of multi-word
alignment pairs that can be extracted from a many-
to-many word alignment between skeleton sen-
tence x and non-skeleton sentence y with the fol-
lowing restrictions: (i) no word in x or y is used
more than once in the set of MCAs; (ii) words
and phrases selected from y cannot be aligned to
null; and (iii) no smaller MCA can be decomposed
from a given pair. This definition is similar to
that of minimal translation units as described in
Quirk and Menezes (2006), although they allow
null words on either side.
Different word alignment approaches will result
in different sets of MCAs. For input lattices, we
want sets of MCAs with as many aligned words
as possible, while minimising the average num-
ber of words in each pair in the set. Experiments
with GIZA++ on the Europarl data showed that
the ?grow-diag-final-and? word alignment sym-
metrization heuristic had the best balance between
coverage and pair length: over 85% of skeleton
words were part of a non-null minimal pair, and
the average length of each pair was roughly 1.5
words. This indicates that our lattices will pre-
serve most of the input space while collapsing eas-
ily alignable sub-segments.
Once a set of phrase alignments has been found,
we construct a lattice over the skeleton sentence
x. For each additional input yn we add a set of
links and nodes for each word in x to any relevant
? podr?a darnos las cifras correspondientes a espa?a y grecia ?
pouvez-vous nous donner les
chiffres pour  l' espagne et la gr?ce ?
siffrorna f?rkan ni ge oss spanien och grekland ?
Figure 4: A multi-lingual alignment between
French, Spanish and Swedish, showing the min-
imal consistent alignments. The lattice generated
by this alignment is shown in Figure 5.
words in yn, rejoining at the last word in x that
is covered by the pair. Figures 4 and 5 show an
example of the alignments and lattice generated by
using a French skeleton with Spanish and Swedish
sentences.
Once a lattice is created, we can submit it to a
phrase-based decoder in place of text input. The
decoder traverses lattice nodes in a manner simi-
lar to how words are traversed in text translation.
Instead of one input word represented by each lo-
cation in the coverage vector as in text input, with
lattices there are a set of possible input word arcs,
each with its own translation possibilities. The
concept of compatible coverage vectors for the lo-
cations of translated words becomes the notion of
reachability between frontier nodes in the lattice
(Dyer et al, 2008).
It is possible to construct multi-skeleton lat-
tices by connecting up a set of N lattices, each
built around a different skeleton xn, in much the
same manner as multiple confusion networks can
be connected to form a lattice in output combina-
tion. With sufficient diversity in the input order-
ing of each skeleton, the decoder need not perform
reordering. Because of the size and complexity
of these multi-skeleton lattices, we attempt only
monotonic decoding. In this scenario, as in con-
sensus decoding, we hope to exploit the additional
word order information provided by the alternative
skeletons.
3 Experiments: Monolingual Input
We start our experimental evaluation by translat-
ing multiple monolingual inputs into a foreign lan-
guage. This is a best-case scenario for testing
and analytic purposes because we have a single
translation model from one source language to one
target language. While translating from multiple
monolingual inputs is not a common use for ma-
chine translation, it could be useful in situations
where we have a number of paraphrases of the in-
put text, e.g., cross-language information retrieval
and summarization.
722
0 5pouvez-vous1? 2kan 7
darnos6nouspodr?a 3ni 4ge oss 9lasles 8siffrornadonner 11
chiffres10cifras 12f?r 13l' 14espa?aspanienapourcorrespondientes espagne 15yetoch 17la 18grecia 16grekland gr?ce 19???
Figure 5: A multi-lingual lattice input for French, Spanish, and Swedish from Europarl dev2006.
Data sets for this condition are readily available
in the form of test sets created for machine trans-
lation evaluation, which contains multiple target
references for each source sentence. By flipping
these test sets around, we create multiple mono-
lingual inputs (the original references) and a sin-
gle reference output (the original source text). We
examine two datasets: the BTEC Italian-English
corpus (Takezawa et al, 2002), and the Multiple
Translation Chinese to English (MTC) corpora,3
as used in past years? NIST MT evaluations.
All of our translation experiments use the
Moses decoder (Koehn et al, 2007), and are eval-
uated using BLEU-4. Moses is a phrase-based
decoder with features for lexicalized reordering,
distance-based reordering, phrase and word trans-
lation probabilities, phrase and word counts, and
an n-gram language model.
3.1 English to Italian
We use the portion of the BTEC data made avail-
able for the Italian-English translation task at
IWSLT 2007, consisting of approximately 24,000
sentences. We also use the Europarl English-
Italian parallel corpus to supplement our train-
ing data with approximately 1.2 million out-of-
domain sentences. We train a 5-gram language
model over both training corpora using SRILM
(Stolcke, 2002) with Kneser-Ney smoothing and
linear interpolation, the interpolation weight cho-
sen to minimise perplexity on the Italian side of
the development tuning set.
For multiple translation data, we use IWSLT
test sets devset1-3 which have sixteen English
translations for each Italian sentence. The Ital-
ian version of the BTEC corpus was created af-
ter the original Japanese-English version, and only
the first English translation was used to generate
the Italian data. The other fifteen versions of each
English sentence were generated as paraphrases
of the primary English translation. We explore
translation conditions using only the fifteen para-
phrased inputs (?Para.? in Table 2), as well as us-
ing all sixteen English inputs (?All?).
3LDC2002T01, LDC2003T17, LDC2004T07 and
LDC2006T04.
All Para.
BEST 40.06 24.02
ORACLE 51.64 47.27
MAX 29.32 23.94
SYSCOMB 32.89 30.39
CN INPUT 31.86 27.62
Table 2: BLEU scores on the BTEC test set for
translating English inputs into Italian.
We tune our translation models on devset1, sys-
tem combination on devset2 and report results on
devset3 for each condition.
When tuning the single input ?Para.? and ?All?
baselines, we include all relevant copies of the 506
lines of devset1 English data, and repeat the Ital-
ian reference fifteen or sixteen times on the target
side, resulting in a total of 7,590 and 8,096 sen-
tence pairs respectively.
The results for devset3 are shown in Table 2.
For comparison, we show the BEST score any in-
put produced, as well as an approximated ORA-
CLE output selection generated by choosing the
best BLEU-scoring output for each sentence using
a greedy search. Our output combination method,
SYSCOMB, uses no system-specific weights to
distinguish the inputs. For SYSCOMB and MAX,
we translated all versions of the English input sep-
arately, and we use the top ten distinct hypothe-
ses from each input sentence for n-best input to
SYSCOMB.
For input combination, CN INPUT, we used the
TER-based monolingual input lattice approach de-
scribed in Section 2.3, choosing as a skeleton the
input with the lowest average TER score when
compared with the other inputs (assessed sepa-
rately for each sentence). Each input was given
equal probability in the confusion network links.
Note that the quality of output from translat-
ing the primary English input is much higher than
from translating any of the paraphrases. The pri-
mary input sentence scores a BLEU of 40.06, while
the highest scoring paraphrased input manages
only a 24.02. When we look at ?Para.? the dif-
ference in the scores when using a single input
723
(BEST) versus all the inputs (SYSCOMB and CN
INPUT) is striking ? clearly there is considerable
information in the other inputs which can radically
improve the translation output. Removing the pri-
mary input from ORACLE reinforces this observa-
tion: the score drops by only 4.37 BLEU despite
the nearly 16 BLEU drop for the single best input.
Interestingly, the output selection technique,
MAX, performs at a similar level to the combina-
tion techniques when we include the primary in-
put, but degrades when given only the lower qual-
ity translations of paraphrased input under condi-
tion ?Para.? In previous work on multi-lingual out-
put selection, the MAX score degraded after two
or three outputs were combined, but even with-
out the primary reference it maintains a score near
the best single paraphrased input when combining
fifteen outputs. One possible explanation for this
is that the inputs are all being translated with the
same translation model, so comparing their scores
can give a more accurate ranking of their relative
translation quality according to the model. The
input combination method, CN INPUT, performs
better than MAX and only slightly worse than the
output combination approach.
3.2 English to Chinese
We can add an extra dimension to monolingual
multi-source translation by considering inputs of
differing quality. A multi-source translation sys-
tem can exploit features indicating the origin of the
input to improve output quality. For these exper-
iments, we use the MTC English-Chinese corpus,
parts 1?4. This data was translated from Chinese
into English by four teams of annotators, denoted
E01?E04. This allows us to examine the results
for translating the same team?s work over multiple
years.
We train on the news domain portion of the of-
ficial NIST data4 (excluding the UN and Hong
Kong data) for both the translation model and the
5-gram Chinese language model.
While we still have a single translation model,
all of our inputs are now of a traceable origin and
are known to have quality differences when judged
by human evaluators. With this information we
can tune one of two ways: We can create a set of
all input systems and replicate the reference as we
did for English to Italian translation (?All tuned?),
4http://www.nist.gov/speech/tests/mt/
2008
Team Tuning Part 3 Part 4
E01 All 16.18 15.52
E01 Self 16.02 15.63
E02 All 14.29 14.00
E02 Self 13.88 14.05
E03 All 14.99 15.06
E03 Self 15.10 14.94
E04 All 14.03 12.65
E04 Self 14.03 12.59
Table 3: BLEU scores using single inputs from
each different team on the MTC. Bold indicates
the better score between All and Self tuning.
Approach Tuning Part 3 Part 4
MAX All 15.06 15.08
MAX Self 14.97 13.75
SYSCOMB All 16.82 16.24
SYSCOMB Self 16.87 16.45
Table 4: BLEU scores for multi-source translations
of MTC test sets. Better score for each output-
based multi-source method is shown in bold.
or we can tune each input using only the version of
the tuning data generated by the same translation
team (?Self tuned?).5 For example, we can tune
a system with the MTC Part 2 data provided by
translation team E01, and then decode E01?s trans-
lations of parts 3 and 4 with the weights obtained
in tuning. The results for each system are shown
in Table 3. Despite the different tuning conditions,
there is no clear advantage to tuning to all inputs
versus tuning to each input separately ? on aver-
age we see a 0.06 BLEU score advantage by using
?All? weights.
With four different inputs to our multi-source
translation system, and two ways of weighting the
features for each input, how can we best utilize
these systems in output selection and combina-
tion? We perform system combination and MAX
selection and obtain the scores shown in Table 4.
The consensus decoding approach uses system-
specific features as described in Section 2.2 to dis-
tinguish between E01-E04.
As with English to Italian, output combination
performs the best of the multi-source techniques.
MAX performs better with translations generated
by ?All? weights than with ?Self?, and the con-
5Note that in the ?Self tuned? setting we have only a quar-
ter as much tuning data as for ?All tuned?.
724
Input Language test2006 test2007
French (FR) 29.72 30.21
Spanish (ES) 29.55 29.62
Swedish (SV) 29.33 29.44
Portuguese (PT) 28.75 28.79
Danish (DA) 27.20 27.48
Greek (EL) 26.93 26.78
Italian (IT) 26.82 26.51
German (DE) 24.04 24.41
Dutch (NL) 23.79 24.28
Finnish (FI) 18.96 18.85
Table 5: BLEU scores for individual translation
systems into English trained on Europarl, from
best to worst.
verse is true for SYSCOMB. Given the robust per-
formance of MAX when translation scores origi-
nated from the same translation model in English
to Italian, it is not surprising that it favors the
case where all the outputs are scored by the same
model (?All tuned?). On the other hand, diversity
amongst the system outputs has been shown to be
important to the performance of system combina-
tion techniques (Macherey and Och, 2007). This
may give an indication as to why the ?Self tuned?
data produced higher scores in consensus decod-
ing ? the outputs will be more highly divergent due
to their different tuning conditions.
4 Experiments: Multilingual Input
Multilingual cases are the traditional realm of
multi-source translation. We no longer have di-
rectly comparable translation models; instead each
input language has a separate set of rules for trans-
lating to the output language. However, the avail-
ability of (and demand for) multi-parallel corpora
makes this form of multi-source translation of
great practical use.
4.1 Lattice Inputs
As described in Section 2.3, lattices can be used
to provide a compact format for translating multi-
lingual inputs to a multi-source translation system.
We trim all non-skeleton node paths to a maximum
length of four to reduce complexity when decod-
ing. Such long paths are mostly a result of errors in
the original word alignments, and therefore prun-
ing these links is largely innocuous.
We train on the Europarl corpus and use the
FR SV ES DA PT IT EL NL DE FI
BLE
U
0.15
0.20
0.25
0.30
0.35
0.40
0.45 OracleMAXSolo
Figure 6: Performance for multilingual multi-
source translation (test2005) as each language in-
put is added, showing Oracle target selection,
MAX score, or just a single language input (Solo).
in-domain test sets provided for previous years?
Workshops on Statistical Machine Translation.
Because of the computational complexity of deal-
ing with so many models, we train on only the first
100,000 sentences of each parallel corpus. Sin-
gle system baseline scores for each language are
shown in Table 5.
Besides comparing the different multi-source
translation methods discussed above, in this task
we also want to examine what happens when we
use different numbers of input languages. To de-
termine the best order to add languages, we per-
formed a greedy search over oracle BLEU scores
for test set test2005. We started with the best scor-
ing single system, French to English, and in each
iteration picked one additional system that would
maximise BLEU if we always selected the trans-
lation system output closest to the reference. The
results are shown in Figure 6.
The oracle selection order differs from the or-
der of the best performing systems, which could
be due to the high scoring systems having very
similar output while lower scoring systems exhibit
greater diversity. Interestingly, the order of the
languages chosen iterates between the Roman and
Germanic language families and includes Greek
early on. This supports our claim that diversity
is important. Note though that Finnish, which is
also in a separate language family, is selected last,
most likely due to difficulties in word alignment
and translation stemming from its morphological
complexity (Birch et al, 2008). This finding might
also carry over to phrase-table triangulation (Cohn
and Lapata, 2007), where multi-parallel data is
used in training to augment a standard translation
725
Approach test2006 test2007
French Only 29.72 30.21
French + Swedish
MAX 29.86 30.13
LATTICE 29.33 29.97
MULTILATTICE 29.55 29.88
SYSCOMB 31.32 31.77
French + Swedish + Spanish
MAX 30.18 30.33
LATTICE 29.98 30.45
MULTILATTICE 30.50 30.50
SYSCOMB 33.77 33.87
6 Languages
MAX 28.37 28.33
LATTICE 30.22 30.91
MULTILATTICE 30.59 30.59
SYSCOMB 35.47 36.03
Table 6: BLEU scores for multi-source translation
systems into English trained on Europarl. Single
source French decoding is shown as a baseline.
system.
We choose to evaluate translation perfor-
mance at three combination levels: two lan-
guages (French and Swedish), three languages
(+Spanish), and six languages (+Danish, Por-
tuguese, Italian). For each combination we ap-
ply MAX, SYSCOMB, French skeleton lattice in-
put translation LATTICE, and monotone decoding
over multiple skeleton lattices, MULTILATTICE.
Results are shown in Table 6.
To enable the decoder used in LATTICE and
MULTILATTICE to learn weights for different
sources, we add a feature to the phrase table for
each of the languages being translated. This fea-
ture takes as its value the number of words on the
source side of the phrase. By weighting this fea-
ture up or down for each language, the decoder can
prefer word links from specific languages.
As seen in previous work in multi-source trans-
lation, MAX output selection performs well with
two or three languages but degrades as more lan-
guages are added to the input. Conversely, our
lattice input method shows upward trends: LAT-
TICE is comparable with MAX on three inputs and
scores increase in the six language case.
Given the higher scores for output combination
over input combination, what differences can we
observe between the systems? Both systems have
features that indicate the contributions of each in-
put language to the final output. With input com-
bination, we are forced by the decoder to take the
maximum scoring path through the lattice, but in
output combination we have the aggregate vote of
word confidences generated by each system. If we
could combine word arc scores across inputs, as in
output combination, we might get a more robust
solution for taking advantage of the available sim-
ilarities on the target side of the translation. This
points to a direction for future research.
Other differences between the systems may ex-
plain the score gap between our input and output
combination approaches. Consensus decoding al-
lows you to mix and match fragments that aren?t
necessarily stored as fragments in the phrase table.
Another difference is the richer space of reorder-
ings in TER-based lattices, due to the ability of the
metric to handle long-distance alignments.
5 Conclusion
We analyzed three approaches for dealing with
multi-source translation. While MAX is mostly a
poor performer, the upper bound of output selec-
tion is stunning. The very positive results for out-
put system combination across all data conditions
are quite promising. Output combination achieves
these results while the using the limited expres-
sive power of n-best inputs. The potential of using
a more expressive format ? such as lattices that
represent the joint search space of multiple mod-
els ? is high. Our first attempts at adapting lattices
to multi-source translation input show promise for
future development. We have only scratched the
surface of methods for constructing input lattices,
and plan to actively continue research into improv-
ing these methods.
Acknowledgments
Thanks to Chris Callison-Burch for many insight-
ful discussions, and to Chris Dyer for his imple-
mentation of lattice decoding in Moses.
This work was supported by the EuroMatrix
project funded by the European Commission (6th
Framework Programme), and has made use of
the resources provided by the Edinburgh Com-
pute and Data Facility (http://www.ecdf.
ed.ac.uk/), which is partially supported by the
eDIKT initiative (http://www.edikt.org).
We also acknowledge the support of the EPSRC
(grant GR/T04557/01).
726
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceed-
ings of ASRU, pages 351?354, Trento, Italy, Decem-
ber.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of NAACL: HLT, pages 16?23, Edmonton, Canada,
May.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proceedings of IEEE ICASSP, pages
1297?1300, Honolulu, Hawaii, USA, April.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of EMNLP, pages 745?754, Honolulu,
Hawaii, USA, October.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use
of multi-parallel corpora. In Proceedings of ACL,
pages 728?735, Prague, Czech Republic, June.
Marta Ruiz Costa-jussa`, Josep M. Crego, Patrik Lam-
bert, Maxim Khalilov, Jose? A. R. Fonollosa, Jose? B.
Mario, and Rafael E. Banchs. 2007. Ngram-based
statistical machine translation enhanced with mul-
tiple weighted reordering hypotheses. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 167?170, Prague, Czech Repub-
lic, June.
Christopher J. Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL: HLT, pages 1012?
1020, Columbus, Ohio, USA, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL: Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit X, pages 79?86, Phuket, Thailand,
September.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems.
In Proceedings of EMNLP-CoNLL, pages 986?995,
Prague, Czech Republic, June.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation for multi-
ple machine translation systems using enhanced hy-
pothesis alignment. In Proceedings of EACL, pages
33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2001. Statis-
tical multi-source translation. In Proceedings of
MT Summit VIII, pages 253?258, Santiago de Com-
postela, Spain, September.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?52.
Matthias Paulik, Kay Rottmann, Jan Niehues, Al-
mut Silja Hildebrand, and Stephan Vogel. 2007.
The ISL phrase-based MT system for the 2007 ACL
workshop on statistical machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 197?202, Prague, Czech
Republic, June.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
statistical machine translation. In Proceedings of
ACL: HLT, Main Conference, pages 9?16, New
York, New York, USA, June.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system
combination for machine translation. In Proceed-
ings of ACL, pages 312?319, Prague, Czech Repub-
lic, June.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bon-
nie J. Dorr. 2007b. Combining output from mul-
tiple machine translation systems. In Proceedings
of NAACL: HLT, pages 228?235, Rochester, New
York, USA, April.
Lane Schwartz. 2008. Multi-source translation meth-
ods. In Proceedings of AMTA, pages 279?288,
Waikiki, Hawaii, USA, October.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of AMTA, pages 223?231,
Boston, Massachusetts, USA, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado, USA, October.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-
aya, Hirofumi Yamamoto, and Seiichi Yamamoto.
2002. Toward a broad-coverage bilingual corpus for
speech translation of travel conversations in the real
world. In Proceedings of LREC, pages 147?152,
Las Palmas, Canary Islands, Spain, May.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings of
IWSLT, Pittsburgh, Pennsylvania, USA, October.
727
Constructing Corpora for the Development
and Evaluation of Paraphrase Systems
Trevor Cohn?
University of Edinburgh
Chris Callison-Burch??
Johns Hopkins University
Mirella Lapata?
University of Edinburgh
Automatic paraphrasing is an important component in many natural language processing tasks.
In this article we present a new parallel corpus with paraphrase annotations. We adopt a defini-
tion of paraphrase based on word alignments and show that it yields high inter-annotator agree-
ment. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is
appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed
in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1)
and also in developing linguistically rich paraphrase models based on syntactic structure.
1. Introduction
The ability to paraphrase text automatically carries much practical import for many
NLP applications ranging from summarization (Barzilay 2003; Zhou et al 2006) to
question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine
translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising
that recent years have witnessed increasing interest in the acquisition of paraphrases
from real world corpora. These are most often monolingual corpora containing parallel
translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and
Marcu 2003). Truly bilingual corpora consisting of documents and their translations have
also been used to acquire paraphrases (Bannard and Callison-Burch 2005; Callison-
Burch 2007) as well as comparable corpora such as collections of articles produced
by two different newswire agencies about the same events (Barzilay and Elhadad
2003).
Although paraphrase induction algorithms differ in many respects?for example,
the acquired paraphrases often vary in granularity as they can be lexical (fighting, battle)
or structural (last week?s fighting, the battle last week), and are represented as words or
? School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk.
?? Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218.
E-mail: ccb@cs.jhu.edu.
? School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for
publication: 26 March 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
syntax trees?they all rely on some form of alignment for extracting paraphrase pairs.
In its simplest form, the alignment can range over individual words, as is often done
in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments
range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay
and Lee 2003).
The obtained paraphrases are typically evaluated via human judgments. Para-
phrase pairs are presented to judges who are asked to decide whether they are seman-
tically equivalent, that is, whether they can be generally substituted for one another in
the same context without great information loss (Barzilay and Lee 2003; Barzilay and
McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In
some cases the automatically acquired paraphrases are compared against manually gen-
erated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance
increase for a specific application, such as machine translation (Callison-Burch, Koehn,
and Osborne 2006).
Unfortunately, manually evaluating paraphrases in this way has at least three draw-
backs. First, it is infeasible to perform frequent evaluations when assessing incremental
system changes or tuning system parameters. Second, it is difficult to replicate results
presented in previous work because there is no standard corpus, and no standard evalu-
ation methodology. Consequently comparisons across systems are few and far between.
The third drawback concerns the evaluation studies themselves, which primarily focus
on precision. Recall is almost never evaluated directly in the literature. And this is
for a good reason: There is no guarantee that participants will identify the same set
of paraphrases as each other or with a computational model. The problem relates to
the nature of the paraphrasing task, which has so far eluded formal definition (see
the discussion in Barzilay [2003]). Such a definition is not so crucial when assessing
precision, because subjects are asked to rate the paraphrases without actually having to
identify them. However, recall might be measured with respect to some set of ?gold-
standard? paraphrases which will have to be collected according to some concrete
definition.
In this article we present a resource that could potentially be used to address
these problems. Specifically, we create a monolingual parallel corpus with human
paraphrase annotations. Our working definition of paraphrase is based on word and
phrase1 alignments between semantically equivalent sentences. Other definitions are
possible, for instance we could have asked our annotators to identify all constituents
that are more or less meaning preserving in our parallel corpus. We chose to work
with alignments for two reasons. First, the notion of alignment appears to be central
in paraphrasing?most existing paraphrase induction algorithms rely on alignments
either implicitly or explicitly for identifying paraphrase units. Secondly, research in
machine translation, where several gold-standard alignment corpora have been created,
shows that word alignments can be identified reliably by annotators (Melamed 1998;
Och andNey 2000b;Mihalcea and Pedersen 2003;Martin,Mihalcea, and Pedersen 2005).
We therefore create word alignments similar to those observed in machine transla-
tion, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links
between words. Alignment blocks larger than one-to-one are used to specify phrase
correspondences.
1 Our definition of the term phrase follows the SMT literature. It refers to any contiguous sequence of
words, whether it is a syntactic constituent or not. See Section 2 for details.
598
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
In the following section we explain how our corpus was created and summarize our
annotation guidelines. Section 3 gives the details of an agreement study, demonstrating
that our annotators can identify and align paraphrases reliably. We measure agreement
using alignment overlap measures from the SMT literature, and also introduce a novel
agreement statistic for non-enumerable labeling spaces. Section 4 illustrates how the
corpus can be used in paraphrase research, for example, as a test set for evaluating
the output of automatic systems or as a training set for the development of paraphrase
systems. Discussion of our results concludes the article.
2. Corpus Creation and Annotation
Our corpus was compiled from three data sources that have been previously used for
paraphrase induction (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003;
Dolan, Quirk, and Brockett 2004): the Multiple-Translation Chinese (MTC) corpus,
Jules Verne?s Twenty Thousand Leagues Under the Sea novel (Leagues), and the Microsoft
Research (MSR) paraphrase corpus. These are monolingual parallel corpora, aligned at
the sentence level. Both source and target sentences are in English, and express the same
content using different surface forms.
The MTC corpus contains news stories from three sources of journalistic Mandarin
Chinese text.2 These stories were translated into English by 11 translation agencies.
Because the majority of the translators were non-native English speakers, occasionally
translations contain syntactic or grammatical errors and are not entirely fluent. After
inspection, we identified four translators with consistently fluent English, and used
their sentences for our corpus. The Leagues corpus contains two English translations
of the French novel Twenty Thousand Leagues Under the Sea. The corpus was created
by Tagyoung Chung and manually aligned at the paragraph level.3 In order to obtain
sentence level paraphrase pairs, we sampled from the subset of one-to-one sentence
alignments. The MSR corpus was harvested automatically from online news sources.4
The obtained sentence pairs were further submitted to judges who rated them as being
semantically equivalent or not (Dolan, Quirk, and Brockett 2004). We only used seman-
tically equivalent pairs. The sentence pairs were filtered for length (? 50) and length
ratio (? 1 : 9 between the shorter and longer sentence). This was necessary to prune out
incorrectly aligned sentences.
We randomly sampled 300 sentence pairs from each corpus (900 in total). Of these,
300 pairs (100 per corpus) were first annotated by two coders to assess inter-annotator
agreement. The remaining 600 sentence pairs were split into two distinct sets, each
consisting of 300 sentences (100 per corpus), and were annotated by a single coder.
Each coder annotated the same amount of data. In addition, we obtained a trial set
of 50 sentences from the MTC corpus which was used for familiarizing our annotators
with the paraphrase alignment task (this set does not form part of the corpus). In sum,
we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly
annotated.
To speed up the annotation process, the data sources were first aligned automati-
cally and then hand-corrected.We usedGiza++ (Och andNey 2003), a publicly available
2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1.
3 The corpus can be downloaded from http://www.isi.edu/?knight/.
4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D9-
20CD-47E3-85BC-A2F65CD28042/Details.aspx.
599
Computational Linguistics Volume 34, Number 4
implementation of the IBM word alignment models (Brown et al 1993). Giza++ was
trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair-
ings of English translations as training instances. This resulted in 55 =
11?(11?1)
2 training
pairs per sentence and a total of 54, 615 training pairs. In addition, we augmented the
training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan
(2004). This follows standard practice in SMT where entries from a bilingual dictionary
are added to the training set (Och and Ney 2000a), except in our case the ?dictionary?
is monolingual and specifies that each word type can be paraphrased as itself. This is
necessary in order to inform Giza++ about word identity.
A common problem with automatic word alignments is that they are asymmetric:
one source word can only be aligned to one target word, whereas one target word can
be aligned to multiple source words. In SMT, word alignments are typically predicted
in both directions: source-to-target and target-to-source. These two alignments are then
merged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).
Symmetrization improves the alignment quality compared to that of a single directional
model, while also allowing a greater range of alignment types (i.e., some many-to-
one, one-to-many, and many-to-many alignments can be produced). Analogously, we
obtained word alignments in both directions6 which we subsequently merged by taking
their intersection. This resulted in a high precision and low recall alignment.
Our annotators (two linguistics graduates) were given pairs of sentences and asked
to show which parts of these were in correspondence by aligning them on a word-by-
word basis.7 Our definition of alignment was fairly general (Och andNey 2003): Given a
source string X = x1, . . . , xN and a target string Y = y1, . . . , yM, an alignmentA between
two word strings is the subset of the Cartesian product of the word positions:
A ? {(n,m) : n = 1, . . . ,N;m = 1, . . . ,M} (1)
We did not provide a formal definition of what constitutes a correspondence. As a
rule of thumb, annotators were told to align words or phrases x ? y in two sentences
(X,Y) whenever the words x could be substituted for y in Y, or vice versa. This relation-
ship should hold within the context of the sentence pair in question: the relation x ? y
need not hold in general contexts. Trivially this definition allowed for identical word
pairs.
Following common practice (Och, Tillmann, and Ney 1999; Och and Ney 2003;
Daume? III and Marcu 2004), we distinguished between sure (S) and possible (P) align-
ments, where S ? P. The intuition here is that sure alignments are clear-cut decisions
and typical of genuinely substitutable words or phrases, whereas possible alignments
flag a correspondence that has slightly divergent syntax or semantics. Annotators were
encouraged to produce sure alignments. They were also instructed to prefer smaller
alignments whenever possible, but were allowed to create larger block alignments.
Smaller alignments were generally used to indicate lexical correspondences, whereas
block alignments were reserved for non-compositional phrase pairs (e.g., idiomatic
expressions) or simply expressions with radically different syntax or vocabulary. In
5 The IBM alignment models require a large amount of parallel data to yield reliable alignments. We
therefore selected the MTC for training purposes as it was the largest of our parallel corpora.
6 We used five iterations for each of Model 1, Model 2, and the HMMmodel.
7 The annotation was conducted using a Web-based alignment tool available at
http://demo.linearb.co.uk/paraphrases/.
600
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Figure 1
Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid. Black
squares represent sure alignment, gray squares represent possible alignment.
cases where information in one sentence was not present in the other, the annotators
were asked to leave this information unaligned.
Finally, annotators were given a list of heuristics to help them decide how to
make alignments in cases of ambiguity. These heuristics handled the alignment of
named entities (e.g., George Bush) and definite descriptions (e.g., the president), tenses
(e.g., had been and shall be), noun phrases with mismatching determiners (e.g., a man
and the man), verb complexes (e.g., was developed and had been developed), phrasal
verbs (e.g., take up and accept), genitives (e.g., Bush?s infrequent speeches and the infre-
quent speeches by Bush), pronouns, repetitions, typographic errors, and approximate
correspondences. For more details, we refer the interested reader to our annotation
guidelines.8
Figure 1 shows the alignment for two sentence pairs from the MTC corpus. The
first pair (Australia is concerned with the issue of carbon dioxide emissions. ? The problem
of greenhouse gases has attracted Australia?s attention.) contains examples of word-to-
word (the ? The; issue ? problem; of ? of ; Australia ? Australia) and many-to-many
alignments (carbon dioxide emissions ? greenhouse gases). Importantly, we do not use
a large many-to-many block for Australia is concerned with and has attracted Australia?s
attention because it is possible to decompose the two phrases into smaller alignments.
The second sentence pair illustrates a possible alignment (could have very long term
effects? was of profound significance) indicated by the gray squares. Possible alignments
are used here because the two phrases only loosely correspond to each other. Possible
alignments are also used to mark significant changes in syntax where the words denote
a similar concept: for example, in cases where two words have the same stem but are
8 Both the corpus and the annotation guidelines can be found at: http://homepages.inf.ed.ac.uk/
tcohn/paraphrase corpus.html.
601
Computational Linguistics Volume 34, Number 4
expressed with different parts of speech, (e.g., co-operative ? cooperation) or when two
verbs are used that are not synonyms (e.g., this is also? this also marks).
3. Human Agreement
As mentioned in the previous section, 300 sentence pairs (100 pairs from each sub-
corpus) were doubly annotated, in order to measure inter-annotator agreement. Here,
we treat one annotator as gold-standard (reference) andmeasure the extent to which the
other annotator deviates from this reference.
Word-Based Measures. The standard technique for evaluating word alignments is to
represent them as a set of links (i.e., pairs of words) and compare them against gold-
standard alignments. The quality of an alignmentA (defined in Equation (1)) compared
to reference alignment B can be then computed using standard recall, precision, and
F1 measures (Och and Ney 2003):
Precision =
|AS ? BP|
|AS|
Recall =
|AP ? BS|
|BS|
F1 = 2 ? Precision ? Recall
Precision+ Recall
(2)
where the subscripts S and P denote sure and possible word alignments, respectively.
Note that both precision and recall are asymmetric in that they compare sets of possible
and sure alignments. This is designed to be maximally generous: sure predictions
which are present in the reference as possibles are not penalized in precision, and the
converse applies for recall. We adopt Fraser and Marcu (2007)?s definition of F1, an
F-measure between precision and recall over the sure and possibles. They argue that
it is a better alternative to the commonly used Alignment Error Rate (AER), which
does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-
lingual, in order to avoid artificial score inflation, we limit the precision and recall
calculations to consider only pairs of non-identical words (and phrases, as discussed
subsequently).
To give an example, consider the sentence pairs in Figure 2, whose alignments have
been produced by the two annotators A (left) and B (right). Table 1 shows the individual
word alignments for each annotator and their type (sure or possible). In order to mea-
sure F1, we must first estimate Precision and Recall (see Equation (2)). Treating annota-
tor B as the gold standard, |AS| = 4, |BS| = 5, |AS ? BP| = 4, and |AP ? BS| = 4. This
results in a precision of 44 = 1, a recall of
4
5 , and F1 of
2?1?0.8
1+0.8 = 0.89. Note that we ignore
alignments over identical words (i.e., discussed ? discussed, the ? the, and ? and,
. ? .).
Phrase-Based Measures. The given definitions are all word-based; however, our annota-
tors, and several paraphrasing models, create correspondences not only between words
but also between phrases. To take this into account, we also evaluate these measures
over larger blocks (similar to Ayan and Dorr [2006]). Specifically, we extract phrase
pairs from the alignments produced by our annotators using a modified version of the
standard SMT phrase extraction heuristic (Och, Tillmann, and Ney 1999). The heuristic
9 Fraser and Marcu (2007) also argue for an unbalanced F-measure to bias towards recall. This is shown to
correlate better with translation quality. For paraphrasing it is not clear if such a bias would be beneficial.
602
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Figure 2
Sample sentence pair showing the word alignments from two annotators.
extracts all phrase pairs consistent with the word alignment. These include phrase pairs
whose words are aligned to each other or nothing, but not to words outside the phrase
boundaries.10 The phrase extraction heuristic creates masses of phrase pairs, many of
which are of dubious quality. This is often due to the inclusion of unaligned words or
simply to the extraction of overly-large phrase pairs which might be better decomposed
into smaller units. For our purposes we wish to be maximally conservative in how we
process the data, and therefore we do not extract phrase pairs with unaligned words on
their boundaries.
Figure 3 illustrates the types of phrase pairs our extraction heuristic permits. Here,
the pair and reached ? and arrived at is consistent with the word alignment. In contrast,
the pair and reached ? and arrived isn?t; there is an alignment outside the hypothetical
phrase boundary which is not accounted for (reached is also aligned to at). The phrase
pair and reached an ? and arrived at is consistent with the word alignment; however it
has an unaligned word (i.e., an) on the phrase boundary, which we disallow.
Our phrase extraction procedure distinguishes between two types of phrase pairs:
atomic, that is, the smallest possible phrase pairs, and composite, which can be created
by combining smaller phrase pairs. For example, the phrase pair and reached ? and
arrived at in Figure 3 is composite, as it can be decomposed into and ? and and
reached ? arrived at. Table 2 shows the atomic and composite phrase pairs extracted
from the possible alignments produced by annotators A and B for the sentence pair in
Figure 2.
We compute recall, precision, and F1 over the phrase pairs extracted from the word
alignments as follows:
Precision =
|Apatom ? B
p|
|Apatom|
Recall =
|Ap ? Bpatom|
|Bpatom|
F1 = 2 ? Precision ? Recall
Precision+ Recall
(3)
10 The term phrase is not used here in the linguistic sense; many extracted phrases will not be constituents.
603
Computational Linguistics Volume 34, Number 4
Table 1
Single word pairs specified by the word alignments from Figure 2, for two annotators, A and B.
The column entries specify the alignment type for each annotator, either sure (S) or possible (P).
Dashes indicate that the word pair was not predicted by the annotator. Italics denote lexically
identical word pairs.
Word alignments A B
they ? both ? P
they ? parties P P
discussed ? discussed S S
the ? the S S
aspects ? specific P ?
in ? specific P P
detail ? specific P P
aspects ? issues P S
in ? issues P ?
detail ? issues P ?
and ? and S S
reached ? arrived S S
reached ? at ? S
an ? a S S
extensive ? general S P
agreement ? consensus S S
. ? . S S
Figure 3
Validity of phrase pairs according to the phrase extraction heuristic. Only the leftmost phrase
pair is valid. The others are inconsistent with the alignment or have an unaligned word on a
boundary, respectively, indicated by a cross.
where Ap and Bp are the predicted and reference phrase pairs, respectively, and
the atom subscript denotes the subset of atomic phrase pairs, Apatom ? A
p. As shown
in Equation (3) we measure precision and recall between atomic phrase pairs and
the full space of atomic and composite phrase pairs. This ensures that we do not
multiply reward composite phrase pair combinations,11 while also not unduly pe-
nalizing non-matching phrase pairs which are composed of atomic phrase pairs in
11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore
might multiply count phrase pairs.
604
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Table 2
Phrase pairs are specified by the word alignments from Figure 2, using the possible alignments.
The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the
remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs.
?This phrase pair is atomic in A but composite in B.
Atomic phrase pairs A B
they ? parties P ?
they ? both parties ? P
discussed ? discussed S S
the ? the S S
aspects in detail ? specific issues P ?
in detail ? specific ? P
aspects ? issues ? S
and ? and S S
reached ? arrived S S
reached ? arrived at ? S
reached an ? arrived at a S P?
an ? a S S
extensive ? general S P
agreement ? consensus S S
. ? . S S
Composite phrase pairs A B
they discussed ? both parties discussed ? P
they discussed ? parties discussed P ?
they discussed the ? both parties discussed the ? P
they discussed the ? parties discussed the P ?
they ... reached an ? both parties ... arrived at a P ?
the aspects in detail ? the specific issues P P
reached an extensive ? arrived at a general S S
extensive agreement . ? general consensus . S S
...
the reference. Returning to the example in Table 2, with annotator B as the gold
standard, |Apatom| = 7, |B
p
atom| = 8, |A
p
atom ? B
p| = 5, and |Ap ? Bpatom| = 4. Consequently,
precision= 57 = 0.71, recall=
4
8 = 0.50, and F1=
2?0.71?0.50
0.71+0.50 = 0.59. Again we ignore
identical phrase pairs.
A potential caveat here concerns the quality of the atomic phrase pairs, which are
automatically induced and may not correspond to linguistic intuition. To evaluate this,
we had two annotators review a random sample of 166 atomic phrase pairs drawn from
the MTC corpus (sure), classifying each phrase pair as correct, incorrect, or uncertain
given the sentence pair as context. From this set, 73% were deemed correct, 22% un-
certain, and 5% incorrect.12 Annotators agreed in their decisions 75% of the time (using
the Kappa13 statistic, their agreement is 0.61). This confirms that the phrase-extraction
process produces reliable phrase pairs from our word-aligned data (although we cannot
claim that it is exhaustive).
12 Taking a more conservative position by limiting the proportion of unaligned words within the phrase
pair improves these figures monotonically to 90% correct and 0% incorrect (fully aligned phrase pairs).
13 This Kappa is computed over three nominal categories (correct, incorrect, and uncertain) and should not
be confused with the agreement measure we develop in the following section for phrase pairs.
605
Computational Linguistics Volume 34, Number 4
Chance-Corrected Agreement. Besides precision and recall, inter-annotator agreement is
commonly measured using the Kappa statistic (Cohen 1960). Thus is a desirable mea-
sure because it is adjusted for agreement due purely to chance:
? =
Pr(A)? Pr(E)
1? Pr(E)
(4)
where Pr(A) is the proportion of times two coders14 agree, corrected by Pr(E), the
proportion of times we would expect them to agree by chance.
Kappa is a suitable agreement measure for nominal data. An example would be a
classification task, where two coders must assign n linguistic instances (e.g., sentences
or words) into one of m categories. Given this situation, it would be possible for each
coder to assign each instance to the same category. Kappa allows us to quantify whether
the coders agree with each other about the category membership of each instance. It is
relatively straightforward to estimate Pr(A)?it is the proportion of instances on which
the two coders agree. Pr(E) requires a model of what would happen if the coders were
to assign categories randomly. Under the assumption that coders r1 and r2 are indepen-
dent, the chance of them agreeing on the jth category is the product of each of them
assigning an instance to that category: Pr(Cj|r1) Pr(Cj|r2). Chance agreement is then the
sum of this product across all categories: Pr(E) =
m
?
j=1
Pr(Cj|r1) Pr(Cj|r2). The literature
describes two different methods for estimating Pr(Cj|ri). Either a separate distribution
is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott
1955; Fleiss 1971; Siegel and Castellan 1988).We refer the interested reader to Di Eugenio
and Glass (2004) and Artstein and Poesio (2008) for a more detailed discussion.
Unfortunately, Kappa is not universally suited to every categorization task. A prime
example is structured labeling problems that allow a wide variety of output categories.
Importantly, the number and type of categories is not fixed in advance and can vary
from instance to instance. In parsing, annotators are given a sentence for which they
must specify a tree, of which there is an exponential number in the sentence length. Sim-
ilarly, in our case the space of possible alignments for a sentence pair is also exponential
in the input sentence lengths. Considering these annotations as nominal variables is
inappropriate.
Besides, alignments are only an intermediate representation that we have used to
facilitate the annotation of paraphrases. Ideally, we would like to measure agreement
over the set of phrase pairs which are specified by our annotators (via the word align-
ments), not the alignment matrices themselves.
Kupper and Hafner (1989) present an alternative measure similar to Kappa that is
especially designed for sets of variables:
C? =
??? ?0
1? ?0
, (5)
where ?? =
I
?
i=1
|Ai ? Bi|
min(|Ai|, |Bi|)
, and ?0 =
1
Ik
?
i
min(|Ai|, |Bi|)
14 Kappa has been extended to more than two coders (Fleiss 1971; Bartko and Carpenter 1976). For
simplicity?s sake our discussion and subsequent examples involve two coders. Also note that we use the
term coder instead of the more common rater. This is because in our task the annotators must identify
(a.k.a. code) the paraphrases rather than rate them.
606
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Here, Ai and Bi are the coders? predictions on sentence pair i from our corpus of I
sentence pairs. Each prediction is a subset of the full space of k items. Expression (5)
measures the agreement (or concordance) between coders A and B and follows the
general form of Kappa from Equation (4), which is defined analogously with Pr(A) and
Pr(E) taking the roles of ?? and ?0, but with different definitions.
Kupper and Hafner (1989) developed their agreement measure with medical diag-
nostic tasks in mind. For example, two physicians classify subjects into k = 3 diagnostic
categories and wish to find out whether they agree in their diagnoses. Here, each coder
must decidewhich (possibly empty) subset from k categories best describes each subject.
The size of k is thus invariant with the instance under consideration. This is not true
in our case, where k will vary across sentence pairs as sentences of different lengths
license different numbers of phrase pairs. More critically, the formulation in Equa-
tion (5) assumes that items in the set are independent: All subsets of the same car-
dinality as k are equally likely, and no combination is impossible. This independence
assumption is inappropriate for the paraphrase annotation task. The phrase extraction
heuristic allows each contiguous span in a sentence to be aligned to either zero or one
span in the other sentence; that is, nominating a phrase pair precludes the choice of
many other possible phrase pairs. Consequently relatively few of the subsets of the
full set of possible phrase pairs are valid. Formally, an alignment can specify only
O(N2) phrase pairs from a total set of k = O(N4) possible phrase pairs. This disparity in
magnitudes leads to increasingly underestimated ?? for larger N, namely, limN?? ?0 =
limN??O(N
2)/O(N4) = 0. The end result is an overestimate of C? on longer sentences.
For these reasons, we adapt the method of Kupper and Hafner (1989) to account for
our highly interdependent item sets. We use C? from Equation (5) as our agreement sta-
tistic defined over sets of atomic phrase pairs, that is, A = Apatom,B = B
p
atom. We redefine
?0 as follows:
?0 =
1
I
I
?
i=1
?
Apatom
?
Bpatom
Pr(Apatom) Pr(B
p
atom)
|Apatom ? B
p
atom|
min(|Apatom|, |B
p
atom|)
(6)
where Apatom and B
p
atom range over the sets of atomic phrase pairs licensed by sentence
pair i, and Pr(Apatom) and Pr(B
p
atom) are priors over these sets for each annotator. A conse-
quence of dropping the independence assumptions is that calculating ?0 is considerably
more difficult.
While it may be possible to calculate ?0 analytically, this gets increasingly compli-
cated for larger phrase pairs or with an expressive prior. For the sake of flexibility we
estimate ?0 using Monte Carlo sampling. Specifically, we approximate the full sum by
drawing samples from a prior distribution over sets of phrase pairs for each of our
annotators (Pr(Apatom) and Pr(B
p
atom) in Equation (6)). These samples are then compared
using the intersection metric. This is repeated many times and the results are then
averaged. More formally:
??0 =
1
I
I
?
i=1
1
J
J
?
j=1
|Apatom
( j)
? Bpatom
( j)
|
min(|Apatom
( j)
|, |Bpatom
( j)
|)
(7)
where for each sentence pair, i, we draw J samples of pairs of sets of phrase pairs,
(Apatom,B
p
atom). We use J = 1, 000, which is ample to give reliable estimates. So far we have
607
Computational Linguistics Volume 34, Number 4
not defined how we sample valid sets of phrase pairs. This is done via the word align-
ments. Recall that the annotators start out with alignments from an automatic word-
aligner. Firstly, we develop a distribution to predict how often an annotator changes a
cell from the initial alignment matrix. We model the number of changes made with a
binomial distribution, that is, each local change is assumed independent and has a fixed
probability, Pr(edit|r,Ni,Mi) where r is the coder andNi andMi are the sentence lengths.
This distribution is fit to each annotator?s predictions using a linear function over the
combined length of two sentences. Next we sample word alignments. Each sample
starts with the automatic alignment, and each cell is changed with probability Pr(edit).
These changes are binary, swapping alignments for non-alignments and vice versa.
Finally, the phrase-extraction heuristic is run over the alignment matrix to produce a
set of phrase pairs. This is done for each annotator, A and B, after which we have a
sample, (Apatom,B
p
atom). Each sample is then fed into Equation (7). Admittedly, this is not
themost accurate prior, as annotators are not just randomly changing the alignment, but
instead are influenced by the content expressed by the sentence pair and other factors
such as syntactic complexity. However, this prior produces estimates for ??0 which are
several orders of magnitude larger than those using Kupper and Hafner?s model of ?0
in Equation (5).
We now illustrate the process of measuring chance-corrected agreement, C?, with
respect to the example in Figure 2. Here, |Apatom| = 7, |B
p
atom| = 8, |A
p
atom ? B
p
atom| = 4, and
therefore ?? = 47 = 0.571. For this sentence our annotators edited eight and nine align-
ment cells, respectively, of the initial alignment matrix. This translates into Pr(edit|r =
A) = 812?13 = 5.13% and Pr(edit|r = B) = 5.77%. Given these priors, we run the Monte
Carlo sampling process from Equation (7), which results in ??0 = 0.147. Combining the
agreement estimate, ??, and chance correction estimate, ??0, using Equation (6) results in
C? = 0.571?0.1471?0.147 = 0.497.
Now, imagine a hypothetical case where ?? = 47 = 0.571 (i.e., the agreement is the
same as before), annotator B edits nine alignment cells, but annotator A chooses not
to make any edits. This leads to an increased estimate of ??0 = 0.259 and a decreased
C? = 0.442. If both annotators were not to make any edits, ??0 = 1 and C? = ??. Interest-
ingly, at the other extreme when Pr(edit|r = A) = Pr(edit|r = B) = 1, agreement is also
perfect, ??0 = 1 and C? = ??. This is because only one phrase pair can be extracted
which consists of the two full sentences.
Results. Tables 3 and 4 display agreement statistics on our three corpora using precision,
recall, F1, and C?. Specifically, we estimate C? by aggregating ?? and ??0 into corpus-
level estimates. Table 3 shows agreement scores for individual words, whereas Table 4
shows agreement for phrase pairs. In both cases the agreement is computed over non-
identical word and phrase pairs which are more likely to correspond to paraphrases.
The agreement figures are broken down into possible (Poss) and sure alignments (Sure)
for precision and recall.
When agreement is measured over words, our annotators obtain high F1 on all
three corpora (MTC, Leagues, and News). Recall on Possibles seems worse on the
News corpus when compared to MTC or Leagues. This is to be expected because this
corpus was automatically harvested from the Web, and some of its instances may not
be representative examples of paraphrases. For example, it is common for one sentence
to provide considerably more details than the other, despite the fact that both describe
the same event. The annotators in turn have difficulty deciding whether such instances
are valid paraphrases. The C? scores for the three corpora are in the same ballpark.
608
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Table 3
Inter-annotator agreement using precision, recall, F1, and C?; the agreement is measured over
words.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.79 0.59 Prec 0.85 0.73 Prec 0.78 0.55
Rec 0.77 0.73 Rec 0.74 0.75 Rec 0.57 0.70
F1 0.76 F1 0.79 F1 0.74
C? 0.85 C? 0.87 C? 0.89
Table 4
Inter-annotator agreement using precision, recall, F1, and C?; the agreement is measured over
atomic phrase pairs.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.77 0.67 Prec 0.74 0.72 Prec 0.72 0.68
Rec 0.77 0.66 Rec 0.77 0.73 Rec 0.69 0.81
F1 0.71 F1 0.74 F1 0.76
C? 0.63 C? 0.62 C? 0.53
Interestingly, C? is highest on the News corpus, whereas F1 is lowest. Whereas precision
and recall are normalized by the number of predictions from annotators A and B,
respectively, C? is normalized by the minimum number of predictions between the two.
Therefore, when the predictions are highly divergent, C? will paint a rosier picture than
F1 (which is the combination of precision and recall). This indeed seems to be the case
for the News corpus, where precision and recall have a higher spread in comparison to
the other two corpora (see the Poss column in Table 3).
Agreement scores tend to be lower when taking phrases into account (see Table 4).
This is expected because annotators are faced with a more complex task; they must
generally make more decisions: for example, determining the phrase boundaries and
how to align their constituent words. An exception to this trend is the News corpus
where the F1 is higher for phrase pairs than for individual word pairs. This is due to the
fact that there are many similar sentence pairs in this data. These have many identical
words and a few different words. The differences are often in a clump (e.g., person
names, verb phrases), rather than distributed throughout the sentence. The annotators
tend to block align these and there is a large scope for disagreement.Whereas estimating
agreement over words heavily penalizes block differences, when phrases are taken
into account in the F1 measure, these are treated more leniently. Note that C? is not
so lenient, as it measures agreement over the sets of atomic phrase pairs rather than
between atomic and composite phrase pairs in the F1 measure. This means that under
C?, choosing different granularities of phrases will be penalized, but would not have
been under the F1 measure.
In Figure 4we show how C? varies with sentence length for our three corpora. Specif-
ically, we plot observed agreement ??, chance agreement ?0, and C? against sentence pairs
609
Computational Linguistics Volume 34, Number 4
Figure 4
Agreement statistics plotted against sentence length for the three sub-corpora. Each group of
three columns correspond to ??, ??0, and C?, respectively. The statistics were measured over
non-identical phrase pairs using all phrase pairs, atomic and composite.
Table 5
Agreement between automatic Giza++ predicted word alignments and our manually corrected
alignments, measured over atomic phrase pairs.
MTC Leagues News
Measure Poss Sure Measure Poss Sure Measure Poss Sure
Prec 0.58 0.55 Prec 0.63 0.60 Prec 0.63 0.65
Rec 0.42 0.49 Rec 0.39 0.47 Rec 0.50 0.64
F1 0.53 F1 0.54 F1 0.63
binned by (the shorter) sentence length. In all cases we observe that chance agreement
is substantially lower than observed agreement for all sentence lengths. We also see that
C? tends to be higher for shorter sentences. Differences in C? across sentence lengths are
mostly of small magnitude across all three corpora. This indicates that disagreements
may be due to other factors, besides sentence length.
Unfortunately, there are no comparable annotation studies that would allow us
to gauge the quality of the obtained agreements. The use of precision, recall, and F1
is widespread in SMT, but these measures evaluate automatic alignments against a
gold standard, rather than the agreement between two or more annotators (but see
Melamed [1998] for an exception). Nevertheless, we would expect the humans to agree
more with each other than with Giza++, given that the latter produces many erroneous
word alignments and is not specifically tuned to the paraphrasing task. Table 5 shows
agreement between one annotator and Giza++ for atomic phrase pairs.15 We obtained
similar results for the other annotator and with the word-based measures. As can be
seen, human?Giza++ agreement is much lower than human?human agreement on all
three corpora (compare Tables 5 and 4). Taken together the results in Tables 3?5 show
a substantial level of agreement, thus indicating that our definition of paraphrases via
word alignments can yield reliable annotations. In the following section we discuss how
our corpus can be usefully employed in the study of paraphrasing.
15 Note that we cannot meaningfully measure C? for this data because the Giza++ predictions are already
being used to estimate ?0 in our formulation. Consequently, P(A) = P(B) and C? is zero.
610
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
4. Experiments
Our annotated corpus can be used in a number of ways to help paraphrase research:
for example, to inform the linguistic analysis of paraphrases, as a training set for the
development of discriminative paraphrase systems, and as a test set for the automatic
evaluation of computational models. Here, we briefly demonstrate some of these uses.
Paraphrase Modeling.Much previous research has focused on lexical paraphrases (but see
Lin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions). We argue
that our corpus should support a richer range of structural (syntactic) paraphrases.
To demonstrate this we have extracted paraphrase rules from our annotations using
the grammar induction algorithm from Cohn and Lapata (2007). Briefly, the algorithm
extracts tree pairs from word-aligned text by choosing aligned constituents in a pair of
equivalent sentences. These pairs are then generalized by factoring out aligned subtrees,
thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable
nodes.
We parsed the MTC corpus with Bikel?s (2002) parser and extracted synchronous
rules from the gold-standard alignments. A sample of these rules are shown in Figure 5.
Here we see three lexical paraphrases, followed by five structural paraphrases. In
example 4, also is replaced with moreover and is moved to the start of the sentence from
the pre-verbal position. Examples 5?8 show various reordering operations, where the
boxed numbers indicate correspondences between non-terminals in the two sides of the
rules.
The synchronous rules in Figure 5 provide insight into the process of paraphrasing
at the syntactic level, and also a practical means for developing algorithms for para-
phrase generation?a task which has received little attention to date. For instance, we
could envisage a paraphrase model that transforms parse trees of an input sentence
into parse trees that represent a sentential paraphrase of that sentence. Our corpus can
be used to learn this mapping using discriminative methods (Cowan, Kuc?erova?, and
Collins 2006; Cohn and Lapata 2007).
Evaluation Set. As mentioned in Section 1, it is currently difficult to compare competing
approaches due to the effort involved in eliciting manual judgments of paraphrase
output. Our corpus could fill the role of a gold-standard test set, allowing for automatic
evaluation techniques.
Developing measures for automatic paraphrase evaluation is outside the scope of
this article. Nevertheless, we illustrate how the corpus can be used for this purpose.
For example we could easily measure the precision and recall of an automatic system
Figure 5
Synchronous grammar rules extracted from the MTC corpus.
611
Computational Linguistics Volume 34, Number 4
against our annotations. Computing precision and recall for an individual system is not
perhaps the most meaningful test, considering the large potential for paraphrasing in
a given sentence pair. A better evaluation strategy would include a comparison across
many systems on the same corpus. We could then rank these systems without, however,
paying so much attention to the absolute precision and recall values. We expect these
comparisons to yield relatively low numbers for many reasons. First and foremost the
task is hard, as shown by our inter-annotator agreement figures in Tables 3 and 4.
Secondly, there may be valid paraphrases that the systems identify but are not listed
in our gold standard. Thirdly, systems may have different biases, for example, towards
producing more lexical or syntactic paraphrases, but our comparison would not take
this into account. Despite all these considerations, we believe that comparison against
our corpus would treat these systems on an equal footing against the same materials
while factoring out nonessential degrees of freedom inherent in human elicitation stud-
ies (e.g., attention span, task familiarity, background).
We evaluated the performance of two systems against our corpus. Our first system
is simply Giza++ trained on the 55, 615 sentence pairs described in Section 4. The
second system uses a co-training-based paraphrase extraction algorithm (Barzilay and
McKeown 2001). It was also trained on the MTC part 1 corpus, on the same data set
used for Giza++, with its default parameters. For each system, we filtered the predicted
paraphrases to just those which match part of a sentence pair in the test set. These
paraphrases were then compared to the sure phrase pairs extracted from our manually
aligned corpus. Giza++?s precision is 55% and recall 49% (see Table 5). The co-training
system obtained a precision of 30% and recall of 16%. To confirm the accuracy of
the precision estimate, we performed a human evaluation on a sample of 48 of the
predicted paraphrases which were treated as errors. Of these, 63% were confirmed as
being incorrect and only 20%were acceptable (the remaining were uncertain). The inter-
annotator agreement in Table 4 can be used as an upper bound for precision and recall
(precision for Sure phrase pairs is 67% and recall 66%). These results seem to suggest
that a hypothetical paraphrase extractor based on automatic word alignments would
obtain performance superior to the co-training approach. However, we must bear in
mind that the co-training system is highly parametrized and was not specifically tuned
to our data set.
5. Conclusions
In this article we have presented a human-annotated paraphrase corpus and argued
that it can be usefully employed for the evaluation and modeling of paraphrases. We
have defined paraphrases as word alignments in a corpus containing pairs of equivalent
sentences and shown that these can be reliably identified by annotators. In measur-
ing agreement, we used the standard measures of precision, recall, and F1, but also
proposed a novel formulation of chance-corrected agreement for word (and phrase)
alignments. Beyond alignment, our formulation could be applied to other structured
tasks including parsing and sequence labeling.
The uses of the corpus are many and varied. It can serve as a test set for eval-
uating the precision and recall of paraphrase induction systems trained on parallel
monolingual corpora. The corpus could be further used to develop new evaluation
metrics for paraphrase acquisition or novel paraphrasing models. An exciting avenue
for future research concerns paraphrase prediction, that is, determiningwhen and how to
paraphrase single sentence input. Because our corpus contains paraphrase annotations
at the sentence level, it could provide a natural test-bed for prediction algorithms.
612
Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems
Acknowledgments
The authors acknowledge the support of the
EPSRC (Cohn, grant GR/T04557/01;
Lapata, grant GR/T04540/01), the National
Science Foundation (Callison-Burch, grant
IIS-071344), and the EuroMatrix project
(Callison-Burch) funded by the European
Commission (6th Framework Programme).
We are grateful to our annotators Vasilis
Karaiskos and Tom Segler. Thanks to Regina
Barzilay for providing us the output of her
system on our data and to the anonymous
referees whose feedback helped to
substantially improve the present article.
References
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Compute System
Sciences, 3(1):37?56.
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for Computational
Linguistics. Computational Linguistics.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 9?16,
Sydney.
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 597?604, Ann Arbor, MI.
Bartko, John J. and William T. Carpenter.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307?317.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization:
Paraphrasing and Generation. Ph.D. thesis,
Columbia University, New York, NY.
Barzilay, Regina and Noemie Elhadad.
2003. Sentence alignment for monolingual
comparable corpora. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 25?32,
Sapporo.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: An unsupervised
approach using multiple-sequence
alignment. In Proceedings of the Human
Language Technology Conference and the
Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 16?23, Edmonton.
Barzilay, Regina and Kathy McKeown. 2001.
Extracting paraphrases from a parallel
corpus. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 50?57, Toulouse.
Bikel, Daniel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of the Human
Language Technology Conference,
pages 24?27, San Diego, CA.
Brown, Peter F., Stephen A. Della-Pietra,
Vincent J. Della-Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Callison-Burch, Chris. 2007. Paraphrasing and
Translation. Ph.D. thesis, University of
Edinburgh, Edinburgh, Scotland.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of the Human Language
Technology Conference and Annual Meeting of
the North American Chapter of the Association
for Computational Linguistics, pages 17?24,
New York, NY.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:37?46.
Cohn, Trevor and Mirella Lapata. 2007. Large
margin synchronous generation and its
application to sentence compression. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing and
on Computational Natural Language
Learning, pages 73?82, Prague.
Cowan, Brooke, Ivona Kuc?erova?, and
Michael Collins. 2006. A discriminative
model for tree-to-tree translation. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 232?241, Sydney.
Daume? III, Hal and Daniel Marcu.
2004. A phrase-based HMM approach
to document/abstract alignment.
In Proceedings of the 2004 Conference
on Empirical Methods in Natural
Language Processing, pages 119?126,
Barcelona.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Dolan, William, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction
of large paraphrase corpora: Exploiting
massively parallel news sources. In
Proceedings of the 20th International
Conference on Computational Linguistics,
pages 350?356, Geneva.
613
Computational Linguistics Volume 34, Number 4
Duboue, Pablo and Jennifer Chu-Carroll.
2006. Answering the question you wish
they had asked: The impact of
paraphrasing for question answering.
In Proceedings of the Human Language
Technology Conference of the NAACL,
Companion Volume: Short Papers,
pages 33?36, New York, NY.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Fraser, Alexander and Daniel Marcu. 2007.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293?303.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Human Language Technology
Conference and Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 48?54,
Edmonton.
Kupper, Lawrence L. and Kerry B. Hafner.
1989. On assessing interrater agreement
for multiple attribute responses. Biometrics,
45(3):957?967.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):342?360.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 67?74,
Ann Arbor, MI.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The Blinker
project. IRCS Technical Report #98-07,
University of Pennsylvania,
Philadelphia, PA.
Mihalcea, Rada and Ted Pedersen. 2003. An
evaluation exercise for word alignment. In
Proceedings of the HLT-NAACL Workshop on
Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,
pages 1?6, Edmonton.
Och, Franz Josef and Hermann Ney. 2000a. A
comparison of alignment models for
statistical machine translation. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 1086?1090, Saarbru?cken.
Och, Franz Josef and Hermann Ney.
2000b. Improved statistical alignment
models. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 440?447,
Hong Kong.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?52.
Och, Franz Josef, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20?28, College Park, MD.
Pang, Bo, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple
translations: Extracting paraphrases and
generating new sentences. In Proceedings of
the Human Language Technology Conference
and the Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 181?188,
Edmonton.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 142?149, Barcelona.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale. Public
Opinion Quarterly, 19:127?141.
Siegel, Sidney and N. John Castellan. 1988.
Non Parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New York.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Munteanu, and Eduard Hovy. 2006.
Paraeval: Using paraphrases to
evaluate summaries automatically. In
Proceedings of the Human Language
Technology Conference, pages 447?454,
New York, NY.
614
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 548?556,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Inducing Compact but Accurate Tree-Substitution Grammars
Trevor Cohn and Sharon Goldwater and Phil Blunsom
School of Informatics
University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
{tcohn,sgwater,pblunsom}@inf.ed.ac.uk
Abstract
Tree substitution grammars (TSGs) are a com-
pelling alternative to context-free grammars
for modelling syntax. However, many popu-
lar techniques for estimating weighted TSGs
(under the moniker of Data Oriented Parsing)
suffer from the problems of inconsistency and
over-fitting. We present a theoretically princi-
pled model which solves these problems us-
ing a Bayesian non-parametric formulation.
Our model learns compact and simple gram-
mars, uncovering latent linguistic structures
(e.g., verb subcategorisation), and in doing so
far out-performs a standard PCFG.
1 Introduction
Many successful models of syntax are based on
Probabilistic Context Free Grammars (PCFGs)
(e.g., Collins (1999)). However, directly learning a
PCFG from a treebank results in poor parsing perfor-
mance, due largely to the unrealistic independence
assumptions imposed by the context-free assump-
tion. Considerable effort is required to coax good
results from a PCFG, in the form of grammar en-
gineering, feature selection and clever smoothing
(Collins, 1999; Charniak, 2000; Charniak and John-
son, 2005; Johnson, 1998). This effort must be re-
peated when moving to different languages, gram-
mar formalisms or treebanks. We propose that much
of this hand-coded knowledge can be obtained auto-
matically as an emergent property of the treebanked
data, thereby reducing the need for human input in
crafting the grammar.
We present a model for automatically learning a
Probabilistic Tree Substitution Grammar (PTSG),
an extension to the PCFG in which non-terminals
can rewrite as entire tree fragments (elementary
trees), not just immediate children. These large frag-
ments can be used to encode non-local context, such
as head-lexicalisation and verb sub-categorisation.
Since no annotated data is available providing TSG
derivations we must induce the PTSG productions
and their probabilities in an unsupervised way from
an ordinary treebank. This is the same problem ad-
dressed by Data Oriented Parsing (DOP, Bod et al
(2003)), a method which uses as productions all sub-
trees of the training corpus. However, many of the
DOP estimation methods have serious shortcomings
(Johnson, 2002), namely inconsistency for DOP1
(Bod, 2003) and overfitting of the maximum like-
lihood estimate (Prescher et al, 2004).
In this paper we develop an alternative means of
learning a PTSG from a treebanked corpus, with the
twin objectives of a) finding a grammar which ac-
curately models the data and b) keeping the gram-
mar as simple as possible, with few, compact, ele-
mentary trees. This is achieved using a prior to en-
courage sparsity and simplicity in a Bayesian non-
parametric formulation. The framework allows us to
perform inference over an infinite space of gram-
mar productions in an elegant and efficient manner.
The net result is a grammar which only uses the in-
creased context afforded by the TSG when necessary
to model the data, and otherwise uses context-free
rules.1 That is, our model learns to use larger rules
when the CFG?s independence assumptions do not
hold. This contrasts with DOP, which seeks to use
all elementary trees from the training set. While our
model is able, in theory, to use all such trees, in prac-
tice the data does not justify such a large grammar.
Grammars that are only about twice the size of a
1While TSGs and CFGs describe the same string lan-
guages, TSGs can describe context-sensitive tree-languages,
which CFGs cannot.
548
treebank PCFG provide large gains in accuracy. We
obtain additional improvements with grammars that
are somewhat larger, but still much smaller than the
DOP all-subtrees grammar. The rules in these gram-
mars are intuitive, potentially offering insights into
grammatical structure which could be used in, e.g.,
the development of syntactic ontologies and guide-
lines for future treebanking projects.
2 Background and related work
A Tree Substitution Grammar2 (TSG) is a 4-tuple,
G = (T,N, S,R), where T is a set of terminal sym-
bols, N is a set of non-terminal symbols, S ? N is
the distinguished root non-terminal and R is a set
of productions (a.k.a. rules). The productions take
the form of elementary trees ? tree fragments of
depth ? 2, where each internal node is labelled with
a non-terminal and each leaf is labelled with either a
terminal or a non-terminal. Non-terminal leaves are
called frontier non-terminals and form the substitu-
tion sites in the generative process of creating trees
with the grammar.
A derivation creates a tree by starting with the
root symbol and rewriting (substituting) it with an
elementary tree, then continuing to rewrite frontier
non-terminals with elementary trees until there are
no remaining frontier non-terminals. Unlike Con-
text Free Grammars (CFGs) a syntax tree may not
uniquely specify the derivation, as illustrated in Fig-
ure 1 which shows two derivations using different
elementary trees to produce the same tree.
A Probabilistic Tree Substitution Grammar
(PTSG), like a PCFG, assigns a probability to each
rule in the grammar. The probability of a derivation
is the product of the probabilities of its component
rules, and the probability of a tree is the sum of the
probabilities of its derivations.
As we mentioned in the introduction, work within
the DOP framework seeks to induce PTSGs from
treebanks by using all possible subtrees as rules, and
one of a variety of methods for estimating rule prob-
abilities.3 Our aim of inducing compact grammars
contrasts with that of DOP; moreover, we develop a
probabilistic estimator which avoids the shortcom-
ings of DOP1 and the maximum likelihood esti-
2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003))
without the adjunction operator.
3TAG induction (Chiang and Bikel, 2002; Xia, 2002) also
tackles a similar learning problem.
mate (Bod, 2000; Bod, 2003; Johnson, 2002). Re-
cent work on DOP estimation also seeks to address
these problems, drawing from estimation theory to
solve the consistency problem (Prescher et al, 2004;
Zollmann and Sima?an, 2005), or incorporating a
grammar brevity term into the learning objective
(Zuidema, 2007). Our work differs from these pre-
vious approaches in that we explicitly model a prior
over grammars within a Bayesian framework.4
Models of grammar refinement (Petrov et al,
2006; Liang et al, 2007; Finkel et al, 2007) also
aim to automatically learn latent structure underly-
ing treebanked data. These models allow each non-
terminal to be split into a number of subcategories.
Theoretically the grammar space of our model is a
sub-space of theirs (projecting the TSG?s elementary
trees into CFG rules). However, the number of non-
terminals required to recreate our TSG grammars
in a PCFG would be exorbitant. Consequently, our
model should be better able to learn specific lexical
patterns, such as full noun-phrases and verbs with
their sub-categorisation frames, while theirs are bet-
ter suited to learning subcategories with larger mem-
bership, such as the terminals for days of the week
and noun-adjective agreement. The approaches are
orthogonal, and we expect that combining a category
refinement model with our TSG model would pro-
vide better performance than either approach alone.
Our model is similar to the Adaptor Grammar
model of Johnson et al (2007b), which is also
a kind of Bayesian nonparametric tree-substitution
grammar. However, Adaptor Grammars require that
each sub-tree expands completely, with only termi-
nal symbols as leaves, while our own model permits
non-terminal frontier nodes. In addition, they disal-
low recursive containment of adapted non-terminals;
we impose no such constraint.
3 Model
Recall the nature of our task: we are given a corpus
of parse trees t and wish to infer a tree-substitution
grammar G that we can use to parse new data.
Rather than inferring a grammar directly, we go
through an intermediate step of inferring a distri-
bution over the derivations used to produce t, i.e.,
4A similar Bayesian model of TSG induction has been de-
veloped independently to this work (O?Donnell et al, 2009b;
O?Donnell et al, 2009a).
549
(a)
S
NP
NP
George
VP
V
hates
NP
NP
broccoli
(b)
S
NP
George
VP
V
V
hates
NP
broccoli
S? NP (VP (V hates) NP)
NP? George
NP? broccoli
S? (NP George) (VP V (NP broccoli))
V? hates
Figure 1: Example derivations for the same tree,
where arrows indicate substitution sites. The ele-
mentary trees used in (a) and (b) are shown below
as grammar productions in bracketed tree notation.
a distribution over sequences of elementary trees e
that compose to form t. We will then essentially read
the grammar off the elementary trees, as described
in Section 5. Our problem therefore becomes one of
identifying the posterior distribution of e given t,
which we can do using Bayes? Rule:
P (e|t) ? P (t|e)P (e) (1)
Since the sequence of elementary trees can be split
into derivations, each of which completely specifies
a tree, P (t|e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the work
in our model is done by the prior distribution over
elementary trees. Note that this is analogous to the
Bayesian model of word segmentation presented by
Goldwater et al (2006); indeed, the problem of in-
ferring e from t can be viewed as a segmentation
problem, where each full tree must be segmented
into one or more elementary trees. As in Goldwater
et al (2006), we wish to favour solutions employing
a relatively small number of elementary units (here,
elementary trees). This can be done using a Dirichlet
process (DP) prior. Specifically, we define the distri-
bution of elementary tree e with root non-terminal
symbol c as
Gc|?c, P0 ? DP(?c, P0(?|c))
e|c ? Gc
whereP0(?|c) (the base distribution) is a distribution
over the infinite space of trees rooted with c, and ?c
(the concentration parameter) controls the model?s
tendency towards either reusing elementary trees or
creating novel ones as each training instance is en-
countered (and consequently, the tendency to infer
larger or smaller sets of elementary trees from the
observed data). We discuss the base distribution in
more detail below.
Rather than representing the distribution Gc ex-
plicitly, we integrate over all possible values of Gc.
The resulting distribution over ei, conditioned on
e<i = e1 . . . ei?1 and the root category c is:
p(ei|e<i, c, ?c, P0) = n
<i
ei,c + ?cP0(ei|c)
n<i?,c + ?c (2)
where n<iei,c is the number number of times ei has
been used to rewrite c in e<i, and n<i?,c =
?
e n<ie,c isthe total count of rewriting c.
As with other DP models, ours can be viewed as a
cache model, where ei can be generated in one of
two ways: by drawing from the base distribution,
where the probability of any particular tree is pro-
portional to ?cP0(ei|c), or by drawing from a cache
of previous expansions of c, where the probability of
any particular expansion is proportional to the num-
ber of times that expansion has been used before.
This view makes it clear that the model embodies
a ?rich-get-richer? dynamic in which a few expan-
sions will occur with high probability, but many will
occur only once or twice, as is typical of natural lan-
guage. Our model is similar in this way to the Adap-
tor Grammar model of Johnson et al (2007a).
We still need to define P0, the base distribution
over tree fragments. We use two such distributions.
The first, PM0 generates each elementary tree by
a series of random decisions: whether to expand a
non-terminal, how many children to produce and
their identities. The probability of expanding a non-
terminal node labelled c is parameterised via a bino-
mial distribution, Bin(?c), while all other decisions
are chosen uniformly at random. The second base
distribution, PC0 , has a similar generative process
but draws non-terminal expansions from a treebank-
trained PCFG instead of a uniform distribution.
Both choices of P0 have the effect of biasing the
model towards simple rules with a small number of
internal nodes. The geometric increase in cost dis-
courages the model from using larger rules; for this
to occur these rules must yield a large increase in the
data likelihood. As PC0 incorporates PCFG probabil-
550
SNP,1
George
VP,0
V,0
hates
NP,1
broccoli
Figure 2: Gibbs state e specifying the derivation in
Figure 1a. Each node is labelled with its substitution
indicator variable.
ities, it assigns higher relative probability to larger
rules, compared to the more draconian PM0 .
4 Training
To train our model we use Gibbs sampling (Geman
and Geman, 1984), a Markov chain Monte Carlo
method in which variables are repeatedly sampled
conditioned on the values of all other variables in
the model. After a period of burn-in, each sam-
pler state (set of variable assignments) is a sample
from the posterior distribution of the model. In our
case, we wish to sample from P (e|t, ?, ?), where
(?, ?) = {?c, ?c} for all categories c. To do so,
we associate a binary variable with each non-root
internal node of each tree in the training set, indi-
cating whether that node is a substitution point or
not. Each substitution point forms the root of some
elementary tree, as well as a frontier non-terminal
of an ancestor node?s elementary tree. Collectively,
the training trees and substitution variables specify
the sequence of elementary trees e that is the current
state of the sampler. Figure 2 shows an example tree
with its substitution variables, corresponding to the
TSG derivation in Figure 1a.
Our Gibbs sampler works by sampling the value
of each substitution variable, one at a time, in ran-
dom order. If d is the node associated with the sub-
stitution variable s under consideration, then the two
possible values of s define two options for e: one
in which d is internal to some elementary tree eM ,
and one in which d is the substitution site con-
necting two smaller trees, eA and eB . In the ex-
ample in Figure 2, when sampling the VP node,
eM = (S NP (VP (V hates) NP)), eA = (S NP VP),
and eB = (VP (V hates) NP). To sample a value for
s, we compute the probabilities of eM and (eA, eB),
conditioned on e?: all other elementary trees in the
training set that share at most a root or frontier non-
terminal with eM , eA, or eB . This is easy to do
because the DP is exchangeable, meaning that the
probability of a set of outcomes does not depend on
their ordering. Therefore, we can treat the elemen-
tary trees under consideration as the last ones to be
sampled, and apply Equation 2, giving us
P (eM |cM )=n
?
eM ,cM + ?cMP0(eM |cM )
n??,cM + ?cM
(3)
P (eA, eB|cA)=n
?
eA,cA + ?cAP0(eA|cA)
n??,cA + ?cA
(4)
?
n?eB ,cB + ?(eA, eB) + ?cBP0(eB|cB)
n??,cB + ?(cA, cB) + ?cB
where cx is the root label of ex, x ? {A,B,M},
the counts n? are with respect to e?, and ?(?, ?) is
the Kronecker delta function, which returns 1 when
its arguments are identical and 0 otherwise. We have
omitted e?, t, ? and ? from the conditioning con-
text. The ? terms in the second factor of (4) account
the changes to n? that would occur after observing
eA, which forms part of the conditioning context for
eB . If the trees eA and eB are identical, then the
count n?eB ,cB would increase by one, and if the treesshare the same root non-terminal, then n??,cB wouldincrease by one.
In the previous discussion, we have assumed
that the model hyperparameters, (?, ?), are known.
However, selecting their values by hand is extremely
difficult and fitting their values on heldout data is of-
ten very time consuming. For this reason we treat
the hyper-parameters as variables in our model and
infer their values during training. We choose vague
priors for each hyper-parameter, encoding our lack
of information about their values. We treat the con-
centration parameters, ?, as being generated by a
vague gamma prior, ?c ? Gamma(0.001, 1000).
We sample a new value ??c using a log-normal dis-
tribution with mean ?c and variance 0.3, which is
then accepted into the distribution p(?c|e, t, ??, ?)
using the Metropolis-Hastings algorithm. We use a
Beta prior for the binomial specification parameters,
?c ? Beta(1, 1). As the Beta distribution is conju-
gate to the binomial, we can directly resample the
? parameters from the posterior, p(?c|e, t, ?, ??).
Both the concentration and substitution parameters
are resampled after every full Gibbs sampling itera-
tion over the training trees.
551
5 Parsing
We now turn to the problem of using the model
to parse novel sentences. This requires finding the
maximiser of
p(t|w, t) =
?
p(t|w, e, ?, ?) p(e, ?, ?|t) de d? d?
(5)
wherew is the sequence of words being parsed and t
the resulting tree, t are the training trees and e their
segmentation into elementary trees.
Unfortunately solving for the maximising parse
tree in (5) is intractable. However, it can approxi-
mated using Monte Carlo techniques. Given a sam-
ple of (e, ?, ?)5 we can reason over the space of
possible trees using a Metropolis-Hastings sampler
(Johnson et al, 2007a) coupled with a Monte Carlo
integral (Bod, 2003). The first step is to sample from
the posterior over derivations, p(d|w, e, ?, ?). This
is achieved by drawing samples from an approxima-
tion grammar, p?(d|w), which are then accepted to
the true distribution using the Metropolis-Hastings
algorithm. The second step records for each sampled
derivation the CFG tree. The counts of trees consti-
tute an approximation to p(t|w, e, ?, ?), from which
we can recover the maximum probability tree.
A natural proposal distribution, p?(d|w), is the
maximum a posterior (MAP) grammar given the el-
ementary tree analysis of our training set (analogous
to the PCFG approximation used in Johnson et al
(2007a)). This is not practical because the approx-
imation grammar is infinite: elementary trees with
zero count in e still have some residual probabil-
ity under P0. In the absence of a better alternative,
we discard (most of) the zero-count rules from MAP
grammar. This results in a tractable grammar repre-
senting the majority of the probability mass, from
which we can sample derivations. We specifically
retain all zero-count PCFG productions observed in
the training set in order to provide greater robustness
on unseen data.
In addition to finding the maximum probability
parse (MPP), we also report results using the maxi-
mum probability derivation (MPD). While this could
be calculated in the manner as described above, we
5Using many samples of (e, ?, ?) in a Monte Carlo inte-
gral is a straight-forward extension to our parsing algorithm. We
did not observe a significant improvement in parsing accuracy
when using a multiple samples compared to a single sample,
and therefore just present results for a single sample.
S ? A | B
A? A A | B B | (A a) (A a) | (B a) (B a)
B ? A A | B B | (A b) (A b) | (B b) (B b)
Figure 3: TSG used to generate synthetic data. All
production probabilities are uniform.
found that using the CYK algorithm (Cocke, 1969)
to find the Viterbi derivation for p? yielded consis-
tently better results. This algorithm maximises an
approximated model, as opposed to approximately
optimising the true model. We also present results
using the tree with the maximum expected count of
CFG rules (MER). This uses counts of the CFG rules
applied at each span (compiled from the derivation
samples) followed by a maximisation step to find the
best tree. This is similar to the MAX-RULE-SUM
algorithm of Petrov and Klein (2007) and maximum
expected recall parsing (Goodman, 2003).
6 Experiments
Synthetic data Before applying the model to
natural language, we first create a synthetic problem
to confirm that the model is capable of recovering
a known tree-substitution grammar. We created 50
random trees from the TSG shown in Figure 3. This
produces binary trees with A and B internal nodes
and ?a? and ?b? as terminals, such that the termi-
nals correspond to their grand-parent non-terminal
(A and a or B and b). These trees cannot be mod-
elled accurately with a CFG because expanding A
and B nodes into terminal strings requires knowing
their parent?s non-terminal.
We train the model for 100 iterations of Gibbs
sampling using annealing to speed convergence.
Annealing amounts to smoothing the distributions
in (3) and (4) by raising them to the power of 1T .Our annealing schedule begins at T = 3 and lin-
early decreases to reach T = 1 in the final iteration.
The sampler converges to the correct grammar, with
the 10 rules from Figure 3.
Penn-treebank parsing We ran our natural lan-
guage experiments on the Penn treebank, using the
standard data splits (sections 2?21 for training, 22
for development and 23 for testing). As our model is
parameter free (the ? and ? parameters are learnt in
training), we do not use the development set for pa-
552
rameter tuning. We expect that fitting these param-
eters to maximise performance on the development
set would lead to a small increase in generalisation
performance, but at a significant cost in runtime. We
replace tokens with count? 1 in the training sample
with one of roughly 50 generic unknown word mark-
ers which convey the token?s lexical features and po-
sition in the sentence, following Petrov et al (2006).
We also right-binarise the trees to reduce the branch-
ing factor in the same manner as Petrov et al (2006).
The predicted trees are evaluated using EVALB6 and
we report the F1 score over labelled constituents and
exact match accuracy over all sentences in the test-
ing sets.
In our experiments, we initialised the sampler by
setting all substitution variables to 0, thus treating
every full tree in the training set as an elementary
tree. Starting with all the variables set to 1 (corre-
sponding to CFG expansions) or a random mix of
0s and 1s considerably increases time until conver-
gence. We hypothesise that this is due to the sampler
getting stuck in modes, from which a series of lo-
cally bad decisions are required to escape. The CFG
solution seems to be a mode and therefore starting
the sampler with maximal trees helps the model to
avoid this mode.
Small data sample For our first treebank exper-
iments, we train on a small data sample by using
only section 2 of the treebank. Bayesian methods
tend to do well with small data samples, while for
larger samples the benefits diminish relative to point
estimates. The models were trained using Gibbs
sampling for 4000 iterations with annealing linearly
decreasing from T = 5 to T = 1, after which
the model performed another 1000 iterations with
T = 1. The final training sample was used in the
parsing algorithm, which used 1000 derivation sam-
ples for each test sentence. All results are the aver-
age of five independent runs.
Table 1 presents the prediction results on the de-
velopment set. The baseline is a maximum likeli-
hood PCFG. The TSG model significantly outper-
forms the baseline with either base distribution PM0
or PC0 . This confirms our hypothesis that CFGs are
not sufficiently powerful to model syntax, but that
the increased context afforded to the TSG can make
a large difference. This result is even more impres-
sive when considering the difference in the sizes of
6See http://nlp.cs.nyu.edu/evalb/.
F1 EX # rules
PCFG 60.20 4.29 3500
TSG PM0 : MPD 72.17 11.92 6609MPP 71.27 12.33 6609
MER 74.25 12.30 6609
TSG PC0 : MPD 75.24 15.18 14923MPP 75.30 15.74 14923
MER 76.89 15.76 14923
SM?=2: MPD 71.93 11.30 16168
MER 74.32 11.77 16168
SM?=5: MPD 75.33 15.64 39758
MER 77.93 16.94 39758
Table 1: Development results for models trained on
section 2 of the Penn tree-bank, showing labelled
constituent F1 and exact match accuracy. Grammar
sizes are the number of rules with count ? 1.
grammar in the PCFG versus TSG models. The TSG
using PM0 achieves its improvements with only dou-
ble as many rules, as a consequence of the prior
which encourages sparse solutions. The TSG results
with the CFG base distribution, PC0 , are more ac-
curate but with larger grammars.7 This base distri-
bution assigns proportionally higher probability to
larger rules than PM0 , and consequently the model
uses these additional rules in a larger grammar.
Surprisingly, the MPP technique is not systemati-
cally better than the MPD approach, with mixed re-
sults under the F1 metric. We conjecture that this is
due to sampling variance for long sentences, where
repeated samples of the same tree are exceedingly
rare. The MER technique results in considerably
better F1 scores than either MPD or MPP, with a
margin of 1.5 to 3 points. This method is less af-
fected by sampling variance due to its use of smaller
tree fragments (PCFG productions at each span).
For comparison, we trained the Berkeley split-
merge (SM) parser (Petrov et al, 2006) on the same
data and decoded using the Viterbi algorithm (MPD)
and expected rule count (MER a.k.a. MAX-RULE-
SUM). We ran two iterations of split-merge training,
after which the development F1 dropped substan-
tially (in contrast, our model is not fit to the devel-
opment data). The result is an accuracy slightly be-
low that of our model (SM?=2). To be fairer to their
model, we adjusted the unknown word threshold to
their default setting, i.e., to apply to word types oc-
7The grammar is nevertheless far smaller than the full DOP
grammar on this data set, which has 700K rules.
553
0 1 2 3 4 5 6 7 8
count
cou
nt of
 cou
nts
0
100
200
300
400
500 depthnodeslexemesvars
Figure 4: Grammar statistics for a TSG PM0 model
trained on section 2 of the Penn treebank, show-
ing a histogram over elementary tree depth, num-
ber of nodes, terminals (lexemes) and frontier non-
terminals (vars).
curring fewer than five times (SM?=5). We expect
that tuning the treatment of unknown words in our
model would also yield further gains. The grammar
sizes are not strictly comparable, as the Berkeley bi-
narised grammars prohibit non-binary rules, and are
therefore forced to decompose each of these rules
into many child rules. But the trend is clear ? our
model produces similar results to a state-of-the-art
parser, and can do so using a small grammar. With
additional rounds of split-merge training, the Berke-
ley grammar grows exponentially larger (200K rules
after six iterations).
Full treebank We now train the model using
PM0 on the full training partition of the Penn tree-
bank, using sections 2?21. We run the Gibbs sampler
for 15,000 iterations while annealing from T = 5 to
T = 1, after which we finish with 5,000 iterations
at T = 1. We repeat this three times, giving an av-
erage F1 of 84.0% on the testing partition using the
maximum expected rule algorithm and 83.0% using
the Viterbi algorithm. This far surpasses the ML-
PCFG (F1 of 70.7%), and is similar to Zuidema?s
(2007) DOP result of 83.8%. However, it still well
below state-of-the art parsers (e.g., the Berkeley
parser trained using the same data representation
scores 87.7%). But we must bear in mind that these
parsers have benefited from years of tuning to the
Penn-treebank, where our model is much simpler
and is largely untuned. We anticipate that careful
data preparation and model tuning could greatly im-
prove our model?s performance.
NP?
(NNP Mr.) NNP
CD (NN %)
(NP CD (NN %)) (PP (IN of) NP)
(NP ($ $) CD) (NP (DT a) (NN share))
(NP (DT the) (N?P (NN company) POS)) N?P
(NP QP (NN %)) (PP (IN of) NP)
(NP CD (NNS cents)) (NP (DT a) (NN share))
(NP (NNP Mr.) (N?P NNP (POS ?s))) NN
QP (NN %)
(NP (NN president)) (PP (IN of) NP)
(NP (NNP Mr.) (N?P NNP (POS ?s))) N?P
NNP (N?P NNP (NNP Corp.))
NNP (N?P NNP (NNP Inc.))
(NP (NN chairman)) (PP (IN of) NP)
VP?
(VBD said) (SBAR (S (NP (PRP it)) VP))
(VBD said) (SBAR (S NP VP))
(VBD rose) (V?P (NP CD (NN %)) V?P)
(VBP want) S
(VBD said) (SBAR (S (NP (PRP he)) VP))
(VBZ plans) S
(VBD said) (SBAR S)
(VBZ says) (SBAR (S NP VP))
(VBP think) (SBAR S)
(VBD agreed) (S (VP (TO to) (VP VB V?P)))
(VBZ includes) NP
(VBZ says) (SBAR (S (NP (PRP he)) VP))
(VBZ wants) S
(VBD closed) (V?P (PP (IN at) NP) (V?P , ADVP))
Table 3: Most frequent lexicalised expansions for
noun and verb phrases, excluding auxiliary verbs.
7 Discussion
So what kinds of non-CFG rules is the model learn-
ing? Figure 4 shows the grammar statistics for a
TSG model trained on the small data sample. This
model has 5611 CFG rules and 1008 TSG rules.
The TSG rules vary in depth from two to nine levels
with the majority between two and four. Most rules
combine a small degree of lexicalisation and a vari-
able or two. This confirms that the model is learn-
ing local structures to encode, e.g., multi-word units,
subcategorisation frames and lexical agreement. The
few very large rules specify full parses for sentences
which were repeated in the training corpus. These
complete trees are also evident in the long tail of
node counts (up to 27; not shown in the figure) and
counts for highly lexicalised rules (up to 8).
To get a better feel for the types of rules being
learnt, it is instructive to examine the rules in the re-
554
NP? PP? ADJP?
DT N?P IN NP JJ
NNS (IN in) NP RB JJ
DT NN (TO to) NP JJ ( ?ADJP CC JJ)
(DT the) N?P TO NP JJ PP
JJ NNS (IN with) NP (RB very) JJ
NP (PP (IN of) NP) (IN of) NP RB ?ADJP
NP PP (IN by) NP (RBR more) JJ
NP (N?P (CC and) NP) (IN at) NP JJ ?ADJP
JJ N?P IN (NP (DT the) N?P) ADJP ( ?ADJP CC ADJP)
NN NNS (IN on) NP RB VBN
(DT the) NNS (IN from) NP RB ( ?ADJP JJ PP)
DT (N?P JJ NN) IN (S (VP VBG NP)) JJ (PP (TO to) NP)
NN IN (NP NP PP) ADJP (PP (IN than) NP)
JJ NN (IN into) NP (RB too) JJ
(NP DT NN) (PP (IN of) NP) (IN for) NP (RB much) JJR
Table 2: Top fifteen expansions sorted by frequency (most frequent at top), taken from the final sample of a
model trained on the full Penn treebank. Non-terminals shown with an over-bar denote a binarised sub span
of the given phrase type.
sultant grammar. Table 2 shows the top fifteen rules
for three phrasal categories for the model trained on
the full Penn treebank. We can see that many of these
rules are larger than CFG rules, showing that the
CFG rules alone are inadequate to model the tree-
bank. Two of the NP rules encode the prevalence
of preposition phrases headed by ?of? within a noun
phrase, as opposed to other prepositions. Also note-
worthy is the lexicalisation of the determiner, which
can affect the type of NP expansion. For instance,
the indefinite article is more likely to have an ad-
jectival modifier, while the definite article appears
more frequently unmodified. Highly specific tokens
are also incorporated into lexicalised rules.
Many of the verb phrase expansions have been
lexicalised, encoding the verb?s subcategorisation,
as shown in Table 3. Notice that each verb here ac-
cepts only one or a small set of argument frames,
indicating that by lexicalising the verb in the VP ex-
pansion the model can find a less ambiguous and
more parsimonious grammar.
The model also learns to use large rules to de-
scribe the majority of root node expansions (we add
a distinguished TOP node to all trees). These rules
mostly describe cases when the S category is used
for a full sentence, which most often include punc-
tuation such as the full stop and quotation marks. In
contrast, the majority of expansions for the S cat-
egory do not include any punctuation. The model
has learnt to differentiate between the two different
classes of S ? full sentence versus internal clause ?
due to their different expansions.
8 Conclusion
In this work we have presented a non-parametric
Bayesian model for inducing tree substitution gram-
mars. By incorporating a structured prior over ele-
mentary rules our model is able to reason over the
infinite space of all such rules, producing compact
and simple grammars. In doing so our model learns
local structures for latent linguistic phenomena, such
as verb subcategorisation and lexical agreement. Our
experimental results show that the induced gram-
mars strongly out-perform standard PCFGs, and are
comparable to a state-of-the-art parser on small data
samples. While our results on the full treebank are
well shy of the best available parsers, we have pro-
posed a number of improvements to the model and
the parsing algorithm that could lead to state-of-the-
art performance in the future.
References
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-oriented parsing. Center for the Study of
Language and Information - Studies in Computational
Linguistics. University of Chicago Press.
Rens Bod. 2000. Combining semantic and syntactic
structure for language modeling. In Proceedings of
the 6th International Conference on Spoken Language
Processing, Beijing, China.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Budapest, Hungary, April.
555
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor, Michigan, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 132?139.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of the
19th International Conference on Computational Lin-
guistics, pages 183?189, Taipei, Taiwan.
John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Courant Institute of
Mathematical Sciences, New York University.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 272?279, Prague, Czech
Republic, June.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of COLING/ACL,
Sydney.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al (Bod et al, 2003),
chapter 8.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 139?146, Rochester,
New York, April.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In Advances in Neural Information Processing Sys-
tems 19.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4),
December.
Mark Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Lingusitics,
28(1):71?76, March.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Computa-
tional Linguistics, pages 483?501. Oxford University
Press, Oxford, England.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697, Prague, Czech
Republic, June.
Timothy J. O?Donnell, Noah D. Goodman, Jesse
Snedeker, and Joshua B. Tenenbaum. 2009a. Com-
putation and reuse in language. In 31st Annual Con-
ference of the Cognitive Science Society, Amsterdam,
The Netherlands, July. To appear.
Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009b. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
Report MIT-CSAIL-TR-2009-013, MIT.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of Hu-
man Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 404?411, Rochester,
New York, April. Association for Computational Lin-
guistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July.
Detlef Prescher, Remko Scha, Khalil Sima?an, and An-
dreas Zollmann. 2004. On the statistical consistency
of dop estimators. In Proceedings of the 14th Meet-
ing of Computational Linguistics in the Netherlands,
Antwerp, Belgium.
Fei Xia. 2002. Automatic grammar generation from
two different perspectives. Ph.D. thesis, University of
Pennsylvania.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics,
10(2):367?388.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 551?560, Prague, Czech Republic, June.
556
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 728?735,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Machine Translation by Triangulation:
Making Effective Use of Multi-Parallel Corpora
Trevor Cohn and Mirella Lapata
Human Computer Research Centre, School of Informatics
University of Edinburgh
{tcohn,mlap}@inf.ed.ac.uk
Abstract
Current phrase-based SMT systems perform
poorly when using small training sets. This
is a consequence of unreliable translation es-
timates and low coverage over source and
target phrases. This paper presents a method
which alleviates this problem by exploit-
ing multiple translations of the same source
phrase. Central to our approach is triangula-
tion, the process of translating from a source
to a target language via an intermediate third
language. This allows the use of a much
wider range of parallel corpora for train-
ing, and can be combined with a standard
phrase-table using conventional smoothing
methods. Experimental results demonstrate
BLEU improvements for triangulated mod-
els over a standard phrase-based system.
1 Introduction
Statistical machine translation (Brown et al, 1993)
has seen many improvements in recent years, most
notably the transition from word- to phrase-based
models (Koehn et al, 2003). Modern SMT sys-
tems are capable of producing high quality transla-
tions when provided with large quantities of training
data. With only a small training sample, the trans-
lation output is often inferior to the output from us-
ing larger corpora because the translation algorithm
must rely on more sparse estimates of phrase fre-
quencies and must also ?back-off? to smaller sized
phrases. This often leads to poor choices of target
phrases and reduces the coherence of the output. Un-
fortunately, parallel corpora are not readily available
in large quantities, except for a small subset of the
world?s languages (see Resnik and Smith (2003) for
discussion), therefore limiting the potential use of
current SMT systems.
In this paper we provide a means for obtaining
more reliable translation frequency estimates from
small datasets. We make use of multi-parallel cor-
pora (sentence aligned parallel texts over three or
more languages). Such corpora are often created
by international organisations, the United Nations
(UN) being a prime example. They present a chal-
lenge for current SMT systems due to their rela-
tively moderate size and domain variability (exam-
ples of UN texts include policy documents, proceed-
ings of meetings, letters, etc.). Our method translates
each target phrase, t, first to an intermediate lan-
guage, i, and then into the source language, s. We
call this two-stage translation process triangulation
(Kay, 1997). We present a probabilistic formulation
through which we can estimate the desired phrase
translation distribution (phrase-table) by marginali-
sation, p(s|t) =
?
i p(s, i|t).
As with conventional smoothing methods (Koehn
et al, 2003; Foster et al, 2006), triangulation in-
creases the robustness of phrase translation esti-
mates. In contrast to smoothing, our method allevi-
ates data sparseness by exploring additional multi-
parallel data rather than adjusting the probabilities of
existing data. Importantly, triangulation provides us
with separately estimated phrase-tables which could
be further smoothed to provide more reliable dis-
tributions. Moreover, the triangulated phrase-tables
can be easily combined with the standard source-
target phrase-table, thereby improving the coverage
over unseen source phrases.
As an example, consider Figure 1 which shows
the coverage of unigrams and larger n-gram phrases
when using a standard source target phrase-table, a
triangulated phrase-table with one (it) and nine lan-
guages (all), and a combination of standard and tri-
angulated phrase-tables (all+standard). The phrases
were harvested from a small French-English bitext
728
and evaluated against a test set. Although very few
small phrases are unknown, the majority of larger
phrases are unseen. The Italian and all results show
that triangulation alone can provide similar or im-
proved coverage compared to the standard source-
target model; further improvement is achieved by
combining the triangulated and standard models
(all+standard). These models and datasets will be
described in detail in Section 3.
We also demonstrate that triangulation can be
used on its own, that is without a source-target dis-
tribution, and still yield acceptable translation out-
put. This is particularly heartening, as it provides a
means of translating between the many ?low den-
sity? language pairs for which we don?t yet have a
source-target bitext. This allows SMT to be applied
to a much larger set of language pairs than was pre-
viously possible.
In the following section we provide an overview
of related work. Section 3 introduces a generative
formulation of triangulation. We present our evalua-
tion framework in Section 4 and results in Section 5.
2 Related Work
The idea of using multiple source languages for
improving the translation quality of the target lan-
guage dates back at least to Kay (1997), who ob-
served that ambiguities in translating from one lan-
guage onto another may be resolved if a transla-
tion into some third language is available. Systems
which have used this notion of triangulation typi-
cally create several candidate sentential target trans-
lations for source sentences via different languages.
A single translation is then selected by finding the
candidate that yields the best overall score (Och and
Ney, 2001; Utiyama and Isahara, 2007) or by co-
training (Callison-Burch and Osborne, 2003). This
ties in with recent work on ensemble combinations
of SMT systems, which have used alignment tech-
niques (Matusov et al, 2006) or simple heuristics
(Eisele, 2005) to guide target sentence selection and
generation. Beyond SMT, the use of an intermediate
language as a translation aid has also found appli-
cation in cross-lingual information retrieval (Gollins
and Sanderson, 2001).
Callison-Burch et al (2006) propose the use of
paraphrases as a means of dealing with unseen
source phrases. Their method acquires paraphrases
by identifying candidate phrases in the source lan-
1 2 3 4 5 6
phrase length
propo
rtion 
of tes
t even
ts in p
hrase
 table
0.005
0.01
0.02
0.05
0.1
0.2
0.5
1 standardItalianallall + standard
Figure 1: Coverage of fr ? en test phrases using a 10,000 sen-
tence bitext. The standard model is shown alongside triangu-
lated models using one (Italian) or nine other languages (all).
guage, translating them into multiple target lan-
guages, and then back to the source. Unknown
source phrases are substituted by the back-translated
paraphrases and translation proceeds on the para-
phrases.
In line with previous work, we exploit multi-
ple source corpora to alleviate data sparseness and
increase translation coverage. However, we differ
in several important respects. Our method oper-
ates over phrases rather than sentences. We propose
a generative formulation which treats triangulation
not as a post-processing step but as part of the trans-
lation model itself. The induced phrase-table entries
are fed directly into the decoder, thus avoiding the
additional inefficiencies of merging the output of
several translation systems.
Although related to Callison-Burch et al (2006)
our method is conceptually simpler and more gen-
eral. Phrase-table entries are created via multiple
source languages without the intermediate step of
paraphrase extraction, thereby reducing the expo-
sure to compounding errors. Our phrase-tables may
well contain paraphrases but these are naturally in-
duced as part of our model, without extra processing
effort. Furthermore, we improve the translation esti-
mates for both seen and unseen phrase-table entries,
whereas Callison-Burch et al concentrate solely on
unknown phrases. In contrast to Utiyama and Isa-
hara (2007), we employ a large number of inter-
mediate languages and demonstrate how triangu-
lated phrase-tables can be combined with standard
phrase-tables to improve translation output.
729
en varm kartoffeleen hete aardappel uma batata quente
une patate une patate chaudd?licate une question d?licate
a hot potato
source
intermediate
target
Figure 2: Triangulation between English (source) and French (target), showing three phrases in Dutch, Danish and Portuguese,
respectively. Arrows denote phrases aligned in a language pair and also the generative translation process.
3 Triangulation
We start with a motivating example before formalis-
ing the mechanics of triangulation. Consider trans-
lating the English phrase a hot potato1 into French,
as shown in Figure 2. In our corpus this English
phrase occurs only three times. Due to errors in
the word alignment the phrase was not included in
the English-French phrase-table. Triangulation first
translates hot potato into a set of intermediate lan-
guages (Dutch, Danish and Portuguese are shown in
the figure), and then these phrases are further trans-
lated into the target language (French). In the ex-
ample, four different target phrases are obtained, all
of which are useful phrase-table entries. We argue
that the redundancy introduced by a large suite of
other languages can correct for errors in the word
alignments and also provide greater generalisation,
since the translation distribution is estimated from a
richer set of data-points. For example, instances of
the Danish en varm kartoffel may be used to trans-
late several English phrases, not only a hot potato.
In general we expect that a wider range of pos-
sible translations are found for any source phrase,
simply due to the extra layer of indirection. So, if a
source phrase tends to align with two different tar-
get phrases, then we would also expect it to align
with two phrases in the ?intermediate? language.
These intermediate phrases should then each align
with two target phrases, yielding up to four target
phrases. Consequently, triangulation will often pro-
duce more varied translation distributions than the
standard source-target approach.
3.1 Formalisation
We now formalise triangulation as a generative
probabilistic process operating independently on
phrase pairs. We start with the conditional distri-
bution over three languages, p(s, i|t), where the ar-
guments denote phrases in the source, intermediate
1An idiom meaning a situation for which no one wants to
claim responsibility.
and target language, respectively. From this distri-
bution, we can find the desired conditional over the
source-target pair by marginalising out the interme-
diate phrases:2
p(s|t) =
?
i
p(s|i, t)p(i|t)
?
?
i
p(s|i)p(i|t) (1)
where (1) imposes a simplifying conditional inde-
pendence assumption: the intermediate phrase fully
represents the information (semantics, syntax, etc.)
in the source phrase, rendering the target phrase re-
dundant in p(s|i, t).
Equation (1) requires that all phrases in the
intermediate-target bitext must also be found in the
source-intermediate bitext, such that p(s|i) is de-
fined. Clearly this will often not be the case. In these
situations we could back-off to another distribution
(by discarding part, or all, of the conditioning con-
text), however we take a more pragmatic approach
and ignore the missing phrases. This problem of
missing contexts is uncommon in multi-parallel cor-
pora, but is more common when the two bitexts are
drawn from different sources.
While triangulation is intuitively appealing, it
may suffer from a few problems. Firstly, as with any
SMT approach, the translation estimates are based
on noisy automatic word alignments. This leads to
many errors and omissions in the phrase-table. With
a standard source-target phrase-table these errors are
only encountered once, however with triangulation
they are encountered twice, and therefore the errors
will compound. This leads to more noisy estimates
than in the source-target phrase-table.
Secondly, the increased exposure to noise means
that triangulation will omit a greater proportion of
large or rare phrases than the standard method. An
2Equation (1) is used with the source and target arguments
reversed to give p(t|s).
730
alignment error in either of the source-intermediate
or intermediate-target bitexts can prevent the extrac-
tion of a source-target phrase pair. This effect can be
seen in Figure 1, where the coverage of the Italian
triangulated phrase-table is worse than the standard
source-target model, despite the two models using
the same sized bitexts. As we explain in the next
section, these problems can be ameliorated by us-
ing the triangulated phrase-table in conjunction with
a standard phrase-table.
Finally, another potential problem stems from the
independence assumption in (1), which may be an
oversimplification and lead to a loss of information.
The experiments in Section 5 show that this effect is
only mild.
3.2 Merging the phrase-tables
Once induced, the triangulated phrase-table can be
usefully combined with the standard source-target
phrase-table. The simplest approach is to use linear
interpolation to combine the two (or more) distribu-
tions, as follows:
p(s, t) =
?
j
?jpj(s, t) (2)
where each joint distribution, pj , has a non-negative
weight, ?j , and the weights sum to one. The joint
distribution for triangulated phrase-tables is defined
in an analogous way to Equation (1). We expect
that the standard phrase-table should be allocated
a higher weight than triangulated phrase-tables, as
it will be less noisy. The joint distribution is now
conditionalised to yield p(s|t) and p(t|s), which are
both used as features in the decoder. Note that the re-
sulting conditional distribution will be drawn solely
from one input distribution when the conditioning
context is unseen in the remaining distributions. This
may lead to an over-reliance on unreliable distribu-
tions, which can be ameliorated by smoothing (e.g.,
Foster et al (2006)).
As an alternative to linear interpolation, we also
employ a weighted product for phrase-table combi-
nation:
p(s|t) ?
?
j
pj(s|t)?j (3)
This has the same form used for log-linear training
of SMT decoders (Och, 2003), which allows us to
treat each distribution as a feature, and learn the mix-
ing weights automatically. Note that we must indi-
vidually smooth the component distributions in (3)
to stop zeros from propagating. For this we use
Simple Good-Turing smoothing (Gale and Samp-
son, 1995) for each distribution, which provides es-
timates for zero count events.
4 Experimental Design
Corpora We used the Europarl corpus (Koehn,
2005) for experimentation. This corpus consists of
about 700,000 sentences of parliamentary proceed-
ings from the European Union in eleven European
languages. We present results on the full corpus for a
range of language pairs. In addition, we have created
smaller parallel corpora by sub-sampling 10,000
sentence bitexts for each language pair. These cor-
pora are likely to have minimal overlap ? about
1.5% of the sentences will be shared between each
pair. However, the phrasal overlap is much greater
(10 to 20%), which allows for triangulation using
these common phrases. This training setting was
chosen to simulate translating to or from a ?low
density? language, where only a few small indepen-
dently sourced parallel corpora are available. These
bitexts were used for direct translation and triangula-
tion. All experimental results were evaluated on the
ACL/WMT 20053 set of 2,000 sentences, and are
reported in BLEU percentage-points.
Decoding Pharaoh (Koehn, 2003), a beam-
search decoder, was used to maximise:
T? = argmax
T
?
j
fj(T,S)?j (4)
where T and S denote a target and source sentence
respectively. The parameters, ?j , were trained using
minimum error rate training (Och, 2003) to max-
imise the BLEU score (Papineni et al, 2002) on
a 150 sentence development set. We used a stan-
dard set of features, comprising a 4-gram language
model, distance based distortion model, forward
and backward translation probabilities, forward and
backward lexical translation scores and the phrase-
and word-counts. The translation models and lex-
ical scores were estimated on the training corpus
which was automatically aligned using Giza++ (Och
et al, 1999) in both directions between source and
target and symmetrised using the growing heuristic
(Koehn et al, 2003).
3For details see http://www.statmt.org/wpt05/
mt-shared-task.
731
Lexical weights The lexical translation score is
used for smoothing the phrase-table translation esti-
mate. This represents the translation probability of a
phrase when it is decomposed into a series of inde-
pendent word-for-word translation steps (Koehn et
al., 2003), and has proven a very effective feature
(Zens and Ney, 2004; Foster et al, 2006). Pharaoh?s
lexical weights require access to word-alignments;
calculating these alignments between the source and
target words in a phrase would prove difficult for
a triangulated model. Therefore we use a modified
lexical score, corresponding to the maximum IBM
model 1 score for the phrase pair:
lex(t|s) =
1
Z
max
a
?
k
p(tk|sak) (5)
where the maximisation4 ranges over all one-to-
many alignments and Z normalises the score by the
number of possible alignments.
The lexical probability is obtained by interpo-
lating a relative frequency estimate on the source-
target bitext with estimates from triangulation, in
the same manner used for phrase translations in (1)
and (2). The addition of the lexical probability fea-
ture yielded a substantial gain of up to two BLEU
points over a basic feature set.
5 Experimental Results
The evaluation of our method was motivated by
three questions: (1) How do different training re-
quirements affect the performance of the triangu-
lated models presented in this paper? We expect
performance gains with triangulation on small and
moderate datasets. (2) Is machine translation out-
put influenced by the choice of the intermediate lan-
guage/s? Here, we would like to evaluate whether
the number and choice of intermediate languages
matters. (3) What is the quality of the triangulated
phrase-table? In particular, we are interested in the
resulting distribution and whether it is sufficiently
distinct from the standard phrase-table.
5.1 Training requirements
Before reporting our results, we briefly discuss the
specific choice of model for our experiments. As
mentioned in Section 3, our method combines the
4The maximisation in (5) can be replaced with a sum with
similar experimental results.
standard interp +indic separate
en ? de 12.03 12.66 12.95 12.25
fr ? en 23.02 24.63 23.86 23.43
Table 1: Different feature sets used with the 10K training
corpora, using a single language (es) for triangulation. The
columns refer to standard, uniform interpolation, interpolation
with 0-1 indicator features, and separate phrase-tables, respec-
tively.
triangulated phrase-table with the standard source-
target one. This is desired in order to compensate for
the noise incurred by the triangulation process. We
used two combination methods, namely linear inter-
polation (see (2)) and a weighted geometric mean
(see (3)).
Table 1 reports the results for two translation tasks
when triangulating with a single language (es) us-
ing three different feature sets, each with different
translation features. The interpolation model uses
uniform linear interpolation to merge the standard
and triangulated phrase-tables. Non-uniform mix-
tures did not provide consistent gains, although,
as expected, biasing towards the standard phrase-
table was more effective than against. The indicator
model uses the same interpolated distribution along
with a series of 0-1 indicator features to identify the
source of each event, i.e., if each (s, t) pair is present
in phrase-table j. We also tried per-context features
with similar results. The separate model has a sepa-
rate feature for each phrase-table.
All three feature sets improve over the standard
source-target system, while the interpolated features
provided the best overall performance. The rela-
tively poorer performance of the separate model
is perhaps surprising, as it is able to differentially
weight the component distributions; this is probably
due to MERT not properly handling the larger fea-
ture sets. In all subsequent experiments we report
results using linear interpolation.
As a proof of concept, we first assessed the ef-
fect of triangulation on corpora consisting of 10,000
sentence bitexts. We expect triangulation to de-
liver performance gains on small corpora, since a
large number of phrase-table entries will be un-
seen. In Table 2 each entry shows the BLEU score
when using the standard phrase-table and the ab-
solute improvement when using triangulation. Here
we have used three languages for triangulation
(it ? {de, en, es, fr}\{s, t}). The source-target lan-
guages were chosen so as to mirror the evaluation
setup of NAACL/WMT. The translation tasks range
732
s ? t ? de en es fr
de - 17.58 16.84 18.06
- +1.20 +1.99 +1.94
en 12.45 - 23.83 24.05
+1.22 - +1.04 +1.48
es 12.31 23.83 - 32.69
+2.24 +1.35 - +0.85
fr 11.76 23.02 31.22 -
+2.41 +2.24 +1.30 -
Table 2: BLEU improvements over the standard phrase-table
(top) when interpolating with three triangulated phrase-tables
(bottom) on the small training sample.
from easy (es ? fr) to very hard (de ? en). In all
cases triangulation resulted in an improvement in
translation quality, with the highest gains observed
for the most difficult tasks (to and from German).
For these tasks the standard systems have poor cov-
erage (due in part to the sizeable vocabulary of Ger-
man phrases) and therefore the gain can be largely
explained by the additional coverage afforded by the
triangulated phrase-tables.
To test whether triangulation can also improve
performance of larger corpora we ran six separate
translation tasks on the full Europarl corpus. The
results are presented in Table 3, for a single trian-
gulation language used alone (triang) or uniformly
interpolated with the standard phrase-table (interp).
These results show that triangulation can produce
high quality translations on its own, which is note-
worthy, as it allows for SMT between a much larger
set of language pairs. Using triangulation in con-
junction with the standard phrase-table improved
over the standard system in most instances, and
only degraded performance once. The improvement
is largest for the German tasks which can be ex-
plained by triangulation providing better robustness
to noisy alignments (which are often quite poor for
German) and better estimates of low-count events.
The difficulty of aligning German with the other lan-
guages is apparent from the Giza++ perplexity: the
final Model 4 perplexities for German are quite high,
as much as double the perplexity for more easily
aligned language pairs (e.g., Spanish-French).
Figure 3 shows the effect of triangulation on dif-
ferent sized corpora for the language pair fr ? en.
It presents learning curves for the standard system
and a triangulated system using one language (es).
As can be seen, gains from triangulation only di-
minish slightly for larger training corpora, and that
task standard interm triang interp
de ? en 23.85 es 23.48 24.36
en ? de 17.24 es 16.28 17.42
es ? en 30.48 fr 29.06 30.52
en ? es 29.09 fr 28.19 29.09
fr ? en 29.66 es 29.59 30.36
en ? fr 30.07 es 28.94 29.62
Table 3: Results on the full training set showing triangulation
with a single language, both alone (triang) and alongside a stan-
dard model (interp).
l
l
l
l
size of training bitext(s)
BLE
U sc
ore
10K 40K 160K 700K
22
24
26
28
30
l standardtrianginterp
Figure 3: Learning curve for fr ? en translation for the standard
source-target model and a triangulated model using Spanish as
an intermediate language.
the purely triangulated models have very competi-
tive performance. The gain from interpolation with
a triangulated model is roughly equivalent to having
twice as much training data.
Finally, notice that triangulation may benefit
when the sentences in each bitext are drawn from the
same source, in that there are no unseen ?intermedi-
ate? phrases, and therefore (1) can be easily evalu-
ated. We investigate this by examining the robust-
ness of our method in the face of disjoint bitexts.
The concepts contained in each bitext will be more
varied, potentially leading to better coverage of the
target language. In lieu of a study on different do-
main bitexts which we plan for the future, we bi-
sected the Europarl corpus for fr ? en, triangulat-
ing with Spanish. The triangulated models were pre-
sented with fr-es and es-en bitexts drawn from either
the same half of the corpus or from different halves,
resulting in scores of 28.37 and 28.13, respectively.5
These results indicate that triangulation is effective
5The baseline source-target system on one half has a score
of 28.85.
733
triang interp
BLE
U sc
ore
19
20
21
22
23
24
25
fi (?14.26)
da
da
de
de
el
el
es
es
fi
it
it
nl
nl
pt
pt
sv
sv
Figure 4: Comparison of different triangulation languages for
fr ? en translation, relative to the standard model (10K training
sample). The bar for fi has been truncated to fit on the graph.
for disjoint bitexts, although ideally we would test
this with independently sourced parallel texts.
5.2 The choice of intermediate languages
The previous experiments used an ad-hoc choice
of ?intermediate? language/s for triangulation, and
we now examine which languages are most effec-
tive. Figure 4 shows the efficacy of the remaining
nine languages when translating fr ? en. Minimum
error-rate training was not used for this experiment,
or the next shown in Figure 5, in order to highlight
the effect of the changing translation estimates. Ro-
mance languages (es, it, pt) give the best results,
both on their own and when used together with the
standard phrase-table (using uniform interpolation);
Germanic languages (de, nl, da, sv) are a distant sec-
ond, with the less related Greek and Finnish the least
useful. Interpolation yields an improvement for all
?intermediate? languages, even Finnish, which has a
very low score when used alone.
The same experiment was repeated for en ? de
translation with similar trends, except that the
Germanic languages out-scored the Romance lan-
guages. These findings suggest that ?intermediate?
languages which exhibit a high degree of similarity
with the source or target language are desirable. We
conjecture that this is a consequence of better auto-
matic word alignments and a generally easier trans-
lation task, as well as a better preservation of infor-
mation between aligned phrases.
Using a single language for triangulation clearly
improves performance, but can we realise further
improvements by using additional languages? Fig-
1 2 3 4 5 6 7 8 9
# intermediate languages
BLEU
 scor
e
22
23
24
25
26 trianginterp
Figure 5: Increasing the number of intermediate languages used
for triangulation increases performance for fr ? en (10K train-
ing sample). The dashed line shows the BLEU score for the
standard phrase-table.
ure 5 shows the performance profile for fr ? en
when adding languages in a fixed order. The lan-
guages were ordered by family, with Romance be-
fore Germanic before Greek and Finnish. Each ad-
dition results in an increase in performance, even for
the final languages, from which we expect little in-
formation. The purely triangulated (triang) and in-
terpolated scores (interp) are converging, suggesting
that the source-target bitext is redundant given suf-
ficient triangulated data. We obtained similar results
for en ? de.
5.3 Evaluating the quality of the phrase-table
Our experimental results so far have shown that
triangulation is not a mere approximation of the
source-target phrase-table, but that it extracts addi-
tional useful translation information. We now as-
sess the phrase-table quality more directly. Com-
parative statistics of a standard and a triangulated
phrase-table are given in Table 4. The coverage over
source and target phrases is much higher in the stan-
dard table than the triangulated tables, which reflects
the reduced ability of triangulation to extract large
phrases ? despite the large increase in the num-
ber of events. The table also shows the overlapping
probability mass which measures the sum of prob-
ability in one table for which the events are present
in the other. This shows that the majority of mass
is shared by both tables (as joint distributions), al-
though there are significant differences. The Jensen-
Shannon divergence is perhaps more appropriate for
the comparison, giving a relatively high divergence
734
standard triang
source phrases (M) 8 2.5
target phrases (M) 7 2.5
events (M) 12 70
overlapping mass 0.646 0.750
Table 4: Comparative statistics of the standard triangulated table
on fr ? en using the full training set and Spanish as an inter-
mediate language.
of 0.3937. This augurs well for the combination of
standard and triangulated phrase-tables, where di-
versity is valued. The decoding results (shown in
Table 3 for fr ? en) indicate that the two meth-
ods have similar efficacy, and that their interpolated
combination provides the best overall performance.
6 Conclusion
In this paper we have presented a novel method for
obtaining more reliable translation estimates from
small datasets. The key premise of our work is that
multi-parallel data can be usefully exploited for im-
proving the coverage and quality of phrase-based
SMT. Our triangulation method translates from a
source to a target via one or many intermediate lan-
guages. We present a generative formulation of this
process and show how it can be used together with
the entries of a standard source-target phrase-table.
We observe large performance gains when trans-
lating with triangulated models trained on small
datasets. Furthermore, when combined with a stan-
dard phrase-table, our models also yield perfor-
mance improvements on larger datasets. Our exper-
iments revealed that triangulation benefits from a
large set of intermediate languages and that perfor-
mance is increased when languages of the same fam-
ily to the source or target are used as intermediates.
We have just scratched the surface of the possi-
bilities for the framework discussed here. Important
future directions lie in combining triangulation with
richer means of conventional smoothing and using
triangulation to translate between low-density lan-
guage pairs.
Acknowledgements The authors acknowledge
the support of EPSRC (grants GR/T04540/01 and
GR/T04557/01). Special thanks to Markus Becker, Chris
Callison-Burch, David Talbot and Miles Osborne for their
helpful comments.
References
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, R. L. Mercer. 1993.
The mathematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics, 19(2):263?311.
C. Callison-Burch, M. Osborne. 2003. Bootstrapping parallel
corpora. In Proceedings of the NAACL Workshop on Build-
ing and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, Edmonton, Canada.
C. Callison-Burch, P. Koehn, M. Osborne. 2006. Improved sta-
tistical machine translation using paraphrases. In Proceed-
ings of the HLT/NAACL, 17?24, New York, NY.
A. Eisele. 2005. First steps towards multi-engine machine
translation. In Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts, 155?158, Ann Arbor, MI.
G. Foster, R. Kuhn, H. Johnson. 2006. Phrase-table smooth-
ing for statistical machine translation. In Proceedings of the
EMNLP, 53?61, Sydney, Australia.
W. A. Gale, G. Sampson. 1995. Good-turing frequency esti-
mation without tears. Journal of Quantitative Linguistics,
2(3):217?237.
T. Gollins, M. Sanderson. 2001. Improving cross language
retrieval with triangulated translation. In Proceedings of the
SIGIR, 90?95, New Orleans, LA.
M. Kay. 1997. The proper place of men and machines in lan-
guage translation. Machine Translation, 12(1?2):3?23.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of the HLT/NAACL, 48?
54, Edomonton, Canada.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, Uni-
versity of Southern California, Los Angeles, California.
P. Koehn. 2005. Europarl: A parallel corpus for evaluation of
machine translation. In Proceedings of MT Summit, Phuket,
Thailand.
E. Matusov, N. Ueffing, H. Ney. 2006. Computing consesus
translation from multiple machine translation systems us-
ing enhanced hypotheses alignment. In Proceedings of the
EACL, 33?40, Trento, Italy.
F. J. Och, H. Ney. 2001. Statistical multi-source translation. In
Proceedings of the MT Summit, 253?258, Santiago de Com-
postela, Spain.
F. J. Och, C. Tillmann, H. Ney. 1999. Improved alignment
models for statistical machine translation. In Proceedings of
the EMNLP and VLC, 20?28, University of Maryland, Col-
lege Park, MD.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of the ACL, 160?167, Sap-
poro, Japan.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. BLEU: A
method for automatic evaluation of machine translation. In
Proceedings of the ACL, 311?318, Philadelphia, PA.
P. Resnik, N. A. Smith. 2003. The Web as a parallel corpus.
Computational Linguistics, 29(3):349?380.
M. Utiyama, H. Isahara. 2007. A comparison of pivot methods
for phrase-based statistical machine translation. In Proceed-
ings of the HLT/NAACL, 484?491, Rochester, NY.
R. Zens, H. Ney. 2004. Improvements in phrase-based statisti-
cal machine translation. In D. M. Susan Dumais, S. Roukos,
eds., Proceedings of the HLT/NAACL, 257?264, Boston,
MA.
735
Proceedings of ACL-08: HLT, pages 200?208,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Discriminative Latent Variable Model
for Statistical Machine Translation
Phil Blunsom, Trevor Cohn and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
{pblunsom,tcohn,miles}@inf.ed.ac.uk
Abstract
Large-scale discriminative machine transla-
tion promises to further the state-of-the-art,
but has failed to deliver convincing gains over
current heuristic frequency count systems. We
argue that a principle reason for this failure is
not dealing with multiple, equivalent transla-
tions. We present a translation model which
models derivations as a latent variable, in both
training and decoding, and is fully discrimina-
tive and globally optimised. Results show that
accounting for multiple derivations does in-
deed improve performance. Additionally, we
show that regularisation is essential for max-
imum conditional likelihood models in order
to avoid degenerate solutions.
1 Introduction
Statistical machine translation (SMT) has seen
a resurgence in popularity in recent years, with
progress being driven by a move to phrase-based and
syntax-inspired approaches. Progress within these
approaches however has been less dramatic. We be-
lieve this is because these frequency count based1
models cannot easily incorporate non-independent
and overlapping features, which are extremely use-
ful in describing the translation process. Discrimi-
native models of translation can include such fea-
tures without making assumptions of independence
or explicitly modelling their interdependence. How-
ever, while discriminative models promise much,
they have not been shown to deliver significant gains
1We class approaches using minimum error rate training
(Och, 2003) frequency count based as these systems re-scale a
handful of generative features estimated from frequency counts
and do not support large sets of non-independent features.
over their simpler cousins. We argue that this is due
to a number of inherent problems that discrimina-
tive models for SMT must address, in particular the
problems of spurious ambiguity and degenerate so-
lutions. These occur when there are many ways to
translate a source sentence to the same target sen-
tence by applying a sequence of steps (a derivation)
of either phrase translations or synchronous gram-
mar rules, depending on the type of system. Exist-
ing discriminative models require a reference deriva-
tion to optimise against, however no parallel cor-
pora annotated for derivations exist. Ideally, a model
would account for this ambiguity by marginalising
out the derivations, thus predicting the best transla-
tion rather than the best derivation. However, doing
so exactly is NP-complete. For this reason, to our
knowledge, all discriminative models proposed to
date either side-step the problem by choosing simple
model and feature structures, such that spurious am-
biguity is lessened or removed entirely (Ittycheriah
and Roukos, 2007; Watanabe et al, 2007), or else ig-
nore the problem and treat derivations as translations
(Liang et al, 2006; Tillmann and Zhang, 2007).
In this paper we directly address the problem of
spurious ambiguity in discriminative models. We
use a synchronous context free grammar (SCFG)
translation system (Chiang, 2007), a model which
has yielded state-of-the-art results on many transla-
tion tasks. We present two main contributions. First,
we develop a log-linear model of translation which
is globally trained on a significant number of paral-
lel sentences. This model maximises the conditional
likelihood of the data, p(e|f), where e and f are the
English and foreign sentences, respectively. Our es-
timation method is theoretically sound, avoiding the
biases of the heuristic relative frequency estimates
200
l
l
l
l
l l
l
l
l
l
l
sentence length
deriv
ation
s
5 7 9 11 13 15
1e+0
3
1e+0
5
1e+0
8
Figure 1. Exponential relationship between sentence
length and the average number of derivations (on a log
scale) for each reference sentence in our training corpus.
(Koehn et al, 2003). Second, within this frame-
work, we model the derivation, d, as a latent vari-
able, p(e,d|f), which is marginalised out in train-
ing and decoding. We show empirically that this
treatment results in significant improvements over a
maximum-derivation model.
The paper is structured as follows. In Section 2
we list the challenges that discriminative SMT must
face above and beyond the current systems. We sit-
uate our work, and previous work, on discrimina-
tive systems in this context. We present our model
in Section 3, including our means of training and de-
coding. Section 4 reports our experimental setup and
results, and finally we conclude in Section 5.
2 Challenges for Discriminative SMT
Discriminative models allow for the use of expres-
sive features, in the order of thousands or millions,
which can reference arbitrary aspects of the source
sentence. Given most successful SMT models have
a highly lexicalised grammar (or grammar equiva-
lent), these features can be used to smuggle in lin-
guistic information, such as syntax and document
context. With this undoubted advantage come four
major challenges when compared to standard fre-
quency count SMT models:
1. There is no one reference derivation. Often
there are thousands of ways of translating a
source sentence into the reference translation.
Figure 1 illustrates the exponential relationship
between sentence length and the number of
derivations. Training is difficult without a clear
target, and predicting only one derivation at test
time is fraught with danger.
2. Parallel translation data is often very noisy,
with such problems as non-literal translations,
poor sentence- and word-alignments. A model
which exactly translates the training data will
inevitably perform poorly on held-out data.
This problem of over-fitting is exacerbated
in discriminative models with large, expres-
sive, feature sets. Regularisation is essential for
models with more than a handful of features.
3. Learning with a large feature set requires many
training examples and typically many iterations
of a solver during training. While current mod-
els focus solely on efficient decoding, discrim-
inative models must also allow for efficient
training.
Past work on discriminative SMT only address
some of these problems. To our knowledge no sys-
tems directly address Problem 1, instead choosing to
ignore the problem by using one or a small handful
of reference derivations in an n-best list (Liang et al,
2006; Watanabe et al, 2007), or else making local
independence assumptions which side-step the issue
(Ittycheriah and Roukos, 2007; Tillmann and Zhang,
2007; Wellington et al, 2006). These systems all in-
clude regularisation, thereby addressing Problem 2.
An interesting counterpoint is the work of DeNero et
al. (2006), who show that their unregularised model
finds degenerate solutions. Some of these discrim-
inative systems have been trained on large training
sets (Problem 3); these systems are the local models,
for which training is much simpler. Both the global
models (Liang et al, 2006; Watanabe et al, 2007)
use fairly small training sets, and there is no evi-
dence that their techniques will scale to larger data
sets.
Our model addresses all three of the above prob-
lems within a global model, without resorting to n-
best lists or local independence assumptions. Fur-
thermore, our model explicitly accounts for spurious
ambiguity without altering the model structure or ar-
bitrarily selecting one derivation. Instead we model
the translation distribution with a latent variable for
the derivation, which we marginalise out in training
and decoding.
201
the hat
le chapeau
red
the hat
le chapeau
red
Figure 2. The dropping of an adjective in this example
means that there is no one segmentation that we could
choose that would allow a system to learn le ? the and
chapeau? hat.
?S? ? ?S 1 X 2 , S 1 X 2 ?
?S? ? ?X 1 , X 1 ?
?X? ? ?ne X 1 pas, does not X 1 ?
?X? ? ?va, go?
?X? ? ?il, he?
Figure 3. A simple SCFG, with non-terminal symbols S
and X, which performs the transduction: il ne vas pas ?
he does not go
This itself provides robustness to noisy data, in
addition to the explicit regularisation from a prior
over the model parameters. For example, in many
cases there is no one perfect derivation, but rather
many imperfect ones which each include some good
translation fragments. The model can learn from
many of these derivations and thereby learn from
all these translation fragments. This situation is il-
lustrated in Figure 2 where the non-translated ad-
jective red means neither segmentation is ?correct?,
although both together present positive evidence for
the two lexical translations.
We present efficient methods for training and pre-
diction, demonstrating their scaling properties by
training on more than a hundred thousand train-
ing sentences. Finally, we stress that our main find-
ings are general ones. These results could ? and
should ? be applied to other models, discriminative
and generative, phrase- and syntax-based, to further
progress the state-of-the-art in machine translation.
3 Discriminative Synchronous
Transduction
A synchronous context free grammar (SCFG) con-
sists of paired CFG rules with co-indexed non-
terminals (Lewis II and Stearns, 1968). By assign-
ing the source and target languages to the respective
sides of a SCFG it is possible to describe translation
as the process of parsing the source sentence using
a CFG, while generating the target translation from
the other (Chiang, 2007). All the models we present
use the grammar extraction technique described in
Chiang (2007), and are bench-marked against our
own implementation of this hierarchical model (Hi-
ero). Figure 3 shows a simple instance of a hierar-
chical grammar with two non-terminals. Note that
our approach is general and could be used with other
synchronous grammar transducers (e.g., Galley et al
(2006)).
3.1 A global log-linear model
Our log-linear translation model defines a condi-
tional probability distribution over the target trans-
lations of a given source sentence. A particular se-
quence of SCFG rule applications which produces a
translation from a source sentence is referred to as a
derivation, and each translation may be produced by
many different derivations. As the training data only
provides source and target sentences, the derivations
are modelled as a latent variable.
The conditional probability of a derivation, d, for
a target translation, e, conditioned on the source, f ,
is given by:
p?(d, e|f) =
exp
?
k ?kHk(d, e, f)
Z?(f)
(1)
where Hk(d, e, f) =
?
r?d
hk(f , r) (2)
Here k ranges over the model?s features, and
? = {?k} are the model parameters (weights for
their corresponding features). The feature functions
Hk are predefined real-valued functions over the
source and target sentences, and can include over-
lapping and non-independent features of the data.
The features must decompose with the derivation,
as shown in (2). The features can reference the en-
tire source sentence coupled with each rule, r, in a
derivation. The distribution is globally normalised
by the partition function, Z?(f), which sums out the
numerator in (1) for every derivation (and therefore
every translation) of f :
Z?(f) =
?
e
?
d??(e,f)
exp
?
k
?kHk(d, e, f)
Given (1), the conditional probability of a target
translation given the source is the sum over all of
its derivations:
p?(e|f) =
?
d??(e,f)
p?(d, e|f) (3)
202
where ?(e, f) is the set of all derivations of the tar-
get sentence e from the source f.
Most prior work in SMT, both generative and dis-
criminative, has approximated the sum over deriva-
tions by choosing a single ?best? derivation using a
Viterbi or beam search algorithm. In this work we
show that it is both tractable and desirable to directly
account for derivational ambiguity. Our findings
echo those observed for latent variable log-linear
models successfully used in monolingual parsing
(Clark and Curran, 2007; Petrov et al, 2007). These
models marginalise over derivations leading to a de-
pendency structure and splits of non-terminal cate-
gories in a PCFG, respectively.
3.2 Training
The parameters of our model are estimated
from our training sample using a maximum a
posteriori (MAP) estimator. This maximises
the likelihood of the parallel training sen-
tences, D = {(e, f)}, penalised using a prior,
i.e., ?MAP = arg max? p?(D)p(?). We use a
zero-mean Gaussian prior with the probability
density function p0(?k) ? exp
(
??2k/2?
2
)
.2 This
results in the following log-likelihood objective and
corresponding gradient:
L =
?
(e,f)?D
log p?(e|f) +
?
k
log p0(?k) (4)
?L
??k
= Ep?(d|e,f)[hk]? Ep?(e|f)[hk]?
?k
?2
(5)
In order to train the model, we maximise equation
(4) using L-BFGS (Malouf, 2002; Sha and Pereira,
2003). This method has been demonstrated to be ef-
fective for (non-convex) log-linear models with la-
tent variables (Clark and Curran, 2004; Petrov et al,
2007). Each L-BFGS iteration requires the objective
value and its gradient with respect to the model pa-
rameters. These are calculated using inside-outside
inference over the feature forest defined by the
SCFG parse chart of f yielding the partition func-
tion, Z?(f), required for the log-likelihood, and the
marginals, required for its derivatives.
Efficiently calculating the objective and its gradi-
ent requires two separate packed charts, each rep-
resenting a derivation forest. The first one is the full
chart over the space of possible derivations given the
2In general, any conjugate prior could be used instead of a
simple Gaussian.
source sentence. The inside-outside algorithm over
this chart gives the marginal probabilities for each
chart cell, from which we can find the feature ex-
pectations. The second chart contains the space of
derivations which produce the reference translation
from the source. The derivations in this chart are a
subset of those in the full derivation chart. Again,
we use the inside-outside algorithm to find the ?ref-
erence? feature expectations from this chart. These
expectations are analogous to the empirical observa-
tion of maximum entropy classifiers.
Given these two charts we can calculate the log-
likelihood of the reference translation as the inside-
score from the sentence spanning cell of the ref-
erence chart, normalised by the inside-score of the
spanning cell from the full chart. The gradient is cal-
culated as the difference of the feature expectations
of the two charts. Clark and Curran (2004) provides
a more complete discussion of parsing with a log-
linear model and latent variables.
The full derivation chart is produced using a CYK
parser in the same manner as Chiang (2005), and has
complexity O(|e|3). We produce the reference chart
by synchronously parsing the source and reference
sentences using a variant of CYK algorithm over two
dimensions, with a time complexity of O(|e|3|f |3).
This is an instance of the ITG alignment algorithm
(Wu, 1997). This step requires the reference transla-
tion for each training instance to be contained in the
model?s hypothesis space. Achieving full coverage
implies inducing a grammar which generates all ob-
served source-target pairs, which is difficult in prac-
tise. Instead we discard the unreachable portion of
the training sample (24% in our experiments). The
proportion of discarded sentences is a function of
the grammar used. Extraction heuristics other than
the method used herein (Chiang, 2007) could allow
complete coverage (e.g., Galley et al (2004)).
3.3 Decoding
Accounting for all derivations of a given transla-
tion should benefit not only training, but also decod-
ing. Unfortunately marginalising over derivations in
decoding is NP-complete. The standard solution is
to approximate the maximum probability translation
using a single derivation (Koehn et al, 2003).
Here we approximate the sum over derivations di-
rectly using a beam search in which we produce a
beam of high probability translation sub-strings for
each cell in the parse chart. This algorithm is sim-
203
X[1,2]
 
on
X
[2,3]
 
the
X
[3,4]
 
table
X
[1,3]
 
on the
X
[2,4]
 
the table
X
[1,3]
 
on the table
X
[3,4]
 
chart
X
[2,4]
 
the chart
X
[1,3]
 
on the chart
 s
1
  
sur  
2
  
la  
3
  
table 
 
4
Figure 4. Hypergraph representation of max translation
decoding. Each chart cell must store the entire target
string generated.
ilar to the methods for decoding with a SCFG in-
tersected with an n-gram language model, which re-
quire language model contexts to be stored in each
chart cell. However, while Chiang (2005) stores an
abbreviated context composed of the n ? 1 target
words on the left and right edge of the target sub-
string, here we store the entire target string. Addi-
tionally, instead of maximising scores in each beam
cell, we sum the inside scores for each derivation
that produces a given string for that cell. When the
beam search is complete we have a list of trans-
lations in the top beam cell spanning the entire
source sentence along with their approximated in-
side derivation scores. Thus we can assign each
translation string a probability by normalising its in-
side score by the sum of the inside scores of all the
translations spanning the entire sentence.
Figure 4 illustrates the search process for the sim-
ple grammar from Table 2. Each graph node repre-
sents a hypothesis translation substring covering a
sub-span of the source string. The space of trans-
lation sub-strings is exponential in each cell?s span,
and our algorithm can only sum over a small fraction
of the possible strings. Therefore the resulting prob-
abilities are only estimates. However, as demon-
strated in Section 4, this algorithm is considerably
more effective than maximum derivation (Viterbi)
decoding.
4 Evaluation
Our model evaluation was motivated by the follow-
ing questions: (1) the effect of maximising transla-
tions rather than derivations in training and decod-
ing; (2) whether a regularised model performs better
than a maximum likelihood model; (3) how the per-
formance of our model compares with a frequency
count based hierarchical system; and (4) how trans-
lation performance scales with the number of train-
ing examples.
We performed all of our experiments on the
Europarl V2 French-English parallel corpus.3 The
training data was created by filtering the full cor-
pus for all the French sentences between five and
fifteen words in length, resulting in 170K sentence
pairs. These limits were chosen as a compromise
between experiment turnaround time and leaving
a large enough corpus to obtain indicative results.
The development and test data was taken from the
2006 NAACL and 2007 ACL workshops on ma-
chine translation, also filtered for sentence length.4
Tuning of the regularisation parameter and MERT
training of the benchmark models was performed on
dev2006, while the test set was the concatenation
of devtest2006, test2006 and test2007, amounting to
315 development and 1164 test sentences.
Here we focus on evaluating our model?s basic
ability to learn a conditional distribution from sim-
ple binary features, directly comparable to those
currently employed in frequency count models. As
such, our base model includes a single binary iden-
tity feature per-rule, equivalent to the p(e|f) param-
eters defined on each rule in standard models.
As previously noted, our model must be able to
derive the reference sentence from the source for it
to be included in training. For both our discrimina-
tive and benchmark (Hiero) we extracted our gram-
mar on the 170K sentence corpus using the approach
described in Chiang (2007), resulting in 7.8 million
rules. The discriminative model was then trained on
the training partition, however only 130K of the sen-
tences were used as the model could not produce
a derivation of the reference for the remaining sen-
tences. There were many grammar rules that the dis-
criminative model did not observe in a reference
derivation, and thus could not assign their feature a
positive weight. While the benchmark model has a
3http://www.statmt.org/europarl/
4http://www.statmt.org/wmt0{6,7}
204
Decoding
Training derivation translation
All Derivations 28.71 31.23
Single Derivation 26.70 27.32
ML (?2 =?) 25.57 25.97
Table 1. A comparison on the impact of accounting for all
derivations in training and decoding (development set).
positive count for every rule (7.8M), the discrimina-
tive model only observes 1.7M rules in actual refer-
ence derivations. Figure 1 illustrates the massive am-
biguity present in the training data, with fifteen word
sentences averaging over 70M reference derivations.
Performance is evaluated using cased BLEU4
score on the test set. Although there is no direct rela-
tionship between BLEU and likelihood, it provides
a rough measure for comparing performance.
Derivational ambiguity Table 1 shows the im-
pact of accounting for derivational ambiguity in
training and decoding.5 There are two options for
training, we could use our latent variable model and
optimise the probability of all derivations of the
reference translation, or choose a single derivation
that yields the reference and optimise its probability
alone. The second option raises the difficult question
of which one, of the thousands available, we should
choose? We use the derivation which contains the
most rules. The intuition is that small rules are likely
to appear more frequently, and thus generalise bet-
ter to a test set. In decoding we can search for the
maximum probability derivation, which is the stan-
dard practice in SMT, or for the maximum probabil-
ity translation which is what we actually want from
our model, i.e. the best translation.
The results clearly indicate the value in opti-
mising translations, rather than derivations. Max-
translation decoding for the model trained on single
derivations has only a small positive effect, while for
the latent variable model the impact is much larger.6
For example, our max-derivation model trained
on the Europarl data translates carte sur la table as
on the table card. This error in the reordering of card
(which is an acceptable translation of carte) is due
to the rule ?X? ? ?carte X 1 , X 1 card? being the
highest scoring rule for carte. This is reasonable, as
5When not explicitly stated, both here and in subsequent re-
sults, the regularisation parameter was set to one, ?2 = 1.
6We also experimented with using max-translation decoding
for standard MER trained translation models, finding that it had
a small negative impact on BLEU score.
l
l
l
l l l
l
beam width
deve
lopm
ent B
LEU 
(%)
29.0
29.5
30.0
30.5
31.0
31.5
100 1k 10k
Figure 5. The effect of the beam width (log-scale) on max-
translation decoding (development set).
carte is a noun, which in the training data, is often
observed with a trailing adjective which needs to be
reordered when translating into English. In the ex-
ample there is no adjective, but the simple hierarchi-
cal grammar cannot detect this. The max-translation
model finds a good translation card on the table.
This is due to the many rules that enforce monotone
ordering around sur la, ?X? ? ?X 1 sur, X 1 in?
?X? ? ?X 1 sur la X 2 , X 1 in the X 2 ? etc.
The scores of these many monotone rules sum to be
greater than the reordering rule, thus allowing the
model to use the weight of evidence to settle on the
correct ordering.
Having established that the search for the best
translation is effective, the question remains as to
how the beam width over partial translations affects
performance. Figure 5 shows the relationship be-
tween beam width and development BLEU. Even
with a very tight beam of 100, max-translation de-
coding outperforms maximum-derivation decoding,
and performance is increasing even at a width of
10k. In subsequent experiments we use a beam of
5k which provides a good trade-off between perfor-
mance and speed.
Regularisation Table 1 shows that the per-
formance of an unregularised maximum likeli-
hood model lags well behind the regularised max-
translation model. From this we can conclude that
the maximum likelihood model is overfitting the
training set. We suggest that is a result of the degen-
erate solutions of the conditional maximum likeli-
hood estimate, as described in DeNero et al (2006).
Here we assert that our regularised maximum a pos-
205
Grammar Rules ML MAP
(?2 =?) (?2 = 1)
?X???carte, map? 1.0 0.5
?X???carte, notice? 0.0 0.5
?X???sur, on? 1.0 1.0
?X???la, the? 1.0 1.0
?X???table, table? 1.0 0.5
?X???table, chart? 0.0 0.5
?X???carte sur, notice on? 1.0 0.5
?X???carte sur, map on? 0.0 0.5
?X???sur la, on the? 1.0 1.0
?X???la table, the table? 0.0 0.5
?X???la table, the chart? 1.0 0.5
Training data:
carte sur la table? map on the table
carte sur la table? notice on the chart
Table 2. Comparison of the susceptibility to degenerate
solutions for a ML and MAP optimised model, using a sim-
ple grammar with one parameter per rule and a monotone
glue rule: ?X? ? ?X 1 X 2 , X 1X 2 ?
teriori model avoids such solutions.
This is illustrated in Table 2, which shows the
conditional probabilities for rules, obtained by lo-
cally normalising the rule feature weights for a sim-
ple grammar extracted from the ambiguous pair of
sentences presented in DeNero et al (2006). The
first column of conditional probabilities corresponds
to a maximum likelihood estimate, i.e., without reg-
ularisation. As expected, the model finds a degener-
ate solution in which overlapping rules are exploited
in order to minimise the entropy of the rule trans-
lation distributions. The second column shows the
solution found by our model when regularised by a
Gaussian prior with unit variance. Here we see that
the model finds the desired solution in which the true
ambiguity of the translation rules is preserved. The
intuition is that in order to find a degenerate solu-
tion, dispreferred rules must be given large negative
weights. However the prior penalises large weights,
and therefore the best strategy for the regularised
model is to evenly distribute probability mass.
Translation comparison Having demonstrated
that accounting for derivational ambiguity leads to
improvements for our discriminative model, we now
place the performance of our system in the context
of the standard approach to hierarchical translation.
To do this we use our own implementation of Hiero
(Chiang, 2007), with the same grammar but with the
traditional generative feature set trained in a linear
model with minimum BLEU training. The feature
set includes: a trigram language model (lm) trained
System Test (BLEU)
Discriminative max-derivation 25.78
Hiero (pd, gr, rc, wc) 26.48
Discriminative max-translation 27.72
Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc) 28.14
Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc, lm) 32.00
Table 3. Test set performance compared with a standard
Hiero system
on the English side of the unfiltered Europarl corpus;
direct and reverse translation scores estimated as rel-
ative frequencies (pd, pr); lexical translation scores
(plexd , p
lex
r ), a binary flag for the glue rule which al-
lows the model to (dis)favour monotone translation
(gr); and rule and target word counts (rc, wc).
Table 3 shows the results of our system on the
test set. Firstly we show the relative scores of our
model against Hiero without using reverse transla-
tion or lexical features.7 This allows us to directly
study the differences between the two translation
models without the added complication of the other
features. As well as both modelling the same dis-
tribution, when our model is trained with a single
parameter per-rule these systems have the same pa-
rameter space, differing only in the manner of esti-
mation.
Additionally we show the scores achieved by
MERT training the full set of features for Hiero, with
and without a language model.8 We provide these
results for reference. To compare our model directly
with these systems we would need to incorporate ad-
ditional features and a language model, work which
we have left for a later date.
The relative scores confirm that our model, with
its minimalist feature set, achieves comparable per-
formance to the standard feature set without the lan-
guage model. This is encouraging as our model was
trained to optimise likelihood rather than BLEU, yet
it is still competitive on that metric. As expected,
the language model makes a significant difference to
BLEU, however we believe that this effect is orthog-
onal to the choice of base translation model, thus we
would expect a similar gain when integrating a lan-
guage model into the discriminative system.
An informal comparison of the outputs on the de-
velopment set, presented in Table 4, suggests that the
7Although the most direct comparison for the discriminative
model would be with pd model alone, omitting the gr, rc and
wc features and MERT training produces poor translations.
8Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc, lm) represents state-
of-the-art performance on this training/testing set.
206
S: C?est pourquoi nous souhaitons que l?affaire nous soit ren-
voye?e.
R: We therefore want the matter re-referred to ourselves.
D: That is why we want the that matters we to be referred
back.
T: That is why we would like the matter to be referred back.
H: That is why we wish that the matter we be referred back.
S: Par contre, la transposition dans les E?tats membres reste
trop lente.
R: But implementation by the Member States has still been
too slow.
D: However, it is implemented in the Member States is still
too slow.
T: However, the implementation measures in Member States
remains too slow.
H: In against, transposition in the Member States remains too
slow.
S: Aussi, je conside`re qu?il reste e?norme?ment a` faire dans ce
domaine.
R: I therefore consider that there is an incredible amount still
to do in this area.
D: So I think remains a lot to be done in this field.
T: So I think there is still much to be done in this area.
H: Therefore, I think it remains a vast amount to do in this
area.
Table 4. Example output produced by the max-
derivation (D), max-translation (T) decoding algorithms
and Hiero(pd, pr, plexd , p
lex
r , gr, rc, wc) (H) models, relative
to the source (S) and reference (R).
translation optimising discriminative model more
often produces quite fluent translations, yet not in
ways that would lead to an increase in BLEU score.9
This could be considered a side-effect of optimising
likelihood rather than BLEU.
Scaling In Figure 6 we plot the scaling charac-
teristics of our models. The systems shown in the
graph use the full grammar extracted on the 170k
sentence corpus. The number of sentences upon
which the iterative training algorithm is used to esti-
mate the parameters is varied from 10k to the max-
imum 130K for which our model can reproduce the
reference translation. As expected, the more data
used to train the system, the better the performance.
However, as the performance is still increasing sig-
nificantly when all the parseable sentences are used,
it is clear that the system?s performance is suffering
from the large number (40k) of sentences that are
discarded before training.
5 Discussion and Further Work
We have shown that explicitly accounting for com-
peting derivations yields translation improvements.
9Hiero was MERT trained on this set and has a 2% higher
BLEU score compared to the discriminative model.
l
l
l
l
l
l
training sentences
deve
lopm
ent 
BLE
U (%
)
26
27
28
29
30
31
10k 25k 50k 75k 100k 130k
Figure 6. Learning curve showing that the model contin-
ues to improve as we increase the number of training sen-
tences (development set)
Our model avoids the estimation biases associated
with heuristic frequency count approaches and uses
standard regularisation techniques to avoid degener-
ate maximum likelihood solutions.
Having demonstrated the efficacy of our model
with very simple features, the logical next step is
to investigate more expressive features. Promising
features might include those over source side re-
ordering rules (Wang et al, 2007) or source con-
text features (Carpuat and Wu, 2007). Rule fre-
quency features extracted from large training cor-
pora would help the model to overcome the issue of
unreachable reference sentences. Such approaches
have been shown to be effective in log-linear word-
alignment models where only a small supervised
corpus is available (Blunsom and Cohn, 2006).
Finally, while in this paper we have focussed on
the science of discriminative machine translation,
we believe that with suitable engineering this model
will advance the state-of-the-art. To do so would
require integrating a language model feature into
the max-translation decoding algorithm. The use of
richer, more linguistic grammars (e.g., Galley et al
(2004)) may also improve the system.
Acknowledgements
The authors acknowledge the support of the EPSRC
(Blunsom & Osborne, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
207
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguis-
tics (COLING/ACL-2006), pages 65?72, Sydney, Aus-
tralia, July.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of the 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2007), pages 61?72, Prague, Czech Republic.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the 43rd
Annual Meeting of the ACL (ACL-2005), pages 263?
270, Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proc. of the
42nd Annual Meeting of the ACL (ACL-2004), pages
103?110, Barcelona, Spain.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics, 33(4).
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on Statistical Machine Translation, pages
31?38, New York City, June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc. of
the 4th International Conference on Human Language
Technology Research and 5th Annual Meeting of the
NAACL (HLT-NAACL 2004), Boston, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of the 44th Annual Meeting of the ACL and 21st In-
ternational Conference on Computational Linguistics
(COLING/ACL-2006), pages 961?968, Sydney, Aus-
tralia, July.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proc. of the 7th International
Conference on Human Language Technology Research
and 8th Annual Meeting of the NAACL (HLT-NAACL
2007), pages 57?64, Rochester, USA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc. of
the 3rd International Conference on Human Language
Technology Research and 4th Annual Meeting of the
NAACL (HLT-NAACL 2003), pages 81?88, Edmonton,
Canada, May.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465?488.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of the 44th An-
nual Meeting of the ACL and 21st International Con-
ference on Computational Linguistics (COLING/ACL-
2006), pages 761?768, Sydney, Australia, July.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
the 6th Conference on Natural Language Learning
(CoNLL-2002), pages 49?55, Taipei, Taiwan, August.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st An-
nual Meeting of the ACL (ACL-2003), pages 160?167,
Sapporo, Japan.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Discrim-
inative log-linear grammars with latent variables. In
Advances in Neural Information Processing Systems
20 (NIPS), Vancouver, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of the
3rd International Conference on Human Language
Technology Research and 4th Annual Meeting of the
NAACL (HLT-NAACL 2003), pages 134?141, Edmon-
ton, Canada.
Christoph Tillmann and Tong Zhang. 2007. A block bi-
gram prediction model for statistical machine transla-
tion. ACM Transactions Speech Language Processing,
4(3):6.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proc. of the 2007 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2007), pages 737?745, Prague, Czech Re-
public.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 764?773, Prague,
Czech Republic.
Benjamin Wellington, Joseph Turian, Chris Pike, and
I. Dan Melamed. 2006. Scalable purely-
discriminative training for word and tree transducers.
In Proc. of the 7th Biennial Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Boston, USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
208
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782?790,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Gibbs Sampler for Phrasal Synchronous Grammar Induction
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Trevor Cohn?
tcohn@inf.ed.ac.uk
Miles Osborne?
miles@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
We present a phrasal synchronous gram-
mar model of translational equivalence.
Unlike previous approaches, we do not
resort to heuristics or constraints from
a word-alignment model, but instead
directly induce a synchronous grammar
from parallel sentence-aligned corpora.
We use a hierarchical Bayesian prior
to bias towards compact grammars with
small translation units. Inference is per-
formed using a novel Gibbs sampler
over synchronous derivations. This sam-
pler side-steps the intractability issues of
previous models which required inference
over derivation forests. Instead each sam-
pling iteration is highly efficient, allowing
the model to be applied to larger transla-
tion corpora than previous approaches.
1 Introduction
The field of machine translation has seen many
advances in recent years, most notably the shift
from word-based (Brown et al, 1993) to phrase-
based models which use token n-grams as trans-
lation units (Koehn et al, 2003). Although very
few researchers use word-based models for trans-
lation per se, such models are still widely used in
the training of phrase-based models. These word-
based models are used to find the latent word-
alignments between bilingual sentence pairs, from
which a weighted string transducer can be induced
(either finite state (Koehn et al, 2003) or syn-
chronous context free grammar (Chiang, 2007)).
Although wide-spread, the disconnect between the
translation model and the alignment model is arti-
ficial and clearly undesirable. Word-based mod-
els are incapable of learning translational equiv-
alences between non-compositional phrasal units,
while the algorithms used for inducing weighted
transducers from word-alignments are based on
heuristics with little theoretical justification. A
model which can fulfil both roles would address
both the practical and theoretical short-comings of
the machine translation pipeline.
The machine translation literature is littered
with various attempts to learn a phrase-based
string transducer directly from aligned sentence
pairs, doing away with the separate word align-
ment step (Marcu and Wong, 2002; Cherry and
Lin, 2007; Zhang et al, 2008b; Blunsom et al,
2008). Unfortunately none of these approaches
resulted in an unqualified success, due largely
to intractable estimation. Large training sets with
hundreds of thousands of sentence pairs are com-
mon in machine translation, leading to a parameter
space of billions or even trillions of possible bilin-
gual phrase-pairs. Moreover, the inference proce-
dure for each sentence pair is non-trivial, prov-
ing NP-complete for learning phrase based models
(DeNero and Klein, 2008) or a high order poly-
nomial (O(|f |3|e|3))1 for a sub-class of weighted
synchronous context free grammars (Wu, 1997).
Consequently, for such models both the param-
eterisation and approximate inference techniques
are fundamental to their success.
In this paper we present a novel SCFG transla-
tion model using a non-parametric Bayesian for-
mulation. The model includes priors to impose a
bias towards small grammars with few rules, each
of which is as simple as possible (e.g., terminal
productions consisting of short phrase pairs). This
explicitly avoids the degenerate solutions of max-
imum likelihood estimation (DeNero et al, 2006),
without resort to the heuristic estimator of Koehn
et al (2003). We develop a novel Gibbs sampler
to perform inference over the latent synchronous
derivation trees for our training instances. The
sampler reasons over the infinite space of possi-
ble translation units without recourse to arbitrary
restrictions (e.g., constraints drawn from a word-
alignment (Cherry and Lin, 2007; Zhang et al,
2008b) or a grammar fixed a priori (Blunsom et al,
1f and e are the input and output sentences respectively.
782
2008)). The sampler performs local edit operations
to nodes in the synchronous trees, each of which
is very fast, leading to a highly efficient inference
technique. This allows us to train the model on
large corpora without resort to punitive length lim-
its, unlike previous approaches which were only
applied to small data sets with short sentences.
This paper is structured as follows: In Sec-
tion 3 we argue for the use of efficient sam-
pling techniques over SCFGs as an effective solu-
tion to the modelling and scaling problems of
previous approaches. We describe our Bayesian
SCFG model in Section 4 and a Gibbs sampler
to explore its posterior. We apply this sampler
to build phrase-based and hierarchical translation
models and evaluate their performance on small
and large corpora.
2 Synchronous context free grammar
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) generalizes context-
free grammars to generate strings concurrently in
two (or more) languages. A string pair is gener-
ated by applying a series of paired rewrite rules
of the form, X ? ?e, f ,a?, where X is a non-
terminal, e and f are strings of terminals and non-
terminals and a specifies a one-to-one alignment
between non-terminals in e and f . In the context of
SMT, by assigning the source and target languages
to the respective sides of a probabilistic SCFG it
is possible to describe translation as the process
of parsing the source sentence, which induces a
parallel tree structure and translation in the tar-
get language (Chiang, 2007). Figure 1 shows an
example derivation for Japanese to English trans-
lation using an SCFG. For efficiency reasons we
only consider binary or ternary branching rules
and don?t allow rules to mix terminals and non-
terminals. This allows our sampler to more effi-
ciently explore the space of grammars (Section
4.2), however more expressive grammars would be
a straightforward extension of our model.
3 Related work
Most machine translation systems adopt the
approach of Koehn et al (2003) for ?training?
a phrase-based translation model.2 This method
starts with a word-alignment, usually the latent
state of an unsupervised word-based aligner such
2We include grammar based transducers, such as Chiang
(2007) and Marcu et al (2006), in our definition of phrase-
based models.
Grammar fragment:
X ? ?X
1
X
2
X
3
, X
1
X
3
X
2
?
X ? ?John-ga, John?
X ? ?ringo-o, an apple?
X ? ?tabeta, ate?
Sample derivation:
?S
1
,S
1
? ? ?X
2
, X
2
?
? ?X
3
X
4
X
5
, X
3
X
5
X
4
?
? ?John-ga X
4
X
5
, John X
5
X
4
?
? ?John-ga ringo-o X
5
, John X
5
an apple?
? ?John-ga ringo-o tabeta, John ate an apple?
Figure 1: A fragment of an SCFG with a ternary
non-terminal expansion and three terminal rules.
as GIZA++. Various heuristics are used to com-
bine source-to-target and target-to-source align-
ments, after which a further heuristic is used to
read off phrase pairs which are ?consistent? with
the alignment. Although efficient, the sheer num-
ber of somewhat arbitrary heuristics makes this
approach overly complicated.
A number of authors have proposed alterna-
tive techniques for directly inducing phrase-based
translation models from sentence aligned data.
Marcu and Wong (2002) proposed a phrase-based
alignment model which suffered from a massive
parameter space and intractable inference using
expectation maximisation. Taking a different tack,
DeNero et al (2008) presented an interesting new
model with inference courtesy of a Gibbs sampler,
which was better able to explore the full space of
phrase translations. However, the efficacy of this
model is unclear due to the small-scale experi-
ments and the short sampling runs. In this work we
also propose a Gibbs sampler but apply it to the
polynomial space of derivation trees, rather than
the exponential space of the DeNero et al (2008)
model. The restrictions imposed by our tree struc-
ture make sampling considerably more efficient
for long sentences.
Following the broad shift in the field from finite
state transducers to grammar transducers (Chiang,
2007), recent approaches to phrase-based align-
ment have used synchronous grammar formalisms
permitting polynomial time inference (Wu, 1997;
783
Cherry and Lin, 2007; Zhang et al, 2008b; Blun-
som et al, 2008). However this asymptotic time
complexity is of high enough order (O(|f |3|e|3))
that inference is impractical for real translation
data. Proposed solutions to this problem include
imposing sentence length limits, using small train-
ing corpora and constraining the search space
using a word-alignment model or parse tree. None
of these limitations are particularly desirable as
they bias inference. As a result phrase-based align-
ment models are not yet practical for the wider
machine translation community.
4 Model
Our aim is to induce a grammar from a train-
ing set of sentence pairs. We use Bayes? rule
to reason under the posterior over grammars,
P (g|x) ? P (x|g)P (g), where g is a weighted
SCFG grammar and x is our training corpus. The
likelihood term, P (x|g), is the probability of the
training sentence pairs under the grammar, while
the prior term, P (g), describes our initial expec-
tations about what consitutes a plausible gram-
mar. Specifically we incorporate priors encoding
our preference for a briefer and more succinct
grammar, namely that: (a) the grammar should be
small, with few rules rewriting each non-terminal;
and (b) terminal rules which specify phrasal trans-
lation correspondence should be small, with few
symbols on their right hand side.
Further, Bayesian non-parametrics allow the
capacity of the model to grow with the data.
Thereby we avoid imposing hard limits on the
grammar (and the thorny problem of model selec-
tion), but instead allow the model to find a gram-
mar appropriately sized for its training data.
4.1 Non-parametric form
Our Bayesian model of SCFG derivations resem-
bles that of Blunsom et al (2008). Given a gram-
mar, each sentence is generated as follows. Start-
ing with a root non-terminal (z1), rewrite each
frontier non-terminal (zi) using a rule chosen from
our grammar expanding zi. Repeat until there are
no remaining frontier non-terminals. This gives
rise to the following derivation probability:
p(d) = p(z1)
?
ri?d
p(ri|zi)
where the derivation is a sequence of rules d =
(r1, . . . , rn), and zi denotes the root node of ri.
We allow two types of rules: non-terminal and
terminal expansions. The former rewrites a non-
terminal symbol as a string of two or three non-
terminals along with an alignment, specifying
the corresponding ordering of the child trees in
the source and target language. Terminal expan-
sions rewrite a non-terminal as a pair of terminal
n-grams, representing a phrasal translation pair,
where either but not both may be empty.
Each rule in the grammar, ri, is generated from
its root symbol, zi, by first choosing a rule type
ti ? {TERM, NON-TERM} from a Bernoulli distribu-
tion, ri ? Bernoulli(?). We treat ? as a random
variable with its own prior, ? ? Beta(?R, ?R) and
integrate out the parameters, ?. This results in the
following conditional probability for ti:
p(ti|r?i, zi, ?R) =
n?iti,zi + ?
R
n?i?,zi + 2?R
where n?iri,zi is the number of times ri has been
used to rewrite zi in the set of all other rules, r?i,
and n?i?,zi =
?
r n
?i
r,zi is the total count of rewriting
zi. The Dirichlet (and thus Beta) distribution are
exchangeable, meaning that any permutation of its
events are equiprobable. This allows us to reason
about each event given previous and subsequent
events (i.e., treat each item as the ?last?.)
When ti = NON-TERM, we generate a binary
or ternary non-terminal production. The non-
terminal sequence and alignment are drawn from
(z, a) ? ?Nzi and, as before, we define a prior over
the parameters, ?Nzi ? Dirichlet(?
T ), and inte-
grate out ?Nzi . This results in the conditional prob-
ability:
p(ri|ti = NON-TERM, r?i, zi, ?N ) =
nN,?iri,zi + ?
N
nN,?i?,zi + |N |?N
where nN,?iri,zi is the count of rewriting zi with non-
terminal rule ri, n
N,?i
?,zi the total count over all non-
terminal rules and |N | is the number of unique
non-terminal rules.
For terminal productions (ti = TERM) we first
decide whether to generate a phrase in both lan-
guages or in one language only, according to a
fixed probability pnull.3 Contingent on this deci-
sion, the terminal strings are then drawn from
3To discourage null alignments, we used pnull = 10?10
for this value in the experiments we report below.
784
either ?Pzi for phrase pairs or ?
null for single lan-
guage phrases. We choose Dirichlet process (DP)
priors for these parameters:
?Pzi ? DP(?
P , PP1 )
?nullzi ? DP(?
null, Pnull1 )
where the base distributions, PP1 and P
null
1 , range
over phrase pairs or monolingual phrases in either
language, respectively.
The most important choice for our model is
the priors on the parameters of these terminal
distributions. Phrasal SCFG models are subject
to a degenerate maximum likelihood solution in
which all probability mass is placed on long, or
whole sentence, phrase translations (DeNero et al,
2006). Therefore, careful consideration must be
given when specifying the P1 distribution on ter-
minals in order to counter this behavior.
To construct a prior over string pairs, first we
define the probability of a monolingual string (s):
PX0 (s) = PPoisson(|s|; 1)?
1
V |s|X
where the PPoisson(k; 1) is the probability under a
Poisson distribution of length k given an expected
length of 1, while VX is the vocabulary size of
language X . This distribution has a strong bias
towards short strings. In particular note that gener-
ally a string of length k will be less probable than
two of length k2 , a property very useful for finding
?minimal? translation units. This contrasts with a
geometric distribution in which a string of length
k will be more probable than its segmentations.
We define Pnull1 as the string probability of the
non-null part of the rule:
Pnull1 (z ? ?e, f?) =
{ 1
2P
E
0 (e) if |f | = 0
1
2P
F
0 (f) if |e| = 0
The terminal translation phrase pair distribution
is a hierarchical Dirichlet Process in which each
phrase are independently distributed according to
DPs:4
PP1 (z ? ?e, f?) = ?
E
z (e)? ?
F
z (f)
?Ez ? DP(?
PE , PE0 )
4This prior is similar to one used by DeNero et al (2008),
who used the expected table count approximation presented
in Goldwater et al (2006). However, Goldwater et al (2006)
contains two major errors: omitting P0, and using the trun-
cated Taylor series expansion (Antoniak, 1974) which fails
for small ?P0 values common in these models. In this work
we track table counts directly.
and ?Fz is defined analogously. This prior encour-
ages frequent phrases to participate in many differ-
ent translation pairs. Moreover, as longer strings
are likely to be less frequent in the corpus this has
a tendency to discourage long translation units.
4.2 A Gibbs sampler for derivations
Markov chain Monte Carlo sampling allows us to
perform inference for the model described in 4.1
without restricting the infinite space of possible
translation rules. To do this we need a method for
sampling a derivation for a given sentence pair
from p(d|d?). One possible approach would be
to first build a packed chart representation of the
derivation forest, calculate the inside probabilities
of all cells in this chart, and then sample deriva-
tions top-down according to their inside probabil-
ities (analogous to monolingual parse tree sam-
pling described in Johnson et al (2007)). A prob-
lem with this approach is that building the deriva-
tion forest would take O(|f |3|e|3) time, which
would be impractical for long sentences.
Instead we develop a collapsed Gibbs sam-
pler (Teh et al, 2006) which draws new sam-
ples by making local changes to the derivations
used in a previous sample. After a period of burn
in, the derivations produced by the sampler will
be drawn from the posterior distribution, p(d|x).
The advantage of this algorithm is that we only
store the current derivation for each training sen-
tence pair (together these constitute the state of
the sampler), but never need to reason over deriva-
tion forests. By integrating over (collapsing) the
parameters we only store counts of rules used
in the current sampled set of derivations, thereby
avoiding explicitly representing the possibly infi-
nite space of translation pairs.
We define two operators for our Gibbs sam-
pler, each of which re-samples local derivation
structures. Figures 2 and 4 illustrate the permu-
tations these operators make to derivation trees.
The omitted tree structure in these figures denotes
the Markov blanket of the operator: the structure
which is held constant when enumerating the pos-
sible outcomes for an operator.
The Split/Join operator iterates through the
positions between each source word sampling
whether a terminal boundary should exist at
that position (Figure 2). If the source position
785
... ... ...
... ...
... ...
... ...
Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The
dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a
solid line is a null alignment.
...
......
...
...
......
...
Figure 4: Rule insert/delete sampler. A pair of
adjacent nodes in a ternary rule can be re-parented
as a binary rule, or vice-versa.
falls between two existing terminals whose tar-
get phrases are adjacent, then any new target seg-
mentation within those target phrases can be sam-
pled, including null alignments. If the two exist-
ing terminals also share the same parent, then any
possible re-ordering is also a valid outcome, as
is removing the terminal boundary to form a sin-
gle phrase pair. Otherwise, if the visited boundary
point falls within an existing terminal, then all tar-
get split and re-orderings are possible outcomes.
The probability for each of these configurations
is evaluated (see Figure 3) from which the new
configuration is sampled.
While the first operator is theoretically capa-
ble of exploring the entire derivation forest (by
flattening the tree into a single phrase and then
splitting), the series of moves required would be
highly improbable. To allow for faster mixing we
employ the Insert/Delete operator which adds and
deletes the parent non-terminal of a pair of adja-
cent nodes. This is illustrated in Figure 4. The
update equations are analogous to those used for
the Split/Join operator in Figure 3. In order for this
operator to be effective we need to allow greater
than binary branching nodes, otherwise deleting a
nodes would require sampling from a much larger
set of outcomes. Hence our adoption of a ternary
branching grammar. Although such a grammar
would be very inefficient for a dynamic program-
ming algorithm, it allows our sampler to permute
the internal structure of the trees more easily.
4.3 Hyperparameter Inference
Our model is parameterised by a vector of hyper-
parameters, ? = (?R, ?N , ?P , ?PE , ?PF , ?null),
which control the sparsity assumption over var-
ious model parameters. We could optimise each
concentration parameter on the training corpus by
hand, however this would be quite an onerous task.
Instead we perform inference over the hyperpa-
rameters following Goldwater and Griffiths (2007)
by defining a vague gamma prior on each con-
centration parameter, ?x ? Gamma(10?4, 104).
This hyper-prior is relatively benign, allowing the
model to consider a wide range of values for
the hyperparameter. We sample a new value for
each ?x using a log-normal distribution with mean
?x and variance 0.3, which is then accepted into
the distribution p(?x|d, ??) using the Metropolis-
Hastings algorithm. Unlike the Gibbs updates, this
calculation cannot be distributed over a cluster
(see Section 4.4) and thus is very costly. Therefore
for small corpora we re-sample the hyperparame-
ter after every pass through the corpus, for larger
experiments we only re-sample every 20 passes.
4.4 A Distributed approximation
While employing a collapsed Gibbs sampler
allows us to efficiently perform inference over the
786
p(JOIN) ? p(ti = TERM|zi, r?)? p(ri = (zi ? ?e, f?)|zi, r?) (1)
p(SPLIT) ? p(ti = NON-TERM|zi, r?)? p(ri = (zi ? ?zl, zr, ai?)|zi, r
?) (2)
? p(tl = TERM|ti, zi, r
?)? p(rl = (zl ? ?el, fl?)|zl, r
?)
? p(tr = TERM|ti, tl, zi, r
?)? p(rr = (zr ? ?er, fr?)|zl, r
? ? (zl ? ?el, fl?))
Figure 3: Gibbs sampling equations for the competing configurations of the Split/Join sampler, shown in
Figure 2. Eq. (1) corresponds to the top-left configuration, and (2) the remaining configurations where the
choice of el, fl, er, fr and ai specifies the string segmentation and the alignment (monotone or reordered).
massive space of possible grammars, it induces
dependencies between all the sentences in the
training corpus. These dependencies make it diffi-
cult to scale our approach to larger corpora by dis-
tributing it across a number of processors. Recent
work (Newman et al, 2007; Asuncion et al, 2008)
suggests that good practical parallel performance
can be achieved by having multiple processors
independently sample disjoint subsets of the cor-
pus. Each process maintains a set of rule counts for
the entire corpus and communicates the changes
it has made to its section of the corpus only
after sampling every sentence in that section. In
this way each process is sampling according to
a slightly ?out-of-date? distribution. However, as
we confirm in Section 5 the performance of this
approximation closely follows the exact collapsed
Gibbs sampler.
4.5 Extracting a translation model
Although we could use our model directly as a
decoder to perform translation, its simple hier-
archical reordering parameterisation is too weak
to be effective in this mode. Instead we use our
sampler to sample a distribution over translation
models for state-of-the-art phrase based (Moses)
and hierarchical (Hiero) decoders (Koehn et al,
2007; Chiang, 2007). Each sample from our model
defines a hierarchical alignment on which we can
apply the standard extraction heuristics of these
models. By extracting from a sequence of samples
we can directly infer a distribution over phrase
tables or Hiero grammars.
5 Evaluation
Our evaluation aims to determine whether the
phrase/SCFG rule distributions created by sam-
pling from the model described in Section 4
impact upon the performance of state-of-the-
art translation systems. We conduct experiments
translating both Chinese (high reordering) and
Arabic (low reordering) into English. We use the
GIZA++ implementation of IBM Model 4 (Brown
et al, 1993; Och and Ney, 2003) coupled with the
phrase extraction heuristics of Koehn et al (2003)
and the SCFG rule extraction heuristics of Chiang
(2007) as our benchmark. All the SCFG models
employ a single X non-terminal, we leave experi-
ments with multiple non-terminals to future work.
Our hypothesis is that our grammar based
induction of translation units should benefit lan-
guage pairs with significant reordering more than
those with less. While for mostly monotone trans-
lation pairs, such as Arabic-English, the bench-
mark GIZA++-based system is well suited due to
its strong monotone bias (the sequential Markov
model and diagonal growing heuristic).
We conduct experiments on both small and
large corpora to allow a range of alignment quali-
ties and also to verify the effectiveness of our dis-
tributed approximation of the Bayesian inference.
The samplers are initialised with trees created
from GIZA++ Model 4 alignments, altered such
that they are consistent with our ternary grammar.
This is achieved by using the factorisation algo-
rithm of Zhang et al (2008a) to first create ini-
tial trees. Where these factored trees contain nodes
with mixed terminals and non-terminals, or more
than three non-terminals, we discard alignment
points until the node factorises correctly. As the
alignments contain many such non-factorisable
nodes, these trees are of poor quality. However,
all samplers used in these experiments are first
?burnt-in? for 1000 full passes through the data.
This allows the sampler to diverge from its ini-
tialisation condition, and thus gives us confidence
that subsequent samples will be drawn from the
posterior. An expectation over phrase tables and
Hiero grammars is built from every 50th sample
after the burn-in, up until the 1500th sample.
We evaluate the translation models using IBM
BLEU (Papineni et al, 2001). Table 1 lists the
statistics of the corpora used in these experiments.
787
IWSLT NIST
English?Chinese English?Chinese English?Arabic
Sentences 40k 300k 290k
Segs./Words 380k 340k 11.0M 8.6M 9.3M 8.5M
Av. Sent. Len. 9 8 36 28 32 29
Longest Sent. 75 64 80 80 80 80
Table 1: Corpora statistics.
System Test 05
Moses (Heuristic) 47.3
Moses (Bayes SCFG) 49.6
Hiero (Heuristic) 48.3
Hiero (Bayes SCFG) 51.8
Table 2: IWSLT Chinese to English translation.
5.1 Small corpus
Firstly we evaluate models trained on a small
Chinese-English corpus using a Gibbs sampler on
a single CPU. This corpus consists of transcribed
utterances made available for the IWSLT work-
shop (Eck and Hori, 2005). The sparse counts and
high reordering for this corpus means the GIZA++
model produces very poor alignments.
Table 2 shows the results for the benchmark
Moses and Hiero systems on this corpus using
both the heuristic phrase estimation, and our pro-
posed Bayesian SCFG model. We can see that
our model has a slight advantage. When we look
at the grammars extracted by the two models we
note that the SCFG model creates considerably
more translation rules. Normally this would sug-
gest the alignments of the SCFG model are a lot
sparser (more unaligned tokens) than those of the
heuristic, however this is not the case. The pro-
jected SCFG derivations actually produce more
alignment points. However these alignments are
much more locally consistent, containing fewer
spurious off-diagonal alignments, than the heuris-
tic (see Figure 5), and thus produce far more valid
phrases/rules.
5.2 Larger corpora
We now test our model?s performance on a larger
corpus, representing a realistic SMT experiment
with millions of words and long sentences. The
Chinese-English training data consists of the FBIS
corpus (LDC2003E14) and the first 100k sen-
tences from the Sinorama corpus (LDC2005E47).
The Arabic-English training data consists of
the eTIRR corpus (LDC2004E72), the Arabic
l
l
l
l
l
l
l
l
l l
l l
Number of Sampling Passes
Negative 
Log?Post
erior
l l
l
l
l
l
l
l l
l l
476
478
480
482
484
486
488
490
20 40 60 80 100 120 140 160 180 200 220 240
single (exact)distributed
Figure 6: The posterior for the single CPU sampler
and distributed approximation are roughly equiva-
lent over a sampling run.
news corpus (LDC2004T17), the Ummah cor-
pus (LDC2004T18), and the sentences with confi-
dence c > 0.995 in the ISI automatically extracted
web parallel corpus (LDC2006T02). The Chinese
text was segmented with a CRF-based Chinese
segmenter optimized for MT (Chang et al, 2008).
The Arabic text was preprocessed according to the
D2 scheme of Habash and Sadat (2006), which
was identified as optimal for corpora this size. The
parameters of the NIST systems were tuned using
Och?s algorithm to maximize BLEU on the MT02
test set (Och, 2003).
To evaluate whether the approximate distributed
inference algorithm described in Section 4.4 is
effective, we compare the posterior probability of
the training corpus when using a single machine,
and when the inference is distributed on an eight
core machine. Figure 6 plots the mean posterior
and standard error for five independent runs for
each scenario. Both sets of runs performed hyper-
parameter inference every twenty passes through
the data. It is clear from the training curves that the
distributed approximation tracks the corpus prob-
ability of the correct sampler sufficiently closely.
This concurs with the findings of Newman et al
788
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(a) Giza++
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(b) Gibbs
Figure 5: Alignment example. The synchronous tree structure is shown for (b) using brackets to indicate
constituent spans; these are omitted for single token constituents. The right alignment is roughly correct,
except that ?of? and ?an? should be left unaligned (? ?to be? is missing from the English translation).
System MT03 MT04 MT05
Moses (Heuristic) 26.2 30.0 25.3
Moses (Bayes SCFG) 26.4 30.2 25.8
Hiero (Heuristic) 26.4 30.8 25.4
Hiero (Bayes SCFG) 26.7 30.9 26.0
Table 3: NIST Chinese to English translation.
System MT03 MT04 MT05
Moses (Heuristic) 48.5 43.9 49.2
Moses (Bayes SCFG) 48.5 43.5 48.7
Hiero (Heuristic) 48.1 43.5 48.4
Hiero (Bayes SCFG) 48.4 43.4 47.7
Table 4: NIST Arabic to English translation.
(2007) who also observed very little empirical dif-
ference between the sampler and its distributed
approximation.
Tables 3 and 4 show the result on the two NIST
corpora when running the distributed sampler on
a single 8-core machine.5 These scores tally with
our initial hypothesis: that the hierarchical struc-
ture of our model suits languages that exhibit less
monotone reordering.
Figure 5 shows the projected alignment of a
headline from the thousandth sample on the NIST
Chinese data set. The effect of the grammar based
alignment can clearly be seen. Where the combi-
nation of GIZA++ and the heuristics creates out-
lier alignments that impede rule extraction, the
SCFG imposes a more rigid hierarchical struc-
ture on the alignments. We hypothesise that this
property may be particularly useful for syntac-
tic translation models which often have difficulty
5Producing the 1.5K samples for each experiment took
approximately one day.
with inconsistent word alignments not correspond-
ing to syntactic structure.
The combined evidence of the ability of our
Gibbs sampler to improve posterior likelihood
(Figure 6) and our translation experiments demon-
strate that we have developed a scalable and effec-
tive method for performing inference over phrasal
SCFG, without compromising the strong theoreti-
cal underpinnings of our model.
6 Discussion and Conclusion
We have presented a Bayesian model of SCFG
induction capable of capturing phrasal units of
translational equivalence. Our novel Gibbs sam-
pler over synchronous derivation trees can effi-
ciently draw samples from the posterior, overcom-
ing the limitations of previous models when deal-
ing with long sentences. This avoids explicitly
representing the full derivation forest required by
dynamic programming approaches, and thus we
are able to perform inference without resorting to
heuristic restrictions on the model.
Initial experiments suggest that this model per-
forms well on languages for which the monotone
bias of existing alignment and heuristic phrase
extraction approaches fail. These results open the
way for the development of more sophisticated
models employing grammars capable of capturing
a wide range of translation phenomena. In future
we envision it will be possible to use the tech-
niques developed here to directly induce gram-
mars which match state-of-the-art decoders, such
as Hiero grammars or tree substitution grammars
of the form used by Galley et al (2004).
789
Acknowledgements
The authors acknowledge the support of
the EPSRC (Blunsom & Osborne, grant
EP/D074959/1; Cohn, grant GR/T04557/01)
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001 (Dyer).
References
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
A. Asuncion, P. Smyth, M. Welling. 2008. Asynchronous
distributed learning of topic models. In NIPS. MIT Press.
P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian syn-
chronous grammar induction. In Proceedings of NIPS 21,
Vancouver, Canada.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, R. L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics,
19(2):263?311.
P.-C. Chang, D. Jurafsky, C. D. Manning. 2008. Optimizing
Chinese word segmentation for machine translation per-
formance. In Proc. of the Third Workshop on Machine
Translation, Prague, Czech Republic.
C. Cherry, D. Lin. 2007. Inversion transduction grammar for
joint phrasal translation modeling. In Proc. of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST 2007), Rochester, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. DeNero, D. Klein. 2008. The complexity of phrase align-
ment problems. In Proceedings of ACL-08: HLT, Short
Papers, 25?28, Columbus, Ohio. Association for Compu-
tational Linguistics.
J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006. Why gener-
ative phrase models underperform surface heuristics. In
Proc. of the HLT-NAACL 2006 Workshop on Statistical
Machine Translation, 31?38, New York City.
J. DeNero, A. Bouchard-Co?te?, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
M. Eck, C. Hori. 2005. Overview of the IWSLT 2005 eval-
uation campaign. In Proc. of the International Workshop
on Spoken Language Translation, Pittsburgh.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004. What?s
in a translation rule? In Proc. of the 4th International Con-
ference on Human Language Technology Research and
5th Annual Meeting of the NAACL (HLT-NAACL 2004),
Boston, USA.
S. Goldwater, T. Griffiths. 2007. A fully bayesian approach
to unsupervised part-of-speech tagging. In Proc. of the
45th Annual Meeting of the ACL (ACL-2007), 744?751,
Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
N. Habash, F. Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In Proc. of the 6th
International Conference on Human Language Technol-
ogy Research and 7th Annual Meeting of the NAACL
(HLT-NAACL 2006), New York City. Association for
Computational Linguistics.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo. In
Proc. of the 7th International Conference on Human Lan-
guage Technology Research and 8th Annual Meeting of the
NAACL (HLT-NAACL 2007), 139?146, Rochester, New
York.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the 3rd International Con-
ference on Human Language Technology Research and
4th Annual Meeting of the NAACL (HLT-NAACL 2003),
81?88, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,
C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses:
Open source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-2007),
Prague.
P. M. Lewis II, R. E. Stearns. 1968. Syntax-directed trans-
duction. J. ACM, 15(3):465?488.
D. Marcu, W. Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Proc. of the
2002 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), 133?139, Philadelphia.
Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006. SPMT:
Statistical machine translation with syntactified target lan-
guage phrases. In Proc. of the 2006 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP-
2006), 44?52, Sydney, Australia.
D. Newman, A. Asuncion, P. Smyth, M. Welling. 2007.
Distributed inference for latent dirichlet alocation. In
NIPS. MIT Press.
F. J. Och, H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?52.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of the 41st Annual Meeting
of the ACL (ACL-2003), 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a
method for automatic evaluation of machine translation,
2001.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
H. Zhang, D. Gildea, D. Chiang. 2008a. Extracting syn-
chronous grammar rules from word-level alignments in
linear time. In Proc. of the 22th International Con-
ference on Computational Linguistics (COLING-2008),
1081?1088, Manchester, UK.
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008b.
Bayesian learning of non-compositional phrases with syn-
chronous parsing. In Proc. of the 46th Annual Conference
of the Association for Computational Linguistics: Human
Language Technologies (ACL-08:HLT), 97?105, Colum-
bus, Ohio.
790
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337?340,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Note on the Implementation of
Hierarchical Dirichlet Processes
Phil Blunsom
?
pblunsom@inf.ed.ac.uk
Sharon Goldwater
?
sgwater@inf.ed.ac.uk
Trevor Cohn
?
tcohn@inf.ed.ac.uk
Mark Johnson
?
mark johnson@brown.edu
?
Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI, USA
Abstract
The implementation of collapsed Gibbs
samplers for non-parametric Bayesian
models is non-trivial, requiring con-
siderable book-keeping. Goldwater et
al. (2006a) presented an approximation
which significantly reduces the storage
and computation overhead, but we show
here that their formulation was incorrect
and, even after correction, is grossly inac-
curate. We present an alternative formula-
tion which is exact and can be computed
easily. However this approach does not
work for hierarchical models, for which
case we present an efficient data structure
which has a better space complexity than
the naive approach.
1 Introduction
Unsupervised learning of natural language is one
of the most challenging areas in NLP. Recently,
methods from nonparametric Bayesian statistics
have been gaining popularity as a way to approach
unsupervised learning for a variety of tasks,
including language modeling, word and mor-
pheme segmentation, parsing, and machine trans-
lation (Teh et al, 2006; Goldwater et al, 2006a;
Goldwater et al, 2006b; Liang et al, 2007; Finkel
et al, 2007; DeNero et al, 2008). These mod-
els are often based on the Dirichlet process (DP)
(Ferguson, 1973) or hierarchical Dirichlet process
(HDP) (Teh et al, 2006), with Gibbs sampling
as a method of inference. Exact implementation
of such sampling methods requires considerable
bookkeeping of various counts, which motivated
Goldwater et al (2006a) (henceforth, GGJ06) to
develop an approximation using expected counts.
However, we show here that their approximation
is flawed in two respects: 1) It omits an impor-
tant factor in the expectation, and 2) Even after
correction, the approximation is poor for hierar-
chical models, which are commonly used for NLP
applications. We derive an improvedO(1) formula
that gives exact values for the expected counts in
non-hierarchical models. For hierarchical models,
where our formula is not exact, we present an
efficient method for sampling from the HDP (and
related models, such as the hierarchical Pitman-
Yor process) that considerably decreases the mem-
ory footprint of such models as compared to the
naive implementation.
As we have noted, the issues described in this
paper apply to models for various kinds of NLP
tasks; for concreteness, we will focus on n-gram
language modeling for the remainder of the paper,
closely following the presentation in GGJ06.
2 The Chinese Restaurant Process
GGJ06 present two nonparametric Bayesian lan-
guage models: a DP unigram model and an HDP
bigram model. Under the DP model, words in a
corpus w = w
1
. . . w
n
are generated as follows:
G|?
0
, P
0
? DP(?
0
, P
0
)
w
i
|G ? G
where G is a distribution over an infinite set of
possible words, P
0
(the base distribution of the
DP) determines the probability that an item will
be in the support of G, and ?
0
(the concentration
parameter) determines the variance of G.
One way of understanding the predictions that
the DP model makes is through the Chinese restau-
rant process (CRP) (Aldous, 1985). In the CRP,
customers (word tokensw
i
) enter a restaurant with
an infinite number of tables and choose a seat. The
table chosen by the ith customer, z
i
, follows the
distribution:
P (z
i
= k|z
?i
) =
{
n
z
?i
k
i?1+?
0
, 0 ? k < K(z
?i
)
?
0
i?1+?
0
, k = K(z
?i
)
337
The
1
meow
4
cats
2
cats 
3
cats
5
a
b
c
d
e f
g
h
Figure 1. A seating assignment describing the state of
a unigram CRP. Letters and numbers uniquely identify
customers and tables. Note that multiple tables may
share a label.
where z
?i
= z
1
. . . z
i?1
are the table assignments
of the previous customers, n
z
?i
k
is the number of
customers at table k in z
?i
, andK(z
?i
) is the total
number of occupied tables. If we further assume
that table k is labeled with a word type `
k
drawn
from P
0
, then the assignment of tokens to tables
defines a distribution over words, with w
i
= `
z
i
.
See Figure 1 for an example seating arrangement.
Using this model, the predictive probability of
w
i
, conditioned on the previous words, can be
found by summing over possible seating assign-
ments for w
i
, and is given by
P (w
i
= w|w
?i
) =
n
w
?i
w
+ ?
0
P
0
i? 1 + ?
0
(1)
This prediction turns out to be exactly that of the
DP model after integrating out the distribution G.
Note that as long as the base distribution P
0
is
fixed, predictions do not depend on the seating
arrangement z
?i
, only on the count of word w
in the previously observed words (n
w
?i
w
). How-
ever, in many situations, we may wish to estimate
the base distribution itself, creating a hierarchical
model. Since the base distribution generates table
labels, estimates of this distribution are based on
the counts of those labels, i.e., the number of tables
associated with each word type.
An example of such a hierarchical model is the
HDP bigram model of GGJ06, in which each word
typew is associated with its own restaurant, where
customers in that restaurant correspond to words
that follow w in the corpus. All the bigram restau-
rants share a common base distribution P
1
over
unigrams, which must be inferred. Predictions in
this model are as follows:
P
2
(w
i
|h
?i
) =
n
h
?i
(w
i?1
,w
i
)
+ ?
1
P
1
(w
i
|h
?i
)
n
h
?i
(w
i?1
,?)
+ ?
1
P
1
(w
i
|h
?i
) =
t
h
?i
w
i
+ ?
0
P
0
(w
i
)
t
h
?i
?
+ ?
0
(2)
where h
?i
= (w
?i
, z
?i
), t
h
?i
w
i
is the number of
tables labelled with w
i
, and t
h
?i
?
is the total num-
ber of occupied tables. Of particular note for our
discussion is that in order to calculate these condi-
tional distributions we must know the table assign-
ments z
?i
for each of the words in w
?i
. Moreover,
in the Gibbs samplers often used for inference in
1         10         100         1000   
0.1
 
  
  
   
1
 
  
  
   
10
 
  
  
   
100
 
 
Me
an 
num
ber
 of 
lex
ica
l en
trie
s
Word frequency (nw)
 
 
Expectation
Antoniak approx.
Empirical, fixed base
Empirical, inferred base
Figure 2. Comparison of several methods of approx-
imating the number of tables occupied by words of
different frequencies. For each method, results using
? = {100, 1000, 10000, 100000} are shown (from bottom
to top). Solid lines show the expected number of tables,
computed using (3) and assuming P
1
is a fixed uni-
form distribution over a finite vocabulary (values com-
puted using the Digamma formulation (7) are the same).
Dashed lines show the values given by the Antoniak
approximation (4) (the line for ? = 100 falls below the
bottom of the graph). Stars show the mean of empirical
table counts as computed over 1000 samples from an
MCMC sampler in which P
1
is a fixed uniform distri-
bution, as in the unigram LM. Circles show the mean
of empirical table counts when P
1
is inferred, as in the
bigram LM. Standard errors in both cases are no larger
than the marker size. All plots are based on the 30114-
word vocabulary and frequencies found in sections 0-20
of the WSJ corpus.
these kinds of models, the counts are constantly
changing over multiple samples, with tables going
in and out of existence frequently. This can create
significant bookkeeping issues in implementation,
and motivated GGJ06 to present a method of com-
puting approximate table counts based on word
frequencies only.
3 Approximating Table Counts
Rather than explicitly tracking the number of
tables t
w
associated with each word w in their
bigram model, GGJ06 approximate the table
counts using the expectation E[t
w
]. Expected
counts are used in place of t
h
?i
w
i
and t
h
?i
?
in (2).
The exact expectation, due to Antoniak (1974), is
E[t
w
] = ?
1
P
1
(w)
n
w
?
i=1
1
?
1
P
1
(w) + i? 1
(3)
338
Antoniak also gives an approximation to this
expectation:
E[t
w
] ? ?
1
P
1
(w) log
n
w
+ ?
1
P
1
(w)
?
1
P
1
(w)
(4)
but provides no derivation. Due to a misinterpre-
tation of Antoniak (1974), GGJ06 use an approx-
imation that leaves out all the P
1
(w) terms from
(4).
1
Figure 2 compares the approximation to
the exact expectation when the base distribution
is fixed. The approximation is fairly good when
?P
1
(w) > 1 (the scenario assumed by Antoniak);
however, in most NLP applications, ?P
1
(w) <
1 in order to effect a sparse prior. (We return
to the case of non-fixed based distributions in a
moment.) As an extreme case of the paucity of
this approximation consider ?
1
P
1
(w) = 1 and
n
w
= 1 (i.e. only one customer has entered the
restaurant): clearly E[t
w
] should equal 1, but the
approximation gives log(2).
We now provide a derivation for (4), which will
allow us to obtain an O(1) formula for the expec-
tation in (3). First, we rewrite the summation in (3)
as a difference of fractional harmonic numbers:
2
H
(?
1
P
1
(w)+n
w
?1)
?H
(?
1
P
1
(w)?1)
(5)
Using the recurrence for harmonic numbers:
E[t
w
] ? ?
1
P
1
(w)
[
H
(?
1
P
1
(w)+n
w
)
?
1
?
1
P
1
(w) + n
w
?H
(?
1
P
1
(w)+n
w
)
+
1
?
1
P
1
(w)
]
(6)
We then use the asymptotic expansion,
H
F
? logF + ? +
1
2F
, omiting trailing terms
which are O(F
?2
) and smaller powers of F :
3
E[t
w
] ? ?
1
P
1
(w) log
n
w
+?
1
P
1
(w)
?
1
P
1
(w)
+
n
w
2(?
1
P
1
(w)+n
w
)
Omitting the trailing term leads to the
approximation in Antoniak (1974). However, we
can obtain an exact formula for the expecta-
tion by utilising the relationship between the
Digamma function and the harmonic numbers:
?(n) = H
n?1
? ?.
4
Thus we can rewrite (5) as:
5
E[t
w
] = ?
1
P
1
(w)?
[
?(?
1
P
1
(w) + n
w
)? ?(?
1
P
1
(w))
]
(7)
1
The authors of GGJ06 realized this error, and current
implementations of their models no longer use these approx-
imations, instead tracking table counts explicitly.
2
Fractional harmonic numbers between 0 and 1 are given
by H
F
=
R
1
0
1?x
F
1?x
dx. All harmonic numbers follow the
recurrence H
F
= H
F?1
+
1
F
.
3
Here, ? is the Euler-Mascheroni constant.
4
AccurateO(1) approximations of the Digamma function
are readily available.
5
(7) can be derived from (3) using: ?(x+1)??(x) =
1
x
.
Explicit table tracking:
customer(w
i
)? table(z
i
)
n
a : 1, b : 1, c : 2, d : 2, e : 3, f : 4, g : 5, h : 5
o
table(z
i
)? label(`)
n
1 : The, 2 : cats, 3 : cats, 4 : meow, 5 : cats
o
Histogram:
word type?
{
table occupancy? frequency
}
n
The : {2 : 1}, cats : {1 : 1, 2 : 2}, meow : {1 : 1}
o
Figure 3. The explicit table tracking and histogram rep-
resentations for Figure 1.
A significant caveat here is that the expected
table counts given by (3) and (7) are only valid
when the base distribution is a constant. However,
in hierarchical models such as GGJ06?s bigram
model and HDP models, the base distribution is
not constant and instead must be inferred. As can
be seen in Figure 2, table counts can diverge con-
siderably from the expectations based on fixed
P
1
when P
1
is in fact not fixed. Thus, (7) can
be viewed as an approximation in this case, but
not necessarily an accurate one. Since knowing
the table counts is only necessary for inference
in hierarchical models, but the table counts can-
not be approximated well by any of the formu-
las presented here, we must conclude that the best
inference method is still to keep track of the actual
table counts. The naive method of doing so is to
store which table each customer in the restaurant
is seated at, incrementing and decrementing these
counts as needed during the sampling process. In
the following section, we describe an alternative
method that reduces the amount of memory neces-
sary for implementing HDPs. This method is also
appropriate for hierarchical Pitman-Yor processes,
for which no closed-form approximations to the
table counts have been proposed.
4 Efficient Implementation of HDPs
As we do not have an efficient expected table
count approximation for hierarchical models we
could fall back to explicitly tracking which table
each customer that enters the restaurant sits at.
However, here we describe a more compact repre-
sentation for the state of the restaurant that doesn?t
require explicit table tracking.
6
Instead we main-
tain a histogram for each dish w
i
of the frequency
of a table having a particular number of customers.
Figure 3 depicts the histogram and explicit repre-
sentations for the CRP state in Figure 1.
Our alternative method of inference for hierar-
chical Bayesian models takes advantage of their
6
Teh et al (2006) also note that the exact table assign-
ments for customers are not required for prediction.
339
Algorithm 1 A new customer enters the restaurant
1: w: word type
2: P
w
0
: Base probability for w
3: HD
w
: Seating Histogram for w
4: procedure INCREMENT(w,P
w
0
,HD
w
)
5: p
share
?
n
w
?1
w
n
w
?1
w
+?
0
. share an existing table
6: p
new
?
?
0
?P
w
0
n
w
?1
w
+?
0
. open a new table
7: r ? random(0, p
share
+ p
new
)
8: if r < p
new
or n
w
?1
w
= 0 then
9: HD
w
[1] = HD
w
[1] + 1
10: else
. Sample from the histogram of customers at tables
11: r ? random(0, n
w
?1
w
)
12: for c ? HD
w
do . c: customer count
13: r = r ? (c? HD
w
[c])
14: if r ? 0 then
15: HD
w
[c] = HD
w
[c] + 1
16: Break
17: n
w
w
= n
w
?1
w
+ 1 . Update token count
Algorithm 2 A customer leaves the restaurant
1: w: word type
2: HD
w
: Seating histogram for w
3: procedure DECREMENT(w,P
w
0
,HD
w
)
4: r ? random(0, n
w
w
)
5: for c ? HD
w
do . c: customer count
6: r = r ? (c? HD
w
[c])
7: if r ? 0 then
8: HD
w
[c] = HD
w
[c]? 1
9: if c > 1 then
10: HD
w
[c? 1] = HD
w
[c? 1] + 1
11: Break
12: n
w
w
= n
w
w
? 1 . Update token count
exchangeability, which makes it unnecessary to
know exactly which table each customer is seated
at. The only important information is how many
tables exist with different numbers of customers,
and what their labels are. We simply maintain a
histogram for each word type w, which stores, for
each number of customersm, the number of tables
labeled with w that have m customers. Figure 3
depicts the explicit representation and histogram
for the CRP state in Figure 1.
Algorithms 1 and 2 describe the two operations
required to maintain the state of a CRP.
7
When
a customer enters the restaurant (Alogrithm 1)),
we sample whether or not to open a new table.
If not, we sample an old table proportional to the
counts of how many customers are seated there
and update the histogram. When a customer leaves
the restaurant (Algorithm 2), we decrement one
of the tables at random according to the number
of customers seated there. By exchangeability, it
doesn?t actually matter which table the customer
was ?really? sitting at.
7
A C++ template class that implements
the algorithm presented is made available at:
http://homepages.inf.ed.ac.uk/tcohn/
5 Conclusion
We?ve shown that the HDP approximation pre-
sented in GGJ06 contained errors and inappropri-
ate assumptions such that it significantly diverges
from the true expectations for the most common
scenarios encountered in NLP. As such we empha-
sise that that formulation should not be used.
Although (7) allowsE[t
w
] to be calculated exactly
for constant base distributions, for hierarchical
models this is not valid and no accurate calculation
of the expectations has been proposed. As a rem-
edy we?ve presented an algorithm that efficiently
implements the true HDP without the need for
explicitly tracking customer to table assignments,
while remaining simple to implement.
Acknowledgements
The authors would like to thank Tom Grif-
fiths for providing the code used to produce
Figure 2 and acknowledge the support of the
EPSRC (Blunsom, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
References
D. Aldous. 1985. Exchangeability and related topics. In
?
Ecole d?
?
Et?e de Probabiliti?es de Saint-Flour XIII 1983, 1?
198. Springer.
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
J. DeNero, A. Bouchard-C?ot?e, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
S. Ferguson. 1973. A Bayesian analysis of some nonpara-
metric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, C. D. Manning. 2007. The infinite
tree. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006a. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
S. Goldwater, T. Griffiths, M. Johnson. 2006b. Interpolating
between types and tokens by estimating power-law gener-
ators. In Y. Weiss, B. Sch?olkopf, J. Platt, eds., Advances
in Neural Information Processing Systems 18, 459?466.
MIT Press, Cambridge, MA.
P. Liang, S. Petrov, M. Jordan, D. Klein. 2007. The infinite
PCFG using hierarchical Dirichlet processes. In Proc. of
the 2007 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2007), 688?697, Prague,
Czech Republic.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
340
Proceedings of the 43rd Annual Meeting of the ACL, pages 10?17,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Scaling Conditional Random Fields Using Error-Correcting Codes
Trevor Cohn
Department of Computer Science
and Software Engineering
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Conditional Random Fields (CRFs) have
been applied with considerable success to
a number of natural language processing
tasks. However, these tasks have mostly
involved very small label sets. When
deployed on tasks with larger label
sets, the requirements for computational
resources mean that training becomes
intractable.
This paper describes a method for train-
ing CRFs on such tasks, using error cor-
recting output codes (ECOC). A number
of CRFs are independently trained on the
separate binary labelling tasks of distin-
guishing between a subset of the labels
and its complement. During decoding,
these models are combined to produce a
predicted label sequence which is resilient
to errors by individual models.
Error-correcting CRF training is much
less resource intensive and has a much
faster training time than a standardly
formulated CRF, while decoding
performance remains quite comparable.
This allows us to scale CRFs to previously
impossible tasks, as demonstrated by our
experiments with large label sets.
1 Introduction
Conditional random fields (CRFs) (Lafferty et
al., 2001) are probabilistic models for labelling
sequential data. CRFs are undirected graphical
models that define a conditional distribution over
label sequences given an observation sequence.
They allow the use of arbitrary, overlapping,
non-independent features as a result of their global
conditioning. This allows us to avoid making
unwarranted independence assumptions over the
observation sequence, such as those required by
typical generative models.
Efficient inference and training methods exist
when the graphical structure of the model forms
a chain, where each position in a sequence is
connected to its adjacent positions. CRFs have been
applied with impressive empirical results to the
tasks of named entity recognition (McCallum and
Li, 2003), simplified part-of-speech (POS) tagging
(Lafferty et al, 2001), noun phrase chunking (Sha
and Pereira, 2003) and extraction of tabular data
(Pinto et al, 2003), among other tasks.
CRFs are usually estimated using gradient-based
methods such as limited memory variable metric
(LMVM). However, even with these efficient
methods, training can be slow. Consequently, most
of the tasks to which CRFs have been applied are
relatively small scale, having only a small number
of training examples and small label sets. For
much larger tasks, with hundreds of labels and
millions of examples, current training methods
prove intractable. Although training can potentially
be parallelised and thus run more quickly on large
clusters of computers, this in itself is not a solution
to the problem: tasks can reasonably be expected
to increase in size and complexity much faster
than any increase in computing power. In order to
provide scalability, the factors which most affect the
resource usage and runtime of the training method
10
must be addressed directly ? ideally the dependence
on the number of labels should be reduced.
This paper presents an approach which enables
CRFs to be used on larger tasks, with a significant
reduction in the time and resources needed for
training. This reduction does not come at the cost
of performance ? the results obtained on benchmark
natural language problems compare favourably,
and sometimes exceed, the results produced from
regular CRF training. Error correcting output
codes (ECOC) (Dietterich and Bakiri, 1995) are
used to train a community of CRFs on binary
tasks, with each discriminating between a subset
of the labels and its complement. Inference is
performed by applying these ?weak? models to an
unknown example, with each component model
removing some ambiguity when predicting the label
sequence. Given a sufficient number of binary
models predicting suitably diverse label subsets, the
label sequence can be inferred while being robust
to a number of individual errors from the weak
models. As each of these weak models are binary,
individually they can be efficiently trained, even
on large problems. The number of weak learners
required to achieve good performance is shown to
be relatively small on practical tasks, such that the
overall complexity of error-correcting CRF training
is found to be much less than that of regular CRF
training methods.
We have evaluated the error-correcting CRF on
the CoNLL 2003 named entity recognition (NER)
task (Sang and Meulder, 2003), where we show
that the method yields similar generalisation perfor-
mance to standardly formulated CRFs, while requir-
ing only a fraction of the resources, and no increase
in training time. We have also shown how the error-
correcting CRF scales when applied to the larger
task of POS tagging the Penn Treebank and also
the even larger task of simultaneously noun phrase
chunking (NPC) and POS tagging using the CoNLL
2000 data-set (Sang and Buchholz, 2000).
2 Conditional random fields
CRFs are undirected graphical models used to spec-
ify the conditional probability of an assignment of
output labels given a set of input observations. We
consider only the case where the output labels of the
model are connected by edges to form a linear chain.
The joint distribution of the label sequence, y, given
the input observation sequence, x, is given by
p(y|x) = 1Z(x) exp
T+1
?
t=1
?
k
?kfk(t,yt?1,yt,x)
where T is the length of both sequences and ?k are
the parameters of the model. The functions fk are
feature functions which map properties of the obser-
vation and the labelling into a scalar value. Z(x)
is the partition function which ensures that p is a
probability distribution.
A number of algorithms can be used to find the
optimal parameter values by maximising the log-
likelihood of the training data. Assuming that the
training sequences are drawn IID from the popula-
tion, the conditional log likelihood L is given by
L =
?
i
log p(y(i)|x(i))
=
?
i
?
?
?
T (i)+1
?
t=1
?
k
?kfk(t,y(i)t?1,y
(i)
t ,x(i))
? log Z(x(i))
}
where x(i) and y(i) are the ith observation and label
sequence. Note that a prior is often included in the
L formulation; it has been excluded here for clar-
ity of exposition. CRF estimation methods include
generalised iterative scaling (GIS), improved itera-
tive scaling (IIS) and a variety of gradient based
methods. In recent empirical studies on maximum
entropy models and CRFs, limited memory variable
metric (LMVM) has proven to be the most efficient
method (Malouf, 2002; Wallach, 2002); accord-
ingly, we have used LMVM for CRF estimation.
Every iteration of LMVM training requires the
computation of the log-likelihood and its deriva-
tive with respect to each parameter. The partition
function Z(x) can be calculated efficiently using
dynamic programming with the forward algorithm.
Z(x) is given by?y ?T (y) where ? are the forward
values, defined recursively as
?t+1(y) =
?
y?
?t(y?) exp
?
k
?kfk(t + 1, y?, y,x)
11
The derivative of the log-likelihood is given by
?L
??k
=
?
i
?
?
?
T (i)+1
?
t=1
fk(t,y(i)t?1,y
(i)
t ,x(i))
?
?
y
p(y|x(i))
T (i)+1
?
t=1
fk(t,yt?1,yt,x(i))
?
?
?
The first term is the empirical count of feature k,
and the second is the expected count of the feature
under the model. When the derivative equals zero ?
at convergence ? these two terms are equal. Evalu-
ating the first term of the derivative is quite simple.
However, the sum over all possible labellings in the
second term poses more difficulties. This term can
be factorised, yielding
?
t
?
y?,y
p(Yt?1 = y?, Yt = y|x(i))fk(t, y?, y,x(i))
This term uses the marginal distribution over pairs of
labels, which can be efficiently computed from the
forward and backward values as
?t?1(y?) exp
?
k ?kfk(t, y?, y,x(i))?t(y)
Z(x(i))
The backward probabilities ? are defined by the
recursive relation
?t(y) =
?
y?
?t+1(y?) exp
?
k
?kfk(t + 1, y, y?,x)
Typically CRF training using LMVM requires
many hundreds or thousands of iterations, each of
which involves calculating of the log-likelihood
and its derivative. The time complexity of a single
iteration is O(L2NTF ) where L is the number
of labels, N is the number of sequences, T is
the average length of the sequences, and F is
the average number of activated features of each
labelled clique. It is not currently possible to state
precise bounds on the number of iterations required
for certain problems; however, problems with a
large number of sequences often require many more
iterations to converge than problems with fewer
sequences. Note that efficient CRF implementations
cache the feature values for every possible clique
labelling of the training data, which leads to a
memory requirement with the same complexity of
O(L2NTF ) ? quite demanding even for current
computer hardware.
3 Error Correcting Output Codes
Since the time and space complexity of CRF
estimation is dominated by the square of the number
of labels, it follows that reducing the number
of labels will significantly reduce the complexity.
Error-correcting coding is an approach which recasts
multiple label problems into a set of binary label
problems, each of which is of lesser complexity than
the full multiclass problem. Interestingly, training a
set of binary CRF classifiers is overall much more
efficient than training a full multi-label model. This
is because error-correcting CRF training reduces
the L2 complexity term to a constant. Decoding
proceeds by predicting these binary labels and then
recovering the encoded actual label.
Error-correcting output codes have been used for
text classification, as in Berger (1999), on which the
following is based. Begin by assigning to each of the
m labels a unique n-bit string Ci, which we will call
the code for this label. Now train n binary classi-
fiers, one for each column of the coding matrix (con-
structed by taking the labels? codes as rows). The j th
classifier, ?j , takes as positive instances those with
label i where Cij = 1. In this way, each classifier
learns a different concept, discriminating between
different subsets of the labels.
We denote the set of binary classifiers as
? = {?1, ?2, . . . , ?n}, which can be used for
prediction as follows. Classify a novel instance x
with each of the binary classifiers, yielding a n-bit
vector ?(x) = {?1(x), ?2(x), . . . , ?n(x)}. Now
compare this vector to the codes for each label. The
vector may not exactly match any of the labels due
to errors in the individual classifiers, and thus we
chose the actual label which minimises the distance
argmini?(?(x), Ci). Typically the Hamming
distance is used, which simply measures the number
of differing bit positions. In this manner, prediction
is resilient to a number of prediction errors by the
binary classifiers, provided the codes for the labels
are sufficiently diverse.
3.1 Error-correcting CRF training
Error-correcting codes can also be applied to
sequence labellers, such as CRFs, which are capable
of multiclass labelling. ECOCs can be used with
CRFs in a similar manner to that given above for
12
classifiers. A series of CRFs are trained, each
on a relabelled variant of the training data. The
relabelling for each binary CRF maps the labels
into binary space using the relevant column of the
coding matrix, such that label i is taken as a positive
for the jth model example if Cij = 1.
Training with a binary label set reduces the time
and space complexity for each training iteration to
O(NTF ); the L2 term is now a constant. Pro-
vided the code is relatively short (i.e. there are
few binary models, or weak learners), this translates
into considerable time and space savings. Coding
theory doesn?t offer any insights into the optimal
code length (i.e. the number of weak learners).
When using a very short code, the error-correcting
CRF will not adequately model the decision bound-
aries between all classes. However, using a long
code will lead to a higher degree of dependency
between pairs of classifiers, where both model simi-
lar concepts. The generalisation performance should
improve quickly as the number of weak learners
(code length) increases, but these gains will diminish
as the inter-classifier dependence increases.
3.2 Error-correcting CRF decoding
While training of error-correcting CRFs is simply
a logical extension of the ECOC classifier method
to sequence labellers, decoding is a different mat-
ter. We have applied three decoding different strate-
gies. The Standalone method requires each binary
CRF to find the Viterbi path for a given sequence,
yielding a string of 0s and 1s for each model. For
each position t in the sequence, the tth bit from
each model is taken, and the resultant bit string
compared to each of the label codes. The label
with the minimum Hamming distance is then cho-
sen as the predicted label for that site. This method
allows for error correction to occur at each site, how-
ever it discards information about the uncertainty of
each weak learner, instead only considering the most
probable paths.
The Marginals method of decoding uses the
marginal probability distribution at each position
in the sequence instead of the Viterbi paths. This
distribution is easily computed using the forward
backward algorithm. The decoding proceeds as
before, however instead of a bit string we have a
vector of probabilities. This vector is compared
to each of the label codes using the L1 distance,
and the closest label is chosen. While this method
incorporates the uncertainty of the binary models, it
does so at the expense of the path information in the
sequence.
Neither of these decoding methods allow the
models to interact, although each individual weak
learner may benefit from the predictions of the
other weak learners. The Product decoding method
addresses this problem. It treats each weak model
as an independent predictor of the label sequence,
such that the probability of the label sequence given
the observations can be re-expressed as the product
of the probabilities assigned by each weak model.
A given labelling y is projected into a bit string for
each weak learner, such that the ith entry in the
string is Ckj for the jth weak learner, where k is
the index of label yi. The weak learners can then
estimate the probability of the bit string; these are
then combined into a global product to give the
probability of the label sequence
p(y|x) = 1Z ?(x)
?
j
pj(bj(y)|x)
where pj(q|x) is the predicted probability of q given
x by the jth weak learner, bj(y) is the bit string
representing y for the jth weak learner and Z ?(x)
is the partition function. The log probability is
?
j
{Fj(bj(y), x) ? ?j ? log Zj(x)} ? log Z ?(x)
where Fj(y, x) = ?T+1t=1 fj(t,yt?1,yt,x). This log
probability can then be maximised using the Viterbi
algorithm as before, noting that the two log terms are
constant with respect to y and thus need not be eval-
uated. Note that this decoding is an equivalent for-
mulation to a uniformly weighted logarithmic opin-
ion pool, as described in Smith et al (2005).
Of the three decoding methods, Standalone
has the lowest complexity, requiring only a binary
Viterbi decoding for each weak learner. Marginals
is slightly more complex, requiring the forward
and backward values. Product, however, requires
Viterbi decoding with the full label set, and many
features ? the union of the features of each weak
learner ? which can be quite computationally
demanding.
13
3.3 Choice of code
The accuracy of ECOC methods are highly depen-
dent on the quality of the code. The ideal code
has diverse rows, yielding a high error-correcting
capability, and diverse columns such that the weak
learners model highly independent concepts. When
the number of labels, k, is small, an exhaustive
code with every unique column is reasonable, given
there are 2k?1 ? 1 unique columns. With larger
label sets, columns must be selected with care to
maximise the inter-row and inter-column separation.
This can be done by randomly sampling the column
space, in which case the probability of poor separa-
tion diminishes quickly as the number of columns
increases (Berger, 1999). Algebraic codes, such as
BCH codes, are an alternative coding scheme which
can provide near-optimal error-correcting capabil-
ity (MacWilliams and Sloane, 1977), however these
codes provide no guarantee of good column separa-
tion.
4 Experiments
Our experiments show that error-correcting CRFs
are highly accurate on benchmark problems with
small label sets, as well as on larger problems with
many more labels, which would be otherwise prove
intractable for traditional CRFs. Moreover, with a
good code, the time and resources required for train-
ing and decoding can be much less than that of the
standardly formulated CRF.
4.1 Named entity recognition
CRFs have been used with strong results on the
CoNLL 2003 NER task (McCallum, 2003) and thus
this task is included here as a benchmark. This data
set consists of a 14,987 training sentences (204,567
tokens) drawn from news articles, tagged for per-
son, location, organisation and miscellaneous enti-
ties. There are 8 IOB-2 style labels.
A multiclass (standardly formulated) CRF was
trained on these data using features covering word
identity, word prefix and suffix, orthographic tests
for digits, case and internal punctuation, word
length, POS tag and POS tag bigrams before and
after the current word. Only features seen at least
once in the training data were included in the model,
resulting in 450,345 binary features. The model was
Model Decoding MLE Regularised
Multiclass 88.04 89.78
Coded standalone 88.23? 88.67?
marginals 88.23? 89.19
product 88.69? 89.69
Table 1: F1 scores on NER task.
trained without regularisation and with a Gaussian
prior. An exhaustive code was created with all
127 unique columns. All of the weak learners
were trained with the same feature set, each having
around 315,000 features. The performance of the
standard and error-correcting models are shown in
Table 1. We tested for statistical significance using
the matched pairs test (Gillick and Cox, 1989) at
p < 0.001. Those results which are significantly
better than the corresponding multiclass MLE or
regularised model are flagged with a ?, and those
which are significantly worse with a ?.
These results show that error-correcting CRF
training achieves quite similar performance to the
multiclass CRF on the task (which incidentally
exceeds McCallum (2003)?s result of 89.0 using
feature induction). Product decoding was the
better of the three methods, giving the best
performance both with and without regularisation,
although this difference was only statistically
significant between the regularised standalone and
the regularised product decoding. The unregularised
error-correcting CRF significantly outperformed
the multiclass CRF with all decoding strategies,
suggesting that the method already provides some
regularisation, or corrects some inherent bias in the
model.
Using such a large number of weak learners is
costly, in this case taking roughly ten times longer
to train than the multiclass CRF. However, much
shorter codes can also achieve similar results. The
simplest code, where each weak learner predicts
only a single label (a.k.a. one-vs-all), achieved an
F score of 89.56, while only requiring 8 weak learn-
ers and less than half the training time as the multi-
class CRF. This code has no error correcting capa-
bility, suggesting that the code?s column separation
(and thus interdependence between weak learners)
is more important than its row separation.
14
An exhaustive code was used in this experiment
simply for illustrative purposes: many columns
in this code were unnecessary, yielding only a
slight gain in performance over much simpler
codes while incurring a very large increase in
training time. Therefore, by selecting a good subset
of the exhaustive code, it should be possible to
reduce the training time while preserving the strong
generalisation performance. One approach is to
incorporate skew in the label distribution in our
choice of code ? the code should minimise the
confusability of commonly occurring labels more
so than that of rare labels. Assuming that errors
made by the weak learners are independent, the
probability of a single error, q, as a function of the
code length n can be bounded by
q(n) ? 1 ?
?
l
p(l)
bhl?12 c
?
i=0
(
n
i
)
p?i(1 ? p?)n?i
where p(l) is the marginal probability of the label l,
hl is the minimum Hamming distance between l and
any other label, and p? is the maximum probability
of an error by a weak learner. The performance
achieved by selecting the code with the minimum
loss bound from a large random sample of codes
is shown in Figure 1, using standalone decoding,
where p? was estimated on the development set. For
comparison, randomly sampled codes and a greedy
oracle are shown. The two random sampled codes
show those samples where no column is repeated,
and where duplicate columns are permitted (random
with replacement). The oracle repeatedly adds to the
code the column which most improves its F1 score.
The minimum loss bound method allows the per-
formance plateau to be reached more quickly than
random sampling; i.e. shorter codes can be used,
thus allowing more efficient training and decoding.
Note also that multiclass CRF training required
830Mb of memory, while error-correcting training
required only 380Mb. Decoding of the test set
(51,362 tokens) with the error-correcting model
(exhaustive, MLE) took between 150 seconds for
standalone decoding and 173 seconds for integrated
decoding. The multiclass CRF was much faster,
taking only 31 seconds, however this time difference
could be reduced with suitable optimisations.
 83
 84
 85
 86
 87
 88
 89
 90
 10  15  20  25  30  35  40  45  50
F1
 s
co
re
code length
random
random with replacement
minimum loss bound
oracle
MLE multiclass CRF
Regularised multiclass CRF
Figure 1: NER F1 scores for standalone decoding
with random codes, a minimum loss code and a
greedy oracle.
Coding Decoding MLE Regularised
Multiclass 95.69 95.78
Coded - 200 standalone 95.63 96.03
marginals 95.68 96.03
One-vs-all product 94.90 96.57
Table 2: POS tagging accuracy.
4.2 Part-of-speech Tagging
CRFs have been applied to POS tagging, however
only with a very simple feature set and small training
sample (Lafferty et al, 2001). We used the Penn
Treebank Wall Street Journal articles, training on
sections 2?21 and testing on section 24. In this
task there are 45,110 training sentences, a total of
1,023,863 tokens and 45 labels.
The features used included word identity, prefix
and suffix, whether the word contains a number,
uppercase letter or a hyphen, and the words one
and two positions before and after the current word.
A random code of 200 columns was used for this
task. These results are shown in Table 2, along with
those of a multiclass CRF and an alternative one-vs-
all coding. As for the NER experiment, the decod-
ing performance levelled off after 100 bits, beyond
which the improvements from longer codes were
only very slight. This is a very encouraging char-
acteristic, as only a small number of weak learners
are required for good performance.
15
The random code of 200 bits required 1,300Mb
of RAM, taking a total of 293 hours to train and
3 hours to decode (54,397 tokens) on similar
machines to those used before. We do not have
figures regarding the resources used by Lafferty et
al.?s CRF for the POS tagging task and our attempts
to train a multiclass CRF for full-scale POS tagging
were thwarted due to lack of sufficient available
computing resources. Instead we trained on a
10,000 sentence subset of the training data, which
required approximately 17Gb of RAM and 208
hours to train.
Our best result on the task was achieved using
a one-vs-all code, which reduced the training
time to 25 hours, as it only required training 45
binary models. This result exceeds Lafferty et al?s
accuracy of 95.73% using a CRF but falls short of
Toutanova et al (2003)?s state-of-the-art 97.24%.
This is most probably due to our only using a
first-order Markov model and a fairly simple feature
set, where Tuotanova et al include a richer set of
features in a third order model.
4.3 Part-of-speech Tagging and Noun Phrase
Segmentation
The joint task of simultaneously POS tagging and
noun phrase chunking (NPC) was included in order
to demonstrate the scalability of error-correcting
CRFs. The data was taken from the CoNLL 2000
NPC shared task, with the model predicting both the
chunk tags and the POS tags. The training corpus
consisted of 8,936 sentences, with 47,377 tokens
and 118 labels.
A 200-bit random code was used, with the follow-
ing features: word identity within a window, pre-
fix and suffix of the current word and the presence
of a digit, hyphen or upper case letter in the cur-
rent word. This resulted in about 420,000 features
for each weak learner. A joint tagging accuracy of
90.78% was achieved using MLE training and stan-
dalone decoding. Despite the large increase in the
number of labels in comparison to the earlier tasks,
the performance also began to plateau at around 100
bits. This task required 220Mb of RAM and took a
total of 30 minutes to train each of the 200 binary
CRFs, this time on Pentium 4 machines with 1Gb
RAM. Decoding of the 47,377 test tokens took 9,748
seconds and 9,870 seconds for the standalone and
marginals methods respectively.
Sutton et al (2004) applied a variant of the CRF,
the dynamic CRF (DCRF), to the same task, mod-
elling the data with two interconnected chains where
one chain predicted NPC tags and the other POS
tags. They achieved better performance and train-
ing times than our model; however, this is not a
fair comparison, as the two approaches are orthogo-
nal. Indeed, applying the error-correcting CRF algo-
rithms to DCRF models could feasibly decrease the
complexity of the DCRF, allowing the method to be
applied to larger tasks with richer graphical struc-
tures and larger label sets.
In all three experiments, error-correcting CRFs
have achieved consistently good generalisation per-
formance. The number of weak learners required
to achieve these results was shown to be relatively
small, even for tasks with large label sets. The time
and space requirements were lower than those of a
traditional CRF for the larger tasks and, most impor-
tantly, did not increase substantially when the num-
ber of labels was increased.
5 Related work
Most recent work on improving CRF performance
has focused on feature selection. McCallum (2003)
describes a technique for greedily adding those
feature conjuncts to a CRF which significantly
improve the model?s log-likelihood. His experi-
mental results show that feature induction yields a
large increase in performance, however our results
show that standardly formulated CRFs can perform
well above their reported 73.3%, casting doubt
on the magnitude of the possible improvement.
Roark et al (2004) have also employed feature
selection to the huge task of language modelling
with a CRF, by partially training a voted perceptron
then removing all features that the are ignored
by the perceptron. The act of automatic feature
selection can be quite time consuming in itself,
while the performance and runtime gains are often
modest. Even with a reduced number of features,
tasks with a very large label space are likely to
remain intractable.
16
6 Conclusion
Standard training methods for CRFs suffer greatly
from their dependency on the number of labels,
making tasks with large label sets either difficult
or impossible. As CRFs are deployed more widely
to tasks with larger label sets this problem will
become more evident. The current ?solutions? to
these scaling problems ? namely feature selection,
and the use of large clusters ? don?t address the
heart of the problem: the dependence on the square
of number of labels.
Error-correcting CRF training allows CRFs to be
applied to larger problems and those with larger
label sets than were previously possible, without
requiring computationally demanding methods such
as feature selection. On standard tasks we have
shown that error-correcting CRFs provide compa-
rable or better performance than the standardly for-
mulated CRF, while requiring less time and space to
train. Only a small number of weak learners were
required to obtain good performance on the tasks
with large label sets, demonstrating that the method
provides efficient scalability to the CRF framework.
Error-correction codes could be applied to
other sequence labelling methods, such as the
voted perceptron (Roark et al, 2004). This may
yield an increase in performance and efficiency
of the method, as its runtime is also heavily
dependent on the number of labels. We plan to
apply error-correcting coding to dynamic CRFs,
which should result in better modelling of naturally
layered tasks, while increasing the efficiency and
scalability of the method. We also plan to develop
higher order CRFs, using error-correcting codes to
curb the increase in complexity.
7 Acknowledgements
This work was supported in part by a PORES travel-
ling scholarship from the University of Melbourne,
allowing Trevor Cohn to travel to Edinburgh.
References
Adam Berger. 1999. Error-correcting output coding for
text classification. In Proceedings of IJCAI: Workshop on
machine learning for information filtering.
Thomas G. Dietterich and Ghulum Bakiri. 1995. Solving mul-
ticlass learning problems via error-correcting output codes.
Journal of Artificial Intelligence Reseach, 2:263?286.
L. Gillick and Stephen Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Pro-
ceedings of the IEEE Conference on Acoustics, Speech and
Signal Processing, pages 532?535, Glasgow, Scotland.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings of
ICML 2001, pages 282?289.
Florence MacWilliams and Neil Sloane. 1977. The theory of
error-correcting codes. North Holland, Amsterdam.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of
CoNLL 2002, pages 49?55.
Andrew McCallum and Wei Li. 2003. Early results for named
entity recognition with conditional random fields, feature
induction and web-enhanced lexicons. In Proceedings of
CoNLL 2003, pages 188?191.
Andrew McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of UAI 2003,
pages 403?410.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, pages 235?242.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In Pro-
ceedings of ACL 2004, pages 48?55.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proceed-
ings of CoNLL 2000 and LLL 2000, pages 127?132.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL 2003,
pages 142?147, Edmonton, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of HLT-NAACL
2003, pages 213?220.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Pro-
ceedings of ACL 2005.
Charles Sutton, Khashayar Rohanimanesh, and Andrew McCal-
lum. 2004. Dynamic conditional random fields: Factorized
probabilistic models for labelling and segmenting sequence
data. In Proceedings of the ICML 2004.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of HLT-
NAACL 2003, pages 252?259.
Hanna Wallach. 2002. Efficient training of conditional random
fields. Master?s thesis, University of Edinburgh.
17
Proceedings of the 43rd Annual Meeting of the ACL, pages 18?25,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Logarithmic Opinion Pools for Conditional Random Fields
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Trevor Cohn
Department of Computer Science
and Software Engineering
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Recent work on Conditional Random
Fields (CRFs) has demonstrated the need
for regularisation to counter the tendency
of these models to overfit. The standard
approach to regularising CRFs involves a
prior distribution over the model parame-
ters, typically requiring search over a hy-
perparameter space. In this paper we ad-
dress the overfitting problem from a dif-
ferent perspective, by factoring the CRF
distribution into a weighted product of in-
dividual ?expert? CRF distributions. We
call this model a logarithmic opinion
pool (LOP) of CRFs (LOP-CRFs). We ap-
ply the LOP-CRF to two sequencing tasks.
Our results show that unregularised expert
CRFs with an unregularised CRF under
a LOP can outperform the unregularised
CRF, and attain a performance level close
to the regularised CRF. LOP-CRFs there-
fore provide a viable alternative to CRF
regularisation without the need for hyper-
parameter search.
1 Introduction
In recent years, conditional random fields (CRFs)
(Lafferty et al, 2001) have shown success on a num-
ber of natural language processing (NLP) tasks, in-
cluding shallow parsing (Sha and Pereira, 2003),
named entity recognition (McCallum and Li, 2003)
and information extraction from research papers
(Peng and McCallum, 2004). In general, this work
has demonstrated the susceptibility of CRFs to over-
fit the training data during parameter estimation. As
a consequence, it is now standard to use some form
of overfitting reduction in CRF training.
Recently, there have been a number of sophisti-
cated approaches to reducing overfitting in CRFs,
including automatic feature induction (McCallum,
2003) and a full Bayesian approach to training and
inference (Qi et al, 2005). These advanced meth-
ods tend to be difficult to implement and are of-
ten computationally expensive. Consequently, due
to its ease of implementation, the current standard
approach to reducing overfitting in CRFs is the use
of a prior distribution over the model parameters,
typically a Gaussian. The disadvantage with this
method, however, is that it requires adjusting the
value of one or more of the distribution?s hyper-
parameters. This usually involves manual or auto-
matic tuning on a development set, and can be an ex-
pensive process as the CRF must be retrained many
times for different hyperparameter values.
In this paper we address the overfitting problem
in CRFs from a different perspective. We factor the
CRF distribution into a weighted product of indi-
vidual expert CRF distributions, each focusing on
a particular subset of the distribution. We call this
model a logarithmic opinion pool (LOP) of CRFs
(LOP-CRFs), and provide a procedure for learning
the weight of each expert in the product. The LOP-
CRF framework is ?parameter-free? in the sense that
it does not involve the requirement to adjust hyper-
parameter values.
LOP-CRFs are theoretically advantageous in that
their Kullback-Leibler divergence with a given dis-
tribution can be explicitly represented as a function
of the KL-divergence with each of their expert dis-
tributions. This provides a well-founded framework
for designing new overfitting reduction schemes:
18
look to factorise a CRF distribution as a set of di-
verse experts.
We apply LOP-CRFs to two sequencing tasks in
NLP: named entity recognition and part-of-speech
tagging. Our results show that combination of un-
regularised expert CRFs with an unregularised stan-
dard CRF under a LOP can outperform the unreg-
ularised standard CRF, and attain a performance
level that rivals that of the regularised standard CRF.
LOP-CRFs therefore provide a viable alternative to
CRF regularisation without the need for hyperpa-
rameter search.
2 Conditional Random Fields
A linear chain CRF defines the conditional probabil-
ity of a state or label sequence s given an observed
sequence o via1:
p(s |o) = 1
Z(o) exp
(T+1
?
t=1
?
k
?k fk(st?1,st ,o, t)
)
(1)
where T is the length of both sequences, ?k are pa-
rameters of the model and Z(o) is the partition func-
tion that ensures (1) represents a probability distri-
bution. The functions fk are feature functions rep-
resenting the occurrence of different events in the
sequences s and o.
The parameters ?k can be estimated by maximis-
ing the conditional log-likelihood of a set of labelled
training sequences. The log-likelihood is given by:
L (? ) = ?
o,s
p?(o,s) log p(s |o;? )
= ?
o,s
p?(o,s)
[T+1
?
t=1
? ? f(s,o, t)
]
? ?
o
p?(o) logZ(o;? )
where p?(o,s) and p?(o) are empirical distributions
defined by the training set. At the maximum like-
lihood solution the model satisfies a set of feature
constraints, whereby the expected count of each fea-
ture under the model is equal to its empirical count
on the training data:
1In this paper we assume there is a one-to-one mapping be-
tween states and labels, though this need not be the case.
Ep?(o,s)[ fk]?Ep(s|o)[ fk] = 0, ?k
In general this cannot be solved for the ?k in
closed form so numerical routines must be used.
Malouf (2002) and Sha and Pereira (2003) show
that gradient-based algorithms, particularly limited
memory variable metric (LMVM), require much
less time to reach convergence, for some NLP tasks,
than the iterative scaling methods (Della Pietra et
al., 1997) previously used for log-linear optimisa-
tion problems. In all our experiments we use the
LMVM method to train the CRFs.
For CRFs with general graphical structure, calcu-
lation of Ep(s|o)[ fk] is intractable, but for the linear
chain case Lafferty et al (2001) describe an efficient
dynamic programming procedure for inference, sim-
ilar in nature to the forward-backward algorithm in
hidden Markov models.
3 Logarithmic Opinion Pools
In this paper an expert model refers a probabilistic
model that focuses on modelling a specific subset of
some probability distribution. The concept of com-
bining the distributions of a set of expert models via
a weighted product has previously been used in a
range of different application areas, including eco-
nomics and management science (Bordley, 1982),
and NLP (Osborne and Baldridge, 2004).
In this paper we restrict ourselves to sequence
models. Given a set of sequence model experts, in-
dexed by ? , with conditional distributions p?(s |o)
and a set of non-negative normalised weights w? , a
logarithmic opinion pool 2 is defined as the distri-
bution:
pLOP(s |o) = 1ZLOP(o) ?? [p?(s |o)]
w? (2)
with w? ? 0 and ?? w? = 1, and where ZLOP(o) is
the normalisation constant:
ZLOP(o) = ?
s
?? [p?(s |o)]
w? (3)
2Hinton (1999) introduced a variant of the LOP idea called
Product of Experts, in which expert distributions are multiplied
under a uniform weight distribution.
19
The weight w? encodes our confidence in the opin-
ion of expert ? .
Suppose that there is a ?true? conditional distri-
bution q(s | o) which each p?(s | o) is attempting to
model. Heskes (1998) shows that the KL divergence
between q(s | o) and the LOP, can be decomposed
into two terms:
K(q, pLOP) = E ?A (4)
= ?
?
w?K (q, p?)??
?
w?K (pLOP, p?)
This tells us that the closeness of the LOP model
to q(s | o) is governed by a trade-off between two
terms: an E term, which represents the closeness
of the individual experts to q(s | o), and an A term,
which represents the closeness of the individual
experts to the LOP, and therefore indirectly to each
other. Hence for the LOP to model q well, we desire
models p? which are individually good models of q
(having low E) and are also diverse (having large A).
3.1 LOPs for CRFs
Because CRFs are log-linear models, we can see
from equation (2) that CRF experts are particularly
well suited to combination under a LOP. Indeed, the
resulting LOP is itself a CRF, the LOP-CRF, with
potential functions given by a log-linear combina-
tion of the potential functions of the experts, with
weights w? . As a consequence of this, the nor-
malisation constant for the LOP-CRF can be calcu-
lated efficiently via the usual forward-backward al-
gorithm for CRFs. Note that there is a distinction be-
tween normalisation constant for the LOP-CRF, ZLOP
as given in equation (3), and the partition function of
the LOP-CRF, Z. The two are related as follows:
pLOP(s |o) = 1ZLOP(o) ?? [p?(s |o)]
w?
= 1
ZLOP(o) ??
[
U?(s |o)
Z?(o)
]w?
= ?? [U?(s |o)]
w?
ZLOP(o)?? [Z?(o)]w?
where U? = exp?T+1t=1 ?k ??k f?k(st?1,st ,o, t) and so
logZ(o) = logZLOP(o)+?
?
w? logZ?(o)
This relationship will be useful below, when we de-
scribe how to train the weights w? of a LOP-CRF.
In this paper we will use the term LOP-CRF
weights to refer to the weights w? in the weighted
product of the LOP-CRF distribution and the term
parameters to refer to the parameters ??k of each
expert CRF ? .
3.2 Training LOP-CRFs
In our LOP-CRF training procedure we first train
the expert CRFs unregularised on the training data.
Then, treating the experts as static pre-trained mod-
els, we train the LOP-CRF weights w? to maximise
the log-likelihood of the training data. This training
process is ?parameter-free? in that neither stage in-
volves the use of a prior distribution over expert CRF
parameters or LOP-CRF weights, and so avoids the
requirement to adjust hyperparameter values.
The likelihood of a data set under a LOP-CRF, as
a function of the LOP-CRF weights, is given by:
L(w) = ?
o,s
pLOP(s |o;w) p?(o,s)
= ?
o,s
[ 1
ZLOP(o;w) ?? p?(s |o)
w?
]p?(o,s)
After taking logs and rearranging, the log-
likelihood can be expressed as:
L (w) = ?
o,s
p?(o,s)?
?
w? log p?(s |o)
? ?
o
p?(o) logZLOP(o;w)
= ?
?
w? ?
o,s
p?(o,s) log p?(s |o)
+ ?
?
w? ?
o
p?(o) logZ?(o)
? ?
o
p?(o) logZ(o;w)
For the first two terms, the quantities that are mul-
tiplied by w? inside the (outer) sums are indepen-
dent of the weights, and can be evaluated once at the
20
beginning of training. The third term involves the
partition function for the LOP-CRF and so is a func-
tion of the weights. It can be evaluated efficiently as
usual for a standard CRF.
Taking derivatives with respect to w? and rear-
ranging, we obtain:
?L (w)
?w? = ?o,s p?(o,s) log p? (s |o)
+ ?
o
p?(o) logZ? (o)
? ?
o
p?(o)EpLOP(s|o)
[
?
t
logU? t(o,s)
]
where U? t(o,s) is the value of the potential function
for expert ? on clique t under the labelling s for ob-
servation o. In a way similar to the representation
of the expected feature count in a standard CRF, the
third term may be re-written as:
??
o
?
t
?
s?,s??
pLOP(st?1 = s?,st = s??,o) logU? t(s?,s??,o)
Hence the derivative is tractable because we can use
dynamic programming to efficiently calculate the
pairwise marginal distribution for the LOP-CRF.
Using these expressions we can efficiently train
the LOP-CRF weights to maximise the log-
likelihood of the data set.3 We make use of the
LMVM method mentioned earlier to do this. We
will refer to a LOP-CRF with weights trained using
this procedure as an unregularised LOP-CRF.
3.2.1 Regularisation
The ?parameter-free? aspect of the training pro-
cedure we introduced in the previous section relies
on the fact that we do not use regularisation when
training the LOP-CRF weights w? . However, there
is a possibility that this may lead to overfitting of
the training data. In order to investigate this, we
develop a regularised version of the training proce-
dure and compare the results obtained with each. We
3We must ensure that the weights are non-negative and nor-
malised. We achieve this by parameterising the weights as func-
tions of a set of unconstrained variables via a softmax transfor-
mation. The values of the log-likelihood and its derivatives with
respect to the unconstrained variables can be derived from the
corresponding values for the weights w? .
use a prior distribution over the LOP-CRF weights.
As the weights are non-negative and normalised we
use a Dirichlet distribution, whose density function
is given by:
p(w) = ?(?? ??)?? ?(??) ?? w
???1?
where the ?? are hyperparameters.
Under this distribution, ignoring terms that are
independent of the weights, the regularised log-
likelihood involves an additional term:
?
?
(?? ?1) logw?
We assume a single value ? across all weights. The
derivative of the regularised log-likelihood with
respect to weight w? then involves an additional
term 1w? (? ? 1). In our experiments we use thedevelopment set to optimise the value of ? . We will
refer to a LOP-CRF with weights trained using this
procedure as a regularised LOP-CRF.
4 The Tasks
In this paper we apply LOP-CRFs to two sequence
labelling tasks in NLP: named entity recognition
(NER) and part-of-speech tagging (POS tagging).
4.1 Named Entity Recognition
NER involves the identification of the location and
type of pre-defined entities within a sentence and is
often used as a sub-process in information extrac-
tion systems. With NER the CRF is presented with
a set of sentences and must label each word so as to
indicate whether the word appears outside an entity
(O), at the beginning of an entity of type X (B-X) or
within the continuation of an entity of type X (I-X).
All our results for NER are reported on the
CoNLL-2003 shared task dataset (Tjong Kim Sang
and De Meulder, 2003). For this dataset the en-
tity types are: persons (PER), locations (LOC),
organisations (ORG) and miscellaneous (MISC).
The training set consists of 14,987 sentences and
204,567 tokens, the development set consists of
3,466 sentences and 51,578 tokens and the test set
consists of 3,684 sentences and 46,666 tokens.
21
4.2 Part-of-Speech Tagging
POS tagging involves labelling each word in a sen-
tence with its part-of-speech, for example noun,
verb, adjective, etc. For our experiments we use the
CoNLL-2000 shared task dataset (Tjong Kim Sang
and Buchholz, 2000). This has 48 different POS
tags. In order to make training time manageable4,
we collapse the number of POS tags from 48 to 5
following the procedure used in (McCallum et al,
2003). In summary:
? All types of noun collapse to category N.
? All types of verb collapse to category V.
? All types of adjective collapse to category J.
? All types of adverb collapse to category R.
? All other POS tags collapse to category O.
The training set consists of 7,300 sentences and
173,542 tokens, the development set consists of
1,636 sentences and 38,185 tokens and the test set
consists of 2,012 sentences and 47,377 tokens.
4.3 Expert sets
For each task we compare the performance of the
LOP-CRF to that of the standard CRF by defining
a single, complex CRF, which we call a monolithic
CRF, and a range of expert sets.
The monolithic CRF for NER comprises a num-
ber of word and POS tag features in a window of
five words around the current word, along with a
set of orthographic features defined on the current
word. These are based on those found in (Curran and
Clark, 2003). Examples include whether the cur-
rent word is capitalised, is an initial, contains a digit,
contains punctuation, etc. The monolithic CRF for
NER has 450,345 features.
The monolithic CRF for POS tagging comprises
word and POS features similar to those in the NER
monolithic model, but over a smaller number of or-
thographic features. The monolithic model for POS
tagging has 188,448 features.
Each of our expert sets consists of a number of
CRF experts. Usually these experts are designed to
4See (Cohn et al, 2005) for a scaling method allowing the
full POS tagging task with CRFs.
focus on modelling a particular aspect or subset of
the distribution. As we saw earlier, the aim here is
to define experts that model parts of the distribution
well while retaining mutual diversity. The experts
from a particular expert set are combined under a
LOP-CRF and the weights are trained as described
previously.
We define our range of expert sets as follows:
? Simple consists of the monolithic CRF and a
single expert comprising a reduced subset of
the features in the monolithic CRF. This re-
duced CRF models the entire distribution rather
than focusing on a particular aspect or subset,
but is much less expressive than the monolithic
model. The reduced model comprises 24,818
features for NER and 47,420 features for POS
tagging.
? Positional consists of the monolithic CRF and
a partition of the features in the monolithic
CRF into three experts, each consisting only of
features that involve events either behind, at or
ahead of the current sequence position.
? Label consists of the monolithic CRF and a
partition of the features in the monolithic CRF
into five experts, one for each label. For NER
an expert corresponding to label X consists
only of features that involve labels B-X or I-
X at the current or previous positions, while for
POS tagging an expert corresponding to label
X consists only of features that involve label
X at the current or previous positions. These
experts therefore focus on trying to model the
distribution of a particular label.
? Random consists of the monolithic CRF and a
random partition of the features in the mono-
lithic CRF into four experts. This acts as a
baseline to ascertain the performance that can
be expected from an expert set that is not de-
fined via any linguistic intuition.
5 Experiments
To compare the performance of LOP-CRFs trained
using the procedure we described previously to that
of a standard CRF regularised with a Gaussian prior,
we do the following for both NER and POS tagging:
22
? Train a monolithic CRF with regularisation us-
ing a Gaussian prior. We use the development
set to optimise the value of the variance hyper-
parameter.
? Train every expert CRF in each expert set with-
out regularisation (each expert set includes the
monolithic CRF, which clearly need only be
trained once).
? For each expert set, create a LOP-CRF from
the expert CRFs and train the weights of the
LOP-CRF without regularisation. We compare
its performance to that of the unregularised and
regularised monolithic CRFs.
? To investigate whether training the LOP-CRF
weights contributes significantly to the LOP-
CRF?s performance, for each expert set we cre-
ate a LOP-CRF with uniform weights and com-
pare its performance to that of the LOP-CRF
with trained weights.
? To investigate whether unregularised training
of the LOP-CRF weights leads to overfitting,
for each expert set we train the weights of the
LOP-CRF with regularisation using a Dirich-
let prior. We optimise the hyperparameter in
the Dirichlet distribution on the development
set. We then compare the performance of the
LOP-CRF with regularised weights to that of
the LOP-CRF with unregularised weights.
6 Results
6.1 Experts
Before presenting results for the LOP-CRFs, we
briefly give performance figures for the monolithic
CRFs and expert CRFs in isolation. For illustration,
we do this for NER models only. Table 1 shows F
scores on the development set for the NER CRFs.
We see that, as expected, the expert CRFs in iso-
lation model the data relatively poorly compared to
the monolithic CRFs. Some of the label experts, for
example, attain relatively low F scores as they focus
only on modelling one particular label. Similar be-
haviour was observed for the POS tagging models.
Expert F score
Monolithic unreg. 88.33
Monolithic reg. 89.84
Reduced 79.62
Positional 1 86.96
Positional 2 73.11
Positional 3 73.08
Label LOC 41.96
Label MISC 22.03
Label ORG 29.13
Label PER 40.49
Label O 60.44
Random 1 70.34
Random 2 67.76
Random 3 67.97
Random 4 70.17
Table 1: Development set F scores for NER experts
6.2 LOP-CRFs with unregularised weights
In this section we present results for LOP-CRFs with
unregularised weights. Table 2 gives F scores for
NER LOP-CRFs while Table 3 gives accuracies for
the POS tagging LOP-CRFs. The monolithic CRF
scores are included for comparison. Both tables il-
lustrate the following points:
? In every case the LOP-CRFs outperform the
unregularised monolithic CRF
? In most cases the performance of LOP-CRFs
rivals that of the regularised monolithic CRF,
and in some cases exceeds it.
We use McNemar?s matched-pairs test (Gillick
and Cox, 1989) on point-wise labelling errors to ex-
amine the statistical significance of these results. We
test significance at the 5% level. At this threshold,
all the LOP-CRFs significantly outperform the cor-
responding unregularised monolithic CRF. In addi-
tion, those marked with ? show a significant im-
provement over the regularised monolithic CRF.
Only the value marked with ? in Table 3 significantly
under performs the regularised monolithic. All other
values a do not differ significantly from those of the
regularised monolithic CRF at the 5% level.
These results show that LOP-CRFs with unreg-
ularised weights can lead to performance improve-
ments that equal or exceed those achieved from a
conventional regularisation approach using a Gaus-
sian prior. The important difference, however, is that
the LOP-CRF approach is ?parameter-free? in the
23
Expert set Development set Test set
Monolithic unreg. 88.33 81.87
Monolithic reg. 89.84 83.98
Simple 90.26 84.22?
Positional 90.35 84.71?
Label 89.30 83.27
Random 88.84 83.06
Table 2: F scores for NER unregularised LOP-CRFs
Expert set Development set Test set
Monolithic unreg. 97.92 97.65
Monolithic reg. 98.02 97.84
Simple 98.31? 98.12?
Positional 98.03 97.81
Label 97.99 97.77
Random 97.99 97.76?
Table 3: Accuracies for POS tagging unregularised
LOP-CRFs
sense that each expert CRF in the LOP-CRF is un-
regularised and the LOP weight training is also un-
regularised. We are therefore not required to search
a hyperparameter space. As an illustration, to ob-
tain our best results for the POS tagging regularised
monolithic model, we re-trained using 15 different
values of the Gaussian prior variance. With the
LOP-CRF we trained each expert CRF and the LOP
weights only once.
As an illustration of a typical weight distribution
resulting from the training procedure, the positional
LOP-CRF for POS tagging attaches weight 0.45 to
the monolithic model and roughly equal weights to
the other three experts.
6.3 LOP-CRFs with uniform weights
By training LOP-CRF weights using the procedure
we introduce in this paper, we allow the weights to
take on non-uniform values. This corresponds to
letting the opinion of some experts take precedence
over others in the LOP-CRF?s decision making. An
alternative, simpler, approach would be to com-
bine the experts under a LOP with uniform weights,
thereby avoiding the weight training stage. We
would like to ascertain whether this approach will
significantly reduce the LOP-CRF?s performance.
As an illustration, Table 4 gives accuracies for LOP-
CRFs with uniform weights for POS tagging. A sim-
ilar pattern is observed for NER. Comparing these
values to those in Tables 2 and 3, we can see that in
Expert set Development set Test set
Simple 98.30 98.12
Positional 97.97 97.79
Label 97.85 97.73
Random 97.82 97.74
Table 4: Accuracies for POS tagging uniform LOP-
CRFs
general LOP-CRFs with uniform weights, although
still performing significantly better than the unreg-
ularised monolithic CRF, generally under perform
LOP-CRFs with trained weights. This suggests that
the choice of weights can be important, and justifies
the weight training stage.
6.4 LOP-CRFs with regularised weights
To investigate whether unregularised training of the
LOP-CRF weights leads to overfitting, we train
the LOP-CRF with regularisation using a Dirich-
let prior. The results we obtain show that in most
cases a LOP-CRF with regularised weights achieves
an almost identical performance to that with unreg-
ularised weights, and suggests there is little to be
gained by weight regularisation. This is probably
due to the fact that in our LOP-CRFs the number
of experts, and therefore weights, is generally small
and so there is little capacity for overfitting. We con-
jecture that although other choices of expert set may
comprise many more experts than in our examples,
the numbers are likely to be relatively small in com-
parison to, for example, the number of parameters in
the individual experts. We therefore suggest that any
overfitting effect is likely to be limited.
6.5 Choice of Expert Sets
We can see from Tables 2 and 3 that the performance
of a LOP-CRF varies with the choice of expert set.
For example, in our tasks the simple and positional
expert sets perform better than those for the label
and random sets. For an explanation here, we re-
fer back to our discussion of equation (5). We con-
jecture that the simple and positional expert sets
achieve good performance in the LOP-CRF because
they consist of experts that are diverse while simulta-
neously being reasonable models of the data. The la-
bel expert set exhibits greater diversity between the
experts, because each expert focuses on modelling a
particular label only, but each expert is a relatively
24
poor model of the entire distribution and the corre-
sponding LOP-CRF performs worse. Similarly, the
random experts are in general better models of the
entire distribution but tend to be less diverse because
they do not focus on any one aspect or subset of it.
Intuitively, then, we want to devise experts that pro-
vide diverse but accurate views on the data.
The expert sets we present in this paper were
motivated by linguistic intuition, but clearly many
choices exist. It remains an important open question
as to how to automatically construct expert sets for
good performance on a given task, and we intend to
pursue this avenue in future research.
7 Conclusion and future work
In this paper we have introduced the logarithmic
opinion pool of CRFs as a way to address overfit-
ting in CRF models. Our results show that a LOP-
CRF can provide a competitive alternative to con-
ventional regularisation with a prior while avoiding
the requirement to search a hyperparameter space.
We have seen that, for a variety of types of expert,
combination of expert CRFs with an unregularised
standard CRF under a LOP with optimised weights
can outperform the unregularised standard CRF and
rival the performance of a regularised standard CRF.
We have shown how these advantages a LOP-
CRF provides have a firm theoretical foundation in
terms of the decomposition of the KL-divergence
between a LOP-CRF and a target distribution, and
how this provides a framework for designing new
overfitting reduction schemes in terms of construct-
ing diverse experts.
In this work we have considered training the
weights of a LOP-CRF using pre-trained, static ex-
perts. In future we intend to investigate cooperative
training of LOP-CRF weights and the parameters of
each expert in an expert set.
Acknowledgements
We wish to thank Stephen Clark, our colleagues in
Edinburgh and the anonymous reviewers for many
useful comments.
References
R. F. Bordley. 1982. A multiplicative formula for aggregating
probability assessments. Management Science, (28):1137?
1148.
T. Cohn, A. Smith, and M. Osborne. 2005. Scaling conditional
random fields using error-correcting codes. In Proc. ACL
2005.
J. Curran and S. Clark. 2003. Language independent NER
using a maximum entropy tagger. In Proc. CoNLL-2003.
S. Della Pietra, Della Pietra V., and J. Lafferty. 1997. Induc-
ing features of random fields. In IEEE PAMI, volume 19(4),
pages 380?393.
L. Gillick and S. Cox. 1989. Some statistical issues in the
comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing, volume 1, pages 532?535.
T. Heskes. 1998. Selecting weighting factors in logarithmic
opinion pools. In Advances in Neural Information Process-
ing Systems 10.
G. E. Hinton. 1999. Product of experts. In ICANN, volume 1,
pages 1?6.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML 2001.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proc. CoNLL-2002.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In Proc. CoNLL-2003.
A. McCallum, K. Rohanimanesh, and C. Sutton. 2003. Dy-
namic conditional random fields for jointly labeling multiple
sequences. In NIPS-2003 Workshop on Syntax, Semantics
and Statistics.
A. McCallum. 2003. Efficiently inducing features of condi-
tional random fields. In Proc. UAI 2003.
M. Osborne and J. Baldridge. 2004. Ensemble-based active
learning for parse selection. In Proc. NAACL 2004.
F. Peng and A. McCallum. 2004. Accurate information extrac-
tion from research papers using conditional random fields.
In Proc. HLT-NAACL 2004.
Y. Qi, M. Szummer, and T. P. Minka. 2005. Bayesian condi-
tional random fields. In Proc. AISTATS 2005.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL 2003.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proc. CoNLL-
2000.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent named
entity recognition. In Proc. CoNLL-2003.
25
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65?72,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Word Alignment with Conditional Random Fields
Phil Blunsom and Trevor Cohn
Department of Software Engineering and Computer Science
University of Melbourne
{pcbl,tacohn}@csse.unimelb.edu.au
Abstract
In this paper we present a novel approach
for inducing word alignments from sen-
tence aligned data. We use a Condi-
tional Random Field (CRF), a discrimina-
tive model, which is estimated on a small
supervised training set. The CRF is condi-
tioned on both the source and target texts,
and thus allows for the use of arbitrary
and overlapping features over these data.
Moreover, the CRF has efficient training
and decoding processes which both find
globally optimal solutions.
We apply this alignment model to both
French-English and Romanian-English
language pairs. We show how a large
number of highly predictive features can
be easily incorporated into the CRF, and
demonstrate that even with only a few hun-
dred word-aligned training sentences, our
model improves over the current state-of-
the-art with alignment error rates of 5.29
and 25.8 for the two tasks respectively.
1 Introduction
Modern phrase based statistical machine transla-
tion (SMT) systems usually break the translation
task into two phases. The first phase induces word
alignments over a sentence-aligned bilingual cor-
pus, and the second phase uses statistics over these
predicted word alignments to decode (translate)
novel sentences. This paper deals with the first of
these tasks: word alignment.
Most current SMT systems (Och and Ney,
2004; Koehn et al, 2003) use a generative model
for word alignment such as the freely available
GIZA++ (Och and Ney, 2003), an implementa-
tion of the IBM alignment models (Brown et al,
1993). These models treat word alignment as a
hidden process, and maximise the probability of
the observed (e, f) sentence pairs1 using the ex-
pectation maximisation (EM) algorithm. After the
maximisation process is complete, the word align-
ments are set to maximum posterior predictions of
the model.
While GIZA++ gives good results when trained
on large sentence aligned corpora, its generative
models have a number of limitations. Firstly,
they impose strong independence assumptions be-
tween features, making it very difficult to incor-
porate non-independent features over the sentence
pairs. For instance, as well as detecting that a
source word is aligned to a given target word,
we would also like to encode syntactic and lexi-
cal features of the word pair, such as their parts-
of-speech, affixes, lemmas, etc. Features such as
these would allow for more effective use of sparse
data and result in a model which is more robust
in the presence of unseen words. Adding these
non-independent features to a generative model
requires that the features? inter-dependence be
modelled explicitly, which often complicates the
model (eg. Toutanova et al (2002)). Secondly, the
later IBM models, such as Model 4, have to re-
sort to heuristic search techniques to approximate
forward-backward and Viterbi inference, which
sacrifice optimality for tractability.
This paper presents an alternative discrimina-
tive method for word alignment. We use a condi-
tional random field (CRF) sequence model, which
allows for globally optimal training and decod-
ing (Lafferty et al, 2001). The inference algo-
1We adopt the standard notation of e and f to denote the
target (English) and source (foreign) sentences, respectively.
65
rithms are tractable and efficient, thereby avoid-
ing the need for heuristics. The CRF is condi-
tioned on both the source and target sentences,
and therefore supports large sets of diverse and
overlapping features. Furthermore, the model al-
lows regularisation using a prior over the parame-
ters, a very effective and simple method for limit-
ing over-fitting. We use a similar graphical struc-
ture to the directed hidden Markov model (HMM)
from GIZA++ (Och and Ney, 2003). This mod-
els one-to-many alignments, where each target
word is aligned with zero or more source words.
Many-to-many alignments are recoverable using
the standard techniques for superimposing pre-
dicted alignments in both translation directions.
The paper is structured as follows. Section
2 presents CRFs for word alignment, describing
their form and their inference techniques. The
features of our model are presented in Section 3,
and experimental results for word aligning both
French-English and Romanian-English sentences
are given in Section 4. Section 5 presents related
work, and we describe future work in Section 6.
Finally, we conclude in Section 7.
2 Conditional random fields
CRFs are undirected graphical models which de-
fine a conditional distribution over a label se-
quence given an observation sequence. We use
a CRF to model many-to-one word alignments,
where each source word is aligned with zero or
one target words, and therefore each target word
can be aligned with many source words. Each
source word is labelled with the index of its
aligned target, or the special value null, denot-
ing no alignment. An example word alignment
is shown in Figure 1, where the hollow squares
and circles indicate the correct alignments. In this
example the French words une and autre would
both be assigned the index 24 ? for the English
word another ? when French is the source lan-
guage. When the source language is English, an-
other could be assigned either index 25 or 26; in
these ambiguous situations we take the first index.
The joint probability density of the alignment,
a (a vector of target indices), conditioned on the
source and target sentences, e and f , is given by:
p?(a|e, f) =
exp
?
t
?
k ?khk(t, at?1, at, e, f)
Z?(e, f)
(1)
where we make a first order Markov assumption
 
they
are
constrained
by
limits
which
are
imposed
in
order
to
ensure
that
the
freedom
of
one
person
does
not
violate
that
of
another
.
  .
au
treun
edece
llesu
r
pa
s
em
pi?
tene
pe
rso
nn
e
un
ede
libe
rt?laqu
e
ga
ran
tir
po
ur
fix?
es?t?on
t
qu
i
lim
ite
s
ce
rta
ine
spa
r
res
tre
int
s
so
ntils 
Figure 1. A word-aligned example from the Canadian
Hansards test set. Hollow squares represent gold stan-
dard sure alignments, circles are gold possible align-
ments, and filled squares are predicted alignments.
over the alignment sequence. Here t ranges over
the indices of the source sentence (f ), k ranges
over the model?s features, and ? = {?k} are the
model parameters (weights for their correspond-
ing features). The feature functions hk are pre-
defined real-valued functions over the source and
target sentences coupled with the alignment labels
over adjacent times (source sentence locations),
t. These feature functions are unconstrained, and
may represent overlapping and non-independent
features of the data. The distribution is globally
normalised by the partition function, Z?(e, f),
which sums out the numerator in (1) for every pos-
sible alignment:
Z?(e, f) =
?
a
exp
?
t
?
k
?khk(t, at?1, at, e, f)
We use a linear chain CRF, which is encoded in
the feature functions of (1).
The parameters of the CRF are usually esti-
mated from a fully observed training sample (word
aligned), by maximising the likelihood of these
data. I.e. ?ML = argmax? p?(D), where D =
{(a, e, f)} are the training data. Because max-
imum likelihood estimators for log-linear mod-
els have a tendency to overfit the training sam-
ple (Chen and Rosenfeld, 1999), we define a prior
distribution over the model parameters and de-
rive a maximum a posteriori (MAP) estimate,
?MAP = argmax? p?(D)p(?). We use a zero-
mean Gaussian prior, with the probability density
function p0(?k) ? exp
(
?
?2k
2?2k
)
. This yields a
log-likelihood objective function of:
L =
?
(a,e,f)?D
log p?(a|e, f) +
?
k
log p0(?k)
66
=
?
(a,e,f)?D
?
t
?
k
?khk(t, at?1, at, e, f)
? logZ?(e, f)?
?
k
?2k
2?2k
+ const. (2)
In order to train the model, we maximize (2).
While the log-likelihood cannot be maximised for
the parameters, ?, in closed form, it is a con-
vex function, and thus we resort to numerical op-
timisation to find the globally optimal parame-
ters. We use L-BFGS, an iterative quasi-Newton
optimisation method, which performs well for
training log-linear models (Malouf, 2002; Sha
and Pereira, 2003). Each L-BFGS iteration re-
quires the objective value and its gradient with
respect to the model parameters. These are cal-
culated using forward-backward inference, which
yields the partition function, Z?(e, f), required
for the log-likelihood, and the pair-wise marginals,
p?(at?1, at|e, f), required for its derivatives.
The Viterbi algorithm is used to find the maxi-
mum posterior probability alignment for test sen-
tences, a? = argmaxa p?(a|e, f). Both the
forward-backward and Viterbi algorithm are dy-
namic programs which make use of the Markov
assumption to calculate efficiently the exact
marginal distributions.
3 The alignment model
Before we can apply our CRF alignment model,
we must first specify the feature set ? the func-
tions hk in (1). Typically CRFs use binary indica-
tor functions as features; these functions are only
active when the observations meet some criteria
and the label at (or label pair, (at?1, at)) matches
a pre-specified label (pair). However, in our model
the labellings are word indices in the target sen-
tence and cannot be compared readily to labellings
at other sites in the same sentence, or in other sen-
tences with a different length. Such naive features
would only be active for one labelling, therefore
this model would suffer from serious sparse data
problems.
We instead define features which are functions
of the source-target word match implied by a la-
belling, rather than the labelling itself. For exam-
ple, from the sentence in Figure 1 for the labelling
of f24 = de with a24 = 16 (for e16 = of ) we
might detect the following feature:
h(t, at?1, at, f , e) =
{
1, if eat = ?of? ? ft = ?de?
0, otherwise
Note that it is the target word indexed by at, rather
than the index itself, which determines whether
the feature is active, and thus the sparsity of the
index label set is not an issue.
3.1 Features
One of the main advantages of using a conditional
model is the ability to explore a diverse range of
features engineered for a specific task. In our
CRFmodel we employ two main types of features:
those defined on a candidate aligned pair of words;
and Markov features defined on the alignment se-
quence predicted by the model.
Dice and Model 1 As we have access to only a
small amount of word aligned data we wish to be
able to incorporate information about word associ-
ation from any sentence aligned data available. A
common measure of word association is the Dice
coefficient (Dice, 1945):
Dice(e, f) =
2? CEF (e, f)
CE(e) + CF (e)
where CE and CF are counts of the occurrences
of the words e and f in the corpus, while CEF is
their co-occurrence count. We treat these Dice val-
ues as translation scores: a high (low) value inci-
dates that the word pair is a good (poor) candidate
translation.
However, the Dice score often over-estimates
the association between common words. For in-
stance, the words the and of both score highly
when combined with either le or de, simply be-
cause these common words frequently co-occur.
The GIZA++ models can be used to provide better
translation scores, as they enforce competition for
alignment beween the words. For this reason, we
used the translation probability distribution from
Model 1 in addition to the DICE scores. Model 1
is a simple position independent model which can
be trained quickly and is often used to bootstrap
parameters for more complex models. It models
the conditional probability distribution:
p(f ,a|e) =
p(|f |||e|)
(|e|+ 1)|f |
?
|f |?
t=1
p(ft|eat)
where p(f |e) are the word translation probabili-
ties.
We use both the Dice value and the Model 1
translation probability as real-valued features for
each candidate pair, as well as a normalised score
67
over all possible candidate alignments for each tar-
get word. We derive a feature from both the Dice
and Model 1 translation scores to allow compe-
tition between sources words for a particular tar-
get algnment. This feature indicates whether a
given alignment has the highest translation score
of all the candidate alignments for a given tar-
get word. For the example in Figure 1, the words
la, de and une all receive a high translation score
when paired with the. To discourage all of these
French words from aligning with the, the best of
these (la) is flagged as the best candidate. This al-
lows for competition between source words which
would otherwise not occur.
Orthographic features Features based on
string overlap allow our model to recognise
cognates and orthographically similar translation
pairs, which are particularly common between
European languages. Here we employ a number
of string matching features inspired by similar
features in Taskar et al (2005). We use an indica-
tor feature for every possible source-target word
pair in the training data. In addition, we include
indicator features for an exact string match, both
with and without vowels, and the edit-distance
between the source and target words as a real-
valued feature. We also used indicator features to
test for matching prefixes and suffixes of length
three. As stated earlier, the Dice translation
score often erroneously rewards alignments with
common words. In order to address this problem,
we include the absolute difference in word length
as a real-valued feature and an indicator feature
testing whether both words are shorter than 4
characters. Together these features allow the
model to disprefer alignments between words
with very different lengths ? i.e. aligning rare
(long) words with frequent (short) determiners,
verbs etc.
POS tags Part-of-speech tags are an effective
method for addressing the sparsity of the lexi-
cal features. Observe in Figure 2 that the noun-
adjective pair Canadian experts aligns with the
adjective-noun pair spe?cialistes canadiens: the
alignment exactly matches the parts-of-speech.
Access to the words? POS tags will allow simple
modelling of such effects. POS can also be useful
for less closely related language pairs, such as En-
glish and Japanese where English determiners are
never aligned; nor are Japanese case markers.
For our French-English language pair we POS
tagged the source and target sentences with Tree-
Tagger.2 We created indicator features over the
POS tags of each candidate source and target word
pair, as well as over the source word and target
POS (and vice-versa). As we didn?t have access to
a Romanian POS tagger, these features were not
used for the Romanian-English language pair.
Bilingual dictionary Dictionaries are another
source of information for word alignment. We
use a single indicator feature which detects when
the source and target words appear in an entry of
the dictionary. For the English-French dictionary
we used FreeDict,3 which contains 8,799 English
words. For Romanian-English we used a dictio-
nary compiled by Rada Mihalcea,4 which contains
approximately 38,000 entries.
Markov features Features defined over adja-
cent aligment labels allow our model to reflect the
tendency for monotonic alignments between Eu-
ropean languages. We define a real-valued align-
ment index jump width feature:
jump width(t? 1, t) = abs(at ? at?1 ? 1)
this feature has a value of 0 if the alignment labels
follow the downward sloping diagonal, and is pos-
itive otherwise. This differs from the GIZA++ hid-
den Markov model which has individual parame-
ters for each different jump width (Och and Ney,
2003; Vogel et al, 1996): we found a single fea-
ture (and thus parameter) to be more effective.
We also defined three indicator features over
null transitions to allow the modelling of the prob-
ability of transition between, to and from null la-
bels.
Relative sentence postion A feature for the
absolute difference in relative sentence position
(abs( at|e| ?
t
|f |)) allows the model to learn a pref-
erence for aligning words close to the alignment
matrix diagonal. We also included two conjunc-
tion features for the relative sentence position mul-
tiplied by the Dice and Model 1 translation scores.
Null We use a number of variants on the above
features for alignments between a source word and
the null target. The maximum translation score
between the source and one of the target words
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
3http://www.freedict.de
4http://lit.csci.unt.edu/?rada/downloads/RoNLP/R.E.tralex
68
model precision recall f-score AER
Model 4 refined 87.4 95.1 91.1 9.81
Model 4 intersection 97.9 86.0 91.6 7.42
French? English 96.7 85.0 90.5 9.21
English? French 97.3 83.0 89.6 10.01
intersection 98.7 78.6 87.5 12.02
refined 95.7 89.2 92.3 7.37
Table 1. Results on the Hansard data using all features
model precision recall f-score AER
Model 4 refined 80.49 64.10 71,37 28.63
Model 4 intersected 95.94 53.56 68.74 31.26
Romanian? English 82.9 61.3 70.5 29.53
English? Romanian 82.8 60.6 70.0 29.98
intersection 94.4 52.5 67.5 32.45
refined 77.1 68.5 72.6 27.41
Table 2. Results on the Romanian data using all fea-
tures
is used as a feature to represent whether there is
a strong alignment candidate. The sum of these
scores is also used as a feature. Each source word
and POS tag pair are used as indicator features
which allow the model to learn particular words
of tags which tend to commonly (or rarely) align.
3.2 Symmetrisation
In order to produce many-to-many alignments we
combine the outputs of two models, one for each
translation direction. We use the refined method
from Och and Ney (2003) which starts from the
intersection of the two models? predictions and
?grows? the predicted alignments to neighbouring
alignments which only appear in the output of one
of the models.
4 Experiments
We have applied our model to two publicly avail-
able word aligned corpora. The first is the
English-French Hansards corpus, which consists
of 1.1 million aligned sentences and 484 word-
aligned sentences. This data set was used for
the 2003 NAACL shared task (Mihalcea and Ped-
ersen, 2003), where the word-aligned sentences
were split into a 37 sentence trial set and a 447 sen-
tence testing set. Unlike the unsupervised entrants
in the 2003 task, we require word-aligned training
data, and therefore must cannibalise the test set for
this purpose. We follow Taskar et al (2005) by us-
ing the first 100 test sentences for training and the
remaining 347 for testing. This means that our re-
sults should not be directly compared to those en-
trants, other than in an approximate manner. We
used the original 37 sentence trial set for feature
engineering and for fitting a Gaussian prior.
The word aligned data are annotated with both
sure (S) and possible (P ) alignments (S ? P ; Och
and Ney (2003)), where the possible alignments
indicate ambiguous or idiomatic alignments. We
measure the performance of our model using
alignment error rate (AER), which is defined as:
AER(A,S, P ) = 1?
|A ? S|+ |A ? P |
|A|+ |S|
where A is the set of predicted alignments.
The second data set is the Romanian-English
parallel corpus from the 2005 ACL shared task
(Martin et al, 2005). This consists of approxi-
mately 50,000 aligned sentences and 448 word-
aligned sentences, which are split into a 248 sen-
tence trial set and a 200 sentence test set. We
used these as our training and test sets, respec-
tively. For parameter tuning, we used the 17 sen-
tence trial set from the Romanian-English corpus
in the 2003 NAACL task (Mihalcea and Pedersen,
2003). For this task we have used the same test
data as the competition entrants, and therefore can
directly compare our results. The word alignments
in this corpus were only annotated with sure (S)
alignments, and therefore the AER is equivalent
to the F1 score. In the shared task it was found
that models which were trained on only the first
four letters of each word obtained superior results
to those using the full words (Martin et al, 2005).
We observed the same result with our model on
the trial set and thus have only used the first four
letters when training the Dice and Model 1 trans-
lation probabilities.
Tables 1 and 2 show the results when all feature
types are employed on both language pairs. We re-
port the results for both translation directions and
when combined using the refined and intersection
methods. The Model 4 results are from GIZA++
with the default parameters and the training data
lowercased. For Romanian, Model 4 was trained
using the first four letters of each word.
The Romanian results are close to the best re-
ported result of 26.10 from the ACL shared task
(Martin et al, 2005). This result was from a sys-
tem based on Model 4 plus additional parameters
such as a dictionary. The standard Model 4 imple-
mentation in the shared task achieved a result of
31.65, while when only the first 4 letters of each
word were used it achieved 28.80.5
5These results differ slightly our Model 4 results reported
in Table 2.
69
 
(
ii
)
(
a
)
Three
vehicles
will
be
used
by
six
Canadian
experts
related
to
the
provision
of
technical
assistance
.
  .
tec
hn
iqu
e
aid
ede
pre
sta
tio
nlade
ca
dreleda
ns
ca
na
die
ns
sp
?c
iali
ste
s6pa
r
uti
lis?
s
se
ron
t
v?
hic
ule
s3)a)ii( 
(a) With Markov features
 
(
ii
)
(
a
)
Three
vehicles
will
be
used
by
six
Canadian
experts
related
to
the
provision
of
technical
assistance
.
  .
tec
hn
iqu
e
aid
ede
pre
sta
tio
nlade
ca
dreleda
ns
ca
na
die
ns
sp
?c
iali
ste
s6pa
r
uti
lis?
s
se
ron
t
v?
hic
ule
s3)a)ii( 
(b) Without Markov features
Figure 2. An example from the Hansard test set, showing the effect of the Markov features.
Table 3 shows the effect of removing each of the
feature types in turn from the full model. The most
useful features are the Dice and Model 1 values
which allow the model to incorporate translation
probabilities from the large sentence aligned cor-
pora. This is to be expected as the amount of word
aligned data are extremely small, and therefore the
model can only estimate translation probabilities
for only a fraction of the lexicon. We would expect
the dependence on sentence aligned data to de-
crease as more word aligned data becomes avail-
able.
The effect of removing the Markov features can
be seen from comparing Figures 2 (a) and (b). The
model has learnt to prefer alignments that follow
the diagonal, thus alignments such as 3 ? three
and prestation ? provision are found, and miss-
alignments such as de ? of, which lie well off the
diagonal, are avoided.
The differing utility of the alignment word pair
feature between the two tasks is probably a result
of the different proportions of word- to sentence-
aligned data. For the French data, where a very
large lexicon can be estimated from the million
sentence alignments, the sparse word pairs learnt
on the word aligned sentences appear to lead to
overfitting. In contrast, for Romanian, where more
word alignments are used to learn the translation
pair features and much less sentence aligned data
are available, these features have a significant im-
pact on the model. Suprisingly the orthographic
features actually worsen the performance in the
tasks (incidentally, these features help the trial
set). Our explanation is that the other features
(eg. Model 1) already adequately model these cor-
respondences, and therefore the orthographic fea-
feature group Rom? Eng Fre? Eng
ALL 27.41 7.37
?orthographic 27.30 7.25
?Dice 27.68 7.73
?dictionary 27.72 7.21
?sentence position 28.30 8.01
?POS ? 8.19
?Model 1 28.62 8.45
?alignment word pair 32.41 7.20
?Markov 32.75 12.44
?Dice & ?Model 1 35.43 14.10
Table 3. The resulting AERs after removing individual
groups of features from the full model.
tures do not add much additional modelling power.
We expect that with further careful feature engi-
neering, and a larger trial set, these orthographic
features could be much improved.
The Romanian-English language pair appears
to offer a more difficult modelling problem than
the French-English pair. With both the transla-
tion score features (Dice and Model 1) removed
? the sentence aligned data are not used ? the
AER of the Romanian is more than twice that of
the French, despite employing more word aligned
data. This could be caused by the lack of possi-
ble (P) alignment markup in the Romanian data,
which provide a boost in AER on the French data
set, rewarding what would otherwise be consid-
ered errors. Interestingly, without any features
derived from the sentence aligned corpus, our
model achieves performance equivalent to Model
3 trained on the full corpus (Och and Ney, 2003).
This is a particularly strong result, indicating that
this method is ideal for data-impoverished align-
ment tasks.
70
4.1 Training with possible alignments
Up to this point our Hansards model has been
trained using only the sure (S) alignments. As
the data set contains many possible (P) alignments,
we would like to use these to improve our model.
Most of the possible alignments flag blocks of
ambiguous or idiomatic (or just difficult) phrase
level alignments. These many-to-many align-
ments cannot be modelled with our many-to-one
setup. However, a number of possibles flag one-
to-one or many-to-one aligments: for this experi-
ment we used these possibles in training to inves-
tigate their effect on recall. Using these additional
alignments our refined precision decreased from
95.7 to 93.5, while recall increased from 89.2 to
92.4. This resulted in an overall decrease in AER
to 6.99. We found no benefit from using many-to-
many possible alignments as they added a signifi-
cant amount of noise to the data.
4.2 Model 4 as a feature
Previous work (Taskar et al, 2005) has demon-
strated that by including the output of Model 4 as
a feature, it is possible to achieve a significant de-
crease in AER. We trained Model 4 in both direc-
tions on the two language pairs. We added two
indicator features (one for each direction) to our
CRF which were active if a given word pair were
aligned in the Model 4 output. Table 4 displays
the results on both language pairs when these ad-
ditional features are used with the refined model.
This produces a large increase in performance, and
when including the possibles, produces AERs of
5.29 and 25.8, both well below that of Model 4
alone (shown in Tables 1 and 2).
4.3 Cross-validation
Using 10-fold cross-validation we are able to gen-
erate results on the whole of the Hansards test data
which are comparable to previously published re-
sults. As the sentences in the test set were ran-
domly chosen from the training corpus we can ex-
pect cross-validation to give an unbiased estimate
of generalisation performance. These results are
displayed in Table 5, using the possible (P) align-
ments for training. As the training set for each fold
is roughly four times as big previous training set,
we see a small improvement in AER.
The final results of 6.47 and 5.19 with and
without Model 4 features both exceed the perfor-
mance of Model 4 alone. However the unsuper-
model precision recall f-score AER
Rom? Eng 79.0 70.0 74.2 25.8
Fre? Eng 97.9 90.8 94.2 5.49
Fre? Eng (P) 95.5 93.7 94.6 5.29
Table 4. Results using features from Model 4 bi-
directional alignments, training with and without the
possible (P) alignments.
model precision recall f-score AER
Fre? Eng 94.6 92.2 93.4 6.47
Fre? Eng (Model 4) 96.1 93.3 94.7 5.19
Table 5. 10-fold cross-validation results, with and with-
out Model 4 features.
vised Model 4 did not have access to the word-
alignments in our training set. Callison-Burch et
al. (2004) demonstrated that the GIZA++ mod-
els could be trained in a semi-supervised manner,
leading to a slight decrease in error. To our knowl-
edge, our AER of 5.19 is the best reported result,
generative or discriminative, on this data set.
5 Related work
Recently, a number of discriminative word align-
ment models have been proposed, however these
early models are typically very complicated with
many proposing intractable problems which re-
quire heuristics for approximate inference (Liu et
al., 2005; Moore, 2005).
An exception is Taskar et al (2005) who pre-
sented a word matching model for discriminative
alignment which they they were able to solve opti-
mally. However, their model is limited to only pro-
viding one-to-one alignments. Also, no features
were defined on label sequences, which reduced
the model?s ability to capture the strong monotonic
relationships present between European language
pairs. On the French-English Hansards task, using
the same training/testing setup as our work, they
achieve an AER of 5.4 with Model 4 features, and
10.7 without (compared to 5.29 and 6.99 for our
CRF). One of the strengths of the CRF MAP es-
timation is the powerful smoothing offered by the
prior, which allows us to avoid heuristics such as
early stopping and hand weighted loss-functions
that were needed for the maximum-margin model.
Liu et al (2005) used a conditional log-linear
model with similar features to those we have em-
ployed. They formulated a global model, without
making a Markovian assumption, leading to the
need for a sub-optimal heuristic search strategies.
Ittycheriah and Roukos (2005) trained a dis-
71
criminative model on a corpus of ten thousand
word aligned Arabic-English sentence pairs that
outperformed a GIZA++ baseline. As with other
approaches, they proposed a model which didn?t
allow a tractably optimal solution and thus had to
resort to a heuristic beam search. They employed
a log-linear model to learn the observation proba-
bilities, while using a fixed transition distribution.
Our CRF model allows both the observation and
transition components of the model to be jointly
optimised from the corpus.
6 Further work
The results presented in this paper were evaluated
in terms of AER. While a low AER can be ex-
pected to improve end-to-end translation quality,
this is may not necessarily be the case. There-
fore, we plan to assess how the recall and preci-
sion characteristics of our model affect translation
quality. The tradeoff between recall and precision
may affect the quality and number of phrases ex-
tracted for a phrase translation table.
7 Conclusion
We have presented a novel approach for induc-
ing word alignments from sentence aligned data.
We showed how conditional random fields could
be used for word alignment. These models al-
low for the use of arbitrary and overlapping fea-
tures over the source and target sentences, making
the most of small supervised training sets. More-
over, we showed how the CRF?s inference and es-
timation methods allowed for efficient processing
without sacrificing optimality, improving on pre-
vious heuristic based approaches.
On both French-English and Romanian-English
we showed that many highly predictive features
can be easily incorporated into the CRF, and
demonstrated that with only a few hundred word-
aligned training sentences, our model outperforms
the generativeModel 4 baseline. When no features
are extracted from the sentence aligned corpus our
model still achieves a low error rate. Furthermore,
when we employ features derived from Model 4
alignments our CRF model achieves the highest
reported results on both data sets.
Acknowledgements
Special thanks to Miles Osborne, Steven Bird,
Timothy Baldwin and the anonymous reviewers
for their feedback and insightful comments.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
C. Callison-Burch, D. Talbot, and M. Osborne. 2004. Statis-
tical machine translation with word- and sentence-aligned
parallel corpora. In Proceedings of ACL, pages 175?182,
Barcelona, Spain, July.
S. Chen and R. Rosenfeld. 1999. A survey of smoothing
techniques for maximum entropy models. IEEE Transac-
tions on Speech and Audio Processing, 8(1):37?50.
L. R. Dice. 1945. Measures of the amount of ecologic asso-
ciation between species. Journal of Ecology, 26:297?302.
A. Ittycheriah and S. Roukos. 2005. A maximum entropy
word aligner for Arabic-English machine translation. In
Proceedings of HLT-EMNLP, pages 89?96, Vancouver,
British Columbia, Canada, October.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT-NAACL, pages
81?88, Edmonton, Alberta.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labelling sequence data. In Proceedings of ICML, pages
282?289.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In Proceedings of ACL, pages 459?466, Ann
Arbor.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proceedings of CoNLL,
pages 49?55.
J. Martin, R. Mihalcea, and T. Pedersen. 2005. Word align-
ment for languages with scarce resources. In Proceed-
ings of the ACL Workshop on Building and Using Parallel
Texts, pages 65?74, Ann Arbor, Michigan, June.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proceedings of HLT-NAACL
2003 Workshop, Building and Using Parrallel Texts: Data
Driven Machine Translation and Beyond, pages 1?6, Ed-
monton, Alberta.
R. C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of HLT-EMNLP,
pages 81?88, Vancouver, Canada.
F. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguis-
tics, 29(1):19?52.
F. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of HLT-NAACL,
pages 213?220.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrimi-
native matching approach to word alignment. In Proceed-
ings of HLT-EMNLP, pages 73?80, Vancouver, British
Columbia, Canada, October.
K. Toutanova, H. Tolga Ilhan, and C Manning. 2002. Ex-
tentions to HMM-based statistical word alignment mod-
els. In Proceedings of EMNLP, pages 87?94, Philadel-
phia, July.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In Proceedings of 16th
Int. Conf. on Computational Linguistics, pages 836?841.
72
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 169?172, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labelling with
Tree Conditional Random Fields
Trevor Cohn and Philip Blunsom
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au and pcbl@csse.unimelb.ed.au
Abstract
In this paper we apply conditional
random fields (CRFs) to the semantic
role labelling task. We define a random
field over the structure of each sentence?s
syntactic parse tree. For each node
of the tree, the model must predict a
semantic role label, which is interpreted
as the labelling for the corresponding
syntactic constituent. We show how
modelling the task as a tree labelling
problem allows for the use of efficient
CRF inference algorithms, while also
increasing generalisation performance
when compared to the equivalent
maximum entropy classifier. We have
participated in the CoNLL-2005 shared
task closed challenge with full syntactic
information.
1 Introduction
The semantic role labelling task (SRL) involves
identifying which groups of words act as arguments
to a given predicate. These arguments must
be labelled with their role with respect to the
predicate, indicating how the proposition should be
semantically interpreted.
We apply conditional random fields (CRFs) to
the task of SRL proposed by the CoNLL shared
task 2005 (Carreras and Ma`rquez, 2005). CRFs are
undirected graphical models which define a condi-
tional distribution over labellings given an obser-
vation (Lafferty et al, 2001). These models allow
for the use of very large sets of arbitrary, over-
lapping and non-independent features. CRFs have
been applied with impressive empirical results to the
tasks of named entity recognition (McCallum and
Li, 2003; Cohn et al, 2005), part-of-speech (PoS)
tagging (Lafferty et al, 2001), noun phrase chunk-
ing (Sha and Pereira, 2003) and extraction of table
data (Pinto et al, 2003), among other tasks.
While CRFs have not been used to date for SRL,
their close cousin, the maximum entropy model has
been, with strong generalisation performance (Xue
and Palmer, 2004; Lim et al, 2004). Most CRF
implementations have been specialised to work with
chain structures, where the labels and observations
form a linear sequence. Framing SRL as a linear
tagging task is awkward, as there is no easy model
of adjacency between the candidate constituent
phrases.
Our approach simultaneously performs both con-
stituent selection and labelling, by defining an undi-
rected random field over the parse tree. This allows
the modelling of interactions between parent and
child constituents, and the prediction of an optimal
argument labelling for all constituents in one pass.
The parse tree forms an acyclic graph, meaning that
efficient exact inference in a CRF is possible using
belief propagation.
2 Data
The data used for this task was taken from the
Propbank corpus, which supplements the Penn
Treebank with semantic role annotation. Full details
of the data set are provided in Carreras and Ma`rquez
(2005).
2.1 Data Representation
From each training instance we derived a tree, using
the parse structure from the Collins parser. The
169
nodes in the trees were relabelled with a semantic
role label indicating how their corresponding syn-
tactic constituent relates to each predicate, as shown
in Figure 1. The role labels are shown as subscripts
in the figure, and both the syntactic categories and
the words at the leaves are shown for clarity only
? these were not included in the tree. Addition-
ally, the dashed lines show those edges which were
pruned, following Xue and Palmer (2004) ? only
nodes which are siblings to a node on the path from
the verb to the root are included in the tree. Child
nodes of included prepositional phrase nodes are
also included. This reduces the size of the resultant
tree whilst only very occasionally excluding nodes
which should be labelled as an argument.
The tree nodes were labelled such that only argu-
ment constituents received the argument label while
all argument children were labelled as outside, O.
Where there were parse errors, such that no con-
stituent exactly covered the token span of an argu-
ment, the smaller subsumed constituents were all
given the argument label.
We experimented with two alternative labelling
strategies: labelling a constituent?s children with a
new ?inside? label, and labelling the children with
the parent?s argument label. In the figure, the IN and
NP children of the PP would be affected by these
changes, both receiving either the inside I label or
AM-LOC label under the respective strategies. The
inside strategy performed nearly identically to the
standard (outside) strategy, indicating that either the
model cannot reliably predict the inside argument,
or that knowing that the children of a given node are
inside an argument is not particularly useful in pre-
dicting its label. The second (duplication) strategy
performed extremely poorly. While this allowed the
internal argument nodes to influence their ancestor
towards a particular labelling, it also dramatically
increased the number of nodes given an argument
label. This lead to spurious over-prediction of argu-
ments.
The model is used for decoding by predicting the
maximum probability argument label assignment to
each of the unlabelled trees. When these predic-
tions were inconsistent, and one argument subsumed
another, the node closest to the root of the tree was
deemed to take precedence over its descendants.
3 Model
We define a CRF over the labelling y given the
observation tree x as:
p(y|x) =
1
Z(x)
exp
?
c?C
?
k
?kfk(c,yc,x)
where C is the set of cliques in the observation tree,
?k are the model?s parameters and fk(?) is the fea-
ture function which maps a clique labelling to a vec-
tor of scalar values. The function Z(?) is the nor-
malising function, which ensures that p is a valid
probability distribution. This can be restated as:
p(y|x) =
1
Z(x)
exp
?
?
?
?
v?C1
?
k
?kgk(v,yv,x)
+
?
u,v?C2
?
j
?jhj(u, v,yu,yv,x)
?
?
?
where C1 are the vertices in the graph and C2 are
the maximal cliques in the graph, consisting of all
(parent, child) pairs. The feature function has been
split into g and h, each dealing with one and two
node cliques respectively.
Preliminary experimentation without any
pair-wise features (h), was used to mimic a
simple maximum entropy classifier. This model
performed considerably worse than the model
with the pair-wise features, indicating that the
added complexity of modelling the parent-child
interactions provides for more accurate modelling
of the data.
The log-likelihood of the training sample was
optimised using limited memory variable metric
(LMVM), a gradient based technique. This required
the repeated calculation of the log-likelihood and
its derivative, which in turn required the use of
dynamic programming to calculate the marginal
probability of each possible labelling of every clique
using the sum-product algorithm (Pearl, 1988).
4 Features
As the conditional random field is conditioned on
the observation, it allows feature functions to be
defined over any part of the observation. The tree
structure requires that features incorporate either a
node labelling or the labelling of a parent and its
170
SNP NP VP
DT NN NN NN
JJ NN V NP PP
CD NNS NP
DT NNP
IN
The luxury auto maker last year sold 1,214 cars in the US
O
A0
A1 AM-LOCV
AM-TMP O
O O
Figure 1: Syntax tree labelled for semantic roles with respect to the predicate sell. The subscripts show the
role labels, and the dotted and dashed edges are those which are pruned from the tree.
child. We have defined node and pairwise clique fea-
tures using data local to the corresponding syntactic
node(s), as well as some features on the predicate
itself.
Each feature type has been made into binary fea-
ture functions g and h by combining (feature type,
value) pairs with a label, or label pair, where this
combination was seen at least once in the training
data. The following feature types were employed,
most of which were inspired by previous works:
Basic features: {Head word, head PoS, phrase
syntactic category, phrase path, position rel-
ative to the predicate, surface distance to the
predicate, predicate lemma, predicate token,
predicate voice, predicate sub-categorisation,
syntactic frame}. These features are common
to many SRL systems and are described in Xue
and Palmer (2004).
Context features {Head word of first NP in prepo-
sition phrase, left and right sibling head words
and syntactic categories, first and last word
in phrase yield and their PoS, parent syntactic
category and head word}. These features are
described in Pradhan et al (2005).
Common ancestor of the verb The syntactic cate-
gory of the deepest shared ancestor of both the
verb and node.
Feature conjunctions The following features were
conjoined: { predicate lemma + syntactic cate-
gory, predicate lemma + relative position, syn-
tactic category + first word of the phrase}.
Default feature This feature is always on, which
allows the classifier to model the prior prob-
ability distribution over the possible argument
labels.
Joint features These features were only defined
over pair-wise cliques: {whether the parent
and child head words do not match, parent syn-
tactic category + and child syntactic category,
parent relative position + child relative posi-
tion, parent relative position + child relative
position + predicate PoS + predicate lemma}.
5 Experimental Results
The model was trained on the full training set
after removing unparsable sentences, yielding
90,388 predicates and 1,971,985 binary features. A
Gaussian prior was used to regularise the model,
with variance ?2 = 1. Training was performed on
a 20 node PowerPC cluster, consuming a total of
62Gb of RAM and taking approximately 15 hours.
Decoding required only 3Gb of RAM and about 5
minutes for the 3,228 predicates in the development
set. Results are shown in Table 1.
171
Precision Recall F?=1
Development 73.51% 68.98% 71.17
Test WSJ 75.81% 70.58% 73.10
Test Brown 67.63% 60.08% 63.63
Test WSJ+Brown 74.76% 69.17% 71.86
Test WSJ Precision Recall F?=1
Overall 75.81% 70.58% 73.10
A0 82.21% 79.48% 80.82
A1 74.56% 71.26% 72.87
A2 63.93% 56.85% 60.18
A3 63.95% 54.34% 58.75
A4 68.69% 66.67% 67.66
A5 0.00% 0.00% 0.00
AM-ADV 54.73% 48.02% 51.16
AM-CAU 75.61% 42.47% 54.39
AM-DIR 54.17% 30.59% 39.10
AM-DIS 77.74% 73.12% 75.36
AM-EXT 65.00% 40.62% 50.00
AM-LOC 60.67% 54.82% 57.60
AM-MNR 54.66% 49.42% 51.91
AM-MOD 98.34% 96.55% 97.44
AM-NEG 99.10% 96.09% 97.57
AM-PNC 49.47% 40.87% 44.76
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 77.20% 68.54% 72.61
R-A0 87.78% 86.61% 87.19
R-A1 82.39% 75.00% 78.52
R-A2 0.00% 0.00% 0.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 71.05% 51.92% 60.00
V 98.73% 98.63% 98.68
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
6 Conclusion
Conditional random fields proved useful in mod-
elling the semantic structure of text when provided
with a parse tree. Our novel use of a tree structure
derived from the syntactic parse, allowed for parent-
child interactions to be accurately modelled, which
provided an improvement over a standard maximum
entropy classifier. In addition, the parse constituent
structure proved quite appropriate to the task, more
so than modelling the data as a sequence of words or
chunks, as has been done in previous approaches.
Acknowledgements
We would both like to thank our research super-
visor Steven Bird for his comments and feedback
on this work. The research undertaken for this
paper was supported by an Australian Postgraduate
Award scholarship, a Melbourne Research Scholar-
ship and a Melbourne University Postgraduate Over-
seas Research Experience Scholarship.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 Shared Task: Semantic Role Labeling. In
Proceedings of the CoNLL-2005.
Trevor Cohn, Andrew Smith, and Miles Osborne. 2005. Scal-
ing conditional random fields using error correcting codes.
In Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics. To appear.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings of the
18th International Conference on Machine Learning, pages
282?289.
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, and Hae-
Chang Rim. 2004. Semantic role labeling using maximum
entropy model. In Proceedings of the CoNLL-2004 Shared
Task.
Andrew McCallum and Wei Li. 2003. Early results for named
entity recognition with conditional random fields, feature
induction and web-enhanced lexicons. In Proceedings of
the 7th Conference on Natural Language Learning, pages
188?191.
Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference. Morgan Kaufmann.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, pages 235?242.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Daniel Jurafsky. 2005. Sup-
port vector learning for semantic argument classification. In
To appear in Machine Learning journal, Special issue on
Speech and Natural Language Processing.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the Human Lan-
guage Technology Conference and North American Chap-
ter of the Association for Computational Linguistics, pages
213?220.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP.
172
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 482?491,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Multi-document summarization using A* search and discriminative training
Ahmet Aker Trevor Cohn
Department of Computer Science
University of Sheffield, Sheffield, S1 4DP, UK
{a.aker, t.cohn, r.gaizauskas}@dcs.shef.ac.uk
Robert Gaizauskas
Abstract
In this paper we address two key challenges
for extractive multi-document summarization:
the search problem of finding the best scoring
summary and the training problem of learn-
ing the best model parameters. We propose an
A* search algorithm to find the best extractive
summary up to a given length, which is both
optimal and efficient to run. Further, we pro-
pose a discriminative training algorithm which
directly maximises the quality of the best sum-
mary, rather than assuming a sentence-level
decomposition as in earlier work. Our ap-
proach leads to significantly better results than
earlier techniques across a number of evalua-
tion metrics.
1 Introduction
Multi-document summarization aims to present
multiple documents in form of a short summary.
This short summary can be used as a replacement
for the original documents to reduce, for instance,
the time a reader would spend if she were to read
the original documents. Following dominant trends
in summarization research (Mani, 2001), we focus
solely on extractive summarization which simplifies
the summarization task to the problem of identify-
ing a subset of units from the document collection
(here sentences) which are concatenated to form the
summary.
Most multi-document summarization systems de-
fine a model which assigns a score to a candidate
summary based on the features of the sentences in-
cluded in the summary. The research challenges are
then twofold: 1) the search problem of finding the
best scoring summary for a given document set, and
2) the training problem of learning the model pa-
rameters to best describe a training set consisting of
pairs of document sets with model or reference sum-
maries ? typically human authored extractive or ab-
stractive summaries.
Search is typically performed by a greedy al-
gorithm which selects each sentence in decreasing
order of model score until the desired summary
length is reached (see, e.g., Saggion (2005)) or us-
ing heuristic strategies based on position in docu-
ment or lexical clues (Edmundson, 1969; Brandow
et al, 1995; Hearst, 1997; Ouyang et al, 2010).1
We show in this paper that the search problem can
be solved optimally and efficiently using A* search
(Russell et al, 1995). Assuming the model only uses
features local to each sentence in the summary, our
algorithm finds the best scoring extractive summary
up to a given length in words.
Framing summarization as search suggests that
many of the popular training techniques are max-
imising the wrong objective. These approaches train
a classifier, regression or ranking model to distin-
guish between good and bad sentences under an
evaluation metric, e.g., ROUGE (Lin, 2004). The
model is then used during search to find a summary
composed of high scoring (?good?) sentences (see
for a review Ouyang et al (2010)). However, there
is a disconnect between the model used for training
and the model used for prediction. In this paper we
present a solution to this disconnect in the form of
a training algorithm that optimises the full predic-
tion model directly with the search algorithm intact.
The training algorithm learns parameters such that
1Genetic algorithms have also been devised for solving the
search problem (see, e.g., Riedhammer et al (2008)), however
these approaches do not guarantee optimality, nor are they effi-
cient enough to be practicable for large datasets.
482
the best scoring whole summary under the model
has a high score under the evaluation metric. We
demonstrate that this leads to significantly better test
performance than a competitive baseline, to the tune
of 3% absolute increase for ROUGE-1, -2 and -SU4.
The paper is structured as follows. Section 2
presents the summarization model. Next in sec-
tion 3 we present an A* search algorithm for finding
the best scoring (argmax) summary under the model
with a constraint on the maximum summary length.
We show that this algorithm performs search effi-
ciently, even for very large document sets composed
of many sentences. The second contribution of the
paper is a new training method which directly opti-
mises the summarization system, and is presented in
section 4. This uses the minimum error-rate training
(MERT) technique from machine translation (Och,
2003) to optimise the summariser?s output to an ar-
bitrary evaluation metric. Section 5 describes our
experimental setup and section 6 the results. Finally
we conclude in section 7.
2 Summarization Model
Extractive multi-document summarization aims to
find the most important sentences from a set of doc-
uments, which are then collated and presented to
the user in form of a short summary. Following
the predominant approach to data-driven summari-
sation, we define a linear model which scores sum-
maries as the weighted sum of their features,
s(y|x) = ?(x,y) ? ? , (1)
where x is the document set, composed of k sen-
tences, y ? {1 . . . k} are the set of selected sen-
tence indices, ?(?, ?) is a feature function which re-
turns a vector of features for the candidate summary
and ? are the model parameters. We further assume
that the features decompose with the sentences in
the summary, ?(x,y) =
?
i?y ?(xi), and there-
fore the scoring function also decomposes along the
same lines,
s(y|x) =
?
i?y
?(xi) ? ? . (2)
While this assumption greatly simplifies inference, it
does constrain the representative power of the model
by disallowing global features, e.g., those which
measure duplication in the summary.2 Under this
model, the search problem is to solve
y? = arg max
y
s(y|x) , (3)
for which we develop a best-first algorithm using A*
search, as described in section 3. The training chal-
lenge is to find the parameters, ?, to best model the
training set. This is achieved by finding ? such that
y? is similar to the gold standard summary accord-
ing to an automatic evaluation metric, as described
in section 4.
3 A* Search
The prediction problem is to find the best scoring
extractive summary (see Equation 3) up to a given
length, L. At first glance, this appears to be a sim-
ple problem that might be solved efficiently with a
greedy algorithm, say by taking the sentences in or-
der of decreasing score and stopping just before the
summary exceeds the length threshold. However,
the greedy algorithm cannot be guaranteed to find
the best summary; to do so requires arbitrary back-
tracking to revise previous incorrect decisions.
The problem of constructing the summary can be
considered a search problem in which we start with
an empty summary and incrementally enlarge the
summary by concatenating a sentence from our doc-
ument set. The search graph starts with an empty
summary (the starting state) and each outgoing edge
adds a sentence to produce a subsequent state, and
is assigned a score under the model. A goal state is
any state with no more words than the given thresh-
old. The summarisation problem is then equivalent
to finding the best scoring path (summed over the
edge scores) between the start state and a goal state.
The novel insight in our work is to use A* search
(Russell et al, 1995) to solve the prediction prob-
lem. A* is a best-first search algorithm which can
efficiently find the best scoring path or the n-best
paths (unlike the greedy algorithm which is not op-
timal, and the backtracking variant which is not ef-
ficient). The search procedure requires a scoring
function for each state, here s(y|x) from (2), and
2Our approach could be adapted to support global features,
which would require changes to the heuristic for A* search to
bound the score obtainable from the global features. This may
incur an additional computational cost over a purely local fea-
ture model and perhaps also necessitate using beam search.
483
a heuristic function which estimates the additional
score to get from a given state to a goal state. For
the search to be optimal ? guaranteed to find the best
scoring path as the first solution ? the heuristic must
be admissible, meaning that it bounds from above
the score for reaching a goal state. We present three
different admissible heuristics later in this section,
which bound the score with differing tightness and
consequently different search cost.
Algorithm 1 presents A* search for our extractive
summarisation model. Given a set of sentences to
summary, a scoring and a heuristic function, it finds
the best scoring summary. This is achieved by build-
ing the search graph incrementally, and storing each
frontier state in a priority queue (line 1) which is
sorted by the sum of the state?s score and its heuris-
tic. These states are popped off the queue (line 3)
and expanded by adding a sentence, which is then
added to the schedule (lines 8?14). We designate
special finishing states using a boolean variable (the
last entry in the tuple in lines 1, 7 and 12). Fin-
ishing states (with value T) denote ceasing to ex-
pand the summary, and consequently their scores
do not include the heuristic component. When-
ever one of these states is popped in line 2, we
know that it outscores all competing hypotheses and
therefore represents the optimal summary (because
the heuristic is guaranteed to never underestimate
the cost to a goal state from an unfinished state).3
Note that in algorithm 1 we create the summary
by building a list of sentence indices in sorted or-
der to avoid spurious ambiguity which would un-
necessarily expand the search space. The function
length(y,x) =
?
n?y length(xn) returns the length
of sentences specified.
We now return to the problem of defining the
heuristic function, h(y;x, l) which provides an up-
per bound on the additional score achievable in
reaching a goal state from state y. We present three
different variants of increasing fidelity, that is, that
bound the cost to a goal state more tightly. Algo-
rithm 2 is the simplest, which simply finds the max-
imum score per word from the set of unused sen-
3To improve the efficiency of Algorithm 1 we make a small
modification to avoid expanding every possible edge in step 8,
of which there are O(k) options. Instead we expand a small
number (here, 3) at a time and defer the remaining items until
later by inserting a special node into the schedule. These special
nodes are represented using a third ?to-be-continued? state into
the done flag.
Algorithm 1 A* search for extractive summarization.
Require: set of sentences, x = x1, . . . , xk
Require: scoring function s(?)
Require: heuristic function h(?)
Require: summary length limit L
1: schedule = [(0, ?, F)] {priority queue of triples}
{(A* score, sentence indices, done flag)}
2: while schedule 6= [] do
3: v,y, f ? pop(schedule)
4: if f = T then
5: return y {success}
6: else
7: push(schedule, (s(y|x),y,T))
8: for y ? [max(y) + 1, k] do
9: y? ? y ? y
10: if length(y?,x) ? L then
11: v? ? s(y?|x) + h(y?;x, l)
12: push(schedule, (v?,y?, F))
13: end if
14: end for
15: end if
16: end while
tences and then extrapolates this out over the re-
maining words available to the length threshold. In
the algorithm, we use the shorthand sn = ?(xn) ? ?
for sentence n?s score, ln = length(xn) for its length
and ly =
?
n?y ln for the total length of the current
state (unfinished summary).
Algorithm 2 Uniform heuristic, h1(y;x, L)
Require: x sorted in order of score/length
1: n? max(y) + 1
2: return (L? ly)max
(
sn
ln
, 0
)
The h1 heuristic is overly simple in that it assumes
we can ?reuse? a high scoring short sentence many
times despite this being disallowed by the model.
For this reason we develop an improved bound, h2,
in Algorithm 3. This incrementally adds each sen-
tence in order of its score-per-word until the length
limit is reached. If the limit is to be exceeded,
the heuristic scales down the final sentence?s score
based on the fraction of words than can be used to
reach the limit.
The fractional usage of the final sentence in h2
could be considered overly optimistic, especially
when the state has length just shy of the limit L. If
the next best ranked sentence is a long one, then it
will be used in the heuristic to over-estimate of the
state. This is complicated to correct, and doing so
exactly would require full backtracking which is in-
tractable and would obviate the entire point of using
A* search. Instead we use a subtle modification in
h3 (Alg. 4) which is equivalent to h2 except in the
484
Algorithm 3 Aggregated heuristic, h2(y;x, L)
Require: x sorted in order of score/length
1: v ? 0
2: l? ? ly
3: for n ? [max(y) + 1, k] do
4: if sn ? 0 then
5: return v
6: end if
7: if l? + ln ? L then
8: l? ? l? + ln
9: v ? v + sn
10: else
11: return v + lnL?l? sn
12: end if
13: end for
14: return v
instance where the next best score/word sentence is
too long, where it skips over these sentences until
it finds the best scoring sentence that does fit. This
helps to address the overestimate of h2 and should
therefore lead to a smaller search graph and faster
runtime due to its early elimination of dead-ends.
Algorithm 4 Agg.+final heuristic, h3(y;x, L)
Require: x sorted in order of score/length
1: n? max(y) + 1
2: if n ? k ? sn > 0 then
3: if ly + ln ? L then
4: return h2(y;x, L)
5: else
6: form ? [n+ 1, k] do
7: if ly + lm ? L then
8: return sm
L?ly
lm
9: end if
10: end for
11: end if
12: end if
13: return 0
The search process is illustrated in figure 1. When
a node is visited in the search, if it satisfied the
length constraint then the all its child nodes are
added to the schedule. These nodes are scored with
the score for the summary thus far plus a heuristic
term. For example, the value of 4+1.5=5.5 for the
{1} node arises from a score of 4 plus a heuristic of
(7? 5) ? 34 = 1.5, reflecting the additional score that
would arise if it were to use half of the next sentence
to finish the summary. Note that in finding the best
two summaries the search process did not need to
instantiate the full search graph.
To test the efficacy of A* search with each of the
different heuristic functions, we now present empir-
ical runtime results. We used the training data as
described in Section 5.2 and for each document set
start
(4+1.5,{1},F)
+1 (3+2,{2},F)+2 (2+2,{3},F)+3
(1+0,{4},F)+4
(0,{},T)
finish
(7+0,{1,2},F)+2 (6+0,{1,3},F)+3
(5+0,{1,4},F)+4
(5+0,{2,3},F)+3
(4+0,{2,4},F)+4
(5,{1,4},T)finish
(6+0,{2,3,4},F)+4
(5,{2,3},T)finish
Figure 1: Example of the A* search graph created to find
the two top scoring summaries of length ? 7 when sum-
marising four sentences with scores of 4, 3, 2 and 1 re-
spectively and lengths of 5, 4, 3 and 1 respectively. The
h1 heuristic was used and the score and heuristic scores
are shown separately for clarity. Bold nodes were visited
while dashed nodes were visited but found to exceed the
length constraint.
generated the 100-best summaries with word limit
L = 200. Figure 2 shows the number of nodes
and edges visited by A* search, reflecting the space
and time cost of the algorithm, as a function of the
number of sentences in the document set being sum-
marised. All three heuristics shown an empirical
increase in complexity that is roughly linear in the
document size, although there are some notable out-
liers, particularly for the uniform heuristic. Surpris-
ingly the aggregated heuristic, h2, is not consider-
ably more efficient than the uniform heuristic h1,
despite bounding the cost more precisely. However,
the aggregated+final heuristic, h3, consistently out-
performs the other two methods. For this reason we
have used h3 in all subsequent experimentation.
4 Training
We frame the training problem as one of finding
model parameters, ?, such that the predicted out-
put, y? closely matches the gold standard, r.4 The
quality of the match is measured using an automatic
evaluation metric. We adopt the standard machine
learning terminology of loss functions, which mea-
sure the degree of error in the prediction, ?(y?, r).
In our case the accuracy is measured by the ROUGE
4The gold standard is typically an abstractive summary, and
as such it is usually impossible for an extractive summarizer to
match it exactly.
485
ll
l
l
l
l
l
l l
l
l
l
l
l
l
ll
l
l
l
l
lll
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l l l
l
ll l
l l
l
l
l
l
l
l
l
l
l l
ll
l l
l
ll
l
l
l
ll
l
l
ll
l
l
l
ll
ll
l l
l
l
l
l
l
l l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
5 10 20 50 100 200 500 1000 2000
1e+
02
1e+
03
1e+
04
1e+
05
1e+
06
sentences in document set
tota
l edg
es a
nd n
odes
l uniformaggregatedaggregated+final
Figure 2: Efficiency of A* search search is roughly linear
in the number of sentences in the document set. The y
axis measures the search graph size in terms of the num-
ber of edges in the schedule and the number of nodes
visited. Measured with the final parameters after training
to optimise ROUGE-2 with the three different heuristics
and expanding five nodes in each step.
score, R, and the loss is simply 1 - R. The training
problem is to solve
?? = arg min
?
?(y?, r) , (4)
where with a slight abuse of notation, y? and r are
taken to range over the corpus of many document-
sets and summaries.
To optimise the weights we use the minimum er-
ror rate training (MERT) technique (Och, 2003), as
used for training statistical machine translation sys-
tems. This approach is a first order optimization
method using Powell search to find the parameters
which minimise the loss on the training data. MERT
requires n-best lists which it uses to approximate
the full space of possible outcomes. We use the
A* search algorithm to construct these n-best lists,5
and use MERT to optimise the ROUGE score on the
training set for the R-1, R-2 and R-SU4 variants of
the metric.
5We used n = 100 in our experiments.
5 Experimental settings
In this section we describe the features for which we
learn weights. We also describe the input data used
in training and testing.
5.1 Summarization system
The summarizer we use is an extractive, query-based
multi-document summarization system. It is given
two inputs: a query (place name) associated with an
image and a set of documents. The summarizer uses
the following features, as reported in previous work
(Edmundson, 1969; Brandow et al, 1995; Radev et
al., 2001; Conroy et al, 2005; Aker and Gaizauskas,
2009; Aker and Gaizauskas, 2010a):
? querySimilarity: Sentence similarity to the
query (cosine similarity over the vector repre-
sentation of the sentence and the query).
? centroidSimilarity: Sentence similarity to the
centroid. The centroid is composed of the 100
most frequently occurring non stop words in
the document collection (cosine similarity over
the vector representation of the sentence and
the centroid). For each word/term in the vec-
tor we store a value which is the product of
the term frequency in the document and the in-
verse document frequency, a measurement of
the term?s distribution over the set of docu-
ments (Salton and Buckley, 1988).
? sentencePosition: Position of the sentence
within its document. The first sentence in the
document gets the score 1 and the last one gets
1
n where n is the number of sentences in the
document.
? inFirst5: Binary feature indicating whether the
sentence occurs is one of the first 5 sentences
of the document.
? isStarter: A sentence gets a binary score if it
starts with the query term (e.g. Westminster
Abbey, The Westminster Abbey, The Westmin-
ster or The Abbey) or with the object type, e.g.
The church. We also allow gaps (up to four
words) between the and the query/object type
to capture cases such as The most magnificent
abbey, etc.
? LMProb: The probability of the sentence un-
der a unigram language model. We trained
a separate language model on Wikipedia arti-
cles about locations for each object type, e.g.,
486
church, bridge, etc. When we generate a sum-
mary about a location of type church, for in-
stance, then we apply the church language
model on the related input documents related
to the location.6
? sentenceCount: Each sentence gets assigned a
value of 1. This feature is used to learn whether
summaries with many sentences are better than
summaries with few sentences or vice versa.
? wordCount: Number of words in the summary,
to decide whether the model should favor long
summaries or short ones.
5.2 Data
For training and testing we use the freely avail-
able image description corpus described in Aker and
Gaizauskas (2010b). The corpus is based around
289 images of static located objects (e.g Eiffel
Tower, Mont Blanc) each with a manually assigned
place name and object type category (e.g. church,
mountain). For each place name there are up to
four model summaries that were created manually
after reading existing image descriptions taken from
the VirtualTourist travel community web-site. Each
summary contains a minimum of 190 and a maxi-
mum of 210 words. We divide this set of 289 place
names into training and testing sets. Both sets are
described in the following subsections.
Training We use 184 place names from the 289
set for training feature weights. For each train-
ing place name we gather all descriptions associ-
ated with it from VirtualTourist. We compute for
each sentence in each description a ROUGE score
by comparing the sentence to those included in the
model summaries for that particular place name and
retaining the highest score. Table 1 gives some de-
tails about this training data.
We use ROUGE as a metric to maximize be-
cause it is also used in DUC7 and TAC.8 How-
ever, it should be noted that any automatic metric
could be used instead of ROUGE. In particular we
use ROUGE 1 (R-1), ROUGE 2 (R-2) and ROUGE
SU4 (R-SU4). R-1 and R-2 compute the number
6For our training and testing sets we manually assigned each
location to its corresponding object type (Aker and Gaizauskas,
2009).
7http://duc.nist.gov/
8http://www.nist.gov/tac/
Max Min Avg
Sentences/place 1724 3 260
Words/sentence 37 3 17
Table 1: The training input data contains 184 place
names with 42333 sentences in total. The numbers in
the columns give detail about the number of sentences
for each place and the lengths of the sentences.
Max Min Avg
Documents/place 20 5 12
Sentences/place 1716 15 132
Sentences/document 275 1 10
Words/sentence 211 1 20
Table 2: In domain test data. The numbers in the columns
give detail about the number of documents (descriptions)
for each place, number of sentences for each place and
document (description) and the lengths of the sentences.
of uni-gram and bi-gram overlaps, respectively, be-
tween the automatic and model summaries. R-SU4
allows bi-grams to be composed of non-contiguous
words, with a maximum of four words between the
bi-grams.
Testing For testing purposes we use the rest of
the place names (105) from the 289 place name
set. For each place name we use a set of input
documents, generate a summary from these docu-
ments using our summarizer and compare the results
against model summaries of that place name using
ROUGE. We experimented with two different input
document types: out of domain and in domain.
The in domain documents are the VirtualTourist
original image descriptions from which the model
summaries were derived. As with the training set
we take all place name descriptions for a particular
place and use them as input documents to our sum-
marizer. Table 2 summarizes these input documents.
The out of domain documents are retrieved from
the web. Compared to the in domain documents
these documents should more challenging to sum-
marize because they will contain different kinds
of documents to those seen in training. For each
place name we retrieved the top ten related web-
documents using the Yahoo! search engine with the
place name as a query. The text from these docu-
ments is extracted using an HTML parser and passed
to the summarizer. Table 3 gives an overview of this
data.
487
Max Min Avg
Sentences/place 1773 55 328
Sentence/document 874 1 32
Words/sentence 236 1 21
Table 3: Out of domain test data. The numbers in the
columns give detail about the number of sentences for
each place and document and the lengths of the sentences.
6 Results
To evaluate our approach we used two different as-
sessment methods: ROUGE (Lin, 2004) and manual
readability. In the following we present the results
of each assessment.
6.1 Automatic Evaluation using ROUGE
We report results for training and testing. In
both training and testing we distinguish between
three different summaries: wordLimit, sentence-
Limit and regression. WordLimit and sentenceLimit
summaries are the ones generated using the model
trained by MERT. As described in section 4 we
trained the summariser using the A* search decoder
to maximise the ROUGE score of the best scoring
summaries. We used the heuristic function h3 in
A* search because it is the best performing heuris-
tic, and 100-best lists. To experiment with differ-
ent summary length conditions we differentiate be-
tween summaries with a word limit (wordLimit, set
to 200 words) and summaries containing N number
of sentences (sentenceLimit) as stop condition in A*
search. We set N so that in both wordLimit and sen-
tenceLimit summaries we obtain more or less the
same number of words (because our training data
contains on average 17 words for each word we set
N to 12, 12*17=194). However, this is only the case
in the training. In the testing for both wordLimit and
sentenceLimit we generate summaries with the same
word limit constraint which allows us to have a fair
comparison between the ROUGE recall scores.
The regression summaries are our baseline. In
these summaries the sentences are ranked based on
the weighted features produced by Support Vec-
tor Regression (SVR).9 Ouyang et al (2010) use
multi-document summarization and linear regres-
sion methods to rank sentences in the documents.
As regression model they used SVR and showed
9We use the term regression to refer to SVR.
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.5792 0.3176 0.3580
R-2 0.5656 0.3208 0.3510
R-SU4 0.5688 0.3197 0.3585
sentenceLimit
R-1 0.5915 0.3507 0.3881
R-2 0.5783 0.3601 0.3890
R-SU4 0.5870 0.3546 0.3929
regression
R-1 0.4993 0.1946 0.2448
R-2 0.4833 0.1949 0.2413
R-SU4 0.5009 0.2031 0.2562
Table 4: ROUGE scores obtained on the training data.
that it out-performed classification and Learning To
Rank methods on the DUC 2005 to 2007 data. For
comparison purpose we use SVR as a baseline sys-
tem for learning feature weights. It should be noted
that these weights are learned based on single sen-
tences. However, to have a fair comparison between
all our summary types we use these weights to gen-
erate summaries using the A* search with the word
limit as constraint. We do this for reporting both for
training and testing results.
The results for training are shown in Table 4. The
table shows ROUGE recall numbers obtained by
comparing model summaries against automatically
generated summaries on the training data. Because
in training we used three different metrics (R-1, R-2,
R-SU4) to train weights we report results for each of
these three different ROUGE metrics.
In Table 4 we can see that the scores for wordLimit
and sentenceLimit type summaries are always at
maximum on the metric they were trained on (this
can be observed by following the main diagonal of
the result matrix). This confirms that MERT is max-
imizing the metric for which it was trained. How-
ever, this is not the case for regression results. The
scores obtained with R-SU4 metric trained weights
achieve higher scores on R-1 and R-2 compared to
the scores obtained using weights trained on those
metrics. This is most likely due to SVR being
trained on sentences rather than over entire sum-
maries, and thereby not adequately optimising the
metric used for evaluation.
The results for testing are shown in Tables 5 and
6. As with the training setting we report ROUGE re-
call scores. We use the testing data described in sec-
tion 5.2 for this setting. However, because we have
two different input document sets we report sepa-
rate results for each of these (Table 5 shows result
for in domain data and Table 6 shows result for out
488
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.3733 0.0842 0.1399
R-2 0.3731 0.0842 0.1402
R-SU4 0.3627 0.0794 0.1340
sentenceLimit
R-1 0.3664 0.0774 0.1321
R-2 0.3559 0.0717 0.1251
R-SU4 0.3629 0.0778 0.1312
regression
R-1 0.3431 0.0669 0.1229
R-2 0.2934 0.0560 0.1043
R-SU4 0.3417 0.0668 0.1226
Table 5: ROUGE scores obtained on the testing data. The
automated summaries are generated using the in domain
input documents.
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.3758 0.0882 0.1421
R-2 0.3755 0.0895 0.1423
R-SU4 0.369 0.0812 0.137
sentenceLimit
R-1 0.3541 0.0693 0.1226
R-2 0.3426 0.0638 0.1157
R-SU4 0.3573 0.073 0.1251
regression
R-1 0.3392 0.0611 0.1179
R-2 0.3422 0.0606 0.1164
R-SU4 0.3413 0.0606 0.1176
Table 6: ROUGE scores obtained on the testing data. The
automated summaries are generated using the out of do-
main input documents.
of domain data). Again as with the training setting
we report results for the different metrics (R-1, R-2,
R-SU4) separately.
From Table 5 we can see that the wordLimit sum-
maries score highest compared to the other two types
of summaries. This is different from the train-
ing results where sentenceLimit summary type sum-
maries are the top scoring ones. As mentioned ear-
lier the sentenceLimit summaries contain exactly 12
sentences, where on average each sentence in the
training data has 17 words. We picked 12 sen-
tences to achieve roughly the same word limit con-
straint (12 ? 17 = 204) so they can be compared
to the wordLimit and regression type summaries.
However, these sentenceLimit summaries have an
average of 221 words, which explains the higher
ROUGE recall scores seen in training compared to
testing (where a 200 word limit was imposed).
The wordLimit summaries are significantly better
than the scores from the other summary types ir-
respective of the evaluation metric.10 It should be
10Significance is reported at level p < 0.001. We used
Wilcoxson signed ranked test to perform significance.
noted that these summaries are the only ones where
the training and testing had the same condition in
A* search concerning the summary word limit con-
straint. The scores in sentenceLimit type summaries
are significantly lower than wordLimit summaries,
despite using MERT to learn the weights. This
shows that training the true model is critical for
getting good accuracy. The regression type sum-
maries achieved the worst ROUGE metric scores.
The weights used to generate these summaries were
trained on single sentences using SVR. These results
indicate that if the goal is to generate high scoring
summaries under a length limit in testing, then the
same constraint should also be used in training.
From Table 5 and 6 we can see that the summaries
obtained from VirtualTourist captions (in domain
data) score roughly the same as the summaries gen-
erated using web-documents (out of domain data) as
input. A possible explanation is that in many cases
the VirtualTourist original captions contain text from
Wikipedia articles, which are also returned as results
from the web search. Therefore the web-document
sets included similar content to the VirtualTourist
captions.
6.2 Manual Evaluation
We also evaluated our summaries using a readabil-
ity assessment as in DUC and TAC. DUC and TAC
manually assess the quality of automatically gener-
ated summaries by asking human subjects to score
each summary using five criteria ? grammaticality,
redundancy, clarity, focus and coherence criteria.
Each criterion is scored on a five point scale with
high scores indicating a better result (Dang, 2005).
For this evaluation we used the best scoring sum-
maries from the wordLimit summary type (R-1, R-2
and R-SU4) generated using web-documents (out of
domain documents) as input. We also evaluate the
regression summary types generated using the same
input documents to investigate the correlation be-
tween high and low ROUGE metric scores to man-
ual evaluation ones. From the regression summary
type we only use summaries under the R2 and RSU4
trained models.
In total we evaluated five different summary types
(three from wordLimit and two from regression).
For each type we randomly selected 30 place names
and asked three people to assess the summaries for
these place names. Each person was shown all 150
489
Criterion wordLimit regression
R1 R2 RSU4 R2 RSU4
clarity 4.03 3.92 3.99 3.00 2.92
coherence 3.31 3.06 2.99 2.12 1.88
focus 3.79 3.56 3.54 2.44 2.29
grammaticality 4.21 4.13 4.13 3.93 3.87
redundancy 4.19 4.33 4.41 4.47 4.44
Table 7: Manual evaluation results for the wordLimit (R1,
R2, RSU4) and regression (R2, RSU4) summary types.
The numbers in the columns are the average scores.
summaries (30 from each summary type) in a ran-
dom way and was asked to assess them according to
the DUC and TAC manual assessment scheme. The
results are shown in Table 7.11
From Table 7 we can see that overall the
wordLimit type summaries perform better than the
regression ones. For each metric in regression sum-
mary types (R-2 and R-SU4) we compute the sig-
nificance of the difference with the same metrics
in wordLimit summary types.12 The results for the
clarity, coherence and focus criteria in wordLimit
summaries are significantly better than in regression
ones (p<0.001) irrespective of the training metric.
These results concur with the automatic evaluation
results as described in section 6.1. However, this
is not the case for the grammaticality and redun-
dancy criteria. Although in regression type sum-
maries the scores for the grammaticality criterion
are lower than those in wordLimit summaries the
difference is not significant. Furthermore, we can
see that the redundancy scores for regression sum-
maries are slightly higher than those for wordLimit
summaries.
One reason for these differences might be the
way we trained feature weights for wordLimit and
regression summaries. As mentioned above, fea-
ture weights for wordLimit summaries are trained
using summaries with a specific word limit con-
straint, whereas the weights for the regression sum-
maries are learned using single sentences. Maxi-
mizing the ROUGE metrics using ?final or output
11We computed the agreement between the users using intra
class correlation with Cronbach?s Alpha where the correlation
coefficient ranges between 0 and 1. Numbers close to 1 indicate
high correlation and numbers close to 0 indicate low correlation.
For the clarity criterion the assessors? correlation coefficient is
0.547, for coherence 0.687, for focus 0.688, for grammaticality
0.232 and for redundancy 0.453.
12We compute significance test for the manual evaluation re-
sults using ? square.
like summaries? will lead to a higher content agree-
ment between the training and the model summaries
whereas this is not guaranteed with single sentences.
With single sentences we have only a guarantee for
high content overlap between single training and
model sentences. However, when these sentences
are combined into summaries it is not guaranteed
that these summaries will also have high content
overlap with the entire model ones. Therefore we
believe if there is a high content agreement between
the training and model summaries this could lead to
more readable summaries. However, as we can see
from Table 7 this hypothesis does not hold for all
criteria. In case of the redundancy criterion we have
compared to wordLimit summary type high scores
in regression summaries although wordLimit sum-
maries are significantly better than regression ones
when it concerns the ROUGE scores. Thus it is
likely that by aggressively optimising the ROUGE
metric the model learns to game the metric, which
does not penalise redundancy in the summaries.
As such it may no longer possible to extrapolate
trends from earlier correlation studies against human
judgements (Lin, 2004).
To minimize redundancy in summaries it is nec-
essary to also take into consideration global features
addressing the linguistic aspects of the summaries.
Furthermore, instead of ROUGE recall scores which
do not take the repetition of information into consid-
eration, ROUGE precision scores could be used as a
metric in order to minimize the redundant content in
the summaries.
7 Conclusion
In this paper we have proposed an A* search ap-
proach for generating a summary from a ranked list
of sentences and learning feature weights for a fea-
ture based extractive multi-document summariza-
tion system. We developed an algorithm to learn
optimize an arbitrary metric and showed that our
approach significantly outperforms state of the art
techniques. Furthermore, we highlighted the impor-
tance of uniformity in training and testing and ar-
gued that if the goal is to generate high scoring sum-
maries under a length limit in testing, then the same
constraint should also be used in training.
In this paper we experimented with sentence-local
features. In the future we plan to expand this fea-
ture set with global features, especially ones mea-
490
suring lexical diversity in the summaries to reduce
the redundancy in them. We will investigate vari-
ous ways of incorporating these global features into
our A* search. However this will incur an additional
computational cost over a purely local feature model
and therefore may necessitate using an approximate
beam search. We also plan to investigate using other
metrics in training in order to reduce redundant in-
formation in the summaries. Finally, we have made
our summarizer publicly available as open-source
software.13
References
A. Aker and R. Gaizauskas. 2009. Summary Gener-
ation for Toponym-Referenced Images using Object
Type Language Models. International Conference
on Recent Advances in Natural Language Processing
(RANLP) September 14-16, 2009, Borovets, Bulgaria.
A. Aker and R. Gaizauskas. 2010a. Generating im-
age descriptions using dependency relational patterns.
Proc. of the ACL 2010, Upsala, Sweden.
A. Aker and R. Gaizauskas. 2010b. Model Summaries
for Location-related Images. In Proc. of the LREC-
2010 Conference.
R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection* 1. Information Processing & Management,
31(5):675?685.
J.M. Conroy, J.D. Schlesinger, and J.G. Stewart. 2005.
CLASSY query-based multi-document summariza-
tion. Proc. of the 2005 Document Understanding
Workshop, Boston.
H.T. Dang. 2005. Overview of DUC 2005. DUC 05
Workshop at HLT/EMNLP.
H. Edmundson, P. 1969. New Methods in Automatic
Extracting. Journal of the Association for Computing
Machinery, 16:264?285.
M.A. Hearst. 1997. TextTiling: segmenting text into
multi-paragraph subtopic passages. Computational
linguistics, 23(1):33?64.
C-Y. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. Text Summarization Branches Out:
Proc. of the ACL-04 Workshop, pages 74?81.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Publishing Company.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. Proc. of the 41st Annual Meeting
on Association for Computational Linguistics-Volume
1, page 167.
13Available from http://www.dcs.shef.ac.uk/
?tcohn/a-star
Y. Ouyang, W. Li, S. Li, and Q. Lu. 2010. Applying
regression models to query-focused multi-document
summarization. Information Processing & Manage-
ment.
D.R. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001.
Experiments in single and multi-document summa-
rization using MEAD. Document Understanding Con-
ference.
K. Riedhammer, D. Gillick, B. Favre, and D. Hakkani-T
?ur. 2008. Packing the meeting summarization knap-
sack. Proc. Interspeech, Brisbane, Australia.
S.J. Russell, P. Norvig, J.F. Canny, J. Malik, and D.D.
Edwards. 1995. Artificial intelligence: a modern ap-
proach. Prentice hall Englewood Cliffs, NJ.
H. Saggion. 2005. Topic-based Summarization at
DUC 2005. Document Understanding Conference
(DUC05).
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management: an International Journal,
24(5):513?523.
491
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204?1213,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Unsupervised Induction of Tree Substitution Grammars
for Dependency Parsing
Phil Blunsom
Computing Laboratory
University of Oxford
Phil.Blunsom@comlab.ox.ac.uk
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Abstract
Inducing a grammar directly from text is
one of the oldest and most challenging tasks
in Computational Linguistics. Significant
progress has been made for inducing depen-
dency grammars, however the models em-
ployed are overly simplistic, particularly in
comparison to supervised parsing models. In
this paper we present an approach to depen-
dency grammar induction using tree substi-
tution grammar which is capable of learn-
ing large dependency fragments and thereby
better modelling the text. We define a hi-
erarchical non-parametric Pitman-Yor Process
prior which biases towards a small grammar
with simple productions. This approach sig-
nificantly improves the state-of-the-art, when
measured by head attachment accuracy.
1 Introduction
Grammar induction is a central problem in Compu-
tational Linguistics, the aim of which is to induce
linguistic structures from an unannotated text cor-
pus. Despite considerable research effort this un-
supervised problem remains largely unsolved, par-
ticularly for traditional phrase-structure parsing ap-
proaches (Clark, 2001; Klein and Manning, 2002).
Phrase-structure parser induction is made difficult
due to two types of ambiguity: the constituent struc-
ture and the constituent labels. In particular the con-
stituent labels are highly ambiguous, firstly we don?t
know a priori how many there are, and secondly la-
bels that appear high in a tree (e.g., an S category
for a clause) rely on the correct inference of all the
latent labels below them. However recent work on
the induction of dependency grammars has proved
more fruitful (Klein and Manning, 2004). Depen-
dency grammars (Mel?c?uk, 1988) should be easier to
induce from text compared to phrase-structure gram-
mars because the set of labels (heads) are directly
observed as the words in the sentence.
Approaches to unsupervised grammar induction,
both for phrase-structure and dependency grammars,
have typically used very simplistic models (Clark,
2001; Klein and Manning, 2004), especially in com-
parison to supervised parsing models (Collins, 2003;
Clark and Curran, 2004; McDonald, 2006). Sim-
ple models are attractive for grammar induction be-
cause they have a limited capacity to overfit, how-
ever they are incapable of modelling many known
linguistic phenomena. We posit that more complex
grammars could be used to better model the unsuper-
vised task, provided that active measures are taken
to prevent overfitting. In this paper we present an
approach to dependency grammar induction using
a tree-substitution grammar (TSG) with a Bayesian
non-parametric prior. This allows the model to learn
large dependency fragments to best describe the text,
with the prior biasing the model towards fewer and
smaller grammar productions.
We adopt the split-head construction (Eisner,
2000; Johnson, 2007) to map dependency parses to
context free grammar (CFG) derivations, over which
we apply a model of TSG induction (Cohn et al,
2009). The model uses a hierarchical Pitman-Yor
process to encode a backoff path from TSG to CFG
rules, and from lexicalised to unlexicalised rules.
Our best lexicalised model achieves a head attach-
ment accuracy of of 55.7% on Section 23 of the WSJ
data set, which significantly improves over state-of-
the-art and far exceeds an EM baseline (Klein and
Manning, 2004) which obtains 35.9%.
1204
CFG Rule DMV Distribution Description
S? LH HR p(root = H) The head of the sentence is H .
LH ? Hl p(STOP |dir = L, head = H, val = 0) H has no left children.
LH ? L1H p(CONT |dir = L, head = H, val = 0) H has at least one left child.
L?H ? Hl p(STOP |dir = L, head = H, val = 1) H has no more left children.
L?H ? L
1
H p(CONT |dir = L, head = H, val = 1) H has another left child.
HR? Hr p(STOP |dir = R, head = H, val = 0) H has no right children.
HR? HR1 p(CONT |dir = R, head = H, val = 0) H has at least one right child.
HR? ? Hr p(STOP |dir = R, head = H, val = 1) H has no more right children.
HR? ? HR1 p(CONT |dir = R, head = H, val = 1) H has another right child.
L1H ? LC CMH? p(C|dir = L, head = H) C is a left child of H .
HR1? H?MC CR p(C|dir = R, head = H) C is a right child of H .
CMH? ? CR L?H p = 1 Unambiguous
H?MC ? HR? LC p = 1 Unambiguous
Table 1: The CFG-DMV grammar schema. Note that the actual CFG is created by instantiating these templates with
part-of-speech tags observed in the data for the variables H and C. Valency (val) can take the value 0 (no attachment
in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent;
superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more
children and X? that it has already attached a child.
2 Background
The most successful framework for unsupervised
dependency induction is the Dependency Model
with Valence (DMV) (Klein and Manning, 2004).
This model has been adapted and extended by a
number of authors and currently represents the state-
of-the-art for dependency induction (Cohen and
Smith, 2009; Headden III et al, 2009). Eisner
(2000) introduced the split-head algorithm which
permits efficient O(|w|3) parsing complexity by
replicating (splitting) each terminal and processing
left and right dependents separately. We employ
the related fold-unfold representation of Johnson
(2007) that defines a CFG equivalent of the split-
head parsing algorithm, allowing us to easily adapt
CFG-based grammar models to dependency gram-
mar. Table 1 shows the equivalent CFG grammar for
the DMV model (CFG-DMV) using the unfold-fold
transformation. The key insight to understanding the
non-terminals in this grammar is that the subscripts
encode the terminals at the boundaries of the span
of that non-terminal. For example the non-terminal
LH encodes that the right most terminal spanned
by this constituent is H (and the reverse for HR),
while AMB encodes that A and B are the left-most
and right-most terminals of the span. The ? and 1
superscripts are used to encode the valency of the
head, both indicate that the head has at least one
attached dependent in the specified direction. This
grammar allows O(|w|3) parsing complexity which
follows from the terminals of the dependency tree
being observed, such that each span of the parse
chart uniquely specifies its possible heads (either the
leftmost, rightmost or both) and therefore the num-
ber of possible non-terminals for each span is con-
stant. The transform is illustrated in figures 1a and
1c which show the CFG tree for an example sentence
and the equivalent dependency tree.
Normally DMV based models have been trained
on part-of-speech tags of the words in a sentence,
rather than the words themselves. Headden III et al
(2009) showed that performance could be improved
by including high frequency words as well as tags
in their model. In this paper we refer to such mod-
els as lexicalised; words which occur more than one
hundred times in the training corpus are represented
by a word/tag pair, while those less frequent are rep-
resented simply by their tags. We are also able to
show that this basic approach to lexicalisation im-
proves the performance of our models.
1205
SLhates[V ]
L1hates[V ]
LN
Nl
NMhates[V]?
NR
Nr
L?hates[V ]
hates[V]l
hates[V ]R
hates[V ]R1
hates[V]?MN
hates[V ]R?
hates[V]r
LN
Nl
NR
Nr
(a) A TSG-DMV derivation for the sentence George hates broc-
coli. George and broccoli occur less than the lexicalisation cutoff
and are thus represented by the part-of-speech N, while hates is
common and therefore is represented by a word/tag pair. Bold
nodes indicate frontier nodes of elementary trees.
S
Lhates[V ]
L1hates[V ]
LN NMhates[V]?
hates[V ]R
hates[V ]R1
hates[V]?MN NR
(b) A TSG-DMV elementary rule from Figure 1a. This rule en-
codes a dependency between the subject and object of hates that
is not present in the CFG-DMV. Note that this rule doesn?t re-
strict hates, or its arguments, to having a single left and right
child. More dependents can be inserted using additional rules
below the M/L/R frontier non-terminals.
George hates broccoli ROOT
(c) A traditional dependency tree representation of the parse tree
in Figure 1a before applying the lexicalisation cutoff.
Figure 1: TSG-DMV representation of dependency trees.
3 Lexicalised TSG-DMV
The models we investigate in this paper build upon
the CFG-DMV by defining a Tree Substitution
Grammar (TSG) over the space of CFG rules. A
TSG is a 4-tuple,G = (T,N, S,R), where T is a set
of terminal symbols, N is a set of non-terminal sym-
bols, S ? N is the distinguished root non-terminal
and R is a set of productions (rules). The produc-
tions take the form of elementary trees ? tree frag-
ments of height ? 1, where each internal node is
labelled with a non-terminal and each leaf is la-
belled with either a terminal or a non-terminal. Non-
terminal leaves are called frontier non-terminals and
form the substitution sites in the generative process
of creating trees with the grammar.
A derivation creates a tree by starting with the
root symbol and rewriting (substituting) it with an
elementary tree, then continuing to rewrite fron-
tier non-terminals with elementary trees until there
are no remaining frontier non-terminals. We can
represent derivations as sequences of elementary
trees, e, by specifying that during the generation of
the tree each elementary tree is substituted for the
left-most frontier non-terminal. Figure 1a shows a
TSG derivation for the dependency tree in Figure 1c
where bold nonterminal labels denote substitution
sites (root/frontier nodes in the elementary trees).
The probability of a derivation, e, is the product
of the probabilities of its component rules,
P (e) =
?
c?e?e
P (e|c) . (1)
where each rewrite is assumed conditionally inde-
pendent of all others given its root nonterminal, c =
root(e). The probability of a tree, t, and string of
words, w, are
P (t) =
?
e:tree(e)=t
P (e) and P (w) =
?
t:yield(t)=w
P (t) ,
respectively, where tree(e) returns the tree for the
derivation e and yield(t) returns the string of termi-
nal symbols at the leaves of t.
A Probabilistic Tree Substitution Grammar
(PTSG), like a PCFG, assigns a probability to each
rule in the grammar, denoted P (e|c). The probabil-
ity of a derivation, e, is the product of the proba-
bilities of its component rules. Estimating a PTSG
requires learning the sufficient statistics for P (e|c)
in (1) based on a training sample. Parsing involves
1206
finding the most probable tree for a given string
(argmaxt P (t|w)). This is typically approximated
by finding the most probable derivation which can
be done efficiently using the CYK algorithm.
3.1 Model
In this work we propose the Tree Substitution Gram-
mar Dependency Model with Valence (TSG-DMV).
We define a hierarchical non-parametric TSG model
on the space of parse trees licensed by the CFG
grammar in Table 1. Our model is a generalisa-
tion of that of Cohn et al (2009) and Cohn et al
(2011). We extend those works by moving from a
single level Dirichlet Process (DP) distribution over
rules to a multi-level Pitman-Yor Process (PYP), and
including lexicalisation. The PYP has been shown
to generate distributions particularly well suited to
modelling language (Teh, 2006; Goldwater et al,
2006). Teh (2006) used a hierarchical PYP to model
backoff in language models, we leverage this same
capability to model backoff in TSG rules. This ef-
fectively allows smoothing from lexicalised to un-
lexicalised grammars, and from TSG to CFG rules.
Here we describe our deepest model which has
a four level hierarchy, depicted graphically in Table
2. In Section 5 we evaluate different subsets of this
hierarchy. The topmost level of our model describes
lexicalised elementary elementary fragments (e) as
produced by a PYP,
e|c ? Gc
Gc|ac, bc,P
lcfg ? PYP(ac, bc,P
lcfg(?|c)) ,
where ac and bc control the strength of the backoff
distribution Plcfg. The space of lexicalised TSG rules
will inevitably be very sparse, so the base distribu-
tion Plcfg backs-off to calculating the probability of
a TSG rules as the product of the CFG rules it con-
tains, multiplied by a geometric distribution over the
size of the rule.
Plcfg(e|c) =
?
f?F(e)
sfc
?
i?I(e)
(1? sic)
?A(lex-cfg-rules(e|c))
?|c ? Ac
Ac|a
lcfg
c , b
lcfg
c ,P
cfg ? PYP(alcfgc , b
lcfg
c ,P
cfg(?|c)),
where I(e) are the set of internal nodes in e exclud-
ing the root, F (e) are the set of frontier non-terminal
nodes, and ci is the non-terminal symbol for node
i and sc is the probability of stopping expanding a
node labelled c. The function lex-cfg-rules(e|c) re-
turns the CFG rules internal to e, each of the form
c? ? ?; each CFG rule is drawn from the back-
off distribution, Ac? . We treat sc as a parameter
which is estimated during training, as described in
Section 4.2.
The next level of backoff (Pcfg) removes the lexi-
calisation from the CFG rules, describing the gener-
ation of a lexicalised rule by first generating an un-
lexicalised rule from a PYP, then generating the lex-
icalisaton from a uniform distribution over words:1
Pcfg(?|c) = B(unlex(?)|unlex(c))
?
1
|w||?|
??|c? ? Bc?
Bc? |a
cfg
c? , b
cfg
c? ,P
sh ? PYP(acfgc? , b
cfg
c? ,P
sh(?|c?)),
where unlex(?) removes the lexicalisation from non-
terminals leaving only the tags.
The final base distribution over CFG-DMV rules
(Psh) is inspired by the skip-head smoothing model
of Headden III et al (2009). This model showed that
smoothing the DMV by removing the heads from the
CFG rules significantly improved performance. We
replicate this behavior through a final level in our hi-
erarchy which generates the CFG rules without their
heads, then generates the heads from a uniform dis-
tribution:
Psh(?|c) = C(drop-head(c? ?))?
1
|P |
?|c ? Cc
Cc|a
sh
c , b
sh
c ? PYP(a
sh
c , b
sh
c ,Uniform(?|c)),
where drop-head(?) removes the symbols that mark
the head on the CFG rules, and P is the set of part-
of-speech tags. Each stage of backoff is illustrated in
Table 2, showing the rules generated from the TSG
elementary tree in Figure 1b.
Note that while the supervised model of Cohn et
al. (2009) used a fixed back-off PCFG distribution,
this model implicitly infers this distribution within
1All unlexicalised words are actually given the generic UNK
symbol as their lexicalisation.
1207
Plcfg Pcfg Psh
S
Lhates[V ] hates[V ]R
Lhates[V ]
L1hates[V ]
S
LV V R
LV
L1V
S
L? ?R
L?
L1?
L1hates[V ]
LN NMhates[V]?
hates[V ]R
hates[V ]R
1
L1V
LN NMV ?
V R
V R1
L1?
LN NM??
?R
?R1
hates[V ]R
1
hates[V]?MN NR
V R1
V ?MN NR
?R1
??MN NR
Table 2: Backoff trees for the elementary tree in Figure 1b.
its hierarchy, essentially learning the DMV model
embedded in the TSG.
In this application to dependency grammar our
model is capable of learning tree fragments which
group CFG parameters. As such the model can learn
to condition dependency links on the valence, e.g. by
combining LH ? L1H and L
1
H ? LC CMH? rules
into a single fragment the model can learn a pa-
rameter that the leftmost child of H is C. By link-
ing together multiple L1H or HR
1 non-terminals the
model can learn groups of dependencies that occur
together, e.g. tree fragments representing the com-
plete preferred argument frame of a verb.
4 Inference
4.1 Training
To train our model we use Markov Chain Monte
Carlo sampling (Geman and Geman, 1984). Where
previous supervised TSG models (Cohn et al, 2009)
permit an efficient local sampler, the lack of an ob-
served parse tree in our unsupervised model makes
this sampler not applicable. Instead we use a re-
cently proposed blocked Metroplis-Hastings (MH)
sampler (Cohn and Blunsom, 2010) which exploits a
factorisation of the derivation probabilities such that
whole trees can be sampled efficiently. See Cohn
and Blunsom (2010) for details. That algorithm is
applied using a dynamic program over an observed
tree, the generalisation to our situation of an inside
pass over the space of all trees is straightforward.
A final consideration is the initialisation of the
sampler. Klein and Manning (2004) emphasised the
importance of the initialiser for achieving good per-
formance with their model. We employ the same
harmonic initialiser as described in that work. The
initial derivations for our sampler are the Viterbi
derivations under the CFG parameterised according
to this initialiser.
4.2 Sampling hyperparameters
We treat the hyper-parameters {(axc , b
x
c , sc) , c ? N}
as random variables in our model and infer their val-
ues during training. We choose quite vague priors
for each hyper-parameter, encoding our lack of in-
formation about their values.
We place prior distributions on the PYP discount
ac and concentration bc hyperparamters and sam-
ple their values using a slice sampler. We use the
range doubling slice sampling technique of (Neal,
2003) to draw a new sample of a?c from its condi-
tional distribution.2 For the discount parameters ac
we employ a uniform Beta distribution, as we have
no strong prior knowledge of what its value should
be (ac ? Beta(1, 1)). Similarly, we treat the concen-
tration parameters, bc, as being generated by a vague
gamma prior, bc ? Gamma(1, 1), and sample a new
value b?c using the same slice-sampling approach as
for ac:
P (bc|z) ? P (z|bc)? Gamma(bc|1, 1).
2We made use of the slice sampler included in
Mark Johnson?s Adaptor Grammar implementation
http://www.cog.brown.edu/?mj/Software.htm.
1208
Corpus Words Sentences
Sections 2-21 (|x| ? 10) 42505 6007
Section 22 (|x| ? 10) 1805 258
Section 23 (|x| ? 10) 2649 398
Section 23 (|x| ? ?) 49368 2416
Table 3: Corpus statistics for the training and testing data
for the TSG-DMV model. All models are trained on the
gold standard part-of-speech tags after removing punctu-
ation.
We use a vague Beta prior for the stopping probabil-
ities in Plcfg, sc ? Beta(1, 1).
All the hyper-parameters are resampled after ev-
ery 10th sample of the corpus derivations.
4.3 Parsing
Unfortunately finding the maximising parse tree for
a string under our TSG-DMV model is intractable
due to the inter-rule dependencies created by the
PYP formulation. Previous work has used Monte
Carlo techniques to sample for one of the maxi-
mum probability parse (MPP), maximum probabil-
ity derivation (MPD) or maximum marginal parse
(MMP) (Cohn et al, 2009; Bod, 2006). We take a
simpler approach and use the Viterbi algorithm to
calculate the MPD under an approximating TSG de-
fined by the last set of derivations sampled for the
corpus during training. Our results indicate that this
is a reasonable approximation, though the experi-
ence of other researchers suggests that calculating
the MMP under the approximating TSG may also
be beneficial for DMV (Cohen et al, 2008).
5 Experiments
We follow the standard evaluation regime for DMV
style models by performing experiments on the text
of the WSJ section of the Penn. Treebank (Marcus et
al., 1993) and reporting head attachment accuracy.
Like previous work we pre-process the training and
test data to remove punctuation, training our unlex-
icalised models on the gold-standard part-of-speech
tags, and including words occurring more than 100
times in our lexicalised models (Headden III et al,
2009). It is very difficult for an unsupervised model
to learn from long training sentences as they contain
a great deal of ambiguity, therefore the majority of
DMV based models have been trained on sentences
restricted in length to ? 10 tokens.3 This has the
added benefit of decreasing the runtime for exper-
iments. We present experiments with this training
scenario. The training data comes from sections 2-
21, while section 23 is used for evaluation. An ad-
vantage of our sampling based approach over pre-
vious work is that we infer all the hyperparameters,
as such we don?t require the use of section 22 for
tuning the model.
The models are evaluated in terms of head attach-
ment accuracy (the percentage of correctly predicted
head indexes for each token in the test data), on two
subsets of the testing data. Although we can argue
that unsupervised models are better learnt from short
sentences, it is much harder to argue that we don?t
then need to be able to parse long sentences with a
trained model. The most commonly employed test
set mirrors the training data by only including sen-
tences ? 10. In this work we focus on the accuracy
of our models on the whole of section 23, without
any pruning for length. The training and testing cor-
pora statistics are presented in Table 3. Subsequent
to the evaluation reported in Table 4 we use section
22 to report the correlation between heldout accu-
racy and the model log-likelihood (LLH) for ana-
lytic purposes.
As we are using a sampler during training, the re-
sult of any single run is non-deterministic and will
exhibit a degree of variance. All our reported results
are the mean and standard deviation (?) from forty
sampling runs.
5.1 Discussion
Table 4 shows the head attachment accuracy results
for our TSG-DMV, plus many other significant pre-
viously proposed models. The subset of hierarchical
priors used by each model is noted in brackets.
The performance of our models is extremely en-
couraging, particularly the fact that it achieves the
highest reported accuracy on the full test set by a
considerable margin. On the |w| ? 10 test set al the
TSG-DMVs are second only to the L-EVG model
of Headden III et al (2009). The L-EVG model
extends DMV by adding additional lexicalisation,
3See Spitkovsky et al (2010a) for an exception to this rule.
1209
Directed Attachment
Accuracy on WSJ23
Model |w| ? 10 |w| ? ?
Attach-Right 38.4 31.7
EM (Klein and Manning, 2004) 46.1 35.9
Dirichlet (Cohen et al, 2008) 46.1 36.9
LN (Cohen et al, 2008) 59.4 40.5
SLN, TIE V&N (Cohen and Smith, 2009) 61.3 41.4
DMV (Headden III et al, 2009) 55.7?=8.0 -
DMV smoothed (Headden III et al, 2009) 61.2?=1.2 -
EVG smoothed (Headden III et al, 2009) 65.0?=5.7 -
L-EVG smoothed (Headden III et al, 2009) 68.8?=4.5 -
Less is More (Spitkovsky et al, 2010a) 56.2 44.1
Leap Frog (Spitkovsky et al, 2010a) 57.1 45.0
Viterbi EM (Spitkovsky et al, 2010b) 65.3 47.9
Hypertext Markup (Spitkovsky et al, 2010c) 69.3 50.4
Adaptor Grammar (Cohen et al, 2010) 50.2 -
TSG-DMV (Pcfg) 65.9?=2.4 53.1?=2.4
TSG-DMV (Pcfg, Psh) 65.1?=2.2 51.5?=2.0
LexTSG-DMV (Plcfg, Pcfg) 67.2?=1.4 55.2?=2.2
LexTSG-DMV (Plcfg, Pcfg, Psh) 67.7?=1.5 55.7?=2.0
Supervised MLE (Cohen and Smith, 2009) 84.5 68.8
Table 4: Mean and variance for the head attachment accu-
racy of our TSG-DMV models (highlighted) with varying
backoff paths, and many other high performing models.
Citations indicate where the model and result were re-
ported. Our models labelled TSG used an unlexicalised
top level Gc PYP, while those labelled LexTSG used the
full lexicalised Gc.
valency conditioning, interpolated back-off smooth-
ing and a random initialiser. In particular Head-
den III et al (2009) shows that the random initialiser
is crucial for good performance, however this ini-
tialiser requires training 1000 models to select a sin-
gle best model for evaluation and results in consider-
able variance in test set performance. Note also that
our model exhibits considerably less variance than
those induced using this random initialiser, suggest-
ing that the combination of the harmonic initialiser
and blocked-MH sampling may be a more practica-
ble training regime.
The recently proposed Adaptor Grammar DMV
model of Cohen et al (2010) is similar in many
way to our TSG model, incorporating a Pitman Yor
prior over units larger than CFG rules. As such it
is surprising that our model is performing signif-
icantly better than this model. We can identify a
number of differences that may impact these results:
the Adaptor Grammar model is trained using vari-
ational inference with the space of tree fragments
truncated, while we employ a sampler which can
nominally explore the full space of tree fragments;
and the adapted tree fragments must be complete
subtrees (i.e. they don?t contain variables), whereas
our model can make use of arbitrary tree fragments.
An interesting avenue for further research would be
to extend the variational algorithm of Cohen et al
(2010) to our TSG model, possibly speeding infer-
ence and allowing easier parallelisation.
In Figure 2a we graph the model LLH on the train-
ing data versus the head attachment accuracy on the
heldout set. The graph was generated by running
160 models for varying numbers of samples and
evaluating their accuracy. This graph indicates that
the improvements in the posterior probability of the
model are correlated with the evaluation, though the
correlation is not as high as we might require in or-
der to use LLH as a model selection criteria similar
to Headden III et al (2009). Further refinements to
the model could improve this correlation.
The scaling perfomance of the model as the num-
ber of samples is increased is shown in Figure 2b.
Performance improves as the training data is sam-
pled for longer, and continues to trend upwards be-
yond 1000 samples (the point for which we?ve re-
ported results in Table 4). This suggests that longer
sampling runs ? and better inference techniques ?
could yield further improvements.
For further analysis Table 5 shows the accuracy
of the model at predicting the head for frequent
types, while Table 6 shows the performance on de-
pendencies of various lengths. We emphasise that
these results are for the single best performing sam-
pler run on the heldout corpus and there is consid-
erable variation in the analyses produced by each
sampler. Unsurprisingly, the model appears to be
more accurate when predicting short dependencies,
a result that is also reflected in the per type accura-
cies. The model is relatively good at identifying the
root verb in each sentence, especially those headed
by past tense verbs (VBD, was), and to a lesser de-
gree VBPs (are). Conjunctions such as and pose
a particular difficulty when evaluating dependency
models as the correct modelling of these remains a
1210
l
ll
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l l
l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
ll l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
ll
l l
l
l
l
l l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
ll
l
ll
l
l
l
l
?5.4 ?5.3 ?5.2 ?5.1 ?5.0 ?4.9 ?4.8
56
58
60
62
64
66
68
Perplexity vs. Accuracy Correlation
PYP.LLH
Dire
cted
.Atta
chm
ent.
Acc
urac
y
(a) Correlation (R2 = 0.2) between the training LLH of the
PYP Model and heldout directed head attachment accuracy
(WSJ Section 22, |w| ? 10) for LexTSG-DMV (Plcfg, Pcfg, Psh).
0 500 1000 1500 2000
59
60
61
62
63
64
65
66
Number of Samples vs. Accuracy
Samples
Dire
cted
 Atta
chm
ent 
Acc
urac
y
l
l
l
l
l
(b) Mean heldout directed head attachment accuracy (WSJ Sec-
tion 22, |w| ? 10) versus the number of samples used during
training for LexTSG-DMV (Plcfg, Pcfg, Psh).
Figure 2
contentious linguistic issue and it?s not clear what
the ?correct? analysis should be. Our model gets a
respectable 75% accuracy for and conjunctions, but
for conjunctions (CC) as a whole, the model per-
forms poorly (39%).
Table 7 list the most frequent TSG rules lexi-
calised with has. The most frequent rule is sim-
ply the single level equivalent of the DMV termi-
nal rule for has. Almost as frequent is rule 3, here
the grammar incorporates the terminal into a larger
elementary fragment, encoding that it is the head
of the past participle occuring immediately to it?s
right. This shows the model?s ability to learn the
verb?s argument position conditioned on both the
head and child type, something lacking in DMV.
Rule 7 further refines this preferred analysis for has
been by lexicalising both the head and child. Rules
(4,5,8,10) employ similar conditioning for proper
and ordinary nouns heading noun phrases to the
left of has. We believe that it is the ability of the
TSG to encode stronger constraints on argument po-
sitions that leads to the model?s higher accuracy
on longer sentences, while other models do well
on shorter sentences but relatively poorly on longer
ones (Spitkovsky et al, 2010c).
6 Conclusion
In this paper we have made two significant contri-
butions to probabilistic modelling and grammar in-
duction. We have shown that it is possible to suc-
cessfully learn hierarchical Pitman-Yor models that
encode deep and complex backoff paths over highly
structured latent spaces. By applying these models
to the induction of dependency grammars we have
also been able to advance the state-of-the-art, in-
creasing the head attachment accuracy on section 23
of the Wall Street Journal Corpus by more than 5%.
Further gains in performance may come from an
exploration of the backoff paths employed within the
model. In particular more extensive experimentation
with alternate priors and larger training data may al-
low the removal of the lexicalisation cutoff which is
currently in place to counter sparsity.
We envisage that in future many grammar for-
malisms that have been shown to be effective in su-
pervised parsing, such as categorial, unification and
tree adjoining grammars, will prove amenable to
unsupervised induction using the hierarchical non-
parametric modelling approaches we have demon-
strated in this paper.
1211
Count LexTSG-DMV Rules
1 94 L?has?V BZ ? (L?has?V BZ has-VBZl)
2 74 L1has?V BZ ? (L
1
has?V BZ (LNN L
1
NN ) NNMhas?V BZ? )
3 71 has?V BZ?MV BN ? (has?V BZ?MV BN (has?V BZR? has-VBZr) LV BN )
4 54 NNMhas?V BZ? ? (NNMhas?V BZ? NNR (L?has?V BZ has-VBZl))
5 36 NNMhas?V BZ? ? (NNMhas?V BZ? NNR L?has?V BZ)
6 36 has?V BZR? ? (has?V BZR? (has?V BZR1 has?V BZ?MV BN (V BNR VBNr)))
7 30 has?V BZ?Mbeen?V BN ? (has?V BZ?Mbeen?V BN (has?V BZR? has-VBZr) Lbeen?V BN )
8 27 NNPMhas?V BZ? ? (NNPMhas?V BZ? NNPR (L?has?V BZ has-VBZl))
9 25 has?V BZR ? (has?V BZR (has?V BZR1 has?V BZ?MNNS (NNSR NNSR1)))
10 18 L1has?V BZ ? (L
1
has?V BZ LNNP NNPMhas?V BZ? )
Table 7: The ten most frequent LexTSG-DMV rules in a final training sample that contain has.
References
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proc. of the 44th Annual Meeting of
the ACL and 21st International Conference on Compu-
tational Linguistics (COLING/ACL-2006), pages 865?
872, Sydney, Australia, July.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proc. of the
42nd Annual Meeting of the ACL (ACL-2004), pages
103?110, Barcelona, Spain.
Alexander Clark. 2001. Unsupervised induction of
stochastic context-free grammars using distributional
clustering. In ConLL ?01: Proceedings of the 2001
workshop on Computational Natural Language Learn-
ing, pages 1?8. Association for Computational Lin-
guistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In NAACL ?09: Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
74?82, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Daphne Koller, Dale
Schuurmans, Yoshua Bengio, and Lon Bottou, editors,
NIPS, pages 321?328. MIT Press.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked inference
in Bayesian tree substitution grammars. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, page To Appear, Uppsala,
Sweden.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In NAACL ?09: Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics on ZZZ, pages 548?556,
Morristown, NJ, USA. Association for Computational
Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2011. Inducing tree-substitution grammars. Journal
of Machine Learning Research. To Appear.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459?466.
MIT Press, Cambridge, MA.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
101?109, Boulder, Colorado, June.
1212
Child Tag Predicted Accuracy
Head Correct (%)
NN 181 0.64
NNP 130 0.71
DT 127 0.87
NNS 108 0.72
VBD 108 0.81
JJ 106 0.80
IN 81 0.55
RB 65 0.61
PRP 64 0.97
VBZ 47 0.80
VBN 36 0.86
VBP 30 0.77
CD 26 0.23
VB 25 0.68
the 42 0.88
was 29 0.97
The 25 0.83
of 18 0.78
a 18 0.90
to 17 0.50
in 16 0.89
is 15 0.79
n?t 15 0.83
were 12 0.86
are 11 0.92
It 11 1.00
for 9 0.64
and 9 0.75
?s 9 1.00
Table 5: Per tag type predicted count and accuracy,
for the most frequent 15 un/lexicalised tokens on the
WSJ Section 22 |w| ? 10 heldout set (LexTSG-DMV
(Plcfg,Pcfg,Psh)).
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 168?175, Prague, Czech Republic, June.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
128?135, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ?04: Proceedings
Distance Precision Recall F1
1 0.70 0.75 0.72
2 0.70 0.62 0.65
3 0.66 0.62 0.64
4 0.56 0.56 0.56
5 0.53 0.49 0.51
6 0.59 0.66 0.62
7 0.50 0.44 0.47
8 0.57 0.33 0.42
9 0.67 0.40 0.50
10 1.00 0.17 0.29
Table 6: Link distance precision, recall and f-score, on
the WSJ Section 22 |w| ? 10 heldout set.
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 478.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Igor? A. Mel?c?uk. 1988. Dependency Syntax: theory and
practice. State University of New York Press, Albany.
Radford Neal. 2003. Slice sampling. Annals of Statis-
tics, 31:705?767.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From Baby Steps to Leapfrog: How
?Less is More? in unsupervised dependency parsing.
In Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D. Manning. 2010b. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning (CoNLL-2010).
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010c. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2010).
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985?
992.
1213
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1191?1200, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Left-to-Right Tree-to-String Decoding with Prediction
Yang Feng? Yang Liu? Qun Liu? Trevor Cohn?
? Department of Computer Science
The University of Sheffield, Sheffield, UK
{y.feng, t.cohn}@sheffield.ac.uk
? State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
liuyang2011@tsinghua.edu.cn
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
liuqun@ict.ac.cn
Abstract
Decoding algorithms for syntax based ma-
chine translation suffer from high compu-
tational complexity, a consequence of in-
tersecting a language model with a con-
text free grammar. Left-to-right decoding,
which generates the target string in order,
can improve decoding efficiency by simpli-
fying the language model evaluation. This
paper presents a novel left to right decod-
ing algorithm for tree-to-string translation, us-
ing a bottom-up parsing strategy and dynamic
future cost estimation for each partial trans-
lation. Our method outperforms previously
published tree-to-string decoders, including a
competing left-to-right method.
1 Introduction
In recent years there has been rapid progress in the
development of tree-to-string models for statistical
machine translation. These models use the syntac-
tic parse tree of the source language to inform its
translation, which allows the models to capture con-
sistent syntactic transformations between the source
and target languages, e.g., from subject-verb-object
to subject-object-verb word orderings. Decoding al-
gorithms for grammar-based translation seek to find
the best string in the intersection between a weighted
context free grammar (the translation mode, given a
source string/tree) and a weighted finite state accep-
tor (an n-gram language model). This intersection
is problematic, as it results in an intractably large
grammar, and makes exact search impossible.
Most researchers have resorted to approximate
search, typically beam search (Chiang, 2007). The
decoder parses the source sentence, recording the
target translations for each span.1 As the partial
translation hypothesis grows, its component ngrams
are scored and the hypothesis score is updated. This
decoding method though is inefficient as it requires
recording the language model context (n? 1 words)
on the left and right edges of each chart cell. These
contexts allow for boundary ngrams to be evaluated
when the cell is used in another grammar produc-
tion. In contrast, if the target string is generated
in left-to-right order, then only one language model
context is required, and the problem of language
model evaluation is vastly simplified.
In this paper, we develop a novel method of left-
to-right decoding for tree-to-string translation using
a shift-reduce parsing strategy. A central issue in
any decoding algorithm is the technique used for
pruning the search space. Our left-to-right decod-
ing algorithm groups hypotheses, which cover the
same number of source words, into a bin. Pruning
requires the evaluation of different hypotheses in the
same bin, and elimating the least promising options.
As each hypotheses may cover different sets of tree
1The process is analogous for tree-to-string models, except
that only rules and spans matching those in the source trees are
considered. Typically nodes are visited according to a post-
order traversal.
1191
nodes, it is necessary to consider the cost of uncov-
ered nodes, i.e., the future cost. We show that a good
future cost estimate is essential for accurate and effi-
cient search, leading to high quality translation out-
put.
Other researchers have also considered the left-
to-right decoding algorithm for tree-to-string mod-
els. Huang and Mi (2010) developed an Earley-
style parsing algorithm (Earley, 1970). In their ap-
proach, hypotheses covering the same number of
tree nodes were binned together. Their method uses
a top-down depth-first search, with a mechanism for
early elimation of some rules which lead to dead-
ends in the search. Huang and Mi (2010)?s method
was shown to outperform the traditional post-order-
traversal decoding algorithm, considering fewer hy-
potheses and thus decoding much faster at the same
level of performance. However their algorithm used
a very rough estimate of future cost, resulting in
more search errors than our approach.
Our experiments show that compared with the
Earley-style left-to-right decoding (Huang and Mi,
2010) and the traditional post-order-traversal de-
coding (Liu et al 2006) algorithms, our algorithm
achieves a significant improvement on search capac-
ity and better translation performance at the same
level of speed.
2 Background
A typical tree-to-string system (Liu et al 2006;
Huang et al 2006) searches through a 1-best source
parse tree for the best derivation. It transduces the
source tree into a target-language string using a Syn-
chronous Tree Substitution Grammar (STSG). The
grammar rules are extracted from bilingual word
alignments using the GHKM algorithm (Galley et
al., 2004).
We will briefly review the traditional decoding al-
gorithm (Liu et al 2006) and the Earley-style top-
down decoding algorithm (Huang and Mi, 2010) for
the tree-to-string model.
2.1 Traditional Decoding
The traditional decoding algorithm processes source
tree nodes one by one according to a post-order
traversal. For each node, it applies matched STSG
rules by substituting each non-terminal with its cor-
in theory beam search
traditional O(nc|?V |4(g?1)) O(ncb2)
top-down O(c(cr)d|V |g?1) O(ncb)
bottom-up O((cr)d|V |g?1) O(nub)
Table 1: Time complexity of different algorithms. tra-
ditional : Liu et al(2006), top-down : Huang and Mi
(2010). n is the source sentence length, b is the beam
width, c is the number of rules used for each node, V
is the target word vocabulary, g is the order of the lan-
guage model, d is the depth of the source parse tree, u is
the number of viable prefixes for each node and r is the
maximum arity of each rule.
responding translation. For the derivation in Figure
1 (b), the traditional algorithm applies r2 at node
NN2
r2 : NN2 (jieguo) ? the result,
to obtain ?the result? as the translation of NN2. Next
it applies r4 at node NP,
r4 : NP ( NN1 (toupiao), x1 : NN2 )
? x1 of the vote
and replaces NN2 with its translation ?the result?,
then it gets the translation of NP as ?the result of the
vote?.
This algorithm needs to contain boundary words
at both left and right extremities of the target string
for the purpose of LM evaluation, which leads to a
high time complexity. The time complexity in the-
ory and with beam search (Huang and Mi, 2010) is
shown in Table 1.
2.2 Earley-style Top-down Decoding
The Earley-style decoding algorithm performs a top-
down depth-first parsing and generates the target
translation left to right. It applies Context-Free
Grammar (CFG) rules and employs three actions:
predict, scan and complete (Section 3.1 describes
how to convert STSG rules into CFG rules). We can
simulate its translation process using a stack with a
dot  indicating which symbol to process next. For
the derivation in Figure 1(b) and CFG rules in Fig-
ure 1(c), Figure 2 illustrates the whole translation
process.
The time complexity is shown in Table 1 .
1192
3 Bottom-Up Left-to-Right Decoding
We propose a novel method of left-to-right decoding
for tree-to-string translation using a bottom-up pars-
ing strategy. We use viable prefixes (Aho and John-
son, 1974) to indicate all possible target strings the
translations of each node should starts with. There-
fore, given a tree node to expand, our algorithm
can drop immediately to target terminals no matter
whether there is a gap or not. We say that there is a
gap between two symbols in a derivation when there
are many rules separating them, e.g. IP r6? ... r4?
NN2. For the derivation in Figure 1(b), our algo-
rithm starts from the root node IP and applies r2
first although there is a gap between IP and NN2.
Then it applies r4, r5 and r6 in sequence to generate
the translation ?the result of the vote was released
at night?. Our algorithm takes the gap as a black-
box and does not need to fix which partial deriva-
tion should be used for the gap at the moment. So it
can get target strings as soon as possible and thereby
perform more accurate pruning. A valid derivation
is generated only when the source tree is completely
matched by rules.
Our bottom-up decoding algorithm involves the
following steps:
1. Match STSG rules against the source tree.
2. Convert STSG rules to CFG rules.
3. Collect the viable prefix set for each node in a
post-order transversal.
4. Search bottom-up for the best derivation.
3.1 From STSG to CFG
After rule matching, each tree node has its applica-
ble STSG rule set. Given a matched STSG rule, our
decoding algorithm only needs to consider the tree
node the rule can be applied to and the target side,
so we follow Huang and Mi (2010) to convert STSG
rules to CFG rules. For example, an STSG rule
NP ( NN1 (toupiao), x1 : NN2 ) ? x1 of the vote
can be converted to a CFG rule
NP ? NN2 of the vote
The target non-terminals are replaced with corre-
sponding source non-terminals. Figure 1 (c) shows
all converted CFG rules for the toy example. Note
IP
NP
NN1
to?up?`ao
NN2
j??eguo?
VP
NT
wa?nsha`ng
VV
go?ngbu`
(a) Source parse tree
r6: IP
NP VP
? ?
r4: NP
NN1
to?up?`ao
NN2
r5: VP
NT
wa?nsha`ng
VV
go?ngbu`
?
r2: NN2
j??eguo?
the result of the vote was released at night
(b) A derivation
r1: NN1 ? the vote
r2: NN2 ? the result
r3: NP ? NN2 of NN1
r4: NP ? NN2 of the vote
r5: VP ? was released at night
r6: IP ? NP VP
r7: IP ? NN2 of the vote VP
r8: IP ? VP NP
(c) Target-side CFG rule set
Figure 1: A toy example.
that different STSG rules might be converted to the
same CFG rule despite having different source tree
structures.
3.2 Viable Prefix
During decoding, how do we decide which rules
should be used next given a partial derivation, es-
pecially when there is a gap? A key observation is
that some rules should be excluded. For example,
any derivation for Figure 1(a) will never begin with
r1 as there is no translation starting with ?the vote?.
In order to know which rules can be excluded for
each node, we can recursively calculate the start-
ing terminal strings for each node. For example,
1193
NN1: {the vote} NN2: {the result}
NT: ? VV: ?
NP: {the result}
VP: {was released at night}
IP: {the result, was released at night}
Table 2: The Viable prefix sets for Figure 1 (c)
according to r1, the starting terminal string of the
translation for NN1 is ?the vote?. According to r2,
the starting terminal string for NN2 is ?the result?.
According to r3, the starting terminal string of NP
must include that of NN2. Table 2 lists the starting
terminal strings of all nodes in Figure 1(a). As the
translations of node IP should begin with either ?the
result? or ?was released at night?, the first rule must
be either r2 or r5. Therefore, r1 will never be used
as the first rule in any derivation.
We refer to starting terminal strings of a node as
a viable prefixes, a term borrowed from LR pars-
ing (Aho and Johnson, 1974). Viable prefixes are
used to decide which rule should be used to ensure
efficient left-to-right target generation. Formally, as-
sume that VN denotes the set of non-terminals (i.e.,
source tree node labels), VT denotes the set of ter-
minals (i.e., target words), v1, v2 ? VN , w ? VT ,
pi ? {VT ? VN}?, we say that w is a viable prefix of
v1 if and only if:
? v1 ? w, or
? v1 ? wv2pi, or
? v1 ? v2pi, and w is a viable prefix of v2.
Note that we bundle all successive terminals in one
symbol.
3.3 Shift-Reduce Parsing
We use a shift-reduce algorithm to search for the
best deviation. The algorithm maintains a stack of
dotted rules (Earley, 1970). Given the source tree in
Figure 1(a), the stack is initialized with a dotted rule
for the root node IP:
[ IP].
Then, the algorithm selects one viable prefix of IP
and appends it to the stack with the dot at the begin-
ning (predict):
[ IP] [ the result]2.
Then, a scan action is performed to produce a partial
translation ?the result?:
[ IP] [the result ].
Next, the algorithm searches for the CFG rules start-
ing with ?the result? and gets r2. Then, it pops the
rightmost dotted rule and append the left-hand side
(LHS) of r2 to the stack (complete):
[ IP] [NN2 ].
Next, the algorithm chooses r4 whose right-hand
side ?NN2 of the vote? matches the rightmost dot-
ted rule in the stack3 and grows the rightmost dotted
rule:
[ IP] [NN2  of the vote].
Figure 3 shows the whole process of derivation
generation.
Formally, we define four actions on the rightmost
rule in the stack:
? Predict. If the symbol after the dot in the right-
most dotted rule is a non-terminal v, this action
chooses a viable prefix w of v and generates a
new dotted rule for w with the dot at the begin-
ning. For example:
[ IP] predict?? [ IP] [ the result]
? Scan. If the symbol after the dot in the right-
most dotted rule is a terminal string w, this ac-
tion advances the dot to update the current par-
tial translation. For example:
[ IP] [ the result] scan?? [ IP] [the result ]
? Complete. If the rightmost dotted rule ends
with a dot and it happens to be the right-hand
side of a rule, then this action removes the
right-most dotted rule. Besides, if the symbol
after the dot in the new rightmost rule corre-
sponds to the same tree node as the LHS non-
terminal of the rule, this action advance the dot.
For example,
[ IP] [NP  VP] [was released at night ]
complete?? [ IP] [NP VP ]
2There are another option: ?was released at night?
3Here there is an alternative: r3 or r7
1194
step action rule used stack hypothesis
0 [ IP]
1 p r6 [ IP] [ NP VP]
2 p r4 [ IP] [ NP VP] [ NN2 of the vote]
3 p r2 [ IP] [ NP VP] [ NN2 of the vote] [ the result]
4 s [ IP] [ NP VP] [ NN2 of the vote] [the result ] the result
5 c [ IP] [ NP VP] [NN2  of the vote] the result
6 s [ IP] [ NP VP] [NN2 of the vote ] the result of the vote
7 c [ IP] [NP  VP] the result of the vote
8 p r5 [ IP] [NP  VP] [ was released at night] the result of the vote
9 s [ IP] [NP  VP] [was released at night ] the ... vote was ... night
10 c [ IP] [NP VP ] the ... vote was ... night
11 c [IP ] the ... vote was ... night
Figure 2: Simulation of top-down translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete. ?the ... vote? and ?was ... released? are the abbreviated form of ?the result of the vote? and ?was released at
night?, respectively.
step action rule used stack number hypothesis
0 [ IP] 0
1 p [ IP] [ the result] 0
2 s [ IP] [the result ] 1 the result
3 c r2 [ IP] [NN2 ] 1 the result
4 g r4 or r7 [ IP] [NN2  of the vote] 1 the result
5 s [ IP] [NN2 of the vote ] 2 the result of the vote
6 c r4 [ IP] [NP ] 2 the result of the vote
7 g r6 [ IP] [NP  VP] 2 the result of the vote
8 p [ IP] [NP  VP] [ was released at night] 2 the result of the vote
9 s [ IP] [NP  VP] [was released at night ] 4 the ... vote was ... night
10 c r5 [ IP] [NP VP ] 4 the ... vote was ... night
11 c r6 [IP ] 4 the ... vote was ... night
Figure 3: Simulation of bottom-up translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete; g, grow. The column of number gives the number of source words the hypothesis covers.
If the string cannot rewrite on the frontier non-
terminal, then we add the LHS to the stack with
the dot after it. For example:
[ IP] [the result ] complete?? [ IP] [NN2 ]
? Grow. If the right-most dotted rule ends with
a dot and it happens to be the starting part of
a CFG rule, this action appends one symbol of
the remainder of that rule to the stack 4. For
example:
4We bundle the successive terminals in one rule into a sym-
bol
[ IP] [NN2 ]
grow?? [ IP] [NN2  of the vote]
From the above definition, we can find that there
may be an ambiguity about whether to use a com-
plete action or a grow action. Similarly, predict ac-
tions must select a viable prefix form the set for a
node. For example in step 5, although we select
to perform complete with r4 in the example, r7 is
applicable, too. In our implementation, if both r4
and r7 are applicable, we apply them both to gener-
ate two seperate hypotheses. To limit the exponen-
tial explosion of hypotheses (Knight, 1999), we use
beam search over bins of similar partial hypotheses
(Koehn, 2004).
1195
IP
NP
NN2 of NN1
of the vote
VP
was released at night
r7
r4 r5
r6
r3
Figure 4: The translation forest composed of applicable
CFG rules for the partial derivation of step 3 in Figure 3.
3.4 Future Cost
Partial derivations covering different tree nodes may
be grouped in the same bin for beam pruning5. In
order to performmore accurate pruning, we take into
consideration future cost, the cost of the uncovered
part. The merit of a derivation is the covered cost
(the cost of the covered part) plus the future cost.
We borrow ideas from the Inside-Outside algorithm
(Charniak and Johnson, 2005; Huang, 2008; Mi et
al., 2008) to compute the merit. In our algorithm,
the merit of a derivation is just the Viterbi inside cost
? of the root node calculated with the derivations
continuing from the current derivation.
Given a partial derivation, we calculate its future
cost by searching through the translation forest de-
fined by all applicable CFG rules. Figure 4 shows
the translation forest for the derivation of step 3. We
calculate the future cost for each node as follows:
given a node v, we define its cost function f(v) as
f(v) =
?
?
?
?
?
1 v is completed
lm(v) v is a terminal string
maxr?Rv f(r)
?
pi?rhs(r) f(pi) otherwise
where VN is the non-terminal set, VT is the terminal
set, v, pi ? VN ? VT+, Rv is the set of currently ap-
plicable rules for v, rhs(r) is the right-hand symbol
set of r, lm is the local language model probability,
f(r) is calculated using a linear model whose fea-
tures are bidirectional translation probabilities and
lexical probabilities of r. For the translation forest
in Figure 4, if we calculate the future cost of NP with
5Section 3.7 will describe the binning scheme
r4, then
f(NP ) = f(r4) ? f(NN2) ? lm(of the vote)
= f(r4) ? 1 ? lm(of the vote)
Note that we calculate lm(of the vote) locally and do
not take ?the result? derived from NN2 as the con-
text. The lm probability of ?the result? has been in-
cluded in the covered cost.
As a partial derivation grows, some CFG rules
will conflict with the derivation (i.e. inapplicable)
and the translation forest will change accordingly.
For example, when we reach step 5 from step 3 (see
Figure 4 for its translation forest), r3 is inapplica-
ble and thereby should be ruled out. Then the nodes
on the path from the last covered node (it is ?of the
vote? in step 5) to the root node should update their
future cost, as they may employ r3 to produce the
future cost. In step 5, NP and IP should be updated.
In this sense, we say that the future cost is dynamic.
3.5 Comparison with Top-Down Decoding
In order to generate the translation ?the result? based
on the derivation in Figure 1(b), Huang and Mi?s
top-down algorithm needs to specify which rules to
apply starting from the root node until it yields ?the
result?. In this derivation, rule r6 is applied to IP, r4
to NP, r2 to NN2. That is to say, it needs to repre-
sent the partial derivation from IP to NN2 explicitly.
This can be a problem when combined with beam
pruning. If the beam size is small, it may discard the
intermediate hypotheses and thus never consider the
string. In our example with a beam of 1, we must
select a rule for IP among r6, r7 and r8 although we
do not get any information for NP and VP.
Instead, our bottom-up algorithm allows top-
down and bottom-up information to be used together
with the help of viable prefixes. This allows us to
encode more candidate derivations than the purely
top-down method. In the above example, our al-
gorithm does not specify the derivation for the gap
from IP and ?the result?. In fact, all derivations
composed of currently applicable rules are allowed.
When needed, our algorithm derives the derivation
dynamically using applicable rules. So when our
algorithm performs pruning at the root node, it has
got much more information and consequently intro-
duces fewer pruning errors.
1196
3.6 Time Complexity
Assume the depth of the source tree is d, the max-
imum number of matched rules for each node is c,
the maximum arity of each rule is r, the language
model order is g and the target-language vocabulary
is V, then the time complexity of our algorithm is
O((cr)d|V |g?1). Analysis is as follows:
Our algorithm expands partial paths with termi-
nal strings to generate new hypotheses, so the time
complexity depends on the number of partial paths
used. We split a path which is from the root node to a
leaf node with a node on it (called the end node) and
get the segment from the root node to the end node
as a partial path, so the length of the partial path is
not definite with a maximum of d. If the length is
d?(d? ? d), then the number of partial paths is (cr)d? .
Besides, we use the rightest g ? 1 words to signa-
ture each partial path, so we can get (cr)d? |V |g?1
states. For each state, the number of viable prefixes
produced by predict operation is cd?d? , so the total
time complexity is f = O((cr)d? |V |g?1cd?d?) =
O(cdrd? |V |g?1) = O((cr)d|V |g?1).
3.7 Beam Search
Tomake decoding tractable, we employ beam search
(Koehn, 2004) and choose ?binning? as follows: hy-
potheses covering the same number of source words
are grouped in a bin. When expanding a hypothe-
sis in a beam (bin), we take series of actions until
new terminals are appended to the hypothesis, then
add the new hypothesis to the corresponding beam.
Figure 3 shows the number of source words each hy-
pothesis covers.
Among the actions, only the scan action changes
the number of source words each hypothesis cov-
ers. Although the complete action does not change
source word number, it changes the covered cost of
hypotheses. So in our implementation, we take scan
and complete as ?closure? actions. That is to say,
once there are some complete actions after a scan ac-
tion, we finish all the compete actions until the next
action is grow. The predict and grow actions decide
which rules can be used to expand hypotheses next,
so we update the applicable rule set during these two
actions.
Given a source sentence with n words, we main-
tain n beams, and let each beam hold b hypotheses
at most. Besides, we prune viable prefixes of each
node up to u, so each hypothesis can expand to u
new hypotheses at most, so the time complexity of
beam search is O(nub).
4 Related Work
Watanabe et al(2006) present a novel Earley-
style top-down decoding algorithm for hierarchical
phrase-based model (Chiang, 2005). Their frame-
work extracts Greibach Normal Form rules only,
which always has at least one terminal on the left
of each rule, and discards other rules.
Dyer and Resnik (2010) describe a translation
model that combines the merits of syntax-based
models and phrase-based models. Their decoder
works in two passes: for first pass, the decoder col-
lects a context-free forest and performs tree-based
source reordering without a LM. For the second
pass, the decoder adds a LM and performs bottom-
up CKY decoding.
Feng et al(2010) proposed a shift-reduce algo-
rithm to add BTG constraints to phrase-based mod-
els. This algorithm constructs a BTG tree in a
reduce-eager manner while the algorithm in this pa-
per searches for a best derivation which must be de-
rived from the source tree.
Galley and Manning (2008) use the shift-reduce
algorithm to conduct hierarchical phrase reordering
so as to capture long-distance reordering. This al-
gorithm shows good performance on phrase-based
models, but can not be applied to syntax-based mod-
els directly.
5 Experiments
In the experiments, we use two baseline systems:
our in-house tree-to-string decoder implemented ac-
cording to Liu et al(2006) (denoted as traditional)
and the Earley-style top-down decoder implemented
according to Huang and Mi (2010) (denoted as top-
down), respectively. We compare our bottom-up
left-to-right decoder (denoted as bottom-up) with
the baseline in terms of performance, translation
quality and decoding speed with different beam
sizes, and search capacity. Lastly, we show the in-
fluence of future cost. All systems are implemented
in C++.
1197
5.1 Data Setup
We used the FBIS corpus consisting of about 250K
Chinese-English sentence pairs as the training set.
We aligned the sentence pairs using the GIZA++
toolkit (Och and Ney, 2003) and extracted tree-to-
string rules according to the GHKM algorithm (Gal-
ley et al 2004). We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus.
We used the 2002 NIST MT Chinese-English test
set (571 sentences) as the development set and the
2005 NIST MT Chinese-English test set (1082 sen-
tences) as the test set. We evaluated translation qual-
ity using BLEU-metric (Papineni et al 2002) with
case-insensitive n-gram matching up to n = 4. We
used the standard minimum error rate training (Och,
2003) to tune feature weights to maximize BLEU
score on the development set.
5.2 Performance Comparison
Our bottom-up left-to-right decoder employs the
same features as the traditional decoder: rule proba-
bility, lexical probability, language model probabil-
ity, rule count and word count. In order to compare
them fairly, we used the same beam size which is 20
and employed cube pruning technique (Huang and
Chiang, 2005).
We show the results in Table 3. From the re-
sults, we can see that the bottom-up decoder out-
performs top-down decoder and traditional decoder
by 1.1 and 0.8 BLEU points respectively and the
improvements are statistically significant using the
sign-test of Collins et al(2005) (p < 0.01). The
improvement may result from dynamically search-
ing for a whole derivation which leads to more ac-
curate estimation of a partial derivation. The addi-
tional time consumption of the bottom-up decoder
against the top-down decoder comes from dynamic
future cost computation.
Next we compare decoding speed versus transla-
tion quality using various beam sizes. The results
are shown in Figure 5. We can see that our bottom-
up decoder can produce better BLEU score at the
same decoding speed. At small beams (decoding
time around 0.5 second), the improvement of trans-
lation quality is much bigger.
System BLEU(%) Time (s)
Traditional 29.8 0.84
Top-down 29.5 0.41
Bottom-up 30.6 0.81
Table 3: Performance comparison.
29.4
29.6
29.8
30.0
30.2
30.4
30.6
30.8
 0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
BL
EU
 S
co
re
Avg Decoding Time (secs per sentence)
bottom-up
top-down
traditional
Figure 5: BLEU score against decoding time with various
beam size.
5.3 Search Capacity Comparison
We also compare the search capacity of the bottom-
up decoder and the traditional decoder. We do this
in the following way: we let both decoders use the
same weights tuned on the traditional decoder, then
we compare their translation scores of the same test
sentence.
From the results in Table 4, we can see that for
many test sentences, the bottom-up decoder finds
target translations with higher score, which have
been ruled out by the traditional decoder. This may
result from more accurate pruning method. Yet for
some sentences, the traditional decoder can attain
higher translation score. The reason may be that the
traditional decoder can hold more than two nonter-
minals when cube pruning, while the bottom-up de-
coder always performs dual-arity pruning.
Next, we check whether higher translation scores
bring higher BLEU scores. We compute the BLEU
score of both decoders on the test sentence set on
which bottom-up decoder gets higher translation
scores than the traditional decoder does. We record
the results in Figure 6. The result shows that higher
score indeed bring higher BLEU score, but the im-
provement of BLEU score is not large. This is be-
cause the features we use don?t reflect the real statis-
1198
28.0
29.0
30.0
31.0
32.0
33.0
34.0
35.0
 10  20  30  40
BL
EU
 S
co
re
Beam Size
bottom-up
traditional
Figure 6: BLEU score with various beam sizes on the sub
test set consisting of sentences on which the bottom-up
decoder gets higher translation score than the traditional
decoder does.
b > = <
10 728 67% 347 32% 7 1%
20 657 61% 412 38% 13 1%
30 615 57% 446 41% 21 2%
40 526 49% 523 48% 33 3%
50 315 29% 705 65% 62 6%
Table 4: Search capacity comparison. The first column is
beam size, the following three columns denote the num-
ber of test sentences, on which the translation scores of
the bottom-up decoder are greater, equal to, lower than
that of the traditional decoder.
System BLEU(%) Time (s)
with 30.6 0.81
without 28.8 0.39
Table 5: Influence of future cost. The results of the
bottom-up decoder with and without future cost are given
in the second and three rows, respectively.
tical distribution of hypotheses well. In addition, the
weights are tuned on the traditional decoder, not on
the bottom-up decoder. The bottom-up decoder can
perform better with weights tuned by itself.
5.4 Influence of Future Cost
Next, we will show the impact of future cost via ex-
periments. We give the results of the bottom-up de-
coder with and without future cost in Table 5. From
the result, we can conclude that future cost plays a
significant role in decoding. If the bottom-up de-
coder does not employ future cost, its performance
will be influenced dramatically. Furthermore, cal-
culating dynamic future cost is time consuming. If
the bottom-up decoder does not use future cost, it
decodes faster than the top-down decoder. This is
because the top-down decoder has |T | beams, while
the bottom-up decoder has n beams, where T is the
source parse tree and n is the length of the source
sentence.
6 Conclusions
In this paper, we describe a bottom-up left-to-right
decoding algorithm for tree-to-string model. With
the help of viable prefixes, the algorithm generates
a translation by constructing a target-side CFG tree
according to a post-order traversal. In addition, it
takes into consideration a dynamic future cost to es-
timate hypotheses.
On the 2005 NIST Chinese-English MT transla-
tion test set, our decoder outperforms the top-down
decoder and the traditional decoder by 1.1 and 0.8
BLEU points respectively and shows more powerful
search ability. Experiments also prove that future
cost is important for more accurate pruning.
7 Acknowledgements
We would like to thank Haitao Mi and Douwe
Gelling for their feedback, and anonymous review-
ers for their valuable comments and suggestions.
This work was supported in part by EPSRC grant
EP/I034750/1 and in part by High Technology R&D
Program Project No. 2011AA01A207.
References
A. V. Aho and S. C. Johnson. 1974. Lr parsing. Com-
puting Surveys, 6:99?124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL, pages 173?180.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
1199
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proc. of NAACL,
pages 858?866, June.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13:94?102.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling, pages
285?293.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc of
NAACL, pages 273?280.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT, pages 53?64.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL,
pages 586?594.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25:607?615.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrased-based statistical machine translation. In
Proc. of AMTA, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192?199.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING, pages
777?784.
1200
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 977?988,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A temporal model of text periodicities using Gaussian Processes
Daniel Preot?iuc-Pietro, Trevor Cohn
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield, S1 4DP, United Kingdom
{daniel,t.cohn}@dcs.shef.ac.uk
Abstract
Temporal variations of text are usually ig-
nored in NLP applications. However, text use
changes with time, which can affect many
applications. In this paper we model peri-
odic distributions of words over time. Focus-
ing on hashtag frequency in Twitter, we first
automatically identify the periodic patterns.
We use this for regression in order to fore-
cast the volume of a hashtag based on past
data. We use Gaussian Processes, a state-of-
the-art bayesian non-parametric model, with
a novel periodic kernel. We demonstrate this
in a text classification setting, assigning the
tweet hashtag based on the rest of its text. This
method shows significant improvements over
competitive baselines.
1 Introduction
Temporal changes in text corpora are central to our
understanding of many linguistic and social phe-
nomena. Social Media platforms and the digital-
ization of libraries provides a vast body of times-
tamped data. This allows studying of the complex
temporal patterns exhibited by text usage includ-
ing highly non-stationary distributions and period-
icities. However, temporal effects have been mostly
ignored by previous work on text analysis or at best
dealt with by making strong assumptions such as
smoothly varying parameters with time (Yogatama
et al, 2011) or modelled using a simple uni-modal
distri bution (Wang and McCallum, 2006). This pa-
per develops a temporal model for classifying mi-
croblog posts which explicitly incorporates mul-
timodal periodic behaviours using Gaussian Pro-
cesses (GPs).
We expect text usage to follow multiple period-
icities at different scales. For example, people on
Social Media might talk about different topics dur-
ing and after work on weekdays, talk every Friday
about the weekend ahead, or comment about their
favorite weekly TV show during its air time. Given
this, text frequencies will display periodic patterns.
This applies to other text related quantities like co-
occurrence values or topic distributions over time,
as well as applications outside NLP like user be-
haviour (Preot?iuc-Pietro and Cohn, 2013).
Modelling temporal patterns and periodicities can
be useful to tasks like text classification. For exam-
ple a tweet containing ?music? is normally attributed
to a general hashtag about music like #np (now play-
ing). However, knowing time, if it occurs during the
(weekly periodic) air time of ?American Idol? it is
more likely for it to belong to #americanidol or if
its mentioned in the days building up to the Video
Music Awards to be assigned to #VMA.
In NLP, temporal models have treated time in
overly simplistic ways and without regard to period-
icities. We propose a model that first broadly iden-
tifies several types of temporal patterns: a) periodic,
b) constant in time, c) falling out of use after enjoy-
ing a brief spell of popularity (e.g. internet memes,
news). This is performed automatically only using
training data and makes no assumptions on the exis-
tence or the length of the periods we aim to model.
We demonstrate the approach by modelling frequen-
cies of hashtag occurrences in Twitter. Hashtags are
user-generated labels included in tweets by their au-
thors in order to assign them to a conversation and
can be considered as a proxy for topics.
To this end, we make use of Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2005), a
977
Bayesian non-parametric model for regression. Us-
ing the Bayesian evidence we automatically perform
model selection to classify temporal patterns. We
aim to use the most suitable model for extrapolation,
i.e. predicting future values from past observations.
The GP is fully defined by the covariance structure
assumed between the observed points, and its hy-
perparameters, which can be automatically learned
from data. We also introduce a new kernel suitable
to model the periodic behaviour we observe in text:
periods of low frequency followed by bursts at reg-
ular time intervals. We demonstrate that the GP ap-
proach is more general and gives better results than
frequentist models (e.g. autoregressive models) be-
cause it incorporates uncertainty explicitly and ele-
gantly, in addition to automatic model selection and
parameter fitting.
To demonstrate the practical importance of our
approach, we use our GP prediction as a prior in a
Na??ve Bayes model for text classification showing
improvements over baselines which do not account
for temporal periodicities. Our approach extends to
more general uses, e.g. to discriminative text regres-
sion and classification. More broadly, we aim to es-
tablish GPs as a state-of-the-art model for regression
and classification in NLP. To our knowledge, this is
the first paper to use GP regression for forecasting
and model selection within a NLP task.
All the hashtag time series data and the imple-
mentation of the PS kernel in the popular open-
source Gaussian Processes packages GPML1 and
GPy2 are available on the author?s website3.
2 Related Work
Time varying text patterns have been of particular
interest in topic modelling. Griffiths and Steyvers
(2004) analyse evolution of topics over time, but
without modelling time explicitly. Extensions that
model time make different assumptions, usually re-
garding smoothing proprieties in (Wang and McCal-
lum, 2006; Blei and Lafferty, 2006; Wang et al,
2008; Hennig et al, 2012). Yogatama et al (2011)
proposed a regulariser for generalised linear models
that encourages local temporal smoothness.
1http://www.gaussianprocess.org/gpml/
code
2https://github.com/SheffieldML/GPy
3http://www.preotiuc.ro
Modelling periodicities is one of the standard ap-
plications of Gaussian Processes (Rasmussen and
Williams, 2005). Recent work by Wilson and Adams
(2013) and Durrande et al (2013) show how differ-
ent periods can be identified from data. In general,
methods that assume certain periodicities at daily or
weekly levels were proposed e.g. in (McInerney et
al., 2013). GPs were used with text by Polajnar et
al. (2011) and for Quality Estimation regression in
(Cohn and Specia, 2013; Shah et al, 2013).
Temporal patterns for short, distinctive lexical
items such as hashtags and memes were quanti-
tatively studied (Leskovec et al, 2009) and clus-
tered (Yang and Leskovec, 2011) in Social Media.
(Yang et al, 2012) studies the dual role of hashtags,
of bookmarks of content and symbols of commu-
nity membership, in the context of hashtag adoption.
(Romero et al, 2011) analyses the patterns of tem-
poral diffusion in Social Media finding that hashtags
have also a persistence factor.
For predicting future popularity of hashtags, Tsur
and Rappoport (2012) use linear regression with a
wide range of features. (Ma et al, 2012; Ma et
al., 2013) frame the problem as classification into
a number of fixed intervals and applies all the stan-
dard classifiers. None of these studies model period-
icities, although the former stresses their importance
for accurate predictions. For predicting the hashtag
given the tweet text, Mazzia and Juett (2011) uses
the Na??ve Bayes classifier with the uniform and em-
pirical prior or TF-IDF weighting.
3 Gaussian Processes
In this paper we consider Gaussian Process (GP)
models of regression (Rasmussen and Williams,
2005). GP is a probabilistic machine learning
framework incorporating kernels and Bayesian non-
parametrics which is widely considered as state-of-
the-art for regression. The GP defines a prior over
functions which applied at each input point gives a
response value. Given data, we can analytically infer
the posterior distribution of these functions assum-
ing Gaussian noise. The kernel of the GP defines the
covariance in response values as a function of its in-
puts.
We can identify two different set-ups for a regres-
sion problem. If the range of values to be predicted
978
lies within the bounds of the training set we call the
prediction task as interpolation. If the range of the
prediction is outside the bounds, then our problem
that of extrapolation. In this respect, extrapolation is
considered a more difficult task and the covariance
kernel which incorporates our prior knowledge plays
a major role in the prediction.
There is the case when multiple covariance ker-
nels can describe our data. For choosing the right
kernel and its hyperparameters only using the train-
ing data we employ Bayesian model selection which
makes a trade-off between the fit of the training
data and model complexity. We now briefly give an
overview of GP regression, kernel choice and model
selection. We refer the interested reader to (Ras-
mussen and Williams, 2005) for a detailed introduc-
tion to GPs.
3.1 Gaussian Process Regression
Consider a time series regression task where we only
have one feature, the value xt at time t. Our training
data consists of n pairs D = {(t, xt)}. The model
will need to predict values xt for values of t greater
than those in the dataset.
GP regression assumes a latent function f that
is drawn from a GP prior f(t) ? GP(m, k(t, t?))
where m is the mean and k a kernel. The predic-
tion value is obtained by the function evaluated at
the corresponding data point, xt = f(t) + , where
 ? N (0, ?2) is white-noise. The GP is defined by
the mean m, here 0, and the covariance kernel func-
tion, k(t, t?).
The posterior at a test point t? is given by:
p(x?|t?,D) =
?
f
p(x?|t?, f) ? p(f |D) (1)
where x? and t? are the test value and time. The pos-
terior p(f |D) shows our belief over possible func-
tions after observing the training set D. The predic-
tive posterior can be solved analytically with solu-
tion:
x? ? N (k
T
? (K + ?
2
nI)
?1t,
k(t?, t?)? k
T
? (K + ?
2
nI)
?1k?)
(2)
where k? = [k(t?, t1)...k(t?, tn)]T are the kernel
evaluations between the test point and all the train-
ing points,K = {k(ti, tj)}
i=1..n
j=1..n is the Gram matrix
3/1 4/1 5/1 6/1 
 GoldSEPS(24)
Figure 1: Interpolation for #goodmorning over 3 days
with SE and PS(p=24,s=3) kernels. Prediction variance
shown in grey for PS(24). Crosses represent training
points.
over the training points and t is the vector of train-
ing points. The posterior of x? includes the mean
response as well as its variance, thus expressing the
uncertainty of the prediction. In this paper, we will
consider the forecast as the expected value. Due to
the matrix inversion in 2, inference takesO(n3) time
where n is the number of training points.
3.2 Kernels
The covariance kernel together with its parameters
fully define the GP (we assume 0 mean). The kernel
induces similarities in the response between pairs of
data points. Intuitively, if we want a smooth func-
tion, closer points should have high covariance com-
pared to points that are further apart. If we want a
periodic behaviour points at period length intervals
should have the highest covariance. Usually, this is
defined by an isotropic kernel, which means its in-
variant to all rigid motions.
For interpolation, a standard kernel (e.g. squared
exponential) that encourages smooth functions is
normally used. Figure 1 shows regression over 3
days for #goodmorning when only a random third
of the values of the function are observed. We see
that both the SE kernel and a periodic kernel (PS,
see below) give good results.
However, for extrapolation, the choice of the ker-
nel is paramount. The kernel encodes our prior belief
about the type of function wish to learn. To illustrate
this, in Figure 3, we show the time series for #good-
morning over 2 weeks and plot the regression for the
979
future week learned by using different kernels.
In this study we will use multiple kernels, each
most suitable for a specific category of temporal pat-
terns in our data. This includes a new kernel inspired
by observed word occurrence patterns. The kernels
we use are:
Constant (C): The constant kernel is kC(t, t?) =
c. Its mean prediction will always be the value c and
its assumption is that the signal is modeled only by
Gaussian noise centred around this value. This de-
scribes the data best when we have a noisy signal
around a stationary mean value.
Squared exponential (SE): The SE kernel or the
Radial Basis Function (RBF) is the standard kernel
used in most interpolation settings.
kSE(t, t
?) = s2 ? exp?
(t? t?)2
2l2
(3)
This gives a smooth transition between neighbour-
ing points and best describes time series with a
smooth shape e.g. a uni-modal burst with a steady
decrease. However, its uncertainty grows with for
predictions well into the future. Its two parameters
s and l are the characteristic lengthscales along the
two axes. Intuitively, they control the distance of in-
puts on a particular axis from which the function
values become uncorrelated. Using the SE kernel
corresponds to Bayesian linear regression with an
infinite number of basis functions (Rasmussen and
Williams, 2005).
Linear (Lin): The linear kernel describes a linear
relationship between outputs.
kLin(t, t
?) = s2 + ?t ? t?? (4)
This can be obtained from linear regression by hav-
ing N (0, 1) priors on the corresponding regression
weights and a prior of N (0, s2) on the bias.
Periodic (PER): The periodic kernel represents a
SE kernel in polar coordinates.
kPER(t, t
?) = s2 ? exp?2 ?
(
sin2(2pi(t? t?)2/p)
l2
)
(5)
It has a sinusoidal shape and is good at modelling
periodically patterns that oscillate between low and
high frequency. s and l are characteristic length-
scales as in the SE kernel and p is the period.
25 50 75 100 1250
0.2
0.4
0.6
0.8
1
 
 
s=1
s=5
s=50
Figure 2: Behaviour of the PS kernel (p=50) with varying
s. Values normalized in [0,1] interval.
Periodic spikes (PS): For textual time series, like
word frequencies, we identify the following periodic
behaviour: abrupt rise in usage, usually with a peak,
followed by periods of low occurrence, which can
be short (e.g. during the night) or long lived (e.g. the
entire week except for a few hours). For modelling
we introduce the following kernel:
kPS(t, t
?) = cos
(
sin
(
2pi ? (t? t?)2
p
))
? exp
(
s cos(2pi ? (t? t?)2)
p
? s
)
(6)
The kernel is parameterised by its period p and a
shape parameter s. The period indicates the time in-
terval between the peaks of the function, while the
shape parameter controls the width of the spike. The
behaviour of the kernel is illustrated in Figure 2. We
constrain s ? 1.
In Figure 3 we see that the forecast is highly de-
pendent on the kernel choice. We expect that for
periodic data the PER and PS kernels will forecast
best, maybe with the PS kernel doing a better job
because it captures multiple modes of the daily in-
crease in volume. We use for both kernels a period
of 168 hours. This is because although a daily pat-
tern exists, the weekly is stronger, with the day of
the week influencing the volume of the hashtag. The
NRMSE (Normalized Root Mean Square Error) in
Table 1 on the held out data confirms this finding,
with PS showing the lowest error.
980
03/
1
10/
1
17/
1
00.20.40.60.81
#go
odm
orn
ing
 
 
Gol
d
Con
st
Lin
ear SE Per
(168
)
PS(1
68)
Figure 3: Extrapolation for #goodmorning over 3 weeks with GPs using different kernels.
Const Lin SE PER PS
NLML -41 -34 -176 -180 -192
NRMSE 0.213 0.214 0.262 0.119 0.107
Table 1: Negative Log Marginal Likelihood (NLML)
shows the best fitted model for the time series in Figure 3.
NRMSE computed on the third unobserved week. Lower
values are better in both cases.
3.3 Model selection and optimisation
We now briefly discuss the concepts of model se-
lection in the GP framework, by which we refer to
choosing the model (kernel) from a set Hi and op-
timising the model hyperparameters ?. In our GP
Bayesian inference scheme, we can compute the
probability of the data given the model which in-
volves the integral over the parameter space. This
is called the marginal likelihood or evidence and is
useful for model selection using only the training
set:
p(x|D, ?,Hi) =
?
f
p(x|D, f,Hi)p(f |?,Hi) (7)
Our first goal is to fit the kernel by minimizing
the negative log marginal likelihood (NLML) with
respect to the kernel parameters ?. This approxima-
tion is also known as type II maximum likelihood
(ML-II). Conditioned on kernel parameters, the evi-
dence of a GP can be computed analytically.
Our second goal is to use the evidence for
model selection because it balances the data fit and
the model complexity by automatically incorporat-
ing Occam?s Razor (Rasmussen and Ghahramani,
2000). Because the evidence must normalise, com-
plex models which can account for many datasets
achieve low evidence. One can think of the evidence
as the probability that a random draw of the param-
eter values from the model class would generate the
dataset D. This way, complex models are penalised
because they can describe many datasets, while the
simple models can describe only a few datasets, thus
the chance of a good data fit being very low. This is
for example the case of the periodic bursts in Fig-
ure 3. Although the periodic kernel can fit the data,
it will incur a high model complexity penalty. The
PS kernel in this respect is a simpler model and can
fit the data and is thus chosen as the right model.
When the dataset is observed, the evidence can se-
lect between the models. More generally, the model
choice actually gives us an implicit classification of
the temporal patterns into classes: a steady signal
with noise (C kernel), a signal with local temporal
patterns (SE kernel), an oscilating periodic pattern
(PER kernel) or a pattern with abrupt periodic peaks
(PS kernel).
We use the NLML for optimising the hyperpa-
rameters only using training data. For optimising the
hyperparameters of the kernel defined in Equation 6,
it is important to first identify the right period. We
consider as possible periods all integer values less
than half the size of the training set, and then tune
the shape parameter using gradient descent to min-
imise NLML. We then take the argmin value of
those considered. We show the NLML for a sample
regression in Figure 4.
The likelihood shows that there are multiple
canyons in the likelihood, which can lead a convex
optimisation method to local optima. These appear
when p is equal or an integer multiple of the main
period of the data, in this case 24. The lowest val-
ues are obtained when p = 168, allowing the model
to accommodate the day of week effect. Our proce-
dure is not guaranteed to reach a global optima, but
981
40 80 120 160 2001
2550
75100
?1000
?500
0
Period pShape s
NLM
L
Figure 4: NLML for #goodmorning on the training set as
a function of the 2 kernel parameters.
is a relatively standard technique for fitting periodic
kernels (Duvenaud et al, 2013).
The flexibility of the GP framework allows us to
combine kernels (e.g. SE ?PS or PS+Lin) in order
to identify a combination of trends (Duvenaud et al,
2013; Go?nen and Alpaydin, 2011). Experiments on a
subset of data showed no major benefits of combin-
ing kernels, but the computational time and model
complexity increased drastically due to the extra hy-
perparameters. Because we will model a proportion
of words within a limited time frame, there are few
linear trends in the data. It might seem limiting that
we only learn a single period, although we could
combine periodic kernels with different periods to-
gether. But, as we have seen in the #goodmorning
example (with overlapping weekly and daily pat-
terns), if there is a combination of periods the model
will select a single period which is the least common
multiple.
4 Data
For our experiments we used data collected from
Twitter using the public Gardenhose stream (10%
representative sample of the entire Twitter stream).
The data collection interval was 1 January ? 28
February 2011. For simplicity in the classification
task, we filtered the stream to include only tweets
that have exactly one hashtag. These represent ap-
proximately 7.8% of our stream.
As text processing steps, we have tokenised all the
tweets and filtered them to be written in English us-
ing the Trendminer pipeline (Preot?iuc-Pietro et al,
2012). We also remove duplicate tweets (retweets
and tweets that had the same first 6 content tokens)
because they likely represent duplicate content, au-
tomated messages or spam which would bias the
dataset, as also stated by Tsur and Rappoport (2012).
In our experiments we use the first month of data
as training and the second month as testing. Note
the challenging nature of this testing configuration
where predictions must be made for up to 28 days
into the future. We keep a total 1176 of hashtags
which appear at least 500 times in both splits of
the data. The vocabulary consists of all the tokens
that occur more than 100 times in the dataset and
start with an alphabetic letter. After processing, our
dataset consists of 6,416,591 tweets with each hav-
ing on average 9.55 tokens.
5 Forecasting hashtag frequency
We treat our task of forecasting the volume of a
Twitter hashtag as a regression problem. Because the
total number of tweets varies depending on the day
and hour of day, we chose to model the proportion of
tweets with the given tag in that hour. Given a time
series of these values as the training set for a hash-
tag, we aim to predict the values in the testing set,
extrapolating to the subsequent month.
Hashtags represent free-form text labels that au-
thors add to a tweet in order to enable other users to
search them to participate in a conversation. Some
users use hashtags as regular words that are integral
to the tweet text, some hashtags are general and re-
fer to the same thing or emotion (#news, #usa, #fail),
others are Twitter games or memes (#2010diss-
apointments, #musicmonday). Other hashtags re-
fer to events which might be short lived (#world-
cup2022), long lived (#25jan) or periodic (#raw,
#americanidol). We chose to model hashtags be-
cause they group similar tweets (like topics), reflect
real world events (some of which are periodic) and
present direct means of evaluation. Note that this
approach could be applied to many other temporal
problems in NLP or other domains. We treat each
regression problem independently, learning for each
hashtag its specific model and set of parameters.
5.1 Methods
We choose multiple baselines for our prediction task
in order to compare the effectiveness of our ap-
982
03/
1
10/
1
17/
1
24/
1
31/
1
07/
2
14/
2
21/
2
28/
2
00.51
#fy
i
 
 
Gol
d
PS Con
st
#fyi
03/
1
10/
1
17/
1
24/
1
31/
1
07/
2
14/
2
21/
2
28/
2
00.51
#co
nfe
ssio
nho
ur
 
 
Gol
d
PS SE
#confessionhour
03/
1
10/
1
17/
1
24/
1
31/
1
07/
2
14/
2
21/
2
28/
2
00.51
#fa
il
 
 
Gol
d
PE
R Con
st
#fail
03/
1
10/
1
17/
1
24/
1
31/
1
07/
2
14/
2
21/
2
28/
2
00.51
#br
eak
fas
t
 
 
Gol
d
PS Con
st
#breakfast
03/
1
10/
1
17/
1
24/
1
31/
1
07/
2
14/
2
21/
2
28/
2
00.51
#ra
w
 
 
Gol
d
PS Con
st
#raw
Figure 5: Sample regressions and their fit using different methods.
proach. These are:
Mean value (M): We use as prediction the mean
of the values in the training set. Note that this is the
same as using a GP model with a constant kernel (+
noise) with a mean equal to the training set mean.
Lag model with GP determined period (Lag+):
The prediction is the mean value in the training set of
the values at lag ? where ? is the period rounded to
the closest integer as determined by our GP model.
This is somewhat similar to an autoregressive (AR)
model with all the coefficients except ? set to 0.
We highlight that given the period ? this is a very
strong model as it gives a mean estimate at each
point. Comparing to this model we can see if the GP
model can recover the underlying function that de-
scribed the periodic variation and filter out the noise
in the observations. Correctly identifying the period
is very challenging as we discuss below.
GP regression: Gaussian Process regression us-
ing only the SE kernel (GP-SE), the periodic ker-
nel (GP-PER), the PS kernel (GP-PS). The method
that chooses between kernels using model selection
as described in Section 3.3 is denoted as GP+. We
will also compare to GP regression the linear kernel
(GP-Lin), but we will not use this as a candidate for
model selection due the poor results shown below.
983
Hashtag
Lag(p) Const SE PER PS
NRMSE NLML NRMSE NLML NRMSE NLML NRMSE NLML NRMSE
#fyi 0.1578 -322 0.1404 -320 0.1898 -321 0.1405 -293 0.1456
#confessionhour 0.0404 -85 0.0107 -186 0.0012 -90 0.0327 -88 0.0440
#fail 0.1431 -376 0.1473 -395 0.4695 -444 0.1387 -424 0.1390
#breakfast 0.1363 -293 0.1508 -333 0.1773 -293 0.1514 -367 0.1276
#raw 0.0464 -1208 0.0863 -1208 0.0863 -1323 0.0668 -1412 0.0454
Table 2: NRMSE shows the best performance for forecasting and NLML shows the best model for all the regressions
in Figure 5. Lower is better.
5.2 Results
We start by qualitatively analysing a few sample re-
gressions that are representative of each category of
time series under study. These are shown in Figure 5.
For clarity, we only plotted a few kernels on each fig-
ure. The full evaluation statistics in NRMSE and the
Bayesian evidence are show in Table 2.
For the hashtag #fyi there is no clear pattern. For
this reason the model that uses the constant kernel
performs best, being the simplest one that can de-
scribe the data, although the others give similar re-
sults in terms of NRMSE on the held-out testing
set. While functions learned using this kernel never
clearly outperform others on NRMSE on held-out
data, this is very useful for interpretation of the time
series, separating noisy time series from those that
have an underlying periodic behaviour.
The #confessionhour example illustrates a be-
haviour best suited for modelling using the SE ker-
nel. We notice a sudden burst in volume which
decays over the next 2 days. This is actually
the behaviour typical of ?internet memes? (this
hashtag tags tweets of people posting things they
would never tell anyone) as presented in Yang and
Leskovec (2011). These cannot be modelled with a
constant kernel or a periodic one as shown by the re-
sults on held-out data and the time series plot. The
periodic kernels will fail in trying to match the large
burst with others in the training data and will at-
tribute to noise the lack of a similar peak, thus dis-
covering wrong periods and making bad predictions.
In this example, forecasts will be very close to 0 un-
der the SE kernel, which is what we would desire
from the model.
The periodic kernel best models hashtags that ex-
hibit an oscillating pattern. For example, this best
fits words that are used frequently during the day
and less so during the night, like #fail. Here, the pe-
riod is chosen to be one week (168) rather than one
day (24) because of the weekly effect superimposed
on the daily one. Our model recovers that there is
a daily pattern with people tweeting about their or
others? failures during the day. On weekends how-
ever, and especially on Friday evenings, people have
better things to do.
The PS kernel introduced in this paper models
best hashtags that have a large and short lived burst
in usage. We show this by two examples. First, we
choose #breakfast which has a daily and weekly pat-
tern. As we would expect, a big rise in usage oc-
curs during the early hours of the day, with very
few occurrences at other times. Our model discov-
ers a weekly pattern as well. This is used mainly
for modelling the difference between weekends and
weekdays. On weekends, the breakfast tag is more
evenly spread during the hours of the morning, be-
cause people do not have to wake up for work and
can have breakfast at a more flexible time than dur-
ing the week. In the second example, we present a
hashtag that is associated to a weekly event: #raw
is used to discuss a wrestling show that airs ev-
ery week for 2 hours on Monday evenings in the
U.S.. With the exception of these 2 hours and the
hour building up to it, the hashtag is rarely used.
This behaviour is modelled very well using our ker-
nel, with a very high value for the shape parame-
ter (s = 200) compared to the previous example
(s = 11) which captures the abrupt trend in usage. In
all cases, our GP model chosen by the evidence per-
forms better than the Lag+ model, which is a very
strong method if presented with the correct period.
This further demonstrates the power of the Gaussian
Process framework to deal with noise in the training
data and to find the underlying function of the time
variation of words.
In Table 3 we present sample tags identified as
984
Const SE PER PS
#funny #2011 #brb #ff
#lego #backintheday #coffee #followfriday
#likeaboss #confessionhour #facebook #goodnight
#money #februarywish #facepalm #jobs
#nbd #haiti #funny #news
#nf #makeachange #love #nowplaying
#notetoself #questionsidontlike #rock #tgif
#priorities #savelibraries #running #twitterafterdark
#social #snow #xbox #twitteroff
#true #snowday #youtube #ww
49 268 493 366
Table 3: Sample hashtags for each category. The last line
shows the total number of hashtags of each type.
Lag+ GP-Lin GP-SE GP-PER GP-PS GP+
7.29% -3.99% -34.5% 0.22% 7.37% 9.22%
Table 4: Average relative gain over mean (M) prediction
for forecasting on the entire month using the different
models
being part of the 4 hashtag categories, and the total
number of hashtags in each.
As a means of quantitative evaluation we com-
pute the relative NRMSE compared to the Mean (M)
method for forecasting. We choose this, because we
consider that NRMSE is not comparable between re-
gression tasks due to the presence of large peaks in
many time series, which distort the NRMSE values.
The results are presented in Table 4 and show that
our Gaussian Process model using model selection
is best. Remarkably, it consistently outperforms the
Lag+ model, which shows the effectiveness of the
GP models to incorporate uncertainty. The GP-PS
model does very well on its own. Although chosen in
the model selection phase in only a third of the tasks,
it performs consistently well across tasks because of
its ability to model well all the periodic hashtags,
be they smooth or abrupt. The GP-Lin model does
worse than the average, mostly due to uni-modal
time series which don?t have high occurrences in the
testing part of the data.
5.3 Discussion
Let us now turn to why the GP model is better
for discovering periodicities than classic time series
modelling methods. Measuring autocorrelation be-
tween points in the time series is used to discover
the hidden periodicities in the data and in building
AR models. However, the downsides of this method
are: a) the incapacity of accurately finding the cor-
rect periods, because all integer multiples of the cor-
0 100 200 300?0.2
0
0.2
0.4
0.6
0.8
Lag
Sam
ple 
Auto
corr
elat
ion
Sample Autocorrelation Function
Figure 6: Sample autocorrelation for #confessionhour
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1?70
?60
?50
?40
?30
?20
?10
Normalized Frequency  (?pi rad/sample)
Pow
er/fr
equ
enc
y (dB
/rad/
samp
le)
Periodogram Power Spectral Density Estimate
Figure 7: Power spectral density for #raw
rect period will be feasible candidates and b) it leads
to incorrect conclusions when there is autocorre-
lated noise. The second case is illustrated in Fig-
ure 6 where #confessionhour shows autocorrelation
but, as seen in Figure 5, lacks a periodic component.
Another approach to discovering periods in data
is by computing the power spectral density. This
has been used in the GP framework by Wilson and
Adams (2013). For some time series, this gives a
good indication of the period, as represented by a
peak in the periodogram at that value. This fails to
discover the correct period when dealing with large
bursts like those exhibited by the #raw time series
as shown in Figure 7. The lowest frequency spike
corresponds to the correct period of 168, but also
other candidate periods are shown as possible. The
reason for this is its reliance on the Fourier Trans-
form which decomposes the time series into a sum
of oscillating patterns. These cannot model step-
functions and other non-smoothly varying signals.
A further discussion falls out of the scope and space
constraints of this paper.
985
Tweet Time Prior Rank Prediction
Bruins Goal!!! Patrice Bergeron makes it 3-1 Boston 2-3am, 2 Feb 2011
E: 0.00017 7 #fb
P: 0.00086 1 #bruins
i need some of Malik people 3-4am, 2 Feb 2011
E: 0.00021 7 #ff
P: 0.00420 1 #thegame
Alfie u doughnut! U didn?t confront Kay? SMH 7-8pm, 3 Feb 2011
E: 0.00027 8 #nowplaying
P: 0.00360 1 #eastenders
Table 5: Example of tweet classification using the Na??ve Bayes model with the two different priors (E - empirical, P -
GP forecast). Rank shows the rank in probability of the correct class (hashtag) under the model. Time is G.M.T.
6 Text based prediction
In this section we demonstrate the usefulness of
our method of modelling in an NLP task: predict-
ing the hashtag of a tweet based on its text. In con-
trast to this classification approach for suggesting a
tweet?s hashtag, information retrieval methods based
on computing similarities between tweets are very
hard to scale to large data (Zangerle et al, 2011).
We choose a simple model for prediction, the
Na??ve Bayes Classifier. This method provides us
with a straightforward way to incorporate our prior
knowledge of how frequent a hashtag is in a certain
time frame. This Na??ve Bayes model (NB-P) uses
the forecasted values for the respective hour as the
prior on the hashtags.
For comparison we use the Most Frequent (MF)
baseline and the Na??ve Bayes with empirical prior
(NB-E) which doesn?t use any temporal forecasting
information. Because there are more than 1000 pos-
sible classes we show the accuracy of the correct
hashtag being amongst the top 1,5 or 50 hashtags
as well as the Mean Reciprocal Rank (MRR). The
results are shown in Table 6.
The results show that incorporating the forecasted
values as a more informative prior for classification
we obtain better predictions. The improvements are
consistent in all the Match values. Also, we high-
light that a 9% improvement in the forecasting task
carries over to about a 2% improvement in classifi-
cation. We show a few examples in which the GP
learned prior makes a difference in classification in
Table 5 together with the values for both priors.
With these experiments, we highlighted that there
are performance gains even with only adding a more
informative prior that uses periodicity information.
This motivates future work to add this information
to discriminative classifiers thus avoiding the need
MF NB-E NB-P
Match@1 7.28% 16.04% 17.39%
Match@5 19.90% 29.51% 31.91%
Match@50 44.92% 59.17% 60.85%
MRR 0.144 0.237 0.252
Table 6: Results for hashtag classification.
for the Na??ve Bayes decomposition. The modelling
framework offered by the GPs can accommodate
classification, although scaling issues arise when us-
ing a large number of features or output classes. Ef-
forts to scale GPs to a large number of variables
are well understood (Candela and Rasmussen, 2005)
and we will try to incorporate this in future work.
7 Conclusion
Periodicities play an important role when analysing
the temporal dimension of text. We have presented
a framework based on Gaussian Process regression
for identifying periodic patterns and their parame-
ters using only training data. We divided the periodic
patterns into 2 categories: oscillating and periodic
bursts by performing model selection using bayesian
evidence. The periodicities we have discovered have
proven useful in an NLP classification task.
In future work, we aim to model time continu-
ously and to perform discriminative clustering in or-
der to make better use of the learned periodicites.
We will consider incorporating periodicities in other
applications, such as topic models.
Acknowledgements
This research was funded by the Trendminer
project, EU FP7-ICT Programme, grant agreement
no.287863. The authors would like to thank James
Hensman, Nicolas Durrande and Neil Lawrence for
advice on Gaussian Processes, Chris Dyer and Noah
Smith for discussions about periodicities in NLP.
986
References
David Blei and John Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd International con-
ference on Machine learning, ICML ?06.
Joaquin Quin?onero Candela and Carl Edward Ras-
mussen. 2005. A Unifying View of Sparse Approx-
imate Gaussian Process Regression. Journal of Ma-
chine Learning Research (JMLR), 6:1939?1959, De-
cember.
Trevor Cohn and Lucia Specia. 2013. Modelling An-
notator Bias with Multi-task Gaussian Processes: An
Application to Machine Translation Quality Estima-
tion. In Proceedings of the Association of Computa-
tional Linguistics, ACL ?13.
Nicolas Durrande, James Hensman, Magnus Rattray, and
Neil Lawrence. 2013. Gaussian Process models
for periodicity detection. In Submitted to JRSSb,
http://arxiv.org/abs/1303.7090.
David Duvenaud, James Robert Lloyd, Roger Grosse,
Joshua B. Tenenbaum, and Zoubin Ghahramani.
2013. Structure discovery in nonparametric regression
through compositional kernel search. In Proceedings
of the International Conference on Machine Learning,
ICML ?13.
Mehmet Go?nen and Ethem Alpaydin. 2011. Multi-
ple Kernel Learning Algorithms. Journal of Machine
Learning Research (JMLR), 12:2211?2268, July.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
(Suppl 1):5228?5235, April.
Philipp Hennig, David H. Stern, Ralf Herbrich, and Thore
Graepel. 2012. Kernel topic models. Journal of Ma-
chine Learning Research (JMLR) - Proceedings Track,
22:511?519.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In Proceedings of the 15th ACM SIGKDD Inter-
national conference on Knowledge discovery and data
mining, KDD ?09.
Zongyang Ma, Aixin Sun, and Gao Cong. 2012. Will
this #hashtag be popular tomorrow? In Proceedings of
the 35th International ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?12.
Zongyang Ma, Aixin Sun, and Gao Cong. 2013. On pre-
dicting the popularity of newly emerging hashtags in
Twitter. Journal of the American Society for Informa-
tion Science and Technology, 64(7):1399?1410.
Allie Mazzia and James Juett. 2011. Sug-
gesting hashtags on Twitter. In http://www-
personal.umich.edu/ amazzia/pubs/545-final.pdf.
James McInerney, Alex Rogers, and Nicholas R Jennings.
2013. Learning periodic human behaviour models
from sparse data for crowdsourcing aid delivery in
developing countries. In Proceedings of the Twenty-
Ninth Conference on Uncertainty in Artificial Intelli-
gence, UAI ?13.
Tamara Polajnar, Simon Rogers, and Mark Girolami.
2011. Protein interaction detection in sentences via
Gaussian Processes: a preliminary evaluation. In-
ternational Journal Data Mining and Bioinformatics,
5(1):52?72, February.
Daniel Preot?iuc-Pietro and Trevor Cohn. 2013. Mining
User Behaviours: A Study of Check-in Patterns in Lo-
cation Based Social Networks. In Proceedings of the
ACM Web Science Conference, Web Science ?13.
Daniel Preot?iuc-Pietro, Sina Samangooei, Trevor Cohn,
Nicholas Gibbins, and Mahesan Niranjan. 2012.
Trendminer: An architecture for real time analysis of
social media text. Proceedings of the Sixth Interna-
tional AAAI Conference on Weblogs and Social Media,
Workshop on Real-Time Analysis and Mining of Social
Streams.
Carl Edward Rasmussen and Zoubin Ghahramani. 2000.
Occam?s razor. In Advances in Neural Information
Processing Systems, NIPS 13.
Carl Edward Rasmussen and Christopher K. I. Williams.
2005. Gaussian Processes for Machine Learning.
MIT Press.
Daniel M. Romero, Brendan Meeder, and Jon Klein-
berg. 2011. Differences in the mechanics of informa-
tion diffusion across topics: idioms, political hashtags,
and complex contagion on Twitter. In Proceedings of
the 20th International conference on World wide web,
WWW ?11.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In MT Summit ?13.
Oren Tsur and Ari Rappoport. 2012. What?s in a hash-
tag? Content based prediction of the spread of ideas
in microblogging communities. In Proceedings of the
fifth ACM International conference on Web search and
data mining, WSDM ?12.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
International conference on Knowledge discovery and
data mining, KDD ?06.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time Dynamic topic models. In
Proceedings of the Twenty-Fourth Conference on Un-
certainty in Artificial Intelligence, UAI ?08.
Andrew Gordon Wilson and Ryan Prescott Adams. 2013.
Gaussian Process covariance kernels for pattern dis-
987
covery and extrapolation. In Proceedings of the Inter-
national Conference on Machine Learning, ICML ?13.
Jaewon Yang and Jure Leskovec. 2011. Patterns of tem-
poral variation in online media. In Proceedings of the
fourth ACM International conference on Web search
and data mining, WSDM ?11.
Lei Yang, Tao Sun, Ming Zhang, and Qiaozhu Mei. 2012.
We know what @you #tag: does the dual role affect
hashtag adoption? In Proceedings of the 21st Interna-
tional conference on World Wide Web, WWW ?12.
Dani Yogatama, Michael Heilman, Brendan O?Connor,
Chris Dyer, Bryan R. Routledge, and Noah A. Smith.
2011. Predicting a scientific community?s response
to an article. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11.
Eva Zangerle, Wolfgang Gassler, and Gunther Specht.
2011. Recommending #-tags in twitter. In Proceed-
ings of the Workshop on Semantic Adaptive Social
Web, UMAP ?11.
988
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886?897,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
What Can We Get From 1000 Tokens?
A Case Study of Multilingual POS Tagging For Resource-Poor Languages
Long Duong,
12
Trevor Cohn,
1
Karin Verspoor,
1
Steven Bird,
1
and Paul Cook
1
1
Department of Computing and Information Systems,
The University of Melbourne
2
National ICT Australia, Victoria Research Laboratory
lduong@student.unimelb.edu.au
{t.cohn, karin.verspoor, sbird, paulcook}@unimelb.edu.au
Abstract
In this paper we address the problem
of multilingual part-of-speech tagging for
resource-poor languages. We use par-
allel data to transfer part-of-speech in-
formation from resource-rich to resource-
poor languages. Additionally, we use a
small amount of annotated data to learn to
?correct? errors from projected approach
such as tagset mismatch between lan-
guages, achieving state-of-the-art perfor-
mance (91.3%) across 8 languages. Our
approach is based on modest data require-
ments, and uses minimum divergence clas-
sification. For situations where no uni-
versal tagset mapping is available, we
propose an alternate method, resulting
in state-of-the-art 85.6% accuracy on the
resource-poor language Malagasy.
1 Introduction
Part-of-speech (POS) tagging is a crucial task for
natural language processing (NLP) tasks, provid-
ing basic information about syntax. Supervised
POS tagging has achieved great success, reach-
ing as high as 95% accuracy for many languages
(Petrov et al., 2012). However, supervised tech-
niques need manually annotated data, and this
is either lacking or limited in most resource-
poor languages. Fully unsupervised POS tagging
is not yet useful in practice due to low accu-
racy (Christodoulopoulos et al., 2010). In this pa-
per, we propose a semi-supervised method to nar-
row the gap between supervised and unsupervised
approaches. We demonstrate that even a small
amount of supervised data leads to substantial im-
provement.
Our method is motivated by the availability of
parallel data. Thanks to the development of mul-
tilingual documents from government projects,
book translations, multilingual websites, and so
forth, parallel data between resource-rich and
resource-poor languages is relatively easy to ac-
quire. This parallel data provides the bridge that
permits us to transfer POS information from a
resource-rich to a resource-poor language.
Systems that make use of cross-lingual tag
projection typically face several issues, includ-
ing mismatches between the tagsets used for the
languages, artifacts from noisy alignments and
cross-lingual syntactic divergence. Our approach
compensates for these issues by training on a
small amount of annotated data on the target side,
demonstrating that only 1k tokens of annotated
data is sufficient to improve performance.
We first tag the resource-rich language using a
supervised POS tagger. We then project POS tags
from the resource-rich language to the resource-
poor language using parallel word alignments.
The projected labels are noisy, and so we use
various heuristics to select only ?good? training
examples. We train the model in two stages.
First, we build a maximum entropy classifier T
on the (noisy) projected data. Next, we train
a supervised classifier P on a small amount of
annotated data (1,000 tokens) in the target lan-
guage, using a minimum divergence technique
to incorporate the first model, T . Compared
with the state of the art (T?ackstr?om et al., 2013),
we make more-realistic assumptions (e.g. relying
on a tiny amount of annotated data rather than
a huge crowd-sourced dictionary) and use less
parallel data, yet achieve a better overall result.
We achieved 91.3% average accuracy over 8 lan-
guages, exceeding T?ackstr?om et al. (2013)?s result
of 88.8%.
The test data we employ makes use of map-
pings from language-specific POS tag inventories
to a universal tagset (Petrov et al., 2012). How-
ever, such a mapping might not be available for
resource-poor languages. Therefore, we also pro-
886
pose a variant of our method which removes the
need for identical tagsets between the projection
model T and the correction model P , based on
a two-output maximum entropy model over tag
pairs. Evaluating on the resource-poor language
Malagasy, we achieved 85.6% accuracy, exceed-
ing the state-of-the-art of 81.2% (Garrette et al.,
2013).
2 Background and Related Work
There is a wealth of prior work on multilingual
POS tagging. The simplest approach takes advan-
tage of the typological similarities that exist be-
tween languages pairs such as Czech and Russian,
or Serbian and Croatian. They build the tagger
? or estimate part of the tagger ? on one lan-
guage and apply it to the other language (Reddy
and Sharoff, 2011, Hana et al., 2004).
Yarowsky and Ngai (2001) pioneered the use of
parallel data for projecting tag information from
a resource-rich language to a resource-poor lan-
guage. Duong et al. (2013b) used a similar method
on using sentence alignment scores to rank the
goodness of sentences. They trained a seed model
from a small part of the data, then applied this
model to the rest of the data using self-training
with revision.
Das and Petrov (2011) also used parallel data
but additionally exploited graph-based label prop-
agation to expand the coverage of labelled tokens.
Each node in the graph represents a trigram in the
target language. Each edge connects two nodes
which have similar context. Originally, only some
nodes received a label from direct label projection,
and then labels were propagated to the rest of the
graph. They only extracted the dictionary from
the graph because the labels of nodes are noisy.
They used the dictionary as the constraints for a
feature-based HMM tagger (Berg-Kirkpatrick et
al., 2010). Both Duong et al. (2013b) and Das and
Petrov (2011) achieved 83.4% accuracy on the test
set of 8 European languages.
Goldberg et al. (2008) pointed out that, with the
presence of a dictionary, even an incomplete one,
a modest POS tagger can be built using simple
methods such as expectation maximization. This
is because most of the time, words have a very
limited number of possible tags, thus a dictionary
that specifies the allowable tags for a word helps
to restrict the search space. With a gold-standard
dictionary, Das and Petrov (2011) achieved an ac-
curacy of approximately 94% on the same 8 lan-
guages. The effectiveness of a gold-standard dic-
tionary is undeniable, however it is costly to build
one, especially for resource-poor languages. Li et
al. (2012) used the dictionary from Wiktionary,
1
a
crowd-sourced dictionary. They scored 84.8% ac-
curacy on the same 8 languages. Currently, Wik-
tionary covers over 170 languages, but the cov-
erage varies substantially between languages and,
unsurprisingly, it is poor for resource-poor lan-
guages. Therefore, relying on Wiktionary is not
effective for building POS taggers for resource-
poor languages.
T?ackstr?om et al. (2013) combined both token
information (from direct projected data) and type
constraints (from Wiktionary?s dictionary) to form
the state-of-the-art multilingual tagger. They built
a tag lattice and used these token and type con-
straints to prune it. The remaining paths are the
training data for a CRF tagger. They achieved
88.8% accuracy on the same 8 languages.
Table 1 summarises the performance of the
above models across all 8 languages. Note that
these methods vary in their reliance on external
resources. Duong et al. (2013b) use the least, i.e.
only the Europarl Corpus (Koehn, 2005). Das and
Petrov (2011) additionally use the United Nation
Parallel Corpus. Li et al. (2012) didn?t use any par-
allel text but used Wiktionary instead. T?ackstr?om
et al. (2013) exploited more parallel data than Das
and Petrov (2011) and also used a dictionary
from Li et al. (2012).
Another approach for resource-poor languages
is based on the availability of a small amount
of annotated data. Garrette et al. (2013) built a
POS tagger for Kinyarwanda and Malagasy. They
didn?t use parallel data but instead exploited four
hours of manual annotation to build?4,000 tokens
or ?3,000 word-types of annotated data. These
tokens or word-types were used to build a tag dic-
tionary. They employed label propagation for ex-
panding the coverage of this dictionary in a sim-
ilar vein to Das and Petrov (2011), but they also
used an external dictionary. They built training
examples using the combined dictionary and then
trained the tagger on this data. They achieved
81.9% and 81.2% accuracy for Kinyarwanda and
Malagasy respectively. Note that their usage of an
external dictionary compromises their claim of us-
ing only 4 hours of annotation.
1
http://www.wiktionary.org/
887
da nl de el it pt es sv Average
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages
? Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish
(sv) ? evaluated on CoNLL data (Buchholz and Marsi, 2006).
The method we propose in this paper is similar
in only using a small amount of annotation. How-
ever, we directly use the annotated data to train
the model rather than using a dictionary. We argue
that with a proper ?guide?, we can take advantage
of very limited annotated data.
2.1 Annotated data
Our annotated data mainly comes from CoNLL
shared tasks on dependency parsing (Buchholz
and Marsi, 2006). The language specific tagsets
are mapped into the universal tagset. We will
use this annotated data mainly for evaluation. Ta-
ble 2 shows the size of annotated data for each
language. The 8 languages we are considering
in this experiment are not actually resource-poor
languages. However, running on these 8 lan-
guages makes our system comparable with pre-
viously proposed methods. Nevertheless, we try
to use as few resources as possible, in order to
simulate the situation for resource-poor languages.
Later in Section 6 we adapt the approach for Mala-
gasy, a truly resource-poor language.
2.2 Universal tagset
We employ the universal tagset from (Petrov et
al., 2012) for our experiment. It consists of 12
common tags: NOUN, VERB, ADJ (adjective),
ADV (adverb), PRON (pronoun), DET (deter-
miner and article), ADP (preposition and post-
position), CONJ (conjunctions), NUM (numeri-
cal), PRT (particle), PUNC (punctuation) and X
(all other categories including foreign words and
abbreviations). Petrov et al. (2012) provide the
mapping from each language-specific tagset to the
universal tagset.
The idea of using the universal tagset is of great
use in multilingual applications, enabling compar-
ison across languages. However, the mapping is
not always straightforward. Table 2 shows the size
of the annotated data for each language, the num-
ber of tags presented in the data, and the list of
tags that are not matched. We can see that only 8
tags are presented in the annotated data for Dan-
ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are
missing.
2
Thus, a classifier using all 12 tags will
be heavily penalized in the evaluation.
Li et al. (2012) considered this problem and
tried to manually modify the Danish mappings.
Moreover, PRT is not really a universal tag since
it only appears in 3 out of the 8 languages. Plank
et al. (2014) pointed out that PRT often gets con-
fused with ADP even in English. We will later
show that the mapping problem causes substantial
degradation in the performance of a POS tagger
exploiting parallel data. The method we present
here is more target-language oriented: our model
is trained on the target language, in this way, only
relevant information from the source language is
retained. Thus, we automatically correct the map-
ping, and other incompatibilities arising from in-
correct alignments and syntactic divergence be-
tween the source and target languages.
Lang Size(k) # Tags Not Matched
da 94 8 DET, PRT, PUNC, NUM
nl 203 11 PRT
de 712 12
el 70 12
it 76 11 PRT
pt 207 11 PRT
es 89 11 PRT
sv 191 11 DET
AVG 205
Table 2: The size of annotated data from
CoNLL (Buchholz and Marsi, 2006), and the
number of tags included and missing for 8 lan-
guages.
2
Many of these are mistakes in the mapping, however,
they are indicative of the kinds of issues expected in low-
resource languages.
888
3 Directly Projected Model (DPM)
In this section we describe a maximum entropy
tagger that only uses information from directly
projected data.
3.1 Parallel data
We first collect Europarl data having English as
the source language, an average of 1.85 million
parallel sentences for each of the 8 language pairs.
In terms of parallel data, we use far less data com-
pared with other recent work. Das and Petrov
(2011) used Europarl and the ODS United Na-
tion dataset, while T?ackstr?om et al. (2013) addi-
tionally used parallel data crawled from the web.
The amount of parallel data is crucial for align-
ment quality. Since DPM uses alignments to trans-
fer tags from source to target language, the per-
formance of DPM (and other models that exploit
projection) largely depends on the quantity of par-
allel data. The ?No LP? model of Das and Petrov
(2011), which only uses directly projected labels
(without label propagation), scored 81.3% for 8
languages. However, using the same model but
with more parallel data, T?ackstr?om et al. (2013)
scored 84.9% on the same test set.
3.2 Label projection
We use the standard alignment tool Giza++ (Och
and Ney, 2003) to word align the parallel data. We
employ the Stanford POS tagger (Toutanova et al.,
2003) to tag the English side of the parallel data
and then project the label to the target side. It has
been confirmed in many studies (T?ackstr?om et al.,
2013, Das and Petrov, 2011, Toutanova and John-
son, 2008) that directly projected labels are noisy.
Thus we need a method to reduce the noise. We
employ the strategy of Yarowsky and Ngai (2001)
of ranking sentences using a their alignment scores
from IBM model 3.
Firstly, we want to know how noisy the pro-
jected data is. Thus, we use the test data to build
a simple supervised POS tagger using the TnT
tagger (Brants, 2000) which employs a second-
order Hidden Markov Model (HMM). We tag the
projected data and compare the label from direct
projection and from the TnT tagger. The labels
from the TnT Tagger are considered as pseudo-
gold labels. Column ?Without Mapping? from Ta-
ble 3 shows the average accuracy for the first n-
sentences (n = 60k, 100k, 200k, 500k) for 8 lan-
guages according to the ranking. Column ?Cov-
erage? shows the percentages of projected label
(the other tokens are Null aligned). We can see
that when we select more data, both coverage and
accuracy fall. In other words, using the sentence
alignment score, we can rank sentences with high
coverage and accuracy first. However, even after
ranking, the accuracy of projected labels is less
than 80% demonstrating how noisy the projected
labels are.
Table 3 (column ?With Mapping?) additionally
shows the accuracy using simple tagset mapping,
i.e. mapping each tag to the tag it is assigned most
frequently in the test data. For example DET, PRT,
PUNC, NUM, missing from Danish gold data, will
be matched to PRON, X, X, ADJ respectively. This
simple matching yields a ? 4% (absolute) im-
provement in average accuracy. This illustrates the
importance of handling tagset mapping carefully.
3.3 The model
In this section, we introduce a maximum entropy
tagger exploiting the projected data. We select the
first 200k sentences from Table 3 for this experi-
ment. This number represents a trade-off between
size and accuracy. More sentences provide more
information but at the cost of noisier data. Duong
et al. (2013b) also used sentence alignment scores
to rank sentences. Their model stabilizes after us-
ing 200k sentences. We conclude that 200k sen-
tences is enough and capture most information
from the parallel data.
Features Descriptions
W@-1 Previous word
W@+1 Next word
W@0 Current word
CAP First character is capitalized
NUMBER Is number
PUNCT Is punctuation
SUFFIX@k Suffix up to length 3 (k <= 3)
WC Word class
Table 4: Feature template for a maximum entropy
tagger
We ignore tokens that don?t have labels, which
arise from null alignments and constitute approxi-
mately 14% of the data. The remaining data (?1.4
million tokens) are used to train a maximum en-
tropy (MaxEnt) model. MaxEnt is one of the
simplest forms of probabilistic classifier, and is
appropriate in this setting due to the incomplete
889
Data Size (k) Coverage (%) Without Mapping With Mapping
60 91.5 79.9 84.2
100 89.1 79.4 83.6
200 86.1 79.1 82.9
500 82.4 78.0 81.5
Table 3: The coverage, and POS tagging accuracy with and without tagset mapping of directly projected
labels, averaged over 8 languages for different data sizes
Model da nl de el it pt es sv Avg
All features 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
- Word Class 64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9
- Suffix 64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6
- Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6
- Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8
Table 5: The accuracy of Directed Project Model (DPM) with different feature sets, removing one feature
set at a time
sequence data. While sequence models such as
HMMs or CRFs can provide more accurate mod-
els of label sequences, they impose a more strin-
gent training requirement.
3
We also experimented
with a first-order linear chain CRF trained on con-
tiguous sub-sequences but observed ? 4% (abso-
lute) drop in performance.
The maximum entropy classifier estimates the
probability of tag t given a word w as
P (t|w) =
1
Z(w)
exp
D
?
j=1
?
j
f
j
(w, t) ,
where Z(w) =
?
t
exp
?
D
j=1
?
j
f
j
(w, t) is the
normalization factor to ensure the probabilities
P (t|w) sum to one. Here f
j
is a feature function
and ?
j
is the weight for this feature, learned as
part of training. We use Maximum A Posteriori
(MAP) estimation to maximize the log likelihood
of the training data, D = {w
i
, t
i
}
N
i=1
, subject to a
zero-mean Gaussian regularisation term,
L = logP (?)
N
?
i=1
P (t
(i)
|w
(i)
)
= ?
D
?
j=1
?
2
j
2?
2
+
N
?
i=1
D
?
j=1
?
j
f
j
(w
i
, t
i
)? logZ(w
i
)
where the regularisation term limits over-fitting,
an important concern when using large feature
3
T?ackstr?om et al. (2013) train a CRF on incomplete data,
using a tag dictionary heuristic to define a ?gold standard?
lattice over label sequences.
sets. For our experiments we set ?
2
= 1. We use
L-BFGS which performs gradient ascent to maxi-
mize L. Table 4 shows the features we considered
for building the DPM. We use mkcls, an unsu-
pervised method for word class induction which is
widely used in machine translation (Och, 1999).
We run mkcls to obtain 100 word classes, using
only the target language side of the parallel data.
Table 5 shows the accuracy of the DPM evalu-
ated on 8 languages (?All features model?). DPM
performs poorly on Danish, probably because of
the tagset mapping issue discussed above. The
DPM result of 80.2% accuracy is encouraging,
particularly because the model had no explicit su-
pervision.
To see what features are meaningful for our
model, we remove features in turn and report
the result. The result in Table 5 disagrees with
T?ackstr?om et al. (2013) on the word class features.
They reported a gain of approximately 3% (ab-
solute) using the word class. However, it seems
to us that these features are not especially mean-
ingful (at least in the present setting). Possible
reasons for the discrepancy are that they train the
word class model on a massive quantity of exter-
nal monolingual data, or their algorithms for word
clustering are better (Uszkoreit and Brants, 2008).
We can see that the most informative features are
Capitalization, Number and Punctuation. This
makes sense because in languages such as Ger-
man, capitalization is a strong indicator of NOUN.
Number and punctuation features ensure that we
classify NUM and PUNCT tags correctly.
890
4 Correction Model
In this section we incorporate the directly pro-
jected model into a second correction model
trained on a small supervised sample of 1,000 an-
notated tokens. Our DPM model is not very accu-
rate; as we have discussed it makes many errors,
due to invalid or inconsistent tag mappings, noisy
alignments, and cross-linguistic syntactic diver-
gence. However, our aim is to see how effectively
we can exploit the strengths of the DPM model
while correcting for its inadequacies using direct
supervision. We select only 1,000 annotated to-
kens to reflect a low resource scenario. A small
supervised training sample is a more realistic form
of supervision than a tag dictionary (noisy or oth-
erwise). Although used in most prior work, a tag
dictionary for a new language requires significant
manual effort to construct. Garrette and Baldridge
(2013) showed that a 1,000 token dataset could be
collected very cheaply, requiring less than 2 hours
of non-expert time.
Our correction model makes use of a mini-
mum divergence (MD) model (Berger et al., 1996),
a variant of the maximum entropy model which
biases the target distribution to be similar to a
static reference distribution. The method has been
used in several language applications including
machine translation (Foster, 2000) and parsing
(Plank and van Noord, 2008, Johnson and Riezler,
2000). These previous approaches have used var-
ious sources of reference distribution, e.g., incor-
porating information from a simpler model (John-
son and Riezler, 2000) or combining in- and out-
of-domain models (Plank and van Noord, 2008).
Plank and van Noord (2008) concluded that this
method for adding prior knowledge only works
with high quality reference distributions, other-
wise performance suffers.
In contrast to these previous approaches, we
consider the specific setting where both the
learned model and the reference model s
o
=
P (t|w) are both maximum entropy models. In this
case we show that the MD setup can be simplified
to a regularization term, namely a Gaussian prior
with a non-zero mean. We model the classification
probability, P
?
(t|w) as the product between a base
model and a maximum entropy classifier,
P
?
(t|w) ? P (t|w) exp
D
?
j=1
?
j
f
j
(w, t)
where here we use the DPM model as base model
P (t|w). Under this setup, where P
?
uses the same
features as P , and both are log-linear models, this
simplifies to
P
?
(t|w) ? exp
?
?
D
?
j=1
?
j
f
j
(w, t) +
D
?
j=1
?
j
f
j
(w, t)
?
?
? exp
D
?
j=1
(?
j
+ ?
j
) f
j
(w, t) (1)
where the constant of proportionality is Z
?
(w) =
?
t
exp
?
D
j=1
(?
j
+ ?
j
) f
j
(w, t). It is clear that
Equation (1) also defines a maximum entropy clas-
sifier, with parameters ?
j
= ?
j
+ ?
j
, and conse-
quently this might seem to be a pointless exercise.
The utility of this approach arises from the prior:
MAP training with a zero mean Gaussian prior
over ? is equivalent to a Gaussian prior over the
aggregate weights, ?
j
? N (?
j
, ?
2
). This prior
enforces parameter sharing between the two mod-
els by penalising parameter divergence from the
underlying DPM model ?. The resulting training
objective is
L
corr
= logP (t|w, ?)?
1
2?
2
D
?
j=1
(?
j
? ?
j
)
2
which can be easily optimised using standard
gradient-based methods, e.g., L-BFGS. The con-
tribution of the regulariser is scaled by the constant
1
2?
2
.
4.1 Regulariser sensitivity
Careful tuning of the regularisation term ?
2
is crit-
ical for the correction model, both to limit over-
fitting on the very small training sample of 1,000
tokens, and to control the extent of the influence
of the DPM model over the correction model.
A larger value of ?
2
lessens the reliance on the
DPM and allows for more flexible modelling of
the training set, while a small value of ?
2
forces
the parameters to be close to the DPM estimates at
the expense of data fit. We expect the best value
to be somewhere between these extremes, and use
line-search to find the optimal value for ?
2
. For
this purpose, we hold out 100 tokens from the
1,000 instance training set, for use as our devel-
opment set for hyper-parameter selection.
From Figure 1, we can see that the model per-
forms poorly on small values of ?
2
. This is under-
standable because the small ?
2
makes the model
891
ll
l
l
l l l l l
l l
0.0
1 0.1 1 10 70 100 100
0
100
00
1e+
05
1e+
06
1e+
07
Variance
80
84
88
Acc
ura
cy (%
) 
l Average Acc
Figure 1: Sensitivity of regularisation parameter
?
2
against the average accuracy measured on 8
languages on the development set
too similar to DPM, which is not very accurate
(80.2%). At the other extreme, if ?
2
is large, the
DPM model is ignored, and the correction model
is equivalent with the supervised model (? 88%
accuracy). We select the value of ?
2
= 70, which
maximizes the accuracy on the development set.
4.2 The model
Using the value of ?
2
= 70, we retrain the model
on the whole 1,000-token training set and evalu-
ate the model on the rest of the annotated data.
Table 6 shows the performance of DPM, Super-
vised model, Correction model and the state-of-
the-art model (T?ackstr?om et al., 2013). The super-
vised model trains a maximum entropy tagger us-
ing the same features as in Table 4 on this 1000 to-
kens. The only difference between the supervised
model and the correction model is that in the cor-
rection model we additionally incorporate DPM as
the prior.
The supervised model performs surprisingly
well confirming that our features are meaning-
ful in distinguishing between tags. This model
achieves high accuracy on Danish compared with
other languages probably because Danish is eas-
ier to learn since it contains only 8 tags. Despite
the fact that the DPM is not very accurate, the cor-
rection model consistently outperforms the super-
vised model on all considered languages, approx-
imately 4.3% (absolute) better on average. This
shows that our method of incorporating DPM to
the model is efficient and robust.
The correction model performs much bet-
ter than the state-of-the-art for 7 languages but
l
l l
l l l
l l
l l l
100 300 500 700 100
0
150
0
200
0
500
0
100
00
150
00
500
00
Data Size
65
75
85
95
Acc
urac
y (%
) 
l Correction ModelSupervised Model
Figure 2: Learning curve for correction model and
supervised model: the x-axis is the size of data
(number of tokens); the y-axis is the average ac-
curacy measured on 8 languages; the dashed line
shows the data condition reported in Table 6
slightly worse for 1 language. On average we
achieve 91.3% accuracy compared with 88.8%
for the state-of-the-art, an error rate reduction of
22.3%. This is despite using fewer resources and
only modest supervision.
5 Analysis
Tagset mismatch In the correction model, we
implicitly resolve the mismatched tagset issue.
DPM might contain tags that don?t appear in the
target language or generally are errors in the map-
ping. However, when incorporating DPM into the
correction model, only the feature weight of tags
that appear in the target language are retained. In
general, because we don?t explicitly do any map-
ping between languages, we might have trouble if
the tagset size of the target language is bigger than
the source language tagset. However, this is not
the case for our experiment because we choose En-
glish as the source-side and English has the full 12
tags.
Learning curve We investigate the impact of
the number of available annotated tokens on the
correction model. Figure 2 shows the learning
curve of the correction model and the supervised
model. We can clearly see the differences be-
tween 2 models when the size of training data is
small. For example, at 100 tokens, the difference
is very large, approximately 18% (absolute), it is
also 6% (absolute) better than DPM. This differ-
ence diminishes as we add more data. This make
sense because when we add more data, the super-
vised model become stronger, while the effective-
892
Model da nl de el it pt es sv Avg
DPM 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0
Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3
DPM (with dict) 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8
Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6
Table 6: The comparison of our Directly Projected Model, Supervised Model, Correction Model and the
state-of-the-art system (T?ackstr?om et al., 2013). The best performance for each language is shown in
bold. The models that are built with a dictionary are provided for reference.
ness of the DPM prior on the correction model is
wearing off. An interesting observation is that the
correction model is always better, even when we
add massive amounts of annotated data. At 50,000
tokens, when the supervised model reaches 96%
accuracy, the correction model is still 0.3% (abso-
lute) better, reaching 96.3%. It means that even
at that high level of confidence, some informa-
tion can still be added from DPM to the correc-
tion model. This improvement probably comes
from the observation that the ambiguity in one
language is explained through the alignment. It
also suggests that this method could improve the
performance of a supervised POS tagger even for
resource-rich languages.
Our methods are also relevant for annotation
projects for resource-poor languages. Assuming
that it is very costly to annotate even 100 tokens,
applying our methods can save annotation effort
but maintain high performance. For example, we
just need 100 tokens to match the accuracy of a su-
pervised method trained on 700 tokens, or we just
need 500 tokens to match the performance with
nearly 2,000 tokens of supervised learning.
Our method is simple, but particularly suitable
for resource-poor languages. We need a small
amount of annotated data for a high performance
POS tagger. For example, we need only around
300 annotated tokens to reach the same accuracy
as the state-of-the-art unsupervised POS tagger
(88.8%).
Tag dictionary Although, it is not our objec-
tive to rely on the dictionary, we are interested
in whether the gains from the correction model
still persist when the DPM performance is im-
proved. We attempt to improve DPM, following
the method of Li et al. (2012) by building a tag dic-
tionary using Wiktionary. This dictionary is then
used as a feature which fires for word-tag pairings
present in the dictionary. We expect that when we
add this additional supervision, the DPM model
should perform better. Table 6 shows the perfor-
mance of DPM and the correction model when in-
corporating the dictionary. The DPM model only
increases 0.6% absolute but the correction model
increases 1.3%. Additionally, it shows that our
model can improve further by incorporating exter-
nal information where available.
CRF Our approach of using simple classifiers
begs the question of whether better results could
be obtained using sequence models, such as con-
ditional random fields (CRFs). As mentioned pre-
viously, a CRF is not well suited for incomplete
data. However, as our second ?correction? model
is trained on complete sequences, we now con-
sider using a CRF in this stage. The training al-
gorithm is as follows: first we estimate the DPM
feature weights on the incomplete data as before,
and next we incorporate the feature weights into a
CRF trained on the 1,000 annotated tokens. This is
complicated by the different feature sets between
the MaxEnt classifier and the CRF, however the
classifier uses a strict subset of the CRF features.
Thus, we use the minimum divergence prior for
the token level features, and a standard zero-mean
prior for the sequence features. That is, the ob-
jective function of the CRF correction model be-
comes:
L
corr
crf
= logP (t|w, ?)
?
1
2?
2
1
?
j?F
1
(?
j
? ?
j
)
2
?
1
2?
2
2
?
j?F
2
?
2
j
(2)
where F
1
is the set of features referring to only
one label as in the DPM maxent model and F
2
is the set of features over label pairs. The union
of F = F
1
? F
2
is the set of all features for
the CRF. We perform grid search using held out
893
data as before for ?
2
1
and ?
2
2
. The CRF correc-
tion model scores 88.1% compared with 86.5% of
the supervised CRF model trained on the 1,000
tokens. Clearly, this is beneficial, however, the
CRF correction model still performs worse than
the MaxEnt correction model (91.3%). We are not
sure why but one reason might be overfitting of
the CRF, due to its large feature set and tiny train-
ing sample. Moreover, this CRF approach is or-
thogonal to T?ackstr?om et al. (2013): we could use
their CRF model as the DPM model and train the
CRF correction model using the same minimum
divergence method, presumably resulting in even
higher performance.
6 Two-output model
Garrette and Baldridge (2013) also use only a
small amount of annotated data, evaluating on
two resource-poor languages Kinyarwanda (KIN)
and Malagasy (MLG). As a simple baseline, we
trained a maxent supervised classifier on this data,
achieving competitive results of 76.4% and 80.0%
accuracy compared with their published results
of 81.9% and 81.2% for KIN and MLG, respec-
tively. Note that the Garrette and Baldridge (2013)
method is much more complicated than this base-
line, and additionally uses an external dictionary.
We want to further improve the accuracy of
MLG using parallel data. Applying the technique
from Section 4 will not work directly, due to the
tagset mismatch (the Malagasy tagset contains 24
tags) which results in highly different feature sets.
Moreover, we don?t have the language expertise
to manually map the tagset. Thus, in this section,
we propose a method capable of handling tagset
mismatch. For data, we use a parallel English-
Malagasy corpus of ?100k sentences,
4
and the
POS annotated dataset developed by Garrette and
Baldridge (2013), which comprises 4230 tokens
for training and 5300 tokens for testing.
6.1 The model
Traditionally, MaxEnt classifiers are trained us-
ing a single label.
5
The method we propose is
trained with pairs of output labels: one for the
4
http://www.ark.cs.cmu.edu/global-voices/
5
Or else a sequence of labels, in the case of a conditional
random field (Lafferty et al., 2001). However, even in this
case, each token is usually assigned a single label. An excep-
tion is the factorial CRF (Sutton et al., 2007), which models
several co-dependent sequences. Our approach is equivalent
to a factorial CRF without edges between tags for adjacent
tokens in the input.
Malagasy tag (t
M
) and one for the universal tag
(t
U
), which are both predicted conditioned on a
Malagasy word (w
M
) in context. Our two-output
model is defined as
P (t
M
, t
U
|w
M
) =
1
Z(w
M
)
exp
(
D
?
j=1
?
j
f
M
j
(w, t
M
)
+
E
?
j=1
?
j
f
U
j
(w, t
U
) +
F
?
j=1
?
j
f
B
j
(w, t
M
, t
U
)
)
(3)
where f
M
, f
U
, f
B
are the feature functions con-
sidering t
M
only, t
U
only, and over both outputs
t
M
and t
U
respectively, and Z(w
M
) is the parti-
tion function. We can think of Eq. (3) as the com-
bination of 3 models: the Malagasy maxent super-
vised model, the DPM model, and the tagset map-
ping model. The central idea behind this model is
to learn to predict not just the MLG tags, as in a
standard supervised model, but also to learn the
mapping between MLG and the noisy projected
universal tags. Framing this as a two output model
allows for information to flow both ways, such that
confident taggings in either space can inform the
other, and accordingly the mapping weights ? are
optimised to maximally exploit this effect.
One important question is how to obtain la-
belled data for training the two-output model, as
our small supervised sample of MLG text is only
annotated for MLG labels t
M
. We resolve this
by first learning the DPM model on the projected
labels, after which we automatically label our
correction training set with predicted tags from
the DPM model. That is, we augment the an-
notated training data from (t
M
, w
M
) to become
(t
M
, t
U
, w
M
). This is then used to train the two-
output maxent classifier, optimising a MAP ob-
jective using standard gradient descent. Note that
it would be possible to apply the same minimum
divergence technique for the two-output maxent
model. In this case the correction model would
include a regularization term over the ? to bias to-
wards the DPM parameters, while ? and ? would
use a zero-mean regularizer. However, we leave
this for future work.
Table 7 summarises the performance of the
state-of-the-art (Garrette et al., 2013), the super-
vised model and the two-output maxent model
evaluated on the Malagasy test set. The two-output
maxent model performs much better than the su-
pervised model, achieving ?5.3% (absolute) im-
894
Model Accuracy (%)
Garrette et al. (2013) 81.2
MaxEnt Supervised 80.0
2-output MaxEnt (Universal tagset) 85.3
2-output MaxEnt (Penn tagset) 85.6
Table 7: The performance of different models for
Malagasy.
provement. An interesting property of this ap-
proach is that we can use different tagsets for the
DPM. We also tried the original Penn treebank
tagset which is much larger than the universal
tagset (48 vs. 12 tags). We observed a small im-
provement reaching 85.6%, suggesting that some
pertinent information is lost in the universal tagset.
All in all, this is a substantial improvement over
the state-of-the-art result of 81.2% (Garrette et al.,
2013) and an error reduction of 23.4%.
7 Conclusion
In this paper, we thoroughly review the work on
multilingual POS tagging of the past decade. We
propose a simple method for building a POS tag-
ger for resource-poor languages by taking advan-
tage of parallel data and a small amount of anno-
tated data. Our method also efficiently resolves
the tagset mismatch issue identified for some lan-
guage pairs. We carefully choose and tune the
model. Comparing with the state-of-the-art, we
are using the more realistic assumption that a
small amount of labelled data can be made avail-
able rather than requiring a crowd-sourced dic-
tionary. We use less parallel data which as we
pointed out in section 3.1, could have been a huge
disadvantage for us. Moreover, we did not exploit
any external monolingual data. Importantly, our
method is simpler but performs better than previ-
ously proposed methods. With only 1,000 anno-
tated tokens, less than 1% of the test data, we can
achieve an average accuracy of 91.3% compared
with 88.8% of the state-of-the-art (error reduction
rate ?22%). Across the 8 languages we are sub-
stantially better at 7 and slightly worse at one. Our
method is reliable and could even be used to im-
prove the performance of a supervised POS tagger.
Currently, we are building the tagger and eval-
uating through several layers of mapping. Each
layer might introduce some noise which accumu-
lates and leads to a biased model. Moreover,
the tagset mappings are not available for many
resource-poor languages. We therefore also pro-
posed a method to automatically match between
tagsets based on a two-output maximum entropy
model. On the resource-poor language Mala-
gasy, we achieved the accuracy of 85.6% com-
pared with the state-of-the-art of 81.2% (Garrette
et al., 2013). Unlike their method, we didn?t use an
external dictionary but instead use a small amount
of parallel data.
In future work, we would like to improve the
performance of DPM by collecting more parallel
data. Duong et al. (2013a) pointed out that using
a different source language can greatly alter the
performance of the target language POS tagger.
We would like to experiment with different source
languages other than English. We assume that we
have 1,000 tokens for each language. Thus, for the
8 languages we considered we will have 8,000 an-
notated tokens. Currently, we treat each language
independently, however, it might also be interest-
ing to find some way to incorporate information
from multiple languages simultaneously to build
the tagger for a single target language.
Acknowledgments
We would like to thank Dan Garreette, Jason
Baldridge and Noah Smith for Malagasy and Kin-
yarwanda datasets. This work was supported by
the University of Melbourne and National ICT
Australia (NICTA). NICTA is funded by the Aus-
tralian Federal and Victoria State Governments,
and the Australian Research Council through the
ICT Centre of Excellence program. Dr Cohn is the
recipient of an Australian Research Council Fu-
ture Fellowship (project number FT130101105).
895
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceeding of
HLT-NAACL, pages 582?590.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39?71.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
(ANLP ?00), pages 224?231, Seattle, Washington,
USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: How far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 575?584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013a. Increasing the quality and quan-
tity of source language data for Unsupervised Cross-
Lingual POS tagging. Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1243?1249. Asian Federation of
Natural Language Processing.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013b. Simpler unsupervised POS tagging
with bilingual projections. Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
634?639. Association for Computational Linguis-
tics.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
pages 138?147, June.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. pages 583?592,
August.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start. In In Proc. ACL, pages
746?754.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?04),
pages 222?229, Barcelona, Spain, July.
Mark Johnson and Stefan Riezler. 2000. Exploit-
ing auxiliary distributions in stochastic unification-
based grammars. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
154?161.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79?86, Phuket, Thailand. AAMT.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference on European Chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach
to domain adaptation of a syntactic disambigua-
tion model. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 9?16.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
896
Computational Linguistics, pages 742?751, Gothen-
burg, Sweden, April.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS taggers (and other tools) for Indian lan-
guages: An experiment with Kannada using Telugu
resources. In Proceedings of IJCNLP workshop on
Cross Lingual Information Access: Computational
Linguistics and the Information Need of Multilin-
gual Societies. (CLIA 2011 at IJNCLP 2011), Chi-
ang Mai, Thailand, November.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn.
Res., 8:693?723, May.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Kristina Toutanova and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In J.C. Platt, D. Koller, and
Y. Singer a nd S.T. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. Curran Associates, Inc.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1 (NAACL ?03), pages 173?180, Edmonton,
Canada.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In In
ACL International Conference Proceedings.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies, NAACL ?01, pages 1?8.
897
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798?1803,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Joint Emotion Analysis via Multi-task Gaussian Processes
Daniel Beck
?
Trevor Cohn
?
Lucia Specia
?
?
Department of Computer Science, University of Sheffield, United Kingdom
{debeck1,l.specia}@sheffield.ac.uk
?
Computing and Information Systems, University of Melbourne, Australia
t.cohn@unimelb.edu.au
Abstract
We propose a model for jointly predicting
multiple emotions in natural language sen-
tences. Our model is based on a low-rank
coregionalisation approach, which com-
bines a vector-valued Gaussian Process
with a rich parameterisation scheme. We
show that our approach is able to learn
correlations and anti-correlations between
emotions on a news headlines dataset. The
proposed model outperforms both single-
task baselines and other multi-task ap-
proaches.
1 Introduction
Multi-task learning (Caruana, 1997) has been
widely used in Natural Language Processing.
Most of these learning methods are aimed for Do-
main Adaptation (Daum?e III, 2007; Finkel and
Manning, 2009), where we hypothesize that we
can learn from multiple domains by assuming sim-
ilarities between them. A more recent use of
multi-task learning is to model annotator bias and
noise for datasets labelled by multiple annotators
(Cohn and Specia, 2013).
The settings mentioned above have one aspect
in common: they assume some degree of posi-
tive correlation between tasks. In Domain Adap-
tation, we assume that some ?general?, domain-
independent knowledge exists in the data. For an-
notator noise modelling, we assume that a ?ground
truth? exists and that annotations are some noisy
deviations from this truth. However, for some set-
tings these assumptions do not necessarily hold
and often tasks can be anti-correlated. For these
cases, we need to employ multi-task methods that
are able to learn these relations from data and
correctly employ them when making predictions,
avoiding negative knowledge transfer.
An example of a problem that shows this be-
haviour is Emotion Analysis, where the goal is to
automatically detect emotions in a text (Strappa-
rava and Mihalcea, 2008; Mihalcea and Strappa-
rava, 2012). This problem is closely related to
Opinion Mining (Pang and Lee, 2008), with sim-
ilar applications, but it is usually done at a more
fine-grained level and involves the prediction of a
set of labels (one for each emotion) instead of a
single label. While we expect some emotions to
have some degree of correlation, this is usually not
the case for all possible emotions. For instance, we
expect sadness and joy to be anti-correlated.
We propose a multi-task setting for Emotion
Analysis based on a vector-valued Gaussian Pro-
cess (GP) approach known as coregionalisation
(
?
Alvarez et al., 2012). The idea is to combine a GP
with a low-rank matrix which encodes task corre-
lations. Our motivation to employ this model is
three-fold:
? Datasets for this task are scarce and small
so we hypothesize that a multi-task approach
will results in better models by allowing a
task to borrow statistical strength from other
tasks;
? The annotation scheme is subjective and very
fine-grained, and is therefore heavily prone to
bias and noise, both which can be modelled
easily using GPs;
? Finally, we also have the goal to learn a
model that shows sound and interpretable
correlations between emotions.
2 Multi-task Gaussian Process
Regression
Gaussian Processes (GPs) (Rasmussen and
Williams, 2006) are a Bayesian kernelised
framework considered the state-of-the-art for
regression. They have been recently used success-
fully for translation quality prediction (Cohn and
Specia, 2013; Beck et al., 2013; Shah et al., 2013)
1798
and modelling text periodicities (Preotiuc-Pietro
and Cohn, 2013). In the following we give a
brief description on how GPs are applied in a
regression setting.
Given an input x, the GP regression assumes
that its output y is a noise corrupted version of a
latent function evaluation, y = f(x) + ?, where
? ? N (0, ?
2
n
) is the added white noise and the
function f is drawn from a GP prior:
f(x) ? GP(?(x), k(x,x
?
)), (1)
where ?(x) is the mean function, which is usually
the 0 constant, and k(x,x
?
) is the kernel or co-
variance function, which describes the covariance
between values of f at locations x and x
?
.
To predict the value for an unseen input x
?
, we
compute the Bayesian posterior, which can be cal-
culated analytically, resulting in a Gaussian distri-
bution over the output y
?
:
1
y
?
? N (k
?
(K + ?
n
I)
?1
y
T
, (2)
k(x
?
,x
?
)? k
T
?
(K + ?
n
I)
?1
k
?
),
where K is the Gram matrix corre-
sponding to the covariance kernel evalu-
ated at every pair of training inputs and
k
?
= [?x
1
,x
?
?, ?x
2
,x
?
?, . . . , ?x
n
,x
?
?] is the
vector of kernel evaluations between the test input
and each training input.
2.1 The Intrinsic Coregionalisation Model
By extending the GP regression framework to
vector-valued outputs we obtain the so-called
coregionalisation models. Specifically, we employ
a separable vector-valued kernel known as Intrin-
sic Coregionalisation Model (ICM) (
?
Alvarez et al.,
2012). Considering a set of D tasks, we define the
corresponding vector-valued kernel as:
k((x, d), (x
?
, d
?
)) = k
data
(x,x
?
)?B
d,d
?
, (3)
where k
data
is a kernel on the input points (here
a Radial Basis Function, RBF), d and d
?
are task
or metadata information for each input and B ?
R
D?D
is the coregionalisation matrix, which en-
codes task covariances and is symmetric and posi-
tive semi-definite.
A key advantage of GP-based modelling is its
ability to learn hyperparameters directly from data
1
We refer the reader to Rasmussen and Williams (2006,
Chap. 2) for an in-depth explanation of GP regression.
by maximising the marginal likelihood:
p(y|X,?) =
?
f
p(y|X,?, f)p(f). (4)
This process is usually performed to learn the
noise variance and kernel hyperparameters, in-
cluding the coregionalisation matrix. In order to
do this, we need to consider how B is parame-
terised.
Cohn and Specia (2013) treat the diagonal val-
ues of B as hyperparameters, and as a conse-
quence are able to leverage the inter-task trans-
fer between each independent task and the global
?pooled? task. They however fix non-diagonal val-
ues to 1, which in practice is equivalent to assum-
ing equal correlation across tasks. This can be lim-
iting, in that this formulation cannot model anti-
correlations between tasks.
In this work we lift this restriction by adopting
a different parameterisation of B that allows the
learning of all task correlations. A straightforward
way to do that would be to consider every corre-
lation as an hyperparameter, but this can result in
a matrix which is not positive semi-definite (and
therefore, not a valid covariance matrix). To en-
sure this property, we follow the method proposed
by Bonilla et al. (2008), which decomposes B us-
ing Probabilistic Principal Component Analysis:
B = U?U
T
+ diag(?), (5)
where U is an D ? R matrix containing the R
principal eigenvectors and ? is a R ? R diago-
nal matrix containing the corresponding eigenval-
ues. The choice of R defines the rank of U?U
T
,
which can be understood as the capacity of the
manifold with which we model the D tasks. The
vector ? allows for each task to behave more or
less independently with respect to the global task.
The final rank of B depends on both terms in
Equation 5.
For numerical stability, we use the incomplete-
Cholesky decomposition over the matrix U?U
T
,
resulting in the following parameterisation for B:
B =
?
L
?
L
T
+ diag(?), (6)
where
?
L is a D ?R matrix. In this setting, we
treat all elements of
?
L as hyperparameters. Set-
ting a larger rank allows more flexibility in mod-
elling task correlations. However, a higher number
of hyperparameters may lead to overfitting prob-
lems or otherwise cause issues in optimisation due
1799
to additional non-convexities in the log likelihood
objective. In our experiments we evaluate this be-
haviour empirically by testing a range of ranks for
each setting.
The low-rank model can subsume the ones pro-
posed by Cohn and Specia (2013) by fixing and
tying some of the hyperparameters:
Independent: fixing
?
L = 0 and ? = 1;
Pooled: fixing
?
L = 1 and ? = 0;
Combined: fixing
?
L = 1 and tying all compo-
nents of ?;
Combined+: fixing
?
L = 1.
These formulations allow us to easily replicate
their modelling approach, which we evaluate as
competitive baselines in our experiments.
3 Experimental Setup
To address the feasibility of our approach, we pro-
pose a set of experiments with three goals in mind:
? To find our whether the ICM is able to learn
sensible emotion correlations;
? To check if these correlations are able to im-
prove predictions for unseen texts;
? To investigate the behaviour of the ICM
model as we increase the training set size.
Dataset We use the dataset provided by the ?Af-
fective Text? shared task in SemEval-2007 (Strap-
parava and Mihalcea, 2007), which is composed
of 1000 news headlines annotated in terms of six
emotions: Anger, Disgust, Fear, Joy, Sadness and
Surprise. For each emotion, a score between 0 and
100 is given, 0 meaning total lack of emotion and
100 maximum emotional load. We use 100 sen-
tences for training and the remaining 900 for test-
ing.
Model For all experiments, we use a Radial Ba-
sis Function (RBF) data kernel over a bag-of-
words feature representation. Words were down-
cased and lemmatized using the WordNet lemma-
tizer in the NLTK
2
toolkit (Bird et al., 2009). We
then use the GPy toolkit
3
to combine this kernel
with a coregionalisation model over the six emo-
tions, comparing a number of low-rank approxi-
mations.
2
http://www.nltk.org
3
http://github.com/SheffieldML/GPy
Baselines and Evaluation We compare predic-
tion results with a set of single-task baselines: a
Support Vector Machine (SVM) using an RBF
kernel with hyperparameters optimised via cross-
validation and a single-task GP, optimised via like-
lihood maximisation. The SVM models were
trained using the Scikit-learn toolkit
4
(Pedregosa
et al., 2011). We also compare our results against
the ones obtained by employing the ?Combined?
and ?Combined+? models proposed by Cohn and
Specia (2013). Following previous work in this
area, we use Pearson?s correlation coefficient as
evaluation metric.
4 Results and Discussion
4.1 Learned Task Correlations
Figure 1 shows the learned coregionalisation ma-
trix setting the initial rank as 1, reordering the
emotions to emphasize the learned structure. We
can see that the matrix follows a block structure,
clustering some of the emotions. This picture
shows two interesting behaviours:
? Sadness and fear are highly correlated. Anger
and disgust also correlate with them, al-
though to a lesser extent, and could be con-
sidered as belonging to the same cluster. We
can also see correlation between surprise and
joy. These are intuitively sound clusters
based on the polarity of these emotions.
? In addition to correlations, the model
learns anti-correlations, especially between
joy/surprise and the other emotions. We also
note that joy has the highest diagonal value,
meaning that it gives preference to indepen-
dent modelling (instead of pooling over the
remaining tasks).
Inspecting the eigenvalues of the learned ma-
trix allows us to empirically determine its result-
ing rank. In this case we find that the model has
learned a matrix of rank 3, which indicates that
our initial assumption of a rank 1 coregionalisa-
tion matrix may be too small in terms of modelling
capacity
5
. This suggests that a higher rank is
justified, although care must be taken due to the
local optima and overfitting issues cited in ?2.1.
4
http://scikit-learn.org
5
The eigenvalues were 592, 62, 86, 4, 3 ? 10
?3
and 9 ?
10
?5
.
1800
Anger Disgust Fear Joy Sadness Surprise All
SVM 0.3084 0.2135 0.3525 0.0905 0.3330 0.1148 0.2603
Single GP 0.1683 0.0035 0.3462 0.2035 0.3011 0.1599 0.3659
ICM GP (Combined) 0.2301 0.1230 0.2913 0.2202 0.2303 0.1744 0.3295
ICM GP (Combined+) 0.1539 0.1240 0.3438 0.2466 0.2850 0.2027 0.3723
ICM GP (Rank 1) 0.2133 0.1075 0.3623 0.2810 0.3137 0.2415 0.3988
ICM GP (Rank 5) 0.2542 0.1799 0.3727 0.2711 0.3157 0.2446 0.3957
Table 1: Prediction results in terms of Pearson?s correlation coefficient (higher is better). Boldface values
show the best performing model for each emotion. The scores for the ?All? column were calculated over
the predictions for all emotions concatenated (instead of just averaging over the scores for each emotion).
Figure 1: Heatmap showing a learned coregional-
isation matrix over the emotions.
4.2 Prediction Results
Table 1 shows the Pearson?s scores obtained in
our experiments. The low-rank models outper-
formed the baselines for the full task (predicting
all emotions) and for fear, joy and surprise sub-
tasks. The rank 5 models were also able to out-
perform all GP baselines for the remaining emo-
tions, but could not beat the SVM baseline. As
expected, the ?Combined? and ?Combined+? per-
formed worse than the low-rank models, probably
due to their inability to model anti-correlations.
4.3 Error analysis
To check why SVM performs better than GPs for
some emotions, we analysed their gold-standard
score distributions. Figure 2 shows the smoothed
distributions for disgust and fear, comparing the
gold-standard scores to predictions from the SVM
and GP models. The distributions for the training
set follow similar shapes.
We can see that GP obtains better matching
score distributions in the case when the gold-
Figure 2: Test score distributions for disgust and
fear. For clarity, only scores between 0 and 50 are
shown. SVM performs better on disgust, while GP
performs better on fear.
standard scores are more spread over the full sup-
port of response values, i.e., [0, 100]. Since our GP
model employs a Gaussian likelihood, it is effec-
tively minimising a squared-error loss. The SVM
model, on the other hand, uses hinge loss, which
is linear beyond the margin envelope constraints.
This affects the treatment of outlier points, which
attract quadratic cf. linear penalties for the GP
and SVM respectively. Therefore, when train-
ing scores are more uniformly distributed (which
is the case for fear), the GP model has to take the
high scores into account, resulting in broader cov-
erage of the full support. For disgust, the scores
are much more peaked near zero, favouring the
1801
more narrow coverage of the SVM.
More importantly, Figure 2 also shows that both
SVM and GP predictions tend to exhibit a Gaus-
sian shape, while the true scores show an expo-
nential behaviour. This suggests that both mod-
els are making wrong prior assumptions about the
underlying score distribution. For SVMs, this is
a non-trivial issue to address, although it is much
easier for GPs, where we can use a different like-
lihood distribution, e.g., a Beta distribution to re-
flect that the outputs are only valid over a bounded
range. Note that non-Gaussian likelihoods mean
that exact inference is no longer tractable, due to
the lack of conjugacy between the prior and likeli-
hood. However a number of approximate infer-
ence methods are appropriate which are already
widely used in the GP literature for use with non-
Gaussian likelihoods, including expectation prop-
agation (Jyl?anki et al., 2011), the Laplace approx-
imation (Williams and Barber, 1998) and Markov
Chain Monte Carlo sampling (Adams et al., 2009).
4.4 Training Set Influence
We expect multi-task models to perform better for
smaller datasets, when compared to single-task
models. This stems from the fact that with small
datasets often there is more uncertainty associated
with each task, a problem which can be alleviated
using statistics from the other tasks. To measure
this behaviour, we performed an additional exper-
iment varying the size of the training sets, while
using 100 sentences for testing.
Figure 3 shows the scores obtained. As ex-
pected, for smaller datasets the single-task mod-
els are outperformed by ICM, but their perfor-
mance become equivalent as the training set size
increases. SVM performance tends to be slightly
worse for most sizes. To study why we obtained
an outlier for the single-task model with 200 sen-
tences, we inspected the prediction values. We
found that, in this case, predictions for joy, sur-
prise and disgust were all around the same value.
6
For larger datasets, this effect disappears and the
single-task models yield good predictions.
5 Conclusions and Future Work
This paper proposed an multi-task approach for
Emotion Analysis that is able to learn correlations
6
Looking at the predictions for smaller datasets, we found
the same behaviour, but because the values found were near
the mean they did not hurt the Pearson?s score as much.
Figure 3: Pearson?s correlation score according to
training set size (in number of sentences).
and anti-correlations between emotions. Our for-
mulation is based on a combination of a Gaussian
Process and a low-rank coregionalisation model,
using a richer parameterisation that allows the
learning of fine-grained task similarities. The pro-
posed model outperformed strong baselines when
applied to a news headline dataset.
As it was discussed in Section 4.3, we plan
to further explore the possibility of using non-
Gaussian likelihoods with the GP models. An-
other research avenue we intend to explore is to
employ multiple layers of metadata, similar to the
model proposed by Cohn and Specia (2013). An
example is to incorporate the dataset provided by
Snow et al. (2008), which provides multiple non-
expert emotion annotations for each sentence, ob-
tained via crowdsourcing. Finally, another possi-
ble extension comes from more advanced vector-
valued GP models, such as the linear model of
coregionalisation (
?
Alvarez et al., 2012) or hierar-
chical kernels (Hensman et al., 2013). These mod-
els can be specially useful when we want to em-
ploy multiple kernels to explain the relation be-
tween the input data and the labels.
Acknowledgements
Daniel Beck was supported by funding from
CNPq/Brazil (No. 237999/2012-9). Dr.
Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
References
Ryan Prescott Adams, Iain Murray, and David J. C.
MacKay. 2009. Tractable Nonparametric Bayesian
1802
Inference in Poisson Processes with Gaussian Pro-
cess Intensities. In Proceedings of ICML, pages 1?8,
New York, New York, USA. ACM Press.
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1?37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337?342.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Rich Caruana. 1997. Multitask Learning. Machine
Learning, 28:41?75.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian Domain Adaptation. In Pro-
ceedings of NAACL.
James Hensman, Neil D Lawrence, and Magnus Rat-
tray. 2013. Hierarchical Bayesian modelling of
gene expression time series across irregularly sam-
pled replicates and clusters. BMC Bioinformatics,
14:252.
Pasi Jyl?anki, Jarno Vanhatalo, and Aki Vehtari. 2011.
Robust Gaussian Process Regression with a Student-
t Likelihood. Journal of Machine Learning Re-
search, 12:3227?3257.
Rada Mihalcea and Carlo Strapparava. 2012. Lyrics,
Music, and Emotions. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 590?599.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of EMNLP.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and Fast - But
is it Good?: Evaluating Non-Expert Annotations
for Natural Language Tasks. In Proceedings of
EMNLP.
Carlo Strapparava and Rada Mihalcea. 2007.
SemEval-2007 Task 14 : Affective Text. In Pro-
ceedings of SEMEVAL.
Carlo Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proceedings of the
2008 ACM Symposium on Applied Computing.
Christopher K. I. Williams and David Barber. 1998.
Bayesian Classification with Gaussian Processes.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342?1351.
1803
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 405?413,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Predicting and Characterising User Impact on Twitter
Vasileios Lampos
1
, Nikolaos Aletras
2
, Daniel Preot?iuc-Pietro
2
and Trevor Cohn
3
1
Department of Computer Science, University College London
2
Department of Computer Science, University of Sheffield
3
Computing and Information Systems, The University of Melbourne
v.lampos@ucl.ac.uk, {n.aletras,d.preotiuc}@dcs.shef.ac.uk, trevor.cohn@gmail.com
Abstract
The open structure of online social net-
works and their uncurated nature give rise
to problems of user credibility and influ-
ence. In this paper, we address the task of
predicting the impact of Twitter users based
only on features under their direct control,
such as usage statistics and the text posted
in their tweets. We approach the problem as
regression and apply linear as well as non-
linear learning methods to predict a user
impact score, estimated by combining the
numbers of the user?s followers, followees
and listings. The experimental results point
out that a strong prediction performance is
achieved, especially for models based on
the Gaussian Processes framework. Hence,
we can interpret various modelling com-
ponents, transforming them into indirect
?suggestions? for impact boosting.
1 Introduction
Online social networks have become a wide spread
medium for information dissemination and inter-
action between millions of users (Huberman et al.,
2009; Kwak et al., 2010), turning, at the same
time, into a popular subject for interdisciplinary
research, involving domains such as Computer Sci-
ence (Sakaki et al., 2010), Health (Lampos and
Cristianini, 2012) and Psychology (Boyd et al.,
2010). Open access along with the property of struc-
tured content retrieval for publicly posted data have
brought the microblogging platform of Twitter into
the spotlight.
Vast quantities of human-generated text from
a range of themes, including opinions, news and
everyday activities, spread over a social network.
Naturally, issues arise, like user credibility (Castillo
et al., 2011) and content attractiveness (Suh et al.,
2010), and quite often trustful or appealing informa-
tion transmitters are identified by an impact assess-
ment.
1
Intuitively, it is expected that user impact
cannot be defined by a single attribute, but depends
on multiple user actions, such as posting frequency
and quality, interaction strategies, and the text or
topics of the written communications.
In this paper, we start by predicting user impact
as a statistical learning task (regression). For that
purpose, we firstly define an impact score function
for Twitter users driven by basic account proper-
ties. Afterwards, from a set of accounts, we mea-
sure several publicly available attributes, such as
the quantity of posts or interaction figures. Textual
attributes are also modelled either by word frequen-
cies or, more generally, by clusters of related words
which quantify a topic-oriented participation. The
main hypothesis being tested is whether textual
and non textual attributes encapsulate patterns that
affect the impact of an account.
To model this data, we present a method based
on nonlinear regression using Gaussian Processes,
a Bayesian non-parametric class of methods (Ras-
mussen and Williams, 2006), proven more effec-
tive in capturing the multimodal user features. The
modelling choice of excluding components that
are not under an account?s direct control (e.g. re-
ceived retweets) combined with a significant user
impact prediction performance (r = .78) enabled
us to investigate further how specific aspects of a
user?s behaviour relate to impact, by examining the
parameters of the inferred model.
Among our findings, we identify relevant fea-
tures for this task and confirm that consistent ac-
tivity and broad interaction are deciding impact
factors. Informativeness, estimated by computing
a joint user-topic entropy, contributes well to the
separation between low and high impact accounts.
Use case scenarios based on combinations of fea-
tures are also explored, leading to findings such as
that engaging about ?serious? or more ?light? topics
may not register a differentiation in impact.
1
For example, the influence assessment metric of Klout ?
http://www.klout.com.
405
2 Data
For the experimental process of this paper, we
formed a Twitter data set (D1) of more than 48 mil-
lion tweets produced by |U | = 38, 020 users geolo-
cated in the UK in the period between 14/04/2011
and 12/04/2012 (both dates included, ?t = 365
days). D1 is a temporal subset of the data set used
for modelling UK voting intentions in (Lampos et
al., 2013). Geolocation of users was carried out
by matching the location field in their profile with
UK city names on DBpedia as well as by check-
ing that the user?s timezone is set to G.M.T. (Rout
et al., 2013). The use of a common greater geo-
graphical area (UK) was essential in order to derive
a data set with language and topic homogeneity.
A distinct Twitter data set (D2) consisting of ap-
prox. 400 million tweets was formed for learning
term clusters (Section 4.2). D2 was retrieved from
Twitter?s Gardenhose stream (a 10% sample of the
entire stream) from 02/01 to 28/02/2011. D1 and
D2 were processed using TrendMiner?s pipeline
(Preot?iuc-Pietro et al., 2012).
3 User Impact Definition
On the microblogging platform of Twitter, user ?
or, in general, account ? popularity is usually quan-
tified by the raw number of followers (?
in
? 0),
i.e. other users interested in this account. Likewise,
a user can follow others, which we denote as his set
of followees (?
out
? 0). It is expected that users
with high numbers of followers are also popular
in the real world, being well-known artists, politi-
cians, brands and so on. However, non popular
entities, the majority in the social network, can also
gain a great number of followers, by exploiting,
for example, a follow-back strategy.
2
Therefore,
using solely the number of followers to quantify
impact may lead to inaccurate outcomes (Cha et al.,
2010). A natural alternative, the ratio of ?
in
/?
out
is not a reliable metric, as it is invariant to scal-
ing, i.e. it cannot differentiate accounts of the type
{?
in
, ?
out
} = {m,n} and {? ? m, ? ? n}. We
resolve this problem by squaring the number of
followers
(
?
2
in
/?
out
)
; note that the previous expres-
sion is equal to (?
in
? ?
out
)? (?
in
/?
out
) +?
in
and
thus, it incorporates the ratio as well as the differ-
ence between followers and followees.
An additional impact indicator is the number of
times an account has been listed by others (?
?
? 0).
Lists provide a way to curate content on Twitter;
thus, users included in many lists are attractors of
2
An account follows other accounts randomly expecting
that they will follow back.
?5 0 5 10 15 20 25 300
0.05
0.1
0.15
Impact Score (S)
Prob
abilit
y De
nsity
@guardian
@David_Cameron
@PaulMasonNews
@lampos
@nikaletras
@spam?
Figure 1: Histogram of the user impact scores in
our data set. The solid black line represents a gen-
eralised extreme value probability distribution fit-
ted in our data, and the dashed line denotes the
mean impact score (= 6.776). User @spam? is a
sample account with ?
in
= 10, ?
out
= 1000 and
?
?
= 0; @lampos is a very active account, whereas
@nikaletras is a regular user.
interest. Indeed, Pearson?s correlation between ?
in
and ?
?
for all the accounts in our data set is equal
to .765 (p < .001); the two metrics are correlated,
but not entirely and on those grounds, it would be
reasonable to use both for quantifying impact.
Consequently, we have chosen to represent user
impact (S) as a log function of the number of fol-
lowers, followees and listings, given by
S(?
in
, ?
out
, ?
?
) = ln
(
(?
?
+ ?) (?
in
+ ?)
2
?
out
+ ?
)
,
(1)
where ? is a smoothing constant set equal to 1 so
that the natural logarithm is always applied on a
real positive number. Figure 1 shows the impact
score distribution for all the users in our sample,
including some pointers to less or more popular
Twitter accounts. The depicted user impact scores
form the response variable in the regression models
presented in the following sections.
4 User Account Features
This section presents the features used in the user
impact prediction task. They are divided into two
categories: non-textual and text-based. All features
have the joint characteristic of being under the
user?s direct control, something essential for char-
acterising impact based on the actions of a user.
Attributes such as the number of received retweets
or @-mentions (of a user in the tweets of others)
were not considered as they are not controlled by
the account itself.
406
a1
# of tweets
a
2
proportion of retweets
a
3
proportion of non-duplicate tweets
a
4
proportion of tweets with hashtags
a
5
hashtag-tokens ratio in tweets
a
6
proportion of tweets with @-mentions
a
7
# of unique @-mentions in tweets
a
8
proportion of tweets with @-replies
a
9
links ratio in tweets
a
10
# of favourites the account made
a
11
total # of tweets (entire history)
a
12
using default profile background (binary)
a
13
using default profile image (binary)
a
14
enabled geolocation (binary)
a
15
population of account?s location
a
16
account?s location latitude
a
17
account?s location longitude
a
18
proportion of days with nonzero tweets
Table 1: Non textual attributes for a Twitter account
used in the modelling process. All attributes refer
to a set of 365 days (?t) with the exception of a
11
,
the total number of tweets in the entire history of an
account. Attributes a
i
, i ? {2? 6, 8, 9} are ratios
of a
1
, whereas attribute a
18
is a proportion of ?t.
4.1 Non textual attributes
The non-textual attributes (a) are derived either
from general user behaviour statistics or directly
from the account?s profile. Table 1 presents the 18
attributes we extracted and used in our models.
4.2 Text features
We process the text in the tweets of D1 and com-
pute daily unigram frequencies. By discarding
terms that appear less than 100 times, we form
a vocabulary of size |V | = 71, 555. We then form
a user term-frequency matrix of size |U |?|V | with
the mean term frequencies per user during the time
interval ?t. All term frequencies are normalised
with the total number of tweets posted by the user.
Apart from single word frequencies, we are also
interested in deriving a more abstract representa-
tion for each user. To achieve this, we learn word
clusters from a distinct reference corpus (D2) that
could potentially represent specific domains of
discussion (or topics). From a multitude of pro-
posed techniques, we have chosen to apply spec-
tral clustering (Shi and Malik, 2000; Ng et al.,
2002), a hard-clustering method appropriate for
high-dimensional data and non-convex clusters
(von Luxburg, 2007). Spectral clustering performs
graph partitioning on the word-by-word similar-
ity matrix. In our case, tweet-term similarity is
reflected by the Normalised Pointwise Mutual In-
formation (NPMI), an information theoretic mea-
sure indicating which words co-occur in the same
context (Bouma, 2009). We use the random walk
graph Laplacian and only keep the largest compo-
nent of the resulting graph, eliminating most stop
words in the process. The number of clusters needs
to be specified in advance and each cluster?s most
representative words are identified by the following
metric of centrality:
C
w
(c) =
?
v?c
NPMI(w, v)
|c| ? 1
, (2)
where w is the target word and c the cluster it be-
longs (|c| denotes the cluster?s size). Examples of
extracted word clusters are illustrated in Table 4.
Other techniques were also applied, such as online
LDA (Hoffman et al., 2010), but we found that
the results were not satisfactory, perhaps due to
the short message length and the foreign terms co-
occuring within a tweet. After forming the clusters
using D2, we compute a topic score (? ) for each
user-topic pair in D1, representing a normalised
user-word frequency sum per topic.
5 Methods
This section presents the various modelling ap-
proaches for the underlying inference task, the im-
pact score (S) prediction of Twitter users based on
a set of their actions.
5.1 Learning functions for regression
We formulate this problem as a regression task,
i.e. we infer a real numbered value based on a set
of observed features. As a simple baseline, we ap-
ply Ridge Regression (RR) (Hoerl and Kennard,
1970), a reguralised version of the ordinary least
squares. Most importantly, we focus on nonlinear
methods for the impact score prediction task given
the multimodality of the feature space. Recently, it
was shown by Cohn and Specia (2013) that Sup-
port Vector Machines for Regression (SVR) (Vap-
nik, 1998; Smola and Sch?olkopf, 2004), commonly
considered the state-of-the-art for NLP regression
tasks, can be outperformed by Gaussian Processes
(GPs), a kernelised, probabilistic approach to learn-
ing (Rasmussen and Williams, 2006). Their setting
is close to ours, in that they had few (17) features
and were also aiming to predict a complex con-
tinuous phenomenon (human post-editing time).
The initial stages of our experimental process con-
firmed that GPs performed better than SVR; thus,
407
we based our modelling around them, including
RR for comparison.
In GP regression, for the inputs x ? R
d
we want
to learn a function f : R
d
? R that is drawn from
a GP prior
f(x) ? GP
(
m(x), k(x,x
?
)
)
, (3)
where m(x) and k(x,x
?
) denote the mean (set to
0 in our experiments) and covariance (or kernel)
functions respectively. The GP kernel function rep-
resents the covariance between pairs of input. We
wish to limit f to smooth functions over the inputs,
with different smoothness in each input dimension,
assuming that some features are more useful than
others. This can be accommodated by a squared ex-
ponential covariance function with Automatic Rele-
vance Determination (ARD) (Neal, 1996; Williams
and Rasmussen, 1996):
k
ard
(x,x
?
) = ?
2
exp
[
d
?
i
?
(x
i
? x
?
i
)
2
2`
2
i
]
, (4)
where ?
2
denotes the overall variance and `
i
is
the length-scale parameter for feature x
i
; all hy-
perparameters are learned from data during model
inference. Parameter `
i
is inversely proportional to
the feature?s relevancy in the model, i.e. high val-
ues of `
i
indicate a low degree of relevance for the
corresponding x
i
. By setting `
i
= ` in Eq. 4, we
learn a common length-scale for all the dimensions
? this is known as the isotropic squared exponen-
tial function (k
iso
) since it is based purely on the
difference |x ? x
?
|. k
iso
is a preferred choice when
the dimensionality of the input space is high. Hav-
ing set our covariance functions, predictions are
conducted using Bayesian integration
P(y
?
|x
?
,O) =
?
f
P(y
?
|x
?
, f)P(f |O), (5)
where y
?
is the response variable,O a labelled train-
ing set and x
?
the current observation. We learn the
hyperparameters of the model by maximising the
log marginal likelihood P(y|O) using gradient as-
cent. However, inference becomes intractable when
many training instances (n) are present as the num-
ber of computations needed is O(n
3
) (Qui?nonero-
Candela and Rasmussen, 2005). Since our training
samples are tens of thousands, we apply a sparse
approximation method (FITC), which bases param-
eter learning on a few inducing points in the train-
ing set (Qui?nonero-Candela and Rasmussen, 2005;
Snelson and Ghahramani, 2006).
5.2 Models
For predicting user impact on Twitter, we develop
three regression models that build on each other.
The first and simplest one (A) uses only the non-
textual attributes as features; the performance of A
is tested using RR,
3
SVR as well as a GP model.
For SVR we used an RBF kernel (equivalent to
k
iso
), whereas for the GP we applied the following
covariance function
k(a,a
?
) = k
ard
(a,a
?
) + k
noise
(a,a
?
) + ?, (6)
where k
noise
(a,a
?
) = ?
2
? ?(a,a
?
), ? is a Kro-
necker delta function and ? is the regression bias;
this function consists of (|a| + 3) hyperparame-
ters. Note that the sum of covariance functions is
also a valid covariance function (Rasmussen and
Williams, 2006).
The second model (AW) extends model A by
adding word-frequencies as features. The 500 most
frequent terms in D1 are discarded as stop words
and we use the following 2, 000 ones (denoted by
w). Setting x = {a,w}, the covariance function
becomes
k(x,x
?
) = k
ard
(a,a
?
) + k
iso
(w,w
?
)
+ k
noise
(x,x
?
) + ?,
(7)
where we apply k
iso
on the term-frequencies due to
their high dimensionality; the number of hyperpa-
rameters is (|a|+ 5). This is an intermediate model
aiming to evaluate whether the incorporation of
text improves prediction performance.
Finally, in the third model (AC) instead of rely-
ing on the high dimensional space of single words,
we use topic-oriented collections of terms extracted
by applying spectral clustering (see Section 4.2).
By denoting the set of different clusters or topics
as ? and the entire feature space as x = {a,? }, the
covariance function now becomes
k(x,x
?
) = k
ard
(x,x
?
) + k
noise
(x,x
?
) + ?. (8)
The number of hyperparameters is equal to (|a|+
|? |+ 3) and this model is applied for |? | = 50 and
100.
6 Experiments
Here we present the experimental results for the
user impact prediction task and then investigate the
factors that can affect it.
6.1 Predictive Accuracy
We evaluated the performance of the proposed
models via 10-fold cross-validation. Results are
presented in Table 2; Root Mean Squared Error
3
Given that the representation of attributes a
16
and a
17
(latitude, longitude) is ambiguous in a linear model, they were
not included in the RR-based models.
408
Linear (RR) Nonlinear (GP)
Model r RMSE r RMSE
A .667 2.642 .759 2.298
AW .712 2.529 .768 2.263
AC, |? | = 50 .703 2.518 .774 2.234
AC, |? | = 100 .714 2.480 .780 2.210
Table 2: Average performance (RMSE and Pear-
son?s r) derived from 10-fold cross-validation for
the task of user impact score prediction.
Model Top relevant features
A a

13
, a
11
, a
7
, a
1
, a
9
, a
8
, a
18
, a
4
, a
6
, a
3
AW a
7
, a
1
, a
11
, a

13
, a
9
, a
8
, a
18
, a
4
, a
6
, a
15
AC, ? = 50 a

13
, a
11
, a
7
, ?
?
1
, a
1
, a
9
, a
8
, ?
?
2
, a
6
, ?
?
3
AC, ? = 100 a

13
, a
11
, a
7
, a
1
, a
9
, ?
1
, ?
2
, ?
3
, a
18
, a
8
Table 3: The 10 most relevant features in descend-
ing relevance order for all GP models. ?
?
i
and ?
i
denote word clusters (may vary in each model).
6
(RMSE) and Pearson?s correlation (r) between pre-
dictions and responses were used as the perfor-
mance metrics. Overall, the best performance in
terms of both RMSE (2.21 impact points) and lin-
ear correlation (r = .78, p < .001) is achieved
by the GP model (AC) that combines non-textual
attributes with a 100 topic clusters; the difference
in performance with all other models is statistically
significant.
4
The linear baseline (RR) follows the
same pattern of improvement through the differ-
ent models, but never manages to reach the perfor-
mance of the nonlinear alternative. As mentioned
previously, we have also tried SVR with an RBF
kernel for model A (parameters were optimised on
a held-out development set) and the performance
(RMSE: 2.33, r = .75, p < .001) was significantly
worse than the one achieved by the GP model.
4
Notice that when word-based features are intro-
duced in model AW, performance improves. This
was one of the motivations for including text in the
modelling, apart from the notion that the posted
content should also affect general impact. Lastly,
turning this problem from regression to classifi-
cation by creating 3 impact score pseudo-classes
based on the .25 and the .9 quantiles of the re-
sponse variable (4.3 and 11.4 impact score points
respectively) and by using the outputs of model
AC (? = 100) in each phase of the 10-fold cross-
validation, we achieve a 75.86% classification ac-
curacy.
5
4
Indicated by performing a t-test (5% significance level).
5
Similar performance scores can be estimated for different
class threshold settings.
0
100
0
100
0
100
0
100
0 10 20 300
100
0 10 20 30
L H
L H
L H
L H
L H
Tweetszinzentirezhistoryz(?11)
Uniquez@-mentionsz(?7)
Linksz(?9)
@-repliesz(?8)
Dayszwithznonzeroztweetsz(?18)
Figure 2: User impact distribution (x-axis: impact
points, y-axis: # of user accounts) for users with a
low (L) or a high (H) participation in a selection
of relevant non-textual attributes. Dot-dashed lines
denote the respective mean impact score; the red
line is the mean of the entire sample (= 6.776).
6.2 Qualitative Analysis
Given the model?s strong performance, we now
conduct a more thorough analysis to identify and
characterise the properties that affect aspects of
the user impact. GP?s length-scale parameters (`
i
)
? which are inversely proportional to feature rele-
vancy ? are used for ranking feature importance.
Note that since our data set consists of UK users,
some results may be biased towards specific cul-
tural properties.
Non-textual attributes. Table 3 lists the 10 most
relevant attributes (or topics, where applicable) as
extracted in each GP model. Ranking is determined
by the mean value of the length-scale parameter for
each feature in the 10-fold cross-validation process.
We do not show feature ranking derived from the
RR models as we focus on the models with the best
performance. Despite this, it is worth mentioning
6
Length-scales are comparable for features of the same
variance (z-scored). Binary features (denoted by ) are not
z-scored, but for comparison purposes we have rescaled their
length-scale using the feature?s variance.
409
Label ?(`)? ?(`) Cluster?s words ranked by centrality |c|
?
1
: Weather 3.73? 1.80 mph, humidity, barometer, gust, winds, hpa, temperature, kt, #weather [...] 309
?
2
: Healthcare
Finance
Housing
5.44? 1.55 nursing, nurse, rn, registered, bedroom, clinical, #news, estate, #hospital,
rent, healthcare, therapist, condo, investment, furnished, medical, #nyc,
occupational, investors, #ny, litigation, tutors, spacious, foreclosure [...]
1281
?
3
: Politics 6.07? 2.86 senate, republican, gop, police, arrested, voters, robbery, democrats, presi-
dential, elections, charged, election, charges, #religion, arrest, repeal, dems,
#christian, reform, democratic, pleads, #jesus, #atheism [...]
950
?
4
: Showbiz
Movies
TV
7.36? 2.25 damon, potter, #tvd, harry, elena, kate, portman, pattinson, hermione, jen-
nifer, kristen, stefan, robert, catholic, stewart, katherine, lois, jackson, vam-
pire, natalie, #vampirediaries, tempah, tinie, weasley, turner, rowland [...]
1943
?
5
: Commerce 7.83? 2.77 chevrolet, inventory, coupon, toyota, mileage, sedan, nissan, adde, jeep, 4x4,
2002, #coupon, enhanced, #deal, dodge, gmc, 20%, suv, 15%, 2005, 2003,
2006, coupons, discount, hatchback, purchase, #ebay, 10% [...]
608
?
6
: Twitter
Hashtags
8.22? 2.98 #teamfollowback, #500aday, #tfb, #instantfollowback, #ifollowback, #in-
stantfollow, #followback, #teamautofollow, #autofollow, #mustfollow [...]
194
?
7
: Social
Unrest
8.37? 5.52 #egypt, #tunisia, #iran, #israel, #palestine, tunisia, arab, #jan25, iran, israel,
protests, egypt, #yemen, #iranelection, israeli, #jordan, regime, yemen,
#gaza, protesters, #lebanon, #syria, egyptian, #protest, #iraq [...]
321
?
8
: Non English 8.45? 3.80 yg, nak, gw, gue, kalo, itu, aku, aja, ini, gak, klo, sih, tak, mau, buat [...] 469
?
9
: Horoscope
Gambling
9.11? 3.07 horoscope, astrology, zodiac, aries, libra, aquarius, pisces, taurus, virgo,
capricorn, horoscopes, sagitarius, comprehensive, lottery, jackpot [...]
1354
?
10
: Religion
Sports
10.29? 6.27 #jesustweeters, psalm, christ, #nhl, proverbs, unto, salvation, psalms, lord,
kjv, righteousness, niv, bible, pastor, #mlb, romans, awards, nhl [...]
1610
Table 4: The 10 most relevant topics (for model AC, |? | = 100) in the prediction of a user?s impact score
together with their most central words. The topics are ranked by their mean length-scale, ?(`), in the
10-fold cross-validation process (?(`) is the respective standard deviation).
that RR?s outputs also followed similar ranking pat-
terns, e.g. the top 5 features in model A were a
18
,
a
7
, a
3
, a
11
and a
9
. Notice that across all models,
among the strongest features are the total number
of posts either in the entire account?s history (a
11
)
or within the 365-day interval of our experiment
(a
1
) and the number of unique @-mentions (a
7
),
good indicators of user activity and user interaction
respectively. Feature a
13
is also a very good predic-
tor, but is of limited utility for modelling our data
set because very few accounts maintain the default
profile photo (0.4%). Less relevant attributes (not
shown) are the ones related to the location of a
user (a
16
, a
17
) signalling that the whereabouts of a
user may not necessarily relate to impact. Another
low relevance attribute is the number of favourites
that an account did (a
10
), something reasonable, as
those weak endorsements are not affecting the main
stream of content updates in the social network.
In Figure 2, we present the distribution of user
impact for accounts with low (left-side) and high
(right-side) participation in a selection of non-
textual attributes. Low (L) and high (H) participa-
tions are defined by selecting the 500 accounts with
lowest and highest scores for this specific attribute.
The means of (L) and (H) are compared with the
mean impact score in our sample. As anticipated,
accounts with low activity (a
11
) are likely to be
assigned impact scores far below the mean, while
very active accounts may follow a quite opposite
pattern. Avoiding mentioning (a
7
) or replying (a
8
)
to others may not affect (on average) an impact
score positively or negatively; however, accounts
that do many unique @-mentions are distributed
around a clearly higher impact score. On the other
hand, users that overdo @-replies are distributed be-
low the mean impact score. Furthermore, accounts
that post irregularly with gaps longer than a day
(a
18
) or avoid using links in their tweets (a
9
) will
probably appear in the low impact score range.
Topics. Regarding prediction accuracy (Table 2),
performance improves when topics are included.
In turn, some of the topics replace non-textual at-
tributes in the relevancy ranking (Table 3). Table 4
presents the 10 most relevant topic word-clusters
based on their mean length-scale ?(`) in the 10-
fold cross-validation process for the best perform-
ing GP model (AC, |? | = 100). We see that clusters
with their most central words representing topics
such as ?Weather?, ?Healthcare/Finance?, ?Politics?
and ?Showbiz? come up on top.
Contrary to the non-textual attributes, accounts
with low participation in a topic (for the vast major-
ity of topics) were distributed along impact score
values lower than the mean. Based on the fact that
word clusters are not small in size, this is a rational
outcome indicating that accounts with small word-
frequency sums (i.e. the ones that do not tweet
much) will more likely be users with small impact
410
0100
0 10 20 300
100
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
?1 ?2 ?3 ?4 ?5
?6 ?7 ?8 ?9 ?10
Figure 3: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for accounts with a
high participation in the 10 most relevant topics. Dot-dashed lines denote mean impact scores; the red line
is the mean of the entire sample (= 6.776).
Nu
mb
er o
f A
cco
unt
s
Impact Score (S)
0 10 20 300
50
100
All
Low Entropy
High Entropy
Figure 4: User impact distribution for accounts with
high (blue) and low (dark grey) topic entropy. Lines
denote the respective mean impact scores.
scores. Hence, in Figure 3 we only show the user
impact distribution for the 500 accounts with the
top participation in each topic. Informally, this is a
way to quantify the contribution of each domain or
topic of discussion in the impact score. Notice that
the topics which ?push? users towards the highest
impact scores fall into the domains of ?Politics? (?
3
)
and ?Showbiz? (?
4
). An equally interesting observa-
tion is that engaging a lot about a specific topic will
more likely result to a higher than average impact;
the only exception is ?
8
which does not deviate
from the mean, but ?
8
rather represents the use of a
non-English language (Indonesian) and therefore,
does not form an actual topic of discussion.
To further understand how participation in the
10 most relevant topics relates to impact, we also
computed the joint user-topic entropy defined by
H(u
i
, ?) = ?
M
?
j=1
P(u
i
, ?
j
)? log
2
P(u
i
, ?
j
), (9)
where u
i
is a user and M = 10 (Shannon, 2001).
This is a measure of user pseudo-informativeness,
meaning that users with high entropy are consid-
ered as more informative (without assessing the
quality of the information). Figure 4 shows the im-
pact score distributions for the 500 accounts with
the lowest and highest entropy. Low and high en-
tropies are separated, with the former being placed
clearly below the mean user impact score and the
latter above. This pictorial assessment suggests that
a connection between informativeness and impact
may exist, at least in their extremes (their correla-
tion in the entire sample is r = .35, p < .001).
Use case scenarios. Most of the previous analysis
focused on the properties of single features. How-
ever, the user impact prediction models we learn
depend on feature combinations. For that reason,
it is of interest to investigate use case scenarios
that bring various attributes together. To reduce
notation in this paragraph, we use x
+
i
(x is ei-
ther a non-textual attribute a or a topic ? ) to ex-
press x
i
> ?(x
i
), the set of users for which the
value of feature x
i
is above the mean; equivalently
x
?
i
: x
i
< ?(x
i
). We also use ?
?
A
to express the
more complex set {?
+
A
? ?
?
j
? ... ? ?
?
z
}, an inter-
section of users that are active in one topic (?
A
),
but not very active in the rest. Figure 5 depicts the
user impact distributions for five use case scenarios.
Scenario A compares interactive to non interac-
tive users, represented by P(a
+
1
, a
+
6
, a
+
7
, a
+
8
) and
P(a
+
1
, a
?
6
, a
?
7
, a
?
8
) respectively; interactivity, de-
fined by an intersection of accounts that tweet regu-
larly, do many @-mentions and @-replies, but also
411
0 10 20 300
150
300
450
600
750
900 IA
NIA
0 10 20 300
100
200
300
400 IAIAC
0 10 20 300
100
200
300
400
500 L
NL
0 10 20 300
100
200
300
400
500 TO
TF
0 10 20 300
50
100
150
200 LT
ST
A B C D E
Figure 5: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for five Twitter
use scenarios based on subsets of the most relevant attributes and topics ? IA: Interactive, IAC: Clique
Interactive, L: Using many links, TO: Topic-Overall, TF: Topic-Focused, LT: ?Light? topics, ST: ?Serious?
topics. (N) denotes negation and lines the respective mean impact scores.
mention many different users, seems to be rewarded
on average with higher impact scores. Interactive
users gain more impact than clique-interactive ac-
counts represented by P(a
+
1
, a
+
6
, a
?
7
, a
+
8
), i.e. users
who interact, but do not mention many differ-
ent accounts, possibly because they are conduct-
ing discussions with a specific circle only (sce-
nario B). The use of links when writing about
the most prevalent topics (?Politics? and ?Show-
biz?) appears to be an important impact-wise fac-
tor (scenario C); the compared probability distri-
butions in that case were P
(
a
+
1
, (?
+
3
? ?
+
4
), a
+
9
)
against P
(
a
+
1
, (?
+
3
? ?
+
4
), a
?
9
)
. Surprisingly, when
links were replaced by hashtags in the previous
distributions, a clear class separation was not
achieved. In scenario D, topic-focused accounts,
i.e. users that write about one topic consistently,
represented by P
(
a
+
1
, (?
?
2
? ?
?
3
? ?
?
4
? ?
?
7
? ?
?
10
)
)
,
have on average slightly worse impact scores when
compared to accounts tweeting about many top-
ics, P(a
+
1
, ?
+
2
, ?
+
3
, ?
+
4
, ?
+
7
, ?
+
10
). Finally, scenario
E shows thats users engaging about more ?seri-
ous? topics, P
(
a
+
1
, ?
?
4
, ?
?
5
, ?
?
9
, (?
+
3
? ?
+
7
)
)
, were
not differentiated from the ones posting about more
?light? topics, P
(
a
+
1
, (?
+
4
? ?
+
5
? ?
+
9
), ?
?
3
, ?
?
7
)
.
7 Related Work
The task of user-impact prediction based on a ma-
chine learning approach that incorporates text fea-
tures is novel, to the best of our knowledge. De-
spite this fact, our work is partly related to research
approaches for quantifying and analysing user in-
fluence in online social networks. For example,
Cha et al. (2010) compared followers, retweets
and @-mentions received as measures of influ-
ence. Bakshy et al. (2011) aggregated all posts by
each user, computed an individual-level influence
and then tried to predict it by modelling user at-
tributes (# of followers, followees, tweets and date
of joining) together with past user influence. Their
method, based on classification and regression trees
(Breiman, 1984), achieved a modest performance
(r = .34). Furthermore, Romero et al. (2011) pro-
posed an algorithm for determining user influence
and passivity based on information-forwarding ac-
tivity, and Luo et al. (2013) exploited user attributes
to predict retweet occurrences. The primary differ-
ence with all the works described above is that we
aim to predict user impact by exploiting features
under the user?s direct control. Hence, our findings
can be used as indirect insights for strategies that in-
dividual users may follow to increase their impact
score. In addition, we incorporate the actual text
posted by the users in the entire modelling process.
8 Conclusions and Future Work
We have introduced the task of user impact pre-
diction on the microblogging platform of Twitter
based on user-controlled textual and non-textual
attributes. Nonlinear methods, in particular Gaus-
sian Processes, were more suitable than linear ap-
proaches for this problem, providing a strong per-
formance (r = .78). That result motivated the anal-
ysis of specific characteristics in the inferred model
to further define and understand the elements that
affect impact. In a nutshell, activity, non clique-
oriented interactivity and engagement on a diverse
set of topics are among the most decisive impact
factors. In future work, we plan to improve various
modelling components and gain a deeper under-
standing of the derived outcomes in collaboration
with domain experts. For more general conclusions,
the consideration of different cultures and media
sources is essential.
Acknowledgments
This research was supported by EU-FP7-ICT
project n.287863 (?TrendMiner?). Lampos also ac-
knowledges the support from EPSRC IRC project
EP/K031953/1.
412
References
Eytan Bakshy, Jake M. Hofman, Winter A. Mason, and Dun-
can J. Watts. 2011. Everyone?s an influencer: quantifying
influence on Twitter. In 4th International Conference on
Web Search and Data Mining, WSDM?11, pages 65?74.
Gerlof Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Biennial GSCL
Conference, pages 31?40.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010. Tweet,
Tweet, Retweet: Conversational Aspects of Retweeting on
Twitter. In System Sciences, HICSS?10, pages 1?10.
Leo Breiman. 1984. Classification and regression trees.
Chapman & Hall.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011.
Information credibility on Twitter. In 20th International
Conference on World Wide Web, WWW?11, pages 675?
684.
Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and
Krishna P. Gummadi. 2010. Measuring User Influence in
Twitter: The Million Follower Fallacy. In 4th International
Conference on Weblogs and Social Media, ICWSM?10,
pages 10?17.
Trevor Cohn and Lucia Specia. 2013. Modelling Annotator
Bias with Multi-task Gaussian Processes: An Application
to Machine Translation Quality Estimation. In 51st Annual
Meeting of the Association for Computational Linguistics,
ACL?13, pages 32?42.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Re-
gression: Biased Estimation for Nonorthogonal Problems.
Technometrics, 12(1):55?67.
Matthew Hoffman, David Blei, and Francis Bach. 2010. On-
line Learning for Latent Dirichlet Allocation. In Advances
in Neural Information Processing Systems, NIPS?10, pages
856?864.
Bernardo A. Huberman, Daniel M. Romero, and Fang Wu.
2009. Social Networks that Matter: Twitter Under the
Microscope. First Monday, 14(1).
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a news
media? In 19th International Conference on World Wide
Web, WWW?10, pages 591?600.
Vasileios Lampos and Nello Cristianini. 2012. Nowcast-
ing Events from the Social Web with Statistical Learning.
ACM Transactions on Intelligent Systems and Technology,
3(4):72:1?72:22.
Vasileios Lampos, Daniel Preot?iuc-Pietro, and Trevor Cohn.
2013. A user-centric model of voting intention from Social
Media. In 51st Annual Meeting of the Association for
Computational Linguistics, ACL?13, pages 993?1003.
Zhunchen Luo, Miles Osborne, Jintao Tang, and Ting Wang.
2013. Who will retweet me?: finding retweeters in Twit-
ter. In 36th International Conference on Research and
Development in Information Retrieval, SIGIR?13, pages
869?872.
Radford M. Neal. 1996. Bayesian Learning for Neural Net-
works. Springer.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2002. On
spectral clustering: Analysis and an algorithm. In Advances
in Neural Information Processing Systems, NIPS?02, pages
849?856.
Daniel Preot?iuc-Pietro, Sina Samangooei, Trevor Cohn,
Nicholas Gibbins, and Mahesan Niranjan. 2012. Trend-
miner: An Architecture for Real Time Analysis of Social
Media Text. In 6th International Conference on Weblogs
and Social Media, ICWSM?12, pages 38?42.
Joaquin Qui?nonero-Candela and Carl E. Rasmussen. 2005.
A unifying view of sparse approximate Gaussian Process
regression. Journal of Machine Learning Research, 6:1939?
1959.
Carl E. Rasmussen and Christopher K. I. Williams. 2006.
Gaussian Processes for Machine Learning. MIT Press.
Daniel M. Romero, Wojciech Galuba, Sitaram Asur, and
Bernardo A. Huberman. 2011. Influence and Passivity
in Social Media. In Machine Learning and Knowledge
Discovery in Databases, volume 6913, pages 18?33.
Dominic Rout, Daniel Preot?iuc-Pietro, Bontcheva Kalina, and
Trevor Cohn. 2013. Where?s @wally: A classification
approach to geolocating users based on their social ties. In
24th Conference on Hypertext and Social Media, HT?13,
pages 11?20.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010.
Earthquake shakes Twitter users: real-time event detection
by social sensors. In 19th International Conference on
World Wide Web, WWW?10, pages 851?860.
Claude E. Shannon. 2001. A mathematical theory of com-
munication. SIGMOBILE Mob. Comput. Commun. Rev.,
5(1):3?55 (reprint with corrections).
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and
image segmentation. Transactions on Pattern Analysis and
Machine Intelligence, 22(8):888?905.
Alex J. Smola and Bernhard Sch?olkopf. 2004. A tutorial
on support vector regression. Statistics and Computing,
14(3):199?222.
Edward Snelson and Zoubin Ghahramani. 2006. Sparse
Gaussian Processes using Pseudo-inputs. In Advances in
Neural Information Processing Systems, NIPS?06, pages
1257?1264.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H. Chi.
2010. Want to be Retweeted? Large Scale Analytics on
Factors Impacting Retweet in Twitter Network. In Social
Computing, SocialCom?10, pages 177?184.
Vladimir N. Vapnik. 1998. Statistical learning theory. Wiley.
Ulrike von Luxburg. 2007. A tutorial on spectral clustering.
Statistics and computing, 17(4):395?416.
Christopher K. I. Williams and Carl E. Rasmussen. 1996.
Gaussian Processes for Regression. In Advances in Neural
Information Processing Systems, NIPS?96, pages 514?520.
413
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 238?241,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Inducing Synchronous Grammars with Slice Sampling
Phil Blunsom
Computing Laboratory
Oxford University
Phil.Blunsom@comlab.ox.ac.uk
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Abstract
This paper describes an efficient sampler for
synchronous grammar induction under a non-
parametric Bayesian prior. Inspired by ideas
from slice sampling, our sampler is able to
draw samples from the posterior distributions
of models for which the standard dynamic pro-
graming based sampler proves intractable on
non-trivial corpora. We compare our sampler
to a previously proposed Gibbs sampler and
demonstrate strong improvements in terms of
both training log-likelihood and performance
on an end-to-end translation evaluation.
1 Introduction
Intractable optimisation algorithms abound in much
of the recent work in Natural Language Process-
ing. In fact, there is an increasing acceptance that
solutions to many of the great challenges of NLP
(e.g. machine translation, summarisation, question
answering) will rest on the quality of approximate
inference. In this work we tackle this problem in
the context of inducing synchronous grammars for
a machine translation system. We concern ourselves
with the lack of a principled, and scalable, algo-
rithm for learning a synchronous context free gram-
mar (SCFG) from sentence-aligned parallel corpora.
The predominant approach for learning phrase-
based translation models (both finite state or syn-
chronous grammar based) uses a cascade of heuris-
tics beginning with predicted word alignments
and producing a weighted set of translation rules
(Koehn et al, 2003). Alternative approaches avoid
such heuristics, instead learning structured align-
ment models directly from sentence aligned data
(e.g., (Marcu and Wong, 2002; Cherry and Lin,
2007; DeNero et al, 2008; Blunsom et al, 2009)).
Although these models are theoretically attractive,
inference is intractable (at least O(|f |3|e|3)). The
efficacy of direct estimation of structured alignment
models therefore rests on the approximations used
to make inference practicable ? typically heuristic
constraints or Gibbs sampling. In this work we show
that naive Gibbs sampling (specifically, Blunsom et
al. (2009)) is ineffectual for inference and reliant on
a high quality initialisation, mixing very slowly and
being easily caught in modes. Instead, blocked sam-
pling over sentence pairs allows much faster mixing,
but done in the obvious way (following Johnson et al
(2007)) would incur a O(|f |3|e|3) time complexity.
Here we draw inspiration from the work of
Van Gael et al (2008) on inference in infinite hid-
den Markov models to develop a novel algorithm
for efficient sampling from a SCFG. We develop an
auxiliary variable ?slice? sampler which can dramati-
cally reduce inference complexity, and thereby make
blocked sampling practicable on real translation cor-
pora. Our evaluation demonstrates that our algorithm
mixes more quickly than the local Gibbs sampler, and
produces translation models which achieve state-of-
the-art BLEU scores without using GIZA++ or sym-
metrisation heuristics for initialisation.
We adopt the generative model of Blunsom et
al. (2009) which creates a parallel sentence pair
by a sequence (derivation) of SCFG productions
d = (r1, r2, ..., rn). The tokens in each language can
be read off the leaves of the derivation tree while
their order is defined hierarchically by the produc-
tions in use. The probability of a derivation is defined
as p(d|?) =
?
r?d ?r where ? are the model param-
eters which are drawn from a Bayesian prior. We
deviate from that models definition of the prior over
phrasal translations, instead adopting the hierarchical
Dirichlet process prior from DeNero et al (2008),
which incorporates IBM Model 1. Blunsom et al
(2009) describe a blocked sampler following John-
son et al (2007) which uses the Metropolis-Hastings
algorithm to correct proposal samples drawn from
an approximating SCFG, however this is discounted
as impractical due to the O(|f |3|e|3) complexity.
Instead a Gibbs sampler is used which samples local
updates to the derivation structure of each training
instance. This avoids the dynamic program of the
238
blocked sampler but at the expense of considerably
slower mixing.
Recently Bouchard-Co?te? et al (2009) proposed
an auxialliary variable sampler, possibly comple-
mentary to ours, which was also evaluated on syn-
chronous parsing. Rather than slice sampling deriva-
tions in a collapsed Bayesian model, this model
employed a secondary proposal model (IBM Mod-
els) and sampled expectations over rule parameters.
2 Slice Sampling a SCFG
It would be advantageous to explore a middle ground
where the scope of the dynamic program is limited to
high probability regions, reducing the running time
to an acceptable level. By employing the technique
of slice sampling (Neal, 2003) we describe an algo-
rithm which stochastically samples from a reduced
space of possible derivations, while ensuring that
these samples are drawn from the correct distribu-
tion. We apply the slice sampler to the approximating
SCFG parameterised by ?, which requires samples
from an inside chart p(d|?) (for brevity, we omit the
dependency on ? in the following).
Slice sampling is an example of auxiliary variable
sampling in which we make use of the fact that if
we can draw samples from a joint distribution, then
we can trivially obtain samples from the marginal
distributions: p(d) =
?
u p(d,u), where d is the
variable of interest and u is an auxiliary variable.
Using a Gibbs sampler we can draw samples from
this joint distribution by alternately sampling from
p(d|u) and p(u|d). The trick is to ensure that u is
defined such that drawing samples from p(d|u) is
more efficient than from p(d).
We define the variable u to contain a slice variable
us for every cell of a synchronous parse chart for
every training instance:1
S = {(i, j, x, y) | 0 ? i < j ? |f |, 0 ? x < y ? |e|}
u = {us ? R | 0 < us < 1, s ? S}
These slice variables act as cutoffs on the probabili-
ties of the rules considered in each cell s: rule appli-
cations rs with ?rs ? us will be pruned from the
dynamic program.2
1The dependence on training instances is omitted here and
subsequently for simplicity. Each instance is independent, and
therefore this formulation can be trivially applied to a set.
2Alternatively, we could naively sample from a pruned chart
using a fixed beam threshold. However, this would not produce
samples from p(d), but some other unknown distribution.
Sampling p(u|d) Unlike Van Gael et al (2008),
there is not a one-to-one correspondence between the
spans of the rules in d and the set S, rather the deriva-
tion?s rule spans form a subset of S . This compli-
cates our definition of p(u|d); we must provide sepa-
rate accounts of how each us is generated depending
on whether there is a corresponding rule for s, i.e.,
rs ? d. We define p(u|d) =
?
s p(us|d), where:
p(us|d) =
{
I(us<?rs )
?rs
, if rs ? d
?(us; a, b) , else
(1)
which mixes a uniform distribution and a Beta dis-
tribution3 depending on the existence of a rule rs
in the derivation d.4 Eq. 1 is constructed such that
only rules with probability greater than the rele-
vant threshold, {rs | ?rs > us}, could have feasibly
been part of a derivation resulting in auxiliary vari-
able u. This is critical in reasoning over the reverse
conditional p(d|u) which only has to consider the
reduced space of rules (formulation below in (4)).
Trivially, the conditioning derivation is recoverable,
?rs ? d, ?rs ? us. We parameterise the ? distribu-
tion in (1) with a heavy skew towards zero in order
to limit the amount of pruning and thereby include
many competing derivations.5
Sampling p(d|u) Recall the probability of a
derivation, p(d) =
?
rs?d ?rs . We draw samples
from the joint distribution, p(d,u), holding u fixed:
p(d|u) ? p(d,u) = p(d)? p(u|d)
=
?
?
?
rs?d
?rs
?
??
( ?
us:rs?d
I(us<?rs )
?rs
?
?
us:rs 6?d ?(us; a, b)
)
=
?
us:rs?d
I (us < ?rs)
?
us:rs 6?d
?(us; a, b) (2)
=
?
us:rs?d
I (us < ?rs)
?(us; a, b)
?
us
?(us; a, b) (3)
?
?
us:rs?d
I (us < ?rs)
?(us; a, b)
(4)
In step (2) we cancel the ?rs terms while in step (3)
we introduce ?(us; a, b) terms to the numerator and
denominator for us : rs ? d to simplify the range
3Any distribution defined over {x ? R | 0 ? x ? 1} may be
used in place of ?, however this may affect the efficiency of the
sampler.
4I(?) returns 1 if the condition is true and 0 otherwise.
5We experiment with a range of a < 1 while fixing b = 1.
239
System BLEU time(s) LLH
Moses (default settings) 47.3 ? ?
LB init. 36.5 ? -257.1
M1 init. 48.8 ? -153.4
M4 init. 49.1 ? -151.4
Gibbs LB init. 45.3 44 -135.4
Gibbs M1 init. 48.2 40 -120.5
Gibbs M4 init. (Blunsom et al, 2009) 49.6 44 -110.3
Slice (a=0.15, b=1) LB init. 47.3 180 -98.9
Slice (a=0.10, b=1) M1 init. 50.4 908 -89.4
Slice (a=0.15, b=1) M1 init. 49.9 144 -90.2
Slice (a=0.25, b=1) M1 init. 49.2 80 -95.6
Table 1: IWSLT Chinese to English translation.
of the second product. The last step (4) discards the
term
?
us ?(us; a, b) which is constant wrt d. The
net result is a formulation which factors with the
derivation structure, thereby eliminating the need to
consider allO(|e|2|f |2) spans in S. Critically p(d|u)
is zero for all spans failing the I (us < ?rs) condition.
To exploit the decomposition of Equation 4 we
require a parsing algorithm that only explores chart
cells whose child cells have not already been pruned
by the slice variables. The standard approach of using
synchronous CYK (Wu, 1997) doesn?t posses this
property: all chart cells would be visited even if they
are to be pruned. Instead we use an agenda based
parsing algorithm, in particular we extend the algo-
rithm of Klein and Manning (2004) to synchronous
parsing.6 Finally, we need a Metropolis-Hastings
acceptance step to account for intra-instance depen-
dencies (the ?rich-get-richer? effect). We omit the
details, save to state that the calculation cancels to
the same test as presented in Johnson et al (2007).7
3 Evaluation
In the following experiments we compare the slice
sampler and the Gibbs sampler (Blunsom et al,
2009), in terms of mixing and translation quality. We
measure mixing in terms of training log-likelihood
(LLH) after a fixed number of sampling iterations.
Translations are produced using Moses (Koehn et al,
2007), initialised with the word alignments from the
final sample, and are evaluated using BLEU(Papineni
et al, 2001). The slice sampled models are restricted
to learning binary branching one-to-one (or null)
alignments,8 while no restriction is placed on the
Gibbs sampler (both use the same model, so have
6Moreover, we only sample values for us as they are visited
by the parser, thus avoiding the quartic complexity.
7Acceptance rates averaged above 99%.
8This restriction is not strictly necessary, however it greatly
simplifies the implementation and increases efficiency.
comparable LLH). Of particular interest is how the
different samplers perform given initialisations of
varying quality. We evaluate three initialisers: M4:
the symmetrised output of GIZA++ factorised into
ITG form (as used in Blunsom et al (2009)); M1:
the output of a heavily pruned ITG parser using the
IBM Model 1 prior for the rule probabilities;9 and
LB: left-branching monotone derivations.10
We experiment with the Chinese?English trans-
lation task from IWSLT, as used in Blunsom et al
(2009).11 Figure 1 shows LLH curves for the sam-
plers initialised with the M1 and LB derivations, plus
the curve for Gibbs sampler with the M4 initialiser.12
Table 1 gives BLEU scores on Test-05 for phrase-
based translation models built from the 1500th sam-
ple for the various models along with the average
time per sample and their final log-likelihood.
4 Discussion
The results are particularly encouraging. The slice
sampler uniformly finds much better solutions than
the Gibbs sampler regardless of initialisation. In
particular, the slice sampled model initialised with
the naive LB structure achieves a higher likelihood
than the M4 initialised model, although this is not
reflected in their relative BLEU scores. In contrast the
Gibbs sampler is more significantly affected by its
initialisation, only deviating slightly before becom-
ing trapped in a mode, as seen in Fig. 1. With suf-
ficient (infinite) time both sampling strategies will
converge on the true posterior regardless of initiali-
sation, however the slice sampler appears to be con-
verging much faster than the Gibbs sampler.
Interestingly, the initialisation heuristics (M1 and
M4) outperform the default heuristics (Koehn et al,
2007) by a considerable margin. This is most likely
because the initialisation heuristics force the align-
ments to factorise with an ITG, resulting in more
aggressive pruning of spurious alignments which in
turn allows for more and larger phrase pairs.
9The following beam heuristics are employed: alignments to
null are only permitted on the longer sentence side; words are
only allowed to align to those whose relative sentence position
is within ?3 words.
10Words of the longer sentence are randomly assigned to null.
11We limit the maximum training sentence length to 40, result-
ing in ? 40k training sentences.
12The GIZA++ M4 alignments don?t readily factorise to
word-based ITG derivations, as such we haven?t produced results
for this initialiser using the slice sampler.
240
0 50 100 150 200 250?
140
?
130
?
120
?
110
?
100
?
90
Samples
Log?
likeli
hood
Slice (a=0.10 b=1) M1Slice (a=0.15 b=1) M1Slice (a=0.20 b=1) M1Slice (a=0.25 b=1) M1Gibbs M1Gibbs M4
0 200 400 600 800 1000
?
200
?
180
?
160
?
140
?
120
?
100
Samples
Log?
likeli
hood
Slice (a=0.15 b=1) M1Slice (a=0.15 b=1) LBGibbs M1Gibbs LBGibbs M4
Figure 1: Training log-likelihood as a function of sampling iteration for Gibbs and slice sampling.
While the LLHs for the slice sampled models and
their BLEU scores appear correlated, this doesn?t
extend to comparisons with the Gibbs sampled mod-
els. We believe that this is because the GIZA++
initialisation alignments also explain the data well,
while not necessarily obtaining a high LLH under
the ITG model. Solutions which score highly in one
model score poorly in the other, despite both produc-
ing good translations.
The slice sampler is slower than the local Gibbs
sampler, its speed depending on the parameterisation
of the Beta distribution (affecting the width of the
beam). In the extreme, exhaustive search using the
full dynamic program is intractable on current hard-
ware,13 and therefore we have achieved our aim of
mediating between local and blocked inference.
This investigation has established the promise
of the SCFG slice sampling technique to provide
a scalable inference algorithm for non-parametric
Bayesian models. With further development, this
work could provide the basis for a family of prin-
cipled inference algorithms for parsing models, both
monolingual and synchronous, and other models that
prove intractable for exact dynamic programming.
References
P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proc. ACL/IJCNLP, 782?790, Suntec,
Singapore. Association for Computational Linguistics.
A. Bouchard-Co?te?, S. Petrov, D. Klein. 2009. Ran-
domized pruning: Efficiently calculating expectations
13Our implementation had not completed a single sample after
a week.
in large dynamic programs. In Advances in Neural
Information Processing Systems 22, 144?152.
C. Cherry, D. Lin. 2007. Inversion transduction grammar
for joint phrasal translation modeling. In Proc. SSST,
Rochester, USA.
J. DeNero, A. Bouchard-Co?te?, D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. EMNLP, 314?323, Honolulu, Hawaii.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo.
In Proc. HLT-NAACL, 139?146, Rochester, New York.
D. Klein, C. D. Manning, 2004. Parsing and hypergraphs,
351?372. Kluwer Academic Publishers, Norwell, MA,
USA, 2004.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, 81?88,
Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst.
2007. Moses: Open source toolkit for statistical
machine translation. In Proc. ACL, Prague.
D. Marcu, W. Wong. 2002. A phrase-based, joint proba-
bility model for statistical machine translation. In Proc.
EMNLP, 133?139, Philadelphia.
R. Neal. 2003. Slice sampling. Annals of Statistics,
31:705?767.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu:
a method for automatic evaluation of machine trans-
lation. Technical Report RC22176 (W0109-022), IBM
Research Division, Thomas J. Watson Research Center,
2001.
J. Van Gael, Y. Saatci, Y. W. Teh, Z. Ghahramani. 2008.
Beam sampling for the infinite hidden markov model.
In ICML, 1088?1095, New York, NY, USA.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
241
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 372?376,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Evaluating a Morphological Analyser of Inuktitut
Jeremy Nicholson?, Trevor Cohn? and Timothy Baldwin?
?Department of Computing and Information Systems, The University of Melbourne, Australia
?Department of Computer Science, The University of Sheffield, UK
jeremymn@csse.unimelb.edu.au, tcohn@dcs.shef.ac.uk, tb@ldwin.net
Abstract
We evaluate the performance of an morpho-
logical analyser for Inuktitut across a medium-
sized corpus, where it produces a useful anal-
ysis for two out of every three types. We
then compare its segmentation to that of sim-
pler approaches to morphology, and use these
as a pre-processing step to a word alignment
task. Our observations show that the richer ap-
proaches provide little as compared to simply
finding the head, which is more in line with
the particularities of the task.
1 Introduction
In this work, we evaluate a morphological analyser
of Inuktitut, whose polysynthetic morphosyntax can
cause particular problems for natural language pro-
cessing; but our observations are also relevant to
other languages with rich morphological systems.
The existing NLP task for Inuktitut is that of word
alignment (Martin et al, 2005), where Inuktitut to-
kens align to entire English clauses. While Langlais
et al (2005) theorises that a morphological analyser
could aid in this task, we observed little to no im-
provement over a baseline model by making use of
its segmentation. Nonetheless, morphological anal-
ysis does provide a great deal of information, but the
task structure tends to disprefer its contribution.
2 Background
2.1 Inuktitut
Inuktitut is a macrolanguage of many more-or-less
mutually intelligible dialects (Gordon, 2005). The
morphosyntax of Inuktitut is particularly marked by
a rich polysynthetic suffixing morphology, including
incorporation of arguments into verbal tokens, as in
natsiviniqtulauqsimavilli in (1). This phenomenon
causes an individual token in Inuktitut to be approx-
imately equivalent to an entire clause in English.
(1) natsiq-
seal
-viniq-
meat
-tuq-
eat
-lauq-
before
-sima-
ever
-vit
INT-2s
-li
but
?But have you ever eaten seal meat before??
Lowe (1996) analyses the morphology as a four-
place relationship: one head morpheme, zero or
more lexical morphemes, one or more grammatical
morphemes, and an optional enclitic. The morpho-
tactics causes, amongst other phenomena, the final
consonant of a morpheme to assimilate the manner
of the initial consonant of the following morpheme
(as in -villi), or to be dropped (as in natsiviniq-).
Consequently, morphemes are not readily accessible
from the realised surface form, thereby motivating
the use of a morphological analyser.
2.2 Morphological analysis
For many languages with a less rich morphol-
ogy than Inuktitut, an inflectional lexicon is of-
ten adequate for morphological analysis (for exam-
ple, CELEX for English (Burnage, 1990), Lefff for
French (Sagot et al, 2006) or Adolphs (2008) for
German). Another typical approach is to perform
morphological analysis at the same time as POS tag-
ging (as in Hajic? and Hladka? (1998) for the fusional
morphology in Czech), as it is often the case that
372
determining the part-of-speech and choosing the ap-
propriate inflectional paradigm are closely linked.
For highly inflecting languages more generally,
morphological analysis is often treated as a segment-
and-normalise problem, amenable to analysis by
weighted finite state transducer (wFST), for exam-
ple, Creutz and Lagus (2002) for Finnish.
3 Resources
3.1 A morphological analyser for Inuktitut
The main resource that we are evaluating in this
work is a morphological analyser of Inuktitut called
Uqa?Ila?Ut.1 It is a rule-based system based on reg-
ular morphological variations of about 3200 head,
350 lexical, and 1500 grammatical morphemes, with
heuristics for ranking the various readings. The head
and lexical morphemes are collated with glosses in
both English and French.
3.2 Word alignment
The training corpus we use in our experiments is a
sentence-aligned segment of the Nunavut Hansards
(Martin et al, 2003). The corpus consists of about
340K sentences, which comprise about 4.0M En-
glish tokens, and 2.2M Inuktitut. The challenge of
the morphology becomes apparent when we contrast
these figures with the types: about 416K for Inukti-
tut, but only 27K for English. On average, there are
only 5 token instances per Inuktitut type; some 338K
types (81%) are singletons.
Inuktitut formed part of one of the shared tasks
in the ACL 2005 workshop on building and us-
ing parallel texts (Martin et al, 2005); for this, the
above corpus was simplistically tokenised, and used
as unsupervised training data. 100 sentences from
this corpus were phrasally aligned by Inuit anno-
tators. These were then extended into word align-
ments, where phrasal alignments of one token in
both the source and target were (generally) called
sure alignments, and one-to-many or many-to-many
mappings were extended to their cartesian product,
and called probable. The test set was composed of
75 of these sentences (about 2K English tokens, 800
Inuktitut tokens, 293 gold-standard sure alignments,
1http://inuktitutcomputing.ca/Uqailaut/
en/IMA.html
and 1679 probable), which we use to evaluate word
alignments.
Our treatment of the alignment problem is most
similar to Schafer and Dra?bek (2005) who examine
four systems: GIZA++ models (Och and Ney, 2000)
for each source-target direction, another where the
Inuktitut input has been syllabised, and a wFST
model. They observe that aggregating these results
through voting can create a very competitive system
for Inuktitut word alignment.
4 Experimental approach
We used an out-of-the-box implementation of the
Berkeley Aligner (DeNero and Klein, 2007), a com-
petitive word alignment system, to construct an un-
supervised alignment over the 75 test sentences,
based on the larger training corpus. The default
implementation of the system involves two jointly-
trained HMMs (one for each source-target direc-
tion) over five iterations,2 with so-called compet-
itive thresholding in the decoding step; these are
more fully described in DeNero and Klein (2007)
and Liang et al (2006).
Our approach examines morphological pre-
processing of the Inuktitut training and test sets,
with the idea of leveraging the morphological in-
formation into a corpus which is more amenable to
alignment. The raw corpus appears to be under-
segmented, where data sparseness from the many
singletons would prevent reliable alignments. Seg-
mentation might aid in this process by making sub-
lexical units with semantic overlap transparent to the
alignment system, so that types appear to have a
greater frequency through the data. Through this,
we attempt to examine the hypothesis that one-to-
one alignments between English and Inuktitut would
hold with the right segmentation. On the other hand,
oversegmentation (for example, down to the charac-
ter level) can leave the resulting sub-lexical items se-
mantically meaningless and cause spurious matches.
We consider five different ways of tackling Inuk-
titut morphology:
1. None: simply treat each Inuktitut token as a
monolithic entity. This is our baseline ap-
proach.
2Better performance was observed with three iterations, but
we preferred to maintain the default parameters of the system.
373
2. Head: attempt to separate the head morpheme
from the non-head periphery. Our hypothesis
is that we will be able to align the clausal head
more reliably, as it tends to correspond to a sin-
gle English token more reliably than the other
morphemes, which may not be realised in the
same manner in English. Head morphs in Inuk-
titut correspond to the first one or two syllables
of a token; we treated them uniformly as two
syllables, as other values caused a substantial
degredation in performance.
3. Syllabification: treat the text as if Inuktitut
had isolating morphology, and transform each
token into a series of single-syllable pseudo-
morphs. This effectively turns the task on its
head, from a primarily one Inukitut-to-many
English token problem to that of one English-
to-many Inuktitut. Despite the overzealousness
of this approach (as most Inuktitut morphemes
are polysyllabic, and consequently there will be
many plausible but spurious matches between
tokens that share a syllable but no semantics),
Schafer and Dra?bek (2005) observed it to be
quite competitive.
4. Morphs: segment each word into morphs,
thereby treating the morphology problem as
pure segmentation. This uses the top output of
the morphological analyser as the oracle seg-
mentation of each Inuktitut token.
5. Morphemes: as previous, except include the
normalisation of each morph to a morpheme,
as provided by the morphological analyser, as
a sort of ?lemmatisation? step. The major ad-
vantage over the morph approach is due to the
regular morphophonemic effects in Inuktitut,
which cause equivalent morphemes to have dif-
ferent surface realisations.
5 Results
5.1 Analyser
In our analysis, the morphological analyser finds at
least one reading for about 218K (= about 65%) of
the Inuktitut types. Of the 120K types without read-
ings, resource contraints account for about 11K. 3
Another 6K types caused difficulties due to punctu-
ation, numerical characters or encoding issues, all of
which could be handled through more sophisticated
tokenisation.
A more interesting cause of gaps for
the analyser was typographical errors (e.g.
*kiinaujaqtaaruasirnirmut for kiinaujaqtaarusiar-
nirmut ?requests for proposals?). This was often
due to consonant gemination, where it was either
missing (e.g. nunavummut ?in Nunavut? appeared
in the corpus as *nunavumut) or added (e.g.
*tamakkununnga instead of tamakkununga ?at
these ones here?). While one might expect these
kinds of error to be rare, because Inuktitut has an
orthography that closely reflects pronunciation,
they instead are common, which means that the
morphological analyser should probably accept
incorrect gemination with a lower weighting.
More difficult to analyse directly is the impact
of foreign words (particularly names) ? these are
typically subjectively transliterated based on Inukti-
tut morphophonology. Schafer and Dra?bek (2005)
use these as motivation for an approach based on
a wFST, but found few instances to analyse its ac-
curacy. Finally, there are certainly missing roots,
and possibly some missing affixes as well, for ex-
ample pirru- ?accident? (cf. pirruaqi- ?to have an
accident?). Finding these automatically remains as
future work.
As for tokens, we briefly analysed the 768 tokens
in the test set, of which 228 (30%) were not given
a reading. Punctuation (typically commas and peri-
ods) account for 117 of these, and numbers another
7. Consonant gemination and foreign words cause
gaps for at least 16 and 6 tokens, respectively (that
we could readily identify).
5.2 Word Alignment
Following Och and Ney (2000), we assess using
alignment error rate (AER) and define precision with
respect to the probable set, and recall with respect to
3We only attempted to parse tokens of 30 characters or
shorter; longer tokens tended to cause exceptions ? this could
presumably be improved with a more efficient analyser. While
the number of analyses will continue to grow with the token
length, which has implications in agglutinative languages, here
there are only about 300 tokens of length greater than 40.
374
Approach Prec Rec AER
None 0.783 0.863 0.195
Head 0.797 0.922 0.176
Syllabification 0.789 0.881 0.192
Morphs 0.777 0.860 0.207
Morphemes 0.777 0.863 0.206
S&D E-I 0.646 0.829 0.327
S&D Syll 0.849 0.826 0.156
Table 1: Precision, recall, and alignment error rate for
various approaches to morphology, with Schafer and
Dra?bek (2005) for comparison
the sure set.
We present word alignment results of the vari-
ous methods ? contrasted with Schafer and Dra?bek
(2005) ? in Table 1. The striking result is in
terms of statistical significance: according to ?2,
most of the various approaches to morphology fail
to give a significantly (P < 0.05) different result
to the baseline system of using entire tokens. For
comparison, whereas our baseline system is signifi-
cantly better than the baseline system of Schafer and
Dra?bek (2005) ? which demonstrates the value that
the Berkeley Aligner provides by training in both
source-target directions ? their syllablised model
is significantly superior in precision (P < 0.001),
while their recall is still worse than our model (P <
0.05). Intuitively, this seems to indicate that their
model is making fewer judgments, but actually the
opposite is true. It seems that their model achieves
better performance than ours because it leverages
many candidate probable alignments into high qual-
ity aggregates using a most-likely heuristic on the
mapping of Inuktitut syllables to English words,
whereas the Berkeley Aligner culls the candidate set
in joint training.
Of the approaches toward morphology that we
consider, only the recall of the head?based sys-
tem improves upon the baseline (P < 0.025).
This squares with our intuitions, where segment-
ing the root morpheme from the larger token al-
lows for more effective alignment of the semanti-
cally straightforward sure alignments.
The three systems that involve a finer segmenta-
tion over the tokens are equivalent in performance to
the baseline system. The oversegmentation seemed
to caused the alignment system to abandon an im-
plicit preference for monotonicity of the order of
tokens between the source and target (which holds
pretty well for the baseline system over the test data,
thanks partly to the fidelity-focused structure of a
Hansard corpus): presumably because the aligner
perceives lexical similarity between disparate tokens
due to them sharing a sublexical unit. This relax-
ing of monotonicity is most apparent for punctua-
tion, where a comma with a correct alignment in the
baseline becomes incorrectly aligned to a different
comma in the sentence for the segmented system.
6 Conclusion
The only improvement toward the task that we ob-
served using morphological approaches is that of
head segmentation, where using two syllables as a
head-surrogate allowed us to capture more of the
sure (one-to-one) alignments in the test set. One
possible extension would be to take the head mor-
pheme as given the analyser, rather than the some-
what arbitrary syllabic approach. For other lan-
guages with rich morphology, it may be similarly
valuable to target substantives for segmentation to
improve alignment.
All in all, it appears that the lexical encoding of
morphology of Inuktitut is so strikingly different
than English, that the assumption of Inuktitut mor-
phemes aligning to English words is untrue or at
least unfindable within the current framework. Nu-
merous common morphemes have no English equiv-
alent, for example, -liaq- ?to go to? which seems to
act as a light verb, or -niq-, a (re-)nominaliser for
abstract nominals. While the output of the morpho-
logical analyser could probably be used more effec-
tively in other tasks, there are still important impacts
in word alignment and machine translation, includ-
ing leveraging a dictionary (which is based on mor-
phemes, not tokens, and as such requires segmenta-
tion and normalisation) or considering grammatical
forms for syntactic approaches.
References
Peter Adolphs. 2008. Acquiring a poor man?s inflec-
tional lexicon for German. In Proc. of the 6th LREC,
375
Marrakech, Morocco.
Gavin Burnage. 1990. CELEX: A guide for users. Tech-
nical report, University of Nijmegen.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proc. of the 6th Workshop
of ACL SIGPHON, pages 21?30, Philadelphia, USA.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Proc.
of the 45th Annual Meeting of the ACL, pages 17?24,
Prague, Czech Republic.
Raymund G. Gordon, Jr, editor. 2005. Ethnologue: Lan-
guages of the World, Fifteenth Edition. SIL Interna-
tional.
Jan Hajic? and Barbora Hladka?. 1998. Tagging inflective
languages: Prediction of morphological categories for
a rich, structured tagset. In Proc. of the 36th Annual
Meeting of the ACL and 17th International Conference
on COLING, pages 483?490, Montre?al, Canada.
Philippe Langlais, Fabrizio Gotti, and Guihong Cao.
2005. NUKTI: English-Inuktitut word alignment sys-
tem description. In Proc. of the ACL Workshop on
Building and Using Parallel Texts, pages 75?78, Ann
Arbor, USA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. of the HLT Conference
of the NAACL, pages 104?111, New York City, USA.
Ronald Lowe. 1996. Grammatical sketches: Inuktitut.
In Jacques Maurais, editor, Quebec?s Aboriginal Lan-
guages: History, Planning and Development, pages
204?232. Multilingual Matters.
Joel Martin, Howard Johnson, Benoit Farley, and Anna
Maclachlan. 2003. Aligning and using an English-
Inuktitut parallel corpus. In Proc. of the HLT-NAACL
2003 Workshop on Building and Using Parallel Texts,
pages 115?118, Edmonton, Canada.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignment for languages with scarce resources.
In Proc. of the ACL Workshop on Building and Using
Parallel Texts, pages 65?74, Ann Arbor, USA.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proc. of the 38th Annual
Meeting of the ACL, pages 440?447, Saarbru?cken,
Germany.
Beno??t Sagot, Lionel Cle?ment, Eric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff syntac-
tic lexicon for French: Architecture, acquisition, use.
In Proc. of the 5th LREC, pages 1348?1351, Genoa,
Italy.
Charles Schafer and Elliott Dra?bek. 2005. Models for
Inuktitut-English word alignment. In Proc. of the ACL
Workshop on Building and Using Parallel Texts, pages
79?82, Ann Arbor, USA.
376
Proceedings of the ACL 2010 Conference Short Papers, pages 225?230,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Blocked Inference in Bayesian Tree Substitution Grammars
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Phil Blunsom
Computing Laboratory
University of Oxford
Phil.Blunsom@comlab.ox.ac.uk
Abstract
Learning a tree substitution grammar is
very challenging due to derivational am-
biguity. Our recent approach used a
Bayesian non-parametric model to induce
good derivations from treebanked input
(Cohn et al, 2009), biasing towards small
grammars composed of small generalis-
able productions. In this paper we present
a novel training method for the model us-
ing a blocked Metropolis-Hastings sam-
pler in place of the previous method?s lo-
cal Gibbs sampler. The blocked sam-
pler makes considerably larger moves than
the local sampler and consequently con-
verges in less time. A core component
of the algorithm is a grammar transforma-
tion which represents an infinite tree sub-
stitution grammar in a finite context free
grammar. This enables efficient blocked
inference for training and also improves
the parsing algorithm. Both algorithms are
shown to improve parsing accuracy.
1 Introduction
Tree Substitution Grammar (TSG) is a compelling
grammar formalism which allows nonterminal
rewrites in the form of trees, thereby enabling
the modelling of complex linguistic phenomena
such as argument frames, lexical agreement and
idiomatic phrases. A fundamental problem with
TSGs is that they are difficult to estimate, even in
the supervised scenario where treebanked data is
available. This is because treebanks are typically
not annotated with their TSG derivations (how to
decompose a tree into elementary tree fragments);
instead the derivation needs to be inferred.
In recent work we proposed a TSG model which
infers an optimal decomposition under a non-
parametric Bayesian prior (Cohn et al, 2009).
This used a Gibbs sampler for training, which re-
peatedly samples for every node in every training
tree a binary value indicating whether the node is
or is not a substitution point in the tree?s deriva-
tion. Aggregated over the whole corpus, these val-
ues and the underlying trees specify the weighted
grammar. Local Gibbs samplers, although con-
ceptually simple, suffer from slow convergence
(a.k.a. poor mixing). The sampler can get easily
stuck because many locally improbable decisions
are required to escape from a locally optimal solu-
tion. This problem manifests itself both locally to
a sentence and globally over the training sample.
The net result is a sampler that is non-convergent,
overly dependent on its initialisation and cannot be
said to be sampling from the posterior.
In this paper we present a blocked Metropolis-
Hasting sampler for learning a TSG, similar to
Johnson et al (2007). The sampler jointly updates
all the substitution variables in a tree, making
much larger moves than the local single-variable
sampler. A critical issue when developing a
Metroplis-Hastings sampler is choosing a suitable
proposal distribution, which must have the same
support as the true distribution. For our model the
natural proposal distribution is a MAP point esti-
mate, however this cannot be represented directly
as it is infinitely large. To solve this problem we
develop a grammar transformation which can suc-
cinctly represent an infinite TSG in an equivalent
finite Context Free Grammar (CFG). The trans-
formed grammar can be used as a proposal dis-
tribution, from which samples can be drawn in
polynomial time. Empirically, the blocked sam-
pler converges in fewer iterations and in less time
than the local Gibbs sampler. In addition, we also
show how the transformed grammar can be used
for parsing, which yields theoretical and empiri-
cal improvements over our previous method which
truncated the grammar.
225
2 Background
A Tree Substitution Grammar (TSG; Bod et
al. (2003)) is a 4-tuple, G = (T,N, S,R), where
T is a set of terminal symbols, N is a set of non-
terminal symbols, S ? N is the distinguished root
nonterminal and R is a set of productions (rules).
The productions take the form of tree fragments,
called elementary trees (ETs), in which each in-
ternal node is labelled with a nonterminal and each
leaf is labelled with either a terminal or a nonter-
minal. The frontier nonterminal nodes in each ET
form the sites into which other ETs can be substi-
tuted. A derivation creates a tree by recursive sub-
stitution starting with the root symbol and finish-
ing when there are no remaining frontier nonter-
minals. Figure 1 (left) shows an example deriva-
tion where the arrows denote substitution. A Prob-
abilistic Tree Substitution Grammar (PTSG) as-
signs a probability to each rule in the grammar,
where each production is assumed to be condi-
tionally independent given its root nonterminal. A
derivation?s probability is the product of the prob-
abilities of the rules therein.
In this work we employ the same non-
parametric TSG model as Cohn et al (2009),
which we now summarise. The inference prob-
lem within this model is to identify the posterior
distribution of the elementary trees e given whole
trees t. The model is characterised by the use of
a Dirichlet Process (DP) prior over the grammar.
We define the distribution over elementary trees e
with root nonterminal symbol c as
Gc|?c, P0 ? DP(?c, P0(?|c))
e|c ? Gc
where P0(?|c) (the base distribution) is a distribu-
tion over the infinite space of trees rooted with c,
and ?c (the concentration parameter) controls the
model?s tendency towards either reusing elemen-
tary trees or creating novel ones as each training
instance is encountered.
Rather than representing the distribution Gc ex-
plicitly, we integrate over all possible values of
Gc. The key result required for inference is that
the conditional distribution of ei, given e?i,=
e1 . . . en\ei and the root category c is:
p(ei|e?i, c, ?c, P0)=
n?iei,c
n?i?,c + ?c
+
?cP0(ei|c)
n?i?,c + ?c
(1)
where n?iei,c is the number number of times ei has
been used to rewrite c in e?i, and n?i?,c =
?
e n
?i
e,c
S
NP
NP
George
VP
V
hates
NP
NP
broccoli
S
NP,1
George
VP,0
V,0
hates
NP,1
broccoli
Figure 1: TSG derivation and its corresponding Gibbs state
for the local sampler, where each node is marked with a bi-
nary variable denoting whether it is a substitution site.
is the total count of rewriting c. Henceforth we
omit the ?i sub-/super-script for brevity.
A primary consideration is the definition of P0.
Each ei can be generated in one of two ways:
by drawing from the base distribution, where the
probability of any particular tree is proportional to
?cP0(ei|c), or by drawing from a cache of previ-
ous expansions of c, where the probability of any
particular expansion is proportional to the number
of times that expansion has been used before. In
Cohn et al (2009) we presented base distributions
that favour small elementary trees which we ex-
pect will generalise well to unseen data. In this
work we show that if P0 is chosen such that it
decomposes with the CFG rules contained within
each elementary tree,1 then we can use a novel dy-
namic programming algorithm to sample deriva-
tions without ever enumerating all the elementary
trees in the grammar.
The model was trained using a local Gibbs sam-
pler (Geman and Geman, 1984), a Markov chain
Monte Carlo (MCMC) method in which random
variables are repeatedly sampled conditioned on
the values of all other random variables in the
model. To formulate the local sampler, we asso-
ciate a binary variable with each non-root inter-
nal node of each tree in the training set, indicat-
ing whether that node is a substitution point or
not (illustrated in Figure 1). The sampler then vis-
its each node in a random schedule and resamples
that node?s substitution variable, where the proba-
bility of the two different configurations are given
by (1). Parsing was performed using a Metropolis-
Hastings sampler to draw derivation samples for
a string, from which the best tree was recovered.
However the sampler used for parsing was biased
1Both choices of base distribution in Cohn et al (2009)
decompose into CFG rules. In this paper we focus on the
better performing one, PC0 , which combines a PCFG applied
recursively with a stopping probability, s, at each node.
226
because it used as its proposal distribution a trun-
cated grammar which excluded all but a handful
of the unseen elementary trees. Consequently the
proposal had smaller support than the true model,
voiding the MCMC convergence proofs.
3 Grammar Transformation
We now present a blocked sampler using the
Metropolis-Hastings (MH) algorithm to perform
sentence-level inference, based on the work of
Johnson et al (2007) who presented a MH sampler
for a Bayesian PCFG. This approach repeats the
following steps for each sentence in the training
set: 1) run the inside algorithm (Lari and Young,
1990) to calculate marginal expansion probabil-
ities under a MAP approximation, 2) sample an
analysis top-down and 3) accept or reject using a
Metropolis-Hastings (MH) test to correct for dif-
ferences between the MAP proposal and the true
model. Though our model is similar to John-
son et al (2007)?s, we have an added complica-
tion: the MAP grammar cannot be estimated di-
rectly. This is a consequence of the base distri-
bution having infinite support (assigning non-zero
probability to infinitely many unseen tree frag-
ments), which means the MAP has an infinite rule
set. For example, if our base distribution licences
the CFG production NP? NP PP then our TSG
grammar will contain the infinite set of elemen-
tary trees NP? NP PP, NP? (NP NP PP) PP,
NP? (NP (NP NP PP) PP) PP, . . . with decreas-
ing but non-zero probability.
However, we can represent the infinite MAP us-
ing a grammar transformation inspired by Good-
man (2003), which represents the MAP TSG in an
equivalent finite PCFG.2 Under the transformed
PCFG inference is efficient, allowing its use as
the proposal distribution in a blocked MH sam-
pler. We represent the MAP using the grammar
transformation in Table 1 which separates the ne,c
and P0 terms in (1) into two separate CFGs, A and
B. Grammar A has productions for every ET with
ne,c ? 1 which are assigned unsmoothed proba-
bilities: omitting the P0 term from (1).3 Grammar
B has productions for every CFG production li-
censed under P0; its productions are denoted using
2Backoff DOP uses a similar packed representation to en-
code the set of smaller subtrees for a given elementary tree
(Sima?an and Buratto, 2003), which are used to smooth its
probability estimate.
3The transform assumes inside inference. For Viterbi re-
place the probability for c? sign(e) with
n?e,c+?cP0(e|c)
n??,c+?c
.
For every ET, e, rewriting c with non-zero count:
c? sign(e)
n?e,c
n??,c+?c
For every internal node ei in e with children ei,1, . . . , ei,n
sign(ei)? sign(ei,1) . . . sign(ei,n) 1
For every nonterminal, c:
c? c? ?c
n??,c+?c
For every pre-terminal CFG production, c? t:
c? ? t PCFG(c? t)
For every unary CFG production, c? a:
c? ? a PCFG(c? a)sa
c? ? a? PCFG(c? a)(1? sa)
For every binary CFG production, c? ab:
c? ? ab PCFG(c? ab)sasb
c? ? ab? PCFG(c? ab)sa(1? sb)
c? ? a?b PCFG(c? ab)(1? sa)sb
c? ? a?b? PCFG(c? ab)(1? sa)(1? sb)
Table 1: Grammar transformation rules to map a MAP TSG
into a CFG. Production probabilities are shown to the right of
each rule. The sign(e) function creates a unique string sig-
nature for an ET e (where the signature of a frontier node is
itself) and sc is the Bernoulli probability of c being a substi-
tution variable (and stopping the P0 recursion).
primed (?) nonterminals. The rule c ? c? bridges
from A to B, weighted by the smoothing term
excluding P0, which is computed recursively via
child productions. The remaining rules in gram-
mar B correspond to every CFG production in the
underlying PCFG base distribution, coupled with
the binary decision whether or not nonterminal
children should be substitution sites (frontier non-
terminals). This choice affects the rule probability
by including a s or 1 ? s factor, and child sub-
stitution sites also function as a bridge back from
grammar B to A. In this way there are often two
equivalent paths to reach the same chart cell using
the same elementary tree ? via grammar A using
observed TSG productions and via grammar B us-
ing P0 backoff; summing these yields the desired
net probability.
Figure 2 shows an example of the transforma-
tion of an elementary tree with non-zero count,
ne,c ? 1, into the two types of CFG rules. Both
parts are capable of parsing the string NP, saw, NP
into a S, as illustrated in Figure 3; summing the
probability of both analyses gives the model prob-
ability from (1). Note that although the probabili-
ties exactly match the true model for a single ele-
mentary tree, the probability of derivations com-
posed of many elementary trees may not match
because the model?s caching behaviour has been
suppressed, i.e., the counts, n, are not incremented
during the course of a derivation.
For training we define the MH sampler as fol-
lows. First we estimate the MAP grammar over
227
S? NP VP{V{saw},NP}
n?e,S
n??,S+?S
VP{V{saw},NP} ? V{saw} NP 1
V{saw} ? saw 1
S? S? ?S
n??,S+?S
S?? NP VP? PCFG(S? NP VP)sNP (1? sV P )
VP?? V? NP PCFG(VP? V NP)(1? sV )sNP
V?? saw PCFG(V? saw)
Figure 2: Example of the transformed grammar for the ET
(S NP (VP (V saw) NP)). Taking the product of the rule
scores above the line yields the left term in (1), and the prod-
uct of the scores below the line yields the right term.
S
S{NP,{VP{V{hates}},NP}}
NP
George
VP{V{hates}},NP
V{hates}
hates
NP
broccoli
S
S?
NP
George
VP?
V?
hates
NP
broccoli
Figure 3: Example trees under the grammar transform, which
both encode the same TSG derivation from Figure 1. The left
tree encodes that the S? NP (VP (V hates) NP elementary
tree was drawn from the cache, while for the right tree this
same elementary tree was drawn from the base distribution
(the left and right terms in (1), respectively).
the derivations of training corpus excluding the
current tree, which we represent using the PCFG
transformation. The next step is to sample deriva-
tions for a given tree, for which we use a con-
strained variant of the inside algorithm (Lari and
Young, 1990). We must ensure that the TSG
derivation produces the given tree, and therefore
during inside inference we only consider spans
that are constituents in the tree and are labelled
with the correct nonterminal. Nonterminals are
said to match their primed and signed counter-
parts, e.g., NP? and NP{DT,NN{car}} both match
NP. Under the tree constraints the time complex-
ity of inside inference is linear in the length of the
sentence. A derivation is then sampled from the
inside chart using a top-down traversal (Johnson
et al, 2007), and converted back into its equiva-
lent TSG derivation. The derivation is scored with
the true model and accepted or rejected using the
MH test; accepted samples then replace the cur-
rent derivation for the tree, and rejected samples
leave the previous derivation unchanged. These
steps are then repeated for another tree in the train-
ing set, and the process is then repeated over the
full training set many times.
Parsing The grammar transform is not only use-
ful for training, but also for parsing. To parse a
sentence we sample a number of TSG derivations
from the MAP which are then accepted or rejected
into the full model using a MH step. The samples
are obtained from the same transformed grammar
but adapting the algorithm for an unsupervised set-
ting where parse trees are not available. For this
we use the standard inside algorithm applied to
the sentence, omitting the tree constraints, which
has time complexity cubic in the length of the sen-
tence. We then sample a derivation from the in-
side chart and perform the MH acceptance test.
This setup is theoretically more appealing than our
previous approach in which we truncated the ap-
proximation grammar to exclude most of the zero
count rules (Cohn et al, 2009). We found that
both the maximum probability derivation and tree
were considerably worse than a tree constructed
to maximise the expected number of correct CFG
rules (MER), based on Goodman?s (2003) algo-
rithm for maximising labelled recall. For this rea-
son we the MER parsing algorithm using sampled
Monte Carlo estimates for the marginals over CFG
rules at each sentence span.
4 Experiments
We tested our model on the Penn treebank using
the same data setup as Cohn et al (2009). Specifi-
cally, we used only section 2 for training and sec-
tion 22 (devel) for reporting results. Our models
were all sampled for 5k iterations with hyperpa-
rameter inference for ?c and sc ? c ? N , but in
contrast to our previous approach we did not use
annealing which we did not find to help general-
isation accuracy. The MH acceptance rates were
in excess of 99% across both training and parsing.
All results are averages over three runs.
For training the blocked MH sampler exhibits
faster convergence than the local Gibbs sam-
pler, as shown in Figure 4. Irrespective of the
initialisation the blocked sampler finds higher
likelihood states in many fewer iterations (the
same trend continues until iteration 5k). To be
fair, the blocked sampler is slower per iteration
(roughly 50% worse) due to the higher overheads
of the grammar transform and performing dy-
namic programming (despite nominal optimisa-
tion).4 Even after accounting for the time differ-
4The speed difference diminishes with corpus size: on
sections 2?22 the blocked sampler is only 19% slower per
228
0 100 200 300 400 500
?
330
000
?
325
000
?
320
000
?
315
000
?
310
000
?
305
000
iteration
log l
ikelih
ood
Block maximal initBlock minimal initLocal minimal initLocal maximal init
Figure 4: Training likelihood vs. iteration. Each sampling
method was initialised with both minimal and maximal ele-
mentary trees.
Training truncated transform
Local minimal init 77.63 77.98
Local maximal init 77.19 77.71
Blocked minimal init 77.98 78.40
Blocked maximal init 77.67 78.24
Table 2: Development F1 scores using the truncated pars-
ing algorithm and the novel grammar transform algorithm for
four different training configurations.
ence the blocked sampler is more effective than the
local Gibbs sampler. Training likelihood is highly
correlated with generalisation F1 (Pearson?s cor-
relation efficient of 0.95), and therefore improving
the sampler convergence will have immediate ef-
fects on performance.
Parsing results are shown in Table 2.5 The
blocked sampler results in better generalisation F1
scores than the local Gibbs sampler, irrespective of
the initialisation condition or parsing method used.
The use of the grammar transform in parsing also
yields better scores irrespective of the underlying
model. Together these results strongly advocate
the use of the grammar transform for inference in
infinite TSGs.
We also trained the model on the standard Penn
treebank training set (sections 2?21). We ini-
tialised the model with the final sample from a
run on the small training set, and used the blocked
sampler for 6500 iterations. Averaged over three
runs, the test F1 (section 23) was 85.3 an improve-
iteration than the local sampler.
5Our baseline ?Local maximal init? slightly exceeds pre-
viously reported score of 76.89% (Cohn et al, 2009).
ment over our earlier 84.0 (Cohn et al, 2009)
although still well below state-of-the-art parsers.
We conjecture that the performance gap is due to
the model using an overly simplistic treatment of
unknown words, and also a further mixing prob-
lems with the sampler. For the full data set the
counts are much larger in magnitude which leads
to stronger modes. The sampler has difficulty es-
caping such modes and therefore is slower to mix.
One way to solve the mixing problem is for the
sampler to make more global moves, e.g., with
table label resampling (Johnson and Goldwater,
2009) or split-merge (Jain and Neal, 2000). An-
other way is to use a variational approximation in-
stead of MCMC sampling (Wainwright and Jor-
dan, 2008).
5 Discussion
We have demonstrated how our grammar trans-
formation can implicitly represent an exponential
space of tree fragments efficiently, allowing us
to build a sampler with considerably better mix-
ing properties than a local Gibbs sampler. The
same technique was also shown to improve the
parsing algorithm. These improvements are in
no way limited to our particular choice of a TSG
parsing model, many hierarchical Bayesian mod-
els have been proposed which would also permit
similar optimised samplers. In particular mod-
els which induce segmentations of complex struc-
tures stand to benefit from this work; Examples
include the word segmentation model of Goldwa-
ter et al (2006) for which it would be trivial to
adapt our technique to develop a blocked sampler.
Hierarchical Bayesian segmentation models have
also become popular in statistical machine transla-
tion where there is a need to learn phrasal transla-
tion structures that can be decomposed at the word
level (DeNero et al, 2008; Blunsom et al, 2009;
Cohn and Blunsom, 2009). We envisage similar
representations being applied to these models to
improve their mixing properties.
A particularly interesting avenue for further re-
search is to employ our blocked sampler for un-
supervised grammar induction. While it is diffi-
cult to extend the local Gibbs sampler to the case
where the tree is not observed, the dynamic pro-
gram for our blocked sampler can be easily used
for unsupervised inference by omitting the tree
matching constraints.
229
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP), pages 782?790, Suntec, Singapore, Au-
gust.
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-oriented parsing. Center for the Study
of Language and Information - Studies in Computa-
tional Linguistics. University of Chicago Press.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
model of syntax-directed tree to string grammar in-
duction. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 352?361, Singapore, August.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 548?556, Boulder, Colorado, June.
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314?323, Honolulu,
Hawaii, October.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721?741.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673?680,
Sydney, Australia, July.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al (Bod et al, 2003),
chapter 8.
Sonia Jain and Radford M. Neal. 2000. A split-merge
Markov chain Monte Carlo procedure for the Dirich-
let process mixture model. Journal of Computa-
tional and Graphical Statistics, 13:158?182.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 139?146,
Rochester, NY, April.
Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4:35?56.
Khalil Sima?an and Luciano Buratto. 2003. Backoff
parameter estimation for the dop model. In Nada
Lavrac, Dragan Gamberger, Ljupco Todorovski, and
Hendrik Blockeel, editors, ECML, volume 2837 of
Lecture Notes in Computer Science, pages 373?384.
Springer.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Vari-
ational Inference. Now Publishers Inc., Hanover,
MA, USA.
230
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 865?874,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Hierarchical Pitman-Yor Process HMM
for Unsupervised Part of Speech Induction
Phil Blunsom
Department of Computer Science
University of Oxford
Phil.Blunsom@cs.ox.ac.uk
Trevor Cohn
Department of Computer Science
University of Sheffield
T.Cohn@dcs.shef.ac.uk
Abstract
In this work we address the problem of
unsupervised part-of-speech induction
by bringing together several strands of
research into a single model. We develop a
novel hidden Markov model incorporating
sophisticated smoothing using a hierarchical
Pitman-Yor processes prior, providing an
elegant and principled means of incorporating
lexical characteristics. Central to our
approach is a new type-based sampling
algorithm for hierarchical Pitman-Yor models
in which we track fractional table counts.
In an empirical evaluation we show that our
model consistently out-performs the current
state-of-the-art across 10 languages.
1 Introduction
Unsupervised part-of-speech (PoS) induction has
long been a central challenge in computational
linguistics, with applications in human language
learning and for developing portable language
processing systems. Despite considerable research
effort, progress in fully unsupervised PoS induction
has been slow and modern systems barely improve
over the early Brown et al (1992) approach
(Christodoulopoulos et al, 2010). One popular
means of improving tagging performance is to
include supervision in the form of a tag dictionary
or similar, however this limits portability and
also comprimises any cognitive conclusions. In
this paper we present a novel approach to fully
unsupervised PoS induction which uniformly
outperforms the existing state-of-the-art across all
our corpora in 10 different languages. Moreover, the
performance of our unsupervised model approaches
that of many existing semi-supervised systems,
despite our method not receiving any human input.
In this paper we present a Bayesian hidden
Markov model (HMM) which uses a non-parametric
prior to infer a latent tagging for a sequence of
words. HMMs have been popular for unsupervised
PoS induction from its very beginnings (Brown
et al, 1992), and justifiably so, as the most
discriminating feature for deciding a word?s PoS is
its local syntactic context.
Our work brings together several strands of
research including Bayesian non-parametric HMMs
(Goldwater and Griffiths, 2007), Pitman-Yor
language models (Teh, 2006b; Goldwater et
al., 2006b), tagging constraints over word types
(Brown et al, 1992) and the incorporation of
morphological features (Clark, 2003). The result
is a non-parametric Bayesian HMM which avoids
overfitting, contains no free parameters, and
exhibits good scaling properties. Our model uses
a hierarchical Pitman-Yor process (PYP) prior to
affect sophisicated smoothing over the transition
and emission distributions. This allows the
modelling of sub-word structure, thereby capturing
tag-specific morphological variation. Unlike many
existing approaches, our model is a principled
generative model and does not include any hand
tuned language specific features.
Inspired by previous successful approaches
(Brown et al, 1992), we develop a new type-
level inference procedure in the form of an
MCMC sampler with an approximate method for
incorporating the complex dependencies that arise
between jointly sampled events. Our experimental
evaluation demonstrates that our model, particularly
when restricted to a single tag per type, produces
865
state-of-the-art results across a range of corpora and
languages.
2 Background
Past research in unsupervised PoS induction has
largely been driven by two different motivations: a
task based perspective which has focussed on induc-
ing word classes to improve various applications,
and a linguistic perspective where the aim is to
induce classes which correspond closely to anno-
tated part-of-speech corpora. Early work was firmly
situtated in the task-based setting of improving gen-
eralisation in language models. Brown et al (1992)
presented a simple first-order HMM which restricted
word types to always be generated from the same
class. Though PoS induction was not their aim, this
restriction is largely validated by empirical analysis
of treebanked data, and moreover conveys the sig-
nificant advantage that all the tags for a given word
type can be updated at the same time, allowing very
efficient inference using the exchange algorithm.
This model has been popular for language mod-
elling and bilingual word alignment, and an imple-
mentation with improved inference called mkcls
(Och, 1999)1 has become a standard part of statis-
tical machine translation systems.
The HMM ignores orthographic information,
which is often highly indicative of a word?s part-
of-speech, particularly so in morphologically rich
languages. For this reason Clark (2003) extended
Brown et al (1992)?s HMM by incorporating a
character language model, allowing the modelling
of limited morphology. Our work draws from these
models, in that we develop a HMM with a one
class per tag restriction and include a character
level language model. In contrast to these previous
works which use the maximum likelihood estimate,
we develop a Bayesian model with a rich prior for
smoothing the parameter estimates, allowing us to
move to a trigram model.
A number of researchers have investigated a semi-
supervised PoS induction task in which a tag dictio-
nary or similar data is supplied a priori (Smith and
Eisner, 2005; Haghighi and Klein, 2006; Goldwater
and Griffiths, 2007; Toutanova and Johnson, 2008;
Ravi and Knight, 2009). These systems achieve
1Available from http://fjoch.com/mkcls.html.
much higher accuracy than fully unsupervised sys-
tems, though it is unclear whether the tag dictionary
assumption has real world application. We focus
solely on the fully unsupervised scenario, which we
believe is more practical for text processing in new
languages and domains.
Recent work on unsupervised PoS induction has
focussed on encouraging sparsity in the emission
distributions in order to match empirical distribu-
tions derived from treebank data (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008). These authors took a Bayesian approach
using a Dirichlet prior to encourage sparse distri-
butions over the word types emitted from each tag.
Conversely, Ganchev et al (2010) developed a tech-
nique to optimize the more desirable reverse prop-
erty of the word types having a sparse posterior dis-
tribution over tags. Recently Lee et al (2010) com-
bined the one class per word type constraint (Brown
et al, 1992) in a HMM with a Dirichlet prior to
achieve both forms of sparsity. However this work
approximated the derivation of the Gibbs sampler
(omitting the interdependence between events when
sampling from a collapsed model), resulting in a
model which underperformed Brown et al (1992)?s
one-class HMM.
Our work also seeks to enforce both forms of
sparsity, by developing an algorithm for type-level
inference under the one class constraint. This work
differs from previous Bayesian models in that we
explicitly model a complex backoff path using a
hierachical prior, such that our model jointly infers
distributions over tag trigrams, bigrams and uni-
grams and whole words and their character level
representation. This smoothing is critical to ensure
adequate generalisation from small data samples.
Research in language modelling (Teh, 2006b;
Goldwater et al, 2006a) and parsing (Cohn et
al., 2010) has shown that models employing
Pitman-Yor priors can significantly outperform the
more frequently used Dirichlet priors, especially
where complex hierarchical relationships exist
between latent variables. In this work we apply
these advances to unsupervised PoS tagging,
developing a HMM smoothed using a Pitman-Yor
process prior.
866
3 The PYP-HMM
We develop a trigram hidden Markov model which
models the joint probability of a sequence of latent
tags, t, and words, w, as
P?(t,w) =
L+1?
l=1
P?(tl|tl?1, tl?2)P?(wl|tl) ,
where L = |w| = |t| and t0 = t?1 = tL+1 = $ are
assigned a sentinel value to denote the start or end of
the sentence. A key decision in formulating such a
model is the smoothing of the tag trigram and emis-
sion distributions, which would otherwise be too dif-
ficult to estimate from small datasets. Prior work
in unsupervised PoS induction has employed simple
smoothing techniques, such as additive smoothing
or Dirichlet priors (Goldwater and Griffiths, 2007;
Johnson, 2007), however this body of work has over-
looked recent advances in smoothing methods used
for language modelling (Teh, 2006b; Goldwater et
al., 2006b). Here we build upon previous work by
developing a PoS induction model smoothed with
a sophisticated non-parametric prior. Our model
uses a hierarchical Pitman-Yor process prior for both
the transition and emission distributions, encoding
a backoff path from complex distributions to suc-
cesssively simpler ones. The use of complex dis-
tributions (e.g., over tag trigrams) allows for rich
expressivity when sufficient evidence is available,
while the hierarchy affords a means of backing off
to simpler and more easily estimated distributions
otherwise. The PYP has been shown to generate
distributions particularly well suited to modelling
language (Teh, 2006a; Goldwater et al, 2006b), and
has been shown to be a generalisation of Kneser-Ney
smoothing, widely recognised as the best smoothing
method for language modelling (Chen and Good-
man, 1996).
The model is depicted in the plate diagram in Fig-
ure 1. At its centre is a standard trigram HMM,
which generates a sequence of tags and words,
tl|tl?1, tl?2, T ? Ttl?1,tl?2
wl|tl, E ? Etl .
U
B
j
T
ij
E
j
C
jk
w
1
t
1
w
2
t
2
w
3
t
3
...
D
j
Figure 1: Plate diagram representation of the trigram
HMM. The indexes i and j range over the set of tags
and k ranges over the set of characters. Hyper-parameters
have been omitted from the figure for clarity.
The trigram transition distribution, Tij , is drawn
from a hierarchical PYP prior which backs off to a
bigram Bj and then a unigram U distribution,
Tij |a
T , bT , Bj ? PYP(a
T , bT , Bj)
Bj |a
B, bB, U ? PYP(aB, bB, U)
U |aU , bU ? PYP(aU , bU ,Uniform) ,
where the prior over U has as its base distribition a
uniform distribution over the set of tags, while the
priors for Bj and Tij back off by discarding an item
of context. This allows the modelling of trigram
tag sequences, while smoothing these estimates with
their corresponding bigram and unigram distribu-
tions. The degree of smoothing is regulated by
the hyper-parameters a and b which are tied across
each length of n-gram; these hyper-parameters are
inferred during training, as described in 3.1.
The tag-specific emission distributions, Ej , are
also drawn from a PYP prior,
Ej |a
E , bE , C ? PYP(aE , bE , Cj) .
We consider two different settings for the base distri-
bution Cj : 1) a simple uniform distribution over the
vocabulary (denoted HMM for the experiments in
section 4); and 2) a character-level language model
(denoted HMM+LM). In many languages morpho-
logical regularities correlate strongly with a word?s
part-of-speech (e.g., suffixes in English), which we
hope to capture using a basic character language
model. This model was inspired by Clark (2003)
867
The big dog
5 23 23 7
b r o w n
Figure 2: The conditioning structure of the hierarchical
PYP with an embedded character language models.
who applied a character level distribution to the sin-
gle class HMM (Brown et al, 1992). We formu-
late the character-level language model as a bigram
model over the character sequence comprising word
wl,
wlk|wlk?1, tl, C ? Ctlwlk?1
Cjk|a
C , bC , Dj ? PYP(a
C , bC , Dj)
Dj |a
D, bD ? PYP(aD, bD,Uniform) ,
where k indexes the characters in the word and,
in a slight abuse of notation, the character itself,
w0 and is set to a special sentinel value denoting
the start of the sentence (ditto for a final end of
sentence marker) and the uniform base distribution
ranges over the set of characters. We expect that
the HMM+LM model will outperform the uniform
HMM as it can capture many consistent morpholog-
ical affixes and thereby better distinguish between
different parts-of-speech. The HMM+LM is shown
in Figure 2, illustrating the decomposition of the tag
sequence into n-grams and a word into its compo-
nent character bigrams.
3.1 Training
In order to induce a tagging under this model we
use Gibbs sampling, a Markov chain Monte Carlo
(MCMC) technique for drawing samples from the
posterior distribution over the tag sequences given
observed word sequences. We present two different
sampling strategies: First, a simple Gibbs sampler
which randomly samples an update to a single tag
given all other tags; and second, a type-level sam-
pler which updates all tags for a given word under a
one-tag-per-word-type constraint. In order to extract
a single tag sequence to test our model against the
gold standard we find the tag at each site with maxi-
mum marginal probability in the sample set.
Following standard practice, we perform
inference using a collapsed sampler whereby
the model parameters U,B, T,E and C are
marginalised out. After marginalisation the
posterior distribution under a PYP prior is described
by a variant of the Chinese Restaurant Process
(CRP). The CRP is based around the analogy of
a restaurant with an infinite number of tables,
with customers entering one at a time and seating
themselves at a table. The choice of table is
governed by
P (zl = k|z?l) =
?
?
?
n?k ?a
l?1+b 1 ? k ? K
?
K?a+b
l?1+b k = K
? + 1
(1)
where zl is the table chosen by the lth customer, z?l
is the seating arrangement of the l? 1 previous cus-
tomers, n?k is the number of customers in z?l who
are seated at table k, K? = K(z?l) is the total num-
ber of tables in z?l, and z1 = 1 by definition. The
arrangement of customers at tables defines a cluster-
ing which exhibits a power-law behavior controlled
by the hyperparameters a and b.
To complete the restaurant analogy, a dish is then
served to each table which is shared by all the cus-
tomers seated there. This corresponds to a draw
from the base distribution, which in our case ranges
over tags for the transition distribution, and words
for the observation distribution. Overall the PYP
leads to a distribution of the form
P T (tl = i|z?l, t?l) =
1
n?h + b
T
? (2)
(
n?hi ?K
?
hia
T +
(
K?h a
T + bT
)
PB(i|z?l, t?l)
)
,
illustrating the trigram transition distribution, where
t?l are all previous tags, h = (tl?2, tl?1) is the con-
ditioning bigram, n?hi is the count of the trigram hi
in t?l, n
?
h the total count over all trigrams beginning
with h, K?hi the number of tables served dish i and
PB(?) is the base distribution, in this case the bigram
distribution.
A hierarchy of PYPs can be formed by making the
base distribution of a PYP another PYP, following a
868
semantics whereby whenever a customer sits at an
empty table in a restaurant, a new customer is also
said to enter the restaurant for its base distribution.
That is, each table at one level is equivalent to a cus-
tomer at the next deeper level, creating the invari-
ants: K?hi = n
?
ui andK
?
ui = n
?
i , where u = tl?1
indicates the unigram backoff context of h. The
recursion terminates at the lowest level where the
base distribution is static. The hierarchical setting
allows for the modelling of elaborate backoff paths
from rich and complex structure to successively sim-
pler structures.
Gibbs samplers Both our Gibbs samplers perform
the same calculation of conditional tag distributions,
and involve first decrementing all trigrams and emis-
sions affected by a sampling action, and then rein-
troducing the trigrams one at a time, conditioning
their probabilities on the updated counts and table
configurations as we progress.
The first local Gibbs sampler (PYP-HMM)
updates a single tag assignment at a time, in a
similar fashion to Goldwater and Griffiths (2007).
Changing one tag affects three trigrams, with
posterior
P (tl|z?l, t?l,w) ? P (tl?2, wl|z?l?2, t?l?2) ,
where l?2 denotes the range l?2, l?1, l, l+1, l+2.
The joint distribution over the three trigrams con-
tained in tl?2 can be calculated using the PYP for-
mulation. This calculation is complicated by the fact
that these events are not independent; the counts of
one trigram can affect the probability of later ones,
and moreover, the table assignment for the trigram
may also affect the bigram and unigram counts, of
particular import when the same tag occurs twice in
a row such as in Figure 2.
Many HMMs used for inducing word classes for
language modelling include the restriction that all
occurrences of a word type always appear with the
same class throughout the corpus (Brown et al,
1992; Och, 1999; Clark, 2003). Our second sampler
(PYP-1HMM) restricts inference to taggings which
adhere to this one tag per type restriction. This
restriction permits efficient inference techniques in
which all tags of all occurrences of a word type are
updated in parallel. Similar techniques have been
used for models with Dirichlet priors (Liang et al,
2010), though one must be careful to manage the
dependencies between multiple draws from the pos-
terior.
The dependency on table counts in the conditional
distributions complicates the process of drawing
samples for both our models. In the non-hierarchical
model (Goldwater and Griffiths, 2007) these
dependencies can easily be accounted for by
incrementing customer counts when such a
dependence occurs. In our model we would need to
sum over all possible table assignments that result
in the same tagging, at all levels in the hierarchy:
tag trigrams, bigrams and unigrams; and also words,
character bigrams and character unigrams. To avoid
this rather onerous marginalisation2 we instead use
expected table counts to calculate the conditional
distributions for sampling. Unfortunately we
know of no efficient algorithm for calculating the
expected table counts, so instead develop a novel
approximation
En+1 [Ki] ? En [Ki] +
(aUEn [K] + bU )P0(i)
(n? En [Ki] bU ) + (aUEn [K] + bU )P0(i)
, (3)
where Ki is the number of tables for the tag uni-
gram i of which there are n + 1 occurrences, En [?]
denotes an expectation after observing n items and
En [K] =
?
j En [Kj ]. This formulation defines
a simple recurrence starting with the first customer
seated at a table, E1 [Ki] = 1, and as each subse-
quent customer arrives we fractionally assign them
to a new table based on their conditional probability
of sitting alone. These fractional counts are then
carried forward for subsequent customers.
This approximation is tight for small n, and there-
fore it should be effective in the case of the local
Gibbs sampler where only three trigrams are being
resampled. For the type based resampling where
large numbers of n are involved (consider resam-
pling the), this approximation can deviate from the
actual value due to errors accumulated in the recur-
sion. Figure 3 illustrates a simulation demonstrating
that the approximation is a close match for small a
and n but underestimates the true value for high a
2Marginalisation is intractable in general, i.e. for the 1HMM
where many sites are sampled jointly.
869
0 20 40 60 80 100
2
4
6
8
10
12
number of customers
expe
cted
 tabl
es
a=0.9a=0.8a=0.5a=0.1
Figure 3: Simulation comparing the expected table
count (solid lines) versus the approximation under Eq. 3
(dashed lines) for various values of a. This data was gen-
erated from a single PYP with b = 1, P0(i) = 14 and
n = 100 customers which all share the same tag.
and n. The approximation was much less sensitive
to the choice of b (not shown).
To resample a sequence of trigrams we start by
removing their counts from the current restaurant
configuration (resulting in z?). For each tag we
simulate adding back the trigrams one at a time,
calculating their probability under the given z? plus
the fractional table counts accumulated by Equation
3. We then calculate the expected table count con-
tribution from this trigram and add it to the accu-
mulated counts. The fractional table count from the
trigram then results in a fractional customer entering
the bigram restaurant, and so on down to unigrams.
At each level we must update the expected counts
before moving on to the next trigram. After per-
forming this process for all trigrams under consider-
ation and for all tags, we then normalise the resulting
tag probabilities and sample an outcome. Once a
tag has been sampled, we then add all the trigrams
to the restaurants sampling their tables assignments
explicitly (which are no longer fractional), recorded
in z. Because we do not marginalise out the table
counts and our expectations are only approximate,
this sampler will be biased. We leave to future work
properly accounting for this bias, e.g., by devising a
Metropolis Hastings acceptance test.
Sampling hyperparameters We treat the
hyper-parameters {(ax, bx) , x ? (U,B, T,E,C)}
as random variables in our model and infer their
values. We place prior distributions on the PYP
discount ax and concentration bx hyperparamters
and sample their values using a slice sampler. For
the discount parameters we employ a uniform
Beta distribution (ax ? Beta(1, 1)), and for
the concentration parameters we use a vague
gamma prior (bx ? Gamma(10, 0.1)). All the
hyper-parameters are resampled after every 5th
sample of the corpus.
The result of this hyperparameter inference is that
there are no user tunable parameters in the model,
an important feature that we believe helps explain its
consistently high performance across test settings.
4 Experiments
We perform experiments with a range of corpora
to both investigate the properties of our proposed
models and inference algorithms, as well as to estab-
lish their robustness across languages and domains.
For our core English experiments we report results
on the entire Penn. Treebank (Marcus et al, 1993),
while for other languages we use the corpora made
available for the CoNLL-X Shared Task (Buchholz
and Marsi, 2006). We report results using the many-
to-one (M-1) and v-measure (VM) metrics consid-
ered best by the evaluation of Christodoulopoulos
et al (2010). M-1 measures the accuracy of the
model after mapping each predicted class to its most
frequent corresponding tag, while VM is a variant
of the F-measure which uses conditional entropy
analogies of precision and recall. The log-posterior
for the HMM sampler levels off after a few hundred
samples, so we report results after five hundred. The
1HMM sampler converges more quickly so we use
two hundred samples for these models. All reported
results are the mean of three sampling runs.
An important detail for any unsupervised
learning algorithm is its initialisation. We used
slightly different initialisation for each of our
inference strategies. For the unrestricted HMM we
randomly assigned each word token to a class. For
the restricted 1HMM we use a similar initialiser to
870
Model M-1 VM
Prototype meta-model (CGS10) 76.1 68.8
MEMM (BBDK10) 75.5 -
mkcls (Och, 1999) 73.7 65.6
MLE 1HMM-LM (Clark, 2003)? 71.2 65.5
BHMM (GG07) 63.2 56.2
PR (Ganchev et al, 2010)? 62.5 54.8
Trigram PYP-HMM 69.8 62.6
Trigram PYP-1HMM 76.0 68.0
Trigram PYP-1HMM-LM 77.5 69.7
Bigram PYP-HMM 66.9 59.2
Bigram PYP-1HMM 72.9 65.9
Trigram DP-HMM 68.1 60.0
Trigram DP-1HMM 76.0 68.0
Trigram DP-1HMM-LM 76.8 69.8
Table 1: WSJ performance comparing previous work
to our own model. The columns display the many-to-1
accuracy and the V measure, both averaged over 5 inde-
pendent runs. Our model was run with the local sampler
(HMM), the type-level sampler (1HMM) and also with
the character LM (1HMM-LM). Also shown are results
using Dirichlet Process (DP) priors by fixing a = 0. The
system abbreviations are CGS10 (Christodoulopoulos et
al., 2010), BBDK10 (Berg-Kirkpatrick et al, 2010) and
GG07 (Goldwater and Griffiths, 2007). Starred entries
denote results reported in CGS10.
Clark (2003), assigning each of the k most frequent
word types to its own class, and then randomly
dividing the rest of the types between the classes.
As a baseline we report the performance of
mkcls (Och, 1999) on all test corpora. This model
seems not to have been evaluated in prior work on
unsupervised PoS tagging, which is surprising given
its consistently good performance.
First we present our results on the most frequently
reported evaluation, the WSJ sections of the Penn.
Treebank, along with a number of state-of-the-art
results previously reported (Table 1). All of these
models are allowed 45 tags, the same number of tags
as in the gold-standard. The performance of our
models is strong, particularly the 1HMM. We also
see that incorporating a character language model
(1HMM-LM) leads to further gains in performance,
improving over the best reported scores under both
M-1 and VM. We have omitted the results for the
HMM-LM as experimentation showed that the local
Gibbs sampler became hopelessly stuck, failing to
0 10 20 30 40 500
2
4
6
8
10
12
14
16
18 x 10
4
Tags sorted by frequency
Fre
qu
en
cy
 
 
Gold tag distribution
1HMM
1HMM?LM
MKCLS
Figure 4: Sorted frequency of tags for WSJ. The gold
standard distribution follows a steep exponential curve
while the induced model distributions are more uniform.
mix due to the model?s deep structure (its peak per-
formance was ? 55%).
To evaluate the effectiveness of the PYP prior we
include results using a Dirichlet Process prior (DP).
We see that for all models the use of the PYP pro-
vides some gain for the HMM, but diminishes for
the 1HMM. This is perhaps a consequence of the
expected table count approximation for the type-
sampled PYP-1HMM: the DP relies less on the table
counts than the PYP.
If we restrict the model to bigrams we see
a considerable drop in performance. Note that
the bigram PYP-HMM outperforms the closely
related BHMM (the main difference being that
we smooth tag bigrams with unigrams). It is also
interesting to compare the bigram PYP-1HMM to
the closely related model of Lee et al (2010). That
model incorrectly assumed independence of the
conditional sampling distributions, resulting in a
accuracy of 66.4%, well below that of our model.
Figures 4 and 5 provide insight into the behavior
of the sampling algorithms. The former shows that
both our models and mkcls induce a more uniform
distribution over tags than specified by the treebank.
It is unclear whether it is desirable for models to
exhibit behavior closer to the treebank, which ded-
icates separate tags to very infrequent phenomena
while lumping the large range of noun types into
a single category. The graph in Figure 5 shows
that the type-based 1HMM sampler finds a good
tagging extremely quickly and then sticks with it,
871
0 50 100 15010
20
30
40
50
60
70
80
Number of samples
M?1
 Acc
urac
y (%
)
 
 
PYP?1HMMPYP?1HMM?LMPYP?HMMPYP?HMM?LM
Figure 5: M-1 accuracy vs. number of samples.
NNIN
NNPDT
JJNNS
,.
CDRB
VBDVB
CCTO
VBZVBN
PRPVBG
VBPMD
POSPRP$
$??
??:
WDTJJR
RPNNPS
WPWRB
JJSRBR
?RRB??LRB?
EXRBS
PDTFW
WP$#
UHSYM
NNIN
NNPDT
JJNNS
,.
CDRB
VBDVB
CCTO
VBZVBN
PRPVBG
VBPMD
POSPRP$
$??
??:
WDTJJR
RPNNPS
WPWRB
JJSRBR
?RRB??LRB?
EXRBS
PDTFW
WP$#
UHSYM
Figure 6: Cooccurence between frequent gold (y-axis)
and predicted (x-axis) tags, comparing mkcls (top) and
PYP-1HMM-LM (bottom). Both axes are sorted in terms
of frequency. Darker shades indicate more frequent cooc-
curence and columns represent the induced tags.
save for the occasional step change demonstrated by
the 1HMM-LM line. The locally sampled model is
far slower to converge, rising slowly and plateauing
well below the other models.
In Figure 6 we compare the distributions over
WSJ tags for mkcls and the PYP-1HMM-LM. On
the macro scale we can see that our model induces a
sparser distribution. With closer inspection we can
identify particular improvements our model makes.
In the first column for mkcls and the third column
for our model we can see similar classes with sig-
nificant counts for DTs and PRPs, indicating a class
that the models may be using to represent the start
of sentences (informed by start transitions or capi-
talisation). This column exemplifies the sparsity of
the PYP model?s posterior.
We continue our evaluation on the CoNLL
multilingual corpora (Table 2). These results show
a highly consistent story of performance for our
models across diverse corpora. In all cases the
PYP-1HMM outperforms the PYP-HMM, which
are both outperformed by the PYP-1HMM-LM.
The character language model provides large
gains in performance on a number of corpora,
in particular those with rich morphology (Arabic
+5%, Portuguese +5%, Spanish +4%). We again
note the strong performance of the mkcls model,
significantly beating recently published state-of-the-
art results for both Dutch and Swedish. Overall our
best model (PYP-1HMM-LM) outperforms both
the state-of-the-art, where previous work exists, as
well as mkcls consistently across all languages.
5 Discussion
The hidden Markov model, originally developed by
Brown et al (1992), continues to be an effective
modelling structure for PoS induction. We have
combined hierarchical Bayesian priors with a tri-
gram HMM and character language model to pro-
duce a model with consistently state-of-the-art per-
formance across corpora in ten languages. How-
ever our analysis indicates that there is still room for
improvement, particularly in model formulation and
developing effective inference algorithms.
Induced tags have already proven their usefulness
in applications such as Machine Translation, thus it
will prove interesting as to whether the improve-
ments seen from our models can lead to gains in
downstream tasks. The continued successes of mod-
els combining hierarchical Pitman-Yor priors with
expressive graphical models attests to this frame-
work?s enduring attraction, we foresee continued
interest in applying this technique to other NLP
tasks.
872
Language mkcls HMM 1HMM 1HMM-LM Best pub. Tokens Tag types
Arabic 58.5 57.1 62.7 67.5 - 54,379 20
Bulgarian 66.8 67.8 69.7 73.2 - 190,217 54
Czech 59.6 62.0 66.3 70.1 - 1,249,408 12c
Danish 62.7 69.9 73.9 76.2 66.7? 94,386 25
Dutch 64.3 66.6 68.7 70.4 67.3? 195,069 13c
Hungarian 54.3 65.9 69.0 73.0 - 131,799 43
Portuguese 68.5 72.1 73.5 78.5 75.3? 206,678 22
Spanish 63.8 71.6 74.7 78.8 73.2? 89,334 47
Swedish 64.3 66.6 67.0 68.6 60.6? 191,467 41
Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published
result (?Berg-Kirkpatrick et al (2010) and ?Lee et al (2010)). This data was taken from the CoNLL-X shared task
training sets, resulting in listed corpus sizes. Fine PoS tags were used for evaluation except for items marked with c,
which used the coarse tags. For each language the systems were trained to produce the same number of tags as the
gold standard.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479, December.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, CoNLL-X ?06, pages 149?
164, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, pages
310?318, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 575?584, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting of the
European Association for Computational Linguistics
(EACL), pages 59?66.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053?3096.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 99:2001?2049, August.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 344?352, Morristown, NJ,
USA. Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), pages 744?751, Prague, Czech Republic,
June.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006a. Contextual dependencies in unsupervised
word segmentation. In Proc. of the 44th Annual Meet-
ing of the ACL and 21st International Conference
on Computational Linguistics (COLING/ACL-2006),
Sydney.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural
873
Information Processing Systems 18, pages 459?466.
MIT Press, Cambridge, MA.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 320?
327, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Mark Johnson. 2007. Why doesnt EM find good
HMM POS-taggers? In Proc. of the 2007 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007), pages 296?305, Prague, Czech
Republic.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 853?861, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based
MCMC. In North American Association for Compu-
tational Linguistics (NAACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 71?76,
Morristown, NJ, USA. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conferenceof the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing (ACL-IJCNLP), pages
504?512.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 354?362, Ann Arbor, Michigan, June.
Y. W. Teh. 2006a. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985?992.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL-44, pages
985?992, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 1521?1528. MIT Press,
Cambridge, MA.
874
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 32?42,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modelling Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Estimation
Trevor Cohn and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{t.cohn,l.specia}@sheffield.ac.uk
Abstract
Annotating linguistic data is often a com-
plex, time consuming and expensive en-
deavour. Even with strict annotation
guidelines, human subjects often deviate
in their analyses, each bringing different
biases, interpretations of the task and lev-
els of consistency. We present novel tech-
niques for learning from the outputs of
multiple annotators while accounting for
annotator specific behaviour. These tech-
niques use multi-task Gaussian Processes
to learn jointly a series of annotator and
metadata specific models, while explicitly
representing correlations between models
which can be learned directly from data.
Our experiments on two machine trans-
lation quality estimation datasets show
uniform significant accuracy gains from
multi-task learning, and consistently out-
perform strong baselines.
1 Introduction
Most empirical work in Natural Language Pro-
cessing (NLP) is based on supervised machine
learning techniques which rely on human anno-
tated data of some form or another. The annota-
tion process is often time consuming, expensive,
and prone to errors; moreover there is often con-
siderable disagreement amongst annotators.
In general, the predominant perspective to deal
with these data annotation issues in previous work
has been that there is a single underlying ground
truth, and that the annotations collected are noisy
and/or biased samples of this. The challenge is
then one of quality control, in order to process
the data by filtering, averaging or similar to dis-
til the truth. We posit that this perspective is
too limiting, especially with respect to linguis-
tic data, where each individual?s idiolect and lin-
guistic background can give rise to many different
? and yet equally valid ? truths. Particularly in
highly subjective annotation tasks, the differences
between annotators cannot be captured by simple
models such as scaling all instances of a certain
annotator by a factor. They can originate from
a number of nuanced aspects. This is the case,
for example, of annotations on the quality of sen-
tences generated using machine translation (MT)
systems, which are often used to build quality es-
timation models (Blatz et al, 2004; Specia et al,
2009) ? our application of interest.
In addition to annotators? own perceptions and
expectations with respect to translation quality, a
number of factors can affect their judgements on
specific sentences. For example, certain anno-
tators may prefer translations produced by rule-
based systems as these tend to be more grammati-
cal, while others would prefer sentences produced
by statistical systems with more adequate lexical
choices. Likewise, some annotators can be biased
by the complexity of the source sentence: lengthy
sentences are often (subconsciously) assumed to
be of low quality by some annotators. An ex-
treme case is the judgement of quality through
post-editing time: annotators have different typing
speeds, as well as levels of expertise in the task
of post-editing, proficiency levels in the language
pair, and knowledge of the terminology used in
particular sentences. These variations result in
time measurements that are not comparable across
annotators. Thus far, the use of post-editing time
has been done on an per-annotator basis (Specia,
2011), or simply averaged across multiple transla-
tors (Plitt and Masselot, 2010), both strategies far
from ideal.
Overall, these myriad of factors affecting qual-
ity judgements make the modelling of multiple
annotators a very challenging problem. This
problem is exacerbated when annotations are
provided by non-professional annotators, e.g.,
through crowdsourcing ? a common strategy used
32
to make annotation cheaper and faster, however at
the cost of less reliable outcomes.
Most related work on quality assurance for data
annotation has been developed in the context of
crowdsourcing. Common practices include fil-
tering out annotators who substantially deviate
from a gold-standard set or present unexpected
behaviours (Raykar et al, 2010; Raykar and Yu,
2012), or who disagree with others using, e.g., ma-
jority or consensus labelling (Snow et al, 2008;
Sheng et al, 2008). Another relevant strand of
work aims to model legitimate, systematic biases
in annotators (including both non-experts and ex-
perts), such as the fact that some annotators tend
to be more negative than others, and that some
annotators use a wider or narrower range of val-
ues (Flach et al, 2010; Ipeirotis et al, 2010).
However, with a few exceptions in Computer Vi-
sion (e.g., Whitehill et al (2009), Welinder et al
(2010)), existing work disregard metadata and its
impact on labelling.
In this paper we model the task of predicting the
quality of sentence translations using datasets that
have been annotated by several judges with differ-
ent levels of expertise and reliability, containing
translations from a variety of MT systems and on
a range of different types of sentences. We ad-
dress this problem using multi-task learning in
which we learn individual models for each context
(the task, incorporating the annotator and other
metadata: translation system and the source sen-
tence) while also modelling correlations between
tasks such that related tasks can mutually inform
one another. Our use of multi-task learning allows
the modelling of a diversity of truths, while also
recognising that they are rarely independent of one
another (annotators often agree) by explicitly ac-
counting for inter-annotator correlations.
Our approach is based on Gaussian Processes
(GPs) (Rasmussen and Williams, 2006), a ker-
nelised Bayesian non-parametric learning frame-
work. We develop multi-task learning models by
representing intra-task transfer simply and explic-
itly as part of a parameterised kernel function. GPs
are an extremely flexible probabilistic framework
and have been successfully adapted for multi-task
learning in a number of ways, e.g., by learning
multi-task correlations (Bonilla et al, 2008), mod-
elling per-task variance (Groot et al, 2011) or per-
annotator biases (Rogers et al, 2010). Our method
builds on the work of Bonilla et al (2008) by
explicitly modelling intra-task transfer, which is
learned automatically from the data, in order to ro-
bustly handle outlier tasks and task variances. We
show in our experiments on two translation qual-
ity datasets that these multi-task learning strate-
gies are far superior to training individual per-task
models or a single pooled model, and moreover
that our multi-task learning approach can achieve
similar performance to these baselines using only
a fraction of the training data.
In addition to showing empirical performance
gains on quality estimation applications, an im-
portant contribution of this paper is in introduc-
ing Gaussian Processes to the NLP community,1
a technique that has great potential to further per-
formance in a wider range of NLP applications.
Moreover, the algorithms proposed herein can be
adapted to improve future annotation efforts, and
subsequent use of noisy crowd-sourced data.
2 Quality Estimation
Quality estimation (QE) for MT aims at providing
an estimate on the quality of each translated seg-
ment ? typically a sentence ? without access to ref-
erence translations. Work in this area has become
increasingly popular in recent years as a conse-
quence of the widespread use of MT among real-
world users such as professional translators. Ex-
amples of applications of QE include improving
post-editing efficiency by filtering out low qual-
ity segments which would require more effort and
time to correct than translating from scratch (Spe-
cia et al, 2009), selecting high quality segments
to be published as they are, without post-editing
(Soricut and Echihabi, 2010), selecting a trans-
lation from either an MT system or a translation
memory for post-editing (He et al, 2010), select-
ing the best translation from multiple MT sys-
tems (Specia et al, 2010), and highlighting sub-
segments that need revision (Bach et al, 2011).
QE is generally addressed as a machine learn-
ing task using a variety of linear and kernel-based
regression or classification algorithms to induce
models from examples of translations described
through a number of features and annotated for
quality. For an overview of various algorithms and
features we refer the reader to the WMT12 shared
task on QE (Callison-Burch et al, 2012).
While initial work used annotations derived
1We are not strictly the first, Polajnar et al (2011) used
GPs for text classification.
33
from automatic MT evaluation metrics (Blatz et
al., 2004) such as BLEU (Papineni et al, 2002)
at training time, it soon became clear that human
labels result in significantly better models (Quirk,
2004). Current work at sentence level is thus based
on some form of human supervision.
As typical of subjective annotation tasks, QE
datasets should contain multiple annotators to lead
to models that are representative. Therefore, work
in QE faces all common issues regarding variabil-
ity in annotators? judgements. The following are a
few other features that make our datasets particu-
larly interesting:
? In order to minimise annotation costs, trans-
lation instances are often spread among anno-
tators, such that each instance is only labelled
by one or a few judges. In fact, for a sizeable
dataset (thousands of instances), the annota-
tion of a complete dataset by a single judge
may become infeasible.
? It is often desirable to include alternative
translations of source sentences produced by
multiple MT systems, which requires multi-
ple annotators for unbiased judgements, par-
ticularly for labels such as post-editing time
(a translation seen a second time will require
less editing effort).
? For crowd-sourced annotations it is often im-
possible to ensure that the same annotators
will label the same subset of cases.
These features ? which are also typical of many
other linguistic annotation tasks ? make the learn-
ing process extremely challenging. Learning mod-
els from datasets annotated by multiple annotators
remains an open challenge in QE, as we show in
Section 4. In what follows, we present our QE
datasets in more detail.
2.1 Datasets
We use two freely available QE datasets to experi-
ment with the techniques proposed in this paper:2
WMT12: This dataset was distributed as part of
the WMT12 shared task on QE (Callison-Burch et
al., 2012). It contains 1, 832 instances for train-
ing, and 422 for test. The English source sen-
tences are a subset of WMT09-12 test sets. The
Spanish MT outputs were created using a standard
PBSMT Moses engine. Each instance was anno-
tated with post-editing effort scores from highest
2Both datasets can be downloaded from http://www.
dcs.shef.ac.uk/?lucia/resources.html.
effort (score 1) to lowest effort (score 5), where
each score identifies an estimated percentage of
the MT output that needs to be corrected. The
post-editing effort scores were produced indepen-
dently by three professional translators based on
a previously post-edited translation by a fourth
translator. In an attempt to accommodate for sys-
tematic biases among annotators, the final effort
score was computed as the weighted average be-
tween the three PE-effort scores, with more weight
given to the judges with higher standard deviation
from their own mean score. This resulted in scores
spread more evenly in the [1, 5] range.
WPTP12: This dataset was distributed by Ko-
ponen et al (2012). It contains 299 English sen-
tences translated into Spanish using two or more
of eight MT systems randomly selected from all
system submissions for WMT11 (Callison-Burch
et al, 2011). These MT systems range from on-
line and customised SMT systems to commercial
rule-based systems. Translations were post-edited
by humans while time was recorded. The labels
are the number of seconds spent by a translator
editing a sentence normalised by source sentence
length. The post-editing was done by eight na-
tive speakers of Spanish, including five profes-
sional translators and three translation students.
Only 20 translations were edited by all eight an-
notators, with the remaining translations randomly
distributed amongst them. The resulting dataset
contains 1, 624 instances, which were randomly
split into 1, 300 for training and 300 for test. Ac-
cording to the analysis in (Koponen et al, 2012),
while on average certain translators were found to
be faster than others, their speed in post-editing
individual sentences varies considerably, i.e., cer-
tain translators are faster at certain sentences. To
our knowledge, no previous work has managed to
successfully model the prediction of post-editing
time from datasets with multiple annotators.
3 Gaussian Process Regression
Machine learning models for quality estimation
typically treat the problem as regression, seeking
to model the relationship between features of the
text input and the human quality judgement as a
continuous response variable. Popular choices in-
clude Support Vector Machines (SVMs), which
have been shown to perform well for quality es-
timation (Callison-Burch et al, 2012) using non-
linear kernel functions such as radial basis func-
34
tions. In this paper we consider Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2006), a
probabilistic machine learning framework incor-
porating kernels and Bayesian non-parametrics,
widely considered state-of-the-art for regression.
Despite this GPs have not been used widely to date
in statistical NLP. GPs are particularly suitable for
modelling QE for a number of reasons: 1) they
explicitly model uncertainty, which is rife in QE
datasets; 2) they allow fitting of expressive kernels
to data, in order to modulate the effect of features
of varying usefulness; and 3) they can naturally
be extended to model correlated tasks using multi-
task kernels. We now give a brief overview of GPs,
following Rasmussen and Williams (2006).
In our regression task3 the data consists of n
pairs D = {(xi, yi)}, where xi ? RF is a F -
dimensional feature vector and yi ? R is the re-
sponse variable. Each instance is a translation and
the feature vector encodes its linguistic features;
the response variable is a numerical quality judge-
ment: post editing time or likert score. As usual,
the modelling challenge is to automatically predict
the value of y based on the x for unseen test input.
GP regression assumes the presence of a la-
tent function, f : RF ? R, which maps from
the input space of feature vectors x to a scalar.
Each response value is then generated from the
function evaluated at the corresponding data point,
yi = f(xi) + ?, where ? ? N (0, ?2n) is added
white-noise. Formally f is drawn from a GP prior,
f(x) ? GP
(
0, k(x,x?)
)
,
which is parameterised by a mean (here, 0) and
a covariance kernel function k(x,x?). The ker-
nel function represents the covariance (i.e., sim-
ilarities in the response) between pairs of data
points. Intuitively, points that are in close proxim-
ity should have high covariance compared to those
that are further apart, which constrains f to be a
smoothly varying function of its inputs. This intu-
ition is embodied in the squared exponential ker-
nel (a.k.a. radial basis function or Gaussian),
k(x,x?) = ?2f exp
(
?12(x? x
?)TA?1(x? x?)
)
(1)
where ?2f is a scaling factor describing the overall
levels of variance, and A = diag(a) is a diagonal
3Our approach generalises to classification, ranking (ordi-
nal regression) or various other training objectives, including
mixtures of objectives. In this paper we use regression for
simplicity of exposition and implementation.
matrix of length scales, encoding the smoothness
of functions f with respect to each feature. Non-
uniform length scales allow for different degrees
of smoothness of f in each dimension, such that
e.g., for unimportant features f is relatively flat
whereas for very important features f is jagged,
such that a small change in the feature value has
a large effect. When the values of a are learned
automatically from data, as we do herein, this is
referred to as the automatic relevance determina-
tion (ARD) kernel.
Given the generative process defined above, we
formulate prediction as Bayesian inference under
the posterior, namely
p(y?|x?,D) =
?
f
p(y?|x?, f)p(f |D)
where x? is a test input and y? is its response
value. The posterior p(f |D) reflects our updated
belief over possible functions after observing the
training set D, i.e., f should pass close to the re-
sponse values for each training instance (but need
not fit exactly due to additive noise). This is bal-
anced against the smoothness constraints that arise
from the GP prior. The predictive posterior can be
solved analytically, resulting in
y? ? N
(
kT? (K + ?2nI)?1y, (2)
k(x?,x?)? kT? (K + ?2nI)?1k?
)
where k? = [k(x?,x1) k(x?,x2) ? ? ? k(x?,xn)]T
are the kernel evaluations between the test point
and the training set, and {Kij = k(xi,xj)} is
the kernel (gram) matrix over the training points.
Note that the posterior in Eq. 2 includes not only
the expected response (the mean) but also the vari-
ance, encoding the model?s uncertainty, which is
important for integration into subsequent process-
ing, e.g., as part of a larger probabilistic model.
GP regression also permits an analytic for-
mulation of the marginal likelihood, p(y|X) =?
f p(y|X, f)p(f), which can be used for modeltraining (X are the training inputs). Specifically,
we can derive the gradient of the (log) marginal
likelihood with respect to the model hyperparam-
eters (i.e., a, ?n, ?s etc.) and thereby find the type
II maximum likelihood estimate using gradient as-
cent. Note that in general the marginal likelihood
is non-convex in the hyperparameter values, and
consequently the solutions may only be locally op-
timal. Here we bootstrap the learning of complex
models with many hyperparameters by initialising
35
with the (good) solutions found for simpler mod-
els, thereby avoiding poor local optima. We refer
the reader to Rasmussen and Williams (2006) for
further details.
At first glance GPs resemble SVMs, which also
admit kernels such as the popular squared expo-
nential kernel in Eq. 1. The key differences are
that GPs are probabilistic models and support ex-
act Bayesian inference in the case of regression
(approximate inference is required for classifica-
tion (Rasmussen and Williams, 2006)). Moreover
GPs provide greater flexibility in fitting the ker-
nel hyperparameters even for complex composite
kernels. In typical usage, the kernel hyperparam-
eters for an SVM are fit using held-out estima-
tion, which is inefficient and often involves ty-
ing together parameters to limit the search com-
plexity (e.g., using a single scale parameter in
the squared exponential). Multiple-kernel learning
(Go?nen and Alpayd?n, 2011) goes some way to ad-
dressing this problem within the SVM framework,
however this technique is limited to reweighting
linear combinations of kernels and has high com-
putational complexity.
3.1 Multi-task Gaussian Process Models
Until now we have considered a standard regres-
sion scenario, where each training point is labelled
with a single output variable. In order to model
multiple different annotators jointly, i.e., multi-
task learning, we need to extend the model to han-
dle many tasks. Conceptually, we can consider
the multi-task model drawing a latent function for
each task, fm(x), where m ? 1, ...,M is the task
identifier. This function is then used to explain
the response values for all the instances for that
task (subject to noise). Importantly, for multi-task
learning to be of benefit, the prior over {fm} must
correlate the functions over different tasks, e.g., by
imposing similarity constraints between the values
for fm(x) and fm?(x).
We can consider two alternative perspectives
for framing the multi-task learning problem: ei-
ther isotopic where we associate each input point
x with a vector of outputs, y ? RM , one for
each of the M tasks; or heterotopic where some
of the outputs are missing, i.e., tasks are not con-
strained to share the same data points (Alvarez et
al., 2011). Given the nature of our datasets, we
opted for the heterotopic approach, which can han-
dle both singly annotated and multiply annotated
data. This can be implemented by augmenting
each input point with an additional task identity
feature, which is paired with a single y response,
and integrated into a GP model with the standard
training and inference algorithms.4
In moving to a task-augmented data representa-
tion, we need to revise our kernel function. We use
a separable multi-task kernel (Bonilla et al, 2008;
Alvarez et al, 2011) of the form
k
(
(x, d), (x?, d?)
)
= kdata(x,x?)Bd,d? , (3)
where kdata(x,x?) is a standard kernel over the in-
put points, typically a squared exponential (see
Eq. 1), and B ? RD?D is a positive semi-definite
matrix encoding task covariances. We develop
a series of increasingly complex choices for B,
which we compare empirically in Section 4.2:
Independent The simplest case is whereB = I ,
i.e., all pairs of different tasks have zero covari-
ance. This corresponds to independent modelling
of each task, although all models share the same
data kernel, so this setting is not strictly equiva-
lent to independent training with independent per-
task data kernels (with different hyperparameters).
Similarly, we might choose to use a single noise
variance, ?2n, or an independent noise variance hy-
perparameter per task.
Pooled Another extreme is B = 1, which ig-
nores the task identity, corresponding to pooling
the multi-task data into one large set. Groot et
al. (2011) present a method for applying GPs for
modelling multi-annotator data using this pool-
ing kernel with independent per-task noise terms.
They show on synthetic data experiments that this
approach works well at extracting the signal from
noise-corrupted inputs.
Combined A simple approach for B is a
weighted combination of Independent and Pool,
i.e., B = 1+ aI , where the hyperparameter a ? 0
controls the amount of inter-task transfer between
each task and the global ?pooled? task.5 For dis-
similar tasks, a high value of a allows each task to
be modelled independently, while for more simi-
lar tasks low a allows the use of a large pool of
4Note that the separable kernel (Eq. 3) gives rise to block
structured kernel matrices which permit various optimisa-
tions (Bonilla et al, 2008) to reduce the computational com-
plexity of inference, e.g., the matrix inversion in Eq. 2.
5Note that larger values of a need not affect the overall
magnitude of k, which can be down-scaled by the ?2f factorin the data kernel (Eq. 1).
36
similar data. A scaled version of this kernel has
been shown to correspond to mean regularisation
in SVMs when combined with a linear data ker-
nel (Evgeniou et al, 2006). A similar multi-task
kernel was proposed by Daume? III (2007), using
a linear data kernel and a = 1, which has shown
to result in excellent performance across a range
of NLP problems. In contrast to these earlier ap-
proaches, we learn the hyperparameter a directly,
fitting the relative amounts of inter- versus intra-
task transfer to the dataset.
Combined+ We consider an extension to the
Combined kernel, B = 1 + diag(a), ad ? 0
in which each task has a different hyperparameter
modulating its independence from the global pool.
This additional flexibility can be used, e.g., to al-
low individual outlier annotators to be modelled
independently of the others, by assigning a high
value to ad. In contrast, Combined ties together
the parameters for all tasks, i.e., all annotators are
assumed to have similar quality in that they devi-
ate from the mean to the same degree.
3.2 Integrating metadata
The approaches above assume that the data is split
into an unstructured set of M tasks, e.g., by anno-
tator. However, it is often the case that we have
additional information about each data instance in
the form of metadata. In our quality estimation
experiments we consider as metadata the MT sys-
tem which produced the translation, and the iden-
tity of the source sentence being translated. Many
other types of metadata, such as the level of expe-
rience of the annotator, could also be used. One
way of integrating such metadata would be to de-
fine a separate task for every observed combina-
tion of metadata values, in which case we treat the
metadata as a task descriptor. Doing so naively
would however incur a significant penalty, as each
task will have very few training instances result-
ing in inaccurate models, even with the inter-task
kernel approaches defined above.
We instead extend the task-level kernels to use
the task descriptors directly to represent task cor-
relations. Let B(i) be a square covariance matrix
for the ith task descriptor ofM , with a column and
row for each value (e.g., annotator identity, trans-
lation system, etc.). We redefine the task level ker-
nel using paired inputs (x,m), where m are the
task descriptors,
k
(
(x,m), (x?,m?)
)
= kdata(x,x?)
M?
i=1
B(i)mi,m?i .
This is equivalent to using a structured task-kernel
B = B(1) ? B(3) ? ? ? ? ? B(M) where ? is the
Kronecker product. Using this formulation we can
consider any of the above choices for B applied
to each task descriptor. In our experiments we
consider the Combined and Combined+ kernels,
which allow the model to learn the relative impor-
tance of each descriptor in terms of independent
modelling versus pooling the data.
4 Multi-task Quality Estimation
4.1 Experimental Setup
Feature sets: In all experiments we use 17 shal-
low QE features that have been shown to perform
well in previous work. These were used by a
highly competitive baseline entry in the WMT12
shared task, and were extracted here using the sys-
tem provided by that shared task.6 They include
simple counts, e.g., the tokens in sentences, as
well as source and target language model proba-
bilities. Each feature was scaled to have zero mean
and unit standard deviation on the training set.
Baselines: The baselines use the SVM regres-
sion algorithm with radial basis function kernel
and parameters ?,  and C optimised through grid-
search and 5-fold cross validation on the training
set. This is generally a very strong baseline: in
the WMT12 QE shared task, only five out of 19
submissions were able to significantly outperform
it, and only by including many complex additional
features, tree kernels, etc. We also present ?, a
trivial baseline based on predicting for each test
instance the training mean (overall, and for spe-
cific tasks).
GP: All GP models were implemented using the
GPML Matlab toolbox.7 Hyperparameter optimi-
sation was performed using conjugate gradient as-
cent of the log marginal likelihood function, with
up to 100 iterations. The simpler models were ini-
tialised with all hyperparameters set to one, while
more complex models were initialised using the
6The software used to extract these (and other) fea-
tures can be downloaded from http://www.quest.
dcs.shef.ac.uk/
7http://www.gaussianprocess.org/gpml/
code
37
Model MAE RMSE
? 0.8279 0.9899
SVM 0.6889 0.8201
Linear ARD 0.7063 0.8480
Squared exp. Isotropic 0.6813 0.8146
Squared exp. ARD 0.6680 0.8098
Rational quadratic ARD 0.6773 0.8238
Matern(5,2) 0.6772 0.8124
Neural network 0.6727 0.8103
Table 1: Single-task learning results on the
WMT12 dataset, trained and evaluated against
the weighted averaged response variable. ? is a
baseline which predicts the training mean, SVM
uses the same system as the WMT12 QE task, and
the remainder are GP regression models with dif-
ferent kernels (all include additive noise).
solution for a simpler model. For instance, mod-
els using ARD kernels were initialised from an
equivalent isotropic kernel (which ties all the hy-
perparameters together), and independent per-task
noise models were initialised from a single noise
model. This approach was more reliable than ran-
dom restarts in terms of accuracy and runtime ef-
ficiency.
Evaluation: We evaluate predictive accuracy
using two measures: mean absolute error,
MAE = 1N
?N
i=1 |yi ? y?i| and root mean square
error, RMSE =
?
1
N
?N
i=1 (yi ? y?i)2, where yi
are the gold standard response values and y?i are
the model predictions.
4.2 Results
Our experiments aim to demonstrate the efficacy
of GP regression, both the single task and multi-
task settings, compared to competitive baselines.
WMT12: Single task We start by comparing
GP regression with alternative approaches using
the WMT12 dataset on the standard task of pre-
dicting a weighted mean quality rating (as it was
done in the WMT12 QE shared task). Table 1
shows the results for baseline approaches and the
GP models, using a variety of different kernels
(see Rasmussen and Williams (2006) for details of
the kernel functions). From this we can see that all
models do much better than the mean baseline and
that most of the GP models have lower error than
the state-of-the-art SVM. In terms of kernels, the
linear kernel performs comparatively worse than
non-linear kernels. Overall the squared exponen-
Model MAE RMSE
? 0.8541 1.0119
Independent SVMs 0.7967 0.9673
EasyAdapt SVM 0.7655 0.9105
Independent 0.7061 0.8534
Pooled 0.7252 0.8754
Pooled & {N} 0.7050 0.8497
Combined 0.6966 0.8448
Combined & {N} 0.6975 0.8476
Combined+ 0.6975 0.8463
Combined+ & {N} 0.7046 0.8595
Table 2: Results on the WMT12 dataset, trained
and evaluated over all three annotator?s judge-
ments. Shown above are the training mean base-
line ?, single-task learning approaches, and multi-
task learning models, with the columns showing
macro average error rates over all three response
values. All systems use a squared exponential
ARD kernel in a product with the named task-
kernel, and with added noise (per-task noise is de-
noted {N}, otherwise has shared noise).
tial ARD kernel has the best performance under
both measures of error, and for this reason we use
this kernel in our subsequent experiments.
WMT12: Multi-task We now turn to the multi-
task setting, where we seek to model each of the
three annotators? predictions. Table 2 presents
the results. Note that here error rates are mea-
sured over all of the three annotators? judgements,
and consequently are higher than those measured
against their average response in Table 1. For com-
parison, taking the predictions of the best model,
Combined, in Table 2 and evaluating its averaged
prediction has a MAE of 0.6588 vs. the averaged
gold standard, significantly outperforming the best
model in Table 1.
There are a number of important findings in Ta-
ble 2. First, the independently trained models do
well, outperforming the pooled model with fixed
noise, indicating that naively pooling the data is
counter-productive and that there are annotator-
specific biases. Including per-annotator noise to
the pooled model provides a boost in performance,
however the best results are obtained using the
Combined kernel which brings the strengths of
both the independent and pooled settings. There
are only minor differences between the different
multi-task kernels, and in this case per-annotator
noise made little difference. An explanation for
the contradictory findings about the importance
38
of independent noise is that differences between
annotators can already be explained by the MTL
model using the multi-task kernel, and need not be
explained as noise.
The GP models significantly improve over
the baselines, including an SVM trained inde-
pendently and using the EasyAdapt method for
multi-task learning (Daume? III, 2007). While
EasyAdapt showed an improvement over the in-
dependent SVM, it was a long way short of the
GP models. A possible explanation is that in
EasyAdapt the multi-task sharing parameter is set
at a = 1, which may not be appropriate for the
task. In contrast the Combined GP model learned
a value of a = 0.01, weighting the value of pool-
ing much more highly than independent training.
A remaining question is how these approaches
cope with smaller datasets, where issues of data
sparsity become more prevalent. To test this, we
trained single-task, pooled and multi-task models
on randomly sub-sampled training sets of differ-
ent sizes, and plot their error rates in Figure 1.
As expected, for very small datasets pooling out-
performs single task learning, however for modest
sized datasets of ? 90 training instances pooling
was inferior. For all dataset sizes multi-task learn-
ing is superior to the other approaches, making
much better use of small and large training sets.
The MTL model trained on 500 samples had an
MAE of 0.7082? 0.0042, close to the best results
from the full dataset in Table 2, despite using 19as much data: here we use 13 as many traininginstances where each is singly (cf. triply) anno-
tated. The same experiments run with multiply-
annotated instances showed much weaker perfor-
mance, presumably due to the more limited sam-
ple of input points and poorer fit of the ARD ker-
nel hyperparameters. This finding suggests that
our multi-task learning approach could be used to
streamline annotation efforts by reducing the need
for extensive multiple annotations.
WPTP12 This dataset involves predicting the
post-editing time for eight annotators, where we
seek to test our model?s capability to use addi-
tional metadata. We model the logarithm of the
per-word post-editing time, in order to make the
response variable more comparable between an-
notators and across sentences, and generally more
Gaussian in shape. In Table 3 immediately we
can see that the baseline of predicting the train-
ing mean is very difficult to beat, and the trained
50 100 150 200 250 300 350 400 450 5000.7
0.72
0.74
0.76
0.78
0.8
0.82
Training examples 
 
STLMTLPooled
Figure 1: Learning curve comparing MAE for dif-
ferent training methods on the WMT12 dataset,
all using a squared exponential ARD data kernel
and tied noise parameter. The MTL model uses the
Combined task kernel. Each point is the average
of 5 runs, and the error bars show ?1 s.d.
systems often do worse. Partitioning the data
by annotator (?A) gives the best baseline result,
while there is less information from the MT sys-
tem or sentence identity. Single-task learning per-
forms only a little better than these baselines, al-
though some approaches such as the naive pool-
ing perform terribly. This suggests that the tasks
are highly different to one another. Interestingly,
adding the per-task noise models to the pooling ap-
proach greatly improves its performance.
The multi-task learning methods performed best
when using the annotator identity as the task de-
scriptor, and less well for the MT system and sen-
tence pair, where they only slightly improved over
the baseline. However, making use of all these lay-
ers of metadata together gives substantial further
improvements, reaching the best result with Com-
binedA,S,T . The effect of adding per-task noise to
these models was less marked than for the pooled
models, as in the WMT12 experiments. Inspecting
the learned hyperparameters, the combined mod-
els learned a large bias towards independent learn-
ing over pooling, in contrast to the WMT12 exper-
iments. This may explain the poor performance of
EasyAdapt on this dataset.
5 Conclusion
This paper presented a novel approach for learning
from human linguistic annotations by explicitly
training models of individual annotators (and pos-
sibly additional metadata) using multi-task learn-
ing. Our method using Gaussian Processes is flex-
ible, allowing easy learning of inter-dependences
between different annotators and other task meta-
39
Model MAE RMSE
? 0.5596 0.7053
?A 0.5184 0.6367
?S 0.5888 0.7588
?T 0.6300 0.8270
Pooled SVM 0.5823 0.7472
IndependentA SVM 0.5058 0.6351
EasyAdapt SVM 0.7027 0.8816
SINGLE-TASK LEARNING
IndependentA 0.5091 0.6362
IndependentS 0.5980 0.7729
Pooled 0.5834 0.7494
Pooled & {N} 0.4932 0.6275
MULTI-TASK LEARNING: Annotator
CombinedA 0.4815 0.6174
CombinedA & {N} 0.4909 0.6268
Combined+A 0.4855 0.6203
Combined+A & {N} 0.4833 0.6102
MULTI-TASK LEARNING: Translation system
CombinedS 0.5825 0.7482
MULTI-TASK LEARNING: Sentence pair
CombinedT 0.5813 0.7410
MULTI-TASK LEARNING: Combinations
CombinedA,S 0.4988 0.6490
CombinedA,S & {NA,S} 0.4707 0.6003
Combined+A,S 0.4772 0.6094
CombinedA,S,T 0.4588 0.5852
CombinedA,S,T & {NA,S} 0.4723 0.6023
Table 3: Results on the WPTP12 dataset, using
the log of the post-editing time per word as the
response variable. Shown above are the training
mean and SVM baselines, single-task learning and
multi-task learning results (micro average). The
subscripts denote the task split: annotator (A), MT
system (S) and sentence identity (T).
data. Our experiments showed how our approach
outperformed competitive baselines on two ma-
chine translation quality regression problems, in-
cluding the highly challenging problem of predict-
ing post-editing time.
In future work we plan to apply these techniques
to new datasets, particularly noisy crowd-sourced
data with much large numbers of annotators, as
well as a wider range of task types and mixtures
thereof (regression, ordinal regression, ranking,
classification). We also have preliminary positive
results for more advanced multi-task kernels, e.g.,
general dense matrices, which can induce clusters
of related tasks.
Our multi-task learning approach has much
wider application. Models of individual annota-
tors could be used to train machine translation
systems to optimise an annotator-specific quality
measure, or in active learning for corpus annota-
tion, where the model can suggest the most ap-
propriate instances for each annotator or the best
annotator for a given instance. Further, our ap-
proach contributes to work based on cheap and fast
crowdsourcing of linguistic annotation by min-
imising the need for careful data curation and
quality control.
Acknowledgements
This work was funded by PASCAL2 Harvest Pro-
gramme, as part of the QuEst project: http:
//www.quest.dcs.shef.ac.uk/. The au-
thors would like to thank Neil Lawerence and
James Hensman for advice on Gaussian Processes,
the QuEst participants, particularly Jose? Guil-
herme Camargo de Souza and Eva Hassler, and the
three anonymous reviewers.
References
Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2011. Kernels for vector-valued func-
tions: A review. Foundations and Trends in Machine
Learning, 4(3):195?266.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: a method for measuring machine
translation confidence. In the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 211?
219, Portland, Oregon.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
40
Sanchis, and Nicola Ueffing. 2004. Confidence Es-
timation for Machine Translation. In the 20th Inter-
national Conference on Computational Linguistics
(Coling 2004), pages 315?321, Geneva.
Edwin Bonilla, Kian Ming Chai, and Christopher
Williams. 2008. Multi-task gaussian process pre-
diction. In Advances in Neural Information Process-
ing Systems (NIPS).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In the Sixth
Workshop on Statistical Machine Translation, pages
22?64, Edinburgh, Scotland.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In the Seventh Workshop
on Statistical Machine Translation, pages 10?51,
Montre?al, Canada.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, Prague, Czech
Republic.
Theodoros Evgeniou, Charles A. Micchelli, and Massi-
miliano Pontil. 2006. Learning multiple tasks with
kernel methods. Journal of Machine Learning Re-
search, 6(1):615.
Peter A. Flach, Sebastian Spiegler, Bruno Gole?nia, Si-
mon Price, John Guiver, Ralf Herbrich, Thore Grae-
pel, and Mohammed J. Zaki. 2010. Novel tools
to streamline the conference review process: experi-
ences from SIGKDD?09. SIGKDD Explor. Newsl.,
11(2):63?67, May.
Mehmet Go?nen and Ethem Alpayd?n. 2011. Multi-
ple kernel learning algorithms. Journal of Machine
Learning Research, 12:2211?2268.
Perry Groot, Adriana Birlutiu, and Tom Heskes. 2011.
Learning from multiple annotators with gaussian
processes. In Proceedings of the 21st international
conference on Artificial neural networks - Volume
Part II, ICANN?11, pages 159?164, Espoo, Finland.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with transla-
tion recommendation. In the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on amazon mechanical
turk. In Proceedings of the ACM SIGKDD Work-
shop on Human Computation, HCOMP ?10, pages
64?67, Washington DC.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing time as a mea-
sure of cognitive effort. In Proceedings of the
AMTA 2012 Workshop on Post-editing Technology
and Practice, WPTP 2012, San Diego, CA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsyl-
vania.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. Prague Bull. Math.
Linguistics, 93:7?16.
Tamara Polajnar, Simon Rogers, and Mark Girolami.
2011. Protein interaction detection in sentences via
gaussian processes; a preliminary evaluation. Int. J.
Data Min. Bioinformatics, 5(1):52?72, February.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence metric. In Proceed-
ings of the International Conference on Language
Resources and Evaluation, volume 4 of LREC 2004,
pages 825?828, Lisbon, Portugal.
Carl E. Rasmussen and Christopher K.I. Williams.
2006. Gaussian processes for machine learning,
volume 1. MIT press Cambridge, MA.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating
spammers and ranking annotators for crowdsourced
labeling tasks. J. Mach. Learn. Res., 13:491?518.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
J. Mach. Learn. Res., 99:1297?1322.
Simon Rogers, Mark Girolami, and Tamara Polajnar.
2010. Semi-parametric analysis of multi-rater data.
Statistics and Computing, 20(3):317?334.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? Improving data
quality and data mining using multiple, noisy la-
belers. In Proceedings of the 14th ACM SIGKDD,
KDD?08, pages 614?622, Las Vegas, Nevada.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
254?263, Honolulu, Hawaii.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 612?621, Uppsala, Swe-
den, July.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the Sentence-Level Quality of Machine Trans-
lation Systems. In the 13th Annual Meeting of
the European Association for Machine Translation
(EAMT?2009), pages 28?37, Barcelona.
41
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine translation evaluation versus quality esti-
mation. Machine Translation, pages 39?50.
Lucia Specia. 2011. Exploiting Objective Annotations
for Measuring Translation Post-editing Effort. In the
15th Annual Meeting of the European Association
for Machine Translation (EAMT?2011), pages 73?
80, Leuven.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The Multidimensional Wis-
dom of Crowds. In Advances in Neural Information
Processing Systems, volume 23, pages 2424?2432.
Jacob Whitehill, Paul Ruvolo, Ting-fan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. Advances in
Neural Information Processing Systems, 22:2035?
2043.
42
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 333?342,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Markov Model of Machine Translation using
Non-parametric Bayesian Inference
Yang Feng and Trevor Cohn
Department of Computer Science
The University of Sheffield
Sheffield, United Kingdom
yangfeng145@gmail.com and t.cohn@sheffield.ac.uk
Abstract
Most modern machine translation systems
use phrase pairs as translation units, al-
lowing for accurate modelling of phrase-
internal translation and reordering. How-
ever phrase-based approaches are much
less able to model sentence level effects
between different phrase-pairs. We pro-
pose a new model to address this im-
balance, based on a word-based Markov
model of translation which generates tar-
get translations left-to-right. Our model
encodes word and phrase level phenom-
ena by conditioning translation decisions
on previous decisions and uses a hierar-
chical Pitman-Yor Process prior to pro-
vide dynamic adaptive smoothing. This
mechanism implicitly supports not only
traditional phrase pairs, but also gapping
phrases which are non-consecutive in the
source. Our experiments on Chinese to
English and Arabic to English translation
show consistent improvements over com-
petitive baselines, of up to +3.4 BLEU.
1 Introduction
Recent years have witnessed burgeoning develop-
ment of statistical machine translation research,
notably phrase-based (Koehn et al, 2003) and
syntax-based approaches (Chiang, 2005; Galley
et al, 2006; Liu et al, 2006). These approaches
model sentence translation as a sequence of sim-
ple translation decisions, such as the application
of a phrase translation in phrase-based methods
or a grammar rule in syntax-based approaches.
In order to simplify modelling, most MT mod-
els make an independence assumption, stating that
the translation decisions in a derivation are in-
dependent of one another. This conflicts with
the intuition behind phrase-based MT, namely that
translation decisions should be dependent on con-
text. On one hand, the use of phrases can mem-
orize local context and hence helps to generate
better translation compared to word-based models
(Brown et al, 1993; Och and Ney, 2003). On the
other hand, this mechanism requires each phrase
to be matched strictly and to be used as a whole,
which precludes the use of discontinuous phrases
and leads to poor generalisation to unseen data
(where large phrases tend not to match).
In this paper we propose a new model to drop
the independence assumption, by instead mod-
elling correlations between translation decisions,
which we use to induce translation derivations
from aligned sentences (akin to word alignment).
We develop a Markov model over translation de-
cisions, in which each decision is conditioned on
previous n most recent decisions. Our approach
employs a sophisticated Bayesian non-parametric
prior, namely the hierarchical Pitman-Yor Process
(Teh, 2006; Teh et al, 2006) to represent back-
off from larger to smaller contexts. As a result,
we need only use very simple translation units
? primarily single words, but can still describe
complex multi-word units through correlations be-
tween their component translation decisions. We
further decompose the process of generating each
target word into component factors: finishing the
translating, jumping elsewhere in the source, emit-
ting a target word and deciding the fertility of the
source words.
Overall our model has the following features:
1. enabling model parameters to be shared be-
tween similar translation decisions, thereby
obtaining more reliable statistics and gener-
alizing better from small training sets.
2. learning a much richer set of transla-
tion fragments, such as gapping phrases,
e.g., the translation for the German werde
. . . ankommen in English is will arrive . . . .
3. providing a unifying framework spanning
word-based and phrase-based model of trans-
lation, while incorporating explicit transla-
333
tion, insertion, deletion and reordering com-
ponents.
We demonstrate our model on Chinese-English
and Arabic-English translation datasets. The
model produces uniformly better translations than
those of a competitive phrase-based baseline,
amounting to an improvement of up to 3.4 BLEU
points absolute.
2 Related Work
Word based models have a long history in machine
translation, starting with the venerable IBM trans-
lation models (Brown et al, 1993) and the hid-
den Markov model (Vogel et al, 1996). These
models are still in wide-spread use today, albeit
only as a preprocessing step for inferring word
level alignments from sentence-aligned parallel
corpora. They combine a number of factors, in-
cluding distortion and fertility, which have been
shown to improve word-alignment and translation
performance over simpler models. Our approach
is similar to these works, as we also develop a
word-based model, and explicitly consider simi-
lar translation decisions, alignment jumps and fer-
tility. We extend these works in two important
respects: 1) while they assume a simple parame-
terisation by making iid assumptions about each
translation factor, we instead allow for rich cor-
relations by modelling sequences of translation
decisions; and 2) we develop our model in the
Bayesian framework, using a hierarchical Pitman-
Yor Process prior with rich backoff semantics be-
tween high and lower order sequences of transla-
tion decisions. Together this results in a model
with rich expressiveness but can still generalize
well to unseen data.
More recently, a number of authors have pro-
posed Markov models for machine translation.
Vaswani et al (2011) propose a rule Markov
model for a tree-to-string model which models
correlations between pairs of mininal rules, and
use Kneser-Ney smoothing to alleviate the prob-
lems of data sparsity. Similarly, Crego et al
(2011) develop a bilingual language model which
incorporates words in the source and target lan-
guages to predict the next unit, which they use as
a feature in a translation system. This line of work
was extended by Le et al (2012) who develop a
novel estimation algorithm based around discrimi-
native projection into continuous spaces. Also rel-
evant is Durrani et al (2011), who present a se-
quence model of translation including reordering.
Our work also uses bilingual information, using
the source words as part of the conditioning con-
text. In contrast to these approaches which pri-
marily address the decoding problem, we focus on
the learning problem of inferring alignments from
parallel sentences. Additionally, we develop a full
generative model using a Bayesian prior, and in-
corporate additional factors besides lexical items,
namely jumps in the source and word fertility.
Another aspect of this paper is the implicit sup-
port for phrase-pairs that are discontinous in the
source language. This idea has been developed
explicitly in a number of previous approaches, in
grammar based (Chiang, 2005) and phrase-based
systems (Galley and Manning, 2010). The latter is
most similar to this paper, and shows that discon-
tinuous phrases compliment standard contiguous
phrases, improving expressiveness and translation
performance. Unlike their work, here we develop
a complimentary approach by constructing a gen-
erative model which can induce these rich rules
directly from sentence-aligned corpora.
3 Model
Given a source sentence, our model infers a la-
tent derivation which produces a target translation
and meanwhile gives a word alignment between
the source and the target. We consider a pro-
cess in which the target string is generated using
a left-to-right order, similar to the decoding strat-
egy used by phrase-based machine translation sys-
tems (Koehn et al, 2003). During this process we
maintain a position in the source sentence, which
can jump around to allow for different sentence
ordering in the target vs. source languages. In
contrast to phrase-based models, we use words as
our basic translation unit, rather than multi-word
phrases. Furthermore, we decompose the deci-
sions involved in generating each target word to
a number of separate factors, where each factor is
modelled separately and conditioned on a rich his-
tory of recent translation decisions.
3.1 Markov Translation
Our model generates target translation left-to-
right word by word. The generative process
employs the following recursive procedure to
construct the target sentence conditioned on the
source:
i? 1
while Not finished do
Decide whether to finish the translation, ?i
334
Step Source sentence Translation finish jump emission
0 Je le prends
1 Je le prends I no monotone Je? I
2 Je le prends I ?ll no insert null? ?ll
3 Je le prends I ?ll take no forward prends? take
4 Je le prends I ?ll take that no backward le? that
5 Je le prends I ?ll take that one no stay le? one
6 Je le prends I ?ll take that one yes
Figure 1: Translation agenda of Je le prends? I ?ll take that one.
if ?i = false then
Select a source word to jump to
Emit a target word for the source word
end if
i? i+ 1
end while
In the generation of each target word, our model
includes three separate factors: the binary finish
decision, a jump decision to move to a different
source word, and emission which translates or oth-
erwise inserts a word in the target string. This gen-
erative process resembles the sequence of transla-
tion decisions considered by a standard MT de-
coder (Koehn et al, 2003), but note that our ap-
proach differs in that there is no constraint that all
words are translated exactly once. Instead source
words can be skipped or repeatedly translated.
This makes the approach more suitable for learn-
ing alignments, e.g., to account for word fertilities
(see ?3.3), while also permitting inference using
Gibbs sampling (?4).
More formally, we can express our probabilistic
model as
pbs(eI1, aI1|fJ1 ) =
I+1?
i=1
p(?i|f i?1ai?n, e
i?1
i?n)
?
I?
i=1
p(?i|f i?1ai?n, e
i?1
i?n)
?
I?
i=1
p(ei|?i, f iai?n, ei?1i?n) (1)
where ?i is the finish decision for target posi-
tion i, ?i is the jump decision to source word fai
and f iai?n is the source words for target positions
i ? n, i ? n + 1, ..., i. Each of the three distribu-
tions (finish, jump and emission) is drawn respec-
tive from hierarchical Pitman-Yor Process priors,
as described in Section 3.2.
The jump decision ?i in Equation 1 demands
further explanation. Instead of modelling jump
distances explicitly, which poses problems for
generalizing between different lengths of sen-
tences and general parameter explosion, we con-
sider a small handful of types of jump based on
the distance between the current source word ai
and the previous source word ai?1, i.e., di =
ai ? ai?1.1 We bin jumps into five types:
a) insert;
b) backward, if di < 0;
c) stay, if di = 0;
d) monotone, if di = 1;
e) forward, if di > 1.
The special jump type insert handles null align-
ments, denoted ai = 0 which licence spurious in-
sertions in the target string.
To illustrate this translation process, Figure 1
shows the example translation <Je le prends, I ?ll
take that one>. Initially we set the source position
before the first source word Je. Then in step 1,
we decide not to finish (finish=no), jump to source
word Je and translate it as I. Next, we again de-
cide not to finish, jump to the null source word
and insert ?ll. The process continues until in step
6 we elect to finish (finish=yes), at which point the
translation is complete, with target string I ?ll take
that one.
3.2 Hierarchical Pitman-Yor Process
The Markov assumption limits the context of each
distribution to the n most recent translation deci-
sions, which limits the number of model param-
eters. However for any non-trivial value n >
0, overfitting is a serious concern. We counter
the problem of a large parameter space using a
Bayesian non-parametric prior, namely the hier-
archical Pitman-Yor Process (PYP). The PYP de-
scribes distributions over possibly infinite event
spaces that follow a power law, with few events
taking the majority of the probability mass and a
long tail of less frequent events. We consider a hi-
erarchical PYP, where a sequence of chained PYP
1For a target position aligned to null, we denote its source
word as null and set its aligned source position as that of the
previous target word that is aligned to non-null.
335
priors allow backoff from larger to smaller con-
texts such that our model can learn rich contextual
models for known (large) contexts while also still
being able to generalize well to unseen contexts
(using smaller histories).
3.2.1 Pitman-Yor Process
A PYP (Pitman and Yor, 1997) is defined by its
discount parameter 0 ? a < 1, strength parameter
b > ?a and base distribution G0. For a distri-
bution drawn from a PYP, G ? PYP(a, b,G0),
marginalising out G leads to a simple distribution
which can be described using a variant of the Chi-
nese Restaurant Process (CRP). In this analogy we
imagine a restaurant has an infinite number of ta-
bles and each table can accommodate an infinite
number of customers. Each customer (a sample
from G) walks in one at a time and seats them-
selves at a table. Finally each table is served a
communal dish (a draw from G0), which is served
to each customer seated at the table. The assign-
ment of customers to tables is such that popular
tables are more likely to be chosen, and this rich-
get-richer dynamic produces power-law distribu-
tions with few events (the dishes at popular tables)
dominating the distribution.
More formally, at time n a customer enters and
selects a table k which is either a table having been
seated (1 ? k ? K?) or an empty table (k =
K? + 1) by
p(tn = k|t?n) =
{
c?tk?a
n?1+b 1 ? k ? K?
aK?+b
n?1+b k = K? + 1
where tn is the table selected by the customer n,
t?n is the seating arrangement of previous n ? 1
customers, c?tk is the number of customers seatedat table k in t?n andK? = K(t?n) is the number
of tables in t?n.
If the customer sits at an empty table, a dish h
is served to his table by the probability of G0(h),
otherwise, he can only share with others the dish
having been served to his table.2 Overall, the prob-
ability of the customer being served a dish h is
p(on = h|t?n,o?n) =
c?oh ? aK?h
n? 1 + b
+ (aK
? + b)
n? 1 + b G0(h)
where on is the dish served to the customer n, o?n
is the dish accommodation of previous n? 1 cus-
tomers, c?oh is the number of customers who are
2We also say the customer is served with this dish.
served with the dish h in o?n and K?h is the num-ber of tables served with the dish h in t?n.
The hierarchical PYP (hPYP; Teh (2006)) is an
extension of the PYP in which the base distribu-
tion G0 is itself a PYP distribution. This parent
(base) distribution can itself have a PYP as a base
distribution, giving rise to hierarchies of arbitrary
depth. Like the PYP, inference under the hPYP
can be also described in terms of CRP whereby
each table in one restaurant corresponds to a dish
in the next deeper level, and is said to share the
same dish. Whenever an empty table is seated in
one level, a customer must enter the restaurant in
the next deeper level and find a table to sit. This
process continues until the customer is assigned a
shared table or the deepest level of the hierarchy
is reached. A similar process occurs when a cus-
tomer leaves, where newly emptied tables must be
propagated up the hierarchy in the form of depart-
ing customers. There is not space for a complete
treatment of the hPYP and the particulars of infer-
ence; we refer the interested reader to Teh (2006).
3.2.2 A Hierarchical PYP Translation Model
We draw the distributions for the various transla-
tion factors from respective hierarchical PYP pri-
ors, as shown in Figure 2 for the finish, jump and
emission factors. For the emission factor (Fig-
ure 2c), we draw the target word ei from a distribu-
tion conditioned on the last two source and target
words, as well as the current source word, fai and
the current jump type ?i. Here the draw of a tar-
get word corresponds to a customer entering and
which target word to emit corresponds to which
dish to be served to the customer in the CRP. The
hierarchical prior encodes a backoff path in which
the jump type is dropped first, followed by pairs of
source and target words from least recent to most
recent. The final backoff stages drop the current
source word, terminating with the uniform base
distribution over the target vocabulary V .
The distributions over the other two factors in
Figure 2 follow a similar pattern. Note however
that these distributions don?t condition on the cur-
rent source word, and consequently have fewer
levels of backoff. The terminating base distribu-
tion for the finish factor is a uniform distribution
with equal probability for finishing versus contin-
uing. The jump factor has an additional condition-
ing variable t which encodes whether the previous
alignment is near the start or end of the source sen-
tence. This information affects which of the jump
values are legal from the current position, such
336
?i|f i?1ai?2, ei?1i?2 ? G?f i?1ai?2,ei?1i?2
G?f i?1ai?2,ei?1i?2 ? PYP(a
?3, b?3, G?fai?1,ei?1)
G?fai?1,ei?1 ? PYP(a?2, b?2, G?)
G? ? PYP(a?1, b?1, G?0)
G?0 ? U(12)
(a) Finish factor
?i|f i?1ai?2, ei?1i?2, t ? G?f i?1ai?2,ei?1i?2,t
G?f i?1ai?2,ei?1i?2,t ? PYP(a
?3 , b?3 , G?fai?1,ei?1,t)
G?fai?1,ei?1,t ? PYP(a?2 , b?2 , G?t )
G?t ? PYP(a?1 , b?1 , G?0,t)
G?0,t ? U
(b) Jump factor
ei|?i, f iai?2, ei?1i?2 ?Ge?i,f iai?2,ei?1i?2
Ge?i,f iai?2,ei?1i?2 ? PYP(a
e5, be5, Gef iai?2,ei?1i?2)
Gef iai?2,ei?1i?2 ? PYP(a
e4, be4, Gef iai?1,ei?1)
Gef iai?1,ei?1 ? PYP(a
e3, be3, Gefai)
Gefai ? PYP(ae2, be2, Ge)
Ge ? PYP(ae1, be1, Ge0)
Ge0 ? U( 1|V |)
(c) Emission factor
Figure 2: Distributions over the translation factors and their hierarchical priors.
that a jump could not go outside the bounds of the
source sentence. Accordingly we maintain sepa-
rate distributions for each setting, and each has a
different uniform base distribution parameterized
according to the number of possible jump types.
3.3 Fertility
For each target position, our Markov model may
select a source word which has been covered,
which means a source word may be linked to sev-
eral target positions. Therefore, we introduce fer-
tility to denote the number of target positions a
source word is linked to in a sentence pair. Brown
et al (1993) have demonstrated the usefulness of
fertility in probability estimation: IBM models 3?
5 exhibit large improvements over models 1?2. On
these grounds, we include fertility to produce our
advanced model,
pad(eI1, aI1|fJ1 )=pbs(eI1, aI1|fJ1 )
J?
j=1
p(?j |f jj?n) (2)
where ?j is the fertility of source word fj in the
sentence pair < fJ1 , eI1 > and pbs is the basic
model defined in Eq. 1. In order to avoid prob-
lems of data sparsity, we bin fertility into three
types, a) zero, if ? = 0; b) single, if ? = 1;
and c) multiple, if ? > 1.
We draw the fertility variables from a hierarchi-
cal PYP distribution, using three levels of backoff,
?j |f jj?1 ? G
?
fjj?1
G?fjj?1
? PYP(a?3 , b?3 , G?fj )
G?fj ? PYP(a
?
2 , b
?
2 , G?)
G? ? PYP(a?1 , b?1 , G?0 )
G?0 ? U(
1
3)
where we condition the fertility of each word to-
ken on the token to its left, which we drop during
the first stage of backoff to simple word-based fer-
tility. The last level of backoff further generalises
to a shared fertility across all words. In this way
we gain the benefits of local context on fertility,
while including more general levels to allow wider
applicability.
4 Gibbs Sampling
To train the model, we use Gibbs sampling, a
Markov Chain Monte Carlo (MCMC) technique
for posterior inference. Specifically we seek to
infer the latent sequence of translation decisions
given a corpus of sentence pairs. Given the struc-
ture of our model, a word alignment uniquely
specifies the translation decisions and the se-
quence follows the order of the target sentence left
to right. Our Gibbs sampler operates by sampling
an update to the alignment of each target word
in the corpus. It visits each sentence pair in the
corpus in a random order and resamples the align-
ments for each target position as follows. First we
discard the alignment to the current target word
and decrement the counts of all factors affected
by this alignment in their top level distributions
(which will percolate down to the lower restau-
rants). Next we calculate posterior probabilities
for all possible alignment to this target word based
on the table occupancies in the hPYP. Finally we
draw an alignment and increment the table counts
for the translation decisions affected by the new
alignment.
More specifically, we consider sampling from
Equation 2 with n = 2. When changing the align-
ment to a target word ei from j? to j, the fin-
ish, jump and emission for three target positions
i, i+ 1, i+ 2 and fertility for two source positions
j, j? may be affected. This leads to the following
337
decrement increment
?(no | null, ?ll, Je, I) ?(no | null, ?ll, Je, I)
?(no | p..s, take, null, ?ll) ?(no | Je, take, null, ?ll)
?(no | le, that, p..s, take) ?(no | le, that, Je, take)
?(f | null, ?ll, Je, I) ?(s| null, ?ll, Je, I)
?(b | p..s, take, null, ?ll) ?(m| Je, take, null, ?ll)
?(s | le, that, p..s, take) ?(s| le, that, Je, take)
e(take |f , p..s, null, ?ll, Je, I) e(take |s, Je, null, ?ll, Je, I)
e(that |b, le, p..s, take, null, ?ll) e(that |m, le, Je, take, null, ?ll)
e(one |s, le, le, that, p..s, take) e(one |s, le, le, that, Je, take)
?(single | p..s, le) ?(multiple | Je, <s>)
Table 1: The count update when changing the
aligned source word of take from prends to Je in
Figure 1. Key: f?forward s?stay b?backward m?
monotone p..s?prends.
posterior probability
p(ai = j|t?i,o?i) ?
i+2?
l=i
p(?l)p(?l)p(el)
? p(?j + 1)p(?j? ? 1)p(?j)p(?j?)
(3)
where ?j , ?j? are the fertilities before changing the
link and for brevity we omit the conditioning con-
texts. For example, in Figure 1, we sample for
target word take and change the aligned source
word from prends to Je, then the items for which
we need to decrement and increment the counts by
one are shown in Table 1 and the posterior prob-
ability corresponding to the new alignment is the
product of the hierarchical PYP probabilities of all
increment items divided by the probability of the
fertility of prends being single.
Maintaining the current state of the hPYP as
events are incremented and decremented is non-
trivial and the naive approach requires significant
book-keeping and has poor runtime behaviour. For
this we adopt the approach of Blunsom et al
(2009b), who present a method for maintaining
table counts without needing to record the table
assignments for each translation decision. Briefly,
this algorithm samples the table assignment during
the increment and decrement operations, which is
then used to maintain aggregate table statistics.
This can be done efficiently and without the need
for explicit table assignment tracking.
4.1 Hyperparameter Inference
In our model, we treat all hyper-parameters
{(ax, bx), x ? (?, ?, e, ?)} as latent random vari-
ables rather than fixed parameters. This means our
model is parameter free, and requires no user inter-
vention when adapting to different data sets. For
the discount parameter, we employ a uniform Beta
distribution ax ? Beta(1, 1) while for the strength
parameter, we employ a vague Gamma distribu-
tion bx ? Gamma(10, 0.1). All restaurants in
the same level share the same hyper-prior and the
hyper-parameters for all levels are resampled us-
ing slice sampling (Johnson and Goldwater, 2009)
every 10 iterations.
4.2 Parallel Implementation
As mentioned above, the hierarchical PYP takes
into consideration a rich history to evaluate the
probabilities of translation decisions. But this
leads to difficulties when applying the model to
large data sets, particularly in terms of tracking
the table and customer counts. We apply the tech-
nique from Blunsom et al (2009a) of using multi-
ple processors to perform approximate Gibbs sam-
pling which they showed achieved equivalent per-
formance to the exact Gibbs sampler. Each pro-
cess performs sampling on a subset of the corpus
using local counts, and communicates changes to
these counts after each full iteration. All the count
deltas are then aggregated by each process to re-
fresh the counts at the end of each iteration. In
this way each process uses slightly ?out-of-date?
counts, but can process the data independently of
the other processes. We found that this approxi-
mation improved the runtime significantly with no
noticeable effect on accuracy.
5 Experiments
In principle our model could be directly used as a
MT decoder or as a feature in a decoder. However
in this paper we limit our focus to inducing word
alignments, i.e., by using the model to infer align-
ments which are then used in a standard phrase-
based translation pipeline. We leave full decod-
ing for later work, which we anticipate would fur-
ther improve performance by exploiting gapping
phrases and other phenomena that implicitly form
part of our model but are not represented in the
phrase-based decoder. Decoding under our model
would be straight-forward in principle, as the gen-
erative process was designed to closely parallel the
search procedure in the phrase-based model.3
Three data sets were used in the experi-
ments: two Chinese to English data sets on small
(IWSLT) and larger corpora (FBIS), and Arabic
3However the reverse translation probability would be in-
tractable, as this does not decompose following a left-to-right
generation order in the target language.
338
to English translation. Our experiments seek to
test how the model compares to a GIZA++ base-
line, quantifies the effect of each factor in the
probabilistic model (i.e., jump, fertility), and the
effect of different initialisations of the sampler.
We present results on translation quality and word
alignment.
5.1 Data Setup
The Markov order of our model in all experiments
was set to n = 2, as shown in Equation 2. For each
data set, Gibbs sampling was performed on the
training set in each direction (source-to-target and
target-to-source), initialized using GIZA++.4 We
used the grow heuristic to combine the GIZA++
alignments in both directions (Koehn et al, 2003),
which we then intersect with the predictions of
GIZA++ in the relevant translation direction. This
initialisation setup gave the best results (we com-
pare other initialisations in ?5.2). The two Gibbs
samplers were ?burned in? for the first 1000 it-
erations, after which we ran a further 500 itera-
tions selecting every 50th sample. A phrase ta-
ble was constructed using these 10 sets of multi-
ple alignments after combining each pair of direc-
tional alignments using the grow-diag-final heuris-
tic. Using multiple samples in this way constitutes
Monte Carlo averaging, which provides a better
estimate of uncertainty cf. using a single sample.5
The alignment used for the baseline results was
produced by combining bidirectional GIZA++
alignments using the grow-diag-final heuristic.
We used the Moses machine translation decoder
(Koehn et al, 2007), using the default features
and decoding settings. We compared the perfor-
mance of Moses using the alignment produced by
our model and the baseline alignment, evaluating
translation quality using BLEU (Papineni et al,
2002) with case-insensitive n-gram matching with
n = 4. We used minimum error rate training (Och,
2003) to tune the feature weights to maximise the
BLEU score on the development set.
5.2 IWSLT Corpus
The first experiments are on the IWSLT data set
for Chinese-English translation. The training data
consists of 44k sentences from the tourism and
travel domain. For the development set we use
both ASR devset 1 and 2 from IWSLT 2005, and
4All GIZA++ alignments used in our experiments were
produced by IBM model4.
5The effect on translation scores is modest, roughly
amounting to +0.2 BLEU versus using a single sample.
System Dev IWSLT05
baseline 45.78 49.98
Markov+fs+e 49.13 51.54
Markov+fs+e+j 49.68 52.55
Markov+fs+e+j+ft 51.32 53.41
Table 2: Impact of adding factors to our Markov
model, showing BLEU scores on IWSLT. Key: fs?
finish e?emission j?jump ft?fertility.
for the test set we use the IWSLT 2005 test set.
The language model is a 3-gram language model
trained using the SRILM toolkit (Stolcke, 2002)
on the English side of the training data. Because
the data set is small, we performed Gibbs sampling
on a single processor.
First we check the effect of the model factors
jump and fertility. Both emission and finish fac-
tors are indispensable to the generative translation
process, and consequently these two factors are in-
cluded in all runs. Table 2 shows translation result
for various models, including a baseline and our
Markov model with different combinations of fac-
tors. Note that even the simplest Markov model far
outperforms the GIZA++ baseline (+1.5 BLEU)
despite the baseline (IBM model 4) including a
number of advanced features (e.g., jump, fertility)
that are not present in the basic Markov model.
This improvement is a result of the Markov model
making use of rich bilingual contextual informa-
tion coupled with sophisticated backoff, as op-
posed to GIZA++ which considers much more lo-
cal events, with nothing larger than word-class bi-
grams. Our model shows large improvements as
the extra factors are included. Jump yields an im-
provement of +1 BLEU by capturing consistent re-
ordering patterns. Adding fertility results in a fur-
ther +1 BLEU point improvement. Like the IBM
models, our approach allows each source word to
produce any number of target words. This capac-
ity allows for many non-sensical alignments such
as dropping many source words, or aligning sin-
gle source words to several target words. Explic-
itly modelling fertility allows for more consistent
alignments, especially for special words such as
punctuation which usually have a fertility of one.
Next we check the stability of our model with
different initialisations. We compare different
combination techniques for merging the GIZA++
alignments: grow-diag-final (denoted as gdf ), in-
tersection and grow. Table 3 shows that the dif-
ferent initialisations have only a small effect on
339
system gdf intersection grow
baseline 49.98 48.44 50.11
our model 52.96 52.79 53.41
Table 3: Machine translation performance in
BLEU % on the IWSLT 2005 Chinese-English test
set. The Gibbs samplers were initialized with three
different alignments, shown as columns.
the results of our model. While the baseline re-
sults vary by up to 1.7 BLEU points for the differ-
ent alignments, our Markov model provided more
stable results with the biggest difference of 0.6.
Among the three initialisations, we get the best
result with the initialisation of grow. Gdf of-
ten introduces alignment links involving function
words which should instead be aligned to null. In-
tersection includes many fewer alignments, typi-
cally only between content words, and the sparsity
means that words can only have a fertility of ei-
ther 0 or 1. This leads to the initialisation being a
strong mode which is difficult to escape from dur-
ing sampling. Despite this problem, it has only
a mild negative effect on the performance of our
model, which is probably due to improvements
in the alignments for words that truly should be
dropped or aligned only to one word. Grow pro-
vides a good compromise between gdf and inter-
section, and we use this initialisation in all our
subsequent experiments.
Figure 3 shows an example comparing align-
ments produced by our model and the GIZA++
baseline, in both cases after combining the two di-
rectional models. Note that GIZA++ has linked
many function words which should be left un-
aligned, by using rare English terms as garbage
collectors. Consequently this only allows for the
extraction of few large phrase-pairs (e.g. <?
?, ?m looking for>) and prevents the extraction
of some good phrases (e.g. <?? ?? ?,
grill-type>, for ??? and ?? ?? are wrongly
aligned to ?grill-type?). In contrast, our model
better aligns the function words, such that many
more useful phrase pairs can be extracted, i.e.,
<?, ?m>,<?, looking for>,<????, grill-
type> and their combinations with neighbouring
phrase pairs.
5.3 FBIS Corpus
Theoretically, Bayesian models should out-
perform maximum likelihood approaches on small
data sets, due to their improved modelling of un-
(a) GIZA++ baseline
? ? ? ? ? ? ? ? , ?
?
? ?
?
?
?
? ?
?
?
i
'm
looking
for
a
nice
,
quiet
grill-type
restaurant
.
(b) our model
Figure 3: Comparison of an alignment inferred by
the baseline vs. our approach.
certainty. For larger datasets, however, the dif-
ference between the two techniques should nar-
row. Hence one might expect that upon moving
to larger translation datasets our gains might evap-
orate. This chain of reasoning ignores the fact that
our model is considerably richer than the baseline
IBM models, in that we model rich contextual cor-
relations between translation decisions, and con-
sequently our approach has a lower inductive bias.
For this reason our model should continue to im-
prove with more data, by inferring better estimates
of translation decision n-grams. A caveat though
is that inference by sampling becomes less effi-
cient on larger data sets due to stronger modes,
requiring more iterations for convergence.
To test whether our improvements carry over to
larger datasets, we assess the performance of our
model on the FBIS Chinese-English data set. Here
the training data consists of the non-UN portions
and non-HK Hansards portions of the NIST train-
ing corpora distributed by the LDC, totalling 303k
sentence pairs with 8m and 9.4m words of Chi-
nese and English, respectively. For the develop-
ment set we use the NIST 2002 test set, and eval-
uate performance on the test sets from NIST 2003
340
NIST02 NIST03 NIST05
baseline 33.31 30.09 29.01
our model 33.83 31.02 30.23
Table 4: Translation performance on Chinese to
English translation, showing BLEU% for models
trained on the FBIS data set.
and 2005. The language model is a 3-gram LM
trained on Xinhua portion of the Gigaword corpus
using the SRILM toolkit with modified Kneser-
Ney smoothing. As the FBIS data set is large, we
employed 3-processor MPI for each Gibbs sam-
pler, which ran in half the time compared to using
a single processor.
Table 4 shows the results on the FBIS data set.
Our model outperforms the baseline on both test
sets by about 1 BLEU. This provides evidence that
our model performs well in the large data setting,
with our rich modelling of context still proving
useful. The non-parametric nature of the model al-
lows for rich dynamic backoff behaviour such that
it can learn accurate models in both high and low
data scenarios.
5.4 Arabic English translation
Translation between Chinese and English is very
difficult, particularly due to word order differences
which are not handled well by phrase-based ap-
proaches. In contrast Arabic to English translation
needs less reordering, and phrase-based models
produce better translations. This translation task
is a good test for the generality of our approach.
Our Ar-En training data comprises several LDC
corpora,6 using the same experimental setup as in
Blunsom et al (2009a). Overall there are 276k
sentence pairs and 8.21m and 8.97m words in Ara-
bic and English, respectively. We evaluate on the
NIST test sets from 2003 and 2005, and the 2002
test set was used for MERT training.
Table 5 shows the results. On all test sets our
approach outperforms the baseline, and for the
NIST03 test set the improvement is substantial,
with a +0.74 BLEU improvement. In general
the improvements are more modest than for the
Chinese-English results above. We suggest that
this is due to the structure of Arabic-English trans-
lation better suiting the modelling assumptions be-
hind IBM model 4, particularly its bias towards
monotone translations. Consequently the addi-
6LDC2004E72, LDC2004T17, LDC2004T18,
LDC2006T02
F1% NIST02 NIST03 NIST05
baseline 64.9 57.00 48.75 48.93
our model 65.7 57.14 49.49 48.96
Table 5: Translation performance on Arabic to
English translation, showing BLEU%. Also shown
is word-alignment alignment accuracy.
tional context provided by our model is less im-
portant. Table 5 also reports alignment results on
manually aligned Ar-En sentence pairs,7 measur-
ing the F1 score for the GIZA++ baseline align-
ments and the alignment from the final sample
with our model.8 Our model outperforms the base-
line, although the improvement is modest.
6 Conclusions and Future Work
This paper proposes a word-based Markov model
of translation which correlates translation deci-
sions by conditioning on recent decisions, and
incorporates a hierarchical Pitman-Yor process
prior permitting elaborate backoff behaviour. The
model can learn sequences of translation deci-
sions, akin to phrases in standard phrase-based
models, while simultaneously learning word level
phenomena. This mechanism generalises the
concept of phrases in phrase-based MT, while
also capturing richer phenomena such as gapping
phrases in the source. Experiments show that our
model performs well both on the small and large
datasets for two different translation tasks, con-
sistently outperforming a competitive baseline. In
this paper the model was only used to infer word
alignments; in future work we intend to develop
a decoding algorithm for directly translating with
the model.
Acknowledgements
This work was supported by the EPSRC (grant
EP/I034750/1).
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009a. A Gibbs sampler for phrasal
synchronous grammar induction. In Proc. of ACL-
IJCNLP, pages 782?790.
7LDC2012T16
8Directional alignments are intersected using the grow-
diag-final heuristic.
341
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and
Mark Johnson. 2009b. A note on the implemen-
tation of hierarchical dirichlet processes. In Proc. of
ACL-IJCNLP, pages 337?340.
Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263?331.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL, pages 263?270.
Josep Maria Crego, Franc?ois Yvon, and Jose? B.
Marin?o. 2011. Ncode: an open source bilingual n-
gram SMT toolkit. Prague Bull. Math. Linguistics,
96:49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proc. of ACL:HLT, pages
1045?1054.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Proc. of NAACL, pages 966?974.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961?968.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adaptor
grammars. In Proc. of HLT-NAACL, pages 317?325.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proc. of NAACL, pages 39?48.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING-ACL, pages 609?
616, July.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Andreas Stolcke. 2002. SRILM: An extensible lan-
guage modeling toolkit. In Proc. of ICSLP.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet processes.
Journal of the American Statistical Association,
101(476):1566?1581.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. of ACL, pages 985?992.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proc. of ACL, pages 856?864.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proc. of COLING, pages 836?841.
342
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780?790,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Infinite Hierarchical Bayesian Model of Phrasal Translation
Trevor Cohn
Department of Computer Science
The University of Sheffield
Sheffield, United Kingdom
t.cohn@sheffield.ac.uk
Gholamreza Haffari
Faculty of Information Technology
Monash University
Clayton, Australia
reza@monash.edu
Abstract
Modern phrase-based machine translation
systems make extensive use of word-
based translation models for inducing
alignments from parallel corpora. This
is problematic, as the systems are inca-
pable of accurately modelling many trans-
lation phenomena that do not decompose
into word-for-word translation. This pa-
per presents a novel method for induc-
ing phrase-based translation units directly
from parallel data, which we frame as
learning an inverse transduction grammar
(ITG) using a recursive Bayesian prior.
Overall this leads to a model which learns
translations of entire sentences, while also
learning their decomposition into smaller
units (phrase-pairs) recursively, terminat-
ing at word translations. Our experiments
on Arabic, Urdu and Farsi to English
demonstrate improvements over competi-
tive baseline systems.
1 Introduction
The phrase-based approach (Koehn et al, 2003)
to machine translation (MT) has transformed MT
from a narrow research topic into a truly useful
technology to end users. Leading translation sys-
tems (Chiang, 2007; Koehn et al, 2007; Marcu et
al., 2006) all use some kind of multi-word transla-
tion unit, which allows translations to be produced
from large canned units of text from the training
corpus. Larger phrases allow for the lexical con-
text to be considered in choosing the translation,
and also limit the number of reordering decisions
required to produce a full translation.
Word-based translation models (Brown et al,
1993) remain central to phrase-based model train-
ing, where they are used to infer word-level align-
ments from sentence aligned parallel data, from
which phrasal translation units are extracted us-
ing a heuristic. Although this approach demon-
strably works, it suffers from a number of short-
comings. Firstly, many phrase-based phenomena
which do not decompose into word translations
(e.g., idioms) will be missed, as the underlying
word-based alignment model is unlikely to pro-
pose the correct alignments. Secondly, the rela-
tionship between different phrase-pairs is not con-
sidered, such as between single word translations
and larger multi-word phrase-pairs or where one
large phrase-pair subsumes another.
This paper develops a phrase-based translation
model which aims to address the above short-
comings of the phrase-based translation pipeline.
Specifically, we formulate translation using in-
verse transduction grammar (ITG), and seek to
learn an ITG from parallel corpora. The novelty
of our approach is that we develop a Bayesian
prior over the grammar, such that a nontermi-
nal becomes a ?cache? learning each production
and its complete yield, which in turn is recur-
sively composed of its child constituents. This is
closely related to adaptor grammars (Johnson et
al., 2007a), which also generate full tree rewrites
in a monolingual setting. Our model learns trans-
lations of entire sentences while also learning their
decomposition into smaller units (phrase-pairs) re-
cursively, terminating at word translations. The
model is richly parameterised, such that it can de-
scribe phrase-based phenomena while also explic-
itly modelling the relationships between phrase-
pairs and their component expansions, thus ame-
liorating the disconnect between the treatment of
words versus phrases in the current MT pipeline.
We develop a Bayesian approach using a Pitman-
Yor process prior, which is capable of modelling
a diverse range of geometrically decaying distri-
butions over infinite event spaces (here translation
phrase-pairs), an approach shown to be state of the
art for language modelling (Teh, 2006).
780
We are not the first to consider this idea; Neu-
big et al (2011) developed a similar approach for
learning an ITG using a form of Pitman-Yor adap-
tor grammar. However Neubig et al?s work was
flawed in a number of respects, most notably in
terms of their heuristic beam sampling algorithm
which does not meet either of the Markov Chain
Monte Carlo criteria of ergodicity or detailed bal-
ance. Consequently their approach does not con-
stitute a valid Bayesian model. In contrast, this
paper provides a more rigorous and theoretically
sound method. Moreover our approach results in
consistent translation improvements across a num-
ber of translation tasks compared to Neubig et al?s
method, and a competitive phrase-based baseline.
2 Related Work
Inversion transduction grammar (or ITG) (Wu,
1997) is a well studied synchronous grammar for-
malism. Terminal productions of the form X ?
e/f generate a word in two languages, and non-
terminal productions allow phrasal movement in
the translation process. Straight productions, de-
noted by their non-terminals inside square brack-
ets [...], generate their symbols in the given or-
der in both languages, while inverted productions,
indicated by angled brackets ?...?, generate their
symbols in the reverse order in the target language.
In the context of machine translation, ITG
has been explored for statistical word alignment
in both unsupervised (Zhang and Gildea, 2005;
Cherry and Lin, 2007; Zhang et al, 2008; Pauls et
al., 2010) and supervised (Haghighi et al, 2009;
Cherry and Lin, 2006) settings, and for decoding
(Petrov et al, 2008). Our paper fits into the re-
cent line of work for jointly inducing the phrase ta-
ble and word alignment (DeNero and Klein, 2010;
Neubig et al, 2011). The work of DeNero and
Klein (2010) presents a supervised approach to
this problem, whereas our work is unsupervised
hence more closely related to Neubig et al (2011)
which we describe in detail below.
A number of other approaches have been de-
veloped for learning phrase-based models from
bilingual data, starting with Marcu and Wong
(2002) who developed an extension to IBM model
1 to handle multi-word units. This pioneer-
ing approach suffered from intractable inference
and moreover, suffers from degenerate solutions
(DeNero and Klein, 2010). Our approach is simi-
lar to these previous works, except that we impose
additional constraints on how phrase-pairs can be
tiled to produce a sentence pair, and moreover,
we seek to model the embedding of phrase-pairs
in one another, something not considered by this
prior work. Another strand of related research is
in estimating a broader class of synchronous gram-
mars than ITGs, such as SCFGs (Blunsom et al,
2009b; Levenberg et al, 2012). Conceptually, our
work could be readily adapted to general SCFGs
using similar techniques.
This work was inspired by adaptor grammars
(Johnson et al, 2007a), a monolingual grammar
formalism whereby a non-terminal rewrites in a
single step as a complete subtree. The model prior
allows for trees to be generated as a mixture of a
cache and a base adaptor grammar. In our case,
we have generalised to a bilingual setting using an
ITG. Additionally, we have extended the model to
allow recursive nesting of adapted non-terminals,
such that we end up with an infinitely recursive
formulation where the top-level and base distribu-
tions are explicitly linked together.
As mentioned above, ours is not the first work
attempting to generalise adaptor grammars for ma-
chine translation; (Neubig et al, 2011) also devel-
oped a similar approach based around ITG using a
Pitman-Yor Process prior. Our approach improves
upon theirs in terms of the model and inference,
and critically, this is borne out in our experiments
where we show uniform improvements in transla-
tion quality over a baseline system, as compared
to their almost entirely negative results. We be-
lieve that their approach had a number of flaws:
For inference they use a beam-search, which may
speed up processing but means that they are no
longer sampling from the true distribution, nor a
distribution with the same support as the posterior.
Moreover they include a Metropolis-Hastings cor-
rection step, which is required to correct the sam-
ples to account for repeated substructures which
will be otherwise underrepresented. Consequently
their approach does not constitute a Markov Chain
Monte Carlo sampler, but rather a complex heuris-
tic.
The other respect in which this work differs
from Neubig et al (2011) is in terms of model for-
mulation. They develop an ITG which generates
phrase-pairs as terminals, while we employ a more
restrictive word-based model which forces the de-
composition of every phrase-pair. This is an im-
portant restriction as it means that we jointly learn
781
a word and phrase based model, such that word
based phenomena can affect the phrasal struc-
tures. Finally our approach models separately the
three different types of ITG production (mono-
tone, swap and lexical emission), allowing for a
richer parameterisation which the model exploits
by learning different hyper-parameter values.
3 Model
The generative process of the model follows that
of ITG with the following simple grammar
X ? [X X] | ?X X?
X ? e/f | e/? | ?/f ,
where [?] denotes monotone ordering and ??? de-
notes a swap in one language. The symbol ? de-
notes the empty string. This corresponds to a sim-
ple generative story, with each stage being a non-
terminal rewrite starting with X and terminating
when there are no frontier non-terminals.
A popular variant is a phrasal ITG, where the
leaves of the ITG tree are phrase-pairs and the
training seeks to learn a segmentation of the source
and target which yields good phrases. We would
not expect this model to do very well as it cannot
consider overlapping phrases, but instead is forced
into selecting between many competing ? and of-
ten equally viable ? options. Our approach im-
proves over the phrasal model by recursively gen-
erating complete phrases. This way we don?t insist
on a single tiling of phrases for a sentence pair, but
explicitly model the set of hierarchically nested
phrases as defined by an ITG derivation. This ap-
proach is closer in spirit to the phrase-extraction
heuristic, which defines a set of ?atomic? terminal
phrase-pairs and then extracts every combination
of these atomic phase-pairs which is contiguous in
the source and target.1
The generative process is that we draw a com-
plete ITG tree, t ? P2(?), as follows:
1. choose the rule type, r ? R, where r ?
{mono, swap, emit}
2. for r = mono
(a) draw the complete subtree expansion,
t = X ? [. . .] ? TM
3. for r = swap
(a) draw the complete subtree expansion,
t = X ? ?. . .? ? TS
1Our technique considers the subset of phrase-pairs which
are consistent with the ITG tree.
4. for r = emit
(a) draw a pair of strings, (e, f) ? E
(b) set t = X ? e/f
Note that we split the problem of drawing a tree
into two steps: first choosing the top-level rule
type and then drawing a rule of that type. This
gives us greater control than simply drawing a tree
of any type from one distribution, due to our pa-
rameterisation of the priors over the model param-
eters TM , TS and E.
To complete the generative story, we need to
specify the prior distributions for TM , TS and
E. First, we deal with the emission distribu-
tion, E which we drawn from a Dirichlet Pro-
cess prior E ? DP(bE , P0). We restrict the emis-
sion rules to generate word pairs rather than phrase
pairs.2 For the base distribution, P0, we use a sim-
ple uniform distribution over word pairs,
P0(e, f) =
?
??
??
?2 1VEVF e 6= ?, f 6= ?
?(1? ?) 1VF e = ?, f 6= ?
?(1? ?) 1VE e 6= ?, f = ?
,
where the constant ? denotes the binomial proba-
bility of a word being aligned.3
We use Pitman-Yor Process priors for the TM
and TS parameters
TM ? PYP(aM , bM , P1(?|r = mono))
TS ? PYP(aS , bS , P1(?|r = swap))
where P1(t1, t2|r) is a distribution over a pair of
trees (the left and right children of a monotone or
swap production). P1 is defined as follows:
1. choose the complete left subtree t1 ? P2,
2. choose the complete right subtree t2 ? P2,
3. set t = X ? [t1 t2] or t = X ? ?t1 t2?
depending on r
This generative process is mutually recursive: P2
makes draws from P1 and P1 makes draws from
P2. The recursion is terminated when the rule type
r = emit is drawn.
Following standard practice in Bayesian mod-
els, we integrate out R, TM , TS and E. This
means draws from P2 (or P1) are no longer iid:
for any non-trivial tree, computing its probabil-
ity under this model is complicated by the fact
2Note that we could allow phrases here, but given the
model can already reason over phrases by way of its hier-
archical formulation, this is an unnecessary complication.
3We also experimented with using word translation prob-
abilities from IBM model 1, based on the prior used by Lev-
enberg et al (2012), however we found little empirical differ-
ence compared with this simpler uniform model.
782
that the probability of its two subtrees are inter-
dependent. This is best understood in terms of
the Chinese Restaurant Franchise (CRF; Teh et al
(2006)), which describes the posterior distribution
after integrating out the model parameters. In our
case we can consider the process of drawing a tree
from P2 as a customer entering a restaurant and
choosing where to sit, from an infinite set of ta-
bles. The seating decision is based on the number
of other customers at each table, such that popular
tables are more likely to be joined than unpopular
or empty ones. If the customer chooses an occu-
pied table, the identity of the tree is then set to
be the same as for the other customers also seated
there. For empty tables the tree must be sampled
from the base distribution P1. In the standard CRF
analogy, this leads to another customer entering
the restaurant one step up in the hierarchy, and
this process can be chained many times. In our
case, however, every new table leads to new cus-
tomers reentering the original restaurant ? these
correspond to the left and right child trees of a
monotone or swap rule. The recursion terminates
when a table is shared, or a new table is labelled
with a emit rule.
3.1 Inference
The probability of a tree (i.e., a draw from P2) un-
der the model is
P2(t) = P (r)P2(t|r) (1)
where r is the rule type, one of mono, swap or
emit. The distribution over types, P (r), is de-
fined as
P (r) = n
T,?
r + bT 13
nT,? + bT
where nT,? are the counts over rules of types.4
The second component in (1), P2(t|r), is de-
fined separately for each rule type. For r = mono
or r = swap rules, it is defined as
P2(t|r) =
n?t,r ?K?t,rar
n?r + br
+ K
?
r ar + br
n?r + br
P1(t1, t2|r) ,
(2)
where n?t,r is the count for tree t in the other train-
ing sentences, K?t,r is the table count for t and n?r
4The conditioning on event and table counts, n?,K? is
omitted for clarity.
and K?r are the total count of trees and tables, re-
spectively. Finally, the probability for r = emit
is given by
P2(t|r = emit) =
n?t,E + bEP0(e, f)
n?r + br
,
where t = X ? e/f .
To complete the derivation we still need to de-
fine P1, which is formulated as
P1(t1, t2) = P2(t1)P2(t2|t1) ,
where the conditioning of the second recursive call
to P2 reflects that the counts n? and K? may
be affected by the first draw from P2. Although
these two draws are assumed iid in the prior, after
marginalising out T they are no longer indepen-
dent. For this reason, evaluating P2(t) is computa-
tionally expensive, requiring tracking of repeated
substructures in descendent sub-trees of t, which
may affect other descendants. This results in an
asymptotic complexity exponential in the number
of nodes in the tree. For this reason we consider
trees annotated with binary values denoting their
table assignment, namely whether they share a ta-
ble or are seated alone. Given this, the calculation
is greatly simplified, and has linear complexity.5
We construct an approximating ITG following
the technique used for sampling trees from mono-
lingual tree-substitution grammars (Cohn et al,
2010). To do so we encode the first term from
(2) separately from the second term (correspond-
ing to draws from P1). Summing together these
two alternate paths ? i.e., during inside inference ?
we recover P2 as shown in (2). The full grammar
transform for inside inference is shown in Table 1.
The sampling algorithm closely follows the
process for sampling derivations from Bayesian
PCFGs (Johnson et al, 2007b). For each sentence-
pair, we first decrement the counts associated with
its current tree, and then sample a new deriva-
tion. This involves first constructing the inside
lattice using the productions in Table 1, and then
performing a top-down sampling pass. After
sampling each derivation from the approximating
grammar, we then convert this into its correspond-
ing ITG tree, which we then score with the full
model and accept or reject the sample using the
5To support this computation, we track explicit table as-
signments for every training tree and their component sub-
trees. We also sample trees labelled with seating indicator
variables.
783
Typ
e X ?M P (r = mono)
X ? S P (r = swap)
X ? E P (r = emit)
Bas
e M ? [XX] K
?
MaM+bM
n?M+bM
S ? ?XX? K?S aS+bSn?S +bS
Co
unt
For every tree, t, of type r = mono, with nt,M > 0:
M ? sig(t) n?t,M?K?t,Marn?M+bMsig(t)? yield(t) 1
For every tree, t, of type r = swap, with nt,S > 0:
S ? sig(t) n?t,S?K?t,SaSn?S +bSsig(t)? yield(t) 1
Em
it For every word pair, e/f in sentence pair,
where one of e, f can be ?:
E ? e/f P2(t)
Table 1: Grammar transformation rules for MAP
inside inference. The function sig(t) returns a
unique identifier for the complete tree t, and
the function yield(t) returns the pair of terminal
strings from the yield of t.
Metropolis-Hastings algorithm.6 Accepted sam-
ples then replace the old tree (otherwise the old
tree is retained) and the model counts are incre-
mented. This process is then repeated for each
sentence pair in the corpus in a random order.
4 Experiments
Datasets We train our model across three
language pairs: Urdu?English (UR-EN),
Farsi?English (FA-EN), and Arabic?English
(AR-EN). The corpora statistics of these trans-
lation tasks are summarised in Table 2. The
UR-EN corpus comes from NIST 2009 translation
evaluation.7 The AR-EN training data consists
of the eTIRR corpus (LDC2004E72), the Ara-
bic news corpus (LDC2004T17), the Ummah
corpus (LDC2004T18), and the sentences with
confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For FA-EN, we use TEP8 Tehran English-Persian
Parallel corpus (Pilevar and Faili, 2011), which
consists of conversational/informal text extracted
6The full model differs from the approximating grammar
in that it accounts for inter-dependencies between subtrees
by recursively tracking the changes in the customer and table
counts while scoring the tree. Around 98% of samples were
accepted in our experiments.
7http://www.itl.nist.gov/iad/mig/tests/mt/2009
8http://ece.ut.ac.ir/NLP/resources.htm
source target sentences
UR-EN 745K 575K 148K
FA-EN 4.7M 4.4M 498K
AR-EN 1.94M 2.08M 113K
Table 2: Corpora statistics showing numbers of
parallel sentences and source and target words for
the training sets.
from 1600 movie subtitles. We tokenized this
corpus, removed noisy single-word sentences,
randomly selected the development and test sets,
and used the rest of the corpus as the training set.
We discard sentences with length above 30 from
the datasets for all experiments.9
Sampler configuration Samplers are initialised
with trees created from GIZA++ alignments
constructed using a SCFG factorisation method
(Blunsom et al, 2009a). This algorithm repre-
sents the translation of a sentence as a large SCFG
rule, which it then factorises into lower rank SCFG
rules, a process akin to rule binarisation com-
monly used in SCFG decoding. Rules that can-
not be reduced to a rank-2 SCFG are simplified
by dropping alignment edges until they can be
factorised, the net result being an ITG derivation
largely respecting the alignments.10
The blocked sampler was run 1000 iterations
for UR-EN, 100 iterations for FA-EN and AR-
EN. After each full sampling iteration, we resam-
ple all the hyper-parameters using slice-sampling,
with the following priors: a ? Beta(1, 1),
b ? Gamma(10, 0.1). Figure 1 shows the poste-
rior probability improves with each full sampling
iterations. The alignment probability was set to
? = 0.99. The sampling was repeated for 5 in-
dependent runs, and we present results where we
combine the outputs of these runs. This is a form
of Monte Carlo integration which allows us to rep-
resent the uncertainty in the posterior, while also
representing multiple modes, if present.
The time complexity of our inference algorithm
is O(n6), which can be prohibitive for large scale
machine translation tasks. We reduce the com-
plexity by constraining the inside inference to
consider only derivations which are compatible
9Hence the BLEU scores we get for the baselines may
appear lower than what reported in the literature.
10Using the factorised alignments directly in a translation
system resulted in a slight loss in BLEU versus using the un-
factorised alignments. Our baseline system uses the latter.
784
0 100 200 300 400 500?9
1000
00
?
9050
000
?
9000
000
?
8950
000
iteration
log p
oster
ior
Figure 1: Training progress on the UR-EN corpus,
showing the posterior probability improving with
each full sampling iteration. Different colours de-
note independent sampling runs.
l
ll
ll
l
l
l
ll
l
l
l
lll
l
l
ll
l
l
l
ll
l
l
lll
l
l l
l l
l
l l
llll
l
l
lll
l
l
ll
lll
lll
l
l
ll
l
l
ll
ll
l
ll l
lll
l
l
lll
ll
llll
lll
l
ll l
l
lll
l
l l
ll
llll
ll
llll
l
l l
ll
l
ll
l l
l
l l
llll
l l
l
llll
lll
l
l l
l
l
l ll
l ll
l
l l
l
l
l ll
llll
l
l
0 5 10 15 20 25 30
1e?0
5
1e?0
3
1e?0
1
average sentence length
time 
(s)
Figure 2: The runtime cost of bottom-up inside in-
ference and top-down sampling as a function of
sentence length (UR-EN), with time shown on a
logarithmic scale. Full ITG inference is shown
with red circles, and restricted inference using the
intersection constraints with blue triangles. The
average time complexity for the latter is roughly
O(l4), as plotted in green t = 2? 10?7l4.
with high confidence alignments from GIZA++.11
Figure 2 shows the sampling time with respect
to the average sentence length, showing that our
alignment-constrained sampling algorithm is bet-
ter than the unconstrained algorithm with empir-
ical complexity of n4. However, the time com-
plexity is still high, so we set the maximum sen-
tence length to 30 to keep our experiments practi-
cable. Presumably other means of inference may
be more efficient, such as Gibbs sampling (Lev-
enberg et al, 2012) or auxiliary variable sampling
(Blunsom and Cohn, 2010); we leave these exten-
sions to future work.
Baselines. Following (Levenberg et al, 2012;
Neubig et al, 2011), we evaluate our model by
using its output word alignments to construct a
phrase table. As a baseline, we train a phrase-
based model using the moses toolkit12 based on
the word alignments obtained using GIZA++ in
both directions and symmetrized using the grow-
diag-final-and heuristic13 (Koehn et al, 2003).
This alignment is used as input to the rule fac-
torisation algorithm, producing the ITG trees with
which we initialise our sampler. To put our results
in the context of the previous work, we also com-
pare against pialign (Neubig et al, 2011), an ITG
algorithm using a Pitman-Yor process prior, as de-
scribed in Section 2.14
In the end-to-end MT pipeline we use a stan-
dard set of features: relative-frequency and lexical
translation model probabilities in both directions;
distance-based distortion model; language model
and word count. We set the distortion limit to
6 and max-phrase-length to 7 in all experiments.
We train 3-gram language models using modified
Kneser-Ney smoothing. For AR-EN experiments
the language model is trained on English data as
(Blunsom et al, 2009a), and for FA-EN and UR-
EN the English data are the target sides of the
bilingual training data. We use minimum error
rate training (Och, 2003) with nbest list size 100
to optimize the feature weights for maximum de-
velopment BLEU.
11These are taken from the final model 4 word alignments,
using the intersection of the source-target and target-source
models. These alignments are very high precision (but have
low recall), and therefore are unlikely to harm the model.
12http://www.statmt.org/moses
13We use the default parameter settings in both moses and
GIZA++.
14http://www.phontron.com/pialign
785
Baselines This paper
GIZA++ pialign individual combination
UR-EN 16.95 15.65 16.68 ? .12 16.97
FA-EN 20.69 21.41 21.36 ? .17 21.50
AR-EN
MT03 44.05 43.30 44.8 ? .28 45.10
MT04 38.15 37.78 38.4 ? .08 38.4
MT05 42.81 42.18 43.13 ? .23 43.45
MT08 32.43 33.00 32.7 ? .15 32.80
Table 3: The BLEU scores for the translation tasks of three language pairs. The individual column show
the average and 95% confidence intervals for 5 independent runs, whereas the combination column show
the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and
those generated by the pialign (Neubig et al, 2011) bold: the best result.
1 2 5 10 20 50 100
1e?0
5
1e?0
3
1e?0
1
rule frequency
fracti
on of
 gram
mar
monotoneswapemit
Figure 3: Fraction of rules with a given frequency,
using a single sample grammar (UR-EN).
4.1 Results
Table 3 shows the BLEU scores for the three trans-
lation tasks UR/AR/FA?EN based on our method
against the baselines. For our models, we report
the average BLEU score of the 5 independent runs
as well as that of the aggregate phrase table gen-
erated by these 5 independent runs. There are
a number of interesting observations in Table 3.
Firstly, combining the phrase tables from indepen-
dent runs results in increased BLEU scores, possi-
bly due to the representation of uncertainty in the
outputs, and the representation of different modes
captured by the individual models. We believe this
type of Monte Carlo model averaging should be
considered in general when sampling techniques
are employed for grammatical inference, e.g. in
parsing and translation. Secondly, our approach
consistently improves over the Giza++ baseline
often by a large margin, whereas pialign under-
performs the GIZA++ baseline in many cases.
Thirdly, our model consistently outperforms pi-
align (except in AR-EN MT08 which is very
close). This highlights the modeling and inference
differences between our method and the pialign.
5 Analysis
In this section, we present some insights about the
learned grammar and the model hyper-parameters.
Firstly, we start by presenting various statistics
about different learned grammars. Figure 3 shows
the fraction of rules with a given frequency for
each of the three rule types. The three types of rule
exhibit differing amounts of high versus low fre-
quency rules, and all roughly follow power laws.
As expected, there is a higher tendency to reuse
high-frequency emissions (or single-word transla-
tion) compared to other rule types, which are the
basic building blocks to compose larger rules (or
phrases). Table 4 lists the high frequency mono-
tone and swap rules in the learned grammar. We
observe the high frequency swap rules capture re-
ordering in verb clusters, preposition-noun inver-
sions and adjective-noun reordering. Similar pat-
terns are seen in the monotone rules, along with
some common canned phrases. Note that ?in Iraq?
appears twice, once as an inversion in UR-EN and
another time in monotone order for AR-EN.
Secondly, we analyse the values learned for
the model hyper-parameters; Figure 4.(a) shows
the posterior distribution over the hyper-parameter
values. There is very little spread in the inferred
values, suggesting the sampling chains may have
converged. Furthermore, there is a large differ-
ence between the learned hyper-parameters for the
monotone rules versus the swap rules. For the
Pitman-Yor Process prior, the values of the hyper-
786
6`bB@1M;HBb?
TBHB;M bm`2fKhK M-+?B27f` vb- vQm bm`2fKhK Mv- BK bm`2fKhK MK- #Qbbf` vb- KF2 bm`2fKhK M-
`2 vQm bm`2fKhK Mv- Mvrvf?` >H- T`2bB/2Mif` vb DK?r`- MQi bm`2fKhK M MvbiK- BK
bm`2fKhK MK F?- B?K bm`2fKhK MK- bm`2fKhK M
Qm` K2i?Q/ bm`2fKhK }- ?p2f/ $ i?- #2f# $- ?p2f/ $ i? # $- H2i K2f `- #2+mb2 Q7fth`-
bm`2fKhK } M- /QfF` `- +QK2 QMfxr/ #- 2t+mb2 K2f##t- FBHHf` #F- +QK2 QMfxr/#-
KQ`2 i?Mf$ i`- #2?BM/fT $- r?i /QfKMwr`i- r?i /Q vQmfKMwr`i - FBHHfF $- /QMi
rQ``vfM;`M M#- Bb Bif$ /?- r2H+QK2ftr $ %- +?B27f` } vb- KF2 bm`2fKhK- BbfKv $-
KF2 bm`2fKhK }- KF2 bm`2fKhK} M- BK bQ``vf##t- H27if;  - B7f;` $
`#B+@1M;HBb?
TBHB;M bB/ Xf??? ?- bii2bf???????- mMBi2/f????????- H@r7/f ? ?????- 2zQ`ibf???- Q7 Kbb
/2bi`m+iBQMf?????? ??????- vQmKf?????- DBMiQf??? ???- HK H vQmKf????? ??????- H@BiiB?/
-f???????- i?2 }2H/ Q7f????- BbHK#/f?????- b+?2/mH2/f?????? ??- H@HK H@vQmKf????? ??????
?- f H@?vif? ??????- T2MBMbmHf??????? ???- K2Mr?BH2f???? ?????- T`BK2f?????- i?2
BMpBiiBQMf?? ????- f H@?vi -f? ??????- FQ`2M T2MBMbmHf??????? ??????? ???- H@M?` ?f??????
DD- /2T`iK2Mif???????? ?????- +Qi2f???- b TQbbB#H2f???? ???- H HK H vQmKf????? ??????- -
H@HK H@vQmK -f????? ??????- i i?2 BMpBiiBQMf?? ???? ?- D+[m2bf??? ???????- r2HH bf???-
TQBMibf???- pH/BKB` TmiBMf????? ???????? ??????- ;2Q`;2 rX #mb?f????
Qm` K2i?Q/ i?2 mMBi2/f???????- mb /QHH`bf??????- T`BK2f??????? ????- +?BM ?f?????- bTQF2bKMf???????
?- KMvf??- Bb 2tT2+i2/f???????- Bb 2tT2+i2/ iQf???????- i H2bif?????- QM im2b/vf???- 2;vTi
?f???- i?m`b/vf???- i?2 mMf???????- QM i?m`b/vf???- 7`B/vf???- QM 7`B/vf???- iQf???-
H@r7/ -f? ?????- i?2 mbf???????- 7Q`f? ??????- }`bi iBK2f?????? ?????- 7m`i?2`f??- B`[
?f??????- Bb`2HB T`BK2f?????????? ??????? ????- i?2 irQf???????- QM bim`/vf???- QM bmM/vf???-
mXbXf???????- pB2rbf?????- b?`QM ?f?????- +QmMi`v ?f??????- ?2 bB/f???- Bb`2H ?f???????- T2QTH2
?f?????- ?2`2f????? ???- +?BM ?f?????- - ?2 bB/f???? ?- 2`HB2`f??? ??- +?BM ?f???????-
i H2bif??? ?? ??- i?2 mXbXf???????- i?2 ;xf????- i?2 ;x bi`BTf????- ?2 //2/f???- `2
2tT2+i2/f???????- `2 2tT2+i2/ iQf???????- `2 2tT2+i2/ iQf??????? ??- KBHHBQM mXbXf?????-
++Q`/BM;f???- iQf?????- Q`/2`f??- BM Q`/2`f??- ?2 TQBMi2/f????- K7 - b?`[ Hf? ??????- K7
- b?`[ H rbif? ??????- `7i ?f?????
k
Table 5: Good phrase pairs in the top-100 high frequency phrase pairs specific to the phrase tables
coming from our method vs that of pialign for FA-EN and AR-EN translation tasks.
parameters affects the rate at which the number of
types grows compared to the number of tokens.
Specifically, as the discount a or the concentra-
tion b parameters increases we expect for a rela-
tive increase in the number of types. If the number
of observed monotone and swap rules were equal,
then there would be a higher chance in reusing the
monotone rules. However, the number of observed
monotone and swap rules are not equal, as plotted
in Figure 4.(b). Similar results were observed for
the other language pairs (figures omitted for space
reasons).
Thirdly, we performed a manual evaluation for
the quality of the phrase-pairs learned exclusively
by our method vs pialign. For each method,
we considered the top-100 high frequency phrase-
pairs which are specific to that method. Then we
asked a bilingual human expert to identify rea-
sonably well phrase-pairs among these top-100
phrase-pairs. The results are summarized in Ta-
ble 5, and show that we learn roughly twice as
many reasonably good phrase-pairs for AR-EN
and FA-EN compared to pialign.
Conclusions
We have presented a novel method for learn-
ing a phrase-based model of translation directly
from parallel data which we have framed as learn-
ing an inverse transduction grammar (ITG) us-
ing a recursive Bayesian prior. This has led
to a model which learns translations of en-
tire sentences, while also learning their decom-
position into smaller units (phrase-pairs) recur-
sively, terminating at word translations. We have
presented a Metropolis-Hastings sampling algo-
rithm for blocked inference in our non-parametric
ITG. Our experiments on Urdu-English, Arabic-
English, and Farsi-English translation tasks all
demonstrate improvements over competitive base-
line systems.
Acknowledgements
The first author was supported by the EPSRC
(grant EP/I034750/1) and an Erasmus-Mundus
scholarship funding a research visit to Melbourne.
The second author was supported by an early ca-
reer research award from Monash University.
787
0.905 0.910 0.915 0.920 0.925
0
200
400
600
800
100
0
am and as
Den
sity
1000 2000 3000 40000
.000
0
0.00
10
0.00
20
bm and bs
Den
sity
0 5 10 15 20 25 30
0.00
0.01
0.02
0.03
0.04
0.05
0.06
be
Den
sity
65000 65500 660000
.000
0
0.00
05
0.00
10
0.00
15
bt
Den
sity
(a)
291000 292000 2930000
.000
0
0.00
04
0.00
08
0.00
12
monotone
176000 1770000
.000
0
0.00
04
0.00
08
0.00
12
swap
Den
sity
(b)
Figure 4: (a) Posterior over the hyper-parameters,
aM , aS , bM , bS , bE , bT , measured for UR-EN us-
ing samples 400?500 for 3 independent sampling
chains, and the intersection constraints. (b) Poste-
rior over the number of monotone and swap rules
in the resultant grammars. The distribution for
emission rules was also peaked about 147k rules.
key ( ( ( ?2fM?rL ) ( ?fMu ) ) ( bB/fF? ) )
kkk ( ? ( bB/fF? ) ( ?fMu ) ? ( i?ifF? ) )
kRN ( ( ( ( ?2fM?rL ) ( ?fMu ) )
( bB/fF? ) ) ( i?ifF? ) )
R93 ( ( ( B7f;` ) ( ?f% ) ) ( vQmfT ) )
Ry3 ( ( ?2fM?rL ) ? ( bB/fF? ) ( ?fMu ) ? )
R3k ? ( rBHHf; ) ( #2f?r ) ?
RkN ? ( Bbf?u ) ( MQifM?vL ) ?
Rkj ? ( ?bf?u ) ( #22Mf;v ) ?
Ry9 ? ( rBHHf; ) ( #2fDvu ) ?
Ryj ? ( BMfKvL ) ( B`[f1`[ ) ?
l`/m@1M;HBb?
3Ny ( ( QM2fvFv ) ( Q7fx ) )
39j ( ? ( v2?f`? ) ( ?f% ) ? ( XfX ) )
dj3 ( ( rBi?f# ) ( K2fKM ) )
e99 ( ( ( ( QFvf# ) ( ?f$ ) ) ( ?f? ) ) ( XfX ) )
ey3 ( ( iQf#? ) ( K2fKM ) )
k8R ? ( Bbf/? ) ( Bif$ ) ?
kky ? ( i2HHf#;r ) ( K2fKM ) ?
RNN ? I ( Bf? ) ( +MfirMK ) ? ( ?ifMKv ) =
RNy ? ( ( r?QfFv ) ( `2f?biv ) ) ( vQmfir ) ?
R3d ? ( iQH/f;7i ) ( K2fKM ) ?
6`bB@1M;HBb?
8ee ( ( BMf?? ) ( B`[f?????? ) )
9R9 ( ( BMf?? ) ( 2;vTif??? ) )
jNR ( ( i?Bbf??? ) ( v2`f????? ) )
j8e ( ( b?`[f????? ) ( H@rbif?????? ) )
jyy ( ( BMf?? ) ( B`[f?????? ) )
9yk9 ? ( Xf ) ( ?f ) ?
RjRk ? I ( i?2f ) ( mMBi2/f??????? ) ?
( bii2bf???????? ) =
ee8 ? ( mMBi2/f??????? ) ( bii2bf???????? ) ?
e8y ? ( Hbif?????? ) ( v2`f????? ) ?
9ed ? I ( i?2f ) ( mMBi2/f??????? ) ?
( MiBQMbf????? ) =
`#B+@1M;HBb?
aQK2 HiBM i2ti M/ BMHBM2 `#B+, ????? ??????
l`/m- 6`bB- `#B+
Table 4: Top 5 monotone and swap productions
and their counts. Rules with mostly punctuation
or encoding 1:many or many:1 alignments were
omitted.
788
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238?241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009a. A Gibbs sampler for phrasal syn-
chronous grammar induction. In ACL2009, Singa-
pore, August.
Phil Blunsom, Trevor Cohn, and Miles Osborne.
2009b. Bayesian synchronous grammar induction.
In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21, pages 161?168. MIT Press.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of COLING/ACL. As-
sociation for Computational Linguistics.
Colin Cherry and Dekany Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In Proc. of the HLT-NAACL Workshop on
Syntax and Structure in Statistical Translation (SSST
2007), Rochester, USA.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053?3096.
John DeNero and Dan Klein. 2010. Discriminative
modeling of extraction sets for machine translation.
In The 48th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL).
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, Suntec, Singapore. Association for
Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007a. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007b. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of the 7th Inter-
national Conference on Human Language Technol-
ogy Research and 8th Annual Meeting of the NAACL
(HLT-NAACL 2007), pages 139?146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the 3rd International Conference on Human Lan-
guage Technology Research and 4th Annual Meet-
ing of the NAACL (HLT-NAACL 2003), pages 81?88,
Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-
2007), Prague.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223?232, Jeju Island, Korea, July.
Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proc. of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), pages 133?139, Philadelphia, July.
Association for Computational Linguistics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 44?52, Sydney, Australia, July.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In The 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 632?641,
Portland, Oregon, USA, 6.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160?
167, Sapporo, Japan.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment
with inversion transduction grammars. In Proceed-
ings of the North American Conference of the Asso-
ciation for Computational Linguistics (NAACL). As-
sociation for Computational Linguistics.
789
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of EMNLP.
Association for Computational Linguistics.
M. T. Pilevar and H. Faili. 2011. Tep: Tehran english-
persian parallel corpus. In Proc. International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing).
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet processes.
Journal of the American Statistical Association,
101(476):1566?1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
985?992.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL). Association for Computational Linguis-
tics.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. of the 46th Annual Conference of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), pages 97?105,
Columbus, Ohio, June.
790
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 993?1003,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A user-centric model of voting intention from Social Media
Vasileios Lampos, Daniel Preot?iuc-Pietro and Trevor Cohn
Computer Science Department
University of Sheffield, UK
{v.lampos,d.preotiuc,t.cohn}@dcs.shef.ac.uk
Abstract
Social Media contain a multitude of user
opinions which can be used to predict real-
world phenomena in many domains in-
cluding politics, finance and health. Most
existing methods treat these problems as
linear regression, learning to relate word
frequencies and other simple features to
a known response variable (e.g., voting
intention polls or financial indicators).
These techniques require very careful fil-
tering of the input texts, as most Social
Media posts are irrelevant to the task. In
this paper, we present a novel approach
which performs high quality filtering au-
tomatically, through modelling not just
words but also users, framed as a bilin-
ear model with a sparse regulariser. We
also consider the problem of modelling
groups of related output variables, us-
ing a structured multi-task regularisation
method. Our experiments on voting inten-
tion prediction demonstrate strong perfor-
mance over large-scale input from Twitter
on two distinct case studies, outperform-
ing competitive baselines.
1 Introduction
Web Social Media platforms have ushered a new
era in human interaction and communication. The
main by-product of this activity is vast amounts of
user-generated content, a type of information that
has already attracted the interest of both marke-
teers and scientists because it offers ? for the first
time at a large-scale ? unmediated access to peo-
ples? observations and opinions.
One exciting avenue of research concentrates
on mining interesting signals automatically from
this stream of text input. For example, by exploit-
ing Twitter posts, it is possible to infer time series
that correlate with financial indicators (Bollen et
al., 2011), track infectious diseases (Lampos and
Cristianini, 2010; Lampos et al, 2010; Paul and
Dredze, 2011) and, in general, nowcast the magni-
tude of events emerging in real-life (Sakaki et al,
2010; Lampos and Cristianini, 2012). Other stud-
ies suggest ways for modelling opinions encap-
sulated in this content in order to forge branding
strategies (Jansen et al, 2009) or understand vari-
ous socio-political trends (Tumasjan et al, 2010;
O?Connor et al, 2010; Lansdall-Welfare et al,
2012). The main theme of the aforementioned
works is linear regression between word frequen-
cies and a real-world quantity. They also tend to
incorporate hand-crafted lists of search terms to
filter irrelevant content and use sentiment analy-
sis lexicons for extracting opinion bias. Conse-
quently, they are quite often restricted to a specific
application and therefore, generalise poorly to new
data sets (Gayo-Avello et al, 2011).
In this paper, we propose a generic method that
aims to be independent of the characteristics de-
scribed above (use of search terms or sentiment
analysis tools). Our approach is able to explore
not only word frequencies, but also the space of
users by introducing a bilinear formulation for
this learning task. Regularised regression on both
spaces allows for an automatic selection of the
most important terms and users, performing at the
same time an improved noise filtering. In addi-
tion, more advanced regularisation functions en-
able multi-task learning schemes that can exploit
shared structure in the feature space. The latter
property becomes very useful in multi-output re-
gression scenarios, where selected features are ex-
pected to have correlated as well as anti-correlated
impact on each output (e.g., when inferring voting
intentions for competing political parties).
We evaluate our methods on the domain of
politics using data from the microblogging ser-
vice of Twitter to infer voting trends. Our pro-
993
posed framework is able to successfully predict
voting intentions for the top-3 and top-4 parties
in the United Kingdom (UK) and Austria respec-
tively. In both case studies ? bound by differ-
ent characteristics (including language, time-span
and number of users) ? the average prediction er-
ror is smaller than 1.5% for our best model using
multi-task learning. Finally, our qualitative analy-
sis shows that the models uncover interesting and
semantically interpretable insights from the data.
2 Data
For the evaluation of the proposed methodologies
we have created two data sets of Social Media con-
tent with different characteristics based in the UK
and Austria respectively. They are used for per-
forming regression aiming to infer voting intention
polls in those countries. Data processing is per-
formed using the TrendMiner architecture for So-
cial Media analysis (Preot?iuc-Pietro et al, 2012).
2.1 Tweets from users in the UK
The first data set (we refer to it as Cuk) used in
our experimental process consists of approx. 60
million tweets produced by approx. 42K UK Twit-
ter users from 30/04/2010 to 13/02/2012. We as-
sumed each user to be from the UK, if the location
field in their profile matched with a list of com-
mon UK locations and their time zone was set to
G.M.T. In this way, we were able to extract hun-
dreds of thousands of UK users, from which we
sub-sampled 42K users to be distributed across the
UK geographical regions proportionally to their
population figures.1
2.2 Tweets for Austria
The second data set (Cau) is shorter in terms of
the number of users involved (1.1K), its time span
(25/01 to 01/12/2012) and, consequently, of the
total number of tweets considered (800K). How-
ever, this time the selection of users has been made
by Austrian political experts who decided which
accounts to monitor by subjectively assessing the
value of information they may provide towards
political-oriented topics. Still, we assume that the
different users will produce information of varying
quality, and some should be eliminated entirely.
However, we emphasise that there may be smaller
1Data collection was performed using Twitter API,
http://dev.twitter.com/, to extract all posts for our
target users.
5 30 55 80 105 130 155 180 205 2300
5
10
15
20
25
30
35
40
45
Vot
ing 
Inte
ntio
n %
Time 
 
CONLABLIB
(a) 240 voting intention polls for the 3 major parties
in the UK (April 2010 to February 2012)
5 20 35 50 65 80 950
5
10
15
20
25
30
Vot
ing 
Inte
ntio
n %
Time 
 
SP??VPFP?GR?
(b) 98 voting intention polls for the 4 major parties in
Austria (January to December 2012)
Figure 1: Voting intention polls for the UK and
Austria.
potential gains from user modelling compared to
the UK case study. Another important distinction
is language, which for this data set is primarily
German with some English.
2.3 Ground Truth
The ground truth for training and evaluating our
regression models is formed by voting intention
polls from YouGov (UK) and a collection of Aus-
trian pollsters2 ? as none performed high fre-
quency polling ? for the Austrian case study.
We focused on the three major parties in the
UK, namely Conservatives (CON), Labour (LAB)
and Liberal Democrats (LBD) and the four ma-
jor parties in Austria, namely the Social Demo-
cratic Party (SPO?), People?s Party (O?VP), Free-
dom Party (FPO?) and the Green Alternative Party
(GRU?). Matching with the time spans of the data
sets described in the previous sections, we have
acquired 240 unique polls for the UK and 65 polls
for Austria. The latter have been expanded to
98 polls by replicating the poll of day i for day
2Wikipedia, http://de.wikipedia.org/wiki/
Nationalratswahl_in_\%D6sterreich_2013.
994
i ? 1 where possible.3 There exists some inter-
esting variability towards the end for the UK polls
(Fig. 1a), whereas for the Austrian case, the main
changing point is between the second and the third
party (Fig. 1b).
3 Methods
The textual content posted on Social Media plat-
forms unarguably contains valuable information,
but quite often it is hidden under vast amounts of
unstructured user generated input. In this section,
we propose a set of methods that build on one an-
other, which aim to filter the non desirable noise
and extract the most informative features not only
based on word frequencies, but also by incorporat-
ing users in this process.
3.1 The bilinear model
There exist a number of different possibilities for
incorporating user information into a regression
model. A simple approach is to expand the fea-
ture set, such that each user?s effect on the re-
sponse variable can be modelled separately. Al-
though flexible, this approach would be doomed
to failure due to the sheer size of the resulting fea-
ture set, and the propensity to overfit all but the
largest of training sets. One solution is to group
users into different types, such as journalist, politi-
cian, activist, etc., but this presupposes a method
for classification or clustering of users which is a
non-trivial undertaking. Besides, these na??ve ap-
proaches fail to account for the fact that most users
use similar words to express their opinions, by
separately parameterising the model for different
users or user groups.
We propose to account for individual users
while restricting all users to share the same vocab-
ulary. This is formulated as a bilinear predictive
model,
f(X) = uTXw + ? , (1)
where X is an m ? p matrix of user-word fre-
quencies and u and w are the model parameters.
Let Q ? Rn?m?p be a tensor which captures our
training inputs, where n, m and p denote the con-
sidered number of samples (each sample usually
refers to a day), terms and users respectively; Q
can simply be interpreted as n versions of X (de-
noted by Qi in the remainder of the script), a dif-
ferent one for each day, put together. Each element
3This has been carried out to ensure an adequate number
of training points in the experimental process.
Qijk holds the frequency of term j for user k dur-
ing the day i in our sample. If a user k has posted
ci?k tweets during day i, and cijk ? ci?k of them
contain a term j, then the frequency of j for this
day and user is defined as Qijk = cijkci?k .Aiming to learn sparse sets of users and terms
that are representative of the voting intention sig-
nal, we formulate our optimisation task as follows:
{w?,u?, ??} = argmin
w,u,?
n?
i=1
(
uTQiw + ? ? yi
)2
+ ?(w, ?1) + ?(u, ?2) ,
(2)
where y ? Rn is the response variable (voting in-
tention), w ? Rm and u ? Rp denote the term
and user weights respectively, uTQiw expresses
the bilinear term, ? ? R is a bias term and ?(?)
is a regularisation function with parameters ?1 or
?2. The first term in Eq. 2 is the standard regulari-
sation loss function, namely the sum squared error
over the training instances.4
In the main formulation of our bilinear model,
as the regularisation function ?(?) we use the elas-
tic net (Zou and Hastie, 2005), an extension of
the well-studied `1-norm regulariser, known as the
LASSO (Tibshirani, 1996). The `1-norm regu-
larisation has found many applications in several
scientific fields as it encourages sparse solutions
which reduce the possibility of overfitting and en-
hance the interpretability of the inferred model
(Hastie et al, 2009). The elastic net applies an
extra penalty on the `2-norm of the weight vector,
and can resolve instability issues of LASSO which
arise when correlated predictors exist in the input
data (Zhao and Yu, 2006). Its regularisation func-
tion ?el(?) is defined by:
?el (w, ?, ?) = ?
(1? ?
2 ?w?
2
2 + ??w?1
)
, (3)
where ? > 0 and ? ? [0, 1); setting parameter
? to its extremes transforms elastic net to ridge
regression (? = 0) or vanilla LASSO (? = 1).
Eq. 2 can be treated as a biconvex learning task
(Al-Khayyal and Falk, 1983), by observing that
for a fixed w, learning u is a convex problem and
vice versa. Biconvex functions and possible ap-
plications have been well studied in the optimi-
sation literature (Quesada and Grossmann, 1995;
4Note that other loss functions could be used here, such
as logistic loss for classification, or more generally bilinear
variations of Generalised Linear Models (Nelder and Wed-
derburn, 1972).
995
Pirsiavash et al, 2009). Their main advantage is
the ability to solve efficiently non-convex prob-
lems by a repeated application of two convex pro-
cesses, i.e., a form of coordinate ascent. In our
case, the bilinear technique makes it possible to
explore both word and user spaces, while main-
taining a modest training complexity.
Therefore, in our bilinear approach we divide
learning in two phases, where we learn word and
user weights respectively. For the first phase we
produce the term-scores matrix V ? Rn?m with
elements given by:
Vij =
p?
z=1
uzQijz. (4)
V contains weighted sums of term frequencies
over all users for the considered set of days. The
weights are held in u and are representative of
each user. The initial optimisation task is formu-
lated as:
{w?, ??} = argmin
w,?
?Vw + ? ? y?22
+ ?el (w, ?1, ?1) ,
(5)
where we aim to learn a sparse but consistent set
of weights w? for the terms of our vocabulary.
In the second phase, we are using w? to form
the user-scores matrix D ? Rn?p:
Dik =
m?
z=1
w?zQizk , (6)
which now contains weighted sums over all terms
for the same set of days. The optimisation task
becomes:
{u?, ??} = argmin
u,?
?Du + ? ? y?22
+ ?el (u, ?2, ?2) .
(7)
This process continues iteratively by inserting
the weights of the second phase back to phase one,
and so on until convergence. We cannot claim that
a global optimum will be reached, but biconvexity
guarantees that our global objective (Eq. 2) will
decrease in each step of this iterative process. In
the remainder of this paper, we refer to the method
described above as Bilinear Elastic Net (BEN).
3.2 Exploiting term-target or user-target
relationships
The previous model assumes that the response
variable y holds information about a single infer-
ence target. However, the task that we are ad-
dressing in this paper usually implies the exis-
tence of several targets, i.e., different political par-
ties or politicians. An important property, there-
fore, is the ability to perform multiple output re-
gression. A simple way of adapting the model to
the multiple output scenario is by framing a sep-
arate learning problem for each output, but tying
together some of the parameters. Here we con-
sider tying together the user weights u, to enforce
that the same set of users are relevant to all tasks,
while learning different term weights. Note that
the converse situation, where w?s are tied and u?s
are independent, can be formulated in an equiva-
lent manner.
Suppose that our target variable y ? R?n refers
now to ? political entities, y = [yT1yT2 ...yT?
]T; in
this formation the top n elements of y match to
the first political entity, the next n elements to the
second and so on. In the first phase of the bilin-
ear model, we would have to solve the following
optimisation task:
{w?, ??} = argmin
w,?
??
i=1
?Vwi + ?i ? yi?22
+
??
i=1
?el (wi, ?1, ?1) ,
(8)
where V is given by Eq. 4 and w? ? R?m de-
notes the vector of weights which can be sliced
into ? sub-vectors {w?1, ...,w??} each one repre-
senting a political entity. In the second phase,
sub-vectorsw?i are used to form the input matrices
Di, i ? {1, ..., ?} with elements given by Eq. 6.
The input matrix D? is formed by the vertical
concatenation of all Di user score matrices, i.e.,
D? =
[
DT1 ... DT?
]T, and the optimisation target is
equivalent to the one expressed in Eq. 7. Since
D? ? R?n?p, the user weight vector u? ? Rp and
thus, we are learning a single weight per user and
not one per political party as in the previous step.
The method described above allows learning
different term weights per response variable and
then binds them under a shared set of user weights.
As mentioned before, one could also try the oppo-
site (i.e., start by expanding the user space); both
those models can also be optimised in an itera-
tive process. However, our experiments revealed
that those approaches did not improve on the
performance of BEN. Still, this behaviour could
be problem-specific, i.e., learning different words
996
from a shared set of users (and the opposite) may
not be a good modelling practice for the domain of
politics. Nevertheless, this observation served as
a motivation for the method described in the next
section, where we extract a consistent set of words
and users that are weighted differently among the
considered political entities.
3.3 Multi-task learning with the `1/`2
regulariser
All previous models ? even when combining all
inference targets ? were not able to explore rela-
tionships across the different task domains; in our
case, a task domain is defined by a specific politi-
cal label or party. Ideally, we would like to make a
sparse selection of words and users but with a reg-
ulariser that promotes inter-task sharing of struc-
ture, so that many features may have a positive
influence towards one or more parties, but nega-
tive towards the remaining one(s). It is possible to
achieve this multi-task learning property by intro-
ducing a different set of regularisation constraints
in the optimisation function.
We perform multi-task learning using an exten-
sion of group LASSO (Yuan and Lin, 2006), a
method known as `1 /` 2 regularisation (Argyriou et
al., 2008; Liu et al, 2009). Group LASSO exploits
a predefined group structure on the feature space
and tries to achieve sparsity in the group-level, i.e.,
it does not perform feature selection (unlike the
elastic net), but group selection. The `1/`2 regu-
lariser extends this notion for a ? -dimensional re-
sponse variable. The global optimisation target is
now formulated as:
{W ?, U?,??} =
argmin
W,U,?
??
t=1
n?
i=1
(
uTtQiwt + ?t ? yti
)2
+ ?1
m?
j=1
?Wj?2 + ?2
p?
k=1
?Uk?2,
(9)
where the input matrix Qi is defined in the same
way as earlier, W = [w1 ... w? ] is the term weight
matrix (each wt refers to the t-th political entity
or task), equivalently U = [u1 ... u? ], Wj and Uj
denote the j-th rows of weight matrices W and
U respectively, and vector ? ? R? holds the bias
terms per task. In this optimisation process, we
aim to enforce sparsity in the feature space but in
a structured manner. Notice that we are now regu-
larising the `2,1 mixed norm ofW and U , which is
defined as the sum of the row `2-norms for those
matrices. As a result, we expect to encourage the
activation of a sparse set of features (correspond-
ing to the rows of W and U ), but with nonzero
weights across the ? tasks (Argyriou et al, 2008).
Consequently, we are performing filtering (many
users and words will have zero weights) and, at the
same time, assign weights of different magnitude
and sign on the selected features, something that
suits a political opinion mining application, where
pro-A often means anti-B.
Eq. 9 can be broken into two convex tasks (fol-
lowing the same notion as in Eqs. 5 and 7), where
we individually learn {W,?} and then {U,?};
each step of the process is a standard linear regres-
sion problem with an `1/`2 regulariser. Again, we
are able iterate this bilinear process and in each
step convexity is guaranteed. We refer to this
method as Bilinear Group `1/`2 (BGL).
4 Experiments
The proposed models are evaluated on Cuk and
Cau which have been introduced in Section 2. We
measure predictive performance, compare it to the
performance of several competitive baselines, and
provide a qualitative analysis of the parameters
learned by the models.
4.1 Data preprocessing
Basic preprocessing has been applied on the vo-
cabulary index of Cuk and Cau aiming to filter out
some of the word features and partially reduce
the dimensionality of the problem. Stop words
and web links were removed in both sets, together
with character sequences of length <4 and <3
for Cuk and Cau respectively.5 As the vocabulary
size of Cuk was significantly larger, for this data
set we have additionally merged Twitter hashtags
(i.e., words starting with ?#?) with their exact non
topic word match, where possible (by dropping the
?#? when the word existed in the index). After
performing the preprocessing routines described
above, the vocabulary sizes for Cuk and Cau were
set to 80,976 and 22,917 respectively.
4.2 Predictive accuracy
To evaluate the predictive accuracy of our meth-
ods, we have chosen to emulate a real-life scenario
5Most of the times those character sequences were not
valid words. This pattern was different in each language and
thus, a different filtering threshold was applied in each data
set.
997
2 4 6 8 10 12 14 16 18 20 22 24 26 28 300
0.4
0.8
1.2
1.6
2
2.4
Step 
 Global ObjectiveRMSE
Figure 2: Global objective function and RMSE on
a validation set for BEN in 15 iterations (30 steps)
of the model.
of voting intention prediction. The evaluation pro-
cess starts by using a fixed set of polls matching
to consecutive time points in the past for training
and validating the parameters of each model. Test-
ing is performed on the following ? (unseen) polls
of the data set. In the next step of the evaluation
process, the training/validation set is increased by
merging it with the previously used test set (?
polls), and testing is now performed on the next
? unseen polls. In our experiments, the number of
steps in this evaluation process is set to 10 and in
each step the size of the test set is set to ? = 5
polls. Hence, each model is tested on 50 unseen
and consecutive in time samples. The loss func-
tion in our evaluation is the standard Mean Square
Error (MSE), but to allow a better interpretation
of the results, we display its root (RMSE) in ta-
bles and figures.6
The parameters of each model (?i for BEN and
?i for BEN and BGL, i ? {1, 2}) are optimised
using a held-out validation set by performing grid
search. Note that it may be tempting to adapt the
regularisation parameters in each phase of the it-
erative training loop, however this would change
the global objective (see Eqs. 2 and 9) and thus
convergence will not be guaranteed. A key ques-
tion is how many iterations of training are required
to reach convergence. Figure 2 illustrates how the
BEN global objective function (Eq. 2) converges
during this iterative process and the model?s per-
formance on an unseen validation set. Notice that
there is a large performance improvement after the
first step (which alone is a linear solver), but over-
fitting occurs after step 11. Based on this result,
for subsequent experiments we run the training
process for two iterations (4 steps), and take the
6RMSE has the same metric units as the response variable.
CON LAB LBD ?
B? 2.272 1.663 1.136 1.69
Blast 2 2.074 1.095 1.723
LEN 3.845 2.912 2.445 3.067
BEN 1.939 1.644 1.136 1.573
BGL 1.785 1.595 1.054 1.478
Table 1: UK case study ? Average RMSEs rep-
resenting the error of the inferred voting intention
percentage for the 10-step validation process; ?
denotes the mean RMSE across the three political
parties for each baseline or inference method.
SPO? O?VP FPO? GRU? ?
B? 1.535 1.373 3.3 1.197 1.851
Blast 1.148 1.556 1.639 1.536 1.47
LEN 1.291 1.286 2.039 1.152 1.442
BEN 1.392 1.31 2.89 1.205 1.699
BGL 1.619 1.005 1.757 1.374 1.439
Table 2: Austrian case study ? Average RMSEs
for the 10-step validation process.
best performing model on the held-out validation
set.
We compare the performance of our methods
with three baselines. The first makes a constant
prediction of the mean value of the response vari-
able y in the training set (B?); the second predicts
the last value of y (Blast); and the third baseline
(LEN) is a linear regression over the terms using
elastic net regularisation. Recalling that each test
set is made of 5 polls, Blast should be considered
as a hard baseline to beat7 given that voting inten-
tions tend to have a smooth behaviour. Moreover,
improving on LEN partly justifies the usefulness
of a bilinear approach compared to a linear one.
Performance results comparing inferred voting
intention percentages and polls for Cuk and Cau are
presented in Tables 1 and 2 respectively. For the
UK case study, both BEN and BGL are able to beat
all baselines in average performance across all par-
ties. However in the Austrian case study, LEN
performs better that BEN, something that could be
justified by the fact that the users in Cau were se-
lected by domain experts, and consequently there
was not much gain to be had by filtering them fur-
ther. Nevertheless, the difference in performance
was rather small (approx. 0.26% error) and the in-
7The last response value could be easily included as a fea-
ture in the model, and would likely improve predictive perfor-
mance.
998
5 10 15 20 25 30 35 40 450
5
10
15
20
25
30
35
40
Vot
ing 
Inte
ntio
n %
Time 
 
CONLABLIB
(a) Ground Truth (polls)
5 10 15 20 25 30 35 40 450
5
10
15
20
25
30
35
40
Vot
ing 
Inte
ntio
n %
Time 
 
CONLABLIB
(b) BEN
5 10 15 20 25 30 35 40 450
5
10
15
20
25
30
35
40
Vot
ing 
Inte
ntio
n %
Time 
 
CONLABLIB
(c) BGL
Figure 3: UK case study ? Voting intention infer-
ence results (50 polls, 3 parties). Sub-figure 3a is
a plot of ground truth as presented in voting inten-
tion polls (Fig. 1a).
ferences of LEN and BEN followed a very similar
pattern (?? = .94 with p < 10?10).8 Multi-task
learning (BGL) delivered the best inference per-
formance in both case studies, which was on aver-
age smaller than 1.48% (RMSE).
Inferences for both BEN and BGL have been
plotted on Figures 3 and 4. They are presented as
continuous lines of 50 inferred points (per party)
which are created by concatenating the inferences
8Pearson?s linear correlation averaged across the four
Austrian parties.
5 10 15 20 25 30 35 40 450
5
10
15
20
25
30
Vot
ing 
Inte
ntio
n %
Time 
 
SP??VPFP?GR?
(a) Ground Truth (polls)
5 10 15 20 25 30 35 40 450
5
10
15
20
25
30
Vot
ing 
Inte
ntio
n %
Time 
 
SP??VPFP?GR?
(b) BEN
5 10 15 20 25 30 35 40 450
5
10
15
20
25
30
Vot
ing 
Inte
ntio
n %
Time 
 
SP??VPFP?GR?
(c) BGL
Figure 4: Austrian case study ? Voting intention
inference results (50 polls, 4 parties). Sub-figure
4a is a plot of ground truth as presented in voting
intention polls (Fig. 1b).
on all test sets.9 For the UK case study, one may
observe that BEN (Fig. 3b) cannot register any
change ? with the exception of one test point ? in
the leading party fight (CON versus LAB); BGL
(Fig. 3c) performs much better in that aspect. In
the Austrian case study this characteristic becomes
more obvious. BEN (Fig. 4b) consistently predicts
the wrong ranking of O?VP and FPO?, whereas BGL
(Fig. 4c) does much better. Most importantly, a
9Voting intention polls were plotted separately to allow a
better presentation.
999
Party Tweet Score Author
CON PM in friendly chat with top EU mate, Sweden?s Fredrik Reinfeldt, before family photo 1.334 Journalist
Have Liberal Democrats broken electoral rules? Blog on Labour complaint to cabinet
secretary
?0.991 Journalist
LAB Blog Post Liverpool: City of Radicals Website now Live <link> #liverpool #art 1.954 Art Fanzine
I am so pleased to hear Paul Savage who worked for the Labour group has been Ap-
pointed the Marketing manager for the baths hall GREAT NEWS
?0.552 Politician
(Labour)
LBD RT @user: Must be awful for TV bosses to keep getting knocked back by all the
women they ask to host election night (via @user)
0.874 LibDem MP
Blog Post Liverpool: City of Radicals 2011 ? More Details Announced #liverpool
#art
?0.521 Art Fanzine
SPO? Inflationsrate in O?. im Juli leicht gesunken: von 2,2 auf 2,1%. Teurer wurde Wohnen,
Wasser, Energie.
Translation: Inflation rate in Austria slightly down in July from 2,2 to 2,1%. Accom-
modation, Water, Energy more expensive.
0.745 Journalist
Hans Rauscher zu Felix #Baumgartner ?A klaner Hitler? <link>
Translation: Hans Rauscher on Felix #Baumgartner ?A little Hitler? <link>
?1.711 Journalist
O?VP #IchPirat setze mich dafu?r ein, dass eine gro?e Koalition mathematisch verhindert
wird! 1.Geige: #Gruene + #FPOe + #OeVP
Translation: #IPirate am committed to prevent a grand coalition mathematically!
Calling the tune: #Greens + #FPO + #OVP
4.953 User
kann das buch ?res publica? von johannes #voggenhuber wirklich empfehlen! so zum
nachdenken und so... #europa #demokratie
Translation: can really recommend the book ?res publica? by johannes
#voggenhuber! Food for thought and so on #europe #democracy
?2.323 User
FPO? Neue Kampagne der #Krone zur #Wehrpflicht: ?GIB BELLO EINE STIMME!?
Translation: New campaign by the #Krone on #Conscription: ?GIVE WOOFY A
VOICE!?
7.44 Political satire
Kampagne der Wiener SPO? ?zum Zusammenleben? spielt Rechtspopulisten in die
Ha?nde <link>
Translation: Campaign of the Viennese SPO? on ?Living together? plays right into the
hands of right-wing populists <link>
?3.44 Human Rights
GRU? Protestsong gegen die Abschaffung des Bachelor-Studiums Internationale Entwick-
lung: <link> #IEbleibt #unibrennt #uniwut
Translation: Protest songs against the closing-down of the bachelor course of Inter-
national Development: <link> #IDremains #uniburns #unirage
1.45 Student Union
Pilz ?ich will in dieser Republik weder kriminelle Asylwerber, noch kriminelle orange
Politiker? - BZO?-Abschiebung ok, aber wohin? #amPunkt
Translation: Pilz ?i want neither criminal asylum-seekers, nor criminal orange politi-
cians in this republic? - BZO?-Deportation OK, but where? #amPunkt
?2.172 User
Table 3: Examples of tweets amongst the ones with top positive and negative scores per party for both
Cuk and Cau data sets (tweets in Austrian have been translated in English as well). Notice that weight
magnitude may differ per case study and party as they are based on the range of the response variable
and the total number of selected features.
general observation is that BEN?s predictions are
smooth and do not vary significantly with time.
This might be a result of overfitting the model
to a single response variable which usually has
a smooth behaviour. On the contrary, the multi-
task learning property of BGL reduces this type of
overfitting providing more statistical evidence for
the terms and users and thus, yielding not only a
better inference performance, but also a more ac-
curate model.
4.3 Qualitative Analysis
In this section, we refer to features that have been
selected and weighted as significant by our bi-
linear learning functions. Based on the weights
for the word and the user spaces that we re-
trieve after the application of BGL in the last step
of the evaluation process (see the previous sec-
tion), we compute a score (weighted sum) for each
tweet in our training data sets for both Cuk and
Cau. Table 3 shows examples of interesting tweets
amongst the top weighted ones (positively as well
as negatively) per party. Together with their text
(anonymised for privacy reasons) and scores, we
also provide an attribute for the author (if present).
In the displayed tweets for the UK study, the only
possible outlier is the ?Art Fanzine?; still, it seems
to register a consistent behaviour (positive towards
1000
LAB, negative towards LBD) and, of course, hid-
den, indirect relationships may exist between po-
litical opinion and art. The Austrian case study
revealed even more interesting tweets since train-
ing was conducted on data from a very active pre-
election period (we made an effort to translate
those tweets in English language as well). For
a better interpretation of the presented tweets, it
may be useful to know that ?Johannes Voggen-
huber? (who receives a positive comment for his
book) and ?Peter Pilz? (whose comment is ques-
tioned) are members of GRU?, ?Krone? (or Kro-
nen Zeitung) is the major newspaper in Austria10
and that FPO? is labelled as a far right party, some-
thing that may cause various reactions from ?Hu-
man Rights? organisations.
5 Related Work
The topic of political opinion mining from So-
cial Media has been the focus of various recent
research works. Several papers have presented
methods that aim to predict the result of an elec-
tion (Tumasjan et al, 2010; Bermingham and
Smeaton, 2011) or to model voting intention and
other kinds of socio-political polls (O?Connor et
al., 2010; Lampos, 2012). Their common fea-
ture is a methodology based on a meta-analysis
of word frequencies using off-the-shelf sentiment
tools such as LIWC (Pennebaker et al, 2007)
or Senti-WordNet (Esuli and Sebastiani, 2006).
Moreover, the proposed techniques tend to incor-
porate posting volume figures as well as hand-
crafted lists of words relevant to the task (e.g.,
names of politicians or parties) in order to filter
the content successfully.
Such papers have been criticised as their meth-
ods do not generalise when applied on different
data sets. According to the work in (Gayo-Avello
et al, 2011), the methods presented in (Tumasjan
et al, 2010) and (O?Connor et al, 2010) failed to
predict the result of US congressional elections in
2009. We disagree with the arguments support-
ing the statement ?you cannot predict elections
with Twitter? (Gayo-Avello, 2012), as many times
in the past actual voting intention polls have also
failed to predict election outcomes, but we agree
that most methods that have been proposed so far
were not entirely generic. It is a fact that the
10?Accused of abusing its near monopoly to manipulate
public opinion in Austria?, Wikipedia, 19/02/2013, http:
//en.wikipedia.org/wiki/Kronen_Zeitung.
majority of sentiment analysis tools are English-
specific (or even American English) and, most
importantly, political word lists (or ontologies)
change in time, per country and per party; hence,
generalisable methods should make an effort to
limit reliance from such tools.
Furthermore, our work ? indirectly ? meets the
guidelines proposed in (Metaxas et al, 2011) as
we have developed a framework of ?well-defined?
algorithms that are ?Social Web aware? (since the
bilinear approach aims to improve noise filtering)
and that have been tested on two evaluation sce-
narios with distinct characteristics.
6 Conclusions and Future Work
We have presented a novel method for text regres-
sion that exploits both word and user spaces by
solving a bilinear optimisation task, and an ex-
tension that applies multi-task learning for multi-
output inference. Our approach performs feature
selection ? hence, noise filtering ? on large-scale
user-generated inputs automatically, generalises
across two languages without manual adaptations
and delivers some significant improvements over
strong performance baselines (< 1.5% error when
predicting polls). The application domain in this
paper was politics, though the presented methods
are generic and could be easily applied on various
other domains, such as health or finance.
Future work may investigate further modelling
improvements achieved by applying different reg-
ularisation functions as well as the adaptation of
the presented models to classification problems.
Finally, in the application level, we aim at an in-
depth analysis of patterns and characteristics in the
extracted sets of features by collaborating with do-
main experts (e.g., political analysts).
Acknowledgments
This work was funded by the TrendMiner project
(EU-FP7-ICT n.287863). All authors would like
to thank the political analysts (and especially Paul
Ringler) from SORA11 for their useful insights on
politics in Austria.
11SORA ? Institute for Social Research and Consulting,
http://www.sora.at.
1001
References
Faiz A Al-Khayyal and James E Falk. 1983. Jointly
Constrained Biconvex Programming. Mathematics
of Operations Research, 8(2):273?286.
Andreas Argyriou, Theodoros Evgeniou, and Massi-
miliano Pontil. 2008. Convex multi-task feature
learning. Machine Learning, 73(3):243?272, Jan-
uary.
Adam Bermingham and Alan F Smeaton. 2011. On
using Twitter to monitor political sentiment and pre-
dict election results. In Proceedings of the Workshop
on Sentiment Analysis where AI meets Psychology
(SAAIP 2011), pages 2?10, November.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8, March.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiWordNet: A publicly available lexical resource
for opinion mining. In Proceeding of the 5th
Conference on Language Resources and Evaluation
(LREC), pages 417?422.
Daniel Gayo-Avello, Panagiotis T Metaxas, and Eni
Mustafaraj. 2011. Limits of Electoral Predictions
using Twitter. In Proceedings of the Fifth Interna-
tional AAAI Conference on Weblogs and Social Me-
dia (ICWSM), pages 490?493.
Daniel Gayo-Avello. 2012. No, You Cannot Predict
Elections with Twitter. IEEE Internet Computing,
16(6):91?94, November.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning.
Springer Series in Statistics. Springer.
Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. Journal of the Ameri-
can Society for Information Science and Technology,
60(11):2169?2188.
Vasileios Lampos and Nello Cristianini. 2010. Track-
ing the flu pandemic by monitoring the Social Web.
In 2nd IAPR Workshop on Cognitive Information
Processing, pages 411?416. IEEE Press.
Vasileios Lampos and Nello Cristianini. 2012. Now-
casting Events from the Social Web with Statistical
Learning. ACM Transactions on Intelligent Systems
and Technology, 3(4):1?22, September.
Vasileios Lampos, Tijl De Bie, and Nello Cristianini.
2010. Flu Detector - Tracking Epidemics on Twitter.
In Proceedings of European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML PKDD), pages 599?
602. Springer.
Vasileios Lampos. 2012. On voting intentions infer-
ence from Twitter content: a case study on UK 2010
General Election. CoRR, April.
Thomas Lansdall-Welfare, Vasileios Lampos, and
Nello Cristianini. 2012. Effects of the recession
on public mood in the UK. In Proceedings of the
21st international conference companion on World
Wide Web, WWW ?12 Companion, pages 1221?
1226. ACM.
Jun Liu, Shuiwang Ji, and Jieping Ye. 2009. Multi-
task feature learning via efficient l2,1-norm mini-
mization. pages 339?348, June.
Panagiotis T Metaxas, Eni Mustafaraj, and Daniel
Gayo-Avello. 2011. How (Not) To Predict Elec-
tions. In IEEE 3rd International Conference on So-
cial Computing (SocialCom), pages 165 ? 171. IEEE
Press.
John A Nelder and Robert W M Wedderburn. 1972.
Generalized Linear Models. Journal of the Royal
Statistical Society - Series A (General), 135(3):370.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media, pages 122?129. AAAI Press.
Michael J Paul and Mark Dredze. 2011. You Are What
You Tweet: Analyzing Twitter for Public Health.
Proceedings of the 5th International AAAI Confer-
ence on Weblogs and Social Media, pages 265?272.
James W Pennebaker, Cindy K Chung, Molly Ire-
land, Amy Gonzales, and Roger J Booth. 2007.
The Development and Psychometric Properties of
LIWC2007. Technical report, Universities of Texas
at Austin & University of Auckland, New Zealand.
Hamed Pirsiavash, Deva Ramanan, and Charless
Fowlkes. 2009. Bilinear classifiers for visual recog-
nition. In Advances in Neural Information Process-
ing Systems, volume 22, pages 1482?1490.
Daniel Preot?iuc-Pietro, Sina Samangooei, Trevor
Cohn, Nicholas Gibbins, and Mahesan Niranjan.
2012. Trendminer: An Architecture for Real Time
Analysis of Social Media Text. In Sixth Interna-
tional AAAI Conference on Weblogs and Social Me-
dia, pages 38?42. AAAI Press, July.
Ignacio Quesada and Ignacio E Grossmann. 1995. A
global optimization algorithm for linear fractional
and bilinear programs. Journal of Global Optimiza-
tion, 6(1):39?76, January.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World Wide
Web (WWW), pages 851?860. ACM.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society - Series B (Methodological), 58(1):267?288.
1002
Andranik Tumasjan, Timm O Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting
elections with Twitter: What 140 characters reveal
about political sentiment. In Proceedings of the 4th
International AAAI Conference on Weblogs and So-
cial Media, pages 178?185. AAAI.
Ming Yuan and Yi Lin. 2006. Model selection and es-
timation in regression with grouped variables. Jour-
nal of the Royal Statistical Society - Series B: Statis-
tical Methodology, 68(1):49?67.
Peng Zhao and Bin Yu. 2006. On model selection
consistency of Lasso. Journal of Machine Learning
Research, 7(11):2541?2563.
Hui Zou and Trevor Hastie. 2005. Regularization
and variable selection via the elastic net. Journal
of the Royal Statistical Society: Series B (Statistical
Methodology), 67(2):301?320, April.
1003
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543?548,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reducing Annotation Effort for Quality Estimation via Active Learning
Daniel Beck and Lucia Specia and Trevor Cohn
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,l.specia,t.cohn}@sheffield.ac.uk
Abstract
Quality estimation models provide feed-
back on the quality of machine translated
texts. They are usually trained on human-
annotated datasets, which are very costly
due to its task-specific nature. We in-
vestigate active learning techniques to re-
duce the size of these datasets and thus
annotation effort. Experiments on a num-
ber of datasets show that with as little as
25% of the training instances it is possible
to obtain similar or superior performance
compared to that of the complete datasets.
In other words, our active learning query
strategies can not only reduce annotation
effort but can also result in better quality
predictors.
1 Introduction
The purpose of machine translation (MT) qual-
ity estimation (QE) is to provide a quality pre-
diction for new, unseen machine translated texts,
without relying on reference translations (Blatz et
al., 2004; Specia et al, 2009; Callison-Burch et
al., 2012). This task is usually addressed with
machine learning models trained on datasets com-
posed of source sentences, their machine transla-
tions, and a quality label assigned by humans. A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
Since quality scores for the training of QE mod-
els are given by human experts, the annotation pro-
cess is costly and subject to inconsistencies due to
the subjectivity of the task. To avoid inconsisten-
cies because of disagreements among annotators,
it is often recommended that a QE model is trained
for each translator, based on labels given by such
a translator (Specia, 2011). This further increases
the annotation costs because different datasets are
needed for different tasks. Therefore, strategies to
reduce the demand for annotated data are needed.
Such strategies can also bring the possibility of se-
lecting data that is less prone to inconsistent anno-
tations, resulting in more robust and accurate pre-
dictions.
In this paper we investigate Active Learning
(AL) techniques to reduce the size of the dataset
while keeping the performance of the resulting
QE models. AL provides methods to select in-
formative data points from a large pool which,
if labelled, can potentially improve the perfor-
mance of a machine learning algorithm (Settles,
2010). The rationale behind these methods is to
help the learning algorithm achieve satisfactory re-
sults from only on a subset of the available data,
thus incurring less annotation effort.
2 Related Work
Most research work on QE for machine transla-
tion is focused on feature engineering and feature
selection, with some recent work on devising more
reliable and less subjective quality labels. Blatz et
al. (2004) present the first comprehensive study on
QE for MT: 91 features were proposed and used
to train predictors based on an automatic metric
(e.g. NIST (Doddington, 2002)) as the quality la-
bel. Quirk (2004) showed that small datasets man-
ually annotated by humans for quality can result
in models that outperform those trained on much
larger, automatically labelled sets.
Since quality labels are subjective to the anno-
tators? judgements, Specia and Farzindar (2010)
evaluated the performance of QE models using
HTER (Snover et al, 2006) as the quality score,
i.e., the edit distance between the MT output and
its post-edited version. Specia (2011) compared
the performance of models based on labels for
543
post-editing effort, post-editing time, and HTER.
In terms of learning algorithms, by and large
most approaches use Support Vector Machines,
particularly regression-based approaches. For an
overview on various feature sets and machine
learning algorithms, we refer the reader to a re-
cent shared task on the topic (Callison-Burch et
al., 2012).
Previous work use supervised learning methods
(?passive learning? following the AL terminol-
ogy) to train QE models. On the other hand, AL
has been successfully used in a number of natural
language applications such as text classification
(Lewis and Gale, 1994), named entity recognition
(Vlachos, 2006) and parsing (Baldridge and Os-
borne, 2004). See Olsson (2009) for an overview
on AL for natural language processing as well as
a comprehensive list of previous work.
3 Experimental Settings
3.1 Datasets
We perform experiments using four MT datasets
manually annotated for quality:
English-Spanish (en-es): 2, 254 sentences
translated by Moses (Koehn et al, 2007), as pro-
vided by the WMT12 Quality Estimation shared
task (Callison-Burch et al, 2012). Effort scores
range from 1 (too bad to be post-edited) to 5 (no
post-editing needed). Three expert post-editors
evaluated each sentence and the final score was
obtained by a weighted average between the three
scores. We use the default split given in the shared
task: 1, 832 sentences for training and 432 for
test.
French-English (fr-en): 2, 525 sentences trans-
lated by Moses as provided in Specia (2011), an-
notated by a single translator. Human labels in-
dicate post-editing effort ranging from 1 (too bad
to be post-edited) to 4 (little or no post-editing
needed). We use a random split of 90% sentences
for training and 10% for test.
Arabic-English (ar-en): 2, 585 sentences trans-
lated by two state-of-the-art SMT systems (de-
noted ar-en-1 and ar-en-2), as provided in (Specia
et al, 2011). A random split of 90% sentences for
training and 10% for test is used. Human labels in-
dicate the adequacy of the translation ranging from
1 (completely inadequate) to 4 (adequate). These
datasets were annotated by two expert translators.
3.2 Query Methods
The core of an AL setting is how the learner will
gather new instances to add to its training data. In
our setting, we use a pool-based strategy, where
the learner queries an instance pool and selects
the best instance according to an informativeness
measure. The learner then asks an ?oracle? (in this
case, the human expert) for the true label of the in-
stance and adds it to the training data.
Query methods use different criteria to predict
how informative an instance is. We experiment
with two of them: Uncertainty Sampling (US)
(Lewis and Gale, 1994) and Information Density
(ID) (Settles and Craven, 2008). In the following,
we denote M(x) the query score with respect to
method M .
According to the US method, the learner selects
the instance that has the highest labelling variance
according to its model:
US(x) = V ar(y|x)
The ID method considers that more dense regions
of the query space bring more useful information,
leveraging the instance uncertainty and its similar-
ity to all the other instances in the pool:
ID(x) = V ar(y|x)?
(
1
U
U?
u=1
sim(x, x(u))
)?
The ? parameter controls the relative importance
of the density term. In our experiments, we set it
to 1, giving equal weights to variance and density.
The U term is the number of instances in the query
pool. As similarity measure sim(x, x(u)), we use
the cosine distance between the feature vectors.
With each method, we choose the instance that
maximises its respective equation.
3.3 Experiments
To build our QE models, we extracted the 17 fea-
tures used by the baseline approach in the WMT12
QE shared task.1 These features were used with a
Support Vector Regressor (SVR) with radial basis
function and fixed hyperparameters (C=5, ?=0.01,
=0.5), using the Scikit-learn toolkit (Pedregosa
et al, 2011). For each dataset and each query
method, we performed 20 active learning simu-
lation experiments and averaged the results. We
1We refer the reader to (Callison-Burch et al, 2012) for
a detailed description of the feature set, but this was a very
strong baseline, with only five out of 19 participating systems
outperforming it.
544
started with 50 randomly selected sentences from
the training set and used all the remaining train-
ing sentences as our query pool, adding one new
sentence to the training set at each iteration.
Results were evaluated by measuring Mean Ab-
solute Error (MAE) scores on the test set. We
also performed an ?oracle? experiment: at each it-
eration, it selects the instance that minimises the
MAE on the test set. The oracle results give an
upper bound in performance for each test set.
Since an SVR does not supply variance values
for its predictions, we employ a technique known
as query-by-bagging (Abe and Mamitsuka, 1998).
The idea is to build an ensemble of N SVRs
trained on sub-samples of the training data. When
selecting a new query, the ensemble is able to re-
turnN predictions for each instance, from where a
variance value can be inferred. We used 20 SVRs
as our ensemble and 20 as the size of each training
sub-sample.2 The variance values are then used
as-is in the case of US strategy and combined with
query densities in case of the ID strategy.
4 Results and Discussion
Figure 1 shows the learning curves for all query
methods and all datasets. The ?random? curves
are our baseline since they are equivalent to pas-
sive learning (with various numbers of instances).
We first evaluated our methods in terms of how
many instances they needed to achieve 99% of the
MAE score on the full dataset. For three datasets,
the AL methods significantly outperformed the
random selection baseline, while no improvement
was observed on the ar-en-1 dataset. Results are
summarised in Table 1.
The learning curves in Figure 1 show an inter-
esting behaviour for most AL methods: some of
them were able to yield lower MAE scores than
models trained on the full dataset. This is par-
ticularly interesting in the fr-en case, where both
methods were able to obtain better scores using
only ?25% of the available instances, with the
US method resulting in 0.03 improvement. The
random selection strategy performs surprisingly
well (for some datasets it is better than the AL
strategies with certain number of instances), pro-
viding extra evidence that much smaller annotated
2We also tried sub-samples with the same size of the cur-
rent training data but this had a large impact in the query
methods running time while not yielding significantly better
results.
Figure 1: Learning curves for different query se-
lection strategies in the four datasets. The horizon-
tal axis shows the number of instances in the train-
ing set and the vertical axis shows MAE scores.
545
US ID Random Full dataset#instances MAE #instances MAE #instances MAE
en-es 959 (52%) 0.6818 549 (30%) 0.6816 1079 (59%) 0.6818 0.6750
fr-en 79 (3%) 0.5072 134 (6%) 0.5077 325 (14%) 0.5070 0.5027
ar-en-1 51 (2%) 0.6067 51 (2%) 0.6052 51 (2%) 0.6061 0.6058
ar-en-2 209 (9%) 0.6288 148 (6%) 0.6289 532 (23%) 0.6288 0.6290
Table 1: Number (proportion) of instances needed to achieve 99% of the performance of the full dataset.
Bold-faced values indicate the best performing datasets.
Best MAE US Best MAE ID Full dataset#instances MAE US MAE Random #instances MAE ID MAE Random
en-es 1832 (100%) 0.6750 0.6750 1122 (61%) 0.6722 0.6807 0.6750
fr-en 559 (25%) 0.4708 0.5010 582 (26%) 0.4843 0.5008 0.5027
ar-en-1 610 (26%) 0.5956 0.6042 351 (15%) 0.5987 0.6102 0.6058
ar-en-2 1782 (77%) 0.6212 0.6242 190 (8%) 0.6170 0.6357 0.6227
Table 2: Best MAE scores obtained in the AL experiments. For each method, the first column shows the
number (proportion) of instances used to obtain the best MAE, the second column shows the MAE score
obtained and the third column shows the MAE score for random instance selection at the same number
of instances. The last column shows the MAE obtained using the full dataset. Best scores are shown in
bold and are significantly better (paired t-test, p < 0.05) than both their randomly selected counterparts
and the full dataset MAE.
datasets than those used currently can be sufficient
for machine translation QE.
The best MAE scores achieved for each dataset
are shown in Table 2. The figures were tested for
significance using pairwise t-test with 95% confi-
dence,3 with bold-faced values in the table indicat-
ing significantly better results.
The lower bounds in MAE given by the ora-
cle curves show that AL methods can indeed im-
prove the performance of QE models: an ideal
query method would achieve a very large improve-
ment in MAE using fewer than 200 instances in all
datasets. The fact that different datasets present
similar oracle curves suggests that this is not re-
lated for a specific dataset but actually a common
behaviour in QE. Although some of this gain in
MAE may be due to overfitting to the test set, the
results obtained with the fr-en and ar-en-2 datasets
are very promising, and therefore we believe that
it is possible to use AL to improve QE results in
other cases, as long as more effective query tech-
niques are designed.
5 Further analysis on the oracle
behaviour
By analysing the oracle curves we can observe an-
other interesting phenomenon which is the rapid
increase in error when reaching the last ?200 in-
stances of the training data. A possible explana-
3We took the average of the MAE scores obtained from
the 20 runs with each query method for that.
tion for this behaviour is the existence of erro-
neous, inconsistent or contradictory labels in the
datasets. Quality annotation is a subjective task by
nature, and it is thus subject to noise, e.g., due to
misinterpretations or disagreements. Our hypothe-
sis is that these last sentences are the most difficult
to annotate and therefore more prone to disagree-
ments.
To investigate this phenomenon, we performed
an additional experiment with the en-es dataset,
the only dataset for which multiple annotations
are available (from three judges). We measure the
Kappa agreement index (Cohen, 1960) between all
pairs of judges in the subset containing the first
300 instances (the 50 initial random instances plus
250 instances chosen by the oracle). We then mea-
sured Kappa in windows of 300 instances until the
last instance of the training set is selected by the
oracle method. We also measure variances in sen-
tence length using windows of 300 instances. The
idea of this experiment is to test whether sentences
that are more difficult to annotate (because of their
length or subjectivity, generating more disagree-
ment between the judges) add noise to the dataset.
The resulting Kappa curves are shown in Fig-
ure 2: the agreement between judges is high for
the initial set of sentences selected, tends to de-
crease until it reaches ?1000 instances, and then
starts to increase again. Figure 3 shows the results
for source sentence length, which follow the same
trend (in a reversed manner). Contrary to our hy-
546
Figure 2: Kappa curves for the en-es dataset. The
horizontal axis shows the number of instances and
the vertical axis shows the kappa values. Each
point in the curves shows the kappa index for a
window containing the last 300 sentences chosen
by the oracle.
pothesis, these results suggest that the most diffi-
cult sentences chosen by the oracle are those in the
middle range instead of the last ones. If we com-
pare this trend against the oracle curve in Figure 1,
we can see that those middle instances are the ones
that do not change the performance of the oracle.
The resulting trends are interesting because they
give evidence that sentences that are difficult to an-
notate do not contribute much to QE performance
(although not hurting it either). However, they do
not confirm our hypothesis about the oracle be-
haviour. Another possible source of disagreement
is the feature set: the features may not be discrim-
inative enough to distinguish among different in-
stances, i.e., instances with very similar features
but different labels might be genuinely different,
but the current features are not sufficient to indi-
cate that. In future work we plan to further inves-
tigate this by hypothesis by using other feature sets
and analysing their behaviour.
6 Conclusions and Future Work
We have presented the first known experiments us-
ing active learning for the task of estimating ma-
chine translation quality. The results are promis-
ing: we were able to reduce the number of in-
stances needed to train the models in three of the
four datasets. In addition, in some of the datasets
active learning yielded significantly better models
using only a small subset of the training instances.
Figure 3: Average source and target sentence
lengths for the en-es dataset. The horizontal axis
shows the number of instances and the vertical
axis shows the length values. Each point in the
curves shows the average length for a window con-
taining the last 300 sentences chosen by the oracle.
The oracle results give evidence that it is possi-
ble to go beyond these encouraging results by em-
ploying better selection strategies in active learn-
ing. In future work we will investigate more
advanced query techniques that consider features
other than variance and density of the data points.
We also plan to further investigate the behaviour
of the oracle curves using not only different fea-
ture sets but also different quality scores such as
HTER and post-editing time. We believe that a
better understanding of this behaviour can guide
further developments not only for instance selec-
tion techniques but also for the design of better
quality features and quality annotation schemes.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Lucia Specia).
References
Naoki Abe and Hiroshi Mamitsuka. 1998. Query
learning strategies using boosting and bagging. In
Proceedings of the Fifteenth International Confer-
ence on Machine Learning, pages 1?9.
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Pro-
ceedings of EMNLP, pages 9?16.
547
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and psychological
measurement, 20(1):37?46.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, pages 128?132.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
David D. Lewis and Willian A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 1?
10.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical report.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and E?douard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC, pages 825?828.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia and Atefeh Farzindar. 2010. Estimating
machine translation post-editing effort with HTER.
In Proceedings of AMTA Workshop Bringing MT to
the User: MT Research and the Translation Indus-
try.
Lucia Specia, M Turchi, Zhuoran Wang, and J Shawe-
Taylor. 2009. Improving the confidence of machine
translation quality estimates. In Proceedings of MT
Summit XII.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine translation
adequacy. In Proceedings of MT Summit XIII.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
Andreas Vlachos. 2006. Active annotation. In Pro-
ceedings of the Workshop on Adaptive Text Extrac-
tion and Mining at EACL.
548
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79?84,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
QuEst - A translation quality estimation framework
Lucia Specia?, Kashif Shah?, Jose G. C. de Souza? and Trevor Cohn?
?Department of Computer Science
University of Sheffield, UK
{l.specia,kashif.shah,t.cohn}@sheffield.ac.uk
?Fondazione Bruno Kessler
University of Trento, Italy
desouza@fbk.eu
Abstract
We describe QUEST, an open source
framework for machine translation quality
estimation. The framework allows the ex-
traction of several quality indicators from
source segments, their translations, exter-
nal resources (corpora, language models,
topic models, etc.), as well as language
tools (parsers, part-of-speech tags, etc.). It
also provides machine learning algorithms
to build quality estimation models. We
benchmark the framework on a number of
datasets and discuss the efficacy of fea-
tures and algorithms.
1 Introduction
As Machine Translation (MT) systems become
widely adopted both for gisting purposes and to
produce professional quality translations, auto-
matic methods are needed for predicting the qual-
ity of a translated segment. This is referred to as
Quality Estimation (QE). Different from standard
MT evaluation metrics, QE metrics do not have
access to reference (human) translations; they are
aimed at MT systems in use. QE has a number of
applications, including:
? Deciding which segments need revision by a
translator (quality assurance);
? Deciding whether a reader gets a reliable gist
of the text;
? Estimating how much effort it will be needed
to post-edit a segment;
? Selecting among alternative translations pro-
duced by different MT systems;
? Deciding whether the translation can be used
for self-training of MT systems.
Work in QE for MT started in the early 2000?s,
inspired by the confidence scores used in Speech
Recognition: mostly the estimation of word pos-
terior probabilities. Back then it was called confi-
dence estimation, which we believe is a narrower
term. A 6-week workshop on the topic at John
Hopkins University in 2003 (Blatz et al, 2004)
had as goal to estimate automatic metrics such as
BLEU (Papineni et al, 2002) and WER. These
metrics are difficult to interpret, particularly at the
sentence-level, and results of their very many trials
proved unsuccessful. The overall quality of MT
was considerably lower at the time, and therefore
pinpointing the very few good quality segments
was a hard problem. No software nor datasets
were made available after the workshop.
A new surge of interest in the field started re-
cently, motivated by the widespread used of MT
systems in the translation industry, as a conse-
quence of better translation quality, more user-
friendly tools, and higher demand for translation.
In order to make MT maximally useful in this
scenario, a quantification of the quality of trans-
lated segments similar to ?fuzzy match scores?
from translation memory systems is needed. QE
work addresses this problem by using more com-
plex metrics that go beyond matching the source
segment with previously translated data. QE can
also be useful for end-users reading translations
for gisting, particularly those who cannot read the
source language.
QE nowadays focuses on estimating more inter-
pretable metrics. ?Quality? is defined according to
the application: post-editing, gisting, etc. A num-
ber of positive results have been reported. Exam-
ples include improving post-editing efficiency by
filtering out low quality segments which would re-
quire more effort or time to correct than translating
from scratch (Specia et al, 2009; Specia, 2011),
selecting high quality segments to be published as
they are, without post-editing (Soricut and Echi-
habi, 2010), selecting a translation from either
an MT system or a translation memory for post-
editing (He et al, 2010), selecting the best trans-
lation from multiple MT systems (Specia et al,
79
2010), and highlighting sub-segments that need re-
vision (Bach et al, 2011).
QE is generally addressed as a supervised ma-
chine learning task using a variety of algorithms to
induce models from examples of translations de-
scribed through a number of features and anno-
tated for quality. For an overview of various al-
gorithms and features we refer the reader to the
WMT12 shared task on QE (Callison-Burch et
al., 2012). Most of the research work lies on
deciding which aspects of quality are more rel-
evant for a given task and designing feature ex-
tractors for them. While simple features such as
counts of tokens and language model scores can be
easily extracted, feature engineering for more ad-
vanced and useful information can be quite labour-
intensive. Different language pairs or optimisation
against specific quality scores (e.g., post-editing
time vs translation adequacy) can benefit from
very different feature sets.
QUEST, our framework for quality estimation,
provides a wide range of feature extractors from
source and translation texts and external resources
and tools (Section 2). These go from simple,
language-independent features, to advanced, lin-
guistically motivated features. They include fea-
tures that rely on information from the MT sys-
tem that generated the translations, and features
that are oblivious to the way translations were
produced (Section 2.1). In addition, by inte-
grating a well-known machine learning toolkit,
scikit-learn,1 and algorithms that are known
to perform well on this task, QUEST provides a
simple and effective way of experimenting with
techniques for feature selection and model build-
ing, as well as parameter optimisation through grid
search (Section 2.2). In Section 3 we present
experiments using the framework with nine QE
datasets.
In addition to providing a practical platform
for quality estimation, by freeing researchers from
feature engineering, QUEST will facilitate work
on the learning aspect of the problem. Quality
estimation poses several machine learning chal-
lenges, such as the fact that it can exploit a large,
diverse, but often noisy set of information sources,
with a relatively small number of annotated data
points, and it relies on human annotations that are
often inconsistent due to the subjectivity of the
task (quality judgements). Moreover, QE is highly
1http://scikit-learn.org/
non-linear: unlike many other problems in lan-
guage processing, considerable improvements can
be achieved using non-linear kernel techniques.
Also, different applications for the quality predic-
tions may benefit from different machine learn-
ing techniques, an aspect that has been mostly ne-
glected so far. Finally, the framework will also
facilitate research on ways of using quality predic-
tions in novel extrinsic tasks, such as self-training
of statistical machine translation systems, and for
estimating quality in other text output applications
such as text summarisation.
2 The QUEST framework
QUEST consists of two main modules: a feature
extraction module and a machine learning mod-
ule. The first module provides a number of feature
extractors, including the most commonly used fea-
tures in the literature and by systems submitted to
the WMT12 shared task on QE (Callison-Burch et
al., 2012). More than 15 researchers from 10 in-
stitutions contributed to it as part of the QUEST
project.2 It is implemented in Java and provides
abstract classes for features, resources and pre-
processing steps so that extractors for new features
can be easily added.
The basic functioning of the feature extraction
module requires raw text files with the source and
translation texts, and a few resources (where avail-
able) such as the source MT training corpus and
language models of source and target. Configura-
tion files are used to indicate the resources avail-
able and a list of features that should be extracted.
The machine learning module provides
scripts connecting the feature files with the
scikit-learn toolkit. It also uses GPy, a
Python toolkit for Gaussian Processes regression,
which outperformed algorithms commonly used
for the task such as SVM regressors.
2.1 Feature sets
In Figure 1 we show the types of features that
can be extracted in QUEST. Although the text
unit for which features are extracted can be of any
length, most features are more suitable for sen-
tences. Therefore, a ?segment? here denotes a sen-
tence.
From the source segments QUEST can extract
features that attempt to quantify the complexity
2http://www.dcs.shef.ac.uk/?lucia/
projects/quest.html
80
Confidence indicatorsComplexity indicators Fluency indicators
Adequacyindicators
Source text TranslationMT system
Figure 1: Families of features in QUEST.
of translating those segments, or how unexpected
they are given what is known to the MT system.
Examples of features include:
? number of tokens in the source segment;
? language model (LM) probability of source
segment using the source side of the parallel
corpus used to train the MT system as LM;
? percentage of source 1?3-grams observed in
different frequency quartiles of the source
side of the MT training corpus;
? average number of translations per source
word in the segment as given by IBM 1
model with probabilities thresholded in dif-
ferent ways.
From the translated segments QUEST can ex-
tract features that attempt to measure the fluency
of such translations. Examples of features include:
? number of tokens in the target segment;
? average number of occurrences of the target
word within the target segment;
? LM probability of target segment using a
large corpus of the target language to build
the LM.
From the comparison between the source and
target segments, QUEST can extract adequacy
features, which attempt to measure whether the
structure and meaning of the source are pre-
served in the translation. Some of these are based
on word-alignment information as provided by
GIZA++. Features include:
? ratio of number of tokens in source and target
segments;
? ratio of brackets and punctuation symbols in
source and target segments;
? ratio of percentages of numbers, content- /
non-content words in the source & target seg-
ments;
? ratio of percentage of nouns/verbs/etc in the
source and target segments;
? proportion of dependency relations between
(aligned) constituents in source and target
segments;
? difference between the depth of the syntactic
trees of the source and target segments;
? difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP phrases in
the source and target;
? difference between the number of per-
son/location/organization entities in source
and target sentences;
? proportion of person/location/organization
entities in source aligned to the same type of
entities in target segment;
? percentage of direct object personal or pos-
sessive pronouns incorrectly translated.
When available, information from the MT sys-
tem used to produce the translations can be very
useful, particularly for statistical machine transla-
tion (SMT). These features can provide an indi-
cation of the confidence of the MT system in the
translations. They are called ?glass-box? features,
to distinguish them from MT system-independent,
?black-box? features. To extract these features,
QUEST assumes the output of Moses-like SMT
systems, taking into account word- and phrase-
alignment information, a dump of the decoder?s
standard output (search graph information), global
model score and feature values, n-best lists, etc.
For other SMT systems, it can also take an XML
file with relevant information. Examples of glass-
box features include:
? features and global score of the SMT system;
? number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? average size of the target phrases;
? proportion of pruned search graph nodes;
? proportion of recombined graph nodes.
We note that some of these features are
language-independent by definition (such as the
confidence features), while others can be depen-
dent on linguistic resources (such as POS taggers),
or very language-specific, such as the incorrect
translation of pronouns, which was designed for
Arabic-English QE.
Some word-level features have also been im-
plemented: they include standard word posterior
probabilities and n-gram probabilities for each tar-
81
get word. These can also be averaged across the
whole sentence to provide sentence-level value.
The complete list of features available is given
as part of QUEST?s documentation. At the current
stage, the number of BB features varies from 80
to 123 depending on the language pair, while GB
features go from 39 to 48 depending on the SMT
system used (see Section 3).
2.2 Machine learning
QUEST provides a command-line interface mod-
ule for the scikit-learn library implemented
in Python. This module is completely indepen-
dent from the feature extraction code and it uses
the extracted feature sets to build QE models.
The dependencies are the scikit-learn li-
brary and all its dependencies (such as NumPy3
and SciPy4). The module can be configured to
run different regression and classification algo-
rithms, feature selection methods and grid search
for hyper-parameter optimisation.
The pipeline with feature selection and hyper-
parameter optimisation can be set using a con-
figuration file. Currently, the module has an
interface for Support Vector Regression (SVR),
Support Vector Classification, and Lasso learn-
ing algorithms. They can be used in conjunction
with the feature selection algorithms (Randomised
Lasso and Randomised decision trees) and the grid
search implementation of scikit-learn to fit
an optimal model of a given dataset.
Additionally, QUEST includes Gaussian Pro-
cess (GP) regression (Rasmussen and Williams,
2006) using the GPy toolkit.5 GPs are an ad-
vanced machine learning framework incorporating
Bayesian non-parametrics and kernel machines,
and are widely regarded as state of the art for
regression. Empirically we found the perfor-
mance to be similar to SVR on most datasets,
with slightly worse MAE and better RMSE.6 In
contrast to SVR, inference in GP regression can
be expressed analytically and the model hyper-
parameters optimised directly using gradient as-
cent, thus avoiding the need for costly grid search.
This also makes the method very suitable for fea-
ture selection.
3http://www.numpy.org/
4http://www.scipy.org/
5https://github.com/SheffieldML/GPy
6This follows from the optimisation objective: GPs use a
quadratic loss (the log-likelihood of a Gaussian) compared to
SVR which penalises absolute margin violations.
Data Training Test
WMT12 (en-es) 1,832 422
EAMT11 (en-es) 900 64
EAMT11 (fr-en) 2,300 225
EAMT09-s1-s4 (en-es) 3,095 906
GALE11-s1-s2 (ar-en) 2,198 387
Table 1: Number of sentences used for training
and testing in our datasets.
3 Benchmarking
In this section we benchmark QUEST on nine ex-
isting datasets using feature selection and learning
algorithms known to perform well in the task.
3.1 Datasets
The statistics of the datasets used in the experi-
ments are shown in Table 1.7
WMT12 English-Spanish sentence translations
produced by an SMT system and judged for
post-editing effort in 1-5 (worst-best), taking a
weighted average of three annotators.
EAMT11 English-Spanish (EAMT11-en-es)
and French-English (EAMT11-fr-en) sentence
translations judged for post-editing effort in 1-4.
EAMT09 English sentences translated by four
SMT systems into Spanish and scored for post-
editing effort in 1-4. Systems are denoted by s1-s4.
GALE11 Arabic sentences translated by two
SMT systems into English and scored for ade-
quacy in 1-4. Systems are denoted by s1-s2.
3.2 Settings
Amongst the various learning algorithms available
in QUEST, to make our results comparable we se-
lected SVR with radial basis function (RBF) ker-
nel, which has been shown to perform very well
in this task (Callison-Burch et al, 2012). The op-
timisation of parameters is done with grid search
using the following ranges of values:
? penalty parameter C: [1, 10, 10]
? ?: [0.0001, 0.1, 10]
? : [0.1, 0.2, 10]
where elements in list denote beginning, end and
number of samples to generate, respectively.
For feature selection, we have experimented
with two techniques: Randomised Lasso and
7The datasets can be downloaded from http://www.
dcs.shef.ac.uk/?lucia/resources.html
82
Gaussian Processes. Randomised Lasso (Mein-
shausen and Bu?hlmann, 2010) repeatedly resam-
ples the training data and fits a Lasso regression
model on each sample. A feature is said to be se-
lected if it was selected (i.e., assigned a non-zero
weight) in at least 25% of the samples (we do this
1000 times). This strategy improves the robust-
ness of Lasso in the presence of high dimensional
and correlated inputs.
Feature selection with Gaussian Processes is
done by fitting per-feature RBF widths (also
known as the automatic relevance determination
kernel). The RBF width denotes the importance
of a feature, the narrower the RBF the more impor-
tant a change in the feature value is to the model
prediction. To make the results comparable with
our baseline systems we select the 17 top ranked
features and then train a SVR on these features.8
As feature sets, we select all features available
in QUEST for each of our datasets. We differen-
tiate between black-box (BB) and glass-box (GB)
features, as only BB are available for all datasets
(we did not have access to the MT systems that
produced the other datasets). For the WMT12 and
GALE11 datasets, we experimented with both BB
and GB features. For each dataset we build four
systems:
? BL: 17 baseline features that performed well
across languages in previous work and were
used as baseline in the WMT12 QE task.
? AF: All features available for dataset.
? FS: Feature selection for automatic ranking
and selection of top features with:
? RL: Randomised Lasso.
? GP: Gaussian Process.
Mean Absolute Error (MAE) and Root Mean
Squared Error (RMSE) are used to evaluate the
models.
3.3 Results
The error scores for all datasets with BB features
are reported in Table 2, while Table 3 shows the re-
sults with GB features, and Table 4 the results with
BB and GB features together. For each table and
dataset, bold-faced figures are significantly better
than all others (paired t-test with p ? 0.05).
It can be seen from the results that adding more
BB features (systems AF) improves the results in
most cases as compared to the baseline systems
8More features resulted in further performance gains on
most tasks, with 25?35 features giving the best results.
Dataset System #feats. MAE RMSE
WMT12
BL 17 0.6802 0.8192
AF 80 0.6703 0.8373
FS(RL) 69 0.6628 0.8107
FS(GP) 17 0.6537 0.8014
EAMT11(en-es)
BL 17 0.4867 0.6288
AF 80 0.4696 0.5438
FS(RL) 29 0.4657 0.5424
FS(GP) 17 0.4640 0.5420
EAMT11(fr-en)
BL 17 0.4387 0.6357
AF 80 0.4275 0.6211
FS(RL) 65 0.4266 0.6196
FS(GP) 17 0.4240 0.6189
EAMT09-s1
BL 17 0.5294 0.6643
AF 80 0.5235 0.6558
FS(RL) 73 0.5190 0.6516
FS(GP) 17 0.5195 0.6511
EAMT09-s2
BL 17 0.4604 0.5856
AF 80 0.4734 0.5973
FS(RL) 59 0.4601 0.5837
FS(GP) 17 0.4610 0.5825
EAMT09-s3
BL 17 0.5321 0.6643
AF 80 0.5437 0.6827
FS(RL) 67 0.5338 0.6627
FS(GP) 17 0.5320 0.6630
EAMT09-s4
BL 17 0.3583 0.4953
AF 80 0.3569 0.5000
FS(RL) 40 0.3554 0.4995
FS(GP) 17 0.3560 0.4949
GALE11-s1
BL 17 0.5456 0.6905
AF 123 0.5359 0.6665
FS(RL) 56 0.5358 0.6649
FS(GP) 17 0.5410 0.6721
GALE11-s2
BL 17 0.5532 0.7177
AF 123 0.5381 0.6933
FS(RL) 54 0.5369 0.6955
FS(GP) 17 0.5424 0.6999
Table 2: Results with BB features.
Dataset System #feats. MAE RMSE
WMT12 AF 47 0.7036 0.8476FS(RL) 26 0.6821 0.8388
FS(GP) 17 0.6771 0.8308
GALE11-s1 AF 39 0.5720 0.7392FS(RL) 46 0.5691 0.7388
FS(GP) 17 0.5711 0.7378
GALE11-s2
AF 48 0.5510 0.6977
FS(RL) 46 0.5512 0.6970
FS(GP) 17 0.5501 0.6978
Table 3: Results with GB features.
Dataset System #feats. MAE RMSE
WMT12 AF 127 0.7165 0.8476FS(RL) 26 0.6601 0.8098
FS(GP) 17 0.6501 0.7989
GALE11-s1 AF 162 0.5437 0.6741FS(RL) 69 0.5310 0.6681
FS(GP) 17 0.5370 0.6701
GALE11-s2
AF 171 0.5222 0.6499
FS(RL) 82 0.5152 0.6421
FS(GP) 17 0.5121 0.6384
Table 4: Results with BB and GB features.
83
BL, however, in some cases the improvements are
not significant. This behaviour is to be expected
as adding more features may bring more relevant
information, but at the same time it makes the rep-
resentation more sparse and the learning prone to
overfitting. In most cases, feature selection with
both or either RL and GP improves over all fea-
tures (AF). It should be noted that RL automati-
cally selects the number of features used for train-
ing while FS(GP) was limited to selecting the top
17 features in order to make the results compara-
ble with our baseline feature set. It is interesting
to note that system FS(GP) outperformed the other
systems in spite of using fewer features. This tech-
nique is promising as it reduces the time require-
ments and overall computational complexity for
training the model, while achieving similar results
compared to systems with many more features.
Another interesting question is whether these
feature selection techniques identify a common
subset of features from the various datasets. The
overall top ranked features are:
? LM perplexities and log probabilities for
source and target;
? size of source and target sentences;
? average number of possible translations of
source words (IBM 1 with thresholds);
? ratio of target by source lengths in words;
? percentage of numbers in the target sentence;
? percentage of distinct unigrams seen in the
MT source training corpus.
Interestingly, not all top ranked features are
among the baseline 17 features which are report-
edly best in literature.
GB features on their own perform worse than
BB features, but in all three datasets, the combi-
nation of GB and BB followed by feature selec-
tion resulted in significantly lower errors than us-
ing only BB features with feature selection, show-
ing that the two features sets are complementary.
4 Remarks
The source code for the framework, the datasets
and extra resources can be downloaded from
http://www.quest.dcs.shef.ac.uk/.
The project is also set to receive contribution from
interested researchers using a GitHub repository:
https://github.com/lspecia/quest.
The license for the Java code, Python and shell
scripts is BSD, a permissive license with no re-
strictions on the use or extensions of the software
for any purposes, including commercial. For pre-
existing code and resources, e.g., scikit-learn, GPy
and Berkeley parser, their licenses apply, but fea-
tures relying on these resources can be easily dis-
carded if necessary.
Acknowledgments
This work was supported by the QuEst (EU
FP7 PASCAL2 NoE, Harvest program) and QT-
LaunchPad (EU FP7 CSA No. 296347) projects.
References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: a method for measuring machine translation
confidence. In ACL11, pages 211?219, Portland.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence Estimation for Machine Transla-
tion. In Coling04, pages 315?321, Geneva.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 workshop on statistical machine translation. In
WMT12, pages 10?51, Montre?al.
Y. He, Y. Ma, J. van Genabith, and A. Way. 2010.
Bridging SMT and TM with Translation Recom-
mendation. In ACL10, pages 622?630, Uppsala.
N. Meinshausen and P. Bu?hlmann. 2010. Stability se-
lection. Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology), 72:417?473.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL02, pages 311?318,
Philadelphia.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaus-
sian processes for machine learning, volume 1. MIT
Press, Cambridge.
R. Soricut and A. Echihabi. 2010. Trustrank: Induc-
ing trust in automatic translations via ranking. In
ACL11, pages 612?621, Uppsala.
L. Specia, M. Turchi, N. Cancedda, M. Dymetman,
and N. Cristianini. 2009. Estimating the Sentence-
Level Quality of Machine Translation Systems. In
EAMT09, pages 28?37, Barcelona.
L. Specia, D. Raj, and M. Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, 24(1):39?50.
L. Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In
EAMT11, pages 73?80, Leuven.
84
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 150?154,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Simple extensions for a reparameterised IBM Model 2
Douwe Gelling
Department of Computer Science
The University of Sheffield
d.gelling@shef.ac.uk
Trevor Cohn
Computing and Information Systems
The University of Melbourne
t.cohn@unimelb.edu.au
Abstract
A modification of a reparameterisation of
IBM Model 2 is presented, which makes
the model more flexible, and able to model
a preference for aligning to words to either
the right or left, and take into account POS
tags on the target side of the corpus. We
show that this extension has a very small
impact on training times, while obtain-
ing better alignments in terms of BLEU
scores.
1 Introduction
Word alignment is at the basis of most statistical
machine translation. The models that are gener-
ally used are often slow to train, and have a large
number of parameters. Dyer et al (2013) present
a simple reparameterization of IBM Model 2 that
is very fast to train, and achieves results similar to
IBM Model 4.
While this model is very effective, it also has
a very low number of parameters, and as such
doesn?t have a large amount of expressive power.
For one thing, it forces the model to consider
alignments on both sides of the diagonal equally
likely. However, it isn?t clear that this is the case,
as for some languages an alignment to earlier or
later in the sentence (above or below the diagonal)
could be common, due to word order differences.
For example, when aligning to Dutch, it may be
common for one verb to be aligned near the end of
the sentence that would be at the beginning in En-
glish. This would mean most of the other words in
the sentence would also align slightly away from
the diagonal in one direction. Figure 1 shows an
example sentence in which this happens. Here, a
circle denotes an alignment, and darker squares
are more likely under the alignment model. In
this case the modified Model 2 would simply make
both directions equally likely, where we would re-
ally like for only one direction to be more likely.
Hij had de man gezien
He
had
seen
the
man
Figure 1: Visualization of aligned sentence pair in
Dutch and English, darker shaded squares have a
higher alignment probability under the model, a
circle indicates a correct alignment. The English
sentence runs from bottom to top, the Dutch sen-
tence left to Right.
In some cases it could be that the prior probability
for a word alignment should be off the diagonal.
Furthermore, it is common in word alignment to
take word classes into account. This is commonly
implemented for the HMM alignment model as
well as Models 4 and 5. Och and Ney (2003) show
that for larger corpora, using word classes leads
to lower Alignment Error Rate (AER). This is not
implemented for Model 2, as it already has an
alignment model that is dependent on both source
and target length, and the position in both sen-
tences, and adding a dependency to word classes
would make the the Model even more prone to
overfitting than it already is. However, using the
reparameterization in (Dyer et al, 2013) would
leave the model simple enough even with a rela-
tively large amount of word classes.
Figure 2 shows an example of how the model
extensions could benefit word alignment. In the
example, all the Dutch words have a different
150
Hij had de man gezien
He
had
seen
the
man
Figure 2: Visualization of aligned sentence pair in
Dutch and English, darker shaded squares have a
higher alignment probability under the model, a
circle indicates a correct alignment. The English
sentence runs from bottom to top, the Dutch sen-
tence left to Right.
word class, and so can have different gradients for
alignment probability over the english words. If
the model has learned that prepositions and nouns
are more likely to align to words later in the sen-
tence, it could have a lower lambda for both word
classes, resulting in a less steep slope. If we also
split lambda into two variables, we can get algn-
ment probabilities as shown above for the Dutch
word ?de?, where aligning to one side of the diag-
onal is made more likely for some word classes.
Finally, instead of just having one side of the di-
agonal less steep than the other, it may be useful
to instead move the peak of the alignment prob-
ability function off the diagonal, while keeping it
equally likely. In Figure 2, this is done for the past
participle ?gezien?.
We will present a simple model for adding the
above extensions to achieve the above (splitting
the parameter, adding an offset and conditioning
the parameters on the POS tag of the target word)
in section 2, results on a set of experiments in sec-
tion 3 and present our conclusions in section 4.
2 Methods
We make use of a modified version of Model 2,
from Dyer et al (2013), which has an alignment
model that is parameterised in its original form
solely on the variable ?. Specifically, the proba-
bility of a sentence e given a sentence f is given
as:
m
?
i=1
n
?
j=0
?(a
i
|i,m, n) ? ?(e
i
|f
a
i
)
here, m is the length of the target sentence e, n
the same for source sentence f , ? is the alignment
model and ? is the translation model. In this pa-
per we are mainly concerned with the alignment
model ?. In the original formulation (with a minor
tweak to ensure symmetry through the center), this
function is defined as:
?(a
i
= j|i,m, n) =
?
?
?
?
?
?
?
p
0
j = 0
(1? p
0
) ?
e
h(i,j,m,n)
Z(i,m,n)
0 < j ? n
0 otherwise
where, h(?) is defined as
h(i, j,m, n) = ??
?
?
?
?
i
m+ 1
?
j
n+ 1
?
?
?
?
and Z
?
(i,m, n) is
?
n
j
?
=1
e
?h(i,j
?
,m,n)
, i.e. a
normalising function. Like the original Model 2
(Brown et al, 1993), this model is trained us-
ing Expectation-Maximisation. However, it is not
possible to directly update the ? parameter during
training, as it cannot be computed analytically. In-
stead, a gradient-based approach is used during the
M-step.
Two different optimisations are employed, the
first of which is used for calculating Z
?
. This
function forms a geometric series away from the
diagonal (for each target word), which can be
computed efficiently for each of the directions
from the diagonal. The second is used during the
M-step when computing the derivative, and is very
similar, but instead of using a geometric series, an
arithmetico-geometric series is used.
In order to allow the model to have a different
parameter above and below the diagonal, the only
change needed is to redefine h(?) to use a different
parameter for ? above and below the diagonal. We
denote these parameters as ? and ? for below and
above the diagonal respectively. Further, the offset
is denoted as ?.
we change the definition of h(?) to the following
instead:
151
h(i, j,m, n) =
?
?
?
?
?
?
?
?
?
??
?
?
?
?
i
m+ 1
?
j
n+ 1
+ ?
?
?
?
?
j <= j
?
??
?
?
?
?
i
m+ 1
?
j
n+ 1
+ ?
?
?
?
?
otherwise
j
?
is the point closest to or on the diagonal here,
calculated as:
max(min(b
i ? (n+ 1)
m+ 1
+ ? ? (n+ 1)c, n), 0)
Here, ? can range from ?1 to 1, and thus the
calculation for the diagonal j
?
is clamped to be in
a valid range for alignments.
As the partition function (Z(?)) used in (Dyer et
al., 2013) consists of 2 calculations for each tar-
get position i, one for above and one for below the
diagonal, we can simply substitute ? for the geo-
metric series calculations in order to use different
parameters for each:
s
?
(e
?h(i,j
?
,m,n)
, r) + s
n??
(e
?h(i,j
?
,m,n)
, r)
where j
?
is j
?
+ 1.
2.1 Optimizing the Parameters
As in the original formulation, we need to use
gradient-based optimisation in order to find good
values for ?, ? and ?. Unfortunately, optimizing
? would require taking the derivative of h(?), and
thus the derivative of the absolute value. This is
unfortunately undefined when the argument is 0,
however we work around this by choosing a sub-
gradient of 0 at that point. This means the steps we
take do not always improve the objective function,
but in practice the method works well.
The first derivative of L with respect to ? at a
single target word becomes:
?
?
L =
j
?
?
k=1
p(a
i
= k|e
i
, f,m, n)h(i, k,m, n)
?
j
?
?
l=1
?(l|i,m, n)h(i, l,m, n)
And similar for finding the first derivative with
respect to ?, but summing from j
?
to n instead.
The first derivative with respect to ? then, is:
?
?
L =
n
?
k=1
p(a
i
= k|e
i
, f,m, n)h
?
(i, k,m, n)
?
j
?
?
l=1
?(l|i,m, n)h
?
(i, l,m, n)
Where h
?
(?) is the first derivative of h(?) with
respect to ?. For obtaining this derivative, the
arithmetico-geometric series (Fernandez et al,
2006) was originally used as an optimization, and
for the gradient with respect to omega a geometric
series should suffice, as an optimization, as there
is no conditioning on the source words. This is
not done in the current work however, so timing
results will not be directly comparable to those
found in (Dyer et al, 2013).
Conditioning on the POS of the target words
then becomes as simple as using a different ?, ?,
and ? for each POS tag in the input, and calculat-
ing a separate derivative for each of them, using
only the derivatives at those target words that use
the POS tag. A minor detail is to keep a count of
alignment positions used for finding the derivative
for each different parameter, and normalizing the
resulting derivatives with those counts, so the step
size can be kept constant across POS tags.
3 Empirical results
The above described model is evaluated with ex-
periments on a set of 3 language pairs, on which
AER scores and BLEU scores are computed. We
use similar corpora as used in (Dyer et al, 2013):
a French-English corpus made up of Europarl ver-
sion 7 and news-commentary corpora, the Arabic-
English parallel data consisting of the non-UN
portions of the NIST training corpora, and the
FBIS Chinese-English corpora.
The models that are compared are the original
reparameterization of Model 2, a version where ?
is split around the diagonal (split), one where pos
tags are used, but ? is not split around the diagonal
(pos), one where an offset is used, but parameters
aren?t split about the diagonal (offset), one that?s
split about the diagonal and uses pos tags (pos &
split) and finally one with all three (pos & split &
offset). All are trained for 5 iterations, with uni-
form initialisation, where the first iteration only
the translation probabilities are updated, and the
other parameters are updated as well in the sub-
sequent iterations. The same hyperparameters are
152
Model Fr-En Ar-En Zh-En
Tokens 111M 46M 17.3M
(after) 110M 29.0M 10.4M
average 1.64 0.76 0.27
Model 4 15.5 6.3 2.2
Table 1: Token counts and average amount of time
to train models (and separately training time for
Model 4) on original corpora in one direction in
hours, by corpus.
used as in (Dyer et al, 2013), with stepsize for up-
dates to ? and ? during gradient ascent is 1000,
and that for ? is 0.03, decaying after every gradi-
ent descent step by 0.9, using 8 steps every iter-
ation. Both ? and ? are initialised to 6, and ? is
initialised to 0. For these experiments the pos and
pos & split use POS tags generated using the Stan-
ford POS tagger (Toutanova and Manning, 2000),
using the supplied models for all of the languages
used in the experiments. For comparison, Model
4 is trained for 5 iterations using 5 iterations each
of Model 1 and Model 3 as initialization, using
GIZA++ (Och and Ney, 2003).
For the comparisons in AER, the corpora are
used as-is, but for the BLEU comparisons, sen-
tences longer than 50 words are filtered out. In
Table 2 the sizes of the corpora before filtering are
listed, as well as the time taken in hours to align
the corpora for AER. As the training times for
the different versions barely differ, only the aver-
age is displayed for the models here described and
Model 4 training times are given for comparison.
Note that the times for the models optimizing only
? and ?, and the model only optimizing ? still cal-
culate the derivatives for the other parameters, and
so could be made to be faster than here displayed.
For both the BLEU and AER results, the align-
ments are generated in both directions, and sym-
metrised using the grow-diag-final-and heuristic,
which in preliminary tests had shown to do best in
terms of AER.
The results are given in Table 2. These scores
were computed using the WMT2012 data as gold
standard. The different extensions to the model
make no difference to the AER scores for Chinese-
English, and actually do slightly worse for French-
English. In both cases, Model 4 does better than
the models introduced here.
Model Fr-En Zh-En
Original 16.3 42.5
Split 16.8 42.5
Pos 16.6 42.5
Offset 16.8 42.5
Pos & Split 16.8 42.5
Pos & Split & Offset 16.7 42.5
Model 4 11.2 40.5
Table 2: AER results on Chinese-English and
French-English data sets
Model Fr-En Ar-En Zh-En
Original 25.9 43.8 32.8
Split 25.9 43.2 32.8
Pos 25.9 43.9 32.9
Offset 26.0 43.9 32.8
Pos & Split 26.0 44.1 33.2
Pos & Split & Offset 26.0 44.2 33.3
Model 4 26.8 43.9 32.4
Table 3: BLEU results on Chinese-English and
French-English data sets
For the comparisons of translation quality, the
models are trained up using a phrase-based trans-
lation system (Koehn et al, 2007) that used the
above listed models to align the data. Language
models were augmented with data outside of the
corpora for Chinese-English (200M words total)
and Arabic-English (100M words total). Test sets
for Chinese are MT02, MT03, MT06 and MT08,
for Arabic they were MT05, MT06 and MT08, and
for French they were the newssyscomb2009 data
and the newstest 2009-2012 data.
The results are listed in Table 3
1
. BLEU scores
for Arabic-English and Chinese-English are com-
puted with multiple references, while those for
French-English are against a single reference. Al-
though the different models made little difference
in AER, there is quite a bit of variation in the
BLEU scores between the different models. In
all cases, the models conditioned on POS tags
did better than the original model, by as much
as 0.5 BLEU points. For Arabic-English as well
as Chinese-English, the full model outperformed
1
The difference in these results compared to those re-
ported in Dyer et al (2013) is due to differences in corpus
size, and the fact that a different translation model is used.
153
Model 4, in the case of Chinese-English by 0.9
BLEU points.
The low impact of the split and offset models
are most likely due to the need to model all align-
ments in the corpus. The distributions can?t skew
too far to aligning to one direction, as that would
lower the probability of a large amount of align-
ments. This is reflected in the resulting parame-
ters ?, ? and ? that are estimated, as the first two
do not differ much from the parameters estimated
when both are kept the same, and the second tends
to be very small.
As for the Pos model, it seems that only vary-
ing the symmetrical slope for the different POS
tags doesn?t capture the differences between dis-
tributions for POS tags. For example, the ? and
? parameters can differ quite a lot in the Pos &
Split model when compared to the Pos model, with
one side having a much smaller parameter and the
other a much larger parameter for a given POS tag
in the first model, and the single parameter being
closer to the model average for the same POS tag
in the second model.
The low variation in results between the differ-
ent models for French-English might be explained
by less word movement when translating between
these languages, which could mean the original
model is sufficient to capture this behaviour.
4 Conclusion
We have shown some extensions to a reparame-
terized IBM Model 2, allowing it to model word
reordering better. Although these models don?t
improve on the baseline in terms of AER, they
do better than the original in all three languages
tested, and outperform M4 in two of these lan-
guages, at little cost in terms of training time. Fu-
ture directions for this work include allowing for
more expressivity of the alignment model by using
a Beta distribution instead of the current exponen-
tial model.
5 Acknowledgments
Dr Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263?311.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteri-
zation of ibm model 2. In Proceedings of NAACL-
HLT, pages 644?648.
P. A. Fernandez, T. Foregger, and J. Pahikkala. 2006.
Arithmetico-geometric series.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51, March.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Process-
ing and Very Large Corpora: Held in Conjunction
with the 38th Annual Meeting of the Association
for Computational Linguistics - Volume 13, EMNLP
?00, pages 63?70, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
154
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 1?3,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Gaussian Processes for Natural Language Processing
Trevor Cohn
Computing and Information Systems
The University of Melbourne
trevor.cohn@gmail.com
Daniel Preot?iuc-Pietro and Neil Lawrence
Department of Computer Science
The University of Sheffield
{daniel,n.lawrence}@dcs.shef.ac.uk
1 Introduction
Gaussian Processes (GPs) are a powerful mod-
elling framework incorporating kernels and
Bayesian inference, and are recognised as state-
of-the-art for many machine learning tasks.
Despite this, GPs have seen few applications in
natural language processing (notwithstanding
several recent papers by the authors). We argue
that the GP framework offers many benefits over
commonly used machine learning frameworks,
such as linear models (logistic regression, least
squares regression) and support vector machines.
Moreover, GPs are extremely flexible and can
be incorporated into larger graphical models,
forming an important additional tool for proba-
bilistic inference. Notably, GPs are one of the
few models which support analytic Bayesian in-
ference, avoiding the many approximation errors
that plague approximate inference techniques in
common use for Bayesian models (e.g. MCMC,
variational Bayes).
1
GPs accurately model not
just the underlying task, but also the uncertainty
in the predictions, such that uncertainty can be
propagated through pipelines of probabilistic
components. Overall, GPs provide an elegant,
flexible and simple means of probabilistic infer-
ence and are well overdue for consideration of the
NLP community.
This tutorial will focus primarily on regression
and classification, both fundamental techniques of
wide-spread use in the NLP community. Within
NLP, linear models are near ubiquitous, because
they provide good results for many tasks, support
efficient inference (including dynamic program-
ming in structured prediction) and support simple
parameter interpretation. However, linear mod-
els are inherently limited in the types of relation-
ships between variables they can model. Often
1
This holds for GP regression, but note that approximate
inference is needed for non-Gaussian likelihoods.
non-linear methods are required for better under-
standing and improved performance. Currently,
kernel methods such as Support Vector Machines
(SVM) represent a popular choice for non-linear
modelling. These suffer from lack of interoper-
ability with down-stream processing as part of a
larger model, and inflexibility in terms of parame-
terisation and associated high cost of hyperparam-
eter optimisation. GPs appear similar to SVMs, in
that they incorporate kernels, however their prob-
abilistic formulation allows for much wider appli-
cability in larger graphical models. Moreover, sev-
eral properties of Gaussian distributions (closure
under integration and Gaussian-Gaussian conju-
gacy) means that GP (regression) supports analytic
formulations for the posterior and predictive infer-
ence.
This tutorial will cover the basic motivation,
ideas and theory of Gaussian Processes and several
applications to natural language processing tasks.
GPs have been actively researched since the early
2000s, and are now reaching maturity: the fun-
damental theory and practice is well understood,
and now research is focused into their applica-
tions, and improve inference algorithms, e.g., for
scaling inference to large and high-dimensional
datasets. Several open-source packages (e.g. GPy
and GPML) have been developed which allow for
GPs to be easily used for many applications. This
tutorial aims to promote GPs, emphasising their
potential for widespread application across many
NLP tasks.
2 Overview
Our goal is to present the main ideas and theory
behind Gaussian Processes in order to increase
awareness within the NLP community. The first
part of the tutorial will focus on the basics of Gaus-
sian Processes in the context of regression. The
Gaussian Process defines a prior over functions
which applied at each input point gives a response
1
value. Given data, we can analytically infer the
posterior distribution of these functions assuming
Gaussian noise.
This tutorial will contrast two main applications
settings for regression: interpolation and extrapo-
lation. Interpolation suits the use of simple radial
basis function kernels which bias towards smooth
latent functions. For extrapolation, however, the
choice of the kernel is paramount, encoding our
prior belief about the type of function wish to
learn. We present several different kernels, includ-
ing non-stationary and kernels for structured data
(string and tree kernels). One of the main issues
for kernel methods is setting the hyperparameters,
which is often done in the support vector literature
using grid search on held-out validation data. In
the GP framework, we can compute the probabil-
ity of the data given the model which involves the
integral over the parameter space. This marginal
likelihood or Bayesian evidence can be used for
model selection using only training data, where by
model selection we refer either to choosing from a
set of given covariance kernels or choosing from
different model hyperparameters (kernel parame-
ters). We will present the key algorithms for type-
II maximum likelihood estimation with respect to
the hyper-parameters, using gradient ascent on the
marginal likelihood.
Many problems in NLP involve learning from
a range of different tasks. We present multi-task
learning models by representing intra-task transfer
simply and explicitly as a part of a parameterised
kernel function. GPs are an extremely flexible
probabilistic framework and have been success-
fully adapted for multi-task learning, by modelling
multiple correlated output variables (Alvarez et
al., 2011). This literature develops early work
from geostatistics (kriging and co-kriging), on
learning latent continuous spatio-temporal models
from sparse point measurements, a problem set-
ting that has clear parallels to transfer learning (in-
cluding domain adaptation).
In the application section, we start by present-
ing an open-source software package for GP mod-
elling in Python: GPy.
2
The first application we
approach the regression task of predicting user in-
fluence on Twitter based on a range or profile and
word features (Lampos et al., 2014). We exem-
plify how to identifying which features are best
for predicting user impact by optimising the hy-
2
http://github.com/SheffieldML/GPy
perparameters (e.g. RBF kernel length-scales) us-
ing Automatic Relevance Determination (ARD).
This basically gives a ranking in importance of
the features, allowing interpretability of the mod-
els. Switching to a multi-task regression setting,
we present an application to Machine Translation
Quality Estimation. Our method shows large im-
provements over previous state-of-the-art (Cohn
and Specia, 2013). Concepts in automatic kernel
selection are exemplified in an extrapolation re-
gression setting, where we model word time series
in Social Media using different kernels (Preot?iuc-
Pietro and Cohn, 2013). The Bayesian evidence
helps to select the most suitable kernel, thus giv-
ing an implicit classification of time series.
In the final section of the tutorial we give a
brief overview of advanced topics in the field of
GPs. First, we look at non-conjugate likelihoods
for modelling classification, count and rank data.
This is harder than regression, as Bayesian pos-
terior inference can no longer be solved analyti-
cally. We will outline strategies for non-conjugate
inference, such as expectation propagation and the
Laplace approximation. Second, we will outline
recent work on scaling GPs to big data using vari-
ational inference to induce sparse kernel matrices
(Hensman et al., 2013). Finally ? time permitting
? we will finish with unsupervised learning in GPs
using the latent variable model (Lawrence, 2004),
a non-linear Bayesian analogue of principle com-
ponent analysis.
3 Outline
1. GP Regression (60 mins)
(a) Weight space view
(b) Function space view
(c) Kernels
2. NLP Applications (60 mins)
(a) Sparse GPs: Predicting user impact
(b) Multi-output GPs: Modelling multi-
annotator data
(c) Model selection: Identifying temporal
patterns in word frequencies
3. Further topics (45 mins)
(a) Non-congjugate likelihoods: classifica-
tion, counts and ranking
(b) Scaling GPs to big data: Sparse GPs and
stochastic variational inference
2
(c) Unsupervised inference with the GP-
LVM
4 Instructors
Trevor Cohn
3
is a Senior Lecturer and ARC Fu-
ture Fellow at the University of Melbourne. His
research deals with probabilistic machine learn-
ing models, particularly structured prediction and
non-parametric Bayesian models. He has recently
published several seminal papers on Gaussian Pro-
cess models for NLP with applications ranging
from translation evaluation to temporal dynamics
in social media.
Daniel Preot?iuc-Pietro
4
is a final year PhD stu-
dent in Natural Language Processing at the Uni-
versity of Sheffield. His research deals with ap-
plying Machine Learning models to model large
volumes of data, mostly coming from Social Me-
dia. Applications include forecasting future be-
haviours of text, users or real world quantities (e.g.
political voting intention), user geo-location and
impact.
Neil Lawrence
5
is a Professor at the University
of Sheffield. He is one of the foremost experts on
Gaussian Processes and non-parametric Bayesian
inference, with a long history of publications and
innovations in the field, including their application
to multi-output scenarios, unsupervised learning,
deep networks and scaling to big data. He has been
programme chair for top machine learning confer-
ences (NIPS, AISTATS), and has run several past
tutorials on Gaussian Processes.
References
Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2011. Kernels for vector-valued func-
tions: A review. Foundations and Trends in Machine
Learning, 4(3):195?266.
Trevor Cohn and Lucia Specia. 2013. Modelling anno-
tator bias with multi-task Gaussian processes: an ap-
plication to machine translation quality estimation.
In Proceedings of the 51st annual meeting of the As-
sociation for Computational Linguistics, ACL.
James Hensman, Nicolo Fusi, and Neil D. Lawrence.
2013. Gaussian processes for big data. In Proceed-
ings of the 29th Conference on Uncertainty in Artifi-
cial Intelligence, UAI.
3
http://staffwww.dcs.shef.ac.uk/
people/T.Cohn
4
http://www.preotiuc.ro
5
http://staffwww.dcs.shef.ac.uk/
people/N.Lawrence
Vasileios Lampos, Nikolaos Aletras, Daniel Preot?iuc-
Pietro, and Trevor Cohn. 2014. Predicting and char-
acterising user impact on Twitter. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, EACL.
Neil D. Lawrence. 2004. Gaussian process latent vari-
able models for visualisation of high dimensional
data. NIPS, 16(329-336):3.
Daniel Preot?iuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An investigation on the effectiveness of features for
translation quality estimation. In Proceedings of the
Machine Translation Summit.
3
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 123?129,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Regression and Ranking based Optimisation for Sentence Level Machine
Translation Evaluation
Xingyi Song and Trevor Cohn
The Department of Computer Science
University of Sheffield
Sheffield, S1 4DP. UK
{xsong2,t.cohn}@shef.ac.uk
Abstract
Automatic evaluation metrics are fundamen-
tally important for Machine Translation, al-
lowing comparison of systems performance
and efficient training. Current evaluation met-
rics fall into two classes: heuristic approaches,
like BLEU, and those using supervised learn-
ing trained on human judgement data. While
many trained metrics provide a better match
against human judgements, this comes at the
cost of including lots of features, leading to
unwieldy, non-portable and slow metrics. In
this paper, we introduce a new trained met-
ric, ROSE, which only uses simple features
that are easy portable and quick to compute.
In addition, ROSE is sentence-based, as op-
posed to document-based, allowing it to be
used in a wider range of settings. Results show
that ROSE performs well on many tasks, such
as ranking system and syntactic constituents,
with results competitive to BLEU. Moreover,
this still holds when ROSE is trained on hu-
man judgements of translations into a different
language compared with that use in testing.
1 Introduction
Human judgements of translation quality are very
expensive. For this reason automatic MT evalu-
ation metrics are used to as an approximation by
comparing predicted translations to human authored
references. An early MT evaluation metric, BLEU
(Papineni et al, 2002), is still the most commonly
used metric in automatic machine translation evalu-
ation. However, several drawbacks have been stated
by many researchers (Chiang et al, 2008a; Callison-
Burch et al, 2006; Banerjee and Lavie, 2005), most
notably that it omits recall (substituting this with a
penalty for overly short output) and not being easily
applied at the sentence level. Later heuristic metrics
such as METEOR (Banerjee and Lavie, 2005) and
TER (Snover et al, 2006) account for both precision
and recall, but their relative weights are difficult to
determine manually.
In contrast to heuristic metrics, trained met-
rics use supervised learning to model directly hu-
man judgements. This allows the combination
of different features and can better fit specific
tasks, such as evaluation focusing more on flu-
ency/adequacy/relative ranks or post editing effort.
Previous work includes approaches using classifica-
tion (Corston-Oliver et al, 2001), regression (Alber-
cht and Hwa, 2008; Specia and Gimenez, 2010), and
ranking (Duh, 2008). Most of which achieved good
results and better correlations with human judg-
ments than heuristic baseline methods.
Overall automatic metrics must find a balance be-
tween several key issues: a) applicability to differ-
ent sized texts (documents vs sentences), b) easy
of portability to different languages, c) runtime re-
quirements and d) correlation with human judge-
ment data. Previous work has typically ignored at
least one of these issues, e.g., BLEU which applies
only to documents (A), trained metrics (Albercht
and Hwa, 2008; Specia and Gimenez, 2010) which
tend to ignore B and C.
This paper presents ROSE, a trained metric which
is loosely based on BLEU, but seeks to further sim-
plify its components such that it can be used for sen-
tence level evaluation. This contrasts with BLEU
which is defined over large documents, and must
123
be coarsely approximated to allow sentence level
application. The increased flexibility of ROSE al-
lows the metric to be used in a wider range of situ-
ations, including during decoding. ROSE is a linear
model with a small number of simple features, and
is trained using regression or ranking against human
judgement data. A benefit of using only simple fea-
tures is that ROSE can be trivially ported between
target languages, and that it can be run very quickly.
Features include precision and recall over different
sized n-grams, and the difference in word counts
between the candidate and the reference sentences,
which is further divided into content word, func-
tion word and punctuation. An extended versions
also includes features over Part of Speech (POS) se-
quences.
The paper is structured as follows: Related work
on metrics for statistical machine translation is de-
scribed in Section 2. Four variations of ROSE and
their features will be introduced in Section 3. In sec-
tion 4 we presents the result, showing how ROSE
correlates well with human judgments on both sys-
tem and sentence levels. Conclusions are given at
the end of the paper.
2 Related Work
The defacto standard metric in machine translation
is BLEU (Papineni et al, 2002). This measures
n-gram precision (n normally equal to 1,2,3,4) be-
tween a document of candidate sentences and a
set of human authored reference documents. The
idea is that high quality translations share many n-
grams with the references. In order to reduce re-
peatedly generating the same word, BLEU clips the
counts of each candidate N-gram to the maximum
counts of that n-gram that in references, and with
a brevity penalty to down-scale the score for out-
put shorter than the reference. In BLEU, each n-
gram precision is given equal weight in geometric
mean, while NIST (Doddington and George, 2002)
extended BLEU by assigning more informative n-
grams higher weight.
However, BLEU and NIST have several draw-
backs, the first being that BLEU uses a geometric
mean over all n-grams which makes BLEU almost
unusable for sentence level evaluations 1. Secondly,
1Note that various approximations exits (Lin and Och, 2004;
BLEU and NIST both use the brevity penalty to re-
place recall, but Banerjee and Lavie (2005) in exper-
iments show that the brevity penalty is a poor sub-
stitute for recall.
Banerjee and Lavie (2005) proposed a METEOR
metric, which that uses recall instead of the BP.
Callison-Burch et al (2007; Callison-Burch et al
(2008) show that METEOR does not perform well in
out of English task. This may because that Stemmer
or WordNet may not available in some languages,
which unable to model synonyms in these cases. In
addition, the performance also varies when adjusting
weights in precision and recall.
Supervised learning approaches have been pro-
posed by many researchers (Corston-Oliver et al,
2001; Duh, 2008; Albercht and Hwa, 2008; Spe-
cia and Gimenez, 2010). Corston-Oliver et al
(2001) use a classification method to measure ma-
chine translation system quality at the sentence level
as being human-like translation (good) or machine
translated (bad). Features extracted from references
and machine translation include heavy linguistic fea-
tures (requires parser).
Quirk (2004) proposed a linear regression model
which is trained to match translation quality. Alber-
cht and Hwa (2008) introduced pseudo-references
when data driven regression does not have enough
training data. Most recently, Specia and Gimenez
(2010) combined confidence estimation (without
reference, just using the source) and reference-based
metrics together in a regression framework to mea-
sure sentence-level machine translation quality.
Duh (2008) compared the ranking with the re-
gression, with the results that with same feature set,
ranking and regression have similar performance,
while ranking can tolerate more training data noise.
3 Model
ROSE is a trained automatic MT evaluation metric
that works on sentence level. It is defined as a linear
model, and its weights will be trained by Support
Vector Machine. It is formulated as
S = ??w ?f(??c ,??r ) (1)
where ??w is the feature weights vector, f(??c ,??r ) is
the feature function which takes candidate transla-
Chiang et al, 2008b)
124
tion (??c ) and reference (??c ), and returns the feature
vector. S is the response variable, measuring the
?goodness? of the candidate translation. A higher
score means a better translation, although the mag-
nitude is not always meaningful.
We present two different method for training:
a linear regression approach ROSE-reg, trained to
match human evaluation score, and a ranking ap-
proach ROSE-rank to match the relative ordering of
pairs of translations assigned by human judge. Un-
like ROSE-reg, ROSE-rank only gives relative score
between sentences, such as A is better than B. The
features that used in ROSE will be listed in section
3.1, and the regression and ranking training are de-
scribed in section 3.2
3.1 ROSE Features
Features used in ROSE listed in Table 1 include
string n-gram matching, Word count and Part of
Speech (POS). String N-gram matching features, are
used for measure how closely of the candidate sen-
tence resembles the reference. Both precision and
recall are considered. Word count features measure
length differences between the candidate and refer-
ence, which is further divided into function words,
punctuation and content words. POS features are
defined over POS n-gram matches between the can-
didate and reference.
3.1.1 String Matching Features
The string matching features include n-gram pre-
cision, n-gram recall and F1-measure. N-gram
precision measures matches between sequence of
words in the candidate sentence compared to the ref-
erences,
Pn =
?
n-gram???c Count(n-gram)Jn-gram ?
??r K
?
n-gram???c Count(ngram)
(2)
where Count are the occurrence counts of n-grams
in the candidate sentence, the numerator measures
the number of predicted n-grams that also occur in
the reference.
Recall is also used in ROSE, so clipping was
deemed unnecessary in precision calculation, where
the repeating words will increasing precision but at
the expense of recall. F-measure is also included,
which is the harmonic mean of precision and recall.
ID Description
1-4 n-gram precision, n=1...4
5-8 n-gram recall, n=1...4
9-12 n-gram f-measure, n=1...4
13 Average n-gram precision
14 Words count
15 Function words count
16 Punctuation count
17 Content words count
18-21 n-gram POS precision, n=1...4
22-25 n-gram POS recall, n=1...4
26-29 n-gram POS f-measure, n=1...4
30-33 n-gram POS string mixed precision,
n=1...4
Table 1: ROSE Features. The first column is the feature
number. The dashed line separates the core features from
the POS extended features.
With there are multiple references, the n-gram preci-
sion error uses the same strategy as BLEU: n-grams
in candidate can match any of the references. For
recall, ROSE will match the n-grams in each refer-
ence separately, and then choose the recall for the
reference with minimum error.
3.1.2 Word Count Features
The word count features measure the length dif-
ference between a candidate sentence and reference
sentence. In a sentence, content words are more in-
formative than function words (grammatical words)
and punctuation. Therefore, the number of content
word candidate is a important indicator in evalua-
tion. In this case, besides measuring the length at
whole sentences, we also measure difference in the
number of function words, punctuation and content
words. We normalise by the length of the refer-
ence which allows comparability between short ver-
sus long sentences. In multiple reference cases we
choose the ratio that is closest to 1.
3.1.3 Part of Speech Features
The string matching features and word count fea-
tures only measure similarities on the lexical level,
but not over sentence structure or synonyms. To add
this capability we also include Part of Speech (POS)
features which work similar to the String Matching
features, but using POS instead of words. The fea-
125
tures measure precision, recall and F-measure over
POS n-grams (n=1...4). In addition, we also include
features that mixed string and POS.
The string/POS mixed feature is used for handling
synonyms. One problem in string n-gram match-
ing is not being able to deal with the synonyms be-
tween the candidate translation and the reference.
One approach for doing so is to use an external re-
source such as WordNet (Banerjee and Lavie, 2005),
however this would limit the portability of the met-
ric. Instead we use POS as a proxy. In most of
the cases, synonyms share the same POS, so this
can be rewarded by forming n-grams over a mix-
ture of tokens and POS. During the matching pro-
cess, both words and its POS shall be considered, if
either matches between reference and candidate, the
n-gram matches will be counted.
Considering the example in table 2, candidate 1
has better translation than candidate 2 and 3. If only
the string N-gram matching is used, that will give the
same score to candidate 1, 2 and 3. The n-gram pre-
cision scores obtained by all candidate sentences in
this example are: 2-gram = 1, 3-gram = 0. However,
we can at least distinguish candidate 1 is better than
candidate 3 if string POS mixed precision is used ,
n-gram precision for candidate 1 will be: 2-gram =
2, 3-gram = 1, which ranks candidate 1 better than
candidate 3.
Example
reference: A/DT red/ADJ vehicle/NN
candidate 1: A/DT red/ADJ car/NN
candidate 2: A/DT red/ADJ rose/NN
candidate 3: A/DT red/ADJ red/ADJ
Table 2: Evaluation Example
3.2 Training
The model was trained on human evaluation data
in two different ways, regression and ranking.These
both used SVM-light (Joachims, 1999). In the rank-
ing model, the training data are candidate translation
and their relative rankings were ranked by human
judge for a given input sentence. The SVM finds
the minimum magnitude weights that are able to cor-
rectly rank training data which is framed as a series
of constraints reflecting all pairwise comparisons. A
soft-margin formulation is used to allow training er-
rors with a penalty (Joachims, 2002). For regres-
sion, the training data is human annotation of post-
edit effort (this will be further described in section
4.1). The Support vector Regression learns weights
with minimum magnitude that limit prediction er-
ror to within an accepted range, again with a soft-
margin formulation (Smola and Schlkopf, 2004).
A linear kernel function will be used, because
non-linear kernels are much slower to use and are
not decomposable. Our experiments showed that the
linear kernel performed at similar accuracy to other
kernel functions (see section 4.2).
4 Experimental Setup
Our experiments test ROSE performance on docu-
ment level with three different Kernel functions: lin-
ear, polynomial and radial basis function. Then we
compare four variants of ROSE with BLEU on both
sentence and system (document) level.
The BLEU version we used here is NIST Open
MT Evaluation tool mteval version 13a, smooth-
ing was disabled and except for the sentence level
evaluation experiment. The system level evalua-
tion procedure follows WMT08 (Callison-Burch et
al., 2008), which ranked each system submitted on
WMT08 in three types of tasks:
? Rank: Human judges candidate sentence rank
in order of quality. On the document level, doc-
uments are ranked according to the proportion
of candidate sentences in a document that are
better than all of the candidates.
? Constituent: The constituent task is the same
as for ranking but operates over chosen syntac-
tic constituents.
? Yes/No: WMT08 Yes/No task is to let human
judge decide whether the particular part of a
sentence is acceptable or not. Document level
Yes/No ranks a document according to their
number of YES sentences
Spearman?s rho correlation was used to measure
the quality of the metrics on system level. Four tar-
get languages (English, German, French and Span-
ish) were used in system level experiments. ROSE-
126
reg and ROSE-rank were tested in all target lan-
guage sets, but ROSE-regpos was only tested in the
into-English set as it requires a POS tagger. On the
sentence level, we compare sentences ranking that
ranked by metrics against human ranking. The eval-
uation quality was examined by Kendall?s tau cor-
relation, and tied results from human judges were
excluded.
Rank es-en fr-en de-en avg
Linear 0.57 0.97 0.69 0.74
Polynomial 0.62 0.97 0.71 0.76
RBF 0.60 0.98 0.62 0.73
Constituent
Linear 0.79 0.90 0.39 0.69
Polynomial 0.80 0.89 0.41 0.70
RBF 0.83 0.93 0.34 0.70
Yes/No
Linear 0.92 0.93 0.67 0.84
Polynomial 0.86 0.90 0.66 0.81
RBF 0.87 0.93 0.65 0.82
Table 3: ROSE-reg in with SVM kernel functions
Metric Kendall?s tau
BLEU-smoothed 0.219
ROSE-reg 0.120
ROSE-regpos 0.164
ROSE-rank 0.206
ROSE-rankpos 0.172
Table 4: Sentence Level Evaluation
4.1 Data
Training data used for ROSE is from WMT10
(Callison-Burch et al, 2010) human judged sen-
tences. A regression model was trained by sentences
with human annotation for post editing effort. The
three levels used in WMT10 are ?OK?, ?EDIT? and
?BAD?, which we treat as response values of 3, 2
and 1. In total 2885 sentences were used in the re-
gression training. The ranking model was trained by
sentences with human annotating sentence ranking,
and tied results are allowed in training. In this exper-
iment, 1675 groups of sentences were used for train-
ing, and each group contains five sentences, which
are manually ranked from 5 (best) to 1 (worst). In or-
der to test the ROSE?s ability to adapt the language
without training data, ROSE was only trained with
English data.
The testing data on sentence level used in this
paper is human ranked sentences from WMT09
(Callison-Burch et al, 2009). Tied rankings were re-
moved, leaving 1702 pairs. We only consider trans-
lations into English sentences. On system level, the
testing data are the submissions for ?test2008? test
set in WMT08 (Callison-Burch et al, 2008). ROSE,
and BLEU were compared with human ranked sub-
mitted system in ?RANK?, ?CONSTITUENT? and
?YES/NO? tasks.
English punctuation and 100 common function
words list of four languages in this experiment were
generated. English POS was tagged by NLTK (Bird
and Loper, 2004).
4.2 Results and Discussion
Table 3 shows the results of ROSE-reg with three
different SVM Kernel functions. Performance are
similar among three different Kernel functions.
However, the linear kernel is the fastest and simplest
and there is no overall winner. Therefore, linear Ker-
nel function was used in ROSE.
The results of Kendall?s tau on sentence level
evaluation are shown in Table 4. According to Ta-
ble 4 ROSE-rank has the highest score in all ver-
sions of ROSE. The score is close to the smoothed
version of BLEU. Results also showed adding POS
feature helped in improving accuracy in the regres-
sion model, but not in ranking, The reason for this is
not clear, but it may be due to over fitting.
Table 5 and Table 6 are the Spearman?s rho in sys-
tem ranking. Table 5 is the task evaluation for trans-
lation into English. ROSE-rank performed the best
in the system ranking task. Also, ROSE-regpos is
the best in the syntactic constituents task. This may
because of ROSE-rank is a ranking based metric and
ROSE-regpos incorporates POS that contains more
linguistic information. Table 6 shows the results of
evaluating translations from English. According to
the table, ROSE performs less accurately than for
the into-English tasks, but overall the ROSE scores
are similar to those of BLEU.
127
Rank es-en fr-en de-en avg
BLEU 0.66 0.97 0.69 0.77
ROSE-reg 0.57 0.97 0.69 0.74
ROSE-rank 0.85 0.96 0.76 0.86
ROSE-regpos 0.59 0.98 0.71 0.76
ROSE-rankpos 0.83 0.96 0.69 0.82
Constituent
BLEU 0.78 0.92 0.30 0.67
ROSE-reg 0.79 0.90 0.39 0.69
ROSE-rank 0.66 0.92 0.33 0.64
ROSE-regpos 0.79 0.90 0.41 0.70
ROSE-rankpos 0.64 0.93 0.31 0.63
Yes/No
BLEU 0.99 0.96 0.66 0.87
ROSE-reg 0.92 0.93 0.67 0.84
ROSE-rank 0.78 0.96 0.61 0.78
ROSE-regpos 0.97 0.93 0.66 0.85
ROSE-rankpos 0.81 0.96 0.57 0.78
Table 5: System Level evaluation that translation into En-
glish
5 Conclusion
We presented the ROSE metric to make up for sev-
eral drawbacks of BLEU and other trained metrics.
Features including string matching, words ratio and
POS were combined by the supervised learning ap-
proach. ROSE?s overall performance was close to
BLEU on system level and sentence level. However,
it is better on tasks ROSE was specifically trained,
such as ROSE-rank in the system level ranking task
and ROSE-regpos in the syntactic constituents task.
Results also showed that when training data is not
available in the right language ROSE produces rea-
sonable results.
Smoothed BLEU slightly outperformed ROSE in
sentence evaluation. This might be due to the train-
ing data not being expert judgments, and conse-
quently very noisy. In further work, we shall mod-
ify the training method to better tolerate noise. In
addition, we will modify ROSE by substitute less
informative features with more informative features
in order to improve its performance and reduce over
fitting.
Rank es-en fr-en de-en avg
BLEU 0.85 0.98 0.88 0.90
ROSE-reg 0.75 0.98 0.93 0.89
ROSE-rank 0.69 0.93 0.94 0.85
Constituent
BLEU 0.83 0.87 0.35 0.68
ROSE-reg 0.73 0.87 0.36 0.65
ROSE-rank 0.72 0.78 0.32 0.61
Yes/No
BLEU 0.75 0.97 0.89 0.87
ROSE-reg 0.72 0.97 0.93 0.87
ROSE-rank 0.82 0.96 0.87 0.88
Table 6: System Level evaluation that translation from
English
References
Josha S. Albercht and Rebecca Hwa. 2008. Regres-
sion for machine translation evaluation at the sentence
level. Machine Translation, 22:1?27.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. Proceedings of the
ACL-05 Workshop.
Steven Bird and Edward Loper. 2004. Nltk: The natural
language toolkit. In Proceedings of the ACL demon-
stration session, pages 214?217, Barcelona, July.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation research. In In EACL, pages 249?
256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
128
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of trans-
lation metrics for improved evaluation and efficient
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 610?619, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008b.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A machine learning approach to the
automatic evaluation of machine translation. In pro-
ceedings of the Association for Computational Lin-
guistics.
Doddington and George. 2002. Automatic evalua-
tion of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, HLT ?02, pages 138?145, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
191?194, Columbus,Ohio,, June.
T. Joachims. 1999. Making large-scale svm learning
practical. Advances in Kernel Methods - Support Vec-
tor Learning,.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
C Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In In: Proceedings of the
international conference on language resources and
evaluation, pages 825?828, Lisbon, Portugal.
Alex J. Smola and Bernhard Schlkopf. 2004. A tuto-
rial on support vector regression. STATISTICS AND
COMPUTING, 14:199?222.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
L. Specia and J. Gimenez. 2010. Combining confidence
estimation and reference-based metrics for segment-
level mt evaluation. In The Ninth Conference of the
Association for Machine Translation in the Americas,
Denver,Colorado.
129
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 39?46,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using Senses in HMM Word Alignment
Douwe Gelling and Trevor Cohn
Department of Computer Science
University of Sheffield, UK
{d.gelling,t.cohn}@sheffield.ac.uk
Abstract
Some of the most used models for statis-
tical word alignment are the IBM models.
Although these models generate acceptable
alignments, they do not exploit the rich in-
formation found in lexical resources, and as
such have no reasonable means to choose bet-
ter translations for specific senses.
We try to address this issue by extending the
IBM HMM model with an extra hidden layer
which represents the senses a word can take,
allowing similar words to share similar output
distributions. We test a preliminary version of
this model on English-French data. We com-
pare different ways of generating senses and
assess the quality of the alignments relative to
the IBM HMM model, as well as the gener-
ated sense probabilities, in order to gauge the
usefulness in Word Sense Disambiguation.
1 Introduction
Modern machine translation is dominated by statis-
tical methods, most of which are trained on word-
aligned parallel corpora (Koehn et al, 2007; Koehn,
2004), which need to be generated separately. One
of the most commonly used methods to generate
these word alignments is to use the IBM models 1-5,
which generate one-directional alignments.
Although the IBM models perform well, they fail
to take into account certain situations. For exam-
ple, if an alignment between two words f1 and e1 is
considered, and f1 is an uncommon translation for
e1, the translation probability will be low. It might
happen, that an alignment to a different nearby word
is preferred by the model. Consider for example
the situation where f1 is ?taal? (Dutch, meaning lan-
guage), and e1 is ?tongue?. The translation probabil-
ity for this may be low, as ?tongue? usually translates
as ?tong?, meaning the body part. In this case the
preference of the alignment model may dominate,
leading to the wrong alignment.
Moreover, the standard tools for word alignment
fail to make use of the lexical resources that already
exist, and which could contribute useful information
for the task. In particular, the ontology defined in
WordNet (Miller, 1995) could be put to good use.
Intuitively, the translation of a word should depend
on the sense of the word being used. The current
work seeks to explore this idea, by explicitly mod-
eling the senses in the translation process. It does
so, by modifying the HMM alignment model to in-
clude synsets as an intermediate stage of translation.
This would facilitate sharing of translation distribu-
tions between words with similar senses that should
generate the correct sense. In terms of the example
above, one of the senses for ?tongue? will share the
translation distribution with ?language?, for which
we will have more relevant translation probabilities.
As well as performing word alignment this model
can be used to generate sense annotations on one
side of a parallel corpus, given an alignment, or even
generate sense annotations while aligning a corpus.
Thus, the model could learn to align a corpus and
do WSD at the same time. In this paper, the effect
the usage of senses has on alignment is investigated,
and the potential usefulness of the model for WSD
is explored. In the next section related work is dis-
cussed, after which in section 3 the current model is
39
discussed.
In section 4 the evaluation of the model is dis-
cussed, in two parts. In the first part, the model is
evaluated for English-French on gold standard man-
ually aligned data and compared to the results of the
base HMM model. In the second part, the model is
qualitatively evaluated by inspecting the senses and
associated output distributions of selected words.
2 Previous Work
Although most researchers agree that Word Sense
Disambiguation (WSD) is a useful field, it hasn?t
been shown to consistently help in related tasks. Ma-
chine Translation is no exception, and whether or
not WSD systems can improve performance of MT
systems is debated. Furthermore, it is unclear how
parallel corpuses can be exploited for WSD systems.
In this section we will present a brief overview of re-
lated work.
(Carpuat and Wu, 2007) report an improvement
in translation quality by incorporating a WSD sys-
tem directly in a phrase-based translation system.
This is in response to earlier work done, where in-
corporating the output of a traditional WSD system
gave disappointing results (Carpuat and Wu, 2005).
The WSD task is redefined, to be similar to choosing
the correct phrasal translation for a word, instead of
choosing a sense from a sense inventory. This sys-
tem is trained on the same data as the SMT system
is.
The output of this model is incorporated into the
machine translation system by providing the WSD
probabilities for a phrase translation as extra features
in a log-linear model (Carpuat and Wu, 2007). This
system consistently outperforms the baseline system
(the same system, but without WSD component), on
multiple metrics, which seems to indicate that WSD
can make a useful contribution to machine transla-
tion. However, the way the system is set up, it could
also be viewed as a way of incorporating translation
probabilities of other systems into the phrase-based
translation model.
(Chan and Ng, 2007) introduce a system very sim-
ilar to that of (Carpuat and Wu, 2007), but as ap-
plied to hierarchical phrase-based translation. They
demonstrate modest improvements in BLEU score
over the unmodified system, as well as some qualita-
tive improvements in the output. Here again, the ar-
gument could be made that what is being done is not
strictly word sense disambiguation, but augmenting
the translation system with extra features for some
of the phrase translations.
In (Tufis? et al, 2004) parallel corpora and aligned
WordNets are exploited for WSD. This is done, by
word aligning the parallel texts, and then for ev-
ery aligned pair, generating a set of wordnet sense
codes (ILI codes, or interlingual index codes) for ei-
ther word, corresponding to the possible senses that
word can take. As the wordnets for both languages
are linked, if the ILI code of a sense is the same, the
sense should be sufficiently similar. Thus, the in-
tersection of both sets of ILI is taken to find an ILI
code that is common to both pairs. If such a code is
found, it represents the sense index of both words.
Otherwise, the closest ILI code to the two most sim-
ilar ILI codes is found, and that is taken as the sense
for the word. The current work however only uses
a lexical resource for one of the languages, and as
such has fewer places to fail, and less demanding
requirements.
Other similar work includes that in (Ng et al,
2003), where a sense-annotated corpus was automat-
ically generated from a parallel corpus. This is done
by word-aligning the parallel corpus, and then find-
ing the senses according to WordNet given a list of
nouns. Two senses are lumped together if they are
translated into the same chinese word. The selec-
tion of correct translations is done manually. Only
those occurrences of the chosen nouns that translate
to one of the chosen chinese words are considered
sense-tagged by the translation.
Although similar in approach to what the current
system would do, this system uses a much more sim-
ple approach to generate sense annotations and it de-
pends on a previously word-aligned corpus, whereas
the current approach would integrate alignment and
sense-tagging, whis may give a higher accuracy.
3 Senses Model
The current model is based on the HMM alignment
model (Vogel et al, 1996), as it is a less complex
model than IBM models 3 and above, but still finds
acceptable alignments. The HMM alignment model
is defined as a HMM model, where the observed
40
e a2a1
fmf1 f2
am
Figure 1: Diagram of HMM model. Arrows indicate
dependencies, grey nodes indicate known values, white
nodes indicate hidden variables.
variables are the words of a sentence in the French
language f, and the hidden variables are alignments
to words in the English sentence e, or to a null state.
See figure 1 for a diagram of the standard HMM
model. Under this model, French words can align to
at most 1 English word. The transition probability
is not dependent on the english words themselves,
but on the size of jumps between alignments and the
length of the English sentence. The probability of
the French sentence given the English sentence is:
Pr(f|e) =
?
a
J?
j=1
p(fj |eaj )p(aj |aj?1, I) (1)
Here, f and e denote the French and English sen-
tences, which have lengths J and I respectively, and
a denotes an alignment of these two sentences. So,
the states in the HMM assign a number from the
range [0, I] to each of the positions j in the French
sentence, effectively assigning one English word eaj
to each French word fj , or a NULL translation e0.
The term p(fj |eaj ) is the translation probability of a
pair of words, and p(aj |aj?1, I) gives the transition
probability in the HMM.
Here, i is the current state of the HMM, and i? is
the previous state of the HMM, each being an index
into the English sentence and p(aj |aj?1, I) is de-
fined as the probability of the gap between i and i?.
So, if in an alignment French word 2 is aligned to the
3rd English word, and the next French Word (3) is
aligned to the 5th English word, p(aj |aj?1, I) isn?t
modelled directly as p(5|3, I), but as p(5? 3|I).
To implement a dependency on senses in the
model an extra hidden layer is added to the HMM
model, representing the senses. The probability of a
s1 s2
e a2a1
fmf1 f2
am
sm
Figure 2: Diagram of SHMM model, with senses gener-
ated by the English words. Arrows indicate dependen-
cies, grey nodes indicate known values, white nodes in-
dicate hidden variables.
french word then depends on the generated sense,
the probability of which depends on the English.
The possible senses for a given English word is con-
strained by an external source, such as WordNet.
The probability under the model of a french sen-
tence f given an English sentence e thus becomes:
Pr(f|e) =
?
a
J?
j=1
p?(fj |eaj )p(aj |aj?1, I) (2)
where
p?(fj |eaj ) =
K?
k=1
p(fj |sk)p(sk|eaj ) (3)
Here, K is the number of senses that english word
associated with this translation pair. The senses will
be constrained either by the English word eaj or by
the French word fj depending on which language
the sense inventory is taken from. The first case,
with senses constrained by the English, will be de-
noted with SHMM1, and the second with SHMM2.
In this work, only SHMM1 is used.
If the amount of senses defined for each word is
exactly 1 and this sense is different for each word,
the model reduces to the HMM model (see Figure
2). However, if the sense inventory is defined such
that for two different words with a sense that is sim-
ilar, the same sense can be used, the model is able
to use translation probabilities drawn from observa-
tions from both these words together. For example,
41
in SHMM1, the words ?small? and ?little? may have
the same sense listed in the sense inventory, which
allows the model to learn a translation distribution to
the French words that both these words often align
to.
For training this model, as with the IBM models,
Expectation-Maximization and initialisation are key.
The more complex IBM models are initialised from
simpler versions, so the complex models can start
out with reasonable estimates, which allow it to find
good alignments. Here, too, the same steps are used.
The HMM model is initialised from Model 1, as de-
scribed in citevogel:1996. From this, the SHMM
models can be initialised.
For the SHMM1, given a translation probability
for a french word given an english word under the
HMM, p(f |e), and a list of valid senses for that
english word e, an equal portion of that translation
probability is given to the new translation probabil-
ity depending on the sense. This is done for all trans-
lation probabilities, and the translation table is then
normalised. Probability of a sense given an english
word is initialised to a uniform distribution over the
valid senses.
For the SHMM2, the probability of french words
given a sense is set to uniform over the words for
which the sense is valid, and the probability of the
sense given the english word is calculated analogous
to the probability of the french word given the sense
in the first case.
After initialisation, the expectation-maximisation
algorithm can be used for training, as with the HMM
model, using the forward-backward algorithm to
find the posterior probabilities of the alignments. As
the senses can be summed out during this phase, the
algorithm can be used as-is, and afterwards the pro-
portion of the partial count that should be assigned
to each sense can be found. By summing out over
the relevant senses and words, the two parts c(fj |qk)
and c(qk|ei) can then be found.
3.1 Generating Senses for Words
In order to be able to use this model, an inventory
of senses is needed for every word in the corpus, for
one of the languages. The most obvious source for
this is the English Wordnet (Miller, 1995), as it has
a large inventory of senses. Note that, in this doc-
ument, the words senses and synsets are used inter-
changeably.
The process of obtaining this inventory is ex-
plained from the viewpoint of using English Word-
Net, but the same basic conditions apply for any
other lexicon, or language. The inventory of senses
is obtained through the WordNet corpus in NLTK
1, which automatically stems the words that synsets
are sought for.
In this model, two senses (synsets) are function-
ally equivalent, if the list of words that have them
in their senselist is the same for both senses. That
is to say, if the partial counts that will be added to
either of the senses will be the same, there is no way
of distinguishing between the two senses under this
model. For example, in WordNet 3.0, among the
synsets listed for the word ?small?, there are 3 that
have as constituent words only ?small? and ?little?.
These 3 synsets would be functionally equivalent for
our purposes. When this occurs, the senses that are
equivalent are collated under one name, so that it?s
possible to find out which senses a particular sense
is made up of.
At this point, there will be some words with only
a sense that is unique to that word (such as those
words that were not in the lexicon, which get a newly
made sense), some words with only shared senses
and some with a mix. We might want to enforce one
of a few distinct options:
? All words have exactly 1 unique sense, and per-
haps a few shared ones (?synthesis? condition)
? Some words have a unique sense, some don?t
(?merge? condition)
? No words have unique senses if they have at
least 1 shared sense (?none? condition)
These conditions are generated by first finding the
filtered list of senses for each word. At this point,
some words have only unique senses, either because
they didn?t occur in WordNet, or because WordNet
only listed unique senses for that word (the ?merge?
condition. The ?synth? condition is made, by finding
all words that have only shared senses, and adding a
new sense, that is unique to that word. The ?none?
1http://www.nltk.org/
42
1 2 3 4 5Number of iterations0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
AER
Model 1HMMSHMM (none)SHMM (merge)SHMM (synth)
Figure 3: AER scores for Model 1, HMM, and 3 SHMM variations trained for 5 iterations each, lower is better.
condition then is found by doing the opposite: re-
moving all unique senses from words that also have
shared senses.
Under each of these 3 conditions, the model might
work slightly differently. Under the ?synthesis? con-
dition, it may generate the translation probabilities
either directly, as in the HMM (which is what hap-
pens for any word with only 1 sense, which is unique
for that word), or from the shared probabilities,
through the senses. In the other models, the model
is increasingly forced to use the shared translation
probabilities.
4 Evaluation
We will evaluate the early results of this model
against the HMM and Model 1 results, and will do
a qualitative analysis of the distribution over senses
and French words that the model obtains, in order
to find out if reasonable predictions for senses are
made.
The sense HMM model will be evaluated using
the three sense inventories suggested in subsection
3.1. The dataset used was a 1 million sentence
aligned English-French corpus, taken from the Eu-
roparl corpus (Koehn, 2005). The data was to-
kenised, length limited to a maximum length of 50,
and lowercased. The results are evaluated on the test
set from the ACL 2005 shared task, using Alignment
Error Rate. The models are all trained for 5 itera-
tions, and a pruning threshold is employed that re-
moves probabilities from the translation tables if it
is below 1.0 ? 10?6.
The results of training models based on senses
generated in the 3 ways listed above is shown in
Figure 3. The three SHMM models are compared
against Model 1, and the standard HMM model,
each of which is trained for 5 iterations. The HMM
model is initialised from Model 1, and the SHMM
models initialised from the HMM model. As the fig-
ure shows, the AER score for the last two iterations
of the HMM model is very similar to the scores that
the three variations of the SHMM model attain. The
scores for the three HMM models range from 0.185
to 0.192
A possible reason for this performance is that the
models didn?t have enough sharing going on be-
tween the senses. The corpus contains 70700 unique
words. Looking at the amount of senses that are
found in the ?none? condition, meaning that all of the
WordNet senses share output probabilities, there are
17194 words that have at least one of these senses
listed, and there are 27120 distinct senses available
in that setting. For the other 53500 senses, no shar-
ing is going on whatsoever.
In the ?merge? and ?synth? conditions, there are
more senses taken from WordNet (for a total from
WordNet of 33133), but these don?t add any shar-
43
Sense Definition P (s|e) Most likely French words in order
severe.s.06 very bad in degree or
extent
0.4861 graves, se?ve`res, des, se?ve`re, grave, de, grave-
ment, une, se?rieuses, les
severe.s.04 unsparing and un-
compromising in
discipline or judg-
ment
0.2358 graves, se?ve`res, des, se?ve`re, grave, de, grave-
ment, une, se?rieuses, les
dangerous.s.02 causing fear or anx-
iety by threatening
great harm
0.1177 grave, des, graves, les, se?rieux, tre`s,
se?rieuses, une, importantes, se?rieuse
austere.s.01 severely simple 0.1148 graves, des, grave, se?ve`re, se?ve`res, tre`s, forte-
ment, forte, rigoureuses, situation
hard.s.04 very strong or vigor-
ous
0.035 dur, plus, importants, des, se?ve`res, durement,
son, une, difficile, tre`s
severe.s.01 intensely or ex-
tremely bad or
unpleasant in degree
or quality
0.01055 terrible, terribles, des, grave, les, mauvais,
dramatique, cette, aussi, terriblement
Table 1: Senses for the word ?severe? in the ?none? version of the SHMM model, their WordNet definition, the proba-
bility of the sense for the word severe, and the most likely French words for the senses given in order of likelihood.
ing. It might be then, that the model has insuffi-
cient opportunity to share output distributions, caus-
ing it to behave much as the HMM alignment model.
Another possibility is, that the senses insufficiently
well-defined, and share probabilities between words
that are too dissimilar, negating any positive effect
this may have and possibly pushing the model to-
wards less sharing. We will suggest possibilities for
dealing with this in section 5.
Regardless of the performance of the model in
word alignment, if the model learns probabilities for
senses that are reasonable, it can be used as a word
sense disambiguation system for parallel corpora,
with the candidate senses being made up from the
senses out of WordNet. Those words not listed in
WordNet, are treated as being monosemous words
in this context. The ?merge? and ?none? conditions
are most useful for this: if a WSD system chooses a
sense that is not linked to a WordNet sense, it is not
clearly defined which sense is meant here.
In order to find out if the model makes sensi-
ble distinctions between different senses, we have
picked a random polysemous word, and looked at
the senses associated with it in the ?none? condition.
The word that was chosen is ?severe?. It has 6 pos-
Sense Associated English words
severe.s.06 (only has basic 3 senses)
severe.s.04 spartan
dangerous.s.02 dangerous, grave, graver,
gravest, grievous, life-
threatening, serious
austere.s.01 austere, stark, starker, starkest,
stern
hard.s.04 hard, harder, hardest
severe.s.01 terrible, wicked
Table 2: Senses for the word ?severe? in the ?none? ver-
sion of the SHMM model and the English words apart
from ?severe?, ?severer? and ?severest? that have the sense
in their senselist
sible senses, listed by main word and definition in
Table 1, along with the probability of the senses,
p(s|e), and the 10 most likely French words for the
senses.
As the table shows, the two most likely senses are
quite similar. In fact, because words are stemmed
before looking up suitable senses, all senses have at
least the following 3 words associated with them:
?severe?, ?severer? and ?severest?. The words that
44
Sense Definition P(s?e) Most likely French words in order
rigorous.s.01 rigidly accurate; al-
lowing no deviation
from a standard
0.8962 rigoureuse, rigoureux, une, rigueur,
rigoureuses, des, un, stricte, strict, strictes
rigorous.s.02 demanding strict at-
tention to rules and
procedures
0.1038 des, strictes, rigoureux, stricte, se?ve`res,
rigoureuses, stricts, rigoureuse, une, se?ve`re
Table 3: Senses for the word ?rigorous? in the ?none? version of the SHMM model, their WordNet definition, the
probability of the senses of the word ?rigorous?, and the most likely French words for the senses given in order of
likelihood.
cause the differences between the senses are listed
in table 2. It can be seen that the only difference
between severe.s.04 and severe.s.06 is the addition
of the word ?spartan? for the first. As ?spartan? only
occurs 67 times in the corpus, versus 484 for severe,
it is possible that they are so similar, because the
counts for ?spartan? get overshadowed.
For the other senses however, the most likely
translations vary quite a bit. The sense ?hard.s.04?,
meaning very strong or vigorous, also includes
translations to ?plus? and ?dur?, which seems more
likely given the sense. Given these translation prob-
abilities though, it should at least be possible to dis-
tinguish between different senses of the word severe,
given that it?s aligned to a different french word.
One more example is listed in table 3, showing
the probabilities for two different senses, and their
most likely translations. The most likely sense for
rigorous under the model is in the sense of ?allowing
no deviation from a standard?. This is the only of the
two senses that can translate to ?rigueur? in french,
literally rigor. The other sense, meaning ?demand-
ing strict attention to rules and procedures?, is more
likely to translate to ?strictes?, ?stricte? and ?se?ve`res?,
which reflects the WordNet definition.
The difference in contributing English words be-
tween these two senses can be found in Table 4. In-
terestingly, the three forms of the word strict are as-
sociated with the sense rigorous.s.01, even though
the naive translations of these words into French are
more likely for rigorous.s.02. Even so, the results
match the WordNet definitions better.
These results show that useful translations are
found, and the corresponding senses can be learned
as well. For sense discrimination in parallel cor-
puses then, this model shows potential, and for
Sense Associated English words
rigorous.s.01 rigorous strict stricter
strictest
rigorous.s.02 rigorous stringent tight
tighter tightest
alignment good alignments can be found, even with
better abstraction in the model.
5 Conclusion
The results have shown that this may be a useful way
to incorporate senses in a word alignment system.
While the alignment results in themselves weren?t
significantly better, alignment probabilities to senses
have been shown to be generated, which make it pos-
sible to distinguish between different senses. This
could open the door to automatically sense annotat-
ing parallel corpora, using a predefined set of senses.
At this early point, several options lay open to
improve upon the results so far. To improve the
alignment results, more encompassing senses may
be generated, for example by integrating similar
synsets. At the same time, the list of synsets for
each word may be improved upon, by filtering out
very unlikely senses for a word.
It should also be possible to employ an already ex-
isting WSD system to annotate the parallel corpus,
and use the counts of the annotated senses to better
initialise the senses, rather than starting out assum-
ing all are equaly likely for a given word. This may
be used as well to initialise the translation probabil-
ities for senses.
45
References
Marine Carpuat and Dekai Wu. 2005. Word sense disam-
biguation vs. statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 387?
394, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In In The 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007, pages 61?72.
Yee Seng Chan and Hwee Tou Ng. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In In 45th Annual Meeting of the Association
for Computational Linguistics (ACL-07, pages 33?40.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004 (Conference of the
Association for Machine Translation in the Americas),
volume 3265, pages 115?124. Springer.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Machine Translation
Summit X, pages 79?86, Phuket, Thailand.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
- Volume 1, ACL ?03, pages 455?462, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Dan Tufis?, Radu Ion, and Nancy Ide. 2004. Fine-grained
word sense disambiguation based on parallel corpora,
word alignment, word clustering and aligned word-
nets. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on
Computational linguistics - Volume 2, COLING ?96,
pages 836?841, Stroudsburg, PA, USA. Association
for Computational Linguistics.
46
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64?80,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The PASCAL Challenge on Grammar Induction
Douwe Gelling and Trevor Cohn
Department of Computer Science
University of Sheffield, UK
{d.gelling,t.cohn}@sheffield.ac.uk
Phil Blunsom
Department of Computer Science
University of Oxford, UK
Phil.Blunsom@cs.ox.ac.uk
Joa?o Grac?a
L2F Spoken Language Systems Laboratory
INESC ID Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Abstract
This paper presents the results of the PASCAL
Challenge on Grammar Induction, a compe-
tition in which competitors sought to predict
part-of-speech and dependency syntax from
text. Although many previous competitions
have featured dependency grammars or parts-
of-speech, these were invariably framed as
supervised learning and/or domain adaption.
This is the first challenge to evaluate unsuper-
vised induction systems, a sub-field of syntax
which is rapidly becoming very popular. Our
challenge made use of a 10 different treebanks
annotated in a range of different linguistic for-
malisms and covering 9 languages. We pro-
vide an overview of the approaches taken by
the participants, and evaluate their results on
each dataset using a range of different evalua-
tion metrics.
1 Introduction
Inducing grammatical structure from text has long
been fundamental problem in Computational Lin-
guistics and Natural Language Processing. In re-
cent years interest has grown, spurred by advances
in unsupervised statistical modelling and machine
learning. The task has relevance to cognitive scien-
tists and linguists attempting to gauge the learnabil-
ity of natural language by human children, and also
natural language processing researchers who seek
syntactic representations for languages with few lin-
guistic resources.
Grammar learning has been popular in previous
challenges. For example the CoNLL shared tasks in
2006 and 2007 (Buchholz and Marsi, 2006; Nivre
et al, 2007) involved supervised learning of de-
pendency parsers across a wide range of different
languages. Our challenge has many similarities to
these, in that we focus on dependency grammars,
however we seek to evaluate unsupervised algo-
rithms only using syntactically annotated data for
evaluation and not for training. Additionally we also
consider the related task of part-of-speech (POS) in-
duction, and the next logical challenge: the joint
task of POS and dependency induction. Other re-
lated challenges can be found in the formal gram-
mar community (e.g., the Omphalos1 competition)
in which competitors seek to learn synthetic lan-
guages. In contrast we seek to model natural lan-
guage text, which entails many different challenges.
Research into unsupervised grammar and POS in-
duction holds considerable promise, although cur-
rent approaches are still a long way from solving
the general problem. For example, the majority of
recent research into dependency grammar induction
has adopted the evaluation setting of Klein and Man-
ning (2004) who learn grammars on strings of POS
tags, rather than on words themselves. One aim of
this challenge is to popularise the more difficult and
ambitious task of inducing grammars directly from
text, which can be viewed as integrating the POS and
grammar induction tasks. A second aim is to foster
grammar and POS induction research across a wider
variety of languages, and improving the standard of
evaluation.
We have collated data from existing treebanks in
a variety of different languages, domains and lin-
guistic formalisms. This gives a diverse range of
1See http://www.irisa.fr/Omphalos
64
data upon which to test induction algorithms, yield-
ing a deeper insight into their strengths and short-
comings. One key problem in grammar induction
research is how to evaluate the models? predictions
given that often many different analyses are linguis-
tically plausible, e.g., the choice of whether deter-
miners or nouns should head noun phrases, or how to
represent coordination. Simply comparing against a
single gold standard often results in poor reported
performance because the model has discovered a
different analysis to that used when annotating the
treebank. For this reason it has been popular to use
lenient measures for comparing predicted trees to
the treebank gold standard trees, such as undirected
accuracy and the neutral edge distance (Schwartz et
al., 2011). As well as evaluating using these popular
metrics, we also propose a new method of evaluation
which is also lenient in that it rewards different types
of linguistically plausible output, but requires con-
sistency in the output, something the previous meth-
ods cannot do.
The paper is organised as follows. Section 2 de-
scribes the tasks and our data format and section 3
outlines the different treebanks used for the chal-
lenge. The baselines, our own benchmark systems
and the competitors entries are described in section
5. In section 6 we present and analyse the results
for the three different tracks. Finally we conclude in
section 7.
2 Task Definition
The three tracks of the WILS challenge are de-
scribed below. First we describe the data format for
the submissions common to the three tracks (POS
induction, Dependency induction, and jointly induc-
ing both), and then the three tracks are described
along with the respective evaluation metrics.
2.1 Data format
All datasets were presented in a file format similar to
that used in the CoNLL tasks, but with slight mod-
ifications. In particular the last two columns are re-
moved, as no projective head or projective depen-
dency relations were used, and an extra POS column
was inserted at column 6 to accommodate the Uni-
versal POS tagset (Petrov et al, 2011). Each line in a
file then either consists of 9 columns, separated by a
tab character, or is an empty line. Empty lines sepa-
rate sentences, and all other lines give the annotation
for a single token in the sentence as follows:
1. ID: Token counter, gives the index of current
word in the sentence. Indexing starts at 1.
2. FORM: Surface form of the token in the sen-
tence.
3. LEMMA: Stemmed form of the word form if
available.
4. CPOSTAG: Coarse-grained POS tag.
5. POSTAG: Fine-grained POS tag, or CPOSTAG
again if not available.
6. UPOSTAG: Universal POS tag, based on the
POSTAG and CPOSTAG.
7. FEATS: List of syntactic / morphological fea-
tures, separated by a vertical pipe (|).
8. HEAD: Syntactic head of the token, with 0 in-
dicating the root node.
9. DEPREL: The general type of the dependency
relation, e.g., subject.
In this setup, the LEMMA, FEATS and DEPREL
columns are optional, in which case an underscore
( ) will be used as a placeholder. Each treebank
was split into training, development and testing par-
titions. The HEAD and DEPREL entries were only
supplied for the development and the final testing
sets,2 but not for the training partition. The com-
petitors were encouraged to develop their unsuper-
vised entries on the union of the three partitions, and
make sparse use of the development set, i.e., for san-
ity checking more than model fitting in order to min-
imise the extent of supervision.
2.2 POS induction
In the POS induction track, participants developed
systems to induce the Part-of-Speech (POS) classes
for each word in the testing corpus. In order to train
the systems, the same training and development sets
were used as for the other tracks. These corpora in-
cluded manually supplied POS tags for each token,
2For the initial test set these fields were omitted.
65
which were not to be used for training, only evalua-
tion. Participants submitted predicted tags for each
token, which were scored against the gold-standard.
For evaluation, we used 4 different metrics. The
first is the many-to-one metric (M-1) (also known
as cluster purity), which is widely used for cluster
evaluation as well as evaluation of POS induction.
This metric assigns each word cluster to its most
common tag, and then measures the proportion of
correctly tagged words. The second metric is the
one-to-one mapping (1-1), a constrained version of
Many-to-one mapping in which each predicted tag
is associated with only one gold-standard tag and
vice versa (Haghighi and Klein, 2006). Word clus-
ters are assigned greedily to tags, and in the event
of there being more word classes than tags, some
word classes will be left unassigned. Another met-
ric that was used is Variation of information (VI)
(Meila, 2003), which is based the conditional en-
tropy of between the two different clusterings (John-
son, 2007). Lastly, we use the V-measure (VM) met-
ric (Rosenberg and Hirschberg, 2007), which is an-
other entropy-based measure, but defined in terms of
a F score to balance precision and recall terms (we
use equal weighting of the two factors). Please see
Christodoulopoulos et al (2010) for further details
about these metrics.3 For these metrics, a higher
score is better, with the exception of VI.
For all these metrics, the induced tags are eval-
uated against the universal pos tags, as this means
there are a consistent number of tags across the lan-
guages. Using these metrics, the results will vary as
a result of predicting a different number of tags (in
particular, more tags will mean a higher score for M-
1, and the converse is true for 1-1). However, using
the universal POS tags, we think will make results
less sensitive to large differences in POS inventory
between languages (such as for the Dutch dataset).
2.3 Dependency induction
For the Dependency induction track, the training
data consisted of the original treebank data, but
without dependency annotations. A development set
was also provided, which included the dependency
annotations, but this was meant mainly as a way to
3Thanks to Christos Christodoulopoulos for sharing his im-
plementation of the POS induction metrics, which we have used
in our evaluation.
verify systems, as we mean to minimise the amount
of supervision in the task. The participants were
later supplied with test sets for which the systems
could generate predictions. Only after the predic-
tions were submitted were the fully annotated test
sets released.
The dependency inductions were evaluated on
3 metrics: directed accuracy, undirected accuracy
and Neutral Edge Detection (NED) (Schwartz et
al., 2011). Directed accuracy is the ratio of cor-
rectly predicted dependencies (including direction)
over total amount of predicted dependencies. Undi-
rected accuracy is much the same, but also considers
a predicted dependency correct if the direction of the
dependency is reversed (e.g. if the predicted depen-
dency is not A ? B, but B ? A). Lastly, the NED
metric is a variant of undirected accuracy that also
rewards cases where an edge-flip occurs, meaning
that the predicted parent of a token is actually the
grandparent of that same token in the gold-standard
data. Note that before evaluating with these metrics
punctuation was removed from all sentences, and
any child words under a punctuation node were re-
attached to their nearest ancestor that wasn?t punc-
tuation.
The final ?joint? task consisted of inducing depen-
dency structure from only the tokens in the corpus,
without recourse to the gold POS tags. Where POS
is predicted (e.g., in a pipeline), we included these
in our general POS evaluation. The induced depen-
dency trees were evaluated with the same metrics as
in the dependency induction track, but are consid-
ered separately. We expect these systems to have
lower scores overall due to the lack of gold-standard
POS tags.
3 Treebanks
We selected a number of different treebanks for use
in the challenge, aiming to represent a wide range
of different languages, dialects and genres of text.
In total we used ten different treebanked corpora
in nine different languages. For the practical rea-
sons of simplifying the administration of the chal-
lenge and allowing the data to be reused in future re-
search, we chose corpora with licences allowing ei-
ther free redistribution, or those held by the Linguis-
66
tic Data Consortium (LDC).4 Many of these datasets
have been used before in dependency grammar or
part-of-speech research, particularly the shared tasks
at CoNLL 2006 and 2007. For the purpose of
the competition, we have updated these datasets to
include any annotation updates or additional data,
where available. It is important for unsupervised ap-
proaches to have sufficient amounts of data, espe-
cially given the common sentence length limitations
imposed by most dependency grammar models. As
described in section 2, we have included an extra
field for the universal part-of-speech (UPOS) using
Petrov et al (2011)?s automatic conversion tool.5
Below we describe the different treebanks used,
and the conversion process into our data format for
the purpose of the competition. Please see Table 1
for statistics on each of the treebanks.
Dependency treebanks We used the following
dependency treebanks: Arabic The Prague Ara-
bic Dependency Treebank V1 (Hajic? et al, 2004).6
Basque The Basque 3lb dependency treebank
(Aduriz et al, 2003). Czech The Prague Depen-
dency Treebank 2.0 (Bo?hmova? et al, 2001).7 Dan-
ish The Copenhagen Dependency Treebank ver-
sion 2 (Buch-Kromann et al, 2007). English The
CHILDES US/Brown subcorpus (Sagae et al,
2007). Slovene The jos500k Treebank (Erjavec et
al., 2010). 8 Swedish The Talbanken treebank
(Nivre et al, 2006). The conversion of each of these
treebanks was quite straightforward as they were al-
ready annotated for dependencies. Moreover, many
of these corpora had been used previously in the
CoNLL 2006 and 2007 shared tasks, and therefore
we were able to reuse this data and/or their conver-
sion scripts. In the case of Arabic and Swedish we
used the exact same data, simply converting from
CoNLL dependency format into our own format (re-
moving redundant columns and adding a UPOS col-
umn). While many of the other corpora had also
4In the following corpus descriptions, when not otherwise
specified the corpus is freely available for research purposes.
5http://code.google.com/p/
universal-pos-tags
6LDC catalogue number LDC2004T23.
7LDC catalogue number LDC2006T01.
8For the shared task, the annotation was converted to english
using the tables found at the JOS website: http://nl.ijs.
si/jos/msd/html-en/index.html
been used previously, our data is different, making
use of subsequent corrections to these treebanks and
additional annotated data now available.
First language acquisition provides an important
motivation for grammar induction research, conse-
quently we have included data from the CHILDES
database of child-directed speech. We use the
Brown sub-corpus, a longitudinal study of parent-
child interactions for three children aged between 18
months and 5 years old. The corpus has been man-
ually annotated with syntactic dependencies (Sagae
et al, 2007) and morphology. From this we take all
child-directed utterances, extracting word, morphol-
ogy, part-of-speech and dependency markup, and
developed our own conversion into UPOS. Our test-
ing and development sets were drawn from the first
15 Eve files which were manually annotated for de-
pendency structure. The rest of the corpus, which
had not been manually annotated for syntax, was
merged to form the training set.
Phrase-structure treebanks As well as depen-
dency treebanks, we used three different phrase-
structure treebanks: The Dutch Alpino treebank
(Bouma et al, 2000), the English Penn Treebank
V3 (Marcus et al, 1993),9 and the Portuguese Flo-
resta Sinta?(c)tica treebank (Afonso et al, 2002). As
these treebanks do not explicitly mark dependen-
cies, we automatically extracted these using head
finding heuristics. Thankfully the difficult work
of creating such scripts has already been done as
part of the CoNLL shared tasks. We have reused
their scripts to create dependency representations of
these treebanks, before converting into our file for-
mat and augmenting with UPOS annotation. In the
case of Dutch, we have reused the same CoNLL
2006 data; note that this dataset includes predicted
part-of-speech rather than gold standard annotation
(Buchholz and Marsi, 2006). For the Portuguese,
we used the same Bosque 7.3 sub-corpus10 from
CoNLL 2006, additionally including in our training
set the recently-annotated Selva 1.0 subcorpus.
The Penn Treebank is the most common data set
in parsing and grammar induction. We have patched
9LDC catalogue number LDC99T42.
10An updated version of this corpus is available, however
the file format had changed significantly and we were unable
to adapt the conversion scripts in time for the competition.
67
ar cs da en-childes en-ptb eu nl pt sl sv
annotation d d d d p d p p d d
Training data
Tokens 106.6k 1.2M 68.5k 312.8k 1.1M 124.7k 192.2k 196.4k 193k 184.6k
Sentences 2.8k 68.5k 3.6k 57.4k 45.4k 9.1k 13k 8.7k 9.4k 10.7k
Tokens/sent 38.4 17.1 18.8 5.5 23.9 13.7 14.8 22.6 20.5 17.3
CPOSTAG 15 12 25 31 31 16 13 16 13 41
POSTAG 21 61 141 76 45 50 300 22 31 41
FEATS 22 75 338 29 0 269 310 146 46 0
Development data
Tokens 5.1k 159k 17k 25.3k 32.9k 12.6k 2.9k 10.3k 20.2k 6.9k
Sentences 139 9.3k 1k 5k 1.3k 1k 386 400 1k 389
Tokens/sent 36.8 17.1 17 5.1 24.4 12.5 7.4 25.8 20.2 17.6
% New words 27.5 26 49.8 9.8 11.4 46.1 18.8 27.5 38.7 13.8
Test data
Tokens 5.1k 173.6k 14.7k 28.4k 56.7k 14.3k 5.6k 5.9k 22.6k 5.7k
Sentences 131 10.1k 1k 5.2k 2.4k 1.1k 386 288 1k 389
Tokens/sent 39.1 17.1 14.7 5.4 23.5 12.7 14.5 20.4 22.6 14.5
% New words 24.3 25.3 43.7 9 12.1 51.5 40.5 25.2 37.1 34.6
Table 1: Properties of the treebanks. We report the linguistic annotation method (dependency vs. phrase-structure),
the size of each treebank, the number of types for the different granularities of part-of-speech tags and morphological
features (note that UPOS has a fixed set of 12 tags), and the proportion of word types that were not present in training.
the treebank to include NP-internal structure using
Vadas and Curran?s annotations (Vadas and Cur-
ran, 2007), which was then converted to dependency
structures using the penn-converter11 script
(Johansson and Nugues, 2007). This tool has a num-
ber of options controlling the linguistic decisions
in converting from phrase-structure to dependency
trees, e.g., the treatment of coordination. We ex-
tracted five versions of the treebank, each encoding
each different sets of linguistic assumptions (Tsar-
faty et al, 2011).12 These are denoted default, old-
LTH, CoNLL-2007, functional and lexical; for the
main results we used the standard options, we also
report separately evaluations using each of the five
variants. The treebank was partitioned into training
(sections 0-22), development (sec. 24) and testing
sets (sec. 23).
4 Baselines and Benchmarks
A number of standard baselines and previously pub-
lished benchmark systems were implemented for
each task in order to place the submitted systems in
context.
11http://nlp.cs.lth.se/software/treebank_
converter
12Note that Tsarfaty et al (2011) also propose an evalua-
tion metric for comparing dependency trees, which we have not
used. Note however that it could, in principle, be used for simi-
lar evaluations.
The standard baseline for grammar induction
models is to assume either left branching or right
branching analyses (LB, RB). These capture the ten-
dency for languages to favour one attachment direc-
tion over another. The most frequently cited and
extended model for dependency induction is DMV
(Klein and Manning, 2004). We provide results for
this model trained on each of the coarse (DMVc), fine
(DMVp), and universal (DMVu) POS tag sets, all ini-
tialised with the original harmonic initialiser. As a
further baseline we also evaluated the dependency
trees resulting from directly using the harmonic ini-
tialiser without any training (H).
As a strong benchmark we include the results of
the non-parametric Bayesian model previously pub-
lished in Blunsom and Cohn (2010) (BC). The stated
results are for the unlexicalised model described in
that paper where the final analysis is formed by
choosing the maximum marginal probability depen-
dency links estimated from forty independent Gibbs
sampler runs.
For part-of-speech tagging we include results
from an implementation of the Brown word clus-
tering algorithm (Brown et al, 1992) (Bc,p,u), and
the mkcls tool written by Franz Och (Och, 1999)
(MKc,p,u). Both of these benchmarks were trained
with the number of classes matching the number
in the gold standard of each of the tagsets in turn:
coarse (c), fine (p), and universal (u). A notable
68
property of both of these word class models is that
they enforce a one-tag-per-type restriction that en-
sures there is a one-to-one mapping between word
types and classes.
For POS tagging we also provide benchmark re-
sults from two previously published models. The
first of these is the Pitman-Yor HMM model de-
scribed in (Blunsom and Cohn, 2011), which in-
corporates ta one-tag-per-type restriction (BC). This
model was trained with the same number of tags as
in the gold standard fine tag set for each corpus. The
second benchmark is the HMM with Sparsity Con-
straints trained using Posterior Regularization (PR)
described in (Grac?a et al, 2011). In this model
the HMM emission probabilitiy distribution are esti-
mated using small Maximum Entropy models (fea-
tures set described in the original paper). The mod-
els were trained for 200 iterations of PR using both
the same number of hidden states as the coarse Gc
and universal Gu gold standard. All parameters were
set to the values described in the original paper.
5 Submissions
The shared task received submissions covering a di-
verse range of approaches to the dependency and
part-of-speech induction challenges. Encouragingly
all of these submissions made significant departures
from the benchmark HMM and DMV approaches
which have dominated the published literature on
these tasks in recent years. The submissions were
characterised by varied choices of model structure,
parameterisation, regularisation, and the degree to
which light supervision was provided through con-
straints or the use of labelled tuning data. In the fol-
lowing sections we summarise the approaches taken
by the systems submitted for each task.
5.1 Part-of-Speech Induction
The part-of-speech induction challenge received two
submission, (Chrupa?a, 2012; Christodoulopoulos et
al., 2012). Both of these submissions based their in-
duction systems on LDA inspired models for cluster-
ing word types by the contexts in which they appear.
Notably, the strongest of the provided benchmarks
and the two submissions modelled part-of-speech
tags at the type level, thus restricting all tokens of
a given word type to share the same tag. Though
clearly out of step with the gold standard tagging,
this one-tag-per-type restriction has previously been
shown to be a crude but effective way of regularising
models towards a good solution. Below we sum-
marise the approach of each submission, identified
by the surname of the first author on the submitted
system description.
Chrupa?a (2012) employed a two stage approach
to inducing part-of-speech tags. The first stage used
an LDA style probabilistic model to induce a dis-
tribution over possible tags for a given word type.
These distributions were then hierarchically clus-
tered and the final tags selected using the prefix of
the path from the root node to the word type in the
cluster tree. The length of the prefixes, and thus the
number of tags, was tuned on the labelled develop-
ment data.
The system of Christodoulopoulos et al (2012)
was based upon an LDA type model which included
both contexts and other conditionally independent
features (Christodoulopoulos et al, 2011). This base
system was then iterated with a DMV system and
with the resultant dependencies being repeatedly fed
back into the POS model as features. This submis-
sion is notable for being one of the first to attempt
joint POS and dependency induction rather than tak-
ing a pipeline approach.
5.2 Dependency Induction
The dependency parsing task saw a variety of ap-
proaches with only a couple based on the previously
dominant DMV system. Two forms of light super-
vision were popular, the first being the inclusion of
pre-specified constraints or rules for allowable de-
pendency links, and the second being the tuning of
model parameters or selecting between competing
models on the labelled development data. Obviously
the merits of such supervision would depend on the
desired application for the induced parser. The di-
rect comparison of models which include a form of
universal prior syntactic information with those that
don?t does permit interesting development linguistic
questions to be explored in future.
Bisk and Hockenmaier (2012) chose to induce a
restricted form of Combinatory Categorial Grammar
(CCG), the parses of which were then mapped to
dependency structures. Restrictions on head-child
dependencies were encoded in the allowable cate-
69
gories for each POS tag and the heads of sentences.
Key features of their approach were a maximum
likelihood objective function and an iterative proce-
dure for generating composite categories from sim-
ple ones. Such composite categories allow the pa-
rameterisation of larger units than just head-child
dependencies, improving over the more limited con-
ditioning of DMV.
Marac?ek and Z?abokrtsky? (2012) introduced a
number of novel features in their dependency induc-
tion submission. Wikipedia articles were used to
quantify the reducibility of word types, the degree
to which the word could be removed from a sen-
tence and grammaticality maintained. This metric
was then used, along with a model of child fertil-
ity and dependency distance, within a probabilistic
model. Inference was performed by using a local
Gibbs sampler to approximate the marginal distribu-
tion over head-child links.
S?gaard (2012) presented two model-free heuris-
tic algorithms. The first was based on heuristically
adding dependency edges based on rules such as ad-
jacency, function words, and morphology. The re-
sulting structure is then run through a PageRank al-
gorithm and another heuristic is used to select a tree
from the resulting ranked dependency edges. The
second approach takes the universal rules of Naseem
et al (2010) but rather than estimating a probabilis-
tic model with these rules, a rule based heuristic is
used to select a parse rather. This second model-free
approach in particular provides a strong baseline for
probabilistic models built upon hand-specified de-
pendency rules.
Tu (2012) described a system based on an ex-
tended DMV model. Their work focussed on the
exploration of multiple forms of regularisation, in-
cluding Dirichlet priors and posterior regularisation,
to favour both sparse conditional distributions and
low ambiguity in the induced parse charts. While
many previous works have included sparse priors
on the conditional head-child distributions the ad-
ditional regularisation of the ambiguity over parse
trees is a novel and interesting addition. The la-
belled development sets were employed to both se-
lect between models employing different regularisa-
tion, and to tune model parameters.
5.3 POS and Dependency Induction
There was only a single submission for the task of
inducing dependencies without gold standard part-
of-speech tags supplied. Christodoulopoulos et al
(2012) submitted the same joint tagging and DMV
system used for the POS induction task to the depen-
dency induction task. Results on the development
data indicated that this iterated joint training had a
significant benefit for the induced tags and a smaller
benefit for the dependency structures induced.
6 Results
The main results for the three tasks are shown in Ta-
bles 2, 3, and 4, for the POS induction, dependency
induction and joint tasks, respectively.13 We now
present a detailed analysis of each of the three tasks.
6.1 POS induction
The main evaluation results for the POS induc-
tion task are shown in Table 2, which compares
the induced clusters against the gold universal tags
(UPOS).14 Given the diversity of scenarions used by
each system (e.g. number of hidden states, tuning
on development data) a direct comparison between
the systems can only be illustrative. A first obser-
vation is that depending on the particular evaluation
metric employed the ranking of the systems changes
substantially, for instance the Gu system is the best
using the 1-1 and VI metric but is the worst of the en-
tries (excepting the baselines) when using the other
two metrics. Focusing on the VM metric, which
was shown empirically not to have low bias with re-
spect to the word classes (Christodoulopoulos et al,
2010), the best entry is the BC system which has the
best performance in 9 out of 10 entries followed by
the CGS and the C system. Note that this ranking
holds also for the comparison against fine POS tags,
shown in Table 7.
An interesting aspect is that almost all systems
beat the strong Brown (B) and mkcls (MK) base-
line across the different metrics when we restrict
our attention to the cases where the same number
13Additional tables of results are in the appendix, and fur-
ther results are online at http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure.
14See also Table 7 for the comparison against the fine POS
tags; we base our analysis on UPOS instead as this tag set has a
fixed size irrespective of the treebank.
70
M-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 83.80 N/A 83.33 83.33 65.05 66.61 66.20 69.89 66.22 71.08 72.72 68.03
basque 80.85 79.54 86.67 86.67 77.37 73.88 74.73 77.49 73.32 74.80 78.63 71.40
czech 83.10 66.78 72.27 77.97 N/A N/A 60.85 75.57 60.42 65.43 79.35 57.16
danish 81.44 77.76 84.13 84.92 68.16 53.78 72.12 79.77 47.09 72.26 82.59 53.07
dutch 80.75 70.13 74.04 76.11 63.37 57.64 57.99 84.17 57.31 68.18 84.78 63.04
en-childes 90.36 85.42 91.50 91.50 N/A N/A 82.65 89.70 70.12 86.27 91.44 75.63
en-ptb 86.73 81.93 78.11 84.35 77.14 71.10 77.29 80.88 63.74 79.99 83.88 63.34
portuguese 81.69 77.38 80.38 81.90 75.54 74.35 70.07 74.25 67.60 70.79 72.90 68.08
slovene 70.81 65.31 75.53 75.92 67.94 59.96 61.58 68.93 58.32 58.43 65.69 50.36
swedish 78.61 80.45 79.60 79.60 69.91 58.79 71.69 71.69 57.55 76.45 76.45 57.30
averages 81.82 76.08 80.56 82.23 70.56 64.51 69.52 77.23 62.17 72.37 78.84 62.74
1-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 53.67 N/A 39.44 39.44 39.83 55.52 40.55 33.57 43.31 51.54 40.24 51.58
basque 36.10 36.03 47.15 47.15 47.09 54.70 32.61 20.53 40.62 34.80 27.28 37.65
czech 31.82 49.30 30.49 27.20 N/A N/A 46.19 26.66 45.10 43.70 24.48 39.25
danish 42.54 42.77 31.67 31.04 39.95 45.58 36.04 17.74 39.19 43.89 22.18 44.23
dutch 42.79 56.15 43.10 39.62 56.45 45.37 48.18 21.36 43.12 55.99 21.32 54.09
en-childes 38.79 42.57 43.76 43.76 N/A N/A 40.78 35.54 57.71 43.45 32.00 59.18
en-ptb 41.55 39.57 43.86 31.56 42.07 51.70 39.79 33.90 46.50 40.55 36.22 51.17
portuguese 59.66 47.45 35.90 35.50 46.50 56.08 51.15 42.68 51.58 44.28 35.38 46.31
slovene 39.02 53.04 33.18 32.50 50.90 48.50 46.83 40.16 42.28 40.34 39.32 40.58
swedish 42.38 32.44 26.45 26.45 34.99 54.92 27.56 27.56 51.34 35.82 35.82 43.60
averages 42.83 44.37 37.50 35.42 44.72 51.55 40.97 29.97 46.07 43.44 31.42 46.76
VM
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 61.75 N/A 51.27 51.27 44.81 47.07 39.93 42.43 39.92 47.47 43.91 44.49
basque 42.17 41.52 43.04 43.04 40.86 40.05 34.85 33.33 36.08 36.32 34.35 33.42
czech 52.26 45.31 40.22 39.20 N/A N/A 38.56 42.90 37.46 41.70 46.03 37.34
danish 56.57 54.63 52.46 52.32 47.26 41.96 47.89 44.37 35.13 50.52 48.17 39.96
dutch 56.96 53.35 54.87 52.90 48.57 45.80 43.34 49.33 43.67 51.37 50.11 47.20
en-childes 64.53 62.32 62.76 62.76 N/A N/A 58.87 60.31 57.06 62.76 60.92 60.51
en-ptb 60.73 57.99 53.14 52.09 55.10 52.54 54.76 55.08 48.04 56.81 57.29 48.46
portuguese 64.17 58.41 52.54 52.32 55.96 58.14 52.09 53.18 50.32 52.48 50.87 50.18
slovene 51.15 51.29 46.60 46.50 50.98 45.98 44.49 45.80 38.61 36.79 43.43 36.43
swedish 57.05 54.21 47.08 47.08 48.89 45.73 45.87 45.87 40.84 49.77 49.77 42.83
averages 56.73 53.23 50.40 49.95 49.05 47.16 46.06 47.26 42.71 48.60 48.48 44.08
VI
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 2.48 N/A 3.70 3.70 3.39 2.98 3.78 3.94 3.53 3.31 3.82 3.30
basque 3.82 3.44 3.98 3.98 3.25 2.82 3.92 4.98 3.45 3.79 4.76 3.58
czech 3.83 3.41 4.92 5.77 N/A N/A 3.70 4.76 3.69 3.63 4.53 3.83
danish 3.36 3.34 4.31 4.38 3.78 3.46 3.86 5.43 3.79 3.64 4.90 3.59
dutch 3.56 3.13 3.28 3.71 3.30 3.44 3.66 5.22 3.60 3.26 5.15 3.39
en-childes 2.81 2.86 3.06 3.06 N/A N/A 3.13 3.34 2.59 2.84 3.33 2.50
en-ptb 3.18 3.28 3.67 4.36 3.34 3.03 3.46 3.62 3.36 3.36 3.52 3.28
portuguese 2.47 2.83 3.96 4.09 2.96 2.62 3.19 3.36 3.10 3.21 3.52 3.15
slovene 3.62 3.14 4.80 4.86 3.16 3.30 3.61 4.09 3.73 4.15 4.33 3.99
swedish 3.31 3.68 4.98 4.98 3.90 3.32 4.46 4.46 3.70 4.07 4.07 3.62
averages 3.24 3.23 4.07 4.29 3.39 3.12 3.68 4.32 3.45 3.53 4.19 3.42
Table 2: Results for the POS induction task, showing one-to-one, many-to-one, VM and VI scores, measured against
the gold UPOS tags. Each system is shown in a column, where the title is an acronym of the authors? last names, or
else the name of a benchmark system (B is the Brown clusterer and MK is mkcls). The superscripts c, p and u denote
different applications of the same method with a number of word classes set to equal the true number of coarse tags,
full tags or universal tags, respectively, for each treebank.
71
of hidden states are used (the exception being the G
system which occasionally under-performed against
MK). Interestingly the assumption of one-tag-per-
word, made by all but the G system, works very
well in practice leading to consistently strong re-
sults. This suggests that dealing with word ambigu-
ity is still an unresolved issue in unsupervised POS
induction.
Comparing the performance of the systems for
different languages, as expected the languages for
which we have a larger corpora (English CHILDES
and PTB and Czech) tend to result in systems with
better accuracies. An interesting future question is
how do the propose methods scale when training on
really large corpora (e.g., wikipedia) both in terms
of performance (accuracy) but also in the resources
they required.
Finally, the wild divergences in the system rank-
ings when considering the different evaluation met-
rics calls for some sort of external evaluation using
the induced clusters as features to other end sys-
tems, for instance semi-supervised tagging. The
main question is if there will be a definitive ranking
between systems for a diverse set of tasks, or if on
the contrary the effectiveness of the output of each
system will vary according to the task at hand.
6.2 Dependency induction
The main evaluation results for the dependency task
are shown in Table 3. From this we make several
observations.15 Firstly, for almost all the corpora
the participants systems have outperformed the sim-
ple baselines, and by a significant margin. There
are three exceptions to this: for Arabic, Basque and
Danish the left or right-branching baselines outper-
forms most or all of the competitors. This may in-
dicate that these languages are inherently difficult,
or may simply be a consequence of these three lan-
guages having the least data of all of our corpora.
Basque and Dutch proved to be the hardest of the
treebanks, with the lowest overall scores, and the
CHILDES (English) and Portuguese were the eas-
iest. The reasons for this are not immediately clear,
15Table 3 evaluates against the full test sets, however it is
traditional to present results for short sentences mirroring the
common training setup. See Tables 8 and 9 for results over
sentences with 10 words or fewer, excluding punctuation. Note
that our analysis is based on the results for the full test set.
although we speculate that Basque is difficult due to
its dissimilarity from other European languages, and
therefore may not match the assumptions underly-
ing models developed primarily on English. Dutch
is difficult as its annotation was non-projective, and
it has a very large set of POS tags, while CHILDES
is made easier due to its extremely short and simple
sentences.
In terms of declaring a ?winner?, it is clear that
Tu?s system ranks best under directed accuracy and
NED, and a very narrow second (to the organisers?
submission, BC) for undirected accuracy. Moreover
Tu?s system was a consistent performed across all
corpora, with no single result well below the results
of the other participants. Note that the three different
metrics often predict the same winner across the dif-
ferent treebanks, however there are some large dis-
crepancies, such as Portuguese and Dutch where the
directed and undirected accuracy metrics concur, but
NED produces a very different ranking. It is unclear
which metric should be trusted more than another;
this could only be assessed by correlating these met-
rics with some form of secondary evaluation, such
as in a task based setting or obtaining human gram-
maticality judgements.16
The benchmark systems include DMV (Klein and
Manning, 2004), which has historical importance
in terms of being the first research systems to out-
perform simple baselines for dependency induction,
and also the model upon which most recent depen-
dency induction research is based, including many
of the competitors in the competition. We ob-
serve that in most cases the competitors have out-
performed the DMV models, in many cases by a
large margin. In all cases DMV improved over
its initialisation condition (the harmonic initialiser),
although often this improvement was only slight,
underscoring the importance of good initialisation.
The effect of inducing DMV grammars from var-
ious different granularity of POS tags made little
difference in most cases, although for Dutch17 and
the English PTB there change was more dramatic.
16It was our intention to include a task-based evaluation for
machine translation, but this proved impractical for the compe-
tition due to the volumes of data that we would require each
participant to process.
17Note that for Dutch the full POS tags were not gold stan-
dard, but were system predictions.
72
Directed
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8
basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4
czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3
danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2
dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5
en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3
en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9
portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7
slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7
swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3
averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9
Undirected
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 57.3 29.7 57.6 62.0 58.7 48.0 58.4 59.3 41.8 42.0 43.7 41.2 61.7 63.9
basque 58.0 47.2 43.3 45.0 43.2 47.5 24.3 53.3 48.1 47.7 40.3 37.6 53.9 53.1
czech 59.0 45.0 57.8 54.3 55.5 49.3 55.8 61.4 46.2 46.7 45.3 38.5 51.5 52.3
danish 60.8 50.7 60.7 56.1 60.3 56.6 60.5 61.6 55.1 54.1 51.6 46.0 58.7 59.9
dutch 61.0 45.0 47.5 51.5 48.9 46.8 51.4 54.6 52.2 45.0 52.2 37.2 50.1 50.8
en-childes 63.5 68.4 67.2 59.9 61.4 62.0 62.4 66.9 63.8 64.0 57.5 49.0 50.0 49.9
en-ptb 66.2 58.1 49.7 57.6 48.2 49.5 58.8 62.1 43.1 53.1 43.0 36.2 51.7 51.5
portuguese 56.6 72.4 61.4 61.9 49.8 52.6 66.9 61.4 44.3 48.1 43.6 41.2 55.7 56.8
slovene 58.1 47.9 45.2 49.1 44.5 42.4 53.5 61.8 42.1 40.6 42.1 32.5 40.8 41.1
swedish 70.0 58.5 58.8 59.3 60.4 53.5 65.2 66.9 51.1 51.1 53.3 44.5 53.0 53.2
averages 61.0 52.3 54.9 55.7 53.1 50.8 55.7 60.9 48.8 49.2 47.3 40.4 52.7 53.2
NED
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 63.6 37.3 59.2 67.1 63.2 56.5 65.8 64.1 48.9 48.0 48.8 47.5 62.7 69.0
basque 69.6 55.8 51.5 55.6 53.4 58.8 38.0 65.8 57.6 57.1 51.5 49.3 67.2 59.1
czech 71.0 55.7 70.2 65.2 67.3 63.2 69.7 71.6 53.2 52.9 54.1 47.6 56.3 68.6
danish 72.0 63.1 72.9 69.5 73.5 65.9 71.8 76.4 64.8 63.5 58.9 53.5 61.6 71.5
dutch 71.6 58.6 68.6 72.0 69.7 60.6 63.8 66.9 63.5 54.5 63.5 46.9 55.1 67.0
en-childes 80.9 79.6 82.8 74.1 72.7 77.1 83.2 80.4 78.1 78.3 77.5 67.2 61.0 75.2
en-ptb 75.2 69.8 69.4 73.8 67.2 64.1 71.6 69.8 49.8 67.0 49.6 44.8 53.9 68.1
portuguese 67.5 79.8 75.6 75.7 71.7 66.9 78.2 80.4 62.1 66.6 61.3 51.8 57.3 75.4
slovene 64.4 60.7 56.9 58.9 57.1 55.9 66.7 68.7 49.2 47.3 49.2 38.9 43.8 56.6
swedish 80.1 70.9 73.2 73.8 72.7 66.7 77.0 77.1 64.0 64.0 62.0 56.0 56.5 71.0
averages 71.6 63.1 68.0 68.6 66.9 63.6 68.6 72.1 59.1 59.9 57.6 50.3 57.6 68.1
Table 3: Directed accuracy, undirected accuracy and NED results for the dependency task (using supplied POS). The
first column (BC) is our benchmark system, the next seven are participants systems, and the remaining columns consist
of the DMV benchmark and various simple baselines. The superscripts c, p and u denote which type of POS was used,
and S1 and S2 denote two different submissions for S?gaard (2012).
Overall the full POS tagset lead to the best perfor-
mance over the coarse and universal tags (consider-
ing undirected accuracy or NED), which is to be ex-
pected as there is considerably more syntactic infor-
mation contained in the full POS. This must be bal-
anced against the additional model complexity from
expanding its parameter space, which may explain
why the difference in performance differences are so
small. The same pattern can also be seen in Marac?ek
and Z?abokrtsky? (2012)?s submission, whose system
using full POS (Mp) outperformed their other vari-
ants.
6.3 Joint task
As we had only one submission for the joint prob-
lem of POS and dependency induction, there are
few conclusions we can draw for this joint task (see
Table 4 for the results, and Table 9 for the short
sentence evaluation). Compared to the dependency
induction task using gold standard POS, as shown
in Table 3, the accuracy for the joint models are
lower. Interestingly, the DMV model performs best
when using the same number of word clusters as
there are POS tags, mirroring the findings reported
73
directed
testset CGS DMVc DMVp DMVu
arabic N/A 35.3 44.4 34.2
basque 24.5 27.5 25.1 28.7
czech 24.7 19.9 33.2 20.0
danish 21.4 23.3 31.9 10.0
dutch 15.1 20.6 33.7 20.5
en-childes 29.9 38.6 42.2 40.3
en-ptb 21.5 22.5 23.3 17.2
portuguese 19.7 28.5 28.0 17.1
slovene 19.2 13.9 11.5 14.4
swedish 23.6 26.4 26.4 20.5
averages 22.2 25.7 30.0 22.3
undirected
testset CGS DMVc DMVp DMVu
arabic N/A 45.5 52.5 45.0
basque 43.5 46.4 47.3 47.0
czech 38.9 37.5 50.9 38.5
danish 51.4 52.2 48.8 37.3
dutch 40.3 41.9 48.6 40.8
en-childes 54.9 59.2 60.8 58.1
en-ptb 43.4 45.4 48.8 39.4
portuguese 45.5 51.8 52.7 39.8
slovene 32.8 33.3 36.7 32.8
swedish 45.6 48.9 48.9 40.3
averages 44.0 46.2 49.6 41.9
NED
testset CGS DMVc DMVp DMVu
arabic N/A 53.4 57.6 53.3
basque 55.9 55.6 54.4 54.7
czech 51.2 49.3 63.4 51.5
danish 61.7 60.3 60.4 46.3
dutch 47.2 57.5 56.8 55.2
en-childes 78.2 77.7 78.1 76.5
en-ptb 53.9 60.2 63.5 47.5
portuguese 50.0 69.4 70.8 57.9
slovene 40.7 38.7 47.5 40.3
swedish 54.5 65.4 65.4 54.3
averages 54.8 58.8 61.8 53.8
Table 4: Directed, undirected and NED accuracy results
for evaluating the predicted dependency structures in the
joint task (i.e., not using supplied POS tags). The first
column is the participant?s system and the next three are
DMV models trained on the Brown word clusters (see
section 6.1).
above with gold standard tags. The best joint sys-
tem was the DMVp model, which only marginally
under-performed the equivalent DMV model trained
on gold POS. This is an encouraging finding, sug-
gesting that word clusters are able to represent im-
portant POS distinctions to inform deeper syntactic
processing.
6.4 Analysis
Until now we have adopted the standard metrics in
dependency evaluation: namely directed head at-
tachment accuracy, and its more lenient counter-
parts, undirected accuracy and NED. The latter met-
rics reward structures that almost match the gold
standard tree, by way of rewarding child-parent
edges that are predicted in the reverse direction, i.e.,
attaching the child as the parent (NED takes this fur-
ther, by also rewarding the grandparent-child edge
when this occurs). This allows some degree of flexi-
bility when considering various contentious linguis-
tic decisions such as whether a preposition should
head a preposition phrase, or the head of the child
noun-phrase. This added leniency comes at a price,
as shown in Table 3 where the undirected accuracy
and NED results are considerably higher than di-
rected accuracy, and display less spread of values
(look in particular at the random trees, Ra). Is is
unclear that the predicted trees are truly predicting
linguistically plausible structures, but instead that
the differences are due largely to chance. Moreover,
systems that predict linguistic phenomena inconsis-
tently between sentences or across types of related
phenomena are rewarded under these lenient met-
rics.
For these reasons we also consider a different,
less permissive, evaluation method, using multiple
references of the treebank where each is annotated
with different styles of dependency. As described
in section 2, we processed the Penn treebank five
times with different options to the LTH conversion
tool. This affected the treatment of coordination,
preposition phrases, subordinate clauses, infinitival
clauses etc. Next we compare the directed accu-
racy of the systems against these five different ?gold
standard? references, which are displayed in Table 5,
alongside the maximum score for each system. Note
that most systems performed well against the stan-
dard, conll2007 and functional references but poorly
against the lexical and oldLTH references.18 Con-
sidering the latter two references, a different system
would be selected as the highest performing, namely
Bisk and Hockenmaier (2012) (BH) over Blunsom
and Cohn (2010) (BC) which wins in the other cases.
18The common difference here is that the latter two refer-
ences do not treat prepositions as heads of PPs.
74
This evaluation method rewards many different lin-
guistically plausible structures, but in such a way
that the predictions must be consistent between dif-
ferent sentences in the testing set, and in their treat-
ment of related linguistic phenomena. One caveat
is that this method can only be used when there
are many references, although in many cases differ-
ent outputs can be generated automatically, e.g., by
adjusting head-finding heuristics in converting be-
tween phrase-structure to dependency trees.
The previous analysis has rated each system in
terms of overall performance against treebank trees,
however this doesn?t necessarily mean that the pre-
dictions of the best ranked system will be the most
useful ones in a task-based setting. Take the ex-
ample of information extraction, in which a central
problem is to identify the arguments (subject, object
etc) of a given verb. This setting gives rise to some
types of dependency edges being more valuable than
others. We present comparative results for the Penn
treebank in Table 6 showing the directed accuracy
for different types of dependency relations. Observe
that there is a wide spread of accuracies for predict-
ing the head word of the sentence (ROOT), and simi-
larly for verbs? subject and object arguments. These
scores are similar to the scores for the local modi-
fiers shown, such as NMOD which describe the ar-
guments of a noun. This is surprising as noun edges
tend to be much shorter than for the arguments to a
verb, and thus should be easier to predict. Also in-
teresting are the spread of results for the CC edges
(these link a coordinating conjunction to its head),
suggesting that the systems learn to represent coor-
dination in very different ways to the method used
in the reference.
Figure 1 illustrates the directed accuracy over dif-
ferent lengths of dependency edge. For all systems
the accuracy diminishes with edge length, however
some fall at a much faster rate. The two best systems
(Tu, BC) have similar overall accuracy results, but
it is clear that Tu does better on short edges while
BC does better on longer ones. The same pattern
was also observed when considering the average ac-
curacy over all treebanks (not shown), although the
systems? results were closer together.
system ROOT SBJ OBJ PRD NMOD COORD CC
Tu 71.0 64.8 53.7 49.4 56.9 36.8 11.4
LB 17.8 40.1 15.3 18.0 41.9 27.7 9.7
BC 74.9 65.7 53.0 50.2 56.8 36.3 71.4
DMVc 17.0 11.7 16.0 31.3 27.8 25.7 9.2
DMVu 17.6 9.3 16.4 25.0 27.8 25.7 8.6
BH 67.5 55.3 44.9 45.6 58.6 27.6 62.7
Mu 29.3 42.4 38.8 51.8 34.5 30.5 33.0
R 12.9 9.4 16.1 21.1 12.1 15.7 2.7
Mc 60.7 47.4 39.9 45.8 36.5 33.9 44.3
RB 17.9 12.4 26.2 36.5 15.3 25.4 1.1
H 19.4 29.3 12.2 22.2 17.3 20.9 10.3
DMVp 54.7 42.0 30.7 30.1 28.9 25.4 24.3
S2 45.2 41.9 44.2 49.8 39.7 25.4 63.8
Mp 67.8 54.3 49.6 59.4 47.7 37.7 49.7
S1 43.1 47.9 36.3 46.7 27.9 23.5 7.6
Table 6: Directed accuracy results on the Penn treebank,
stratified by dependency relation. For clarity, only 9 im-
portant relation types are shown. The vertical bars sepa-
rate different groups of relations, from left to right, relat-
ing to the main verb, general modifiers and coordination.
7 Conclusion
This challenge set out to evaluate the state-of-the-
art in part-of-speech and dependency grammar in-
duction, promoting research in this field and, im-
portantly, providing a fair means of evaluation. The
participants submissions used a wide variety of dif-
ferent approaches, many of which we shown to
have improved over competitive benchmark sys-
tems. While the results were overall very positive,
it is fair to say that the tasks of part-of-speech and
grammar induction are still very much open chal-
lenges, and that there is still considerable room for
improvement. The data submitted to this evaluation
campaign will provide a great resource for devising
new methods of evaluation, and we plan to pursue
this avenue in future work, in particular task-based
evaluation such as in an information extraction or
machine translation setting.
8 Acknowledgements
This challenge was funded by the PASCAL 2 (Pat-
tern Analysis, Statistical Modelling and Compu-
tational Intelligence) European Network of Excel-
lence. We would also like to thank the treebank
providers for allowing us to use their resources, as-
sisting us in converting these into our desired for-
mat, and helping to resolve various questions. In
particular, special thanks to Zdenek Zabokrtsky and
Jan (Czech and Arabic), Tomaz Erjavec (Slovene),
and Eckhard Bick and Diana Santos (Portuguese).
We are also indebted to the organisers of the previ-
75
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
conll2007 54.9 51.7 40.4 49.2 36.8 32.2 41.7 54.2 20.9 33.2 20.4 18.0 30.1 20.3
functional 59.6 52.4 41.5 47.4 36.2 30.6 40.0 58.5 20.9 37.2 20.6 19.3 29.2 23.7
lexical 40.6 41.9 28.5 37.3 24.8 27.7 35.5 39.5 23.5 23.1 23.0 14.4 33.1 10.1
oldLTH 41.4 43.6 28.8 37.8 24.6 28.6 36.1 39.5 22.3 23.7 21.8 14.3 32.0 10.7
standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4
best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7
Table 5: Directed accuracy results measured against different conversions of the Penn Treebank into dependency trees.
l l
l
l
l
l l
l l
2 4 6 8
10
20
30
40
50
60
70
edge length
direc
ted a
ccura
cy (%)
l TuBCBHMZ?pS?2DMV?p
Figure 1: Directed accuracy on the Penn treebank strat-
ified by dependency length. For clarity only a subset of
the systems are shown, and edges of length 10 or more
were omitted.
ous CoNLL 2006 and 2007 competitions, who con-
tributed significant efforts into collating so many
treebanks and developing treebank conversion tools,
making our job much easier than it would other-
wise have been. Thanks to Sebastian Reidel, Joakim
Nivre and Sabine Buchholz for promptly answer-
ing our questions. We would like to thank the
LDC, who allowed their licenced data to be used
free of charge by the competitors, and Ilya Ahtaridis
who administered the licencing and corpus distribu-
tion. Thanks also to Valentin Spitkovski and Chris-
tos Christodoulopoulos who kindly provided us with
their evaluation scripts, and finally, the participants
themselves for taking part.
References
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT).
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. Floresta sinta?(c)tica: a treebank
for Portuguese. In Proceedings of the Third Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2002), pages 1698?1703, May.
Yonatan Bisk and Julia Hockenmaier. 2012. Induction of
linguistic structure with combinatory categorial gram-
mars. In Proceedings of the NAACL-HLT 2012 Work-
shop on Inducing Linguistic Structure Shared Task,
June.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised in-
duction of tree substitution grammars for dependency
parsing. In Proceedings of the 2010 Conference on
Empirical Methods on Natural Language Processing
(EMNLP), pages 1204?1213, Cambridge, MA, USA.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2001. The Prague Dependency Treebank: A
Three-Level Annotation Scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Syntactically
Annotated Corpora, pages 103?127. Kluwer Aca-
demic Publishers.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2000. Alpino: Wide coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands (CLIN 2000), pages 45?59.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479, December.
Matthias Buch-Kromann, Ju?rgen Wedekind,
and Jakob Elming. 2007. The Copen-
hagen Danish-English dependency tree-
bank. http://code.google.com/p/
copenhagen-dependency-treebank.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
76
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning (CoNLL-X), pages
149?164.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: how far have we come? In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, EMNLP ?10, pages
575?584.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for PoS induction using multiple features. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 638?647, Edin-
burgh, Scotland, UK., July.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2012. Turning the pipeline into
a loop: Iterated unsupervised dependency parsing
and pos induction. In Proceedings of the NAACL-
HLT 2012 Workshop on Inducing Linguistic Structure
Shared Task, June.
Grzegorz Chrupa?a. 2012. Hierarchical clustering
of word class distributions. In Proceedings of the
NAACL-HLT 2012 Workshop on Inducing Linguistic
Structure Shared Task, June.
Tomaz? Erjavec, Darja Fis?er, Simon Krek, and Nina
Ledinek. 2010. The JOS linguistically tagged corpus
of Slovene. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10).
Joa?o Grac?a, Kuzman Ganchev, Lu??sa Coheur, Fernando
Pereira, and Benjamin Taskar. 2011. Controlling
complexity in part-of-speech induction. J. Artif. Intell.
Res. (JAIR), 41:527?551.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (HLT-NAACL
?06), pages 320?327.
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf, and
Emanuel Bes?ka. 2004. Prague Arabic dependency
treebank: Development in data and tools. In Proceed-
ings of the NEMLAR International Conference on Ara-
bic Language Resources and Tools, pages 110?117.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of the 16th Nordic Conference of Compu-
tational Linguistics (NODALIDA 2007).
Mark Johnson. 2007. Why doesn?t EM find good hmm
pos-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?07, pages 296?305.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ?04: Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 478.
David Marac?ek and Zdene?k Z?abokrtsky?. 2012. Unsuper-
vised dependency parsing using reducibility and fertil-
ity features. In Proceedings of the NAACL-HLT 2012
Workshop on Inducing Linguistic Structure Shared
Task, June.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Marina Meila. 2003. Comparing Clusterings by the Vari-
ation of Information. Learning Theory and Kernel
Machines, pages 173?187.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1234?1244, October.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of the fifth
international conference on Language Resources and
Evaluation (LREC2006).
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
June.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan T. McDonald.
2011. A universal part-of-speech tagset. CoRR,
abs/1104.2086.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2007. High-accuracy annotation and pars-
ing of CHILDES transcripts. In Proceedings of the
ACL-2007 Workshop on Cognitive Aspects of Compu-
tational Language Acquisition., June.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
77
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 663?672.
Anders S?gaard. 2012. Two baselines for unsupervised
dependency parsing. In Proceedings of the NAACL-
HLT 2012 Workshop on Inducing Linguistic Structure
Shared Task, June.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-annotation evaluation. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 385?396,
Edinburgh, UK, July.
Kewei Tu. 2012. Combining the sparsity and unambi-
guity biases for grammar induction. In Proceedings of
the NAACL-HLT 2012 Workshop on Inducing Linguis-
tic Structure Shared Task, June.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL-07), pages 240?247,
June.
Appendix
Directed
testset CGS DMVc DMVp DMVu
arabic 36.1 42.6 51.9 49.1
basque 28.4 28.9 27.1 30.2
czech 33.1 28.6 38.2 28.3
danish 27.9 36.4 38.4 18.2
dutch 31.0 39.0 41.1 40.3
en-childes 31.2 40.8 44.3 42.1
en-ptb 22.7 25.1 23.1 23.1
portuguese 26.7 38.4 34.5 31.1
slovene 26.3 20.6 19.2 22.6
swedish 29.0 30.9 30.9 26.5
averages 29.3 33.1 34.9 31.1
Undirected
testset CGS DMVc DMVp DMVu
arabic 58.3 52.8 58.3 61.1
basque 49.3 49.2 50.5 50.0
czech 48.7 45.9 57.2 47.9
danish 56.3 60.9 57.0 43.7
dutch 47.0 53.6 57.2 53.8
en-childes 56.3 61.0 62.7 59.9
en-ptb 50.7 52.9 54.1 46.9
portuguese 51.8 61.1 59.4 51.6
slovene 40.5 41.9 45.3 41.2
swedish 52.5 57.1 57.1 48.6
averages 51.1 53.6 55.9 50.5
NED
testset CGS DMVc DMVp DMVu
arabic 62.0 63.0 66.7 67.6
basque 67.4 62.8 62.5 62.3
czech 65.1 60.7 72.0 64.0
danish 72.0 72.4 73.2 60.3
dutch 57.7 64.9 65.0 64.7
en-childes 79.9 79.6 79.9 78.4
en-ptb 67.9 73.4 74.3 63.7
portuguese 58.2 80.7 79.5 72.6
slovene 55.7 52.3 59.4 51.9
swedish 64.8 78.4 78.4 65.7
averages 65.1 68.8 71.1 65.1
Table 9: Evaluation of the joint task on the dependency
output using a maximum sentence length of 10.
78
M-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 75.06 N/A 79.00 79.00 62.20 62.24 61.81 64.60 61.55 65.69 67.82 63.23
basque 71.58 69.20 75.23 75.23 65.97 56.37 64.37 68.58 62.17 63.59 68.22 60.49
czech 74.84 61.53 66.97 76.00 N/A N/A 55.60 72.51 55.01 60.38 73.38 51.96
danish 56.48 55.41 70.28 71.62 49.24 35.32 50.21 66.50 33.57 49.40 61.60 35.02
dutch 80.72 70.13 74.04 76.08 63.37 57.64 57.99 83.76 57.31 68.18 84.64 63.04
en-childes 84.23 77.57 85.35 85.35 N/A N/A 76.34 85.11 59.75 77.55 86.39 59.55
en-ptb 78.26 72.26 62.86 73.46 63.15 56.32 65.10 68.10 48.76 70.31 73.96 47.91
portuguese 76.00 72.05 75.47 77.13 68.40 65.86 65.52 69.61 61.84 64.63 66.81 62.95
slovene 67.29 59.78 72.18 72.71 63.95 55.23 54.85 63.68 52.15 54.08 59.31 45.40
swedish 66.20 67.86 73.55 73.55 60.10 48.43 61.21 61.21 47.51 64.39 64.39 46.04
averages 73.07 67.31 73.49 76.01 62.05 54.68 61.30 70.37 53.96 63.82 70.65 53.56
1-1
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 50.90 N/A 39.15 39.15 41.49 53.92 39.89 34.23 40.63 50.53 41.98 50.27
basque 42.45 46.25 52.38 52.38 48.91 45.55 41.29 33.49 50.80 43.27 35.73 43.43
czech 31.45 48.24 32.18 31.74 N/A N/A 43.12 33.55 41.94 38.93 28.33 35.31
danish 43.08 43.64 32.17 31.77 40.56 34.83 33.48 30.87 26.33 38.92 30.59 32.95
dutch 43.22 55.85 43.26 39.98 56.45 45.37 48.13 21.88 43.10 55.86 22.42 54.04
en-childes 64.10 63.62 64.50 64.50 N/A N/A 59.96 56.87 59.75 63.43 53.40 57.68
en-ptb 57.63 56.02 45.52 41.35 48.95 53.15 55.43 49.60 47.57 54.10 51.80 45.43
portuguese 59.71 50.18 36.13 35.38 54.42 60.08 49.57 45.00 48.25 46.57 38.37 45.10
slovene 42.62 50.66 33.23 32.59 56.55 50.30 44.97 44.34 40.62 41.24 40.01 38.93
swedish 48.76 40.54 34.07 34.07 38.21 46.32 36.12 36.12 44.57 41.90 41.90 38.05
averages 48.39 50.55 41.26 40.29 48.19 48.69 45.20 38.59 44.36 47.48 38.45 44.12
VM
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 61.59 N/A 52.95 52.95 46.99 47.18 40.75 43.16 40.40 47.95 45.09 45.14
basque 53.83 51.34 54.45 54.45 49.34 44.26 44.18 45.46 45.37 45.02 44.90 42.76
czech 56.80 50.22 45.06 46.76 N/A N/A 42.50 49.93 41.51 45.15 51.38 39.62
danish 61.57 59.00 63.39 63.62 53.35 43.20 51.83 58.38 33.46 52.52 58.44 39.46
dutch 57.82 53.94 55.01 53.40 48.99 46.26 44.08 50.49 44.37 52.02 51.33 47.99
en-childes 80.17 76.59 78.18 78.18 N/A N/A 73.67 76.47 65.44 76.14 76.87 68.25
en-ptb 71.44 68.12 59.90 61.31 63.90 60.04 63.79 63.64 52.96 66.40 66.50 54.33
portuguese 67.49 60.37 54.61 54.74 58.91 59.58 53.30 54.99 50.26 53.15 52.67 50.76
slovene 54.80 52.13 51.85 51.88 52.99 48.55 45.33 48.33 40.13 39.25 45.73 38.68
swedish 61.52 58.23 56.09 56.09 55.02 48.69 51.76 51.76 43.39 54.28 54.28 44.51
averages 62.70 58.88 57.15 57.34 53.69 49.72 51.12 54.26 45.73 53.19 54.72 47.15
VI
testset BC CGS Cc Cp Gc Gu Bc Bp Bu MKc MKp MKu
arabic 2.65 N/A 3.76 3.76 3.47 3.19 3.96 4.12 3.73 3.49 3.96 3.48
basque 3.65 3.50 3.78 3.78 3.45 3.36 4.09 4.79 3.67 4.00 4.72 3.83
czech 3.80 3.49 4.96 5.48 N/A N/A 3.92 4.57 3.91 3.85 4.46 4.17
danish 3.76 3.85 4.07 4.08 4.29 4.53 4.54 4.91 5.24 4.45 4.78 4.85
dutch 3.53 3.14 3.31 3.72 3.33 3.47 3.68 5.16 3.61 3.26 5.07 3.39
en-childes 1.86 2.12 2.11 2.11 N/A N/A 2.39 2.32 2.59 2.17 2.31 2.47
en-ptb 2.69 2.90 3.67 4.03 3.16 3.08 3.24 3.41 3.66 3.05 3.19 3.50
portuguese 2.40 2.90 4.01 4.11 2.97 2.74 3.35 3.46 3.35 3.40 3.63 3.37
slovene 3.65 3.40 4.65 4.68 3.34 3.48 3.92 4.23 4.03 4.38 4.51 4.25
swedish 3.36 3.78 4.57 4.57 3.89 3.65 4.45 4.45 4.11 4.17 4.17 4.07
averages 3.13 3.23 3.89 4.03 3.49 3.44 3.75 4.14 3.79 3.62 4.08 3.74
Table 7: One to one, Many to one, VM and VI scores of POS induction results evaluated against fine POS tags (c.f.,
Table 2 which used UPOS).
79
Directed
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 61.1 47.2 15.7 64.8 64.8 47.2 54.6 66.7 46.3 45.4 50.0 42.6 9.3 64.8
basque 56.3 50.3 28.0 30.3 27.2 33.3 22.3 58.6 46.3 43.2 31.3 21.8 34.3 24.4
czech 50.0 48.5 61.3 57.5 57.7 45.5 51.2 59.0 30.1 31.2 31.8 24.7 28.9 34.3
danish 46.2 49.3 60.2 51.3 61.4 56.9 60.5 60.8 47.2 50.2 35.3 36.4 18.7 49.2
dutch 50.5 50.8 37.0 49.5 38.4 38.9 50.0 51.7 48.7 39.7 49.1 35.1 34.0 39.5
en-childes 48.1 62.2 56.8 47.2 51.8 50.5 53.5 56.0 51.7 51.9 39.0 31.7 36.0 23.3
en-ptb 72.1 73.7 58.9 67.4 52.2 44.8 61.0 74.7 31.7 44.7 30.6 35.2 40.4 19.9
portuguese 54.3 76.3 63.6 59.9 44.3 47.7 71.1 55.7 27.1 37.2 26.9 31.1 28.1 37.7
slovene 65.8 53.9 42.1 51.4 39.2 39.7 50.3 67.7 35.7 37.2 35.6 25.7 35.9 14.7
swedish 65.8 66.7 61.4 63.7 70.8 48.2 72.0 76.5 44.2 44.2 45.8 39.1 33.2 31.3
averages 57.0 57.9 48.5 54.3 50.8 45.3 54.7 62.7 40.9 42.5 37.5 32.3 29.9 33.9
Undirected
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 69.4 59.3 59.3 69.4 69.4 59.3 65.7 67.6 52.8 53.7 55.6 54.6 61.1 69.4
basque 65.6 59.5 49.8 50.1 48.4 54.3 32.8 66.4 60.1 58.1 48.5 42.2 56.4 53.9
czech 65.9 59.9 69.2 66.6 67.6 62.3 63.5 70.1 51.6 51.2 50.5 47.2 54.2 56.9
danish 67.9 63.5 70.6 64.4 71.1 67.9 70.7 70.2 65.0 64.3 60.0 56.4 59.7 63.9
dutch 63.2 59.9 57.5 63.3 58.0 58.5 58.5 60.5 62.7 56.7 62.9 51.1 56.3 60.7
en-childes 65.3 70.6 69.4 62.4 63.7 64.3 63.6 69.1 65.7 66.1 59.5 51.2 51.2 51.0
en-ptb 79.4 79.2 65.9 72.5 62.4 62.5 75.1 78.8 53.8 65.3 53.2 52.0 58.8 54.9
portuguese 66.3 81.9 71.6 70.2 62.3 65.8 78.5 72.1 54.0 60.9 54.3 53.3 56.7 63.8
slovene 70.6 63.7 56.3 59.1 55.1 54.8 63.7 72.0 46.4 53.1 46.3 44.9 45.5 46.0
swedish 82.3 73.5 70.1 71.1 75.4 66.5 77.3 83.7 64.5 64.5 66.1 59.2 59.2 59.5
averages 69.6 67.1 64.0 64.9 63.3 61.6 64.9 71.0 57.7 59.4 55.7 51.2 55.9 58.0
NED
testset BC BH MZc MZp MZu S1 S2 Tu DMVc DMVp DMVu H LB RB
arabic 78.7 68.5 66.7 75.9 75.9 68.5 72.2 71.3 61.1 63.0 63.0 63.0 64.8 75.9
basque 77.9 68.6 62.9 65.2 64.2 70.1 55.0 79.5 69.3 69.0 63.8 58.2 73.2 64.0
czech 79.9 72.6 81.8 78.6 79.9 76.0 78.1 81.2 62.9 61.7 63.6 60.3 63.2 73.8
danish 81.7 76.3 83.2 78.4 84.3 77.3 84.4 85.0 77.9 75.8 69.5 67.8 66.2 77.5
dutch 71.0 71.8 77.1 78.4 76.8 72.6 68.5 71.0 73.2 64.6 73.0 60.5 64.4 71.0
en-childes 82.4 81.6 84.5 76.7 74.8 79.0 84.9 82.5 79.9 80.4 79.9 70.1 63.2 76.9
en-ptb 86.7 89.4 87.5 88.4 84.0 78.7 87.1 84.3 64.0 80.7 64.2 64.3 65.2 75.0
portuguese 78.2 90.7 87.8 87.8 87.5 82.9 91.9 90.2 75.6 81.7 76.5 67.7 60.9 82.4
slovene 79.3 76.8 70.8 72.8 70.4 68.1 78.9 79.8 59.4 62.2 59.4 55.8 54.3 62.2
swedish 91.7 85.8 83.1 85.6 87.1 80.8 87.6 92.1 76.1 76.1 76.4 75.3 67.2 79.1
averages 80.8 78.2 78.5 78.8 78.5 75.4 78.9 81.7 69.9 71.5 68.9 64.3 64.3 73.8
Table 8: Evaluation of the dependency task using a maximum sentence length of 10. See also Table 3 which presents
the same results with no length restriction.
80
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
SHEF-Lite: When Less is More for Translation Quality Estimation
Daniel Beck and Kashif Shah and Trevor Cohn and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.uk
Abstract
We describe the results of our submissions
to the WMT13 Shared Task on Quality
Estimation (subtasks 1.1 and 1.3). Our
submissions use the framework of Gaus-
sian Processes to investigate lightweight
approaches for this problem. We focus on
two approaches, one based on feature se-
lection and another based on active learn-
ing. Using only 25 (out of 160) fea-
tures, our model resulting from feature
selection ranked 1st place in the scoring
variant of subtask 1.1 and 3rd place in
the ranking variant of the subtask, while
the active learning model reached 2nd
place in the scoring variant using only
?25% of the available instances for train-
ing. These results give evidence that
Gaussian Processes achieve the state of
the art performance as a modelling ap-
proach for translation quality estimation,
and that carefully selecting features and
instances for the problem can further im-
prove or at least maintain the same per-
formance levels while making the problem
less resource-intensive.
1 Introduction
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al,
2004; Specia et al, 2009; Callison-burch et al,
2012). A common use of quality predictions is
the decision between post-editing a given machine
translated sentence and translating its source from
scratch, based on whether its post-editing effort is
estimated to be lower than the effort of translating
the source sentence.
The WMT13 QE shared task defined a group
of tasks related to QE. In this paper, we present
the submissions by the University of Sheffield
team. Our models are based on Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2006), a
non-parametric probabilistic framework. We ex-
plore the application of GP models in two con-
texts: 1) improving the prediction performance by
applying a feature selection step based on opti-
mised hyperparameters and 2) reducing the dataset
size (and therefore the annotation effort) by per-
forming Active Learning (AL). We submitted en-
tries for two of the four proposed tasks.
Task 1.1 focused on predicting HTER scores
(Human Translation Error Rate) (Snover et al,
2006) using a dataset composed of 2254 English-
Spanish news sentences translated by Moses
(Koehn et al, 2007) and post-edited by a profes-
sional translator. The evaluation used a blind test
set, measuring MAE (Mean Absolute Error) and
RMSE (Root Mean Square Error), in the case of
the scoring variant, and DeltaAvg and Spearman?s
rank correlation in the case of the ranking vari-
ant. Our submissions reached 1st (feature selec-
tion) and 2nd (active learning) places in the scor-
ing variant, the task the models were optimised
for, and outperformed the baseline by a large mar-
gin in the ranking variant.
The aim of task 1.3 aimed at predicting post-
editing time using a dataset composed of 800
English-Spanish news sentences also translated by
Moses but post-edited by five expert translators.
Evaluation was done based on MAE and RMSE
on a blind test set. For this task our models were
not able to beat the baseline system, showing that
more advanced modelling techniques should have
been used for challenging quality annotation types
and datasets such as this.
2 Features
In our experiments, we used a set of 160 features
which are grouped into black box (BB) and glass
box (GB) features. They were extracted using the
337
open source toolkit QuEst1 (Specia et al, 2013).
We briefly describe them here, for a detailed de-
scription we refer the reader to the lists available
on the QuEst website.
The 112 BB features are based on source and
target segments and attempt to quantify the source
complexity, the target fluency and the source-
target adequacy. Examples of them include:
? Word and n-gram based features:
? Number of tokens in source and target
segments;
? Language model (LM) probability of
source and target segments;
? Percentage of source 1?3-grams ob-
served in different frequency quartiles of
the source side of the MT training cor-
pus;
? Average number of translations per
source word in the segment as given by
IBM 1 model with probabilities thresh-
olded in different ways.
? POS-based features:
? Ratio of percentage of nouns/verbs/etc
in the source and target segments;
? Ratio of punctuation symbols in source
and target segments;
? Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
? Syntactic features:
? Source and target Probabilistic Context-
free Grammar (PCFG) parse log-
likelihood;
? Source and target PCFG average confi-
dence of all possible parse trees in the
parser?s n-best list;
? Difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP
phrases in the source and target;
? Other features:
? Kullback-Leibler divergence of source
and target topic model distributions;
? Jensen-Shannon divergence of source
and target topic model distributions;
1http://www.quest.dcs.shef.ac.uk
? Source and target sentence intra-lingual
mutual information;
? Source-target sentence inter-lingual mu-
tual information;
? Geometric average of target word prob-
abilities under a global lexicon model.
The 48 GB features are based on information
provided by the Moses decoder, and attempt to in-
dicate the confidence of the system in producing
the translation. They include:
? Features and global score of the SMT model;
? Number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? Average size of the target phrases;
? Relative frequency of the words in the trans-
lation in the n-best list;
? Ratio of SMT model score of the top transla-
tion to the sum of the scores of all hypothesis
in the n-best list;
? Average size of hypotheses in the n-best list;
? N-best list density (vocabulary size / average
sentence length);
? Fertility of the words in the source sentence
compared to the n-best list in terms of words
(vocabulary size / source sentence length);
? Edit distance of the current hypothesis to the
centre hypothesis;
? Proportion of pruned search graph nodes;
? Proportion of recombined graph nodes.
3 Model
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : RF ? R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
f(x) ? GP(0, k(x,x?))
which is parameterized by a mean function (here,
0) and a covariance kernel function k(x,x?). Each
338
response value is then generated from the function
evaluated at the corresponding input, yi = f(xi)+
?, where ? ? N (0, ?2n) is added white-noise.
Prediction is formulated as a Bayesian inference
under the posterior:
p(y?|x?,D) =
?
f
p(y?|x?, f)p(f |D)
where x? is a test input, y? is the test response
value andD is the training set. The predictive pos-
terior can be solved analitically, resulting in:
y? ? N (kT? (K + ?2nI)?1y,
k(x?, x?)? kT? (K + ?2nI)?1k?)
where k? = [k(x?,x1)k(x?,x2) . . . k(x?,xd)]T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs.
A nice property of this formulation is that y?
is actually a probability distribution, encoding the
model uncertainty and making it possible to inte-
grate it into subsequent processing. In this work,
we used the variance values given by the model in
an active learning setting, as explained in Section
4.
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work on QE using GP (Cohn and
Specia, 2013; Shah et al, 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
k(x,x?) = ?2f exp
(
?12
F?
i=1
xi ? x?i
li
)
where F is the number of features, ?2f is the co-
variance magnitude and li > 0 are the feature
length scales.
The resulting model hyperparameters (SE vari-
ance ?2f , noise variance ?2n and SE length scales li)
were learned from data by maximising the model
likelihood. In general, the likelihood function is
non-convex and the optimisation procedure may
lead to local optima. To avoid poor hyperparam-
eter values due to this, we performed a two-step
procedure where we first optimise a model with all
the SE length scales tied to the same value (which
is equivalent to an isotropic model) and we used
the resulting values as starting point for the ARD
optimisation.
All our models were trained using the GPy2
toolkit, an open source implementation of GPs
written in Python.
3.1 Feature Selection
To perform feature selection, we followed the ap-
proach used in Shah et al (2013) and ranked the
features according to their learned length scales
(from the lowest to the highest). The length scales
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
For task 1.1, we performed this feature selection
over all 160 features mentioned in Section 2. For
task 1.3, we used a subset of the 80 most general
BB features as in (Shah et al, 2013), for which we
had all the necessary resources available for the
extraction. We selected the top 25 features for both
models, based on empirical results found by Shah
et al (2013) for a number of datasets, and then
retrained the GP using only the selected features.
4 Active Learning
Active Learning (AL) is a machine learning
paradigm that let the learner decide which data it
wants to learn from (Settles, 2010). The main goal
of AL is to reduce the size of the dataset while
keeping similar model performance (therefore re-
ducing annotation costs). In previous work with
17 baseline features, we have shown that with only
?30% of instances it is possible to achieve 99%
of the full dataset performance in the case of the
WMT12 QE dataset (Beck et al, 2013).
To investigate if a reduced dataset can achieve
competitive performance in a blind evaluation set-
ting, we submitted an entry for both tasks 1.1 and
1.3 composed of models trained on a subset of in-
stances selected using AL, and paired with fea-
ture selection. Our AL procedure starts with a
model trained on a small number of randomly se-
lected instances from the training set and then uses
this model to query the remaining instances in the
training set (our query pool). At every iteration,
the model selects the more ?informative? instance,
asks an oracle for its true label (which in our case
is already given in the dataset, and therefore we
2http://sheffieldml.github.io/GPy/
339
only simulate AL) and then adds it to the training
set. Our procedure started with 50 instances for
task 1.1 and 20 instances for task 1.3, given its re-
duced training set size. We optimised the Gaussian
Process hyperparameters every 20 new instances,
for both tasks.
As a measure of informativeness we used Infor-
mation Density (ID) (Settles and Craven, 2008).
This measure leverages between the variance
among instances and how dense the region (in the
feature space) where the instance is located is:
ID(x) = V ar(y|x)?
(
1
U
U?
u=1
sim(x,x(u))
)?
The ? parameter controls the relative impor-
tance of the density term. In our experiments, we
set it to 1, giving equal weights to variance and
density. The U term is the number of instances
in the query pool. The variance values V ar(y|x)
are given by the GP prediction while the similar-
ity measure sim(x,x(u)) is defined as the cosine
distance between the feature vectors.
In a real annotation setting, it is important to
decide when to stop adding new instances to the
training set. In this work, we used the confidence
method proposed by Vlachos (2008). This is an
method that measures the model?s confidence on
a held-out non-annotated dataset every time a new
instance is added to the training set and stops the
AL procedure when this confidence starts to drop.
In our experiments, we used the average test set
variance as the confidence measure.
In his work, Vlachos (2008) showed a correla-
tion between the confidence and test error, which
motivates its use as a stop criterion. To check if
this correlation also occurs in our task, we measure
the confidence and test set error for task 1.1 using
the WMT12 split (1832/422 instances). However,
we observed a different behaviour in our experi-
ments: Figure 1 shows that the confidence does
not raise or drop according to the test error but it
stabilises around a fixed value at the same point as
the test error also stabilises. Therefore, instead of
using the confidence drop as a stop criterion, we
use the point where the confidence stabilises. In
Figure 2 we can observe that the confidence curve
for the WMT13 test set stabilises after ?580 in-
stances. We took that point as our stop criterion
and used the first 580 selected instances as the AL
dataset.
Figure 1: Test error and test confidence curves
for HTER prediction (task 1.1) using the WMT12
training and test sets.
Figure 2: Test confidence for HTER prediction
(task 1.1) using the official WMT13 training and
test sets.
We repeated the experiment with task 1.3, mea-
suring the relationship between test confidence
and error using a 700/100 instances split (shown
on Figure 3). For this task, the curves did not fol-
low the same behaviour: the confidence do not
seem to stabilise at any point in the AL proce-
dure. The same occurred when using the official
training and test sets (shown on Figure 4). How-
ever, the MAE curve is quite flat, stabilising after
about 100 sentences. This may simply be a conse-
quence of the fact that our model is too simple for
post-editing time prediction. Nevertheless, in or-
der to analyse the performance of AL for this task
we submitted an entry using the first 100 instances
chosen by the AL procedure for training.
The observed peaks in the confidence curves re-
340
Task 1.1 - Ranking Task 1.1 - Scoring Task 1.3
DeltaAvg ? Spearman ? MAE ? RMSE ? MAE ? RMSE ?
SHEF-Lite-FULL 9.76 0.57 12.42 15.74 55.91 103.11
SHEF-Lite-AL 8.85 0.50 13.02 17.03 64.62 99.09
Baseline 8.52 0.46 14.81 18.22 51.93 93.36
Table 1: Submission results for tasks 1.1 and 1.3. The bold value shows a winning entry in the shared
task.
Figure 3: Test error and test confidence curves
for post-editing time prediction (task 1.3) using a
700/100 split on the WMT13 training set.
Figure 4: Test confidence for post-editing time
prediction (task 1.3) using the official WMT13
training and test sets.
sult from steps where the hyperparameter optimi-
sation got stuck at bad local optima. These de-
generated results set the variances (?2f , ?2n) to very
high values, resulting in a model that considers all
data as pure noise. Since this behaviour tends to
disappear as more instances are added to the train-
ing set, we believe that increasing the dataset size
helps to tackle this problem. We plan to investi-
gate this issue in more depth in future work.
For both AL datasets we repeated the feature se-
lection procedure explained in Section 3.1, retrain-
ing the models on the selected features.
5 Results
Table 1 shows the results for both tasks. SHEF-
Lite-FULL represents GP models trained on the
full dataset (relative to each task) with a feature
selection step. SHEF-Lite-AL corresponds to the
same models trained on datasets obtained from
each active learning procedure and followed by
feature selection.
For task 1.1, our submission SHEF-Lite-FULL
was the winning system in the scoring subtask, and
ranked third in the ranking subtask. These results
show that GP models achieve the state of the art
performance in QE. These are particularly positive
results considering that there is room for improve-
ment in the feature selection procedure to identify
the optimal number of features to be selected. Re-
sults for task 1.3 were below the baseline, once
again evidencing the fact that the noise model used
in our experiments is probably too simple for post-
editing time prediction. Post-editing time is gener-
ally more prone to large variations and noise than
HTER, especially when annotations are produced
by multiple post-editors. Therefore we believe that
kernels that encode more advanced noise models
(such as the multi-task kernel used by Cohn and
Specia (2013)) should be used for better perfor-
mance. Another possible reason for that is the
smaller set of features used for this task (black-
box features only).
Our SHEF-Lite-AL submissions performed bet-
ter than the baseline in both scoring and ranking
in task 1.1, ranking 2nd place in the scoring sub-
task. Considering that the dataset is composed by
only ?25% of the full training set, these are very
encouraging results in terms of reducing data an-
341
notation needs. We note however that these results
are below those obtained with the full training set,
but Figure 1 shows that it is possible to achieve
the same or even better results with an AL dataset.
Since the curves shown in Figure 1 were obtained
using the full feature set, we believe that advanced
feature selection strategies can help AL datasets to
achieve better results.
6 Conclusions
The results obtained by our submissions confirm
the potential of Gaussian Processes to become the
state of the art approach for Quality Estimation.
Our models were able to achieve the best perfor-
mance in predicting HTER. They also offer the ad-
vantage of inferring a probability distribution for
each prediction. These distributions provide richer
information (like variance values) that can be use-
ful, for example, in active learning settings.
In the future, we plan to further investigate these
models by devising more advanced kernels and
feature selection methods. Specifically, we want
to employ our feature set in a multi-task kernel set-
ting, similar to the one proposed by Cohn and Spe-
cia (2013). These kernels have the power to model
inter-annotator variance and noise, which can lead
to better results in the prediction of post-editing
time.
We also plan to pursue better active learning
procedures by investigating query methods specif-
ically tailored for QE, as well as a better stop cri-
teria. Our goal is to be able to reduce the dataset
size significantly without hurting the performance
of the model. This is specially interesting in the
case of QE, since it is a very task-specific problem
that may demand a large annotation effort.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Kashif Shah and Lucia Specia).
References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013.
Reducing Annotation Effort for Quality Estimation
via Active Learning. In Proceedings of ACL (to ap-
pear).
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Chris Callison-burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL (to appear).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV (to appear).
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jose? G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session (to appear).
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech & Language,
22(3):295?312, July.
342
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 151?159,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Factored Markov Translation with Robust Modeling
Yang Feng
?
Trevor Cohn
?
Xinkai Du
?
Information Sciences Institue
?
Computing and Information Systems
Computer Science Department The University of Melbourne
University of Southern California VIC 3010 Australia
{yangfeng145, xinkaid}@gmail.com t.cohn@unimelb.edu.au
Abstract
Phrase-based translation models usually
memorize local translation literally and
make independent assumption between
phrases which makes it neither generalize
well on unseen data nor model sentence-
level effects between phrases. In this pa-
per we present a new method to model
correlations between phrases as a Markov
model and meanwhile employ a robust
smoothing strategy to provide better gen-
eralization. This method defines a re-
cursive estimation process and backs off
in parallel paths to infer richer structures.
Our evaluation shows an 1.1?3.2% BLEU
improvement over competitive baselines
for Chinese-English and Arabic-English
translation.
1 Introduction
Phrase-based methods to machine translation
(Koehn et al., 2003; Koehn et al., 2007) have dras-
tically improved beyond word-based approaches,
primarily by using phrase-pairs as translation
units, which can memorize local lexical con-
text and reordering patterns. However, this lit-
eral memorization mechanism makes it general-
ize poorly to unseen data. Moreover, phrase-based
models make an independent assumption, stating
that the application of phrases in a derivation is in-
dependent to each other which conflicts with the
underlying truth that the translation decisions of
phrases should be dependent on context.
There are some work aiming to solve the two
problems. Feng and Cohn (2013) propose a
word-based Markov model to integrate translation
and reordering into one model and use the so-
phisticated hierarchical Pitman-Yor process which
backs off from larger to smaller context to pro-
vide dynamic adaptive smoothing. This model
shows good generalization to unseen data while
it uses words as the translation unit which can-
not handle multiple-to-multiple links in real word
alignments. Durrani et al. (2011) and Durrani et
al. (2013) propose an operation sequence model
(OSM) which models correlations between mini-
mal translation units (MTUs) and evaluates proba-
bilities with modified Kneser-Ney smoothing. On
one hand the use of MTUs can help retain the
multiple-to-multiple alignments, on the other hand
its definition of operations where source words
and target words are bundled into one operation
makes it subjected to sparsity. The common fea-
ture of the above two methods is they both back off
in one fixed path by dropping least recent events
first which precludes some useful structures. For
the segment pairs <b?a t?a k?aol`v j`?nq`u, take it into
account> in Figure 1, the more common structure
is <b?a ... k?aol`v j`?nq`u, take ... into account>. If
we always drop the least recent events first, then
we can only learn the pattern <... t?a k?aol`v j`?nq`u,
... it into account>.
On these grounds, we propose a method with
new definition of correlations and more robust
probability modeling. This method defines a
Markov model over correlations between minimal
phrases where each is decomposed into three fac-
tors (source, target and jump). In the meantime
it employs a fancier smoothing strategy for the
Markov model which backs off by dropping mul-
tiple conditioning factors in parallel in order to
learn richer structures. Both the uses of factors
and parallel backoff give rise to robust modeling
against sparsity. In addition, modeling bilingual
information and reorderings into one model in-
stead of adding them to the linear model as sep-
arate features allows for using more sophisticated
estimation methods rather than get a loose weight
for each feature from tuning algorithms.
We compare the performance of our model with
that of the phrase-based model and the hierarchi-
cal phrase-based model on the Chinese-English
and Arabic-English NIST test sets, and get an im-
151
Figure 1: Example Chinese-English sentence pair
with word alignments shown as filled grid squares.
provement up to 3.2 BLEU points absolute.
1
2 Modelling
Our model is phrase-based and works like a
phrase-based decoder by generating target trans-
lation left to right using phrase-pairs while jump-
ing around the source sentence. For each deriva-
tion, we can easily get its minimal phrase (MPs)
sequence where MPs are ordered according to the
order of their target side. Then this sequence of
events is modeled as a Markov model and the log
probability under this Markov model is included
as an additional feature into the linear SMT model
(Och, 2003).
A MP denotes a phrase which cannot contain
other phrases. For example, in the sentence pair
in Figure 1, <b?a t?a , take it> is a phrase but not
a minimal phrase, as it contains smaller phrases
of <b?a , take> and <t?a , it>. MPs are a com-
plex event representation for sequence modelling,
and using these naively would be a poor choice
because few bigrams and trigrams will be seen
often enough for reliable estimation. In order
to reason more effectively from sparse data, we
consider more generalized representations by de-
composing MPs into their component events: the
source phrase (source
?
f ), the target phrase (tar-
get e?) and the jump distance from the preceding
MP (jump j), where the jump distance is counted
in MPs, not in words. For sparsity reasons, we
do not use the jump distance directly but instead
group it into 12 buckets:
{insert,? ?5,?4,?3,?2,?1, 0, 1, 2, 3, 4,? 5},
where the jump factor is denoted as insert when
the source side is NULL. For the sentence pair in
1
We will contribute the code to Moses.
Figure 1, the MP sequence is shown in Figure 2.
To evaluate the Markov model, we condition
each MP on the previous k ? 1 MPs and model
each of the three factors separately based on a
chain rule decomposition. Given a source sentence
f and a target translation e, the joint probability is
defined as
p(
?
e
I
1
, j
I
1
,
?
f
I
1
) =
I
?
i=1
p(e?
i
|
?
f
i
i?k+1
, j
i
i?k+1
, e?
i?1
i?k+1
)
?
I
?
i=1
p(
?
f
i
|
?
f
i?1
i?k+1
, j
i
i?k+1
, e?
i?1
i?k+1
)
?
I
?
i=1
p(j
i
|
?
f
i?1
i?k+1
, j
i?1
i?k+1
, e?
i?1
i?k+1
)
(1)
where
?
f
i
, e?
i
and j
i
are the factors of MP
i
,
?
f
I
1
=
(
?
f
1
,
?
f
2
, . . . ,
?
f
I
) is the sequence of source MPs,
?
e
I
1
= (e?
1
, e?
2
, . . . , e?
I
) is the sequence of tar-
get MPs, and j
I
1
= (j
1
, j
2
, . . . , j
I
) is the vec-
tor of jump distance between MP
i?1
and MP
i
, or
insert for MPs with null source sides.
2
To eval-
uate each of the k-gram models, we use modified
Keneser-Ney smoothing to back off from larger
context to smaller context recursively.
In summary, adding the Markov model into the
decoder involves two passes: 1) training a model
over the MP sequences extracted from a word
aligned parallel corpus; and 2) calculating the
probability of the Markov model for each trans-
lation hypothesis during decoding. This Markov
model is combined with a standard phrase-based
model
3
(Koehn et al., 2007) and used as an addi-
tional feature in the linear model.
In what follows, we will describe how to estati-
mate the k-gram Markov model, focusing on back-
off (?2.1) and smoothing (?2.2).
2.1 Parallel Backoff
Backoff is a technique used in language model ?
when estimating a higher-order gram, instead of
using the raw occurrence count, only a portion is
used and the remainder is computed using a lower-
order model in which one of the context factors
2
Note that factors at indices 0,?1, . . . ,?(k ? 1) are set
to a sentinel value to denote the start of sentence.
3
The phrase-based model considers larger phrase-pairs
than just MPs, while our Markov model consider only MPs.
As each phrase-pair is composed of a sequence of MPs un-
der fixed word alignment, by keeping the word alignment for
each phrase, a decoder derivation unambiguously specifies
the MP sequence for scoring under our Markov model.
152
index sentence pair minimal phrase sequence
w?om?en y
?
ingg?ai b?a t?a y?e k?aol`v j`?nq`u jump source target
1 We T
1
1 w?om?en We
2 should T
2
1 y
?
ingg?ai should
3 also T
3
3 y?e also
4 take T
4
-2 b?a take
5 it T
5
1 t?a it
6 into account T
6
2 k?aol`v j`?nq`u into account
Figure 2: The minimal phrase sequence T
1
, ..., T
6
extracted from the sentence pair in Figure 1.
step 3-gram e?
3
|
?
f
3
, j
3
, e?
2
,
?
f
2
, j
2
, e?
1
,
?
f
1
, j
1
0 into account | k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2
? 1
1 into account | k?aol`v j`?nq`u, 2, it, t?a, ?, take, b?a, -2
? t?a
2 into account | k?aol`v j`?nq`u, 2, it, ?, ?, take, b?a, -2
? it
3 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, b?a, -2
? -2
4 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, b?a, ?
? b?a
5 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, ?, ?
? take
6 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, ?, ?, ?
? 2
7 into account | k?aol`v j`?nq`u, ?, ?, ?, ?, ?, ?, ?
? k?aol`v j`?nq`u
8 into account | ?, ?, ?, ?, ?, ?, ?, ?
Figure 3: One backoff path for the 3-gram in
Equation 2. The symbols besides each arrow mean
the current factor to drop; ??? is a placeholder for
factors which can take any value.
is dropped. Here the probabilities of the lower-
order which is used to construct the higher-order is
called the backoff probability of the higher-order
gram. Different from standard language models
which drop the least recent words first, we em-
ploy a different backoff strategy which considers
all possible backoff paths. Taking as an example
the 3-gram T
4
T
5
T
6
in Figure 2, when estimating
the probability of the target factor
p(into account | k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2 ) ,
(2)
Figure 4: The backoff graph for the 3-gram model
of the target factor. The symbol beside each arrow
is the factor to drop.
we consider two backoff paths: path
1
drops the
factors in the order -2, b?a, take, 1, t?a, it, 2,
k?aol`v j`?nq`u; path
2
uses order 1, t?a, it, -2, b?a,
take, 2, k?aol`v j`?nq`u. Figure 3 shows the backoff
process for path
2
. In this example with two back-
off paths, the backoff probability g is estimated as
g(into acc.|c) =
1
2
p(into acc.|c
?
)+
1
2
p(into acc.|c
??
) ,
where c =< k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2 >,
c
?
=< k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, ? > and
c
??
=< k?aol`v j`?nq`u, 2, it, t?a, ?, take, b?a, -2 >.
Formally, we use the notion of backoff graph to
define the recursive backoff process of a k-gram
153
and denote as nodes the k-gram and the lower-
order grams generated by the backoff. Once one
node occurs in the training data fewer than ? times,
then estimates are calculated by backing off to the
nodes in the next lower level where one factor is
dropped (denoted using the placeholder ? in Fig-
ure 4). One node can have one or several candidate
backoff nodes. In the latter case, the backoff prob-
ability is defined as the average of the probabilities
of the backoff nodes in the next lower level.
We define the backoff process for the 3-gram
model predicting the target factor, e?
3
, as illustrated
in Figure 4. The top level is the full 3-gram, from
which we derive two backoff paths by dropping
factors from contextual events, one at a time. For-
mally, the backoff strategy is to drop the previ-
ous two MPs one by one while for each MP the
dropping routine is first the jump factor, then the
source factor and final the target factor. Each step
on the path corresponds to dropping an individ-
ual contextual factor from the context. The paths
converge when only the third MP left, then the
backoff proceeds by dropping the jump action, j
3
,
then finally the source phrase,
?
f
3
. The paths B-
D-F-H-J and C-E-G-I-K show all the possible or-
derings (corresponding to c
??
and c
?
, respectively)
for dropping the two previous MPs. The exam-
ple backoff in Figure 3 corresponds the path A-
B-D-F-H-J-L-M-N in Figure 4, shown as heavier
lines. When generizing to the k-gram for target
p(e?
k
|
?
f
k
1
, j
k
1
, e?
k?1
1
), the backoff strategy is to first
drop the previous k-1 MPs one by one (for each
MP, still drops in the order of jump, source and
target), then the kth jump factor and finally the kth
source factor. According to the strategy, the top
node has k-1 nodes to back off to and for the node
e?
k
|
?
f
k
2
, j
k
2
, e?
k?1
2
where only the factors of MP
1
are
dropped, there are k-2 nodes to back off to.
2.2 Probability Estimation
We adopt the technique used in factor language
models (Bilmes and Kirchhoff, 2003; Kirchhoff et
al., 2007) to estimate the probability of a k-gram
p(e?
i
|c) where c =
?
f
i
i?k+1
, j
i
i?k+1
, e?
?1
i?k+1
. Ac-
cording to the definition of backoff, only when the
count of the k-gram exceeds some given threshold,
its maximum-likelihood estimate, p
ML
(e?
k
|c) =
N(e?
k
,c)
N(c) is used, where N(?) is the count of an
event and/or context. Otherwise, only a portion of
p
ML
(e?
k
|c) is used and the remainder is constructed
from a lower-level (by dropping a factor). In or-
der to ensure valid probability estimates, i.e. sums
to unity, probability mass needs to be ?stolen?
from the higher level and given to the lower level.
Hence, the whole definition is
p(e?
i
|c) =
{
d
N(e?
i
,c)pml(e?i|c) if N(e?i, c) > ?k
?(c)g(e?
i
, c) otherwise
(3)
where d
N(e?
i
,c) is a discount parameter which re-
serves probability from the maximum-likelihood
estimate for backoff smoothing at the next lower-
level, and we estimate d
N(e?
i
,c) using modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1996); ?
k
is the threshold for
the count of the k-gram, ?(c) is the backoff weight
used to make sure the entire distribution still sums
to unity,
?(c) =
1?
?
e?:N(e?,c)>?
k
d
N(e?,c)pML(e?|c)
?
e?:N(e?,c)??
k
g(e?, c)
,
and g(e?
i
, c) is the backoff probability which we
estimate by averaging over the nodes in the next
lower level,
g(e?
i
, c) =
1
?
?
c?
p(e?
i
|c
?
) ,
where ? is the number of nodes to back off, c
?
is
the lower-level context after dropping one factor
from c.
The k-gram for the source and jump factors are
estimated in the same way, using the same backoff
semantics.
4
Note (3) is applied independently to
each of the three models, so the use of backoff may
differ in each case.
3 Discussion
As a part of the backoff process our method
can introduce gaps in estimating rule probabili-
ties; these backoff patterns often bear close re-
semblance to SCFG productions in the hierarchi-
cal phrase-based model (Chiang, 2007). For ex-
ample, in step 0 in Figure 3, as all the jump factors
are present, this encodes the full ordering of the
MPs and gives rise to the aligned MP pairs shown
in Figure 5 (a). Note that an X
1
placeholder is
included to ensure the jump distance from the pre-
vious MP to the MP <b?a, take> is -2. The ap-
proximate SCFG production for the MP pairs is
<b?a t?a X
1
k?aol`v j`?nq`u, X
1
take it into account>.
4
Although there are fewer final steps, L-M-N in Fig. 4,
as we assume the MP is generated in the order jump, source
phrase then target phrase in a chain rule decomposition.
154
Figure 5: Approximate SCFG patterns for step 0,
3 of Figure 3. X is a non-terminal which can only
be rewritten by one MP. ? and ? ? ? denote gaps
introduced by the left-to-right decoding algorithm
and ? can only cover one MP while ? ? ? can
cover zero or more MPs.
In step 1, as the jump factor 1 is dropped, we do
not know the orientation between b?a and t?a. How-
ever several jump distances are known: from X
1
to b?a is distance -2 and t?a to k?aol`v j`?nq`u is 2. In
this case, the source side can be
b?a t?a X
1
k?aol`v j`?nq`u,
b?a ? X
1
? ? ? t?a ? k?aol`v j`?nq`u,
t?a b?a k?aol`v j`?nq`u X
1
,
t?a ? k?aol`v j`?nq`u ? ? ? b
?a ? X
1
,
where X and ? can only hold one MP while ? ? ?
can cover zero or more MPs. In step 3 after drop-
ping t?a and it, we introduce a gap X
2
as shown in
Figure 5 (b).
From above, we can see that our model has two
kinds of gaps: 1) in the source due to the left-to-
right target ordering (such as the ? in step 3); and
2) in the target, arising from backoff (such as the
X
2
in step 3). Accordingly our model supports
rules than cannot be represented by a 2-SCFG
(e.g., step 3 in Figure 5 requires a 4-SCFG). In
contrast, the hierarchical phrase-based model al-
lows only 2-SCFG as each production can rewrite
as a maximum of two nonterminals. On the other
hand, our approach does not enforce a valid hier-
archically nested derivation which is the case for
Chiang?s approach.
4 Related Work
The method introduced in this paper uses fac-
tors defined in the same manner as in Feng and
Cohn (2013), but the two methods are quite differ-
ent. That method (Feng and Cohn, 2013) is word-
based and under the frame of Bayesian model
while this method is MP-based and uses a sim-
pler Kneser-Ney smoothing method. Durrani et
al. (2013) also present a Markov model based on
MPs (they call minimal translation units) and fur-
ther define operation sequence over MPs which
are taken as the events in the Markov model. For
the probability estimation, they use Kneser-Ney
smoothing with a single backoff path. Different
from operation sequence, our method gives a neat
definition of factors which uses jump distance di-
rectly and avoids the bundle of source words and
target words like in their method, and hence miti-
gates sparsity. Moreover, the use of parallel back-
off infers richer structures and provides robust
modeling.
There are several other work focusing on mod-
eling bilingual information into a Markov model.
Crego et al. (2011) develop a bilingual language
model which incorporates words in the source and
target languages to predict the next unit, and use
it as a feature in a translation system. This line
of work was extended by Le et al. (2012) who de-
velop a novel estimation algorithm based around
discriminative projection into continuous spaces.
Neither work includes the jump distance, and nor
155
do they consider dynamic strategies for estimating
k-gram probabilities.
Galley and Manning (2010) propose a method
to introduce discontinuous phrases into the phrase-
based model. It makes use of the decoding mecha-
nism of the phrase-based model which jumps over
the source words and hence can hold discontin-
uous phrases naturally. However, their method
doesn?t touch the correlations between phrases and
probability modeling which are the key points we
focus on.
5 Experiments
We design experiments to first compare our
method with the phrase-based model (PB), the op-
eration sequence model (OSM) and the hierarchi-
cal phrase-based model (HPB), then we present
several experiments to test:
1. how each of the factors in our model and par-
allel backoff affect overall performance;
2. how the language model order affects the rel-
ative gains, in order to test if we are just learn-
ing a high order LM, or something more use-
ful;
3. how the Markov model interplay with the
distortion and lexical reordering models of
Moses, and are they complemenatary;
4. whether using MPs as translation units is bet-
ter in our approach than the simpler tactic of
using only word pairs.
5.1 Data Setup
We consider two language pairs: Chinese-English
and Arabic-English. The Chinese-English paral-
lel training data is made up of the non-UN por-
tions and non-HK Hansards portions of the NIST
training corpora, distributed by the LDC, having
1,658k sentence pairs with 40m and 44m Chinese
and English words. We used the NIST 02 test set
as the development set and evaluated performance
on the test sets from NIST 03 and 05.
For the Arabic-English task, the training data
comprises several LDC corpora,
5
including 276k
sentence pairs and 8.21m and 8.97m words in Ara-
bic and English, respectively. We evaluated on the
NIST test sets from 2003 and 2005, and the NIST
02 test set was used for parameter tuning.
On both cases, we used the factor language
model module (Kirchhoff et al., 2007) of the
SRILM toolkit (Stolcke, 2002) to train a Markov
5
LDC2004E72, LDC2004T17, LDC2004T18,
LDC2006T02
model with the order = 3 over the MP sequences.
6
The threshold count of backoff for all nodes was
? = 2.
We aligned the training data sets by first using
GIZA++ toolkit (Och and Ney, 2003) to produce
word alignments on both directions and then com-
bining them with the diag-final-and heuristic. All
experiments used a 5-gram language model which
was trained on the Xinhua portion of the GIGA-
WORD corpus using the SRILM toolkit. Transla-
tion performance was evaluated using BLEU (Pa-
pineni et al., 2002) with case-insensitive n ? 4-
grams. We used minimum error rate training (Och,
2003) to tune the feature weights to maximize the
BLEU score on the development set.
We used Moses for PB and Moses-chart for
HPB with the configuration as follows. For both,
max-phrase-length=7, ttable-limit
7
=20, stack-
size=50 and max-pop-limit=500; For Moses,
search-algorithm=1 and distortion-limit=6; For
Moses-chart, search-algorithm=3 and max-char-
span
8
=20 for Moses-chart. We used both the dis-
tortion model and the lexical reordering model for
Moses (denoted as Moses-l) except in ?5.5 we only
used the distortion model (denoted as Moses-d).
We implemented the OSM according to Durrani
et al. (2013) and used the same configuration with
Moses-l. For our method we used the same config-
uration as Moses-l but adding an additional feature
of the Markov model over MPs.
5.2 Performance Comparison
We first give the results of performance compar-
ison. Here we add another system (denoted as
Moses-l+trgLM): Moses-l together with the target
language model trained on the training data set,
using the same configuration with Moses-l. This
system is used to test whether our model gains im-
provement just for using additional information on
the training set. We use the open tool of Clark et
al. (2011) to control for optimizer stability and test
statistical significance.
The results are shown in Tables 1 and 2. The
two language pairs we used are quite different:
Chinese has a much bigger word order differ-
ence c.f. English than does Arabic. The results
show that our system can outperform the baseline
6
We only employed MPs with the length ? 3. If a MP had
more than 3 words on either side, we omitted the alignment
links to the first target word of this MP and extracted MPs
according to the new alignment.
7
The maximum number of lexical rules for each source
span.
8
The maximum span on the source a rule can cover.
156
System NIST 02 (dev) NIST 03 NIST 05
Moses-l 36.0 32.8 32.0
Moses-chart 36.9 33.6 32.6
Moses-l+trgLM 36.4 33.9 32.9
OSM 36.6 34.0 33.1
our model 37.9 36.0 35.1
Table 1: BLEU % scores on the Chinese-English
data set.
System NIST 02 (dev) NIST 03 NIST 05
Moses-l 60.4 52.0 52.8
Moses-chart 60.7 51.8 52.4
Moses-l+trgLM 60.8 52.6 53.3
OSM 61.1 52.9 53.4
our model 62.2 53.6 53.9
Table 2: BLEU % scores on the Arabic-English
data set.
systems significantly (with p < 0.005) on both
language pairs, nevertheless, the improvement on
Chinese-English is bigger. The big improvement
over Moses-l+trgLM proves that the better perfor-
mance of our model does not solely comes from
the use of the training data. And the gain over
OSM means our definition of factors gives a better
handling to sparsity. We also notice that HPB does
not give a higher BLEU score on Arabic-English
than PB. The main difference between HPB and
PB is that HPB employs gapped rules, so this re-
sult suggests that gaps are detrimental for Arabic-
English translation. In ?5.3, we experimentally
validate this claim with our Markov model.
5.3 Impact of Factors and Parallel Backoff
We now seek to test the contribution of target,
jump, source factors, as well as the parallel back-
off technique in terms of BLEU score. We
performed experiments on both Chinese-English
and Arabic-English to test whether the contri-
bution was related to language pairs. We de-
signed the experiments as follows. We first
trained a 3-gram Markov model only over tar-
get factors, p(
?
e
I
1
|
?
f
I
1
) =
?
I
i=1
p(e?
i
|e?
i?1
i?2
), de-
noted +t. Then we added the jump fac-
tor (+t+j), such that we now considering
both target and jump events, p(
?
e
I
1
,
?
j
I
1
|
?
f
I
1
) =
?
I
i=1
p(e?
i
|
?
j
i
i?2
, e?
i?1
i?2
)p(
?
j
i
|
?
j
i?1
i?2
, e?
i?1
i?2
). Next we
added the source factor (+t+j+s) such that now all
three factors are included from Equation 1. For
the above three Markov models we used simple
least-recent backoff (akin to a standard language
model), and consequently these methods cannot
represent gaps in the target. Finally, we trained an-
System Chinese-English Arabic-English
NIST 02 NIST 03 NIST 02 NIST 03
Moses-l 36.0 32.8 60.4 52.0
+t 36.3 33.8 60.9 52.4
+t+j 37.1 34.7 62.1 53.4
+t+j+s 37.6 34.8 62.5 53.9
+t+j+s+p 37.9 36.0 62.2 53.6
Table 3: The impact of factors and parallel back-
off. Key: t?target, j?jump, s?source, p?parallel
backoff.
System 2gram 3gram 4gram 5gram 6gram
Moses-l 27.2 32.4 33.0 32.8 33.2
our method 31.6 34.0 35.8 36.0 36.2
Table 4: The impact of the order of the standard
language models.
other Markov model by introducing parallel back-
off to the third one as described in ?2.1. Each
of the four Markov model approaches are imple-
mented as adding an additional feature, respec-
tively, into the Moses-l baseline.
The results are shown in Table 3. Observe that
adding each factor results in near uniform per-
formance improvements on both language pairs.
The jump factor gives big improvements of about
1% BLEU in both language pairs. However when
using parallel backoff, the performance improves
greatly for Chinese-English but degrades slightly
on Arabic-English. The reason may be parallel
backoff is used to encode common structures to
capture the different word ordering between Chi-
nese and English while for Arabic-English there
are fewer consistent reordering patterns. This is
also consistent with the results in Table 1 and 2
where HPB gets a little bit lower BLEU scores.
5.4 Impact of LM order
Our system resembles a language model in com-
mon use in SMT systems, in that it uses a Markov
model over target words, among other factors.
This raises the question of whether its improve-
ments are due to it functioning as a target language
model. Our experiments use order k = 3 over MP
sequences and each MP can have at most 3 words.
Therefore the model could in principle memorize
9-grams, although usually MPs are much smaller.
To test whether our improvements are from using
a higher-order language model or other reasons,
we evaluate our system and the baseline system
with a range of LMs of different order. If we can
get consistent improvements over the baseline for
157
System NIST 02 (dev) NIST 03
Moses-d 35.1 31.3
Moses-l 36.0 32.8
Moses-d+M 36.4 34.8
Moses-l+M 37.9 36.0
Table 5: Comparison between our Markov model
(denoted as M) and the lexical reordering model
of Moses.
both small and large n, this suggests it?s not the
long context that plays the key role but is other
information we have learned (e.g., jumps or rich
structures).
Table 4 shows the results of using standard lan-
guage models with orders 2 ? 6 in Moses-l and
our method. We can see that language model or-
der is very important. When we increase the order
from 2 to 4, the BLEU scores for both systems in-
creases drastically, but levels off for 4-gram and
larger. Note that our system outperforms Moses-l
by 4.4, 1.6, 2.8, 3.2 and 3.0 BLEU points, respec-
tively. The large gain for 2-grams is likely due to
the model behaving like a LM, however the fact
that consistent gains are still realized for higher
k suggests that the approach brings considerable
complementary information, i.e., it is doing much
more than simply language modelling.
5.5 Comparison with Lexical Reordering
Our Markov model learns a joint model of jump,
source and target factors and this is similar to the
lexical reordering model of Moses (Koehn et al.,
2007), which learns general orientations of pairs
of adjacent phrases (classed as monotone, swap or
other). Our method is more complex, by learning
explicit jump distances, while also using broader
context. Here we compare the two methods, and
test whether our approach is complementary by re-
alizing gains over the lexicalized reordering base-
line. We test this hypothesis by comparing the
results of Moses with its simple distortion model
(Moses-d), then with both simple distortion and
lexicalized reordering (Moses-l), and then with our
Markov model (denoted as Moses-d+M or Moses-
l+M, for both baselines respectively).
The results are shown in Table 5. Comparing
the results of Moses-l and Moses-d, we can see that
the lexical reordering model outperforms the dis-
tortion model by a margin of 1.5% BLEU. Com-
paring Moses-d+M with Moses-l, our Markov
model provides further improvements of 2.0%
System NIST 02 (dev) NIST 03
Moses-l 36.0 32.8
Moses-l+word 36.9 34.0
Moses-l+MP 37.6 34.8
Table 6: Comparison between the MP-based
Markov model and the word-based Markov model.
BLEU. Our approach does much more than model
reordering, so it is unlikely that this improvement
is solely due to being better a model of distor-
tion. This is underscored by the final result in
Table 5, for combining lexicalized distortion with
our model (Moses-l+M) which gives the highest
BLEU score, yielding another 1.2% increase.
5.6 Comparison with Word-based Markov
Our approach uses minimal phrases as its basic
unit of translation, in order to preserve the many-
to-many links found from the word alignments.
However we now seek to assess the impact of the
choice of these basic units, considering instead a
simpler word-based setting which retains only 1-
to-1 links in a Markov model. To do this, we
processed target words left-to-right and for tar-
get words with multiple links, we only retained
the link which had the highest lexical translation
probability. Then we trained a 3-gram word-based
Markov model which backs off by dropping the
factors of the least recent word pairs in the order of
first jump then source then target. This model was
included as a feature in the Moses-l baseline (de-
noted as Moses-l+word), which we compared to a
system using a MP-based Markov model backing
off in the same way (denoted as Moses-l+MP).
According to the results in Table 6, using MPs
leads to better performance. Surprisingly even
the word based method outperforms the baseline.
This points to inadequate phrase-pair features in
the baseline, which can be more robustly esti-
mated using a Markov decomposition. In addition
to allowing for advanced smoothing, the Markov
model can be considered to tile phrases over one
another (each k-gram overlaps k?1 others) rather
than enforcing a single segmentation as is done in
the PB and HPB approaches. Fox (2002) states
that phrases tend to move as a whole during re-
ordering, i.e., breaking MPs into words opens the
possibility of making more reordering errors. We
could easily use larger phrase pairs as the basic
unit, such as the phrases used during decoding.
However, doing this involves a hard segmentation
158
and would exacerbate issues of data sparsity.
6 Conclusions
In this paper we try to give a solution to the prob-
lems in phrase-based models, including weak gen-
eralization to unseen data and negligence of cor-
relations between phrases. Our solution is to de-
fine a Markov model over minimal phrases so as
to model translation conditioned on context and
meanwhile use a fancy smoothing technique to
learn richer structures such that can be applied to
unseen data. Our method further decomposes each
minimal phrase into three factors and operates in
the unit of factors in the backoff process to provide
a more robust modeling.
In our experiments, we prove that our defini-
tion of factored Markov model provides comple-
mentary information to lexicalized reordering and
high order language models and the use of paral-
lel backoff infers richer structures even those out
of the reach of 2-SCFG and hence brings big per-
formance improvements. Overall our approach
gives significant improvements over strong base-
lines, giving consistent improvements of between
1.1 and 3.2 BLEU points on large scale Chinese-
English and Arabic-English evaluations.
7 Acknowledges
The first author is supported by DARPA BOLT,
contract HR0011-12-C-0014. The second au-
thor is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105). Thank the anonymous reviews for
their insightful comments.
References
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proc. of HLT-NAACL.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proc. of ACL, pages 310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL-HLT, pages 176?
181.
Josep Maria Crego, Franc?ois Yvon, and Jos?e B.
Mari?no. 2011. Ncode: an open source bilingual
n-gram smt toolkit. Prague Bull. Math. Linguistics,
96:49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proc. of ACL-HLT, pages
1045?1054, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proc. of NAACL, pages 1?11.
Yang Feng and Trevor Cohn. 2013. A markov
model of machine translation using non-parametric
bayesian inference. In Proc. of ACL, pages 333?
342.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP, pages 304?
311, July.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Proc. of NAACL, pages 966?974.
Katrin Kirchhoff, Jeff Bilmes, and Kevin Duh. 2007.
Factored language models tutorial.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume 1, pages 181?184.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, Demonstration Ses-
sion.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proc. of NAACL, pages 39?48.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
159
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 13?17,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Extracting Socioeconomic Patterns from the News: Modelling Text and
Outlet Importance Jointly
Vasileios Lampos
1
, Daniel Preot?iuc-Pietro
2
, Sina Samangooei
3
,
Douwe Gelling
2
, and Trevor Cohn
4
1
Department of Computer Science, University College London ? v.lampos@ucl.ac.uk
2
Department of Computer Science, The University of Sheffield ? {d.preotiuc,d.gelling}@shef.ac.uk
3
Electronics and Computer Science, University of Southampton ? ss@ecs.soton.ac.uk
4
Computing and Information Systems, The University of Melbourne ? t.cohn@unimelb.edu.au
Abstract
Information from news articles can be used
to study correlations between textual dis-
course and socioeconomic patterns. This
work focuses on the task of understanding
how words contained in the news as well as
the news outlets themselves may relate to
a set of indicators, such as economic senti-
ment or unemployment rates. The bilinear
nature of the applied regression model fa-
cilitates learning jointly word and outlet
importance, supervised by these indicators.
By evaluating the predictive ability of the
extracted features, we can also assess their
relevance to the target socioeconomic phe-
nomena. Therefore, our approach can be
formulated as a potential NLP tool, partic-
ularly suitable to the computational social
science community, as it can be used to in-
terpret connections between vast amounts
of textual content and measurable society-
driven factors.
1 Introduction
Vast amounts of user-generated content on the Inter-
net as well as digitised textual resources allow us to
study text in connection to real world events across
large intervals of time. Over the last decade, there
has been a shift in user news consumption starting
with a move from offline to online sources (Lin
et al., 2005); in more recent years user-generated
news have also become prominent. However, tra-
ditional news outlets continue to be a central refer-
ence point (Nah and Chung, 2012) as they still have
the advantage of being professionally authored, al-
leviating the noisy nature of citizen journalism for-
mats.
Here, we present a framework for analysing so-
cioeconomic patterns in news articles. In contrast
to prior approaches, which primarily focus on the
textual contents, our analysis shows how Machine
Learning methods can be used to gain insights into
the interplay between text in news articles, the news
outlets and socioeconomic indicators. Our experi-
ments are performed on a set of EU-related news
summaries spanning over 8 years, with the inten-
tion to study two basic economic factors: EU?s
unemployment rate and Economic Sentiment Index
(ESI) (European Commision, 1997). To determine
connections between the news, the outlets and the
indicators of interest, we formulate our learning
task as bilinear text-based regression (Lampos et
al., 2013).
Approaches to learning the correlation of news,
or text in general, with real world indicators have
been performed in both unsupervised and super-
vised settings. For example, Flaounas et al. (2010)
uncover interesting patterns in EU?s Mediasphere,
whereas Schumaker and Chen (2009) demonstrate
that news articles can predict financial indicators.
Conversely, Bentley et al. (2014) show that emo-
tions in the textual content of books reflect back
on inflation and unemployment rates during the
20th century. Recently, Social Media text has been
intensively studied as a quicker, unobtrusive and
cheaper alternative to traditional surveys. Applica-
tion areas include politics (O?Connor et al., 2010),
finance (Bollen and Mao, 2011), health (Lampos
and Cristianini, 2012; Paul and Dredze, 2011) or
psychology (De Choudhury et al., 2013; Schwartz
et al., 2013).
In this paper, we apply a modified version of a
bilinear regularised regression model (BEN) pro-
posed for the task of voting intention inference
from Twitter content (Lampos et al., 2013). The
main characteristic of BEN is the ability of mod-
elling word frequencies as well as individual user
importance in a joint optimisation task. By apply-
ing it in the context of supervised news analysis,
we are able to visualise relevant discourse to a par-
ticular socioeconomic factor, identifying relevant
words together with important outlets.
13
2 Data
We compiled a data set by crawling summaries
on news articles written in English language, pub-
lished by the Open Europe Think Tank.
1
The press
summaries are daily aggregations of news items
about the EU or member countries with a focus
on politics; the news outlets used to compile each
summary are listed below the summary?s text. The
site is updated every weekday, with the major news
being covered in a couple of paragraphs, and other
less prevalent issues being mentioned in one para-
graph to as little as one sentence. The news sum-
maries were first published on February 2006; we
collected all of them up to mid-November 2013,
creating a data set with the temporal resolution of
1913 days (or 94 months).
The text was tokenised using the NLTK li-
brary (Bird et al., 2009). News outlets with fewer
than 5 mentions were removed, resulting in a total
of 435 sources. Each summary contains on average
14 news items, with an average of 3 news sources
per item; where multiple sources were present, the
summary was assigned to all the referenced news
outlets. After removing stop words, we ended up
with 8, 413 unigrams and 19, 045 bigrams; their
daily occurrences were normalised using the total
number of news items for that day.
For the purposes of our supervised analysis, we
use the response variables of ESI and unemploy-
ment rate across the EU. The monthly time series
of these socioeconomic indicators were retrieved
from Eurostat, EU?s statistical office (see the red
lines in Fig. 1a and 1b respectively). ESI is a com-
posite indicator often seen as an early predictor for
future economic developments (Gelper and Croux,
2010). It consists of five confidence indicators with
different weights: industrial (40%), services (30%),
consumer (20%), construction (5%) and retail trade
(5%). The unemployment rate is a seasonally ad-
justed ratio of the non employed persons over the
entire EU labour force.
2
3 Models
A common approach to regression arises through
the application of generalised linear models. These
models use a feature vector inputx and aim to build
a linear function of x for predicting a response
1
http://www.openeurope.org.uk/Page/
PressSummary/en/
2
http://epp.eurostat.ec.europa.
eu/statistics_explained/index.php/
Unemployment_statistics
variable y:
f(x) = x
T
w + ? where x,w ? R
m
. (1)
The objective is to find an f , which minimises a
model-dependent loss function (e.g. sum squared
error), optionally subject to a regularisation penalty
?; `
2
-norm regularisation (ridge regression) pe-
nalises high weights (Hoerl and Kennard, 1970),
while `
1
-norm regularisation (lasso) encourages
sparse solutions (Tibshirani, 1994). Sparsity is de-
sirable for avoiding overfitting, especially when
the dimensionality m is larger than the number of
training examples n (Hastie et al., 2009). Elastic
Net formulates a combination of `
1
and `
2
-norm
regularisation defined by the objective:
{w
?
, ?
?
} =argmin
w,?
n
?
i=1
(x
T
i
?w + ? ? y
i
)
2
+ ?
EN
(w, ?) ,
(2)
where ? denotes the regularisation parameters (Zou
and Hastie, 2005); we refer to this model as LEN
(Linear Elastic Net) in the remainder of the script.
In the context of voting intention inference from
Twitter content, Lampos et al. (2013) extended
LEN to a bilinear formulation, where a set of two
vector weights are learnt: one for words (w) and
one for users (u). This was motivated by the ob-
servation that only a sparse set of users may have
predictive value. The model now becomes:
f(X) = u
T
Xw + ? , (3)
where X is a matrix of word ? users frequencies.
The bilinear optimisation objective is formulated
as:
{w
?
,u
?
, ?
?
} =argmin
w,u,?
n
?
i=1
(
u
T
X
i
w + ? ? y
i
)
2
+ ?
EN
(w, ?
1
) + ?
EN
(u, ?
2
) ,
(4)
where X
i
is the word ? user frequency matrix, and
?
1
, ?
2
are the word and user regularisation param-
eters. This can be treated as a biconvex learning
task and be solved by iterating over two convex
processes: fixingw and learning u, and vice versa
(Lampos et al., 2013). Regularised regression on
both user and word spaces allows for an automatic
selection of the most important words and users,
performing at the same time an improved noise
filtering.
14
In our experiments, news outlets and socioeco-
nomic indicators replace users and voting intention
in the previous model formulation. To ease the in-
terpretation of the outputs, we further impose a
positivity constraint on the outlet weights u, i.e.
min(u) ? 0; this makes the model more restric-
tive, but, in our case, did not affect the prediction
performance. We refer to this model as BEN (Bi-
linear Elastic Net).
4 Experiments
Both models are applied to the news summaries
data set with the aim to predict EU?s ESI and rate
of unemployment. The predictive capability of the
derived models, assessed by their respective infer-
ence performance, is used as a metric for judging
the degree of relevance between the learnt model
parameters ? word and outlet weights ? and the
response variable. A strong predictive performance
increases confidence on the soundness of those pa-
rameters.
To match input with the monthly temporal reso-
lution of the response variables, we compute the
mean monthly term frequencies for each outlet.
Evaluation is performed via a 10-fold validation,
where each fold?s training set is based on a mov-
ing window of p = 64 contiguous months, and the
test set consists of the following q = 3 months;
formally, the training and test sets for fold i are
based on months {q(i? 1) + 1, ..., q(i? 1) + p}
and {q(i? 1) + p+ 1, ..., q(i? 1) + p+ q} re-
spectively. In this way, we emulate a scenario
where we always train on past and predict future
points.
Performance results for LEN and BEN are pre-
sented in Table 1; we show the average Root Mean
Squared Error (RMSE) as well as an error rate
(RMSE over ?(y)) across folds to allow for a bet-
ter interpretation. BEN outperforms LEN in both
tasks, with a clearer improvement when predict-
ing ESI. Predictions for all folds are depicted in
Fig. 1a and 1b together with the actual values. Note
that reformulating the problem into a multi-task
learning scenario, where ESI and unemployment
are modelled jointly did not improve inference per-
formance.
The relatively small average error rates (< 8.8%)
make meaningful a further analysis of the model?s
outputs. Due to space limitations, we choose to fo-
cus on the most recent results, depicting the models
derived in the 10th fold. Following the example of
Schwartz et al. (2013), we use a word cloud visu-
ESI Unemployment
LEN 9.253 (9.89%) 0.9275 (8.75%)
BEN 8.209 (8.77%) 0.9047 (8.52%)
Table 1: 10-fold validation average RMSEs (and
error rates) for LEN and BEN on ESI and unem-
ployment rates prediction.
2007 2008 2009 2010 2011 2012 20130
50
100
 
 
actualpredictions
(a) ESI
2007 2008 2009 2010 2011 2012 20130
5
10
 
 
actualpredictions
(b) Unemployment
Figure 1: Time series of ESI and unemployment
together with BEN predictions (smoothed using a
3-point moving average).
alisation, where the font size is proportional to the
derived weights by applying BEN, flipped terms de-
note negative weights and colours are determined
by the frequency of use in the corpus (Fig. 2). Word
clouds depict the top-60 positively and negatively
weighted n-grams (120 in total) together with the
top-30 outlets; bigrams are separated by ? ?.
5 Discussion and Future Work
Our visualisations (Fig. 2) present various inter-
esting insights into the news and socioeconomic
features being explored, serving as a demonstra-
tion of the potential power of the proposed mod-
elling. Firstly, we notice that in the word cloud,
the size of a feature (BEN?s weight) is not tightly
connected with its colour (frequency in the corpus).
Also, the word clouds suggest that mostly different
terms and outlets are selected for the two indicators.
For example, ?sky.it? is predominant for ESI but
not for unemployment, while the opposite is true
for ?hedgefundsreview.com?. Some of the words
selected for ESI reflect economical issues, such as
?stimulus? and ?spending?, whereas key politicians
15
(a) ESI
(b) Unemployment
Frequency
Word
Outlet
Weight
a
a
Polarity
Yes
Yes
+
-
Figure 2: Word clouds for words and outlets visualising the outputs of BEN.
like ?david cameron? and ?berlusconi?, are major
participants in the word cloud for unemployment.
In addition, the visualisations show a strong neg-
ative relationship between unemployment and the
terms ?food?, ?russia? and ?agriculture?, but no such
relationship with respect to ESI. The disparity of
these selections is evidence for our framework?s
capability to highlight features of lesser or greater
importance to a given socioeconomic time series.
The exact interpretation of the selected words and
outlets is, perhaps, context-dependent and beyond
the scope of this work.
In this paper, we presented a framework for per-
forming a supervised analysis on news. An impor-
tant factor for this process is that the bilinear nature
of the learning function allows for a joint selection
of important words and news outlets. Prediction
performance is used as a reference point for de-
termining whether the extracted outputs (i.e. the
model?s parameters) encapsulate relevant informa-
tion regarding to the given indicator. Experiments
were conducted on a set of EU-related news sum-
maries and the supervising socioeconomic factors
were the EU-wide ESI and unemployment. BEN
outperformed the linear alternative (LEN), produc-
ing error rates below 8.8%.
The performance of our framework motivates
several extensions to be explored in future work.
Firstly, the incorporation of additional textual fea-
tures may improve predictive capability and allow
for richer interpretations of the term weights. For
example, we could extend our term vocabulary us-
ing n-grams with n > 2, POS tags of words and
entities (people, companies, places, etc.). Further-
more, multi-task learning approaches as well as
models which incorporate the regularised learning
of weights for different countries might give us fur-
ther insights into the relationship between news,
geographic location and socioeconomic indicators.
Most importantly, we plan to gain a better under-
standing of the outputs by conducting a thorough
analysis in collaboration with domain experts.
16
Acknowledgements
VL acknowledges the support from the EPSRC
IRC project EP/K031953/1. DPP, SS, DG and TC
were supported by EU-FP7-ICT project n.287863
(?TrendMiner?).
References
R. Alexander Bentley, Alberto Acerbi, Paul Ormerod,
and Vasileios Lampos. 2014. Books average previ-
ous decade of economic misery. PLoS ONE, 9(1).
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. O?Reilly
Media.
Johan Bollen and Huina Mao. 2011. Twitter mood as a
stock market predictor. IEEE Computer, 44(10):91?
94.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Social media as a measurement tool
of depression in populations. In Proceedings of ACM
WebSci?13, pages 47?56.
European Commision. 1997. The joint harmonised EU
programme of business and consumer surveys. Euro-
pean economy: Reports and studies.
Ilias Flaounas, Marco Turchi, Omar Ali, Nick Fyson,
Tijl De Bie, Nick Mosdell, Justin Lewis, and Nello
Cristianini. 2010. The Structure of the EU Medias-
phere. PLoS ONE, 5(12), 12.
Sarah Gelper and Christophe Croux. 2010. On the con-
struction of the European Economic Sentiment Indi-
cator. Oxford Bulletin of Economics and Statistics,
72(1):47?62.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: biased estimation for nonorthogonal prob-
lems. Technometrics, 12:55?67.
Vasileios Lampos and Nello Cristianini. 2012. Now-
casting events from the Social Web with statistical
learning. ACM TIST, 3(4):72:1?72:22.
Vasileios Lampos, Daniel Preot?iuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from Social Media. In Proceedings of ACL?13,
pages 993?1003.
Carolyn Lin, Michael B. Salwen, Bruce Garrison, and
Paul D. Driscoll. 2005. Online news as a functional
substitute for offline news. Online news and the pub-
lic, pages 237?255.
Seungahn Nah and Deborah S. Chung. 2012. When cit-
izens meet both professional and citizen journalists:
Social trust, media credibility, and perceived journal-
istic roles among online community news readers.
Journalism, 13(6):714?730.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: linking text sentiment to
public opinion time series. In Proceedings of AAAI
ICWSM?10, pages 122?129.
Michael J. Paul and Mark Dredze. 2011. You
Are What You Tweet: Analyzing Twitter for Public
Health. In Proceedings of AAAI ICWSM?11, pages
265?272.
Robert P. Schumaker and Hsinchun Chen. 2009. Tex-
tual analysis of stock market prediction using break-
ing financial news: the AZFin text system. ACM
TOIS, 27(2):12:1?12:19.
H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M. Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin E. P. Seligman, and
Lyle H. Ungar. 2013. Personality, Gender, and
Age in the Language of Social Media: The Open-
Vocabulary Approach. PLoS ONE, 8(9).
Robert Tibshirani. 1994. Regression shrinkage and
selection via the lasso. JRSS: Series B, 58:267?288.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. JRSS: Series B,
67(2):301?320.
17
