Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 717?727, Prague, June 2007. c?2007 Association for Computational Linguistics
Effective Information Extraction with Semantic Affinity Patterns and
Relevant Regions
Siddharth Patwardhan and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{sidd,riloff}@cs.utah.edu
Abstract
We present an information extraction system
that decouples the tasks of finding relevant
regions of text and applying extraction pat-
terns. We create a self-trained relevant sen-
tence classifier to identify relevant regions,
and use a semantic affinity measure to au-
tomatically learn domain-relevant extraction
patterns. We then distinguish primary pat-
terns from secondary patterns and apply the
patterns selectively in the relevant regions.
The resulting IE system achieves good per-
formance on the MUC-4 terrorism corpus
and ProMed disease outbreak stories. This
approach requires only a few seed extraction
patterns and a collection of relevant and ir-
relevant documents for training.
1 Introduction
Many information extraction (IE) systems rely on
rules or patterns to extract words and phrases based
on their surrounding context (e.g., (Soderland et al,
1995; Riloff, 1996; Califf and Mooney, 1999; Yan-
garber et al, 2000)). For example, a pattern like
?<subject> was assassinated? can reliably identify
a victim of a murder event. Classification-based IE
systems (e.g., (Freitag, 1998; Freitag and McCal-
lum, 2000; Chieu et al, 2003)) also generally de-
cide whether to extract words based on properties of
the words themselves as well as properties associ-
ated with their surrounding context.
In this research, we propose an alternative ap-
proach to IE that decouples the tasks of finding a rel-
evant region of text and finding a desired extraction.
In a typical pattern-based IE system, the extraction
patterns perform two tasks: (a) they recognize that
a relevant incident has occurred, and (b) they iden-
tify and extract some information about that event.
In contrast, our approach first identifies relevant re-
gions of a document that describes relevant events,
and then applies extraction patterns only in these rel-
evant regions.
This decoupled approach to IE has several po-
tential advantages. First, even seemingly good pat-
terns can produce false hits due to metaphor and id-
iomatic expressions. However, by restricting their
use to relevant regions of text, we could avoid such
false positives. For example, ?John Kerry attacked
George Bush? is a metaphorical description of a ver-
bal tirade, but could be easily mistaken for a physi-
cal attack. Second, IE systems are prone to errors of
omission when relevant information is not explicitly
linked to an event. For instance, a phrase like ?the
gun was found...? does not directly state that the the
gun was used in a terrorist attack. But if the gun is
mentioned in a region that clearly describes a terror-
ist attack, then it can be reasonably inferred to have
been used in the attack. Third, if the IE patterns are
restricted to areas of text that are known to be rel-
evant, then it may suffice to use relatively general
patterns, which may be easier to learn or acquire.
Our approach begins with a relevant sentence
classifier that is trained using only a few seed pat-
terns and a set of relevant and irrelevant documents
(but no sentence-level annotations) for the domain of
interest. The classifier is then responsible for identi-
fying sentences that are relevant to the IE task. Next,
we learn ?semantically appropriate? extraction pat-
717
terns by evaluating candidate patterns using a se-
mantic affinity metric. We then separate the pat-
terns into primary and secondary patterns, and ap-
ply them selectively to sentences based on the rel-
evance judgments produced by the classifier. We
evaluate our IE system on two data sets: the MUC-
4 IE terrorism corpus and ProMed disease outbreak
articles. Our results show that this approach works
well, often outperforming the AutoSlog-TS IE sys-
tem which benefits from human review.
2 Motivation and Related Work
Our research focuses on event-oriented information
extraction (IE), where the goal of the IE system
is to extract facts associated with domain-specific
events from unstructured text. Many different ap-
proaches to information extraction have been devel-
oped, but generally speaking they fall into two cate-
gories: classifier-based approaches and rule/pattern-
based approaches.
Classifier-based IE systems use machine learning
techniques to train a classifier that sequentially pro-
cesses a document looking for words to be extracted.
Examples of classifier-based IE systems are SRV
(Freitag, 1998), HMM approaches (Freitag and Mc-
Callum, 2000), ALICE (Chieu et al, 2003), and Re-
lational Markov Networks (Bunescu and Mooney,
2004). The classifier typically decides whether a
word should be extracted by considering features as-
sociated with that word as well as features of the
words around it.
Another common approach to information ex-
traction uses a set of explicit patterns or rules
to find relevant information. Some older sys-
tems relied on hand-crafted patterns, while more
recent systems learn them automatically or semi-
automatically. Examples of rule/pattern-based ap-
proaches to information extraction are FASTUS
(Hobbs et al, 1997), PALKA (Kim and Moldovan,
1993), LIEP (Huffman, 1996), CRYSTAL (Soder-
land et al, 1995), AutoSlog/AutoSlog-TS (Riloff,
1993; Riloff, 1996), RAPIER (Califf and Mooney,
1999), WHISK (Soderland, 1999), ExDisco (Yan-
garber et al, 2000), SNOWBALL (Agichtein and
Gravano, 2000), (LP)2 (Ciravegna, 2001), subtree
patterns (Sudo et al, 2003), predicate-argument
rules (Yakushiji et al, 2006) and KnowItAll
(Popescu et al, 2004).
One commonality behind all of these approaches
is that they simultaneously decide whether a context
is relevant and whether a word or phrase is a desir-
able extraction. Classifier-based systems rely on fea-
tures that consider both the word and its surround-
ing context, and rule/pattern-based systems typi-
cally use patterns or rules that match both the words
around a candidate extraction and (sometimes) prop-
erties of the candidate extraction itself.
There is a simplicity and elegance to having a sin-
gle model that handles both of these problems at the
same time, but we hypothesized that there may be
benefits to decoupling these tasks. We investigate an
alternative approach that involves two passes over a
document. In the first pass, we apply a relevant re-
gion identifier to identify regions of the text that ap-
pear to be especially relevant to the domain of inter-
est. In the second pass, we apply extraction patterns
inside the relevant regions. We hypothesize three
possible benefits of this decoupled approach.
First, if a system is certain that a region is rele-
vant, then it can be more aggressive about searching
for extractions. For example, consider the domain
of terrorist event reports, where a goal is to identify
the weapons that were used. Existing systems gen-
erally require rules/patterns to recognize a context
in which a weapon is explicitly linked to an event
or its consequences (e.g., ?attack with <np>?, or
?<np> caused damage?). However, weapons are
not always directly linked to an event in text, but
they may be inferred through context. For instance,
an article may mention that a weapon was ?found?
or ?used? without explicitly stating that it was in-
volved in a terrorist event. However, if we know in
advance that we are in a relevant context, then we
can reliably infer that the weapon was, most likely,
used in the event.
Second, some patterns may seem to be relevant
locally, but they can be deemed irrelevant when the
global context is considered. For example, consider
these sentences from the MUC-4 terrorism corpus:
D?Aubuisson unleashed harsh attacks on
Duarte ...
Other brave minds that advocated reform
had been killed before in that struggle.
Locally, patterns such as ?<subject> unleashed
718
attacks? and ?<subject> had been killed? seem
likely to identify the perpetrators and victims of a
physical attack. But when read in the full context
of these sentences, it becomes clear that they are not
related to a specific physical attack.
Third, decoupling these tasks may simplify the
learning process. Identifying relevant regions
amounts to a text classification task, albeit the goal is
to identify not just relevant documents, but relevant
sub-regions of documents. Within a relevant region
the patterns may not need to be as discriminating.
So a more general learning approach may suffice.
In this paper, we describe an IE system that con-
sists of two decoupled modules for relevant sentence
identification and extraction pattern learning. In
Section 3, we describe the self-trained sentence clas-
sifier, which requires only a few seed patterns and
relevant and irrelevant documents for training. Sec-
tion 4 describes the extraction pattern learning mod-
ule, which identifies semantically appropriate pat-
terns for the IE system using a semantic affinity mea-
sure. Section 5 explains how we distinguish Primary
patterns from Secondary patterns. Section 6 presents
experimental results on two domains. Finally, Sec-
tion 7 lists our conclusions and future work.
3 A Self-Trained Relevant Sentence
Classifier
Our hypothesis is that if a system can reliably iden-
tify relevant regions of text, then extracting informa-
tion only from these relevant regions can improve IE
performance. There are many possible definitions
for relevant region (e.g., Salton et al (1993), Callan
(1994)), and exploring the range of possibilities is
an interesting avenue for future work. For our ini-
tial investigations of this idea, we begin by simply
defining a sentence as our region size. This has the
advantage of being an easy boundary line to draw
(i.e., it is relatively easy to identify sentence bound-
aries) and it is a small region size yet includes more
context than most current IE systems do1.
Our goal is to create a classifier that can determine
whether a sentence contains information that should
be extracted. Furthermore, we wanted to create a
classifier that does not depend on manually anno-
1Most IE systems only consider a context window consisting
of a few words or phrases on either side of a potential extraction.
tated sentence data so that our system can be eas-
ily ported across domains. Therefore, we devised a
method to self-train a classifier using a training set
of relevant and irrelevant documents for the domain,
and a few seed patterns as input. However, this re-
sults in an asymmetry in the training set. By defini-
tion, if a document is irrelevant to the IE task, then
it cannot contain any relevant information. Con-
sequently, all sentences in an irrelevant document
must be irrelevant, so these sentences form our ini-
tial irrelevant sentences pool. In contrast, if a doc-
ument is relevant to the IE task, then there must be
at least one sentence that contains relevant informa-
tion. However, most documents contain a mix of
both relevant and irrelevant sentences. Therefore,
the sentences from the relevant documents form our
unlabeled sentences pool.
Figure 1 shows the self-training procedure, which
begins with a handful of seed patterns to initiate the
learning process. The seed patterns should be able
to reliably identify some information that is relevant
to the IE task. For instance, to build an IE system for
terrorist incident reports, we used seed patterns such
as ?<subject> was kidnapped? and ?assassination
of <np>?. The patterns serve as a simple pattern-
based classifier to automatically identify some rel-
evant sentences. In iteration 0 of the self-training
loop (shown as dotted lines in Figure 1), the pattern-
based classifier is applied to the unlabeled sentences
to automatically label some of them as relevant.
Next, an SVM (Vapnik, 1995) classifier2 is
trained using these relevant sentences and an equal
number of irrelevant sentences randomly drawn
from the irrelevant sentences pool. We artificially
created a balanced training set because the set of ir-
relevant sentences is initially much larger than the
set of relevant sentences, and we want the classi-
fier to learn how to identify new relevant sentences.
The feature set consists of all unigrams that appear
in the training set. The SVM is trained using a lin-
ear kernel with the default parameter settings. In a
self-training loop, the classifier is then applied to the
unlabeled sentences, and all sentences that it classi-
fies as relevant are added to the relevant sentences
pool. The classifier is then retrained with all of the
2We used the freely available SVMlight (Joachims, 1998)
implementation: http://svmlight.joachims.org
719
pattern?based
classifier
relevant
sentences
seed
patterns
SVM
classifier
unlabeled
sentences
SVM
training
irrelevant
sentences
irrelevant 
documents
Figure 1: The Training Process to Create a Relevant Sentence Classifier
relevant sentences and an equal number of irrelevant
sentences, and the process repeats. We ran this self-
training procedure for three iterations and then used
the resulting classifier as our relevant sentence clas-
sifier in the IE experiments described in Section 6.3.
4 Learning Semantic Affinity-based
Extraction Patterns
One motivation for creating a relevant region classi-
fier is to reduce the responsibilities of the extraction
patterns. Once we know that we are in a domain-
relevant area of text, patterns that simply identify
words and phrases belonging to a relevant seman-
tic class may be sufficient. In this section, we de-
scribe a method to automatically identify semanti-
cally appropriate extraction patterns for use with the
sentence classifier.
In previous work (Patwardhan and Riloff, 2006),
we introduced a metric called semantic affinity
which was used to automatically assign event roles
to extraction patterns. Semantic affinity measures
the tendency of a pattern to extract noun phrases
that belong to a specific set of semantic categories.
To use this metric for information extraction, a
mapping must be defined between semantic cate-
gories and the event roles that are relevant to the
IE task. For example, one role in the terrorism do-
main is physical target, which refers to physical ob-
jects that are the target of an attack. Most phys-
ical targets fall into one of two general semantic
categories: BUILDING or VEHICLE. Consequently,
we define the mapping ?Target ? BUILDING, VE-
HICLE?. Similarly, we might define the mapping
?Victim ? HUMAN, ANIMAL, PLANT? to charac-
terize possible victims of disease outbreaks. Each
semantic category must be mapped to a single event
role. This is a limitation of our approach for do-
mains where multiple roles can be filled by the same
class of fillers. However, sometimes a general se-
mantic class can be partitioned into subclasses that
are associated with different roles. For example, in
the terrorism domain, both perpetrators and victims
belong to the general semantic class HUMAN. But
we used the subclasses TERRORIST-HUMAN, which
represents likely perpetrator words (e.g., ?terrorist?,
?guerrilla?, and ?gunman?) and CIVILIAN-HUMAN,
which represents ordinary people (e.g., ?photogra-
pher?,?rancher?, and ?tourist?), in order to generate
different semantic affinity estimates for the perpetra-
tor and victim roles.
To determine the semantic category of a noun, we
use the Sundance parser (Riloff and Phillips, 2004),
which contains a dictionary of words that have se-
mantic category labels. Alternatively, a resource
such as WordNet (Fellbaum, 1998) could be used
to obtain this information. All semantic categories
that cannot be mapped to a relevant event role are
mapped to a special Other role.
To estimate the semantic affinity of a pattern p
for an event role rk, the system computes f(p, rk),
which is the number of pattern p?s extractions that
have a head noun belonging to a semantic category
mapped to rk. These frequency counts are obtained
by applying each pattern to the training corpus and
collecting its extractions. The semantic affinity of a
pattern p with respect to an event role rk is formally
defined as:
sem aff(p, rk) =
f(p, rk)
?|R|
i=1 f(p, ri)
log2 f(p, rk) (1)
where R is the set of event roles {r1, r2, . . . , r|R|}.
Semantic affinity is essentially the probability that
a phrase extracted by pattern p will be a semanti-
cally appropriate filler for role rk, weighted by the
log of the frequency.3 Note that it is possible for a
3This formula is very similar to pattern ranking metrics used
by previous IE systems (Riloff, 1996; Yangarber et al, 2000),
although not for semantics.
720
pattern to have a semantic affinity for multiple event
roles. For instance, a terrorism pattern like ?attack
on <np>? may have a semantic affinity for both
Targets and Victims.
To generate extraction patterns for an IE task, we
first apply the AutoSlog (Riloff, 1993) extraction
pattern generator to the training corpus exhaustively,
so that it literally generates a pattern to extract every
noun phrase in the corpus. Then for each event role,
we rank the patterns based on their semantic affinity
for that role.
Figure 2 shows the 10 patterns with the highest se-
mantic affinity scores for 4 event roles. In the terror-
ism domain, we show patterns that extract weapons
and perpetrator organizations (PerpOrg). In the dis-
ease outbreaks domain, we show patterns that ex-
tract diseases and victims. The patterns rely on shal-
low parsing, syntactic role assignment (e.g., subject
(subject) and direct object (dobj) identification), and
active/passive voice recognition, but they are shown
here in a simplified form for readability. The por-
tion in brackets (between < and >) is extracted, and
the other words must match the surrounding con-
text. In some cases, all of the matched words are
extracted (e.g., ?<# birds>?). Most of the highest-
ranked victim patterns recognize noun phrases that
refer to people or animals because they are common
in the disease outbreak stories and these patterns do
not extract information that is associated with any
competing event roles.
5 Distinguishing Primary and Secondary
Patterns
So far, our goal has been to find relevant areas
of text, and then apply semantically appropriate
patterns in those regions. Our expectation was
that fairly general, semantically appropriate patterns
could be effective if their range is restricted to re-
gions that are known to be relevant. If our relevant
sentence classifier was perfect, then performing IE
only on relevant regions would be ideal. However,
identifying relevant regions is a difficult problem in
its own right, and our relevant sentence classifier is
far from perfect.
Consequently, one limitation of our proposed ap-
proach is that no IE would be performed in sentences
that are not deemed to be relevant by the classifier,
Top Terrorism Patterns
Weapon PerpOrg
<subject> exploded <subject> claimed
planted <dobj> panama from <np>
fired <dobj> <np> claimed responsibility
<subject> was planted command of <np>
explosion of <np> wing of <np>
<subject> was detonated kidnapped by <np>
<subject> was set off guerillas of <np>
set off <dobj> <subject> operating
hurled <dobj> kingpins of <np>
<subject> was placed attacks by <np>
Top Disease Outbreak Patterns
Disease Victim
cases of <np> <# people>
spread of <np> <# cases>
outbreak of <np> <# birds>
<#th outbreak> <# animals>
<# outbreaks> <subject> died
case of <np> <# crows>
contracted <dobj> <subject> know
outbreaks of <np> <# pigs>
<# viruses> <# cattle>
spread of <np> <# sheep>
Figure 2: Top-Ranked Extraction Patterns
and this could negatively affect recall. We addressed
this issue by allowing reliable patterns to be applied
to all sentences in the text, irrespective of the output
of the sentence classifier. For example, the pattern
?<subject> was assassinated? is a clear indicator
of a murder event, and does not need to be restricted
by the sentence classifier4. We will refer to such
reliable patterns as Primary Patterns. In contrast,
patterns that are not necessarily reliable and need to
be restricted to relevant regions will be called Sec-
ondary Patterns.
To automatically distinguish Primary Patterns
from Secondary Patterns, we compute the condi-
tional probability of a pattern p being relevant,
Pr(relevant | p), based on the relevant and irrele-
vant documents in our training set. We then define
an upper conditional probability threshold ?u to sep-
arate Primary patterns from Secondary Patterns. If
a pattern has a high correlation with relevant docu-
ments, then our assumption is that it is generally a
reliable pattern that is not likely to occur in irrele-
vant contexts.
On the flip side, we can also use this condi-
tional probability to weed out patterns that rarely
4In other words, if such a pattern matches a sentence that is
classified as irrelevant, then the classifier is probably incorrect.
721
appear in relevant documents. Such patterns (e.g.,
?<subject> held?, ?<subject> saw?, etc.) could
potentially have a high semantic affinity for one of
the semantic categories, but they are not likely to be
useful if they mainly occur in irrelevant documents.
As a result, we also define a lower conditional proba-
bility threshold ?l that identifies irrelevant extraction
patterns.
The two thresholds ?u and ?l are used with seman-
tic affinity to identify the most appropriate Primary
and Secondary patterns for the task. This is done by
first removing from our extraction pattern collection
all patterns with probability less than ?l. For each
event role, we then sort the remaining patterns based
on their semantic affinity score for that role and se-
lect the top N patterns. Next, we use the ?u prob-
ability threshold to separate these N patterns into
two subsets. Patterns with a probability above ?u
are considered to be Primary patterns for that role,
and those below become the Secondary patterns.
6 Experiments and Results
6.1 Data Sets
We evaluated the performance of our IE system on
two data sets: the MUC-4 terrorism corpus (Sund-
heim, 1992), and a ProMed disease outbreaks cor-
pus. The MUC-4 IE task is to extract information
about Latin American terrorist events. We focused
our analysis on five MUC-4 string roles: perpetrator
individuals, perpetrator organizations, physical tar-
gets, victims, and weapons. The disease outbreaks
corpus consists of electronic reports about disease
outbreak events. For this domain we focused on two
string roles: diseases and victims5.
The MUC-4 data set consists of 1700 documents,
divided into 1300 development (DEV) texts, and
four test sets of 100 texts each (TST1, TST2, TST3,
and TST4). We used 1300 texts (DEV) as our train-
ing set, 200 texts (TST1+TST2) for tuning, and 200
texts (TST3+TST4) as a test set. All 1700 docu-
ments have answer key templates. For the training
set, we used the answer keys to separate the doc-
uments into relevant and irrelevant subsets. Any
document containing at least one relevant event was
considered relevant.
5The ?victims? can be people, animals, or plants that are
affected by a disease.
For the disease outbreak domain the data set
was collected from ProMed-mail6, an open-source,
global electronic reporting system for outbreaks
of infectious diseases. We collected thousands of
ProMed reports and created answer key templates
for 245 randomly selected articles. We used 125 as
a tuning set, and 120 as the test set. We used 2000
different documents as the relevant documents for
training. Most of the ProMed articles contain email
headers, footers, citations, and other snippets of non-
narrative text, so we wrote a ?zoner? program7 to
automatically strip off some of this extraneous in-
formation.
To obtain irrelevant documents, we collected
4000 biomedical abstracts from PubMed8, a free
archive of biomedical literature. We collected twice
as many irrelevant documents because the PubMed
articles are roughly half the size of the ProMed arti-
cles, on average. To ensure that the PubMed articles
were truly irrelevant (i.e. did not contain any disease
outbreak reports) we used specific queries to exclude
disease outbreak abstracts.
The complete IE task involves the creation of
answer key templates, one template per incident9.
Template generation is a complex process, requir-
ing coreference resolution and discourse analysis to
determine how many incidents were reported and
which facts belong with each incident. Our work fo-
cuses on extraction pattern learning and not template
generation, so we evaluated our systems directly on
the extractions themselves, before template genera-
tion would take place. This approach directly mea-
sures how accurately the patterns find relevant infor-
mation, without confounding factors from the tem-
plate generation process. For example, if a coref-
erence resolver incorrectly decides that two extrac-
tions are coreferent and merges them, then only one
extraction would be scored. We used a head noun
scoring scheme, where an extraction is considered
to be correct if its head noun matches the head noun
in the answer key10. Also, pronouns were discarded
from both the system responses and the answer keys
since no coreference resolution is done. Duplicate
6http://www.promedmail.org
7The term zoner was introduced by Yangarber et al (2002).
8http://www.pubmedcentral.nih.gov
9Many of the stories have multiple incidents per article.
10For example, ?armed men? will match ?5 armed men?.
722
extractions (e.g., the same string extracted by differ-
ent patterns) were conflated before being scored, so
they count as just one hit or one miss.
6.2 Relevant Sentence Classifier Results
First, we evaluated the performance of the relevant
sentence classifier described in Section 3. We auto-
matically generated seed patterns from the training
texts. AutoSlog (Riloff, 1993) was used to gener-
ate all extraction patterns that appear in the train-
ing documents, and only those patterns with fre-
quency > 50 were kept. These were then ranked
by Pr(relevant | p), and the top 20 patterns were
chosen as seeds. In the disease outbreak domain, 54
patterns had a frequency > 50 and probability of 1.0.
We wanted to use the same number of seeds in both
domains for consistency, so we manually reviewed
them and used the 20 most domain-specific patterns
as seeds.
Due to the greater stylistic differences between
the relevant and irrelevant documents in the disease
outbreak domain (since they were gathered from dif-
ferent sources), we decided to make the classifier for
that domain more conservative in classifying docu-
ments as relevant. To do this we used the prediction
scores output by the SVM as a measure of confi-
dence in the classification. These scores are essen-
tially the distance of the test examples from the sup-
port vectors of the SVM. For the disease outbreaks
domain we used a cutoff of 1.0 and in the terrorism
domain we used the default of 0.
Since we do not have sentence annotated data,
there is no direct way to evaluate the classifiers.
However, we did an indirect evaluation by using the
answer keys from the tuning set. If a sentence in
a tuning document contained a string that occurred
in the corresponding answer key template, then we
considered that sentence to be relevant. Otherwise,
the sentence was deemed irrelevant. This evaluation
is not perfect for two reasons: (1) answer key strings
do not always appear in relevant sentences.11, and
(2) some arguably relevant sentences may not con-
tain an answer key string (e.g., they may contain a
pronoun that refers to the answer, but the pronoun it-
self is not the desired extraction). However, judging
11This happens due to coreference, e.g., when multiple oc-
currences of an answer appear in a document, some of them
may occur in relevant sentences while others do not.
Irrelevant Relevant
Acc Rec Pr F Rec Pr F
Terrorism
Iter #1 .84 .93 .89 .91 .41 .55 .47
Iter #2 .84 .90 .91 .90 .54 .51 .53
Iter #3 .82 .85 .92 .89 .63 .46 .53
Disease Outbreaks
Iter #1 .75 .96 .76 .85 .21 .66 .32
Iter #2 .71 .76 .82 .79 .58 .48 .53
Iter #3 .63 .60 .85 .70 .72 .41 .52
Table 1: Relevant Sentence Classifier Evaluation
the relevance of sentences without relying on answer
keys is also tricky, so we decided that this approach
was probably good enough to get a reasonable as-
sessment of the classifier. Using this criterion, 17%
of the sentences in the terrorism articles are relevant,
and 28% of the sentences in the disease outbreaks
articles are relevant.
Table 1 shows the accuracy, recall, precision, and
F scores of the SVM classifiers after each self-
training iteration. The classifiers generated after the
third iteration were used in our IE experiments. The
final accuracy is 82% in the terrorism domain, and
63% for the disease outbreaks domain. The preci-
sion on irrelevant sentences is high in both domains,
but the precision on relevant sentences is relatively
weak. Despite this, we will show in Section 6.3 that
the classifier is effective for the IE task. The rea-
son why the classifier improves IE performance is
because it favorably alters the proportion of relevant
sentences that are passed along to the IE system. For
example, an analysis of the tuning set shows that re-
moving the sentences deemed to be irrelevant by the
classifier increases the proportion of relevant sen-
tences from 17% to 46% in the terrorism domain,
and from 28% to 41% in the disease outbreaks do-
main.
We will also see in Section 6.3 that IE recall only
drops a little when the sentence classifier is used,
despite the fact that its recall on relevant sentences
is only 63% in terrorism and 72% for disease out-
breaks. One possible explanation is that the an-
swer keys often contain multiple acceptable answer
strings (e.g., ?John Kennedy? and ?JFK? might both
be acceptable answers). On average, the answer
keys contain approximately 1.64 acceptable strings
per answer in the terrorism domain, and 1.77 accept-
able strings per answer in the disease outbreaks do-
723
Terrorism
Patterns App Rec Pr F Rec Pr F
PerpInd PerpOrg
ASlogTS All .49 .35 .41 .33 .49 .40
ASlogTS Rel .41 .50 .45 .27 .58 .37
Target Victim
ASlogTS All .64 .42 .51 .52 .48 .50
ASlogTS Rel .57 .49 .53 .48 .54 .51
Weapon
ASlogTS All .45 .39 .42
ASlogTS Rel .40 .51 .45
Disease Outbreaks
Disease Victim
ASlogTS All .51 .27 .36 .48 .35 .41
ASlogTS Rel .46 .31 .37 .44 .38 .41
Table 2: AutoSlog-TS Results
main. Thus, even if the sentence classifier discards
some relevant sentences, an equally acceptable an-
swer may be found in a different sentence.
6.3 Information Extraction Results
We first conducted two experiments with an exist-
ing IE pattern learner, AutoSlog-TS (Riloff, 1996)
to give us a baseline against which to compare our
results. The ?All? rows in Table 2 show these results,
where ?All? means that the IE patterns were applied
to all of the sentences in the test set. AutoSlog-TS12
produced F scores between 40-51% on the MUC-4
test set, and 36-41% on the ProMed test set. The
terrorism scores are competitive with the MUC-4
scores reported by Chieu et al (2003), although they
are not directly comparable because those scores are
based on template generation. Since we created the
ProMed test set ourselves, we are the first to report
results on it13.
Next, we evaluated the performance of AutoSlog-
TS? extraction patterns when they are applied only in
the sentences deemed to be relevant by our relevant
sentence classifier. The purpose of this experiment
was to determine whether the relevant sentence clas-
sifier can be beneficial when used with IE patterns
known to be of good quality. The ?Rel? rows in Ta-
12AutoSlog-TS was trained on a much larger data set of 4,958
ProMed and 10,191 PubMed documents for the disease out-
breaks domain. AutoSlog-TS requires a human review of the
top-ranked patterns, which resulted in 396 patterns for the ter-
rorism domain and 125 patterns for the disease outbreaks do-
main.
13Some previous work has been done with ProMed articles
(Grishman et al, 2002a; Grishman et al, 2002b), but we are not
aware of any IE evaluations on them.
Disease Victim
Patterns App Rec Pr F Rec Pr F
ASlogTS All .51 .27 .36 .48 .35 .41
SA-50 All .51 .25 .34 .47 .41 .44
SA-50 Rel .49 .31 .38 .44 .43 .43
SA-50 Sel .50 .29 .36 .46 .41 .44
SA-100 All .57 .22 .32 .52 .33 .40
SA-100 Rel .55 .28 .37 .49 .36 .41
SA-100 Sel .56 .26 .35 .51 .34 .41
SA-150 All .66 .20 .31 .55 .27 .37
SA-150 Rel .61 .26 .36 .51 .31 .38
SA-150 Sel .63 .24 .35 .53 .29 .37
SA-200 All .68 .19 .30 .56 .26 .36
SA-200 Rel .63 .25 .35 .52 .30 .38
SA-200 Sel .65 .23 .34 .54 .28 .37
Table 3: ProMed Disease Outbreak Results
ble 2 show the scores for this experiment. Precision
increased substantially on all 7 roles, although with
some recall loss. This shows that a sentence classi-
fier that has a high precision on irrelevant sentences
but only a moderate precision on relevant sentences
can be useful for information extraction.
Tables 3 and 4 show the results of our IE system,
which uses the top N Semantic Affinity (SA) pat-
terns and the relevant sentence classifier. We also
show the AutoSlog-TS results again in the top row
for comparison. The best F score for each role is
shown in boldface. We used a lower probability
threshold ?l of 0.5 to filter out irrelevant patterns.
We then ranked the remaining patterns based on se-
mantic affinity, and evaluated the performance of the
top 50, 100, 150, and 200 patterns. The App column
indicates how the patterns were applied: for All they
were applied in all sentences in the test set, for Rel
they were applied only in the relevant sentences (as
judged by our sentence classifier). For the Sel con-
dition, the Primary patterns were applied in all sen-
tences but the Secondary patterns were applied only
in relevant sentences. To separate Primary and Sec-
ondary patterns we used an upper probability thresh-
old ?u of 0.8.
Looking at the rows with the All condition, we
see that the semantic affinity patterns achieve good
recall (e.g., the top 200 patterns have a recall over
50% for most roles), but precision is often quite low.
This is not surprising because high semantic affin-
ity patterns do not necessarily have to be relevant to
the domain, so long as they recognize semantically
appropriate things.
724
PerpInd PerpOrg Target Victim Weapon
Patterns App Rec Pr F Rec Pr F Rec Pr F Rec Pr F Rec Pr F
ASlogTS All .49 .35 .41 .33 .49 .40 .64 .42 .51 .52 .48 .50 .45 .39 .42
SA-50 All .24 .29 .26 .20 .42 .27 .42 .43 .42 .41 .43 .42 .53 .46 .50
SA-50 Rel .19 .32 .24 .18 .60 .28 .38 .48 .42 .37 .52 .43 .41 .56 .48
SA-50 Sel .20 .33 .25 .20 .54 .29 .42 .50 .45 .38 .52 .44 .43 .53 .48
SA-100 All .40 .30 .34 .30 .43 .35 .56 .38 .45 .45 .37 .41 .55 .43 .48
SA-100 Rel .36 .39 .38 .25 .59 .35 .52 .45 .48 .40 .47 .44 .45 .51 .48
SA-100 Sel .38 .40 .39 .27 .55 .36 .56 .46 .50 .41 .47 .44 .47 .49 .48
SA-150 All .50 .27 .35 .34 .39 .37 .62 .30 .40 .50 .33 .40 .55 .39 .45
SA-150 Rel .46 .39 .42 .28 .58 .38 .56 .37 .45 .44 .45 .45 .45 .50 .47
SA-150 Sel .48 .39 .43 .31 .55 .40 .60 .37 .46 .46 .44 .45 .47 .47 .47
SA-200 All .73 .08 .15 .42 .43 .42 .64 .29 .40 .54 .32 .40 .64 .17 .27
SA-200 Rel .67 .15 .24 .34 .61 .43 .58 .36 .45 .47 .43 .45 .52 .29 .37
SA-200 Sel .71 .12 .21 .36 .58 .45 .61 .35 .45 .48 .43 .45 .53 .22 .31
Table 4: MUC-4 Terrorism Results
Next, we can compare each All row with the Rel
row immediately below it. We observe that in every
case precision improves, often dramatically. This
demonstrates that our sentence classifier is having
the desired effect. However, observe that the preci-
sion gain comes with some loss in recall points.
Clearly, this drop in recall is due to the answers
embedded inside relevant sentences incorrectly clas-
sified as irrelevant. To counter this, we apply the Pri-
mary patterns to all the sentences. Thus, if we com-
pare each Rel row with the Sel row immediately be-
low it, we see the effect of loosening the reins on the
Primary patterns (the Secondary patterns are still re-
stricted to the relevant sentences). In most cases, the
recall improves with a relatively small drop in preci-
sion, or no drop at all. In the terrorism domain, the
highest F score for four of the five roles occurs under
the Sel condition. In the disease outbreaks domain,
the best F score for diseases occurs in the Rel con-
dition, while the best score for victims is achieved
under both the All and the Sel conditions.
Finally, we note that the best F scores produced
by our information extraction system are higher than
those produced by AutoSlog-TS for all of the roles
except Targets and Victims, and our best perfor-
mance on Targets is only slightly lower. These re-
sults are particularly noteworthy because AutoSlog-
TS requires a human to manually review the patterns
and assign event roles to them. In contrast, our ap-
proach is fully automated.
These results validate our hypothesis that decou-
pling the processes of finding relevant regions and
applying semantically appropriate patterns can cre-
ate an effective IE system.
7 Conclusions
In this work, we described an information extraction
system based on a relevant sentence classifier and
extraction patterns learned using a semantic affin-
ity metric. The sentence classifier was self-trained
using only relevant and irrelevant documents plus a
handful of seed extraction patterns. We showed that
separating the task of relevant region identification
from that of pattern extraction can be effective for in-
formation extraction. In addition, we observed that
the use of a relevant sentence classifier is beneficial
for an IE system.
There are several avenues that need to be explored
for future work. First, it would be interesting to see
if the use of richer features can improve classifier
performance, and if that in turn improves the perfor-
mance of the IE system. We would also like to ex-
periment with different region sizes and study their
effect on information extraction. Finally, other tech-
niques for learning semantically appropriate extrac-
tion patterns could be investigated.
Acknowledgments
This research was supported by NSF Grant IIS-
0208985, Department of Homeland Security Grant
N0014-07-1-0152, and the Institute for Scientific
Computing Research and the Center for Applied
Scientific Computing within Lawrence Livermore
National Laboratory. We are grateful to Sean Igo
and Rich Warren for annotating the disease out-
breaks corpus.
725
References
E. Agichtein and L. Gravano. 2000. Snowball: Extract-
ing Relations from Large Plain-Text Collections. In
Proceedings of the Fifth ACM Conference on Digital
Libraries, pages 85?94, San Antonio, TX, June.
R. Bunescu and R. Mooney. 2004. Collective Informa-
tion Extraction with Relational Markov Networks. In
Proceeding of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 438?445,
Barcelona, Spain, July.
M. Califf and R. Mooney. 1999. Relational Learning
of Pattern-matching Rules for Information Extraction.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence, pages 328?334, Orlando, FL,
July.
J. Callan. 1994. Passage-Level Evidence in Document
Retrieval. In Proceedings of the 17th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 302?310,
Dublin, Ireland, July.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
Gap: Learning-Based Information Extraction Rivaling
Knowledge-Engineering Methods. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 216?223, Sapporo, Japan,
July.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of Seventeenth International Joint Con-
ference on Artificial Intelligence, pages 1251?1256,
Seattle, WA, August.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
D. Freitag and A. McCallum. 2000. Information Ex-
traction with HMM Structures Learned by Stochas-
tic Optimization. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence, pages
584?589, Austin, TX, August.
D. Freitag. 1998. Toward General-Purpose Learning
for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, pages 404?408, Mon-
treal, Quebec, August.
R. Grishman, S. Huttunen, and R. Yangarber. 2002a. In-
formation Extraction for Enhanced Access to Disease
Outbreak Reports. Journal of Biomedical Informatics,
35(4):236?246, August.
R. Grishman, S. Huttunen, and R. Yangarber. 2002b.
Real-Time Event Extraction for Infectious Disease
Outbreaks. In Proceedings of the 3rd Annual Human
Language Technology Conference, San Diego, CA,
March.
J. Hobbs, D. Appelt, J. Bear, D. Israel, M. Kameyama,
M. Stickel, and M. Tyson. 1997. FASTUS: A Cas-
caded Finite-state Transducer for Extracting Informa-
tion for Natural-Language Text. In E. Roche and
Y. Schabes, editors, Finite-State Language Processing,
pages 383?406. MIT Press, Cambridge, MA.
S. Huffman. 1996. Learning Information Extraction
Patterns from Examples. In S. Wermter, E. Riloff,
and G. Scheler, editors, Connectionist, Statistical, and
Symbolic Approaches to Learning for Natural Lan-
guage Processing, pages 246?260. Springer, Berlin.
T. Joachims. 1998. Text Categorization with Support
Vector Machines: Learning with Many Relevant Fea-
tures. In Proceedings of the Tenth European Confer-
ence on Machine Learning, pages 137?142, April.
J. Kim and D. Moldovan. 1993. PALKA: A System for
Lexical Knowledge Acquisition. In Proceedings of
the Second International Conference on Information
and Knowledge Management, pages 124?131, Wash-
ington, DC, November.
S. Patwardhan and E. Riloff. 2006. Learning Domain-
Specific Information Extraction Patterns from the
Web. In Proceedings of the ACL 2006 Workshop on
Information Extraction Beyond the Document, pages
66?73, Sydney, Australia, July.
A. Popescu, A. Yates, and O. Etzioni. 2004. Class Ex-
traction from the World Wide Web. In Ion Muslea,
editor, Adaptive Text Extraction and Mining: Papers
from the 2004 AAAI Workshop, pages 68?73, San Jose,
CA, July.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceedings
of the Eleventh National Conference on Artificial In-
telligence, pages 811?816, Washington, DC, July.
E. Riloff. 1996. Automatically Generating Extrac-
tion Patterns from Untagged Text. In Proceedings of
the Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049, Portland, OR, August.
G. Salton, J. Allan, and C. Buckley. 1993. Approaches
to Passage Retrieval in Full Text Information Systems.
In Proceedings of the 16th Annual International ACM
SIGIR Conference on Research and Development on
Information Retrieval, pages 49?58, Pittsburgh, PA,
June.
726
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a Conceptual Dictionary.
In Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence, pages 1314?
1319, Montreal, Canada, August.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233?272, February.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Patterns Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 224?231, Sapporo, Japan,
July.
B. Sundheim. 1992. Overview of the Fourth Message
Understanding Evaluation and Conference. In Pro-
ceedings of the Fourth Message Understanding Con-
ference (MUC-4), pages 3?21, McLean, VA, June.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York, NY.
A. Yakushiji, Y. Miyao, T. Ohta, Y. Tateisi, and J. Tsu-
jii. 2006. Construction of Predicate-argument Struc-
ture Patterns for Biomedical Information Extraction.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 284?
292, Sydney, Australia, July.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics, pages 940?946, Saarbru?cken, Ger-
many, August.
R. Yangarber, W. Lin, and R. Grishman. 2002. Unsu-
pervised Learning of Generalized Names. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics, pages 154?160, Taipei, Taiwan,
August.
727
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 151?160,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Unified Model of Phrasal and Sentential Evidence
for Information Extraction
Siddharth Patwardhan and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{sidd,riloff}@cs.utah.edu
Abstract
Information Extraction (IE) systems that
extract role fillers for events typically look
at the local context surrounding a phrase
when deciding whether to extract it. Of-
ten, however, role fillers occur in clauses
that are not directly linked to an event
word. We present a new model for event
extraction that jointly considers both the
local context around a phrase along with
the wider sentential context in a proba-
bilistic framework. Our approach uses a
sentential event recognizer and a plausible
role-filler recognizer that is conditioned on
event sentences. We evaluate our system
on two IE data sets and show that our
model performs well in comparison to ex-
isting IE systems that rely on local phrasal
context.
1 Introduction
Information Extraction (IE) systems typically use
extraction patterns (e.g., Soderland et al (1995),
Riloff (1996), Yangarber et al (2000), Califf
and Mooney (2003)) or classifiers (e.g., Freitag
(1998), Freitag and McCallum (2000), Chieu et al
(2003), Bunescu and Mooney (2004)) to extract
role fillers for events. Most IE systems consider
only the immediate context surrounding a phrase
when deciding whether to extract it. For tasks such
as named entity recognition, immediate context is
usually sufficient. But for more complex tasks,
such as event extraction, a larger field of view is
often needed to understand how facts tie together.
Most IE systems are designed to identify role
fillers that appear as arguments to event verbs
or nouns, either explicitly via syntactic relations
or implicitly via proximity (e.g., John murdered
Tom or the murder of Tom by John). But many
facts are presented in clauses that do not contain
event words, requiring discourse relations or deep
structural analysis to associate the facts with event
roles. For example, consider the sentences below:
Seven people have died
. . . and 30 were injured in India after terror-
ists launched an attack on the Taj Hotel.
. . . in Mexico City and its surrounding sub-
urbs in a Swine Flu outbreak.
. . . after a tractor-trailer collided with a bus
in Arkansas.
Two bridges were destroyed
. . . in Baghdad last night in a resurgence of
bomb attacks in the capital city.
. . . and $50 million in damage was caused by
a hurricane that hit Miami on Friday.
. . . to make way for modern, safer bridges
that will be constructed early next year.
These examples illustrate a common phenomenon
in text where information is not explicitly stated
as filling an event role, but readers have no trou-
ble making this inference. The role fillers above
(seven people, two bridges) occur as arguments to
verbs that reveal state information (death, destruc-
tion) but are not event-specific (i.e., death and de-
struction can result from a wide variety of incident
types). IE systems often fail to extract these role
fillers because these systems do not recognize the
immediate context as being relevant to the specific
type of event that they are looking for.
We propose a new model for information ex-
traction that incorporates both phrasal and senten-
tial evidence in a unified framework. Our uni-
fied probabilistic model, called GLACIER, consists
of two components: a model for sentential event
recognition and a model for recognizing plausi-
ble role fillers. The Sentential Event Recognizer
offers a probabilistic assessment of whether a sen-
tence is discussing a domain-relevant event. The
151
Plausible Role-Filler Recognizer is then condi-
tioned to identify phrases as role fillers based upon
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic
model allows the two components to jointly make
decisions based upon both the local evidence sur-
rounding each phrase and the ?peripheral vision?
afforded by the sentential event recognizer.
This paper is organized as follows. Section
2 positions our research with respect to related
work. Section 3 presents our unified probabilistic
model for information extraction. Section 4 shows
experimental results on two IE data sets, and Sec-
tion 5 discusses directions for future work.
2 Related Work
Many event extraction systems rely heavily on the
local context around words or phrases that are can-
didates for extraction. Some systems use extrac-
tion patterns (Soderland et al, 1995; Riloff, 1996;
Yangarber et al, 2000; Califf and Mooney, 2003),
which represent the immediate contexts surround-
ing candidate extractions. Similarly, classifier-
based approaches (Freitag, 1998; Freitag and Mc-
Callum, 2000; Chieu et al, 2003; Bunescu and
Mooney, 2004) rely on features in the immedi-
ate context of the candidate extractions. Our work
seeks to incorporate additional context into IE.
Indeed, several recent approaches have shown
the need for global information to improve IE per-
formance. Maslennikov and Chua (2007) use dis-
course trees and local syntactic dependencies in
a pattern-based framework to incorporate wider
context. Finkel et al (2005) and Ji and Grish-
man (2008) incorporate global information by en-
forcing event role or label consistency over a doc-
ument or across related documents. In contrast,
our approach simply creates a richer IE model for
individual extractions by expanding the ?field of
view? to include the surrounding sentence.
The two components of the unified model pre-
sented in this paper are somewhat similar to our
previous work (Patwardhan and Riloff, 2007),
where we employ a relevant region identification
phase prior to pattern-based extraction. In that
work we adopted a pipeline paradigm, where a
classifier identifies relevant sentences and only
those sentences are fed to the extraction module.
Our unified probabilistic model described in this
paper does not draw a hard line between rele-
vant and irrelevant sentences, but gently balances
the influence of both local and sentential contexts
through probability estimates.
3 A Unified IE Model that Combines
Phrasal and Sentential Evidence
We introduce a probabilistic model for event-
based IE that balances the influence of two kinds
of contextual information. Our goal is to create
a model that has the flexibility to make extraction
decisions based upon strong evidence from the lo-
cal context, or strong evidence from the wider con-
text coupled with a more general local context. For
example, some phrases explicitly refer to an event,
so they almost certainly warrant extraction regard-
less of the wider context (e.g., terrorists launched
an attack).1 In contrast, some phrases are poten-
tially relevant but too general to warrant extrac-
tion on their own (e.g., people died could be the
result of different incident types). If we are confi-
dent that the sentence discusses an event of inter-
est, however, then such phrases could be reliably
extracted.
Our unified model for IE (GLACIER) combines
two types of contextual information by incorpo-
rating it into a probabilistic framework. To deter-
mine whether a noun phrase instance NP
i
should
be extracted as a filler for an event role, GLACIER
computes the joint probability that NP
i
:
(1) appears in an event sentence, and
(2) is a legitimate filler for the event role.
Thus, GLACIER is designed for noun phrase ex-
traction and, mathematically, its decisions are
based on the following joint probability:
P (EvSent(S
NP
i
),PlausFillr(NP
i
))
where S
NP
i
is the sentence containing noun phrase
NP
i
. This probability estimate is based on con-
textual features F appearing within S
NP
i
and in
the local context of NP
i
. Including F in the joint
probability, and applying the product rule, we can
split our probability into two components:
P (EvSent(S
NP
i
),PlausFillr(NP
i
)|F ) =
P (EvSent(S
NP
i
)|F )
? P (PlausFillr(NP
i
)|EvSent(S
NP
i
), F )
These two probability components, in the expres-
sion above, form the basis of the two modules in
1There are always exceptions of course, such as hypothet-
ical statements, but they are relatively uncommon.
152
our IE system ? the sentential event recognizer and
the plausible role-filler recognizer. In arriving at
a decision to extract a noun phrase, our unified
model for IE uses these modules to estimate the
two probabilities based on the set of contextual
features F . Note that having these two probability
components allows the system to gently balance
the influence from the sentential and phrasal con-
texts, without having to make hard decisions about
sentence relevance or phrases in isolation.
In this system, the sentential event recog-
nizer is embodied in the probability compo-
nent P (EvSent(S
NP
i
)|F ). This is essentially
the probability of a sentence describing a rel-
evant event. Similarly, the plausible role-
filler recognizer is embodied by the probabil-
ity P (PlausFillr(NP
i
)|EvSent(S
NP
i
), F ). This
component, therefore, estimates the probability
that a noun phrase fills a specific event role, as-
suming that the noun phrase occurs in an event
sentence. Many different techniques could be used
to produce these probability estimates. In the rest
of this section, we present the specific models that
we used for each of these components.
3.1 Plausible Role-Filler Recognizer
The plausible role-filler recognizer is similar to
most traditional IE systems, where the goal is to
determine whether a noun phrase can be a legiti-
mate filler for a specific type of event role based on
its local context. Pattern-based approaches match
the context surrounding a phrase using lexico-
syntactic patterns or rules. However, most of these
approaches do not produce probability estimates
for the extractions. Classifier-based approaches
use machine learning classifiers to make extrac-
tion decisions, based on features associated with
the local context. Any classifier that can generate
probability estimates, or similar confidence val-
ues, could be plugged into our model.
In our work, we use a Na??ve Bayes classifier as
our plausible role-filler recognizer. The probabili-
ties are computed using a generative Na??ve Bayes
framework, based on local contextual features sur-
rounding a noun phrase. These clues include lexi-
cal matches, semantic features, and syntactic rela-
tions, and will be described in more detail in Sec-
tion 3.3. The Na??ve Bayes (NB) plausible role-
filler recognizer is defined as follows:
P (PlausFillr(NP
i
)|EvSent(S
NP
i
), F ) =
1
Z
P (PlausFillr(NP
i
)|EvSent(S
NP
i
)) ?
?
f
i
?F
P (f
i
|PlausFillr(NP
i
),EvSent(S
NP
i
))
where F is the set of local contextual features
and Z is the normalizing constant. The prior
P (PlausFillr(NP
i
)|EvSent(S
NP
i
)) is estimated
from the fraction of role fillers in the training data.
The product term in the equation is the likelihood,
which makes the simplifying assumption that all
of the features in F are independent of one an-
other. It is important to note that these probabil-
ities are conditioned on the noun phrase NP
i
ap-
pearing in an event sentence.
Most IE systems need to extract several differ-
ent types of role fillers for each event. For in-
stance, to extract information about terrorist inci-
dents a system may extract the names of perpetra-
tors, victims, targets, and weapons. We create a
separate IE model for each type of event role. To
construct a unified IE model for an event role, we
must specifically create a plausible role-filler rec-
ognizer for that event role, but we can use a single
sentential event recognizer for all of the role filler
types.
3.2 Sentential Event Recognizer
The task at hand for the sentential event recognizer
is to analyze features in a sentence and estimate
the probability that the sentence is discussing a
relevant event. This is very similar to the task per-
formed by text classification systems, with some
minor differences. Firstly, we are dealing with
the classification of sentences, as opposed to en-
tire documents. Secondly, we need to generate a
probability estimate of the ?class?, and not just
a class label. Like the plausible role-filler recog-
nizer, here too we employ machine learning clas-
sifiers to estimate the desired probabilities.
3.2.1 Na??ve Bayes Event Recognizer
Since Na??ve Bayes classifiers estimate class prob-
abilities, we employ such a classifier to create a
sentential event recognizer:
P (EvSent(S
NP
i
)|F ) =
1
Z
P (EvSent(S
NP
i
))
?
?
f
i
?F
P (f
i
|EvSent(S
NP
i
))
where Z is the normalizing constant and F is the
set of contextual features in the sentence. The
153
prior P (EvSent
S(NP
i
)
) is obtained from the ra-
tio of event and non-event sentences in the train-
ing data. The product term in the equation is the
likelihood, which makes the simplifying assump-
tion that the features in F are independent of one
another. The features used by the model will be
described in Section 3.3.
A known issue with Na??ve Bayes classifiers is
that, even though their classification accuracy is
often quite reasonable, their probability estimates
are often poor (Domingos and Pazzani, 1996;
Zadrozny and Elkan, 2001; Manning et al, 2008).
The problem is that these classifiers tend to overes-
timate the probability of the predicted class, result-
ing in a situation where most probability estimates
from the classifier tend to be either extremely close
to 0.0 or extremely close to 1.0. We observed this
problem in our classifier too, so we decided to ex-
plore an additional model to estimate probabilities
for the sentential event recognizer. This second
model, based on SVMs, is described next.
3.2.2 SVM Event Recognizer
Given the all-or-nothing nature of the probability
estimates that we observed from the Na??ve Bayes
model, we decided to try using a Support Vector
Machine (SVM) (Vapnik, 1995; Joachims, 1998)
classifier as an alternative to Na??ve Bayes. One
of the issues with doing this is that SVMs are not
probabilistic classifiers. SVMsmake classification
decisions using on a decision boundary defined by
support vectors identified during training. A deci-
sion function is applied to unseen test examples
to determine which side of the decision bound-
ary those examples lie. While the values obtained
from the decision function only indicate class as-
signments for the examples, we used these val-
ues to produce confidence scores for our sentential
event recognizer.
To produce a confidence score from the SVM
classifier, we take the values generated by the deci-
sion function for each test instance and normalize
them based on the minimum and maximum values
produced across all of the test instances. This nor-
malization process produces values between 0 and
1 that we use as a rough indicator of the confidence
in the SVM?s classification. We observed that we
could effect a consistent recall/precision trade-off
by using these values as thresholds for classifica-
tion decisions, which suggests that this approach
worked reasonably well for our task.
3.3 Contextual Features
We used a variety of contextual features in both
components of our system. The plausible role-
filler recognizer uses the following types of fea-
tures for each candidate noun phrase NP
i
: lexical
head ofNP
i
, semantic class ofNP
i
?s lexical head,
named entity tags associated with NP
i
and lexico-
syntactic patterns that represent the local context
surrounding NP
i
. The feature set is automatically
generated from the texts. Each feature is assigned
a binary value for each instance, indicating either
the presence or absence of the feature.
The named-entity features are generated by the
freely available Stanford NER tagger (Finkel et
al., 2005). We use the pre-trained NER model
that comes with the software to identify person,
organization and location names. The syntac-
tic and semantic features are generated by the
Sundance/AutoSlog system (Riloff and Phillips,
2004). We use the Sundance shallow parser to
identify lexical heads, and use its semantic dictio-
naries to assign semantic features to words. The
AutoSlog pattern generator (Riloff, 1996) is used
to create the lexico-syntactic pattern features that
capture local context around each noun phrase.
Our training sets produce a very large number
of features, which initially bogged down our clas-
sifiers. Consequently, we reduced the size of the
feature set by discarding all features that appeared
four times or less in the training set.
Our sentential event recognizer uses the same
contextual features as the plausible role-filler rec-
ognizer, except that features are generated for
every NP in the sentence. In addition, it uses
three types of sentence-level features: sentence
length, bag of words, and verb tense, which are
also binary features. We have two binary sentence
length features indicating that the sentence is long
(greater than 35 words) or is short (shorter than 5
words). Additionally, all of the words in each sen-
tence in the training data are generated as bag of
words features for the sentential model. Finally,
we generate verb tense features from all verbs ap-
pearing in each sentence. Here too we apply a fre-
quency cutoff and eliminate all features that ap-
pear four times or less in the training data.
4 IE Evaluation
4.1 Data Sets
We evaluated the performance of our IE system on
two data sets: the MUC-4 terrorism corpus (Sund-
154
heim, 1992), and a ProMed disease outbreaks cor-
pus (Phillips and Riloff, 2007; Patwardhan and
Riloff, 2007). The MUC-4 data set is a standard
IE benchmark collection of news stories about ter-
rorist events. It contains 1700 documents divided
into 1300 development (DEV) texts, and four test
sets of 100 texts each (TST1, TST2, TST3, and
TST4). Unless otherwise stated, our experiments
adopted the same training/test split used in pre-
vious research: the 1300 DEV texts for training,
200 texts (TST1+TST2) for tuning, and 200 texts
(TST3+TST4) as the blind test set. We evaluated
our system on five MUC-4 string roles: perpetra-
tor individuals, perpetrator organizations, physi-
cal targets, victims, and weapons.
The ProMed corpus consists of 120 documents
obtained from ProMed-mail2, a freely accessible
global electronic reporting system for outbreaks
of diseases. These 120 documents are paired with
corresponding answer key templates. Unless oth-
erwise noted, all of our experiments on this data
set used 5-fold cross validation. We extracted two
types of event roles: diseases and victims3.
Unlike some other IE data sets, many of the
texts in these collections do not describe a rele-
vant event. Only about half of the MUC-4 arti-
cles describe a specific terrorist incident4, and only
about 80% of the ProMed articles describe a dis-
ease outbreak. The answer keys for the irrelevant
documents are therefore empty. IE systems are es-
pecially susceptible to false hits when they can be
given texts that contain no relevant events.
The complete IE task involves the creation of
answer key templates, one template per incident
(many documents in our data sets describe multi-
ple events). Our work focuses on accurately ex-
tracting the facts from the text and not on tem-
plate generation per se (e.g., we are not concerned
with coreference resolution or which extraction
belongs in which template). Consequently, our ex-
periments evaluate the accuracy of the extractions
individually. We used head noun scoring, where
an extraction is considered to be correct if its head
noun matches the head noun in the answer key.5
2http://www.promedmail.org
3The ?victims? can be people, animals, or plants.
4With respect to the definition of terrorist incidents in the
MUC-4 guidelines (Sundheim, 1992).
5Pronouns were discarded from both the system responses
and the answer keys since we do not perform coreference res-
olution. Duplicate extractions (e.g., the same string extracted
multiple times from the same document) were conflated be-
fore being scored, so they count as just one hit or one miss.
4.2 Baselines
We generated three baselines to use as compar-
isons with our IE system. As our first baseline,
we used AutoSlog-TS (Riloff, 1996), which is a
weakly-supervised, pattern-based IE system avail-
able as part of the Sundance/AutoSlog software
package (Riloff and Phillips, 2004). Our previous
work in event-based IE (Patwardhan and Riloff,
2007) also used a pattern-based approach that ap-
plied semantic affinity patterns to relevant regions
in text. We use this system as our second base-
line. As a third baseline, we trained a Na??ve Bayes
IE classifier that is analogous to the plausible role-
filler recognizer in our unified IE model, except
that this baseline system is not conditioned on the
assumption of having an event sentence. Conse-
quently, this baseline NB classifier is akin to a tra-
ditional supervised learning-based IE system that
uses only local contextual features to make extrac-
tion decisions. Formally, the baseline NB classi-
fier uses the formula:
P (PlausFillr(NP
i
)|F ) =
1
Z
P (PlausFillr(NP
i
))
?
?
f
i
?F
P (f
i
|PlausFillr(NP
i
))
where F is the set of local features,
P (PlausFillr(NP
i
)) is the prior probability,
and Z is the normalizing constant. We used the
Weka (Witten and Frank, 2005) implementation
of Na??ve Bayes for this baseline NB system.
New Jersey, February, 26. An outbreak of Ebola has
been confirmed in Mercer County, New Jersey. Five teenage
boys appear to have contracted the deadly virus from an
unknown source. The CDC is investigating the cases and is
taking measures to prevent the spread. . .
Disease: Ebola
Victims: Five teenage boys
Location: Mercer County, New Jersey
Date: February 26
Figure 1: A Disease Outbreak Event Template
Both the MUC-4 and ProMed data sets have
separate answer keys rather than annotated source
documents. Figure 1 shows an example of a doc-
ument and its corresponding answer key template.
To train the baseline NB system, we identify all
instances of each answer key string in the source
document and consider every instance a positive
training example. This produces noisy training
data, however, because some instances occur in
155
PerpInd PerpOrg Target Victim Weapon
P R F P R F P R F P R F P R F
AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41
Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50
NB .50 .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10
NB .70 .41 .25 .31 .43 .31 .36 .58 .42 .48 .58 .37 .45 1.00 .04 .07
NB .90 .51 .17 .25 .56 .15 .24 .67 .30 .41 .75 .23 .36 1.00 .02 .04
Table 1: Baseline Results on MUC-4
Disease Victim
P R F P R F
AutoSlog-TS .33 .60 .43 .36 .49 .41
Sem Affinity .31 .49 .38 .41 .47 .44
NB .50 .20 .73 .31 .29 .56 .39
NB .70 .23 .67 .34 .37 .52 .44
NB .90 .34 .59 .43 .47 .39 .43
Table 2: Baseline Results on ProMed
undesirable contexts. For example, if the string
?man? appears in an answer key as a victim, one
instance of ?man? may refer to the actual vic-
tim in an event sentence, while another instance
of ?man? may occur in a non-event context (e.g.,
background information) or may refer to a com-
pletely different person.
We report three evaluation metrics in our exper-
iments: precision (P), recall (R), and F-score (F),
where recall and precision are equally weighted.
For the Na??ve Bayes classifier, the natural thresh-
old for distinguishing between positive and nega-
tive classes is 0.5, but we also evaluated this clas-
sifier with thresholds of 0.7 and 0.9 to see if we
could effect a recall/precision trade-off. Tables 1
and 2 present the results of our three baseline sys-
tems. The NB classifier performs comparably to
AutoSlog-TS and Semantic Affinity on most event
roles, although a threshold of 0.90 is needed to
reach comparable performance on ProMed. The
relatively low numbers across the board indicate
that these corpora are challenging, but these re-
sults suggest that our plausible role-filler recog-
nizer is competitive with other existing IE sys-
tems. In Section 4.4 we will show how our unified
IE model compares to these baselines. But before
that (in the next section) we evaluate the quality of
the second component of our IE system: the sen-
tential event recognizer.
4.3 Sentential Event Recognizer Models
The sentential event recognizer is one of the core
contributions of this research, so in this section we
evaluate it by itself, before we employ it within the
unified framework. The purpose of the sentential
event recognizer is to determine whether a sen-
tence is discussing a domain-relevant event. For
our data sets, the classifier must decide whether a
sentence is discussing a terrorist incident (MUC-
4) or a disease outbreak (ProMed). Ideally, we
want such a classifier to operate independently
from the answer keys and the extraction task per
se. For example, a terrorism IE system could be
designed to extract only perpetrators and victims
of terrorist events, or it could be designed to ex-
tract only targets and locations. The job of the sen-
tential event recognizer remains the same: to iden-
tify sentences that discuss a terrorist event. How to
train and evaluate such a system is a difficult ques-
tion. In this section, we present two approaches
that we explored to generate the training data: (a)
using the IE answer keys, and (b) using human
judgements.
4.3.1 Sentence Annotation via Answer Keys
We have argued that the event relevance of a sen-
tence should not be tied to a specific set of event
roles. However, the IE answer keys can be used
to identify some sentences that describe an event,
because they contain an answer string. So we can
map the answer strings back to sentences in the
source documents to automatically generate event
sentence annotations.6 These annotations will be
noisy, though, because an answer string can appear
in a non-event sentence, and some event sentences
may not contain any answer strings. The alterna-
tive, however, is sentence annotations by humans,
which (as we will discuss in Section 4.3.2) is chal-
lenging.
4.3.2 Sentence Annotation via Human
Judgements
For many sentences there is a clear consensus
among people that an event is being discussed. For
example, most readers would agree that sentence
(1) below is describing a terrorist event, while sen-
6A similar strategy was used in previous work (Patward-
han and Riloff, 2007) to generate a test set for the evaluation
of a relevant region classifier.
156
Evaluation on Answer Keys Evaluation on Human Annotations
Event Non-Event Event Non-Event
Acc Pr Rec F Pr Rec F Acc Pr Rec F Pr Rec F
MUC-4 (Terrorism)
An
s NB .80 .57 .55 .56 .86 .87 .87 .81 .46 .60 .52 .91 .85 .88
SVM .80 .68 .42 .52 .84 .93 .88 .83 .55 .44 .49 .88 .91 .90
H
u
m NB .82 .64 .48 .55 .85 .92 .88 .85 .56 .57 .57 .91 .91 .91
SVM .79 .64 .41 .50 .83 .91 .87 .84 .62 .51 .56 .90 .91 .91
ProMed (Disease Outbreaks)
An
s NB .75 .62 .61 .61 .81 .82 .82 .72 .43 .58 .50 .86 .77 .81
SVM .74 .78 .31 .44 .74 .95 .83 .76 .51 .26 .35 .80 .92 .86
H
u
m NB .73 .61 .46 .52 .77 .86 .81 .79 .56 .57 .56 .87 .86 .86
SVM .70 .62 .32 .42 .73 .89 .81 .79 .62 .42 .50 .84 .90 .87
Table 3: Sentential Event Recognizers Results (5-fold Cross-Validation)
Evaluation on Human Annotations
Event Non-Event
Acc Pr Rec F Pr Rec F
NB .83 .50 .70 .58 .94 .86 .90
SVM .89 .83 .39 .53 .89 .98 .94
Table 4: Sentential Event Recognizer Results for
MUC-4 using 1300 Documents for Training
tence (2) is not. However it is difficult to draw a
clear line. Sentence (3), for example, describes an
action taken in response to a terrorist event. Is this
a terrorist event sentence? Precisely how to define
an event sentence is not obvious.
(1) Al Qaeda operatives launched an at-
tack on the Madrid subway system.
(2) Madrid has a population of about
3.2 million people.
(3) City officials stepped up security in
response to the attacks.
We tackled this issue by creating detailed an-
notation guidelines to define the notion of an
event sentence, and conducting a human annota-
tion study. The guidelines delineated a general
time frame for the beginning and end of an event,
and constrained the task to focus on specific inci-
dents that were reported in the IE answer key. We
gave the annotators a brief description (e.g., mur-
der in Peru) of each event that had a filled answer
key in the data set. They only labeled sentences
that discussed those particular events.
We employed two human judges, who anno-
tated 120 documents from the ProMed test set,
and 100 documents from the MUC-4 test set. We
asked both judges to label 30 of the same docu-
ments from each data set so that we could compute
inter-annotator agreement. The annotators had an
agreement of 0.72 Cohen?s ? on the ProMed data,
and 0.77 Cohen?s ? on the MUC-4 data. Given
the difficulty of this task, we were satisfied that
this task is reasonably well-defined and the anno-
tations are of good quality.
4.3.3 Event Recognizer Results
We evaluated the two sentential event recognizer
models described in Section 3.2 in two ways:
(1) using the answer key sentence annotations for
training/testing, and (2) using the human annota-
tions for training/testing. Table 3 shows the re-
sults for all combinations of training/testing data.
Since we only have human annotations for 100
MUC-4 texts and 120 ProMed texts, we performed
5-fold cross-validation on these documents. For
our classifiers, we used the Weka (Witten and
Frank, 2005) implementation of Na??ve Bayes and
the SVMLight (Joachims, 1998) implementation
of the SVM. For each classifier we report overall
accuracy, and precision, recall and F-scores with
respect to both the positive and negative classes
(event vs. non-event sentences).
The rows labeled Ans show the results for mod-
els trained via answer keys, and the rows labeled
Hum show the results for the models trained with
human annotations. The left side of the table
shows the results using the answer key annotations
for evaluation, and the right side of the table shows
the results using the human annotations for evalua-
tion. One expects classifiers to perform best when
they are trained and tested on the same type of
data, and our results bear this out ? the classifiers
that were trained and tested on the same kind of
annotations do best. The boldfaced numbers rep-
resent the best accuracies achieved for each do-
main. As we would expect, the classifiers that are
both trained and tested with human annotations
(Hum) show the best performance, with the Na??ve
Bayes achieving the best accuracy of 85% on the
157
PerpInd PerpOrg Target Victim Weapon
P R F P R F P R F P R F P R F
AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41
Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50
NB (baseline) .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10
GLACIER
NB/NB .90 .39 .59 .47 .33 .51 .40 .39 .72 .51 .52 .54 .53 .47 .55 .51
NB/SVM .40 .51 .58 .54 .34 .45 .38 .42 .72 .53 .55 .58 .56 .57 .53 .55
NB/SVM .50 .66 .47 .55 .41 .26 .32 .50 .62 .55 .62 .36 .45 .64 .43 .52
Table 5: Unified IE Model on MUC-4
MUC-4 texts, and the SVM achieving the best ac-
curacy of 79% on the ProMed texts.
The recall and precision for non-event sentences
is much higher than for event sentences. This clas-
sifier is forced to draw a hard line between the
event and non-event sentences, which is a difficult
task even for people. One of the advantages of our
unified IE model, which will be described in the
next section, is that it does not require hard deci-
sions but instead uses a probabilistic estimate of
how ?event-ish? a sentence is.
Table 3 showed that models trained on human
annotations outperform models trained on answer
key annotations. But with the MUC-4 data, we
have the luxury of 1300 training documents with
answer keys, while we only have 100 documents
with human annotations. Even though the answer
key annotations are noisier, we have 13 times as
much training data.
So we trained another sentential event recog-
nizer using the entire MUC-4 training set. These
results are shown in Table 4. Observe that using
this larger (albeit noisy) training data does not ap-
pear to affect the Na??ve Bayes model very much.
Compared with the model trained on 100 manu-
ally annotated documents, its accuracy decreases
by 2% from 85% to 83%. The SVM model, on
the other hand, achieves an 89% accuracy when
trained with the larger MUC-4 training data, com-
pared to 84% accuracy for the model trained from
the 100 manually labeled documents. Conse-
quently, the sentential event recognizer models
used in our unified IE framework (described in
Section 4.4) are trained with this 1300 document
training set.
4.4 Evaluation of the Unified IE Model
We now evaluate the performance of our unified IE
model, GLACIER, which allows a plausible role-
filler recognizer and a sentential event recognizer
to make joint decisions about phrase extractions.
Tables 5 and 6 present the results of the unified
Disease Victim
P R F P R F
AutoSlog-TS .33 .60 .43 .36 .49 .41
Sem Affinity .31 .49 .38 .41 .47 .44
NB (baseline) .34 .59 .43 .47 .39 .43
GLACIER
NB/NB .90 .41 .61 .49 .38 .52 .44
NB/SVM .40 .31 .66 .42 .32 .55 .41
NB/SVM .50 .38 .54 .44 .42 .47 .44
Table 6: Unified IE Model on ProMed
IE model on the MUC-4 and ProMed data sets.
The NB/NB systems use Na??ve Bayes models for
both components, while the NB/SVM systems use
a Na??ve Bayes model for the plausible role-filler
recognizer and an SVM for the sentential event
recognizer. As with our baseline system, we ob-
tain good results using a threshold of 0.90 for our
NB/NB model (i.e., only NPs with probability ?
0.90 are extracted). For our NB/SVM models, we
evaluated using the default threshold (0.50) but ob-
served that recall was sometimes low. So we also
use a threshold of 0.40, which produces superior
results. Here too, we used the Weka (Witten and
Frank, 2005) implementation of the Na??ve Bayes
model and the SVMLight (Joachims, 1998) imple-
mentation of the SVM.
For the MUC-4 data, our unified IE model us-
ing the SVM (0.40) outperforms all 3 baselines
on three roles (PerpInd, Victim, Weapon) and
outperforms 2 of the 3 baselines on the Target
role. When GLACIER outperforms the other sys-
tems it is often by a wide margin: the F-score
for PerpInd jumped from 0.43 for the best base-
line (Sem Affinity) to 0.54 for GLACIER, and the
F-scores for Victim and Weapon each improved
by 5% over the best baseline. These gains came
from both increased recall and increased precision,
demonstrating that GLACIER extracts some infor-
mation that was missed by the other systems and
is also less prone to false hits.
Only the PerpOrg role shows inferior per-
formance. Organizations perpetrating a terrorist
158
event are often discussed later in a document, far
removed from the main event description. For ex-
ample, a statement that Al Qaeda is believed to
be responsible for an attack would typically ap-
pear after the event description. As a result, the
sentential event recognizer tends to generate low
probabilities for such sentences. We believe that
addressing this issue would require the use of dis-
course relations or the use of even larger context
sizes. We intend to explore these avenues of re-
search in future work.
On the ProMed data, GLACIER produces results
that are similar to the baselines for theVictim role,
but it outperforms the baselines for the Disease
role. We find that for this domain, the unified IE
model with the Na??ve Bayes sentential event rec-
ognizer is superior to the unified IE model with
the SVM classifier. For the Disease role, the F-
score jumped 6%, from 0.43 for the best base-
line systems (AutoSlog-TS and the NB baseline)
to 0.49 for GLACIER
NB/NB
. In contrast to the
MUC-4 data, this improvement was mostly due
to an increase in precision (up to 0.41), indicating
that our unified IE model was effective at elimi-
nating many false hits. For the Victim role, the
performance of the unified model is comparable
to the baselines. On this event role, the F-score
of GLACIER
NB/NB
(0.44) matches that of the best
baseline system (Sem Affinity, with 0.44). How-
ever, note that GLACIER
NB/NB
can achieve a 5%
gain in recall over this baseline, at the cost of a 3%
precision loss.
4.5 Specific Examples
Figure 2 presents some specific examples of ex-
tractions that are failed to be extracted by the
baseline models, but are correctly identified by
GLACIER because of its use of sentential evidence.
Observe that in each of these examples, GLACIER
correctly extracts the underlined phrases, in spite
of the inconclusive evidence in the local contexts
around them. In the last sentence in Figure 2, for
example, GLACIER correctly makes the inference
that the policemen in the bus (which was traveling
on the bridge) are likely the victims of the terrorist
event. Thus, we see that our system manages to
balance the influence of the two probability com-
ponents to make extraction decisions that would
be impossible to make by relying only on the local
phrasal context. In addition, the sentential event
recognizer can also help improve precision by pre-
THE MNR REPORTED ON 12 JANUARY THAT HEAVILY
ARMED MEN IN CIVILIAN CLOTHES HAD INTERCEPTED
A VEHICLE WITH OQUELI AND FLORES ENROUTE FOR
LA AURORA AIRPORT AND THAT THE TWO POLITICAL
LEADERS HAD BEEN KIDNAPPED AND WERE REPORTED
MISSING.
PerpInd: HEAVILY ARMED MEN
THE SCANT POLICE INFORMATION SAID THAT THE
DEVICES WERE APPARENTLY LEFT IN FRONT OF THE TWO
BANK BRANCHES MINUTES BEFORE THE CURFEW BEGAN
FOR THE 6TH CONSECUTIVE DAY ? PRECISELY TO
COUNTER THE WAVE OF TERRORISM CAUSED BY DRUG
TRAFFICKERS.
Weapon: THE DEVICES
THOSE WOUNDED INCLUDE THREE EMPLOYEES OF THE
GAS STATION WHERE THE CAR BOMB WENT OFF AND
TWO PEOPLE WHO WERE WALKING BY THE GAS STATION
AT THE MOMENT OF THE EXPLOSION.
Victim: THREE EMPLOYEES OF THE GAS STATION
Victim: TWO PEOPLE
MEMBERS OF THE BOMB SQUAD HAVE DEACTIVATED
A POWERFUL BOMB PLANTED AT THE ANDRES AVELINO
CACERES PARK, WHERE PRESIDENT ALAN GARCIA WAS
DUE TO PARTICIPATE IN THE COMMEMORATION OF THE
BATTLE OF TARAPACA.
Victim: PRESIDENT ALAN GARCIA
EPL [POPULAR LIBERATION ARMY] GUERRILLAS BLEW
UP A BRIDGE AS A PUBLIC BUS, IN WHICH SEVERAL
POLICEMEN WERE TRAVELING, WAS CROSSING IT.
Victim: SEVERAL POLICEMEN
Figure 2: Examples of GLACIER Extractions
venting extractions from non-event sentences.
5 Conclusions
We presented a unified model for IE that balances
the influence of sentential context with local con-
textual evidence to improve the performance of
event-based IE. Our experimental results showed
that using sentential contexts indeed produced bet-
ter results on two IE data sets. Our current model
uses supervised learning, so one direction for fu-
ture work is to adapt the model for weakly super-
vised learning. We also plan to incorporate dis-
course features and investigate even wider con-
texts to capture broader discourse effects.
Acknowledgments
This work has been supported in part by the De-
partment of Homeland Security Grant N0014-07-
1-0152. We are grateful to Nathan Gilbert and
Adam Teichert for their help with the annotation
of event sentences.
159
References
R. Bunescu and R. Mooney. 2004. Collective In-
formation Extraction with Relational Markov Net-
works. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 438?445, Barcelona, Spain, July.
M. Califf and R. Mooney. 2003. Bottom-Up Rela-
tional Learning of Pattern Matching Rules for In-
formation Extraction. Journal of Machine Learning
Research, 4:177?210, December.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
Gap: Learning-Based Information Extraction Rival-
ing Knowledge-Engineering Methods. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 216?223, Sap-
poro, Japan, July.
P. Domingos and M. Pazzani. 1996. Beyond Inde-
pendence: Conditions for the Optimality of the Sim-
ple Bayesian Classifier. In Proceedings of the Thir-
teenth International Conference on Machine Learn-
ing, pages 105?112, Bari, Italy, July.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 363?370, Ann
Arbor, MI, June.
D. Freitag and A. McCallum. 2000. Informa-
tion Extraction with HMM Structures Learned by
Stochastic Optimization. In Proceedings of the Sev-
enteenth National Conference on Artificial Intelli-
gence, pages 584?589, Austin, TX, August.
D. Freitag. 1998. Toward General-Purpose Learning
for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics, pages 404?408,
Montreal, Quebec, August.
H. Ji and R. Grishman. 2008. Refining Event Ex-
traction through Cross-Document Inference. In Pro-
ceedings of ACL-08: HLT, pages 254?262, Colum-
bus, OH, June.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rele-
vant Features. In Proceedings of the Tenth European
Conference on Machine Learning, pages 137?142,
April.
C. Manning, P. Raghavan, and H Schu?tze. 2008. Intro-
duction to Information Retrieval. Cambridge Uni-
versity Press, New York, NY.
M. Maslennikov and T. Chua. 2007. A Multi-
resolution Framework for Information Extraction
from Free Text. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 592?599, Prague, Czech Republic,
June.
S. Patwardhan and E. Riloff. 2007. Effective Informa-
tion Extraction with Semantic Affinity Patterns and
Relevant Regions. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 717?727, Prague, Czech Re-
public, June.
W. Phillips and E. Riloff. 2007. Exploiting Role-
Identifying Nouns and Expressions for Informa-
tion Extraction. In Proceedings of International
Conference on Recent Advances in Natural Lan-
guage Processing, pages 165?172, Borovets, Bul-
garia, September.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049, Portland, OR, August.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a Conceptual Dictio-
nary. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence, pages
1314?1319, Montreal, Canada, August.
B. Sundheim. 1992. Overview of the Fourth Message
Understanding Evaluation and Conference. In Pro-
ceedings of the Fourth Message Understanding Con-
ference (MUC-4), pages 3?21, McLean, VA, June.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer, New York, NY.
I. Witten and E. Frank. 2005. Data Mining - Practical
Machine Learning Tools and Techniques. Morgan?
Kaufmann, San Francisco, CA.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 940?946, Saarbru?cken,
Germany, August.
B. Zadrozny and C. Elkan. 2001. Obtaining Cal-
ibrated Probability Estimates from Decision Trees
and Na??ve Bayesian Classiers. In Proceedings of the
Eighteenth International Conference on Machine
Learning, pages 609?616, Williamstown, MA, June.
160
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 355?362, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Identifying Sources of Opinions with Conditional Random Fields and
Extraction Patterns
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Abstract
Recent systems have been developed for
sentiment classification, opinion recogni-
tion, and opinion analysis (e.g., detect-
ing polarity and strength). We pursue an-
other aspect of opinion analysis: identi-
fying the sources of opinions, emotions,
and sentiments. We view this problem as
an information extraction task and adopt
a hybrid approach that combines Con-
ditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff,
1996a). While CRFs model source iden-
tification as a sequence tagging task, Au-
toSlog learns extraction patterns. Our re-
sults show that the combination of these
two methods performs better than either
one alone. The resulting system identifies
opinion sources with 79.3% precision and
59.5% recall using a head noun matching
measure, and 81.2% precision and 60.6%
recall using an overlap measure.
1 Introduction
In recent years, there has been a great deal of in-
terest in methods for automatically identifying opin-
ions, emotions, and sentiments in text. Much of
this research explores sentiment classification, a text
categorization task in which the goal is to classify
a document as having positive or negative polar-
ity (e.g., Das and Chen (2001), Pang et al (2002),
Turney (2002), Dave et al (2003), Pang and Lee
(2004)). Other research efforts analyze opinion ex-
pressions at the sentence level or below to recog-
nize opinions, their polarity, and their strength (e.g.,
Dave et al (2003), Pang and Lee (2004), Wilson et
al. (2004), Yu and Hatzivassiloglou (2003), Wiebe
and Riloff (2005)). Many applications could ben-
efit from these opinion analyzers, including prod-
uct reputation tracking (e.g., Morinaga et al (2002),
Yi et al (2003)), opinion-oriented summarization
(e.g., Cardie et al (2004)), and question answering
(e.g., Bethard et al (2004), Yu and Hatzivassiloglou
(2003)).
We focus here on another aspect of opinion
analysis: automatically identifying the sources of
the opinions. Identifying opinion sources will
be especially critical for opinion-oriented question-
answering systems (e.g., systems that answer ques-
tions of the form ?How does [X] feel about [Y]??)
and opinion-oriented summarization systems, both
of which need to distinguish the opinions of one
source from those of another.1
The goal of our research is to identify direct and
indirect sources of opinions, emotions, sentiments,
and other private states that are expressed in text.
To illustrate the nature of this problem, consider the
examples below:
S1: Taiwan-born voters favoring independence...
1In related work, we investigate methods to identify the
opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and
Riloff (2005), Wilson et al (2005)) and the nesting structure
of sources (e.g., Breck and Cardie (2004)). The target of each
opinion, i.e., what the opinion is directed towards, is currently
being annotated manually for our corpus.
355
S2: According to the report, the human rights
record in China is horrendous.
S3: International officers believe that the EU will
prevail.
S4: International officers said US officials want the
EU to prevail.
In S1, the phrase ?Taiwan-born voters? is the di-
rect (i.e., first-hand) source of the ?favoring? sen-
timent. In S2, ?the report? is the direct source of
the opinion about China?s human rights record. In
S3, ?International officers? are the direct source of
an opinion regarding the EU. The same phrase in
S4, however, denotes an indirect (i.e., second-hand,
third-hand, etc.) source of an opinion whose direct
source is ?US officials?.
In this paper, we view source identification as an
information extraction task and tackle the problem
using sequence tagging and pattern matching tech-
niques simultaneously. Using syntactic, semantic,
and orthographic lexical features, dependency parse
features, and opinion recognition features, we train a
linear-chain Conditional Random Field (CRF) (Laf-
ferty et al, 2001) to identify opinion sources. In ad-
dition, we employ features based on automatically
learned extraction patterns and perform feature in-
duction on the CRF model.
We evaluate our hybrid approach using the NRRC
corpus (Wiebe et al, 2005), which is manually
annotated with direct and indirect opinion source
information. Experimental results show that the
CRF model performs well, and that both the extrac-
tion patterns and feature induction produce perfor-
mance gains. The resulting system identifies opinion
sources with 79.3% precision and 59.5% recall us-
ing a head noun matching measure, and 81.2% pre-
cision and 60.6% recall using an overlap measure.
2 The Big Picture
The goal of information extraction (IE) systems is
to extract information about events, including the
participants of the events. This task goes beyond
Named Entity recognition (e.g., Bikel et al (1997))
because it requires the recognition of role relation-
ships. For example, an IE system that extracts in-
formation about corporate acquisitions must distin-
guish between the company that is doing the acquir-
ing and the company that is being acquired. Sim-
ilarly, an IE system that extracts information about
terrorism must distinguish between the person who
is the perpetrator and the person who is the victim.
We hypothesized that IE techniques would be well-
suited for source identification because an opinion
statement can be viewed as a kind of speech event
with the source as the agent.
We investigate two very different learning-based
methods from information extraction for the prob-
lem of opinion source identification: graphical mod-
els and extraction pattern learning. In particular, we
consider Conditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff, 1996a).
CRFs have been used successfully for Named En-
tity recognition (e.g., McCallum and Li (2003),
Sarawagi and Cohen (2004)), and AutoSlog has per-
formed well on information extraction tasks in sev-
eral domains (Riloff, 1996a). While CRFs treat
source identification as a sequence tagging task, Au-
toSlog views the problem as a pattern-matching task,
acquiring symbolic patterns that rely on both the
syntax and lexical semantics of a sentence. We hy-
pothesized that a combination of the two techniques
would perform better than either one alone.
Section 3 describes the CRF approach to identify-
ing opinion sources and the features that the system
uses. Section 4 then presents a new variation of Au-
toSlog, AutoSlog-SE, which generates IE patterns to
extract sources. Section 5 describes the hybrid sys-
tem: we encode the IE patterns as additional features
in the CRF model. Finally, Section 6 presents our
experimental results and error analysis.
3 Semantic Tagging via Conditional
Random Fields
We defined the problem of opinion source identifi-
cation as a sequence tagging task via CRFs as fol-
lows. Given a sequence of tokens, x = x1x2...xn,
we need to generate a sequence of tags, or labels,
y = y1y2...yn. We define the set of possible label
values as ?S?, ?T?, ?-?, where ?S? is the first to-
ken (or Start) of a source, ?T? is a non-initial token
(i.e., a conTinuation) of a source, and ?-? is a token
that is not part of any source.2
A detailed description of CRFs can be found in
2This is equivalent to the IOB tagging scheme used in syn-
tactic chunkers (Ramshaw and Marcus, 1995).
356
Lafferty et al (2001). For our sequence tagging
problem, we create a linear-chain CRF based on
an undirected graph G = (V,E), where V is the
set of random variables Y = {Yi|1 ? i ? n},
one for each of n tokens in an input sentence;
and E = {(Yi?1, Yi)|1 < i ? n} is the set
of n ? 1 edges forming a linear chain. For each
sentence x, we define a non-negative clique poten-
tial exp(
?K
k=1 ?kfk(yi?1, yi, x)) for each edge, and
exp(?K?k=1 ??kf ?k(yi, x)) for each node, where fk(...)
is a binary feature indicator function, ?k is a weight
assigned for each feature function, and K and K ?
are the number of features defined for edges and
nodes respectively. Following Lafferty et al (2001),
the conditional probability of a sequence of labels y
given a sequence of tokens x is:
P (y|x) = 1Zx
exp
?
X
i,k
?k fk(yi?1, yi, x)+
X
i,k
??k f ?k(yi, x)
?
(1)
Zx =
X
y
exp
?
X
i,k
?k fk(yi?1, yi, x) +
X
i,k
??k f ?k(yi, x)
?
(2)
where Zx is a normalization constant for each
x. Given the training data D, a set of sen-
tences paired with their correct ?ST-? source la-
bel sequences, the parameters of the model are
trained to maximize the conditional log-likelihood
?
(x,y)?D P (y|x). For inference, given a sentence x
in the test data, the tagging sequence y is given by
argmaxy?P (y?|x).
3.1 Features
To develop features, we considered three properties
of opinion sources. First, the sources of opinions are
mostly noun phrases. Second, the source phrases
should be semantic entities that can bear or express
opinions. Third, the source phrases should be di-
rectly related to an opinion expression. When con-
sidering only the first and second criteria, this task
reduces to named entity recognition. Because of the
third condition, however, the task requires the recog-
nition of opinion expressions and a more sophisti-
cated encoding of sentence structure to capture re-
lationships between source phrases and opinion ex-
pressions.
With these properties in mind, we define the fol-
lowing features for each token/word xi in an input
sentence. For pedagogical reasons, we will describe
some of the features as being multi-valued or cate-
gorical features. In practice, however, all features
are binarized for the CRF model.
Capitalization features We use two boolean fea-
tures to represent the capitalization of a word:
all-capital, initial-capital.
Part-of-speech features Based on the lexical cat-
egories produced by GATE (Cunningham et al,
2002), each token xi is classified into one of a set
of coarse part-of-speech tags: noun, verb, adverb,
wh-word, determiner, punctuation, etc. We do the
same for neighboring words in a [?2,+2] window
in order to assist noun phrase segmentation.
Opinion lexicon features For each token xi, we in-
clude a binary feature that indicates whether or not
the word is in our opinion lexicon ? a set of words
that indicate the presence of an opinion. We do the
same for neighboring words in a [?1,+1] window.
Additionally, we include for xi a feature that in-
dicates the opinion subclass associated with xi, if
available from the lexicon. (e.g., ?bless? is clas-
sified as ?moderately subjective? according to the
lexicon, while ?accuse? and ?berate? are classified
more specifically as ?judgments?.) The lexicon is
initially populated with approximately 500 opinion
words 3 from (Wiebe et al, 2002), and then aug-
mented with opinion words identified in the training
data. The training data contains manually produced
phrase-level annotations for all expressions of opin-
ions, emotions, etc. (Wiebe et al, 2005). We col-
lected all content words that occurred in the training
set such that at least 50% of their occurrences were
in opinion annotations.
Dependency tree features For each token xi, we
create features based on the parse tree produced by
the Collins (1999) dependency parser. The purpose
of the features is to (1) encode structural informa-
tion, and (2) indicate whether xi is involved in any
grammatical relations with an opinion word. Two
pre-processing steps are required before features can
be constructed:
3Some words are drawn from Levin (1993); others are from
Framenet lemmas (Baker et al 1998) associated with commu-
nication verbs.
357
1. Syntactic chunking. We traverse the depen-
dency tree using breadth-first search to identify
and group syntactically related nodes, produc-
ing a flatter, more concise tree. Each syntac-
tic ?chunk? is also assigned a grammatical role
(e.g., subject, object, verb modifier, time,
location, of-pp, by-pp) based on its con-
stituents. Possessives (e.g., ?Clinton?s idea?)
and the phrase ?according to X? are handled as
special cases in the chunking process.
2. Opinion word propagation. Although the
opinion lexicon contains only content words
and no multi-word phrases, actual opinions of-
ten comprise an entire phrase, e.g., ?is really
willing? or ?in my opinion?. As a result, we
mark as an opinion the entire chunk that con-
tains an opinion word. This allows each token
in the chunk to act as an opinion word for fea-
ture encoding.
After syntactic chunking and opinion word propa-
gation, we create the following dependency tree fea-
tures for each token xi:
? the grammatical role of its chunk
? the grammatical role of xi?1?s chunk
? whether the parent chunk includes an opinion
word
? whether xi?s chunk is in an argument position
with respect to the parent chunk
? whether xi represents a constituent boundary
Semantic class features We use 7 binary fea-
tures to encode the semantic class of each word
xi: authority, government, human, media,
organization or company, proper name,
and other. The other class captures 13 seman-
tic classes that cannot be sources, such as vehicle
and time.
Semantic class information is derived from named
entity and semantic class labels assigned to xi by the
Sundance shallow parser (Riloff, 2004). Sundance
uses named entity recognition rules to label noun
phrases as belonging to named entity classes, and
assigns semantic tags to individual words based on
a semantic dictionary. Table 1 shows the hierarchy
that Sundance uses for semantic classes associated
with opinion sources. Sundance is also used to rec-
ognize and instantiate the source extraction patterns
PROPER NAMEAUTHORITY LOCATION
CITY
COUNTRY
PLANET
PROVINCE
PERSON NAME
PERSON DESC
NATIONALITY
TITLE
COMPANY
GOVERNMENT
MEDIA
ORGANIZATION
HUMAN
SOURCE
Figure 1: The semantic hierarchy for opinion
sources
that are learned by AutoSlog-SE, which is described
in the next section.
4 Semantic Tagging via Extraction
Patterns
We also learn patterns to extract opinion sources us-
ing a statistical adaptation of the AutoSlog IE learn-
ing algorithm. AutoSlog (Riloff, 1996a) is a super-
vised extraction pattern learner that takes a train-
ing corpus of texts and their associated answer keys
as input. A set of heuristics looks at the context
surrounding each answer and proposes a lexico-
syntactic pattern to extract that answer from the text.
The heuristics are not perfect, however, so the result-
ing set of patterns needs to be manually reviewed by
a person.
In order to build a fully automatic system that
does not depend on manual review, we combined
AutoSlog?s heuristics with statistics from the an-
notated training data to create a fully automatic
supervised learner. We will refer to this learner
as AutoSlog-SE (Statistically Enhanced variation
of AutoSlog). AutoSlog-SE?s learning process has
three steps:
Step 1: AutoSlog?s heuristics are applied to every
noun phrase (NP) in the training corpus. This
generates a set of extraction patterns that, col-
lectively, can extract every NP in the training
corpus.
Step 2: The learned patterns are augmented with
selectional restrictions that semantically con-
strain the types of noun phrases that are legiti-
mate extractions for opinion sources. We used
358
the semantic classes shown in Figure 1 as se-
lectional restrictions.
Step 3: The patterns are applied to the training cor-
pus and statistics are gathered about their ex-
tractions. We count the number of extrac-
tions that match annotations in the corpus (cor-
rect extractions) and the number of extractions
that do not match annotations (incorrect extrac-
tions). These counts are then used to estimate
the probability that the pattern will extract an
opinion source in new texts:
P (source | patterni) =
correct sources
correct sources + incorrect sources
This learning process generates a set of extraction
patterns coupled with probabilities. In the next sec-
tion, we explain how these extraction patterns are
represented as features in the CRF model.
5 Extraction Pattern Features for the CRF
The extraction patterns provide two kinds of infor-
mation. SourcePatt indicates whether a word
activates any source extraction pattern. For exam-
ple, the word ?complained? activates the pattern
?<subj> complained? because it anchors the ex-
pression. SourceExtr indicates whether a word is
extracted by any source pattern. For example, in the
sentence ?President Jacques Chirac frequently com-
plained about France?s economy?, the words ?Pres-
ident?, ?Jacques?, and ?Chirac? would all be ex-
tracted by the ?<subj> complained? pattern.
Each extraction pattern has frequency and prob-
ability values produced by AutoSlog-SE, hence we
create four IE pattern-based features for each token
xi: SourcePatt-Freq, SourceExtr-Freq,
SourcePatt-Prob, and SourceExtr-Prob,
where the frequency values are divided into three
ranges: {0, 1, 2+} and the probability values are di-
vided into five ranges of equal size.
6 Experiments
We used the Multi-Perspective Question Answering
(MPQA) corpus4 for our experiments. This corpus
4The MPQA corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
consists of 535 documents that have been manu-
ally annotated with opinion-related information in-
cluding direct and indirect sources. We used 135
documents as a tuning set for model development
and feature engineering, and used the remaining 400
documents for evaluation, performing 10-fold cross
validation. These texts are English language ver-
sions of articles that come from many countries and
cover many topics.5
We evaluate performance using 3 measures: over-
lap match (OL), head match (HM), and exact match
(EM). OL is a lenient measure that considers an ex-
traction to be correct if it overlaps with any of the an-
notated words. HM is a more conservative measure
that considers an extraction to be correct if its head
matches the head of the annotated source. We report
these somewhat loose measures because the annota-
tors vary in where they place the exact boundaries
of a source. EM is the strictest measure that requires
an exact match between the extracted words and the
annotated words. We use three evaluation metrics:
recall, precision, and F-measure with recall and pre-
cision equally weighted.
6.1 Baselines
We developed three baseline systems to assess the
difficulty of our task. Baseline-1 labels as sources
all phrases that belong to the semantic categories
authority, government, human, media,
organization or company, proper name.
Table 1 shows that the precision is poor, suggest-
ing that the third condition described in Section 3.1
(opinion recognition) does play an important role in
source identification. The recall is much higher but
still limited due to sources that fall outside of the se-
mantic categories or are not recognized as belong-
ing to these categories. Baseline-2 labels a noun
phrase as a source if any of the following are true:
(1) the NP is the subject of a verb phrase containing
an opinion word, (2) the NP follows ?according to?,
(3) the NP contains a possessive and is preceded by
an opinion word, or (4) the NP follows ?by? and at-
taches to an opinion word. Baseline-2?s heuristics
are designed to address the first and the third condi-
tions in Section 3.1. Table 1 shows that Baseline-2
is substantially better than Baseline-1. Baseline-3
5This data was obtained from the Foreign Broadcast Infor-
mation Service (FBIS), a U.S. government agency.
359
Recall Prec F1
OL 77.3 28.8 42.0
Baseline-1 HM 71.4 28.6 40.8
EM 65.4 20.9 31.7
OL 62.4 60.5 61.4
Baseline-2 HM 59.7 58.2 58.9
EM 50.8 48.9 49.8
OL 49.9 72.6 59.2
Baseline-3 HM 47.4 72.5 57.3
EM 44.3 58.2 50.3
OL 48.5 81.3 60.8
Extraction Patterns HM 46.9 78.5 58.7
EM 41.9 70.2 52.5
CRF: OL 56.1 81.0 66.3
basic features HM 55.1 79.2 65.0
EM 50.0 72.4 59.2
CRF: OL 59.1 82.4 68.9
basic + IE pattern HM 58.1 80.5 67.5
features EM 52.5 73.3 61.2
CRF-FI: OL 57.7 80.7 67.3
basic features HM 56.8 78.8 66.0
EM 51.7 72.4 60.3
CRF-FI: OL 60.6 81.2 69.4
basic + IE pattern HM 59.5 79.3 68.0
features EM 54.1 72.7 62.0
Table 1: Source identification performance table
labels a noun phrase as a source if it satisfies both
Baseline-1 and Baseline-2?s conditions (this should
satisfy all three conditions described in Section 3.1).
As shown in Table 1, the precision of this approach
is the best of the three baselines, but the recall is the
lowest.
6.2 Extraction Pattern Experiment
We evaluated the performance of the learned extrac-
tion patterns on the source identification task. The
learned patterns were applied to the test data and
the extracted sources were scored against the manual
annotations.6 Table 1 shows that the extraction pat-
terns produced lower recall than the baselines, but
with considerably higher precision. These results
show that the extraction patterns alone can identify
6These results were obtained using the patterns that had a
probability > .50 and frequency > 1.
nearly half of the opinion sources with good accu-
racy.
6.3 CRF Experiments
We developed our CRF model using the MALLET
code from McCallum (2002). For training, we used
a Gaussian prior of 0.25, selected based on the tun-
ing data. We evaluate the CRF using the basic fea-
tures from Section 3, both with and without the IE
pattern features from Section 5. Table 1 shows that
the CRF with basic features outperforms all of the
baselines as well as the extraction patterns, achiev-
ing an F-measure of 66.3 using the OL measure,
65.0 using the HM measure, and 59.2 using the
EM measure. Adding the IE pattern features fur-
ther increases performance, boosting recall by about
3 points for all of the measures and slightly increas-
ing precision as well.
CRF with feature induction. One limitation of
log-linear function models like CRFs is that they
cannot form a decision boundary from conjunctions
of existing features, unless conjunctions are explic-
itly given as part of the feature vector. For the
task of identifying opinion sources, we observed
that the model could benefit from conjunctive fea-
tures. For instance, instead of using two separate
features, HUMAN and PARENT-CHUNK-INCLUDES-
OPINION-EXPRESSION, the conjunction of the two
is more informative.
For this reason, we applied the CRF feature in-
duction approach introduced by McCallum (2003).
As shown in Table 1, where CRF-FI stands for the
CRF model with feature induction, we see consis-
tent improvements by automatically generating con-
junctive features. The final system, which com-
bines the basic features, the IE pattern features,
and feature induction achieves an F-measure of 69.4
(recall=60.6%, precision=81.2%) for the OL mea-
sure, an F-measure of 68.0 (recall=59.5%, preci-
sion=79.3%) for the HM measure, and an F-measure
of 62.0 (recall=54.1%, precision=72.7%) for the EM
measure.
6.4 Error Analysis
An analysis of the errors indicated some common
mistakes:
? Some errors resulted from error propagation in
360
our subsystems. Errors from the sentence bound-
ary detector in GATE (Cunningham et al, 2002)
were especially problematic because they caused
the Collins parser to fail, resulting in no depen-
dency tree information.
? Some errors were due to complex and unusual
sentence structure, which our rather simple fea-
ture encoding for CRF could not capture well.
? Some errors were due to the limited coverage of
the opinion lexicon. We failed to recognize some
cases when idiomatic or vague expressions were
used to express opinions.
Below are some examples of errors that we found
interesting. Doubly underlined phrases indicate in-
correctly extracted sources (either false positives
or false negatives). Opinion words are singly
underlined.
False positives:
(1) Actually, these three countries do have one common
denominator, i.e., that their values and policies do not
agree with those of the United States and none of them
are on good terms with the United States.
(2) Perhaps this is why Fidel Castro has not spoken out
against what might go on in Guantanamo.
In (1), ?their values and policies? seems like a rea-
sonable phrase to extract, but the annotation does not
mark this as a source, perhaps because it is some-
what abstract. In (2), ?spoken out? is negated, which
means that the verb phrase does not bear an opinion,
but our system failed to recognize the negation.
False negatives:
(3) And for this reason, too, they have a moral duty to
speak out, as Swedish Foreign Minister Anna Lindh,
among others, did yesterday.
(4) In particular, Iran and Iraq are at loggerheads with
each other to this day.
Example (3) involves a complex sentence structure
that our system could not deal with. (4) involves an
uncommon opinion expression that our system did
not recognize.
7 Related Work
To our knowledge, our research is the first to auto-
matically identify opinion sources using the MPQA
opinion annotation scheme. The most closely re-
lated work on opinion analysis is Bethard et al
(2004), who use machine learning techniques to
identify propositional opinions and their holders
(sources). However, their work is more limited
in scope than ours in several ways. Their work
only addresses propositional opinions, which are
?localized in the propositional argument? of cer-
tain verbs such as ?believe? or ?realize?. In con-
trast, our work aims to find sources for all opinions,
emotions, and sentiments, including those that are
not related to a verb at all. Furthermore, Berthard
et al?s task definition only requires the identifica-
tion of direct sources, while our task requires the
identification of both direct and indirect sources.
Bethard et al evaluate their system on manually
annotated FrameNet (Baker et al, 1998) and Prop-
Bank (Palmer et al, 2005) sentences and achieve
48% recall with 57% precision.
Our IE pattern learner can be viewed as a cross
between AutoSlog (Riloff, 1996a) and AutoSlog-
TS (Riloff, 1996b). AutoSlog is a supervised learner
that requires annotated training data but does not
compute statistics. AutoSlog-TS is a weakly super-
vised learner that does not require annotated data
but generates coarse statistics that measure each pat-
tern?s correlation with relevant and irrelevant docu-
ments. Consequently, the patterns learned by both
AutoSlog and AutoSlog-TS need to be manually re-
viewed by a person to achieve good accuracy. In
contrast, our IE learner, AutoSlog-SE, computes
statistics directly from the annotated training data,
creating a fully automatic variation of AutoSlog.
8 Conclusion
We have described a hybrid approach to the problem
of extracting sources of opinions in text. We cast
this problem as an information extraction task, using
both CRFs and extraction patterns. Our research is
the first to identify both direct and indirect sources
for all types of opinions, emotions, and sentiments.
Directions for future work include trying to in-
crease recall by identifying relationships between
opinions and sources that cross sentence boundaries,
and relationships between multiple opinion expres-
sions by the same source. For example, the fact that
a coreferring noun phrase was marked as a source
in one sentence could be a useful clue for extracting
the source from another sentence. The probability or
the strength of an opinion expression may also play
a useful role in encouraging or suppressing source
extraction.
361
9 Acknowledgments
We thank the reviewers for their many helpful com-
ments, and the Cornell NLP group for their advice
and suggestions for improvement. This work was
supported by the Advanced Research and Develop-
ment Activity (ARDA), by NSF Grants IIS-0208028
and IIS-0208985, and by the Xerox Foundation.
References
C. Baker, C. Fillmore & J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the COLING-ACL.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou & D. Juraf-
sky. 2004. Automatic extraction of opinion propositions and
their holders. In Proceedings of AAAI Spring Symposium on
Exploring Attitude and Affect in Text.
D. Bikel, S. Miller, R. Schwartz & R. Weischedel. 1997.
Nymble: A High-Performance Learning Name-Finder. In
Proceedings of the Fifth Conference on Applied Natural Lan-
guage Processing.
E. Breck & C. Cardie. 2004. Playing the Telephone Game:
Determining the Hierarchical Structure of Perspective and
Speech Expressions. In Proceedings of 20th International
Conference on Computational Linguistics.
C. Cardie, J. Wiebe, T. Wilson & D. Litman. 2004. Low-
level annotations and summary representations of opinions
for multiperspective QA. In New Directions in Question An-
swering. AAAI Press/MIT Press.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
H. Cunningham, D. Maynard, K. Bontcheva & V. Tablan. 2002.
GATE: A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In Proceed-
ings of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
S. Das & M. Chen. 2001. Yahoo for amazon: Extracting market
sentiment from stock message boards. In Proceedings of the
8th Asia Pacific Finance Association Annual Conference.
K. Dave, S. Lawrence & D. Pennock. 2003. Mining the peanut
gallery: Opinion extraction and semantic classification of
product reviews. In International World Wide Web Confer-
ence.
J. Lafferty, A. K. McCallum & F. Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning.
B. Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
A. K. McCallum. 2002. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu.
A. K. McCallum. 2003. Efficiently Inducing Features of Con-
ditional Random Fields. In Conference on Uncertainty in
Artificial Intelligence.
A. K. McCallum & W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields, Feature
Induction and Web-Enhanced Lexicons. In Conference on
Natural Language Learning.
S. Morinaga, K. Yamanishi, K. Tateishi & T. Fukushima 2002.
Mining Product Reputations on the Web. In Proceedings of
the 8th Internatinal Conference on Knowledge Discover and
Data Mining.
M. Palmer, D. Gildea & P. Kingsbury. 2005. The Proposition
Bank: An Annotated Corpus of Semantic Roles. In Compu-
tational Linguistics 31.
B. Pang, L. Lee & S. Vaithyanathan. 2002. Thumbs up? sen-
timent classification using machine learning techniques. In
Proceedings of the 2002 Conference on Empirical Methods
in Natural Language Processing.
B. Pang & L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics.
L. A. Ramshaw & M. P. Marcus. 1995. Nymble: A High-
Performance Learning Name-Finder. In Proceedings of the
3rd Workshop on Very Large Corpora.
E. Riloff. 1996a. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
In Artificial Intelligence, Vol. 85.
E. Riloff. 1996b. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of the 13th National
Conference on Artificial Intelligence.
E. Riloff & J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of 2003 Conference
on Empirical Methods in Natural Language Processing.
E. Riloff & W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems Technical Report UUCS-04-
015, School of Computing, University of Utah.
S. Sarawagi & W. W. Cohen. 2004. Semi-Markov Condi-
tional Random Fields for Information Extraction 18th An-
nual Conference on Neural Information Processing Systems.
P. Turney. 2002. Thumbs up or thumbs down? semantic orien-
tation applied to unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics.
T. Wilson, J. Wiebe & R. Hwa. 2004. Just how mad are you?
finding strong and weak opinion clauses. In Proceedings of
the 9th National Conference on Artificial Intelligence.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe,
Y. Choi, C. Cardie, E. Riloff & S. Patwardhan. 2005. Opin-
ionFinder: A system for subjectivity analysis. Demonstra-
tion Description in Conference on Empirical Methods in
Natural Language Processing.
J. Yi, T. Nasukawa, R. Bunescu & W. Niblack. 2003. Sentiment
Analyzer: Extracting Sentiments about a Given Topic using
Natural Language Processing Techniques. In Proceedings of
the 3rd IEEE International Conference on Data Mining.
H. Yu & V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identify-
ing the polarity of opinion sentences. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser,
D. Litman, D. Pierce, E. Riloff & T. Wilson. 2002. NRRC
Summer Workshop on Multiple-Perspective Question An-
swering: Final Report.
J. Wiebe & E. Riloff. 2005. Creating subjective and objective
sentence classifiers from unannotated texts. Sixth Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics.
J. Wiebe, T. Wilson & C. Cardie. 2005. Annotating expressions
of opinions and emotions in language. Language Resources
and Evaluation, 1(2).
362
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35,
Vancouver, October 2005.
OpinionFinder: A system for subjectivity analysis
Theresa Wilson?, Paul Hoffmann?, Swapna Somasundaran?, Jason Kessler?,
Janyce Wiebe??, Yejin Choi?, Claire Cardie?, Ellen Riloff?, Siddharth Patwardhan?
?Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, Cornell University, Ithaca, NY 14853
?School of Computing, University of Utah, Salt Lake City, UT 84112
{twilson,hoffmanp,swapna,jsk44,wiebe}@cs.pitt.edu,
{ychoi,cardie}@cs.cornell.edu, {riloff,sidd}@cs.utah.edu
1 Introduction
OpinionFinder is a system that performs subjectivity
analysis, automatically identifying when opinions,
sentiments, speculations, and other private states are
present in text. Specifically, OpinionFinder aims to
identify subjective sentences and to mark various as-
pects of the subjectivity in these sentences, includ-
ing the source (holder) of the subjectivity and words
that are included in phrases expressing positive or
negative sentiments.
Our goal with OpinionFinder is to develop a sys-
tem capable of supporting other Natural Language
Processing (NLP) applications by providing them
with information about the subjectivity in docu-
ments. Of particular interest are question answering
systems that focus on being able to answer opinion-
oriented questions, such as the following:
How is Bush?s decision not to ratify the
Kyoto Protocol looked upon by Japan and
other US allies?
How do the Chinese regard the human
rights record of the United States?
To answer these types of questions, a system needs
to be able to identify when opinions are expressed in
text and who is expressing them. Other applications
that would benefit from knowledge of subjective lan-
guage include systems that summarize the various
viewpoints in a document or that mine product re-
views. Even typical fact-oriented applications, such
as information extraction, can benefit from subjec-
tivity analysis by filtering out opinionated sentences
(Riloff et al, 2005).
2 OpinionFinder
OpinionFinder runs in two modes, batch and inter-
active. Document processing is largely the same for
both modes. In batch mode, OpinionFinder takes a
list of documents to process. Interactive mode pro-
vides a front-end that allows a user to query on-line
news sources for documents to process.
2.1 System Architecture Overview
OpinionFinder operates as one large pipeline. Con-
ceptually, the pipeline can be divided into two parts.
The first part performs mostly general purpose doc-
ument processing (e.g., tokenization and part-of-
speech tagging). The second part performs the sub-
jectivity analysis. The results of the subjectivity
analysis are returned to the user in the form of
SGML/XML markup of the original documents.
2.2 Document Processing
For general document processing, OpinionFinder
first runs the Sundance partial parser (Riloff and
Phillips, 2004) to provide semantic class tags, iden-
tify Named Entities, and match extraction patterns
that correspond to subjective language (Riloff and
Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tok-
enize, sentence split, and part-of-speech tag the data,
and the Abney stemmer2 is used to stem. In batch
mode, OpinionFinder parses the data again, this time
to obtain constituency parse trees (Collins, 1997),
which are then converted to dependency parse trees
(Xia and Palmer, 2001). Currently, this stage is only
1http://opennlp.sourceforge.net/
2SCOL version 1g available at http://www.vinartus.net/spa/
34
available for batch mode processing due to the time
required for parsing. Finally, a clue-finder is run to
identify words and phrases from a large subjective
language lexicon.
2.3 Subjectivity Analysis
The subjectivity analysis has four components.
2.3.1 Subjective Sentence Classification
The first component is a Naive Bayes classifier
that distinguishes between subjective and objective
sentences using a variety of lexical and contextual
features (Wiebe and Riloff, 2005; Riloff and Wiebe,
2003). The classifier is trained using subjective and
objective sentences, which are automatically gener-
ated from a large corpus of unannotated data by two
high-precision, rule-based classifiers.
2.3.2 Speech Events and Direct Subjective
Expression Classification
The second component identifies speech events
(e.g., ?said,? ?according to?) and direct subjective
expressions (e.g., ?fears,? ?is happy?). Speech
events include both speaking and writing events.
Direct subjective expressions are words or phrases
where an opinion, emotion, sentiment, etc. is di-
rectly described. A high-precision, rule-based clas-
sifier is used to identify these expressions.
2.3.3 Opinion Source Identification
The third component is a source identifier that
combines a Conditional Random Field sequence
tagging model (Lafferty et al, 2001) and extraction
pattern learning (Riloff, 1996) to identify the sources
of speech events and subjective expressions (Choi
et al, 2005). The source of a speech event is the
speaker; the source of a subjective expression is the
experiencer of the private state. The source identifier
is trained on the MPQA Opinion Corpus3 using a
variety of features. Because the source identifier re-
lies on dependency parse information, it is currently
only available in batch mode.
2.3.4 Sentiment Expression Classification
The final component uses two classifiers to iden-
tify words contained in phrases that express pos-
itive or negative sentiments (Wilson et al, 2005).
3The MPQA Opinion Corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
The first classifier focuses on identifying sentiment
expressions. The second classifier takes the senti-
ment expressions and identifies those that are pos-
itive and negative. Both classifiers were developed
using BoosTexter (Schapire and Singer, 2000) and
trained on the MPQA Corpus.
3 Related Work
Please see (Wiebe and Riloff, 2005; Choi et al,
2005; Wilson et al, 2005) for discussions of related
work in automatic opinion and sentiment analysis.
4 Acknowledgments
This work was supported by the Advanced Research
and Development Activity (ARDA), by the NSF
under grants IIS-0208028, IIS-0208798 and IIS-
0208985, and by the Xerox Foundation.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In HLT/EMNLP 2005.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-2001.
E. Riloff and W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems. Technical Report UUCS-04-
015, School of Computing, University of Utah.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction. In
AAAI-2005.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT/EMNLP 2005.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
35
WordNet::Similarity - Measuring the Relatedness of Concepts
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84102
sidd@cs.utah.edu
http://search.cpan.org/dist/WordNet-Similarity
http://wn-similarity.sourceforge.net
Jason Michelizzi
Department of Computer Science
University of Minnesota
Duluth, MN 55812
mich0212@d.umn.edu
Abstract
WordNet::Similarity is a freely available soft-
ware package that makes it possible to mea-
sure the semantic similarity and relatedness be-
tween a pair of concepts (or synsets). It pro-
vides six measures of similarity, and three mea-
sures of relatedness, all of which are based on
the lexical database WordNet. These measures
are implemented as Perl modules which take
as input two concepts, and return a numeric
value that represents the degree to which they
are similar or related.
1 Introduction
WordNet::Similarity implements measures of similarity
and relatedness that are all in some way based on the
structure and content of WordNet.
Measures of similarity use information found in an is?
a hierarchy of concepts (or synsets), and quantify how
much concept A is like (or is similar to) concept B. For
example, such a measure might show that an automobile
is more like a boat than it is a tree, due to the fact that
automobile and boat share vehicle as an ancestor in the
WordNet noun hierarchy.
WordNet is particularly well suited for similarity mea-
sures, since it organizes nouns and verbs into hierarchies
of is?a relations. In version 2.0, there are nine separate
noun hierarchies that include 80,000 concepts, and 554
verb hierarchies that are made up of 13,500 concepts.
Is?a relations in WordNet do not cross part of speech
boundaries, so similarity measures are limited to mak-
ing judgments between noun pairs (e.g., cat and dog) and
verb pairs (e.g., run and walk). While WordNet alo in-
cludes adjectives and adverbs, these are not organized
into is?a hierarchies so similarity measures can not be
applied.
However, concepts can be related in many ways be-
yond being similar to each other. For example, a wheel is
a part of a car, night is the opposite of day, snow is made
up of water, a knife is used to cut bread, and so forth. As
such WordNet provides relations beyond is?a, including
has?part, is?made?of, and is?an?attribute?of. In addi-
tion, each concept is defined by a short gloss that may
include an example usage. All of this information can be
brought to bear in creating measures of relatedness. As
a result these measures tend to be more flexible, and al-
low for relatedness values to be assigned across parts of
speech (e.g., the verb murder and the noun gun).
This paper continues with an overview of the mea-
sures supported in WordNet::Similarity, and then pro-
vides a brief description of how the package can be used.
We close with a summary of research that has employed
WordNet::Similarity.
2 Similarity Measures
Three of the six measures of similarity are based on the
information content of the least common subsumer (LCS)
of concepts A and B. Information content is a measure of
the specificity of a concept, and the LCS of concepts A
and B is the most specific concept that is an ancestor of
both A and B. These measures include res (Resnik, 1995),
lin (Lin, 1998), and jcn (Jiang and Conrath, 1997).
The lin and jcn measures augment the information con-
tent of the LCS with the sum of the information content
of concepts A and B themselves. The lin measure scales
the information content of the LCS by this sum, while
jcn takes the difference of this sum and the information
content of the LCS.
The default source for information content for concepts
is the sense?tagged corpus SemCor. However, there are
also utility programs available with WordNet::Similarity
that allow a user to compute information content values
from the Brown Corpus, the Penn Treebank, the British
National Corpus, or any given corpus of raw text.
> similarity.pl --type WordNet::Similarity::lin car#n#2 bus#n#1
car#n#2 bus#n#1 0.530371390319309 # railway car versus motor coach
> similarity.pl --type WordNet::Similarity::lin car#n bus#n
car#n#1 bus#n#1 0.618486790769613 # automobile versus motor coach
> similarity.pl --type WordNet::Similarity::lin --allsenses car#n bus#n#1
car#n#1 bus#n#1 0.618486790769613 # automobile versus motor coach
car#n#2 bus#n#1 0.530371390319309 # railway car versus motor coach
car#n#3 bus#n#1 0.208796988315133 # cable car versus motor coach
Figure 1: Command Line Interface
Three similarity measures are based on path lengths
between a pair of concepts: lch (Leacock and Chodorow,
1998), wup (Wu and Palmer, 1994), and path. lch finds
the shortest path between two concepts, and scales that
value by the maximum path length found in the is?a hi-
erarchy in which they occur. wup finds the depth of the
LCS of the concepts, and then scales that by the sum of
the depths of the individual concepts. The depth of a con-
cept is simply its distance to the root node. The measure
path is a baseline that is equal to the inverse of the short-
est path between two concepts.
WordNet::Similarity supports two hypothetical root
nodes that can be turned on and off. When on, one root
node subsumes all of the noun concepts, and another sub-
sumes all of the verb concepts. This allows for similarity
measures to be applied to any pair of nouns or verbs. If
the hypothetical root nodes are off, then concepts must
be in the same physical hierarchy for a measurement to
be taken.
3 Measures of Relatedness
Measures of relatedness are more general in that they can
be made across part of speech boundaries, and they are
not limited to considering is-a relations. There are three
such measures in the package: hso (Hirst and St-Onge,
1998), lesk (Banerjee and Pedersen, 2003), and vector
(Patwardhan, 2003).
The hso measures classifies relations in WordNet as
having direction, and then establishes the relatedness be-
tween two concepts A and B by finding a path that is
neither too long nor that changes direction too often.
The lesk and vector measures incorporate information
from WordNet glosses. The lesk measure finds overlaps
between the glosses of concepts A and B, as well as con-
cepts that are directly linked to A and B. The vector mea-
sure creates a co?occurrence matrix for each word used in
the WordNet glosses from a given corpus, and then repre-
sents each gloss/concept with a vector that is the average
of these co?occurrence vectors.
4 Using WordNet::Similarity
WordNet::Similarity can be utilized via a command line
interface provided by the utility program similarity.pl.
This allows a user to run the measures interactively. In
addition, there is a web interface that is based on this
utility. WordNet::Similarity can also be embedded within
Perl programs by including it as a module and calling its
methods.
4.1 Command Line
The utility similarity.pl allows a user to measure specific
pairs of concepts when given in word#pos#sense form.
For example, car#n#3 refers to the third WordNet noun
sense of car. It also allows for the specification of all
the possible senses associated with a word or word#pos
combination.
For example, in Figure 1, the first command requests
the value of the lin measure of similarity for the second
noun sense of car (railway car) and the first noun sense of
bus (motor coach). The second command will return the
score of the pair of concepts that have the highest similar-
ity value for the nouns car and bus. In the third command,
the ?allsenses switch causes the similarity measurements
of all the noun senses of car to be calculated relative to
the first noun sense of bus.
4.2 Programming Interface
WordNet::Similarity is implemented with Perl?s object
oriented features. It uses the WordNet::QueryData pack-
age (Rennie, 2000) to create an object representing Word-
Net. There are a number of methods available that allow
for the inclusion of existing measures in Perl source code,
and also for the development of new measures.
When an existing measure is to be used, an object of
that measure must be created via the new() method. Then
the getRelatedness() method can be called for a pair of
word senses, and this will return the relatedness value.
For example, the program in Figure 2 creates an object of
the lin measure, and then finds the similarity between the
#!/usr/bin/perl -w
use WordNet::QueryData; # use interface to WordNet
use WordNet::Similarity::lin; # use Lin measure
$wnObj = new WordNet::QueryData; # create a WordNet object
$linObj = new WordNet::Similarity::lin($wnObj); # create a lin object
$value = $linObj -> getRelatedness (?car#n#1?, ?bus#n#2?); # how similar?
Figure 2: Programming Interface
first sense of the noun car (automobile) and the second
sense of the noun bus (network bus).
WordNet::Similarity enables detailed tracing that
shows a variety of diagnostic information specific to each
of the different kinds of measures. For example, for the
measures that rely on path lengths (lch, wup, path) the
tracing shows all the paths found between the concepts.
Tracing for the information content measures (res, lin,
jcn) includes both the paths between concepts as well as
the least common subsumer. Tracing for the hso measure
shows the actual paths found through WordNet, while
the tracing for lesk shows the gloss overlaps in Word-
Net found for the two concepts and their nearby relatives.
The vector tracing shows the word vectors that are used
to create the gloss vector of a concept.
5 Software Architecture
Similarity.pm is the super class of all modules, and pro-
vides general services used by all of the measures such as
validation of synset identifier input, tracing, and caching
of results. There are four modules that provide all of the
functionality required by any of the supported measures:
PathFinder.pm, ICFinder.pm, DepthFinder.pm, and LCS-
Finder.pm.
PathFinder.pm provides getAllPaths(), which finds all
of the paths and their lengths between two input synsets,
and getShortestPath() which determines the length of the
shortest path between two concepts.
ICFinder.pm includes the method IC(), which gets the
information content value of a synset. probability() and
getFrequency() find the probability and frequency count
of a synset based on whatever corpus has been used to
compute information content. Note that these values are
pre?computed, so these methods are simply reading from
an information content file.
DepthFinder.pm provides methods that read values that
have been pre?computed by the wnDepths.pl utility. This
program finds the depth of every synset in WordNet,
and also shows the is?a hierarchy in which a synset oc-
curs. If a synset has multiple parents, then each possible
depth and home hierarchy is returned. The depth of a
synset is returned by getDepthOfSynset() and getTaxono-
myDepth() provides the maximum depth for a given is?a
hierarchy.
LCSFinder.pm provides methods that find the least
common subsumer of two concepts using three differ-
ent criteria. These are necessary since there is multiple
inheritance of concepts in WordNet, and different LCS
can be selected for a pair of concepts if one or both of
them have multiple parents in an is?a hiearchy. getLCS-
byIC() chooses the LCS for a pair of concepts that has the
highest information content, getLCSbyDepth() selects the
LCS with the greatest depth, and getLCSbyPath() selects
the LCS that results in the shortest path.
6 Related Work
Our work with measures of semantic similarity and relat-
edness began while adapting the Lesk Algorithm for word
sense disambiguation to WordNet (Banerjee and Peder-
sen, 2002). That evolved in a generalized approach to
disambiguation based on semantic relatedness (Patward-
han et al, 2003) that is implemented in the SenseRe-
late package (http://senserelate.sourceforge.net), which
utilizes WordNet::Similarity. The premise behind this al-
gorithm is that the sense of a word can be determined by
finding which of its senses is most related to the possible
senses of its neighbors.
WordNet::Similarity has been used by a number of
other researchers in an interesting array of domains.
(Zhang et al, 2003) use it as a source of semantic fea-
tures for identifying cross?document structural relation-
ships between pairs of sentences found in related docu-
ments. (McCarthy et al, 2004) use it in conjunction with
a thesaurus derived from raw text in order to automati-
cally identify the predominent sense of a word. (Jarmasz
and Szpakowicz, 2003) compares measures of similarity
derived from WordNet and Roget?s Thesaurus. The com-
parisons are based on correlation with human related-
ness values, as well as the TOEFL synonym identification
tasks. (Baldwin et al, 2003) use WordNet::Similarity to
provide an evaluation tool for multiword expressions that
are identified via Latent Semantic Analysis. (Diab, 2003)
combines a number of similarity measures that are then
used as a feature in the disambiguation of verb senses.
7 Availability
WordNet::Similarity is written in Perl and is freely dis-
tributed under the Gnu Public License. It is avail-
able from the Comprehensive Perl Archive Network
(http://search.cpan.org/dist/WordNet-Similarity) and via
SourceForge, an Open Source development platform
(http://wn-similarity.sourceforge.net).
8 Acknowledgements
WordNet::Similarity was preceeded by the distance.pl
program, which was released in June 2002. This was
converted into the object oriented WordNet::Similarity
package, which was first released in April 2003 as ver-
sion 0.03. The most current version as of this writing is
0.07, which was released in March 2004.
The distance.pl program and all versions of Word-
Net::Similarity up to and including 0.06 were designed
and implemented by Siddharth Patwardhan as a part of
his Master?s thesis at the University of Minnesota, Du-
luth. Version 0.07 was designed and implemented by Ja-
son Michelizzi as a part of his Master?s thesis.
The lesk measure in WordNet::Similarity was origi-
nally designed and implemented by Satanjeev Banerjee,
who developed this measure as a part of his Master?s
thesis at the University of Minnesota, Duluth. There-
after Siddharth Patwardhan ported this measure to Word-
Net::Similarity.
This work has been partially supported by a National
Science Foundation Faculty Early CAREER Develop-
ment award (#0092784), and by a Grant-in-Aid of Re-
search, Artistry and Scholarship from the Office of the
Vice President for Research and the Dean of the Gradu-
ate School of the University of Minnesota.
References
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows.
2003. An empirical model of multiword expression
decomposability. In Proceedings of the of the ACL-
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, pages 89?96, Sapporo,
Japan.
S. Banerjee and T. Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using Word-
Net. In Proceedings of the Third International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, pages 136?145, Mexico City, February.
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco,
August.
M. Diab. 2003. Word Sense Disambiguation within a
Multilingual Framework. Ph.D. thesis, The University
of Maryland.
G. Hirst and D. St-Onge. 1998. Lexical chains as repre-
sentations of context for the detection and correction
of malapropisms. In C. Fellbaum, editor, WordNet:
An electronic lexical database, pages 305?332. MIT
Press.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s thesaurus
and semantic similarity. In Proceedings of the Con-
ference on Recent Advances in Natural Language Pro-
cessing, pages 212?219, Borovets, Bulgaria.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings on International Conference on Research in Com-
putational Linguistics, pages 19?33, Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identifi-
cation. In C. Fellbaum, editor, WordNet: An electronic
lexical database, pages 265?283. MIT Press.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the International Con-
ference on Machine Learning, Madison, August.
D. McCarthy, R. Koeling, and J. Weeds. 2004. Rank-
ing WordNet senses automatically. Technical Report
CSRP 569, University of Sussex, January.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, pages 241?257, Mexico
City, February.
S. Patwardhan. 2003. Incorporating dictionary and cor-
pus information into a context vector measure of se-
mantic relatedness. Master?s thesis, University of Min-
nesota, Duluth, August.
J. Rennie. 2000. WordNet::QueryData: a Perl
module for accessing the WordNet database.
http://www.ai.mit.edu/people/jrennie/WordNet.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
the 14th International Joint Conference on Artificial
Intelligence, pages 448?453, Montreal, August.
Z. Wu and M. Palmer. 1994. Verb semantics and lexi-
cal selection. In 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 133?138,
Las Cruces, New Mexico.
Z. Zhang, J. Otterbacher, and D. Radev. 2003. Learning
cross-document structural relationships using boost-
ing. In Proceedings of the 12th International Con-
ference on Information and Knowledge Management,
pages 124?130.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 73?76, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SenseRelate::TargetWord ? A Generalized Framework
for Word Sense Disambiguation
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
sidd@cs.utah.edu
Satanjeev Banerjee
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15213
satanjeev@cmu.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Abstract
We have previously introduced a method
of word sense disambiguation that com-
putes the intended sense of a target word,
using WordNet-based measures of seman-
tic relatedness (Patwardhan et al, 2003).
SenseRelate::TargetWord is a Perl pack-
age that implements this algorithm. The
disambiguation process is carried out by
selecting that sense of the target word
which is most related to the context words.
Relatedness between word senses is mea-
sured using the WordNet::Similarity Perl
modules.
1 Introduction
Many words have different meanings when used in
different contexts. Word Sense Disambiguation is
the task of identifying the intended meaning of a
given target word from the context in which it is
used. (Lesk, 1986) performed disambiguation by
counting the number of overlaps between the dic-
tionary definitions (i.e., glosses) of the target word
and those of the neighboring words in the con-
text. (Banerjee and Pedersen, 2002) extended this
method of disambiguation by expanding the glosses
of words to include glosses of related words, accord-
ing to the structure of WordNet (Fellbaum, 1998).
In subsequent work, (Patwardhan et al, 2003) and
(Banerjee and Pedersen, 2003) proposed that mea-
suring gloss overalps is just one way of determin-
ing semantic relatedness, and that word sense dis-
ambiguation can be performed by finding the most
related sense of a target word to its surrounding con-
text using a wide variety of measures of relatedness.
SenseRelate::TargetWord is a Perl package that
implements these ideas, and is able to disambiguate
a target word in context by finding the sense that is
most related to its neighbors according to a speci-
fied measure. A user of this package is able to make
a variety of choices for text preprocessing options,
context selection, relatedness measure selection and
the selection of an algorithm for computing the over-
all relatedness between each sense of the target word
and words in the surrounding context. The user can
customize each of these choices to fit the needs of
her specific disambiguation task. Further, the vari-
ous sub-tasks in the package are implemented in a
modular fashion, allowing the user to easily replace
a module with her own module if needed.
The following sections describe the generalized
framework for Word Sense Disambiguation, the ar-
chitecture and usage of SenseRelate::TargetWord,
and a description of the user interfaces (command
line and GUI).
2 The Framework
The package has a highly modular architecture. The
disambiguation process is divided into a number of
smaller sub-tasks, each of which is represented by
a separate module. Each of the sequential sub-tasks
or stages accepts data from a previous stage, per-
forms a transformation on the data, and then passes
on the processed data structures to the next stage in
the pipeline. We have created a protocol that defines
the structure and format of the data that is passed
between the stages. The user can create her own
73
Relatedness 
Measure
Context
Target Sense
Preprocessing
Format Filter
Sense Inventory
Context Selection Postprocessing
Pick Sense
Figure 1: A generalized framework for Word Sense Disambiguation.
modules to perform any of these sub-tasks as long
as the modules adhere to the protocol laid down by
the package.
Figure 1 projects an overview of the architecture
of the system and shows the various sub-tasks that
need to be performed to carry out word sense dis-
ambiguation. The sub-tasks in the dotted boxes are
optional. Further, each of the sub-tasks can be per-
formed in a number of different ways, implying that
the package can be customized in a large number of
ways to suit different disambiguation needs.
2.1 Format Filter
The filter takes as input file(s) annotated in the
SENSEVAL-2 lexical sample format, which is an
XML?based format that has been used for both the
SENSEVAL-2 and SENSEVAL-3 exercises. A file in
this format includes a number of instances, each one
made up of 2 to 3 lines of text where a single tar-
get word is designated with an XML tag. The fil-
ter parses the input file to build data structures that
represent the instances to be disambiguated, which
includes a single target word and the surrounding
words that define the context.
2.2 Preprocessing
SenseRelate::TargetWord expects zero or more text
preprocessing modules, each of which perform a
transformation on the input words. For example, the
Compound Detection Module identifies sequences
of tokens that form compound words that are known
as concepts to WordNet (such as ?New York City?).
In order to ensure that compounds are treated as a
single unit, the package replaces them in the instance
with the corresponding underscore?connected form
(?New York City?).
Multiple preprocessing modules can be chained
together, the output of one connected to the input of
the next, to form a single preprocessing stage. For
example, a part of speech tagging module could be
added after compound detection.
2.3 Context Selection
Disambiguation is performed by finding the sense of
the target word that is most related to the words in
its surrounding context. The package allows for var-
ious methods of determining what exactly the sur-
rounding context should consist of. In the current
implementation, the context selection module uses
an n word window around the target word as con-
text. The window includes the target word, and ex-
tends to both the left and right. The module selects
the n? 1 words that are located closest to the target
word, and sends these words (and the target) on to
the next module for disambiguation. Note that these
words must all be known to WordNet, and should
not include any stop?words.
However, not all words in the surrounding context
are indicative of the correct sense of the target word.
An intelligent selection of the context words used in
the disambiguation process could potentially yield
much better results and generate a solution faster
than if all the nearby words were used. For exam-
ple, we could instead select the nouns from the win-
dow of context that have a high term?frequency to
document?frequency ratio. Or, we could identify
lexical chains in the surrounding context, and only
include those words that are found in chains that in-
clude the target word.
74
2.4 Sense Inventory
After having reduced the context to n words, the
Sense Inventory stage determines the possible senses
of each of the n words. This list can be obtained
from a dictionary, such as WordNet. A thesaurus
could also be used for the purpose. Note however,
that the subsequent modules in the pipeline should
be aware of the codes assigned to the word senses.
In our system, this module first decides the base
(uninflected) form of each of the n words. It then
retrieves all the senses for each word from the sense
inventory. We use WordNet for our sense inventory.
2.5 Postprocessing
Some optional processing can be performed on the
data structures generated by the Sense Inventory
module. This would include tasks such as sense
pruning, which is the process of removing some
senses from the inventory, based on simple heuris-
tics, algorithms or options. For example, the user
may decide to preclude all verb senses of the target
word from further consideration in the disambigua-
tion process.
2.6 Identifying the Sense
The disambiguation module takes the lists of senses
of the target word and those of the context words and
uses this information to pick one sense of the tar-
get word as the answer. Many different algorithms
could be used to do this. We have modules Local
and Global that (in different ways) determine the re-
latedness of each of the senses of the target word
with those of the context words, and pick the most
related sense as the answer. These are described
in greater detail by (Banerjee and Pedersen, 2002),
but in general the Local method compares the target
word to its neighbors in a pair-wise fashion, while
the Global method carries out an exhaustive compar-
ison between all the senses of the target word and all
the senses of the neighbors.
3 Using SenseRelate::TargetWord
SenseRelate::TargetWord can be used via the
command-line interface provided by the utility pro-
gram called disamb.pl. It provides a rich variety of
options for controlling the process of disambigua-
tion. Or, it can be embedded into Perl programs,
by including it as a module and calling its various
methods. Finally, there is a graphical interface to
the package that allows a user to highlight a word in
context to be disambiguated.
3.1 Command Line
The command-line interface disamb.pl takes as input
a SENSEVAL-2 formatted lexical sample file. The
program disambiguates the marked up word in each
instance and prints to screen the instance ID, along
with the disambiguated sense of the target word.
Command line options are available to control the
disambiguation process. For example, a user can
specify which relatedness measure they would like
to use, whether disambiguation should be carried out
using Local or Global methods, how large a win-
dow of context around the target word is to be used,
and whether or not all the parts of speech of a word
should be considered.
3.2 Programming Interface
SenseRelate::TargetWord is distributed as a Perl
package. It is programmed in object-oriented Perl
as a group of Perl classes. Objects of these classes
can be instantiated in user programs, and meth-
ods can be called on these objects. The pack-
age requires that the Perl interface to WordNet,
WordNet::QueryData1 be installed on the system.
The disambiguation algorithms also require that the
semantic relatedness measures WordNet::Similarity
(Pedersen et al, 2004) be installed.
3.3 Graphical User Interface
We have developed a graphical interface for the
package in order to conveniently access the disam-
biguation modules. The GUI is written in Gtk-Perl
? a Perl API to the Gtk toolkit. Unlike the command
line interface, the graphical interface is not tied to
any input file format. The interface allows the user to
input text, and to select the word to disambiguate. It
also provides the user with numerous configuration
options corresponding to the various customizations
described above.
1http://search.cpan.org/dist/WordNet-QueryData
75
4 Related Work
There is a long history of work in Word Sense Dis-
ambiguation that uses Machine Readable Dictionar-
ies, and are highly related to our approach.
One of the first approaches was that of (Lesk,
1986), which treated every dictionary definition of
a concept as a bag of words. To identify the in-
tended sense of the target word, the Lesk algorithm
would determine the number of word overlaps be-
tween the definitions of each of the meanings of the
target word, and those of the context words. The
meaning of the target word with maximum defini-
tion overlap with the context words was selected as
the intended sense.
(Wilks et al, 1993) developed a context vector
approach for performing word sense disambigua-
tion. Their algorithm built co-occurrence vectors
from dictionary definitions using Longman?s Dictio-
nary of Contemporary English (LDOCE). They then
determined the extent of overlap between the sum of
the vectors of the words in the context and the sum
of the vectors of the words in each of the definitions
(of the target word). For vectors, the extent of over-
lap is defined as the dot product of the vectors. The
meaning of the target word that had the maximum
overlap was selected as the answer.
More recently, (McCarthy et al, 2004) present a
method that performs disambiguation by determing
the most frequent sense of a word in a particular do-
main. This is based on measuring the relatedness
of the different possible senses of a target word (us-
ing WordNet::Similarity) to a set of words associated
with a particular domain that have been identified
using distributional methods. The relatedness scores
between a target word and the members of this set
are scaled by the distributional similarity score.
5 Availability
SenseRelate::TargetWord is written in Perl and is
freely distributed under the Gnu Public License. It
is available via SourceForge, an Open Source de-
velopment platform2, and the Comprehensive Perl
Archive Network (CPAN)3.
2http://senserelate.sourceforge.net
3http://search.cpan.org/dist/WordNet-SenseRelate-
TargetWord
6 Acknowledgements
This research is partially supported by a National
Science Foundation Faculty Early CAREER Devel-
opment Award (#0092784).
References
S. Banerjee and T. Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using Word-
Net. In Proceedings of the Third International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, Mexico City, February.
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Pro-
ceedings of the Eighteenth International Conference
on Artificial Intelligence (IJCAI-03), Acapulco, Mex-
ico, August.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from a ice cream cone. In Proceedings of SIGDOC
?86.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics (CICLING-03), Mex-
ico City, Mexico, February.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::Similarity - Measuring the Re-
latedness of Concepts. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and
B. Slator. 1993. Providing machine tractable dictio-
nary tools. In J. Pustejovsky, editor, Semantics and
the Lexicon. Kluwer Academic Press, Dordrecht and
Boston.
76
Generative Models for Semantic Role Labeling
Cynthia A. Thompson
cindi@cs.utah.edu
Siddharth Patwardhan
sidd@cs.utah.edu
School of Computing
University of Utah
Salt Lake City, UT 84112
Carolin Arnold
csarnold@cs.utah.edu
Abstract
This paper describes the four entries from the Uni-
versity of Utah in the semantic role labeling task
of SENSEVAL-3. All the entries took a statisti-
cal machine learning approach, using the subset
of the FrameNet corpus provided by SENSEVAL-3
as training data. Our approach was to develop a
model of natural language generation from seman-
tics, and train the model using maximum likelihood
and smoothing. Our models performed satisfacto-
rily in the competition, and can flexibly handle vary-
ing permutations of provided versus inferred infor-
mation.
1 Introduction
The goal in the SENSEVAL-3 semantic role labeling
task is to identify roles and optionally constituent
boundaries for each role, given a natural language
sentence, target, and frame. The Utah approach to
this task is to apply machine learning techniques
to create a model capable of semantically analyz-
ing unseen sentences. We have developed a set of
generative models (Jordan, 1999) that have the ad-
vantages of flexibility, power, and ease of applica-
bility for semi-supervised learning scenarios. We
can supplement any of the generative models with
a constituent classifier that determines, given a sen-
tence and parse, which parse constituents are most
likely to correspond to a role. We apply the com-
bination to the ?hard,? or restricted version of the
role labeling task, in which the system is provided
only with the sentence, target, and frame, and must
determine which sentence constituents to label with
roles.
We discuss our overall model, the constituent
classifier we use in the hard task, and the classifier?s
use at role-labeling time. We entered four sets of
answers, as discussed in Section 5. The first two
correspond to the ?easy? task, in which the role-
bearing constituents ? those parts of the sentence
corresponding to a role ? are provided to the system
with the target and frame. The second two are vari-
ants for the ?hard? task. Finally, we discuss Future
Work and conclude the paper.
2 Role Labeler
Our general approach is to use a generative model
defining a joint probability distribution over targets,
frames, roles, and constituents. The advantage of
such a model is its generality: it can determine the
probability of any subset of the variables given val-
ues for the others. Three of our entries used the
generative model illustrated in Figure 1, and the
fourth used a model grouping all roles together, as
described further below. The first model functions
T
F
R
1
R
2
   R
n
C
1
C
2
   C
n
Figure 1: First Order Model.
as follows. First, a target, T , is chosen, which then
generates a frame, F . The frame generates a (lin-
earized) role sequence, R
1
through R
n
which in
turn generates each constituent of the sentence, C
1
through C
n
. Note that, conditioned on a particular
frame, the model is just a first-order Hidden Markov
Model.
The second generative model treats all roles as a
group. It is no longer based on a Hidden Markov
model, but all roles are generated, in order, simul-
taneously. Therefore, the role sequence in Figure 1
is replaced by a single node containing all n roles.
This can be compared to a case-based approach that
memorizes all seen role sequences and calculates
their likelihood. It is also similar to Gildea & Juraf-
sky?s (2002) frame element groups, though we dis-
tinguish between different role orderings, whereas
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
they do not. However, we still model constituent
generation sequentially.
The FrameNet corpus contains annotations for all
of the model components described above. We rep-
resent each constituent by its phrasal category to-
gether with its head word. As in Gildea & Jurafsky?s
(2002) approach, we determine head words from the
sentence?s syntactic parse, using a simple heuristic1
when syntactic alignment with a parse is not avail-
able.
We estimate most of the model parameters us-
ing a straightforward maximum likelihood estimate
based on fully labeled training data. We smooth
emission probabilities with phrase type labels, due
to the sparseness of head words. To label a test ex-
ample, consisting of a target, frame, and constituent
sequence, with a role label, we use the Viterbi algo-
rithm. For further details, see Thompson (2003).
3 Constituent Classification for Role
Labeling
To address the ?hard? task we build a constituent
classifier, whose goal is to detect the role-bearing
constituents of a sentence. We use a Naive Bayes
classifier from the Weka Machine Learning toolkit
(Witten and Frank, 2000) to classify every sentence
constituent as role-bearing or not. In our cross-
validation studies, Naive Bayes was both accurate
and efficient. To generate the training examples for
the classifier, we generate a parse tree for every sen-
tence in the SENSEVAL-3 training data, using the
Collins (1996) statistical parser. We call each node
in this tree a constituent. Once it is trained, the clas-
sifier can sift through a new constituent list and de-
cide which are likely to be role-bearing. The se-
lected constituents are passed on to the Role La-
beler for labeling with semantic roles, as described
in Section 4.
We train the classifier on examples extracted from
the SENSEVAL-3 training data. Each example is a
list of attributes corresponding to a constituent in
a sentence?s parse tree, along with its classification
as role-bearing or not. We extract the attributes by
traversing each sentence?s parse tree from the root
node down to nodes all of whose children are pre-
terminals.2 We create a training example for every
visited node.
We decided to use the following attributes from
the parse trees and FrameNet examples:
Target Position: The position of the target word as
1The heuristic chooses the preposition for PP?s and the last
word of the phrase for all other phrases.
2We later fixed this to traverse the tree to the pre-terminals
themselves, as discussed further in Section 5.
being BEFORE, AFTER, or CONTAINS (contained
in) the constituent.
Distance from target: The number of words be-
tween the start of the constituent and target word.
Depth: The depth of the constituent in the parse
tree.
Height: The number of levels in the parse tree be-
low the constituent.
Word Count: The number of words in the con-
stituent.
Path to Target: Gildea and Jurafsky (2002) show
that the path from a constituent node to the node
corresponding to the target word is a good indicator
that a constituent corresponds to a role. We use the
35 most frequently occurring paths in the training
corpus as attribute values, as these cover about 68%
of the paths in the training corpus. The remaining
paths are specified as ?OTHER?.
Length of Path to Target: The number of nodes
between the constituent and the target in the path.
Constituent Phrase Type
Target POS: The target word?s part-of-speech ?
noun, verb, or adjective.
Frame ID
By generating examples in the manner described
above, we create a data set that is heavily biased
towards negative examples ? 90.8% of the con-
stituents are not role bearing. Therefore, the classi-
fier can obtain high accuracy by labeling everything
as negative. This is undesirable since then no con-
stituents would be passed to the Role Labeler. How-
ever, passing all constituents to the labeler would
cause it to try to label all of them and thus achieve
lower accuracy. This results in the classic precision-
recall tradeoff. We chose to try to bias the classifier
towards high recall by using a cost matrix that pe-
nalizes missed positive examples more than missed
negatives. The resulting classifier?s cross-validation
precision was 0.19 and its recall was 0.91. If we
do not use the cost matrix, the precision is 0.30 and
the recall is 0.82. We are still short of our goal of
perfect recall and reasonable precision, but this pro-
vides a good filtering mechanism for the next step
of role labeling.
4 Combining Constituent Classification
with Role Labeling
The constituent classifier correctly picks out most of
the role bearing constituents. However, as we have
seen, it still omits some constituents and, as it was
designed to, includes several irrelevant constituents
per sentence. For this paper, because we plan to
improve the constituent classifier further, we did not
use it to bias the Role Labeler at training time, but
only used it to filter constituents at test time for the
hard task.
When using the classifier with the Role Labeler
at testing time, there are two possibilities. First, all
constituents deemed relevant by the classifier could
be presented to the labeler. However, because we
aimed for high recall but possibly low precision,
this would allow many irrelevant constituents as in-
put. This both lowers accuracy and increases the
computational complexity of labeling. The second
possibility is thus to choose some reasonable subset
of the positively identified constituents to present
to the labeler. The options we considered were a
top-down search, a bottom-up search, and a greedy
search; we chose a top-down search for simplic-
ity. In this case, the algorithm searches from the
root down in the parse tree until it finds a posi-
tively labeled constituent. While this assumes that
no subtree of a role-bearing constituent is also role-
bearing, we discovered that some role-bearing con-
stituents do overlap with each other in the parse
trees. However, in the Senseval training corpus,
only 1.2% of the sentences contain a (single) over-
lapping constituent. In future work we plan to inves-
tigate alternative approaches for constituent choice.
After filtering via our top down technique, we
present the resulting constituent sequence to the
role labeler. Since the role labeler is trained on
sequences containing only true role-bearing con-
stituents but tested on sequences with potentially
missing and potentially irrelevant constituents, this
stage provides an opportunity for errors to creep into
the process. However, because of the Markovian as-
sumption, the presence of an irrelevant constituent
has only local effects on the overall choice of a role
sequence.
5 Evaluation
The SENSEVAL-3 committee chose 40 of the most
frequent 100 frames from FrameNet II for the
competition. In experiments with validation sets,
our algorithm performed better using only the
SENSEVAL-3 training data, as opposed to also us-
ing sentences from the remaining frames, so all our
models were trained only on that data. We cal-
culated performance using SENSEVAL-3?s scoring
software.
We submitted two set of answers for each task.
We summarize each system?s performance in Ta-
ble 1. For the easy task, we used both the grouped
(FEG Easy) and first order (FirstOrder Easy) mod-
els. The grouped model performed better on exper-
iments with validation sets, perhaps due to the fact
that many frames have a small number of possible
System Precision Recall Overlap
FEG Easy 85.8% 84.9% 85.7%
FirstOrder Easy 72.8% 72.1% 72.5%
CostSens Hard 38.7% 33.5% 29.5%
Hard 35.5% 45.3% 25.5%
Table 1: System Scores.
System Precision Recall Overlap
CostSens Hard 47.2% 42.2% 41.5%
Hard 60.2% 24.7% 57.1%
Table 2: Newer System Scores.
role permutations corresponding to a given number
of constituents. In less artificial conditions this ver-
sion would be less flexible in incorporating both rel-
evant and irrelevant constituents.
For the hard task, we used only the first order
model, due both to its greater flexibility and to the
low precision of our classifier: if all positively clas-
sified constituents were passed to the group model,
the sequence length would be greater than any seen
at training time, when only correct constituents are
given to the labeler. We used both the cost sensi-
tive classifier (CostSens Hard) and the regular con-
stituent classifier to filter constituents (Hard). There
is a precision/recall tradeoff in using the different
classifiers. We were surprised how poorly our la-
beler was performing on validation sets as we pre-
pared our results. We found out that our classi-
fier was omitting about 70% of the role-bearing
constituents from consideration, because they only
matched a parse constituent at a pre-terminal node.
We fixed this bug after submission, learned a new
constituent classifier, and used the same role labeler
as before. The improved results are shown in Ta-
ble 2. Note that our recall has an upper limit of
85.8% due to mismatches between roles and parse
tree constituents.
6 Future Work
We have identified three problems for future re-
search. First, our constituent classifier should be
improved to produce fewer false positives and to in-
clude a higher percentage of true positives. To do
this, we first plan to enhance the feature set. We will
also explore improved approaches to combining the
results of the classifier with the role labeler. For ex-
ample, in preliminary studies, a bottom-up search
for positive constituents in the parse tree seems to
yield better results than our current top-down ap-
proach.
Second, since false positives cannot be entirely
avoided, the labeler needs to better handle con-
stituents that should not be labeled with a role. To
solve this problem, we will adapt the idea of null
generated words from machine translation (Brown
et al, 1993). Instead of having a word in the target
language that corresponds to no word in the source
language, we have a constituent that corresponds to
no state in the role sequence.
Finally, we will address roles that do not label a
constituent, called null-instantiated roles. An exam-
ple is the sentence ?The man drove to the station,?
in which the VEHICLE role does not have a con-
stituent, but is implicitly there, since the man obvi-
ously drove something to the station. This problem
is more difficult, since it involves obtaining infor-
mation not actually in the sentence. One possibility
is to consider inserting null-instantiated roles at ev-
ery step. We will consider only roles seen as null-
instantiated at training time. This method will re-
strict the search space, which would otherwise be
extremely large.
7 Conclusions
In conclusion, our generative model performs ro-
bustly on the easy version of the SENSEVAL-3 role
labeling task. The combination of our constituent
classifier with the role labeling has more room for
improvement, but performed reasonably well con-
sidering the difficulties of the task and the sparse
feature set that we incorporated into our generative
model. Alternative sentence chunking models for
semantic analysis, and the extension of our gener-
ative models, should lead to future improvements.
The key advantage of our approach is the treatment
of a sentence?s roles as a sequence. This allows the
model to consider relationships between roles as it
semantically analyzes a sentence.
8 Acknowledgements
This work was supported in part by the Advanced
Research and Development Activity?s Advanced
Question Answering for Intelligence Program. The
basic Role Labeler was developed in collaboration
with Chris Manning and Roger Levy.
References
P. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?311.
M. J. Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Assoc.
for Computational Linguistics, pages 184?191,
Santa Cruz, CA.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, 28:245?288.
M. Jordan, editor. 1999. Learning in Graphical
Models. MIT Press, Cambridge, MA.
C. A. Thompson, R. Levy, and C. Manning. 2003.
A generative model for semantic role labeling.
In Proceedings of the Fourteenth European Con-
ference on Machine Learning, pages 397?408,
Croatia.
I. Witten and E. Frank. 2000. Data Mining: Practi-
cal Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann, San
Francisco.
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 66?73,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Domain-Specific Information Extraction Patterns from the Web
Siddharth Patwardhan and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{sidd,riloff}@cs.utah.edu
Abstract
Many information extraction (IE) systems
rely on manually annotated training data
to learn patterns or rules for extracting in-
formation about events. Manually anno-
tating data is expensive, however, and a
new data set must be annotated for each
domain. So most IE training sets are rel-
atively small. Consequently, IE patterns
learned from annotated training sets of-
ten have limited coverage. In this paper,
we explore the idea of using the Web to
automatically identify domain-specific IE
patterns that were not seen in the training
data. We use IE patterns learned from the
MUC-4 training set as anchors to identify
domain-specific web pages and then learn
new IE patterns from them. We compute
the semantic affinity of each new pattern
to automatically infer the type of informa-
tion that it will extract. Experiments on
the MUC-4 test set show that these new IE
patterns improved recall with only a small
precision loss.
1 Introduction
Information Extraction (IE) is the task of identi-
fying event descriptions in natural language text
and extracting information related to those events.
Many IE systems use extraction patterns or rules
to identify the relevant information (Soderland et
al., 1995; Riloff, 1996; Califf and Mooney, 1999;
Soderland, 1999; Yangarber et al, 2000). Most of
these systems use annotated training data to learn
pattern matching rules based on lexical, syntactic,
and/or semantic information. The learned patterns
are then used to locate relevant information in new
texts.
IE systems typically focus on information about
events that are relevant to a specific domain, such
as terrorism (Sundheim, 1992; Soderland et al,
1995; Riloff, 1996; Chieu et al, 2003), man-
agement succession (Sundheim, 1995; Yangarber
et al, 2000), or job announcements (Califf and
Mooney, 1999; Freitag and McCallum, 2000).
Supervised learning systems for IE depend on
domain-specific training data, which consists of
texts associated with the domain that have been
manually annotated with event information.
The need for domain-specific training data has
several disadvantages. Because of the manual la-
bor involved in annotating a corpus, and because a
new corpus must be annotated for each domain,
most annotated IE corpora are relatively small.
Language is so expressive that it is practically
impossible for the patterns learned from a rela-
tively small training set to cover all the different
ways of describing events. Consequently, the IE
patterns learned from manually annotated train-
ing sets typically represent only a subset of the IE
patterns that could be useful for the task. Many
recent approaches in natural language processing
(Yarowsky, 1995; Collins and Singer, 1999; Riloff
and Jones, 1999; Nigam et al, 2000; Wiebe and
Riloff, 2005) have recognized the need to use
unannotated data to improve performance.
While the Web provides a vast repository of
unannotated texts, it is non-trivial to identify texts
that belong to a particular domain. The difficulty
is that web pages are not specifically annotated
with tags categorizing their content. Nevertheless,
in this paper we look to the Web as a vast dynamic
resource for domain-specific IE learning. Our ap-
proach exploits an existing set of IE patterns that
were learned from annotated training data to auto-
matically identify new, domain-specific texts from
66
the Web. These web pages are then used for ad-
ditional IE training, yielding a new set of domain-
specific IE patterns. Experiments on the MUC-4
test set show that the new IE patterns improve cov-
erage for the domain.
This paper is organized as follows. Section 2
presents the MUC-4 IE task and data that we use in
our experiments. Section 3 describes how we cre-
ate a baseline IE system from the MUC-4 training
data. Section 4 describes the collection and pre-
processing of potentially relevant web pages. Sec-
tion 5 then explains how we use the IE patterns
learned from the MUC-4 training set as anchors to
learn new IE patterns from the web pages. We also
compute the semantic affinity of each new pattern
to automatically infer the type of information that
it will extract. Section 6 shows experimental re-
sults for two types of extractions, victims and tar-
gets, on the MUC-4 test set. Finally, Section 7
compares our approach to related research, and
Section 8 concludes with ideas for future work.
2 The MUC-4 IE Task and Data
The focus of our research is on the MUC-4 infor-
mation extraction task (Sundheim, 1992), which is
to extract information about terrorist events. The
MUC-4 corpus contains 1700 stories, mainly news
articles related to Latin American terrorism, and
associated answer key templates containing the in-
formation that should be extracted from each story.
We focused our efforts on two of the MUC-4
string slots, which require textual extractions: hu-
man targets (victims) and physical targets. The
MUC-4 data has proven to be an especially dif-
ficult IE task for a variety of reasons, including
the fact that the texts are entirely in upper case,
roughly 50% of the texts are irrelevant (i.e., they
do not describe a relevant terrorist event), and
many of the stories that are relevant describe mul-
tiple terrorist events that need to be teased apart.
The best results reported across all string slots
in MUC-4 were in the 50%-70% range for re-
call and precision (Sundheim, 1992), with most
of the MUC-4 systems relying on heavily hand-
engineered components. Chieu et al (2003) re-
cently developed a fully automatic template gen-
erator for the MUC-4 IE task. Their best system
produced recall scores of 41%-44% with precision
scores of 49%-51% on the TST3 and TST4 test
sets.
3 Learning IE Patterns from a Fixed
Training Set
As our baseline system, we created an IE
system for the MUC-4 terrorism domain us-
ing the AutoSlog-TS extraction pattern learn-
ing system (Riloff, 1996; Riloff and Phillips,
2004), which is freely available for research use.
AutoSlog-TS is a weakly supervised learner that
requires two sets of texts for training: texts that
are relevant to the domain and texts that are irrel-
evant to the domain. The MUC-4 data includes
relevance judgments (implicit in the answer keys),
which we used to partition our training set into rel-
evant and irrelevant subsets.
AutoSlog-TS? learning process has two phases.
In the first phase, syntactic patterns are applied
to the training corpus in an exhaustive fashion,
so that extraction patterns are generated for (lit-
erally) every lexical instantiation of the patterns
that appears in the corpus. For example, the syn-
tactic pattern ?<subj> PassVP? would generate
extraction patterns for all verbs that appear in the
corpus in a passive voice construction. The sub-
ject of the verb will be extracted. In the terrorism
domain, some of these extraction patterns might
be: ?<subj> PassVP(murdered)? and ?<subj>
PassVP(bombed).? These would match sentences
such as: ?the mayor was murdered?, and ?the em-
bassy and hotel were bombed?. Figure 1 shows
the 17 types of extraction patterns that AutoSlog-
TS currently generates. PassVP refers to passive
voice verb phrases (VPs), ActVP refers to active
voice VPs, InfVP refers to infinitive VPs, and
AuxVP refers to VPs where the main verb is a
form of ?to be? or ?to have?. Subjects (subj), di-
rect objects (dobj), PP objects (np), and posses-
sives can be extracted by the patterns.
In the second phase, AutoSlog-TS applies all
of the generated extraction patterns to the training
corpus and gathers statistics for how often each
pattern occurs in relevant versus irrelevant texts.
The extraction patterns are subsequently ranked
based on their association with the domain, and
then a person manually reviews the patterns, de-
ciding which ones to keep1 and assigning thematic
roles to them. We manually defined selectional
restrictions for each slot type (victim and target)
1Typically, many patterns are strongly associated with the
domain but will not extract information that is relevant to the
IE task. For example, in this work we only care about patterns
that will extract victims and targets. Patterns that extract other
types of information are not of interest.
67
Pattern Type Example Pattern
<subj> PassVP <victim> was murdered
<subj> ActVP <perp> murdered
<subj> ActVP Dobj <weapon> caused damage
<subj> ActInfVP <perp> tried to kill
<subj> PassInfVP <weapon> was intended to kill
<subj> AuxVP Dobj <victim> was casualty
<subj> AuxVP Adj <victim> is dead
ActVP <dobj> bombed <target>
InfVP <dobj> to kill <victim>
ActInfVP <dobj> planned to bomb <target>
PassInfVP <dobj> was planned to kill <victim>
Subj AuxVP <dobj> fatality is <victim>
NP Prep <np> attack against <target>
ActVP Prep <np> killed with <weapon>
PassVP Prep <np> was killed with <weapon>
InfVP Prep <np> to destroy with <weapon>
<possessive> NP <victim>?s murder
Figure 1: AutoSlog-TS? pattern types and sample
IE patterns
and then automatically added these to each pattern
when the role was assigned.
On our training set, AutoSlog-TS generated
40,553 distinct extraction patterns. A person man-
ually reviewed all of the extraction patterns that
had a score ? 0.951 and frequency ? 3. This
score corresponds to AutoSlog-TS? RlogF metric,
described in (Riloff, 1996). The lowest ranked pat-
terns that passed our thresholds had at least 3 rel-
evant extractions out of 5 total extractions. In all,
2,808 patterns passed the thresholds. The reviewer
ultimately decided that 396 of the patterns were
useful for the MUC-4 IE task, of which 291 were
useful for extracting victims and targets.
4 Data Collection
In this research, our goal is to automatically learn
IE patterns from a large, domain-independent text
collection, such as the Web. The billions of freely
available documents on the World Wide Web and
its ever-growing size make the Web a potential
source of data for many corpus-based natural lan-
guage processing tasks. Indeed, many researchers
have recently tapped the Web as a data-source
for improving performance on NLP tasks (e.g.,
Resnik (1999), Ravichandran and Hovy (2002),
Keller and Lapata (2003)). Despite these suc-
cesses, numerous problems exist with collecting
data from the Web, such as web pages contain-
ing information that is not free text, including ad-
vertisements, embedded scripts, tables, captions,
etc. Also, the documents cover many genres, and
it is not easy to identify documents of a particular
genre or domain. Additionally, most of the doc-
uments are in HTML, and some amount of pro-
cessing is required to extract the free text. In the
following subsections we describe the process of
collecting a corpus of terrorism-related CNN news
articles from the Web.
4.1 Collecting Domain-Specific Texts
Our goal was to automatically identify and collect
a set of documents that are similar in domain to the
MUC-4 terrorism text collection. To create such
a corpus, we used hand-crafted queries given to
a search engine. The queries to the search engine
were manually created to try to ensure that the ma-
jority of the documents returned by the search en-
gine would be terrorism-related. Each query con-
sisted of two parts: (1) the name of a terrorist or-
ganization, and (2) a word or phrase describing a
terrorist action (such as bombed, kidnapped, etc.).
The following lists of 5 terrorist organizations and
16 terrorist actions were used to create search en-
gine queries:
Terrorist organizations: Al Qaeda,
ELN, FARC, HAMAS, IRA
Terrorist actions: assassinated, assas-
sination, blew up, bombed, bombing,
bombs, explosion, hijacked, hijacking,
injured, kidnapped, kidnapping, killed,
murder, suicide bomber, wounded.
We created a total of 80 different queries repre-
senting each possible combination of a terrorist or-
ganization and a terrorist action.
We used the Google2 search engine with the
help of the freely available Google API3 to lo-
cate the texts on the Web. To ensure that we re-
trieved only CNN news articles, we restricted the
search to the domain ?cnn.com? by adding the
?site:? option to each of the queries. We also
restricted the search to English language docu-
ments by initializing the API with the lang en
option. We deleted documents whose URLs con-
tained the word ?transcript? because most of these
were transcriptions of CNN?s TV shows and were
stylistically very different from written text. We
ran the 80 queries twice, once in December 2005
and once in April 2005, which produced 3,496
documents and 3,309 documents, respectively.
After removing duplicate articles, we were left
2http://www.google.com
3http://www.google.com/apis
68
with a total of 6,182 potentially relevant terrorism
articles.
4.2 Processing the Texts
The downloaded documents were all HTML doc-
uments containing HTML tags and JavaScript in-
termingled with the news text. The CNN web-
pages typically also contained advertisements, text
for navigating the website, headlines and links to
other stories. All of these things could be problem-
atic for our information extraction system, which
was designed to process narrative text using a shal-
low parser. Thus, simply deleting all HTML tags
on the page would not have given us natural lan-
guage sentences. Instead, we took advantage of
the uniformity of the CNN web pages to ?clean?
them and extract just the sentences corresponding
to the news story.
We used a tool called HTMLParser4 to parse
the HTML code, and then deleted all nodes in the
HTML parse trees corresponding to tables, com-
ments, and embedded scripts (such as JavaScript
or VBScript). The system automatically extracted
news text starting from the headline (embedded
in an H1 HTML element) and inferred the end of
the article text using a set of textual clues such as
?Feedback:?, ?Copyright 2005?, ?contributed to
this report?, etc. In case of any ambiguity, all of
the text on the web page was extracted.
The size of the text documents ranged from 0
bytes to 255 kilobytes. The empty documents
were due to dead links that the search engine had
indexed at an earlier time, but which no longer ex-
isted. Some extremely small documents also re-
sulted from web pages that had virtually no free
text on them, so only a few words remained af-
ter the HTML had been stripped. Consequently,
we removed all documents less than 10 bytes in
size. Upon inspection, we found that many of the
largest documents were political articles, such as
political party platforms and transcriptions of po-
litical speeches, which contained only brief refer-
ences to terrorist events. To prevent the large doc-
uments from skewing the corpus, we also deleted
all documents over 10 kilobytes in size. At the end
of this process we were left with a CNN terrorism
news corpus of 5,618 documents, each with an av-
erage size of about 648 words. In the rest of the
paper we will refer to these texts as ?the CNN ter-
rorism web pages?.
4http://htmlparser.sourceforge.net
5 Learning Domain-Specific IE Patterns
from Web Pages
Having created a large domain-specific corpus
from the Web, we are faced with the problem
of identifying the useful extraction patterns from
these new texts. Our basic approach is to use the
patterns learned from the fixed training set as seed
patterns to identify sentences in the CNN terror-
ism web pages that describe a terrorist event. We
hypothesized that extraction patterns occurring in
the same sentence as a seed pattern are likely to be
associated with terrorism.
Our process for learning new domain-specific
IE patterns has two phases, which are described in
the following sections. Section 5.1 describes how
we produce a ranked list of candidate extraction
patterns from the CNN terrorism web pages. Sec-
tion 5.2 explains how we filter these patterns based
on the semantic affinity of their extractions, which
is a measure of the tendency of the pattern to ex-
tract entities of a desired semantic category.
5.1 Identifying Candidate Patterns
The first goal was to identify extraction patterns
that were relevant to our domain: terrorist events.
We began by exhaustively generating every pos-
sible extraction pattern that occurred in our CNN
terrorism web pages. We applied the AutoSlog-TS
system (Riloff, 1996) to the web pages to automat-
ically generate all lexical instantiations of patterns
in the corpus. Collectively, the resulting patterns
were capable of extracting every noun phrase in
the CNN collection. In all, 147,712 unique extrac-
tion patterns were created as a result of this pro-
cess.
Next, we computed the statistical correlation
of each extraction pattern with the seed patterns
based on the frequency of their occurrence in the
same sentence. IE patterns that never occurred
in the same sentence as a seed pattern were dis-
carded. We used Pointwise Mutual Information
(PMI) (Manning and Schu?tze, 1999; Banerjee and
Pedersen, 2003) as the measure of statistical corre-
lation. Intuitively, an extraction pattern that occurs
more often than chance in the same sentence as a
seed pattern will have a high PMI score.
The 147,712 extraction patterns acquired from
the CNN terrorism web pages were then ranked
by their PMI correlation to the seed patterns. Ta-
ble 1 lists the most highly ranked patterns. Many
of these patterns do seem to be related to terrorism,
69
<subj> killed sgt <subj> destroyed factories
<subj> burned flag explode after <np>
sympathizers of <np> <subj> killed heir
<subj> kills bystanders <subj> shattered roof
rescued within <np> fled behind <np>
Table 1: Examples of candidate patterns that are
highly correlated with the terrorism seed patterns
but many of them are not useful to our IE task (for
this paper, identifying the victims and physical tar-
gets of a terrorist attack). For example, the pattern
?explode after <np>? will not extract victims or
physical targets, while the pattern ?sympathizers
of <np>? may extract people but they would not
be the victims of an attack. In the next section, we
explain how we filter and re-rank these candidate
patterns to identify the ones that are directly useful
to our IE task.
5.2 Filtering Patterns based upon their
Semantic Affinity
Our next goal is to filter out the patterns that are
not useful for our IE task, and to automatically
assign the correct slot type (victim or target) to
the ones that are relevant. To automatically deter-
mine the mapping between extractions and slots,
we define a measure called semantic affinity. The
semantic affinity of an extraction pattern to a se-
mantic category is a measure of its tendency to
extract NPs belonging to that semantic category.
This measure serves two purposes:
(a) It allows us to filter out candidate patterns
that do not have a strong semantic affinity to
our categories of interest.
(b) It allows us to define a mapping between the
extractions of the candidate patterns and the
desired slot types.
We computed the semantic affinity of each can-
didate extraction pattern with respect to six seman-
tic categories: target, victim, perpetrator, organi-
zation, weapon and other. Targets and victims are
our categories of interest. Perpetrators, organiza-
tions, and weapons are common semantic classes
in this domain which could be ?distractors?. The
other category is a catch-all to represent all other
semantic classes. To identify the semantic class of
each noun phrase, we used the Sundance package
(Riloff and Phillips, 2004), which is a freely avail-
able shallow parser that uses dictionaries to assign
semantic classes to words and phrases.
We counted the frequencies of the semantic cat-
egories extracted by each candidate pattern and
applied the RLogF measure used by AutoSlog-TS
(Riloff, 1996) to rank the patterns based on their
affinity for the target and victim semantic classes.
For example, the semantic affinity of an extraction
pattern for the target semantic class would be cal-
culated as:
affinitypattern =
ftarget
fall
? log2ftarget (1)
where ftarget is the number of target semantic
class extractions and fall = ftarget + fvictim +
fperp +forg +fweapon +fother. This is essentially
a probability P (target) weighted by the log of the
frequency.
We then used two criteria to remove patterns
that are not strongly associated with a desired se-
mantic category. If the semantic affinity of a pat-
tern for category C was (1) greater than a thresh-
old, and (2) greater than its affinity for the other
category, then the pattern was deemed to have a
semantic affinity for category C. Note that we
intentionally allow for a pattern to have an affin-
ity for more than one semantic category (except
for the catch-all other class) because this is fairly
common in practice. For example, the pattern ?at-
tack on <np>? frequently extracts both targets
(e.g., ?an attack on the U.S. embassy?) and vic-
tims (e.g., ?an attack on the mayor of Bogota?).
Our hope is that such a pattern would receive a
high semantic affinity ranking for both categories.
Table 2 shows the top 10 high frequency
(freq ? 50) patterns that were judged to have a
strong semantic affinity for the target and victim
categories. There are clearly some incorrect en-
tries (e.g., ?<subj> fired missiles? is more likely
to identify perpetrators than targets), but most of
the patterns are indeed good extractors for the de-
sired categories. For example, ?fired into <np>?,
?went off in <np>?, and ?car bomb near <np>?
are all good patterns for identifying targets of a
terrorist attack. In general, the semantic affinity
measure seemed to do a reasonably good job of
filtering patterns that are not relevant to our task,
and identifying patterns that are useful for extract-
ing victims and targets.
6 Experiments and Results
Our goal has been to use IE patterns learned from
a fixed, domain-specific training set to automat-
ically learn additional IE patterns from a large,
70
Target Patterns Victim Patterns
<subj> fired missiles wounded in <np>
missiles at <np> <subj> was identified
bomb near <np> wounding <dobj>
fired into <np> <subj> wounding
died on <np> identified <dobj>
went off in <np> <subj> identified
car bomb near <np> including <dobj>
exploded outside <np> <subj> ahmed
gunmen on <np> <subj> lying
killed near <np> <subj> including
Table 2: Top 10 high-frequency target and victim
patterns learned from the Web
domain-independent text collection, such as the
Web. Although many of the patterns learned
from the CNN terrorism web pages look like good
extractors, an open question was whether they
would actually be useful for the original IE task.
For example, some of the patterns learned from
the CNN web pages have to do with behead-
ings (e.g., ?beheading of <np>? and ?beheaded
<np>?), which are undeniably good victim ex-
tractors. But the MUC-4 corpus primarily con-
cerns Latin American terrorism that does not in-
volve beheading incidents. In general, the ques-
tion is whether IE patterns learned from a large, di-
verse text collection can be valuable for a specific
IE task above and beyond the patterns that were
learned from the domain-specific training set, or
whether the newly learned patterns will simply not
be applicable. To answer this question, we evalu-
ated the newly learned IE patterns on the MUC-4
test set.
The MUC-4 data set is divided into 1300 devel-
opment (DEV) texts, and four test sets of 100 texts
each (TST1, TST2, TST3, and TST4).5 All of
these texts have associated answer key templates.
We used 1500 texts (DEV+TST1+TST2) as our
training set, and 200 texts (TST3+TST4) as our
test set.
The IE process typically involves extracting
information from individual sentences and then
mapping that information into answer key tem-
plates, one template for each terrorist event de-
scribed in the story. The process of template gen-
eration requires discourse processing to determine
how many events took place and which facts cor-
respond to which event. Discourse processing and
5The DEV texts were used for development in MUC-3
and MUC-4. The TST1 and TST2 texts were used as test sets
for MUC-3 and then as development texts for MUC-4. The
TST3 and TST4 texts were used as the test sets for MUC-4.
template generation are not the focus of this paper.
Our research aims to produce a larger set of extrac-
tion patterns so that more information will be ex-
tracted from the sentences, before discourse anal-
ysis would begin. Consequently, we evaluate the
performance of our IE system at that stage: after
extracting information from sentences, but before
template generation takes place. This approach di-
rectly measures how well we are able to improve
the coverage of our extraction patterns for the do-
main.
6.1 Baseline Results on the MUC-4 IE Task
The AutoSlog-TS system described in Section 3
used the MUC-4 training set to learn 291 target
and victim IE patterns. These patterns produced
64% recall with 43% precision on the targets, and
50% recall with 52% precision on the victims.6
These numbers are not directly comparable to
the official MUC-4 scores, which evaluate tem-
plate generation, but our recall is in the same ball-
park. Our precision is lower, but this is to be ex-
pected because we do not perform discourse anal-
ysis.7 These 291 IE patterns represent our base-
line IE system that was created from the MUC-4
training data.
6.2 Evaluating the Newly Learned Patterns
We used all 396 terrorism extraction patterns
learned from the MUC-4 training set8 as seeds to
identify relevant text regions in the CNN terrorism
web pages. We then produced a ranked list of new
terrorism IE patterns using a semantic affinity cut-
off of 3.0. We selected the top N patterns from the
ranked list, with N ranging from 50 to 300, and
added these N patterns to the baseline system.
Table 3 lists the recall, precision and F-measure
for the increasingly larger pattern sets. For the tar-
6We used a head noun scoring scheme, where we scored
an extraction as correct if its head noun matched the head
noun in the answer key. This approach allows for different
leading modifiers in an NP as long as the head noun is the
same. For example, ?armed men? will successfully match
?5 armed men?. We also discarded pronouns (they were not
scored at all) because our system does not perform corefer-
ence resolution.
7Among other things, discourse processing merges seem-
ingly disparate extractions based on coreference resolution
(e.g., ?the guerrillas? may refer to the same people as ?the
armed men?) and applies task-specific constraints (e.g., the
MUC-4 task definition has detailed rules about exactly what
types of people are considered to be terrorists).
8This included not only the 291 target and victim patterns,
but also 105 patterns associated with other types of terrorism
information.
71
Targets Victims
Precision Recall F-measure Precision Recall F-measure
baseline 0.425 0.642 0.511 0.498 0.517 0.507
50+baseline 0.420 0.642 0.508 0.498 0.517 0.507
100+baseline 0.419 0.650 0.510 0.496 0.521 0.508
150+baseline 0.415 0.650 0.507 0.480 0.521 0.500
200+baseline 0.412 0.667 0.509 0.478 0.521 0.499
250+baseline 0.401 0.691 0.507 0.478 0.521 0.499
300+baseline 0.394 0.691 0.502 0.471 0.542 0.504
Table 3: Performance of new IE patterns on MUC-4 test set
get slot, the recall increases from 64.2% to 69.1%
with a small drop in precision. The F-measure
drops by about 1% because recall and precision
are less balanced. But we gain more in recall
(+5%) than we lose in precision (-3%). For the
victim patterns, the recall increases from 51.7% to
54.2% with a similar small drop in precision. The
overall drop in the F-measure in this case is neg-
ligible. These results show that our approach for
learning IE patterns from a large, diverse text col-
lection (the Web) can indeed improve coverage on
a domain-specific IE task, with a small decrease in
precision.
7 Related Work
Unannotated texts have been used successfully for
a variety of NLP tasks, including named entity
recognition (Collins and Singer, 1999), subjectiv-
ity classification (Wiebe and Riloff, 2005), text
classification (Nigam et al, 2000), and word sense
disambiguation (Yarowsky, 1995). The Web has
become a popular choice as a resource for large
quantities of unannotated data. Many research
ideas have exploited the Web in unsupervised or
weakly supervised algorithms for natural language
processing (e.g., Resnik (1999), Ravichandran and
Hovy (2002), Keller and Lapata (2003)).
The use of unannotated data to improve in-
formation extraction is not new. Unannotated
texts have been used for weakly supervised train-
ing of IE systems (Riloff, 1996) and in boot-
strapping methods that begin with seed words
or patterns (Riloff and Jones, 1999; Yangarber
et al, 2000). However, those previous sys-
tems rely on pre-existing domain-specific cor-
pora. For example, EXDISCO (Yangarber et
al., 2000) used Wall Street Journal articles for
training. AutoSlog-TS (Riloff, 1996) and Meta-
bootstrapping (Riloff and Jones, 1999) used the
MUC-4 training texts. Meta-bootstrapping was
also trained on web pages, but the ?domain? was
corporate relationships so domain-specific web
pages were easily identified simply by gathering
corporate web pages.
The KNOWITALL system (Popescu et al, 2004)
also uses unannotated web pages for information
extraction. However, this work is quite differ-
ent from ours because KNOWITALL focuses on
extracting domain-independent relationships with
the aim of extending an ontology. In contrast,
our work focuses on using the Web to augment
a domain-specific, event-oriented IE system with
new, automatically generated domain-specific IE
patterns acquired from the Web.
8 Conclusions and Future Work
We have shown that it is possible to learn new
extraction patterns for a domain-specific IE task
by automatically identifying domain-specific web
pages using seed patterns. Our approach produced
a 5% increase in recall for extracting targets and a
3% increase in recall for extracting victims of ter-
rorist events. Both increases in recall were at the
cost of a small loss in precision.
In future work, we plan to develop improved
ranking methods and more sophisticated seman-
tic affinity measures to further improve coverage
and minimize precision loss. Another possible av-
enue for future work is to embed this approach in a
bootstrapping mechanism so that the most reliable
new IE patterns can be used to collect additional
web pages, which can then be used to learn more
IE patterns in an iterative fashion. Also, while
most of this process is automated, some human in-
tervention is required to create the search queries
for the document collection process, and to gener-
ate the seed patterns. We plan to look into tech-
niques to automate these manual tasks as well.
72
Acknowledgments
This research was supported by NSF Grant IIS-
0208985 and the Institute for Scientific Comput-
ing Research and the Center for Applied Scientific
Computing within Lawrence Livermore National
Laboratory.
References
S. Banerjee and T. Pedersen. 2003. The Design, Im-
plementation, and Use of the Ngram Statistics Pack-
age. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 370?381, Mexico City,
Mexico, February.
M. Califf and R. Mooney. 1999. Relational Learning
of Pattern-matching Rules for Information Extrac-
tion. In Proceedings of the Sixteenth National Con-
ference on Artificial Intelligence, pages 328?334,
Orlando, FL, July.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
Gap: Learning-Based Information Extraction Rival-
ing Knowledge-Engineering Methods. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 216?223, Sap-
poro, Japan, July.
M. Collins and Y. Singer. 1999. Unsupervised Models
for Named Entity Classification. In Proceedings of
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 100?110, College Park, MD, June.
D. Freitag and A. McCallum. 2000. Informa-
tion Extraction with HMM Structures Learned by
Stochastic Optimization. In Proceedings of the Sev-
enteenth National Conference on Artificial Intelli-
gence, pages 584?589, Austin, TX, August.
F. Keller and M. Lapata. 2003. Using the Web to
Obtain Frequencies for Unseen Bigrams. Compu-
tational Linguistics, 29(3):459?484, September.
C. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, MA.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text Classification from Labeled and Un-
labeled Documents using EM. Machine Learning,
39(2-3):103?134, May.
A. Popescu, A. Yates, and O. Etzioni. 2004. Class Ex-
traction from the World Wide Web. In Ion Muslea,
editor, Adaptive Text Extraction and Mining: Papers
from the 2004 AAAI Workshop, pages 68?73, San
Jose, CA, July.
D. Ravichandran and E. Hovy. 2002. Learning Surface
Text Patterns for a Question Answering System. In
Proceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, pages 41?47,
Philadelphia, PA, July.
P. Resnik. 1999. Mining the Web for Bilingual Text.
In Proceedings of the 37th meeting of the Associa-
tion for Computational Linguistics, pages 527?534,
College Park, MD, June.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, pages 474?
479, Orlando, FL, July.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049, Portland, OR, August.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a Conceptual Dictio-
nary. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence, pages
1314?1319, Montreal, Canada, August.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233?272, February.
B. Sundheim. 1992. Overview of the Fourth Message
Understanding Evaluation and Conference. In Pro-
ceedings of the Fourth Message Understanding Con-
ference (MUC-4), pages 3?21, McLean, VA, June.
B. Sundheim. 1995. Overview of the Results of
the MUC-6 Evaluation. In Proceedings of the
Sixth Message Understanding Conference (MUC-6),
pages 13?31, Columbia, MD, November.
J. Wiebe and E. Riloff. 2005. Creating Subjective
and Objective Sentence Classifiers from Unanno-
tated Texts. In Proceedings of the 6th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 486?497, Mexico City,
Mexico, February.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 940?946, Saarbru?cken,
Germany, August.
D. Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189?196,
Cambridge, MA, June.
73
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 440?448,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Feature Subsumption for Opinion Analysis
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
Lexical features are key to many ap-
proaches to sentiment analysis and opin-
ion detection. A variety of representations
have been used, including single words,
multi-word Ngrams, phrases, and lexico-
syntactic patterns. In this paper, we use a
subsumption hierarchy to formally define
different types of lexical features and their
relationship to one another, both in terms
of representational coverage and perfor-
mance. We use the subsumption hierar-
chy in two ways: (1) as an analytic tool
to automatically identify complex features
that outperform simpler features, and (2)
to reduce a feature set by removing un-
necessary features. We show that reduc-
ing the feature set improves performance
on three opinion classification tasks, espe-
cially when combined with traditional fea-
ture selection.
1 Introduction
Sentiment analysis and opinion recognition are ac-
tive research areas that have many potential ap-
plications, including review mining, product rep-
utation analysis, multi-document summarization,
and multi-perspective question answering. Lexi-
cal features are key to many approaches, and a va-
riety of representations have been used, including
single words, multi-word Ngrams, phrases, and
lexico-syntactic patterns. It is common for dif-
ferent features to overlap representationally. For
example, the unigram ?happy? will match all of
the texts that the bigram ?very happy? matches.
Since both features represent a positive sentiment
and the bigram matches fewer contexts than the
unigram, it is probably sufficient just to have the
unigram. However, there are many cases where
a feature captures a subtlety or non-compositional
meaning that a simpler feature does not. For exam-
ple, ?basket case? is a highly opinionated phrase,
but the words ?basket? and ?case? individually
are not. An open question in opinion analysis is
how often more complex feature representations
are needed, and which types of features are most
valuable. Our first goal is to devise a method to
automatically identify features that are represen-
tationally subsumed by a simpler feature but that
are better opinion indicators. These subjective ex-
pressions could then be added to a subjectivity lex-
icon (Esuli and Sebastiani, 2005), and used to gain
understanding about which types of complex fea-
tures capture meaningful expressions that are im-
portant for opinion recognition.
Many opinion classifiers are created by adopt-
ing a ?kitchen sink? approach that throws together
a variety of features. But in many cases adding
new types of features does not improve perfor-
mance. For example, Pang et al (2002) found that
unigrams outperformed bigrams, and unigrams
outperformed the combination of unigrams plus
bigrams. Our second goal is to automatically iden-
tify features that are unnecessary because similar
features provide equal or better coverage and dis-
criminatory value. Our hypothesis is that a re-
duced feature set, which selectively combines un-
igrams with only the most valuable complex fea-
tures, will perform better than a larger feature set
that includes the entire ?kitchen sink? of features.
In this paper, we explore the use of a subsump-
tion hierarchy to formally define the subsump-
tion relationships between different types of tex-
tual features. We use the subsumption hierarchy
in two ways. First, we use subsumption as an an-
440
alytic tool to compare features of different com-
plexities and automatically identify complex fea-
tures that substantially outperform their simpler
counterparts. Second, we use the subsumption hi-
erarchy to reduce a feature set based on represen-
tational overlap and on performance. We conduct
experiments with three opinion data sets and show
that the reduced feature sets can improve classifi-
cation performance.
2 The Subsumption Hierarchy
2.1 Text Representations
We analyze two feature representations that have
been used for opinion analysis: Ngrams and Ex-
traction Patterns. Information extraction (IE)
patterns are lexico-syntactic patterns that rep-
resent expressions which identify role relation-
ships. For example, the pattern ?<subj>
ActVP(recommended)? extracts the subject of
active-voice instances of the verb ?recommended?
as the recommender. The pattern ?<subj>
PassVP(recommended)? extracts the subject of
passive-voice instances of ?recommended? as the
object being recommended.
(Riloff and Wiebe, 2003) explored the idea
of using extraction patterns to represent more
complex subjective expressions that have non-
compositional meanings. For example, the expres-
sion ?drive (someone) up the wall? expresses the
feeling of being annoyed, but the meanings of the
words ?drive?, ?up?, and ?wall? have no emotional
connotations individually. Furthermore, this ex-
pression is not a fixed word sequence that can be
adequately modeled by Ngrams. Any noun phrase
can appear between the words ?drive? and ?up?, so
a flexible representation is needed to capture the
general pattern ?drives <NP> up the wall?.
This example represents a general phenomenon:
many expressions allow intervening noun phrases
and/or modifying terms. For example:
?stepped on <mods> toes?
Ex: stepped on the boss? toes
?dealt <np> <mods> blow?
Ex: dealt the company a decisive blow
?brought <np> to <mods> knees?
Ex: brought the man to his knees
(Riloff and Wiebe, 2003) also showed that syn-
tactic variations of the same verb phrase can be-
have very differently. For example, they found that
passive-voice constructions of the verb ?ask? had
a 100% correlation with opinion sentences, but
active-voice constructions had only a 63% corre-
lation with opinions.
Pattern Type Example Pattern
<subj> PassVP <subj> is satisfied
<subj> ActVP <subj> complained
<subj> ActVP Dobj <subj> dealt blow
<subj> ActInfVP <subj> appear to be
<subj> PassInfVP <subj> is meant to be
<subj> AuxVP Dobj <subj> has position
<subj> AuxVP Adj <subj> is happy
ActVP <dobj> endorsed <dobj>
InfVP <dobj> to condemn <dobj>
ActInfVP <dobj> get to know <dobj>
PassInfVP <dobj> is meant to be <dobj>
Subj AuxVP <dobj> fact is <dobj>
NP Prep <np> opinion on <np>
ActVP Prep <np> agrees with <np>
PassVP Prep <np> is worried about <np>
InfVP Prep <np> to resort to <np>
<possessive> NP <noun>?s speech
Figure 1: Extraction Pattern Types
Our goal is to use the subsumption hierarchy
to identify Ngram and extraction pattern features
that are more strongly associated with opinions
than simpler features. We used three types of fea-
tures in our research: unigrams, bigrams, and IE
patterns. The Ngram features were generated us-
ing the Ngram Statistics Package (NSP) (Baner-
jee and Pedersen, 2003).1 The extraction pat-
terns (EPs) were automatically generated using
the Sundance/AutoSlog software package (Riloff
and Phillips, 2004). AutoSlog relies on the Sun-
dance shallow parser and can be applied exhaus-
tively to a text corpus to generate IE patterns that
can extract every noun phrase in the corpus. Au-
toSlog has been used to learn IE patterns for the
domains of terrorism, joint ventures, and micro-
electronics (Riloff, 1996), as well as for opinion
analysis (Riloff and Wiebe, 2003). Figure 1 shows
the 17 types of extraction patterns that AutoSlog
generates. PassVP refers to passive-voice verb
phrases (VPs), ActVP refers to active-voice VPs,
InfVP refers to infinitive VPs, and AuxVP refers
1NSP is freely available for use under the GPL from
http://search.cpan.org/dist/Text-NSP. We discarded Ngrams
that consisted entirely of stopwords. We used a list of 281
stopwords.
441
to VPs where the main verb is a form of ?to be?
or ?to have?. Subjects (subj), direct objects (dobj),
PP objects (np), and possessives can be extracted
by the patterns.2
2.2 The Subsumption Hierarchy
We created a subsumption hierarchy that defines
the representational scope of different types of fea-
tures. We will say that feature A representation-
ally subsumes feature B if the set of text spans
that match feature A is a superset of the set of text
spans that match feature B. For example, the uni-
gram ?happy? subsumes the bigram ?very happy?
because the set of text spans that match ?happy?
includes the text spans that match ?very happy?.
First, we define a hierarchy of valid subsump-
tion relationships, shown in Figure 2. The 2Gram
node, for example, is a child of the 1Gram node
because a 1Gram can subsume a 2Gram. Ngrams
may subsume extraction patterns as well. Ev-
ery extraction pattern has at least one correspond-
ing 1Gram that will subsume it.3. For example,
the 1Gram ?recommended? subsumes the pattern
?<subj> ActVP(recommended)? because the pat-
tern only matches active-voice instances of ?rec-
ommended?. An extraction pattern may also
subsume another extraction pattern. For exam-
ple, ?<subj> ActVP(recommended)? subsumes
?<subj> ActVP(recommended) Dobj(movie)?.
To compare specific features we need to for-
mally define the representation of each type of
feature in the hierarchy. For example, the hierar-
chy dictates that a 2Gram can subsume the pattern
?ActInfVP <dobj>?, but this should hold only if
the words in the bigram correspond to adjacent
words in the pattern. For example, the 2Gram ?to
fish? subsumes the pattern ?ActInfVP(like to fish)
<dobj>?. But the 2Gram ?like fish? should not
subsume it. Similarly, consider the pattern ?In-
fVP(plan) <dobj>?, which represents the infini-
tive ?to plan?. This pattern subsumes the pattern
?ActInfVP(want to plan) <dobj>?, but it should
not subsume the pattern ?ActInfVP(plan to start)?.
To ensure that different features truly subsume
each other representationally, we formally define
each type of feature based on words, sequential
2However, the items extracted by the patterns are not ac-
tually used by our opinion classifiers; only the patterns them-
selves are matched against the text.
3Because every type of extraction pattern shown in Fig-
ure 1 contains at least one word (not including the extracted
phrases, which are not used as part of our feature representa-
tion).
dependencies, and syntactic dependencies. A se-
quential dependency between words wi and wi+1
means that wi and wi+1 must be adjacent, and that
wi must precede wi+1. Figure 3 shows the formal
definition of a bigram (2Gram) node. The bigram
is defined as two words with a sequential depen-
dency indicating that they must be adjacent.
Name = 2Gram
Constituent[0] = WORD1
Constituent[1] = WORD2
Dependency = Sequential(0, 1)
Figure 3: 2Gram Definition
A syntactic dependency between words wi and
wi+1 means that wi has a specific syntactic rela-
tionship to wi+1, and wi must precede wi+1. For
example, consider the extraction pattern ?NP Prep
<np>?, in which the object of the preposition at-
taches to the NP. Figure 4 shows the definition of
this extraction pattern in the hierarchy. The pat-
tern itself contains three components: the NP, the
attaching preposition, and the object of the prepo-
sition (which is the NP that the pattern extracts).
The definition also includes two syntactic depen-
dencies: the first dependency is between the NP
and the preposition (meaning that the preposition
syntactically attaches to the NP), while the second
dependency is between the preposition and the ex-
traction (meaning that the extracted NP is the syn-
tactic object of the preposition).
Name = NP Prep <np>
Constituent[0] = NP
Constituent[1] = PREP
Constituent[2] = NP EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
Figure 4: ?NP Prep <np>? Pattern Definition
Consequently, the bigram ?affair with? will not
subsume the extraction pattern ?affair with <np>?
because the bigram requires the noun and preposi-
tion to be adjacent but the pattern does not. For ex-
ample, the extraction pattern matches the text ?an
affair in his mind with Countess Olenska? but the
bigram does not. Conversely, the extraction pat-
tern does not subsume the bigram either because
the pattern requires syntactic attachment but the
bigram does not. For example, the bigram matches
442
<subj> ActVP
<subj> ActInfVP
<subj> ActVP Dobj
<subj> PassVP
<subj> PassInfVP
InfVP <dobj>
ActInfVP <dobj>
PassInfVP <dobj>
1Gram
2Gram
<possessive> NP
<subj> AuxVP AdjP
<subj> AuxVP Dobj
ActVP <dobj>
ActVP Prep <np>
NP Prep <np>
PassVP Prep <np>
Subj AuxVP <dobj>
3Gram
ActVP Prep:OF <np>
InfVP Prep <np>
NP Prep:OF <np>
PassVP Prep:OF <np>
4Gram
InfVP Prep:OF <np>
Figure 2: The Subsumption Hierarchy
the sentence ?He ended the affair with a sense of
relief?, but the extraction pattern does not.
Figure 5 shows the definition of another ex-
traction pattern, ?InfVP <dobj>?, which includes
both syntactic and sequential dependencies. This
pattern would match the text ?to protest high
taxes?. The pattern definition has three compo-
nents: the infinitive ?to?, a verb, and the direct ob-
ject of the verb (which is the NP that the pattern
extracts). The definition also shows two syntac-
tic dependencies. The first dependency indicates
that the verb syntactically attaches to the infinitive
?to?. The second dependency indicates that the ex-
tracted NP syntactically attaches to the verb (i.e.,
it is the direct object of that particular verb).
The pattern definition also includes a sequen-
tial dependency, which specifies that ?to? must be
adjacent to the verb. Strictly speaking, our parser
does not require them to be adjacent. For exam-
ple, the parser allows intervening adverbs to split
infinitives (e.g., ?to strongly protest high taxes?),
and this does happen occasionally. But split in-
finitives are relatively rare, so in the vast major-
ity of cases the infinitive ?to? will be adjacent to
the verb. Consequently, we decided that a bigram
(e.g., ?to protest?) should representationally sub-
sume this extraction pattern because the syntac-
tic flexibility afforded by the pattern is negligi-
ble. The sequential dependency link represents
this judgment call that the infinitive ?to? and the
verb are adjacent in most cases.
For all of the node definitions, we used our best
judgment to make decisions of this kind. We tried
to represent major distinctions between features,
without getting caught up in minor differences that
were likely to be negligible in practice.
Name = InfVP <dobj>
Constituent[0] = INFINITIVE TO
Constituent[1] = VERB
Constituent[2] = DOBJ EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
Dependency = Sequential(0, 1)
Figure 5: ?InfVP <dobj>? Pattern Definition
To use the subsumption hierarchy, we assign
each feature to its appropriate node in the hierar-
chy based on its type. Then we perform a top-
down breadth-first traversal. Each feature is com-
pared with the features at its ancestor nodes. If
a feature?s words and dependencies are a superset
of an ancestor?s words and dependencies, then it
is subsumed by the (more general) ancestor and
discarded.4 When the subsumption process is fin-
ished, a feature remains in the hierarchy only if
4The words that they have in common must also be in the
same relative order.
443
there are no features above it that subsume it.
2.3 Performance-based Subsumption
Representational subsumption is concerned with
whether one feature is more general than another.
But the purpose of using the subsumption hier-
archy is to identify more complex features that
outperform simpler ones. Applying the subsump-
tion hierarchy to features without regard to per-
formance would simply eliminate all features that
have a more general counterpart in the feature set.
For example, all bigrams would be discarded if
their component unigrams were also present in the
hierarchy.
To estimate the quality of a feature, we use In-
formation Gain (IG) because that has been shown
to work well as a metric for feature selection (For-
man, 2003). We will say that feature A be-
haviorally subsumes feature B if two criteria are
met: (1) A representationally subsumes B, and (2)
IG(A) ? IG(B) - ?, where ? is a parameter repre-
senting an acceptable margin of performance dif-
ference. For example, if ?=0 then condition (2)
means that feature A is just as valuable as fea-
ture B because its information gain is the same or
higher. If ?>0 then feature A is allowed to be a lit-
tle worse than feature B, but within an acceptable
margin. For example, ?=.0001 means that A?s in-
formation gain may be up to .0001 lower than B?s
information gain, and that is considered to be an
acceptable performance difference (i.e., A is good
enough that we are comfortable discarding B in
favor of the more general feature A).
Note that based on the subsumption hierarchy
shown in Figure 2, all 1Grams will always sur-
vive the subsumption process because they cannot
be subsumed by any other types of features. Our
goal is to identify complex features that are worth
adding to a set of unigram features.
3 Data Sets
We used three opinion-related data sets for our
analyses and experiments: the OP data set created
by (Wiebe et al, 2004), the Polarity data set5 cre-
ated by (Pang and Lee, 2004), and the MPQA data
set created by (Wiebe et al, 2005).6 The OP and
Polarity data sets involve document-level opinion
classification, while the MPQA data set involves
5Version v2.0, which is available at:
http://www.cs.cornell.edu/people/pabo/movie-review-data/
6Available at http://www.cs.pitt.edu/mpqa/databaserelease/
sentence-level classification.
The OP data consists of 2,452 documents from
the Penn Treebank (Marcus et al, 1993). Metadata
tags assigned by the Wall Street Journal define the
opinion/non-opinion classes: the class of any doc-
ument labeled Editorial, Letter to the Editor, Arts
& Leisure Review, or Viewpoint by the Wall Street
Journal is opinion, and the class of documents in
all other categories (such as Business and News)
is non-opinion. This data set is highly skewed,
with only 9% of the documents belonging to the
opinion class. Consequently, a trivial (but useless)
opinion classifier that labels all documents as non-
opinion articles would achieve 91% accuracy.
The Polarity data consists of 700 positive and
700 negative reviews from the Internet Movie
Database (IMDb) archive. The positive and neg-
ative classes were derived from author ratings ex-
pressed in stars or numerical values. The MPQA
data consists of English language versions of ar-
ticles from the world press. It contains 9,732
sentences that have been manually annotated for
subjective expressions. The opinion/non-opinion
classes are derived from the lower-level annota-
tions: a sentence is an opinion if it contains a sub-
jective expression of medium or higher intensity;
otherwise, it is a non-opinion sentence. 55% of the
sentences belong to the opinion class.
4 Using the Subsumption Hierarchy for
Analysis
In this section, we illustrate how the subsump-
tion hierarchy can be used as an analytic tool to
automatically identify features that substantially
outperform simpler counterparts. These features
represent specialized usages and expressions that
would be good candidates for addition to a sub-
jectivity lexicon. Figure 6 shows pairs of features,
where the first is more general and the second is
more specific. These feature pairs were identified
by the subsumption hierarchy as being representa-
tionally similar but behaviorally different (so the
more specific feature was retained). The IGain
column shows the information gain values pro-
duced from the training set of one cross-validation
fold. The Class column shows the class that the
more specific feature is correlated with (the more
general feature is usually not strongly correlated
with either class).
The top table in Figure 6 contains examples for
the opinion/non-opinion classification task from
444
Opinion/Non-Opinion Classification
ID Feature IGain Class Example
A1 line .0016 - . . . issue consists of notes backed by credit line receivables
A2 the line .0075 opin ...lays it on the line; ...steps across the line
B1 nation .0046 - . . . has 750,000 cable-tv subscribers around the nation
B2 a nation .0080 opin It?s not that we are spawning a nation of ascetics . . .
C1 begin .0006 - Campeau buyers will begin writing orders...
C2 begin with .0036 opin To begin with, we should note that in contrast...
D1 benefits .0040 - . . . earlier period included $235,000 in tax benefits.
DEP NP Prep(benefits to) .0090 opin . . . boon to the rich with no proven benefits to the economy
E1 due .0001 - . . . an estimated $ 1.23 billion in debt due next spring
EEP ActVP Prep(due to) .0038 opin It?s all due to the intense scrutiny...
Positive/Negative Sentiment Classification
ID Feature IGain Class Example
F1 short .0014 - to make a long story short...
F2 nothing short .0039 pos nothing short of spectacular
G1 ugly .0008 - ...an ugly monster on a cruise liner
G2 and ugly .0054 neg it?s a disappointment to see something this dumb and ugly
H1 disaster .0010 - ...rated pg-13 for disaster related elements
HEP AuxVP Dobj(be disaster) .0048 neg . . . this is such a confused disaster of a film
I1 work .0002 - the next day during the drive to work...
IEP ActVP(work) .0062 pos the film will work just as well...
J1 manages .0003 - he still manages to find time for his wife
JEP ActInfVP(manages to keep) .0054 pos this film manages to keep up a rapid pace
Figure 6: Sample features that behave differently, as revealed by the subsumption hierarchy.
(1 ? unigram; 2 ? bigram; EP ? extraction pattern)
the OP data. The more specific features are more
strongly correlated with opinion articles. Surpris-
ingly, simply adding a determiner can dramatically
change behavior. Consider A2. There are many
subjective idioms involving ?the line? (two are
shown in the table; others include ?toe the line?
and ?draw the line?), while objective language
about credit lines, phone lines, etc. uses the deter-
miner less often. Similarly, consider B2. Adding
?a? to ?nation? often corresponds to an abstract
reference used when making an argument (e.g.,
?a nation of ascetics?), whereas other instances
of ?nation? are used more literally (e.g., ?the 6th
largest in the nation?). 21% of feature B1?s in-
stances appear in opinion articles, while 70% of
feature B2?s instances are in opinion articles.
?Begin with? (C2) captures an adverbial phrase
used in argumentation (?To begin with...?) but
does not match objective usages such as ?will
begin? an action. The word ?benets? alone
(D1) matches phrases like ?tax benets? and ?em-
ployee benets? that are not opinion expressions,
while DEP typically matches positive senses of
the word ?benets?. Interestingly, the bigram
?benets to? is not highly correlated with opin-
ions because it matches infinitive phrases such
as ?tax benets to provide? and ?health benets
to cut?. In this case, the extraction pattern ?NP
Prep(benefits to)? is more discriminating than the
bigram for opinion classification. The extraction
pattern EEP is also highly correlated with opin-
ions, while the unigram ?due? and the bigram
?due to? are not.
The bottom table in Figure 6 shows feature
pairs identified for their behavioral differences on
the Polarity data set, where the task is to distin-
guish positive reviews from negative reviews. F2
and G2 are bigrams that behave differently from
their component unigrams. The expression ?noth-
ing short (of)? is typically used to express posi-
tive sentiments, while ?nothing? and ?short? by
themselves are not. The word ?ugly? is often used
as a descriptive modifier that is not expressing
a sentiment per se, while ?and ugly? appears in
predicate adjective constructions that are express-
ing a negative sentiment. The extraction pattern
HEP is more discriminatory than H1 because it
distinguishes negative sentiments (?the lm is a
disaster!?) from plot descriptions (?the disaster
movie...?). IEP shows that active-voice usages of
?work? are strong positive indicators, while the
unigram ?work? appears in a variety of both pos-
itive and negative contexts. Finally, JEP shows
that the expression ?manages to keep? is a strong
positive indicator, while ?manages? by itelf is
much less discriminating.
445
These examples illustrate that the subsumption
hierarchy can be a powerful tool to better under-
stand the behaviors of different kinds of features,
and to identify specific features that may be desir-
able for inclusion in specialized lexical resources.
5 Using the Subsumption Hierarchy to
Reduce Feature Sets
When creating opinion classifiers, people often
throw in a variety of features and trust the ma-
chine learning algorithm to figure out how to make
the best use of them. However, we hypothesized
that classifiers may perform better if we can proac-
tively eliminate features that are not necesary be-
cause they are subsumed by other features. In this
section, we present a series of experiments to ex-
plore this hypothesis. First, we present the results
for an SVM classifier trained using different sets
of unigram, bigram, and extraction pattern fea-
tures, both before and after subsumption. Next, we
evaluate a standard feature selection approach as
an alternative to subsumption and then show that
combining subsumption with standard feature se-
lection produces the best results of all.
5.1 Classification Experiments
To see whether feature subsumption can improve
classification performance, we trained an SVM
classifier for each of the three opinion data sets.
We used the SVMlight (Joachims, 1998) package
with a linear kernel. For the Polarity and OP data
we discarded all features that have frequency < 5,
and for the MPQA data we discarded features that
have frequency < 2 because this data set is sub-
stantially smaller. All of our experimental results
are averages over 3-fold cross-validation.
First, we created 4 baseline classifiers: a 1Gram
classifier that uses only the unigram features; a
1+2Gram classifier that uses unigram and bigram
features; a 1+EP classifier that uses unigram and
extraction pattern features, and a 1+2+EP classi-
fier that uses all three types of features. Next, we
created analogous 1+2Gram, 1+EP, and 1+2+EP
classifiers but applied the subsumption hierar-
chy first to eliminate unnecessary features be-
fore training the classifier. We experimented with
three delta values for the subsumption process:
?=.0005, .001, and .002.
Figures 7, 8, and 9 show the results. The sub-
sumption process produced small but consistent
improvements on all 3 data sets. For example, Fig-
ure 8 shows the results on the OP data, where all
of the accuracy values produced after subsumption
(the rightmost 3 columns) are higher than the ac-
curacy values produced without subsumption (the
Base[line] column). For all three data sets, the best
overall accuracy (shown in boldface) was always
achieved after subsumption.
Features Base ?=.0005 ?=.001 ?=.002
1Gram 79.8
1+2Gram 81.2 81.0 81.3 81.0
1+EP 81.7 81.4 81.4 82.0
1+2+EP 81.7 82.3 82.3 82.7
Figure 7: Accuracies on Polarity Data
Features Base ?=.0005 ?=.001 ?=.002
1Gram 97.5 - - -
1+2Gram 98.0 98.7 98.6 98.7
1+EP 97.2 97.8 97.9 97.9
1+2+EP 97.8 98.6 98.7 98.7
Figure 8: Accuracies on OP Data
Features Base ?=.0005 ?=.001 ?=.002
1Gram 74.8
1+2Gram 74.3 74.9 74.6 74.8
1+EP 74.4 74.6 74.6 74.6
1+2+EP 74.4 74.9 74.7 74.6
Figure 9: Accuracies on MPQA Data
We also observed that subsumption had a dra-
matic effect on the F-measure scores on the OP
data, which are shown in Figure 10. The OP data
set is fundamentally different from the other data
sets because it is so highly skewed, with 91% of
the documents belonging to the non-opinion class.
Without subsumption, the classifier was conser-
vative about assigning documents to the opinion
class, achieving F-measure scores in the 82-88
range. After subsumption, the overall accuracy
improved but the F-measure scores increased more
dramatically. These numbers show that the sub-
sumption process produced not only a more ac-
curate classifier, but a more useful classifier that
identifies more documents as being opinion arti-
cles.
For the MPQA data, we get a very small im-
provement of 0.1% (74.8% ? 74.9%) using sub-
sumption. But note that without subsumption the
performance actually decreased when bigrams and
446
Features Base ?=.0005 ?=.001 ?=.002
1Gram 84.5
1+2Gram 88.0 92.5 92.0 92.3
1+EP 82.4 86.9 87.4 87.4
1+2+EP 86.7 91.8 92.5 92.3
Figure 10: F-measures on OP Data
 97.6
 97.8
 98
 98.2
 98.4
 98.6
 98.8
 99
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.002
Feature Selection
Subsumption ?=0.002 + Feature Selection
Figure 11: Feature Selection on OP Data
extraction patterns were added! The subsumption
process counteracted the negative effect of adding
the more complex features.
5.2 Feature Selection Experiments
We conducted a second series of experiments to
determine whether a traditional feature selection
approach would produce the same, or better, im-
provements as subsumption. For each feature, we
computed its information gain (IG) and then se-
lected the N features with the highest scores.7 We
experimented with values of N ranging from 1,000
to 10,000 in increments of 1,000.
We hypothesized that applying subsumption be-
fore traditional feature selection might also help to
identify a more diverse set of high-performing fea-
tures. In a parallel set of experiments, we explored
this hypothesis by first applying subsumption to
reduce the size of the feature set, and then select-
ing the best N features using information gain.
Figures 11, 12, and 13 show the results of these
experiments for the 1+2+EP classifiers. Each
graph shows four lines. One line corresponds to
the baseline classifier with no subsumption, and
another line corresponds to the baseline classifier
with subsumption using the best ? value for that
data set. Each of these two lines corresponds to
7In the case of ties, we included all features with the same
score as the Nth-best as well.
 78
 78.5
 79
 79.5
 80
 80.5
 81
 81.5
 82
 82.5
 83
 83.5
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.002
Feature Selection
Subsumption ?=0.002 + Feature Selection
Figure 12: Feature Selection on Polarity Data
 72
 72.5
 73
 73.5
 74
 74.5
 75
 75.5
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.0005
Feature Selection
Subsumption ?=0.0005 + Feature Selection
Figure 13: Feature Selection on MPQA Data
just a single data point (accuracy value), but we
drew that value as a line across the graph for the
sake of comparison. The other two lines on the
graph correspond to (a) feature selection for dif-
ferent values of N (shown on the x-axis), and (b)
subsumption followed by feature selection for dif-
ferent values of N.
On all 3 data sets, traditional feature selection
performs worse than the baseline in some cases,
and it virtually never outperforms the best classi-
fier trained after subsumption (but without feature
selection). Furthermore, the combination of sub-
sumption plus feature selection generally performs
best of all, and nearly always outperforms feature
selection alone. For all 3 data sets, our best ac-
curacy results were achieved by performing sub-
sumption prior to feature selection. The best accu-
racy results are 99.0% on the OP data, 83.1% on
the Polarity data, and 75.4% on the MPQA data.
For the OP data, the improvement over baseline
for both accuracy and F-measure are statistically
significant at the p < 0.05 level (paired t-test). For
the MPQA data, the improvement over baseline is
447
statistically significant at the p < 0.10 level.
6 Related Work
Many features and classification algorithms have
been explored in sentiment analysis and opinion
recognition. Lexical cues of differing complexi-
ties have been used, including single words and
Ngrams (e.g., (Mullen and Collier, 2004; Pang et
al., 2002; Turney, 2002; Yu and Hatzivassiloglou,
2003; Wiebe et al, 2004)), as well as phrases
and lexico-syntactic patterns (e.g, (Kim and Hovy,
2004; Hu and Liu, 2004; Popescu and Etzioni,
2005; Riloff and Wiebe, 2003; Whitelaw et al,
2005)). While many of these studies investigate
combinations of features and feature selection,
this is the first work that uses the notion of sub-
sumption to compare Ngrams and lexico-syntactic
patterns to identify complex features that outper-
form simpler counterparts and to reduce a com-
bined feature set to improve opinion classification.
7 Conclusions
This paper uses a subsumption hierarchy of
feature representations as (1) an analytic tool
to compare features of different complexities,
and (2) an automatic tool to remove unneces-
sary features to improve opinion classification
performance. Experiments with three opinion
data sets showed that subsumption can improve
classification accuracy, especially when combined
with feature selection.
Acknowledgments
This research was supported by NSF Grants IIS-
0208798 and IIS-0208985, the ARDA AQUAINT
Program, and the Institute for Scientific Comput-
ing Research and the Center for Applied Scientific
Computing within Lawrence Livermore National
Laboratory.
References
S. Banerjee and T. Pedersen. 2003. The Design, Imple-
mentation, and Use of the Ngram Statistics Package.
In Proc. Fourth Int?l Conference on Intelligent Text
Processing and Computational Linguistics.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis.
In Proc. CIKM-05.
G. Forman. 2003. An Extensive Empirical Study of
Feature Selection Metrics for Text Classification. J.
Mach. Learn. Res., 3:1289?1305.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. KDD-04.
T. Joachims. 1998. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proc. COLING-04.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Mullen and N. Collier. 2004. Sentiment Analysis
Using Support Vector Machines with Diverse Infor-
mation Sources. In Proc. EMNLP-04.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. ACL-04.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification using Machine Learn-
ing Techniques. In Proc. EMNLP-02.
A-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc.
HLT-EMNLP-05.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff and J. Wiebe. 2003. Learning Extraction Pat-
terns for Subjective Expressions. In Proc. EMNLP-
03.
E. Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction
in Three Domains. Artificial Intelligence, 85:101?
134.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL-02.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Proc.
CIKM-05.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Mar-
tin. 2004. Learning subjective language. Computa-
tional Linguistics, 30(3):277?308.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2/3).
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. EMNLP-03.
448
Using WordNet-based Context Vectors
to Estimate the Semantic Relatedness of Concepts
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT, 84112, USA
sidd@cs.utah.edu
Ted Pedersen
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN, 55812, USA
tpederse@d.umn.edu
Abstract
In this paper, we introduce a WordNet-
based measure of semantic relatedness
by combining the structure and content
of WordNet with co?occurrence informa-
tion derived from raw text. We use the
co?occurrence information along with the
WordNet definitions to build gloss vectors
corresponding to each concept in Word-
Net. Numeric scores of relatedness are as-
signed to a pair of concepts by measuring
the cosine of the angle between their re-
spective gloss vectors. We show that this
measure compares favorably to other mea-
sures with respect to human judgments
of semantic relatedness, and that it per-
forms well when used in a word sense dis-
ambiguation algorithm that relies on se-
mantic relatedness. This measure is flex-
ible in that it can make comparisons be-
tween any two concepts without regard to
their part of speech. In addition, it can
be adapted to different domains, since any
plain text corpus can be used to derive the
co?occurrence information.
1 Introduction
Humans are able to quickly judge the relative se-
mantic relatedness of pairs of concepts. For exam-
ple, most would agree that feather is more related
to bird than it is to tree.
This ability to assess the semantic relatedness
among concepts is important for Natural Lan-
guage Understanding. Consider the following sen-
tence: He swung the bat, hitting the ball into the
stands. A reader likely uses domain knowledge of
sports along with the realization that the baseball
senses of hitting, bat, ball and stands are all se-
mantically related, in order to determine that the
event being described is a baseball game.
Consequently, a number of techniques have
been proposed over the years, that attempt to au-
tomatically compute the semantic relatedness of
concepts to correspond closely with human judg-
ments (Resnik, 1995; Jiang and Conrath, 1997;
Lin, 1998; Leacock and Chodorow, 1998). It has
also been shown that these techniques prove use-
ful for tasks such as word sense disambiguation
(Patwardhan et al, 2003), real-word spelling cor-
rection (Budanitsky and Hirst, 2001) and informa-
tion extraction (Stevenson and Greenwood, 2005),
among others.
In this paper we introduce a WordNet-based
measure of semantic relatedness inspired by Har-
ris? Distributional Hypothesis (Harris, 1985). The
distributional hypothesis suggests that words that
are similar in meaning tend to occur in similar lin-
guistic contexts. Additionally, numerous studies
(Carnine et al, 1984; Miller and Charles, 1991;
McDonald and Ramscar, 2001) have shown that
context plays a vital role in defining the mean-
ings of words. (Landauer and Dumais, 1997) de-
scribe a context vector-based method that simu-
lates learning of word meanings from raw text.
(Schu?tze, 1998) has also shown that vectors built
from the contexts of words are useful representa-
tions of word meanings.
Our Gloss Vector measure of semantic related-
ness is based on second order co?occurrence vec-
tors (Schu?tze, 1998) in combination with the struc-
ture and content of WordNet (Fellbaum, 1998), a
semantic network of concepts. This measure cap-
tures semantic information for concepts from con-
textual information drawn from corpora of text.
We show that this measure compares favorably
1
to other measures with respect to human judg-
ments of semantic relatedness, and that it performs
well when used in a word sense disambiguation al-
gorithm that relies on semantic relatedness. This
measure is flexible in that it can make comparisons
between any two concepts without regard to their
part of speech. In addition, it is adaptable since
any corpora can be used to derive the word vec-
tors.
This paper is organized as follows. We start
with a description of second order context vectors
in general, and then define the Gloss Vector mea-
sure in particular. We present an extensive evalua-
tion of the measure, both with respect to human re-
latedness judgments and also relative to its perfor-
mance when used in a word sense disambiguation
algorithm based on semantic relatedness. The pa-
per concludes with an analysis of our results, and
some discussion of related and future work.
2 Second Order Context Vectors
Context vectors are widely used in Information
Retrieval and Natural Language Processing. Most
often they represent first order co?occurrences,
which are simply words that occur near each other
in a corpus of text. For example, police and car are
likely first order co?occurrences since they com-
monly occur together. A first order context vector
for a given word would simply indicate all the first
order co?occurrences of that word as found in a
corpus.
However, our Gloss Vector measure is based on
second order co?occurrences (Schu?tze, 1998). For
example, if car and mechanic are first order co?
occurrences, then mechanic and police would be
second order co?occurrences since they are both
first order co?occurrences of car.
Schu?tze?s method starts by creating a Word
Space, which is a co?occurrence matrix where
each row can be viewed as a first order context
vector. Each cell in this matrix represents the fre-
quency with which two words occur near one an-
other in a corpus of text. The Word Space is usu-
ally quite large and sparse, since there are many
words in the corpus and most of them don?t occur
near each other. In order to reduce the dimension-
ality and the amount of noise, non?content stop
words such as the, for, a, etc. are excluded from
being rows or columns in the Word Space.
Given a Word Space, a context can then be rep-
resented by second order co?occurrences (context
vector). This is done by finding the resultant of the
first order context vectors corresponding to each
of the words in that context. If a word in a context
does not have a first order context vector created
for it, or if it is a stop word, then it is excluded
from the resultant.
For example, suppose we have the following
context:
The paintings were displayed in the art
gallery.
The second order context vector would be the
resultant of the first order context vectors for
painting, display, art, and gallery. The words
were, in, and the are excluded from the resultant
since we consider them as stop words in this ex-
ample. Figure 1 shows how the second order con-
text vector might be visualized in a 2-dimensional
space.
dim1
dim2
Context
Vector
gallery
display
art
painting
Figure 1: Creating a context vector from word vec-
tors
Intuitively, the orientation of each second order
context vector is an indicator of the domains or
topics (such as biology or baseball) that the con-
text is associated with. Two context vectors that lie
close together indicate a considerable contextual
overlap, which suggests that they are pertaining to
the same meaning of the target word.
3 Gloss Vectors in Semantic Relatedness
In this research, we create a Gloss Vector for each
concept (or word sense) represented in a dictio-
nary. While we use WordNet as our dictionary,
the method can apply to other lexical resources.
3.1 Creating Vectors from WordNet Glosses
A Gloss Vector is a second order context vector
formed by treating the dictionary definition of a
2
concept as a context, and finding the resultant of
the first order context vectors of the words in the
definition.
In particular, we define a Word Space by cre-
ating first order context vectors for every word w
that is not a stop word and that occurs above a min-
imum frequency in our corpus. The specific steps
are as follows:
1. Initialize the first order context vector to a
zero vector
?w.
2. Find every occurrence of w in the given cor-
pus.
3. For each occurrence of w, increment those di-
mensions of ?w that correspond to the words
from the Word Space and are present within
a given number of positions around w in the
corpus.
The first order context vector ?w, therefore, en-
codes the co?occurrence information of word w.
For example, consider the gloss of lamp ? an ar-
tificial source of visible illumination. The Gloss
Vector for lamp would be formed by adding the
first order context vectors of artificial, source, vis-
ible and illumination.
In these experiments, we use WordNet as the
corpus of text for deriving first order context vec-
tors. We take the glosses for all of the concepts
in WordNet and view that as a large corpus of
text. This corpus consists of approximately 1.4
million words, and results in a Word Space of
approximately 20,000 dimensions, once low fre-
quency and stop words are removed. We chose the
WordNet glosses as a corpus because we felt the
glosses were likely to contain content rich terms
that would distinguish between the various con-
cepts more distinctly than would text drawn from
a more generic corpus. However, in our future
work we will experiment with other corpora as the
source of first order context vectors, and other dic-
tionaries as the source of glosses.
The first order context vectors as well as the
Gloss Vectors usually have a very large number
of dimensions (usually tens of thousands) and it is
not easy to visualize this space. Figure 2 attempts
to illustrate these vectors in two dimensions. The
words tennis and food are the dimensions of this 2-
dimensional space. We see that the first order con-
text vector for serve is approximately halfway be-
tween tennis and food, since the word serve could
Normalized
gloss vector
for "fork"
Food
Tennis
Eat
Serve
= Word Vector
= Gloss Vector
Cutlery
Figure 2: First Order Context Vectors and a Gloss
Vector
mean to ?serve the ball? in the context of tennis or
could mean ?to serve food? in another context.
The first order context vectors for eat and cut-
lery are very close to food, since they do not have
a sense that is related to tennis. The gloss for the
word fork, ?cutlery used to serve and eat food?,
contains the words cutlery, serve, eat and food.
The Gloss Vector for fork is formed by adding the
first order context vectors of cutlery, serve, eat and
food. Thus, fork has a Gloss Vector which is heav-
ily weighted towards food. The concept of food,
therefore, is in the same semantic space as and is
related to the concept of fork.
Similarly, we expect that in a high dimensional
space, the Gloss Vector of fork would be heavily
weighted towards all concepts that are semanti-
cally related to the concept of fork. Additionally,
the previous demonstration involved a small gloss
for representing fork. Using augmented glosses,
described in section 3.2, we achieve better repre-
sentations of concepts to build Gloss Vectors upon.
3.2 Augmenting Glosses Using WordNet
Relations
The formulation of the Gloss Vector measure de-
scribed above is independent of the dictionary
used and is independent of the corpus used. How-
ever, dictionary glosses tend to be rather short, and
it is possible that even closely related concepts will
be defined using different sets of words. Our be-
lief is that two synonyms that are used in different
glosses will tend to have similar Word Vectors (be-
cause their co?occurrence behavior should be sim-
ilar). However, the brevity of dictionary glosses
may still make it difficult to create Gloss Vectors
that are truly representative of the concept.
3
(Banerjee and Pedersen, 2003) encounter a sim-
ilar issue when measuring semantic relatedness by
counting the number of matching words between
the glosses of two different concepts. They ex-
pand the glosses of concepts in WordNet with the
glosses of concepts that are directly linked by a
WordNet relation. We adopt the same technique
here, and use the relations in WordNet to augment
glosses for the Gloss Vector measure. We take the
gloss of a given concept, and concatenate to it the
glosses of all the concepts to which it is directly
related according to WordNet. The Gloss Vector
for that concept is then created from this big con-
catenated gloss.
4 Other Measures of Relatedness
Below we briefly describe five alternative mea-
sures of semantic relatedness, and then go on to
include them as points of comparison in our exper-
imental evaluation of the Gloss Vector measure.
All of these measures depend in some way upon
WordNet. Four of them limit their measurements
to nouns located in the WordNet is-a hierarchy.
Each of these measures takes two WordNet con-
cepts (i.e., word senses or synsets) c1 and c2 as in-
put and return a numeric score that quantifies their
degree of relatedness.
(Leacock and Chodorow, 1998) finds the path
length between c1 and c2 in the is-a hierarchy of
WordNet. The path length is then scaled by the
depth of the hierarchy (D) in which they reside to
obtain the relatedness of the two concepts.
(Resnik, 1995) introduced a measure that is
based on information content, which are numeric
quantities that indicate the specificity of concepts.
These values are derived from corpora, and are
used to augment the concepts in WordNet?s is-a hi-
erarchy. The measure of relatedness between two
concepts is the information content of the most
specific concept that both concepts have in com-
mon (i.e., their lowest common subsumer in the
is-a hierarchy).
(Jiang and Conrath, 1997) extends Resnik?s
measure to combine the information contents of
c1, c2 and their lowest common subsumer.
(Lin, 1998) also extends Resnik?s measure, by
taking the ratio of the shared information content
to that of the individual concepts.
(Banerjee and Pedersen, 2003) introduce Ex-
tended Gloss Overlaps, which is a measure that de-
termines the relatedness of concepts proportional
to the extent of overlap of their WordNet glosses.
This simple definition is extended to take advan-
tage of the complex network of relations in Word-
Net, and allows the glosses of concepts to include
the glosses of synsets to which they are directly
related in WordNet.
5 Evaluation
As was done by (Budanitsky and Hirst, 2001), we
evaluated the measures of relatedness in two ways.
First, they were compared against human judg-
ments of relatedness. Second, they were used in an
application that would benefit from the measures.
The effectiveness of the particular application was
an indirect indicator of the accuracy of the related-
ness measure used.
5.1 Comparison with Human Judgment
One obvious metric for evaluating a measure of se-
mantic relatedness is its correspondence with the
human perception of relatedness. Since semantic
relatedness is subjective, and depends on the hu-
man view of the world, comparison with human
judgments is a self-evident metric for evaluation.
This was done by (Budanitsky and Hirst, 2001) in
their comparison of five measures of semantic re-
latedness. We follow a similar approach in evalu-
ating the Gloss Vector measure.
We use a set of 30 word pairs from a study
carried out by (Miller and Charles, 1991). These
word pairs are a subset of 65 word pairs used by
(Rubenstein and Goodenough, 1965), in a similar
study almost 25 years earlier. In this study, human
subjects assigned relatedness scores to the selected
word pairs. The word pairs selected for this study
ranged from highly related pairs to unrelated pairs.
We use these human judgments for our evaluation.
Each of the word pairs have been scored by hu-
mans on a scale of 0 to 5, where 5 is the most re-
lated. The mean of the scores of each pair from all
subjects is considered as the ?human relatedness
score? for that pair. The pairs are then ranked with
respect to their scores. The most related pair is the
first on the list and the least related pair is at the
end of the list. We then have each of the measures
of relatedness score the word pairs and a another
ranking of the word pairs is created corresponding
to each of the measures.
4
Table 1: Correlation to human perception
Relatedness Measures M & C R & G
Gloss Vector 0.91 0.90
Extended Gloss Overlaps 0.81 0.83
Jiang & Conrath 0.73 0.75
Resnik 0.72 0.72
Lin 0.70 0.72
Leacock & Chodorow 0.74 0.77
Spearman?s Correlation Coefficient (Spearman,
1904) is used to assess the equivalence of two
rankings. If the two rankings are exactly the
same, the Spearman?s correlation coefficient be-
tween these two rankings is 1. A completely re-
versed ranking gets a value of ?1. The value is 0
when there is no relation between the rankings.
We determine the correlation coefficient of the
ranking of each measure with that of the human
relatedness. We use the relatedness scores from
both the human studies ? the Miller and Charles
study as well as the Rubenstein and Goodenough
research. Table 1 summarizes the results of our
experiment. We observe that the Gloss Vector has
the highest correlation with humans in both cases.
Note that in our experiments with the Gloss
Vector measure, we have used not only the gloss
of the concept but augmented that with the gloss
of all the concepts directly related to it accord-
ing to WordNet. We observed a significant drop
in performance when we used just the glosses of
the concept alone, showing that the expansion is
necessary. In addition, the frequency cutoffs used
to construct the Word Space played a critical role.
The best setting of the frequency cutoffs removed
both low and high frequency words, which elimi-
nates two different sources of noise. Very low fre-
quency words do not occur enough to draw dis-
tinctions among different glosses, whereas high
frequency words occur in many glosses, and again
do not provide useful information to distinguish
among glosses.
5.2 Application-based Evaluation
An application-oriented comparison of five mea-
sures of semantic relatedness was presented in
(Budanitsky and Hirst, 2001). In that study they
evaluate five WordNet-based measures of seman-
tic relatedness with respect to their performance in
context sensitive spelling correction.
We present the results of an application-oriented
Table 2: WSD on SENSEVAL-2 (nouns)
Measure Nouns
Jiang & Conrath 0.45
Extended Gloss Overlaps 0.44
Gloss Vector 0.41
Lin 0.36
Resnik 0.30
Leacock & Chodorow 0.30
evaluation of the measures of semantic related-
ness. Each of the seven measures of semantic re-
latedness was used in a word sense disambigua-
tion algorithm described by (Banerjee and Peder-
sen, 2003).
Word sense disambiguation is the task of deter-
mining the meaning (from multiple possibilities)
of a word in its given context. For example, in the
sentence The ex-cons broke into the bank on Elm
street, the word bank has the ?financial institution?
sense as opposed to the ?edge of a river? sense.
Banerjee and Pedersen attempt to perform this
task by measuring the relatedness of the senses of
the target word to those of the words in its context.
The sense of the target word that is most related to
its context is selected as the intended sense of the
target word.
The experimental data used for this evaluation
is the SENSEVAL-2 test data. It consists of 4,328
instances (or contexts) that each includes a single
ambiguous target word. Each instance consists of
approximately 2-3 sentences and one occurrence
of a target word. 1,754 of the instances include
nouns as target words, while 1,806 are verbs and
768 are adjectives. We use the noun data to com-
pare all six of the measures, since four of the mea-
sures are limited to nouns as input. The accuracy
of disambiguation when performed using each of
the measures for nouns is shown in Table 2.
6 Gloss Vector Tuning
As discussed in earlier sections, the Gloss Vector
measure builds a word space consisting of first or-
der context vectors corresponding to every word in
a corpus. Gloss vectors are the resultant of a num-
ber of first order context vectors. All of these vec-
tors encode semantic information about the con-
cepts or the glosses that the vectors represent.
We note that the quality of the words used as the
dimensions of these vectors plays a pivotal role in
5
getting accurate relatedness scores. We find that
words corresponding to very specific concepts and
are highly indicative of a few topics, make good
dimensions. Words that are very general in nature
and that appear all over the place add noise to the
vectors.
In an earlier section we discussed using stop
words and frequency cutoffs to keep only the high
?information content? words. In addition to those,
we also experimented with a term frequency ? in-
verse document frequency cutoff.
Term frequency and inverse document frequency
are commonly used metrics in information re-
trieval. For a given word, term frequency (tf ) is
the number of times a word appears in the corpus.
The document frequency is number of documents
in which the word occurs. Inverse document fre-
quency (idf ) is then computed as
idf = logNumber of DocumentsDocument Frequency (1)
The tf ? idf value is an indicator of the speci-
ficity of a word. The higher the tf ? idf value, the
lower the specificity.
Figure 3 shows a plot of tf ? idf cutoff on the
x-axis against the correlation of the Gloss Vector
measure with human judgments on the y-axis.
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  500  1000  1500  2000  2500  3000  3500  4000  4500
Co
rre
la
tio
n
tf.idf cutoff
M&C
R&G
Figure 3: Plot of tf ? idf cutoff vs. correlation
The tf ? idf values ranged from 0 to about 4200.
Note that we get lower correlation as the cutoff is
raised.
7 Analysis
We observe from the experimental results that the
Gloss Vector measure corresponds the most with
human judgment of relatedness (with a correlation
of almost 0.9). We believe this is probably be-
cause the Gloss Vector measure most closely im-
itates the representation of concepts in the human
mind. (Miller and Charles, 1991) suggest that the
cognitive representation of a word is an abstrac-
tion derived from its contexts (encountered by the
person). Their study also suggested the semantic
similarity of two words depends on the overlap be-
tween their contextual representations. The Gloss
Vector measure uses the contexts of the words and
creates a vector representation of these. The over-
lap between these vector representations is used to
compute the semantic similarity of concepts.
(Landauer and Dumais, 1997) additionally per-
form singular value decomposition (SVD) on their
context vector representation of words and they
show that reducing the number of dimensions of
the vectors using SVD more accurately simulates
learning in humans. We plan to try SVD on the
Gloss Vector measure in future work.
In the application-oriented evaluation, the Gloss
Vector measure performed relatively well (about
41% accuracy). However, unlike the human study,
it did not outperform all the other measures. We
think there are two possible explanations for this.
First, the word pairs used in the human relatedness
study are all nouns, and it is possible that the Gloss
Vector measure performs better on nouns than on
other parts of speech. In the application-oriented
evaluation the measure had to make judgments for
all parts of speech. Second, the application itself
affects the performance of the measure. The Word
Sense Disambiguation algorithm starts by select-
ing a context of 5 words from around the target
word. These context words contain words from all
parts of speech. Since the Jiang-Conrath measure
assigns relatedness scores only to noun concepts,
its behavior would differ from that of the Vector
measure which would accept all words and would
be affected by the noise introduced from unrelated
concepts. Thus the context selection factors into
the accuracy obtained. However, for evaluating
the measure as being suitable for use in real ap-
plications, the Gloss Vector measure proves rela-
tively accurate.
The Gloss Vector measure can draw conclu-
sions about any two concepts, irrespective of part-
of-speech. The only other measure that can make
this same claim is the Extended Gloss Overlaps
measure. We would argue that Gloss Vectors
present certain advantages over it. The Extended
6
Gloss Overlap measure looks for exact string over-
laps to measure relatedness. This ?exactness?
works against the measure, in that it misses po-
tential matches that intuitively would contribute to
the score (For example, silverware with spoon).
The Gloss Vector measure is more robust than the
Extended Gloss Overlap measure, in that exact
matches are not required to identify relatedness.
The Gloss Vector measure attempts to overcome
this ?exactness? by using vectors that capture the
contextual representation of all words. So even
though silverware and spoon do not overlap, their
contextual representations would overlap to some
extent.
8 Related Work
(Wilks et al, 1990) describe a word sense disam-
biguation algorithm that also uses vectors to de-
termine the intended sense of an ambiguous word.
In their approach, they use dictionary definitions
from LDOCE (Procter, 1978). The words in these
definitions are used to build a co?occurrence ma-
trix, which is very similar to our technique of
using the WordNet glosses for our Word Space.
They augment their dictionary definitions with
similar words, which are determined using the co?
occurrence matrix. Each concept in LDOCE is
then represented by an aggregate vector created by
adding the co?occurrence counts for each of the
words in the augmented definition of the concept.
The next step in their algorithm is to form a con-
text vector. The context of the ambiguous word
is first augmented using the co?occurrence ma-
trix, just like the definitions. The context vector
is formed by taking the aggregate of the word vec-
tors of the words in the augmented context. To
disambiguate the target word, the context vector
is compared to the vectors corresponding to each
meaning of the target word in LDOCE, and that
meaning is selected whose vector is mathemati-
cally closest to that of the context.
Our approach differs from theirs in two primary
respects. First, rather than creating an aggregate
vector for the context we compare the vector of
each meaning of the ambiguous word with the vec-
tors of each of the meanings of the words in the
context. This adds another level of indirection in
the comparison and attempts to use only the rele-
vant meanings of the context words. Secondly, we
use the structure of WordNet to augment the short
glosses with other related glosses.
(Niwa and Nitta, 1994) compare dictionary
based vectors with co?occurrence based vectors,
where the vector of a word is the probability that
an origin word occurs in the context of the word.
These two representations are evaluated by apply-
ing them to real world applications and quantify-
ing the results. Both measures are first applied to
word sense disambiguation and then to the learn-
ing of positives or negatives, where it is required
to determine whether a word has a positive or neg-
ative connotation. It was observed that the co?
occurrence based idea works better for the word
sense disambiguation and the dictionary based ap-
proach gives better results for the learning of pos-
itives or negatives. From this, the conclusion is
that the dictionary based vectors contain some dif-
ferent semantic information about the words and
warrants further investigation. It is also observed
that for the dictionary based vectors, the network
of words is almost independent of the dictionary
that is used, i.e. any dictionary should give us al-
most the same network.
(Inkpen and Hirst, 2003) also use gloss?based
context vectors in their work on the disambigua-
tion of near?synonyms ? words whose senses
are almost indistinguishable. They disambiguate
near?synonyms in text using various indicators,
one of which is context-vector-based. Context
Vectors are created for the context of the target
word and also for the glosses of each sense of the
target word. Each gloss is considered as a bag
of words, where each word has a corresponding
Word Vector. These vectors for the words in a
gloss are averaged to get a Context Vector corre-
sponding to the gloss. The distance between the
vector corresponding to the text and that corre-
sponding to the gloss is measured (as the cosine
of the angle between the vectors). The nearness
of the vectors is used as an indicator to pick the
correct sense of the target word.
9 Conclusion
We introduced a new measure of semantic relat-
edness based on the idea of creating a Gloss Vec-
tor that combines dictionary content with corpus
based data. We find that this measure correlates
extremely well with the results of these human
studies, and this is indeed encouraging. We be-
lieve that this is due to the fact that the context vec-
tor may be closer to the semantic representation
of concepts in humans. This measure can be tai-
7
lored to particular domains depending on the cor-
pus used to derive the co?occurrence matrices, and
makes no restrictions on the parts of speech of the
concept pairs to be compared.
We also demonstrated that the Vector measure
performs relatively well in an application-oriented
setup and can be conveniently deployed in a real
world application. It can be easily tweaked and
modified to work in a restricted domain, such as
bio-informatics or medicine, by selecting a spe-
cialized corpus to build the vectors.
10 Acknowledgments
This research was partially supported by a Na-
tional Science Foundation Faculty Early CAREER
Development Award (#0092784).
All of the experiments in this paper were
carried out with the WordNet::Similarity pack-
age, which is freely available for download from
http://search.cpan.org/dist/WordNet-Similarity.
References
S. Banerjee and T. Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In
Proceedings of the Eighteenth International Confer-
ence on Artificial Intelligence (IJCAI-03), Acapulco,
Mexico, August.
A. Budanitsky and G. Hirst. 2001. Semantic distance
in WordNet: An experimental, application-oriented
evaluation of five measures. In Workshop on Word-
Net and Other Lexical Resources, Second meeting of
the North American Chapter of the Association for
Computational Linguistics, Pittsburgh, June.
D. Carnine, E. J. Kameenui, and G. Coyle. 1984. Uti-
lization of contextual information in determining the
meaning of unfamiliar words. Reading Research
Quarterly, 19:188?204.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
Oxford University Press, New York.
D. Inkpen and G. Hirst. 2003. Automatic sense disam-
biguation of the near-synonyms in a dictionary en-
try. In Proceedings of the 4th Conference on Intel-
ligent Text Processing and Computational Linguis-
tics (CICLing-2003), pages 258?267, Mexico City,
February.
J. Jiang and D. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, Taiwan.
T. K. Landauer and S. T. Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104:211?240.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database, pages 265?283. MIT Press.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of International Confer-
ence on Machine Learning, Madison, Wisconsin,
August.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of the 23rd Annual Conference of the Cognitive Sci-
ence Society, Edinburgh, Scotland.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Y. Niwa and Y. Nitta. 1994. Co-occurrence vec-
tors from corpora versus distance vectors from dic-
tionaries. In Proceedings of the Fifteenth Inter-
national Conference on Computational Linguistics,
pages 304?309, Kyoto, Japan.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics (CICLING-
03), Mexico City, Mexico, February.
P. Procter, editor. 1978. Longman Dictionary of Con-
temporary English. Longman Group Ltd., Essex,
UK.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, Montreal, August.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8:627?633, October.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
C. Spearman. 1904. Proof and measurement of as-
sociation between two things. American Journal of
Psychology, 15:72?101.
M. Stevenson and M. Greenwood. 2005. A seman-
tic approach to ie pattern induction. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, Michigan, June.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and
B. Slator. 1990. Providing machine tractable dictio-
nary tools. Machine Translation, 5:99?154.
8
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 390?393,
Prague, June 2007. c?2007 Association for Computational Linguistics
UMND1: Unsupervised Word Sense Disambiguation Using Contextual
Semantic Relatedness
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112.
sidd@cs.utah.edu
Satanjeev Banerjee
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15217.
banerjee@cs.cmu.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812.
tpederse@d.umn.edu
Abstract
In this paper we describe an unsuper-
vised WordNet-based Word Sense Disam-
biguation system, which participated (as
UMND1) in the SemEval-2007 Coarse-
grained English Lexical Sample task. The
system disambiguates a target word by using
WordNet-based measures of semantic relat-
edness to find the sense of the word that
is semantically most strongly related to the
senses of the words in the context of the tar-
get word. We briefly describe this system,
the configuration options used for the task,
and present some analysis of the results.
1 Introduction
WordNet::SenseRelate::TargetWord1 (Patwardhan
et al, 2005; Patwardhan et al, 2003) is an unsuper-
vised Word Sense Disambiguation (WSD) system,
which is based on the hypothesis that the intended
sense of an ambiguous word is related to the
words in its context. For example, if the ?financial
institution? sense of bank is intended in a context,
then it is highly likely the context would contain
related words such as money, transaction, interest
rate, etc. The algorithm, therefore, determines
the intended sense of a word (target word) in a
given context by measuring the relatedness of each
sense of that word with the words in its context.
The sense of the target word that is most related
to its context is selected as the intended sense of
the target word. The system uses WordNet-based
1http://senserelate.sourceforge.net
measures of semantic relatedness2 (Pedersen et
al., 2004) to measure the relatedness between the
different senses of the target word and the words in
its context.
This system is completely unsupervised and re-
quires no annotated data for training. The lexical
database WordNet (Fellbaum, 1998) is the only re-
source that the system uses to measure the related-
ness between words and concepts. Thus, our system
is classified under the closed track of the task.
2 System Description
Our WSD system consists of a modular framework,
which allows different algorithms for the different
subtasks to be plugged into the system. We divide
the disambiguation task into two primary subtasks:
context selection and sense selection. The context
selection module tries to select words from the con-
text that are most likely to be indicative of the sense
of the target word. The sense selection module then
uses the set of selected context words to choose one
of the senses of the target word as the answer.
Figure 1 shows a block schematic of the system,
which takes SemEval-2007 English Lexical Sample
instances as input. Each instance is a made up of
a few English sentences, and one word from these
sentences is marked as the target word to be dis-
ambiguated. The system processes each instance
through multiple modules arranged in a sequential
pipeline. The final output of the pipeline is the sense
that is most appropriate for the target word in the
given context.
2http://wn-similarity.sourceforge.net
390
Instance
Preprocessing
Format Filter
Target Sense
Context Selection
Postprocessing
Sense Selection Relatedness Measure
Figure 1: System Architecture
2.1 Data Preparation
The input text is first passed through a format fil-
ter, whose task is to parse the input XML file. This
is followed by a preprocessing step. Each instance
passed to the preprocessing stage is first segmented
into words, and then all compound words are iden-
tified. Any sequence of words known to be a com-
pound in WordNet is combined into a single entity.
2.2 Context Selection
Although each input instance consists of a large
number of words, only a few of these are likely to
be useful for disambiguating the target word. We
use the context selection algorithm to select a subset
of the context words to be used for sense selection.
By removing the unimportant words, the computa-
tional complexity of the algorithm is reduced.
In this work, we use the NearestWords context
selection algorithm. This algorithm algorithm se-
lects 2n + 1 content words surrounding the target
word (including the target word) as the context. A
stop list is used to identify closed-class non-content
words. Additionally, any word not found in Word-
Net is also discarded. The algorithm then selects n
content words before and n content words follow-
ing the target word, and passes this unordered set of
2n + 1 words to the Sense Selection module.
2.3 Sense Selection Algorithm
The sense selection module takes the set of words
output by the context selection module, one of which
is the target word to be disambiguated. For each of
the words in this set, it retrieves a list of senses from
WordNet, based on which it determines the intended
sense of the target word.
The package provides two main algorithms for
Sense Selection: the local and the global algorithms,
as described in previous work (Banerjee and Peder-
sen, 2002; Patwardhan et al, 2003). In this work,
we use the local algorithm, which is faster and was
shown to perform as well as the global algorithm.
The local sense selection algorithm measures the
semantic relatedness of each sense of the target word
with the senses of the words in the context, and se-
lects that sense of the target word which is most re-
lated to the context word-senses. Given the 2n + 1
context words, the system scores each sense of the
target word. Suppose the target word t has T senses,
enumerated as t1, t2, . . . , tT . Also, suppose w1, w2,
. . . , w2n are the words in the context of t, each hav-
ing W1, W2, . . . , W2n senses, respectively. Then for
each ti a score is computed as
score(ti) =
2n
?
j=1
max
k=1 to Wj
(relatedness(ti, wjk))
where wjk is the kth sense of word wj . The sense ti
of target word t with the highest score is selected as
the intended sense of the target word.
The relatedness between two word senses is com-
puted using a measure of semantic relatedness de-
fined in the WordNet::Similarity software package
(Pedersen et al, 2004), which is a suite of Perl mod-
ules implementing a number WordNet-based mea-
sures of semantic relatedness. For this work, we
used the Context Vector measure (Patwardhan and
Pedersen, 2006). The relatedness of concepts is
computed based on word co-occurrence statistics
derived from WordNet glosses. Given two WordNet
senses, this module returns a score between 0 and 1,
indicating the relatedness of the two senses.
Our system relies on WordNet as its sense inven-
tory. However, this task used OntoNotes (Hovy et
al., 2006) as the sense inventory. OntoNotes word
senses are groupings of similar WordNet senses.
Thus, we used the training data answer key to gen-
erate a mapping between the OntoNotes senses of
the given lexical elements and their corresponding
WordNet senses. We had to manually create the
mappings for some of the WordNet senses, which
had no corresponding OntoNotes senses. The sense
selection algorithm performed all of its computa-
tions with respect to the WordNet senses, and finally
the OntoNotes sense corresponding to the selected
WordNet sense of the target word was output as the
391
answer for each instance.
3 Results and Analysis
For this task, we used the freely available Word-
Net::SenseRelate::TargetWord v0.10 and the Word-
Net::Similarity v1.04 packages. WordNet v2.1 was
used as the underlying knowledge base for these.
The context selection module used a window size
of five (including the target word). The semantic re-
latedness of concepts was measured using the Con-
text Vector measure, with configuration options as
defined in previous research (Patwardhan and Ped-
ersen, 2006). Since we always predict exactly one
sense for each instance, the precision and recall val-
ues of all our experiments were always the same.
Therefore, in this section we will use the name ?ac-
curacy? to mean both precision and recall.
3.1 Overall Results, and Baselines
The overall accuracy of our system on the test data
is 0.538. This represents 2,609 correctly disam-
biguated instances, out of a total of 4,851 instances.
As baseline, we compare against the random al-
gorithm where for each instance, we randomly pick
one of the WordNet senses for the lexical element
in that instance, and report the OntoNotes senseid it
maps to as the answer. This algorithm gets an ac-
curacy of 0.417. Thus, our algorithm gets an im-
provement of 12% absolute (29% relative) over this
random baseline.
Additionally, we compare our algorithm against
the WordNet SenseOne algorithm. In this algorithm,
we pick the first sense among the WordNet senses
of the lexical element in each instance, and report
its corresponding OntoNotes sense as the answer for
that instance. This algorithm leverages the fact that
(in most cases) the WordNet senses for a particular
word are listed in the database in descending order
of their frequency of occurrence in the corpora from
which the sense inventory was created. If the new
test data has a similar distribution of senses, then this
algorithm amounts to a ?majority baseline?. This
algorithm achieves an accuracy of 0.681 which is
15% absolute (27% relative) better than our algo-
rithm. Although this seemingly na??ve algorithm out-
performs our algorithm, we choose to avoid using
this information in our algorithms because it repre-
sents a large amount of human supervision in the
form of manual sense tagging of text, whereas our
goal is to create a purely unsupervised algorithm.
Additionally, our algorithms can, with little change,
work with other sense inventories besides WordNet
that may not have this information.
3.2 Results Disaggregated by Part of Speech
In our past experience, we have found that av-
erage disambiguation accuracy differs significantly
between words of different parts of speech. For the
given test data, we separately evaluated the noun and
verb instances. We obtained an accuracy of 0.399
for the noun targets and 0.692 for the verb targets.
Thus, we find that our algorithm performs much bet-
ter on verbs than on nouns, when evaluated using the
OntoNotes sense inventory. This is different from
our experience with SENSEVAL data from previous
years where performance on nouns was uniformly
better than that on verbs. One possible reason for the
better performance on verbs is that the OntoNotes
sense inventory has, on average, fewer senses per
verb word (4.41) than per noun word (5.71). How-
ever, additional experimentation is needed to more
fully understand the difference in performance.
3.3 Results Disaggregated by Lexical Element
To gauge the accuracy of our algorithm on different
words (lexical elements), we disaggregated the re-
sults by individual word. Table 1 lists the accuracy
values over instances of individual verb lexical ele-
ments, and Table 2 lists the accuracy values for noun
lexical elements. Our algorithm gets all instances
correct for 13 verb lexical elements, and for none of
the noun lexical elements. More generally, our al-
gorithm gets an accuracy of 50% or more on 45 out
of the 65 verb lexical elements, and on 15 out of the
35 noun lexical elements. For nouns, when the ac-
curacy results are viewed in sorted order (as in Table
2), one can observe a sudden degradation of results
between the accuracy of the word system.n ? 0.443
? and the word source.n ? 0.257. It is unclear why
there is such a jump; there is no such sudden degra-
dation in the results for the verb lexical elements.
4 Conclusions
This paper describes our system UMND1, which
participated in the SemEval-2007 Coarse-grained
392
Word Accuracy Word Accuracy
remove 1.000 purchase 1.000
negotiate 1.000 improve 1.000
hope 1.000 express 1.000
exist 1.000 estimate 1.000
describe 1.000 cause 1.000
avoid 1.000 attempt 1.000
affect 1.000 say 0.969
explain 0.944 complete 0.938
disclose 0.929 remember 0.923
allow 0.914 announce 0.900
kill 0.875 occur 0.864
do 0.836 replace 0.800
maintain 0.800 complain 0.786
believe 0.764 receive 0.750
approve 0.750 buy 0.739
produce 0.727 regard 0.714
propose 0.714 need 0.714
care 0.714 feel 0.706
recall 0.667 examine 0.667
claim 0.667 report 0.657
find 0.607 grant 0.600
work 0.558 begin 0.521
build 0.500 keep 0.463
go 0.459 contribute 0.444
rush 0.429 start 0.421
raise 0.382 end 0.381
prove 0.364 enjoy 0.357
see 0.296 set 0.262
promise 0.250 hold 0.250
lead 0.231 prepare 0.222
join 0.222 ask 0.207
come 0.186 turn 0.048
fix 0.000
Table 1: Verb Lexical Element Accuracies
English Lexical Sample task. The system is based
on WordNet::SenseRelate::TargetWord, which is a
freely available unsupervised Word Sense Disam-
biguation software package. The system uses
WordNet-based measures of semantic relatedness to
select the intended sense of an ambiguous word. The
system required no training data and using WordNet
as its only knowledge source achieved an accuracy
of 54% on the blind test set.
Acknowledgments
This research was partially supported by a National
Science Foundation Early CAREER Development
award (#0092784).
References
S. Banerjee and T. Pedersen. 2002. An Adapted Lesk Al-
gorithm for Word Sense Disambiguation Using Word-
Net. In Proceedings of the Third International Con-
Word Accuracy Word Accuracy
policy 0.949 people 0.904
future 0.870 drug 0.870
space 0.857 capital 0.789
effect 0.767 condition 0.765
job 0.692 bill 0.686
area 0.676 base 0.650
management 0.600 power 0.553
development 0.517 chance 0.467
exchange 0.459 order 0.456
part 0.451 president 0.446
system 0.443 source 0.257
network 0.218 state 0.208
share 0.192 rate 0.186
hour 0.167 plant 0.109
move 0.085 point 0.080
value 0.068 defense 0.048
position 0.044 carrier 0.000
authority 0.000
Table 2: Noun Lexical Element Accuracies
ference on Intelligent Text Processing and Computa-
tional Linguistics, pages 136?145, Mexico City, Mex-
ico, February.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the ACL, pages 57?60, New York, NY, June.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8, Trento, Italy, April.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing Measures of Semantic Relatedness for Word Sense
Disambiguation. In Proceedings of the Fourth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, pages 241?257, Mex-
ico City, Mexico, February.
S. Patwardhan, T. Pedersen, and S. Banerjee. 2005.
SenseRelate::TargetWord - A Generalized Framework
for Word Sense Disambiguation. In Proceedings of
the Twentieth National Conference on Artificial In-
telligence (Intelligent Systems Demonstrations), pages
1692?1693, Pittsburgh, PA, July.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics Demonstrations, pages
38?41, Boston, MA, May.
393
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 546?554,
Beijing, August 2010
Learning to Predict Readability using Diverse Linguistic Features
Rohit J. Kate1 Xiaoqiang Luo2 Siddharth Patwardhan2 Martin Franz2
Radu Florian2 Raymond J. Mooney1 Salim Roukos2 Chris Welty2
1Department of Computer Science
The University of Texas at Austin
{rjkate,mooney}@cs.utexas.edu
2IBM Watson Research Center
{xiaoluo,spatward,franzm,raduf,roukos,welty}@us.ibm.com
Abstract
In this paper we consider the problem of
building a system to predict readability
of natural-language documents. Our sys-
tem is trained using diverse features based
on syntax and language models which are
generally indicative of readability. The
experimental results on a dataset of docu-
ments from a mix of genres show that the
predictions of the learned system are more
accurate than the predictions of naive hu-
man judges when compared against the
predictions of linguistically-trained expert
human judges. The experiments also com-
pare the performances of different learn-
ing algorithms and different types of fea-
ture sets when used for predicting read-
ability.
1 Introduction
An important aspect of a document is whether it
is easily processed and understood by a human
reader as intended by its writer, this is termed
as the document?s readability. Readability in-
volves many aspects including grammaticality,
conciseness, clarity, and lack of ambiguity. Teach-
ers, journalists, editors, and other professionals
routinely make judgements on the readability of
documents. We explore the task of learning to
automatically judge the readability of natural-
language documents.
In a variety of applications it would be useful to
be able to automate readability judgements. For
example, the results of a web-search can be or-
dered taking into account the readability of the
retrieved documents thus improving user satisfac-
tion. Readability judgements can also be used
for automatically grading essays, selecting in-
structional reading materials, etc. If documents
are generated by machines, such as summariza-
tion or machine translation systems, then they are
prone to be less readable. In such cases, a read-
ability measure can be used to automatically fil-
ter out documents which have poor readability.
Even when the intended consumers of text are
machines, for example, information extraction or
knowledge extraction systems, a readability mea-
sure can be used to filter out documents of poor
readability so that the machine readers will not ex-
tract incorrect information because of ambiguity
or lack of clarity in the documents.
As part of the DARPA Machine Reading Pro-
gram (MRP), an evaluation was designed and con-
ducted for the task of rating documents for read-
ability. In this evaluation, 540 documents were
rated for readability by both experts and novice
human subjects. Systems were evaluated based on
whether they were able to match expert readabil-
ity ratings better than novice raters. Our system
learns to match expert readability ratings by em-
ploying regression over a set of diverse linguistic
features that were deemed potentially relevant to
readability. Our results demonstrate that a rich
combination of features from syntactic parsers,
language models, as well as lexical statistics all
contribute to accurately predicting expert human
readability judgements. We have also considered
the effect of different genres in predicting read-
ability and how the genre-specific language mod-
els can be exploited to improve the readability pre-
dictions.
546
2 Related Work
There is a significant amount of published work
on a related problem: predicting the reading diffi-
culty of documents, typically, as the school grade-
level of the reader from grade 1 to 12. Some early
methods measure simple characteristics of docu-
ments like average sentence length, average num-
ber of syllables per word, etc. and combine them
using a linear formula to predict the grade level of
a document, for example FOG (Gunning, 1952),
SMOG (McLaughlin, 1969) and Flesh-Kincaid
(Kincaid et al, 1975) metrics. These methods
do not take into account the content of the doc-
uments. Some later methods use pre-determined
lists of words to determine the grade level of a
document, for example the Lexile measure (Sten-
ner et al, 1988), the Fry Short Passage measure
(Fry, 1990) and the Revised Dale-Chall formula
(Chall and Dale, 1995). The word lists these
methods use may be thought of as very simple
language models. More recently, language mod-
els have been used for predicting the grade level
of documents. Si and Callan (2001) and Collins-
Thompson and Callan (2004) train unigram lan-
guage models to predict grade levels of docu-
ments. In addition to language models, Heilman
et al (2007) and Schwarm and Ostendorf (2005)
also use some syntactic features to estimate the
grade level of texts.
Pitler and Nenkova (2008) consider a differ-
ent task of predicting text quality for an educated
adult audience. Their system predicts readabil-
ity of texts from Wall Street Journal using lex-
ical, syntactic and discourse features. Kanungo
and Orr (2009) consider the task of predicting
readability of web summary snippets produced by
search engines. Using simple surface level fea-
tures like the number of characters and syllables
per word, capitalization, punctuation, ellipses etc.
they train a regression model to predict readability
values.
Our work differs from this previous research in
several ways. Firstly, the task we have consid-
ered is different, we predict the readability of gen-
eral documents, not their grade level. The doc-
uments in our data are also not from any single
domain, genre or reader group, which makes our
task more general. The data includes human writ-
ten as well as machine generated documents. The
task and the data has been set this way because it
is aimed at filtering out documents of poor quality
for later processing, like for extracting machine-
processable knowledge from them. Extracting
knowledge from openly found text, such as from
the internet, is becoming popular but the quality
of text found ?in the wild?, like found through
searching the internet, vary considerably in qual-
ity and genre. If the text is of poor readability then
it is likely to lead to extraction errors and more
problems downstream. If the readers are going
to be humans instead of machines, then also it is
best to filter out poorly written documents. Hence
identifying readability of general text documents
coming from various sources and genres is an im-
portant task. We are not aware of any other work
which has considered such a task.
Secondly, we note that all of the above ap-
proaches that use language models train a lan-
guage model for each difficulty level using the
training data for that level. However, since the
amount of training data annotated with levels
is limited, they can not train higher-order lan-
guage models, and most just use unigram models.
In contrast, we employ more powerful language
models trained on large quantities of generic text
(which is not from the training data for readabil-
ity) and use various features obtained from these
language models to predict readability. Thirdly,
we use a more sophisticated combination of lin-
guistic features derived from various syntactic
parsers and language models than any previous
work. We also present ablation results for differ-
ent sets of features. Fourthly, given that the doc-
uments in our data are not from a particular genre
but from a mix of genres, we also train genre-
specific language models and show that including
these as features improves readability predictions.
Finally, we also show comparison between var-
ious machine learning algorithms for predicting
readability, none of the previous work compared
learning algorithms.
3 Readability Data
The readability data was collected and re-
leased by LDC. The documents were collected
547
from the following diverse sources or genres:
newswire/newspaper text, weblogs, newsgroup
posts, manual transcripts, machine translation out-
put, closed-caption transcripts and Wikipedia arti-
cles. Documents for newswire, machine transla-
tion and closed captioned genres were collected
automatically by first forming a candidate pool
from a single collection stream and then randomly
selecting documents. Documents for weblogs,
newsgroups and manual transcripts were also col-
lected in the same way but were then reviewed
by humans to make sure they were not simply
spam articles or something objectionable. The
Wikipedia articles were collected manually, by
searching through a data archive or the live web,
using keyword and other search techniques. Note
that the information about genres of the docu-
ments is not available during testing and hence
was not used when training our readability model.
A total of 540 documents were collected in this
way which were uniformly distributed across the
seven genres. Each document was then judged
for its readability by eight expert human judges.
These expert judges are native English speakers
who are language professionals and who have
specialized training in linguistic analysis and an-
notation, including the machine translation post-
editing task. Each document was also judged for
its readability by six to ten naive human judges.
These non-expert (naive) judges are native En-
glish speakers who are not language professionals
(e.g. editors, writers, English teachers, linguistic
annotators, etc.) and have no specialized language
analysis or linguistic annotation training. Both ex-
pert and naive judges provided readability judg-
ments using a customized web interface and gave
a rating on a 5-point scale to indicate how readable
the passage is (where 1 is lowest and 5 is highest
readability) where readability is defined as a sub-
jective judgment of how easily a reader can extract
the information the writer or speaker intended to
convey.
4 Readability Model
We want to answer the question whether a
machine can accurately estimate readability as
judged by a human. Therefore, we built a
machine-learning system that predicts the read-
ability of documents by training on expert hu-
man judgements of readability. The evaluation
was then designed to compare how well machine
and naive human judges predict expert human
judgements. In order to make the machine?s pre-
dicted score comparable to a human judge?s score
(details about our evaluation metrics are in Sec-
tion 6.1), we also restricted the machine scores to
integers. Hence, the task is to predict an integer
score from 1 to 5 that measures the readability of
the document.
This task could be modeled as a multi-class
classification problem treating each integer score
as a separate class, as done in some of the previ-
ous work (Si and Callan, 2001; Collins-Thompson
and Callan, 2004). However, since the classes
are numerical and not unrelated (for example, the
score 2 is in between scores 1 and 3), we de-
cided to model the task as a regression problem
and then round the predicted score to obtain the
closest integer value. Preliminary results verified
that regression performed better than classifica-
tion. Heilman et al (2008) also found that it
is better to treat the readability scores as ordinal
than as nominal. We take the average of the ex-
pert judge scores for each document as its gold-
standard score. Regression was also used by Ka-
nungo and Orr (2009), although their evaluation
did not constrain machine scores to be integers.
We tested several regression algorithms avail-
able in the Weka1 machine learning package, and
in Section 6.2 we report results for several which
performed best. The next section describes the
numerically-valued features that we used as input
for regression.
5 Features for Predicting Readability
Good input features are critical to the success of
any regression algorithm. We used three main cat-
egories of features to predict readability: syntac-
tic features, language-model features, and lexical
features, as described below.
5.1 Features Based on Syntax
Many times, a document is found to be unreadable
due to unusual linguistic constructs or ungram-
1http://www.cs.waikato.ac.nz/ml/weka/
548
matical language that tend to manifest themselves
in the syntactic properties of the text. There-
fore, syntactic features have been previously used
(Bernth, 1997) to gauge the ?clarity? of written
text, with the goal of helping writers improve their
writing skills. Here too, we use several features
based on syntactic analyses. Syntactic analyses
are obtained from the Sundance shallow parser
(Riloff and Phillips, 2004) and from the English
Slot Grammar (ESG) (McCord, 1989).
Sundance features: The Sundance system is a
rule-based system that performs a shallow syntac-
tic analysis of text. We expect that this analysis
over readable text would be ?well-formed?, adher-
ing to grammatical rules of the English language.
Deviations from these rules can be indications of
unreadable text. We attempt to capture such de-
viations from grammatical rules through the fol-
lowing Sundance features computed for each text
document: proportion of sentences with no verb
phrases, average number of clauses per sentence,
average sentence length in tokens, average num-
ber of noun phrases per sentence, average number
of verb phrases per sentence, average number of
prepositional phrases per sentence, average num-
ber of phrases (all types) per sentence and average
number of phrases (all types) per clause.
ESG features: ESG uses slot grammar rules to
perform a deeper linguistic analysis of sentences
than the Sundance system. ESG may consider
several different interpretations of a sentence, be-
fore deciding to choose one over the other inter-
pretations. Sometimes ESG?s grammar rules fail
to produce a single complete interpretation of a
sentence, in which case it generates partial parses.
This typically happens in cases when sentences
are ungrammatical, and possibly, less readable.
Thus, we use the proportion of such incomplete
parses within a document as a readability feature.
In case of extremely short documents, this propor-
tion of incomplete parses can be misleading. To
account for such short documents, we introduce
a variation of the above incomplete parse feature,
by weighting it with a log factor as was done in
(Riloff, 1996; Thelen and Riloff, 2002).
We also experimented with some other syn-
tactic features such as average sentence parse
scores from Stanford parser and an in-house maxi-
mum entropy statistical parer, average constituent
scores etc., however, they slightly degraded the
performance in combination with the rest of the
features and hence we did not include them in
the final set. One possible explanation could be
that averaging diminishes the effect of low scores
caused by ungrammaticality.
5.2 Features Based on Language Models
A probabilistic language model provides a predic-
tion of how likely a given sentence was generated
by the same underlying process that generated a
corpus of training documents. In addition to a
general n-gram language model trained on a large
body of text, we also exploit language models
trained to recognize specific ?genres? of text. If a
document is translated by a machine, or casually
produced by humans for a weblog or newsgroup,
it exhibits a character that is distinct from docu-
ments that go through a dedicated editing process
(e.g., newswire and Wikipedia articles). Below
we describe features based on generic as well as
genre-specific language models.
Normalized document probability: One obvi-
ous proxy for readability is the score assigned to
a document by a generic language model (LM).
Since the language model is trained on well-
written English text, it penalizes documents de-
viating from the statistics collected from the LM
training documents. Due to variable document
lengths, we normalize the document-level LM
score by the number of words and compute the
normalized document probability NP (D) for a
document D as follows:
NP (D) =
(
P (D|M)
) 1
|D| , (1)
where M is a general-purpose language model
trained on clean English text, and |D| is the num-
ber of words in the document D.
Perplexities from genre-specific language mod-
els: The usefulness of LM-based features in
categorizing text (McCallum and Nigam, 1998;
Yang and Liu, 1999) and evaluating readability
(Collins-Thompson and Callan, 2004; Heilman
et al, 2007) has been investigated in previous
work. In our experiments, however, since doc-
uments were acquired through several different
channels, such as machine translation or web logs,
549
we also build models that try to predict the genre
of a document. Since the genre information for
many English documents is readily available, we
trained a series of genre-specific 5-gram LMs us-
ing the modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Stanley and Goodman, 1996). Ta-
ble 1 contains a list of a base LM and genre-
specific LMs.
Given a document D consisting of tokenized
word sequence {wi : i = 1, 2, ? ? ? , |D|}, its per-
plexity L(D|Mj) with respect to a LM Mj is
computed as:
L(D|Mj) = e
(
? 1|D|
P|D|
i=1 logP (wi|hi;Mj)
)
, (2)
where |D| is the number of words in D and hi are
the history words for wi, and P (wi|hi;Mj) is the
probability Mj assigns to wi, when it follows the
history words hi.
Posterior perplexities from genre-specific lan-
guagemodels: While perplexities computed from
genre-specific LMs reflect the absolute probabil-
ity that a document was generated by a specific
model, a model?s relative probability compared to
other models may be a more useful feature. To this
end, we also compute the posterior perplexity de-
fined as follows. Let D be a document, {Mi}Gi=1
be G genre-specific LMs, and L(D|Mi) be the
perplexity of the document D with respect to Mi,
then the posterior perplexity, R(Mi|D), is de-
fined as:
R(Mi|D) =
L(D|Mi)?G
j=1 L(D|Mj)
. (3)
We use the term ?posterior? because if a uni-
form prior is adopted for {Mi}Gi=1,R(Mi|D) can
be interpreted as the posterior probability of the
genre LM Mi given the document D.
5.3 Lexical Features
The final set of features involve various lexical
statistics as described below.
Out-of-vocabulary (OOV) rates: We conjecture
that documents containing typographical errors
(e.g., for closed-caption and web log documents)
may receive low readability ratings. Therefore,
we compute the OOV rates of a document with re-
spect to the various LMs shown in Table 1. Since
modern LMs often have a very large vocabulary,
to get meaningful OOV rates, we truncate the vo-
cabularies to the top (i.e., most frequent) 3000
words. For the purpose of OOV computation, a
document D is treated as a sequence of tokenized
words {wi : i = 1, 2, ? ? ? , |D|}. Its OOV rate
with respect to a (truncated) vocabulary V is then:
OOV (D|V) =
?D
i=1 I(wi /? V)
|D| , (4)
where I(wi /? V) is an indicator function taking
value 1 if wi is not in V , and 0 otherwise.
Ratio of function words: A characteristic of doc-
uments generated by foreign speakers and ma-
chine translation is a failure to produce certain
function words, such as ?the,? or ?of.? So we pre-
define a small set of function words (mainly En-
glish articles and frequent prepositions) and com-
pute the ratio of function words over the total
number words in a document:
RF (D) =
?D
i=1 I(wi ? F)
|D| , (5)
where I(wi ? F) is 1 ifwi is in the set of function
words F , and 0 otherwise.
Ratio of pronouns: Many foreign languages that
are source languages of machine-translated docu-
ments are pronoun-drop languages, such as Ara-
bic, Chinese, and romance languages. We conjec-
ture that the pronoun ratio may be a good indica-
tor whether a document is translated by machine
or produced by humans, and for each document,
we first run a POS tagger, and then compute the
ratio of pronouns over the number of words in the
document:
RP (D) =
?D
i=1 I(POS(wi) ? P)
|D| , (6)
where I(POS(wi) ? F) is 1 if the POS tag of wi
is in the set of pronouns, P , and 0 otherwise.
Fraction of known words: This feature measures
the fraction of words in a document that occur
either in an English dictionary or a gazetteer of
names of people and locations.
6 Experiments
This section describes the evaluation methodol-
ogy and metrics and presents and discusses our
550
Genre Training Size(M tokens) Data Sources
base 5136.8 mostly LDC?s GigaWord set
NW 143.2 newswire subset of base
NG 218.6 newsgroup subset of base
WL 18.5 weblog subset of base
BC 1.6 broadcast conversation subset of base
BN 1.1 broadcast news subset of base
wikipedia 2264.6 Wikipedia text
CC 0.1 closed caption
ZhEn 79.6 output of Chinese to English Machine Translation
ArEn 126.8 output of Arabic to English Machine Translation
Table 1: Genre-specific LMs: the second column contains the number of tokens in LM training data (in million tokens).
experimental results. The results of the official
evaluation task are also reported.
6.1 Evaluation Metric
The evaluation process for the DARPAMRP read-
ability test was designed by the evaluation team
led by SAIC. In order to compare a machine?s
predicted readability score to those assigned by
the expert judges, the Pearson correlation coef-
ficient was computed. The mean of the expert-
judge scores was taken as the gold-standard score
for a document.
To determine whether the machine predicts
scores closer to the expert judges? scores than
what an average naive judge would predict, a
sampling distribution representing the underlying
novice performance was computed. This was ob-
tained by choosing a random naive judge for every
document, calculating the Pearson correlation co-
efficient with the expert gold-standard scores and
then repeating this procedure a sufficient number
of times (5000). The upper critical value was set
at 97.5% confidence, meaning that if the machine
performs better than the upper critical value then
we reject the null hypothesis that machine scores
and naive scores come from the same distribution
and conclude that the machine performs signifi-
cantly better than naive judges in matching the ex-
pert judges.
6.2 Results and Discussion
We evaluated our readability system on the dataset
of 390 documents which was released earlier dur-
ing the training phase of the evaluation task. We
Algorithm Correlation
Bagged Decision Trees 0.8173
Decision Trees 0.7260
Linear Regression 0.7984
SVM Regression 0.7915
Gaussian Process Regression 0.7562
Naive Judges
Upper Critical Value 0.7015
Distribution Mean 0.6517
Baselines
Uniform Random 0.0157
Proportional Random -0.0834
Table 2: Comparing different algorithms on the readability
task using 13-fold cross-validation on the 390 documents us-
ing all the features. Exceeding the upper critical value of the
naive judges? distribution indicates statistically significantly
better predictions than the naive judges.
used stratified 13-fold cross-validation in which
the documents from various genres in each fold
was distributed in roughly the same proportion as
in the overall dataset. We first conducted experi-
ments to test different regression algorithms using
all the available features. Next, we ablated various
feature sets to determine how much each feature
set was contributing to making accurate readabil-
ity judgements. These experiments are described
in the following subsections.
6.2.1 Regression Algorithms
We used several regression algorithms available
in theWeka machine learning package and Table 2
shows the results obtained. The default values
551
Feature Set Correlation
Lexical 0.5760
Syntactic 0.7010
Lexical + Syntactic 0.7274
Language Model based 0.7864
All 0.8173
Table 3: Comparison of different linguistic feature sets.
in Weka were used for all parameters, changing
these values did not show any improvement. We
used decision tree (reduced error pruning (Quin-
lan, 1987)) regression, decision tree regression
with bagging (Breiman, 1996), support vector re-
gression (Smola and Scholkopf, 1998) using poly-
nomial kernel of degree two,2 linear regression
and Gaussian process regression (Rasmussen and
Williams, 2006). The distribution mean and the
upper critical values of the correlation coefficient
distribution for the naive judges are also shown in
the table.
Since they are above the upper critical value, all
algorithms predicted expert readability scores sig-
nificantly more accurately than the naive judges.
Bagged decision trees performed slightly better
than other methods. As shown in the following
section, ablating features affects predictive accu-
racy much more than changing the regression al-
gorithm. Therefore, on this task, the choice of re-
gression algorithm was not very critical once good
readability features are used. We also tested two
simple baseline strategies: predicting a score uni-
formly at random, and predicting a score propor-
tional to its frequency in the training data. As
shown in the last two rows of Table 2, these base-
lines perform very poorly, verifying that predict-
ing readability on this dataset as evaluated by our
evaluation metric is not trivial.
6.2.2 Ablations with Feature Sets
We evaluated the contributions of different fea-
ture sets through ablation experiments. Bagged
decision-tree was used as the regression algorithm
in all of these experiments. First we compared
syntactic, lexical and language-model based fea-
tures as described in Section 5, and Table 3 shows
2Polynomial kernels with other degrees and RBF kernel
performed worse.
the results. The language-model feature set per-
forms the best, but performance improves when it
is combined with the remaining features. The lex-
ical feature set by itself performs the worst, even
below the naive distribution mean (shown in Ta-
ble 2); however, when combined with syntactic
features it performs well.
In our second ablation experiment, we com-
pared the performance of genre-independent and
genre-based features. Since the genre-based fea-
tures exploit knowledge of the genres of text used
in the MRP readability corpus, their utility is
somewhat tailored to this specific corpus. There-
fore, it is useful to evaluate the performance of the
system when genre information is not exploited.
Of the lexical features described in subsection 5.3,
the ratio of function words, ratio of pronoun words
and all of the out-of-vocabulary rates except for
the base language model are genre-based features.
Out of the language model features described in
the Subsection 5.2, all of the perplexities except
for the base language model and all of the poste-
rior perplexities3 are genre-based features. All of
the remaining features are genre-independent. Ta-
ble 4 shows the results comparing these two fea-
ture sets. The genre-based features do well by
themselves but the rest of the features help fur-
ther improve the performance. While the genre-
independent features by themselves do not exceed
the upper critical value of the naive judges? dis-
tribution, they are very close to it and still out-
perform its mean value. These results show that
for a dataset like ours, which is composed of a mix
of genres that themselves are indicative of read-
ability, features that help identify the genre of a
text improve performance significantly.4 For ap-
plications mentioned in the introduction and re-
lated work sections, such as filtering less readable
documents from web-search, many of the input
documents could come from some of the common
genres considered in our dataset.
In our final ablation experiment, we evaluated
3Base model for posterior perplexities is computed using
other genre-based LMs (equation 3) hence it can not be con-
sidered genre-independent.
4We note that none of the genre-based features were
trained on supervised readability data, but were trained on
readily-available large unannotated corpora as shown in Ta-
ble 1.
552
Feature Set Correlation
Genre-independent 0.6978
Genre-based 0.7749
All 0.8173
Table 4: Comparison of genre-independent and genre-
based feature sets.
Feature Set By itself Ablated
from All
Sundance features 0.5417 0.7993
ESG features 0.5841 0.8118
Perplexities 0.7092 0.8081
Posterior perplexities 0.7832 0.7439
Out-of-vocabulary rates 0.3574 0.8125
All 0.8173 -
Table 5: Ablations with some individual feature sets.
the contribution of various individual feature sets.
Table 5 shows that posterior perplexities perform
the strongest on their own, but without them, the
remaining features also do well. When used by
themselves, some feature sets perform below the
naive judges? distribution mean, however, remov-
ing them from the rest of the feature sets de-
grades the performance. This shows that no indi-
vidual feature set is critical for good performance
but each further improves the performance when
added to the rest of the feature sets.
6.3 Official Evaluation Results
An official evaluation was conducted by the eval-
uation team SAIC on behalf of DARPA in which
three teams participated including ours. The eval-
uation task required predicting the readability of
150 test documents using the 390 training docu-
ments. Besides the correlation metric, two addi-
tional metrics were used. One of them computed
for a document the difference between the aver-
age absolute difference of the naive judge scores
from the mean expert score and the absolute dif-
ference of the machine?s score from the mean ex-
pert score. This was then averaged over all the
documents. The other one was ?target hits? which
measured if the predicted score for a document
fell within the width of the lowest and the highest
expert scores for that document, and if so, com-
System Correl. Avg. Diff. Target Hits
Our (A) 0.8127 0.4844 0.4619
System B 0.6904 0.3916 0.4530
System C 0.8501 0.5177 0.4641
Upper CV 0.7423 0.0960 0.3713
Table 6: Results of the systems that participated in the
DARPA?s readability evaluation task. The three metrics used
were correlation, average absolute difference and target hits
measured against the expert readability scores. The upper
critical values are for the score distributions of naive judges.
puted a score inversely proportional to that width.
The final target hits score was then computed by
averaging it across all the documents. The upper
critical values for these metrics were computed in
a way analogous to that for the correlation met-
ric which was described before. Higher score is
better for all the three metrics. Table 6 shows the
results of the evaluation. Our system performed
favorably and always scored better than the up-
per critical value on each of the metrics. Its per-
formance was in between the performance of the
other two systems. The performances of the sys-
tems show that the correlation metric was the most
difficult of the three metrics.
7 Conclusions
Using regression over a diverse combination of
syntactic, lexical and language-model based fea-
tures, we built a system for predicting the read-
ability of natural-language documents. The sys-
tem accurately predicts readability as judged by
linguistically-trained expert human judges and
exceeds the accuracy of naive human judges.
Language-model based features were found to be
most useful for this task, but syntactic and lexical
features were also helpful. We also found that for
a corpus consisting of documents from a diverse
mix of genres, using features that are indicative
of the genre significantly improve the accuracy of
readability predictions. Such a system could be
used to filter out less readable documents for ma-
chine or human processing.
Acknowledgment
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
553
References
Bernth, Arendse. 1997. Easyenglish: A tool for improv-
ing document quality. In Proceedings of the fifth con-
ference on Applied Natural Language Processing, pages
159?165, Washington DC, April.
Breiman, Leo. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
Chall, J.S. and E. Dale. 1995. Readability Revisited: The
New Dale-Chall Readability Formula. Brookline Books,
Cambridge, MA.
Collins-Thompson, Kevyn and James P. Callan. 2004. A
language modeling approach to predicting reading diffi-
culty. In Proc. of HLT-NAACL 2004, pages 193?200.
Fry, E. 1990. A readability formula for short passages. Jour-
nal of Reading, 33(8):594?597.
Gunning, R. 1952. The Technique of Clear Writing.
McGraw-Hill, Cambridge, MA.
Heilman, Michael, Kevyn Collins-Thompson, Jamie Callan,
and Maxine Eskenazi. 2007. Combining lexical and
grammatical features to improve readability measures for
first and second language texts. In Proc. of NAACL-HLT
2007, pages 460?467, Rochester, New York, April.
Heilman, Michael, Kevyn Collins-Thompson, and Maxine
Eskenazi. 2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. In Proceedings of
the Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 71?79, Columbus,
Ohio, June. Association for Computational Linguistics.
Kanungo, Tapas and David Orr. 2009. Predicting the read-
ability of short web summaries. In Proc. of WSDM 2009,
pages 202?211, Barcelona, Spain, February.
Kincaid, J. P., R. P. Fishburne, R. L. Rogers, and B.S.
Chissom. 1975. Derivation of new readability formulas
for navy enlisted personnel. Technical Report Research
Branch Report 8-75, Millington, TN: Naval Air Station.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc. of
ICASSP-95, pages 181?184.
McCallum, Andrew and Kamal Nigam. 1998. A comparison
of event models for naive Bayes text classification. In Pa-
pers from the AAAI-98 Workshop on Text Categorization,
pages 41?48, Madison, WI, July.
McCord, Michael C. 1989. Slot grammar: A system for
simpler construction of practical natural language gram-
mars. In Proceedings of the International Symposium on
Natural Language and Logic, pages 118?145, May.
McLaughlin, G. H. 1969. Smog: Grading: A new readabil-
ity formula. Journal of Reading, 12:639?646.
Pitler, Emily and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proc. of EMNLP 2008, pages 186?195,
Waikiki,Honolulu,Hawaii, October.
Quinlan, J. R. 1987. Simplifying decision trees. Interna-
tional Journal of Man-Machine Studies, 27:221?234.
Rasmussen, Carl and Christopher Williams. 2006. Gaussian
Processes for Machine Leanring. MIT Press, Cambridge,
MA.
Riloff, E. and W. Phillips. 2004. An introduction to the Sun-
dance and Autoslog systems. Technical Report UUCS-
04-015, University of Utah School of Computing.
Riloff, Ellen. 1996. Automatically generating extraction
patterns from untagged text. In Proc. of 13th Natl. Conf.
on Artificial Intelligence (AAAI-96), pages 1044?1049,
Portland, OR.
Schwarm, Sarah E. andMari Ostendorf. 2005. Reading level
assessment using support vector machines and statistical
language models. In Proc. of ACL 2005, pages 523?530,
Ann Arbor, Michigan.
Si, Luo and James P. Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM 2001, pages 574?
576.
Smola, Alex J. and Bernhard Scholkopf. 1998. A tutorial
on support vector regression. Technical Report NC2-TR-
1998-030, NeuroCOLT2.
Stanley, Chen and Joshua Goodman. 1996. An empirical
study of smoothing techniques for language modeling. In
Proc. of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL-96), pages 310?318.
Stenner, A. J., I. Horabin, D. R. Smith, and M. Smith. 1988.
The Lexile Framework. Durham, NC: MetaMetrics.
Thelen, M. and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proc. of EMNLP 2002, Philadelphia, PA, July.
Yang, Yiming and Xin Liu. 1999. A re-examination of text
cateogrization methods. In Proc. of 22nd Intl. ACM SI-
GIR Conf. on Research and Development in Information
Retrieval, pages 42?48, Berkeley, CA.
554
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712?724,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Using Syntactic and Semantic Structural Kernels for
Classifying Definition Questions in Jeopardy!
Alessandro Moschitti? Jennifer Chu-Carroll? Siddharth Patwardhan?
James Fan? Giuseppe Riccardi?
?Department of Information Engineering and Computer Science
University of Trento, 38123 Povo (TN), Italy
{moschitti,riccardi}@disi.unitn.it
?IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598, U.S.A.
{jencc,siddharth,fanj}@us.ibm.com
Abstract
The last decade has seen many interesting ap-
plications of Question Answering (QA) tech-
nology. The Jeopardy! quiz show is certainly
one of the most fascinating, from the view-
points of both its broad domain and the com-
plexity of its language. In this paper, we study
kernel methods applied to syntactic/semantic
structures for accurate classification of Jeop-
ardy! definition questions. Our extensive em-
pirical analysis shows that our classification
models largely improve on classifiers based on
word-language models. Such classifiers are
also used in the state-of-the-art QA pipeline
constituting Watson, the IBM Jeopardy! sys-
tem. Our experiments measuring their impact
on Watson show enhancements in QA accu-
racy and a consequent increase in the amount
of money earned in game-based evaluation.
1 Introduction
Question Answering (QA) is an important research
area of Information Retrieval applications, which re-
quires the use of core NLP capabilities, such as syn-
tactic and semantic processing for a more effective
user experience. While the development of most
existing QA systems are driven by organized eval-
uation efforts such as TREC (Voorhees and Dang,
2006), CLEF (Giampiccolo et al, 2007), and NT-
CIR (Sasaki et al, 2007), there exist efforts that
leverage data from popular quiz shows, such as Who
Wants to be a Millionaire (Clarke et al, 2001; Lam
et al, 2003) and Jeopardy! (Ferrucci et al, 2010), to
demonstrate the generality of the technology.
Jeopardy! is a popular quiz show in the US which
has been on the air for 27 years. In each game, three
contestants compete for the opportunity to answer
60 questions in 12 categories of 5 questions each.
Jeopardy! questions cover an incredibly broad do-
main, from science, literature, history, to popular
culture. We are drawn to Jeopardy! as a test bed
for open-domain QA technology due to its broad do-
main, complex language, as well as the emphasis on
accuracy, confidence, and speed during game play.
While the vast majority of Jeopardy! questions
are factoid questions, we find several other types
of questions in the Jeopardy! data, which can ben-
efit from specialized processing in the QA system.
The additional processing in these questions com-
plements that of the factoid questions to achieve im-
proved overall QA performance. Among the various
types of questions handled by the system are defini-
tion questions shown in the examples below:
(1) GON TOMORROW: It can be the basket
below a hot-air balloon or a flat-bottomed
boat used on a canal (answer: gondola);
(2) I LOVE YOU, ?MIN?: Overbearing (an-
swer: domineering);
(3) INVEST: From the Latin for ?year?, it?s
an investment or retirement fund that pays
out yearly (answer: an annuity)
where the upper case text indicates the Jeop-
ardy! category for each question1.
Several characteristics of this class of questions
warrant special processing: first, the clue (question)
1A Jeopardy! category indicates a theme is common among
its 5 questions.
712
often aligns well with dictionary entries, making
dictionary resources potentially effective. Second,
these clues often do not indicate an answer type,
which is an important feature for identifying cor-
rect answers in factoid questions (in the examples
above, only (3) provided an answer type, ?fund?).
Third, definition questions are typically shorter in
length than the average factoid question. These dif-
ferences, namely the shorter clue length and the lack
of answer types, make the use of a specialized ma-
chine learning model potentially promising for im-
proving the overall system accuracy. The first step
for handling definitions is, of course, the automatic
separation of definitions from other question types,
which is not a simple task in the Jeopardy! domain.
For instance, consider the following example which
is a variation of (3) above:
(4) INVEST: From the Latin for ?year?,
an annuity is an investment or retirement
fund that pays out this often (answer:
yearly)
Even though the clue is nearly identical to (3), the
clue does not provide a definition for the answer
yearly, although at first glance we may have been
misled. The source of complexity is given by the fact
that Jeopardy! clues are not phrased in interrogative
form as questions typically are. This complicates the
design of definition classifiers since we cannot di-
rectly use either typical structural patterns that char-
acterize definition/description questions, or previous
approaches, e.g. (Ahn et al, 2004; Kaisser and Web-
ber, 2007; Blunsom et al, 2006). Given the com-
plexity and the novelty of the task, we found it use-
ful to exploit the kernel methods technology. This
has shown state-of-the-art performance in Question
Classification (QC), e.g. (Zhang and Lee, 2003;
Suzuki et al, 2003; Moschitti et al, 2007) and it
is very well suited for engineering feature represen-
tations for novel tasks.
In this paper, we apply SVMs and kernel meth-
ods to syntactic/semantic structures for modeling
accurate classification of Jeopardy! definition ques-
tions. For this purpose, we use several levels of lin-
guistic information: word and POS tag sequences,
dependency, constituency and predicate argument
structures and we combined them using state-of-
the-art structural kernels, e.g. (Collins and Duffy,
2002; Shawe-Taylor and Cristianini, 2004; Mos-
chitti, 2006). The extensive empirical analysis of
several advanced models shows that our best model,
which combines different kernels, improves the F1
of our baseline model by 67% relative, from 40.37
to 67.48. Surprisingly, with respect to previous find-
ings on standard QC, e.g. (Zhang and Lee, 2003;
Moschitti, 2006), the Syntactic Tree Kernel (Collins
and Duffy, 2002) is not effective whereas the ex-
ploitation of partial tree patterns proves to be es-
sential. This is due to the different nature of Jeop-
ardy! questions, which are not expressed in the usual
interrogative form.
To demonstrate the benefit of our question clas-
sifier, we integrated it into our Watson by coupling
it with search and candidate generation against spe-
cialized dictionary resources. We show that in end-
to-end evaluations, Watson with kernel-based defi-
nition classification and specialized definition ques-
tion processing achieves statistically significant im-
provement compared to our baseline systems.
In the reminder of this paper, Section 2 describes
Watson by focusing on the problem of definition
question classification, Section 3 describes our mod-
els for such classifiers, Section 4 presents our exper-
iments on QC, whereas Section 5 shows the final im-
pact on Watson. Finally, Section 6 discusses related
work and Section 7 derives the conclusions.
2 Watson: The IBM Jeopardy! System
This section gives a quick overview of Watson and
the problem of classification of definition questions,
which is the focus of this paper.
2.1 Overview
Watson is a massively parallel probabilistic
evidence-based architecture for QA (Ferrucci et
al., 2010). It consists of several major stages for
underlying sub-tasks, including analysis of the
question, retrieval of relevant content, scoring and
ranking of candidate answers, as depicted in Figure
1. In the rest of this section, we provide an overview
of Watson, focusing on the task of answering
definitional questions.
Question Analysis: The first stage of the pipeline,
it applies several analytic components to identify
key characteristics of the question (such as answer
713
Figure 1: Overview of Watson
type, question classes, etc.) used by later stages of
the Watson pipeline. Various general purpose NLP
components, such as a parser and named entity de-
tector, are combined with task-specific modules for
this analysis.
The task-specific analytics include several QC
components, which determine if the question be-
longs to one or more broad ?question classes?.
These question classes can influence later stages of
the Watson pipeline. For instance, a question de-
tected as an abbreviation question can invoke spe-
cialized candidate generators to produce possible ex-
pansions of the abbreviated term in the clue. Simi-
larly, the question classes can impact the methods
for answer scoring and the machine learning mod-
els used for ranking candidate answers. The focus
of this paper is on the definition class, which is de-
scribed in the next section.
Hypothesis Generation: Following question anal-
ysis, the Watson pipeline searches its document col-
lection for relevant documents and passages that are
likely to contain the correct answer to the question.
This stage of the pipeline generates search queries
based on question analysis results, and obtains a
ranked list of documents and passages most relevant
to the search queries. A variety of candidate gen-
eration techniques are then applied to the retrieved
results to produce a set of candidate answers.
Information obtained from question analysis can
be used to influence the search and candidate gener-
ation processes. The question classes detected dur-
ing question analysis can focus the search towards
specific subsets of the corpus. Similarly, during can-
didate generation, strategies used to generate the set
of candidate answers are selected based on the de-
tected question classes.
Hypothesis and Evidence Scoring: A wide variety
of answer scorers are then used to gather evidence
supporting each candidate answer as the correct an-
swer to the given question. The scorers include both
context dependent as well as context independent
scorers, relying on various structured and unstruc-
tured resources for their supporting evidence.
Candidate Ranking: Finally, machine learning
models are used to weigh the gathered evidence and
rank the candidate answers. The models generate a
ranked list of answers each with an associated con-
fidence. The system can also choose to refrain from
answering a question if it has low confidence in all
candidates. This stage of the pipeline employs sev-
eral machine learning models specially trained to
handle various types of questions. These models are
trained using selected feature sets based on question
classes and candidate answers are ?routed? to the
appropriate model according to the question classes
detected during question analysis.
2.2 Answering Definition Questions
Among the many question classes that Watson iden-
tifies and leverages for special processing, of partic-
ular interest for this paper is the class we refer to
as definition questions. These are questions whose
clue texts contain one or more definitions of the cor-
rect answer. For instance, in example (3), the main
clause in the question corresponds to a dictionary
definition of the correct answer (annuity). Looking
up this definition in dictionary resources could en-
able us to answer this question correctly and with
high confidence. This suggests that special process-
714
ing of such definition questions could allow us to
hone in on the correct answer through processes dif-
ferent from those used for other types of questions.
This paper explores strategies for definition ques-
tion processing to improve overall question answer-
ing performance. A key challenge we have to ad-
dress is that of accurate recognition of such ques-
tions. Given an input question the Watson question
analysis stage uses a definition question recognizer
to detect this specific class of questions. We explore
several approaches for recognition, including a rule
based approach and a variety of statistical models.
Questions that are recognized as definition ques-
tions invoke search processes targeted towards
dictionary-like sources in our system. We use a va-
riety of such sources, such as standard English dic-
tionaries, Wiktionary, WordNet, etc. After gather-
ing supporting evidence for candidate answers ex-
tracted from these sources, our system routes the
candidates to definition-specific candidate ranking
models, which have been trained with selected fea-
ture sets.
The following sections present a description and
evaluation of our approach for identifying and an-
swering definition questions.
3 Kernel Models for Question
Classification
Previous work (Zhang and Lee, 2003; Suzuki et al,
2003; Blunsom et al, 2006; Moschitti et al, 2007)
as shown that syntactic structures are essential for
QC. Given the novelty of both the domain and the
type of our classification items, we rely on kernel
methods to study and design effective representa-
tions. Indeed, these are excellent tools for auto-
matic feature engineering, especially for unknown
tasks and domains. Our approach consists of using
SVMs and kernels for structured data applied to sev-
eral types of structural lexical, syntactic and shallow
semantic information.
3.1 Tree and Sequence Kernels
Kernel functions are implicit scalar products be-
tween data examples (i.e. questions in our case)
in the very high dimensional space of substructures,
where each of the latter is a component of the im-
plicit vectors associated with the examples.
ROOT
SBARQ
WHADVP
WRB
When
S
VP
VBN
hit
PP
IN
by
NP
NNS
electrons
,
,
NP
DT
a
NN
phosphor
VP
VBZ
gives
PRP
RP
off
NP
NP
JJ
electromagnetic
NN
energy
PP
IN
in
NP
DT
this
NN
form
1
Figure 2: Constituency Tree
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
A2
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
ROOT
VBZ
OBJ
NN
NMOD
IN
PMOD
NN
formNMOD
DT
this
in
energyNMOD
JJ
electromag.
PRT
RP
off
givesSBJ
NN
phosphorNMOD
DT
a
P
,
TMP
VBN
LGS
IN
PMOD
NNS
electrons
by
hitTMP
WRB
when
1
Figure 3: Dependency Tree
negative mistake STK, ok PTK
NP
ADJP
JJ
conceited
CC
or
JJ
arrogant
NP
NN
meaning
NN
adjective
NN
5-letter
NN
fowl
positive mistake STK, ok PTK
NP
VP
PP
NP
NN
field
VBG
playing
DT
a
IN
on
VBN
used
NP
NN
grass
JJ
green
JJ
artificial
NP
VP
PP
NP
NN
canal
DT
a
IN
on
VBN
used
NP
NN
boat
JJ
flat-bottomed
DT
a
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
AM-MNR
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
1
Figure 4: A tree encoding a Predicate Argument Structure Set
Although several kernels for structured data have
been developed (see Section 6), the main distinc-
tions in terms of feature spaces is given by the fol-
lowing three different kernels:
? Sequence Kernels (SK); we implemented the
discontinuous string kernels described in (Shawe-
Taylor and Cristianini, 2004). This allows for rep-
resenting a string of symbols in terms of its possi-
ble substrings with gaps, i.e. an arbitrary number of
symbols can be skipped during the generation of a
substring. The symbols we used in the sequential de-
scriptions of questions are words and part-of-speech
tags (in two separate sequences). Consequently, all
possible multiwords with gaps are features of the im-
plicitly generated vector space.
715
? Syntactic Tree Kernel (STK) (Collins and Duffy,
2002) applied to constituency parse trees. This gen-
erates all possible tree fragments as features with
the conditions that sibling nodes from the original
trees cannot be separated. In other words, substruc-
tures are composed by atomic building blocks cor-
responding to nodes along with all their direct chil-
dren. These, in case of a syntactic parse tree, are
complete production rules of the associated parser
grammar2.
? Partial Tree Kernel (PTK) (Moschitti, 2006) ap-
plied to both constituency and dependency parse
trees. This generates all possible tree fragments, as
above, but sibling nodes can be separated (so they
can be part of different tree fragments). In other
words, a fragment is any possible tree path, from
whose nodes other tree paths can depart. Conse-
quently, an extremely rich feature space is gener-
ated. Of course, PTK subsumes STK but sometimes
the latter provides more effective solutions as the
number of irrelevant features is smaller as well.
When applied to sequences and tree structures, the
kernels discussed above produce many different
kinds of features. Therefore, the design of appro-
priate syntactic/semantic structures determines the
representational power of the kernels. Hereafter, we
show the models we used.
3.2 Syntactic Semantic Structures
We applied the above kernels to different structures.
These can be divided in sequences of words (WS)
and part of speech tags (PS) and different kinds of
trees. For example, given the non-definition Jeop-
ardy! question:
(5) GENERAL SCIENCE: When hit by elec-
trons, a phosphor gives off electromag-
netic energy in this form. (answer: light
or photons),
we use the following sequences:
WS: [when][hit][by][electrons][,][a][phosphor][gives]
[off][electromagnetic][energy][in][this][form]
PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in]
[dt][nn]
Additionally, we use constituency trees (CTs), see
2From here the name syntactic tree kernels
Figure 2 and dependency structures converted into
the dependency trees (DTs), e.g. shown in Figure
3. Note that, the POS-tags are central nodes, the
grammatical relation label is added as a father
node and all the relations with the other nodes are
described by means of the connecting edges. Words
are considered additional children of the POS-tag
nodes (in this case the connecting edge just serves
to add a lexical feature to the target POS-tag node).
Finally, we also use predicate argument structures
generated by verbal and nominal relations accord-
ing to PropBank (Palmer et al, 2005) and NomBank
(Meyers et al, 2004). Given the target sentence, the
set of its predicates are extracted and converted into
a forest, then a fake root node, PAS, is used to con-
nect these trees. For example, Figure 4 illustrates a
Predicate Argument Structures Set (PASS) encoding
two relations, give and hit, as well as the nominaliza-
tion energy along with all their arguments.
4 Experiments on Definition Question
Classification
In these experiments, we study the role of kernel
technology for the design of accurate classification
of definition questions. We build several classifiers
based on SVMs and kernel methods. Each classi-
fier uses advanced syntactic/semantic structural fea-
tures and their combination. We carry out an exten-
sive comparison in terms of F1 between the different
models on the Jeopardy! datasets.
4.1 Experimental Setup
Corpus: the data for our QC experiments consists
of a randomly selected set of 33 Jeopardy! games3.
These questions were manually annotated based on
whether or not they are considered definitional. This
resulted in 306 definition and 4964 non-definition
clues. Each test set is stored in a separate file con-
sisting of one line per question, which contains tab-
separated clue information and the Jeopardy! cate-
gory, e.g. INVEST in example (4).
Tools: for SVM learning, we used the SVMLight-
TK software4, which includes structural kernels in
SVMLight (Joachims, 1999)5. For generating con-
3Past Jeopardy! games can be downloaded from
http://www.j-archive.com.
4Available at http://dit.unitn.it/?moschitt
5http://svmlight.joachims.org
716
stituency trees, we used the Charniak parser (Char-
niak, 2000). We also used the syntactic?semantic
parser by Johansson and Nugues (2008) to gener-
ate dependency trees (Mel?c?uk, 1988) and predicate
argument trees according to the PropBank (Palmer
et al, 2005) and NomBank (Meyers et al, 2004)
frameworks.
Baseline Model: the first model that we used as a
baseline is a rule-based classifier (RBC). The RBC
leverages a set of rules that matches against lexical
and syntactic information in the clue to make a bi-
nary decision on whether or not the clue is consid-
ered definitional. The rule set was manually devel-
oped by a human expert, and consists of rules that
attempt to identify roughly 70 different constructs
in the clues. For instance, one of the rules matches
the parse tree structure for ?It?s X or Y?, which will
identify example (1) as a definition question.
Kernel Models: we apply the kernels described
in Section 3 to the structures extracted from Jeop-
ardy! clues. In particular, we design the following
models: BOW, i.e. linear kernel on bag-of-words
from the clues; WSK, PSK and CSK, i.e. SK applied
to the word and POS-tag sequences from the clues,
and the word sequence taken from the question cat-
egories, respectively; STK-CT, i.e. STK applied to
CTs of the clue; PTK-CT and PTK-DT, i.e. PTK
applied to CTs and DTs of the clues, respectively;
PASS, i.e. PTK applied to the Predicate Argument
Structure Set extracted from the clues; and RBC, i.e.
a linear kernel applied to the vector only constituted
by the 1/0 output of RBC.
Learning Setting: there is no particular parameteri-
zation. Since there is an imbalance between positive
and negative examples, we used a Precision/Recall
trade-off parameter in SVM-Light-TK equal to 5.6
Measures: the performance is measured with Pre-
cision, Recall and F1-measure. We estimated them
by means of Leave-One-Out7 (LOO) on the question
set.
4.2 Results and Discussion
Table 1 shows the performance obtained using dif-
ferent kernels (feature spaces) with SVMs. We note
6We have selected 5 as a reasonable value, which kept bal-
anced Precision and Recall on a validation set.
7LOO applied to a corpus ofN instances consists in training
on N ? 1 examples and testing on the single held-out example.
This process is repeated for all instances.
Kernel Space Prec. Rec. F1
RBC 28.27 70.59 40.38
BOW 47.67 46.73 47.20
WSK 47.11 50.65 48.82
STK-CT 50.51 32.35 39.44
PTK-CT 47.84 57.84 52.37
PTK-DT 44.81 57.84 50.50
PASS 33.50 21.90 26.49
PSK 39.88 45.10 42.33
CSK 39.07 77.12 51.86
Table 1: Kernel performance using leave-one-out cross-
validation.
that: first, RBC has good Recall but poor Precision.
This is interesting since, on one hand, these results
validate the complexity of the task: in order to cap-
ture the large variability of the positive examples,
the rules developed by a skilled human designer are
unable to be sufficiently precise to limit the recog-
nition to those examples. On the other hand, RBC,
being a rather different approach from SVMs, can be
successfully exploited in a joint model with them.
Second, BOW yields better F1 than RBC but it
does not generalize well since its F1 is still low.
When n-grams are also added to the model by
means of WSK, the F1 improves by about 1.5 ab-
solute points. As already shown in (Zhang and Lee,
2003; Moschitti et al, 2007), syntactic structures are
needed to improve generalization.
Third, surprisingly with respect to previous work,
STK applied to CT8 provides accuracy lower than
BOW, about 8 absolute points. The reason is due to
the different nature of the Jeopardy! questions: large
syntactic variability reduces the probability of find-
ing general and well formed patterns, i.e. structures
generated by entire production rules. This suggests
that PTK, which can capture patterns derived from
partial production rules, can be more effective. In-
deed, PTK-CT achieves the highest F1, outperform-
ing WSK also when used with a different syntactic
paradigm, i.e. PTK-DT.
Next, PSK and PASS provide a lower accuracy
but they may be useful in kernel combinations as
they can complement the information captured by
the other models. Interestingly, CSK alone is rather
effective for classifying definition questions. We be-
8Applying it to DT does not make much sense as already
pointed out in (Moschitti, 2006).
717
?Figure 5: Similarity according to PTK and STK
lieve this is because definition questions are some-
times clustered into categories such as 4-LETTER
WORDS or BEGINS WITH ?B?.
Moreover, we carried out qualitative error analy-
sis on the PTK and STK outcome, which supported
our initial hypothesis. Let us consider the bottom
tree in Figure 5 in the training set. The top tree is
a test example correctly classified by PTK but in-
correctly classified by STK. The dashed line in the
top tree contains the largest subtree matched by PTK
(against the bottom tree), whereas the dashed line in
the bottom tree indicates the largest subtree matched
by STK (against the top tree). As the figure shows,
PTK can exploit a larger number of partial patterns.
Finally, the above points suggest that different
kernels produce complementary information. It is
thus promising to experiment with their combina-
tions. The joint models can be simply built by
summing kernel functions together. The results are
shown in Table 2. We note that: (i) CSK comple-
ments the WSK information, achieving a substan-
tially better result, i.e. 62.95; (ii) PTK-CT+CSK
performs even better than WSK+CSK (as PTK out-
performs WSK); and (iii) adding RBC improves
further on the above combinations, i.e. 68.11 and
67.32, respectively. This evidently demonstrates
that RBC captures complementary information. Fi-
nally, more complex kernels, especially the overall
kernel summation, do not seem to improve the per-
Kernel Space Prec. Rec. F1
WSK+CSK 70.00 57.19 62.95
PTK-CT+CSK 69.43 60.13 64.45
PTK-CT+WSK+CSK 68.59 62.09 65.18
CSK+RBC 47.80 74.51 58.23
PTK-CT+CSK+RBC 59.33 74.84 65.79
BOW+CSK+RBC 60.65 73.53 66.47
PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32
PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56
WSK+CSK+RBC 69.26 66.99 68.11
ALL 61.42 67.65 64.38
Table 2: Performance of Kernel Combinations using
leave-one-out cross-validation.
formance. This is also confirmed by the PASS re-
sults derived in (Moschitti et al, 2007) on TREC
QC.
5 Experiments on the Jeopardy System
Since the kernel-based classifiers perform substan-
tially better than RBC, we incorporate the PTK-
CT+WSK+CSK model9 into Watson for definition
classification and evaluated the QA performance
against two baseline systems. For the end-to-end ex-
periments, we used Watson?s English Slot Grammar
parser (McCord, 1980) to generate the constituency
trees. The component level evaluation shows that
we achieved comparable performance as previously
discussed with ESG.
5.1 Experimental Setup
We integrated the classifier into the question analy-
sis module, and incorporated additional components
to search against dictionary resources and extract
candidate answers from these search results when a
question is classified as definitional. In the final ma-
chine learning models, a separate model is trained
for definition questions to enable scoring tailored to
the specific characteristics of those questions.
Based on our manually annotated gold standard,
less than 10% of Jeopardy! questions are classified
as definition questions. Due to their relatively low
frequency we conduct two types of evaluations. The
first is definition-only evaluation, in which we apply
our definition question classifier to identify a large
9Since we aim to compare a purely statistical approach to
the rule-based approach, we did not experiment with the model
that uses RBC as a feature in our end-to-end experiments.
718
set of definition questions and evaluate the end-to-
end system?s performance on this large set of ques-
tions. These results enable us to draw statistically
significant conclusions about our approach to ad-
dressing definition questions.
The second type of evaluation is game-based
evaluation, which assesses the impact of our defi-
nition question processing on Watson performance
while preserving the natural distribution of these
question types in Jeopardy! data. Game-based eval-
uations situate the system?s performance on defini-
tion questions relative to other types of questions,
and enable us to gauge the component?s contribu-
tions in a game-based setting.
For both evaluation settings, three configurations
of Watson are used as follows:
? the NoDef system, in which Watson is config-
ured without definition classification and pro-
cessing, thereby treating all definition ques-
tions as regular factoid questions;
? the StatDef system, which leverages the sta-
tistical classifier and subsequent definition spe-
cific search and candidate generation compo-
nents as described above; and
? the RuleDef system, in which Watson adopts
RBC and employs the same additional defini-
tion search and candidate generation compo-
nents as the StatDef system.
For the definition-only evaluation, we selected all
questions recognized as definitional by the statistical
classifier from roughly 1000 unseen games (60000
questions), resulting in a test set of 1606 questions.
Due to the size of the initial set, it is impractical to
manually create a gold standard for measuring Pre-
cision and Recall of the classifier. Instead, we com-
pare the StatDef system against the NoDef on these
1606 questions using two metrics: accuracy, defined
as the percentage of questions correctly answered,
and p@70, the system?s Precision when answering
only the top 70% most confident questions. P@70 is
an important metric in Jeopardy! game play as well
as in real world applications where the system may
refrain from answering a question when it is not con-
fident about any of its answers. Since RBC identifies
significantly more definition questions, we started
NoDef StatDef NoDef RuleDef
# Questions 1606 1606 1875 1875
Accuracy 63.76% 65.57% 56.64% 57.51%
P@70 82.22% 84.53% 72.73% 74.87%
Table 3: Definition-Only Evaluation Results
with an initial set of roughly 300 games, from which
the RBC identified 1875 questions as definitional.
We compared the RuleDef system?s performance on
these questions against the NoDef baseline using the
accuracy and p@70 metrics.
For the game-based evaluation, we randomly se-
lected 66 unseen Jeopardy! games, consisting of
3546 questions after excluding audio/visual ques-
tions.10 We contrast the StatDef system perfor-
mance against that of NoDef and RuleDef along
several dimensions: accuracy and p@70, described
above, as well as earnings, the average amount of
money earned for each game.
5.2 Definition-Only Evaluation
For the definition-only evaluation, we compared the
StatDef system against the NoDef system on a set of
1606 questions that the StatDef system classified as
definitional. The results are shown in the first two
columns in Table 3. To contrast the gain obtained
by the StatDef system against that achieved by the
RuleDef system, we ran the RuleDef system over
the 1875 questions identified as definitional by the
rule-based classifier. We contrast the RuleDef sys-
tem performance with that of the NoDef system, as
shown in the last two columns in Table 3.
Our results show that based on both evaluation
metrics, StatDef improved upon the NoDef baseline
more than RuleDef improved on the same baseline
system. Furthermore, for the accuracy metric where
all samples are paired and independent, the differ-
ence in performance between the StatDef and NoDef
systems is statistically significant at p<0.05, while
that between the RuleDef and NoDef systems is not.
5.3 Game-Based Evaluation
The game-based evaluation was carried out on 66
unseen games (roughly 3500 questions). Of these
10Audio/visual questions are those accompanied by either an
image or an audio clip. The text portions of these questions are
often insufficient for identifying the correct answers.
719
# Def Q?s Accuracy P@70 Earnings
NoDef 0 69.71% 86.79% $24,818
RuleDef 480 69.23% 86.31% $24,397
StatDef 131 69.85% 87.19% $25,109
Table 4: Game-Based Evaluation Results
questions, the StatDef system classified 131 of them
as definitional while the RuleDef system identified
480 definition questions. Both systems were com-
pared against the NoDef system using the accuracy,
p@70, and earnings metric computed over all ques-
tions, as shown in Table 4.
Our results show that even though in the
definition-only evaluation both the RuleDef and
StatDef systems outperformed the NoDef baseline,
in our game-based evaluation, the RuleDef system
performed worse than the NoDef baseline. The low-
ered performance is due to the fact that the Preci-
sion of the RBC is much lower than that of the sta-
tistical classifier, and the special definition process-
ing applied to questions that are erroneously clas-
sified as definitional was harmful. Our evaluation
of this false positive set showed that its accuracy
dropped by 6% compared to the NoDef system. On
the other hand, the StatDef system outperformed the
two other systems, and its accuracy improvement
upon the RuleDef system is statistically significant
at p<0.05.
6 Related Work
Our paper studies the use of advanced representa-
tion for QC in the Jeopardy! domain. As previously
mentioned Jeopardy! questions are stated as affir-
mative sentences, which are different from the typ-
ical QA questions. For the design of our models,
we have carefully taken into account previous work.
This shows that semantics and syntax are essential
to retrieve precise answers, e.g (Hickl et al, 2006;
Voorhees, 2004; Small et al, 2004).
We focus on definition questions, which typically
require more complex processing than factoid ques-
tions (Blair-Goldensohn et al, 2004; Chen et al,
2006; Shen and Lapata, 2007; Bilotti et al, 2007;
Moschitti et al, 2007; Surdeanu et al, 2008; Echi-
habi and Marcu, 2003). For example, language mod-
els were applied to definitional QA in (Cui et al,
2005) to learn soft pattern models based on bigrams.
Other related work, such as (Sasaki, 2005; Suzuki
et al, 2002), was also very tied to bag-of-words
features. Predicate argument structures have been
mainly used for reranking (Shen and Lapata, 2007;
Bilotti et al, 2007; Moschitti et al, 2007; Surdeanu
et al, 2008).
Our work and methods are similar to (Zhang and
Lee, 2003; Moschitti et al, 2007), which achieved
the state-of-the-art in QC by applying SVMs along
with STK-CT. The results were derived by experi-
menting with a TREC dataset11(Li and Roth, 2002),
reaching an accuracy of 91.8%. However, such data
refers to typical instances from QA, whose syntactic
patterns can be easily generalized by STK. In con-
trast, we have shown that STK-CT is not effective
for our domain, as it presents very innovative ele-
ments: questions in affirmative and highly variable
format. Thus, we employed new methods such as
PTK, dependency structures, multiple sequence ker-
nels including category information and many com-
binations.
Regarding the use of Kernel Methods, there is
a considerably large body of work in Natural Lan-
guage Processing, e.g. regarding syntactic parsing
(Collins and Duffy, 2002; Kudo et al, 2005; Shen
et al, 2003; Kudo and Matsumoto, 2003; Titov and
Henderson, 2006; Toutanova et al, 2004), named
entity recognition and chunking (Cumby and Roth,
2003; Daume? III and Marcu, 2004), relation extrac-
tion (Zelenko et al, 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005; Zhang et al,
2005; Bunescu, 2007; Nguyen et al, 2009a), text
categorization (Cancedda et al, 2003), word sense
disambiguation (Gliozzo et al, 2005) and seman-
tic role labeling (SRL), e.g. (Kazama and Torisawa,
2005; Che et al, 2006a; Moschitti et al, 2008).
However, ours is the first study on the use of sev-
eral combinations of kernels applied to several struc-
tures on very complex data from the Jeopardy! do-
main.
7 Final Remarks and Conclusion
In this paper we have experimented with advanced
structural kernels applied to several kinds of syntac-
tic/semantic linguistic structures for the classifica-
tion of questions in a new application domain, i.e.
Jeopardy!. Our findings are summarized hereafter:
11Available at http://cogcomp.cs.illinois.
edu/Data/QA/QC/
720
First, it should be noted that basic kernels, such
as STK, PTK and SK, when applied to new repre-
sentations, i.e. syntactic/semantic structures, con-
stitute new kernels. Thus structural representations
play a major role and, from this perspective, our pa-
per makes a significant contribution.
Second, the experimental results show that the
higher variability of Jeopardy! questions prevents us
from achieving generalization with typical syntactic
patterns even if they are derived by powerful meth-
ods such as STK. In contrast, partial patterns, such
as those provided by PTK applied to constituency
(or dependency) trees, prove to be effective.
In particular, STK has been considered as the best
kernel for exploiting syntactic information in con-
stituency trees, e.g. it is state-of-the-art in: QC
(Zhang and Lee, 2003; Moschitti et al, 2007; Mos-
chitti, 2008); SRL, (Moschitti et al, 2008; Mos-
chitti et al, 2005; Che et al, 2006b); pronominal
coreference resolution (Yang et al, 2006; Versley
et al, 2008) and Relation Extraction (Zhang et al,
2006; Nguyen et al, 2009b). We showed that, in
the complex domain of Jeopardy!, STK surprisingly
provides low accuracy whereas PTK is rather ef-
fective and greatly outperforms STK. We have also
provided an explanation of such behavior by means
of error analysis: in contrast with traditional ques-
tion classification, which focuses on basic syntactic
patterns (e.g. ?what?, ?where?, ?who? and ?how?).
Figure 5 shows that PTK captures partial patterns
that are important for more complex questions like
those in Jeopardy!
Third, we derived other interesting findings for
NLP related to this novel domain, e.g.: (i) the im-
pact of dependency trees is similar to the one of
constituency trees. (ii) A simple computational rep-
resentation of shallow semantics, i.e. PASS (Mos-
chitti, 2008), does not work in Jeopardy!. (iii) Se-
quence kernels on category cues, i.e., higher level of
lexical semantics, improve question classification.
(iv) RBC jointly used with statistical approaches is
helpful to tackle the Jeopardy! complexity.
Next, our kernel models improve up to 20 abso-
lute percent points over n-grams based approaches,
reaching a significant accuracy of about 70%. Wat-
son, exploiting such a classifier, improved previ-
ous versions using RBC and no definition classifica-
tion both in definition-only evaluations and in game-
based evaluations.
Finally, we point out that:
? Jeopardy! has a variety of different special ques-
tion types that are handled differently. We focus on
kernel methods for definition question for two rea-
sons. First, their recognition relies heavily on parse
structures and is therefore more amenable to the ap-
proach proposed in the paper than the recognition
of other question types. Second, definition is by far
the most frequent special question type in Jeopardy!;
therefore, we can obtain sufficient data for training
and testing.
? We were unable to address the whole QC prob-
lem using a statistical model due to the lack of suffi-
cient training data for most special question classes.
Furthermore, we focused only on the definition clas-
sification and its impact on system performance due
to space reasons.
? Our RBC has a rather imbalanced trade-off be-
tween Precision and Recall. This may not be the
best operating point, but the optimal point is diffi-
cult to obtain empirically for an RBC, which is a
strong motivation of the work in this paper. We ex-
perimented with tuning the trade-off between Preci-
sion and Recall with the RBC, but since RBC uses
hand-crafted rules and does not have a parameter for
that, ultimately the statistical approach proved more
effective.
In future work, we plan to extend the current re-
search by investigating models capable of exploit-
ing predicate argument structures for question clas-
sification and answer reranking. The use of syntac-
tic/semantic kernels is a promising research direc-
tion (Basili et al, 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). In this
perspective kernel learning is a very interesting re-
search line, considering the complexity of represen-
tation and classification problems in which our ker-
nels operate.
Acknowledgements
This work has been supported by the IBM?s Open
Collaboration Research (OCR) awards program. We
are deeply in debt with Richard Johansson, who pro-
duced the earlier syntactic/semantic representations
of the Jeopardy! questions from the text format.
721
References
Kisuh Ahn, Johan Bos, Stephen Clark, James R. Cur-
ran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B.
Smillie, and Bonnie Webber. 2004. Question an-
swering with qed and wee at trec-2004. In E. M.
Voorhees and L. P. Buckland, editors, The Thirteenth
Text REtrieval Conference, TREC 2004, pages 595?
599, Gaitersburg, MD.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1?8, Ann Arbor, Michigan. Association
for Computational Linguistics.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of ACM SIGIR.
S. Blair-Goldensohn, K. R. McKeown, and A. H.
Schlaikjer. 2004. Answering definitional questions:
A hybrid approach. In M. Maybury, editor, Proceed-
ings of AAAI 2004. AAAI Press.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear models.
In SIGIR ?06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 615?616, New
York, NY, USA. ACM.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006a. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 73?
80, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006b. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 73?80, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
Proceedings of ACL.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern
models for definitional QA. In Proceedings of SIGIR,
Salvador, Brazil. ACM.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proceedings of ACL.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Danilo Giampiccolo, Pamela Froner, Anselmo Pen?as,
Christelle Ayache, Dan Cristea, Valentin Jijkoun,
Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, and
Richard Suteliffe. 2007. Overview of the CLEF 2007
multilingual question anwering track. In Proceedings
of the Cross Language Evaluation Forum.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lcc chaucer
at trec 2006. In Proceedings of TREC.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ? Sup-
port Vector Learning, 13.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
722
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings of
the Workshop on Deep Linguistic Processing, DeepLP
?07, pages 41?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP?05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Shyong Lam, David Pennock, Dan Cosley, and Steve
Lawrence. 2003. 1 billion pages = 1 milllion dollars?
mining the web to pay ?who wants to be a millionaire?
In Proceedings of the 19th Conference on Uncertainty
in AI.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL.
Michael C. McCord. 1980. Slot grammars. Computa-
tional Linguistics.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31, Boston, United
States.
Alessandro Moschitti, Bonaventura Coppola, Ana-Maria
Giuglea, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In CoNLL 2005 shared task.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ?08, NY, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009a. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378?1387, Singapore, August.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009b. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1378?1387, Morristown,
NJ, USA. Association for Computational Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?105.
Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, and
Hsin-Hsi Chen. 2007. Overview of the NTCIR-6
cross-lingual question answering (CLQA) task. In
Proceedings of the 6th NTCIR Workshop on Evalua-
tion of Information Access Technologies.
Y. Sasaki. 2005. Question answering as question-biased
term extraction: A new approach toward multilingual
qa. In Proceedings of ACL, pages 215?222.
John Shawe-Taylor and Nello Cristianini. 2004. LaTeX
User?s Guide and Document Reference Manual. Ker-
nel Methods for Pattern Analysis, Cambridge Univer-
sity Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL.
L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In Proceedings of
EMNLP, Sapporo, Japan.
S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin,
N. Shimizu, P. Kantor, D. Kelly, and N. Wacholder.
2004. Hitiqa: Towards analytical question answering.
In Proceedings of COLING.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT, Columbus, Ohio.
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. Svm an-
swer selection for open-domain question answering.
In Proceedings of Coling, pages 974?980.
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku
Maeda. 2003. Question classification using hdag ker-
nel. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answering,
pages 61?68, Sapporo, Japan, July. Association for
Computational Linguistics.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing?08), Manchester, England.
723
Ellen M. Voorhees and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering track.
In Proceedings of the TREC 2005 Conference.
E. M. Voorhees. 2004. Overview of the trec 2004 ques-
tion answering track. In Proceedings of TREC 2004.
Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM Press.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP?2005, Lecture Notes in Computer Science (LNCS
3651), pages 378?389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
724
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?193,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
When Did that Happen? ? Linking Events and Relations to Timestamps
Dirk Hovy*, James Fan, Alfio Gliozzo, Siddharth Patwardhan and Chris Welty
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
dirkh@isi.edu, {fanj,gliozzo,siddharth,welty}@us.ibm.com
Abstract
We present work on linking events and flu-
ents (i.e., relations that hold for certain
periods of time) to temporal information
in text, which is an important enabler for
many applications such as timelines and
reasoning. Previous research has mainly
focused on temporal links for events, and
we extend that work to include fluents
as well, presenting a common methodol-
ogy for linking both events and relations
to timestamps within the same sentence.
Our approach combines tree kernels with
classical feature-based learning to exploit
context and achieves competitive F1-scores
on event-time linking, and comparable F1-
scores for fluents. Our best systems achieve
F1-scores of 0.76 on events and 0.72 on flu-
ents.
1 Introduction
It is a long-standing goal of NLP to process natu-
ral language content in such a way that machines
can effectively reason over the entities, relations,
and events discussed within that content. The ap-
plications of such technology are numerous, in-
cluding intelligence gathering, business analytics,
healthcare, education, etc. Indeed, the promise
of machine reading is actively driving research in
this area (Etzioni et al 2007; Barker et al 2007;
Clark and Harrison, 2010; Strassel et al 2010).
Temporal information is a crucial aspect of this
task. For a machine to successfully understand
natural language text, it must be able to associate
time points and temporal durations with relations
and events it discovers in text.
?The first author conducted this research during an in-
ternship at IBM Research.
In this paper we present methods to estab-
lish links between events (e.g. ?bombing? or
?election?) or fluents (e.g. ?spouseOf? or ?em-
ployedBy?) and temporal expressions (e.g. ?last
Tuesday? and ?November 2008?). While previ-
ous research has mainly focused on temporal links
for events only, we deal with both events and flu-
ents with the same method. For example, consider
the sentence below
Before his death in October, Steve Jobs
led Apple for 15 years.
For a machine reading system processing this
sentence, we would expect it to link the fluent
CEO of (Steve Jobs, Apple) to time duration ?15
years?. Similarly we expect it to link the event
?death? to the time expression ?October?.
We do not take a strong ?ontological? position
on what events and fluents are, as part of our
task these distinctions are made a priori. In other
words, events and fluents are input to our tempo-
ral linking framework. In the remainder of this pa-
per, we also do not make a strong distinction be-
tween relations in general and fluents in particu-
lar, and use them interchangeably, since our focus
is only on the specific types of relations that rep-
resent fluents. While we only use binary relations
in this work, there is nothing in the framework
that would prevent the use of n-ary relations. Our
work focuses on accurately identifying temporal
links for eventual use in a machine reading con-
text.
In this paper, we describe a single approach that
applies to both fluents and events, using feature
engineering as well as tree kernels. We show that
we can achieve good results for both events and
fluents using the same feature space, and advocate
185
the versatility of our approach by achieving com-
petitive results on yet another similar task with a
different data set.
Our approach requires us to capture contextual
properties of text surrounding events, fluents and
time expressions that enable an automatic system
to detect temporal linking within our framework.
A common strategy for this is to follow standard
feature engineering methodology and manually
develop features for a machine learning model
from the lexical, syntactic and semantic analysis
of the text. A key contribution of our work in this
paper is to demonstrate a shallow tree-like repre-
sentation of the text that enables us to employ tree
kernel models, and more accurately detect tempo-
ral linking. The feature space represented by such
tree kernels is far larger than a manually engi-
neered feature space, and is capable of capturing
the contextual information required for temporal
linking.
The remainder of this paper goes into the de-
tails of our approach for temporal linking, and
presents empirical evidence for the effectiveness
of our approach. The contributions of this paper
can be summarized as follows:
1. We define a common methodology to link
events and fluents to timestamps.
2. We use tree kernels in combination with clas-
sical feature-based approaches to obtain sig-
nificant gains by exploiting context.
3. Empirical evidence illustrates that our
framework for temporal linking is very ef-
fective for the task, achieving an F1-score of
0.76 on events and 0.72 on fluents/relations,
as well as 0.65 for TempEval2, approaching
state-of-the-art.
2 Related Work
Most of the previous work on relation extraction
focuses on entity-entity relations, such as in the
ACE (Doddington et al 2004) tasks. Temporal
relations are part of this, but to a lesser extent.
The primary research effort in event temporality
has gone into ordering events with respect to one
another (e.g., Chambers and Jurafsky (2008)), and
detecting their typical durations (e.g., Pan et al
(2006)).
Recently, TempEval workshops have focused
on the temporal related issues in NLP. Some of
the TempEval tasks overlap with ours in many
ways. Our task is similar to task A and C of
TempEval-1 (Verhagen et al 2007) in the sense
that we attempt to identify temporal relation be-
tween events and time expressions or document
dates. However, we do not use a restricted set of
events, but focus primarily on a single temporal
relation tlink instead of named relations like BE-
FORE, AFTER or OVERLAP (although we show
that we can incorporate these as well). Part of our
task is similar to task C of TempEval-2 (Verha-
gen et al 2010), determining the temporal rela-
tion between an event and a time expression in
the same sentence. In this paper, we do apply our
system to TempEval-2 data and compare our per-
formance to the participating systems.
Our work is similar to that of Boguraev and
Ando (2005), whose research only deals with
temporal links between events and time expres-
sions (and does not consider relations at all). They
employ a sequence tagging model with manual
feature engineering for the task and achieved
state-of-the-art results on Timebank (Pustejovsky
et al 2003) data. Our task is slightly different be-
cause we include relations in the temporal linking,
and our use of tree kernels enables us to explore a
wider feature space very quickly.
Filatova and Hovy (2001) also explore tempo-
ral linking with events, but do not assume that
events and time stamps have been provided by an
external process. They used a heuristics-based ap-
proach to assign temporal expressions to events
(also relying on the proximity as a base case).
They report accuracy of the assignment for the
correctly classified events, the best being 82.29%.
Our best event system achieves an accuracy of
84.83%. These numbers are difficult to compare,
however, since accuracy does not efficiently cap-
ture the performance of a system on a task with so
many negative examples.
Mirroshandel et al(2011) describe the use of
syntactic tree kernels for event-time links. Their
results on TempEval are comparable to ours. In
contrast to them, we found, though, that syntactic
tree kernels alone do not perform as well as using
several flat tree representations.
3 Problem Definition
The task of linking events and relations to time
stamps can be defined as the following: given a set
of expressions denoting events or relation men-
186
tions in a document, and a set of time expressions
in the same document, find all instances of the
tlink relation between elements of the two input
sets. The existence of a tlink(e, t) means that e,
which is an event or a relation mention, occurs
within the temporal context specified by the time
expression t.
Thus, our task can be cast as a binary rela-
tion classification task: for each possible pair
of (event/relation, time) in a document, decide
whether there exists a link between the two, and
if so, express it in the data.
In addition, we make these assumptions about
the data:
1. There does not exist a timestamp for ev-
ery event/relation in a document. Although
events and relations typically have temporal
context, it may not be explicitly stated in a
document.
2. Every event/relation has at most one time ex-
pression associated with it. This is a simpli-
fying assumption, which in the case of rela-
tions we explore as future work.
3. Each temporal expression can be linked to
one or more events or relations. Since mul-
tiple events or relations may happen for a
given time, it is safe to assume that each tem-
poral expression can be linked to more than
one event/relation.
In general, the events/relations and their associ-
ated timestamps may occur within the same sen-
tence or may occur across different sentences. In
this paper, we focus on our effort and our evalua-
tion on the same sentence linking task.
In order to solve the problem of temporal link-
ing completely, however, it will be important to
also address the links that hold between entities
across sentences. We estimate, based on our data
set, that across sentence links account for 41% of
all correct event-time pairs in a document. For flu-
ents, the ratio is much higher, more than 80% of
the correct fluent-time links are across sentences.
One of the main obstacles for our approach in the
cross-sentence case is the very low ratio of posi-
tive to negative instances (3 : 100) in the set of all
pairs in a document. Most pairs are not linked to
one another.
4 Temporal Linking Framework
As previously mentioned, we approach the tem-
poral linking problem as a classification task. In
the framework of classification, we refer to each
pair of (event/relation, temporal expression) oc-
curring within a sentence as an instance. The goal
is to devise a classifier that separates positive (i.e.,
linked) instances from negative ones, i.e., pairs
where there is no link between the event/relation
and the temporal expression in question. The lat-
ter case is far more frequent, so we have an inher-
ent bias toward negative examples in our data.1
Note that the basis of the positive and nega-
tive links is the context around the target terms.
It is impossible even for humans to determine the
existence of a link based only on the two terms
without their context. For instance, given just two
words (e.g., ?said? and ?yesterday?) there is no
way to tell if it is a positive or a negative example.
We need the context to decide.
Therefore, we base our classification models on
contextual features drawn from lexical and syn-
tactic analyses of the text surrounding the target
terms. For this, we first define a feature-based
approach, then we improve it by using tree ker-
nels. These two subsections, plus the treatment
of fluent relations, are the main contributions of
this paper. In all of this work, we employ SVM
classifiers (Vapnik, 1995) for machine learning.
4.1 Feature Engineering
A manual analysis of development data provided
several intuitions about the kinds of features that
would be useful in this task. Based on this anal-
ysis and with inspiration from previous work (cf.
Boguraev and Ando (2005)) we established three
categories of features whose description follows.
Features describing events or relations. We
check whether the event or relation is phrasal, a
verb, or noun, whether it is present tense, past
tense, or progressive, the type assigned to the
event/relation by the UIMA type system used for
processing, and whether it includes certain trig-
ger words, such as reporting verbs (?said?, ?re-
ported?, etc.).
1Initially, we employed an instance filtering method to
address this, which proved to be ineffective and was subse-
quently left out.
187
Features describing temporal expressions.
We check for the presence of certain trigger words
(last, next, old, numbers, etc.) and the type of
the expression (DURATION, TIME, or DATE) as
specified by the UIMA type system.
Features describing context. We also in-
clude syntactic/structural features, such as testing
whether the relation/event dominates the temporal
expression, which one comes first in the sentence
order, and whether either of them is dominated
by a separate verb, preposition, ?that? (which of-
ten indicates a subordinate sentence) or counter-
factual nouns or verbs (which would negate the
temporal link).
It is not surprising that some of the most in-
formative features (event comes before tempo-
ral expression, time is syntactic child of event)
are strongly correlated with the baselines. Less
salient features include the test for certain words
indicating the event is a noun, a verb, and if so
which tense it has and whether it is a reporting
verb.
4.2 Tree Kernel Engineering
We expect that there exist certain patterns be-
tween the entities of a temporal link, which mani-
fest on several levels: some on the lexical level,
others expressed by certain sequences of POS
tags, NE labels, or other representations. Kernels
provide a principled way of expanding the number
of dimensions in which we search for a decision
boundary, and allow us to easily model local se-
quences and patterns in a natural way (Giuliano et
al., 2009). While it is possible to define a space
in which we find a decision boundary that sepa-
rates positive and negative instances with manu-
ally engineered features, these features can hardly
capture the notion of context as well as those ex-
plored by a tree kernel.
Tree Kernels are a family of kernel functions
developed to compute the similarity between tree
structures by counting the number of subtrees
they have in common. This generates a high-
dimensional feature space that can be handled ef-
ficiently using dynamic programming techniques
(Shawe-Taylor and Christianini, 2004). For our
purposes we used an implementation of the Sub-
tree and Subset Tree (SST) (Moschitti, 2006).
The advantages of using tree kernels are
two-fold: thanks to an existing implementation
(SVMlight with tree kernels, Moschitti (2004)), it
is faster and easier than traditional feature engi-
neering. The tree structure also allows us to use
different levels of representations (POS, lemma,
etc.) and combine their contributions, while at the
same time taking into account the ordering of la-
bels. We use POS, lemma, semantic type, and a
representation that replaces each word with a con-
catenation of its features (capitalization, count-
able, abstract/concrete noun, etc.).
We developed a shallow tree representation that
captures the context of the target terms, without
encoding too much structure (which may prevent
generalization). In essence, our tree structure in-
duces behavior somewhat similar to a string ker-
nel. In addition, we can model the tasks by pro-
viding specific markup on the generated tree. For
example, in our experiment we used the labels
EVENT (or equivalently RELATION) and TIME-
STAMP to mark our target terms. In order to re-
duce the complexity of this comparison, we focus
on the substring between event/relation and time
stamp and the rest of the tree structure is trun-
cated.
Figure 1 illustrates an example of the structure
described so far for both lemmas and POS tags
(note that the lowest level of the tree contains tok-
enized items, so their number can differ form the
actual words, as in ?attorney general?). Similar
trees are produced for each level of representa-
tions used, and for each instance (i.e., pair of time
expressions and event/relation). If a sentence con-
tains more than one event/relation, we create sep-
arate trees for each of them, which differ in the po-
sition of the EVENT/RELATION marks (at level
1 of the tree).
The tree kernel implicitly expands this struc-
ture into a number of substructures allowing us
to capture sequential patterns in the data. As we
will see, this step provides significant boosts to
the task performance.
Curiously, using a full-parse syntactic tree as
input representation did not help performance.
This is in line with our finding that syntactic re-
lations are less important than sequential patterns
(see also Section 5.2). Therefore we adopted the
?string kernel like? representation illustrated in
Figure 1.
188
Scores of supporters of detained Egyptian opposition leader Nur demonstrated outside the attorney general?s
office in Cairo last Saturday, demanding he be freed immediately.
BOW
TIME
TOK
saturday
TOK
last
TERM
TOK
cairo
TERM
TOK
in
TERM
TOK
office
TERM
TOK
attorney general
TERM
TOK
outside
EVENT
TOK
demonstrate
BOP
TIME
TOK
NNP
TOK
JJ
TERM
TOK
NNP
TERM
TOK
IN
TERM
TOK
NN
TERM
TOK
NNP
TERM
TOK
ADV
EVENT
TOK
VBD
Figure 1: Input Sentence and Tree Kernel Representations for Bag of Words (BOW) and POS tags (BOP)
5 Evaluation
We now apply our models to real world data, and
empirically demonstrate their effectiveness at the
task of temporal linking. In this section, we de-
scribe the data sets that were used for evaluation,
the baselines for comparison, parameter settings,
and the results of the experiments.
5.1 Benchmark
We evaluated our approach in 3 different tasks:
1. Linking Timestamps and Events in the IC
domain
2. Linking Timestamps and Relations in the IC
domain
3. Linking Events to Temporal Expressions
(TempEval-2, task C)
The first two data sets contained annotations
in the intelligence community (IC) domain, i.e.,
mainly news reports about terrorism. It com-
prised 169 documents. This dataset has been de-
veloped in the context of the machine reading pro-
gram (MRP) (Strassel et al 2010). In both cases
our goal is to develop a binary classifier to judge
whether the event (or relation) overlaps with the
time interval denoted by the timestamp. Success
of this classification can be measured by precision
and recall on annotated data.
We originally considered using accuracy as a
measure of performance, but this does not cor-
rectly reflect the true performance of the system:
given the skewed nature of the data (much smaller
number of positive examples), we could achieve a
high accuracy simply by classifying all instances
as negative, i.e., not assigning a time stamp at all.
We thus decided to report precision, recall and F1.
Unless stated otherwise, results were achieved via
10-fold cross-validation (10-CV).
The number of instances (i.e., pairs of event
and temporal expression) for each of the differ-
ent cases listed above was (in brackets the ratio of
positive to negative instances).
? events: 2046 (505 positive, 1541 negative)
? relations: 6526 (1847 positive, 4679 nega-
tive)
The size of the relation data set after filtering is
5511 (1847 positive, 3395 negative).
In order to increase the originally lower number
of event instances, we made use of the annotated
event-coreference as a sort of closure to add more
instances: if events A and B corefer, and there
is a link between A and time expression t, then
there is also a link between B and t. This was not
explicitly expressed in the data.
For the task at hand, we used gold standard
annotations for timestamps, events and relations.
The task was thus not the identification of these
objects (a necessary precursor and a difficult task
in itself), but the decision as to which events and
time expressions could and should be linked.
We also evaluated our system on TempEval-
2 (Verhagen et al 2010) for better comparison
189
to the state-of-the-art. TempEval-2 data included
the task of linking events to temporal expressions
(there called ?task C?), using several link types
(OVERLAP, BEFORE, AFTER, BEFORE-OR-
OVERLAP, OVERLAP-OR-AFTER). This is a
bit different from our settings as it required the
implementation of a multi-class classifier. There-
fore we trained three different binary classifiers
(using the same feature set) for the first three of
those types (for which there was sufficient train-
ing data) and we used a one-versus-all strategy to
distinguish positive from negative examples. The
output of the system is the category with the high-
est SVM decision score. Since we only use three
labels, we incur an error every time the gold la-
bel is something else. Note that this is stricter
than the evaluation in the actual task, which left
contestants with the option of skipping examples
their systems could not classify.
5.2 Baselines
Intuitively, one would expect temporal expres-
sions to be close to the event they denote, or even
syntactically related. In order to test this, we ap-
plied two baselines. In the first, each temporal ex-
pression was linked to the closest event (as mea-
sured in token distance). In the second, we at-
tached each temporal expression to its syntactic
head, if the head was an event. Results are re-
ported in Figure 2.
While these results are encouraging for our
task, it seems at first counter-intuitive that the
syntactic baseline does worse than the proximity-
based one. It does, however, reveal two facts:
events are not always synonymous with syntactic
units, and they are not always bound to tempo-
ral expressions through direct syntactic links. The
latter makes even more sense given that the links
can even occur across sentence boundaries. Pars-
ing quality could play a role, yet seems far fetched
to account for the difference.
More important than syntactic relations seem
to be sequential patterns on different levels, a fact
we exploit with the different tree representations
used (POS tags, NE types, etc.).
For relations, we only applied the closest-
relation baseline. Since relations consist of two or
more arguments that occur in different, often sep-
arated syntactic constituents, a syntactic approach
seems futile, especially given our experience with
events. Results are reported in Figure 3.
baseline comparison
Page 1
Precision Recall F1
0
20
40
60
80
100
35.0
63.0
45.048.0
88.0
62.063.0
75.4 68.376.6 76.5 76.2
Evaluation Measures Events
BL-parent BL-closest features +tree kernel
metric
%
Figure 2: Performance on events
System Accuracy
TRIOS 65%
this work 64.5%
JU-CSE, NCSU-indi
TRIPS, USFD2
all 63%
Table 1: Comparison to Best Systems in TempEval-2
5.3 Events
Figure 2 shows the improvements of the feature-
based approach over the two baseline, and the ad-
ditional gain obtained by using the tree kernel.
Both the features and tree kernels mainly improve
precision, while the tree kernel adds a small boost
in recall. It is remarkable, though, that the closest-
event baseline has a very high recall value. This
suggests that most of the links actually do occur
between items that are close to one another. For a
possible explanation for the low precision value,
see the error analysis (Section 5.5).
Using a two-tailed t-test, we compute the sig-
nificance in the difference between the F1-scores.
Both the feature-based and the tree kernel ap-
proach improvements are statistically significant
at p < 0.001 over the baseline scores.
Table 1 compares the performances of our sys-
tem to the state-of-the-art systems on TempEval-2
Data, task C, showing that our approach is very
competitive. The best systems there used sequen-
tial models. We attribute the competitive nature
of our results to the use of tree kernels, which en-
ables us to make use of contextual information.
5.4 Relations
In general, performance for relations is not as high
as for events (see Figure 3). The reason here is
two-fold: relations consist of two (or more) ele-
ments, which can be in various positions with re-
spect to one another and the temporal expression,
and each relation can be expressed in a number of
190
baseline comparison
Page 1
Precision Recall F1
0
10
20
30
40
50
60
70
80
90
100
35.0
24.0 29.0
63.1
80.6
70.470.8 74.0 72.2
Evaluation Metric Relations
BL-closest features +tree kernel
metric
%
Figure 3: Performance on relations/fluents
learning curves
Page 1
0 10 20 30 40 50 60 70 80 90 100
40
45
50
55
60
65
70
75
80
Learning Curves Relations
features w/ tree 
kernel
% of data
F1
 sc
or
e
Figure 4: Learning curves for relation models
different ways.
Again, we perform significance tests on the dif-
ference in F1 scores and find that our improve-
ments over the baseline are statistically significant
at p < 0.001. The improvement of the tree kernel
over the feature-based approach, however, are not
statistically significant at the same value.
The learning curve over parts of the training
data (exemplary shown here for relations, Figure
4)2 indicates that there is another advantage to us-
ing tree kernels: the approach can benefit from
more data. This is conceivably because it allows
the kernel to find more common subtrees in the
various representations the more examples it gets,
while the feature space rather finds more instances
that invalidate the expressiveness of features (i.e.,
it encounters positive and negative instances that
have very similar feature vectors). The curve sug-
gests that tree kernels could yield even better re-
sults with more data, while there is little to no ex-
pected gain using only features.
5.5 Error Analysis
Examining the misclassified examples in our data,
we find that both feature-based and tree-kernel
approaches struggle to correctly classify exam-
2The learning curve for events looks similar and is omit-
ted due to space constraints.
ples where time expression and event/relation are
immediately adjacent, but unrelated, as in ?the
man arrested last Tuesday told the police ...?,
where last Tuesday modifies arrested. It limits
the amount of context that is available to the tree
kernels, since we truncate the tree representations
to the words between those two elements. This
case closely resembles the problem we see in the
closest-event/relation baseline, which, as we have
seen, does not perform too well. In this case, the
incorrect event (?told?) is as close to the time ex-
pression as the correct one (?arrested?), resulting
in a false positive that affects precision. Features
capturing the order of the elements do not seem
help here, since the elements can be arranged in
any order (i.e., temporal expression before or af-
ter the event/relation). The only way to solve this
problem would be to include additional informa-
tion about whether a time expression is already
attached to another event/relation.
5.6 Ablations
To quantify the utility of each tree representation,
we also performed all-but-one ablation tests, i.e.,
left out each of the tree representations in turn, ran
10-fold cross-validation on the data and observed
the effect on F1. The larger the loss in F1, the
more informative the left-out-representation. We
performed ablations for both events and relations,
and found that the ranking of the representations
is the same for both.
In events and relations alike, leaving out POS
trees has the greatest effect on F1, followed by
the feature-bundle representation. Lemma and se-
mantic type representation have less of an impact.
We hypothesize that the former two capture un-
derlying regularities better by representing differ-
ent words with the same label. Lemmas in turn
are too numerous to form many recurring pat-
terns, and semantic type, while having a smaller
label alphabet, does not assign a label to every
word, thus creating a very sparse representation
that picks up more noise than signal.
In preliminary tests, we also used annotated
dependency trees as input to the tree kernel, but
found that performance improved when they were
left out. This is at odds with work that clearly
showed the value of syntactic tree kernels (Mir-
roshandel et al 2011). We identify two poten-
tial causes?either our setup was not capable of
correctly capturing and exploiting the information
191
from the dependency trees, or our formulation of
the task was not amenable to it. We did not inves-
tigate this further, but leave it to future work.
6 Conclusion and Future Work
We cast the problem of linking events and rela-
tions to temporal expressions as a classification
task using a combination of features and tree ker-
nels, with probabilistic type filtering. Our main
contributions are:
? We showed that within-sentence temporal
links for both events and relations can be ap-
proached with a common strategy.
? We developed flat tree representations and
showed that these produce considerable
gains, with significant improvements over
different baselines.
? We applied our technique without great ad-
justments to an existing data set and achieved
competitive results.
? Our best systems achieve F1 score of 0.76
on events and 0.72 on relations, and are ef-
fective at the task of temporal linking.
We developed the models as part of a machine
reading system and are currently evaluating it in
an end-to-end task.
Following tasks proposed in TempEval-2, we
plan to use our approach for across-sentence clas-
sification, as well as a similar model for linking
entities to the document creation date.
Acknowledgements
We would like to thank Alessandro Moschitti for
his help with the tree kernel setup, and the review-
ers who supplied us with very constructive feed-
back. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA
Machine Reading Program.
References
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw,
James Fan, Noah Friedland, Michael Glass, Jerry
Hobbs, Eduard Hovy, David Israel, Doo Soon Kim,
Rutu Mulkar-Mehta, Sourabh Patwardhan, Bruce
Porter, Dan Tecuci, and Peter Yeh. 2007. Learn-
ing by reading: A prototype system, performance
baseline and lessons learned. In Proceedings of
the 22nd National Conference for Artificial Intelli-
gence, Vancouver, Canada, July.
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal rea-
soning. In Proceedings of IJCAI, volume 5, pages
997?1003. IJCAI.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. pages
789?797. Association for Computational Linguis-
tics.
Peter Clark and Phil Harrison. 2010. Machine read-
ing as a process of partial question-answering. In
Proceedings of the NAACL HLT Workshop on For-
malisms and Methodology for Learning by Reading,
Los Angeles, CA, June.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion program ? tasks, data and evaluation. In Pro-
ceedings of the LREC Conference, Canary Islands,
Spain, July.
Oren Etzioni, Michele Banko, and Michael Cafarella.
2007. Machine reading. In Proceedings of the
AAAI Spring Symposium Series, Stanford, CA,
March.
Elena Filatova and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Proceedings of
the workshop on Temporal and spatial information
processing, volume 13, pages 1?8. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and
Carlo Strapparava. 2009. Kernel methods for min-
imally supervised wsd. Computational Linguistics,
35(4).
Seyed A. Mirroshandel, Mahdy Khayyamian, and
Gholamreza Ghassem-Sani. 2011. Syntactic tree
kernels for event-time temporal relation learning.
Human Language Technology. Challenges for Com-
puter Science and Linguistics, pages 213?223.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 335?es. Associa-
tion for Computational Linguistics.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL, volume 6.
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs. 2006.
Learning event durations from event descriptions.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 393?400. Association for Computa-
tional Linguistics.
James Pustejovsky, Patrick Hanks, Roser Saur?, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
192
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics
2003, pages 647?656.
John Shawe-Taylor and Nello Christianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA Machine Read-
ing Program-Encouraging Linguistic and Reason-
ing Research with a Series of Reading Tasks. In
Proceedings of LREC 2010.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Puste-
jovsky. 2007. Semeval-2007 task 15: Tempeval
temporal relation identification. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 75?80. Association for Computational
Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
193
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 85?92,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Technologies in IBM WatsonTM
Alfio Gliozzo
IBM Watson Research Center
Yorktown Heights, NY 10598
gliozzo@us.ibm.com
Or Biran
Columbia University
New York, NY 10027
orb@cs.columbia.edu
Siddharth Patwardhan
IBM Watson Research Center
Yorktown Heights, NY 10598
siddharth@us.ibm.com
Kathleen McKeown
Columbia University
New York, NY 10027
kathy@cs.columbia.edu
Abstract
This paper describes a seminar course de-
signed by IBM and Columbia University
on the topic of Semantic Technologies,
in particular as used in IBM WatsonTM
? a large scale Question Answering sys-
tem which famously won at Jeopardy! R?
against two human grand champions. It
was first offered at Columbia University
during the 2013 spring semester, and will
be offered at other institutions starting in
the fall semester. We describe the course?s
first successful run and its unique features:
a class centered around a specific indus-
trial technology; a large-scale class project
which student teams can choose to par-
ticipate in and which serves as the ba-
sis for an open source project that will
continue to grow each time the course is
offered; publishable papers, demos and
start-up ideas; evidence that the course can
be self-evaluating, which makes it poten-
tially appropriate for an online setting; and
a unique model where a large company
trains instructors and contributes to creat-
ing educational material at no charge to
qualifying institutions.
1 Introduction
In 2007, IBM Research took on the grand chal-
lenge of building a computer system that can per-
form well enough on open-domain question an-
swering to compete with champions at the game of
Jeopardy! In 2011, the open-domain question an-
swering system dubbed Watson beat the two high-
est ranked players in a two-game Jeopardy! match.
To be successful at Jeopardy!, players must re-
tain enormous amounts of information, must have
strong language skills, must be able to understand
precisely what is being asked, and must accurately
determine the likelihood they know the right an-
swer. Over a four year period, the team at IBM
developed the Watson system that competed on
Jeopardy! and the underlying DeepQA question
answering technology (Ferrucci et al, 2010). Wat-
son played many games of Jeopardy! against cel-
ebrated Jeopardy! champions and, in games tele-
vised in February 2011, won against the greatest
players of all time, Ken Jennings and Brad Rutter.
DeepQA has applications well beyond Jeop-
ardy!, however. DeepQA is a software architec-
ture for analyzing natural language content in both
questions and knowledge sources. DeepQA dis-
covers and evaluates potential answers and gathers
and scores evidence for those answers in both un-
structured sources, such as natural language doc-
uments, and structured sources such as relational
databases and knowledge bases. Figure 1 presents
a high-level view of the DeepQA architecture.
DeepQA utilizes a massively parallel, component-
based pipeline architecture (Ferrucci, 2012) which
uses an extensible set of structured and unstruc-
tured content sources as well as a broad range of
pluggable search and scoring components that al-
low integration of many different analytic tech-
niques. Machine Learning techniques are used to
learn the weights for each scoring component in
order to combine them into a single final score.
Watson components include a large variety of state
of the art solutions originating in the fields of Nat-
ural Language Processing (NLP), Machine Learn-
ing (ML), Information Retrieval (IR), Semantic
Web and Cloud Computing. IBM is now aggres-
sively investing in turning IBM Watson from a re-
search prototype to an industry level highly adapt-
able system to be applied in dozens of business ap-
85
Figure 1: Overview of the DeepQA architecture
plications ranging from healthcare to finance (Fer-
rucci et al, 2012).
Finding that particular combination of skills in
the entry-level job market is hard: in many cases
students have some notion of Machine Learning
but are not strong in Natural Language Processing;
in other cases they have background in Knowledge
Management and some of the basics of Semantic
Web, but lack an understanding of statistical mod-
els and Machine Learning. In most cases semantic
integration is not a topic of interest, and so un-
derstanding sophisticated platforms like Apache
UIMATM (Ferrucci and Lally, 2004) is a chal-
lenge. Learning how to develop the large scale in-
frastructure and technology needed for IBM Wat-
son prepares students for the real-world challenges
of large-scale natural language projects that are
common in industry settings and which students
have little experience with before graduation.
Of course, IBM is interested in hiring entry-
level students as a powerful way of scaling Wat-
son. Therefore, it has resolved to start an ed-
ucational program focused on these topics. Ini-
tially, tutorials were given at scientific conferences
(NAACL, ISWC and WWW, among others), uni-
versities and summer schools. The great number
of attendees (usually in the range of 50 to 150)
and strongly positive feedback received from the
students was a motivation to transform the didac-
tic material collected so far into a full graduate-
level course, which has been offered for the first
time at Columbia University. The course (which
is described in the rest of this paper) received very
positive evaluations from the students and will be
used as a template to be replicated by other part-
ner universities in the following year. Our ultimate
goal is to develop high quality didactic material
for an educational curriculum that can be used by
interested universities and professors all over the
world.
2 Syllabus and Didactic Material
The syllabus1 is divided equally between classes
specifically on the Watson system, its architec-
ture and technologies used within it, and classes
on more general topics that are relevant to these
technologies. In particular, background classes on
Natural Language Processing; Distributional Se-
mantics; the Semantic Web; Domain Adaptation
and the UIMA framework are essential for under-
standing the Watson system and producing suc-
cessful projects.
The course at Columbia included four lectures
by distinguished guest speakers from IBM, which
were advertised to the general Columbia commu-
nity as open talks. Instead of exams, the course
included two workshop-style presentation days:
one at the mid term and another at the end of the
1The syllabus is accessible on line http://www.
columbia.edu/?ag3366
86
course. During these workshops, all student teams
gave presentations on their various projects. At the
mid-term workshop, teams presented their project
idea and timeline, as well as related work and the
state-of-the-art of the field. At the final workshop,
they presented their completed projects, final re-
sults and demos. This workshop was also made
open to the Columbia community and in particu-
lar to faculty and affiliates interested in start-ups.
The workshops will be discussed in further detail
in the following sections. The syllabus is briefly
detailed here.
? Introduction: The Jeopardy! Challenge
The motivation behind Watson, the task and
its challenges (Prager et al, 2012; Tesauro et
al., 2012; Lewis, 2012).
? The DeepQA Architecture Chu-Carroll et
al. (2012b), Ferrucci (2012), Chu-Carroll et
al. (2012a), Lally et al (2012).
? Natural Language Processing Background
Pre-processing, tokenization, POS tagging,
named entity recognition, syntactic parsing,
semantic role labeling, word sense disam-
biguation, evaluation best practices and met-
rics.
? Natural Language Processing in Watson
Murdock et al (2012a), McCord et al (2012).
? Structured Knowledge in Watson Murdock
et al (2012b), Kalyanpur et al (2012), Fan et
al. (2012).
? Semantic Web OWL, RDF, Semantic Web
resources.
? Domain Adaptation Ferrucci et al (2012).
? UIMA The UIMA framework, Annotators,
Types, Descriptors, tools. Hands-on exercise
with the class project architecture (Epstein et
al., 2012).
? Midterm Workshop Presentations of each
team?s project idea and their research into re-
lated work and the state of the art.
? Distributional Semantics Miller et al
(2012), Gliozzo and Isabella (2005).
? Machine Learning and Strategy in Watson
? What Watson Tells Us About Cognitive
Computing
? Final Workshop Presentations of each
team?s final project implementation, evalua-
tion, demo and future plans.
3 Watson-like Architecture for Projects
The goal of the class projects was for the stu-
dents to learn to design and develop language tech-
nology components in an environment very sim-
ilar to IBM?s Watson architecture. We provided
the students with a plug-in framework for seman-
tic search, into which they could integrate their
project code. Student projects will be described
in the following section. This section details the
framework that was made available to the students
in order to develop their projects.
Like the Watson system, the project framework
for this class was built on top of Apache UIMA
(Ferrucci and Lally, 2004)2 ? an open-source
software architecture for building applications that
handle unstructured information.
The Watson system makes extensive use of
UIMA to enable interoperability and scale-out of a
large question answering system. The architecture
(viz., DeepQA) of Watson (Ferrucci, 2012) defines
several high-level ?stages? of analysis in the pro-
cessing pipeline, such as Question and Topic Anal-
ysis, Primary Search, Candidate Answer Genera-
tion, etc. Segmentation of the system into high-
level stages enabled a group of 25 researchers at
IBM to independently work on different aspects
of the system with little overhead for interoper-
ability and system integration. Each stage of the
pipeline clearly defined the inputs and outputs ex-
pected of components developed for that particu-
lar stage. The researchers needed only to adhere
to these input/output requirements for their indi-
vidual components to easily integrate them into
the system. Furthermore, the high-level stages in
Watson, enabled massive scale-out of the system
through the use of the asynchronous scaleout ca-
pability of UIMA-AS.
Using the Watson architecture for inspitration,
we developed a semantic search framework for the
class projects. As shown in Figure 2, the frame-
work consists of a UIMA pipeline that has several
high-level stages (similar to those of the Watson
system):
2http://uima.apache.org
87
Figure 2: Overview of the class project framework
1. Query Analysis
2. Primary Document Search
3. Structured Data Search
4. Query Expansion
5. Expanded Query Analysis
6. Secondary Document Search
The input to this system is provided by a Query
Collection Reader, which reads a list of search
queries from a text file. The Query Collec-
tion Reader is a UIMA ?collection reader? that
reads the text queries into memory data struc-
tures (UIMA CAS structures) ? one for each
text query. These UIMA CASes flow through the
pipeline and are processed by the various process-
ing stages. The processing stages are set up so
that new components designed to perform the task
of each processing stage can easily be added to the
pipeline (or existing components easily modified).
The expected inputs and outputs of components in
each processing stage are clearly defined, which
makes the task of the team building the component
simpler: they no longer have to deal with man-
aging data structures and are spared the overhead
of converting from and into formats of data ex-
changed between various components. All of the
overhead is handled by UIMA. Furthermore, some
of the processing stages generate new CAS struc-
tures and the flow of all the UIMA CAS structures
through this pipeline is controlled by a ?Flow Con-
troller? designed by us for this framework.
The framework was made available to each of
the student teams, and their task was to build
their project by extending this framework. Even
though we built the framework to perform seman-
tic search over a text corpus, many of the teams
in this course had projects that went far beyond
just semantic search. Our hope was that each team
would be able to able independently develop inter-
esting new components for the processing stages
of the pipeline, and at the end of the course we
would be able to merge the most interesting com-
ponents to create a single useful application. In the
following section, we describe the various projects
undertaken by the student teams in the class, while
Section 5 discusses the integration of components
from student projects and the demo application
that resulted from the integrated system.
4 Class Projects
Projects completed for this course fall into three
types: scientific projects, where the aim is to
produce a publishable paper; integrated projects,
where the aim is to create a component that will be
integrated into the class open-source project; and
independent demo projects, where the aim is to
produce an independent working demo/prototype.
The following section describes the integrated
projects briefly.
4.1 Selected Project Descriptions
As described in section 3, the integrated class
project is a system with an architecture which, al-
though greatly simplified, is reminiscent of Wat-
son?s. While originally intended to be simply a
semantic search tool, some of the student teams
created additional components which resulted in
a full question answering system. Those projects
88
as well as a few other related ones are described
below.
Question Categorization: Using the DBPedia
ontology (Bizer et al, 2009) as a semantic
type system, this project classifies questions
by their answer type. It can be seen as a sim-
plified version of the question categorization
system in Watson. The classification is based
on a simple bag-of-words approach with a
few additional features.
Answer Candidate Ranking: Given the answer
type as well as additional features derived by
the semantic search component, this project
uses regression to rank the candidate an-
swers which themselves come from semantic
search.
Twitter Semantic Search: Search in Twitter is
difficult due to the huge variations among
tweets in lexical terms, spelling and style, and
the limited length of the tweets. This project
employs LSA (Landauer and Dumais, 1997)
to cluster similar tweets and increase search
accuracy.
Fine-Grained NER in the Open Domain: This
project uses DBPedia?s ontology as a type
system for named entities of type Person.
Given results from a standard NER system,
it attempts to find the fine-grained classifica-
tion of each Person entity by finding the most
similar type. Similarity is computed using
traditional distributional methods, using the
context of the entity and the contexts of each
type, collected from Wikipedia.
News Frame Induction: Working with a large
corpus of news data collected by Columbia
Newsblaster, this team used the Machine
Linking API to tag entities with semantic
types. From there, they distributionally col-
lected ?frames? prevalent in the news do-
main such as ?[U.S President] meeting with
[British Prime Minister]?.
Other projects took on problems such as Sense
Induction, NER in the Biomedical domain, Se-
mantic Role Labeling, Semantic Video Search,
and a mobile app for Event Search.
5 System Integration and Demonstration
The UIMA-based architecture described in section
3 allows us to achieve a relatively easy integra-
tion of different class projects, independently de-
veloped by different teams, in a common archi-
tecture and expose their functionality with a com-
bined class project demo. The demo is a collab-
oratively developed semantic search engine which
is able to retrieve knowledge from structured data
and visualize it for the user in a very concise way.
The input is a query; it can be a natural language
question or simply a set of keywords. The output
is a set of entities and their relations, visualized
as an entity graph. Figure 3 shows the results of
the current status of our class project demo on the
following Jeopardy! question.
This nation of 200 million has fought
small independence movements like
those in Aceh and East Timor.
The output is a set of DBPedia entities related to
the question, grouped by Type (provided by the
DBPedia ontology). The correct answer, ?Indone-
sia?, is among the candidate entities of type Place.
Note that only answers of type Place and Agent
have been selected: this is due to the question cate-
gorization component, implemented by one of the
student teams, that allows us to restrict the gener-
ated answer set to those answers having the right
types.
The demo will be hosted for one year fol-
lowing the end of the course at http://
watsonclass.no-ip.biz. Our goal is to
incrementally improve this demo, leveraging any
new projects developed in future versions of the
course, and to build an open source software com-
munity involving students taking the course.
6 Evaluation
The course at Columbia drew a relatively large au-
dience. A typical size for a seminar course on a
special topic is estimated at 15-20 students, while
ours drew 35. The vast majority were Master?s stu-
dents; there were also three PhD students and five
undergraduates.
During the student workshops, students were
asked to provide grades for each team?s presen-
tation and project. After the instructor indepen-
dently gave his own grades, we looked at the cor-
relation between the average grades given by the
students and those give by the instructor. While
89
Figure 3: Screenshot of the project demo
90
Team 1 2 3 4 5 6 7 8 9 10 11
Instructor?s grade B+ B C+ A- B- A+ B B- B+ A B-
TA?s grade B+ B B A B- A B- B+ B+ A C+
Class? average grade B/B+ B+/A- B/B+ A- B/B+ A- B+ A-/A B+/A- A-/A B/B+
Table 1: Grades assigned to class projects
the students tended to be more ?generous? (their
average grade for each team was usually half a
grade above the instructor?s), the agreement was
quite high. Table 1 shows the grades given by the
instructor, the teaching assistant and the class av-
erage for the midterm workshop.
Feedback about the course from the students
was very good. Columbia provides electonic
course evaluations to the students which are com-
pletely optional. Participation in the evaluation for
this course was just under 50% in the midterm
evaluation and just over 50% in the final eval-
uation. The scores (all in the 0-5 range) given
by the students in relevant categories were quite
high: ?Overall Quality? got an average score of
4.23, ?Amount Learned? got 4, ?Appropriateness
of Workload? 4.33 and ?Fairness of Grading Pro-
cess? got 4.42.
The course resulted in multiple papers that are
or will soon be under submission, as well as a few
projects that may be developed into start-ups. Al-
most all student teams agreed to share their code
in an open source project that is currently being
set up, and which will include the current question
answering and semantic search system as well as
additional side projects.
7 Conclusion
We described a course on the topic of Semantic
Technologies and the IBM Watson system, which
features a diverse curriculum tied together by its
relevance to an exciting, demonstrably successful
real-world system. Through a combined architec-
ture inspired by Watson itself, the students get the
experience of developing an NLP-heavy compo-
nent with specifications mandated by the larger
architecture, which requires a combination of re-
search and software engineering skills that is com-
mon in the industry.
An exciting result of this course is that the
class project architecture and many of the student
projects are to be maintained as an open source
project which the students can, if they choose,
continue to be involved with. The repository and
community of this project can be expected to grow
each time the class is offered. Even after one class,
it already contains an impressive semantic search
system.
Feedback for this course from the students
was excellent, and many teams have achieved
their personal goals as stated at the beginning of
the semester, including paper submissions, opera-
tional web demos and mobile apps.
Our long term goal is to replicate this course in
multiple top universities around the world. While
IBM does not have enough resources to always
do this with its own researchers, it is instead go-
ing to provide the content material and the open
source code generated so far to other universities,
encouraging professors to teach the course them-
selves. Initially we will work on a pilot phase
involving only a restricted number of professors
and researchers that are already in collaboration
with IBM Research, and eventually (if the posi-
tive feedback we have seen so far is repeated in
the pilot phase) give access to the same content to
a larger group.
References
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,
R. Cyganiak, and S. Hellmann. 2009. DBpedia?
Crystallization Point for the Web of Data. Journal
of Web Semantics: Science, Services and Agents on
the World Wide Web, 7(3):154?165, September.
J. Chu-Carroll, J. Fan, B. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012a. Finding Nee-
dles in the Haystack: Search and Candidate Gener-
ation. IBM Journal of Research and Development,
56(3.4):6:1?6:12.
J. Chu-Carroll, J. Fan, N. Schlaefer, and W. Zadrozny.
2012b. Textual Resource Acquisition and Engineer-
ing. IBM Journal of Research and Development,
56(3.4):4:1?4:11.
E. Epstein, M. Schor, B. Iyer, A. Lally, E. Brown, and
J. Cwiklik. 2012. Making Watson Fast. IBM Jour-
nal of Research and Development, 56(3.4):15:1?
15:12.
J. Fan, A. Kalyanpur, D. Gondek, and D. Ferrucci.
2012. Automatic Knowledge Extraction from Doc-
uments. IBM Journal of Research and Development,
56(3.4):5:1?5:10.
91
D. Ferrucci and A. Lally. 2004. UIMA: an Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan,
D. Gondek, A. Kalyanpur, A. Lally, J. W. Murdock,
E. Nyberg, J. Prager, N. Schlaefer, and C. Welty.
2010. Building Watson: An Overview of the
DeepQA project. AI magazine, 31(3):59?79.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and
E. Mueller. 2012. Watson: Beyond Jeopardy. Arti-
ficial Intelligence (in press).
D. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3.4):1:1?1:15.
A. Gliozzo and T. Isabella. 2005. Semantic Domains
in Computational Linguistics. Technical report.
A. Kalyanpur, B. Boguraev, S. Patwardhan, J. W.
Murdock, A. Lally, C. Welty, J. Prager, B. Cop-
pola, A. Fokoue-Nkoutche, L. Zhang, Y. Pan, and
Z. Qiu. 2012. Structured Data and Inference in
DeepQA. IBM Journal of Research and Develop-
ment, 56(3.4):10:1?10:14.
A. Lally, J. Prager, M. McCord, B. Boguraev, S. Pat-
wardhan, J. Fan, P. Fodor, and J. Chu-Carroll. 2012.
Question Analysis: How Watson Reads a Clue. IBM
Journal of Research and Development, 56(3.4):2:1?
2:14.
T. Landauer and S. Dumais. 1997. A Solution to
Plato?s Problem: the Latent Semantic Analysis The-
ory of Acquisition, Induction and Representation
of Knowledge. Psychological Review, 104(2):211?
240.
B. Lewis. 2012. In the Game: The Interface between
Watson and Jeopardy! IBM Journal of Research and
Development, 56(3.4):17:1?17:6.
M. McCord, J. W. Murdock, and B. Boguraev. 2012.
Deep Parsing in Watson. IBM Journal of Research
and Development, 56(3.4):3:1?3:15.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych.
2012. Using Distributional Similarity for Lexical
Expansion in Knowledge-based Word Sense Disam-
biguation. In Proceedings of the International Con-
ference on Computational Linguistics, pages 1781?
1796, Mumbai, India, December.
J. W. Murdock, J. Fan, A. Lally, H. Shima, and
B. Boguraev. 2012a. Textual Evidence Gathering
and Analysis. IBM Journal of Research and Devel-
opment, 56(3.4):8:1?8:14.
J. W. Murdock, A. Kalyanpur, C. Welty, J. Fan, D. Fer-
rucci, D. Gondek, L. Zhang, and H. Kanayama.
2012b. Typing Candidate Answers Using Type Co-
ercion. IBM Journal of Research and Development,
56(3.4):7:1?7:13.
J. Prager, E. Brown, and J. Chu-Carroll. 2012. Spe-
cial Questions and Techniques. IBM Journal of Re-
search and Development, 56(3.4):11:1?11:13.
G. Tesauro, D. Gondek, J. Lenchner, J. Fan, and
J. Prager. 2012. Simulation, Learning, and Op-
timization Techniques in Watson?s Game Strate-
gies. IBM Journal of Research and Development,
56(3.4):16:1?16:11.
92
